<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.evalnlgeval">
  <volume id="1" ingest-date="2021-01-18">
    <meta>
      <booktitle>Proceedings of the 1st Workshop on Evaluating NLG Evaluation</booktitle>
      <editor><first>Shubham</first><last>Agarwal</last></editor>
      <editor><first>Ondřej</first><last>Dušek</last></editor>
      <editor><first>Sebastian</first><last>Gehrmann</last></editor>
      <editor><first>Dimitra</first><last>Gkatzia</last></editor>
      <editor><first>Ioannis</first><last>Konstas</last></editor>
      <editor><first>Emiel</first><last>Van Miltenburg</last></editor>
      <editor><first>Sashank</first><last>Santhanam</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online (Dublin, Ireland)</address>
      <month>December</month>
      <year>2020</year>
      <url hash="6419da1d">2020.evalnlgeval-1</url>
      <venue>evalnlgeval</venue>
    </meta>
    <frontmatter>
      <url hash="7c82f099">2020.evalnlgeval-1.0</url>
      <bibkey>evalnlgeval-2020-evaluating</bibkey>
    </frontmatter>
    <paper id="1">
      <title>A proof of concept on triangular test evaluation for Natural Language Generation</title>
      <author><first>Javier González</first><last>Corbelle</last></author>
      <author><first>José María Alonso</first><last>Moral</last></author>
      <author><first>Alberto Bugarín</first><last>Diz</last></author>
      <pages>1–9</pages>
      <abstract>The evaluation of Natural Language Generation (NLG) systems has recently aroused much interest in the research community, since it should address several challenging aspects, such as readability of the generated texts, adequacy to the user within a particular context and moment and linguistic quality-related issues (e.g., correctness, coherence, understandability), among others. In this paper, we propose a novel technique for evaluating NLG systems that is inspired on the triangular test used in the field of sensory analysis. This technique allows us to compare two texts generated by different subjects and to i) determine whether statistically significant differences are detected between them when evaluated by humans and ii) quantify to what extent the number of evaluators plays an important role in the sensitivity of the results. As a proof of concept, we apply this evaluation technique in a real use case in the field of meteorology, showing the advantages and disadvantages of our proposal.</abstract>
      <url hash="a899076d">2020.evalnlgeval-1.1</url>
      <bibkey>corbelle-etal-2020-proof</bibkey>
    </paper>
    <paper id="2">
      <title>“This is a Problem, Don’t You Agree?” Framing and Bias in Human Evaluation for Natural Language Generation</title>
      <author><first>Stephanie</first><last>Schoch</last></author>
      <author><first>Diyi</first><last>Yang</last></author>
      <author><first>Yangfeng</first><last>Ji</last></author>
      <pages>10–16</pages>
      <abstract>Despite recent efforts reviewing current human evaluation practices for natural language generation (NLG) research, the lack of reported question wording and potential for framing effects or cognitive biases influencing results has been widely overlooked. In this opinion paper, we detail three possible framing effects and cognitive biases that could be imposed on human evaluation in NLG. Based on this, we make a call for increased transparency for human evaluation in NLG and propose the concept of human evaluation statements. We make several recommendations for design details to report that could potentially influence results, such as question wording, and suggest that reporting pertinent design details can help increase comparability across studies as well as reproducibility of results.</abstract>
      <url hash="c40850fe">2020.evalnlgeval-1.2</url>
      <bibkey>schoch-etal-2020-problem</bibkey>
    </paper>
    <paper id="3">
      <title>Evaluation rules! On the use of grammars and rule-based systems for <fixed-case>NLG</fixed-case> evaluation</title>
      <author><first>Emiel</first><last>van Miltenburg</last></author>
      <author><first>Chris</first><last>van der Lee</last></author>
      <author><first>Thiago</first><last>Castro-Ferreira</last></author>
      <author><first>Emiel</first><last>Krahmer</last></author>
      <pages>17–27</pages>
      <abstract>NLG researchers often use uncontrolled corpora to train and evaluate their systems, using textual similarity metrics, such as BLEU. This position paper argues in favour of two alternative evaluation strategies, using grammars or rule-based systems. These strategies are particularly useful to identify the strengths and weaknesses of different systems. We contrast our proposals with the (extended) WebNLG dataset, which is revealed to have a skewed distribution of predicates. We predict that this distribution affects the quality of the predictions for systems trained on this data. However, this hypothesis can only be thoroughly tested (without any confounds) once we are able to systematically manipulate the skewness of the data, using a rule-based approach.</abstract>
      <url hash="7bd8ff34">2020.evalnlgeval-1.3</url>
      <bibkey>van-miltenburg-etal-2020-evaluation</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
    </paper>
    <paper id="4">
      <title><fixed-case>NUBIA</fixed-case>: <fixed-case>N</fixed-case>e<fixed-case>U</fixed-case>ral Based Interchangeability Assessor for Text Generation</title>
      <author><first>Hassan</first><last>Kane</last></author>
      <author><first>Muhammed Yusuf</first><last>Kocyigit</last></author>
      <author><first>Ali</first><last>Abdalla</last></author>
      <author><first>Pelkins</first><last>Ajanoh</last></author>
      <author><first>Mohamed</first><last>Coulibali</last></author>
      <pages>28–37</pages>
      <abstract>We present NUBIA, a methodology to build automatic evaluation metrics for text generation using only machine learning models as core components. A typical NUBIA model is composed of three modules: a neural feature extractor, an aggregator and a calibrator. We demonstrate an implementation of NUBIA showing competitive performance with stateof-the art metrics used to evaluate machine translation and state-of-the art results for image captions quality evaluation. In addition to strong performance, NUBIA models have the advantage of being modular and improve in synergy with advances in text generation models.</abstract>
      <url hash="7f3390b3">2020.evalnlgeval-1.4</url>
      <bibkey>kane-etal-2020-nubia</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
    <paper id="5">
      <title>On the interaction of automatic evaluation and task framing in headline style transfer</title>
      <author><first>Lorenzo De</first><last>Mattei</last></author>
      <author><first>Michele</first><last>Cafagna</last></author>
      <author><first>Huiyuan</first><last>Lai</last></author>
      <author><first>Felice</first><last>Dell’Orletta</last></author>
      <author><first>Malvina</first><last>Nissim</last></author>
      <author><first>Albert</first><last>Gatt</last></author>
      <pages>38–43</pages>
      <abstract>An ongoing debate in the NLG community concerns the best way to evaluate systems, with human evaluation often being considered the most reliable method, compared to corpus-based metrics. However, tasks involving subtle textual differences, such as style transfer, tend to be hard for humans to perform. In this paper, we propose an evaluation method for this task based on purposely-trained classifiers, showing that it better reflects system differences than traditional metrics such as BLEU.</abstract>
      <url hash="ef437ace">2020.evalnlgeval-1.5</url>
      <bibkey>mattei-etal-2020-interaction</bibkey>
      <pwccode url="https://github.com/michelecafagna26/CHANGE-IT" additional="false">michelecafagna26/CHANGE-IT</pwccode>
    </paper>
  </volume>
</collection>
