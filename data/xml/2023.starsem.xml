<?xml version='1.0' encoding='UTF-8'?>
<collection id="2023.starsem">
  <volume id="1" ingest-date="2023-07-21" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 12th Joint Conference on Lexical and Computational Semantics (*SEM 2023)</booktitle>
      <editor><first>Alexis</first><last>Palmer</last></editor>
      <editor><first>Jose</first><last>Camacho-collados</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Toronto, Canada</address>
      <month>July</month>
      <year>2023</year>
      <url hash="2a908a8b">2023.starsem-1</url>
      <venue>starsem</venue>
    </meta>
    <frontmatter>
      <url hash="dcac658b">2023.starsem-1.0</url>
      <bibkey>sem-2023-joint</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Including Facial Expressions in Contextual Embeddings for Sign Language Generation</title>
      <author><first>Carla</first><last>Viegas</last><affiliation>Carnegie Mellon University, NOVA University Lisbon</affiliation></author>
      <author><first>Mert</first><last>Inan</last><affiliation>University of Pittsburgh</affiliation></author>
      <author><first>Lorna</first><last>Quandt</last><affiliation>Gallaudet University</affiliation></author>
      <author><first>Malihe</first><last>Alikhani</last><affiliation>University of Pittsburgh</affiliation></author>
      <pages>1-10</pages>
      <abstract>State-of-the-art sign language generation frameworks lack expressivity and naturalness which is the result of only focusing manual signs, neglecting the affective, grammatical and semantic functions of facial expressions. The purpose of this work is to augment semantic representation of sign language through grounding facial expressions. We study the effect of modeling the relationship between text, gloss, and facial expressions on the performance of the sign generation systems. In particular, we propose a Dual Encoder Transformer able to generate manual signs as well as facial expressions by capturing the similarities and differences found in text and sign gloss annotation. We take into consideration the role of facial muscle activity to express intensities of manual signs by being the first to employ facial action units in sign language generation. We perform a series of experiments showing that our proposed model improves the quality of automatically generated sign language.</abstract>
      <url hash="a8e1cec1">2023.starsem-1.1</url>
      <bibkey>viegas-etal-2023-including</bibkey>
      <doi>10.18653/v1/2023.starsem-1.1</doi>
    </paper>
    <paper id="2">
      <title>Leverage Points in Modality Shifts: Comparing Language-only and Multimodal Word Representations</title>
      <author><first>Alexey</first><last>Tikhonov</last><affiliation>Inworld.AI</affiliation></author>
      <author><first>Lisa</first><last>Bylinina</last><affiliation>University of Groningen</affiliation></author>
      <author><first>Denis</first><last>Paperno</last><affiliation>Utrecht University</affiliation></author>
      <pages>11-17</pages>
      <abstract>Multimodal embeddings aim to enrich the semantic information in neural representations of language compared to text-only models. While different embeddings exhibit different applicability and performance on downstream tasks, little is known about the systematic representation differences attributed to the visual modality. Our paper compares word embeddings from three vision-and-language models (CLIP, OpenCLIP and Multilingual CLIP, Radford et al. 2021; Ilharco et al. 2021; Carlsson et al. 2022) and three text-only models, with static (FastText, Bojanowski et al. 2017) as well as contextual representations (multilingual BERT Devlin et al. 2018; XLM-RoBERTa, Conneau et al. 2019). This is the first large-scale study of the effect of visual grounding on language representations, including 46 semantic parameters. We identify meaning properties and relations that characterize words whose embeddings are most affected by the inclusion of visual modality in the training data; that is, points where visual grounding turns out most important. We find that the effect of visual modality correlates most with denotational semantic properties related to concreteness, but is also detected for several specific semantic classes, as well as for valence, a sentiment-related connotational property of linguistic expressions.</abstract>
      <url hash="7c2d0bf7">2023.starsem-1.2</url>
      <bibkey>bylinina-etal-2023-leverage</bibkey>
      <doi>10.18653/v1/2023.starsem-1.2</doi>
    </paper>
    <paper id="3">
      <title>Revisiting Syntax-Based Approach in Negation Scope Resolution</title>
      <author><first>Asahi</first><last>Yoshida</last><affiliation>Nagoya University</affiliation></author>
      <author><first>Yoshihide</first><last>Kato</last><affiliation>Nagoya University</affiliation></author>
      <author><first>Shigeki</first><last>Matsubara</last><affiliation>Nagoya University</affiliation></author>
      <pages>18-23</pages>
      <abstract>Negation scope resolution is the process of detecting the negated part of a sentence. Unlike the syntax-based approach employed in previous research, state-of-the-art methods performed better without the explicit use of syntactic structure. This work revisits the syntax-based approach and re-evaluates the effectiveness of syntactic structure in negation scope resolution. We replace the parser utilized in the prior works with state-of-the-art parsers and modify the syntax-based heuristic rules. The experimental results demonstrate that the simple modifications enhance the performance of the prior syntax-based method to the same level as state-of-the-art end-to-end neural-based methods.</abstract>
      <url hash="5754c689">2023.starsem-1.3</url>
      <bibkey>yoshida-etal-2023-revisiting</bibkey>
      <doi>10.18653/v1/2023.starsem-1.3</doi>
    </paper>
    <paper id="4">
      <title>When Truth Matters - Addressing Pragmatic Categories in Natural Language Inference (<fixed-case>NLI</fixed-case>) by Large Language Models (<fixed-case>LLM</fixed-case>s)</title>
      <author><first>Reto</first><last>Gubelmann</last><affiliation>University of St.Gallen</affiliation></author>
      <author><first>Aikaterini-lida</first><last>Kalouli</last><affiliation>Ludwig Maximilian University of Munich</affiliation></author>
      <author><first>Christina</first><last>Niklaus</last><affiliation>University of St. Gallen</affiliation></author>
      <author><first>Siegfried</first><last>Handschuh</last><affiliation>University of St. Gallen</affiliation></author>
      <pages>24-39</pages>
      <abstract>In this paper, we focus on the ability of large language models (LLMs) to accommodate different pragmatic sentence types, such as questions, commands, as well as sentence fragments for natural language inference (NLI). On the commonly used notion of logical inference, nothing can be inferred from a question, an order, or an incomprehensible sentence fragment. We find MNLI, arguably the most important NLI dataset, and hence models fine-tuned on this dataset, insensitive to this fact. Using a symbolic semantic parser, we develop and make publicly available, fine-tuning datasets designed specifically to address this issue, with promising results. We also make a first exploration of ChatGPT’s concept of entailment.</abstract>
      <url hash="f3b70045">2023.starsem-1.4</url>
      <attachment type="dataset" hash="a4bf6db4">2023.starsem-1.4.dataset.zip</attachment>
      <bibkey>gubelmann-etal-2023-truth</bibkey>
      <doi>10.18653/v1/2023.starsem-1.4</doi>
    </paper>
    <paper id="5">
      <title>Analyzing Syntactic Generalization Capacity of Pre-trained Language Models on <fixed-case>J</fixed-case>apanese Honorific Conversion</title>
      <author><first>Ryo</first><last>Sekizawa</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Hitomi</first><last>Yanaka</last><affiliation>The University of Tokyo</affiliation></author>
      <pages>40-47</pages>
      <abstract>Using Japanese honorifics is challenging because it requires not only knowledge of the grammatical rules but also contextual information, such as social relationships. It remains unclear whether pre-trained large language models (LLMs) can flexibly handle Japanese honorifics like humans. To analyze this, we introduce an honorific conversion task that considers social relationships among people mentioned in a conversation. We construct a Japanese honorifics dataset from problem templates of various sentence structures to investigate the syntactic generalization capacity of GPT-3, one of the leading LLMs, on this task under two settings: fine-tuning and prompt learning. Our results showed that the fine-tuned GPT-3 performed better in a context-aware honorific conversion task than the prompt-based one. The fine-tuned model demonstrated overall syntactic generalizability towards compound honorific sentences, except when tested with the data involving direct speech.</abstract>
      <url hash="7c05b574">2023.starsem-1.5</url>
      <bibkey>sekizawa-yanaka-2023-analyzing</bibkey>
      <doi>10.18653/v1/2023.starsem-1.5</doi>
    </paper>
    <paper id="6">
      <title>Improving Toponym Resolution with Better Candidate Generation, Transformer-based Reranking, and Two-Stage Resolution</title>
      <author><first>Zeyu</first><last>Zhang</last><affiliation>School of Information, the University of Arizona</affiliation></author>
      <author><first>Steven</first><last>Bethard</last><affiliation>University of Arizona</affiliation></author>
      <pages>48-60</pages>
      <url hash="2f7a2e91">2023.starsem-1.6</url>
      <bibkey>zhang-bethard-2023-improving</bibkey>
      <abstract>Geocoding is the task of converting location mentions in text into structured data that encodes the geospatial semantics. We propose a new architecture for geocoding, GeoNorm. GeoNorm first uses information retrieval techniques to generate a list of candidate entries from the geospatial ontology. Then it reranks the candidate entries using a transformer-based neural network that incorporates information from the ontology such as the entry’s population. This generate-and-rerank process is applied twice: first to resolve the less ambiguous countries, states, and counties, and second to resolve the remaining location mentions, using the identified countries, states, and counties as context. Our proposed toponym resolution framework achieves state-of-the-art performance on multiple datasets. Code and models are available at \url{https://github.com/clulab/geonorm}.</abstract>
      <doi>10.18653/v1/2023.starsem-1.6</doi>
    </paper>
    <paper id="7">
      <title><fixed-case>CRAPES</fixed-case>:Cross-modal Annotation Projection for Visual Semantic Role Labeling</title>
      <author><first>Abhidip</first><last>Bhattacharyya</last><affiliation>University of Colorado Boulder</affiliation></author>
      <author><first>Martha</first><last>Palmer</last><affiliation>University of Colorado</affiliation></author>
      <author><first>Christoffer</first><last>Heckman</last><affiliation>University of Colorado Boulder</affiliation></author>
      <pages>61-70</pages>
      <abstract>Automatic image comprehension is an important yet challenging task that includes identifying actions in an image and corresponding action participants. Most current approaches to this task, now termed Grounded Situation Recognition (GSR), start by predicting a verb that describes the action and then predict the nouns that can participate in the action as arguments to the verb. This problem formulation limits each image to a single action even though several actions could be depicted. In contrast, text-based Semantic Role Labeling (SRL) aims to label all actions in a sentence, typically resulting in at least two or three predicate argument structures per sentence. We hypothesize that expanding GSR to follow the more liberal SRL text-based approach to action and participant identification could improve image comprehension results. To test this hypothesis and to preserve generalization capabilities, we use general-purpose vision and language components as a front-end. This paper presents our results, a substantial 28.6 point jump in performance on the SWiG dataset, which confirm our hypothesis. We also discuss the benefits of loosely coupled broad-coverage off-the-shelf components which generalized well to out of domain images, and can decrease the need for manual image semantic role annotation.</abstract>
      <url hash="f78a4636">2023.starsem-1.7</url>
      <bibkey>bhattacharyya-etal-2023-crapes</bibkey>
      <doi>10.18653/v1/2023.starsem-1.7</doi>
    </paper>
    <paper id="8">
      <title>Not All Counterhate Tweets Elicit the Same Replies: A Fine-Grained Analysis</title>
      <author><first>Abdullah</first><last>Albanyan</last><affiliation>University of North Texas</affiliation></author>
      <author><first>Ahmed</first><last>Hassan</last><affiliation>University of Science and Technology at Zewailcity</affiliation></author>
      <author><first>Eduardo</first><last>Blanco</last><affiliation>University of Arizona</affiliation></author>
      <pages>71-88</pages>
      <abstract>Counterhate arguments can effectively fight and limit the spread of hate speech. However, they can also exacerbate the hate, as some people may respond with aggression if they feel threatened or targeted by the counterhate. In this paper, we investigate replies to counterhate arguments beyond whether the reply agrees or disagrees with the counterhate argument. We present a corpus with 2,621 replies to counterhate arguments countering hateful tweets, and annotate them with fine-grained characteristics. We show that (a) half of the replies (51%) to the counterhate arguments disagree with the argument, and (b) this kind of reply often supports the hateful tweet (40%). We also analyze the language of counterhate arguments that elicit certain types of replies. Experimental results show that it is feasible to anticipate the kind of replies a counterhate argument will elicit.</abstract>
      <url hash="38264288">2023.starsem-1.8</url>
      <bibkey>albanyan-etal-2023-counterhate</bibkey>
      <doi>10.18653/v1/2023.starsem-1.8</doi>
    </paper>
    <paper id="9">
      <title>Evaluating Factual Consistency of Texts with Semantic Role Labeling</title>
      <author><first>Jing</first><last>Fan</last><affiliation>Heidelberg University</affiliation></author>
      <author><first>Dennis</first><last>Aumiller</last><affiliation>Heidelberg University</affiliation></author>
      <author><first>Michael</first><last>Gertz</last><affiliation>Heidelberg University</affiliation></author>
      <pages>89-100</pages>
      <abstract>Automated evaluation of text generation systems has recently seen increasing attention, particularly checking whether generated text stays truthful to input sources. Existing methods frequently rely on an evaluation using task-specific language models, which in turn allows for little interpretability of generated scores. We introduce SRLScore, a reference-free evaluation metric designed with text summarization in mind. Our approach generates fact tuples constructed from Semantic Role Labels, applied to both input and summary texts.A final factuality score is computed by an adjustable scoring mechanism, which allows for easy adaption of the method across domains. Correlation with human judgments on English summarization datasets shows that SRLScore is competitive with state-of-the-art methods and exhibits stable generalization across datasets without requiring further training or hyperparameter tuning. We experiment with an optional co-reference resolution step, but find that the performance boost is mostly outweighed by the additional compute required. Our metric is available online at: <url>https://github.com/heyjing/SRLScore</url></abstract>
      <url hash="d6557a1a">2023.starsem-1.9</url>
      <bibkey>fan-etal-2023-evaluating</bibkey>
      <doi>10.18653/v1/2023.starsem-1.9</doi>
    </paper>
    <paper id="10">
      <title>Language models are not naysayers: an analysis of language models on negation benchmarks</title>
      <author><first>Thinh Hung</first><last>Truong</last><affiliation>The University of Melbourne</affiliation></author>
      <author><first>Timothy</first><last>Baldwin</last><affiliation>MBZUAI</affiliation></author>
      <author><first>Karin</first><last>Verspoor</last><affiliation>RMIT University</affiliation></author>
      <author><first>Trevor</first><last>Cohn</last><affiliation>University of Melbourne</affiliation></author>
      <pages>101-114</pages>
      <abstract>Negation has been shown to be a major bottleneck for masked language models, such as BERT. However, whether this finding still holds for larger-sized auto-regressive language models (“LLMs”) has not been studied comprehensively. With the ever-increasing volume of research and applications of LLMs, we take a step back to evaluate the ability of current-generation LLMs to handle negation, a fundamental linguistic phenomenon that is central to language understanding. We evaluate different LLMs - including the open-source GPT-neo, GPT-3, and InstructGPT - against a wide range of negation benchmarks. Through systematic experimentation with varying model sizes and prompts, we show that LLMs have several limitations including insensitivity to the presence of negation, an inability to capture the lexical semantics of negation, and a failure to reason under negation.</abstract>
      <url hash="8f4efa53">2023.starsem-1.10</url>
      <bibkey>truong-etal-2023-language</bibkey>
      <doi>10.18653/v1/2023.starsem-1.10</doi>
    </paper>
    <paper id="11">
      <title><fixed-case>JSEEG</fixed-case>raph: Joint Structured Event Extraction as Graph Parsing</title>
      <author><first>Huiling</first><last>You</last><affiliation>University of Oslo</affiliation></author>
      <author><first>Lilja</first><last>Vrelid</last><affiliation>Dept of Informatics, University of Oslo</affiliation></author>
      <author><first>Samia</first><last>Touileb</last><affiliation>University of Bergen</affiliation></author>
      <pages>115-127</pages>
      <abstract>We propose a graph-based event extraction framework JSEEGraph that approaches the task of event extraction as general graph parsing in the tradition of Meaning Representation Parsing. It explicitly encodes entities and events in a single semantic graph, and further has the flexibility to encode a wider range of additional IE relations and jointly infer individual tasks. JSEEGraph performs in an end-to-end manner via general graph parsing: (1) instead of flat sequence labelling, nested structures between entities/triggers are efficiently encoded as separate nodes in the graph, allowing for nested and overlapping entities and triggers; (2) both entities, relations, and events can be encoded in the same graph, where entities and event triggers are represented as nodes and entity relations and event arguments are constructed via edges; (3) joint inference avoids error propagation and enhances the interpolation of different IE tasks. We experiment on two benchmark datasets of varying structural complexities; ACE05 and Rich ERE, covering three languages: English, Chinese, and Spanish. Experimental results show that JSEEGraph can handle nested event structures, that it is beneficial to solve different IE tasks jointly, and that event argument extraction in particular benefits from entity extraction. Our code and models are released as open-source.</abstract>
      <url hash="d3129a4a">2023.starsem-1.11</url>
      <bibkey>you-etal-2023-jseegraph</bibkey>
      <doi>10.18653/v1/2023.starsem-1.11</doi>
    </paper>
    <paper id="12">
      <title>Generative Data Augmentation for Aspect Sentiment Quad Prediction</title>
      <author><first>An</first><last>Wang</last><affiliation>Tokyo Institute of Technology</affiliation></author>
      <author><first>Junfeng</first><last>Jiang</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Youmi</first><last>Ma</last><affiliation>Tokyo Institute of Technology</affiliation></author>
      <author><first>Ao</first><last>Liu</last><affiliation>Tokyo Institute of Technology</affiliation></author>
      <author><first>Naoaki</first><last>Okazaki</last><affiliation>Tokyo Institute of Technology</affiliation></author>
      <pages>128-140</pages>
      <abstract>Aspect sentiment quad prediction (ASQP) analyzes the aspect terms, opinion terms, sentiment polarity, and aspect categories in a text. One challenge in this task is the scarcity of data owing to the high annotation cost. Data augmentation techniques are commonly used to address this issue. However, existing approaches simply rewrite texts in the training data, restricting the semantic diversity of the generated data and impairing the quality due to the inconsistency between text and quads. To address these limitations, we augment quads and train a quads-to-text model to generate corresponding texts. Furthermore, we designed novel strategies to filter out low-quality data and balance the sample difficulty distribution of the augmented dataset. Empirical studies on two ASQP datasets demonstrate that our method outperforms other data augmentation methods and achieves state-of-the-art performance on the benchmarks. The source code will be released upon acceptance.</abstract>
      <url hash="12f3cffd">2023.starsem-1.12</url>
      <bibkey>wang-etal-2023-generative</bibkey>
      <doi>10.18653/v1/2023.starsem-1.12</doi>
    </paper>
    <paper id="13">
      <title>Are Language Models Sensitive to Semantic Attraction? A Study on Surprisal</title>
      <author><first>Yan</first><last>Cong</last><affiliation>Purdue University</affiliation></author>
      <author><first>Emmanuele</first><last>Chersoni</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Yu-yin</first><last>Hsu</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Alessandro</first><last>Lenci</last><affiliation>University of Pisa</affiliation></author>
      <pages>141-148</pages>
      <abstract>In psycholinguistics, semantic attraction is a sentence processing phenomenon in which a given argument violates the selectional requirements of a verb, but this violation is not perceived by comprehenders due to its attraction to another noun in the same sentence, which is syntactically unrelated but semantically sound. In our study, we use autoregressive language models to compute the sentence-level and the target phrase-level Surprisal scores of a psycholinguistic dataset on semantic attraction. Our results show that the models are sensitive to semantic attraction, leading to reduced Surprisal scores, although none of them perfectly matches the human behavioral pattern.</abstract>
      <url hash="84723760">2023.starsem-1.13</url>
      <bibkey>cong-etal-2023-language</bibkey>
      <doi>10.18653/v1/2023.starsem-1.13</doi>
    </paper>
    <paper id="14">
      <title>Syntax and Semantics Meet in the “Middle”: Probing the Syntax-Semantics Interface of <fixed-case>LM</fixed-case>s Through Agentivity</title>
      <author><first>Lindia</first><last>Tjuatja</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Emmy</first><last>Liu</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Lori</first><last>Levin</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Graham</first><last>Neubig</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>149-164</pages>
      <abstract>Recent advances in large language models have prompted researchers to examine their abilities across a variety of linguistic tasks, but little has been done to investigate how models handle the interactions in meaning across words and larger syntactic forms—i.e. phenomena at the intersection of syntax and semantics. We present the semantic notion of agentivity as a case study for probing such interactions. We created a novel evaluation dataset by utilitizing the unique linguistic properties of a subset of optionally transitive English verbs. This dataset was used to prompt varying sizes of three model classes to see if they are sensitive to agentivity at the lexical level, and if they can appropriately employ these word-level priors given a specific syntactic context. Overall, GPT-3 text-davinci-003 performs extremely well across all experiments, outperforming all other models tested by far. In fact, the results are even better correlated with human judgements than both syntactic and semantic corpus statistics. This suggests that LMs may potentially serve as more useful tools for linguistic annotation, theory testing, and discovery than select corpora for certain tasks.</abstract>
      <url hash="958ed004">2023.starsem-1.14</url>
      <bibkey>tjuatja-etal-2023-syntax</bibkey>
      <doi>10.18653/v1/2023.starsem-1.14</doi>
    </paper>
    <paper id="15">
      <title>Can Pretrained Language Models Derive Correct Semantics from Corrupt Subwords under Noise?</title>
      <author><first>Xinzhe</first><last>Li</last><affiliation>Deakin University</affiliation></author>
      <author><first>Ming</first><last>Liu</last><affiliation>Deakin University</affiliation></author>
      <author><first>Shang</first><last>Gao</last><affiliation>Deakin University</affiliation></author>
      <pages>165-173</pages>
      <abstract>For Pretrained Language Models (PLMs), their susceptibility to noise has recently been linked to subword segmentation. However, it is unclear which aspects of segmentation affect their understanding. This study assesses the robustness of PLMs against various disrupted segmentation caused by noise. An evaluation framework for subword segmentation, named Contrastive Lexical Semantic (CoLeS) probe, is proposed. It provides a systematic categorization of segmentation corruption under noise and evaluation protocols by generating contrastive datasets with canonical-noisy word pairs. Experimental results indicate that PLMs are unable to accurately compute word meanings if the noise introduces completely different subwords, small subword fragments, or a large number of additional subwords, particularly when they are inserted within other subwords.</abstract>
      <url hash="ecb3d205">2023.starsem-1.15</url>
      <attachment type="software" hash="d000e8a7">2023.starsem-1.15.software.zip</attachment>
      <bibkey>li-etal-2023-pretrained</bibkey>
      <doi>10.18653/v1/2023.starsem-1.15</doi>
    </paper>
    <paper id="16">
      <title>How Are Idioms Processed Inside Transformer Language Models?</title>
      <author><first>Ye</first><last>Tian</last><affiliation>Wluper &amp; Theia Insights</affiliation></author>
      <author><first>Isobel</first><last>James</last><affiliation>Wluper</affiliation></author>
      <author><first>Hye</first><last>Son</last><affiliation>Wluper</affiliation></author>
      <pages>174-179</pages>
      <abstract>Idioms such as “call it a day” and “piece of cake,” are prevalent in natural language. How do Transformer language models process idioms? This study examines this question by analysing three models - BERT, Multilingual BERT, and DistilBERT. We compare the embeddings of idiomatic and literal expressions across all layers of the networks at both the sentence and word levels. Additionally, we investigate the attention directed from other sentence tokens towards a word within an idiom as opposed to in a literal context. Results indicate that while the three models exhibit slightly different internal mechanisms, they all represent idioms distinctively compared to literal language, with attention playing a critical role. These findings suggest that idioms are semantically and syntactically idiosyncratic, not only for humans but also for language models.</abstract>
      <url hash="0f3355bf">2023.starsem-1.16</url>
      <bibkey>tian-etal-2023-idioms</bibkey>
      <doi>10.18653/v1/2023.starsem-1.16</doi>
    </paper>
    <paper id="17">
      <title>Is Shortest Always Best? The Role of Brevity in Logic-to-Text Generation</title>
      <author><first>Eduardo</first><last>Calò</last><affiliation>Utrecht University</affiliation></author>
      <author><first>Jordi</first><last>Levy</last><affiliation>IIIA, CSIC</affiliation></author>
      <author><first>Albert</first><last>Gatt</last><affiliation>Utrecht University</affiliation></author>
      <author><first>Kees</first><last>Van Deemter</last><affiliation>Utrecht University</affiliation></author>
      <pages>180-192</pages>
      <abstract>Some applications of artificial intelligence make it desirable that logical formulae be converted computationally to comprehensible natural language sentences. As there are many logical equivalents to a given formula, finding the most suitable equivalent to be used as input for such a “logic-to-text” generation system is a difficult challenge. In this paper, we focus on the role of brevity: Are the shortest formulae the most suitable? We focus on propositional logic (PL), framing formula minimization (i.e., the problem of finding the shortest equivalent of a given formula) as a Quantified Boolean Formulae (QBFs) satisfiability problem. We experiment with several generators and selection strategies to prune the resulting candidates. We conduct exhaustive automatic and human evaluations of the comprehensibility and fluency of the generated texts. The results suggest that while, in many cases, minimization has a positive impact on the quality of the sentences generated, formula minimization may ultimately not be the best strategy.</abstract>
      <url hash="b21ac557">2023.starsem-1.17</url>
      <bibkey>calo-etal-2023-shortest</bibkey>
      <doi>10.18653/v1/2023.starsem-1.17</doi>
    </paper>
    <paper id="18">
      <title>Seeking Clozure: Robust Hypernym extraction from <fixed-case>BERT</fixed-case> with Anchored Prompts</title>
      <author><first>Chunhua</first><last>Liu</last><affiliation>The University of Melbourne</affiliation></author>
      <author><first>Trevor</first><last>Cohn</last><affiliation>University of Melbourne</affiliation></author>
      <author><first>Lea</first><last>Frermann</last><affiliation>Melbourne University</affiliation></author>
      <pages>193-206</pages>
      <abstract>The automatic extraction of hypernym knowledge from large language models like BERT is an open problem, and it is unclear whether methods fail due to a lack of knowledge in the model or shortcomings of the extraction methods. In particular, methods fail on challenging cases which include rare or abstract concepts, and perform inconsistently under paraphrased prompts. In this study, we revisit the long line of work on pattern-based hypernym extraction, and use it as a diagnostic tool to thoroughly examine the hypernomy knowledge encoded in BERT and the limitations of hypernym extraction methods. We propose to construct prompts from established pattern structures: definitional (X is a Y); lexico-syntactic (Y such as X); and their anchored versions (Y such as X or Z). We devise an automatic method for anchor prediction, and compare different patterns in: (i) their effectiveness for hypernym retrieval from BERT across six English data sets; (ii) on challenge sets of rare and abstract concepts; and (iii) on consistency under paraphrasing. We show that anchoring is particularly useful for abstract concepts and in enhancing consistency across paraphrases, demonstrating how established methods in the field can inform prompt engineering.</abstract>
      <url hash="346feea0">2023.starsem-1.18</url>
      <bibkey>liu-etal-2023-seeking</bibkey>
      <doi>10.18653/v1/2023.starsem-1.18</doi>
    </paper>
    <paper id="19">
      <title><fixed-case>LEXPLAIN</fixed-case>: Improving Model Explanations via Lexicon Supervision</title>
      <author><first>Orevaoghene</first><last>Ahia</last><affiliation>University of Washington</affiliation></author>
      <author><first>Hila</first><last>Gonen</last><affiliation>UW and FAIR</affiliation></author>
      <author><first>Vidhisha</first><last>Balachandran</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Yulia</first><last>Tsvetkov</last><affiliation>University of Washington</affiliation></author>
      <author><first>Noah A.</first><last>Smith</last><affiliation>University of Washington</affiliation></author>
      <pages>207-216</pages>
      <abstract>Model explanations that shed light on the model’s predictions are becoming a desired additional output of NLP models, alongside their predictions. Challenges in creating these explanations include making them trustworthy and faithful to the model’s predictions. In this work, we propose a novel framework for guiding model explanations by supervising them explicitly. To this end, our method, LEXplain, uses task-related lexicons to directly supervise model explanations. This approach consistently improves the model’s explanations without sacrificing performance on the task, as we demonstrate on sentiment analysis and toxicity detection. Our analyses show that our method also demotes spurious correlations (i.e., with respect to African American English dialect) when performing the task, improving fairness.</abstract>
      <url hash="ea26db23">2023.starsem-1.19</url>
      <bibkey>ahia-etal-2023-lexplain</bibkey>
      <doi>10.18653/v1/2023.starsem-1.19</doi>
    </paper>
    <paper id="20">
      <title><fixed-case>KGLM</fixed-case>: Integrating Knowledge Graph Structure in Language Models for Link Prediction</title>
      <author><first>Jason</first><last>Youn</last><affiliation>University of California, Davis</affiliation></author>
      <author><first>Ilias</first><last>Tagkopoulos</last><affiliation>University of California, Davis</affiliation></author>
      <pages>217-224</pages>
      <abstract>The ability of knowledge graphs to represent complex relationships at scale has led to their adoption for various needs including knowledge representation, question-answering, and recommendation systems. Knowledge graphs are often incomplete in the information they represent, necessitating the need for knowledge graph completion tasks. Pre-trained and fine-tuned language models have shown promise in these tasks although these models ignore the intrinsic information encoded in the knowledge graph, namely the entity and relation types. In this work, we propose the Knowledge Graph Language Model (KGLM) architecture, where we introduce a new entity/relation embedding layer that learns to differentiate distinctive entity and relation types, therefore allowing the model to learn the structure of the knowledge graph. In this work, we show that further pre-training the language models with this additional embedding layer using the triples extracted from the knowledge graph, followed by the standard fine-tuning phase sets a new state-of-the-art performance for the link prediction task on the benchmark datasets.</abstract>
      <url hash="9bc04b15">2023.starsem-1.20</url>
      <bibkey>youn-tagkopoulos-2023-kglm</bibkey>
      <doi>10.18653/v1/2023.starsem-1.20</doi>
    </paper>
    <paper id="21">
      <title>Probing Out-of-Distribution Robustness of Language Models with Parameter-Efficient Transfer Learning</title>
      <author><first>Hyunsoo</first><last>Cho</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Choonghyun</first><last>Park</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Junyeob</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Hyuhng Joon</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Kang Min</first><last>Yoo</last><affiliation>NAVER AI Lab</affiliation></author>
      <author><first>Sang-goo</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <pages>225-235</pages>
      <abstract>As the size of the pre-trained language model (PLM) continues to increase, numerous parameter-efficient transfer learning methods have been proposed recently to compensate for the high cost of fine-tuning. While large PLMs and various PETL methods have achieved impressive results on various benchmarks, it is uncertain whether they can effectively handle inputs that have been distributionally shifted. In this study, we systematically explore how the ability to detect out-of-distribution (OOD) changes as the size of the PLM grows or the transfer methods are altered. Specifically, we evaluated various PETL techniques, including fine-tuning, Adapter, LoRA, and prefix-tuning, with various language models with different scales.</abstract>
      <url hash="48861b0b">2023.starsem-1.21</url>
      <bibkey>cho-etal-2023-probing</bibkey>
      <doi>10.18653/v1/2023.starsem-1.21</doi>
    </paper>
    <paper id="22">
      <title>Limits for learning with language models</title>
      <author><first>Nicholas</first><last>Asher</last><affiliation>CNRS Institut de Recherche en Informatique de Toulouse</affiliation></author>
      <author><first>Swarnadeep</first><last>Bhar</last><affiliation>Institut de Recherche en Informatique de Toulouse</affiliation></author>
      <author><first>Akshay</first><last>Chaturvedi</last><affiliation>Institut de Recherche en Informatique de Toulouse</affiliation></author>
      <author><first>Julie</first><last>Hunter</last><affiliation>LINAGORA</affiliation></author>
      <author><first>Soumya</first><last>Paul</last><affiliation>Proximus Luxembourg S.A.</affiliation></author>
      <pages>236-248</pages>
      <abstract>With the advent of large language models (LLMs), the trend in NLP has been to train LLMs on vast amounts of data to solve diverse language understanding and generation tasks. The list of LLM successes is long and varied. Nevertheless, several recent papers provide empirical evidence that LLMs fail to capture important aspects of linguistic meaning. Focusing on universal quantification, we provide a theoretical foundation for these empirical findings by proving that LLMs cannot learn certain fundamental semantic properties including semantic entailment and consistency as they are defined in formal semantics. More generally, we show that LLMs are unable to learn concepts beyond the first level of the Borel Hierarchy, which imposes severe limits on the ability of LMs, both large and small, to capture many aspects of linguistic meaning. This means that LLMs will operate without formal guarantees on tasks that require entailments and deep linguistic understanding.</abstract>
      <url hash="90508532">2023.starsem-1.22</url>
      <bibkey>asher-etal-2023-limits</bibkey>
      <doi>10.18653/v1/2023.starsem-1.22</doi>
    </paper>
    <paper id="23">
      <title>Does Character-level Information Always Improve <fixed-case>DRS</fixed-case>-based Semantic Parsing?</title>
      <author><first>Tomoya</first><last>Kurosawa</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Hitomi</first><last>Yanaka</last><affiliation>The University of Tokyo</affiliation></author>
      <pages>249-258</pages>
      <abstract>Even in the era of massive language models, it has been suggested that character-level representations improve the performance of neural models. The state-of-the-art neural semantic parser for Discourse Representation Structures uses character-level representations, improving performance in the four languages (i.e., English, German, Dutch, and Italian) in the Parallel Meaning Bank dataset. However, how and why character-level information improves the parser’s performance remains unclear. This study provides an in-depth analysis of performance changes by order of character sequences. In the experiments, we compare F1-scores by shuffling the order and randomizing character sequences after testing the performance of character-level information. Our results indicate that incorporating character-level information does not improve the performance in English and German. In addition, we find that the parser is not sensitive to correct character order in Dutch. Nevertheless, performance improvements are observed when using character-level information.</abstract>
      <url hash="3c4bf592">2023.starsem-1.23</url>
      <bibkey>kurosawa-yanaka-2023-character</bibkey>
      <doi>10.18653/v1/2023.starsem-1.23</doi>
    </paper>
    <paper id="24">
      <title>Testing Paraphrase Models on Recognising Sentence Pairs at Different Degrees of Semantic Overlap</title>
      <author><first>Qiwei</first><last>Peng</last><affiliation>University of Sussex</affiliation></author>
      <author><first>David</first><last>Weir</last><affiliation>University of Sussex</affiliation></author>
      <author><first>Julie</first><last>Weeds</last><affiliation>University of Sussex</affiliation></author>
      <pages>259-269</pages>
      <abstract>Paraphrase detection is useful in many natural language understanding applications. Current works typically formulate this problem as a sentence pair binary classification task. However, this setup is not a good fit for many of the intended applications of paraphrase models. In particular, such applications often involve finding the closest paraphrases of the target sentence from a group of candidate sentences where they exhibit different degrees of semantic overlap with the target sentence. To apply models to this paraphrase retrieval scenario, the model must be sensitive to the degree to which two sentences are paraphrases of one another. However, many existing datasets ignore and fail to test models in this setup. In response, we propose adversarial paradigms to create evaluation datasets, which could examine the sensitivity to different degrees of semantic overlap. Empirical results show that, while paraphrase models and different sentence encoders appear successful on standard evaluations, measuring the degree of semantic overlap still remains a big challenge for them.</abstract>
      <url hash="7b3f0a49">2023.starsem-1.24</url>
      <bibkey>peng-etal-2023-testing</bibkey>
      <doi>10.18653/v1/2023.starsem-1.24</doi>
    </paper>
    <paper id="25">
      <title>„Mann“ is to “Donna” as「国王」is to « Reine » Adapting the Analogy Task for Multilingual and Contextual Embeddings</title>
      <author><first>Timothee</first><last>Mickus</last><affiliation>University of Helsinki</affiliation></author>
      <author><first>Eduardo</first><last>Calò</last><affiliation>Utrecht University</affiliation></author>
      <author><first>Léo</first><last>Jacqmin</last><affiliation>Orange &amp; Aix-Marseille University</affiliation></author>
      <author><first>Denis</first><last>Paperno</last><affiliation>Utrecht University</affiliation></author>
      <author><first>Mathieu</first><last>Constant</last><affiliation>Universit de Lorraine, CNRS, ATILF</affiliation></author>
      <pages>270-283</pages>
      <abstract>How does the word analogy task fit in the modern NLP landscape? Given the rarity of comparable multilingual benchmarks and the lack of a consensual evaluation protocol for contextual models, this remains an open question. In this paper, we introduce MATS: a multilingual analogy dataset, covering forty analogical relations in six languages, and evaluate human as well as static and contextual embedding performances on the task. We find that not all analogical relations are equally straightforward for humans, static models remain competitive with contextual embeddings, and optimal settings vary across languages and analogical relations. Several key challenges remain, including creating benchmarks that align with human reasoning and understanding what drives differences across methodologies.</abstract>
      <url hash="d38f3531">2023.starsem-1.25</url>
      <bibkey>mickus-etal-2023-mann</bibkey>
      <doi>10.18653/v1/2023.starsem-1.25</doi>
    </paper>
    <paper id="26">
      <title>Scalable Performance Analysis for Vision-Language Models</title>
      <author><first>Santiago</first><last>Castro</last><affiliation>University of Michigan</affiliation></author>
      <author><first>Oana</first><last>Ignat</last><affiliation>University of Michigan</affiliation></author>
      <author><first>Rada</first><last>Mihalcea</last><affiliation>University of Michigan</affiliation></author>
      <pages>284-294</pages>
      <abstract>Joint vision-language models have shown great performance over a diverse set of tasks. However, little is known about their limitations, as the high dimensional space learned by these models makes it difficult to identify semantic errors. Recent work has addressed this problem by designing highly controlled probing task benchmarks. Our paper introduces a more scalable solution that relies on already annotated benchmarks. Our method consists of extracting a large set of diverse features from a vision-language benchmark and measuring their correlation with the output of the target model. We confirm previous findings that CLIP behaves like a bag of words model and performs better with nouns and verbs; we also uncover novel insights such as CLIP getting confused by concrete words. Our framework is available at <url>https://github.com/MichiganNLP/Scalable-VLM-Probing</url> and can be used with other multimodal models and benchmarks.</abstract>
      <url hash="f0ff8916">2023.starsem-1.26</url>
      <bibkey>castro-etal-2023-scalable</bibkey>
      <doi>10.18653/v1/2023.starsem-1.26</doi>
    </paper>
    <paper id="27">
      <title><fixed-case>PCFG</fixed-case>-Based Natural Language Interface Improves Generalization for Controlled Text Generation</title>
      <author><first>Jingyu</first><last>Zhang</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>James</first><last>Glass</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Tianxing</first><last>He</last><affiliation>University of Washington</affiliation></author>
      <pages>295-313</pages>
      <abstract>Existing work on controlled text generation (CTG) assumes a control interface of categorical attributes. In this work, we propose a natural language (NL) interface, where we craft a PCFG to embed the control attributes into natural language commands, and propose variants of existing CTG models that take commands as input. In our experiments, we design tailored setups to test the model’s generalization abilities. We find our PCFG-based command generation approach is effective for handling unseen commands compared to fix-set templates. Further, our proposed NL models can effectively generalize to unseen attributes (a new ability enabled by the NL interface), as well as unseen attribute combinations. Interestingly, in model comparisons, the simple conditional generation approach, enhanced with our proposed NL interface, is shown to be a strong baseline in those challenging settings.</abstract>
      <url hash="1e3544ac">2023.starsem-1.27</url>
      <bibkey>zhang-etal-2023-pcfg</bibkey>
      <doi>10.18653/v1/2023.starsem-1.27</doi>
    </paper>
    <paper id="28">
      <title>True Detective: A Deep Abductive Reasoning Benchmark Undoable for <fixed-case>GPT</fixed-case>-3 and Challenging for <fixed-case>GPT</fixed-case>-4</title>
      <author><first>Maksym</first><last>Del</last><affiliation>University of Tartu</affiliation></author>
      <author><first>Mark</first><last>Fishel</last><affiliation>University of Tartu</affiliation></author>
      <pages>314-322</pages>
      <abstract>Large language models (LLMs) have demonstrated solid zero-shot reasoning capabilities, which is reflected in their performance on the current test tasks. This calls for a more challenging benchmark requiring highly advanced reasoning ability to be solved. In this paper, we introduce such a benchmark, consisting of 191 long-form (1200 words on average) mystery narratives constructed as detective puzzles. Puzzles are sourced from the “5 Minute Mystery” platform and include a multiple-choice question for evaluation. Only 47% of humans solve a puzzle successfully on average, while the best human solvers achieve over 80% success rate. We show that GPT-3 models barely outperform random on this benchmark (with 28% accuracy) while state-of-the-art GPT-4 solves only 38% of puzzles. This indicates that there is still a significant gap in the deep reasoning abilities of LLMs and humans and highlights the need for further research in this area. Our work introduces a challenging benchmark for future studies on reasoning in language models and contributes to a better understanding of the limits of LLMs’ abilities.</abstract>
      <url hash="aff17626">2023.starsem-1.28</url>
      <bibkey>del-fishel-2023-true</bibkey>
      <doi>10.18653/v1/2023.starsem-1.28</doi>
    </paper>
    <paper id="29">
      <title>Guiding Zero-Shot Paraphrase Generation with Fine-Grained Control Tokens</title>
      <author><first>Teemu</first><last>Vahtola</last><affiliation>University of Helsinki</affiliation></author>
      <author><first>Mathias</first><last>Creutz</last><affiliation>University of Helsinki</affiliation></author>
      <author><first>Jrg</first><last>Tiedemann</last><affiliation>University of Helsinki</affiliation></author>
      <pages>323-337</pages>
      <abstract>Sequence-to-sequence paraphrase generation models often struggle with the generation of diverse paraphrases. This deficiency constrains the viability of leveraging paraphrase generation in different Natural Language Processing tasks. We propose a translation-based guided paraphrase generation model that learns useful features for promoting surface form variation in generated paraphrases from cross-lingual parallel data. Our proposed method leverages multilingual neural machine translation pretraining to learn zero-shot paraphrasing. Furthermore, we incorporate dedicated prefix tokens into the training of the machine translation models to promote variation. The prefix tokens are designed to affect various linguistic features related to surface form realizations, and can be applied during inference to guide the decoding process towards a desired solution. We assess the proposed guided model on paraphrase generation in three languages, English, Finnish, and Swedish, and provide analysis on the feasibility of the prefix tokens to guided paraphrasing. Our analysis suggests that the attributes represented by the prefix tokens are useful in promoting variation, by pushing the paraphrases generated by the guided model to diverge from the input sentence while preserving semantics conveyed by the sentence well.</abstract>
      <url hash="176c9047">2023.starsem-1.29</url>
      <bibkey>vahtola-etal-2023-guiding</bibkey>
      <doi>10.18653/v1/2023.starsem-1.29</doi>
    </paper>
    <paper id="30">
      <title>A Tale of Two Laws of Semantic Change: Predicting Synonym Changes with Distributional Semantic Models</title>
      <author><first>Bastien</first><last>Lietard</last><affiliation>INRIA</affiliation></author>
      <author><first>Mikaela</first><last>Keller</last><affiliation>Universite de Lille</affiliation></author>
      <author><first>Pascal</first><last>Denis</last><affiliation>INRIA</affiliation></author>
      <pages>338-352</pages>
      <abstract>Lexical Semantic Change is the study of how the meaning of words evolves through time. Another related question is whether and how lexical relations over pairs of words, such as synonymy, change over time. There are currently two competing, apparently opposite hypotheses in the historical linguistic literature regarding how synonymous words evolve: the Law of Differentiation (LD) argues that synonyms tend to take on different meanings over time, whereas the Law of Parallel Change (LPC) claims that synonyms tend to undergo the same semantic change and therefore remain synonyms. So far, there has been little research using distributional models to assess to what extent these laws apply on historical corpora. In this work, we take a first step toward detecting whether LD or LPC operates for given word pairs. After recasting the problem into a more tractable task, we combine two linguistic resources to propose the first complete evaluation framework on this problem and provide empirical evidence in favor of a dominance of LD. We then propose various computational approaches to the problem using Distributional Semantic Models and grounded in recent literature on Lexical Semantic Change detection. Our best approaches achieve a balanced accuracy above 0.6 on our dataset. We discuss challenges still faced by these approaches, such as polysemy or the potential confusion between synonymy and hypernymy.</abstract>
      <url hash="b5717c01">2023.starsem-1.30</url>
      <bibkey>lietard-etal-2023-tale</bibkey>
      <doi>10.18653/v1/2023.starsem-1.30</doi>
    </paper>
    <paper id="31">
      <title>Semantically-informed Hierarchical Event Modeling</title>
      <author><first>Shubhashis</first><last>Roy Dipta</last><affiliation>University of Maryland, Baltimore County</affiliation></author>
      <author><first>Mehdi</first><last>Rezaee</last><affiliation>University of Maryland, Baltimore County</affiliation></author>
      <author><first>Francis</first><last>Ferraro</last><affiliation>University of Maryland, Baltimore County</affiliation></author>
      <pages>353-369</pages>
      <abstract>Prior work has shown that coupling sequential latent variable models with semantic ontological knowledge can improve the representational capabilities of event modeling approaches. In this work, we present a novel, doubly hierarchical, semi-supervised event modeling framework that provides structural hierarchy while also accounting for ontological hierarchy. Our approach consistsof multiple layers of structured latent variables, where each successive layer compresses and abstracts the previous layers. We guide this compression through the injection of structured ontological knowledge that is defined at the type level of events: importantly, our model allows for partial injection of semantic knowledge and it does not depend on observing instances at any particular level of the semantic ontology. Across two different datasets and four different evaluation metrics, we demonstrate that our approach is able to out-perform the previous state-of-the-art approaches by up to 8.5%, demonstrating the benefits of structured and semantic hierarchical knowledge for event modeling.</abstract>
      <url hash="4666fc25">2023.starsem-1.31</url>
      <bibkey>roy-dipta-etal-2023-semantically</bibkey>
      <doi>10.18653/v1/2023.starsem-1.31</doi>
    </paper>
    <paper id="32">
      <title>Representation of Lexical Stylistic Features in Language Models’ Embedding Space</title>
      <author><first>Qing</first><last>Lyu</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Marianna</first><last>Apidianaki</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Chris</first><last>Callison-burch</last><affiliation>University of Pennsylvania</affiliation></author>
      <pages>370-387</pages>
      <abstract>The representation space of pretrained Language Models (LMs) encodes rich information about words and their relationships (e.g., similarity, hypernymy, polysemy) as well as abstract semantic notions (e.g., intensity). In this paper, we demonstrate that lexical stylistic notions such as complexity, formality, and figurativeness, can also be identified in this space. We show that it is possible to derive a vector representation for each of these stylistic notions from only a small number of seed pairs. Using these vectors, we can characterize new texts in terms of these dimensions by performing simple calculations in the corresponding embedding space. We conduct experiments on five datasets and find that static embeddings encode these features more accurately at the level of words and phrases, whereas contextualized LMs perform better on sentences. The lower performance of contextualized representations at the word level is partially attributable to the anisotropy of their vector space, which can be corrected to some extent using techniques like standardization.</abstract>
      <url hash="9483c9b5">2023.starsem-1.32</url>
      <attachment type="software" hash="df2764a2">2023.starsem-1.32.software.zip</attachment>
      <bibkey>lyu-etal-2023-representation</bibkey>
      <doi>10.18653/v1/2023.starsem-1.32</doi>
    </paper>
    <paper id="33">
      <title>Event Semantic Knowledge in Procedural Text Understanding</title>
      <author><first>Ghazaleh</first><last>Kazeminejad</last><affiliation>University of Colorado Boulder</affiliation></author>
      <author><first>Martha</first><last>Palmer</last><affiliation>University of Colorado</affiliation></author>
      <pages>388-398</pages>
      <abstract>The task of entity state tracking aims to automatically analyze procedural texts – texts that describe a step-by-step process (e.g. a baking recipe). Specifically, the goal is to track various states of the entities participating in a given process. Some of the challenges for this NLP task include annotated data scarcity and annotators’ reliance on commonsense knowledge to annotate implicit state information. Zhang et al. (2021) successfully incorporated commonsense entity-centric knowledge from ConceptNet into their BERT-based neural-symbolic architecture. Since English mostly encodes state change information in verbs, we attempted to test whether injecting semantic knowledge of events (retrieved from the state-of-the-art VerbNet parser) into a neural model can also improve the performance on this task. To achieve this, we adapt the methodology introduced by Zhang et al. (2021) for incorporating symbolic entity information from ConceptNet to the incorporation of VerbNet event semantics. We evaluate the performance of our model on the ProPara dataset (Mishra et al., 2018). In addition, we introduce a purely symbolic model for entity state tracking that uses a simple set of case statements, and is informed mostly by linguistic knowledge retrieved from various computational lexical resources. Our approach is inherently domain-agnostic, and our model is explainable and achieves state-of-the-art results on the Recipes dataset (Bosselut et al., 2017).</abstract>
      <url hash="364eec68">2023.starsem-1.33</url>
      <bibkey>kazeminejad-palmer-2023-event</bibkey>
      <doi>10.18653/v1/2023.starsem-1.33</doi>
    </paper>
    <paper id="34">
      <title>Leveraging Active Learning to Minimise <fixed-case>SRL</fixed-case> Annotation Across Corpora</title>
      <author><first>Skatje</first><last>Myers</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <author><first>Martha</first><last>Palmer</last><affiliation>University of Colorado</affiliation></author>
      <pages>399-408</pages>
      <url hash="b6bb757a">2023.starsem-1.34</url>
      <bibkey>myers-palmer-2023-leveraging</bibkey>
      <abstract>In this paper we investigate the application of active learning to semantic role labeling (SRL) using Bayesian Active Learning by Disagreement (BALD). Our new predicate-focused selection method quickly improves efficiency on three different specialised domain corpora. This is encouraging news for researchers wanting to port SRL to domain specific applications. Interestingly, with the large and diverse \textit{OntoNotes} corpus, the sentence selection approach, that collects a larger number of predicates, taking more time to annotate, fares better than the predicate approach. In this paper, we analyze both the selections made by our two selections methods for the various domains and the differences between these corpora in detail.</abstract>
      <doi>10.18653/v1/2023.starsem-1.34</doi>
    </paper>
    <paper id="35">
      <title>Estimating Semantic Similarity between In-Domain and Out-of-Domain Samples</title>
      <author><first>Rhitabrat</first><last>Pokharel</last><affiliation>Portland State University</affiliation></author>
      <author><first>Ameeta</first><last>Agrawal</last><affiliation>Portland State University</affiliation></author>
      <pages>409-416</pages>
      <abstract>Prior work typically describes out-of-domain (OOD) or out-of-distribution (OODist) samples as those that originate from dataset(s) or source(s) different from the training set but for the same task. When compared to in-domain (ID) samples, the models have been known to usually perform poorer on OOD samples, although this observation is not consistent. Another thread of research has focused on OOD detection, albeit mostly using supervised approaches. In this work, we first consolidate and present a systematic analysis of multiple definitions of OOD and OODist as discussed in prior literature. Then, we analyze the performance of a model under ID and OOD/OODist settings in a principled way. Finally, we seek to identify an unsupervised method for reliably identifying OOD/OODist samples without using a trained model. The results of our extensive evaluation using 12 datasets from 4 different tasks suggest the promising potential of unsupervised metrics in this task.</abstract>
      <url hash="a62fefe4">2023.starsem-1.35</url>
      <bibkey>pokharel-agrawal-2023-estimating</bibkey>
      <doi>10.18653/v1/2023.starsem-1.35</doi>
    </paper>
    <paper id="36">
      <title>Query Generation Using <fixed-case>GPT</fixed-case>-3 for <fixed-case>CLIP</fixed-case>-Based Word Sense Disambiguation for Image Retrieval</title>
      <author><first>Xiaomeng</first><last>Pan</last><affiliation>Tokyo Metropolitan University</affiliation></author>
      <author><first>Zhousi</first><last>Chen</last><affiliation>Tokyo Metropolitan University</affiliation></author>
      <author><first>Mamoru</first><last>Komachi</last><affiliation>Hitotsubashi University</affiliation></author>
      <pages>417-422</pages>
      <abstract>In this study, we propose using the GPT-3 as a query generator for the backend of CLIP as an implicit word sense disambiguation (WSD) component for the SemEval 2023 shared task Visual Word Sense Disambiguation (VWSD). We confirmed previous findings — human-like prompts adapted for WSD with quotes benefit both CLIP and GPT-3, whereas plain phrases or poorly templated prompts give the worst results.</abstract>
      <url hash="cd3fd720">2023.starsem-1.36</url>
      <bibkey>pan-etal-2023-query</bibkey>
      <doi>10.18653/v1/2023.starsem-1.36</doi>
    </paper>
    <paper id="37">
      <title>Functional Distributional Semantics at Scale</title>
      <author><first>Chun Hei</first><last>Lo</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Hong</first><last>Cheng</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Wai</first><last>Lam</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Guy</first><last>Emerson</last><affiliation>University of Cambridge</affiliation></author>
      <pages>423-436</pages>
      <abstract>Functional Distributional Semantics is a linguistically motivated framework for modelling lexical and sentence-level semantics with truth-conditional functions using distributional information. Previous implementations of the framework focus on subjectverbobject (SVO) triples only, which largely limits the contextual information available for training and thus the capability of the learnt model. In this paper, we discuss the challenges of extending the previous architectures to training on arbitrary sentences. We address the challenges by proposing a more expressive lexical model that works over a continuous semantic space. This improves the flexibility and computational efficiency of the model, as well as its compatibility with present-day machine-learning frameworks. Our proposal allows the model to be applied to a wider range of semantic tasks, and improved performances are demonstrated from experimental results.</abstract>
      <url hash="c774238a">2023.starsem-1.37</url>
      <bibkey>lo-etal-2023-functional</bibkey>
      <doi>10.18653/v1/2023.starsem-1.37</doi>
    </paper>
    <paper id="38">
      <title><fixed-case>FEED</fixed-case> <fixed-case>PET</fixed-case>s: Further Experimentation and Expansion on the Disambiguation of Potentially Euphemistic Terms</title>
      <author><first>Patrick</first><last>Lee</last><affiliation>Montclair State University</affiliation></author>
      <author><first>Iyanuoluwa</first><last>Shode</last><affiliation>Montclair State University</affiliation></author>
      <author><first>Alain</first><last>Trujillo</last><affiliation>Montclair State University</affiliation></author>
      <author><first>Yuan</first><last>Zhao</last><affiliation>Montclair State University</affiliation></author>
      <author><first>Olumide</first><last>Ojo</last><affiliation>Montclair State University</affiliation></author>
      <author><first>Diana</first><last>Plancarte</last><affiliation>Montclair State University</affiliation></author>
      <author><first>Anna</first><last>Feldman</last><affiliation>Montclair State University</affiliation></author>
      <author><first>Jing</first><last>Peng</last><affiliation>Montclair State University</affiliation></author>
      <pages>437-448</pages>
      <abstract>Transformers have been shown to work well for the task of English euphemism disambiguation, in which a potentially euphemistic term (PET) is classified as euphemistic or non-euphemistic in a particular context. In this study, we expand on the task in two ways. First, we annotate PETs for vagueness, a linguistic property associated with euphemisms, and find that transformers are generally better at classifying vague PETs, suggesting linguistic differences in the data that impact performance. Second, we present novel euphemism corpora in three different languages: Yoruba, Spanish, and Mandarin Chinese. We perform euphemism disambiguation experiments in each language using multilingual transformer models mBERT and XLM-RoBERTa, establishing preliminary results from which to launch future work.</abstract>
      <url hash="9cd841d5">2023.starsem-1.38</url>
      <bibkey>lee-etal-2023-feed</bibkey>
      <doi>10.18653/v1/2023.starsem-1.38</doi>
    </paper>
    <paper id="39">
      <title>Monolingual Phrase Alignment as Parse Forest Mapping</title>
      <author><first>Sora</first><last>Kadotani</last><affiliation>Osaka University</affiliation></author>
      <author><first>Yuki</first><last>Arase</last><affiliation>Osaka University</affiliation></author>
      <pages>449-455</pages>
      <abstract>We tackle the problem of monolingual phrase alignment conforming to syntactic structures. The existing method formalises the problem as unordered tree mapping; hence, the alignment quality is easily affected by syntactic ambiguities. We address this problem by expanding the method to align parse forests rather than 1-best trees, where syntactic structures and phrase alignment are simultaneously identified. The proposed method achieves efficient alignment by mapping forests on a packed structure. The experimental results indicated that our method improves the phrase alignment quality of the state-of-the-art method by aligning forests rather than 1-best trees.</abstract>
      <url hash="51d79d0b">2023.starsem-1.39</url>
      <bibkey>kadotani-arase-2023-monolingual</bibkey>
      <doi>10.18653/v1/2023.starsem-1.39</doi>
    </paper>
    <paper id="40">
      <title>Empirical Sufficiency Lower Bounds for Language Modeling with Locally-Bootstrapped Semantic Structures</title>
      <author><first>Jakob</first><last>Prange</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Emmanuele</first><last>Chersoni</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <pages>456-468</pages>
      <abstract>In this work we build upon negative results from an attempt at language modeling with predicted semantic structure, in order to establish empirical lower bounds on what could have made the attempt successful. More specifically, we design a concise binary vector representation of semantic structure at the lexical level and evaluate in-depth how good an incremental tagger needs to be in order to achieve better-than-baseline performance with an end-to-end semantic-bootstrapping language model. We envision such a system as consisting of a (pretrained) sequential-neural component and a hierarchical-symbolic component working together to generate text with low surprisal and high linguistic interpretability. We find that (a) dimensionality of the semantic vector representation can be dramatically reduced without losing its main advantages and (b) lower bounds on prediction quality cannot be established via a single score alone, but need to take the distributions of signal and noise into account.</abstract>
      <url hash="cfba2c7e">2023.starsem-1.40</url>
      <bibkey>prange-chersoni-2023-empirical</bibkey>
      <doi>10.18653/v1/2023.starsem-1.40</doi>
    </paper>
    <paper id="41">
      <title>Probing neural language models for understanding of words of estimative probability</title>
      <author><first>Damien</first><last>Sileo</last><affiliation>Inria</affiliation></author>
      <author><first>Marie-francine</first><last>Moens</last><affiliation>KU Leuven</affiliation></author>
      <pages>469-476</pages>
      <url hash="6ee26cab">2023.starsem-1.41</url>
      <bibkey>sileo-moens-2023-probing</bibkey>
      <abstract>Words of Estimative Probability (WEP) are phrases used to express the plausibility of a statement. Examples include terms like \textit{probably, maybe, likely, doubt, unlikely}, and \textit{impossible}. Surveys have shown that human evaluators tend to agree when assigning numerical probability levels to these WEPs. For instance, the term \textit{highly likely} equates to a median probability of $0.90{\pm}0.08$ according to a survey by \citet{fagen-ulmschneider}.In this study, our focus is to gauge the competency of neural language processing models in accurately capturing the consensual probability level associated with each WEP. Our first approach is utilizing the UNLI dataset \cite{chen-etal-2020-uncertain}, which links premises and hypotheses with their perceived joint probability $p$. From this, we craft prompts in the form: "[\textsc{Premise}]. [\textsc{Wep}], [\textsc{Hypothesis}].” This allows us to evaluate whether language models can predict if the consensual probability level of a WEP aligns closely with $p$.In our second approach, we develop a dataset based on WEP-focused probabilistic reasoning to assess if language models can logically process WEP compositions. For example, given the prompt "[\textsc{EventA}] \textit{is likely}. [\textsc{EventB}] \textit{is impossible}.”, a well-functioning language model should not conclude that [\textsc{EventA$\&amp;amp;$B}] is likely. Through our study, we observe that both tasks present challenges to out-of-the-box English language models. However, we also demonstrate that fine-tuning these models can lead to significant and transferable improvements.</abstract>
      <doi>10.18653/v1/2023.starsem-1.41</doi>
    </paper>
    <paper id="42">
      <title>Arithmetic-Based Pretraining Improving Numeracy of Pretrained Language Models</title>
      <author><first>Dominic</first><last>Petrak</last><affiliation>TU Darmstadt</affiliation></author>
      <author><first>Nafise Sadat</first><last>Moosavi</last><affiliation>Department of Computer Science, The University of Sheffield</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>UKP Lab, Technische Universitt Darmstadt</affiliation></author>
      <pages>477-493</pages>
      <abstract>State-of-the-art pretrained language models tend to perform below their capabilities when applied out-of-the-box on tasks that require understanding and working with numbers (usually referred to as numeracy). Recent work suggests two main reasons for this: (1) popular tokenisation algorithms have limited expressiveness for numbers, and (2) common pretraining objectives do not target numeracy. Approaches that address these shortcomings usually require architectural changes or pretraining from scratch. In this paper, we propose a new extended pretraining approach called Arithmetic-Based Pretraining that jointly addresses both in one extended pretraining step without requiring architectural changes or pretraining from scratch. Arithmetic-Based Pretraining combines contrastive learning to improve the number representation, and a novel extended pretraining objective called Inferable Number Prediction Task to improve numeracy. Our experiments show the effectiveness of Arithmetic-Based Pretraining in three different tasks that require improved numeracy, i.e., reading comprehension in the DROP dataset, inference-on-tables in the InfoTabs dataset, and table-to-text generation in the WikiBio and SciGen datasets.</abstract>
      <url hash="2844dd00">2023.starsem-1.42</url>
      <bibkey>petrak-etal-2023-arithmetic</bibkey>
      <doi>10.18653/v1/2023.starsem-1.42</doi>
    </paper>
    <paper id="43">
      <title>Robust Integration of Contextual Information for Cross-Target Stance Detection</title>
      <author><first>Tilman</first><last>Beck</last><affiliation>UKP Lab, Technical University of Darmstadt</affiliation></author>
      <author><first>Andreas</first><last>Waldis</last><affiliation>HSLU / TU Darmstadt</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>UKP Lab, Technische Universitt Darmstadt</affiliation></author>
      <pages>494-511</pages>
      <abstract>Stance detection deals with identifying an author’s stance towards a target. Most existing stance detection models are limited because they do not consider relevant contextual information which allows for inferring the stance correctly. Complementary context can be found in knowledge bases but integrating the context into pretrained language models is non-trivial due to the graph structure of standard knowledge bases. To overcome this, we explore an approach to integrate contextual information as text which allows for integrating contextual information from heterogeneous sources, such as structured knowledge sources and by prompting large language models. Our approach can outperform competitive baselines on a large and diverse stance detection benchmark in a cross-target setup, i.e. for targets unseen during training. We demonstrate that it is more robust to noisy context and can regularize for unwanted correlations between labels and target-specific vocabulary. Finally, it is independent of the pretrained language model in use.</abstract>
      <url hash="dea168c2">2023.starsem-1.43</url>
      <bibkey>beck-etal-2023-robust</bibkey>
      <doi>10.18653/v1/2023.starsem-1.43</doi>
    </paper>
    <paper id="44">
      <title>Adverbs, Surprisingly</title>
      <author><first>Dmitry</first><last>Nikolaev</last><affiliation>University of Stuttgart</affiliation></author>
      <author><first>Collin</first><last>Baker</last><affiliation>International Computer Science Institute</affiliation></author>
      <author><first>Miriam R. L.</first><last>Petruck</last><affiliation>FrameNet</affiliation></author>
      <author><first>Sebastian</first><last>Padó</last><affiliation>Stuttgart University</affiliation></author>
      <pages>512-526</pages>
      <abstract>This paper begins with the premise that adverbs are neglected in computational linguistics. This view derives from two analyses: a literature review and a novel adverb dataset to probe a state-of-the-art language model, thereby uncovering systematic gaps in accounts for adverb meaning. We suggest that using Frame Semantics for characterizing word meaning, as in FrameNet, provides a promising approach to adverb analysis, given its ability to describe ambiguity, semantic roles, and null instantiation.</abstract>
      <url hash="17f3c927">2023.starsem-1.44</url>
      <bibkey>nikolaev-etal-2023-adverbs</bibkey>
      <doi>10.18653/v1/2023.starsem-1.44</doi>
    </paper>
    <paper id="45">
      <title>Can Sequence-to-Sequence Transformers Naturally Understand Sequential Instructions?</title>
      <author><first>Xiang</first><last>Zhou</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Aditya</first><last>Gupta</last><affiliation>Google</affiliation></author>
      <author><first>Shyam</first><last>Upadhyay</last><affiliation>Google</affiliation></author>
      <author><first>Mohit</first><last>Bansal</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Manaal</first><last>Faruqui</last><affiliation>Google</affiliation></author>
      <pages>527-534</pages>
      <abstract>While many real-life tasks require reasoning over multi-step sequential instructions, collecting fine-grained annotations for each intermediate step can be prohibitively expensive. In this work, we study how general pretrained sequence-to-sequence transformers perform under varying types of annotation for sequential instruction understanding. We conduct experiments using T5 (Raffel et al., 2020) on a commonly-used multi-step instruction understanding dataset SCONE (Long et al., 2016) that includes three sub-tasks. First, we show that with only gold supervision for the final step of a multi-step instruction sequence, depending on the sequential properties of different tasks, transformers may exhibit extremely bad performance on intermediate steps, in stark contrast with their performance on the final step. Next, we explore two directions to relieve this problem. We show that with the same limited annotation budget, using supervision uniformly distributed across different steps (instead of only final-step supervision), we can greatly improve the performance on intermediate steps with a drop in final-step performance. Further, we explore a contrastive learning approach to provide training signals on intermediate steps with zero intermediate gold supervision. This, however, achieves mixed results. It significantly improves the model’s bad intermediate-step performance on one subtask, but also shows decreased performance on another subtask.</abstract>
      <url hash="0ee3b649">2023.starsem-1.45</url>
      <bibkey>zhou-etal-2023-sequence</bibkey>
      <doi>10.18653/v1/2023.starsem-1.45</doi>
    </paper>
  </volume>
</collection>
