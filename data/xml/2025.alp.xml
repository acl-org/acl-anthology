<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.alp">
  <volume id="1" ingest-date="2025-04-26" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Second Workshop on Ancient Language Processing</booktitle>
      <editor><first>Adam</first><last>Anderson</last></editor>
      <editor><first>Shai</first><last>Gordin</last></editor>
      <editor><first>Bin</first><last>Li</last></editor>
      <editor><first>Yudong</first><last>Liu</last></editor>
      <editor><first>Marco C.</first><last>Passarotti</last></editor>
      <editor><first>Rachele</first><last>Sprugnoli</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>The Albuquerque Convention Center, Laguna</address>
      <month>May</month>
      <year>2025</year>
      <url hash="01b9123a">2025.alp-1</url>
      <venue>alp</venue>
      <venue>ws</venue>
      <isbn>979-8-89176-235-0</isbn>
    </meta>
    <frontmatter>
      <url hash="59c72f4f">2025.alp-1.0</url>
      <bibkey>alp-ws-2025-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Automatic Text Segmentation of Ancient and Historic <fixed-case>H</fixed-case>ebrew</title>
      <author><first>Elisha</first><last>Rosensweig</last></author>
      <author><first>Benjamin</first><last>Resnick</last></author>
      <author><first>Hillel</first><last>Gershuni</last></author>
      <author><first>Joshua</first><last>Guedalia</last></author>
      <author><first>Nachum</first><last>Dershowitz</last></author>
      <author><first>Avi</first><last>Shmidman</last></author>
      <pages>1-11</pages>
      <abstract>Ancient texts often lack punctuation marks, making it challenging to determine sentence boundaries and clause boundaries. Texts may contain sequences of hundreds of words without any period or indication of a full stop. Determining such boundaries is a crucial step in various NLP pipelines, especially regarding language models such as BERT that have context window constraints and regarding machine translation models which may become far less accurate when fed too much text at a time. In this paper, we consider several novel approaches to automatic segmentation of unpunctuated ancient texts into grammatically complete or semi-complete units. Our work here focuses on ancient and historical Hebrew and Aramaic texts, but the tools developed can be applied equally to similar languages. We explore several approaches to addressing this task: masked language models (MLM) to predict the next token; fewshot completions via an open-source foundational LLM; and the “Segment-Any-Text” (SaT) tool by Frohmann et al. (Frohmann et al., 2024). These are then compared to instructbased flows using commercial (closed, managed) LLMs, to be used as a benchmark. To evaluate these approaches, we also introduce a new ground truth (GT) dataset of manually segmented texts. We explore the performance of our different approaches on this dataset. We release both our segmentation tools and the dataset to support further research into computational processing and analysis of ancient texts, which can be found here ‘https://github.com/ERC-Midrash/rabbinic_chunker’.</abstract>
      <url hash="16c74385">2025.alp-1.1</url>
      <bibkey>rosensweig-etal-2025-automatic</bibkey>
    </paper>
    <paper id="2">
      <title>Integrating Semantic and Statistical Features for Authorial Clustering of Qumran Scrolls</title>
      <author><first>Yonatan</first><last>Lourie</last></author>
      <author><first>Jonathan</first><last>Ben-Dov</last></author>
      <author><first>Roded</first><last>Sharan</last></author>
      <pages>12-21</pages>
      <abstract>We present a novel framework for authorial classification and clustering of the Qumran Dead Sea Scrolls (DSS). Our approach com-bines modern Hebrew BERT embeddings with traditional natural language processing features in a graph neural network (GNN) architecture. Our results outperform baseline methods on both the Dead Sea Scrolls and a validation dataset of the Hebrew Bible. In particular, we leverage our model to provide significant insights into long-standing debates, including the classification of sectarian and non-sectarian texts and the division of the Hodayot collection of hymns.</abstract>
      <url hash="c55cc6a0">2025.alp-1.2</url>
      <bibkey>lourie-etal-2025-integrating</bibkey>
    </paper>
    <paper id="3">
      <title>Assignment of account type to proto-cuneiform economic texts with Multi-Class Support Vector Machines</title>
      <author><first>Piotr</first><last>Zadworny</last></author>
      <author><first>Shai</first><last>Gordin</last></author>
      <pages>22-30</pages>
      <abstract>We investigate the use of machine learning for classifying proto-cuneiform economic texts (3,500-3,000 BCE), leveraging Multi-Class Support Vector Machines (MSVM) to assign text type based on content. Proto-cuneiform presents unique challenges, as it does not en-code spoken language, yet is transcribed into linear formats that obscure original structural elements. We address this by reformatting tran-scriptions, experimenting with different tok-enization strategies, and optimizing feature ex-traction. Our workflow achieves high label-ing reliability and enables significant metadata enrichment. In addition to improving digital corpus organization, our approach opens the chance to identify economic institutions in an-cient Mesopotamian archives, providing a new tool for Assyriological research.</abstract>
      <url hash="6f7c2588">2025.alp-1.3</url>
      <bibkey>zadworny-gordin-2025-assignment</bibkey>
    </paper>
    <paper id="4">
      <title>Using Cross-Linguistic Data Formats to Enhance the Annotation of <fixed-case>A</fixed-case>ncient <fixed-case>C</fixed-case>hinese Documents Written on Bamboo Slips</title>
      <author><first>Michele</first><last>Pulini</last></author>
      <author><first>Johann-Mattis</first><last>List</last></author>
      <pages>31-37</pages>
      <abstract>Ancient Chinese documents written on bam-boo slips more than 2000 years ago offer a rich resource for research in linguistics, paleogra-phy, and historiography. However, since most documents are only available in the form of scans, additional steps of analysis are needed to turn them into interactive digital editions, amenable both for manual and computational exploration. Here, we present a first attempt to establish a workflow for the annotation of an-cient bamboo slips. Based on a recently redis-covered dialogue on warfare, we illustrate how a digital edition amenable for manual and com-putational exploration can be created by inte-grating standards originally designed for cross-linguistic data collections.</abstract>
      <url hash="aa8dc73b">2025.alp-1.4</url>
      <bibkey>pulini-list-2025-using</bibkey>
    </paper>
    <paper id="5">
      <title>Accessible <fixed-case>S</fixed-case>anskrit: A Cascading System for Text Analysis and Dictionary Access</title>
      <author><first>Giacomo</first><last>De Luca</last></author>
      <pages>38-46</pages>
      <abstract>Sanskrit text processing presents unique com-putational challenges due to its complex mor-phology, frequent compound formation, and the phenomenon of Sandhi. While several ap-proaches to Sanskrit word segmentation ex-ist, the field lacks integrated tools that make texts accessible while maintaining high accu-racy. We present a hybrid approach combining rule-based and statistical methods that achieves reliable Sanskrit text analysis through a cascade mechanism in which a deterministic matching using inflection tables is used for simple cases and statistical approaches are used for the more complex ones. The goal of the system is to provide automatic text annotation and inflected dictionary search, returning for each word root forms, comprehensive grammatical analysis, inflection tables, and dictionary entries from multiple sources. The system is evaluated on 300 randomly selected compounds from the GRETIL corpus across different length cate-gories and maintains 90% accuracy regardless of compound length, with 91% accuracy on the 40+ characters long compounds. The approach is also tested on the complete text of the Yoga Sutra, demonstrating 96% accuracy in the prac-tical use case. This approach is implemented both as an open-source Python library and a web application, making Sanskrit text analysis accessible to scholars and interested readers while retaining state-of-the-art accuracy.</abstract>
      <url hash="de7905f5">2025.alp-1.5</url>
      <bibkey>de-luca-2025-accessible</bibkey>
    </paper>
    <paper id="6">
      <title>Towards an Integrated Methodology of Dating Biblical Texts: The Case of the Book of Jeremiah</title>
      <author><first>Martijn</first><last>Naaijer</last></author>
      <author><first>Aren</first><last>Wilson-Wright</last></author>
      <pages>47-52</pages>
      <abstract>In this paper we describe our research project on dating the language of the Book of Jeremiah using a combination of traditional biblical scholarship and machine learning. Jeremiah is a book with a long history of composing and editing, and the historical background of many of the sections in the book are unclear. Moreover, redaction criticism and historical linguistics are mostly separate fields within the discipline of Biblical Studies. With our approach we want to integrate these areas of research and make new strides in uncovering the compositional history of Book of Jeremiah.</abstract>
      <url hash="d05f221e">2025.alp-1.6</url>
      <bibkey>naaijer-wilson-wright-2025-towards</bibkey>
    </paper>
    <paper id="7">
      <title>The Development of <fixed-case>H</fixed-case>ebrew in Antiquity – A Computational Linguistic Study</title>
      <author><first>Hallel</first><last>Baitner</last></author>
      <author><first>Dimid</first><last>Duchovny</last></author>
      <author><first>Lee-Ad</first><last>Gottlieb</last></author>
      <author><first>Amir</first><last>Yorav</last></author>
      <author><first>Nachum</first><last>Dershowitz</last></author>
      <author><first>Eshbal</first><last>Ratzon</last></author>
      <pages>53-58</pages>
      <abstract>The linguistic nature of Qumran Hebrew (QH) remains a central debate in the study of the Dead Sea Scrolls (DSS). Although some schol-ars view QH as an artificial imitation of Biblical Hebrew (BH), others argue that it represents a spoken dialect of ancient Judea. The present study employs computational lin-guistic techniques, clustering, classification, and machine learning, to analyze the relation-ship of QH with Biblical and Mishnaic He-brew. Preliminary findings confirm existing scholarly conclusions regarding the linguistic affinity of certain texts. This demonstrates that our methodology has a fundamental capacity to identify linguistic relationships. They also contribute new leads, on which we are now working to refine and enhance our analytical methods so as to provide founded insights into the historical development of Hebrew and the process of DSS textual composition.</abstract>
      <url hash="e1072939">2025.alp-1.7</url>
      <bibkey>baitner-etal-2025-development</bibkey>
    </paper>
    <paper id="8">
      <title>A Dataset of <fixed-case>A</fixed-case>ncient <fixed-case>C</fixed-case>hinese Math Word Problems and an Application for Research in Historic Mathematics</title>
      <author><first>Florian</first><last>Keßler</last></author>
      <pages>59-70</pages>
      <abstract>Solving math word problems, i.e. mathemati-cal problems stated in natural language, has re-ceived much attention in the Artificial Intelli-gence (AI) community over the last years. Un-surprisingly, research has focused on problems stated in contemporary languages. In contrast to this, in this article, we introduce a dataset of math word problems that is extracted from an-cient Chinese mathematical texts. The dataset is made available.1 We report a baseline per-formance for GPT-4o solving the problems in the dataset using a Program-of-Thought paradigm that translates the mathematical pro-cedures in the original texts into Python code, giving acceptable performance but showing that the model often struggles with understand-ing the pre-modern language. Finally, we de-scribe how the generated code can be used for research into the history of mathematics, by of-fering a way to search the texts by abstract op-erations instead of specific lexemes.</abstract>
      <url hash="4daab070">2025.alp-1.8</url>
      <bibkey>kessler-2025-dataset</bibkey>
    </paper>
    <paper id="9">
      <title>Evaluating Evaluation Metrics for <fixed-case>A</fixed-case>ncient <fixed-case>C</fixed-case>hinese to <fixed-case>E</fixed-case>nglish Machine Translation</title>
      <author><first>Eric R.</first><last>Bennett</last></author>
      <author><first>HyoJung</first><last>Han</last></author>
      <author><first>Xinchen</first><last>Yang</last></author>
      <author><first>Andrew</first><last>Schonebaum</last></author>
      <author><first>Marine</first><last>Carpuat</last></author>
      <pages>71-76</pages>
      <abstract>Evaluation metrics are an important driver of progress in Machine Translation (MT), but they have been primarily validated on high-resource modern languages. In this paper, we conduct an empirical evaluation of metrics commonly used to evaluate MT from Ancient Chinese into English. Using LLMs, we construct a contrastive test set, pairing high-quality MT and purposefully flawed MT of the same Pre-Qin texts. We then evaluate the ability of each metric to discriminate between accurate and flawed translations.</abstract>
      <url hash="e9555663">2025.alp-1.9</url>
      <bibkey>bennett-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="10">
      <title>From Clay to Code: Transforming <fixed-case>H</fixed-case>ittite Texts for Machine Learning</title>
      <author><first>Emma</first><last>Yavasan</last></author>
      <author><first>Shai</first><last>Gordin</last></author>
      <pages>77-86</pages>
      <abstract>This paper presents a comprehensive method-ology for transforming XML-encoded Hittite cuneiform texts into computationally accessi-ble formats for machine learning applications. Drawing from a corpus of 8,898 texts (558,349 tokens in total) encompassing 145 cataloged genres and compositions, we develop a struc-tured approach to preserve both linguistic and philological annotations while enabling compu-tational analysis. Our methodology addresses key challenges in ancient language processing, including the handling of fragmentary texts, multiple language layers, and complex anno-tation systems. We demonstrate the applica-tion of our corpus through experiments with T5 models, achieving significant improvements in Hittite-to-German translation (ROUGE-1: 0.895) while identifying limitations in morpho-logical glossing tasks. This work establishes a standardized, machine-readable dataset in Hit-tite cuneiform, which also maintains a balance with philological accuracy and current state-of-the-art.</abstract>
      <url hash="fcfde185">2025.alp-1.10</url>
      <bibkey>yavasan-gordin-2025-clay</bibkey>
    </paper>
    <paper id="11">
      <title>Towards <fixed-case>A</fixed-case>ncient <fixed-case>M</fixed-case>eroitic Decipherment: A Computational Approach</title>
      <author><first>Joshua N.</first><last>Otten</last></author>
      <author><first>Antonios</first><last>Anastasopoulos</last></author>
      <pages>87-98</pages>
      <abstract>The discovery of the Rosetta Stone was one of the keys that helped unlock the secrets of Ancient Egypt and its hieroglyphic lan-guage. But what about languages with no such “Rosetta Stone?” Meroitic is an ancient lan-guage from what is now present-day Sudan, but even though it is connected to Egyptian in many ways, much of its grammar and vocabu-lary remains undeciphered. In this work, we in-troduce the challenge of Meroitic decipherment as a computational task, and present the first Meroitic machine-readable corpus. We then train embeddings and perform intrinsic evalu-ations, as well as cross-lingual alignment ex-periments between Meroitic and Late-Egyptian. We conclude by outlining open problems and potential research directions.</abstract>
      <url hash="b846f30b">2025.alp-1.11</url>
      <bibkey>otten-anastasopoulos-2025-towards</bibkey>
    </paper>
    <paper id="12">
      <title>Neural Models for Lemmatization and <fixed-case>POS</fixed-case>-Tagging of Earlier and Late <fixed-case>E</fixed-case>gyptian (Supporting Hieroglyphic Input) and Demotic</title>
      <author><first>Aleksi</first><last>Sahala</last></author>
      <author><first>Eliese-Sophia</first><last>Lincke</last></author>
      <pages>99-104</pages>
      <abstract>We present updated models for BabyLemma-tizer for lemmatizing and POS-tagging De-motic, Late Egyptian and Earlier Egyptian with a support for using hieroglyphs as an input. In this paper, we also use data that has not been cleaned from breakages. We achieve consistent UPOS tagging accuracy of 94% or higher and an XPOS tagging accuracy of 93% and higher for all languages. For lemmatization, which is challenging in all of our test languages due to extensive ambiguity, we demonstrate accu-racies from 77% up to 92% depending on the language and the input script.</abstract>
      <url hash="3c729f65">2025.alp-1.12</url>
      <bibkey>sahala-lincke-2025-neural</bibkey>
    </paper>
    <paper id="13">
      <title>Bringing Suzhou Numerals into the Digital Age: A Dataset and Recognition Study on <fixed-case>A</fixed-case>ncient <fixed-case>C</fixed-case>hinese Trade Records</title>
      <author><first>Ting-Lin</first><last>Wu</last></author>
      <author><first>Zih-Ching</first><last>Chen</last></author>
      <author><first>Chen-Yuan</first><last>Chen</last></author>
      <author><first>Pi-Jhong</first><last>Chen</last></author>
      <author><first>Li-Chiao</first><last>Wang</last></author>
      <pages>105-111</pages>
      <abstract>Suzhou numerals, a specialized numerical no-tation system historically used in Chinese com-merce and accounting, played a pivotal role in financial transactions from the Song Dynasty to the early 20th century. Despite their his-torical significance, they remain largely absent from modern OCR benchmarks, limiting com-putational access to archival trade documents. This paper presents a curated dataset of 773 expert-annotated Suzhou numeral samples ex-tracted from late Qing-era trade ledgers. We provide a statistical analysis of character distri-butions, offering insights into their real-world usage in historical bookkeeping. Additionally, we evaluate baseline performance with hand-written text recognition (HTR) model, high-lighting the challenges of recognizing low-resource brush-written numerals. By introduc-ing this dataset and initial benchmark results, we aim to facilitate research in historical doc-umentation in ancient Chinese characters, ad-vancing the digitization of early Chinese finan-cial records. The dataset is publicly available at our huggingface hub, and our codebase can be accessed at our github repository.</abstract>
      <url hash="3e46d6c2">2025.alp-1.13</url>
      <bibkey>wu-etal-2025-bringing</bibkey>
    </paper>
    <paper id="14">
      <title>Detecting Honkadori based on <fixed-case>W</fixed-case>aka Embeddings</title>
      <author><first>Hayato</first><last>Ogawa</last></author>
      <author><first>Kaito</first><last>Horio</last></author>
      <author><first>Daisuke</first><last>Kawahara</last></author>
      <pages>112-119</pages>
      <abstract>We develop an embedding model specifically designed for Waka poetry and use it to build a model for detecting Honkadori. Waka is a tradi-tional form of old Japanese poetry that has been composed since ancient times. Honkadori is a sophisticated poetic technique in Japanese clas-sical literature where poets incorporate words or poetic sentiments from old Wakas (Honka) into their own work. First, we fine-tune a pre-trained language model using contrastive learn-ing to construct a Waka-specialized embedding model. Then, using the embedding vectors ob-tained from this model and features extracted from them, we train a machine learning model to detect the Honka (original poem) of Wakas that employ the Honkadori technique. Using paired data of Honka and Wakas that are consid-ered to use Honkadori, we evaluated the Honka detection model and demonstrated that it can detect Honka with reasonable accuracy.</abstract>
      <url hash="25b765d8">2025.alp-1.14</url>
      <bibkey>ogawa-etal-2025-detecting</bibkey>
    </paper>
    <paper id="15">
      <title>The Historian’s Fingerprint: A Computational Stylometric Study of the Zuo Commentary and Discourses of the States</title>
      <author><first>Wenjie</first><last>Hua</last></author>
      <pages>120-126</pages>
      <abstract>Previous studies suggest that authorship can be inferred through stylistic features like func-tion word usage and grammatical patterns, yet such analyses remain limited for Old Chinese texts with disputed authorship. Computational methods enable a more nuanced exploration of these texts. This study applies stylometric anal-ysis to examine the authorship controversy be-tween the Zuo Commentary and the Discourses of the States. Using PoS 4-grams, Kullback-Leibler divergence, and multidimensional scal-ing (MDS), we systematically compare their stylistic profiles. Results show that the Zuo Commentary exhibits high internal consistency, especially in the later eight Dukes chapters, supporting its integration by a single scholarly tradition. In contrast, the Discourses of the States displays greater stylistic diversity, align-ing with the multiple-source compilation the-ory. Further analysis reveals partial stylistic similarities among the Lu, Jin, and Chu-related chapters, suggesting shared influences. These findings provide quantitative support for Tong Shuye’s arguments and extend statistical vali-dation of Bernhard Karlgren’s assertion on the textual unity of the Zuo Commentary.</abstract>
      <url hash="8c38b0da">2025.alp-1.15</url>
      <bibkey>hua-2025-historians</bibkey>
    </paper>
    <paper id="16">
      <title>Incorporating Lexicon-Aligned Prompting in Large Language Model for <fixed-case>T</fixed-case>angut–<fixed-case>C</fixed-case>hinese Translation</title>
      <author><first>Yuxi</first><last>Zheng</last></author>
      <author><first>Jingsong</first><last>Yu</last></author>
      <pages>127-136</pages>
      <abstract>This paper proposes a machine translation approach for Tangut–Chinese using a large language model (LLM) enhanced with lexical knowledge. We fine-tune a Qwen-based LLM using Tangut–Chinese parallel corpora and dictionary definitions. Experimental results demonstrate that incorporating single-character dictionary definitions leads to the best BLEU-4 score of 72.33 for literal translation. Additionally, applying a chain-of-thought prompting strategy significantly boosts free translation performance to 64.20. The model also exhibits strong few-shot learning abilities, with performance improving as the training dataset size increases. Our approach effectively translates both simple and complex Tangut sentences, offering a robust solution for low-resource language translation and contributing to the digital preservation of Tangut texts.</abstract>
      <url hash="c60f1d89">2025.alp-1.16</url>
      <bibkey>zheng-yu-2025-incorporating</bibkey>
    </paper>
    <paper id="17">
      <title><fixed-case>P</fixed-case>arsi<fixed-case>P</fixed-case>y: <fixed-case>NLP</fixed-case> Toolkit for Historical <fixed-case>P</fixed-case>ersian Texts in Python</title>
      <author><first>Farhan</first><last>Farsi</last></author>
      <author><first>Parnian</first><last>Fazel</last></author>
      <author><first>Sepand</first><last>Haghighi</last></author>
      <author><first>Sadra</first><last>Sabouri</last></author>
      <author><first>Farzaneh</first><last>Goshtasb</last></author>
      <author><first>Nadia</first><last>Hajipour</last></author>
      <author><first>Ehsaneddin</first><last>Asgari</last></author>
      <author><first>Hossein</first><last>Sameti</last></author>
      <pages>137-149</pages>
      <abstract>The study of historical languages presents unique challenges due to their complex ortho-graphic systems, fragmentary textual evidence, and the absence of standardized digital repre-sentations of text in those languages. Tack-ling these challenges needs special NLP digi-tal tools to handle phonetic transcriptions and analyze ancient texts. This work introduces ParsiPy1, an NLP toolkit designed to facili-tate the analysis of historical Persian languages by offering modules for tokenization, lemma-tization, part-of-speech tagging, phoneme-to-transliteration conversion, and word embed-ding. We demonstrate the utility of our toolkit through the processing of Parsig (Middle Per-sian) texts, highlighting its potential for ex-panding computational methods in the study of historical languages. Through this work, we contribute to the field of computational philol-ogy, offering tools that can be adapted for the broader study of ancient texts and their digital preservation.</abstract>
      <url hash="872833d6">2025.alp-1.17</url>
      <bibkey>farsi-etal-2025-parsipy</bibkey>
    </paper>
    <paper id="18">
      <title>Exploring the Application of 7<fixed-case>B</fixed-case> <fixed-case>LLM</fixed-case>s for Named Entity Recognition in <fixed-case>C</fixed-case>hinese Ancient Texts</title>
      <author><first>Chenrui</first><last>Zheng</last></author>
      <author><first>Yicheng</first><last>Zhu</last></author>
      <author><first>Han</first><last>Bi</last></author>
      <pages>150-155</pages>
      <abstract>This paper explores the application of fine-tuning methods based on 7B large language models (LLMs) for named entity recognition (NER) tasks in Chinese ancient texts. Targeting the complex semantics and domain-specific characteristics of ancient texts, particularly in Traditional Chinese Medicine (TCM) texts, we propose a comprehensive fine-tuning and pre-training strategy. By introducing multi-task learning, domain-specific pre-training, and efficient fine-tuning techniques based on LoRA, we achieved significant performance improvements in ancient text NER tasks. Experimental results show that the pre-trained and fine-tuned 7B model achieved an F1 score of 0.93, significantly outperforming general-purpose large language models.</abstract>
      <url hash="636e8b20">2025.alp-1.18</url>
      <bibkey>zheng-etal-2025-exploring</bibkey>
    </paper>
    <paper id="19">
      <title>Overview of <fixed-case>E</fixed-case>va<fixed-case>H</fixed-case>an2025: The First International Evaluation on <fixed-case>A</fixed-case>ncient <fixed-case>C</fixed-case>hinese Named Entity Recognition</title>
      <author><first>Bin</first><last>Li</last></author>
      <author><first>Bolin</first><last>Chang</last></author>
      <author><first>Ruilin</first><last>Liu</last></author>
      <author><first>Xue</first><last>Zhao</last></author>
      <author><first>Si</first><last>Shen</last></author>
      <author><first>Lihong</first><last>Liu</last></author>
      <author><first>Yan</first><last>Zhu</last></author>
      <author><first>Zhixing</first><last>Xu</last></author>
      <author><first>Weiguang</first><last>Qu</last></author>
      <author><first>Dongbo</first><last>Wang</last></author>
      <pages>156-164</pages>
      <abstract>Ancient Chinese books have great values in history and cultural studies. Named en-tities like person, location, time are cru-cial elements, thus automatic Named En-tity Recognition (NER) is considered a ba-sic task in ancient Chinese text processing. This paper introduces EvaHan2025, the first international ancient Chinese Named Entity Recognition bake-off. The evalua-tion introduces a rigorous benchmark for assessing NER performance across histori-cal and medical texts, covering 12 named entity types. A total of 13 teams par-ticipated in the competition, submitting 77 system runs. In the closed modality, where participants were restricted to us-ing only the training data, the highest F1 scores reached 85.04% on TestA and 90.28% on TestB, both derived from his-torical texts, while performance on medi-cal texts (TestC) reached 84.49%. The re-sults indicate that text genre significantly impacts model performance, with histori-cal texts generally yielding higher scores. Additionally, the intrinsic characteristics of named entities also influence recogni-tion performance. These findings high-light the challenges and opportunities in ancient Chinese NER and underscore the importance of domain adaptation and en-tity type diversity in future research.</abstract>
      <url hash="63b68ef3">2025.alp-1.19</url>
      <bibkey>li-etal-2025-overview</bibkey>
    </paper>
    <paper id="20">
      <title>Construction of <fixed-case>NER</fixed-case> Model in <fixed-case>A</fixed-case>ncient <fixed-case>C</fixed-case>hinese: Solution of <fixed-case>E</fixed-case>va<fixed-case>H</fixed-case>an 2025 Challenge</title>
      <author><first>Yi</first><last>Lu</last></author>
      <author><first>Minyi</first><last>Lei</last></author>
      <pages>165-169</pages>
      <abstract>This paper introduces the system submit-ted for EvaHan 2025, focusing on the Named Entity Recognition (NER) task for ancient Chinese texts. Our solution is built upon two specified pre-trained BERT models, namely GujiRoBERTa_jian_fan and GujiRoBERTa_fan, and further en-hanced by a deep BiLSTM network with a Conditional Random Field (CRF) decod-ing layer. Extensive experiments on three test dataset splits demonstrate that our system’s performance, 84.58% F1 in the closed-modality track and 82.78% F1 in the open-modality track, significantly out-performs the official baseline, achieving no-table improvements in F1 score.</abstract>
      <url hash="ae00c7fe">2025.alp-1.20</url>
      <bibkey>lu-lei-2025-construction</bibkey>
    </paper>
    <paper id="21">
      <title><fixed-case>LLM</fixed-case>’s Weakness in <fixed-case>NER</fixed-case> Doesn’t Stop It from Enhancing a Stronger <fixed-case>SLM</fixed-case></title>
      <author><first>Weilu</first><last>Xu</last></author>
      <author><first>Renfei</first><last>Dang</last></author>
      <author><first>Shujian</first><last>Huang</last></author>
      <pages>170-175</pages>
      <abstract>Large Language Models (LLMs) demonstrate strong semantic understanding ability and extensive knowledge, but struggle with Named Entity Recognition (NER) due to hallucination and high training costs. Meanwhile, supervised Small Language Models (SLMs) efficiently provide structured predictions but lack adaptability to unseen entities and complex contexts. In this study, we investigate how a relatively weaker LLM can effectively support a supervised model in NER tasks. We first improve the LLM using LoRA-based fine-tuning and similarity-based prompting, achieving performance comparable to a SLM baseline. To further improve results, we propose a fusion strategy that integrates both models: prioritising SLM’s predictions while using LLM guidance in low confidence cases. Our hybrid approach outperforms both baselines on three classic Chinese NER datasets.</abstract>
      <url hash="f95956e5">2025.alp-1.21</url>
      <bibkey>xu-etal-2025-llms-weakness</bibkey>
    </paper>
    <paper id="22">
      <title>Named Entity Recognition in Context: <fixed-case>E</fixed-case>dit_<fixed-case>D</fixed-case>unhuang team Technical Report for Evahan2025 <fixed-case>NER</fixed-case> Competition</title>
      <author><first>Colin</first><last>Brisson</last></author>
      <author><first>Ayoub</first><last>Kahfy</last></author>
      <author><first>Marc</first><last>Bui</last></author>
      <author><first>Frédéric</first><last>Constant</last></author>
      <pages>176-181</pages>
      <abstract>We present the Named Entity Recognition sys-tem developed by the Edit Dunhuang team for the EvaHan2025 competition. Our approach in-tegrates three core components: (1) Pindola, a modern transformer-based bidirectional en-coder pretrained on a large corpus of Classi-cal Chinese texts; (2) a retrieval module that fetches relevant external context for each target sequence; and (3) a generative reasoning step that summarizes retrieved context in Classical Chinese for more robust entity disambiguation. Using this approach, we achieve an average F1 score of 85.58, improving upon the competition baseline by nearly 5 points.</abstract>
      <url hash="b1cbb330">2025.alp-1.22</url>
      <bibkey>brisson-etal-2025-named</bibkey>
    </paper>
    <paper id="23">
      <title>Make Good Use of <fixed-case>G</fixed-case>uji<fixed-case>R</fixed-case>o<fixed-case>BERT</fixed-case>a to Identify Entities in <fixed-case>A</fixed-case>ncient <fixed-case>C</fixed-case>hinese</title>
      <author><first>Lihan</first><last>Lin</last></author>
      <author><first>Yiming</first><last>Wang</last></author>
      <author><first>Jiachen</first><last>Li</last></author>
      <author><first>Huan</first><last>Ouyang</last></author>
      <author><first>Si</first><last>Li</last></author>
      <pages>182-186</pages>
      <abstract>This report describes our model submitted for the EvaHan 2025 shared task on named entity recognition for ancient Chinese literary works. Since we participated in the task of closed modality, our method is based on the appointed pretrained language model GujiRoBERTajian-fan and we used appointed datasets.We carried out experiments on decodingstrategies and schedulers to verify the effect of our method. In the final test, our method outperformed the official baseline, demonstrating its effectiveness. In the end, for the results, this report gives an analysis from the perspective of data composition.</abstract>
      <url hash="525fc089">2025.alp-1.23</url>
      <bibkey>lin-etal-2025-make</bibkey>
    </paper>
    <paper id="24">
      <title><fixed-case>GR</fixed-case>o<fixed-case>WE</fixed-case>: A <fixed-case>G</fixed-case>uji<fixed-case>R</fixed-case>o<fixed-case>BERT</fixed-case>a-Enhanced Approach to <fixed-case>A</fixed-case>ncient <fixed-case>C</fixed-case>hinese <fixed-case>NER</fixed-case> via Word-Word Relation Classification and Model Ensembling</title>
      <author><first>Tian</first><last>Xia</last></author>
      <author><first>Yilin</first><last>Wang</last></author>
      <author><first>Xinkai</first><last>Wang</last></author>
      <author><first>Yahe</first><last>Yang</last></author>
      <author><first>Qun</first><last>Zhao</last></author>
      <author><first>Menghui</first><last>Yang</last></author>
      <pages>187-191</pages>
      <abstract>Named entity recognition is a fundamental task in ancient Chinese text analysis.Based on the pre-trained language model of ancient Chinese texts, this paper proposes a new named entity recognition method GRoWE. It uses the ancient Chinese texts pre-trained language model GujiRoBERTa as the base model, and the wordword relation prediction model is superposed upon the base model to construct a superposition model. Then ensemble strategies are used to multiple superposition models. On the EvaHan 2025 public test set, the F1 value of the proposed method reaches 86.79%, which is 6.18% higher than that of the mainstream BERT_LSTM_CRF baseline model, indicating that the model architecture and ensemble strategy play an important role in improving the recognition effect of naming entities in ancient Chinese texts.</abstract>
      <url hash="95f3bd2a">2025.alp-1.24</url>
      <bibkey>xia-etal-2025-growe</bibkey>
    </paper>
    <paper id="25">
      <title>When Less Is More: Logits-Constrained Framework with <fixed-case>R</fixed-case>o<fixed-case>BERT</fixed-case>a for <fixed-case>A</fixed-case>ncient <fixed-case>C</fixed-case>hinese <fixed-case>NER</fixed-case></title>
      <author><first>Wenjie</first><last>Hua</last></author>
      <author><first>Shenghan</first><last>Xu</last></author>
      <pages>192-196</pages>
      <abstract>This report presents our team’s work on ancient Chinese Named Entity Recognition (NER) for EvaHan 20251. We propose a two-stage framework combining GujiRoBERTa with a Logits-Constrained (LC) mechanism. The first stage generates contextual embeddings using GujiRoBERTa, followed by dynamically masked decoding to enforce valid BMES transitions. Experiments on EvaHan 2025 datasets demonstrate the framework’s effectiveness. Key findings include the LC framework’s superiority over CRFs in high-label scenarios and the detrimental effect of BiLSTM modules. We also establish empirical model selection guidelines based on label complexity and dataset size.</abstract>
      <url hash="f179513c">2025.alp-1.25</url>
      <bibkey>hua-xu-2025-less</bibkey>
    </paper>
    <paper id="26">
      <title>Lemmatization of Cuneiform Languages Using the <fixed-case>B</fixed-case>y<fixed-case>T</fixed-case>5 Model</title>
      <author><first>Pengxiu</first><last>Lu</last></author>
      <author><first>Yonglong</first><last>Huang</last></author>
      <author><first>Jing</first><last>Xu</last></author>
      <author><first>Minxuan</first><last>Feng</last></author>
      <author><first>Chao</first><last>Xu</last></author>
      <pages>197-205</pages>
      <abstract>Lemmatization of cuneiform languages presents a unique challenge due to their complex writing system, which combines syllabic and logographic elements. In this study, we investigate the effectiveness of the ByT5 model in addressing this challenge by developing and evaluating a ByT5-based lemmatization system. Experimental results demonstrate that ByT5 outperforms mT5 in this task, achieving an accuracy of 80.55% on raw lemmas and 82.59% on generalized lemmas, where sense numbers are removed. These findings highlight the potential of ByT5 for lemmatizing cuneiform languages and provide useful insights for future work on ancient text lemmatization.</abstract>
      <url hash="e9306192">2025.alp-1.26</url>
      <bibkey>lu-etal-2025-lemmatization</bibkey>
    </paper>
    <paper id="27">
      <title>Simple Named Entity Recognition (<fixed-case>NER</fixed-case>) System with <fixed-case>R</fixed-case>o<fixed-case>BERT</fixed-case>a for <fixed-case>A</fixed-case>ncient <fixed-case>C</fixed-case>hinese</title>
      <author><first>Yunmeng</first><last>Zhang</last></author>
      <author><first>Meiling</first><last>Liu</last></author>
      <author><first>Hanqi</first><last>Tang</last></author>
      <author><first>Shige</first><last>Lu</last></author>
      <author><first>Lang</first><last>Xue</last></author>
      <pages>206-212</pages>
      <abstract>Named Entity Recognition (NER) is a fun-damental task in Natural Language Process-ing (NLP), particularly in the analysis of Chi-nese historical texts. In this work, we pro-pose an innovative NER model based on Gu-jiRoBERTa, incorporating Conditional Ran-dom Fields (CRF) and Long Short Term Mem-ory Network(LSTM) to enhance sequence la-beling performance. Our model is evaluated on three datasets from the EvaHan2025 competi-tion, demonstrating superior performance over the baseline model, SikuRoBERTa-BiLSTM-CRF. The proposed approach effectively cap-tures contextual dependencies and improves entity boundary recognition. Experimental re-sults show that our method achieves consistent improvements across almost all evaluation met-rics, highlighting its robustness and effective-ness in handling ancient Chinese texts.</abstract>
      <url hash="1c4adbcf">2025.alp-1.27</url>
      <bibkey>zhang-etal-2025-simple</bibkey>
    </paper>
    <paper id="28">
      <title>Multi-Strategy Named Entity Recognition System for <fixed-case>A</fixed-case>ncient <fixed-case>C</fixed-case>hinese</title>
      <author><first>Wenxuan</first><last>Dong</last></author>
      <author><first>Meiling</first><last>Liu</last></author>
      <pages>213-220</pages>
      <abstract>We present a multi-strategy Named Entity Recognition (NER) system for ancient Chi-nese texts in EvaHan2025. Addressing dataset heterogeneity, we use a Conditional Random Field (CRF) for Tasks A and C to handle six entity types’ complex dependencies, and a lightweight Softmax classifier for Task B’s simpler three-entity tagset. Ablation studies on training data confirm CRF’s superiority in capturing sequence dependencies and Softmax’s computational advantage for simpler tasks. On blind tests, our system achieves F1-scores of 83.94%, 88.31%, and 82.15% for Test A, B, and C—outperforming baselines by 2.46%, 0.81%, and 9.75%. With an overall F1 improvement of 4.30%, it excels across historical and medical domains. This adaptability enhances knowledge extraction from ancient texts, offering a scalable NER framework for low-resource, complex languages.</abstract>
      <url hash="bab609b7">2025.alp-1.28</url>
      <bibkey>dong-liu-2025-multi</bibkey>
    </paper>
    <paper id="29">
      <title>Finetuning <fixed-case>LLM</fixed-case>s for <fixed-case>E</fixed-case>va<fixed-case>C</fixed-case>un 2025 token prediction shared task</title>
      <author><first>Josef</first><last>Jon</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <pages>221-225</pages>
      <abstract>In this paper, we present our submission for the token prediction task of EvaCun 2025. Our sys-tems are based on LLMs (Command-R, Mistral, and Aya Expanse) fine-tuned on the task data provided by the organizers. As we only pos-sess a very superficial knowledge of the subject field and the languages of the task, we simply used the training data without any task-specific adjustments, preprocessing, or filtering. We compare 3 different approaches (based on 3 different prompts) of obtaining the predictions, and we evaluate them on a held-out part of the data.</abstract>
      <url hash="6d76c4b9">2025.alp-1.29</url>
      <bibkey>jon-bojar-2025-finetuning</bibkey>
    </paper>
    <paper id="30">
      <title>Beyond Base Predictors: Using <fixed-case>LLM</fixed-case>s to Resolve Ambiguities in <fixed-case>A</fixed-case>kkadian Lemmatization</title>
      <author><first>Frederick</first><last>Riemenschneider</last></author>
      <pages>226-231</pages>
      <abstract>We present a hybrid approach for Akkadian lemmatization in the EvaCun 2025 Shared Task that combines traditional NLP techniques with large language models (LLMs). Our system employs three Base Predictors–a dictionary lookup and two T5 models–to establish initial lemma candidates. For cases where these pre-dictors disagree (18.72% of instances), we im-plement an LLM Resolution module, enhanced with direct access to the electronic Babylonian Library (eBL) dictionary entries. This module includes a Predictor component that generates initial lemma predictions based on dictionary information, and a Validator component that refines these predictions through contextual rea-soning. Error analysis reveals that the system struggles most with small differences (like cap-italization) and certain ambiguous logograms (like BI). Our work demonstrates the benefits of combining traditional NLP approaches with the reasoning capabilities of LLMs when provided with appropriate domain knowledge.</abstract>
      <url hash="6fe0e0fb">2025.alp-1.30</url>
      <bibkey>riemenschneider-2025-beyond</bibkey>
    </paper>
    <paper id="31">
      <title>A Low-Shot Prompting Approach to Lemmatization in the <fixed-case>E</fixed-case>va<fixed-case>C</fixed-case>un 2025 Shared Task</title>
      <author><first>John</first><last>Sbur</last></author>
      <author><first>Brandi</first><last>Wilkins</last></author>
      <author><first>Elizabeth</first><last>Paul</last></author>
      <author><first>Yudong</first><last>Liu</last></author>
      <pages>232-236</pages>
      <abstract>This study explores the use of low-shot prompt-ing techniques for the lemmatization of ancient cuneiform languages using Large Language Models (LLMs). To structure the input data and systematically design effective prompt tem-plates, we employed a hierarchical clustering approach based on Levenshtein distance The prompt design followed established engineer-ing patterns, incorporating instructional and response-guiding elements to enhance model comprehension. We employed the In-Context Learning (ICL) prompting strategy, selecting example words primarily based on lemma fre-quency, ensuring a balance between commonly occurring words and rare cases to improve gen-eralization. During testing on the develop-ment set, prompts included structured examples and explicit formatting rules, with accuracy assessed by comparing model predictions to ground truth lemmas. The results showed that model performance varied significantly across different configurations, with accuracy reach-ing approximately 90% in the best case for in-vocabulary words and around 9% in the best case for out-of-vocabulary (OOV) words. De-spite resource constraints and the lack of input from a language expert, oour findings suggest that prompt engineering strategies hold promise for improving LLM performance in cuneiform language lemmatization.</abstract>
      <url hash="8b02a0cb">2025.alp-1.31</url>
      <bibkey>sbur-etal-2025-low</bibkey>
    </paper>
    <paper id="32">
      <title>Multi-Domain <fixed-case>A</fixed-case>ncient <fixed-case>C</fixed-case>hinese Named Entity Recognition Based on Attention-Enhanced Pre-trained Language Model</title>
      <author><first>Qi</first><last>Zhang</last></author>
      <author><first>Zhiya</first><last>Duan</last></author>
      <author><first>Shijie</first><last>Ma</last></author>
      <author><first>Shengyu</first><last>Liu</last></author>
      <author><first>Zibo</first><last>Yuan</last></author>
      <author><first>RuiMin</first><last>Ma</last></author>
      <pages>237-241</pages>
      <abstract>Recent advancements in digital humanities have intensified the demand for intelligent processing of ancient Chinese texts, particularly across specialized domains such as historical records and ancient medical literature. Among related research areas, Named Entity Recognition (NER) plays a crucial role, serving as the foundation for knowledge graph construction and deeper humanities computing studies. In this paper, we introduce a architecture specifically designed for multi-domain ancient Chinese NER tasks based on a pre-trained language model (PLM). Building upon the GujiRoberta backbone, we propose the GujiRoberta-BiLSTM-Attention-CRF model. Experimental results on three distinct domain-specific datasets demonstrate that our approach significantly outperforms the official baselines across all three datasets, highlighting the particular effectiveness of integrating an attention mechanism within our architecture.</abstract>
      <url hash="ccc6e792">2025.alp-1.32</url>
      <bibkey>zhang-etal-2025-multi</bibkey>
    </paper>
    <paper id="33">
      <title><fixed-case>E</fixed-case>va<fixed-case>C</fixed-case>un 2025 Shared Task: Lemmatization and Token Prediction in <fixed-case>A</fixed-case>kkadian and <fixed-case>S</fixed-case>umerian using <fixed-case>LLM</fixed-case>s</title>
      <author><first>Shai</first><last>Gordin</last></author>
      <author><first>Aleksi</first><last>Sahala</last></author>
      <author><first>Shahar</first><last>Spencer</last></author>
      <author><first>Stav</first><last>Klein</last></author>
      <pages>242-250</pages>
      <abstract>The EvaCun 2025 Shared Task, organized as part of ALP 2025 workshop and co-located with NAACL 2025, explores how Large Language Models (LLMs) and transformer-based models can be used to improve lemmatization and token prediction tasks for low-resource ancient cuneiform texts. This year our datasets focused on the best attested ancient Near Eastern languages written in cuneiform, namely, Akkadian and Sumerian texts. However, we utilized the availability of datasets never before used on scale in NLP tasks, primarily first millennium literature (i.e. “Canonical”) provided by the Electronic Babylonian Library (eBL), and Old Babylonian letters and archival texts, provided by Archibab. We aim to encourage the development of new computational methods to better analyze and reconstruct cuneiform inscriptions, pushing NLP forward for ancient and low-resource languages. Three teams competed for the lemmatization subtask and one for the token prediction subtask. Each subtask was evaluated alongside a baseline model, provided by the organizers.</abstract>
      <url hash="d7090bbc">2025.alp-1.33</url>
      <bibkey>gordin-etal-2025-evacun</bibkey>
    </paper>
  </volume>
</collection>
