<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.gebnlp">
  <volume id="1" ingest-date="2024-07-25" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 5th Workshop on Gender Bias in Natural Language Processing (GeBNLP)</booktitle>
      <editor><first>Agnieszka</first><last>Faleńska</last></editor>
      <editor><first>Christine</first><last>Basta</last></editor>
      <editor><first>Marta</first><last>Costa-jussà</last></editor>
      <editor><first>Seraphina</first><last>Goldfarb-Tarrant</last></editor>
      <editor><first>Debora</first><last>Nozza</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Bangkok, Thailand</address>
      <month>August</month>
      <year>2024</year>
      <url hash="92b04185">2024.gebnlp-1</url>
      <venue>gebnlp</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="4c8e5205">2024.gebnlp-1.0</url>
      <bibkey>gebnlp-2024-gender</bibkey>
    </frontmatter>
    <paper id="1">
      <title>A Parameter-Efficient Multi-Objective Approach to Mitigate Stereotypical Bias in Language Models</title>
      <author><first>Yifan</first><last>Wang</last></author>
      <author><first>Vera</first><last>Demberg</last><affiliation>Universität des Saarlandes</affiliation></author>
      <pages>1-19</pages>
      <abstract>Pre-trained language models have shown impressive abilities of understanding and generating natural languages. However, they typically inherit undesired human-like bias and stereotypes from training data, which raises concerns about putting these models into use in real-world scenarios. Although prior research has proposed to reduce bias using different fairness objectives, they usually fail to capture different representations of bias and, therefore, struggle with fully debiasing models. In this work, we introduce a multi-objective probability alignment approach to overcome current challenges by incorporating multiple debiasing losses to locate and penalize bias in different forms. Compared to existing methods, our proposed method can more effectively and comprehensively reduce stereotypical bias, and maintains the language ability of pre-trained models at the same time. Besides, we adopt prefix-tuning to optimize fairness objectives, and results show that it can achieve better bias removal than full fine-tuning while requiring much fewer computational resources. Our code and data are available at https://github.com/Ewanwong/debias_NLG.</abstract>
      <url hash="986eda18">2024.gebnlp-1.1</url>
      <bibkey>wang-demberg-2024-parameter</bibkey>
      <doi>10.18653/v1/2024.gebnlp-1.1</doi>
    </paper>
    <paper id="2">
      <title>Do <fixed-case>PLM</fixed-case>s and Annotators Share the Same Gender Bias? Definition, Dataset, and Framework of Contextualized Gender Bias</title>
      <author><first>Shucheng</first><last>Zhu</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Bingjie</first><last>Du</last></author>
      <author><first>Jishun</first><last>Zhao</last></author>
      <author><first>Ying</first><last>Liu</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Pengyuan</first><last>Liu</last><affiliation>Beijing Language and Culture University</affiliation></author>
      <pages>20-32</pages>
      <abstract>Pre-trained language models (PLMs) have achieved success in various of natural language processing (NLP) tasks. However, PLMs also introduce some disquieting safety problems, such as gender bias. Gender bias is an extremely complex issue, because different individuals may hold disparate opinions on whether the same sentence expresses harmful bias, especially those seemingly neutral or positive. This paper first defines the concept of contextualized gender bias (CGB), which makes it easy to measure implicit gender bias in both PLMs and annotators. We then construct CGBDataset, which contains 20k natural sentences with gendered words, from Chinese news. Similar to the task of masked language models, gendered words are masked for PLMs and annotators to judge whether a male word or a female word is more suitable. Then, we introduce CGBFrame to measure the gender bias of annotators. By comparing the results measured by PLMs and annotators, we find that though there are differences on the choices made by PLMs and annotators, they show significant consistency in general.</abstract>
      <url hash="c96c4e8a">2024.gebnlp-1.2</url>
      <bibkey>zhu-etal-2024-plms</bibkey>
      <doi>10.18653/v1/2024.gebnlp-1.2</doi>
    </paper>
    <paper id="3">
      <title>We Don’t Talk About That: Case Studies on Intersectional Analysis of Social Bias in Large Language Models</title>
      <author><first>Hannah</first><last>Devinney</last></author>
      <author><first>Jenny</first><last>Björklund</last><affiliation>Uppsala University</affiliation></author>
      <author><first>Henrik</first><last>Björklund</last><affiliation>Dept. Computing Science, Umeå University</affiliation></author>
      <pages>33-44</pages>
      <abstract>Despite concerns that Large Language Models (LLMs) are vectors for reproducing and amplifying social biases such as sexism, transphobia, islamophobia, and racism, there is a lack of work qualitatively analyzing <i>how</i> such patterns of bias are generated by LLMs. We use mixed-methods approaches and apply a feminist, intersectional lens to the problem across two language domains, Swedish and English, by generating narrative texts using LLMs. We find that hegemonic norms are consistently reproduced; dominant identities are often treated as ‘default’; and discussion of identity itself may be considered ‘inappropriate’ by the safety features applied to some LLMs. Due to the differing behaviors of models, depending both on their design and the language they are trained on, we observe that strategies of identifying “bias” must be adapted to individual models and their socio-cultural contexts._Content warning: This research concerns the identification of harms, including stereotyping, denigration, and erasure of minoritized groups. Examples, including transphobic and racist content, are included and discussed._</abstract>
      <url hash="2e7a8dcb">2024.gebnlp-1.3</url>
      <bibkey>devinney-etal-2024-dont</bibkey>
      <doi>10.18653/v1/2024.gebnlp-1.3</doi>
    </paper>
    <paper id="4">
      <title>An Explainable Approach to Understanding Gender Stereotype Text</title>
      <author><first>Manuela</first><last>Jeyaraj</last></author>
      <author><first>Sarah</first><last>Delany</last><affiliation>Technological University Dublin</affiliation></author>
      <pages>45-59</pages>
      <abstract>Gender Stereotypes refer to the widely held beliefs and assumptions about the typical traits, behaviours, and roles associated with a collective group of individuals of a particular gender in society. These typical beliefs about how people of a particular gender are described in text can cause harmful effects to individuals leading to unfair treatment. In this research, the aim is to identify the words and language constructs that can influence a text to be considered a gender stereotype. To do so, a transformer model with attention is fine-tuned for gender stereotype detection. Thereafter, words/language constructs used for the model’s decision are identified using a combined use of attention- and SHAP (SHapley Additive exPlanations)-based explainable approaches. Results show that adjectives and verbs were highly influential in predicting gender stereotypes. Furthermore, applying sentiment analysis showed that words describing male gender stereotypes were more positive than those used for female gender stereotypes.</abstract>
      <url hash="e6f968bf">2024.gebnlp-1.4</url>
      <bibkey>jeyaraj-delany-2024-explainable</bibkey>
      <doi>10.18653/v1/2024.gebnlp-1.4</doi>
    </paper>
    <paper id="5">
      <title>A Fairness Analysis of Human and <fixed-case>AI</fixed-case>-Generated Student Reflection Summaries</title>
      <author><first>Bhiman</first><last>Baghel</last><affiliation>University of Pittsburgh</affiliation></author>
      <author><first>Arun Balajiee</first><last>Lekshmi Narayanan</last><affiliation>University of Pittsburgh</affiliation></author>
      <author><first>Michael Miller</first><last>Yoder</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <pages>60-77</pages>
      <abstract>This study examines the fairness of human- and AI-generated summaries of student reflections in university STEM classes, focusing on potential gender biases. Using topic modeling, we first identify topics that are more prevalent in reflections from female students and others that are more common among male students. We then analyze whether human and AI-generated summaries reflect the concerns of students of any particular gender over others. Our analysis reveals that though human-generated and extractive AI summarization techniques do not show a clear bias, abstractive AI-generated summaries exhibit a bias towards male students. Pedagogical themes are over-represented from male reflections in these summaries, while concept-specific topics are under-represented from female reflections. This research contributes to a deeper understanding of AI-generated bias in educational contexts, highlighting the need for future work on mitigating these biases.</abstract>
      <url hash="4da27e8b">2024.gebnlp-1.5</url>
      <bibkey>baghel-etal-2024-fairness</bibkey>
      <doi>10.18653/v1/2024.gebnlp-1.5</doi>
    </paper>
    <paper id="6">
      <title>On Shortcuts and Biases: How Finetuned Language Models Distinguish Audience-Specific Instructions in <fixed-case>I</fixed-case>talian and <fixed-case>E</fixed-case>nglish</title>
      <author><first>Nicola</first><last>Fanton</last><affiliation>University of Stuttgart, Universität Stuttgart</affiliation></author>
      <author><first>Michael</first><last>Roth</last><affiliation>University of Stuttgart</affiliation></author>
      <pages>78-93</pages>
      <abstract>Instructional texts for different audience groups can help to address specific needs, but at the same time run the risk of perpetrating biases. In this paper, we extend previous findings on disparate social norms and subtle stereotypes in wikiHow in two directions: We explore the use of fine-tuned language models to determine how audience-specific instructional texts can be distinguished and we transfer the methodology to another language, Italian, to identify cross-linguistic patterns. We find that language models mostly rely on group terms, gender markings, and attributes reinforcing stereotypes.</abstract>
      <url hash="53076bc8">2024.gebnlp-1.6</url>
      <bibkey>fanton-roth-2024-shortcuts</bibkey>
      <doi>10.18653/v1/2024.gebnlp-1.6</doi>
    </paper>
    <paper id="7">
      <title>The power of Prompts: Evaluating and Mitigating Gender Bias in <fixed-case>MT</fixed-case> with <fixed-case>LLM</fixed-case>s</title>
      <author><first>Aleix</first><last>Sant</last></author>
      <author><first>Carlos</first><last>Escolano</last><affiliation>Barcelona Supercomputing Center</affiliation></author>
      <author><first>Audrey</first><last>Mash</last><affiliation>Barcelona Supercomputing Center</affiliation></author>
      <author><first>Francesca</first><last>De Luca Fornaciari</last><affiliation>Barcelona Supercomputing Center and Universidad del País Vasco</affiliation></author>
      <author><first>Maite</first><last>Melero</last><affiliation>Barcelona Supercomputing Center</affiliation></author>
      <pages>94-139</pages>
      <abstract>This paper studies gender bias in machine translation through the lens of Large Language Models (LLMs). Four widely-used test sets are employed to benchmark various base LLMs, comparing their translation quality and gender bias against state-of-the-art Neural Machine Translation (NMT) models for English to Catalan (En → Ca) and English to Spanish (En → Es) translation directions. Our findings reveal pervasive gender bias across all models, with base LLMs exhibiting a higher degree of bias compared to NMT models.To combat this bias, we explore prompting engineering techniques applied to an instruction-tuned LLM. We identify a prompt structure that significantly reduces gender bias by up to 12% on the WinoMT evaluation dataset compared to more straightforward prompts. These results significantly reduce the gender bias accuracy gap between LLMs and traditional NMT systems.</abstract>
      <url hash="0fe5b2c8">2024.gebnlp-1.7</url>
      <bibkey>sant-etal-2024-power</bibkey>
      <doi>10.18653/v1/2024.gebnlp-1.7</doi>
    </paper>
    <paper id="8">
      <title>Detecting Gender Discrimination on Actor Level Using Linguistic Discourse Analysis</title>
      <author><first>Stefanie</first><last>Urchs</last><affiliation>Ludwig-Maximilians-Universität München and Hochschule München</affiliation></author>
      <author><first>Veronika</first><last>Thurner</last><affiliation>Hochschule München</affiliation></author>
      <author><first>Matthias</first><last>Aßenmacher</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Christian</first><last>Heumann</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Stephanie</first><last>Thiemichen</last><affiliation>Hochschule München</affiliation></author>
      <pages>140-149</pages>
      <abstract>With the usage of tremendous amounts of text data for training powerful large language models such as ChatGPT, the issue of analysing and securing data quality has become more pressing than ever. Any biases, stereotypes and discriminatory patterns that exist in the training data can be reproduced, reinforced or broadly disseminated by the models in production. Therefore, it is crucial to carefully select and monitor the text data that is used as input to train the model. Due to the vast amount of training data, this process needs to be (at least partially) automated. In this work, we introduce a novel approach for automatically detecting gender discrimination in text data on the actor level based on linguistic discourse analysis. Specifically, we combine existing information extraction (IE) techniques to partly automate the qualitative research done in linguistic discourse analysis. We focus on two important steps: Identifying the respectiveperson-named-entity (an actor) and all forms it is referred to (Nomination), and detecting the characteristics it is ascribed (Predication). Asa proof of concept, we integrate these two steps into a pipeline for automated text analysis. The separate building blocks of the pipeline could be flexibly adapted, extended, and scaled for bigger datasets to accommodate a wide range of usage scenarios and specific ML tasks or help social scientists with analysis tasks. We showcase and evaluate our approach on several real and simulated exemplary texts.</abstract>
      <url hash="968886cf">2024.gebnlp-1.8</url>
      <bibkey>urchs-etal-2024-detecting</bibkey>
      <doi>10.18653/v1/2024.gebnlp-1.8</doi>
    </paper>
    <paper id="9">
      <title>What Can Go Wrong in Authorship Profiling: Cross-Domain Analysis of Gender and Age Prediction</title>
      <author><first>Hongyu</first><last>Chen</last></author>
      <author><first>Michael</first><last>Roth</last><affiliation>University of Stuttgart</affiliation></author>
      <author><first>Agnieszka</first><last>Falenska</last><affiliation>Interchange Forum for Reflecting on Intelligent Systems, University of Stuttgart</affiliation></author>
      <pages>150-166</pages>
      <abstract>Authorship Profiling (AP) aims to predict the demographic attributes (such as gender and age) of authors based on their writing styles. Ever-improving models mean that this task is gaining interest and application possibilities. However, with greater use also comes the risk that authors are misclassified more frequently, and it remains unclear to what extent the better models can capture the bias and who is affected by the models’ mistakes. In this paper, we investigate three established datasets for AP as well as classical and neural classifiers for this task. Our analyses show that it is often possible to predict the demographic information of the authors based on textual features. However, some features learned by the models are specific to datasets. Moreover, models are prone to errors based on stereotypes associated with topical bias.</abstract>
      <url hash="39c753d4">2024.gebnlp-1.9</url>
      <bibkey>chen-etal-2024-go</bibkey>
      <doi>10.18653/v1/2024.gebnlp-1.9</doi>
    </paper>
    <paper id="10">
      <title>Towards Fairer <fixed-case>NLP</fixed-case> Models: Handling Gender Bias In Classification Tasks</title>
      <author><first>Nasim</first><last>Sobhani</last></author>
      <author><first>Sarah</first><last>Delany</last><affiliation>Technological University Dublin</affiliation></author>
      <pages>167-178</pages>
      <abstract>Measuring and mitigating gender bias in natural language processing (NLP) systems is crucial to ensure fair and ethical AI. However, a key challenge is the lack of explicit gender information in many textual datasets. This paper proposes two techniques, Identity Term Sampling (ITS) and Identity Term Pattern Extraction (ITPE), as alternatives to template-based approaches for measuring gender bias in text data. These approaches identify test data for measuring gender bias in the dataset itself and can be used to measure gender bias on any NLP classifier. We demonstrate the use of these approaches for measuring gender bias across various NLP classification tasks, including hate speech detection, fake news identification, and sentiment analysis. Additionally, we show how these techniques can benefit gender bias mitigation, proposing a variant of Counterfactual Data Augmentation (CDA), called Gender-Selective CDA (GS-CDA), which reduces the amount of data augmentation required in training data while effectively mitigating gender bias and maintaining overall classification performance.</abstract>
      <url hash="f4ef4c93">2024.gebnlp-1.10</url>
      <bibkey>sobhani-delany-2024-towards</bibkey>
      <doi>10.18653/v1/2024.gebnlp-1.10</doi>
    </paper>
    <paper id="11">
      <title>Investigating Gender Bias in <fixed-case>STEM</fixed-case> Job Advertisements</title>
      <author><first>Malika</first><last>Dikshit</last></author>
      <author><first>Houda</first><last>Bouamor</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Nizar</first><last>Habash</last><affiliation>New York University Abu Dhabi</affiliation></author>
      <pages>179-189</pages>
      <abstract>Gender inequality has been historically prevalent in academia, especially within the fields of Science, Technology, Engineering, and Mathematics (STEM). In this study, we propose to examine gender bias in academic job descriptions in the STEM fields. We go a step further than previous studies that merely identify individual words as masculine-coded and feminine-coded and delve into the contextual language used in academic job advertisements. We design a novel approach to detect gender biases in job descriptions using Natural Language Processing techniques. Going beyond binary masculine-feminine stereotypes, we propose three big group types to understand gender bias in the language of job descriptions, namely agentic, balanced, and communal. We cluster similar information in job descriptions into these three groups using contrastive learning and various clustering techniques. This research contributes to the field of gender bias detection by providing a novel approach and methodology for categorizing gender bias in job descriptions, which can aid more effective and targeted job advertisements that will be equally appealing across all genders.</abstract>
      <url hash="154dcf04">2024.gebnlp-1.11</url>
      <bibkey>dikshit-etal-2024-investigating</bibkey>
      <doi>10.18653/v1/2024.gebnlp-1.11</doi>
    </paper>
    <paper id="12">
      <title>Dissecting Biases in Relation Extraction: A Cross-Dataset Analysis on People’s Gender and Origin</title>
      <author><first>Marco</first><last>Stranisci</last></author>
      <author><first>Pere-Lluís</first><last>Huguet Cabot</last></author>
      <author><first>Elisa</first><last>Bassignana</last></author>
      <author><first>Roberto</first><last>Navigli</last><affiliation>Sapienza University of Rome</affiliation></author>
      <pages>190-202</pages>
      <abstract>Relation Extraction (RE) is at the core of many Natural Language Understanding tasks, including knowledge-base population and Question Answering. However, any Natural Language Processing system is exposed to biases, and the analysis of these has not received much attention in RE. We propose a new method for inspecting bias in the RE pipeline, which is completely transparent in terms of interpretability. Specifically, in this work we analyze biases related to gender and place of birth. Our methodology includes (i) obtaining semantic triplets (subject, object, semantic relation) involving ‘person’ entities from RE resources, (ii) collecting meta-information (‘gender’ and ‘place of birth’) using Entity Linking technologies, and then (iii) analyze the distribution of triplets across different groups (e.g., men versus women). We investigate bias at two levels: In the training data of three commonly used RE datasets (SREDFM, CrossRE, NYT), and in the predictions of a state-of-the-art RE approach (ReLiK). To enable cross-dataset analysis, we introduce a taxonomy of relation types mapping the label sets of different RE datasets to a unified label space. Our findings reveal that bias is a compounded issue affecting underrepresented groups within data and predictions for RE.</abstract>
      <url hash="3623c60f">2024.gebnlp-1.12</url>
      <bibkey>stranisci-etal-2024-dissecting</bibkey>
      <doi>10.18653/v1/2024.gebnlp-1.12</doi>
    </paper>
    <paper id="13">
      <title>Gender Bias in <fixed-case>T</fixed-case>urkish Word Embeddings: A Comprehensive Study of Syntax, Semantics and Morphology Across Domains</title>
      <author><first>Duygu</first><last>Altinok</last></author>
      <pages>203-218</pages>
      <abstract>Gender bias in word representations has emerged as a prominent research area in recent years. While numerous studies have focused on measuring and addressing bias in English word embeddings, research on the Turkish language remains limited. This work aims to bridge this gap by conducting a comprehensive evaluation of gender bias in Turkish word embeddings, considering the dimensions of syntax, semantics, and morphology. We employ subword-based static word vectors trained on three distinct domains: web crawl, academical text, and medical text. Through the analysis of gender-associated words in each domain, we not only uncover gender bias but also gain insights into the unique characteristics of these domains. Additionally, we explore the influence of Turkish suffixes on word gender, providing a novel perspective on gender bias. Our findings reveal the pervasive nature of gender biases across various aspects of the Turkish language, including word frequency, semantics, parts-of-speech, and even the smallest linguistic unit - suffixes. Notably, we demonstrate that the majority of noun and verb lemmas, as well as adverbs and adjectives, exhibit masculine gendering in the general-purpose written language. This study is the first of its kind to offer a comprehensive examination of gender bias in the Turkish language.</abstract>
      <url hash="a1307356">2024.gebnlp-1.13</url>
      <bibkey>altinok-2024-gender</bibkey>
      <doi>10.18653/v1/2024.gebnlp-1.13</doi>
    </paper>
    <paper id="14">
      <title>Disagreeable, Slovenly, Honest and Un-named Women? Investigating Gender Bias in <fixed-case>E</fixed-case>nglish Educational Resources by Extending Existing Gender Bias Taxonomies</title>
      <author><first>Haotian</first><last>Zhu</last></author>
      <author><first>Kexin</first><last>Gao</last><affiliation>University of Washington</affiliation></author>
      <author><first>Fei</first><last>Xia</last><affiliation>University of Washington, Seattle</affiliation></author>
      <author><first>Mari</first><last>Ostendorf</last><affiliation>University of Washington</affiliation></author>
      <pages>219-236</pages>
      <abstract>Gender bias has been extensively studied in both the educational field and the Natural Language Processing (NLP) field, the former using human coding to identify patterns associated with and causes of gender bias in text and the latter to detect, measure and mitigate gender bias in NLP output and models. This work aims to use NLP to facilitate automatic, quantitative analysis of educational text within the framework of a gender bias taxonomy. Analyses of both educational texts and a lexical resource (WordNet) reveal patterns of bias that can inform and aid educators in updating textbooks and lexical resources and in designing assessment items.</abstract>
      <url hash="50682e27">2024.gebnlp-1.14</url>
      <bibkey>zhu-etal-2024-disagreeable</bibkey>
      <doi>10.18653/v1/2024.gebnlp-1.14</doi>
    </paper>
    <paper id="15">
      <title>Generating Gender Alternatives in Machine Translation</title>
      <author><first>Sarthak</first><last>Garg</last><affiliation>Apple</affiliation></author>
      <author><first>Mozhdeh</first><last>Gheini</last><affiliation>USC/ISI</affiliation></author>
      <author><first>Clara</first><last>Emmanuel</last><affiliation>University of New South Wales</affiliation></author>
      <author><first>Tatiana</first><last>Likhomanenko</last><affiliation>Apple</affiliation></author>
      <author><first>Qin</first><last>Gao</last><affiliation>Apple</affiliation></author>
      <author><first>Matthias</first><last>Paulik</last><affiliation>Apple</affiliation></author>
      <pages>237-254</pages>
      <abstract>Machine translation (MT) systems often translate terms with ambiguous gender (e.g., English term “the nurse”) into the gendered form that is most prevalent in the systems’ training data (e.g., “enfermera”, the Spanish term for a female nurse). This often reflects and perpetuates harmful stereotypes present in society. With MT user interfaces in mind that allow for resolving gender ambiguity in a frictionless manner, we study the problem of generating all grammatically correct gendered translation alternatives. We open source train and test datasets for five language pairs and establish benchmarks for this task. Our key technical contribution is a novel semi-supervised solution for generating alternatives that integrates seamlessly with standard MT models and maintains high performance without requiring additional components or increasing inference overhead.</abstract>
      <url hash="b72aae24">2024.gebnlp-1.15</url>
      <bibkey>garg-etal-2024-generating</bibkey>
      <doi>10.18653/v1/2024.gebnlp-1.15</doi>
    </paper>
    <paper id="16">
      <title>Beyond Binary Gender Labels: Revealing Gender Bias in <fixed-case>LLM</fixed-case>s through Gender-Neutral Name Predictions</title>
      <author><first>Zhiwen</first><last>You</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>HaeJin</first><last>Lee</last></author>
      <author><first>Shubhanshu</first><last>Mishra</last><affiliation>shubhanshu.com</affiliation></author>
      <author><first>Sullam</first><last>Jeoung</last></author>
      <author><first>Apratim</first><last>Mishra</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Jinseok</first><last>Kim</last></author>
      <author><first>Jana</first><last>Diesner</last><affiliation>Technische Universität München</affiliation></author>
      <pages>255-268</pages>
      <abstract>Name-based gender prediction has traditionally categorized individuals as either female or male based on their names, using a binary classification system. That binary approach can be problematic in the cases of gender-neutral names that do not align with any one gender, among other reasons. Relying solely on binary gender categories without recognizing gender-neutral names can reduce the inclusiveness of gender prediction tasks. We introduce an additional gender category, i.e., “neutral”, to study and address potential gender biases in Large Language Models (LLMs). We evaluate the performance of several foundational and large language models in predicting gender based on first names only. Additionally, we investigate the impact of adding birth years to enhance the accuracy of gender prediction, accounting for shifting associations between names and genders over time. Our findings indicate that most LLMs identify male and female names with high accuracy (over 80%) but struggle with gender-neutral names (under 40%), and the accuracy of gender prediction is higher for English-based first names than non-English names. The experimental results show that incorporating the birth year does not improve the overall accuracy of gender prediction, especially for names with evolving gender associations. We recommend using caution when applying LLMs for gender identification in downstream tasks, particularly when dealing with non-binary gender labels.</abstract>
      <url hash="3cb2ff4b">2024.gebnlp-1.16</url>
      <bibkey>you-etal-2024-beyond</bibkey>
      <doi>10.18653/v1/2024.gebnlp-1.16</doi>
    </paper>
    <paper id="17">
      <title>Is there Gender Bias in Dependency Parsing? Revisiting “Women’s Syntactic Resilience”</title>
      <author><first>Paul</first><last>Go</last></author>
      <author><first>Agnieszka</first><last>Falenska</last><affiliation>Interchange Forum for Reflecting on Intelligent Systems, University of Stuttgart</affiliation></author>
      <pages>269-279</pages>
      <abstract>In this paper, we revisit the seminal work of Garimella et al. 2019, who reported that dependency parsers learn demographically-related signals from their training data and perform differently on sentences authored by people of different genders. We re-run all the parsing experiments from Garimella et al. 2019 and find that their results are not reproducible. Additionally, the original patterns suggesting the presence of gender biases fail to generalize to other treebank and parsing architecture. Instead, our data analysis uncovers methodological shortcomings in the initial study that artificially introduced differences into female and male datasets during preprocessing. These disparities potentially compromised the validity of the original conclusions.</abstract>
      <url hash="8460b762">2024.gebnlp-1.17</url>
      <bibkey>go-falenska-2024-gender</bibkey>
      <doi>10.18653/v1/2024.gebnlp-1.17</doi>
    </paper>
    <paper id="18">
      <title>From ‘Showgirls’ to ‘Performers’: Fine-tuning with Gender-inclusive Language for Bias Reduction in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Marion</first><last>Bartl</last></author>
      <author><first>Susan</first><last>Leavy</last><affiliation>University College Dublin</affiliation></author>
      <pages>280-294</pages>
      <abstract>Gender bias is not only prevalent in Large Language Models (LLMs) and their training data, but also firmly ingrained into the structural aspects of language itself. Therefore, adapting linguistic structures within LLM training data to promote gender-inclusivity can make gender representations within the model more inclusive.The focus of our work are gender-exclusive affixes in English, such as in ‘show-girl’ or ‘man-cave’, which can perpetuate gender stereotypes and binary conceptions of gender.We use an LLM training dataset to compile a catalogue of 692 gender-exclusive terms along with gender-neutral variants and from this, develop a gender-inclusive fine-tuning dataset, the ‘Tiny Heap’. Fine-tuning three different LLMs with this dataset, we observe an overall reduction in gender-stereotyping tendencies across the models. Our approach provides a practical method for enhancing gender inclusivity in LLM training data and contributes to incorporating queer-feminist linguistic activism in bias mitigation research in NLP.</abstract>
      <url hash="1d8c98d9">2024.gebnlp-1.18</url>
      <bibkey>bartl-leavy-2024-showgirls</bibkey>
      <doi>10.18653/v1/2024.gebnlp-1.18</doi>
    </paper>
    <paper id="19">
      <title>Sociodemographic Bias in Language Models: A Survey and Forward Path</title>
      <author><first>Vipul</first><last>Gupta</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Pranav</first><last>Narayanan Venkit</last></author>
      <author><first>Shomir</first><last>Wilson</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Rebecca</first><last>Passonneau</last><affiliation>Pennsylvania State University</affiliation></author>
      <pages>295-322</pages>
      <abstract>Sociodemographic bias in language models (LMs) has the potential for harm when deployed in real-world settings. This paper presents a comprehensive survey of the past decade of research on sociodemographic bias in LMs, organized into a typology that facilitates examining the different aims: types of bias, quantifying bias, and debiasing techniques. We track the evolution of the latter two questions, then identify current trends and their limitations, as well as emerging techniques. To guide future research towards more effective and reliable solutions, and to help authors situate their work within this broad landscape, we conclude with a checklist of open questions.</abstract>
      <url hash="2bc2ecae">2024.gebnlp-1.19</url>
      <bibkey>gupta-etal-2024-sociodemographic</bibkey>
      <doi>10.18653/v1/2024.gebnlp-1.19</doi>
    </paper>
    <paper id="20">
      <title>Stop! In the Name of Flaws: Disentangling Personal Names and Sociodemographic Attributes in <fixed-case>NLP</fixed-case></title>
      <author><first>Vagrant</first><last>Gautam</last><affiliation>Saarland University</affiliation></author>
      <author><first>Arjun</first><last>Subramonian</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Anne</first><last>Lauscher</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Os</first><last>Keyes</last></author>
      <pages>323-337</pages>
      <abstract>Personal names simultaneously differentiate individuals and categorize them in ways that are important in a given society. While the natural language processing community has thus associated personal names with sociodemographic characteristics in a variety of tasks, researchers have engaged to varying degrees with the established methodological problems in doing so. To guide future work that uses names and sociodemographic characteristics, we provide an overview of relevant research: first, we present an interdisciplinary background on names and naming. We then survey the issues inherent to associating names with sociodemographic attributes, covering problems of validity (e.g., systematic error, construct validity), as well as ethical concerns (e.g., harms, differential impact, cultural insensitivity). Finally, we provide guiding questions along with normative recommendations to avoid validity and ethical pitfalls when dealing with names and sociodemographic characteristics in natural language processing.</abstract>
      <url hash="9355f6e5">2024.gebnlp-1.20</url>
      <bibkey>gautam-etal-2024-stop</bibkey>
      <doi>10.18653/v1/2024.gebnlp-1.20</doi>
    </paper>
    <paper id="21">
      <title>Evaluating Gender Bias in Multilingual Multimodal <fixed-case>AI</fixed-case> Models: Insights from an <fixed-case>I</fixed-case>ndian Context</title>
      <author><first>Kshitish</first><last>Ghate</last></author>
      <author><first>Arjun</first><last>Choudhry</last></author>
      <author><first>Vanya</first><last>Bannihatti Kumar</last></author>
      <pages>338-350</pages>
      <abstract>We evaluate gender biases in multilingual multimodal image and text models in two settings: text-to-image retrieval and text-to-image generation, to show that even seemingly gender-neutral traits generate biased results. We evaluate our framework in the context of people from India, working with two languages: English and Hindi. We work with frameworks built around mCLIP-based models to ensure a thorough evaluation of recent state-of-the-art models in the multilingual setting due to their potential for widespread applications. We analyze the results across 50 traits for retrieval and 8 traits for generation, showing that current multilingual multimodal models are biased towards men for most traits, and this problem is further exacerbated for lower-resource languages like Hindi. We further discuss potential reasons behind this observation, particularly stemming from the bias introduced by the pretraining datasets.</abstract>
      <url hash="d2ee98a9">2024.gebnlp-1.21</url>
      <bibkey>ghate-etal-2024-evaluating</bibkey>
      <doi>10.18653/v1/2024.gebnlp-1.21</doi>
    </paper>
    <paper id="22">
      <title>Detecting and Mitigating <fixed-case>LGBTQIA</fixed-case>+ Bias in Large <fixed-case>N</fixed-case>orwegian Language Models</title>
      <author><first>Selma</first><last>Bergstrand</last></author>
      <author><first>Björn</first><last>Gambäck</last><affiliation>Norwegian University of Science and Technology</affiliation></author>
      <pages>351-364</pages>
      <abstract>The paper aims to detect and mitigate LGBTQIA+ bias in large language models (LLMs). As the usage of LLMs quickly increases, so does the significance of the harms they may cause due to bias. The research field of bias in LLMs has seen massive growth, but few attempts have been made to detect or mitigate other biases than gender bias, and most focus has been on English LLMs. This work shows experimentally that LLMs may cause representational harms towards LGBTQIA+ individuals when evaluated on sentence completion tasks and on a benchmark dataset constructed from stereotypes reported by the queer community of Norway, collected through a survey in order to directly involve the affected community. Furthermore, Norwegian training corpora are probed for queer bias, revealing strong associations between queer terms and anti-queer slurs, as well as words related to pedophilia. Finally, a fine-tuning-based debiasing method is applied to two Norwegian LLMs. This method does not consistently reduce bias, but shows that queer bias can be altered, laying the foundation for future debiasing approaches. By shedding light on the severe discrimination that can occur through the usage of LLMs, this paper contributes to the ongoing fight for equal rights for the LGBTQIA+ community.</abstract>
      <url hash="fda56984">2024.gebnlp-1.22</url>
      <bibkey>bergstrand-gamback-2024-detecting</bibkey>
      <doi>10.18653/v1/2024.gebnlp-1.22</doi>
    </paper>
    <paper id="23">
      <title>Whose wife is it anyway? Assessing bias against same-gender relationships in machine translation</title>
      <author><first>Ian</first><last>Stewart</last><affiliation>Pacific Northwest National Laboratory</affiliation></author>
      <author><first>Rada</first><last>Mihalcea</last><affiliation>University of Michigan</affiliation></author>
      <pages>365-375</pages>
      <abstract>Machine translation often suffers from biased data and algorithms that can lead to unacceptable errors in system output. While bias in gender norms has been investigated, less is known about whether MT systems encode bias about social relationships, e.g., “the lawyer kissed her wife.” We investigate the degree of bias against same-gender relationships in MT systems, using generated template sentences drawn from several noun-gender languages (e.g., Spanish) and comprised of popular occupation nouns. We find that three popular MT services consistently fail to accurately translate sentences concerning relationships between entities of the same gender. The error rate varies considerably based on the context, and same-gender sentences referencing high female-representation occupations are translated with lower accuracy. We provide this work as a case study in the evaluation of intrinsic bias in NLP systems with respect to social relationships.</abstract>
      <url hash="8cf34f43">2024.gebnlp-1.23</url>
      <bibkey>stewart-mihalcea-2024-whose</bibkey>
      <doi>10.18653/v1/2024.gebnlp-1.23</doi>
    </paper>
    <paper id="24">
      <title>Analysis of Annotator Demographics in Sexism Detection</title>
      <author><first>Narjes</first><last>Tahaei</last></author>
      <author><first>Sabine</first><last>Bergler</last><affiliation>Concordia University, Montreal</affiliation></author>
      <pages>376-383</pages>
      <abstract>This study explores the effect of annotators’ demographic features on labeling sexist content in social media datasets, specifically focusing on the EXIST dataset, which includes direct sexist messages, reports and descriptions of sexist experiences and stereotypes. We investigate how various demographic backgrounds influence annotation outcomes and examine methods to incorporate these features into BERT-based model training. Our experiments demonstrate that adding demographic information improves performance in detecting sexism and assessing intention of the author.</abstract>
      <url hash="b45d13d4">2024.gebnlp-1.24</url>
      <bibkey>tahaei-bergler-2024-analysis</bibkey>
      <doi>10.18653/v1/2024.gebnlp-1.24</doi>
    </paper>
    <paper id="25">
      <title>An Empirical Study of Gendered Stereotypes in Emotional Attributes for <fixed-case>B</fixed-case>angla in Multilingual Large Language Models</title>
      <author><first>Jayanta</first><last>Sadhu</last></author>
      <author><first>Maneesha</first><last>Saha</last></author>
      <author><first>Rifat</first><last>Shahriyar</last><affiliation>Bangladesh University of Engineering and Technology</affiliation></author>
      <pages>384-398</pages>
      <abstract>The influence of Large Language Models (LLMs) is rapidly growing, automating more jobs over time. Assessing the fairness of LLMs is crucial due to their expanding impact. Studies reveal the reflection of societal norms and biases in LLMs, which creates a risk of propagating societal stereotypes in downstream tasks. Many studies on bias in LLMs focus on gender bias in various NLP applications. However, there’s a gap in research on bias in emotional attributes, despite the close societal link between emotion and gender. This gap is even larger for low-resource languages like Bangla. Historically, women are associated with emotions like empathy, fear, and guilt, while men are linked to anger, bravado, and authority. This pattern reflects societal norms in Bangla-speaking regions. We offer the first thorough investigation of gendered emotion attribution in Bangla for both closed and open source LLMs in this work. Our aim is to elucidate the intricate societal relationship between gender and emotion specifically within the context of Bangla. We have been successful in showing the existence of gender bias in the context of emotions in Bangla through analytical methods and also show how emotion attribution changes on the basis of gendered role selection in LLMs. All of our resources including code and data are made publicly available to support future research on Bangla NLP. Warning: This paper contains explicit stereotypical statements that many may find offensive.</abstract>
      <url hash="e7d58203">2024.gebnlp-1.25</url>
      <bibkey>sadhu-etal-2024-empirical</bibkey>
      <doi>10.18653/v1/2024.gebnlp-1.25</doi>
    </paper>
    <paper id="26">
      <title>Overview of the Shared Task on Machine Translation Gender Bias Evaluation with Multilingual Holistic Bias</title>
      <author><first>Marta</first><last>Costa-jussà</last><affiliation>FAIR, Meta</affiliation></author>
      <author><first>Pierre</first><last>Andrews</last><affiliation>FAIR, Meta</affiliation></author>
      <author><first>Christine</first><last>Basta</last></author>
      <author><first>Juan</first><last>Ciro</last><affiliation>Dynabench</affiliation></author>
      <author><first>Agnieszka</first><last>Falenska</last><affiliation>Interchange Forum for Reflecting on Intelligent Systems, University of Stuttgart</affiliation></author>
      <author><first>Seraphina</first><last>Goldfarb-Tarrant</last></author>
      <author><first>Rafael</first><last>Mosquera</last><affiliation>Dynabench</affiliation></author>
      <author><first>Debora</first><last>Nozza</last><affiliation>Bocconi University</affiliation></author>
      <author><first>Eduardo</first><last>Sánchez</last><affiliation>FAIR, Meta</affiliation></author>
      <pages>399-404</pages>
      <abstract>We describe the details of the Shared Task of the 5th ACL Workshop on Gender Bias in Natural Language Processing (GeBNLP 2024). The task uses dataset to investigate the quality of Machine Translation systems on a particular case of gender robustness. We report baseline results as well as the results of the first participants. The shared task will be permanently available in the Dynabench platform.</abstract>
      <url hash="165506c2">2024.gebnlp-1.26</url>
      <bibkey>costa-jussa-etal-2024-overview</bibkey>
      <doi>10.18653/v1/2024.gebnlp-1.26</doi>
    </paper>
  </volume>
</collection>
