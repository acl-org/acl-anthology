<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.cogalex">
  <volume id="1" ingest-date="2020-11-29">
    <meta>
      <booktitle>Proceedings of the Workshop on the Cognitive Aspects of the Lexicon</booktitle>
      <editor><first>Michael</first><last>Zock</last></editor>
      <editor><first>Emmanuele</first><last>Chersoni</last></editor>
      <editor><first>Alessandro</first><last>Lenci</last></editor>
      <editor><first>Enrico</first><last>Santus</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>December</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="54df1291">2020.cogalex-1.0</url>
    </frontmatter>
    <paper id="1">
      <title>Individual corpora predict fast memory retrieval during reading</title>
      <author><first>Markus J.</first><last>Hofmann</last></author>
      <author><first>Lara</first><last>Müller</last></author>
      <author><first>Andre</first><last>Rölke</last></author>
      <author><first>Ralph</first><last>Radach</last></author>
      <author><first>Chris</first><last>Biemann</last></author>
      <pages>1–11</pages>
      <abstract>The corpus, from which a predictive language model is trained, can be considered the experience of a semantic system. We recorded everyday reading of two participants for two months on a tablet, generating individual corpus samples of 300/500K tokens. Then we trained word2vec models from individual corpora and a 70 million-sentence newspaper corpus to obtain individual and norm-based long-term memory structure. To test whether individual corpora can make better predictions for a cognitive task of long-term memory retrieval, we generated stimulus materials consisting of 134 sentences with uncorrelated individual and norm-based word probabilities. For the subsequent eye tracking study 1-2 months later, our regression analyses revealed that individual, but not norm-corpus-based word probabilities can account for first-fixation duration and first-pass gaze duration. Word length additionally affected gaze duration and total viewing duration. The results suggest that corpora representative for an individual’s long-term memory structure can better explain reading performance than a norm corpus, and that recently acquired information is lexically accessed rapidly.</abstract>
      <url hash="fc19dd91">2020.cogalex-1.1</url>
    </paper>
    <paper id="2">
      <title>Investigating Rich Feature Sources for Conceptual Representation Encoding</title>
      <author><first>Lu</first><last>Cao</last></author>
      <author><first>Yulong</first><last>Chen</last></author>
      <author><first>Dandan</first><last>Huang</last></author>
      <author><first>Yue</first><last>Zhang</last></author>
      <pages>12–22</pages>
      <abstract>Functional Magnetic Resonance Imaging (fMRI) provides a means to investigate human conceptual representation in cognitive and neuroscience studies, where researchers predict the fMRI activations with elicited stimuli inputs. Previous work mainly uses a single source of features, particularly linguistic features, to predict fMRI activations. However, relatively little work has been done on investigating rich-source features for conceptual representation. In this paper, we systematically compare the linguistic, visual as well as auditory input features in conceptual representation, and further introduce associative conceptual features, which are obtained from Small World of Words game, to predict fMRI activations. Our experimental results show that those rich-source features can enhance performance in predicting the fMRI activations. Our analysis indicates that information from rich sources is present in the conceptual representation of human brains. In particular, the visual feature weights the most on conceptual representation, which is consistent with the recent cognitive science study.</abstract>
      <url hash="272c8781">2020.cogalex-1.2</url>
    </paper>
    <paper id="3">
      <title>General patterns and language variation: Word frequencies across <fixed-case>E</fixed-case>nglish, <fixed-case>G</fixed-case>erman, and <fixed-case>C</fixed-case>hinese</title>
      <author><first>Annika</first><last>Tjuka</last></author>
      <pages>23–32</pages>
      <abstract>Cross-linguistic studies of concepts provide valuable insights for the investigation of the mental lexicon. Recent developments of cross-linguistic databases facilitate an exploration of a diverse set of languages on the basis of comparative concepts. These databases make use of a well-established reference catalog, the Concepticon, which is built from concept lists published in linguistics. A recently released feature of the Concepticon includes data on norms, ratings, and relations for words and concepts. The present study used data on word frequencies to test two hypotheses. First, I examined the assumption that related languages (i.e., English and German) share concepts with more similar frequencies than non-related languages (i.e., English and Chinese). Second, the variation of frequencies across both language pairs was explored to answer the question of whether the related languages share fewer concepts with a large difference between the frequency than the non-related languages. The findings indicate that related languages experience less variation in their frequencies. If there is variation, it seems to be due to cultural and structural differences. The implications of this study are far-reaching in that it exemplifies the use of cross-linguistic data for the study of the mental lexicon.</abstract>
      <url hash="2dc145a6">2020.cogalex-1.3</url>
    </paper>
    <paper id="4">
      <title>Less is Better: A cognitively inspired unsupervised model for language segmentation</title>
      <author><first>Jinbiao</first><last>Yang</last></author>
      <author><first>Stefan L.</first><last>Frank</last></author>
      <author><first>Antal</first><last>van den Bosch</last></author>
      <pages>33–45</pages>
      <abstract>Language users process utterances by segmenting them into many cognitive units, which vary in their sizes and linguistic levels. Although we can do such unitization/segmentation easily, its cognitive mechanism is still not clear. This paper proposes an unsupervised model, Less-is-Better (LiB), to simulate the human cognitive process with respect to language unitization/segmentation. LiB follows the principle of least effort and aims to build a lexicon which minimizes the number of unit tokens (alleviating the effort of analysis) and number of unit types (alleviating the effort of storage) at the same time on any given corpus. LiB’s workflow is inspired by empirical cognitive phenomena. The design makes the mechanism of LiB cognitively plausible and the computational requirement light-weight. The lexicon generated by LiB performs the best among different types of lexicons (e.g. ground-truth words) both from an information-theoretical view and a cognitive view, which suggests that the LiB lexicon may be a plausible proxy of the mental lexicon.</abstract>
      <url hash="57dc6bea">2020.cogalex-1.4</url>
    </paper>
    <paper id="5">
      <title>The <fixed-case>C</fixed-case>og<fixed-case>AL</fixed-case>ex Shared Task on Monolingual and Multilingual Identification of Semantic Relations</title>
      <author><first>Rong</first><last>Xiang</last></author>
      <author><first>Emmanuele</first><last>Chersoni</last></author>
      <author><first>Luca</first><last>Iacoponi</last></author>
      <author><first>Enrico</first><last>Santus</last></author>
      <pages>46–53</pages>
      <abstract>The shared task of the CogALex-VI workshop focuses on the monolingual and multilingual identification of semantic relations. We provided training and validation data for the following languages: English, German and Chinese. Given a word pair, systems had to be trained to identify which relation holds between them, with possible choices being synonymy, antonymy, hypernymy and no relation at all. Two test sets were released for evaluating the participating systems. One containing pairs for each of the training languages (systems were evaluated in a monolingual fashion) and the other proposing a surprise language to test the crosslingual transfer capabilities of the systems. Among the submitted systems, top performance was achieved by a transformer-based model in both the monolingual and in the multilingual setting, for all the tested languages, proving the potentials of this recently-introduced neural architecture. The shared task description and the results are available at https://sites.google.com/site/cogalexvisharedtask/.</abstract>
      <url hash="179b23b8">2020.cogalex-1.5</url>
    </paper>
    <paper id="6">
      <title>Extracting meaning by idiomaticity: Description of the <fixed-case>HS</fixed-case>em<fixed-case>ID</fixed-case> system at <fixed-case>C</fixed-case>og<fixed-case>AL</fixed-case>ex <fixed-case>VI</fixed-case> (2020)</title>
      <author><first>Jean-Pierre</first><last>Colson</last></author>
      <pages>54–58</pages>
      <abstract>The HSemID system, submitted to the CogALex VI Shared Task is a hybrid system relying mainly on metric clusters measured in large web corpora, complemented by a vector space model using cosine similarity to detect semantic associations. Although the system reached ra-ther weak results for the subcategories of synonyms, antonyms and hypernyms, with some dif-ferences from one language to another, it is able to measure general semantic associations (as being random or not-random) with an F1 score close to 0.80. The results strongly suggest that idiomatic constructions play a fundamental role in semantic associations. Further experiments are necessary in order to fine-tune the model to the subcategories of synonyms, antonyms, hy-pernyms and to explain surprising differences across languages. 1 Introduction</abstract>
      <url hash="b99756f7">2020.cogalex-1.6</url>
    </paper>
    <paper id="7">
      <title><fixed-case>C</fixed-case>og<fixed-case>AL</fixed-case>ex-<fixed-case>VI</fixed-case> Shared Task: Transrelation - A Robust Multilingual Language Model for Multilingual Relation Identification</title>
      <author><first>Lennart</first><last>Wachowiak</last></author>
      <author><first>Christian</first><last>Lang</last></author>
      <author><first>Barbara</first><last>Heinisch</last></author>
      <author><first>Dagmar</first><last>Gromann</last></author>
      <pages>59–64</pages>
      <abstract>We describe our submission to the CogALex-VI shared task on the identification of multilingual paradigmatic relations building on XLM-RoBERTa (XLM-R), a robustly optimized and multilingual BERT model. In spite of several experiments with data augmentation, data addition and ensemble methods with a Siamese Triple Net, Translrelation, the XLM-R model with a linear classifier adapted to this specific task, performed best in testing and achieved the best results in the final evaluation of the shared task, even for a previously unseen language.</abstract>
      <url hash="f6c6dda6">2020.cogalex-1.7</url>
    </paper>
    <paper id="8">
      <title><fixed-case>C</fixed-case>og<fixed-case>AL</fixed-case>ex-<fixed-case>VI</fixed-case> Shared Task: Bidirectional Transformer based Identification of Semantic Relations</title>
      <author><first>Saurav</first><last>Karmakar</last></author>
      <author><first>John P.</first><last>McCrae</last></author>
      <pages>65–71</pages>
      <abstract>This paper presents a bidirectional transformer based approach for recognising semantic relationships between a pair of words as proposed by CogALex VI shared task in 2020. The system presented here works by employing BERT embeddings of the words and passing the same over tuned neural network to produce a learning model for the pair of words and their relationships. Afterwards the very same model is used for the relationship between unknown words from the test set. CogALex VI provided Subtask 1 as the identification of relationship of three specific categories amongst English pair of words and the presented system opts to work on that. The resulted relationships of the unknown words are analysed here which shows a balanced performance in overall characteristics with some scope for improvement.</abstract>
      <url hash="560a438b">2020.cogalex-1.8</url>
    </paper>
    <paper id="9">
      <title>Leveraging Contextual Embeddings and Idiom Principle for Detecting Idiomaticity in Potentially Idiomatic Expressions</title>
      <author><first>Reyhaneh</first><last>Hashempour</last></author>
      <author><first>Aline</first><last>Villavicencio</last></author>
      <pages>72–80</pages>
      <abstract>The majority of studies on detecting idiomatic expressions have focused on discovering potentially idiomatic expressions overlooking the context. However, many idioms like blow the whistle could be interpreted idiomatically or literally depending on the context. In this work, we leverage the Idiom Principle (Sinclair et al., 1991) and contextualized word embeddings (CWEs), focusing on Context2Vec (Melamud et al., 2016) and BERT (Devlin et al., 2019) to distinguish between literal and idiomatic senses of such expressions in context. We also experiment with a non-contextualized word embedding baseline, in this case word2Vec (Mikolov et al., 2013) and compare its performance with that of CWEs. The results show that CWEs outperform the non-CWEs, especially when the Idiom Principle is applied, as it improves the results by 6%. We further show that the Context2Vec model, trained based on Idiom Principle, can place potentially idiomatic expressions into distinct ‘sense’ (idiomatic/literal) regions of the embedding space, whereas Word2Vec and BERT seem to lack this capacity. The model is also capable of producing suitable substitutes for ambiguous expressions in context which is promising for downstream tasks like text simplification.</abstract>
      <url hash="670bc825">2020.cogalex-1.9</url>
    </paper>
    <paper id="10">
      <title>Definition Extraction Feature Analysis: From Canonical to Naturally-Occurring Definitions</title>
      <author><first>Mireia</first><last>Roig Mirapeix</last></author>
      <author><first>Luis</first><last>Espinosa Anke</last></author>
      <author><first>Jose</first><last>Camacho-Collados</last></author>
      <pages>81–91</pages>
      <abstract>Textual definitions constitute a fundamental source of knowledge when seeking the meaning of words, and they are the cornerstone of lexical resources like glossaries, dictionaries, encyclopedia or thesauri. In this paper, we present an in-depth analytical study on the main features relevant to the task of definition extraction. Our main goal is to study whether linguistic structures from canonical (the Aristotelian or genus et differentia model) can be leveraged to retrieve definitions from corpora in different domains of knowledge and textual genres alike. To this end, we develop a simple linear classifier and analyze the contribution of several (sets of) linguistic features. Finally, as a result of our experiments, we also shed light on the particularities of existing benchmarks as well as the most challenging aspects of the task.</abstract>
      <url hash="38348f5b">2020.cogalex-1.10</url>
    </paper>
    <paper id="11">
      <title>Speech Disfluencies occur at Higher Perplexities</title>
      <author><first>Priyanka</first><last>Sen</last></author>
      <pages>92–97</pages>
      <abstract>Speech disfluencies have been hypothesized to occur before words that are less predictable and therefore more cognitively demanding. In this paper, we revisit this hypothesis by using OpenAI’s GPT-2 to calculate predictability of words as language model perplexity. Using the Switchboard corpus, we find that 51% of disfluencies occur at the highest, second highest, or within one token of the highest perplexity, and this distribution is not random. We also show that disfluencies precede words with significantly higher perplexity than fluent contexts. Based on our results, we offer new evidence that disfluencies are more likely to occur before less predictable words.</abstract>
      <url hash="e9f80573">2020.cogalex-1.11</url>
    </paper>
    <paper id="12">
      <title>Bilingual Lexical Access and Cognate Idiom Comprehension</title>
      <author><first>Eve</first><last>Fleisig</last></author>
      <pages>98–106</pages>
      <abstract>Language transfer can facilitate learning L2 words whose form and meaning are similar to L1 words, or hinder speakers when the languages differ. L2 idioms introduce another layer of challenge, as language transfer could occur on the literal or figurative level of meaning. Thus, the mechanics of language transfer for idiom processing shed light on how literal and figurative meaning is stored in the bilingual lexicon. Three factors appear to influence how language transfer affects idiom comprehension: bilingual fluency, processing of literal-figurative vs. figurative cognate idioms (idioms with the same wording and meaning in both languages, or the same meaning only), and comprehension of literal vs. figurative meaning of a given idiom. To examine the relationship between these factors, this study investigated English-Spanish bilinguals’ reaction time on a lexical decision task examining literal-figurative and figurative cognate idioms. The results suggest that fluency increases processing speed rather than slow it down due to language transfer, and that language transfer from L1 to L2 occurs on the level of figurative meaning in L1-dominant bilinguals.</abstract>
      <url hash="0a71319e">2020.cogalex-1.12</url>
    </paper>
    <paper id="13">
      <title>Schwa-deletion in <fixed-case>G</fixed-case>erman noun-noun compounds</title>
      <author><first>Tom S</first><last>Juzek</last></author>
      <author><first>Jana</first><last>Haeussler</last></author>
      <pages>107–111</pages>
      <abstract>We report ongoing research on linking elements in German compounds, with a focus on noun-noun compounds in which the first constituent is ending in schwa. We present a corpus of about 3000 nouns ending in schwa, annotated for various phonological and morpho-syntactic features, and critically, the dominant linking strategy. The corpus analysis is complemented by an unsuccessful attempt to train neural networks and by a pilot experiment asking native speakers to indicate their preferred linking strategy. In addition to existing nouns, the experimental stimuli included nonce words, also ending in schwa. While neither the corpus study nor the experiment offer a clear picture, the results nevertheless provide interesting insights into the intricacies of German compounding. Overall, we find a predominance of the paradigmatic linking element -n for feminine and masculine nouns. At the same time, the results for nonce words show that -n is not a default strategy.</abstract>
      <url hash="6dfe0fdc">2020.cogalex-1.13</url>
    </paper>
    <paper id="14">
      <title>Translating Collocations: The Need for Task-driven Word Associations</title>
      <author><first>Oi Yee</first><last>Kwong</last></author>
      <pages>112–116</pages>
      <abstract>Existing dictionaries may help collocation translation by suggesting associated words in the form of collocations, thesaurus, and example sentences. We propose to enhance them with task-driven word associations, illustrating the need by a few scenarios and outlining a possible approach based on word embedding. An example is given, using pre-trained word embedding, while more extensive investigation with more refined methods and resources is underway.</abstract>
      <url hash="ac742e0a">2020.cogalex-1.14</url>
    </paper>
    <paper id="15">
      <title>Characterizing Dynamic Word Meaning Representations in the Brain</title>
      <author><first>Nora</first><last>Aguirre-Celis</last></author>
      <author><first>Risto</first><last>Miikkulainen</last></author>
      <pages>117–128</pages>
      <abstract>During sentence comprehension, humans adjust word meanings according to the combination of the concepts that occur in the sentence. This paper presents a neural network model called CEREBRA (Context-dEpendent meaning REpresentation in the BRAin) that demonstrates this process based on fMRI sentence patterns and the Concept Attribute Rep-resentation (CAR) theory. In several experiments, CEREBRA is used to quantify conceptual combination effect and demonstrate that it matters to humans. Such context-based representations could be used in future natural language processing systems allowing them to mirror human performance more accurately.</abstract>
      <url hash="274a0291">2020.cogalex-1.15</url>
    </paper>
    <paper id="16">
      <title>Contextualized Word Embeddings Encode Aspects of Human-Like Word Sense Knowledge</title>
      <author><first>Sathvik</first><last>Nair</last></author>
      <author><first>Mahesh</first><last>Srinivasan</last></author>
      <author><first>Stephan</first><last>Meylan</last></author>
      <pages>129–141</pages>
      <abstract>Understanding context-dependent variation in word meanings is a key aspect of human language comprehension supported by the lexicon. Lexicographic resources (e.g., WordNet) capture only some of this context-dependent variation; for example, they often do not encode how closely senses, or discretized word meanings, are related to one another. Our work investigates whether recent advances in NLP, specifically contextualized word embeddings, capture human-like distinctions between English word senses, such as polysemy and homonymy. We collect data from a behavioral, web-based experiment, in which participants provide judgments of the relatedness of multiple WordNet senses of a word in a two-dimensional spatial arrangement task. We find that participants’ judgments of the relatedness between senses are correlated with distances between senses in the BERT embedding space. Specifically, homonymous senses (e.g., bat as mammal vs. bat as sports equipment) are reliably more distant from one another in the embedding space than polysemous ones (e.g., chicken as animal vs. chicken as meat). Our findings point towards the potential utility of continuous-space representations of sense meanings.</abstract>
      <url hash="57e59d12">2020.cogalex-1.16</url>
    </paper>
    <paper id="17">
      <title>Automatic Word Association Norms (<fixed-case>AWAN</fixed-case>)</title>
      <author><first>Jorge</first><last>Reyes-Magaña</last></author>
      <author><first>Gerardo</first><last>Sierra Martínez</last></author>
      <author><first>Gemma</first><last>Bel-Enguix</last></author>
      <author><first>Helena</first><last>Gomez-Adorno</last></author>
      <pages>142–153</pages>
      <abstract>Word Association Norms (WAN) are collections that present stimuli words and the set of their associated responses. The corpus is widely used in diverse areas of expertise. In order to reduce the effort to have a good quality resource that can be reproduced in many languages with minimum sources, a methodology to build Automatic Word Association Norms is proposed (AWAN). The methodology has an input of two simple elements: a) dictionary, and b) pre-processed Word Embeddings. This new kind of WAN is evaluated in two ways: i) learning word embeddings based on the node2vec algorithm and comparing them with human annotated benchmarks, and ii) performing a lexical search for a reverse dictionary. Both evaluations are done in a weighted graph with the AWAN lexical elements. The results showed that the methodology produces good quality AWANs.</abstract>
      <url hash="f3e1102b">2020.cogalex-1.17</url>
    </paper>
  </volume>
</collection>
