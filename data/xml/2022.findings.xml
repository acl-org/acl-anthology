<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.findings">
  <volume id="acl" ingest-date="2022-05-15">
    <meta>
      <booktitle>Findings of the Association for Computational Linguistics: ACL 2022</booktitle>
      <editor><first>Smaranda</first><last>Muresan</last></editor>
      <editor><first>Preslav</first><last>Nakov</last></editor>
      <editor><first>Aline</first><last>Villavicencio</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Dublin, Ireland</address>
      <month>May</month>
      <year>2022</year>
      <url hash="698a5d93">2022.findings-acl</url>
    </meta>
    <frontmatter>
      <url hash="eb6cc2a4">2022.findings-acl.0</url>
      <bibkey>findings-2022-findings</bibkey>
    </frontmatter>
    <paper id="1">
      <title>“Is Whole Word Masking Always Better for <fixed-case>C</fixed-case>hinese <fixed-case>BERT</fixed-case>?”: Probing on <fixed-case>C</fixed-case>hinese Grammatical Error Correction</title>
      <author><first>Yong</first><last>Dai</last></author>
      <author><first>Linyang</first><last>Li</last></author>
      <author><first>Cong</first><last>Zhou</last></author>
      <author><first>Zhangyin</first><last>Feng</last></author>
      <author><first>Enbo</first><last>Zhao</last></author>
      <author><first>Xipeng</first><last>Qiu</last></author>
      <author><first>Piji</first><last>Li</last></author>
      <author><first>Duyu</first><last>Tang</last></author>
      <pages>1-8</pages>
      <abstract>Whole word masking (WWM), which masks all subwords corresponding to a word at once, makes a better English BERT model. For the Chinese language, however, there is no subword because each token is an atomic character. The meaning of a word in Chinese is different in that a word is a compositional unit consisting of multiple characters. Such difference motivates us to investigate whether WWM leads to better context understanding ability for Chinese BERT. To achieve this, we introduce two probing tasks related to grammatical error correction and ask pretrained models to revise or insert tokens in a masked language modeling manner. We construct a dataset including labels for 19,075 tokens in 10,448 sentences. We train three Chinese BERT models with standard character-level masking (CLM), WWM, and a combination of CLM and WWM, respectively. Our major findings are as follows: First, when one character needs to be inserted or replaced, the model trained with CLM performs the best. Second, when more than one character needs to be handled, WWM is the key to better performance. Finally, when being fine-tuned on sentence-level downstream tasks, models trained with different masking strategies perform comparably.</abstract>
      <url hash="df716d18">2022.findings-acl.1</url>
      <bibkey>dai-etal-2022-whole</bibkey>
    </paper>
    <paper id="2">
      <title>Compilable Neural Code Generation with Compiler Feedback</title>
      <author><first>Xin</first><last>Wang</last></author>
      <author><first>Yasheng</first><last>Wang</last></author>
      <author><first>Yao</first><last>Wan</last></author>
      <author><first>Fei</first><last>Mi</last></author>
      <author><first>Yitong</first><last>Li</last></author>
      <author><first>Pingyi</first><last>Zhou</last></author>
      <author><first>Jin</first><last>Liu</last></author>
      <author><first>Hao</first><last>Wu</last></author>
      <author><first>Xin</first><last>Jiang</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <pages>9-19</pages>
      <abstract>Automatically generating compilable programs with (or without) natural language descriptions has always been a touchstone problem for computational linguistics and automated software engineering. Existing deep-learning approaches model code generation as text generation, either constrained by grammar structures in decoder, or driven by pre-trained language models on large-scale code corpus (e.g., CodeGPT, PLBART, and CodeT5). However, few of them account for compilability of the generated programs. To improve compilability of the generated programs, this paper proposes COMPCODER, a three-stage pipeline utilizing compiler feedback for compilable code generation, including language model fine-tuning, compilability reinforcement, and compilability discrimination. Comprehensive experiments on two code generation tasks demonstrate the effectiveness of our proposed approach, improving the success rate of compilation from 44.18 to 89.18 in code completion on average and from 70.3 to 96.2 in text-to-code generation, respectively, when comparing with the state-of-the-art CodeGPT.</abstract>
      <url hash="a411c1bf">2022.findings-acl.2</url>
      <bibkey>wang-etal-2022-compilable</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/codesearchnet">CodeSearchNet</pwcdataset>
    </paper>
    <paper id="3">
      <title>Towards Unifying the Label Space for Aspect- and Sentence-based Sentiment Analysis</title>
      <author><first>Yiming</first><last>Zhang</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <author><first>Sai</first><last>Wu</last></author>
      <author><first>Junbo</first><last>Zhao</last></author>
      <pages>20-30</pages>
      <abstract>The aspect-based sentiment analysis (ABSA) is a fine-grained task that aims to determine the sentiment polarity towards targeted aspect terms occurring in the sentence. The development of the ABSA task is very much hindered by the lack of annotated data. To tackle this, the prior works have studied the possibility of utilizing the sentiment analysis (SA) datasets to assist in training the ABSA model, primarily via pretraining or multi-task learning. In this article, we follow this line, and for the first time, we manage to apply the Pseudo-Label (PL) method to merge the two homogeneous tasks. While it seems straightforward to use generated pseudo labels to handle this case of label granularity unification for two highly related tasks, we identify its major challenge in this paper and propose a novel framework, dubbed as Dual-granularity Pseudo Labeling (DPL). Further, similar to PL, we regard the DPL as a general framework capable of combining other prior methods in the literature. Through extensive experiments, DPL has achieved state-of-the-art performance on standard benchmarks surpassing the prior work significantly.</abstract>
      <url hash="7ab13df3">2022.findings-acl.3</url>
      <bibkey>zhang-etal-2022-towards</bibkey>
    </paper>
    <paper id="4">
      <title>Input-specific Attention Subnetworks for Adversarial Detection</title>
      <author><first>Emil</first><last>Biju</last></author>
      <author><first>Anirudh</first><last>Sriram</last></author>
      <author><first>Pratyush</first><last>Kumar</last></author>
      <author><first>Mitesh</first><last>Khapra</last></author>
      <pages>31-44</pages>
      <abstract>Self-attention heads are characteristic of Transformer models and have been well studied for interpretability and pruning. In this work, we demonstrate an altogether different utility of attention heads, namely for adversarial detection. Specifically, we propose a method to construct input-specific attention subnetworks (IAS) from which we extract three features to discriminate between authentic and adversarial inputs. The resultant detector significantly improves (by over 7.5%) the state-of-the-art adversarial detection accuracy for the BERT encoder on 10 NLU datasets with 11 different adversarial attack types. We also demonstrate that our method (a) is more accurate for larger models which are likely to have more spurious correlations and thus vulnerable to adversarial attack, and (b) performs well even with modest training sets of adversarial examples.</abstract>
      <url hash="5581daa2">2022.findings-acl.4</url>
      <attachment type="software" hash="933945e1">2022.findings-acl.4.software.zip</attachment>
      <bibkey>biju-etal-2022-input</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="5">
      <title><fixed-case>R</fixed-case>elation<fixed-case>P</fixed-case>rompt: Leveraging Prompts to Generate Synthetic Data for Zero-Shot Relation Triplet Extraction</title>
      <author><first>Yew Ken</first><last>Chia</last></author>
      <author><first>Lidong</first><last>Bing</last></author>
      <author><first>Soujanya</first><last>Poria</last></author>
      <author><first>Luo</first><last>Si</last></author>
      <pages>45-57</pages>
      <abstract>Despite the importance of relation extraction in building and representing knowledge, less research is focused on generalizing to unseen relations types. We introduce the task setting of Zero-Shot Relation Triplet Extraction (ZeroRTE) to encourage further research in low-resource relation extraction methods. Given an input sentence, each extracted triplet consists of the head entity, relation label, and tail entity where the relation label is not seen at the training stage. To solve ZeroRTE, we propose to synthesize relation examples by prompting language models to generate structured texts. Concretely, we unify language model prompts and structured text approaches to design a structured prompt template for generating synthetic relation samples when conditioning on relation label prompts (RelationPrompt). To overcome the limitation for extracting multiple relation triplets in a sentence, we design a novel Triplet Search Decoding method. Experiments on FewRel and Wiki-ZSL datasets show the efficacy of RelationPrompt for the ZeroRTE task and zero-shot relation classification. Our code and data are available at github.com/declare-lab/RelationPrompt.</abstract>
      <url hash="8e567b7a">2022.findings-acl.5</url>
      <bibkey>chia-etal-2022-relationprompt</bibkey>
      <pwccode url="https://github.com/declare-lab/relationprompt" additional="false">declare-lab/relationprompt</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel">FewRel</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wiki-zsl">Wiki-ZSL</pwcdataset>
    </paper>
    <paper id="6">
      <title>Pre-Trained Multilingual Sequence-to-Sequence Models: A Hope for Low-Resource Language Translation?</title>
      <author><first>En-Shiun</first><last>Lee</last></author>
      <author><first>Sarubi</first><last>Thillainathan</last></author>
      <author><first>Shravan</first><last>Nayak</last></author>
      <author><first>Surangika</first><last>Ranathunga</last></author>
      <author><first>David</first><last>Adelani</last></author>
      <author><first>Ruisi</first><last>Su</last></author>
      <author><first>Arya</first><last>McCarthy</last></author>
      <pages>58-67</pages>
      <abstract>What can pre-trained multilingual sequence-to-sequence models like mBART contribute to translating low-resource languages? We conduct a thorough empirical experiment in 10 languages to ascertain this, considering five factors: (1) the amount of fine-tuning data, (2) the noise in the fine-tuning data, (3) the amount of pre-training data in the model, (4) the impact of domain mismatch, and (5) language typology. In addition to yielding several heuristics, the experiments form a framework for evaluating the data sensitivities of machine translation systems. While mBART is robust to domain differences, its translations for unseen and typologically distant languages remain below 3.0 BLEU. In answer to our title’s question, mBART is not a low-resource panacea; we therefore encourage shifting the emphasis from new models to new data.</abstract>
      <url hash="57ee5031">2022.findings-acl.6</url>
      <attachment type="software" hash="a436de63">2022.findings-acl.6.software.zip</attachment>
      <bibkey>lee-etal-2022-pre</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/pmindia">PMIndia</pwcdataset>
    </paper>
    <paper id="7">
      <title>Multi-Scale Distribution Deep Variational Autoencoder for Explanation Generation</title>
      <author><first>ZeFeng</first><last>Cai</last></author>
      <author><first>Linlin</first><last>Wang</last></author>
      <author><first>Gerard</first><last>de Melo</last></author>
      <author><first>Fei</first><last>Sun</last></author>
      <author><first>Liang</first><last>He</last></author>
      <pages>68-78</pages>
      <abstract>Generating explanations for recommender systems is essential for improving their transparency, as users often wish to understand the reason for receiving a specified recommendation. Previous methods mainly focus on improving the generation quality, but often produce generic explanations that fail to incorporate user and item specific details. To resolve this problem, we present Multi-Scale Distribution Deep Variational Autoencoders (MVAE).These are deep hierarchical VAEs with a prior network that eliminates noise while retaining meaningful signals in the input, coupled with a recognition network serving as the source of information to guide the learning of the prior network. Further, the Multi-scale distribution Learning Framework (MLF) along with a Target Tracking Kullback-Leibler divergence (TKL) mechanism are proposed to employ multi KL divergences at different scales for more effective learning. Extensive empirical experiments demonstrate that our methods can generate explanations with concrete input-specific contents.</abstract>
      <url hash="d9aa8f0a">2022.findings-acl.7</url>
      <bibkey>cai-etal-2022-multi</bibkey>
    </paper>
    <paper id="8">
      <title>Dual Context-Guided Continuous Prompt Tuning for Few-Shot Learning</title>
      <author><first>Jie</first><last>Zhou</last></author>
      <author><first>Le</first><last>Tian</last></author>
      <author><first>Houjin</first><last>Yu</last></author>
      <author><first>Zhou</first><last>Xiao</last></author>
      <author><first>Hui</first><last>Su</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>79-84</pages>
      <abstract>Prompt-based paradigm has shown its competitive performance in many NLP tasks. However, its success heavily depends on prompt design, and the effectiveness varies upon the model and training data. In this paper, we propose a novel dual context-guided continuous prompt (DCCP) tuning method. To explore the rich contextual information in language structure and close the gap between discrete prompt tuning and continuous prompt tuning, DCCP introduces two auxiliary training objectives and constructs input in a pair-wise fashion.Experimental results demonstrate that our method is applicable to many NLP tasks, and can often outperform existing prompt tuning methods by a large margin in the few-shot setting.</abstract>
      <url hash="9656d26e">2022.findings-acl.8</url>
      <bibkey>zhou-etal-2022-dual</bibkey>
    </paper>
    <paper id="9">
      <title>Extract-Select: A Span Selection Framework for Nested Named Entity Recognition with Generative Adversarial Training</title>
      <author><first>Peixin</first><last>Huang</last></author>
      <author><first>Xiang</first><last>Zhao</last></author>
      <author><first>Minghao</first><last>Hu</last></author>
      <author><first>Yang</first><last>Fang</last></author>
      <author><first>Xinyi</first><last>Li</last></author>
      <author><first>Weidong</first><last>Xiao</last></author>
      <pages>85-96</pages>
      <abstract>Nested named entity recognition (NER) is a task in which named entities may overlap with each other. Span-based approaches regard nested NER as a two-stage span enumeration and classification task, thus having the innate ability to handle this task. However, they face the problems of error propagation, ignorance of span boundary, difficulty in long entity recognition and requirement on large-scale annotated data. In this paper, we propose Extract-Select, a span selection framework for nested NER, to tackle these problems. Firstly, we introduce a span selection framework in which nested entities with different input categories would be separately extracted by the extractor, thus naturally avoiding error propagation in two-stage span-based approaches. In the inference phase, the trained extractor selects final results specific to the given entity category. Secondly, we propose a hybrid selection strategy in the extractor, which not only makes full use of span boundary but also improves the ability of long entity recognition. Thirdly, we design a discriminator to evaluate the extraction result, and train both extractor and discriminator with generative adversarial training (GAT). The use of GAT greatly alleviates the stress on the dataset size. Experimental results on four benchmark datasets demonstrate that Extract-Select outperforms competitive nested NER models, obtaining state-of-the-art results. The proposed model also performs well when less labeled data are given, proving the effectiveness of GAT.</abstract>
      <url hash="ab6638a3">2022.findings-acl.9</url>
      <bibkey>huang-etal-2022-extract</bibkey>
    </paper>
    <paper id="10">
      <title>Controlled Text Generation Using Dictionary Prior in Variational Autoencoders</title>
      <author><first>Xianghong</first><last>Fang</last></author>
      <author><first>Jian</first><last>Li</last></author>
      <author><first>Lifeng</first><last>Shang</last></author>
      <author><first>Xin</first><last>Jiang</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <author><first>Dit-Yan</first><last>Yeung</last></author>
      <pages>97-111</pages>
      <abstract>While variational autoencoders (VAEs) have been widely applied in text generation tasks, they are troubled by two challenges: insufficient representation capacity and poor controllability. The former results from the posterior collapse and restrictive assumption, which impede better representation learning. The latter arises as continuous latent variables in traditional formulations hinder VAEs from interpretability and controllability. In this paper, we propose Dictionary Prior (DPrior), a new data-driven prior that enjoys the merits of expressivity and controllability. To facilitate controlled text generation with DPrior, we propose to employ contrastive learning to separate the latent space into several parts. Extensive experiments on both language modeling and controlled text generation demonstrate the effectiveness of the proposed approach.</abstract>
      <url hash="b4e105c5">2022.findings-acl.10</url>
      <bibkey>fang-etal-2022-controlled</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="11">
      <title>Challenges to Open-Domain Constituency Parsing</title>
      <author><first>Sen</first><last>Yang</last></author>
      <author><first>Leyang</first><last>Cui</last></author>
      <author><first>Ruoxi</first><last>Ning</last></author>
      <author><first>Di</first><last>Wu</last></author>
      <author><first>Yue</first><last>Zhang</last></author>
      <pages>112-127</pages>
      <abstract>Neural constituency parsers have reached practical performance on news-domain benchmarks. However, their generalization ability to other domains remains weak. Existing findings on cross-domain constituency parsing are only made on a limited number of domains. Tracking this, we manually annotate a high-quality constituency treebank containing five domains. We analyze challenges to open-domain constituency parsing using a set of linguistic features on various strong constituency parsers. Primarily, we find that 1) BERT significantly increases parsers’ cross-domain performance by reducing their sensitivity on the domain-variant features.2) Compared with single metrics such as unigram distribution and OOV rate, challenges to open-domain constituency parsing arise from complex features, including cross-domain lexical and constituent structure variations.</abstract>
      <url hash="037dfd3a">2022.findings-acl.11</url>
      <bibkey>yang-etal-2022-challenges</bibkey>
      <pwccode url="https://github.com/ringos/multi-domain-parsing-analysis" additional="false">ringos/multi-domain-parsing-analysis</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="12">
      <title>Going “Deeper”: Structured Sememe Prediction via Transformer with Tree Attention</title>
      <author><first>Yining</first><last>Ye</last></author>
      <author><first>Fanchao</first><last>Qi</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <pages>128-138</pages>
      <abstract>Sememe knowledge bases (SKBs), which annotate words with the smallest semantic units (i.e., sememes), have proven beneficial to many NLP tasks. Building an SKB is very time-consuming and labor-intensive. Therefore, some studies have tried to automate the building process by predicting sememes for the unannotated words. However, all existing sememe prediction studies ignore the hierarchical structures of sememes, which are important in the sememe-based semantic description system. In this work, we tackle the structured sememe prediction problem for the first time, which is aimed at predicting a sememe tree with hierarchical structures rather than a set of sememes. We design a sememe tree generation model based on Transformer with adjusted attention mechanism, which shows its superiority over the baselines in experiments. We also conduct a series of quantitative and qualitative analyses of the effectiveness of our model. All the code and data of this paper are available at https://github.com/thunlp/STG.</abstract>
      <url hash="1ec3124f">2022.findings-acl.12</url>
      <attachment type="software" hash="c59dd366">2022.findings-acl.12.software.zip</attachment>
      <bibkey>ye-etal-2022-going</bibkey>
      <pwccode url="https://github.com/thunlp/stg" additional="false">thunlp/stg</pwccode>
    </paper>
    <paper id="13">
      <title>Table-based Fact Verification with Self-adaptive Mixture of Experts</title>
      <author><first>Yuxuan</first><last>Zhou</last></author>
      <author><first>Xien</first><last>Liu</last></author>
      <author><first>Kaiyin</first><last>Zhou</last></author>
      <author><first>Ji</first><last>Wu</last></author>
      <pages>139-149</pages>
      <abstract>The table-based fact verification task has recently gained widespread attention and yet remains to be a very challenging problem. It inherently requires informative reasoning over natural language together with different numerical and logical reasoning on tables (e.g., count, superlative, comparative). Considering that, we exploit mixture-of-experts and present in this paper a new method: Self-adaptive Mixture-of-Experts Network (SaMoE). Specifically, we have developed a mixture-of-experts neural network to recognize and execute different types of reasoning—the network is composed of multiple experts, each handling a specific part of the semantics for reasoning, whereas a management module is applied to decide the contribution of each expert network to the verification result. A self-adaptive method is developed to teach the management module combining results of different experts more efficiently without external knowledge. The experimental results illustrate that our framework achieves 85.1% accuracy on the benchmark dataset TabFact, comparable with the previous state-of-the-art models. We hope our framework can serve as a new baseline for table-based verification. Our code is available at https://github.com/THUMLP/SaMoE.</abstract>
      <url hash="005f294b">2022.findings-acl.13</url>
      <bibkey>zhou-etal-2022-table</bibkey>
      <pwccode url="https://github.com/thumlp/samoe" additional="false">thumlp/samoe</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/tabfact">TabFact</pwcdataset>
    </paper>
    <paper id="14">
      <title>Investigating Data Variance in Evaluations of Automatic Machine Translation Metrics</title>
      <author><first>Jiannan</first><last>Xiang</last></author>
      <author><first>Huayang</first><last>Li</last></author>
      <author><first>Yahui</first><last>Liu</last></author>
      <author><first>Lemao</first><last>Liu</last></author>
      <author><first>Guoping</first><last>Huang</last></author>
      <author><first>Defu</first><last>Lian</last></author>
      <author><first>Shuming</first><last>Shi</last></author>
      <pages>150-157</pages>
      <abstract>Current practices in metric evaluation focus on one single dataset, e.g., Newstest dataset in each year’s WMT Metrics Shared Task. However, in this paper, we qualitatively and quantitatively show that the performances of metrics are sensitive to data. The ranking of metrics varies when the evaluation is conducted on different datasets. Then this paper further investigates two potential hypotheses, i.e., insignificant data points and the deviation of i.i.d assumption, which may take responsibility for the issue of data variance. In conclusion, our findings suggest that when evaluating automatic translation metrics, researchers should take data variance into account and be cautious to report the results on unreliable datasets, because it may leads to inconsistent results with most of the other datasets.</abstract>
      <url hash="de0daa53">2022.findings-acl.14</url>
      <bibkey>xiang-etal-2022-investigating</bibkey>
    </paper>
    <paper id="15">
      <title>Sememe Prediction for <fixed-case>B</fixed-case>abel<fixed-case>N</fixed-case>et Synsets using Multilingual and Multimodal Information</title>
      <author><first>Fanchao</first><last>Qi</last></author>
      <author><first>Chuancheng</first><last>Lv</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Xiaojun</first><last>Meng</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <author><first>Hai-Tao</first><last>Zheng</last></author>
      <pages>158-168</pages>
      <abstract>In linguistics, a sememe is defined as the minimum semantic unit of languages. Sememe knowledge bases (KBs), which are built by manually annotating words with sememes, have been successfully applied to various NLP tasks. However, existing sememe KBs only cover a few languages, which hinders the wide utilization of sememes. To address this issue, the task of sememe prediction for BabelNet synsets (SPBS) is presented, aiming to build a multilingual sememe KB based on BabelNet, a multilingual encyclopedia dictionary. By automatically predicting sememes for a BabelNet synset, the words in many languages in the synset would obtain sememe annotations simultaneously. However, previous SPBS methods have not taken full advantage of the abundant information in BabelNet. In this paper, we utilize the multilingual synonyms, multilingual glosses and images in BabelNet for SPBS. We design a multimodal information fusion model to encode and combine this information for sememe prediction. Experimental results show the substantial outperformance of our model over previous methods (about 10 MAP and F1 scores). All the code and data of this paper can be obtained at https://github.com/thunlp/MSGI.</abstract>
      <url hash="e4577237">2022.findings-acl.15</url>
      <attachment type="software" hash="7d6f2fd3">2022.findings-acl.15.software.zip</attachment>
      <bibkey>qi-etal-2022-sememe</bibkey>
      <pwccode url="https://github.com/thunlp/msgi" additional="false">thunlp/msgi</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imagenet">ImageNet</pwcdataset>
    </paper>
    <paper id="16">
      <title>Query and Extract: Refining Event Extraction as Type-oriented Binary Decoding</title>
      <author><first>Sijia</first><last>Wang</last></author>
      <author><first>Mo</first><last>Yu</last></author>
      <author><first>Shiyu</first><last>Chang</last></author>
      <author><first>Lichao</first><last>Sun</last></author>
      <author><first>Lifu</first><last>Huang</last></author>
      <pages>169-182</pages>
      <abstract>Event extraction is typically modeled as a multi-class classification problem where event types and argument roles are treated as atomic symbols. These approaches are usually limited to a set of pre-defined types. We propose a novel event extraction framework that uses event types and argument roles as natural language queries to extract candidate triggers and arguments from the input text. With the rich semantics in the queries, our framework benefits from the attention mechanisms to better capture the semantic correlation between the event types or argument roles and the input text. Furthermore, the query-and-extract formulation allows our approach to leverage all available event annotations from various ontologies as a unified model. Experiments on ACE and ERE demonstrate that our approach achieves state-of-the-art performance on each dataset and significantly outperforms existing methods on zero-shot event extraction.</abstract>
      <url hash="9299452c">2022.findings-acl.16</url>
      <bibkey>wang-etal-2022-query</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/maven">MAVEN</pwcdataset>
    </paper>
    <paper id="17">
      <title><fixed-case>LEVEN</fixed-case>: A Large-Scale <fixed-case>C</fixed-case>hinese Legal Event Detection Dataset</title>
      <author><first>Feng</first><last>Yao</last></author>
      <author><first>Chaojun</first><last>Xiao</last></author>
      <author><first>Xiaozhi</first><last>Wang</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Lei</first><last>Hou</last></author>
      <author><first>Cunchao</first><last>Tu</last></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <author><first>Yun</first><last>Liu</last></author>
      <author><first>Weixing</first><last>Shen</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <pages>183-201</pages>
      <abstract>Recognizing facts is the most fundamental step in making judgments, hence detecting events in the legal documents is important to legal case analysis tasks. However, existing Legal Event Detection (LED) datasets only concern incomprehensive event types and have limited annotated data, which restricts the development of LED methods and their downstream applications. To alleviate these issues, we present LEVEN a large-scale Chinese LEgal eVENt detection dataset, with 8,116 legal documents and 150,977 human-annotated event mentions in 108 event types. Not only charge-related events, LEVEN also covers general events, which are critical for legal case understanding but neglected in existing LED datasets. To our knowledge, LEVEN is the largest LED dataset and has dozens of times the data scale of others, which shall significantly promote the training and evaluation of LED methods. The results of extensive experiments indicate that LED is challenging and needs further effort. Moreover, we simply utilize legal events as side information to promote downstream applications. The method achieves improvements of average 2.2 points precision in low-resource judgment prediction, and 1.5 points mean average precision in unsupervised case retrieval, which suggests the fundamentality of LED. The source code and dataset can be obtained from https://github.com/thunlp/LEVEN.</abstract>
      <url hash="35ebebe5">2022.findings-acl.17</url>
      <attachment type="software" hash="4ef7c368">2022.findings-acl.17.software.zip</attachment>
      <bibkey>yao-etal-2022-leven</bibkey>
      <pwccode url="https://github.com/thunlp/leven" additional="false">thunlp/leven</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/maven">MAVEN</pwcdataset>
    </paper>
    <paper id="18">
      <title>Analyzing Dynamic Adversarial Training Data in the Limit</title>
      <author><first>Eric</first><last>Wallace</last></author>
      <author><first>Adina</first><last>Williams</last></author>
      <author><first>Robin</first><last>Jia</last></author>
      <author><first>Douwe</first><last>Kiela</last></author>
      <pages>202-217</pages>
      <abstract>To create models that are robust across a wide range of test inputs, training datasets should include diverse examples that span numerous phenomena. Dynamic adversarial data collection (DADC), where annotators craft examples that challenge continually improving models, holds promise as an approach for generating such diverse training sets. Prior work has shown that running DADC over 1-3 rounds can help models fix some error types, but it does not necessarily lead to better generalization beyond adversarial test data. We argue that running DADC over many rounds maximizes its training-time benefits, as the different rounds can together cover many of the task-relevant phenomena. We present the first study of longer-term DADC, where we collect 20 rounds of NLI examples for a small set of premise paragraphs, with both adversarial and non-adversarial approaches. Models trained on DADC examples make 26% fewer errors on our expert-curated test set compared to models trained on non-adversarial data. Our analysis shows that DADC yields examples that are more difficult, more lexically and syntactically diverse, and contain fewer annotation artifacts compared to non-adversarial examples.</abstract>
      <url hash="63428b6c">2022.findings-acl.18</url>
      <bibkey>wallace-etal-2022-analyzing</bibkey>
      <pwccode url="https://github.com/facebookresearch/dadc-limit" additional="false">facebookresearch/dadc-limit</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="19">
      <title><fixed-case>A</fixed-case>bduction<fixed-case>R</fixed-case>ules: Training Transformers to Explain Unexpected Inputs</title>
      <author><first>Nathan</first><last>Young</last></author>
      <author><first>Qiming</first><last>Bao</last></author>
      <author><first>Joshua</first><last>Bensemann</last></author>
      <author><first>Michael</first><last>Witbrock</last></author>
      <pages>218-227</pages>
      <abstract>Transformers have recently been shown to be capable of reliably performing logical reasoning over facts and rules expressed in natural language, but abductive reasoning - inference to the best explanation of an unexpected observation - has been underexplored despite significant applications to scientific discovery, common-sense reasoning, and model interpretability.This paper presents AbductionRules, a group of natural language datasets designed to train and test generalisable abduction over natural-language knowledge bases.We use these datasets to finetune pretrained Transformers and discuss their performance, finding that our models learned generalisable abductive techniques but also learned to exploit the structure of our data.Finally, we discuss the viability of this approach to abductive reasoning and ways in which it may be improved in future work.</abstract>
      <url hash="16eed3fd">2022.findings-acl.19</url>
      <bibkey>young-etal-2022-abductionrules</bibkey>
      <pwccode url="https://github.com/strong-ai-lab/abductionrules" additional="false">strong-ai-lab/abductionrules</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/proofwriter">ProofWriter</pwcdataset>
    </paper>
    <paper id="20">
      <title>On the Importance of Data Size in Probing Fine-tuned Models</title>
      <author><first>Houman</first><last>Mehrafarin</last></author>
      <author><first>Sara</first><last>Rajaee</last></author>
      <author><first>Mohammad Taher</first><last>Pilehvar</last></author>
      <pages>228-238</pages>
      <abstract>Several studies have investigated the reasons behind the effectiveness of fine-tuning, usually through the lens of probing. However, these studies often neglect the role of the size of the dataset on which the model is fine-tuned. In this paper, we highlight the importance of this factor and its undeniable role in probing performance. We show that the extent of encoded linguistic knowledge depends on the number of fine-tuning samples. The analysis also reveals that larger training data mainly affects higher layers, and that the extent of this change is a factor of the number of iterations updating the model during fine-tuning rather than the diversity of the training samples. Finally, we show through a set of experiments that fine-tuning data size affects the recoverability of the changes made to the model’s linguistic knowledge.</abstract>
      <url hash="2f6fef4a">2022.findings-acl.20</url>
      <bibkey>mehrafarin-etal-2022-importance</bibkey>
      <pwccode url="https://github.com/hmehrafarin/data-size-analysis" additional="false">hmehrafarin/data-size-analysis</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="21">
      <title><fixed-case>R</fixed-case>u<fixed-case>CC</fixed-case>o<fixed-case>N</fixed-case>: Clinical Concept Normalization in <fixed-case>R</fixed-case>ussian</title>
      <author><first>Alexandr</first><last>Nesterov</last></author>
      <author><first>Galina</first><last>Zubkova</last></author>
      <author><first>Zulfat</first><last>Miftahutdinov</last></author>
      <author><first>Vladimir</first><last>Kokh</last></author>
      <author><first>Elena</first><last>Tutubalina</last></author>
      <author><first>Artem</first><last>Shelmanov</last></author>
      <author><first>Anton</first><last>Alekseev</last></author>
      <author><first>Manvel</first><last>Avetisian</last></author>
      <author><first>Andrey</first><last>Chertok</last></author>
      <author><first>Sergey</first><last>Nikolenko</last></author>
      <pages>239-245</pages>
      <abstract>We present RuCCoN, a new dataset for clinical concept normalization in Russian manually annotated by medical professionals. It contains over 16,028 entity mentions manually linked to over 2,409 unique concepts from the Russian language part of the UMLS ontology. We provide train/test splits for different settings (stratified, zero-shot, and CUI-less) and present strong baselines obtained with state-of-the-art models such as SapBERT. At present, Russian medical NLP is lacking in both datasets and trained models, and we view this work as an important step towards filling this gap. Our dataset and annotation guidelines are available at https://github.com/sberbank-ai-lab/RuCCoN.</abstract>
      <url hash="8f620f3a">2022.findings-acl.21</url>
      <bibkey>nesterov-etal-2022-ruccon</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/xl-bel">XL-BEL</pwcdataset>
    </paper>
    <paper id="22">
      <title>A Sentence is Worth 128 Pseudo Tokens: A Semantic-Aware Contrastive Learning Framework for Sentence Embeddings</title>
      <author><first>Haochen</first><last>Tan</last></author>
      <author><first>Wei</first><last>Shao</last></author>
      <author><first>Han</first><last>Wu</last></author>
      <author><first>Ke</first><last>Yang</last></author>
      <author><first>Linqi</first><last>Song</last></author>
      <pages>246-256</pages>
      <abstract>Contrastive learning has shown great potential in unsupervised sentence embedding tasks, e.g., SimCSE (CITATION).However, these existing solutions are heavily affected by superficial features like the length of sentences or syntactic structures. In this paper, we propose a semantic-aware contrastive learning framework for sentence embeddings, termed Pseudo-Token BERT (PT-BERT), which is able to explore the pseudo-token space (i.e., latent semantic space) representation of a sentence while eliminating the impact of superficial features such as sentence length and syntax. Specifically, we introduce an additional pseudo token embedding layer independent of the BERT encoder to map each sentence into a sequence of pseudo tokens in a fixed length. Leveraging these pseudo sequences, we are able to construct same-length positive and negative pairs based on the attention mechanism to perform contrastive learning. In addition, we utilize both the gradient-updating and momentum-updating encoders to encode instances while dynamically maintaining an additional queue to store the representation of sentence embeddings, enhancing the encoder’s learning performance for negative examples. Experiments show that our model outperforms the state-of-the-art baselines on six standard semantic textual similarity (STS) tasks. Furthermore, experiments on alignments and uniformity losses, as well as hard examples with different sentence lengths and syntax, consistently verify the effectiveness of our method.</abstract>
      <url hash="8b6b9cf4">2022.findings-acl.22</url>
      <bibkey>tan-etal-2022-sentence</bibkey>
      <pwccode url="https://github.com/namco0816/pt-bert" additional="false">namco0816/pt-bert</pwccode>
    </paper>
    <paper id="23">
      <title>Eider: Empowering Document-level Relation Extraction with Efficient Evidence Extraction and Inference-stage Fusion</title>
      <author><first>Yiqing</first><last>Xie</last></author>
      <author><first>Jiaming</first><last>Shen</last></author>
      <author><first>Sha</first><last>Li</last></author>
      <author><first>Yuning</first><last>Mao</last></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <pages>257-268</pages>
      <abstract>Document-level relation extraction (DocRE) aims to extract semantic relations among entity pairs in a document. Typical DocRE methods blindly take the full document as input, while a subset of the sentences in the document, noted as the evidence, are often sufficient for humans to predict the relation of an entity pair. In this paper, we propose an evidence-enhanced framework, Eider, that empowers DocRE by efficiently extracting evidence and effectively fusing the extracted evidence in inference. We first jointly train an RE model with a lightweight evidence extraction model, which is efficient in both memory and runtime. Empirically, even training the evidence model on silver labels constructed by our heuristic rules can lead to better RE performance. We further design a simple yet effective inference process that makes RE predictions on both extracted evidence and the full document, then fuses the predictions through a blending layer. This allows Eider to focus on important sentences while still having access to the complete information in the document. Extensive experiments show that Eider outperforms state-of-the-art methods on three benchmark datasets (e.g., by 1.37/1.26 Ign F1/F1 on DocRED).</abstract>
      <url hash="b95e5be7">2022.findings-acl.23</url>
      <bibkey>xie-etal-2022-eider</bibkey>
      <pwccode url="https://github.com/veronicium/eider" additional="false">veronicium/eider</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/docred">DocRED</pwcdataset>
    </paper>
    <paper id="24">
      <title>Meta-X<tex-math>_{NLG}</tex-math>: A Meta-Learning Approach Based on Language Clustering for Zero-Shot Cross-Lingual Transfer and Generation</title>
      <author><first>Kaushal</first><last>Maurya</last></author>
      <author><first>Maunendra</first><last>Desarkar</last></author>
      <pages>269-284</pages>
      <abstract>Recently, the NLP community has witnessed a rapid advancement in multilingual and cross-lingual transfer research where the supervision is transferred from high-resource languages (HRLs) to low-resource languages (LRLs). However, the cross-lingual transfer is not uniform across languages, particularly in the zero-shot setting. Towards this goal, one promising research direction is to learn shareable structures across multiple tasks with limited annotated data. The downstream multilingual applications may benefit from such a learning setup as most of the languages across the globe are low-resource and share some structures with other languages. In this paper, we propose a novel meta-learning framework (called Meta-X<tex-math>_{NLG}</tex-math>) to learn shareable structures from typologically diverse languages based on meta-learning and language clustering. This is a step towards uniform cross-lingual transfer for unseen languages. We first cluster the languages based on language representations and identify the centroid language of each cluster. Then, a meta-learning algorithm is trained with all centroid languages and evaluated on the other languages in the zero-shot setting. We demonstrate the effectiveness of this modeling on two NLG tasks (Abstractive Text Summarization and Question Generation), 5 popular datasets and 30 typologically diverse languages. Consistent improvements over strong baselines demonstrate the efficacy of the proposed framework. The careful design of the model makes this end-to-end NLG setup less vulnerable to the accidental translation problem, which is a prominent concern in zero-shot cross-lingual NLG tasks.</abstract>
      <url hash="d679e8fd">2022.findings-acl.24</url>
      <attachment type="software" hash="5d215e2b">2022.findings-acl.24.software.zip</attachment>
      <bibkey>maurya-desarkar-2022-meta</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mlqa">MLQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tydi-qa">TyDi QA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikilingua">WikiLingua</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xquad">XQuAD</pwcdataset>
    </paper>
    <paper id="25">
      <title><fixed-case>MR</fixed-case>-<fixed-case>P</fixed-case>: A Parallel Decoding Algorithm for Iterative Refinement Non-Autoregressive Translation</title>
      <author><first>Hao</first><last>Cheng</last></author>
      <author><first>Zhihua</first><last>Zhang</last></author>
      <pages>285-296</pages>
      <abstract>Non-autoregressive translation (NAT) predicts all the target tokens in parallel and significantly speeds up the inference process. The Conditional Masked Language Model (CMLM) is a strong baseline of NAT. It decodes with the Mask-Predict algorithm which iteratively refines the output. Most works about CMLM focus on the model structure and the training objective. However, the decoding algorithm is equally important. We propose a simple, effective, and easy-to-implement decoding algorithm that we call MaskRepeat-Predict (MR-P). The MR-P algorithm gives higher priority to consecutive repeated tokens when selecting tokens to mask for the next iteration and stops the iteration after target tokens converge. We conduct extensive experiments on six translation directions with varying data sizes. The results show that MR-P significantly improves the performance with the same model parameters. Specifically, we achieve a BLEU increase of 1.39 points in the WMT’14 En-De translation task.</abstract>
      <url hash="9668043b">2022.findings-acl.25</url>
      <bibkey>cheng-zhang-2022-mr</bibkey>
    </paper>
    <paper id="26">
      <title>Open Relation Modeling: Learning to Define Relations between Entities</title>
      <author><first>Jie</first><last>Huang</last></author>
      <author><first>Kevin</first><last>Chang</last></author>
      <author><first>Jinjun</first><last>Xiong</last></author>
      <author><first>Wen-mei</first><last>Hwu</last></author>
      <pages>297-308</pages>
      <abstract>Relations between entities can be represented by different instances, e.g., a sentence containing both entities or a fact in a Knowledge Graph (KG). However, these instances may not well capture the general relations between entities, may be difficult to understand by humans, even may not be found due to the incompleteness of the knowledge source. In this paper, we introduce the Open Relation Modeling problem - given two entities, generate a coherent sentence describing the relation between them. To solve this problem, we propose to teach machines to generate definition-like relation descriptions by letting them learn from defining entities. Specifically, we fine-tune Pre-trained Language Models (PLMs) to produce definitions conditioned on extracted entity pairs. To help PLMs reason between entities and provide additional relational knowledge to PLMs for open relation modeling, we incorporate reasoning paths in KGs and include a reasoning path selection mechanism. Experimental results show that our model can generate concise but informative relation descriptions that capture the representative characteristics of entities.</abstract>
      <url hash="dc90219e">2022.findings-acl.26</url>
      <attachment type="software" hash="0120116c">2022.findings-acl.26.software.zip</attachment>
      <bibkey>huang-etal-2022-open</bibkey>
      <pwccode url="https://github.com/jeffhj/open-relation-modeling" additional="false">jeffhj/open-relation-modeling</pwccode>
    </paper>
    <paper id="27">
      <title>A Slot Is Not Built in One Utterance: Spoken Language Dialogs with Sub-Slots</title>
      <author><first>Sai</first><last>Zhang</last></author>
      <author><first>Yuwei</first><last>Hu</last></author>
      <author><first>Yuchuan</first><last>Wu</last></author>
      <author><first>Jiaman</first><last>Wu</last></author>
      <author><first>Yongbin</first><last>Li</last></author>
      <author><first>Jian</first><last>Sun</last></author>
      <author><first>Caixia</first><last>Yuan</last></author>
      <author><first>Xiaojie</first><last>Wang</last></author>
      <pages>309-321</pages>
      <abstract>A slot value might be provided segment by segment over multiple-turn interactions in a dialog, especially for some important information such as phone numbers and names. It is a common phenomenon in daily life, but little attention has been paid to it in previous work. To fill the gap, this paper defines a new task named Sub-Slot based Task-Oriented Dialog (SSTOD) and builds a Chinese dialog dataset SSD for boosting research on SSTOD. The dataset includes a total of 40K dialogs and 500K utterances from four different domains: Chinese names, phone numbers, ID numbers and license plate numbers. The data is well annotated with sub-slot values, slot values, dialog states and actions. We find some new linguistic phenomena and interactive manners in SSTOD which raise critical challenges of building dialog agents for the task. We test three state-of-the-art dialog models on SSTOD and find they cannot handle the task well on any of the four domains. We also investigate an improved model by involving slot knowledge in a plug-in manner. More work should be done to meet the new challenges raised from SSTOD which widely exists in real-life applications. The dataset and code are publicly available via https://github.com/shunjiu/SSTOD.</abstract>
      <url hash="21611972">2022.findings-acl.27</url>
      <bibkey>zhang-etal-2022-slot</bibkey>
      <pwccode url="https://github.com/shunjiu/sstod" additional="false">shunjiu/sstod</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ssd-1">SSD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ssd-id">SSD_ID</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ssd-name">SSD_NAME</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ssd-phone">SSD_PHONE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ssd-plate">SSD_PLATE</pwcdataset>
    </paper>
    <paper id="28">
      <title>Towards Transparent Interactive Semantic Parsing via Step-by-Step Correction</title>
      <author><first>Lingbo</first><last>Mo</last></author>
      <author><first>Ashley</first><last>Lewis</last></author>
      <author><first>Huan</first><last>Sun</last></author>
      <author><first>Michael</first><last>White</last></author>
      <pages>322-342</pages>
      <abstract>Existing studies on semantic parsing focus on mapping a natural-language utterance to a logical form (LF) in one turn. However, because natural language may contain ambiguity and variability, this is a difficult challenge. In this work, we investigate an interactive semantic parsing framework that explains the predicted LF step by step in natural language and enables the user to make corrections through natural-language feedback for individual steps. We focus on question answering over knowledge bases (KBQA) as an instantiation of our framework, aiming to increase the transparency of the parsing process and help the user trust the final answer. We construct INSPIRED, a crowdsourced dialogue dataset derived from the ComplexWebQuestions dataset. Our experiments show that this framework has the potential to greatly improve overall parse accuracy. Furthermore, we develop a pipeline for dialogue simulation to evaluate our framework w.r.t. a variety of state-of-the-art KBQA models without further crowdsourcing effort. The results demonstrate that our framework promises to be effective across such models.</abstract>
      <url hash="e6aed9e5">2022.findings-acl.28</url>
      <bibkey>mo-etal-2022-towards</bibkey>
      <pwccode url="https://github.com/molingbo/inspired" additional="false">molingbo/inspired</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/break">BREAK</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/gem">GEM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/splash">SPLASH</pwcdataset>
    </paper>
    <paper id="29">
      <title><fixed-case>MINER</fixed-case>: Multi-Interest Matching Network for News Recommendation</title>
      <author><first>Jian</first><last>Li</last></author>
      <author><first>Jieming</first><last>Zhu</last></author>
      <author><first>Qiwei</first><last>Bi</last></author>
      <author><first>Guohao</first><last>Cai</last></author>
      <author><first>Lifeng</first><last>Shang</last></author>
      <author><first>Zhenhua</first><last>Dong</last></author>
      <author><first>Xin</first><last>Jiang</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <pages>343-352</pages>
      <abstract>Personalized news recommendation is an essential technique to help users find interested news. Accurately matching user’s interests and candidate news is the key to news recommendation. Most existing methods learn a single user embedding from user’s historical behaviors to represent the reading interest. However, user interest is usually diverse and may not be adequately modeled by a single user embedding. In this paper, we propose a poly attention scheme to learn multiple interest vectors for each user, which encodes the different aspects of user interest. We further propose a disagreement regularization to make the learned interests vectors more diverse. Moreover, we design a category-aware attention weighting strategy that incorporates the news category information as explicit interest signals into the attention mechanism. Extensive experiments on the MIND news recommendation benchmark demonstrate that our approach significantly outperforms existing state-of-the-art methods.</abstract>
      <url hash="92b4b110">2022.findings-acl.29</url>
      <bibkey>li-etal-2022-miner</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mind">MIND</pwcdataset>
    </paper>
    <paper id="30">
      <title><fixed-case>KSAM</fixed-case>: Infusing Multi-Source Knowledge into Dialogue Generation via Knowledge Source Aware Multi-Head Decoding</title>
      <author><first>Sixing</first><last>Wu</last></author>
      <author><first>Ying</first><last>Li</last></author>
      <author><first>Dawei</first><last>Zhang</last></author>
      <author><first>Zhonghai</first><last>Wu</last></author>
      <pages>353-363</pages>
      <abstract>Knowledge-enhanced methods have bridged the gap between human beings and machines in generating dialogue responses. However, most previous works solely seek knowledge from a single source, and thus they often fail to obtain available knowledge because of the insufficient coverage of a single knowledge source. To this end, infusing knowledge from multiple sources becomes a trend. This paper proposes a novel approach Knowledge Source Aware Multi-Head Decoding, KSAM, to infuse multi-source knowledge into dialogue generation more efficiently. Rather than following the traditional single decoder paradigm, KSAM uses multiple independent source-aware decoder heads to alleviate three challenging problems in infusing multi-source knowledge, namely, the diversity among different knowledge sources, the indefinite knowledge alignment issue, and the insufficient flexibility/scalability in knowledge usage. Experiments on a Chinese multi-source knowledge-aligned dataset demonstrate the superior performance of KSAM against various competitive approaches.</abstract>
      <url hash="1aca8e3e">2022.findings-acl.30</url>
      <bibkey>wu-etal-2022-ksam</bibkey>
    </paper>
    <paper id="31">
      <title>Towards Responsible Natural Language Annotation for the Varieties of <fixed-case>A</fixed-case>rabic</title>
      <author><first>A.</first><last>Bergman</last></author>
      <author><first>Mona</first><last>Diab</last></author>
      <pages>364-371</pages>
      <abstract>When building NLP models, there is a tendency to aim for broader coverage, often overlooking cultural and (socio)linguistic nuance. In this position paper, we make the case for care and attention to such nuances, particularly in dataset annotation, as well as the inclusion of cultural and linguistic expertise in the process. We present a playbook for responsible dataset creation for polyglossic, multidialectal languages. This work is informed by a study on Arabic annotation of social media content.</abstract>
      <url hash="f48868e9">2022.findings-acl.31</url>
      <bibkey>bergman-diab-2022-towards</bibkey>
    </paper>
    <paper id="32">
      <title>Dynamically Refined Regularization for Improving Cross-corpora Hate Speech Detection</title>
      <author><first>Tulika</first><last>Bose</last></author>
      <author><first>Nikolaos</first><last>Aletras</last></author>
      <author><first>Irina</first><last>Illina</last></author>
      <author><first>Dominique</first><last>Fohr</last></author>
      <pages>372-382</pages>
      <abstract>Hate speech classifiers exhibit substantial performance degradation when evaluated on datasets different from the source. This is due to learning spurious correlations between words that are not necessarily relevant to hateful language, and hate speech labels from the training corpus. Previous work has attempted to mitigate this problem by regularizing specific terms from pre-defined static dictionaries. While this has been demonstrated to improve the generalizability of classifiers, the coverage of such methods is limited and the dictionaries require regular manual updates from human experts. In this paper, we propose to automatically identify and reduce spurious correlations using attribution methods with dynamic refinement of the list of terms that need to be regularized during training. Our approach is flexible and improves the cross-corpora performance over previous work independently and in combination with pre-defined dictionaries.</abstract>
      <url hash="c6b773c3">2022.findings-acl.32</url>
      <bibkey>bose-etal-2022-dynamically</bibkey>
      <pwccode url="https://github.com/tbose20/d-ref" additional="false">tbose20/d-ref</pwccode>
    </paper>
    <paper id="33">
      <title>Towards Large-Scale Interpretable Knowledge Graph Reasoning for Dialogue Systems</title>
      <author><first>Yi-Lin</first><last>Tuan</last></author>
      <author><first>Sajjad</first><last>Beygi</last></author>
      <author><first>Maryam</first><last>Fazel-Zarandi</last></author>
      <author><first>Qiaozi</first><last>Gao</last></author>
      <author><first>Alessandra</first><last>Cervone</last></author>
      <author><first>William Yang</first><last>Wang</last></author>
      <pages>383-395</pages>
      <abstract>Users interacting with voice assistants today need to phrase their requests in a very specific manner to elicit an appropriate response. This limits the user experience, and is partly due to the lack of reasoning capabilities of dialogue platforms and the hand-crafted rules that require extensive labor. One possible solution to improve user experience and relieve the manual efforts of designers is to build an end-to-end dialogue system that can do reasoning itself while perceiving user’s utterances. In this work, we propose a novel method to incorporate the knowledge reasoning capability into dialog systems in a more scalable and generalizable manner. Our proposed method allows a single transformer model to directly walk on a large-scale knowledge graph to generate responses. To the best of our knowledge, this is the first work to have transformer models generate responses by reasoning over differentiable knowledge graphs. We investigate the reasoning abilities of the proposed method on both task-oriented and domain-specific chit-chat dialogues. Empirical results show that this method can effectively and efficiently incorporate a knowledge graph into a dialogue system with fully-interpretable reasoning paths.</abstract>
      <url hash="53c555c0">2022.findings-acl.33</url>
      <bibkey>tuan-etal-2022-towards</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/opendialkg">OpenDialKG</pwcdataset>
    </paper>
    <paper id="34">
      <title><fixed-case>MDER</fixed-case>ank: A Masked Document Embedding Rank Approach for Unsupervised Keyphrase Extraction</title>
      <author><first>Linhan</first><last>Zhang</last></author>
      <author><first>Qian</first><last>Chen</last></author>
      <author><first>Wen</first><last>Wang</last></author>
      <author><first>Chong</first><last>Deng</last></author>
      <author><first>ShiLiang</first><last>Zhang</last></author>
      <author><first>Bing</first><last>Li</last></author>
      <author><first>Wei</first><last>Wang</last></author>
      <author><first>Xin</first><last>Cao</last></author>
      <pages>396-409</pages>
      <abstract>Keyphrase extraction (KPE) automatically extracts phrases in a document that provide a concise summary of the core content, which benefits downstream information retrieval and NLP tasks. Previous state-of-the-art methods select candidate keyphrases based on the similarity between learned representations of the candidates and the document. They suffer performance degradation on long documents due to discrepancy between sequence lengths which causes mismatch between representations of keyphrase candidates and the document. In this work, we propose a novel unsupervised embedding-based KPE approach, Masked Document Embedding Rank (MDERank), to address this problem by leveraging a mask strategy and ranking candidates by the similarity between embeddings of the source document and the masked document. We further develop a KPE-oriented BERT (KPEBERT) model by proposing a novel self-supervised contrastive learning method, which is more compatible to MDERank than vanilla BERT. Comprehensive evaluations on six KPE benchmarks demonstrate that the proposed MDERank outperforms state-of-the-art unsupervised KPE approach by average 1.80 <tex-math>F1@15</tex-math> improvement. MDERank further benefits from KPEBERT and overall achieves average 3.53 <tex-math>F1@15</tex-math> improvement over SIFRank.</abstract>
      <url hash="dc9b7e27">2022.findings-acl.34</url>
      <bibkey>zhang-etal-2022-mderank</bibkey>
      <pwccode url="https://github.com/linhanz/mderank" additional="false">linhanz/mderank</pwccode>
    </paper>
    <paper id="35">
      <title>Visualizing the Relationship Between Encoded Linguistic Information and Task Performance</title>
      <author><first>Jiannan</first><last>Xiang</last></author>
      <author><first>Huayang</first><last>Li</last></author>
      <author><first>Defu</first><last>Lian</last></author>
      <author><first>Guoping</first><last>Huang</last></author>
      <author><first>Taro</first><last>Watanabe</last></author>
      <author><first>Lemao</first><last>Liu</last></author>
      <pages>410-422</pages>
      <abstract>Probing is popular to analyze whether linguistic information can be captured by a well-trained deep neural model, but it is hard to answer how the change of the encoded linguistic information will affect task performance. To this end, we study the dynamic relationship between the encoded linguistic information and task performance from the viewpoint of Pareto Optimality. Its key idea is to obtain a set of models which are Pareto-optimal in terms of both objectives. From this viewpoint, we propose a method to optimize the Pareto-optimal models by formalizing it as a multi-objective optimization problem. We conduct experiments on two popular NLP tasks, i.e., machine translation and language modeling, and investigate the relationship between several kinds of linguistic information and task performances. Experimental results demonstrate that the proposed method is better than a baseline method. Our empirical findings suggest that some syntactic information is helpful for NLP tasks whereas encoding more syntactic information does not necessarily lead to better performance, because the model architecture is also an important factor.</abstract>
      <url hash="42104ce7">2022.findings-acl.35</url>
      <bibkey>xiang-etal-2022-visualizing</bibkey>
    </paper>
    <paper id="36">
      <title>Efficient Argument Structure Extraction with Transfer Learning and Active Learning</title>
      <author><first>Xinyu</first><last>Hua</last></author>
      <author><first>Lu</first><last>Wang</last></author>
      <pages>423-437</pages>
      <abstract>The automation of extracting argument structures faces a pair of challenges on (1) encoding long-term contexts to facilitate comprehensive understanding, and (2) improving data efficiency since constructing high-quality argument structures is time-consuming. In this work, we propose a novel context-aware Transformer-based argument structure prediction model which, on five different domains, significantly outperforms models that rely on features or only encode limited contexts. To tackle the difficulty of data annotation, we examine two complementary methods: (i) transfer learning to leverage existing annotated data to boost model performance in a new target domain, and (ii) active learning to strategically identify a small amount of samples for annotation. We further propose model-independent sample acquisition strategies, which can be generalized to diverse domains. With extensive experiments, we show that our simple-yet-effective acquisition strategies yield competitive results against three strong comparisons. Combined with transfer learning, substantial F1 score boost (5-25) can be further achieved during the early iterations of active learning across domains.</abstract>
      <url hash="52042a9a">2022.findings-acl.36</url>
      <bibkey>hua-wang-2022-efficient</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cdcp">CDCP</pwcdataset>
    </paper>
    <paper id="37">
      <title>Plug-and-Play Adaptation for Continuously-updated <fixed-case>QA</fixed-case></title>
      <author><first>Kyungjae</first><last>Lee</last></author>
      <author><first>Wookje</first><last>Han</last></author>
      <author><first>Seung-won</first><last>Hwang</last></author>
      <author><first>Hwaran</first><last>Lee</last></author>
      <author><first>Joonsuk</first><last>Park</last></author>
      <author><first>Sang-Woo</first><last>Lee</last></author>
      <pages>438-447</pages>
      <abstract>Language models (LMs) have shown great potential as implicit knowledge bases (KBs). And for their practical use, knowledge in LMs need to be updated periodically. However, existing tasks to assess LMs’ efficacy as KBs do not adequately consider multiple large-scale updates. To this end, we first propose a novel task—Continuously-updated QA (CuQA)—in which multiple large-scale updates are made to LMs, and the performance is measured with respect to the success in adding and updating knowledge while retaining existing knowledge. We then present LMs with plug-in modules that effectively handle the updates. Experiments conducted on zsRE QA and NQ datasets show that our method outperforms existing approaches. We find that our method is 4x more effective in terms of updates/forgets ratio, compared to a fine-tuning baseline.</abstract>
      <url hash="5d1d8fdd">2022.findings-acl.37</url>
      <attachment type="software" hash="06685060">2022.findings-acl.37.software.zip</attachment>
      <bibkey>lee-etal-2022-plug</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/situatedqa">SituatedQA</pwcdataset>
    </paper>
    <paper id="38">
      <title>Reinforced Cross-modal Alignment for Radiology Report Generation</title>
      <author><first>Han</first><last>Qin</last></author>
      <author><first>Yan</first><last>Song</last></author>
      <pages>448-458</pages>
      <abstract>Medical images are widely used in clinical decision-making, where writing radiology reports is a potential application that can be enhanced by automatic solutions to alleviate physicians’ workload. In general, radiology report generation is an image-text task, where cross-modal mappings between images and texts play an important role in generating high-quality reports. Although previous studies attempt to facilitate the alignment via the co-attention mechanism under supervised settings, they suffer from lacking valid and accurate correspondences due to no annotation of such alignment. In this paper, we propose an approach with reinforcement learning (RL) over a cross-modal memory (CMM) to better align visual and textual features for radiology report generation. In detail, a shared memory is used to record the mappings between visual and textual information, and the proposed reinforced algorithm is performed to learn the signal from the reports to guide the cross-modal alignment even though such reports are not directly related to how images and texts are mapped. Experimental results on two English radiology report datasets, i.e., IU X-Ray and MIMIC-CXR, show the effectiveness of our approach, where the state-of-the-art results are achieved. We further conduct human evaluation and case study which confirm the validity of the reinforced algorithm in our approach.</abstract>
      <url hash="96e63023">2022.findings-acl.38</url>
      <attachment type="software" hash="4730ab3c">2022.findings-acl.38.software.zip</attachment>
      <bibkey>qin-song-2022-reinforced</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/chexpert">CheXpert</pwcdataset>
    </paper>
    <paper id="39">
      <title>What Works and Doesn’t Work, A Deep Decoder for Neural Machine Translation</title>
      <author><first>Zuchao</first><last>Li</last></author>
      <author><first>Yiran</first><last>Wang</last></author>
      <author><first>Masao</first><last>Utiyama</last></author>
      <author><first>Eiichiro</first><last>Sumita</last></author>
      <author><first>Hai</first><last>Zhao</last></author>
      <author><first>Taro</first><last>Watanabe</last></author>
      <pages>459-471</pages>
      <abstract>Deep learning has demonstrated performance advantages in a wide range of natural language processing tasks, including neural machine translation (NMT). Transformer NMT models are typically strengthened by deeper encoder layers, but deepening their decoder layers usually results in failure. In this paper, we first identify the cause of the failure of the deep decoder in the Transformer model. Inspired by this discovery, we then propose approaches to improving it, with respect to model structure and model training, to make the deep decoder practical in NMT. Specifically, with respect to model structure, we propose a cross-attention drop mechanism to allow the decoder layers to perform their own different roles, to reduce the difficulty of deep-decoder learning. For model training, we propose a collapse reducing training approach to improve the stability and effectiveness of deep-decoder training. We experimentally evaluated our proposed Transformer NMT model structure modification and novel training methods on several popular machine translation benchmarks. The results showed that deepening the NMT model by increasing the number of decoder layers successfully prevented the deepened decoder from degrading to an unconditional language model. In contrast to prior work on deepening an NMT model on the encoder, our method can deepen the model on both the encoder and decoder at the same time, resulting in a deeper model and improved performance.</abstract>
      <url hash="a6d228a1">2022.findings-acl.39</url>
      <bibkey>li-etal-2022-works</bibkey>
    </paper>
    <paper id="40">
      <title><fixed-case>S</fixed-case>y<fixed-case>MC</fixed-case>o<fixed-case>M</fixed-case> - Syntactic Measure of Code Mixing A Study Of <fixed-case>E</fixed-case>nglish-<fixed-case>H</fixed-case>indi Code-Mixing</title>
      <author><first>Prashant</first><last>Kodali</last></author>
      <author><first>Anmol</first><last>Goel</last></author>
      <author><first>Monojit</first><last>Choudhury</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <author><first>Ponnurangam</first><last>Kumaraguru</last></author>
      <pages>472-480</pages>
      <abstract>Code mixing is the linguistic phenomenon where bilingual speakers tend to switch between two or more languages in conversations. Recent work on code-mixing in computational settings has leveraged social media code mixed texts to train NLP models. For capturing the variety of code mixing in, and across corpus, Language ID (LID) tags based measures (CMI) have been proposed. Syntactical variety/patterns of code-mixing and their relationship vis-a-vis computational model’s performance is under explored. In this work, we investigate a collection of English(en)-Hindi(hi) code-mixed datasets from a syntactic lens to propose, <tex-math>SyMCoM</tex-math>, an indicator of syntactic variety in code-mixed text, with intuitive theoretical bounds. We train SoTA en-hi PoS tagger, accuracy of 93.4%, to reliably compute PoS tags on a corpus, and demonstrate the utility of <tex-math>SyMCoM</tex-math> by applying it on various syntactical categories on a collection of datasets, and compare datasets using the measure.</abstract>
      <url hash="11f49965">2022.findings-acl.40</url>
      <bibkey>kodali-etal-2022-symcom</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/lince">LinCE</pwcdataset>
    </paper>
    <paper id="41">
      <title><fixed-case>H</fixed-case>ybri<fixed-case>D</fixed-case>ialogue: An Information-Seeking Dialogue Dataset Grounded on Tabular and Textual Data</title>
      <author><first>Kai</first><last>Nakamura</last></author>
      <author><first>Sharon</first><last>Levy</last></author>
      <author><first>Yi-Lin</first><last>Tuan</last></author>
      <author><first>Wenhu</first><last>Chen</last></author>
      <author><first>William Yang</first><last>Wang</last></author>
      <pages>481-492</pages>
      <abstract>A pressing challenge in current dialogue systems is to successfully converse with users on topics with information distributed across different modalities. Previous work in multiturn dialogue systems has primarily focused on either text or table information. In more realistic scenarios, having a joint understanding of both is critical as knowledge is typically distributed over both unstructured and structured forms. We present a new dialogue dataset, HybriDialogue, which consists of crowdsourced natural conversations grounded on both Wikipedia text and tables. The conversations are created through the decomposition of complex multihop questions into simple, realistic multiturn dialogue interactions. We propose retrieval, system state tracking, and dialogue response generation tasks for our dataset and conduct baseline experiments for each. Our results show that there is still ample opportunity for improvement, demonstrating the importance of building stronger dialogue systems that can reason over the complex setting of informationseeking dialogue grounded on tables and text.</abstract>
      <url hash="0a2a1c44">2022.findings-acl.41</url>
      <bibkey>nakamura-etal-2022-hybridialogue</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/coqa">CoQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/doqa">DoQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hybridqa">HybridQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ott-qa">OTT-QA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/recipeqa">RecipeQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sqa">SQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sharc">ShARC</pwcdataset>
    </paper>
    <paper id="42">
      <title><fixed-case>NEWTS</fixed-case>: A Corpus for News Topic-Focused Summarization</title>
      <author><first>Seyed Ali</first><last>Bahrainian</last></author>
      <author><first>Sheridan</first><last>Feucht</last></author>
      <author><first>Carsten</first><last>Eickhoff</last></author>
      <pages>493-503</pages>
      <abstract>Text summarization models are approaching human levels of fidelity. Existing benchmarking corpora provide concordant pairs of full and abridged versions of Web, news or professional content. To date, all summarization datasets operate under a one-size-fits-all paradigm that may not reflect the full range of organic summarization needs. Several recently proposed models (e.g., plug and play language models) have the capacity to condition the generated summaries on a desired range of themes. These capacities remain largely unused and unevaluated as there is no dedicated dataset that would support the task of topic-focused summarization.This paper introduces the first topical summarization corpus NEWTS, based on the well-known CNN/Dailymail dataset, and annotated via online crowd-sourcing. Each source article is paired with two reference summaries, each focusing on a different theme of the source document. We evaluate a representative range of existing techniques and analyze the effectiveness of different prompting methods.</abstract>
      <url hash="b277e1fe">2022.findings-acl.42</url>
      <bibkey>bahrainian-etal-2022-newts</bibkey>
    </paper>
    <paper id="43">
      <title>Classification without (Proper) Representation: Political Heterogeneity in Social Media and Its Implications for Classification and Behavioral Analysis</title>
      <author><first>Kenan</first><last>Alkiek</last></author>
      <author><first>Bohan</first><last>Zhang</last></author>
      <author><first>David</first><last>Jurgens</last></author>
      <pages>504-522</pages>
      <abstract>Reddit is home to a broad spectrum of political activity, and users signal their political affiliations in multiple ways—from self-declarations to community participation. Frequently, computational studies have treated political users as a single bloc, both in developing models to infer political leaning and in studying political behavior. Here, we test this assumption of political users and show that commonly-used political-inference models do not generalize, indicating heterogeneous types of political users. The models remain imprecise at best for most users, regardless of which sources of data or methods are used. Across a 14-year longitudinal analysis, we demonstrate that the choice in definition of a political user has significant implications for behavioral analysis. Controlling for multiple factors, political users are more toxic on the platform and inter-party interactions are even more toxic—but not all political users behave this way. Last, we identify a subset of political users who repeatedly flip affiliations, showing that these users are the most controversial of all, acting as provocateurs by more frequently bringing up politics, and are more likely to be banned, suspended, or deleted.</abstract>
      <url hash="4c1bc63e">2022.findings-acl.43</url>
      <bibkey>alkiek-etal-2022-classification</bibkey>
    </paper>
    <paper id="44">
      <title>Toward More Meaningful Resources for Lower-resourced Languages</title>
      <author><first>Constantine</first><last>Lignos</last></author>
      <author><first>Nolan</first><last>Holley</last></author>
      <author><first>Chester</first><last>Palen-Michel</last></author>
      <author><first>Jonne</first><last>Sälevä</last></author>
      <pages>523-532</pages>
      <abstract>In this position paper, we describe our perspective on how meaningful resources for lower-resourced languages should be developed in connection with the speakers of those languages. Before advancing that position, we first examine two massively multilingual resources used in language technology development, identifying shortcomings that limit their usefulness. We explore the contents of the names stored in Wikidata for a few lower-resourced languages and find that many of them are not in fact in the languages they claim to be, requiring non-trivial effort to correct. We discuss quality issues present in WikiAnn and evaluate whether it is a useful supplement to hand-annotated data. We then discuss the importance of creating annotations for lower-resourced languages in a thoughtful and ethical way that includes the language speakers as part of the development process. We conclude with recommended guidelines for resource development.</abstract>
      <url hash="5339d65b">2022.findings-acl.44</url>
      <bibkey>lignos-etal-2022-toward</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/masakhaner">MasakhaNER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikiann-1">WikiAnn</pwcdataset>
    </paper>
    <paper id="45">
      <title>Better Quality Estimation for Low Resource Corpus Mining</title>
      <author><first>Muhammed</first><last>Kocyigit</last></author>
      <author><first>Jiho</first><last>Lee</last></author>
      <author><first>Derry</first><last>Wijaya</last></author>
      <pages>533-543</pages>
      <abstract>Quality Estimation (QE) models have the potential to change how we evaluate and maybe even train machine translation models. However, these models still lack the robustness to achieve general adoption. We show that Stateof-the-art QE models, when tested in a Parallel Corpus Mining (PCM) setting, perform unexpectedly bad due to a lack of robustness to out-of-domain examples. We propose a combination of multitask training, data augmentation and contrastive learning to achieve better and more robust QE performance. We show that our method improves QE performance significantly in the MLQE challenge and the robustness of QE models when tested in the Parallel Corpus Mining setup. We increase the accuracy in PCM by more than 0.80, making it on par with state-of-the-art PCM methods that use millions of sentence pairs to train their models. In comparison, we use a thousand times less data, 7K parallel sentences in total, and propose a novel low resource PCM method.</abstract>
      <url hash="168b2e37">2022.findings-acl.45</url>
      <attachment type="software" hash="82443e78">2022.findings-acl.45.software.zip</attachment>
      <bibkey>kocyigit-etal-2022-better</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mlqe">MLQE</pwcdataset>
    </paper>
    <paper id="46">
      <title>End-to-End Segmentation-based News Summarization</title>
      <author id="yang-liu-microsoft"><first>Yang</first><last>Liu</last></author>
      <author><first>Chenguang</first><last>Zhu</last></author>
      <author><first>Michael</first><last>Zeng</last></author>
      <pages>544-554</pages>
      <abstract>In this paper, we bring a new way of digesting news content by introducing the task of segmenting a news article into multiple sections and generating the corresponding summary to each section. We make two contributions towards this new task. First, we create and make available a dataset, SegNews, consisting of 27k news articles with sections and aligned heading-style section summaries. Second, we propose a novel segmentation-based language generation model adapted from pre-trained language models that can jointly segment a document and produce the summary for each section. Experimental results on SegNews demonstrate that our model can outperform several state-of-the-art sequence-to-sequence generation models for this new task.</abstract>
      <url hash="4749987b">2022.findings-acl.46</url>
      <bibkey>liu-etal-2022-end</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
    </paper>
    <paper id="47">
      <title>Fast Nearest Neighbor Machine Translation</title>
      <author><first>Yuxian</first><last>Meng</last></author>
      <author><first>Xiaoya</first><last>Li</last></author>
      <author><first>Xiayu</first><last>Zheng</last></author>
      <author><first>Fei</first><last>Wu</last></author>
      <author><first>Xiaofei</first><last>Sun</last></author>
      <author><first>Tianwei</first><last>Zhang</last></author>
      <author><first>Jiwei</first><last>Li</last></author>
      <pages>555-565</pages>
      <abstract>Though nearest neighbor Machine Translation (<tex-math>k</tex-math>NN-MT) (CITATION) has proved to introduce significant performance boosts over standard neural MT systems, it is prohibitively slow since it uses the entire reference corpus as the datastore for the nearest neighbor search. This means each step for each beam in the beam search has to search over the entire reference corpus. <tex-math>k</tex-math>NN-MT is thus two-orders slower than vanilla MT models, making it hard to be applied to real-world applications, especially online services. In this work, we propose Fast <tex-math>k</tex-math>NN-MT to address this issue. Fast <tex-math>k</tex-math>NN-MT constructs a significantly smaller datastore for the nearest neighbor search: for each word in a source sentence, Fast <tex-math>k</tex-math>NN-MT first selects its nearest token-level neighbors, which is limited to tokens that are the same as the query token. Then at each decoding step, in contrast to using the entire corpus as the datastore, the search space is limited to target tokens corresponding to the previously selected reference source tokens. This strategy avoids search through the whole datastore for nearest neighbors and drastically improves decoding efficiency. Without loss of performance, Fast <tex-math>k</tex-math>NN-MT is two-orders faster than <tex-math>k</tex-math>NN-MT, and is only two times slower than the standard NMT model. Fast <tex-math>k</tex-math>NN-MT enables the practical use of <tex-math>k</tex-math>NN-MT systems in real-world MT applications. The code is available at <url>https://github.com/ShannonAI/fast-knn-nmt</url>.</abstract>
      <url hash="6757763e">2022.findings-acl.47</url>
      <bibkey>meng-etal-2022-fast</bibkey>
      <pwccode url="https://github.com/ShannonAI/fast-knn-nmt" additional="false">ShannonAI/fast-knn-nmt</pwccode>
    </paper>
    <paper id="48">
      <title>Extracting Latent Steering Vectors from Pretrained Language Models</title>
      <author><first>Nishant</first><last>Subramani</last></author>
      <author><first>Nivedita</first><last>Suresh</last></author>
      <author><first>Matthew</first><last>Peters</last></author>
      <pages>566-581</pages>
      <abstract>Prior work on controllable text generation has focused on learning how to control language models through trainable decoding, smart-prompt design, or fine-tuning based on a desired objective. We hypothesize that the information needed to steer the model to generate a target sentence is already encoded within the model. Accordingly, we explore a different approach altogether: extracting latent vectors directly from pretrained language model decoders without fine-tuning. Experiments show that there exist steering vectors, which, when added to the hidden states of the language model, generate a target sentence nearly perfectly (&gt; 99 BLEU) for English sentences from a variety of domains. We show that vector arithmetic can be used for unsupervised sentiment transfer on the Yelp sentiment benchmark, with performance comparable to models tailored to this task. We find that distances between steering vectors reflect sentence similarity when evaluated on a textual similarity benchmark (STS-B), outperforming pooled hidden states of models. Finally, we present an analysis of the intrinsic properties of the steering vectors. Taken together, our results suggest that frozen LMs can be effectively controlled through their latent steering space.</abstract>
      <url hash="4d0c3e8a">2022.findings-acl.48</url>
      <bibkey>subramani-etal-2022-extracting</bibkey>
      <pwccode url="https://github.com/nishantsubramani/steering_vectors" additional="false">nishantsubramani/steering_vectors</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/styleptb">StylePTB</pwcdataset>
    </paper>
    <paper id="49">
      <title>Domain Generalisation of <fixed-case>NMT</fixed-case>: Fusing Adapters with Leave-One-Domain-Out Training</title>
      <author><first>Thuy-Trang</first><last>Vu</last></author>
      <author><first>Shahram</first><last>Khadivi</last></author>
      <author><first>Dinh</first><last>Phung</last></author>
      <author><first>Gholamreza</first><last>Haffari</last></author>
      <pages>582-588</pages>
      <abstract>Generalising to unseen domains is under-explored and remains a challenge in neural machine translation. Inspired by recent research in parameter-efficient transfer learning from pretrained models, this paper proposes a fusion-based generalisation method that learns to combine domain-specific parameters. We propose a leave-one-domain-out training strategy to avoid information leaking to address the challenge of not knowing the test domain during training time. Empirical results on three language pairs show that our proposed fusion method outperforms other baselines up to +0.8 BLEU score on average.</abstract>
      <url hash="af446f61">2022.findings-acl.49</url>
      <bibkey>vu-etal-2022-domain</bibkey>
    </paper>
    <paper id="50">
      <title>Reframing Instructional Prompts to <fixed-case>GPT</fixed-case>k’s Language</title>
      <author><first>Swaroop</first><last>Mishra</last></author>
      <author><first>Daniel</first><last>Khashabi</last></author>
      <author><first>Chitta</first><last>Baral</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <author><first>Hannaneh</first><last>Hajishirzi</last></author>
      <pages>589-612</pages>
      <abstract>What kinds of instructional prompts are easier to follow for Language Models (LMs)? We study this question by conducting extensive empirical analysis that shed light on important features of successful instructional prompts. Specifically, we study several classes of reframing techniques for manual reformulation of prompts into more effective ones. Some examples include decomposing a complex task instruction into multiple simpler tasks or itemizing instructions into sequential steps. Our experiments compare the zero-shot and few-shot performance of LMs prompted with reframed instructions on 12 NLP tasks across 6 categories. Compared with original instructions, our reframed instructions lead to significant improvements across LMs with different sizes. For example, the same reframed prompts boost few-shot performance of GPT3-series and GPT2-series by 12.5% and 6.7% respectively averaged over all tasks. Furthermore, reframed instructions reduce the number of examples required to prompt LMs in the few-shot setting. We hope these empirically-driven techniques will pave the way towards more effective future prompting algorithms.</abstract>
      <url hash="d7d6729f">2022.findings-acl.50</url>
      <bibkey>mishra-etal-2022-reframing</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cosmosqa">CosmosQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mc-taco">MC-TACO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qasc">QASC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/winogrande">WinoGrande</pwcdataset>
    </paper>
    <paper id="51">
      <title>Read Top News First: A Document Reordering Approach for Multi-Document News Summarization</title>
      <author><first>Chao</first><last>Zhao</last></author>
      <author><first>Tenghao</first><last>Huang</last></author>
      <author><first>Somnath</first><last>Basu Roy Chowdhury</last></author>
      <author><first>Muthu Kumar</first><last>Chandrasekaran</last></author>
      <author><first>Kathleen</first><last>McKeown</last></author>
      <author><first>Snigdha</first><last>Chaturvedi</last></author>
      <pages>613-621</pages>
      <abstract>A common method for extractive multi-document news summarization is to re-formulate it as a single-document summarization problem by concatenating all documents as a single meta-document. However, this method neglects the relative importance of documents. We propose a simple approach to reorder the documents according to their relative importance before concatenating and summarizing them. The reordering makes the salient content easier to learn by the summarization model. Experiments show that our approach outperforms previous state-of-the-art methods with more complex architectures.</abstract>
      <url hash="5491b6fd">2022.findings-acl.51</url>
      <bibkey>zhao-etal-2022-read</bibkey>
      <pwccode url="https://github.com/zhaochaocs/mds-dr" additional="false">zhaochaocs/mds-dr</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multi-news">Multi-News</pwcdataset>
    </paper>
    <paper id="52">
      <title>Human Language Modeling</title>
      <author><first>Nikita</first><last>Soni</last></author>
      <author><first>Matthew</first><last>Matero</last></author>
      <author><first>Niranjan</first><last>Balasubramanian</last></author>
      <author><first>H.</first><last>Schwartz</last></author>
      <pages>622-636</pages>
      <abstract>Natural language is generated by people, yet traditional language modeling views words or documents as if generated independently. Here, we propose human language modeling (HuLM), a hierarchical extension to the language modeling problem where by a human- level exists to connect sequences of documents (e.g. social media messages) and capture the notion that human language is moderated by changing human states. We introduce, HaRT, a large-scale transformer model for solving HuLM, pre-trained on approximately 100,000 social media users, and demonstrate it’s effectiveness in terms of both language modeling (perplexity) for social media and fine-tuning for 4 downstream tasks spanning document- and user-levels. Results on all tasks meet or surpass the current state-of-the-art.</abstract>
      <url hash="7ec3f959">2022.findings-acl.52</url>
      <bibkey>soni-etal-2022-human</bibkey>
      <pwccode url="https://github.com/humanlab/hart" additional="false">humanlab/hart</pwccode>
    </paper>
    <paper id="53">
      <title>Inverse is Better! Fast and Accurate Prompt for Few-shot Slot Tagging</title>
      <author><first>Yutai</first><last>Hou</last></author>
      <author><first>Cheng</first><last>Chen</last></author>
      <author><first>Xianzhen</first><last>Luo</last></author>
      <author><first>Bohan</first><last>Li</last></author>
      <author><first>Wanxiang</first><last>Che</last></author>
      <pages>637-647</pages>
      <abstract>Prompting methods recently achieve impressive success in few-shot learning. These methods modify input samples with prompt sentence pieces, and decode label tokens to map samples to corresponding labels. However, such a paradigm is very inefficient for the task of slot tagging. Since slot tagging samples are multiple consecutive words in a sentence, the prompting methods have to enumerate all n-grams token spans to find all the possible slots, which greatly slows down the prediction. To tackle this, we introduce an inverse paradigm for prompting. Different from the classic prompts mapping tokens to labels, we reversely predict slot values given slot types. Such inverse prompting only requires a one-turn prediction for each slot type and greatly speeds up the prediction. Besides, we propose a novel Iterative Prediction Strategy, from which the model learns to refine predictions by considering the relations between different slot types. We find, somewhat surprisingly, the proposed method not only predicts faster but also significantly improves the effect (improve over 6.1 F1-scores on 10-shot setting) and achieves new state-of-the-art performance.</abstract>
      <url hash="bd9d02cc">2022.findings-acl.53</url>
      <bibkey>hou-etal-2022-inverse</bibkey>
      <pwccode url="https://github.com/atmahou/promptslottagging" additional="false">atmahou/promptslottagging</pwccode>
    </paper>
    <paper id="54">
      <title>Cross-Modal Cloze Task: A New Task to Brain-to-Word Decoding</title>
      <author><first>Shuxian</first><last>Zou</last></author>
      <author><first>Shaonan</first><last>Wang</last></author>
      <author><first>Jiajun</first><last>Zhang</last></author>
      <author><first>Chengqing</first><last>Zong</last></author>
      <pages>648-657</pages>
      <abstract>Decoding language from non-invasive brain activity has attracted increasing attention from both researchers in neuroscience and natural language processing. Due to the noisy nature of brain recordings, existing work has simplified brain-to-word decoding as a binary classification task which is to discriminate a brain signal between its corresponding word and a wrong one. This pairwise classification task, however, cannot promote the development of practical neural decoders for two reasons. First, it has to enumerate all pairwise combinations in the test set, so it is inefficient to predict a word in a large vocabulary. Second, a perfect pairwise decoder cannot guarantee the performance on direct classification. To overcome these and go a step further to a realistic neural decoder, we propose a novel Cross-Modal Cloze (CMC) task which is to predict the target word encoded in the neural image with a context as prompt. Furthermore, to address this task, we propose a general approach that leverages the pre-trained language model to predict the target word. To validate our method, we perform experiments on more than 20 participants from two brain imaging datasets. Our method achieves 28.91% top-1 accuracy and 54.19% top-5 accuracy on average across all participants, significantly outperforming several baselines. This result indicates that our model can serve as a state-of-the-art baseline for the CMC task. More importantly, it demonstrates that it is feasible to decode a certain word within a large vocabulary from its neural brain activity.</abstract>
      <url hash="3ea873e7">2022.findings-acl.54</url>
      <bibkey>zou-etal-2022-cross</bibkey>
      <pwccode url="https://github.com/littletreezou/cross-modal-cloze-task" additional="false">littletreezou/cross-modal-cloze-task</pwccode>
    </paper>
    <paper id="55">
      <title>Mitigating Gender Bias in Distilled Language Models via Counterfactual Role Reversal</title>
      <author><first>Umang</first><last>Gupta</last></author>
      <author><first>Jwala</first><last>Dhamala</last></author>
      <author><first>Varun</first><last>Kumar</last></author>
      <author><first>Apurv</first><last>Verma</last></author>
      <author><first>Yada</first><last>Pruksachatkun</last></author>
      <author><first>Satyapriya</first><last>Krishna</last></author>
      <author><first>Rahul</first><last>Gupta</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <author><first>Greg</first><last>Ver Steeg</last></author>
      <author><first>Aram</first><last>Galstyan</last></author>
      <pages>658-678</pages>
      <abstract>Language models excel at generating coherent text, and model compression techniques such as knowledge distillation have enabled their use in resource-constrained settings. However, these models can be biased in multiple ways, including the unfounded association of male and female genders with gender-neutral professions. Therefore, knowledge distillation without any fairness constraints may preserve or exaggerate the teacher model’s biases onto the distilled model. To this end, we present a novel approach to mitigate gender disparity in text generation by learning a fair model during knowledge distillation. We propose two modifications to the base knowledge distillation based on counterfactual role reversal—modifying teacher probabilities and augmenting the training set. We evaluate gender polarity across professions in open-ended text generated from the resulting distilled and finetuned GPT–2 models and demonstrate a substantial reduction in gender disparity with only a minor compromise in utility. Finally, we observe that language models that reduce gender polarity in language generation do not improve embedding fairness or downstream classification fairness.</abstract>
      <url hash="2406173b">2022.findings-acl.55</url>
      <bibkey>gupta-etal-2022-mitigating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/webtext">WebText</pwcdataset>
    </paper>
    <paper id="56">
      <title>Domain Representative Keywords Selection: A Probabilistic Approach</title>
      <author><first>Pritom Saha</first><last>Akash</last></author>
      <author><first>Jie</first><last>Huang</last></author>
      <author><first>Kevin</first><last>Chang</last></author>
      <author><first>Yunyao</first><last>Li</last></author>
      <author><first>Lucian</first><last>Popa</last></author>
      <author><first>ChengXiang</first><last>Zhai</last></author>
      <pages>679-692</pages>
      <abstract>We propose a probabilistic approach to select a subset of a <i>target domain representative keywords</i> from a candidate set, contrasting with a context domain. Such a task is crucial for many downstream tasks in natural language processing. To contrast the target domain and the context domain, we adapt the <i>two-component mixture model</i> concept to generate a distribution of candidate keywords. It provides more importance to the <i>distinctive</i> keywords of the target domain than common keywords contrasting with the context domain. To support the <i>representativeness</i> of the selected keywords towards the target domain, we introduce an <i>optimization algorithm</i> for selecting the subset from the generated candidate distribution. We have shown that the optimization algorithm can be efficiently implemented with a near-optimal approximation guarantee. Finally, extensive experiments on multiple domains demonstrate the superiority of our approach over other baselines for the tasks of keyword summary generation and trending keywords selection.</abstract>
      <url hash="d905da42">2022.findings-acl.56</url>
      <bibkey>akash-etal-2022-domain</bibkey>
      <pwccode url="https://github.com/pritomsaha/keyword-selection" additional="false">pritomsaha/keyword-selection</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/aminer">AMiner</pwcdataset>
    </paper>
    <paper id="57">
      <title>Hierarchical Inductive Transfer for Continual Dialogue Learning</title>
      <author><first>Shaoxiong</first><last>Feng</last></author>
      <author><first>Xuancheng</first><last>Ren</last></author>
      <author><first>Kan</first><last>Li</last></author>
      <author><first>Xu</first><last>Sun</last></author>
      <pages>693-699</pages>
      <abstract>Pre-trained models have achieved excellent performance on the dialogue task. However, for the continual increase of online chit-chat scenarios, directly fine-tuning these models for each of the new tasks not only explodes the capacity of the dialogue system on the embedded devices but also causes knowledge forgetting on pre-trained models and knowledge interference among diverse dialogue tasks. In this work, we propose a hierarchical inductive transfer framework to learn and deploy the dialogue skills continually and efficiently. First, we introduce the adapter module into pre-trained models for learning new dialogue tasks. As the only trainable module, it is beneficial for the dialogue system on the embedded devices to acquire new dialogue skills with negligible additional parameters. Then, for alleviating knowledge interference between tasks yet benefiting the regularization between them, we further design hierarchical inductive transfer that enables new tasks to use general knowledge in the base adapter without being misled by diverse knowledge in task-specific adapters. Empirical evaluation and analysis indicate that our framework obtains comparable performance under deployment-friendly model capacity.</abstract>
      <url hash="d38f5210">2022.findings-acl.57</url>
      <bibkey>feng-etal-2022-hierarchical</bibkey>
    </paper>
    <paper id="58">
      <title>Why Exposure Bias Matters: An Imitation Learning Perspective of Error Accumulation in Language Generation</title>
      <author><first>Kushal</first><last>Arora</last></author>
      <author><first>Layla</first><last>El Asri</last></author>
      <author><first>Hareesh</first><last>Bahuleyan</last></author>
      <author><first>Jackie</first><last>Cheung</last></author>
      <pages>700-710</pages>
      <abstract>Current language generation models suffer from issues such as repetition, incoherence, and hallucinations. An often-repeated hypothesis for this brittleness of generation models is that it is caused by the training and the generation procedure mismatch, also referred to as exposure bias. In this paper, we verify this hypothesis by analyzing exposure bias from an imitation learning perspective. We show that exposure bias leads to an accumulation of errors during generation, analyze why perplexity fails to capture this accumulation of errors, and empirically show that this accumulation results in poor generation quality.</abstract>
      <url hash="c1bd655f">2022.findings-acl.58</url>
      <bibkey>arora-etal-2022-exposure</bibkey>
      <pwccode url="https://github.com/kushalarora/quantifying_exposure_bias" additional="false">kushalarora/quantifying_exposure_bias</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
    </paper>
    <paper id="59">
      <title>Question Answering Infused Pre-training of General-Purpose Contextualized Representations</title>
      <author><first>Robin</first><last>Jia</last></author>
      <author><first>Mike</first><last>Lewis</last></author>
      <author><first>Luke</first><last>Zettlemoyer</last></author>
      <pages>711-728</pages>
      <abstract>We propose a pre-training objective based on question answering (QA) for learning general-purpose contextual representations, motivated by the intuition that the representation of a phrase in a passage should encode all questions that the phrase can answer in context. To this end, we train a bi-encoder QA model, which independently encodes passages and questions, to match the predictions of a more accurate cross-encoder model on 80 million synthesized QA pairs. By encoding QA-relevant information, the bi-encoder’s token-level representations are useful for non-QA downstream tasks without extensive (or in some cases, any) fine-tuning. We show large improvements over both RoBERTa-large and previous state-of-the-art results on zero-shot and few-shot paraphrase detection on four datasets, few-shot named entity recognition on two datasets, and zero-shot sentiment analysis on three datasets.</abstract>
      <url hash="37aba859">2022.findings-acl.59</url>
      <bibkey>jia-etal-2022-question</bibkey>
      <pwccode url="https://github.com/facebookresearch/quip" additional="false">facebookresearch/quip</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrqa-2019">MRQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsqa">NewsQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paws">PAWS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/searchqa">SearchQA</pwcdataset>
    </paper>
    <paper id="60">
      <title>Automatic Song Translation for Tonal Languages</title>
      <author><first>Fenfei</first><last>Guo</last></author>
      <author><first>Chen</first><last>Zhang</last></author>
      <author><first>Zhirui</first><last>Zhang</last></author>
      <author><first>Qixin</first><last>He</last></author>
      <author><first>Kejun</first><last>Zhang</last></author>
      <author><first>Jun</first><last>Xie</last></author>
      <author><first>Jordan</first><last>Boyd-Graber</last></author>
      <pages>729-743</pages>
      <abstract>This paper develops automatic song translation (AST) for tonal languages and addresses the unique challenge of aligning words’ tones with melody of a song in addition to conveying the original meaning. We propose three criteria for effective AST—preserving meaning, singability and intelligibility—and design metrics for these criteria. We develop a new benchmark for English–Mandarin song translation and develop an unsupervised AST system, Guided AliGnment for Automatic Song Translation (GagaST), which combines pre-training with three decoding constraints. Both automatic and human evaluations show GagaST successfully balances semantics and singability.</abstract>
      <url hash="84bff581">2022.findings-acl.60</url>
      <bibkey>guo-etal-2022-automatic</bibkey>
    </paper>
    <paper id="61">
      <title>Read before Generate! Faithful Long Form Question Answering with Machine Reading</title>
      <author><first>Dan</first><last>Su</last></author>
      <author><first>Xiaoguang</first><last>Li</last></author>
      <author><first>Jindi</first><last>Zhang</last></author>
      <author><first>Lifeng</first><last>Shang</last></author>
      <author><first>Xin</first><last>Jiang</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <pages>744-756</pages>
      <abstract>Long-form question answering (LFQA) aims to generate a paragraph-length answer for a given question. While current work on LFQA using large pre-trained model for generation are effective at producing fluent and somewhat relevant content, one primary challenge lies in how to generate a faithful answer that has less hallucinated content. We propose a new end-to-end framework that jointly models answer generation and machine reading. The key idea is to augment the generation model with fine-grained, answer-related salient information which can be viewed as an emphasis on faithful facts. State-of-the-art results on two LFQA datasets, ELI5 and MS MARCO, demonstrate the effectiveness of our method, in comparison with strong baselines on automatic and human evaluation metrics. A detailed analysis further proves the competency of our methods in generating fluent, relevant, and more faithful answers.</abstract>
      <url hash="7ed815bf">2022.findings-acl.61</url>
      <bibkey>su-etal-2022-read</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/eli5">ELI5</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/kilt">KILT</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
    </paper>
    <paper id="62">
      <title>A Simple yet Effective Relation Information Guided Approach for Few-Shot Relation Extraction</title>
      <author id="yang-liu-hk"><first>Yang</first><last>Liu</last></author>
      <author><first>Jinpeng</first><last>Hu</last></author>
      <author><first>Xiang</first><last>Wan</last></author>
      <author><first>Tsung-Hui</first><last>Chang</last></author>
      <pages>757-763</pages>
      <abstract>Few-Shot Relation Extraction aims at predicting the relation for a pair of entities in a sentence by training with a few labelled examples in each relation. Some recent works have introduced relation information (i.e., relation labels or descriptions) to assist model learning based on Prototype Network. However, most of them constrain the prototypes of each relation class implicitly with relation information, generally through designing complex network structures, like generating hybrid features, combining with contrastive learning or attention networks. We argue that relation information can be introduced more explicitly and effectively into the model. Thus, this paper proposes a direct addition approach to introduce relation information. Specifically, for each relation class, the relation representation is first generated by concatenating two views of relations (i.e., [CLS] token embedding and the mean value of embeddings of all tokens) and then directly added to the original prototype for both train and prediction. Experimental results on the benchmark dataset FewRel 1.0 show significant improvements and achieve comparable results to the state-of-the-art, which demonstrates the effectiveness of our proposed approach. Besides, further analyses verify that the direct addition is a much more effective way to integrate the relation representations and the original prototypes.</abstract>
      <url hash="ab7978e8">2022.findings-acl.62</url>
      <attachment type="software" hash="0e1fede5">2022.findings-acl.62.software.zip</attachment>
      <bibkey>liu-etal-2022-simple</bibkey>
      <pwccode url="https://github.com/lylylylylyly/simplefsre" additional="false">lylylylylyly/simplefsre</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel">FewRel</pwcdataset>
    </paper>
    <paper id="63">
      <title><fixed-case>MIMIC</fixed-case>ause: <fixed-case>R</fixed-case>epresentation and automatic extraction of causal relation types from clinical notes</title>
      <author><first>Vivek</first><last>Khetan</last></author>
      <author><first>Md Imbesat</first><last>Rizvi</last></author>
      <author><first>Jessica</first><last>Huber</last></author>
      <author><first>Paige</first><last>Bartusiak</last></author>
      <author><first>Bogdan</first><last>Sacaleanu</last></author>
      <author><first>Andrew</first><last>Fano</last></author>
      <pages>764-773</pages>
      <abstract>Understanding causal narratives communicated in clinical notes can help make strides towards personalized healthcare. Extracted causal information from clinical notes can be combined with structured EHR data such as patients’ demographics, diagnoses, and medications. This will enhance healthcare providers’ ability to identify aspects of a patient’s story communicated in the clinical notes and help make more informed decisions. In this work, we propose annotation guidelines, develop an annotated corpus and provide baseline scores to identify types and direction of causal relations between a pair of biomedical concepts in clinical notes; communicated implicitly or explicitly, identified either in a single sentence or across multiple sentences. We annotate a total of 2714 de-identified examples sampled from the 2018 n2c2 shared task dataset and train four different language model based architectures. Annotation based on our guidelines achieved a high inter-annotator agreement i.e. Fleiss’ kappa (<tex-math>\kappa</tex-math>) score of 0.72, and our model for identification of causal relations achieved a macro F1 score of 0.56 on the test data. The high inter-annotator agreement for clinical text shows the quality of our annotation guidelines while the provided baseline F1 score sets the direction for future research towards understanding narratives in clinical texts. </abstract>
      <url hash="36479e9c">2022.findings-acl.63</url>
      <bibkey>khetan-etal-2022-mimicause</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mimic-iii">MIMIC-III</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/rocstories">ROCStories</pwcdataset>
    </paper>
    <paper id="64">
      <title>Compressing Sentence Representation for Semantic Retrieval via Homomorphic Projective Distillation</title>
      <author><first>Xuandong</first><last>Zhao</last></author>
      <author><first>Zhiguo</first><last>Yu</last></author>
      <author><first>Ming</first><last>Wu</last></author>
      <author><first>Lei</first><last>Li</last></author>
      <pages>774-781</pages>
      <abstract>How to learn highly compact yet effective sentence representation? Pre-trained language models have been effective in many NLP tasks. However, these models are often huge and produce large sentence embeddings. Moreover, there is a big performance gap between large and small models. In this paper, we propose Homomorphic Projective Distillation (HPD) to learn compressed sentence embeddings. Our method augments a small Transformer encoder model with learnable projection layers to produce compact representations while mimicking a large pre-trained language model to retain the sentence representation quality. We evaluate our method with different model sizes on both semantic textual similarity (STS) and semantic retrieval (SR) tasks. Experiments show that our method achieves 2.7-4.5 points performance gain on STS tasks compared with previous best representations of the same size. In SR tasks, our method improves retrieval speed (8.2×) and memory usage (8.0×) compared with state-of-the-art large models. Our implementation is available at <url>https://github.com/XuandongZhao/HPD</url>.</abstract>
      <url hash="3c74a3f8">2022.findings-acl.64</url>
      <bibkey>zhao-etal-2022-compressing</bibkey>
      <pwccode url="https://github.com/xuandongzhao/hpd" additional="false">xuandongzhao/hpd</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="65">
      <title>Debiasing Event Understanding for Visual Commonsense Tasks</title>
      <author><first>Minji</first><last>Seo</last></author>
      <author><first>YeonJoon</first><last>Jung</last></author>
      <author><first>Seungtaek</first><last>Choi</last></author>
      <author><first>Seung-won</first><last>Hwang</last></author>
      <author><first>Bei</first><last>Liu</last></author>
      <pages>782-787</pages>
      <abstract>We study event understanding as a critical step towards visual commonsense tasks.Meanwhile, we argue that current object-based event understanding is purely likelihood-based, leading to incorrect event prediction, due to biased correlation between events and objects.We propose to mitigate such biases with <tex-math>do</tex-math>-calculus, proposed in causality research, but overcoming its limited robustness, by an optimized aggregation with association-based prediction.We show the effectiveness of our approach, intrinsically by comparing our generated events with ground-truth event annotation, and extrinsically by downstream commonsense tasks.</abstract>
      <url hash="5e4bf4fd">2022.findings-acl.65</url>
      <attachment type="software" hash="c4f67a1e">2022.findings-acl.65.software.zip</attachment>
      <bibkey>seo-etal-2022-debiasing</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/vcr">VCR</pwcdataset>
    </paper>
    <paper id="66">
      <title>Fact-Tree Reasoning for N-ary Question Answering over Knowledge Graphs</title>
      <author><first>Yao</first><last>Zhang</last></author>
      <author><first>Peiyao</first><last>Li</last></author>
      <author><first>Hongru</first><last>Liang</last></author>
      <author><first>Adam</first><last>Jatowt</last></author>
      <author><first>Zhenglu</first><last>Yang</last></author>
      <pages>788-802</pages>
      <abstract>Current Question Answering over Knowledge Graphs (KGQA) task mainly focuses on performing answer reasoning upon KGs with binary facts. However, it neglects the n-ary facts, which contain more than two entities. In this work, we highlight a more challenging but under-explored task: n-ary KGQA, i.e., answering n-ary facts questions upon n-ary KGs. Nevertheless, the multi-hop reasoning framework popular in binary KGQA task is not directly applicable on n-ary KGQA. We propose two feasible improvements: 1) upgrade the basic reasoning unit from entity or relation to fact, and 2) upgrade the reasoning structure from chain to tree. Therefore, we propose a novel fact-tree reasoning framework, FacTree, which integrates the above two upgrades. FacTree transforms the question into a fact tree and performs iterative fact reasoning on the fact tree to infer the correct answer. Experimental results on the n-ary KGQA dataset we constructed and two binary KGQA benchmarks demonstrate the effectiveness of FacTree compared with state-of-the-art methods.</abstract>
      <url hash="248c692c">2022.findings-acl.66</url>
      <bibkey>zhang-etal-2022-fact</bibkey>
    </paper>
    <paper id="67">
      <title><fixed-case>D</fixed-case>eep<fixed-case>S</fixed-case>truct: Pretraining of Language Models for Structure Prediction</title>
      <author><first>Chenguang</first><last>Wang</last></author>
      <author><first>Xiao</first><last>Liu</last></author>
      <author><first>Zui</first><last>Chen</last></author>
      <author><first>Haoyun</first><last>Hong</last></author>
      <author><first>Jie</first><last>Tang</last></author>
      <author><first>Dawn</first><last>Song</last></author>
      <pages>803-823</pages>
      <abstract>We introduce a method for improving the structural understanding abilities of language models. Unlike previous approaches that finetune the models with task-specific augmentation, we pretrain language models to generate structures from the text on a collection of task-agnostic corpora. Our structure pretraining enables zero-shot transfer of the learned knowledge that models have about the structure tasks. We study the performance of this approach on 28 datasets, spanning 10 structure prediction tasks including open information extraction, joint entity and relation extraction, named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, factual probe, intent detection, and dialogue state tracking. We further enhance the pretraining with the task-specific training sets. We show that a 10B parameter language model transfers non-trivially to most tasks and obtains state-of-the-art performance on 21 of 28 datasets that we evaluate. Our code and datasets will be made publicly available.</abstract>
      <url hash="658a9ec9">2022.findings-acl.67</url>
      <bibkey>wang-etal-2022-deepstruct</bibkey>
      <revision id="1" href="2022.findings-acl.67v1" hash="2f9acce6"/>
      <revision id="2" href="2022.findings-acl.67v2" hash="658a9ec9" date="2022-05-16">In the appendix, revises descriptions of certain datasets used in the experiments to provide more clarity and details.</revision>
      <pwccode url="https://github.com/cgraywang/deepstruct" additional="false">cgraywang/deepstruct</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/atis">ATIS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel">FewRel</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/kelm">KELM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multiwoz">MultiWOZ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/opiec">OPIEC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/t-rex">T-REx</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tekgen">TekGen</pwcdataset>
    </paper>
    <paper id="68">
      <title>The Change that Matters in Discourse Parsing: Estimating the Impact of Domain Shift on Parser Error</title>
      <author><first>Katherine</first><last>Atwell</last></author>
      <author><first>Anthony</first><last>Sicilia</last></author>
      <author><first>Seong Jae</first><last>Hwang</last></author>
      <author><first>Malihe</first><last>Alikhani</last></author>
      <pages>824-845</pages>
      <abstract>Discourse analysis allows us to attain inferences of a text document that extend beyond the sentence-level. The current performance of discourse models is very low on texts outside of the training distribution’s coverage, diminishing the practical utility of existing models. There is need for a measure that can inform us to what extent our model generalizes from the training to the test sample when these samples may be drawn from distinct distributions. While this can be estimated via distribution shift, we argue that this does not directly correlate with change in the observed error of a classifier (i.e. error-gap). Thus, we propose to use a statistic from the theoretical domain adaptation literature which can be directly tied to error-gap. We study the bias of this statistic as an estimator of error-gap both theoretically and through a large-scale empirical study of over 2400 experiments on 6 discourse datasets from domains including, but not limited to: news, biomedical texts, TED talks, Reddit posts, and fiction. Our results not only motivate our proposal and help us to understand its limitations, but also provide insight on the properties of discourse models and datasets which improve performance in domain adaptation. For instance, we find that non-news datasets are slightly easier to transfer to than news datasets when the training and test sets are very different. Our code and an associated Python package are available to allow practitioners to make more informed model and dataset choices.</abstract>
      <url hash="65f823c8">2022.findings-acl.68</url>
      <bibkey>atwell-etal-2022-change</bibkey>
      <pwccode url="https://github.com/anthonysicilia/change-that-matters-acl2022" additional="false">anthonysicilia/change-that-matters-acl2022</pwccode>
    </paper>
    <paper id="69">
      <title>Mukayese: <fixed-case>T</fixed-case>urkish <fixed-case>NLP</fixed-case> Strikes Back</title>
      <author><first>Ali</first><last>Safaya</last></author>
      <author><first>Emirhan</first><last>Kurtuluş</last></author>
      <author><first>Arda</first><last>Goktogan</last></author>
      <author><first>Deniz</first><last>Yuret</last></author>
      <pages>846-863</pages>
      <abstract>Having sufficient resources for language X lifts it from the under-resourced languages class, but not necessarily from the under-researched class. In this paper, we address the problem of the absence of organized benchmarks in the Turkish language. We demonstrate that languages such as Turkish are left behind the state-of-the-art in NLP applications. As a solution, we present Mukayese, a set of NLP benchmarks for the Turkish language that contains several NLP tasks. We work on one or more datasets for each benchmark and present two or more baselines. Moreover, we present four new benchmarking datasets in Turkish for language modeling, sentence segmentation, and spell checking. All datasets and baselines are available under: https://github.com/alisafaya/mukayese</abstract>
      <url hash="e0c245a3">2022.findings-acl.69</url>
      <bibkey>safaya-etal-2022-mukayese</bibkey>
      <pwccode url="https://github.com/alisafaya/mukayese" additional="false">alisafaya/mukayese</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
    <paper id="70">
      <title>Virtual Augmentation Supported Contrastive Learning of Sentence Representations</title>
      <author><first>Dejiao</first><last>Zhang</last></author>
      <author><first>Wei</first><last>Xiao</last></author>
      <author><first>Henghui</first><last>Zhu</last></author>
      <author><first>Xiaofei</first><last>Ma</last></author>
      <author><first>Andrew</first><last>Arnold</last></author>
      <pages>864-876</pages>
      <abstract>Despite profound successes, contrastive representation learning relies on carefully designed data augmentations using domain-specific knowledge. This challenge is magnified in natural language processing, where no general rules exist for data augmentation due to the discrete nature of natural language. We tackle this challenge by presenting a Virtual augmentation Supported Contrastive Learning of sentence representations (VaSCL). Originating from the interpretation that data augmentation essentially constructs the neighborhoods of each training instance, we, in turn, utilize the neighborhood to generate effective data augmentations. Leveraging the large training batch size of contrastive learning, we approximate the neighborhood of an instance via its K-nearest in-batch neighbors in the representation space. We then define an instance discrimination task regarding the neighborhood and generate the virtual augmentation in an adversarial training manner. We access the performance of VaSCL on a wide range of downstream tasks and set a new state-of-the-art for unsupervised sentence representation learning.</abstract>
      <url hash="308247c5">2022.findings-acl.70</url>
      <bibkey>zhang-etal-2022-virtual</bibkey>
      <pwccode url="https://github.com/amazon-research/sentence-representations" additional="false">amazon-research/sentence-representations</pwccode>
    </paper>
    <paper id="71">
      <title><fixed-case>M</fixed-case>o<fixed-case>E</fixed-case>fication: Transformer Feed-forward Layers are Mixtures of Experts</title>
      <author><first>Zhengyan</first><last>Zhang</last></author>
      <author><first>Yankai</first><last>Lin</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Peng</first><last>Li</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>877-890</pages>
      <abstract>Recent work has shown that feed-forward networks (FFNs) in pre-trained Transformers are a key component, storing various linguistic and factual knowledge. However, the computational patterns of FFNs are still unclear. In this work, we study the computational patterns of FFNs and observe that most inputs only activate a tiny ratio of neurons of FFNs. This phenomenon is similar to the sparsity of the human brain, which drives research on functional partitions of the human brain. To verify whether functional partitions also emerge in FFNs, we propose to convert a model into its MoE version with the same parameters, namely MoEfication. Specifically, MoEfication consists of two phases: (1) splitting the parameters of FFNs into multiple functional partitions as experts, and (2) building expert routers to decide which experts will be used for each input. Experimental results show that MoEfication can conditionally use 10% to 30% of FFN parameters while maintaining over 95% original performance for different models on various downstream tasks. Besides, MoEfication brings two advantages: (1) it significantly reduces the FLOPS of inference, i.e., 2x speedup with 25% of FFN parameters, and (2) it provides a fine-grained perspective to study the inner mechanism of FFNs. The source code of this paper can be obtained from https://github.com/thunlp/MoEfication.</abstract>
      <url hash="adc169ce">2022.findings-acl.71</url>
      <bibkey>zhang-etal-2022-moefication</bibkey>
      <pwccode url="https://github.com/thunlp/moefication" additional="false">thunlp/moefication</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="72">
      <title><fixed-case>DS</fixed-case>-<fixed-case>TOD</fixed-case>: Efficient Domain Specialization for Task-Oriented Dialog</title>
      <author><first>Chia-Chien</first><last>Hung</last></author>
      <author><first>Anne</first><last>Lauscher</last></author>
      <author><first>Simone</first><last>Ponzetto</last></author>
      <author><first>Goran</first><last>Glavaš</last></author>
      <pages>891-904</pages>
      <abstract>Recent work has shown that self-supervised dialog-specific pretraining on large conversational datasets yields substantial gains over traditional language modeling (LM) pretraining in downstream task-oriented dialog (TOD). These approaches, however, exploit general dialogic corpora (e.g., Reddit) and thus presumably fail to reliably embed domain-specific knowledge useful for concrete downstream TOD domains. In this work, we investigate the effects of domain specialization of pretrained language models (PLMs) for TOD. Within our DS-TOD framework, we first automatically extract salient domain-specific terms, and then use them to construct DomainCC and DomainReddit – resources that we leverage for domain-specific pretraining, based on (i) masked language modeling (MLM) and (ii) response selection (RS) objectives, respectively. We further propose a resource-efficient and modular domain specialization by means of domain adapters – additional parameter-light layers in which we encode the domain knowledge. Our experiments with prominent TOD tasks – dialog state tracking (DST) and response retrieval (RR) – encompassing five domains from the MultiWOZ benchmark demonstrate the effectiveness of DS-TOD. Moreover, we show that the light-weight adapter-based specialization (1) performs comparably to full fine-tuning in single domain setups and (2) is particularly suitable for multi-domain specialization, where besides advantageous computational footprint, it can offer better TOD performance.</abstract>
      <url hash="741450bc">2022.findings-acl.72</url>
      <attachment type="software" hash="39f682c7">2022.findings-acl.72.software.zip</attachment>
      <bibkey>hung-etal-2022-ds</bibkey>
      <pwccode url="https://github.com/umanlp/ds-tod" additional="false">umanlp/ds-tod</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ccnet">CCNet</pwcdataset>
    </paper>
    <paper id="73">
      <title>Distinguishing Non-natural from Natural Adversarial Samples for More Robust Pre-trained Language Model</title>
      <author><first>Jiayi</first><last>Wang</last></author>
      <author><first>Rongzhou</first><last>Bao</last></author>
      <author><first>Zhuosheng</first><last>Zhang</last></author>
      <author><first>Hai</first><last>Zhao</last></author>
      <pages>905-915</pages>
      <abstract>Recently, the problem of robustness of pre-trained language models (PrLMs) has received increasing research interest. Latest studies on adversarial attacks achieve high attack success rates against PrLMs, claiming that PrLMs are not robust. However, we find that the adversarial samples that PrLMs fail are mostly non-natural and do not appear in reality. We question the validity of the current evaluation of robustness of PrLMs based on these non-natural adversarial samples and propose an anomaly detector to evaluate the robustness of PrLMs with more natural adversarial samples. We also investigate two applications of the anomaly detector: (1) In data augmentation, we employ the anomaly detector to force generating augmented data that are distinguished as non-natural, which brings larger gains to the accuracy of PrLMs. (2) We apply the anomaly detector to a defense framework to enhance the robustness of PrLMs. It can be used to defend all types of attacks and achieves higher accuracy on both adversarial samples and compliant samples than other defense frameworks.</abstract>
      <url hash="4d370eed">2022.findings-acl.73</url>
      <bibkey>wang-etal-2022-distinguishing</bibkey>
      <pwccode url="https://github.com/lilynlp/distinguishing-non-natural" additional="false">lilynlp/distinguishing-non-natural</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="74">
      <title>Learning Adaptive Axis Attentions in Fine-tuning: Beyond Fixed Sparse Attention Patterns</title>
      <author><first>Zihan</first><last>Wang</last></author>
      <author><first>Jiuxiang</first><last>Gu</last></author>
      <author><first>Jason</first><last>Kuen</last></author>
      <author><first>Handong</first><last>Zhao</last></author>
      <author><first>Vlad</first><last>Morariu</last></author>
      <author><first>Ruiyi</first><last>Zhang</last></author>
      <author><first>Ani</first><last>Nenkova</last></author>
      <author><first>Tong</first><last>Sun</last></author>
      <author><first>Jingbo</first><last>Shang</last></author>
      <pages>916-925</pages>
      <abstract>We present a comprehensive study of sparse attention patterns in Transformer models. We first question the need for pre-training with sparse attention and present experiments showing that an efficient fine-tuning only approach yields a slightly worse but still competitive model. Then we compare the widely used local attention pattern and the less-well-studied global attention pattern, demonstrating that global patterns have several unique advantages. We also demonstrate that a flexible approach to attention, with different patterns across different layers of the model, is beneficial for some tasks. Drawing on this insight, we propose a novel Adaptive Axis Attention method, which learns—during fine-tuning—different attention patterns for each Transformer layer depending on the downstream task. Rather than choosing a fixed attention pattern, the adaptive axis attention method identifies important tokens—for each task and model layer—and focuses attention on those. It does not require pre-training to accommodate the sparse patterns and demonstrates competitive and sometimes better performance against fixed sparse attention patterns that require resource-intensive pre-training.</abstract>
      <url hash="bd5e2a54">2022.findings-acl.74</url>
      <bibkey>wang-etal-2022-learning</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/lra">LRA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
    </paper>
    <paper id="75">
      <title>Using Interactive Feedback to Improve the Accuracy and Explainability of Question Answering Systems Post-Deployment</title>
      <author><first>Zichao</first><last>Li</last></author>
      <author><first>Prakhar</first><last>Sharma</last></author>
      <author><first>Xing Han</first><last>Lu</last></author>
      <author><first>Jackie</first><last>Cheung</last></author>
      <author><first>Siva</first><last>Reddy</last></author>
      <pages>926-937</pages>
      <abstract>Most research on question answering focuses on the pre-deployment stage; i.e., building an accurate model for deployment.In this paper, we ask the question: Can we improve QA systems further post-deployment based on user interactions? We focus on two kinds of improvements: 1) improving the QA system’s performance itself, and 2) providing the model with the ability to explain the correctness or incorrectness of an answer.We collect a retrieval-based QA dataset, FeedbackQA, which contains interactive feedback from users. We collect this dataset by deploying a base QA system to crowdworkers who then engage with the system and provide feedback on the quality of its answers.The feedback contains both structured ratings and unstructured natural language explanations.We train a neural model with this feedback data that can generate explanations and re-score answer candidates. We show that feedback data not only improves the accuracy of the deployed QA system but also other stronger non-deployed systems. The generated explanations also help users make informed decisions about the correctness of answers.</abstract>
      <url hash="6ba1dacd">2022.findings-acl.75</url>
      <bibkey>li-etal-2022-using</bibkey>
    </paper>
    <paper id="76">
      <title>To be or not to be an Integer? Encoding Variables for Mathematical Text</title>
      <author><first>Deborah</first><last>Ferreira</last></author>
      <author><first>Mokanarangan</first><last>Thayaparan</last></author>
      <author><first>Marco</first><last>Valentino</last></author>
      <author><first>Julia</first><last>Rozanova</last></author>
      <author><first>Andre</first><last>Freitas</last></author>
      <pages>938-948</pages>
      <abstract>The application of Natural Language Inference (NLI) methods over large textual corpora can facilitate scientific discovery, reducing the gap between current research and the available large-scale scientific knowledge. However, contemporary NLI models are still limited in interpreting mathematical knowledge written in Natural Language, even though mathematics is an integral part of scientific argumentation for many disciplines. One of the fundamental requirements towards mathematical language understanding, is the creation of models able to meaningfully represent variables. This problem is particularly challenging since the meaning of a variable should be assigned exclusively from its defining type, i.e., the representation of a variable should come from its context. Recent research has formalised the variable typing task, a benchmark for the understanding of abstract mathematical types and variables in a sentence. In this work, we propose VarSlot, a Variable Slot-based approach, which not only delivers state-of-the-art results in the task of variable typing, but is also able to create context-based representations for variables.</abstract>
      <url hash="e1f13195">2022.findings-acl.76</url>
      <attachment type="software" hash="04b2843f">2022.findings-acl.76.software.zip</attachment>
      <bibkey>ferreira-etal-2022-integer</bibkey>
    </paper>
    <paper id="77">
      <title><fixed-case>GRS</fixed-case>: Combining Generation and Revision in Unsupervised Sentence Simplification</title>
      <author><first>Mohammad</first><last>Dehghan</last></author>
      <author><first>Dhruv</first><last>Kumar</last></author>
      <author><first>Lukasz</first><last>Golab</last></author>
      <pages>949-960</pages>
      <abstract>We propose GRS: an unsupervised approach to sentence simplification that combines text generation and text revision. We start with an iterative framework in which an input sentence is revised using explicit edit operations, and add paraphrasing as a new edit operation. This allows us to combine the advantages of generative and revision-based approaches: paraphrasing captures complex edit operations, and the use of explicit edit operations in an iterative manner provides controllability and interpretability. We demonstrate these advantages of GRS compared to existing methods on the Newsela and ASSET datasets.</abstract>
      <url hash="d8045c78">2022.findings-acl.77</url>
      <bibkey>dehghan-etal-2022-grs</bibkey>
      <pwccode url="https://github.com/imohammad12/grs" additional="false">imohammad12/grs</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/asset">ASSET</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsela">Newsela</pwcdataset>
    </paper>
    <paper id="78">
      <title><fixed-case>BPE</fixed-case> vs. Morphological Segmentation: A Case Study on Machine Translation of Four Polysynthetic Languages</title>
      <author><first>Manuel</first><last>Mager</last></author>
      <author><first>Arturo</first><last>Oncevay</last></author>
      <author><first>Elisabeth</first><last>Mager</last></author>
      <author><first>Katharina</first><last>Kann</last></author>
      <author><first>Thang</first><last>Vu</last></author>
      <pages>961-971</pages>
      <abstract>Morphologically-rich polysynthetic languages present a challenge for NLP systems due to data sparsity, and a common strategy to handle this issue is to apply subword segmentation. We investigate a wide variety of supervised and unsupervised morphological segmentation methods for four polysynthetic languages: Nahuatl, Raramuri, Shipibo-Konibo, and Wixarika. Then, we compare the morphologically inspired segmentation methods against Byte-Pair Encodings (BPEs) as inputs for machine translation (MT) when translating to and from Spanish. We show that for all language pairs except for Nahuatl, an unsupervised morphological segmentation algorithm outperforms BPEs consistently and that, although supervised methods achieve better segmentation scores, they under-perform in MT challenges. Finally, we contribute two new morphological segmentation datasets for Raramuri and Shipibo-Konibo, and a parallel corpus for Raramuri–Spanish.</abstract>
      <url hash="6cdee1b8">2022.findings-acl.78</url>
      <bibkey>mager-etal-2022-bpe</bibkey>
    </paper>
    <paper id="79">
      <title>Distributed <fixed-case>NLI</fixed-case>: Learning to Predict Human Opinion Distributions for Language Reasoning</title>
      <author><first>Xiang</first><last>Zhou</last></author>
      <author><first>Yixin</first><last>Nie</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>972-987</pages>
      <abstract>We introduce distributed NLI, a new NLU task with a goal to predict the distribution of human judgements for natural language inference. We show that by applying additional distribution estimation methods, namely, Monte Carlo (MC) Dropout, Deep Ensemble, Re-Calibration, and Distribution Distillation, models can capture human judgement distribution more effectively than the softmax baseline. We show that MC Dropout is able to achieve decent performance without any distribution annotations while Re-Calibration can give further improvements with extra distribution annotations, suggesting the value of multiple annotations for one example in modeling the distribution of human judgements. Despite these improvements, the best results are still far below the estimated human upper-bound, indicating that predicting the distribution of human judgements is still an open, challenging problem with a large room for improvements. We showcase the common errors for MC Dropout and Re-Calibration. Finally, we give guidelines on the usage of these methods with different levels of data availability and encourage future work on modeling the human opinion distribution for language reasoning.</abstract>
      <url hash="ed25a73b">2022.findings-acl.79</url>
      <attachment type="software" hash="5a4914c7">2022.findings-acl.79.software.zip</attachment>
      <bibkey>zhou-etal-2022-distributed</bibkey>
      <pwccode url="https://github.com/easonnie/ChaosNLI" additional="false">easonnie/ChaosNLI</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/chaosnli">ChaosNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="80">
      <title>Morphological Processing of Low-Resource Languages: Where We Are and What’s Next</title>
      <author><first>Adam</first><last>Wiemerslage</last></author>
      <author><first>Miikka</first><last>Silfverberg</last></author>
      <author><first>Changbing</first><last>Yang</last></author>
      <author><first>Arya</first><last>McCarthy</last></author>
      <author><first>Garrett</first><last>Nicolai</last></author>
      <author><first>Eliana</first><last>Colunga</last></author>
      <author><first>Katharina</first><last>Kann</last></author>
      <pages>988-1007</pages>
      <abstract>Automatic morphological processing can aid downstream natural language processing applications, especially for low-resource languages, and assist language documentation efforts for endangered languages. Having long been multilingual, the field of computational morphology is increasingly moving towards approaches suitable for languages with minimal or no annotated resources. First, we survey recent developments in computational morphology with a focus on low-resource languages. Second, we argue that the field is ready to tackle the logical next challenge: understanding a language’s morphology from raw text alone. We perform an empirical study on a truly unsupervised version of the paradigm completion task and show that, while existing state-of-the-art models bridged by two newly proposed models we devise perform reasonably, there is still much room for improvement. The stakes are high: solving this task will increase the language coverage of morphological resources by a number of magnitudes.</abstract>
      <url hash="1d7407bb">2022.findings-acl.80</url>
      <bibkey>wiemerslage-etal-2022-morphological</bibkey>
    </paper>
    <paper id="81">
      <title>Learning and Evaluating Character Representations in Novels</title>
      <author><first>Naoya</first><last>Inoue</last></author>
      <author><first>Charuta</first><last>Pethe</last></author>
      <author><first>Allen</first><last>Kim</last></author>
      <author><first>Steven</first><last>Skiena</last></author>
      <pages>1008-1019</pages>
      <abstract>We address the problem of learning fixed-length vector representations of characters in novels. Recent advances in word embeddings have proven successful in learning entity representations from short texts, but fall short on longer documents because they do not capture full book-level information. To overcome the weakness of such text-based embeddings, we propose two novel methods for representing characters: (i) graph neural network-based embeddings from a full corpus-based character network; and (ii) low-dimensional embeddings constructed from the occurrence pattern of characters in each novel. We test the quality of these character embeddings using a new benchmark suite to evaluate character representations, encompassing 12 different tasks. We show that our representation techniques combined with text-based embeddings lead to the best character representations, outperforming text-based embeddings in four tasks. Our dataset and evaluation script will be made publicly available to stimulate additional work in this area.</abstract>
      <url hash="7c807cab">2022.findings-acl.81</url>
      <bibkey>inoue-etal-2022-learning</bibkey>
      <pwccode url="https://github.com/naoya-i/charembench" additional="false">naoya-i/charembench</pwccode>
    </paper>
    <paper id="82">
      <title>Answer Uncertainty and Unanswerability in Multiple-Choice Machine Reading Comprehension</title>
      <author><first>Vatsal</first><last>Raina</last></author>
      <author><first>Mark</first><last>Gales</last></author>
      <pages>1020-1034</pages>
      <abstract>Machine reading comprehension (MRC) has drawn a lot of attention as an approach for assessing the ability of systems to understand natural language. Usually systems focus on selecting the correct answer to a question given a contextual paragraph. However, for many applications of multiple-choice MRC systems there are two additional considerations. For multiple-choice exams there is often a negative marking scheme; there is a penalty for an incorrect answer. In terms of an MRC system this means that the system is required to have an idea of the uncertainty in the predicted answer. The second consideration is that many multiple-choice questions have the option of none-of-the-above (NOA) indicating that none of the answers is applicable, rather than there always being the correct answer in the list of choices. This paper investigates both of these issues by making use of predictive uncertainty. Whether the system should propose an answer is a direct application of answer uncertainty. There are two possibilities when considering the NOA option. The simplest is to explicitly build a system on data that includes this option. Alternatively uncertainty can be applied to detect whether the other options include the correct answer. If the system is not sufficiently confident it will select NOA. As there is no standard corpus available to investigate these topics, the ReClor corpus is modified by removing the correct answer from a subset of possible answers. A high-performance MRC system is used to evaluate whether answer uncertainty can be applied in these situations. It is shown that uncertainty does allow questions that the system is not confident about to be detected. Additionally it is shown that uncertainty outperforms a system explicitly built with an NOA option.</abstract>
      <url hash="b86f8bff">2022.findings-acl.82</url>
      <bibkey>raina-gales-2022-answer</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/reclor">ReClor</pwcdataset>
    </paper>
    <paper id="83">
      <title>Measuring the Language of Self-Disclosure across Corpora</title>
      <author><first>Ann-Katrin</first><last>Reuel</last></author>
      <author><first>Sebastian</first><last>Peralta</last></author>
      <author><first>João</first><last>Sedoc</last></author>
      <author><first>Garrick</first><last>Sherman</last></author>
      <author><first>Lyle</first><last>Ungar</last></author>
      <pages>1035-1047</pages>
      <abstract>Being able to reliably estimate self-disclosure – a key component of friendship and intimacy – from language is important for many psychology studies. We build single-task models on five self-disclosure corpora, but find that these models generalize poorly; the within-domain accuracy of predicted message-level self-disclosure of the best-performing model (mean Pearson’s r=0.69) is much higher than the respective across data set accuracy (mean Pearson’s r=0.32), due to both variations in the corpora (e.g., medical vs. general topics) and labeling instructions (target variables: self-disclosure, emotional disclosure, intimacy). However, some lexical features, such as expression of negative emotions and use of first person personal pronouns such as ‘I’ reliably predict self-disclosure across corpora. We develop a multi-task model that yields better results, with an average Pearson’s r of 0.37 for out-of-corpora prediction.</abstract>
      <url hash="0acc04dc">2022.findings-acl.83</url>
      <bibkey>reuel-etal-2022-measuring</bibkey>
    </paper>
    <paper id="84">
      <title>When Chosen Wisely, More Data Is What You Need: A Universal Sample-Efficient Strategy For Data Augmentation</title>
      <author><first>Ehsan</first><last>Kamalloo</last></author>
      <author><first>Mehdi</first><last>Rezagholizadeh</last></author>
      <author><first>Ali</first><last>Ghodsi</last></author>
      <pages>1048-1062</pages>
      <abstract>Data Augmentation (DA) is known to improve the generalizability of deep neural networks. Most existing DA techniques naively add a certain number of augmented samples without considering the quality and the added computational cost of these samples. To tackle this problem, a common strategy, adopted by several state-of-the-art DA methods, is to adaptively generate or re-weight augmented samples with respect to the task objective during training. However, these adaptive DA methods: (1) are computationally expensive and not sample-efficient, and (2) are designed merely for a specific setting. In this work, we present a universal DA technique, called Glitter, to overcome both issues. Glitter can be plugged into any DA method, making training sample-efficient without sacrificing performance. From a pre-generated pool of augmented samples, Glitter adaptively selects a subset of worst-case samples with maximal loss, analogous to adversarial DA. Without altering the training strategy, the task objective can be optimized on the selected subset. Our thorough experiments on the GLUE benchmark, SQuAD, and HellaSwag in three widely used training setups including consistency training, self-distillation and knowledge distillation reveal that Glitter is substantially faster to train and achieves a competitive performance, compared to strong baselines.</abstract>
      <url hash="6bded542">2022.findings-acl.84</url>
      <bibkey>kamalloo-etal-2022-chosen</bibkey>
      <pwccode url="https://github.com/huawei-noah/kd-nlp" additional="false">huawei-noah/kd-nlp</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/anli">ANLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hellaswag">HellaSwag</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sick">SICK</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="85">
      <title>Explaining Classes through Stable Word Attributions</title>
      <author><first>Samuel</first><last>Rönnqvist</last></author>
      <author><first>Aki-Juhani</first><last>Kyröläinen</last></author>
      <author><first>Amanda</first><last>Myntti</last></author>
      <author><first>Filip</first><last>Ginter</last></author>
      <author><first>Veronika</first><last>Laippala</last></author>
      <pages>1063-1074</pages>
      <abstract>Input saliency methods have recently become a popular tool for explaining predictions of deep learning models in NLP. Nevertheless, there has been little work investigating methods for aggregating prediction-level explanations to the class level, nor has a framework for evaluating such class explanations been established. We explore explanations based on XLM-R and the Integrated Gradients input attribution method, and propose 1) the Stable Attribution Class Explanation method (SACX) to extract keyword lists of classes in text classification tasks, and 2) a framework for the systematic evaluation of the keyword lists. We find that explanations of individual predictions are prone to noise, but that stable explanations can be effectively identified through repeated training and explanation. We evaluate on web register data and show that the class explanations are linguistically meaningful and distinguishing of the classes.</abstract>
      <url hash="244dcb03">2022.findings-acl.85</url>
      <attachment type="software" hash="69b60bdf">2022.findings-acl.85.software.tgz</attachment>
      <bibkey>ronnqvist-etal-2022-explaining</bibkey>
      <pwccode url="https://github.com/turkunlp/class-explainer" additional="false">turkunlp/class-explainer</pwccode>
    </paper>
    <paper id="86">
      <title>What to Learn, and How: <fixed-case>T</fixed-case>oward Effective Learning from Rationales</title>
      <author><first>Samuel</first><last>Carton</last></author>
      <author><first>Surya</first><last>Kanoria</last></author>
      <author><first>Chenhao</first><last>Tan</last></author>
      <pages>1075-1088</pages>
      <abstract>Learning from rationales seeks to augment model prediction accuracy using human-annotated rationales (i.e. subsets of input tokens) that justify their chosen labels, often in the form of intermediate or multitask supervision. While intuitive, this idea has proven elusive in practice. We make two observations about human rationales via empirical analyses:1) maximizing rationale supervision accuracy is not necessarily the optimal objective for improving model accuracy; 2) human rationales vary in whether they provide sufficient information for the model to exploit for prediction.Building on these insights, we propose several novel loss functions and learning strategies, and evaluate their effectiveness on three datasets with human rationales. Our results demonstrate consistent improvements over baselines in both label and rationale accuracy, including a 3% accuracy improvement on MultiRC. Our work highlights the importance of understanding properties of human explanations and exploiting them accordingly in model training.</abstract>
      <url hash="9ba00c79">2022.findings-acl.86</url>
      <bibkey>carton-etal-2022-learn</bibkey>
      <pwccode url="https://github.com/chicagohai/learning-from-rationales" additional="false">chicagohai/learning-from-rationales</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multirc">MultiRC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/e-snli">e-SNLI</pwcdataset>
    </paper>
    <paper id="87">
      <title>Listening to Affected Communities to Define Extreme Speech: Dataset and Experiments</title>
      <author><first>Antonis</first><last>Maronikolakis</last></author>
      <author><first>Axel</first><last>Wisiorek</last></author>
      <author><first>Leah</first><last>Nann</last></author>
      <author><first>Haris</first><last>Jabbar</last></author>
      <author><first>Sahana</first><last>Udupa</last></author>
      <author><first>Hinrich</first><last>Schuetze</last></author>
      <pages>1089-1104</pages>
      <abstract>Building on current work on multilingual hate speech (e.g., Ousidhoum et al. (2019)) and hate speech reduction (e.g., Sap et al. (2020)), we present XTREMESPEECH, a new hate speech dataset containing 20,297 social media passages from Brazil, Germany, India and Kenya. The key novelty is that we directly involve the affected communities in collecting and annotating the data – as opposed to giving companies and governments control over defining and combatting hate speech. This inclusive approach results in datasets more representative of actually occurring online speech and is likely to facilitate the removal of the social media content that marginalized communities view as causing the most harm. Based on XTREMESPEECH, we establish novel tasks with accompanying baselines, provide evidence that cross-country training is generally not feasible due to cultural differences between countries and perform an interpretability analysis of BERT’s predictions.</abstract>
      <url hash="e8cf54ea">2022.findings-acl.87</url>
      <bibkey>maronikolakis-etal-2022-listening</bibkey>
      <pwccode url="https://github.com/antmarakis/xtremespeech" additional="false">antmarakis/xtremespeech</pwccode>
    </paper>
    <paper id="88">
      <title>Entropy-based Attention Regularization Frees Unintended Bias Mitigation from Lists</title>
      <author><first>Giuseppe</first><last>Attanasio</last></author>
      <author><first>Debora</first><last>Nozza</last></author>
      <author><first>Dirk</first><last>Hovy</last></author>
      <author><first>Elena</first><last>Baralis</last></author>
      <pages>1105-1119</pages>
      <abstract>Natural Language Processing (NLP) models risk overfitting to specific terms in the training data, thereby reducing their performance, fairness, and generalizability. E.g., neural hate speech detection models are strongly influenced by identity terms like gay, or women, resulting in false positives, severe unintended bias, and lower performance.Most mitigation techniques use lists of identity terms or samples from the target domain during training. However, this approach requires a-priori knowledge and introduces further bias if important terms are neglected.Instead, we propose a knowledge-free Entropy-based Attention Regularization (EAR) to discourage overfitting to training-specific terms. An additional objective function penalizes tokens with low self-attention entropy.We fine-tune BERT via EAR: the resulting model matches or exceeds state-of-the-art performance for hate speech classification and bias metrics on three benchmark corpora in English and Italian.EAR also reveals overfitting terms, i.e., terms most likely to induce bias, to help identify their effect on the model, task, and predictions.</abstract>
      <url hash="9b36ea6e">2022.findings-acl.88</url>
      <attachment type="software" hash="527aa339">2022.findings-acl.88.software.zip</attachment>
      <bibkey>attanasio-etal-2022-entropy</bibkey>
      <pwccode url="https://github.com/g8a9/ear" additional="false">g8a9/ear</pwccode>
    </paper>
    <paper id="89">
      <title>From <fixed-case>BERT</fixed-case>‘s <fixed-case>P</fixed-case>oint of <fixed-case>V</fixed-case>iew: <fixed-case>R</fixed-case>evealing the <fixed-case>P</fixed-case>revailing <fixed-case>C</fixed-case>ontextual <fixed-case>D</fixed-case>ifferences</title>
      <author><first>Carolin</first><last>Schuster</last></author>
      <author><first>Simon</first><last>Hegelich</last></author>
      <pages>1120-1138</pages>
      <abstract>Though successfully applied in research and industry large pretrained language models of the BERT family are not yet fully understood. While much research in the field of BERTology has tested whether specific knowledge can be extracted from layer activations, we invert the popular probing design to analyze the prevailing differences and clusters in BERT’s high dimensional space. By extracting coarse features from masked token representations and predicting them by probing models with access to only partial information we can apprehend the variation from ‘BERT’s point of view’. By applying our new methodology to different datasets we show how much the differences can be described by syntax but further how they are to a great extent shaped by the most simple positional information.</abstract>
      <url hash="f82c8085">2022.findings-acl.89</url>
      <attachment type="software" hash="bf9e3053">2022.findings-acl.89.software.zip</attachment>
      <bibkey>schuster-hegelich-2022-berts</bibkey>
    </paper>
    <paper id="90">
      <title>Learning Bias-reduced Word Embeddings Using Dictionary Definitions</title>
      <author><first>Haozhe</first><last>An</last></author>
      <author><first>Xiaojiang</first><last>Liu</last></author>
      <author><first>Donald</first><last>Zhang</last></author>
      <pages>1139-1152</pages>
      <abstract>Pre-trained word embeddings, such as GloVe, have shown undesirable gender, racial, and religious biases. To address this problem, we propose DD-GloVe, a train-time debiasing algorithm to learn word embeddings by leveraging <tex-math>\underline{d}</tex-math>ictionary <tex-math>\underline{d}</tex-math>efinitions. We introduce dictionary-guided loss functions that encourage word embeddings to be similar to their relatively neutral dictionary definition representations. Existing debiasing algorithms typically need a pre-compiled list of seed words to represent the bias direction, along which biased information gets removed. Producing this list involves subjective decisions and it might be difficult to obtain for some types of biases. We automate the process of finding seed words: our algorithm starts from a single pair of initial seed words and automatically finds more words whose definitions display similar attributes traits. We demonstrate the effectiveness of our approach with benchmark evaluations and empirical analyses. Our code is available at https://github.com/haozhe-an/DD-GloVe. </abstract>
      <url hash="71949aa4">2022.findings-acl.90</url>
      <bibkey>an-etal-2022-learning</bibkey>
      <pwccode url="https://github.com/haozhe-an/dd-glove" additional="false">haozhe-an/dd-glove</pwccode>
    </paper>
    <paper id="91">
      <title>Knowledge Graph Embedding by Adaptive Limit Scoring Loss Using Dynamic Weighting Strategy</title>
      <author><first>Jinfa</first><last>Yang</last></author>
      <author><first>Xianghua</first><last>Ying</last></author>
      <author><first>Yongjie</first><last>Shi</last></author>
      <author><first>Xin</first><last>Tong</last></author>
      <author><first>Ruibin</first><last>Wang</last></author>
      <author><first>Taiyan</first><last>Chen</last></author>
      <author><first>Bowei</first><last>Xing</last></author>
      <pages>1153-1163</pages>
      <abstract>Knowledge graph embedding aims to represent entities and relations as low-dimensional vectors, which is an effective way for predicting missing links in knowledge graphs. Designing a strong and effective loss framework is essential for knowledge graph embedding models to distinguish between correct and incorrect triplets. The classic margin-based ranking loss limits the scores of positive and negative triplets to have a suitable margin. The recently proposed Limit-based Scoring Loss independently limits the range of positive and negative triplet scores. However, these loss frameworks use equal or fixed penalty terms to reduce the scores of positive and negative sample pairs, which is inflexible in optimization. Our intuition is that if a triplet score deviates far from the optimum, it should be emphasized. To this end, we propose Adaptive Limit Scoring Loss, which simply re-weights each triplet to highlight the less-optimized triplet scores. We apply this loss framework to several knowledge graph embedding models such as TransE, TransH and ComplEx. The experimental results on link prediction and triplet classification show that our proposed method has achieved performance on par with the state of the art.</abstract>
      <url hash="6f749bd6">2022.findings-acl.91</url>
      <bibkey>yang-etal-2022-knowledge</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/fb15k-237">FB15k-237</pwcdataset>
    </paper>
    <paper id="92">
      <title><fixed-case>OCR</fixed-case> Improves Machine Translation for Low-Resource Languages</title>
      <author><first>Oana</first><last>Ignat</last></author>
      <author><first>Jean</first><last>Maillard</last></author>
      <author><first>Vishrav</first><last>Chaudhary</last></author>
      <author><first>Francisco</first><last>Guzmán</last></author>
      <pages>1164-1174</pages>
      <abstract>We aim to investigate the performance of current OCR systems on low resource languages and low resource scripts.We introduce and make publicly available a novel benchmark, OCR4MT, consisting of real and synthetic data, enriched with noise, for 60 low-resource languages in low resource scripts. We evaluate state-of-the-art OCR systems on our benchmark and analyse most common errors. We show that OCR monolingual data is a valuable resource that can increase performance of Machine Translation models, when used in backtranslation. We then perform an ablation study to investigate how OCR errors impact Machine Translation performance and determine what is the minimum level of OCR quality needed for the monolingual data to be useful for Machine Translation.</abstract>
      <url hash="8d03ec3c">2022.findings-acl.92</url>
      <bibkey>ignat-etal-2022-ocr</bibkey>
    </paper>
    <paper id="93">
      <title><fixed-case>C</fixed-case>o<fixed-case>C</fixed-case>o<fixed-case>LM</fixed-case>: Complex Commonsense Enhanced Language Model with Discourse Relations</title>
      <author><first>Changlong</first><last>Yu</last></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Yangqiu</first><last>Song</last></author>
      <author><first>Wilfred</first><last>Ng</last></author>
      <pages>1175-1187</pages>
      <abstract>Large-scale pre-trained language models have demonstrated strong knowledge representation ability. However, recent studies suggest that even though these giant models contain rich simple commonsense knowledge (e.g., bird can fly and fish can swim.), they often struggle with complex commonsense knowledge that involves multiple eventualities (verb-centric phrases, e.g., identifying the relationship between “Jim yells at Bob” and “Bob is upset”). To address this issue, in this paper, we propose to help pre-trained language models better incorporate complex commonsense knowledge. Unlike direct fine-tuning approaches, we do not focus on a specific task and instead propose a general language model named CoCoLM. Through the careful training over a large-scale eventuality knowledge graph ASER, we successfully teach pre-trained language models (i.e., BERT and RoBERTa) rich multi-hop commonsense knowledge among eventualities.Experiments on multiple commonsense tasks that require the correct understanding of eventualities demonstrate the effectiveness of CoCoLM.</abstract>
      <url hash="3a8e6fdf">2022.findings-acl.93</url>
      <bibkey>yu-etal-2022-cocolm</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/copa">COPA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/lama">LAMA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/rocstories">ROCStories</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="94">
      <title>Learning to Robustly Aggregate Labeling Functions for Semi-supervised Data Programming</title>
      <author><first>Ayush</first><last>Maheshwari</last></author>
      <author><first>Krishnateja</first><last>Killamsetty</last></author>
      <author><first>Ganesh</first><last>Ramakrishnan</last></author>
      <author><first>Rishabh</first><last>Iyer</last></author>
      <author><first>Marina</first><last>Danilevsky</last></author>
      <author><first>Lucian</first><last>Popa</last></author>
      <pages>1188-1202</pages>
      <abstract>A critical bottleneck in supervised machine learning is the need for large amounts of labeled data which is expensive and time-consuming to obtain. Although a small amount of labeled data cannot be used to train a model, it can be used effectively for the generation of humaninterpretable labeling functions (LFs). These LFs, in turn, have been used to generate a large amount of additional noisy labeled data in a paradigm that is now commonly referred to as data programming. Previous methods of generating LFs do not attempt to use the given labeled data further to train a model, thus missing opportunities for improving performance. Additionally, since the LFs are generated automatically, they are likely to be noisy, and naively aggregating these LFs can lead to suboptimal results. In this work, we propose an LF-based bi-level optimization framework WISDOM to solve these two critical limitations. WISDOM learns a joint model on the (same) labeled dataset used for LF induction along with any unlabeled data in a semi-supervised manner, and more critically, reweighs each LF according to its goodness, influencing its contribution to the semi-supervised loss using a robust bi-level optimization algorithm. We show that WISDOM significantly outperforms prior approaches on several text classification datasets.</abstract>
      <url hash="d59c3118">2022.findings-acl.94</url>
      <attachment type="software" hash="064c9fcf">2022.findings-acl.94.software.zip</attachment>
      <bibkey>maheshwari-etal-2022-learning</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="95">
      <title>Multi-Granularity Semantic Aware Graph Model for Reducing Position Bias in Emotion Cause Pair Extraction</title>
      <author><first>Yinan</first><last>Bao</last></author>
      <author><first>Qianwen</first><last>Ma</last></author>
      <author><first>Lingwei</first><last>Wei</last></author>
      <author><first>Wei</first><last>Zhou</last></author>
      <author><first>Songlin</first><last>Hu</last></author>
      <pages>1203-1213</pages>
      <abstract>The emotion cause pair extraction (ECPE) task aims to extract emotions and causes as pairs from documents. We observe that the relative distance distribution of emotions and causes is extremely imbalanced in the typical ECPE dataset. Existing methods have set a fixed size window to capture relations between neighboring clauses. However, they neglect the effective semantic connections between distant clauses, leading to poor generalization ability towards position-insensitive data. To alleviate the problem, we propose a novel <tex-math>\textbf{M}</tex-math>ulti-<tex-math>\textbf{G}</tex-math>ranularity <tex-math>\textbf{S}</tex-math>emantic <tex-math>\textbf{A}</tex-math>ware <tex-math>\textbf{G}</tex-math>raph model (MGSAG) to incorporate fine-grained and coarse-grained semantic features jointly, without regard to distance limitation. In particular, we first explore semantic dependencies between clauses and keywords extracted from the document that convey fine-grained semantic features, obtaining keywords enhanced clause representations. Besides, a clause graph is also established to model coarse-grained semantic relations between clauses. Experimental results indicate that MGSAG surpasses the existing state-of-the-art ECPE models. Especially, MGSAG outperforms other models significantly in the condition of position-insensitive data.</abstract>
      <url hash="c858ac28">2022.findings-acl.95</url>
      <bibkey>bao-etal-2022-multi</bibkey>
    </paper>
    <paper id="96">
      <title>Cross-lingual Inference with A <fixed-case>C</fixed-case>hinese Entailment Graph</title>
      <author><first>Tianyi</first><last>Li</last></author>
      <author><first>Sabine</first><last>Weber</last></author>
      <author><first>Mohammad Javad</first><last>Hosseini</last></author>
      <author><first>Liane</first><last>Guillou</last></author>
      <author><first>Mark</first><last>Steedman</last></author>
      <pages>1214-1233</pages>
      <abstract>Predicate entailment detection is a crucial task for question-answering from text, where previous work has explored unsupervised learning of entailment graphs from typed open relation triples. In this paper, we present the first pipeline for building Chinese entailment graphs, which involves a novel high-recall open relation extraction (ORE) method and the first Chinese fine-grained entity typing dataset under the FIGER type ontology. Through experiments on the Levy-Holt dataset, we verify the strength of our Chinese entailment graph, and reveal the cross-lingual complementarity: on the parallel Levy-Holt dataset, an ensemble of Chinese and English entailment graphs outperforms both monolingual graphs, and raises unsupervised SOTA by 4.7 AUC points.</abstract>
      <url hash="65f04b84">2022.findings-acl.96</url>
      <attachment type="software" hash="b379bced">2022.findings-acl.96.software.zip</attachment>
      <bibkey>li-etal-2022-cross</bibkey>
      <pwccode url="https://github.com/teddy-li/chineseentgraph" additional="false">teddy-li/chineseentgraph</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/clue">CLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/figer">FIGER</pwcdataset>
    </paper>
    <paper id="97">
      <title>Multi-task Learning for Paraphrase Generation With Keyword and Part-of-Speech Reconstruction</title>
      <author><first>Xuhang</first><last>Xie</last></author>
      <author><first>Xuesong</first><last>Lu</last></author>
      <author><first>Bei</first><last>Chen</last></author>
      <pages>1234-1243</pages>
      <abstract>Paraphrase generation using deep learning has been a research hotspot of natural language processing in the past few years. While previous studies tackle the problem from different aspects, the essence of paraphrase generation is to retain the key semantics of the source sentence and rewrite the rest of the content. Inspired by this observation, we propose a novel two-stage model, PGKPR, for paraphrase generation with keyword and part-of-speech reconstruction. The rationale is to capture simultaneously the possible keywords of a source sentence and the relations between them to facilitate the rewriting. In the first stage, we identify the possible keywords using a prediction attribution technique, where the words obtaining higher attribution scores are more likely to be the keywords. In the second stage, we train a transformer-based model via multi-task learning for paraphrase generation. The novel learning task is the reconstruction of the keywords and part-of-speech tags, respectively, from a perturbed sequence of the source sentence. The learned encodings are then decoded to generate the paraphrase. We conduct the experiments on two commonly-used datasets, and demonstrate the superior performance of PGKPR over comparative models on multiple evaluation metrics.</abstract>
      <url hash="d2d5d95b">2022.findings-acl.97</url>
      <attachment type="software" hash="6e514ed0">2022.findings-acl.97.software.zip</attachment>
      <bibkey>xie-etal-2022-multi</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
    </paper>
    <paper id="98">
      <title><fixed-case>MDCS</fixed-case>pell: A Multi-task Detector-Corrector Framework for <fixed-case>C</fixed-case>hinese Spelling Correction</title>
      <author><first>Chenxi</first><last>Zhu</last></author>
      <author><first>Ziqiang</first><last>Ying</last></author>
      <author><first>Boyu</first><last>Zhang</last></author>
      <author><first>Feng</first><last>Mao</last></author>
      <pages>1244-1253</pages>
      <abstract>Chinese Spelling Correction (CSC) is a task to detect and correct misspelled characters in Chinese texts. CSC is challenging since many Chinese characters are visually or phonologically similar but with quite different semantic meanings. Many recent works use BERT-based language models to directly correct each character of the input sentence. However, these methods can be sub-optimal since they correct every character of the sentence only by the context which is easily negatively affected by the misspelled characters. Some other works propose to use an error detector to guide the correction by masking the detected errors. Nevertheless, these methods dampen the visual or phonological features from the misspelled characters which could be critical for correction. In this work, we propose a novel general detector-corrector multi-task framework where the corrector uses BERT to capture the visual and phonological features from each character in the raw sentence and uses a late fusion strategy to fuse the hidden states of the corrector with that of the detector to minimize the negative impact from the misspelled characters. Comprehensive experiments on benchmarks demonstrate that our proposed method can significantly outperform the state-of-the-art methods in the CSC task.</abstract>
      <url hash="c16669e5">2022.findings-acl.98</url>
      <bibkey>zhu-etal-2022-mdcspell</bibkey>
    </paper>
    <paper id="99">
      <title><fixed-case>S</fixed-case><tex-math>^2</tex-math><fixed-case>SQL</fixed-case>: Injecting Syntax to Question-Schema Interaction Graph Encoder for Text-to-<fixed-case>SQL</fixed-case> Parsers</title>
      <author><first>Binyuan</first><last>Hui</last></author>
      <author><first>Ruiying</first><last>Geng</last></author>
      <author><first>Lihan</first><last>Wang</last></author>
      <author><first>Bowen</first><last>Qin</last></author>
      <author><first>Yanyang</first><last>Li</last></author>
      <author><first>Bowen</first><last>Li</last></author>
      <author><first>Jian</first><last>Sun</last></author>
      <author><first>Yongbin</first><last>Li</last></author>
      <pages>1254-1262</pages>
      <abstract>The task of converting a natural language question into an executable SQL query, known as text-to-SQL, is an important branch of semantic parsing. The state-of-the-art graph-based encoder has been successfully used in this task but does not model the question syntax well. In this paper, we propose S<tex-math>^2</tex-math>SQL, injecting Syntax to question-Schema graph encoder for Text-to-SQL parsers, which effectively leverages the syntactic dependency information of questions in text-to-SQL to improve the performance. We also employ the decoupling constraint to induce diverse relational edge embedding, which further improves the network’s performance. Experiments on the Spider and robustness setting Spider-Syn demonstrate that the proposed approach outperforms all existing methods when pre-training models are used, resulting in a performance ranks first on the Spider leaderboard.</abstract>
      <url hash="441c333e">2022.findings-acl.99</url>
      <attachment type="software" hash="a9f19b91">2022.findings-acl.99.software.zip</attachment>
      <bibkey>hui-etal-2022-s2sql</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/spider-1">SPIDER</pwcdataset>
    </paper>
    <paper id="100">
      <title>Constructing Open Cloze Tests Using Generation and Discrimination Capabilities of Transformers</title>
      <author><first>Mariano</first><last>Felice</last></author>
      <author><first>Shiva</first><last>Taslimipoor</last></author>
      <author><first>Paula</first><last>Buttery</last></author>
      <pages>1263-1273</pages>
      <abstract>This paper presents the first multi-objective transformer model for generating open cloze tests that exploits generation and discrimination capabilities to improve performance. Our model is further enhanced by tweaking its loss function and applying a post-processing re-ranking algorithm that improves overall test structure. Experiments using automatic and human evaluation show that our approach can achieve up to 82% accuracy according to experts, outperforming previous work and baselines. We also release a collection of high-quality open cloze tests along with sample system output and human annotations that can serve as a future benchmark.</abstract>
      <url hash="340484a3">2022.findings-acl.100</url>
      <bibkey>felice-etal-2022-constructing</bibkey>
    </paper>
    <paper id="101">
      <title><fixed-case>C</fixed-case>o-training an <fixed-case>U</fixed-case>nsupervised <fixed-case>C</fixed-case>onstituency <fixed-case>P</fixed-case>arser with <fixed-case>W</fixed-case>eak <fixed-case>S</fixed-case>upervision</title>
      <author><first>Nickil</first><last>Maveli</last></author>
      <author><first>Shay</first><last>Cohen</last></author>
      <pages>1274-1291</pages>
      <abstract>We introduce a method for unsupervised parsing that relies on bootstrapping classifiers to identify if a node dominates a specific span in a sentence. There are two types of classifiers, an inside classifier that acts on a span, and an outside classifier that acts on everything outside of a given span. Through self-training and co-training with the two classifiers, we show that the interplay between them helps improve the accuracy of both, and as a result, effectively parse. A seed bootstrapping technique prepares the data to train these classifiers. Our analyses further validate that such an approach in conjunction with weak supervision using prior branching knowledge of a known language (left/right-branching) and minimal heuristics injects strong inductive bias into the parser, achieving 63.1 F<tex-math>_1</tex-math> on the English (PTB) test set. In addition, we show the effectiveness of our architecture by evaluating on treebanks for Chinese (CTB) and Japanese (KTB) and achieve new state-of-the-art results.</abstract>
      <url hash="6960059b">2022.findings-acl.101</url>
      <bibkey>maveli-cohen-2022-co</bibkey>
      <pwccode url="https://github.com/Nickil21/weakly-supervised-parsing" additional="false">Nickil21/weakly-supervised-parsing</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/chinese-treebank">Chinese Treebank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="102">
      <title><fixed-case>H</fixed-case>i<fixed-case>S</fixed-case>truct+: Improving Extractive Text Summarization with Hierarchical Structure Information</title>
      <author><first>Qian</first><last>Ruan</last></author>
      <author><first>Malte</first><last>Ostendorff</last></author>
      <author><first>Georg</first><last>Rehm</last></author>
      <pages>1292-1308</pages>
      <abstract>Transformer-based language models usually treat texts as linear sequences. However, most texts also have an inherent hierarchical structure, i.e., parts of a text can be identified using their position in this hierarchy. In addition, section titles usually indicate the common topic of their respective sentences. We propose a novel approach to formulate, extract, encode and inject hierarchical structure information explicitly into an extractive summarization model based on a pre-trained, encoder-only Transformer language model (HiStruct+ model), which improves SOTA ROUGEs for extractive summarization on PubMed and arXiv substantially. Using various experimental settings on three datasets (i.e., CNN/DailyMail, PubMed and arXiv), our HiStruct+ model outperforms a strong baseline collectively, which differs from our model only in that the hierarchical structure information is not injected. It is also observed that the more conspicuous hierarchical structure the dataset has, the larger improvements our method gains. The ablation study demonstrates that the hierarchical position information is the main contributor to our model’s SOTA performance.</abstract>
      <url hash="35aee5d5">2022.findings-acl.102</url>
      <attachment type="software" hash="21541437">2022.findings-acl.102.software.zip</attachment>
      <bibkey>ruan-etal-2022-histruct</bibkey>
      <pwccode url="https://github.com/QianRuan/histruct" additional="false">QianRuan/histruct</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/pubmed">Pubmed</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/arxiv">arXiv</pwcdataset>
    </paper>
    <paper id="103">
      <title>An Isotropy Analysis in the Multilingual <fixed-case>BERT</fixed-case> Embedding Space</title>
      <author><first>Sara</first><last>Rajaee</last></author>
      <author><first>Mohammad Taher</first><last>Pilehvar</last></author>
      <pages>1309-1316</pages>
      <abstract>Several studies have explored various advantages of multilingual pre-trained models (such as multilingual BERT) in capturing shared linguistic knowledge. However, less attention has been paid to their limitations. In this paper, we investigate the multilingual BERT for two known issues of the monolingual models: anisotropic embedding space and outlier dimensions. We show that, unlike its monolingual counterpart, the multilingual BERT model exhibits no outlier dimension in its representations while it has a highly anisotropic space. There are a few dimensions in the monolingual BERT with high contributions to the anisotropic distribution. However, we observe no such dimensions in the multilingual BERT. Furthermore, our experimental results demonstrate that increasing the isotropy of multilingual space can significantly improve its representation power and performance, similarly to what had been observed for monolingual CWRs on semantic similarity tasks. Our analysis indicates that, despite having different degenerated directions, the embedding spaces in various languages tend to be partially similar with respect to their structures.</abstract>
      <url hash="4da36a67">2022.findings-acl.103</url>
      <bibkey>rajaee-pilehvar-2022-isotropy</bibkey>
      <pwccode url="https://github.com/sara-rajaee/multilingual-isotropy" additional="false">sara-rajaee/multilingual-isotropy</pwccode>
    </paper>
    <paper id="104">
      <title>Multi-Stage Prompting for Knowledgeable Dialogue Generation</title>
      <author><first>Zihan</first><last>Liu</last></author>
      <author><first>Mostofa</first><last>Patwary</last></author>
      <author><first>Ryan</first><last>Prenger</last></author>
      <author><first>Shrimai</first><last>Prabhumoye</last></author>
      <author><first>Wei</first><last>Ping</last></author>
      <author><first>Mohammad</first><last>Shoeybi</last></author>
      <author><first>Bryan</first><last>Catanzaro</last></author>
      <pages>1317-1337</pages>
      <abstract>Existing knowledge-grounded dialogue systems typically use finetuned versions of a pretrained language model (LM) and large-scale knowledge bases. These models typically fail to generalize on topics outside of the knowledge base, and require maintaining separate potentially large checkpoints each time finetuning is needed. In this paper, we aim to address these limitations by leveraging the inherent knowledge stored in the pretrained LM as well as its powerful generation ability. We propose a multi-stage prompting approach to generate knowledgeable responses from a single pretrained LM. We first prompt the LM to generate knowledge based on the dialogue context. Then, we further prompt it to generate responses based on the dialogue context and the previously generated knowledge. Results show that our knowledge generator outperforms the state-of-the-art retrieval-based model by 5.8% when combining knowledge relevance and correctness. In addition, our multi-stage prompting outperforms the finetuning-based dialogue model in terms of response knowledgeability and engagement by up to 10% and 5%, respectively. Furthermore, we scale our model up to 530 billion parameters and demonstrate that larger LMs improve the generation correctness score by up to 10%, and response relevance, knowledgeability and engagement by up to 10%. Our code is available at: https://github.com/NVIDIA/Megatron-LM.</abstract>
      <url hash="e9a1c9c8">2022.findings-acl.104</url>
      <bibkey>liu-etal-2022-multi</bibkey>
      <pwccode url="https://github.com/NVIDIA/Megatron-LM" additional="false">NVIDIA/Megatron-LM</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wizard-of-wikipedia">Wizard of Wikipedia</pwcdataset>
    </paper>
    <paper id="105">
      <title><tex-math>\textrm{DuReader}_{\textrm{vis}}</tex-math>: A <fixed-case>C</fixed-case>hinese Dataset for Open-domain Document Visual Question Answering</title>
      <author><first>Le</first><last>Qi</last></author>
      <author><first>Shangwen</first><last>Lv</last></author>
      <author><first>Hongyu</first><last>Li</last></author>
      <author><first>Jing</first><last>Liu</last></author>
      <author><first>Yu</first><last>Zhang</last></author>
      <author><first>Qiaoqiao</first><last>She</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <author><first>Haifeng</first><last>Wang</last></author>
      <author><first>Ting</first><last>Liu</last></author>
      <pages>1338-1351</pages>
      <abstract>Open-domain question answering has been used in a wide range of applications, such as web search and enterprise search, which usually takes clean texts extracted from various formats of documents (e.g., web pages, PDFs, or Word documents) as the information source. However, designing different text extraction approaches is time-consuming and not scalable. In order to reduce human cost and improve the scalability of QA systems, we propose and study an <tex-math>\textbf{Open-domain}</tex-math>
        <tex-math>\textbf{Doc}</tex-math>ument <tex-math>\textbf{V}</tex-math>isual <tex-math>\textbf{Q}</tex-math>uestion <tex-math>\textbf{A}</tex-math>nswering (Open-domain DocVQA) task, which requires answering questions based on a collection of document images directly instead of only document texts, utilizing layouts and visual features additionally. Towards this end, we introduce the first Chinese Open-domain DocVQA dataset called <tex-math>\textrm{DuReader}_{\textrm{vis}}</tex-math>, containing about 15K question-answering pairs and 158K document images from the Baidu search engine. There are three main challenges in <tex-math>\textrm{DuReader}_{\textrm{vis}}</tex-math>: (1) long document understanding, (2) noisy texts, and (3) multi-span answer extraction. The extensive experiments demonstrate that the dataset is challenging. Additionally, we propose a simple approach that incorporates the layout and visual features, and the experimental results show the effectiveness of the proposed approach. The dataset and code will be publicly available at https://github.com/baidu/DuReader/tree/master/DuReader-vis.</abstract>
      <url hash="efec9c15">2022.findings-acl.105</url>
      <bibkey>qi-etal-2022-dureadervis</bibkey>
      <pwccode url="https://github.com/baidu/DuReader" additional="false">baidu/DuReader</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/docvqa">DocVQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/infographicvqa">InfographicVQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visualmrc">VisualMRC</pwcdataset>
    </paper>
    <paper id="106">
      <title>Coloring the Blank Slate: Pre-training Imparts a Hierarchical Inductive Bias to Sequence-to-sequence Models</title>
      <author><first>Aaron</first><last>Mueller</last></author>
      <author><first>Robert</first><last>Frank</last></author>
      <author><first>Tal</first><last>Linzen</last></author>
      <author><first>Luheng</first><last>Wang</last></author>
      <author><first>Sebastian</first><last>Schuster</last></author>
      <pages>1352-1368</pages>
      <abstract>Relations between words are governed by hierarchical structure rather than linear ordering. Sequence-to-sequence (seq2seq) models, despite their success in downstream NLP applications, often fail to generalize in a hierarchy-sensitive manner when performing syntactic transformations—for example, transforming declarative sentences into questions. However, syntactic evaluations of seq2seq models have only observed models that were not pre-trained on natural language data before being trained to perform syntactic transformations, in spite of the fact that pre-training has been found to induce hierarchical linguistic generalizations in language models; in other words, the syntactic capabilities of seq2seq models may have been greatly understated. We address this gap using the pre-trained seq2seq models T5 and BART, as well as their multilingual variants mT5 and mBART. We evaluate whether they generalize hierarchically on two transformations in two languages: question formation and passivization in English and German. We find that pre-trained seq2seq models generalize hierarchically when performing syntactic transformations, whereas models trained from scratch on syntactic transformations do not. This result presents evidence for the learnability of hierarchical syntactic information from non-annotated natural language text while also demonstrating that seq2seq models are capable of syntactic generalization, though only after exposure to much more language data than human learners receive.</abstract>
      <url hash="9a53b9e5">2022.findings-acl.106</url>
      <bibkey>mueller-etal-2022-coloring</bibkey>
      <pwccode url="https://github.com/sebschu/multilingual-transformations" additional="false">sebschu/multilingual-transformations</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mc4">mC4</pwcdataset>
    </paper>
    <paper id="107">
      <title><fixed-case>C</fixed-case><tex-math>^3</tex-math><fixed-case>KG</fixed-case>: A <fixed-case>C</fixed-case>hinese Commonsense Conversation Knowledge Graph</title>
      <author><first>Dawei</first><last>Li</last></author>
      <author><first>Yanran</first><last>Li</last></author>
      <author><first>Jiayi</first><last>Zhang</last></author>
      <author><first>Ke</first><last>Li</last></author>
      <author><first>Chen</first><last>Wei</last></author>
      <author><first>Jianwei</first><last>Cui</last></author>
      <author><first>Bin</first><last>Wang</last></author>
      <pages>1369-1383</pages>
      <abstract>Existing commonsense knowledge bases often organize tuples in an isolated manner, which is deficient for commonsense conversational models to plan the next steps. To fill the gap, we curate a large-scale multi-turn human-written conversation corpus, and create the first Chinese commonsense conversation knowledge graph which incorporates both social commonsense knowledge and dialog flow information. To show the potential of our graph, we develop a graph-conversation matching approach, and benchmark two graph-grounded conversational tasks. All the resources in this work will be released to foster future research.</abstract>
      <url hash="e140b04a">2022.findings-acl.107</url>
      <bibkey>li-etal-2022-c3kg</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/atomic">ATOMIC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mod-1">MOD</pwcdataset>
    </paper>
    <paper id="108">
      <title>Graph Neural Networks for Multiparallel Word Alignment</title>
      <author><first>Ayyoob</first><last>Imani</last></author>
      <author><first>Lütfi Kerem</first><last>Senel</last></author>
      <author><first>Masoud</first><last>Jalili Sabet</last></author>
      <author><first>François</first><last>Yvon</last></author>
      <author><first>Hinrich</first><last>Schuetze</last></author>
      <pages>1384-1396</pages>
      <abstract>After a period of decrease, interest in word alignments is increasing again for their usefulness in domains such as typological research, cross-lingual annotation projection and machine translation. Generally, alignment algorithms only use bitext and do not make use of the fact that many parallel corpora are multiparallel. Here, we compute high-quality word alignments between multiple language pairs by considering all language pairs together. First, we create a multiparallel word alignment graph, joining all bilingual word alignment pairs in one graph. Next, we use graph neural networks (GNNs) to exploit the graph structure. Our GNN approach (i) utilizes information about the meaning, position and language of the input words, (ii) incorporates information from multiple parallel sentences, (iii) adds and removes edges from the initial alignments, and (iv) yields a prediction model that can generalize beyond the training sentences. We show that community detection algorithms can provide valuable information for multiparallel word alignment. Our method outperforms previous work on three word alignment datasets and on a downstream task.</abstract>
      <url hash="c8f2efd4">2022.findings-acl.108</url>
      <bibkey>imani-etal-2022-graph</bibkey>
    </paper>
    <paper id="109">
      <title>Sentiment Word Aware Multimodal Refinement for Multimodal Sentiment Analysis with <fixed-case>ASR</fixed-case> Errors</title>
      <author><first>Yang</first><last>Wu</last></author>
      <author><first>Yanyan</first><last>Zhao</last></author>
      <author><first>Hao</first><last>Yang</last></author>
      <author><first>Song</first><last>Chen</last></author>
      <author><first>Bing</first><last>Qin</last></author>
      <author><first>Xiaohuan</first><last>Cao</last></author>
      <author><first>Wenting</first><last>Zhao</last></author>
      <pages>1397-1406</pages>
      <abstract>Multimodal sentiment analysis has attracted increasing attention and lots of models have been proposed. However, the performance of the state-of-the-art models decreases sharply when they are deployed in the real world. We find that the main reason is that real-world applications can only access the text outputs by the automatic speech recognition (ASR) models, which may be with errors because of the limitation of model capacity. Through further analysis of the ASR outputs, we find that in some cases the sentiment words, the key sentiment elements in the textual modality, are recognized as other words, which makes the sentiment of the text change and hurts the performance of multimodal sentiment analysis models directly. To address this problem, we propose the sentiment word aware multimodal refinement model (SWRM), which can dynamically refine the erroneous sentiment words by leveraging multimodal sentiment clues. Specifically, we first use the sentiment word position detection module to obtain the most possible position of the sentiment word in the text and then utilize the multimodal sentiment word refinement module to dynamically refine the sentiment word embeddings. The refined embeddings are taken as the textual inputs of the multimodal feature fusion module to predict the sentiment labels. We conduct extensive experiments on the real-world datasets including MOSI-Speechbrain, MOSI-IBM, and MOSI-iFlytek and the results demonstrate the effectiveness of our model, which surpasses the current state-of-the-art models on three datasets. Furthermore, our approach can be adapted for other multimodal feature fusion models easily.</abstract>
      <url hash="c8c28ec6">2022.findings-acl.109</url>
      <bibkey>wu-etal-2022-sentiment</bibkey>
      <pwccode url="https://github.com/albertwy/SWRM" additional="false">albertwy/SWRM</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multimodal-opinionlevel-sentiment-intensity">Multimodal Opinionlevel Sentiment Intensity</pwcdataset>
    </paper>
    <paper id="110">
      <title>A Novel Framework Based on Medical Concept Driven Attention for Explainable Medical Code Prediction via External Knowledge</title>
      <author><first>Tao</first><last>Wang</last></author>
      <author><first>Linhai</first><last>Zhang</last></author>
      <author><first>Chenchen</first><last>Ye</last></author>
      <author><first>Junxi</first><last>Liu</last></author>
      <author><first>Deyu</first><last>Zhou</last></author>
      <pages>1407-1416</pages>
      <abstract>Medical code prediction from clinical notes aims at automatically associating medical codes with the clinical notes. Rare code problem, the medical codes with low occurrences, is prominent in medical code prediction. Recent studies employ deep neural networks and the external knowledge to tackle it. However, such approaches lack interpretability which is a vital issue in medical application. Moreover, due to the lengthy and noisy clinical notes, such approaches fail to achieve satisfactory results. Therefore, in this paper, we propose a novel framework based on medical concept driven attention to incorporate external knowledge for explainable medical code prediction. In specific, both the clinical notes and Wikipedia documents are aligned into topic space to extract medical concepts using topic modeling. Then, the medical concept-driven attention mechanism is applied to uncover the medical code related concepts which provide explanations for medical code prediction. Experimental results on the benchmark dataset show the superiority of the proposed framework over several state-of-the-art baselines.</abstract>
      <url hash="ccce9501">2022.findings-acl.110</url>
      <bibkey>wang-etal-2022-novel</bibkey>
    </paper>
    <paper id="111">
      <title>Effective Unsupervised Constrained Text Generation based on Perturbed Masking</title>
      <author><first>Yingwen</first><last>Fu</last></author>
      <author><first>Wenjie</first><last>Ou</last></author>
      <author><first>Zhou</first><last>Yu</last></author>
      <author><first>Yue</first><last>Lin</last></author>
      <pages>1417-1427</pages>
      <abstract>Unsupervised constrained text generation aims to generate text under a given set of constraints without any supervised data. Current state-of-the-art methods stochastically sample edit positions and actions, which may cause unnecessary search steps. In this paper, we propose PMCTG to improve effectiveness by searching for the best edit position and action in each step. Specifically, PMCTG extends perturbed masking technique to effectively search for the most incongruent token to edit. Then it introduces four multi-aspect scoring functions to select edit action to further reduce search difficulty. Since PMCTG does not require supervised data, it could be applied to different generation tasks. We show that under the unsupervised setting, PMCTG achieves new state-of-the-art results in two representative tasks, namely keywords- to-sentence generation and paraphrasing.</abstract>
      <url hash="540266d4">2022.findings-acl.111</url>
      <bibkey>fu-etal-2022-effective</bibkey>
    </paper>
    <paper id="112">
      <title>Combining (Second-Order) Graph-Based and Headed-Span-Based Projective Dependency Parsing</title>
      <author><first>Songlin</first><last>Yang</last></author>
      <author><first>Kewei</first><last>Tu</last></author>
      <pages>1428-1434</pages>
      <abstract>Graph-based methods, which decompose the score of a dependency tree into scores of dependency arcs, are popular in dependency parsing for decades. Recently, (CITATION) propose a headed-span-based method that decomposes the score of a dependency tree into scores of headed spans. They show improvement over first-order graph-based methods. However, their method does not score dependency arcs at all, and dependency arcs are implicitly induced by their cubic-time algorithm, which is possibly sub-optimal since modeling dependency arcs is intuitively useful. In this work, we aim to combine graph-based and headed-span-based methods, incorporating both arc scores and headed span scores into our model. First, we show a direct way to combine with <tex-math>O(n^4)</tex-math> parsing complexity. To decrease complexity, inspired by the classical head-splitting trick, we show two <tex-math>O(n^3)</tex-math> dynamic programming algorithms to combine first- and second-order graph-based and headed-span-based methods. Our experiments on PTB, CTB, and UD show that combining first-order graph-based and headed-span-based methods is effective. We also confirm the effectiveness of second-order graph-based parsing in the deep learning age, however, we observe marginal or no improvement when combining second-order graph-based and headed-span-based methods . </abstract>
      <url hash="89dac78d">2022.findings-acl.112</url>
      <bibkey>yang-tu-2022-combining</bibkey>
      <pwccode url="https://github.com/sustcsonglin/span-based-dependency-parsing" additional="false">sustcsonglin/span-based-dependency-parsing</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="113">
      <title>End-to-End Speech Translation for Code Switched Speech</title>
      <author><first>Orion</first><last>Weller</last></author>
      <author><first>Matthias</first><last>Sperber</last></author>
      <author><first>Telmo</first><last>Pires</last></author>
      <author><first>Hendra</first><last>Setiawan</last></author>
      <author><first>Christian</first><last>Gollan</last></author>
      <author><first>Dominic</first><last>Telaar</last></author>
      <author><first>Matthias</first><last>Paulik</last></author>
      <pages>1435-1448</pages>
      <abstract>Code switching (CS) refers to the phenomenon of interchangeably using words and phrases from different languages. CS can pose significant accuracy challenges to NLP, due to the often monolingual nature of the underlying systems. In this work, we focus on CS in the context of English/Spanish conversations for the task of speech translation (ST), generating and evaluating both transcript and translation. To evaluate model performance on this task, we create a novel ST corpus derived from existing public data sets. We explore various ST architectures across two dimensions: cascaded (transcribe then translate) vs end-to-end (jointly transcribe and translate) and unidirectional (source -&gt; target) vs bidirectional (source &lt;-&gt; target). We show that our ST architectures, and especially our bidirectional end-to-end architecture, perform well on CS speech, even when no CS training data is used.</abstract>
      <url hash="630890be">2022.findings-acl.113</url>
      <bibkey>weller-etal-2022-end</bibkey>
      <pwccode url="https://github.com/apple/ml-code-switched-speech-translation" additional="false">apple/ml-code-switched-speech-translation</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/covost">CoVoST</pwcdataset>
    </paper>
    <paper id="114">
      <title>A Transformational Biencoder with In-Domain Negative Sampling for Zero-Shot Entity Linking</title>
      <author><first>Kai</first><last>Sun</last></author>
      <author><first>Richong</first><last>Zhang</last></author>
      <author><first>Samuel</first><last>Mensah</last></author>
      <author><first>Yongyi</first><last>Mao</last></author>
      <author><first>Xudong</first><last>Liu</last></author>
      <pages>1449-1458</pages>
      <abstract>Recent interest in entity linking has focused in the zero-shot scenario, where at test time the entity mention to be labelled is never seen during training, or may belong to a different domain from the source domain. Current work leverage pre-trained BERT with the implicit assumption that it bridges the gap between the source and target domain distributions. However, fine-tuned BERT has a considerable underperformance at zero-shot when applied in a different domain. We solve this problem by proposing a Transformational Biencoder that incorporates a transformation into BERT to perform a zero-shot transfer from the source domain during training. As like previous work, we rely on negative entities to encourage our model to discriminate the golden entities during training. To generate these negative entities, we propose a simple but effective strategy that takes the domain of the golden entity into perspective. Our experimental results on the benchmark dataset Zeshel show effectiveness of our approach and achieve new state-of-the-art.</abstract>
      <url hash="034dd78c">2022.findings-acl.114</url>
      <attachment type="software" hash="eecc7c72">2022.findings-acl.114.software.zip</attachment>
      <bibkey>sun-etal-2022-transformational</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/zeshel">ZESHEL</pwcdataset>
    </paper>
    <paper id="115">
      <title>Finding the Dominant Winning Ticket in Pre-Trained Language Models</title>
      <author><first>Zhuocheng</first><last>Gong</last></author>
      <author><first>Di</first><last>He</last></author>
      <author><first>Yelong</first><last>Shen</last></author>
      <author><first>Tie-Yan</first><last>Liu</last></author>
      <author><first>Weizhu</first><last>Chen</last></author>
      <author><first>Dongyan</first><last>Zhao</last></author>
      <author><first>Ji-Rong</first><last>Wen</last></author>
      <author><first>Rui</first><last>Yan</last></author>
      <pages>1459-1472</pages>
      <abstract>The Lottery Ticket Hypothesis suggests that for any over-parameterized model, a small subnetwork exists to achieve competitive performance compared to the backbone architecture. In this paper, we study whether there is a winning lottery ticket for pre-trained language models, which allow the practitioners to fine-tune the parameters in the ticket but achieve good downstream performance. To achieve this, we regularize the fine-tuning process with L1 distance and explore the subnetwork structure (what we refer to as the “dominant winning ticket”). Empirically, we show that (a) the dominant winning ticket can achieve performance that is comparable with that of the full-parameter model, (b) the dominant winning ticket is transferable across different tasks, (c) and the dominant winning ticket has a natural structure within each parameter matrix. Strikingly, we find that a dominant winning ticket that takes up 0.05% of the parameters can already achieve satisfactory performance, indicating that the PLM is significantly reducible during fine-tuning.</abstract>
      <url hash="dc13167d">2022.findings-acl.115</url>
      <bibkey>gong-etal-2022-finding</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
    </paper>
    <paper id="116">
      <title><fixed-case>T</fixed-case>hai Nested Named Entity Recognition Corpus</title>
      <author><first>Weerayut</first><last>Buaphet</last></author>
      <author><first>Can</first><last>Udomcharoenchaikit</last></author>
      <author><first>Peerat</first><last>Limkonchotiwat</last></author>
      <author><first>Attapol</first><last>Rutherford</last></author>
      <author><first>Sarana</first><last>Nutanong</last></author>
      <pages>1473-1486</pages>
      <abstract>This paper presents the first Thai Nested Named Entity Recognition (N-NER) dataset. Thai N-NER consists of 264,798 mentions, 104 classes, and a maximum depth of 8 layers obtained from 4,894 documents in the domains of news articles and restaurant reviews. Our work, to the best of our knowledge, presents the largest non-English N-NER dataset and the first non-English one with fine-grained classes. To understand the new challenges our proposed dataset brings to the field, we conduct an experimental study on (i) cutting edge N-NER models with the state-of-the-art accuracy in English and (ii) baseline methods based on well-known language model architectures. From the experimental results, we obtained two key findings. First, all models produced poor F1 scores in the tail region of the class distribution. There is little or no performance improvement provided by these models with respect to the baseline methods with our Thai dataset. These findings suggest that further investigation is required to make a multilingual N-NER solution that works well across different languages.</abstract>
      <url hash="7bfeb1fc">2022.findings-acl.116</url>
      <bibkey>buaphet-etal-2022-thai</bibkey>
      <pwccode url="https://github.com/vistec-ai/thai-nner" additional="false">vistec-ai/thai-nner</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dan">DaN+</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/nne">NNE</pwcdataset>
    </paper>
    <paper id="117">
      <title>Two-Step Question Retrieval for Open-Domain <fixed-case>QA</fixed-case></title>
      <author><first>Yeon</first><last>Seonwoo</last></author>
      <author><first>Juhee</first><last>Son</last></author>
      <author><first>Jiho</first><last>Jin</last></author>
      <author><first>Sang-Woo</first><last>Lee</last></author>
      <author><first>Ji-Hoon</first><last>Kim</last></author>
      <author><first>Jung-Woo</first><last>Ha</last></author>
      <author><first>Alice</first><last>Oh</last></author>
      <pages>1487-1492</pages>
      <abstract>The retriever-reader pipeline has shown promising performance in open-domain QA but suffers from a very slow inference speed. Recently proposed question retrieval models tackle this problem by indexing question-answer pairs and searching for similar questions. These models have shown a significant increase in inference speed, but at the cost of lower QA performance compared to the retriever-reader models. This paper proposes a two-step question retrieval model, SQuID (Sequential Question-Indexed Dense retrieval) and distant supervision for training. SQuID uses two bi-encoders for question retrieval. The first-step retriever selects top-k similar questions, and the second-step retriever finds the most similar question from the top-k questions. We evaluate the performance and the computational efficiency of SQuID. The results show that SQuID significantly increases the performance of existing question retrieval models with a negligible loss on inference speed.</abstract>
      <url hash="aecb05fb">2022.findings-acl.117</url>
      <attachment type="software" hash="6ed1ed63">2022.findings-acl.117.software.zip</attachment>
      <bibkey>seonwoo-etal-2022-two</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paq">PAQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
    </paper>
    <paper id="118">
      <title>Semantically Distributed Robust Optimization for Vision-and-Language Inference</title>
      <author><first>Tejas</first><last>Gokhale</last></author>
      <author><first>Abhishek</first><last>Chaudhary</last></author>
      <author><first>Pratyay</first><last>Banerjee</last></author>
      <author><first>Chitta</first><last>Baral</last></author>
      <author><first>Yezhou</first><last>Yang</last></author>
      <pages>1493-1513</pages>
      <abstract>Analysis of vision-and-language models has revealed their brittleness under linguistic phenomena such as paraphrasing, negation, textual entailment, and word substitutions with synonyms or antonyms.While data augmentation techniques have been designed to mitigate against these failure modes, methods that can integrate this knowledge into the training pipeline remain under-explored.In this paper, we present <b>SDRO</b>, a model-agnostic method that utilizes a set linguistic transformations in a distributed robust optimization setting, along with an ensembling technique to leverage these transformations during inference.Experiments on benchmark datasets with images (NLVR<tex-math>^2</tex-math>) and video (VIOLIN) demonstrate performance improvements as well as robustness to adversarial attacks.Experiments on binary VQA explore the generalizability of this method to other V&amp;L tasks.</abstract>
      <url hash="6b0de454">2022.findings-acl.118</url>
      <bibkey>gokhale-etal-2022-semantically</bibkey>
      <pwccode url="https://github.com/asu-apg/vli_sdro" additional="false">asu-apg/vli_sdro</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/violin">Violin</pwcdataset>
    </paper>
    <paper id="119">
      <title>Learning from Missing Relations: Contrastive Learning with Commonsense Knowledge Graphs for Commonsense Inference</title>
      <author><first>Yong-Ho</first><last>Jung</last></author>
      <author><first>Jun-Hyung</first><last>Park</last></author>
      <author><first>Joon-Young</first><last>Choi</last></author>
      <author><first>Mingyu</first><last>Lee</last></author>
      <author><first>Junho</first><last>Kim</last></author>
      <author><first>Kang-Min</first><last>Kim</last></author>
      <author><first>SangKeun</first><last>Lee</last></author>
      <pages>1514-1523</pages>
      <abstract>Commonsense inference poses a unique challenge to reason and generate the physical, social, and causal conditions of a given event. Existing approaches to commonsense inference utilize commonsense transformers, which are large-scale language models that learn commonsense knowledge graphs. However, they suffer from a lack of coverage and expressive diversity of the graphs, resulting in a degradation of the representation quality. In this paper, we focus on addressing missing relations in commonsense knowledge graphs, and propose a novel contrastive learning framework called SOLAR. Our framework contrasts sets of semantically similar and dissimilar events, learning richer inferential knowledge compared to existing approaches. Empirical results demonstrate the efficacy of SOLAR in commonsense inference of diverse commonsense knowledge graphs. Specifically, SOLAR outperforms the state-of-the-art commonsense transformer on commonsense inference with ConceptNet by 1.84% on average among 8 automatic evaluation metrics. In-depth analysis of SOLAR sheds light on the effects of the missing relations utilized in learning commonsense knowledge graphs.</abstract>
      <url hash="ce9fc9a2">2022.findings-acl.119</url>
      <bibkey>jung-etal-2022-learning</bibkey>
      <pwccode url="https://github.com/yongho94/solar-framework_commonsense-inference" additional="false">yongho94/solar-framework_commonsense-inference</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/event2mind">Event2Mind</pwcdataset>
    </paper>
    <paper id="120">
      <title>Capture Human Disagreement Distributions by Calibrated Networks for Natural Language Inference</title>
      <author><first>Yuxia</first><last>Wang</last></author>
      <author><first>Minghan</first><last>Wang</last></author>
      <author><first>Yimeng</first><last>Chen</last></author>
      <author><first>Shimin</first><last>Tao</last></author>
      <author><first>Jiaxin</first><last>Guo</last></author>
      <author><first>Chang</first><last>Su</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <author><first>Hao</first><last>Yang</last></author>
      <pages>1524-1535</pages>
      <abstract>Natural Language Inference (NLI) datasets contain examples with highly ambiguous labels due to its subjectivity. Several recent efforts have been made to acknowledge and embrace the existence of ambiguity, and explore how to capture the human disagreement distribution. In contrast with directly learning from gold ambiguity labels, relying on special resource, we argue that the model has naturally captured the human ambiguity distribution as long as it’s calibrated, i.e. the predictive probability can reflect the true correctness likelihood. Our experiments show that when model is well-calibrated, either by label smoothing or temperature scaling, it can obtain competitive performance as prior work, on both divergence scores between predictive probability and the true human opinion distribution, and the accuracy. This reveals the overhead of collecting gold ambiguity labels can be cut, by broadly solving how to calibrate the NLI network.</abstract>
      <url hash="27f686db">2022.findings-acl.120</url>
      <bibkey>wang-etal-2022-capture</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/chaosnli">ChaosNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
    </paper>
    <paper id="121">
      <title>Efficient, Uncertainty-based Moderation of Neural Networks Text Classifiers</title>
      <author><first>Jakob Smedegaard</first><last>Andersen</last></author>
      <author><first>Walid</first><last>Maalej</last></author>
      <pages>1536-1546</pages>
      <abstract>To maximize the accuracy and increase the overall acceptance of text classifiers, we propose a framework for the efficient, in-operation moderation of classifiers’ output. Our framework focuses on use cases in which F1-scores of modern Neural Networks classifiers (ca. 90%) are still inapplicable in practice. We suggest a semi-automated approach that uses prediction uncertainties to pass unconfident, probably incorrect classifications to human moderators. To minimize the workload, we limit the human moderated data to the point where the accuracy gains saturate and further human effort does not lead to substantial improvements. A series of benchmarking experiments based on three different datasets and three state-of-the-art classifiers show that our framework can improve the classification F1-scores by 5.1 to 11.2% (up to approx. 98 to 99%), while reducing the moderation load up to 73.3% compared to a random moderation.</abstract>
      <url hash="2b0f4afc">2022.findings-acl.121</url>
      <attachment type="software" hash="fdb8e0b4">2022.findings-acl.121.software.zip</attachment>
      <bibkey>andersen-maalej-2022-efficient</bibkey>
      <pwccode url="https://github.com/jsandersen/cmt" additional="false">jsandersen/cmt</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="122">
      <title>Revisiting Automatic Evaluation of Extractive Summarization Task: Can We Do Better than <fixed-case>ROUGE</fixed-case>?</title>
      <author><first>Mousumi</first><last>Akter</last></author>
      <author><first>Naman</first><last>Bansal</last></author>
      <author><first>Shubhra Kanti</first><last>Karmaker</last></author>
      <pages>1547-1560</pages>
      <abstract>It has been the norm for a long time to evaluate automated summarization tasks using the popular ROUGE metric. Although several studies in the past have highlighted the limitations of ROUGE, researchers have struggled to reach a consensus on a better alternative until today. One major limitation of the traditional ROUGE metric is the lack of semantic understanding (relies on direct overlap of n-grams). In this paper, we exclusively focus on the extractive summarization task and propose a semantic-aware nCG (normalized cumulative gain)-based evaluation metric (called Sem-nCG) for evaluating this task. One fundamental contribution of the paper is that it demonstrates how we can generate more reliable semantic-aware ground truths for evaluating extractive summarization tasks without any additional human intervention. To the best of our knowledge, this work is the first of its kind. We have conducted extensive experiments with this new metric using the widely used CNN/DailyMail dataset. Experimental results show that the new Sem-nCG metric is indeed semantic-aware, shows higher correlation with human judgement (more reliable) and yields a large number of disagreements with the original ROUGE metric (suggesting that ROUGE often leads to inaccurate conclusions also verified by humans).</abstract>
      <url hash="1da33b09">2022.findings-acl.122</url>
      <bibkey>akter-etal-2022-revisiting</bibkey>
    </paper>
    <paper id="123">
      <title>Open Vocabulary Extreme Classification Using Generative Models</title>
      <author><first>Daniel</first><last>Simig</last></author>
      <author><first>Fabio</first><last>Petroni</last></author>
      <author><first>Pouya</first><last>Yanki</last></author>
      <author><first>Kashyap</first><last>Popat</last></author>
      <author><first>Christina</first><last>Du</last></author>
      <author><first>Sebastian</first><last>Riedel</last></author>
      <author><first>Majid</first><last>Yazdani</last></author>
      <pages>1561-1583</pages>
      <abstract>The extreme multi-label classification (XMC) task aims at tagging content with a subset of labels from an extremely large label set. The label vocabulary is typically defined in advance by domain experts and assumed to capture all necessary tags. However in real world scenarios this label set, although large, is often incomplete and experts frequently need to refine it. To develop systems that simplify this process, we introduce the task of open vocabulary XMC (OXMC): given a piece of content, predict a set of labels, some of which may be outside of the known tag set. Hence, in addition to not having training data for some labels–as is the case in zero-shot classification–models need to invent some labels on-thefly. We propose GROOV, a fine-tuned seq2seq model for OXMC that generates the set of labels as a flat sequence and is trained using a novel loss independent of predicted label order. We show the efficacy of the approach, experimenting with popular XMC datasets for which GROOV is able to predict meaningful labels outside the given vocabulary while performing on par with state-of-the-art solutions for known labels.</abstract>
      <url hash="33d9845e">2022.findings-acl.123</url>
      <bibkey>simig-etal-2022-open</bibkey>
    </paper>
    <paper id="124">
      <title>Decomposed Meta-Learning for Few-Shot Named Entity Recognition</title>
      <author><first>Tingting</first><last>Ma</last></author>
      <author><first>Huiqiang</first><last>Jiang</last></author>
      <author><first>Qianhui</first><last>Wu</last></author>
      <author><first>Tiejun</first><last>Zhao</last></author>
      <author><first>Chin-Yew</first><last>Lin</last></author>
      <pages>1584-1596</pages>
      <abstract>Few-shot named entity recognition (NER) systems aim at recognizing novel-class named entities based on only a few labeled examples. In this paper, we present a decomposed meta-learning approach which addresses the problem of few-shot NER by sequentially tackling few-shot span detection and few-shot entity typing using meta-learning. In particular, we take the few-shot span detection as a sequence labeling problem and train the span detector by introducing the model-agnostic meta-learning (MAML) algorithm to find a good model parameter initialization that could fast adapt to new entity classes. For few-shot entity typing, we propose MAML-ProtoNet, i.e., MAML-enhanced prototypical networks to find a good embedding space that can better distinguish text span representations from different entity classes. Extensive experiments on various benchmarks show that our approach achieves superior performance over prior methods.</abstract>
      <url hash="cc1f91ee">2022.findings-acl.124</url>
      <attachment type="software" hash="f0183f32">2022.findings-acl.124.software.zip</attachment>
      <bibkey>ma-etal-2022-decomposed</bibkey>
      <pwccode url="https://github.com/microsoft/vert-papers" additional="false">microsoft/vert-papers</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2002">CoNLL 2002</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/few-nerd">Few-NERD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wnut-2017-emerging-and-rare-entity">WNUT 2017</pwcdataset>
    </paper>
    <paper id="125">
      <title><fixed-case>T</fixed-case>eg<fixed-case>T</fixed-case>ok: Augmenting Text Generation via Task-specific and Open-world Knowledge</title>
      <author><first>Chao-Hong</first><last>Tan</last></author>
      <author><first>Jia-Chen</first><last>Gu</last></author>
      <author><first>Chongyang</first><last>Tao</last></author>
      <author><first>Zhen-Hua</first><last>Ling</last></author>
      <author><first>Can</first><last>Xu</last></author>
      <author><first>Huang</first><last>Hu</last></author>
      <author><first>Xiubo</first><last>Geng</last></author>
      <author><first>Daxin</first><last>Jiang</last></author>
      <pages>1597-1609</pages>
      <abstract>Generating natural and informative texts has been a long-standing problem in NLP. Much effort has been dedicated into incorporating pre-trained language models (PLMs) with various open-world knowledge, such as knowledge graphs or wiki pages. However, their ability to access and manipulate the task-specific knowledge is still limited on downstream tasks, as this type of knowledge is usually not well covered in PLMs and is hard to acquire. To address the problem, we propose augmenting TExt Generation via Task-specific and Open-world Knowledge (TegTok) in a unified framework. Our model selects knowledge entries from two types of knowledge sources through dense retrieval and then injects them into the input encoding and output decoding stages respectively on the basis of PLMs. With the help of these two types of knowledge, our model can learn what and how to generate. Experiments on two text generation tasks of dialogue generation and question generation, and on two datasets show that our method achieves better performance than various baseline models.</abstract>
      <url hash="50290aea">2022.findings-acl.125</url>
      <bibkey>tan-etal-2022-tegtok</bibkey>
      <pwccode url="https://github.com/lxchtan/tegtok" additional="false">lxchtan/tegtok</pwccode>
    </paper>
    <paper id="126">
      <title><fixed-case>E</fixed-case>mo<fixed-case>C</fixed-case>aps: Emotion Capsule based Model for Conversational Emotion Recognition</title>
      <author><first>Zaijing</first><last>Li</last></author>
      <author><first>Fengxiao</first><last>Tang</last></author>
      <author><first>Ming</first><last>Zhao</last></author>
      <author><first>Yusen</first><last>Zhu</last></author>
      <pages>1610-1618</pages>
      <abstract>Emotion recognition in conversation (ERC) aims to analyze the speaker’s state and identify their emotion in the conversation. Recent works in ERC focus on context modeling but ignore the representation of contextual emotional tendency. In order to extract multi-modal information and the emotional tendency of the utterance effectively, we propose a new structure named Emoformer to extract multi-modal emotion vectors from different modalities and fuse them with sentence vector to be an emotion capsule. Furthermore, we design an end-to-end ERC model called EmoCaps, which extracts emotion vectors through the Emoformer structure and obtain the emotion classification results from a context analysis model. Through the experiments with two benchmark datasets, our model shows better performance than the existing state-of-the-art models.</abstract>
      <url hash="747bc2e6">2022.findings-acl.126</url>
      <attachment type="software" hash="d5d998e9">2022.findings-acl.126.software.zip</attachment>
      <bibkey>li-etal-2022-emocaps</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/iemocap">IEMOCAP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/meld">MELD</pwcdataset>
    </paper>
    <paper id="127">
      <title>Logic-Driven Context Extension and Data Augmentation for Logical Reasoning of Text</title>
      <author><first>Siyuan</first><last>Wang</last></author>
      <author><first>Wanjun</first><last>Zhong</last></author>
      <author><first>Duyu</first><last>Tang</last></author>
      <author><first>Zhongyu</first><last>Wei</last></author>
      <author><first>Zhihao</first><last>Fan</last></author>
      <author><first>Daxin</first><last>Jiang</last></author>
      <author><first>Ming</first><last>Zhou</last></author>
      <author><first>Nan</first><last>Duan</last></author>
      <pages>1619-1629</pages>
      <abstract>Logical reasoning of text requires identifying critical logical structures in the text and performing inference over them. Existing methods for logical reasoning mainly focus on contextual semantics of text while struggling to explicitly model the logical inference process. In this paper, we not only put forward a logic-driven context extension framework but also propose a logic-driven data augmentation algorithm. The former follows a three-step reasoning paradigm, and each step is respectively to extract logical expressions as elementary reasoning units, symbolically infer the implicit expressions following equivalence laws and extend the context to validate the options. The latter augments literally similar but logically different instances and incorporates contrastive learning to better capture logical information, especially logical negative and conditional relationships. We conduct experiments on two benchmark datasets, ReClor and LogiQA. The results show that our method achieves state-of-the-art performance on both datasets, and even surpasses human performance on the ReClor dataset.</abstract>
      <url hash="579b2036">2022.findings-acl.127</url>
      <attachment type="software" hash="d2fab7a1">2022.findings-acl.127.software.zip</attachment>
      <bibkey>wang-etal-2022-logic</bibkey>
      <pwccode url="https://github.com/WangsyGit/LReasoner" additional="false">WangsyGit/LReasoner</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/reclor">ReClor</pwcdataset>
    </paper>
    <paper id="128">
      <title>Transfer Learning and Prediction Consistency for Detecting Offensive Spans of Text</title>
      <author><first>Amir</first><last>Pouran Ben Veyseh</last></author>
      <author><first>Ning</first><last>Xu</last></author>
      <author><first>Quan</first><last>Tran</last></author>
      <author><first>Varun</first><last>Manjunatha</last></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Thien</first><last>Nguyen</last></author>
      <pages>1630-1637</pages>
      <abstract>Toxic span detection is the task of recognizing offensive spans in a text snippet. Although there has been prior work on classifying text snippets as offensive or not, the task of recognizing spans responsible for the toxicity of a text is not explored yet. In this work, we introduce a novel multi-task framework for toxic span detection in which the model seeks to simultaneously predict offensive words and opinion phrases to leverage their inter-dependencies and improve the performance. Moreover, we introduce a novel regularization mechanism to encourage the consistency of the model predictions across similar inputs for toxic span detection. Our extensive experiments demonstrate the effectiveness of the proposed model compared to strong baselines.</abstract>
      <url hash="555ea717">2022.findings-acl.128</url>
      <attachment type="software" hash="2cfb564e">2022.findings-acl.128.software.zip</attachment>
      <bibkey>pouran-ben-veyseh-etal-2022-transfer</bibkey>
    </paper>
    <paper id="129">
      <title>Learning Reasoning Patterns for Relational Triple Extraction with Mutual Generation of Text and Graph</title>
      <author><first>Yubo</first><last>Chen</last></author>
      <author><first>Yunqi</first><last>Zhang</last></author>
      <author><first>Yongfeng</first><last>Huang</last></author>
      <pages>1638-1647</pages>
      <abstract>Relational triple extraction is a critical task for constructing knowledge graphs. Existing methods focused on learning text patterns from explicit relational mentions. However, they usually suffered from ignoring relational reasoning patterns, thus failed to extract the implicitly implied triples. Fortunately, the graph structure of a sentence’s relational triples can help find multi-hop reasoning paths. Moreover, the type inference logic through the paths can be captured with the sentence’s supplementary relational expressions that represent the real-world conceptual meanings of the paths’ composite relations. In this paper, we propose a unified framework to learn the relational reasoning patterns for this task. To identify multi-hop reasoning paths, we construct a relational graph from the sentence (text-to-graph generation) and apply multi-layer graph convolutions to it. To capture the relation type inference logic of the paths, we propose to understand the unlabeled conceptual expressions by reconstructing the sentence from the relational graph (graph-to-text generation) in a self-supervised manner. Experimental results on several benchmark datasets demonstrate the effectiveness of our method.</abstract>
      <url hash="f200a93e">2022.findings-acl.129</url>
      <bibkey>chen-etal-2022-learning</bibkey>
    </paper>
    <paper id="130">
      <title>Document-Level Event Argument Extraction via Optimal Transport</title>
      <author><first>Amir</first><last>Pouran Ben Veyseh</last></author>
      <author><first>Minh Van</first><last>Nguyen</last></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Bonan</first><last>Min</last></author>
      <author><first>Thien</first><last>Nguyen</last></author>
      <pages>1648-1658</pages>
      <abstract>Event Argument Extraction (EAE) is one of the sub-tasks of event extraction, aiming to recognize the role of each entity mention toward a specific event trigger. Despite the success of prior works in sentence-level EAE, the document-level setting is less explored. In particular, whereas syntactic structures of sentences have been shown to be effective for sentence-level EAE, prior document-level EAE models totally ignore syntactic structures for documents. Hence, in this work, we study the importance of syntactic structures in document-level EAE. Specifically, we propose to employ Optimal Transport (OT) to induce structures of documents based on sentence-level syntactic structures and tailored to EAE task. Furthermore, we propose a novel regularization technique to explicitly constrain the contributions of unrelated context words in the final prediction for EAE. We perform extensive experiments on the benchmark document-level EAE dataset RAMS that leads to the state-of-the-art performance. Moreover, our experiments on the ACE 2005 dataset reveals the effectiveness of the proposed model in the sentence-level EAE by establishing new state-of-the-art results.</abstract>
      <url hash="d82688c1">2022.findings-acl.130</url>
      <attachment type="software" hash="e2c7df95">2022.findings-acl.130.software.zip</attachment>
      <bibkey>pouran-ben-veyseh-etal-2022-document</bibkey>
    </paper>
    <paper id="131">
      <title>N-Shot Learning for Augmenting Task-Oriented Dialogue State Tracking</title>
      <author><first>Ibrahim</first><last>Aksu</last></author>
      <author><first>Zhengyuan</first><last>Liu</last></author>
      <author><first>Min-Yen</first><last>Kan</last></author>
      <author><first>Nancy</first><last>Chen</last></author>
      <pages>1659-1671</pages>
      <abstract>Augmentation of task-oriented dialogues has followed standard methods used for plain-text such as back-translation, word-level manipulation, and paraphrasing despite its richly annotated structure. In this work, we introduce an augmentation framework that utilizes belief state annotations to match turns from various dialogues and form new synthetic dialogues in a bottom-up manner. Unlike other augmentation strategies, it operates with as few as five examples. Our augmentation strategy yields significant improvements when both adapting a DST model to a new domain, and when adapting a language model to the DST task, on evaluations with TRADE and TOD-BERT models. Further analysis shows that our model performs better on seen values during training, and it is also more robust to unseen values.We conclude that exploiting belief state annotations enhances dialogue augmentation and results in improved models in n-shot training scenarios.</abstract>
      <url hash="0e632aa9">2022.findings-acl.131</url>
      <bibkey>aksu-etal-2022-n</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/multiwoz">MultiWOZ</pwcdataset>
    </paper>
    <paper id="132">
      <title>Document-Level Relation Extraction with Adaptive Focal Loss and Knowledge Distillation</title>
      <author><first>Qingyu</first><last>Tan</last></author>
      <author><first>Ruidan</first><last>He</last></author>
      <author><first>Lidong</first><last>Bing</last></author>
      <author><first>Hwee Tou</first><last>Ng</last></author>
      <pages>1672-1681</pages>
      <abstract>Document-level Relation Extraction (DocRE) is a more challenging task compared to its sentence-level counterpart. It aims to extract relations from multiple sentences at once. In this paper, we propose a semi-supervised framework for DocRE with three novel components. Firstly, we use an axial attention module for learning the interdependency among entity-pairs, which improves the performance on two-hop relations. Secondly, we propose an adaptive focal loss to tackle the class imbalance problem of DocRE. Lastly, we use knowledge distillation to overcome the differences between human annotated data and distantly supervised data. We conducted experiments on two DocRE datasets. Our model consistently outperforms strong baselines and its performance exceeds the previous SOTA by 1.36 F1 and 1.46 Ign_F1 score on the DocRED leaderboard.</abstract>
      <url hash="2d20ba84">2022.findings-acl.132</url>
      <attachment type="software" hash="d9e434b3">2022.findings-acl.132.software.zip</attachment>
      <bibkey>tan-etal-2022-document</bibkey>
      <pwccode url="https://github.com/tonytan48/kd-docre" additional="false">tonytan48/kd-docre</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/docred">DocRED</pwcdataset>
    </paper>
    <paper id="133">
      <title>Calibration of Machine Reading Systems at Scale</title>
      <author><first>Shehzaad</first><last>Dhuliawala</last></author>
      <author><first>Leonard</first><last>Adolphs</last></author>
      <author><first>Rajarshi</first><last>Das</last></author>
      <author><first>Mrinmaya</first><last>Sachan</last></author>
      <pages>1682-1693</pages>
      <abstract>In typical machine learning systems, an estimate of the probability of the prediction is used to assess the system’s confidence in the prediction.This confidence measure is usually uncalibrated; i.e. the system’s confidence in the prediction does not match the true probability of the predicted output.In this paper, we present an investigation into calibrating open setting machine reading systemssuch as open-domain question answering and claim verification systems.We show that calibrating such complex systems which contain discrete retrieval and deep reading components is challenging and current calibration techniques fail to scale to these settings. We propose simple extensions to existing calibration approaches that allows us to adapt them to these settings.Our experimental results reveal that the approach works well, and can be useful to selectively predict answers when question answering systems are posed with unanswerable or out-of-the-training distribution questions.</abstract>
      <url hash="b3b952b0">2022.findings-acl.133</url>
      <bibkey>dhuliawala-etal-2022-calibration</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
    </paper>
    <paper id="134">
      <title>Towards Adversarially Robust Text Classifiers by Learning to Reweight Clean Examples</title>
      <author><first>Jianhan</first><last>Xu</last></author>
      <author><first>Cenyuan</first><last>Zhang</last></author>
      <author><first>Xiaoqing</first><last>Zheng</last></author>
      <author><first>Linyang</first><last>Li</last></author>
      <author><first>Cho-Jui</first><last>Hsieh</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <author><first>Xuanjing</first><last>Huang</last></author>
      <pages>1694-1707</pages>
      <abstract>Most of the existing defense methods improve the adversarial robustness by making the models adapt to the training set augmented with some adversarial examples. However, the augmented adversarial examples may not be natural, which might distort the training distribution, resulting in inferior performance both in clean accuracy and adversarial robustness. In this study, we explore the feasibility of introducing a reweighting mechanism to calibrate the training distribution to obtain robust models. We propose to train text classifiers by a sample reweighting method in which the example weights are learned to minimize the loss of a validation set mixed with the clean examples and their adversarial ones in an online learning manner. Through extensive experiments, we show that there exists a reweighting mechanism to make the models more robust against adversarial attacks without the need to craft the adversarial examples for the entire training set.</abstract>
      <url hash="e4b68c0a">2022.findings-acl.134</url>
      <attachment type="software" hash="9a1d1e50">2022.findings-acl.134.software.zip</attachment>
      <bibkey>xu-etal-2022-towards</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="135">
      <title>Morphosyntactic Tagging with Pre-trained Language Models for <fixed-case>A</fixed-case>rabic and its Dialects</title>
      <author><first>Go</first><last>Inoue</last></author>
      <author><first>Salam</first><last>Khalifa</last></author>
      <author><first>Nizar</first><last>Habash</last></author>
      <pages>1708-1719</pages>
      <abstract>We present state-of-the-art results on morphosyntactic tagging across different varieties of Arabic using fine-tuned pre-trained transformer language models. Our models consistently outperform existing systems in Modern Standard Arabic and all the Arabic dialects we study, achieving 2.6% absolute improvement over the previous state-of-the-art in Modern Standard Arabic, 2.8% in Gulf, 1.6% in Egyptian, and 8.3% in Levantine. We explore different training setups for fine-tuning pre-trained transformer language models, including training data size, the use of external linguistic resources, and the use of annotated data from other dialects in a low-resource scenario. Our results show that strategic fine-tuning using datasets from other high-resource dialects is beneficial for a low-resource dialect. Additionally, we show that high-quality morphological analyzers as external linguistic resources are beneficial especially in low-resource settings.</abstract>
      <url hash="2d0842f4">2022.findings-acl.135</url>
      <bibkey>inoue-etal-2022-morphosyntactic</bibkey>
      <pwccode url="https://github.com/camel-lab/camelbert_morphosyntactic_tagger" additional="false">camel-lab/camelbert_morphosyntactic_tagger</pwccode>
    </paper>
    <paper id="136">
      <title>How Pre-trained Language Models Capture Factual Knowledge? A Causal-Inspired Analysis</title>
      <author><first>Shaobo</first><last>Li</last></author>
      <author><first>Xiaoguang</first><last>Li</last></author>
      <author><first>Lifeng</first><last>Shang</last></author>
      <author><first>Zhenhua</first><last>Dong</last></author>
      <author><first>Chengjie</first><last>Sun</last></author>
      <author><first>Bingquan</first><last>Liu</last></author>
      <author><first>Zhenzhou</first><last>Ji</last></author>
      <author><first>Xin</first><last>Jiang</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <pages>1720-1732</pages>
      <abstract>Recently, there has been a trend to investigate the factual knowledge captured by Pre-trained Language Models (PLMs). Many works show the PLMs’ ability to fill in the missing factual words in cloze-style prompts such as ”Dante was born in [MASK].” However, it is still a mystery how PLMs generate the results correctly: relying on effective clues or shortcut patterns? We try to answer this question by a causal-inspired analysis that quantitatively measures and evaluates the word-level patterns that PLMs depend on to generate the missing words. We check the words that have three typical associations with the missing words: knowledge-dependent, positionally close, and highly co-occurred. Our analysis shows: (1) PLMs generate the missing factual words more by the positionally close and highly co-occurred words than the knowledge-dependent words; (2) the dependence on the knowledge-dependent words is more effective than the positionally close and highly co-occurred words. Accordingly, we conclude that the PLMs capture the factual knowledge ineffectively because of depending on the inadequate associations.</abstract>
      <url hash="2bc28acb">2022.findings-acl.136</url>
      <bibkey>li-etal-2022-pre</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/lama">LAMA</pwcdataset>
    </paper>
    <paper id="137">
      <title>Metadata Shaping: A Simple Approach for Knowledge-Enhanced Language Models</title>
      <author><first>Simran</first><last>Arora</last></author>
      <author><first>Sen</first><last>Wu</last></author>
      <author><first>Enci</first><last>Liu</last></author>
      <author><first>Christopher</first><last>Re</last></author>
      <pages>1733-1745</pages>
      <abstract>Popular language models (LMs) struggle to capture knowledge about rare tail facts and entities. Since widely used systems such as search and personal-assistants must support the long tail of entities that users ask about, there has been significant effort towards enhancing these base LMs with factual knowledge. We observe proposed methods typically start with a base LM and data that has been annotated with entity metadata, then change the model, by modifying the architecture or introducing auxiliary loss terms to better capture entity knowledge. In this work, we question this typical process and ask to what extent can we match the quality of model modifications, with a simple alternative: using a base LM and only changing the data. We propose metadata shaping, a method which inserts substrings corresponding to the readily available entity metadata, e.g. types and descriptions, into examples at train and inference time based on mutual information. Despite its simplicity, metadata shaping is quite effective. On standard evaluation benchmarks for knowledge-enhanced LMs, the method exceeds the base-LM baseline by an average of 4.3 F1 points and achieves state-of-the-art results. We further show the gains are on average 4.4x larger for the slice of examples containing tail vs. popular entities.</abstract>
      <url hash="be7d0b7a">2022.findings-acl.137</url>
      <bibkey>arora-etal-2022-metadata</bibkey>
      <pwccode url="https://github.com/simran-arora/metadatashaping" additional="false">simran-arora/metadatashaping</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel">FewRel</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/open-entity-1">Open Entity</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tacred">TACRED</pwcdataset>
    </paper>
    <paper id="138">
      <title>Enhancing Natural Language Representation with Large-Scale Out-of-Domain Commonsense</title>
      <author><first>Wanyun</first><last>Cui</last></author>
      <author><first>Xingran</first><last>Chen</last></author>
      <pages>1746-1756</pages>
      <abstract>We study how to enhance text representation via textual commonsense. We point out that commonsense has the nature of domain discrepancy. Namely, commonsense has different data formats and is domain-independent from the downstream task. This nature brings challenges to introducing commonsense in general text understanding tasks. A typical method of introducing textual knowledge is continuing pre-training over the commonsense corpus. However, it will cause catastrophic forgetting to the downstream task due to the domain discrepancy. In addition, previous methods of directly using textual descriptions as extra input information cannot apply to large-scale commonsense.In this paper, we propose to use large-scale out-of-domain commonsense to enhance text representation. In order to effectively incorporate the commonsense, we proposed OK-Transformer (Out-of-domain Knowledge enhanced Transformer). OK-Transformer effectively integrates commonsense descriptions and enhances them to the target text representation. In addition, OK-Transformer can adapt to the Transformer-based language models (e.g. BERT, RoBERTa) for free, without pre-training on large-scale unsupervised corpora. We have verified the effectiveness of OK-Transformer in multiple applications such as commonsense reasoning, general text classification, and low-resource commonsense settings.</abstract>
      <url hash="de8ffff0">2022.findings-acl.138</url>
      <bibkey>cui-chen-2022-enhancing</bibkey>
      <pwccode url="https://github.com/chenxran/ok-transformer" additional="false">chenxran/ok-transformer</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/commonsenseqa">CommonsenseQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wsc">WSC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/winogrande">WinoGrande</pwcdataset>
    </paper>
    <paper id="139">
      <title>Weighted self Distillation for <fixed-case>C</fixed-case>hinese word segmentation</title>
      <author><first>Rian</first><last>He</last></author>
      <author><first>Shubin</first><last>Cai</last></author>
      <author><first>Zhong</first><last>Ming</last></author>
      <author><first>Jialei</first><last>Zhang</last></author>
      <pages>1757-1770</pages>
      <abstract>Recent researches show that multi-criteria resources and n-gram features are beneficial to Chinese Word Segmentation (CWS). However, these methods rely heavily on such additional information mentioned above and focus less on the model itself. We thus propose a novel neural framework, named Weighted self Distillation for Chinese word segmentation (WeiDC). The framework, which only requires unigram features, adopts self-distillation technology with four hand-crafted weight modules and two teacher models configurations. Experiment results show that WeiDC can make use of character features to learn contextual knowledge and successfully achieve state-of-the-art or competitive performance in terms of strictly closed test settings on SIGHAN Bakeoff benchmark datasets. Moreover, further experiments and analyses also demonstrate the robustness of WeiDC. Source codes of this paper are available on Github.</abstract>
      <url hash="201ddc2d">2022.findings-acl.139</url>
      <attachment type="software" hash="d5cb4ee0">2022.findings-acl.139.software.zip</attachment>
      <bibkey>he-etal-2022-weighted</bibkey>
      <pwccode url="https://github.com/anzi20/weidc" additional="false">anzi20/weidc</pwccode>
    </paper>
    <paper id="140">
      <title>Sibylvariant Transformations for Robust Text Classification</title>
      <author><first>Fabrice</first><last>Harel-Canada</last></author>
      <author><first>Muhammad Ali</first><last>Gulzar</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <author><first>Miryung</first><last>Kim</last></author>
      <pages>1771-1788</pages>
      <abstract>The vast majority of text transformation techniques in NLP are inherently limited in their ability to expand input space coverage due to an implicit constraint to preserve the original class label. In this work, we propose the notion of sibylvariance (SIB) to describe the broader set of transforms that relax the label-preserving constraint, knowably vary the expected class, and lead to significantly more diverse input distributions. We offer a unified framework to organize all data transformations, including two types of SIB: (1) Transmutations convert one discrete kind into another, (2) Mixture Mutations blend two or more classes together. To explore the role of sibylvariance within NLP, we implemented 41 text transformations, including several novel techniques like Concept2Sentence and SentMix. Sibylvariance also enables a unique form of adaptive training that generates new input mixtures for the most confused class pairs, challenging the learner to differentiate with greater nuance. Our experiments on six benchmark datasets strongly support the efficacy of sibylvariance for generalization performance, defect detection, and adversarial robustness.</abstract>
      <url hash="941e183d">2022.findings-acl.140</url>
      <bibkey>harel-canada-etal-2022-sibylvariant</bibkey>
      <pwccode url="https://github.com/ucla-seal/sibyl" additional="false">ucla-seal/sibyl</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="141">
      <title><fixed-case>D</fixed-case>a<fixed-case>LC</fixed-case>: Domain Adaptation Learning Curve Prediction for Neural Machine Translation</title>
      <author><first>Cheonbok</first><last>Park</last></author>
      <author><first>Hantae</first><last>Kim</last></author>
      <author><first>Ioan</first><last>Calapodescu</last></author>
      <author><first>Hyun Chang</first><last>Cho</last></author>
      <author><first>Vassilina</first><last>Nikoulina</last></author>
      <pages>1789-1807</pages>
      <abstract>Domain Adaptation (DA) of Neural Machine Translation (NMT) model often relies on a pre-trained general NMT model which is adapted to the new domain on a sample of in-domain parallel data. Without parallel data, there is no way to estimate the potential benefit of DA, nor the amount of parallel samples it would require. It is however a desirable functionality that could help MT practitioners to make an informed decision before investing resources in dataset creation. We propose a Domain adaptation Learning Curve prediction (DaLC) model that predicts prospective DA performance based on in-domain monolingual samples in the source language. Our model relies on the NMT encoder representations combined with various instance and corpus-level features. We demonstrate that instance-level is better able to distinguish between different domains compared to corpus-level frameworks proposed in previous studies Finally, we perform in-depth analyses of the results highlighting the limitations of our approach, and provide directions for future research.</abstract>
      <url hash="1a30adf6">2022.findings-acl.141</url>
      <bibkey>park-etal-2022-dalc</bibkey>
    </paper>
    <paper id="142">
      <title>Hey <fixed-case>AI</fixed-case>, Can You Solve Complex Tasks by Talking to Agents?</title>
      <author><first>Tushar</first><last>Khot</last></author>
      <author><first>Kyle</first><last>Richardson</last></author>
      <author><first>Daniel</first><last>Khashabi</last></author>
      <author><first>Ashish</first><last>Sabharwal</last></author>
      <pages>1808-1823</pages>
      <abstract>Training giant models from scratch for each complex task is resource- and data-inefficient. To help develop models that can leverage existing systems, we propose a new challenge: Learning to solve complex tasks by communicating with existing agents (or models) in natural language. We design a synthetic benchmark, CommaQA, with three complex reasoning tasks (explicit, implicit, numeric) designed to be solved by communicating with existing QA agents. For instance, using text and table QA agents to answer questions such as “Who had the longest javelin throw from USA?”. We show that black-box models struggle to learn this task from scratch (accuracy under 50%) even with access to each agent’s knowledge and gold facts supervision. In contrast, models that learn to communicate with agents outperform black-box models, reaching scores of 100% when given gold decomposition supervision. However, we show that the challenge of learning to solve complex tasks by communicating with existing agents <i>without relying on any auxiliary supervision or data</i> still remains highly elusive. We will release CommaQA, along with a compositional generalization test split, to advance research in this direction.</abstract>
      <url hash="cf9af099">2022.findings-acl.142</url>
      <bibkey>khot-etal-2022-hey</bibkey>
      <pwccode url="https://github.com/allenai/commaqa" additional="false">allenai/commaqa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mathqa">MathQA</pwcdataset>
    </paper>
    <paper id="143">
      <title>Modality-specific Learning Rates for Effective Multimodal Additive Late-fusion</title>
      <author><first>Yiqun</first><last>Yao</last></author>
      <author><first>Rada</first><last>Mihalcea</last></author>
      <pages>1824-1834</pages>
      <abstract>In multimodal machine learning, additive late-fusion is a straightforward approach to combine the feature representations from different modalities, in which the final prediction can be formulated as the sum of unimodal predictions. While it has been found that certain late-fusion models can achieve competitive performance with lower computational costs compared to complex multimodal interactive models, how to effectively search for a good late-fusion model is still an open question. Moreover, for different modalities, the best unimodal models may work under significantly different learning rates due to the nature of the modality and the computational flow of the model; thus, selecting a global learning rate for late-fusion models can result in a vanishing gradient for some modalities. To help address these issues, we propose a Modality-Specific Learning Rate (MSLR) method to effectively build late-fusion multimodal models from fine-tuned unimodal models. We investigate three different strategies to assign learning rates to different modalities. Our experiments show that MSLR outperforms global learning rates on multiple tasks and settings, and enables the models to effectively learn each modality.</abstract>
      <url hash="7b9dc305">2022.findings-acl.143</url>
      <bibkey>yao-mihalcea-2022-modality</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/meld">MELD</pwcdataset>
    </paper>
    <paper id="144">
      <title><fixed-case>B</fixed-case>i<fixed-case>S</fixed-case>yn-<fixed-case>GAT</fixed-case>+: Bi-Syntax Aware Graph Attention Network for Aspect-based Sentiment Analysis</title>
      <author><first>Shuo</first><last>Liang</last></author>
      <author><first>Wei</first><last>Wei</last></author>
      <author><first>Xian-Ling</first><last>Mao</last></author>
      <author><first>Fei</first><last>Wang</last></author>
      <author><first>Zhiyong</first><last>He</last></author>
      <pages>1835-1848</pages>
      <abstract>Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis task that aims to align aspects and corresponding sentiments for aspect-specific sentiment polarity inference. It is challenging because a sentence may contain multiple aspects or complicated (e.g., conditional, coordinating, or adversative) relations. Recently, exploiting dependency syntax information with graph neural networks has been the most popular trend. Despite its success, methods that heavily rely on the dependency tree pose challenges in accurately modeling the alignment of the aspects and their words indicative of sentiment, since the dependency tree may provide noisy signals of unrelated associations (e.g., the “conj” relation between “great” and “dreadful” in Figure 2). In this paper, to alleviate this problem, we propose a Bi-Syntax aware Graph Attention Network (BiSyn-GAT+). Specifically, BiSyn-GAT+ fully exploits the syntax information (e.g., phrase segmentation and hierarchical structure) of the constituent tree of a sentence to model the sentiment-aware context of every single aspect (called intra-context) and the sentiment relations across aspects (called inter-context) for learning. Experiments on four benchmark datasets demonstrate that BiSyn-GAT+ outperforms the state-of-the-art methods consistently.</abstract>
      <url hash="b49fda84">2022.findings-acl.144</url>
      <bibkey>liang-etal-2022-bisyn</bibkey>
      <pwccode url="https://github.com/CCIIPLab/BiSyn_GAT_plus" additional="false">CCIIPLab/BiSyn_GAT_plus</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mams">MAMS</pwcdataset>
    </paper>
    <paper id="145">
      <title><fixed-case>I</fixed-case>ndic<fixed-case>BART</fixed-case>: A Pre-trained Model for Indic Natural Language Generation</title>
      <author><first>Raj</first><last>Dabre</last></author>
      <author><first>Himani</first><last>Shrotriya</last></author>
      <author><first>Anoop</first><last>Kunchukuttan</last></author>
      <author><first>Ratish</first><last>Puduppully</last></author>
      <author><first>Mitesh</first><last>Khapra</last></author>
      <author><first>Pratyush</first><last>Kumar</last></author>
      <pages>1849-1863</pages>
      <abstract>In this paper, we study pre-trained sequence-to-sequence models for a group of related languages, with a focus on Indic languages. We present IndicBART, a multilingual, sequence-to-sequence pre-trained model focusing on 11 Indic languages and English. IndicBART utilizes the orthographic similarity between Indic scripts to improve transfer learning between similar Indic languages. We evaluate IndicBART on two NLG tasks: Neural Machine Translation (NMT) and extreme summarization. Our experiments on NMT and extreme summarization show that a model specific to related languages like IndicBART is competitive with large pre-trained models like mBART50 despite being significantly smaller. It also performs well on very low-resource translation scenarios where languages are not included in pre-training or fine-tuning. Script sharing, multilingual training, and better utilization of limited model capacity contribute to the good performance of the compact IndicBART model.</abstract>
      <url hash="ad155507">2022.findings-acl.145</url>
      <bibkey>dabre-etal-2022-indicbart</bibkey>
      <pwccode url="https://github.com/AI4Bharat/indic-bart" additional="false">AI4Bharat/indic-bart</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/flores">FLoRes</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/indiccorp">IndicCorp</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/samanantar">Samanantar</pwcdataset>
    </paper>
    <paper id="146">
      <title>Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models</title>
      <author><first>Jianmo</first><last>Ni</last></author>
      <author><first>Gustavo</first><last>Hernandez Abrego</last></author>
      <author><first>Noah</first><last>Constant</last></author>
      <author><first>Ji</first><last>Ma</last></author>
      <author><first>Keith</first><last>Hall</last></author>
      <author><first>Daniel</first><last>Cer</last></author>
      <author><first>Yinfei</first><last>Yang</last></author>
      <pages>1864-1874</pages>
      <abstract>We provide the first exploration of sentence embeddings from text-to-text transformers (T5) including the effects of scaling up sentence encoders to 11B parameters. Sentence embeddings are broadly useful for language processing tasks. While T5 achieves impressive performance on language tasks, it is unclear how to produce sentence embeddings from encoder-decoder models. We investigate three methods to construct Sentence-T5 (ST5) models: two utilize only the T5 encoder and one using the full T5 encoder-decoder. We establish a new sentence representation transfer benchmark, SentGLUE, which extends the SentEval toolkit to nine tasks from the GLUE benchmark. Our encoder-only models outperform the previous best models on both SentEval and SentGLUE transfer tasks, including semantic textual similarity (STS). Scaling up ST5 from millions to billions of parameters shown to consistently improve performance. Finally, our encoder-decoder method achieves a new state-of-the-art on STS when using sentence embeddings.</abstract>
      <url hash="bc7bd944">2022.findings-acl.146</url>
      <bibkey>ni-etal-2022-sentence</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/reqa">ReQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/senteval">SentEval</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="147">
      <title>Improving Relation Extraction through Syntax-induced Pre-training with Dependency Masking</title>
      <author><first>Yuanhe</first><last>Tian</last></author>
      <author><first>Yan</first><last>Song</last></author>
      <author><first>Fei</first><last>Xia</last></author>
      <pages>1875-1886</pages>
      <abstract>Relation extraction (RE) is an important natural language processing task that predicts the relation between two given entities, where a good understanding of the contextual information is essential to achieve an outstanding model performance. Among different types of contextual information, the auto-generated syntactic information (namely, word dependencies) has shown its effectiveness for the task. However, most existing studies require modifications to the existing baseline architectures (e.g., adding new components, such as GCN, on the top of an encoder) to leverage the syntactic information. To offer an alternative solution, we propose to leverage syntactic information to improve RE by training a syntax-induced encoder on auto-parsed data through dependency masking. Specifically, the syntax-induced encoder is trained by recovering the masked dependency connections and types in first, second, and third orders, which significantly differs from existing studies that train language models or word embeddings by predicting the context words along the dependency paths. Experimental results on two English benchmark datasets, namely, ACE2005EN and SemEval 2010 Task 8 datasets, demonstrate the effectiveness of our approach for RE, where our approach outperforms strong baselines and achieve state-of-the-art results on both datasets.</abstract>
      <url hash="1490f450">2022.findings-acl.147</url>
      <attachment type="software" hash="473352cb">2022.findings-acl.147.software.zip</attachment>
      <bibkey>tian-etal-2022-improving</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/semeval-2010-task-8">SemEval-2010 Task 8</pwcdataset>
    </paper>
    <paper id="148">
      <title>Striking a Balance: Alleviating Inconsistency in Pre-trained Models for Symmetric Classification Tasks</title>
      <author><first>Ashutosh</first><last>Kumar</last></author>
      <author><first>Aditya</first><last>Joshi</last></author>
      <pages>1887-1895</pages>
      <abstract>While fine-tuning pre-trained models for downstream classification is the conventional paradigm in NLP, often task-specific nuances may not get captured in the resultant models. Specifically, for tasks that take two inputs and require the output to be invariant of the order of the inputs, inconsistency is often observed in the predicted labels or confidence scores.We highlight this model shortcoming and apply a consistency loss function to alleviate inconsistency in symmetric classification. Our results show an improved consistency in predictions for three paraphrase detection datasets without a significant drop in the accuracy scores. We examine the classification performance of six datasets (both symmetric and non-symmetric) to showcase the strengths and limitations of our approach.</abstract>
      <url hash="4063f841">2022.findings-acl.148</url>
      <attachment type="software" hash="610a5f56">2022.findings-acl.148.software.zip</attachment>
      <bibkey>kumar-joshi-2022-striking</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paws">PAWS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="149">
      <title>Diversifying Content Generation for Commonsense Reasoning with Mixture of Knowledge Graph Experts</title>
      <author><first>Wenhao</first><last>Yu</last></author>
      <author><first>Chenguang</first><last>Zhu</last></author>
      <author><first>Lianhui</first><last>Qin</last></author>
      <author><first>Zhihan</first><last>Zhang</last></author>
      <author><first>Tong</first><last>Zhao</last></author>
      <author><first>Meng</first><last>Jiang</last></author>
      <pages>1896-1906</pages>
      <abstract>Generative commonsense reasoning (GCR) in natural language is to reason about the commonsense while generating coherent text. Recent years have seen a surge of interest in improving the generation quality of commonsense reasoning tasks. Nevertheless, these approaches have seldom investigated diversity in the GCR tasks, which aims to generate alternative explanations for a real-world situation or predict all possible outcomes. Diversifying GCR is challenging as it expects to generate multiple outputs that are not only semantically different but also grounded in commonsense knowledge. In this paper, we propose MoKGE, a novel method that diversifies the generative reasoning by a mixture of expert (MoE) strategy on commonsense knowledge graphs (KG). A set of knowledge experts seek diverse reasoning on KG to encourage various generation outputs. Empirical experiments demonstrated that MoKGE can significantly improve the diversity while achieving on par performance on accuracy on two GCR benchmarks, based on both automatic and human evaluations.</abstract>
      <url hash="0f1f93d2">2022.findings-acl.149</url>
      <bibkey>yu-etal-2022-diversifying</bibkey>
      <pwccode url="https://github.com/DM2-ND/MoKGE" additional="false">DM2-ND/MoKGE</pwccode>
    </paper>
    <paper id="150">
      <title>Dict-<fixed-case>BERT</fixed-case>: Enhancing Language Model Pre-training with Dictionary</title>
      <author><first>Wenhao</first><last>Yu</last></author>
      <author><first>Chenguang</first><last>Zhu</last></author>
      <author><first>Yuwei</first><last>Fang</last></author>
      <author><first>Donghan</first><last>Yu</last></author>
      <author><first>Shuohang</first><last>Wang</last></author>
      <author><first>Yichong</first><last>Xu</last></author>
      <author><first>Michael</first><last>Zeng</last></author>
      <author><first>Meng</first><last>Jiang</last></author>
      <pages>1907-1918</pages>
      <abstract>Pre-trained language models (PLMs) aim to learn universal language representations by conducting self-supervised training tasks on large-scale corpora. Since PLMs capture word semantics in different contexts, the quality of word representations highly depends on word frequency, which usually follows a heavy-tailed distributions in the pre-training corpus. Therefore, the embeddings of rare words on the tail are usually poorly optimized. In this work, we focus on enhancing language model pre-training by leveraging definitions of the rare words in dictionaries (e.g., Wiktionary). To incorporate a rare word definition as a part of input, we fetch its definition from the dictionary and append it to the end of the input text sequence. In addition to training with the masked language modeling objective, we propose two novel self-supervised pre-training tasks on word and sentence-level alignment between input text sequence and rare word definitions to enhance language modeling representation with dictionary. We evaluate the proposed Dict-BERT model on the language understanding benchmark GLUE and eight specialized domain benchmark datasets. Extensive experiments demonstrate that Dict-BERT can significantly improve the understanding of rare words and boost model performance on various NLP downstream tasks.</abstract>
      <url hash="c59a5f28">2022.findings-acl.150</url>
      <bibkey>yu-etal-2022-dict</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wnlampro">WNLaMPro</pwcdataset>
    </paper>
    <paper id="151">
      <title>A Feasibility Study of Answer-Unaware Question Generation for Education</title>
      <author><first>Liam</first><last>Dugan</last></author>
      <author><first>Eleni</first><last>Miltsakaki</last></author>
      <author><first>Shriyash</first><last>Upadhyay</last></author>
      <author><first>Etan</first><last>Ginsberg</last></author>
      <author><first>Hannah</first><last>Gonzalez</last></author>
      <author><first>DaHyeon</first><last>Choi</last></author>
      <author><first>Chuning</first><last>Yuan</last></author>
      <author><first>Chris</first><last>Callison-Burch</last></author>
      <pages>1919-1926</pages>
      <abstract>We conduct a feasibility study into the applicability of answer-unaware question generation models to textbook passages. We show that a significant portion of errors in such systems arise from asking irrelevant or un-interpretable questions and that such errors can be ameliorated by providing summarized input. We find that giving these models human-written summaries instead of the original text results in a significant increase in acceptability of generated questions (33% <tex-math>\rightarrow</tex-math> 83%) as determined by expert annotators. We also find that, in the absence of human-written summaries, automatic summarization can serve as a good middle ground.</abstract>
      <url hash="c5c12bf2">2022.findings-acl.151</url>
      <bibkey>dugan-etal-2022-feasibility</bibkey>
      <pwccode url="https://github.com/liamdugan/summary-qg" additional="false">liamdugan/summary-qg</pwccode>
    </paper>
    <paper id="152">
      <title>Relevant <fixed-case>C</fixed-case>ommon<fixed-case>S</fixed-case>ense Subgraphs for “What if...” Procedural Reasoning</title>
      <author><first>Chen</first><last>Zheng</last></author>
      <author><first>Parisa</first><last>Kordjamshidi</last></author>
      <pages>1927-1933</pages>
      <abstract>We study the challenge of learning causal reasoning over procedural text to answer “What if...” questions when external commonsense knowledge is required. We propose a novel multi-hop graph reasoning model to 1) efficiently extract a commonsense subgraph with the most relevant information from a large knowledge graph; 2) predict the causal answer by reasoning over the representations obtained from the commonsense subgraph and the contextual interactions between the questions and context. We evaluate our model on WIQA benchmark and achieve state-of-the-art performance compared to the recent models.</abstract>
      <url hash="ebc5e748">2022.findings-acl.152</url>
      <bibkey>zheng-kordjamshidi-2022-relevant</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wiqa">WIQA</pwcdataset>
    </paper>
    <paper id="153">
      <title>Combining Feature and Instance Attribution to Detect Artifacts</title>
      <author><first>Pouya</first><last>Pezeshkpour</last></author>
      <author><first>Sarthak</first><last>Jain</last></author>
      <author><first>Sameer</first><last>Singh</last></author>
      <author><first>Byron</first><last>Wallace</last></author>
      <pages>1934-1946</pages>
      <abstract>Training the deep neural networks that dominate NLP requires large datasets. These are often collected automatically or via crowdsourcing, and may exhibit systematic biases or annotation artifacts. By the latter we mean spurious correlations between inputs and outputs that do not represent a generally held causal relationship between features and classes; models that exploit such correlations may appear to perform a given task well, but fail on out of sample data. In this paper, we evaluate use of different attribution methods for aiding identification of training data artifacts. We propose new hybrid approaches that combine saliency maps (which highlight important input features) with instance attribution methods (which retrieve training samples influential to a given prediction). We show that this proposed training-feature attribution can be used to efficiently uncover artifacts in training data when a challenging validation set is available. We also carry out a small user study to evaluate whether these methods are useful to NLP researchers in practice, with promising results. We make code for all methods and experiments in this paper available.</abstract>
      <url hash="648a8d6a">2022.findings-acl.153</url>
      <bibkey>pezeshkpour-etal-2022-combining</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/boolq">BoolQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="154">
      <title>Leveraging Expert Guided Adversarial Augmentation For Improving Generalization in Named Entity Recognition</title>
      <author><first>Aaron</first><last>Reich</last></author>
      <author><first>Jiaao</first><last>Chen</last></author>
      <author><first>Aastha</first><last>Agrawal</last></author>
      <author><first>Yanzhe</first><last>Zhang</last></author>
      <author><first>Diyi</first><last>Yang</last></author>
      <pages>1947-1955</pages>
      <abstract>Named Entity Recognition (NER) systems often demonstrate great performance on in-distribution data, but perform poorly on examples drawn from a shifted distribution. One way to evaluate the generalization ability of NER models is to use adversarial examples, on which the specific variations associated with named entities are rarely considered. To this end, we propose leveraging expert-guided heuristics to change the entity tokens and their surrounding contexts thereby altering their entity types as adversarial attacks. Using expert-guided heuristics, we augmented the CoNLL 2003 test set and manually annotated it to construct a high-quality challenging set. We found that state-of-the-art NER systems trained on CoNLL 2003 training data drop performance dramatically on our challenging set. By training on adversarial augmented training examples and using mixup for regularization, we were able to significantly improve the performance on the challenging set as well as improve out-of-domain generalization which we evaluated by using OntoNotes data. We have publicly released our dataset and code at https://github.com/GT-SALT/Guided-Adversarial-Augmentation.</abstract>
      <url hash="a1167047">2022.findings-acl.154</url>
      <attachment type="software" hash="1438e5c9">2022.findings-acl.154.software.zip</attachment>
      <bibkey>reich-etal-2022-leveraging</bibkey>
      <pwccode url="https://github.com/gt-salt/guided-adversarial-augmentation" additional="false">gt-salt/guided-adversarial-augmentation</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
    </paper>
    <paper id="155">
      <title>Label Semantics for Few Shot Named Entity Recognition</title>
      <author><first>Jie</first><last>Ma</last></author>
      <author><first>Miguel</first><last>Ballesteros</last></author>
      <author><first>Srikanth</first><last>Doss</last></author>
      <author><first>Rishita</first><last>Anubhai</last></author>
      <author><first>Sunil</first><last>Mallya</last></author>
      <author><first>Yaser</first><last>Al-Onaizan</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>1956-1971</pages>
      <abstract>We study the problem of few shot learning for named entity recognition. Specifically, we leverage the semantic information in the names of the labels as a way of giving the model additional signal and enriched priors. We propose a neural architecture that consists of two BERT encoders, one to encode the document and its tokens and another one to encode each of the labels in natural language format. Our model learns to match the representations of named entities computed by the first encoder with label representations computed by the second encoder. The label semantics signal is shown to support improved state-of-the-art results in multiple few shot NER benchmarks and on-par performance in standard benchmarks. Our model is especially effective in low resource settings.</abstract>
      <url hash="a7cbbc10">2022.findings-acl.155</url>
      <bibkey>ma-etal-2022-label</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ncbi-disease-1">NCBI Disease</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wnut-2017-emerging-and-rare-entity">WNUT 2017</pwcdataset>
    </paper>
    <paper id="156">
      <title>Detection, Disambiguation, Re-ranking: Autoregressive Entity Linking as a Multi-Task Problem</title>
      <author><first>Khalil</first><last>Mrini</last></author>
      <author><first>Shaoliang</first><last>Nie</last></author>
      <author><first>Jiatao</first><last>Gu</last></author>
      <author><first>Sinong</first><last>Wang</last></author>
      <author><first>Maziar</first><last>Sanjabi</last></author>
      <author><first>Hamed</first><last>Firooz</last></author>
      <pages>1972-1983</pages>
      <abstract>We propose an autoregressive entity linking model, that is trained with two auxiliary tasks, and learns to re-rank generated samples at inference time. Our proposed novelties address two weaknesses in the literature. First, a recent method proposes to learn mention detection and then entity candidate selection, but relies on predefined sets of candidates. We use encoder-decoder autoregressive entity linking in order to bypass this need, and propose to train mention detection as an auxiliary task instead. Second, previous work suggests that re-ranking could help correct prediction errors. We add a new, auxiliary task, match prediction, to learn re-ranking. Without the use of a knowledge base or candidate sets, our model sets a new state of the art in two benchmark datasets of entity linking: COMETA in the biomedical domain, and AIDA-CoNLL in the news domain. We show through ablation studies that each of the two auxiliary tasks increases performance, and that re-ranking is an important factor to the increase. Finally, our low-resource experimental results suggest that performance on the main task benefits from the knowledge learned by the auxiliary tasks, and not just from the additional training data.</abstract>
      <url hash="f1136e7f">2022.findings-acl.156</url>
      <bibkey>mrini-etal-2022-detection</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/aida-conll-yago">AIDA CoNLL-YAGO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cometa">COMETA</pwcdataset>
    </paper>
    <paper id="157">
      <title><fixed-case>VISITRON</fixed-case>: Visual Semantics-Aligned Interactively Trained Object-Navigator</title>
      <author><first>Ayush</first><last>Shrivastava</last></author>
      <author><first>Karthik</first><last>Gopalakrishnan</last></author>
      <author id="yang-liu-icsi"><first>Yang</first><last>Liu</last></author>
      <author><first>Robinson</first><last>Piramuthu</last></author>
      <author><first>Gokhan</first><last>Tur</last></author>
      <author><first>Devi</first><last>Parikh</last></author>
      <author><first>Dilek</first><last>Hakkani-Tur</last></author>
      <pages>1984-1994</pages>
      <abstract>Interactive robots navigating photo-realistic environments need to be trained to effectively leverage and handle the dynamic nature of dialogue in addition to the challenges underlying vision-and-language navigation (VLN). In this paper, we present VISITRON, a multi-modal Transformer-based navigator better suited to the interactive regime inherent to Cooperative Vision-and-Dialog Navigation (CVDN). VISITRON is trained to: i) identify and associate object-level concepts and semantics between the environment and dialogue history, ii) identify when to interact vs. navigate via imitation learning of a binary classification head. We perform extensive pre-training and fine-tuning ablations with VISITRON to gain empirical insights and improve performance on CVDN. VISITRON’s ability to identify when to interact leads to a natural generalization of the game-play mode introduced by Roman et al. (2020) for enabling the use of such models in different environments. VISITRON is competitive with models on the static CVDN leaderboard and attains state-of-the-art performance on the Success weighted by Path Length (SPL) metric.</abstract>
      <url hash="2fe39c00">2022.findings-acl.157</url>
      <bibkey>shrivastava-etal-2022-visitron</bibkey>
      <pwccode url="https://github.com/alexa/visitron" additional="false">alexa/visitron</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/matterport3d">Matterport3D</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/rxr">RxR</pwcdataset>
    </paper>
    <paper id="158">
      <title>Investigating Selective Prediction Approaches Across Several Tasks in <fixed-case>IID</fixed-case>, <fixed-case>OOD</fixed-case>, and Adversarial Settings</title>
      <author><first>Neeraj</first><last>Varshney</last></author>
      <author><first>Swaroop</first><last>Mishra</last></author>
      <author><first>Chitta</first><last>Baral</last></author>
      <pages>1995-2002</pages>
      <abstract>In order to equip NLP systems with ‘selective prediction’ capability, several task-specific approaches have been proposed. However, which approaches work best across tasks or even if they consistently outperform the simplest baseline MaxProb remains to be explored. To this end, we systematically study selective prediction in a large-scale setup of 17 datasets across several NLP tasks. Through comprehensive experiments under in-domain (IID), out-of-domain (OOD), and adversarial (ADV) settings, we show that despite leveraging additional resources (held-out data/computation), none of the existing approaches consistently and considerably outperforms MaxProb in all three settings. Furthermore, their performance does not translate well across tasks. For instance, Monte-Carlo Dropout outperforms all other approaches on Duplicate Detection datasets but does not fare well on NLI datasets, especially in the OOD setting. Thus, we recommend that future selective prediction approaches should be evaluated across tasks and settings for reliable estimation of their capabilities.</abstract>
      <url hash="81ccc0aa">2022.findings-acl.158</url>
      <attachment type="software" hash="64c1717d">2022.findings-acl.158.software.zip</attachment>
      <bibkey>varshney-etal-2022-investigating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="159">
      <title>Unsupervised Natural Language Inference Using <fixed-case>PHL</fixed-case> Triplet Generation</title>
      <author><first>Neeraj</first><last>Varshney</last></author>
      <author><first>Pratyay</first><last>Banerjee</last></author>
      <author><first>Tejas</first><last>Gokhale</last></author>
      <author><first>Chitta</first><last>Baral</last></author>
      <pages>2003-2016</pages>
      <abstract>Transformer-based models achieve impressive performance on numerous Natural Language Inference (NLI) benchmarks when trained on respective training datasets. However, in certain cases, training samples may not be available or collecting them could be time-consuming and resource-intensive. In this work, we address the above challenge and present an explorative study on unsupervised NLI, a paradigm in which no human-annotated training samples are available. We investigate it under three settings: PH, P, and NPH that differ in the extent of unlabeled data available for learning. As a solution, we propose a procedural data generation approach that leverages a set of sentence transformations to collect PHL (Premise, Hypothesis, Label) triplets for training NLI models, bypassing the need for human-annotated training data. Comprehensive experiments with several NLI datasets show that the proposed approach results in accuracies of up to 66.75%, 65.9%, 65.39% in PH, P, and NPH settings respectively, outperforming all existing unsupervised baselines. Furthermore, fine-tuning our model with as little as ~0.1% of the human-annotated training dataset (500 instances) leads to 12.2% higher accuracy than the model trained from scratch on the same 500 instances. Supported by this superior performance, we conclude with a recommendation for collecting high-quality task-specific data.</abstract>
      <url hash="470bd790">2022.findings-acl.159</url>
      <attachment type="software" hash="983dbc30">2022.findings-acl.159.software.zip</attachment>
      <bibkey>varshney-etal-2022-unsupervised</bibkey>
      <pwccode url="https://github.com/nrjvarshney/unsupervised_nli" additional="false">nrjvarshney/unsupervised_nli</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="160">
      <title>Data Augmentation and Learned Layer Aggregation for Improved Multilingual Language Understanding in Dialogue</title>
      <author><first>Evgeniia</first><last>Razumovskaia</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <author><first>Anna</first><last>Korhonen</last></author>
      <pages>2017-2033</pages>
      <abstract>Scaling dialogue systems to a multitude of domains, tasks and languages relies on costly and time-consuming data annotation for different domain-task-language configurations. The annotation efforts might be substantially reduced by the methods that generalise well in zero- and few-shot scenarios, and also effectively leverage external unannotated data sources (e.g., Web-scale corpora). We propose two methods to this aim, offering improved dialogue natural language understanding (NLU) across multiple languages: 1) Multi-SentAugment, and 2) LayerAgg. Multi-SentAugment is a self-training method which augments available (typically few-shot) training data with similar (automatically labelled) in-domain sentences from large monolingual Web-scale corpora. LayerAgg learns to select and combine useful semantic information scattered across different layers of a Transformer model (e.g., mBERT); it is especially suited for zero-shot scenarios as semantically richer representations should strengthen the model’s cross-lingual capabilities. Applying the two methods with state-of-the-art NLU models obtains consistent improvements across two standard multilingual NLU datasets covering 16 diverse languages. The gains are observed in zero-shot, few-shot, and even in full-data scenarios. The results also suggest that the two methods achieve a synergistic effect: the best overall performance in few-shot setups is attained when the methods are used together.</abstract>
      <url hash="b76d6024">2022.findings-acl.160</url>
      <bibkey>razumovskaia-etal-2022-data</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cc100">CC100</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xsid">xSID</pwcdataset>
    </paper>
    <paper id="161">
      <title>Ranking-Constrained Learning with Rationales for Text Classification</title>
      <author><first>Juanyan</first><last>Wang</last></author>
      <author><first>Manali</first><last>Sharma</last></author>
      <author><first>Mustafa</first><last>Bilgic</last></author>
      <pages>2034-2046</pages>
      <abstract>We propose a novel approach that jointly utilizes the labels and elicited rationales for text classification to speed up the training of deep learning models with limited training data. We define and optimize a ranking-constrained loss function that combines cross-entropy loss with ranking losses as rationale constraints. We evaluate our proposed rationale-augmented learning approach on three human-annotated datasets, and show that our approach provides significant improvements over classification approaches that do not utilize rationales as well as other state-of-the-art rationale-augmented baselines.</abstract>
      <url hash="ed9f5299">2022.findings-acl.161</url>
      <bibkey>wang-etal-2022-ranking</bibkey>
    </paper>
    <paper id="162">
      <title><fixed-case>C</fixed-case>a<fixed-case>M</fixed-case>-<fixed-case>G</fixed-case>en: <fixed-case>C</fixed-case>ausally Aware Metric-Guided Text Generation</title>
      <author><first>Navita</first><last>Goyal</last></author>
      <author><first>Roodram</first><last>Paneri</last></author>
      <author><first>Ayush</first><last>Agarwal</last></author>
      <author><first>Udit</first><last>Kalani</last></author>
      <author><first>Abhilasha</first><last>Sancheti</last></author>
      <author><first>Niyati</first><last>Chhaya</last></author>
      <pages>2047-2060</pages>
      <abstract>Content is created for a well-defined purpose, often described by a metric or signal represented in the form of structured information. The relationship between the goal (metrics) of target content and the content itself is non-trivial. While large-scale language models show promising text generation capabilities, guiding the generated text with external metrics is challenging.These metrics and content tend to have inherent relationships and not all of them may be of consequence. We introduce CaM-Gen: Causally aware Generative Networks guided by user-defined target metrics incorporating the causal relationships between the metric and content features. We leverage causal inference techniques to identify causally significant aspects of a text that lead to the target metric and then explicitly guide generative models towards these by a feedback mechanism. We propose this mechanism for variational autoencoder and Transformer-based generative models. The proposed models beat baselines in terms of the target metric control while maintaining fluency and language quality of the generated text. To the best of our knowledge, this is one of the early attempts at controlled generation incorporating a metric guide using causal inference.</abstract>
      <url hash="04a83ef0">2022.findings-acl.162</url>
      <bibkey>goyal-etal-2022-cam</bibkey>
    </paper>
    <paper id="163">
      <title>Training Dynamics for Text Summarization Models</title>
      <author><first>Tanya</first><last>Goyal</last></author>
      <author><first>Jiacheng</first><last>Xu</last></author>
      <author><first>Junyi Jessy</first><last>Li</last></author>
      <author><first>Greg</first><last>Durrett</last></author>
      <pages>2061-2073</pages>
      <abstract>Pre-trained language models (e.g. BART) have shown impressive results when fine-tuned on large summarization datasets. However, little is understood about this fine-tuning process, including what knowledge is retained from pre-training time or how content selection and generation strategies are learnt across iterations. In this work, we analyze the training dynamics for generation models, focusing on summarization. Across different datasets (CNN/DM, XSum, MediaSum) and summary properties, such as abstractiveness and hallucination, we study what the model learns at different stages of its fine-tuning process. We find that a propensity to copy the input is learned early in the training process consistently across all datasets studied. On the other hand, factual errors, such as hallucination of unsupported facts, are learnt in the later stages, though this behavior is more varied across domains. Based on these observations, we explore complementary approaches for modifying training: first, disregarding high-loss tokens that are challenging to learn and second, disregarding low-loss tokens that are learnt very quickly in the latter stages of the training process. We show that these simple training modifications allow us to configure our model to achieve different goals, such as improving factuality or improving abstractiveness.</abstract>
      <url hash="47a0b4e3">2022.findings-acl.163</url>
      <bibkey>goyal-etal-2022-training</bibkey>
    </paper>
    <paper id="164">
      <title>Richer Countries and Richer Representations</title>
      <author><first>Kaitlyn</first><last>Zhou</last></author>
      <author><first>Kawin</first><last>Ethayarajh</last></author>
      <author><first>Dan</first><last>Jurafsky</last></author>
      <pages>2074-2085</pages>
      <abstract>We examine whether some countries are more richly represented in embedding space than others. We find that countries whose names occur with low frequency in training corpora are more likely to be tokenized into subwords, are less semantically distinct in embedding space, and are less likely to be correctly predicted: e.g., Ghana (the correct answer and in-vocabulary) is not predicted for, “The country producing the most cocoa is [MASK].”. Although these performance discrepancies and representational harms are due to frequency, we find that frequency is highly correlated with a country’s GDP; thus perpetuating historic power and wealth inequalities. We analyze the effectiveness of mitigation strategies; recommend that researchers report training word frequencies; and recommend future work for the community to define and design representational guarantees.</abstract>
      <url hash="e7c3f5c9">2022.findings-acl.164</url>
      <bibkey>zhou-etal-2022-richer</bibkey>
      <pwccode url="https://github.com/katezhou/country_distortions" additional="false">katezhou/country_distortions</pwccode>
    </paper>
    <paper id="165">
      <title><fixed-case>BBQ</fixed-case>: A hand-built bias benchmark for question answering</title>
      <author><first>Alicia</first><last>Parrish</last></author>
      <author><first>Angelica</first><last>Chen</last></author>
      <author><first>Nikita</first><last>Nangia</last></author>
      <author><first>Vishakh</first><last>Padmakumar</last></author>
      <author><first>Jason</first><last>Phang</last></author>
      <author><first>Jana</first><last>Thompson</last></author>
      <author><first>Phu Mon</first><last>Htut</last></author>
      <author><first>Samuel</first><last>Bowman</last></author>
      <pages>2086-2105</pages>
      <abstract>It is well documented that NLP models learn social biases, but little work has been done on how these biases manifest in model outputs for applied tasks like question answering (QA). We introduce the Bias Benchmark for QA (BBQ), a dataset of question-sets constructed by the authors that highlight attested social biases against people belonging to protected classes along nine social dimensions relevant for U.S. English-speaking contexts. Our task evaluate model responses at two levels: (i) given an under-informative context, we test how strongly responses reflect social biases, and (ii) given an adequately informative context, we test whether the model’s biases override a correct answer choice. We find that models often rely on stereotypes when the context is under-informative, meaning the model’s outputs consistently reproduce harmful biases in this setting. Though models are more accurate when the context provides an informative answer, they still rely on stereotypes and average up to 3.4 percentage points higher accuracy when the correct answer aligns with a social bias than when it conflicts, with this difference widening to over 5 points on examples targeting gender for most models tested.</abstract>
      <url hash="a354af36">2022.findings-acl.165</url>
      <bibkey>parrish-etal-2022-bbq</bibkey>
      <pwccode url="https://github.com/nyu-mll/bbq" additional="false">nyu-mll/bbq</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bbq">BBQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
    </paper>
    <paper id="166">
      <title>Zero-shot Learning for Grapheme to Phoneme Conversion with Language Ensemble</title>
      <author><first>Xinjian</first><last>Li</last></author>
      <author><first>Florian</first><last>Metze</last></author>
      <author><first>David</first><last>Mortensen</last></author>
      <author><first>Shinji</first><last>Watanabe</last></author>
      <author><first>Alan</first><last>Black</last></author>
      <pages>2106-2115</pages>
      <abstract>Grapheme-to-Phoneme (G2P) has many applications in NLP and speech fields. Most existing work focuses heavily on languages with abundant training datasets, which limits the scope of target languages to less than 100 languages. This work attempts to apply zero-shot learning to approximate G2P models for all low-resource and endangered languages in Glottolog (about 8k languages). For any unseen target language, we first build the phylogenetic tree (i.e. language family tree) to identify top-<tex-math>k</tex-math> nearest languages for which we have training sets. Then we run models of those languages to obtain a hypothesis set, which we combine into a confusion network to propose a most likely hypothesis as an approximation to the target language. We test our approach on over 600 unseen languages and demonstrate it significantly outperforms baselines.</abstract>
      <url hash="1ce8da67">2022.findings-acl.166</url>
      <bibkey>li-etal-2022-zero</bibkey>
      <pwccode url="https://github.com/xinjli/transphone" additional="false">xinjli/transphone</pwccode>
    </paper>
    <paper id="167">
      <title>Dim Wihl Gat Tun: <fixed-case>T</fixed-case>he Case for Linguistic Expertise in <fixed-case>NLP</fixed-case> for Under-Documented Languages</title>
      <author><first>Clarissa</first><last>Forbes</last></author>
      <author><first>Farhan</first><last>Samir</last></author>
      <author><first>Bruce</first><last>Oliver</last></author>
      <author><first>Changbing</first><last>Yang</last></author>
      <author><first>Edith</first><last>Coates</last></author>
      <author><first>Garrett</first><last>Nicolai</last></author>
      <author><first>Miikka</first><last>Silfverberg</last></author>
      <pages>2116-2130</pages>
      <abstract>Recent progress in NLP is driven by pretrained models leveraging massive datasets and has predominantly benefited the world’s political and economic superpowers. Technologically underserved languages are left behind because they lack such resources. Hundreds of underserved languages, nevertheless, have available data sources in the form of interlinear glossed text (IGT) from language documentation efforts. IGT remains underutilized in NLP work, perhaps because its annotations are only semi-structured and often language-specific. With this paper, we make the case that IGT data can be leveraged successfully provided that target language expertise is available. We specifically advocate for collaboration with documentary linguists. Our paper provides a roadmap for successful projects utilizing IGT data: (1) It is essential to define which NLP tasks can be accomplished with the given IGT data and how these will benefit the speech community. (2) Great care and target language expertise is required when converting the data into structured formats commonly employed in NLP. (3) Task-specific and user-specific evaluation can help to ascertain that the tools which are created benefit the target language speech community. We illustrate each step through a case study on developing a morphological reinflection system for the Tsimchianic language Gitksan.</abstract>
      <url hash="32ecd05a">2022.findings-acl.167</url>
      <bibkey>forbes-etal-2022-dim</bibkey>
    </paper>
    <paper id="168">
      <title>Question Generation for Reading Comprehension Assessment by Modeling How and What to Ask</title>
      <author><first>Bilal</first><last>Ghanem</last></author>
      <author><first>Lauren</first><last>Lutz Coleman</last></author>
      <author><first>Julia</first><last>Rivard Dexter</last></author>
      <author><first>Spencer</first><last>von der Ohe</last></author>
      <author><first>Alona</first><last>Fyshe</last></author>
      <pages>2131-2146</pages>
      <abstract>Reading is integral to everyday life, and yet learning to read is a struggle for many young learners. During lessons, teachers can use comprehension questions to increase engagement, test reading skills, and improve retention. Historically such questions were written by skilled teachers, but recently language models have been used to generate comprehension questions. However, many existing Question Generation (QG) systems focus on generating extractive questions from the text, and have no way to control the type of the generated question. In this paper, we study QG for reading comprehension where inferential questions are critical and extractive techniques cannot be used. We propose a two-step model (HTA-WTA) that takes advantage of previous datasets, and can generate questions for a specific targeted comprehension skill. We propose a new reading comprehension dataset that contains questions annotated with story-based reading comprehension skills (SBRCS), allowing for a more complete reader assessment. Across several experiments, our results show that HTA-WTA outperforms multiple strong baselines on this new dataset. We show that the HTA-WTA model tests for strong SCRS by asking deep inferential questions.</abstract>
      <url hash="797336f3">2022.findings-acl.168</url>
      <bibkey>ghanem-etal-2022-question</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cosmosqa">CosmosQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="169">
      <title><fixed-case>TAB</fixed-case>i: <fixed-case>T</fixed-case>ype-Aware Bi-Encoders for Open-Domain Entity Retrieval</title>
      <author><first>Megan</first><last>Leszczynski</last></author>
      <author><first>Daniel</first><last>Fu</last></author>
      <author><first>Mayee</first><last>Chen</last></author>
      <author><first>Christopher</first><last>Re</last></author>
      <pages>2147-2166</pages>
      <abstract>Entity retrieval—retrieving information about entity mentions in a query—is a key step in open-domain tasks, such as question answering or fact checking. However, state-of-the-art entity retrievers struggle to retrieve rare entities for ambiguous mentions due to biases towards popular entities. Incorporating knowledge graph types during training could help overcome popularity biases, but there are several challenges: (1) existing type-based retrieval methods require mention boundaries as input, but open-domain tasks run on unstructured text, (2) type-based methods should not compromise overall performance, and (3) type-based methods should be robust to noisy and missing types. In this work, we introduce TABi, a method to jointly train bi-encoders on knowledge graph types and unstructured text for entity retrieval for open-domain tasks. TABi leverages a type-enforced contrastive loss to encourage entities and queries of similar types to be close in the embedding space. TABi improves retrieval of rare entities on the Ambiguous Entity Retrieval (AmbER) sets, while maintaining strong overall retrieval performance on open-domain tasks in the KILT benchmark compared to state-of-the-art retrievers. TABi is also robust to incomplete type systems, improving rare entity retrieval over baselines with only 5% type coverage of the training dataset. We make our code publicly available.</abstract>
      <url hash="65676acf">2022.findings-acl.169</url>
      <bibkey>leszczynski-etal-2022-tabi</bibkey>
      <pwccode url="https://github.com/hazyresearch/tabi" additional="false">hazyresearch/tabi</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/figer">FIGER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/kilt">KILT</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
    </paper>
    <paper id="170">
      <title>Hierarchical Recurrent Aggregative Generation for Few-Shot <fixed-case>NLG</fixed-case></title>
      <author><first>Giulio</first><last>Zhou</last></author>
      <author><first>Gerasimos</first><last>Lampouras</last></author>
      <author><first>Ignacio</first><last>Iacobacci</last></author>
      <pages>2167-2181</pages>
      <abstract>Large pretrained models enable transfer learning to low-resource domains for language generation tasks. However, previous end-to-end approaches do not account for the fact that some generation sub-tasks, specifically aggregation and lexicalisation, can benefit from transfer learning in different extents. To exploit these varying potentials for transfer learning, we propose a new hierarchical approach for few-shot and zero-shot generation. Our approach consists of a three-moduled jointly trained architecture: the first module independently lexicalises the distinct units of information in the input as sentence sub-units (e.g. phrases), the second module recurrently aggregates these sub-units to generate a unified intermediate output, while the third module subsequently post-edits it to generate a coherent and fluent final text. We perform extensive empirical analysis and ablation studies on few-shot and zero-shot settings across 4 datasets. Automatic and human evaluation shows that the proposed hierarchical approach is consistently capable of achieving state-of-the-art results when compared to previous work.</abstract>
      <url hash="108609b2">2022.findings-acl.170</url>
      <bibkey>zhou-etal-2022-hierarchical</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/sgd">SGD</pwcdataset>
    </paper>
    <paper id="171">
      <title>Training Text-to-Text Transformers with Privacy Guarantees</title>
      <author><first>Natalia</first><last>Ponomareva</last></author>
      <author><first>Jasmijn</first><last>Bastings</last></author>
      <author><first>Sergei</first><last>Vassilvitskii</last></author>
      <pages>2182-2193</pages>
      <abstract>Recent advances in NLP often stem from large transformer-based pre-trained models, which rapidly grow in size and use more and more training data. Such models are often released to the public so that end users can fine-tune them on a task dataset. While it is common to treat pre-training data as public, it may still contain personally identifiable information (PII), such as names, phone numbers, and copyrighted material. Recent findings show that the capacity of these models allows them to memorize parts of the training data, and suggest differentially private (DP) training as a potential mitigation. While there is recent work on DP fine-tuning of NLP models, the effects of DP pre-training are less well understood: it is not clear how downstream performance is affected by DP pre-training, and whether DP pre-training mitigates some of the memorization concerns. We focus on T5 and show that by using recent advances in JAX and XLA we can train models with DP that do not suffer a large drop in pre-training utility, nor in training speed, and can still be fine-tuned to high accuracies on downstream tasks (e.g. GLUE). Moreover, we show that T5’s span corruption is a good defense against data memorization.</abstract>
      <url hash="cc2febb1">2022.findings-acl.171</url>
      <bibkey>ponomareva-etal-2022-training</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/c4">C4</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
    </paper>
    <paper id="172">
      <title>Revisiting Uncertainty-based Query Strategies for Active Learning with Transformers</title>
      <author><first>Christopher</first><last>Schröder</last></author>
      <author><first>Andreas</first><last>Niekler</last></author>
      <author><first>Martin</first><last>Potthast</last></author>
      <pages>2194-2203</pages>
      <abstract>Active learning is the iterative construction of a classification model through targeted labeling, enabling significant labeling cost savings. As most research on active learning has been carried out before transformer-based language models (“transformers”) became popular, despite its practical importance, comparably few papers have investigated how transformers can be combined with active learning to date. This can be attributed to the fact that using state-of-the-art query strategies for transformers induces a prohibitive runtime overhead, which effectively nullifies, or even outweighs the desired cost savings. For this reason, we revisit uncertainty-based query strategies, which had been largely outperformed before, but are particularly suited in the context of fine-tuning transformers. In an extensive evaluation, we connect transformers to experiments from previous research, assessing their performance on five widely used text classification benchmarks. For active learning with transformers, several other uncertainty-based approaches outperform the well-known prediction entropy query strategy, thereby challenging its status as most popular uncertainty baseline in active learning for text classification.</abstract>
      <url hash="a7a75b3f">2022.findings-acl.172</url>
      <bibkey>schroder-etal-2022-revisiting</bibkey>
      <pwccode url="https://github.com/webis-de/acl-22" additional="false">webis-de/acl-22</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mr">MR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/subj">SUBJ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/trec-10">TREC-10</pwcdataset>
    </paper>
    <paper id="173">
      <title>The impact of lexical and grammatical processing on generating code from natural language</title>
      <author><first>Nathanaël</first><last>Beau</last></author>
      <author><first>Benoit</first><last>Crabbé</last></author>
      <pages>2204-2214</pages>
      <abstract>Considering the seq2seq architecture of Yin and Neubig (2018) for natural language to code translation, we identify four key components of importance: grammatical constraints, lexical preprocessing, input representations, and copy mechanisms. To study the impact of these components, we use a state-of-the-art architecture that relies on BERT encoder and a grammar-based decoder for which a formalization is provided. The paper highlights the importance of the lexical substitution component in the current natural language to code systems.</abstract>
      <url hash="28af5fc0">2022.findings-acl.173</url>
      <bibkey>beau-crabbe-2022-impact</bibkey>
      <pwccode url="https://gitlab.com/codegenfact/BertranX" additional="false">codegenfact/BertranX</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conala">CoNaLa</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/django">Django</pwcdataset>
    </paper>
    <paper id="174">
      <title><fixed-case>S</fixed-case>eq2<fixed-case>P</fixed-case>ath: Generating Sentiment Tuples as Paths of a Tree</title>
      <author><first>Yue</first><last>Mao</last></author>
      <author><first>Yi</first><last>Shen</last></author>
      <author><first>Jingchao</first><last>Yang</last></author>
      <author><first>Xiaoying</first><last>Zhu</last></author>
      <author><first>Longjun</first><last>Cai</last></author>
      <pages>2215-2225</pages>
      <abstract>Aspect-based sentiment analysis (ABSA) tasks aim to extract sentiment tuples from a sentence. Recent generative methods such as Seq2Seq models have achieved good performance by formulating the output as a sequence of sentiment tuples. However, the orders between the sentiment tuples do not naturally exist and the generation of the current tuple should not condition on the previous ones. In this paper, we propose Seq2Path to generate sentiment tuples as paths of a tree. A tree can represent “1-to-n” relations (e.g., an aspect term may correspond to multiple opinion terms) and the paths of a tree are independent and do not have orders. For training, we treat each path as an independent target, and we calculate the average loss of the ordinary Seq2Seq model over paths. For inference, we apply beam search with constrained decoding. By introducing an additional discriminative token and applying a data augmentation technique, valid paths can be automatically selected. We conduct experiments on five tasks including AOPE, ASTE, TASD, UABSA, ACOS. We evaluate our method on four common benchmark datasets including Laptop14, Rest14, Rest15, Rest16. Our proposed method achieves state-of-the-art results in almost all cases.</abstract>
      <url hash="cce06e84">2022.findings-acl.174</url>
      <attachment type="software" hash="85d89133">2022.findings-acl.174.software.zip</attachment>
      <bibkey>mao-etal-2022-seq2path</bibkey>
    </paper>
    <paper id="175">
      <title>Mitigating the Inconsistency Between Word Saliency and Model Confidence with Pathological Contrastive Training</title>
      <author><first>Pengwei</first><last>Zhan</last></author>
      <author><first>Yang</first><last>Wu</last></author>
      <author><first>Shaolei</first><last>Zhou</last></author>
      <author><first>Yunjian</first><last>Zhang</last></author>
      <author><first>Liming</first><last>Wang</last></author>
      <pages>2226-2244</pages>
      <abstract>Neural networks are widely used in various NLP tasks for their remarkable performance. However, the complexity makes them difficult to interpret, i.e., they are not guaranteed right for the right reason. Besides the complexity, we reveal that the model pathology - the inconsistency between word saliency and model confidence, further hurts the interpretability. We show that the pathological inconsistency is caused by the representation collapse issue, which means that the representation of the sentences with tokens in different saliency reduced is somehow collapsed, and thus the important words cannot be distinguished from unimportant words in terms of model confidence changing. In this paper, to mitigate the pathology and obtain more interpretable models, we propose Pathological Contrastive Training (PCT) framework, which adopts contrastive learning and saliency-based samples augmentation to calibrate the sentences representation. Combined with qualitative analysis, we also conduct extensive quantitative experiments and measure the interpretability with eight reasonable metrics. Experiments show that our method can mitigate the model pathology and generate more interpretable models while keeping the model performance. Ablation study also shows the effectiveness.</abstract>
      <url hash="cd5c81fa">2022.findings-acl.175</url>
      <attachment type="software" hash="62374b66">2022.findings-acl.175.software.zip</attachment>
      <bibkey>zhan-etal-2022-mitigating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="176">
      <title>Your fairness may vary: Pretrained language model fairness in toxic text classification</title>
      <author><first>Ioana</first><last>Baldini</last></author>
      <author><first>Dennis</first><last>Wei</last></author>
      <author><first>Karthikeyan</first><last>Natesan Ramamurthy</last></author>
      <author><first>Moninder</first><last>Singh</last></author>
      <author><first>Mikhail</first><last>Yurochkin</last></author>
      <pages>2245-2262</pages>
      <abstract>The popularity of pretrained language models in natural language processing systems calls for a careful evaluation of such models in down-stream tasks, which have a higher potential for societal impact. The evaluation of such systems usually focuses on accuracy measures. Our findings in this paper call for attention to be paid to fairness measures as well. Through the analysis of more than a dozen pretrained language models of varying sizes on two toxic text classification tasks (English), we demonstrate that focusing on accuracy measures alone can lead to models with wide variation in fairness characteristics. Specifically, we observe that fairness can vary even more than accuracy with increasing training data size and different random initializations. At the same time, we find that little of the fairness variation is explained by model size, despite claims in the literature. To improve model fairness without retraining, we show that two post-processing methods developed for structured, tabular data can be successfully applied to a range of pretrained language models. Warning: This paper contains samples of offensive text.</abstract>
      <url hash="08332a7e">2022.findings-acl.176</url>
      <bibkey>baldini-etal-2022-fairness</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/hatexplain">HateXplain</pwcdataset>
    </paper>
    <paper id="177">
      <title><fixed-case>C</fixed-case>hart<fixed-case>QA</fixed-case>: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning</title>
      <author><first>Ahmed</first><last>Masry</last></author>
      <author><first>Do</first><last>Long</last></author>
      <author><first>Jia Qing</first><last>Tan</last></author>
      <author><first>Shafiq</first><last>Joty</last></author>
      <author><first>Enamul</first><last>Hoque</last></author>
      <pages>2263-2279</pages>
      <abstract>Charts are very popular for analyzing data. When exploring charts, people often ask a variety of complex reasoning questions that involve several logical and arithmetic operations. They also commonly refer to visual features of a chart in their questions. However, most existing datasets do not focus on such complex reasoning questions as their questions are template-based and answers come from a fixed-vocabulary. In this work, we present a large-scale benchmark covering 9.6K human-written questions as well as 23.1K questions generated from human-written chart summaries. To address the unique challenges in our benchmark involving visual and logical reasoning over charts, we present two transformer-based models that combine visual features and the data table of the chart in a unified way to answer questions. While our models achieve the state-of-the-art results on the previous datasets as well as on our benchmark, the evaluation also reveals several challenges in answering complex reasoning questions.</abstract>
      <url hash="9d0a6c50">2022.findings-acl.177</url>
      <bibkey>masry-etal-2022-chartqa</bibkey>
      <pwccode url="https://github.com/vis-nlp/chartqa" additional="false">vis-nlp/chartqa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/dvqa">DVQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/figureqa">FigureQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/leaf-qa">LEAF-QA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/plotqa">PlotQA</pwcdataset>
    </paper>
    <paper id="178">
      <title>A Novel Perspective to Look At Attention: Bi-level Attention-based Explainable Topic Modeling for News Classification</title>
      <author><first>Dairui</first><last>Liu</last></author>
      <author><first>Derek</first><last>Greene</last></author>
      <author><first>Ruihai</first><last>Dong</last></author>
      <pages>2280-2290</pages>
      <abstract>Many recent deep learning-based solutions have adopted the attention mechanism in various tasks in the field of NLP. However, the inherent characteristics of deep learning models and the flexibility of the attention mechanism increase the models’ complexity, thus leading to challenges in model explainability. To address this challenge, we propose a novel practical framework by utilizing a two-tier attention architecture to decouple the complexity of explanation and the decision-making process. We apply it in the context of a news article classification task. The experiments on two large-scaled news corpora demonstrate that the proposed model can achieve competitive performance with many state-of-the-art alternatives and illustrate its appropriateness from an explainability perspective. We release the source code here.</abstract>
      <url hash="dcddf4d9">2022.findings-acl.178</url>
      <bibkey>liu-etal-2022-novel</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mind">MIND</pwcdataset>
    </paper>
    <paper id="179">
      <title>Learn and Review: Enhancing Continual Named Entity Recognition via Reviewing Synthetic Samples</title>
      <author><first>Yu</first><last>Xia</last></author>
      <author><first>Quan</first><last>Wang</last></author>
      <author><first>Yajuan</first><last>Lyu</last></author>
      <author><first>Yong</first><last>Zhu</last></author>
      <author><first>Wenhao</first><last>Wu</last></author>
      <author><first>Sujian</first><last>Li</last></author>
      <author><first>Dai</first><last>Dai</last></author>
      <pages>2291-2300</pages>
      <abstract>Traditional methods for named entity recognition (NER) classify mentions into a fixed set of pre-defined entity types. However, in many real-world scenarios, new entity types are incrementally involved. To investigate this problem, continual learning is introduced for NER. However, the existing method depends on the relevance between tasks and is prone to inter-type confusion.In this paper, we propose a novel two-stage framework Learn-and-Review (L&amp;R) for continual NER under the type-incremental setting to alleviate the above issues.Specifically, for the learning stage, we distill the old knowledge from teacher to a student on the current dataset. For the reviewing stage, we first generate synthetic samples of old types to augment the dataset. Then, we further distill new knowledge from the above student and old knowledge from the teacher to get an enhanced student on the augmented dataset. This stage has the following advantages: (1) The synthetic samples mitigate the gap between the old and new task and thus enhance the further distillation; (2) Different types of entities are jointly seen during training which alleviates the inter-type confusion. Experimental results show that L&amp;R outperforms the state-of-the-art method on CoNLL-03 and OntoNotes-5.0.</abstract>
      <url hash="2b1f451d">2022.findings-acl.179</url>
      <bibkey>xia-etal-2022-learn</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
    </paper>
    <paper id="180">
      <title>Phoneme transcription of endangered languages: an evaluation of recent <fixed-case>ASR</fixed-case> architectures in the single speaker scenario</title>
      <author><first>Gilles</first><last>Boulianne</last></author>
      <pages>2301-2308</pages>
      <abstract>Transcription is often reported as the bottleneck in endangered language documentation, requiring large efforts from scarce speakers and transcribers. In general, automatic speech recognition (ASR) can be accurate enough to accelerate transcription only if trained on large amounts of transcribed data. However, when a single speaker is involved, several studies have reported encouraging results for phonetic transcription even with small amounts of training. Here we expand this body of work on speaker-dependent transcription by comparing four ASR approaches, notably recent transformer and pretrained multilingual models, on a common dataset of 11 languages. To automate data preparation, training and evaluation steps, we also developed a phoneme recognition setup which handles morphologically complex languages and writing systems for which no pronunciation dictionary exists.We find that fine-tuning a multilingual pretrained model yields an average phoneme error rate (PER) of 15% for 6 languages with 99 minutes or less of transcribed data for training. For the 5 languages with between 100 and 192 minutes of training, we achieved a PER of 8.4% or less. These results on a number of varied languages suggest that ASR can now significantly reduce transcription efforts in the speaker-dependent situation common in endangered language work.</abstract>
      <url hash="60fa5a43">2022.findings-acl.180</url>
      <bibkey>boulianne-2022-phoneme</bibkey>
    </paper>
    <paper id="181">
      <title>Does <fixed-case>BERT</fixed-case> really agree ? Fine-grained Analysis of Lexical Dependence on a Syntactic Task</title>
      <author><first>Karim</first><last>Lasri</last></author>
      <author><first>Alessandro</first><last>Lenci</last></author>
      <author><first>Thierry</first><last>Poibeau</last></author>
      <pages>2309-2315</pages>
      <abstract>Although transformer-based Neural Language Models demonstrate impressive performance on a variety of tasks, their generalization abilities are not well understood. They have been shown to perform strongly on subject-verb number agreement in a wide array of settings, suggesting that they learned to track syntactic dependencies during their training even without explicit supervision. In this paper, we examine the extent to which BERT is able to perform lexically-independent subject-verb number agreement (NA) on targeted syntactic templates. To do so, we disrupt the lexical patterns found in naturally occurring stimuli for each targeted structure in a novel fine-grained analysis of BERT’s behavior. Our results on nonce sentences suggest that the model generalizes well for simple templates, but fails to perform lexically-independent syntactic generalization when as little as one attractor is present.</abstract>
      <url hash="77797ea1">2022.findings-acl.181</url>
      <bibkey>lasri-etal-2022-bert</bibkey>
    </paper>
    <paper id="182">
      <title>Combining Static and Contextualised Multilingual Embeddings</title>
      <author><first>Katharina</first><last>Hämmerl</last></author>
      <author><first>Jindřich</first><last>Libovický</last></author>
      <author><first>Alexander</first><last>Fraser</last></author>
      <pages>2316-2329</pages>
      <abstract>Static and contextual multilingual embeddings have complementary strengths. Static embeddings, while less expressive than contextual language models, can be more straightforwardly aligned across multiple languages. We combine the strengths of static and contextual models to improve multilingual representations. We extract static embeddings for 40 languages from XLM-R, validate those embeddings with cross-lingual word retrieval, and then align them using VecMap. This results in high-quality, highly multilingual static embeddings. Then we apply a novel continued pre-training approach to XLM-R, leveraging the high quality alignment of our static embeddings to better align the representation space of XLM-R. We show positive results for multiple complex semantic tasks. We release the static embeddings and the continued pre-training code. Unlike most previous work, our continued pre-training approach does not require parallel text.</abstract>
      <url hash="3bea225f">2022.findings-acl.182</url>
      <attachment type="software" hash="f8f1075d">2022.findings-acl.182.software.zip</attachment>
      <bibkey>hammerl-etal-2022-combining</bibkey>
      <pwccode url="https://github.com/kathyhaem/combining-static-contextual" additional="false">kathyhaem/combining-static-contextual</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/tydi-qa">TyDi QA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xquad">XQuAD</pwcdataset>
    </paper>
    <paper id="183">
      <title>An Accurate Unsupervised Method for Joint Entity Alignment and Dangling Entity Detection</title>
      <author><first>Shengxuan</first><last>Luo</last></author>
      <author><first>Sheng</first><last>Yu</last></author>
      <pages>2330-2339</pages>
      <abstract>Knowledge graph integration typically suffers from the widely existing dangling entities that cannot find alignment cross knowledge graphs (KGs). The dangling entity set is unavailable in most real-world scenarios, and manually mining the entity pairs that consist of entities with the same meaning is labor-consuming. In this paper, we propose a novel accurate Unsupervised method for joint Entity alignment (EA) and Dangling entity detection (DED), called UED. The UED mines the literal semantic information to generate pseudo entity pairs and globally guided alignment information for EA and then utilizes the EA results to assist the DED. We construct a medical cross-lingual knowledge graph dataset, MedED, providing data for both the EA and DED tasks. Extensive experiments demonstrate that in the EA task, UED achieves EA results comparable to those of state-of-the-art supervised EA baselines and outperforms the current state-of-the-art EA methods by combining supervised EA data. For the DED task, UED obtains high-quality results without supervision.</abstract>
      <url hash="f72e3ebf">2022.findings-acl.183</url>
      <attachment type="software" hash="68be3234">2022.findings-acl.183.software.zip</attachment>
      <bibkey>luo-yu-2022-accurate</bibkey>
      <pwccode url="https://github.com/luosx18/ued" additional="false">luosx18/ued</pwccode>
    </paper>
    <paper id="184">
      <title>Square One Bias in <fixed-case>NLP</fixed-case>: Towards a Multi-Dimensional Exploration of the Research Manifold</title>
      <author><first>Sebastian</first><last>Ruder</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>2340-2354</pages>
      <abstract>The prototypical NLP experiment trains a standard architecture on labeled English data and optimizes for accuracy, without accounting for other dimensions such as fairness, interpretability, or computational efficiency. We show through a manual classification of recent NLP research papers that this is indeed the case and refer to it as the square one experimental setup. We observe that NLP research often goes beyond the square one setup, e.g, focusing not only on accuracy, but also on fairness or interpretability, but typically only along a single dimension. Most work targeting multilinguality, for example, considers only accuracy; most work on fairness or interpretability considers only English; and so on. Such one-dimensionality of most research means we are only exploring a fraction of the NLP research search space. We provide historical and recent examples of how the square one bias has led researchers to draw false conclusions or make unwise choices, point to promising yet unexplored directions on the research manifold, and make practical recommendations to enable more multi-dimensional research. We open-source the results of our annotations to enable further analysis.</abstract>
      <url hash="291713f7">2022.findings-acl.184</url>
      <bibkey>ruder-etal-2022-square</bibkey>
    </paper>
    <paper id="185">
      <title>Systematicity, Compositionality and Transitivity of Deep <fixed-case>NLP</fixed-case> Models: a Metamorphic Testing Perspective</title>
      <author><first>Edoardo</first><last>Manino</last></author>
      <author><first>Julia</first><last>Rozanova</last></author>
      <author><first>Danilo</first><last>Carvalho</last></author>
      <author><first>Andre</first><last>Freitas</last></author>
      <author><first>Lucas</first><last>Cordeiro</last></author>
      <pages>2355-2366</pages>
      <abstract>Metamorphic testing has recently been used to check the safety of neural NLP models. Its main advantage is that it does not rely on a ground truth to generate test cases. However, existing studies are mostly concerned with robustness-like metamorphic relations, limiting the scope of linguistic properties they can test. We propose three new classes of metamorphic relations, which address the properties of systematicity, compositionality and transitivity. Unlike robustness, our relations are defined over multiple source inputs, thus increasing the number of test cases that we can produce by a polynomial factor. With them, we test the internal consistency of state-of-the-art NLP models, and show that they do not always behave according to their expected linguistic properties. Lastly, we introduce a novel graphical notation that efficiently summarises the inner structure of metamorphic relations.</abstract>
      <url hash="93456b9e">2022.findings-acl.185</url>
      <attachment type="software" hash="c85e0fb0">2022.findings-acl.185.software.zip</attachment>
      <bibkey>manino-etal-2022-systematicity</bibkey>
    </paper>
    <paper id="186">
      <title>Improving Neural Political Statement Classification with Class Hierarchical Information</title>
      <author><first>Erenay</first><last>Dayanik</last></author>
      <author><first>Andre</first><last>Blessing</last></author>
      <author><first>Nico</first><last>Blokker</last></author>
      <author><first>Sebastian</first><last>Haunss</last></author>
      <author><first>Jonas</first><last>Kuhn</last></author>
      <author><first>Gabriella</first><last>Lapesa</last></author>
      <author><first>Sebastian</first><last>Pado</last></author>
      <pages>2367-2382</pages>
      <abstract>Many tasks in text-based computational social science (CSS) involve the classification of political statements into categories based on a domain-specific codebook. In order to be useful for CSS analysis, these categories must be fine-grained. The typically skewed distribution of fine-grained categories, however, results in a challenging classification problem on the NLP side. This paper proposes to make use of the hierarchical relations among categories typically present in such codebooks:e.g., markets and taxation are both subcategories of economy, while borders is a subcategory of security. We use these ontological relations as prior knowledge to establish additional constraints on the learned model, thusimproving performance overall and in particular for infrequent categories. We evaluate several lightweight variants of this intuition by extending state-of-the-art transformer-based textclassifiers on two datasets and multiple languages. We find the most consistent improvement for an approach based on regularization.</abstract>
      <url hash="04b06c85">2022.findings-acl.186</url>
      <attachment type="software" hash="3affc077">2022.findings-acl.186.software.zip</attachment>
      <bibkey>dayanik-etal-2022-improving</bibkey>
    </paper>
    <paper id="187">
      <title>Enabling Multimodal Generation on <fixed-case>CLIP</fixed-case> via Vision-Language Knowledge Distillation</title>
      <author><first>Wenliang</first><last>Dai</last></author>
      <author><first>Lu</first><last>Hou</last></author>
      <author><first>Lifeng</first><last>Shang</last></author>
      <author><first>Xin</first><last>Jiang</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <pages>2383-2395</pages>
      <abstract>The recent large-scale vision-language pre-training (VLP) of dual-stream architectures (e.g., CLIP) with a tremendous amount of image-text pair data, has shown its superiority on various multimodal alignment tasks. Despite its success, the resulting models are not capable of multimodal generative tasks due to the weak text encoder. To tackle this problem, we propose to augment the dual-stream VLP model with a textual pre-trained language model (PLM) via vision-language knowledge distillation (VLKD), enabling the capability for multimodal generation. VLKD is pretty data- and computation-efficient compared to the pre-training from scratch. Experimental results show that the resulting model has strong zero-shot performance on multimodal generation tasks, such as open-ended visual question answering and image captioning. For example, it achieves 44.5% zero-shot accuracy on the VQAv2 dataset, surpassing the previous state-of-the-art zero-shot model with <tex-math>7\times</tex-math> fewer parameters. Furthermore, the original textual language understanding and generation ability of the PLM is maintained after VLKD, which makes our model versatile for both multimodal and unimodal tasks.</abstract>
      <url hash="1fb59ca9">2022.findings-acl.187</url>
      <bibkey>dai-etal-2022-enabling</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ok-vqa">OK-VQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/nocaps">nocaps</pwcdataset>
    </paper>
    <paper id="188">
      <title>Co-<fixed-case>VQA</fixed-case> : Answering by Interactive Sub Question Sequence</title>
      <author><first>Ruonan</first><last>Wang</last></author>
      <author><first>Yuxi</first><last>Qian</last></author>
      <author><first>Fangxiang</first><last>Feng</last></author>
      <author><first>Xiaojie</first><last>Wang</last></author>
      <author><first>Huixing</first><last>Jiang</last></author>
      <pages>2396-2408</pages>
      <abstract>Most existing approaches to Visual Question Answering (VQA) answer questions directly, however, people usually decompose a complex question into a sequence of simple sub questions and finally obtain the answer to the original question after answering the sub question sequence(SQS). By simulating the process, this paper proposes a conversation-based VQA (Co-VQA) framework, which consists of three components: Questioner, Oracle, and Answerer. Questioner raises the sub questions using an extending HRED model, and Oracle answers them one-by-one. An Adaptive Chain Visual Reasoning Model (ACVRM) for Answerer is also proposed, where the question-answer pair is used to update the visual representation sequentially. To perform supervised learning for each model, we introduce a well-designed method to build a SQS for each question on VQA 2.0 and VQA-CP v2 datasets. Experimental results show that our method achieves state-of-the-art on VQA-CP v2. Further analyses show that SQSs help build direct semantic connections between questions and images, provide question-adaptive variable-length reasoning chains, and with explicit interpretability as well as error traceability.</abstract>
      <url hash="43bde968">2022.findings-acl.188</url>
      <bibkey>wang-etal-2022-co</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/guesswhat">GuessWhat?!</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-genome">Visual Genome</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering-v2-0">Visual Question Answering v2.0</pwcdataset>
    </paper>
    <paper id="189">
      <title>A Simple Hash-Based Early Exiting Approach For Language Understanding and Generation</title>
      <author><first>Tianxiang</first><last>Sun</last></author>
      <author><first>Xiangyang</first><last>Liu</last></author>
      <author><first>Wei</first><last>Zhu</last></author>
      <author><first>Zhichao</first><last>Geng</last></author>
      <author><first>Lingling</first><last>Wu</last></author>
      <author><first>Yilong</first><last>He</last></author>
      <author><first>Yuan</first><last>Ni</last></author>
      <author><first>Guotong</first><last>Xie</last></author>
      <author><first>Xuanjing</first><last>Huang</last></author>
      <author><first>Xipeng</first><last>Qiu</last></author>
      <pages>2409-2421</pages>
      <abstract>Early exiting allows instances to exit at different layers according to the estimation of difficulty.Previous works usually adopt heuristic metrics such as the entropy of internal outputs to measure instance difficulty, which suffers from generalization and threshold-tuning. In contrast, learning to exit, or learning to predict instance difficulty is a more appealing way. Though some effort has been devoted to employing such “learn-to-exit” modules, it is still unknown whether and how well the instance difficulty can be learned. As a response, we first conduct experiments on the learnability of instance difficulty, which demonstrates that modern neural models perform poorly on predicting instance difficulty. Based on this observation, we propose a simple-yet-effective Hash-based Early Exiting approach HashEE) that replaces the learn-to-exit modules with hash functions to assign each token to a fixed exiting layer. Different from previous methods, HashEE requires no internal classifiers nor extra parameters, and therefore is more efficient.HashEE can be used in various tasks (including language understanding and generation) and model architectures such as seq2seq models. Experimental results on classification, regression, and generation tasks demonstrate that HashEE can achieve higher performance with fewer FLOPs and inference time compared with previous state-of-the-art early exiting methods.</abstract>
      <url hash="9dc7392f">2022.findings-acl.189</url>
      <bibkey>sun-etal-2022-simple</bibkey>
      <pwccode url="https://github.com/txsun1997/hashee" additional="false">txsun1997/hashee</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="190">
      <title>Auxiliary tasks to boost Biaffine Semantic Dependency Parsing</title>
      <author><first>Marie</first><last>Candito</last></author>
      <pages>2422-2429</pages>
      <abstract>The biaffine parser of (CITATION) was successfully extended to semantic dependency parsing (SDP) (CITATION). Its performance on graphs is surprisingly high given that, without the constraint of producing a tree, all arcs for a given sentence are predicted independently from each other (modulo a shared representation of tokens).To circumvent such an independence of decision, while retaining the <tex-math>O(n^2)</tex-math> complexity and highly parallelizable architecture, we propose to use simple auxiliary tasks that introduce some form of interdependence between arcs. Experiments on the three English acyclic datasets of SemEval-2015 task 18 (CITATION), and on French deep syntactic cyclic graphs (CITATION) show modest but systematic performance gains on a near-state-of-the-art baseline using transformer-based contextualized representations. This provides a simple and robust method to boost SDP performance.</abstract>
      <url hash="1c1bbd82">2022.findings-acl.190</url>
      <attachment type="software" hash="fc9f57cf">2022.findings-acl.190.software.tgz</attachment>
      <bibkey>candito-2022-auxiliary</bibkey>
      <pwccode url="https://github.com/mcandito/aux-tasks-biaffine-graph-parser-findingsacl22" additional="false">mcandito/aux-tasks-biaffine-graph-parser-findingsacl22</pwccode>
    </paper>
    <paper id="191">
      <title>Syntax-guided Contrastive Learning for Pre-trained Language Model</title>
      <author><first>Shuai</first><last>Zhang</last></author>
      <author><first>Wang</first><last>Lijie</last></author>
      <author><first>Xinyan</first><last>Xiao</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <pages>2430-2440</pages>
      <abstract>Syntactic information has been proved to be useful for transformer-based pre-trained language models. Previous studies often rely on additional syntax-guided attention components to enhance the transformer, which require more parameters and additional syntactic parsing in downstream tasks. This increase in complexity severely limits the application of syntax-enhanced language model in a wide range of scenarios. In order to inject syntactic knowledge effectively and efficiently into pre-trained language models, we propose a novel syntax-guided contrastive learning method which does not change the transformer architecture. Based on constituency and dependency structures of syntax trees, we design phrase-guided and tree-guided contrastive objectives, and optimize them in the pre-training stage, so as to help the pre-trained language model to capture rich syntactic knowledge in its representations. Experimental results show that our contrastive method achieves consistent improvements in a variety of tasks, including grammatical error detection, entity tasks, structural probing and GLUE. Detailed analysis further verifies that the improvements come from the utilization of syntactic information, and the learned attention weights are more explainable in terms of linguistics.</abstract>
      <url hash="fd00729d">2022.findings-acl.191</url>
      <bibkey>zhang-etal-2022-syntax</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/blimp">BLiMP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fce">FCE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/open-entity-1">Open Entity</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
    </paper>
    <paper id="192">
      <title>Improved Multi-label Classification under Temporal Concept Drift: Rethinking Group-Robust Algorithms in a Label-Wise Setting</title>
      <author><first>Ilias</first><last>Chalkidis</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>2441-2454</pages>
      <abstract>In document classification for, e.g., legal and biomedical text, we often deal with hundreds of classes, including very infrequent ones, as well as temporal concept drift caused by the influence of real world events, e.g., policy changes, conflicts, or pandemics. Class imbalance and drift can sometimes be mitigated by resampling the training data to simulate (or compensate for) a known target distribution, but what if the target distribution is determined by unknown future events? Instead of simply resampling uniformly to hedge our bets, we focus on the underlying optimization algorithms used to train such document classifiers and evaluate several group-robust optimization algorithms, initially proposed to mitigate group-level disparities. Reframing group-robust algorithms as adaptation algorithms under concept drift, we find that Invariant Risk Minimization and Spectral Decoupling outperform sampling-based approaches to class imbalance and concept drift, and lead to much better performance on minority classes. The effect is more pronounced the larger the label set.</abstract>
      <url hash="4bcf8dd4">2022.findings-acl.192</url>
      <attachment type="software" hash="03ea652c">2022.findings-acl.192.software.zip</attachment>
      <bibkey>chalkidis-sogaard-2022-improved</bibkey>
      <pwccode url="https://github.com/coastalcph/lw-robust" additional="false">coastalcph/lw-robust</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bioasq">BioASQ</pwcdataset>
    </paper>
    <paper id="193">
      <title><fixed-case>ASCM</fixed-case>: An Answer Space Clustered Prompting Method without Answer Engineering</title>
      <author><first>Zhen</first><last>Wang</last></author>
      <author><first>Yating</first><last>Yang</last></author>
      <author><first>Zhou</first><last>Xi</last></author>
      <author><first>Bo</first><last>Ma</last></author>
      <author><first>Lei</first><last>Wang</last></author>
      <author><first>Rui</first><last>Dong</last></author>
      <author><first>Azmat</first><last>Anwar</last></author>
      <pages>2455-2469</pages>
      <abstract>Prompt-based learning, which exploits knowledge from pre-trained language models by providing textual prompts and designing appropriate answer-category mapping methods, has achieved impressive successes on few-shot text classification and natural language inference (NLI). Because of the diverse linguistic expression, there exist many answer tokens for the same category. However, both manual answer design and automatic answer search constrain answer space and therefore hardly achieve ideal performance. To address this issue, we propose an answer space clustered prompting model (ASCM) together with a synonym initialization method (SI) which automatically categorizes all answer tokens in a semantic-clustered embedding space. We also propose a stable semi-supervised method named stair learning (SL) that orderly distills knowledge from better models to weaker models. Extensive experiments demonstrate that our ASCM+SL significantly outperforms existing state-of-the-art techniques in few-shot settings.</abstract>
      <url hash="27a6ba4f">2022.findings-acl.193</url>
      <bibkey>wang-etal-2022-ascm</bibkey>
      <pwccode url="https://github.com/miaomiao1215/ascm" additional="false">miaomiao1215/ascm</pwccode>
    </paper>
    <paper id="194">
      <title>Why don’t people use character-level machine translation?</title>
      <author><first>Jindřich</first><last>Libovický</last></author>
      <author><first>Helmut</first><last>Schmid</last></author>
      <author><first>Alexander</first><last>Fraser</last></author>
      <pages>2470-2485</pages>
      <abstract>We present a literature and empirical survey that critically assesses the state of the art in character-level modeling for machine translation (MT). Despite evidence in the literature that character-level systems are comparable with subword systems, they are virtually never used in competitive setups in WMT competitions. We empirically show that even with recent modeling innovations in character-level natural language processing, character-level MT systems still struggle to match their subword-based counterparts. Character-level MT systems show neither better domain robustness, nor better morphological generalization, despite being often so motivated. However, we are able to show robustness towards source side noise and that translation quality does not degrade with increasing beam size at decoding time.</abstract>
      <url hash="cb530acb">2022.findings-acl.194</url>
      <attachment type="software" hash="d50242f4">2022.findings-acl.194.software.tgz</attachment>
      <bibkey>libovicky-etal-2022-dont</bibkey>
    </paper>
    <paper id="195">
      <title>Seeking Patterns, Not just Memorizing Procedures: Contrastive Learning for Solving Math Word Problems</title>
      <author><first>Zhongli</first><last>Li</last></author>
      <author><first>Wenxuan</first><last>Zhang</last></author>
      <author><first>Chao</first><last>Yan</last></author>
      <author><first>Qingyu</first><last>Zhou</last></author>
      <author><first>Chao</first><last>Li</last></author>
      <author><first>Hongzhi</first><last>Liu</last></author>
      <author><first>Yunbo</first><last>Cao</last></author>
      <pages>2486-2496</pages>
      <abstract>Math Word Problem (MWP) solving needs to discover the quantitative relationships over natural language narratives. Recent work shows that existing models memorize procedures from context and rely on shallow heuristics to solve MWPs. In this paper, we look at this issue and argue that the cause is a lack of overall understanding of MWP patterns. We first investigate how a neural network understands patterns only from semantics, and observe that, if the prototype equations are the same, most problems get closer representations and those representations apart from them or close to other prototypes tend to produce wrong solutions. Inspired by it, we propose a contrastive learning approach, where the neural network perceives the divergence of patterns. We collect contrastive examples by converting the prototype equation into a tree and seeking similar tree structures. The solving model is trained with an auxiliary objective on the collected examples, resulting in the representations of problems with similar prototypes being pulled closer. We conduct experiments on the Chinese dataset Math23k and the English dataset MathQA. Our method greatly improves the performance in monolingual and multilingual settings.</abstract>
      <url hash="c8f2f130">2022.findings-acl.195</url>
      <bibkey>li-etal-2022-seeking</bibkey>
      <pwccode url="https://github.com/zwx980624/mwp-cl" additional="false">zwx980624/mwp-cl</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/math23k">Math23K</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mathqa">MathQA</pwcdataset>
    </paper>
    <paper id="196">
      <title>x<fixed-case>GQA</fixed-case>: Cross-Lingual Visual Question Answering</title>
      <author><first>Jonas</first><last>Pfeiffer</last></author>
      <author><first>Gregor</first><last>Geigle</last></author>
      <author><first>Aishwarya</first><last>Kamath</last></author>
      <author><first>Jan-Martin</first><last>Steitz</last></author>
      <author><first>Stefan</first><last>Roth</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <pages>2497-2511</pages>
      <abstract>Recent advances in multimodal vision and language modeling have predominantly focused on the English language, mostly due to the lack of multilingual multimodal datasets to steer modeling efforts. In this work, we address this gap and provide xGQA, a new multilingual evaluation benchmark for the visual question answering task. We extend the established English GQA dataset to 7 typologically diverse languages, enabling us to detect and explore crucial challenges in cross-lingual visual question answering. We further propose new adapter-based approaches to adapt multimodal transformer-based models to become multilingual, and—vice versa—multilingual models to become multimodal. Our proposed methods outperform current state-of-the-art multilingual multimodal models (e.g., M3P) in zero-shot cross-lingual settings, but the accuracy remains low across the board; a performance drop of around 38 accuracy points in target languages showcases the difficulty of zero-shot cross-lingual transfer for this task. Our results suggest that simple cross-lingual transfer of multimodal models yields latent multilingual multimodal misalignment, calling for more sophisticated methods for vision and multilingual language modeling.</abstract>
      <url hash="8f6fc9c4">2022.findings-acl.196</url>
      <bibkey>pfeiffer-etal-2022-xgqa</bibkey>
      <pwccode url="https://github.com/adapter-hub/xgqa" additional="false">adapter-hub/xgqa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/gqa">GQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/iglue">IGLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multisubs">MultiSubs</pwcdataset>
    </paper>
    <paper id="197">
      <title>Automatic Speech Recognition and Query By Example for Creole Languages Documentation</title>
      <author><first>Cécile</first><last>Macaire</last></author>
      <author><first>Didier</first><last>Schwab</last></author>
      <author><first>Benjamin</first><last>Lecouteux</last></author>
      <author><first>Emmanuel</first><last>Schang</last></author>
      <pages>2512-2520</pages>
      <abstract>We investigate the exploitation of self-supervised models for two Creole languages with few resources: Gwadloupéyen and Morisien. Automatic language processing tools are almost non-existent for these two languages. We propose to use about one hour of annotated data to design an automatic speech recognition system for each language. We evaluate how much data is needed to obtain a query-by-example system that is usable by linguists. Moreover, our experiments show that multilingual self-supervised models are not necessarily the most efficient for Creole languages.</abstract>
      <url hash="f8c16e05">2022.findings-acl.197</url>
      <bibkey>macaire-etal-2022-automatic</bibkey>
      <pwccode url="https://github.com/macairececile/asr-qbe-creole" additional="false">macairececile/asr-qbe-creole</pwccode>
    </paper>
    <paper id="198">
      <title><fixed-case>MR</fixed-case>e<fixed-case>D</fixed-case>: A Meta-Review Dataset for Structure-Controllable Text Generation</title>
      <author><first>Chenhui</first><last>Shen</last></author>
      <author><first>Liying</first><last>Cheng</last></author>
      <author><first>Ran</first><last>Zhou</last></author>
      <author><first>Lidong</first><last>Bing</last></author>
      <author><first>Yang</first><last>You</last></author>
      <author><first>Luo</first><last>Si</last></author>
      <pages>2521-2535</pages>
      <abstract>When directly using existing text generation datasets for controllable generation, we are facing the problem of not having the domain knowledge and thus the aspects that could be controlled are limited. A typical example is when using CNN/Daily Mail dataset for controllable text summarization, there is no guided information on the emphasis of summary sentences. A more useful text generator should leverage both the input text and the control signal to guide the generation, which can only be built with deep understanding of the domain knowledge. Motivated by this vision, our paper introduces a new text generation dataset, named MReD. Our new dataset consists of 7,089 meta-reviews and all its 45k meta-review sentences are manually annotated with one of the 9 carefully defined categories, including abstract, strength, decision, etc. We present experimental results on start-of-the-art summarization models, and propose methods for structure-controlled generation with both extractive and abstractive models using our annotated data. By exploring various settings and analyzing the model behavior with respect to the control signal, we demonstrate the challenges of our proposed task and the values of our dataset MReD. Meanwhile, MReD also allows us to have a better understanding of the meta-review domain.</abstract>
      <url hash="6f1cec49">2022.findings-acl.198</url>
      <bibkey>shen-etal-2022-mred</bibkey>
      <pwccode url="https://github.com/shen-chenhui/mred" additional="false">shen-chenhui/mred</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
    </paper>
    <paper id="199">
      <title>Single Model Ensemble for Subword Regularized Models in Low-Resource Machine Translation</title>
      <author><first>Sho</first><last>Takase</last></author>
      <author><first>Tatsuya</first><last>Hiraoka</last></author>
      <author><first>Naoaki</first><last>Okazaki</last></author>
      <pages>2536-2541</pages>
      <abstract>Subword regularizations use multiple subword segmentations during training to improve the robustness of neural machine translation models.In previous subword regularizations, we use multiple segmentations in the training process but use only one segmentation in the inference.In this study, we propose an inference strategy to address this discrepancy.The proposed strategy approximates the marginalized likelihood by using multiple segmentations including the most plausible segmentation and several sampled segmentations.Because the proposed strategy aggregates predictions from several segmentations, we can regard it as a single model ensemble that does not require any additional cost for training.Experimental results show that the proposed strategy improves the performance of models trained with subword regularization in low-resource machine translation tasks.</abstract>
      <url hash="16e62edb">2022.findings-acl.199</url>
      <bibkey>takase-etal-2022-single</bibkey>
    </paper>
    <paper id="200">
      <title>Detecting Various Types of Noise for Neural Machine Translation</title>
      <author><first>Christian</first><last>Herold</last></author>
      <author><first>Jan</first><last>Rosendahl</last></author>
      <author><first>Joris</first><last>Vanvinckenroye</last></author>
      <author><first>Hermann</first><last>Ney</last></author>
      <pages>2542-2551</pages>
      <abstract>The filtering and/or selection of training data is one of the core aspects to be considered when building a strong machine translation system.In their influential work, Khayrallah and Koehn (2018) investigated the impact of different types of noise on the performance of machine translation systems.In the same year the WMT introduced a shared task on parallel corpus filtering, which went on to be repeated in the following years, and resulted in many different filtering approaches being proposed.In this work we aim to combine the recent achievements in data filtering with the original analysis of Khayrallah and Koehn (2018) and investigate whether state-of-the-art filtering systems are capable of removing all the suggested noise types.We observe that most of these types of noise can be detected with an accuracy of over 90% by modern filtering systems when operating in a well studied high resource setting.However, we also find that when confronted with more refined noise categories or when working with a less common language pair, the performance of the filtering systems is far from optimal, showing that there is still room for improvement in this area of research.</abstract>
      <url hash="e5c38443">2022.findings-acl.200</url>
      <bibkey>herold-etal-2022-detecting</bibkey>
    </paper>
    <paper id="201">
      <title><fixed-case>DU</fixed-case>-<fixed-case>VLG</fixed-case>: Unifying Vision-and-Language Generation via Dual Sequence-to-Sequence Pre-training</title>
      <author><first>Luyang</first><last>Huang</last></author>
      <author><first>Guocheng</first><last>Niu</last></author>
      <author><first>Jiachen</first><last>Liu</last></author>
      <author><first>Xinyan</first><last>Xiao</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <pages>2552-2566</pages>
      <abstract>Due to the limitations of the model structure and pre-training objectives, existing vision-and-language generation models cannot utilize pair-wise images and text through bi-directional generation. In this paper, we propose DU-VLG, a framework which unifies vision-and-language generation as sequence generation problems. DU-VLG is trained with novel dual pre-training tasks: multi-modal denoising autoencoder tasks and modality translation tasks. To bridge the gap between image understanding and generation, we further design a novel commitment loss. We compare pre-training objectives on image captioning and text-to-image generation datasets. Results show that DU-VLG yields better performance than variants trained with uni-directional generation objectives or the variant without the commitment loss. We also obtain higher scores compared to previous state-of-the-art systems on three vision-and-language generation tasks. In addition, human judges further confirm that our model generates real and relevant images as well as faithful and informative captions.</abstract>
      <url hash="a6c16797">2022.findings-acl.201</url>
      <bibkey>huang-etal-2022-du</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
    </paper>
    <paper id="202">
      <title><fixed-case>H</fixed-case>i<fixed-case>CLRE</fixed-case>: A Hierarchical Contrastive Learning Framework for Distantly Supervised Relation Extraction</title>
      <author><first>Dongyang</first><last>Li</last></author>
      <author><first>Taolin</first><last>Zhang</last></author>
      <author><first>Nan</first><last>Hu</last></author>
      <author><first>Chengyu</first><last>Wang</last></author>
      <author><first>Xiaofeng</first><last>He</last></author>
      <pages>2567-2578</pages>
      <abstract>Distant supervision assumes that any sentence containing the same entity pairs reflects identical relationships. Previous works of distantly supervised relation extraction (DSRE) task generally focus on sentence-level or bag-level de-noising techniques independently, neglecting the explicit interaction with cross levels. In this paper, we propose a hierarchical contrastive learning Framework for Distantly Supervised relation extraction (HiCLRE) to reduce noisy sentences, which integrate the global structural information and local fine-grained interaction. Specifically, we propose a three-level hierarchical learning framework to interact with cross levels, generating the de-noising context-aware representations via adapting the existing multi-head self-attention, named Multi-Granularity Recontextualization. Meanwhile, pseudo positive samples are also provided in the specific level for contrastive learning via a dynamic gradient-based data augmentation strategy, named Dynamic Gradient Adversarial Perturbation. Experiments demonstrate that HiCLRE significantly outperforms strong baselines in various mainstream DSRE datasets.</abstract>
      <url hash="49c2116c">2022.findings-acl.202</url>
      <bibkey>li-etal-2022-hiclre</bibkey>
      <pwccode url="https://github.com/matnlp/hiclre" additional="false">matnlp/hiclre</pwccode>
    </paper>
    <paper id="203">
      <title>Prompt-Driven Neural Machine Translation</title>
      <author><first>Yafu</first><last>Li</last></author>
      <author><first>Yongjing</first><last>Yin</last></author>
      <author><first>Jing</first><last>Li</last></author>
      <author><first>Yue</first><last>Zhang</last></author>
      <pages>2579-2590</pages>
      <abstract>Neural machine translation (NMT) has obtained significant performance improvement over the recent years. However, NMT models still face various challenges including fragility and lack of style flexibility. Moreover, current methods for instance-level constraints are limited in that they are either constraint-specific or model-specific. To this end, we propose prompt-driven neural machine translation to incorporate prompts for enhancing translation control and enriching flexibility. Empirical results demonstrate the effectiveness of our method in both prompt responding and translation quality. Through human evaluation, we further show the flexibility of prompt control and the efficiency in human-in-the-loop translation.</abstract>
      <url hash="2e9cfdd3">2022.findings-acl.203</url>
      <bibkey>li-etal-2022-prompt</bibkey>
      <pwccode url="https://github.com/yafuly/promptnmt" additional="false">yafuly/promptnmt</pwccode>
    </paper>
    <paper id="204">
      <title>On Controlling Fallback Responses for Grounded Dialogue Generation</title>
      <author><first>Hongyuan</first><last>Lu</last></author>
      <author><first>Wai</first><last>Lam</last></author>
      <author><first>Hong</first><last>Cheng</last></author>
      <author><first>Helen</first><last>Meng</last></author>
      <pages>2591-2601</pages>
      <abstract>Dialogue agents can leverage external textual knowledge to generate responses of a higher quality. To our best knowledge, most existing works on knowledge grounded dialogue settings assume that the user intention is always answerable. Unfortunately, this is impractical as there is no guarantee that the knowledge retrievers could always retrieve the desired knowledge. Therefore, this is crucial to incorporate fallback responses to respond to unanswerable contexts appropriately while responding to the answerable contexts in an informative manner. We propose a novel framework that automatically generates a control token with the generator to bias the succeeding response towards informativeness for answerable contexts and fallback for unanswerable contexts in an end-to-end manner. Since no existing knowledge grounded dialogue dataset considers this aim, we augment the existing dataset with unanswerable contexts to conduct our experiments. Automatic and human evaluation results indicate that naively incorporating fallback responses with controlled text generation still hurts informativeness for answerable context. In contrast, our proposed framework effectively mitigates this problem while still appropriately presenting fallback responses to unanswerable contexts. Such a framework also reduces the extra burden of the additional classifier and the overheads introduced in the previous works, which operates in a pipeline manner.</abstract>
      <url hash="5c880298">2022.findings-acl.204</url>
      <attachment type="software" hash="66939f03">2022.findings-acl.204.software.zip</attachment>
      <bibkey>lu-etal-2022-controlling</bibkey>
    </paper>
    <paper id="205">
      <title><fixed-case>CRAFT</fixed-case>: A Benchmark for Causal Reasoning About Forces and in<fixed-case>T</fixed-case>eractions</title>
      <author><first>Tayfun</first><last>Ates</last></author>
      <author><first>M.</first><last>Ateşoğlu</last></author>
      <author><first>Çağatay</first><last>Yiğit</last></author>
      <author><first>Ilker</first><last>Kesen</last></author>
      <author><first>Mert</first><last>Kobas</last></author>
      <author><first>Erkut</first><last>Erdem</last></author>
      <author><first>Aykut</first><last>Erdem</last></author>
      <author><first>Tilbe</first><last>Goksun</last></author>
      <author><first>Deniz</first><last>Yuret</last></author>
      <pages>2602-2627</pages>
      <abstract>Humans are able to perceive, understand and reason about causal events. Developing models with similar physical and causal understanding capabilities is a long-standing goal of artificial intelligence. As a step towards this direction, we introduce CRAFT, a new video question answering dataset that requires causal reasoning about physical forces and object interactions. It contains 58K video and question pairs that are generated from 10K videos from 20 different virtual environments, containing various objects in motion that interact with each other and the scene. Two question categories in CRAFT include previously studied descriptive and counterfactual questions. Additionally, inspired by the Force Dynamics Theory in cognitive linguistics, we introduce a new causal question category that involves understanding the causal interactions between objects through notions like cause, enable, and prevent. Our results show that even though the questions in CRAFT are easy for humans, the tested baseline models, including existing state-of-the-art methods, do not yet deal with the challenges posed in our benchmark.</abstract>
      <url hash="b5c5714a">2022.findings-acl.205</url>
      <bibkey>ates-etal-2022-craft</bibkey>
      <pwccode url="https://github.com/hucvl/craft" additional="false">hucvl/craft</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/clevr">CLEVR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/phyre">PHYRE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tvqa">TVQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tvqa-1">TVQA+</pwcdataset>
    </paper>
    <paper id="206">
      <title>A Graph Enhanced <fixed-case>BERT</fixed-case> Model for Event Prediction</title>
      <author><first>Li</first><last>Du</last></author>
      <author><first>Xiao</first><last>Ding</last></author>
      <author><first>Yue</first><last>Zhang</last></author>
      <author><first>Ting</first><last>Liu</last></author>
      <author><first>Bing</first><last>Qin</last></author>
      <pages>2628-2638</pages>
      <abstract>Predicting the subsequent event for an existing event context is an important but challenging task, as it requires understanding the underlying relationship between events. Previous methods propose to retrieve relational features from event graph to enhance the modeling of event correlation. However, the sparsity of event graph may restrict the acquisition of relevant graph information, and hence influence the model performance. To address this issue, we consider automatically building of event graph using a BERT model. To this end, we incorporate an additional structured variable into BERT to learn to predict the event connections in the training process.Hence, in the test process, the connection relationship for unseen events can be predicted by the structured variable.Results on two event prediction tasks: script event prediction and story ending prediction, show that our approach can outperform state-of-the-art baseline methods.</abstract>
      <url hash="e8511dac">2022.findings-acl.206</url>
      <attachment type="software" hash="03b914e7">2022.findings-acl.206.software.zip</attachment>
      <bibkey>du-etal-2022-graph</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/rocstories">ROCStories</pwcdataset>
    </paper>
    <paper id="207">
      <title>Long Time No See! Open-Domain Conversation with Long-Term Persona Memory</title>
      <author><first>Xinchao</first><last>Xu</last></author>
      <author><first>Zhibin</first><last>Gou</last></author>
      <author><first>Wenquan</first><last>Wu</last></author>
      <author><first>Zheng-Yu</first><last>Niu</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <author><first>Haifeng</first><last>Wang</last></author>
      <author><first>Shihang</first><last>Wang</last></author>
      <pages>2639-2650</pages>
      <abstract>Most of the open-domain dialogue models tend to perform poorly in the setting of long-term human-bot conversations. The possible reason is that they lack the capability of understanding and memorizing long-term dialogue history information. To address this issue, we present a novel task of Long-term Memory Conversation (LeMon) and then build a new dialogue dataset DuLeMon and a dialogue generation framework with Long-Term Memory (LTM) mechanism (called PLATO-LTM). This LTM mechanism enables our system to accurately extract and continuously update long-term persona memory without requiring multiple-session dialogue datasets for model training. To our knowledge, this is the first attempt to conduct real-time dynamic management of persona information of both parties, including the user and the bot. Results on DuLeMon indicate that PLATO-LTM can significantly outperform baselines in terms of long-term dialogue consistency, leading to better dialogue engagingness.</abstract>
      <url hash="c29ec385">2022.findings-acl.207</url>
      <bibkey>xu-etal-2022-long</bibkey>
      <pwccode url="https://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2022-DuLeMon" additional="false">PaddlePaddle/Research</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/delemon">DuLeMon</pwcdataset>
    </paper>
    <paper id="208">
      <title>Lacking the Embedding of a Word? Look it up into a Traditional Dictionary</title>
      <author><first>Elena Sofia</first><last>Ruzzetti</last></author>
      <author><first>Leonardo</first><last>Ranaldi</last></author>
      <author><first>Michele</first><last>Mastromattei</last></author>
      <author><first>Francesca</first><last>Fallucchi</last></author>
      <author><first>Noemi</first><last>Scarpato</last></author>
      <author><first>Fabio Massimo</first><last>Zanzotto</last></author>
      <pages>2651-2662</pages>
      <abstract>Word embeddings are powerful dictionaries, which may easily capture language variations. However, these dictionaries fail to give sense to rare words, which are surprisingly often covered by traditional dictionaries. In this paper, we propose to use definitions retrieved in traditional dictionaries to produce word embeddings for rare words. For this purpose, we introduce two methods: Definition Neural Network (DefiNNet) and Define BERT (DefBERT). In our experiments, DefiNNet and DefBERT significantly outperform state-of-the-art as well as baseline methods devised for producing embeddings of unknown words. In fact, DefiNNet significantly outperforms FastText, which implements a method for the same task-based on n-grams, and DefBERT significantly outperforms the BERT method for OOV words. Then, definitions in traditional dictionaries are useful to build word embeddings for rare words.</abstract>
      <url hash="a0d5de34">2022.findings-acl.208</url>
      <attachment type="software" hash="86b8a2fa">2022.findings-acl.208.software.zip</attachment>
      <bibkey>ruzzetti-etal-2022-lacking</bibkey>
    </paper>
    <paper id="209">
      <title><fixed-case>MTR</fixed-case>ec: Multi-Task Learning over <fixed-case>BERT</fixed-case> for News Recommendation</title>
      <author><first>Qiwei</first><last>Bi</last></author>
      <author><first>Jian</first><last>Li</last></author>
      <author><first>Lifeng</first><last>Shang</last></author>
      <author><first>Xin</first><last>Jiang</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <author><first>Hanfang</first><last>Yang</last></author>
      <pages>2663-2669</pages>
      <abstract>Existing news recommendation methods usually learn news representations solely based on news titles. To sufficiently utilize other fields of news information such as category and entities, some methods treat each field as an additional feature and combine different feature vectors with attentive pooling. With the adoption of large pre-trained models like BERT in news recommendation, the above way to incorporate multi-field information may encounter challenges: the shallow feature encoding to compress the category and entity information is not compatible with the deep BERT encoding. In this paper, we propose a multi-task method to incorporate the multi-field information into BERT, which improves its news encoding capability. Besides, we modify the gradients of auxiliary tasks based on their gradient conflicts with the main task, which further boosts the model performance. Extensive experiments on the MIND news recommendation benchmark show the effectiveness of our approach.</abstract>
      <url hash="632dfd18">2022.findings-acl.209</url>
      <bibkey>bi-etal-2022-mtrec</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mind">MIND</pwcdataset>
    </paper>
    <paper id="210">
      <title>Cross-domain Named Entity Recognition via Graph Matching</title>
      <author><first>Junhao</first><last>Zheng</last></author>
      <author><first>Haibin</first><last>Chen</last></author>
      <author><first>Qianli</first><last>Ma</last></author>
      <pages>2670-2680</pages>
      <abstract>Cross-domain NER is a practical yet challenging problem since the data scarcity in the real-world scenario. A common practice is first to learn a NER model in a rich-resource general domain and then adapt the model to specific domains. Due to the mismatch problem between entity types across domains, the wide knowledge in the general domain can not effectively transfer to the target domain NER model. To this end, we model the label relationship as a probability distribution and construct label graphs in both source and target label spaces. To enhance the contextual representation with label structures, we fuse the label graph into the word embedding output by BERT. By representing label relationships as graphs, we formulate cross-domain NER as a graph matching problem. Furthermore, the proposed method has good applicability with pre-training methods and is potentially capable of other cross-domain prediction tasks. Empirical results on four datasets show that our method outperforms a series of transfer learning, multi-task learning, and few-shot learning methods.</abstract>
      <url hash="2350fdc4">2022.findings-acl.210</url>
      <attachment type="software" hash="4c4335d1">2022.findings-acl.210.software.zip</attachment>
      <bibkey>zheng-etal-2022-cross</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/crossner">CrossNER</pwcdataset>
    </paper>
    <paper id="211">
      <title>Assessing Multilingual Fairness in Pre-trained Multimodal Representations</title>
      <author><first>Jialu</first><last>Wang</last></author>
      <author id="yang-liu-umich"><first>Yang</first><last>Liu</last></author>
      <author><first>Xin</first><last>Wang</last></author>
      <pages>2681-2695</pages>
      <abstract>Recently pre-trained multimodal models, such as CLIP, have shown exceptional capabilities towards connecting images and natural language. The textual representations in English can be desirably transferred to multilingualism and support downstream multimodal tasks for different languages. Nevertheless, the principle of multilingual fairness is rarely scrutinized: do multilingual multimodal models treat languages equally? Are their performances biased towards particular languages? To answer these questions, we view language as the fairness recipient and introduce two new fairness notions, multilingual individual fairness and multilingual group fairness, for pre-trained multimodal models. Multilingual individual fairness requires that text snippets expressing similar semantics in different languages connect similarly to images, while multilingual group fairness requires equalized predictive performance across languages. We characterize the extent to which pre-trained multilingual vision-and-language representations are individually fair across languages. However, extensive experiments demonstrate that multilingual representations do not satisfy group fairness: (1) there is a severe multilingual accuracy disparity issue; (2) the errors exhibit biases across languages conditioning the group of people in the images, including race, gender and age.</abstract>
      <url hash="d74051dc">2022.findings-acl.211</url>
      <attachment type="software" hash="5844056e">2022.findings-acl.211.software.tgz</attachment>
      <bibkey>wang-etal-2022-assessing</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/fairface">FairFace</pwcdataset>
    </paper>
    <paper id="212">
      <title>More Than Words: Collocation Retokenization for <fixed-case>L</fixed-case>atent <fixed-case>D</fixed-case>irichlet <fixed-case>A</fixed-case>llocation Models</title>
      <author><first>Jin</first><last>Cheevaprawatdomrong</last></author>
      <author><first>Alexandra</first><last>Schofield</last></author>
      <author><first>Attapol</first><last>Rutherford</last></author>
      <pages>2696-2704</pages>
      <abstract>Traditionally, Latent Dirichlet Allocation (LDA) ingests words in a collection of documents to discover their latent topics using word-document co-occurrences. Previous studies show that representing bigrams collocations in the input can improve topic coherence in English. However, it is unclear how to achieve the best results for languages without marked word boundaries such as Chinese and Thai. Here, we explore the use of retokenization based on chi-squared measures, <tex-math>t</tex-math>-statistics, and raw frequency to merge frequent token ngrams into collocations when preparing input to the LDA model. Based on the goodness of fit and the coherence metric, we show that topics trained with merged tokens result in topic keys that are clearer, more coherent, and more effective at distinguishing topics than those of unmerged models.</abstract>
      <url hash="6887b674">2022.findings-acl.212</url>
      <bibkey>cheevaprawatdomrong-etal-2022-words</bibkey>
    </paper>
    <paper id="213">
      <title><i>Generalized but not Robust?</i> Comparing the Effects of Data Modification Methods on Out-of-Domain Generalization and Adversarial Robustness</title>
      <author><first>Tejas</first><last>Gokhale</last></author>
      <author><first>Swaroop</first><last>Mishra</last></author>
      <author><first>Man</first><last>Luo</last></author>
      <author><first>Bhavdeep</first><last>Sachdeva</last></author>
      <author><first>Chitta</first><last>Baral</last></author>
      <pages>2705-2718</pages>
      <abstract>Data modification, either via additional training datasets, data augmentation, debiasing, and dataset filtering, has been proposed as an effective solution for generalizing to out-of-domain (OOD) inputs, in both natural language processing and computer vision literature.However, the effect of data modification on adversarial robustness remains unclear.In this work, we conduct a comprehensive study of common data modification strategies and evaluate not only their in-domain and OOD performance, but also their adversarial robustness (AR).We also present results on a two-dimensional synthetic dataset to visualize the effect of each method on the training distribution.This work serves as an empirical study towards understanding the relationship between generalizing to unseen domains and defending against adversarial perturbations.Our findings suggest that more data (either via additional datasets or data augmentation) benefits both OOD accuracy and AR.However, data filtering (previously shown to improve OOD accuracy on natural language inference) hurts OOD accuracy on other tasks such as question answering and image classification.We provide insights from our experiments to inform future work in this direction.</abstract>
      <url hash="62e863c3">2022.findings-acl.213</url>
      <bibkey>gokhale-etal-2022-generalized</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mnist">MNIST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mnist-m">MNIST-M</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrqa-2019">MRQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/svhn">SVHN</pwcdataset>
    </paper>
    <paper id="214">
      <title><fixed-case>ASSIST</fixed-case>: Towards Label Noise-Robust Dialogue State Tracking</title>
      <author><first>Fanghua</first><last>Ye</last></author>
      <author><first>Yue</first><last>Feng</last></author>
      <author><first>Emine</first><last>Yilmaz</last></author>
      <pages>2719-2731</pages>
      <abstract>The MultiWOZ 2.0 dataset has greatly boosted the research on dialogue state tracking (DST). However, substantial noise has been discovered in its state annotations. Such noise brings about huge challenges for training DST models robustly. Although several refined versions, including MultiWOZ 2.1-2.4, have been published recently, there are still lots of noisy labels, especially in the training set. Besides, it is costly to rectify all the problematic annotations. In this paper, instead of improving the annotation quality further, we propose a general framework, named ASSIST (lAbel noiSe-robuSt dIalogue State Tracking), to train DST models robustly from noisy labels. ASSIST first generates pseudo labels for each sample in the training set by using an auxiliary model trained on a small clean dataset, then puts the generated pseudo labels and vanilla noisy labels together to train the primary model. We show the validity of ASSIST theoretically. Experimental results also demonstrate that ASSIST improves the joint goal accuracy of DST by up to 28.16% on MultiWOZ 2.0 and 8.41% on MultiWOZ 2.4, compared to using only the vanilla noisy labels.</abstract>
      <url hash="288defd2">2022.findings-acl.214</url>
      <attachment type="software" hash="c0a77648">2022.findings-acl.214.software.zip</attachment>
      <bibkey>ye-etal-2022-assist</bibkey>
      <pwccode url="https://github.com/smartyfh/dst-assist" additional="false">smartyfh/dst-assist</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multiwoz">MultiWOZ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sgd">SGD</pwcdataset>
    </paper>
    <paper id="215">
      <title>Graph Refinement for Coreference Resolution</title>
      <author><first>Lesly</first><last>Miculicich</last></author>
      <author><first>James</first><last>Henderson</last></author>
      <pages>2732-2742</pages>
      <abstract>The state-of-the-art models for coreference resolution are based on independent mention pair-wise decisions. We propose a modelling approach that learns coreference at the document-level and takes global decisions. For this purpose, we model coreference links in a graph structure where the nodes are tokens in the text, and the edges represent the relationship between them. Our model predicts the graph in a non-autoregressive manner, then iteratively refines it based on previous predictions, allowing global dependencies between decisions. The experimental results show improvements over various baselines, reinforcing the hypothesis that document-level information improves conference resolution.</abstract>
      <url hash="df6ef574">2022.findings-acl.215</url>
      <bibkey>miculicich-henderson-2022-graph</bibkey>
    </paper>
    <paper id="216">
      <title><fixed-case>ECO</fixed-case> v1: Towards Event-Centric Opinion Mining</title>
      <author><first>Ruoxi</first><last>Xu</last></author>
      <author><first>Hongyu</first><last>Lin</last></author>
      <author><first>Meng</first><last>Liao</last></author>
      <author><first>Xianpei</first><last>Han</last></author>
      <author><first>Jin</first><last>Xu</last></author>
      <author><first>Wei</first><last>Tan</last></author>
      <author><first>Yingfei</first><last>Sun</last></author>
      <author><first>Le</first><last>Sun</last></author>
      <pages>2743-2753</pages>
      <abstract>Events are considered as the fundamental building blocks of the world. Mining event-centric opinions can benefit decision making, people communication, and social good. Unfortunately, there is little literature addressing event-centric opinion mining, although which significantly diverges from the well-studied entity-centric opinion mining in connotation, structure, and expression. In this paper, we propose and formulate the task of event-centric opinion mining based on event-argument structure and expression categorizing theory. We also benchmark this task by constructing a pioneer corpus and designing a two-step benchmark framework. Experiment results show that event-centric opinion mining is feasible and challenging, and the proposed task, dataset, and baselines are beneficial for future studies.</abstract>
      <url hash="323fc93a">2022.findings-acl.216</url>
      <attachment type="software" hash="f04afa7c">2022.findings-acl.216.software.zip</attachment>
      <bibkey>xu-etal-2022-eco</bibkey>
    </paper>
    <paper id="217">
      <title>Deep Reinforcement Learning for Entity Alignment</title>
      <author><first>Lingbing</first><last>Guo</last></author>
      <author><first>Yuqiang</first><last>Han</last></author>
      <author><first>Qiang</first><last>Zhang</last></author>
      <author><first>Huajun</first><last>Chen</last></author>
      <pages>2754-2765</pages>
      <abstract>Embedding-based methods have attracted increasing attention in recent entity alignment (EA) studies. Although great promise they can offer, there are still several limitations. The most notable is that they identify the aligned entities based on cosine similarity, ignoring the semantics underlying the embeddings themselves. Furthermore, these methods are shortsighted, heuristically selecting the closest entity as the target and allowing multiple entities to match the same candidate. To address these limitations, we model entity alignment as a sequential decision-making task, in which an agent sequentially decides whether two entities are matched or mismatched based on their representation vectors. The proposed reinforcement learning (RL)-based entity alignment framework can be flexibly adapted to most embedding-based EA methods. The experimental results demonstrate that it consistently advances the performance of several state-of-the-art methods, with a maximum improvement of 31.1% on Hits@1.</abstract>
      <url hash="d56eb2b6">2022.findings-acl.217</url>
      <attachment type="software" hash="b2de8f78">2022.findings-acl.217.software.zip</attachment>
      <bibkey>guo-etal-2022-deep</bibkey>
      <pwccode url="https://github.com/guolingbing/rlea" additional="false">guolingbing/rlea</pwccode>
    </paper>
    <paper id="218">
      <title>Breaking Down Multilingual Machine Translation</title>
      <author><first>Ting-Rui</first><last>Chiang</last></author>
      <author><first>Yi-Pei</first><last>Chen</last></author>
      <author><first>Yi-Ting</first><last>Yeh</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <pages>2766-2780</pages>
      <abstract>While multilingual training is now an essential ingredient in machine translation (MT) systems, recent work has demonstrated that it has different effects in different multilingual settings, such as many-to-one, one-to-many, and many-to-many learning. These training settings expose the encoder and the decoder in a machine translation model with different data distributions. In this paper, we examine how different varieties of multilingual training contribute to learning these two components of the MT model. Specifically, we compare bilingual models with encoders and/or decoders initialized by multilingual training. We show that multilingual training is beneficial to encoders in general, while it only benefits decoders for low-resource languages (LRLs). We further find the important attention heads for each language pair and compare their correlations during inference. Our analysis sheds light on how multilingual translation models work and also enables us to propose methods to improve performance by training with highly related languages. Our many-to-one models for high-resource languages and one-to-many models for LRL outperform the best results reported by Aharoni et al. (2019).</abstract>
      <url hash="72ed420c">2022.findings-acl.218</url>
      <bibkey>chiang-etal-2022-breaking</bibkey>
    </paper>
    <paper id="219">
      <title>Mitigating Contradictions in Dialogue Based on Contrastive Learning</title>
      <author><first>Weizhao</first><last>Li</last></author>
      <author><first>Junsheng</first><last>Kong</last></author>
      <author><first>Ben</first><last>Liao</last></author>
      <author><first>Yi</first><last>Cai</last></author>
      <pages>2781-2788</pages>
      <abstract>Chatbot models have achieved remarkable progress in recent years but tend to yield contradictory responses. In this paper, we exploit the advantage of contrastive learning technique to mitigate this issue. To endow the model with the ability of discriminating contradictory patterns, we minimize the similarity between the target response and contradiction related negative example. The negative example is generated with learnable latent noise, which receives contradiction related feedback from the pretrained critic. Experimental results show that our method helps to avoid contradictions in response generation while preserving response fluency, outperforming existing methods on both automatic and human evaluation.</abstract>
      <url hash="24ff9057">2022.findings-acl.219</url>
      <attachment type="software" hash="8c6b78e0">2022.findings-acl.219.software.zip</attachment>
      <bibkey>li-etal-2022-mitigating</bibkey>
    </paper>
    <paper id="220">
      <title><fixed-case>ELLE</fixed-case>: Efficient Lifelong Pre-training for Emerging Data</title>
      <author><first>Yujia</first><last>Qin</last></author>
      <author><first>Jiajie</first><last>Zhang</last></author>
      <author><first>Yankai</first><last>Lin</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Peng</first><last>Li</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>2789-2810</pages>
      <abstract>Current pre-trained language models (PLM) are typically trained with static data, ignoring that in real-world scenarios, streaming data of various sources may continuously grow. This requires PLMs to integrate the information from all the sources in a lifelong manner. Although this goal could be achieved by exhaustive pre-training on all the existing data, such a process is known to be computationally expensive. To this end, we propose ELLE, aiming at efficient lifelong pre-training for emerging data. Specifically, ELLE consists of (1) function preserved model expansion, which flexibly expands an existing PLM’s width and depth to improve the efficiency of knowledge acquisition; and (2) pre-trained domain prompts, which disentangle the versatile knowledge learned during pre-training and stimulate the proper knowledge for downstream tasks. We experiment ELLE with streaming data from 5 domains on BERT and GPT. The results show the superiority of ELLE over various lifelong learning baselines in both pre-training efficiency and downstream performances. The codes are publicly available at https://github.com/thunlp/ELLE.</abstract>
      <url hash="767b6e04">2022.findings-acl.220</url>
      <attachment type="software" hash="37e6acdc">2022.findings-acl.220.software.zip</attachment>
      <bibkey>qin-etal-2022-elle</bibkey>
      <pwccode url="https://github.com/thunlp/elle" additional="false">thunlp/elle</pwccode>
    </paper>
    <paper id="221">
      <title><fixed-case>E</fixed-case>n<fixed-case>CBP</fixed-case>: A New Benchmark Dataset for Finer-Grained Cultural Background Prediction in <fixed-case>E</fixed-case>nglish</title>
      <author><first>Weicheng</first><last>Ma</last></author>
      <author><first>Samiha</first><last>Datta</last></author>
      <author><first>Lili</first><last>Wang</last></author>
      <author><first>Soroush</first><last>Vosoughi</last></author>
      <pages>2811-2823</pages>
      <abstract>While cultural backgrounds have been shown to affect linguistic expressions, existing natural language processing (NLP) research on culture modeling is overly coarse-grained and does not examine cultural differences among speakers of the same language. To address this problem and augment NLP models with cultural background features, we collect, annotate, manually validate, and benchmark EnCBP, a finer-grained news-based cultural background prediction dataset in English. Through language modeling (LM) evaluations and manual analyses, we confirm that there are noticeable differences in linguistic expressions among five English-speaking countries and across four states in the US. Additionally, our evaluations on nine syntactic (CoNLL-2003), semantic (PAWS-Wiki, QNLI, STS-B, and RTE), and psycholinguistic tasks (SST-5, SST-2, Emotion, and Go-Emotions) show that, while introducing cultural background information does not benefit the Go-Emotions task due to text domain conflicts, it noticeably improves deep learning (DL) model performance on other tasks. Our findings strongly support the importance of cultural background modeling to a wide variety of NLP tasks and demonstrate the applicability of EnCBP in culture-related research.</abstract>
      <url hash="0ae6be5f">2022.findings-acl.221</url>
      <bibkey>ma-etal-2022-encbp</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/emotion">CARER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/goemotions">GoEmotions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="222">
      <title>Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models</title>
      <author><first>Robert</first><last>Logan IV</last></author>
      <author><first>Ivana</first><last>Balazevic</last></author>
      <author><first>Eric</first><last>Wallace</last></author>
      <author><first>Fabio</first><last>Petroni</last></author>
      <author><first>Sameer</first><last>Singh</last></author>
      <author><first>Sebastian</first><last>Riedel</last></author>
      <pages>2824-2835</pages>
      <abstract>Prompting language models (LMs) with training examples and task descriptions has been seen as critical to recent successes in few-shot learning. In this work, we show that finetuning LMs in the few-shot setting can considerably reduce the need for prompt engineering. In fact, one can use null prompts, prompts that contain neither task-specific templates nor training examples, and achieve competitive accuracy to manually-tuned prompts across a wide range of tasks. While finetuning LMs does introduce new parameters for each downstream task, we show that this memory overhead can be substantially reduced: finetuning only the bias terms can achieve comparable or better accuracy than standard finetuning while only updating 0.1% of the parameters. All in all, we recommend finetuning LMs for few-shot learning as it is more accurate, robust to different prompts, and can be made nearly as efficient as using frozen LMs.</abstract>
      <url hash="d4d7e92e">2022.findings-acl.222</url>
      <attachment type="software" hash="076748db">2022.findings-acl.222.software.zip</attachment>
      <bibkey>logan-iv-etal-2022-cutting</bibkey>
      <pwccode url="https://github.com/ucinlp/null-prompts" additional="true">ucinlp/null-prompts</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
    </paper>
    <paper id="223">
      <title>u<fixed-case>FACT</fixed-case>: Unfaithful Alien-Corpora Training for Semantically Consistent Data-to-Text Generation</title>
      <author><first>Tisha</first><last>Anders</last></author>
      <author><first>Alexandru</first><last>Coca</last></author>
      <author><first>Bill</first><last>Byrne</last></author>
      <pages>2836-2841</pages>
      <abstract>We propose uFACT (Un-Faithful Alien Corpora Training), a training corpus construction method for data-to-text (d2t) generation models. We show that d2t models trained on uFACT datasets generate utterances which represent the semantic content of the data sources more accurately compared to models trained on the target corpus alone. Our approach is to augment the training set of a given target corpus with alien corpora which have different semantic representations. We show that while it is important to have faithful data from the target corpus, the faithfulness of additional corpora only plays a minor role. Consequently, uFACT datasets can be constructed with large quantities of unfaithful data. We show how uFACT can be leveraged to obtain state-of-the-art results on the WebNLG benchmark using METEOR as our performance metric. Furthermore, we investigate the sensitivity of the generation faithfulness to the training corpus structure using the PARENT metric, and provide a baseline for this metric on the WebNLG (Gardent et al., 2017) benchmark to facilitate comparisons with future work.</abstract>
      <url hash="b9cccf42">2022.findings-acl.223</url>
      <bibkey>anders-etal-2022-ufact</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/viggo">ViGGO</pwcdataset>
    </paper>
    <paper id="224">
      <title>Good Night at 4 pm?! Time Expressions in Different Cultures</title>
      <author><first>Vered</first><last>Shwartz</last></author>
      <pages>2842-2853</pages>
      <abstract>We propose the task of culture-specific time expression grounding, i.e. mapping from expressions such as “morning” in English or “Manhã” in Portuguese to specific hours in the day. We propose 3 language-agnostic methods, one of which achieves promising results on gold standard annotations that we collected for a small number of languages. We then apply this method to 27 languages and analyze the similarities across languages in the grounding of time expressions.</abstract>
      <url hash="97bb54df">2022.findings-acl.224</url>
      <bibkey>shwartz-2022-good</bibkey>
      <pwccode url="https://github.com/vered1986/time_expressions" additional="false">vered1986/time_expressions</pwccode>
    </paper>
    <paper id="225">
      <title>Extracting Person Names from User Generated Text: Named-Entity Recognition for Combating Human Trafficking</title>
      <author><first>Yifei</first><last>Li</last></author>
      <author><first>Pratheeksha</first><last>Nair</last></author>
      <author><first>Kellin</first><last>Pelrine</last></author>
      <author><first>Reihaneh</first><last>Rabbany</last></author>
      <pages>2854-2868</pages>
      <abstract>Online escort advertisement websites are widely used for advertising victims of human trafficking. Domain experts agree that advertising multiple people in the same ad is a strong indicator of trafficking. Thus, extracting person names from the text of these ads can provide valuable clues for further analysis. However, Named-Entity Recognition (NER) on escort ads is challenging because the text can be noisy, colloquial and often lacking proper grammar and punctuation. Most existing state-of-the-art NER models fail to demonstrate satisfactory performance in this task. In this paper, we propose NEAT (Name Extraction Against Trafficking) for extracting person names. It effectively combines classic rule-based and dictionary extractors with a contextualized language model to capture ambiguous names (e.g penny, hazel) and adapts to adversarial changes in the text by expanding its dictionary. NEAT shows 19% improvement on average in the F1 classification score for name extraction compared to previous state-of-the-art in two domain-specific datasets.</abstract>
      <url hash="d03852e0">2022.findings-acl.225</url>
      <bibkey>li-etal-2022-extracting</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wnut-2017-emerging-and-rare-entity">WNUT 2017</pwcdataset>
    </paper>
    <paper id="226">
      <title><fixed-case>O</fixed-case>ne<fixed-case>A</fixed-case>ligner: Zero-shot Cross-lingual Transfer with One Rich-Resource Language Pair for Low-Resource Sentence Retrieval</title>
      <author><first>Tong</first><last>Niu</last></author>
      <author><first>Kazuma</first><last>Hashimoto</last></author>
      <author><first>Yingbo</first><last>Zhou</last></author>
      <author><first>Caiming</first><last>Xiong</last></author>
      <pages>2869-2882</pages>
      <abstract>Aligning parallel sentences in multilingual corpora is essential to curating data for downstream applications such as Machine Translation. In this work, we present OneAligner, an alignment model specially designed for sentence retrieval tasks. This model is able to train on only one language pair and transfers, in a cross-lingual fashion, to low-resource language pairs with negligible degradation in performance. When trained with all language pairs of a large-scale parallel multilingual corpus (OPUS-100), this model achieves the state-of-the-art result on the Tateoba dataset, outperforming an equally-sized previous model by 8.0 points in accuracy while using less than 0.6% of their parallel data. When finetuned on a single rich-resource language pair, be it English-centered or not, our model is able to match the performance of the ones finetuned on all language pairs under the same data budget with less than 2.0 points decrease in accuracy. Furthermore, with the same setup, scaling up the number of rich-resource language pairs monotonically improves the performance, reaching a minimum of 0.4 points discrepancy in accuracy, making it less mandatory to collect any low-resource parallel data. Finally, we conclude through empirical results and analyses that the performance of the sentence alignment task depends mostly on the monolingual and parallel data size, up to a certain size threshold, rather than on what language pairs are used for training or evaluation.</abstract>
      <url hash="4cbfa809">2022.findings-acl.226</url>
      <bibkey>niu-etal-2022-onealigner</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cc100">CC100</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikimatrix">WikiMatrix</pwcdataset>
    </paper>
    <paper id="227">
      <title>Suum Cuique: Studying Bias in Taboo Detection with a Community Perspective</title>
      <author><first>Osama</first><last>Khalid</last></author>
      <author><first>Jonathan</first><last>Rusert</last></author>
      <author><first>Padmini</first><last>Srinivasan</last></author>
      <pages>2883-2896</pages>
      <abstract>Prior research has discussed and illustrated the need to consider linguistic norms at the community level when studying taboo (hateful/offensive/toxic etc.) language. However, a methodology for doing so, that is firmly founded on community language norms is still largely absent. This can lead both to biases in taboo text classification and limitations in our understanding of the causes of bias. We propose a method to study bias in taboo classification and annotation where a community perspective is front and center. This is accomplished by using special classifiers tuned for each community’s language. In essence, these classifiers represent community level language norms. We use these to study bias and find, for example, biases are largest against African Americans (7/10 datasets and all 3 classifiers examined). In contrast to previous papers we also study other communities and find, for example, strong biases against South Asians. In a small scale user study we illustrate our key idea which is that common utterances, i.e., those with high alignment scores with a community (community classifier confidence scores) are unlikely to be regarded taboo. Annotators who are community members contradict taboo classification decisions and annotations in a majority of instances. This paper is a significant step toward reducing false positive taboo decisions that over time harm minority communities.</abstract>
      <url hash="7a5cf3cb">2022.findings-acl.227</url>
      <attachment type="software" hash="c11ad0a0">2022.findings-acl.227.software.zip</attachment>
      <bibkey>khalid-etal-2022-suum</bibkey>
      <pwccode url="https://github.com/jonrusert/suumcuique" additional="false">jonrusert/suumcuique</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/olid">OLID</pwcdataset>
    </paper>
    <paper id="228">
      <title>Modeling Intensification for Sign Language Generation: A Computational Approach</title>
      <author><first>Mert</first><last>Inan</last></author>
      <author><first>Yang</first><last>Zhong</last></author>
      <author><first>Sabit</first><last>Hassan</last></author>
      <author><first>Lorna</first><last>Quandt</last></author>
      <author><first>Malihe</first><last>Alikhani</last></author>
      <pages>2897-2911</pages>
      <abstract>End-to-end sign language generation models do not accurately represent the prosody in sign language. A lack of temporal and spatial variations leads to poor-quality generated presentations that confuse human interpreters. In this paper, we aim to improve the prosody in generated sign languages by modeling intensification in a data-driven manner. We present different strategies grounded in linguistics of sign language that inform how intensity modifiers can be represented in gloss annotations. To employ our strategies, we first annotate a subset of the benchmark PHOENIX-14T, a German Sign Language dataset, with different levels of intensification. We then use a supervised intensity tagger to extend the annotated dataset and obtain labels for the remaining portion of it. This enhanced dataset is then used to train state-of-the-art transformer models for sign language generation. We find that our efforts in intensification modeling yield better results when evaluated with automatic metrics. Human evaluation also indicates a higher preference of the videos generated using our model.</abstract>
      <url hash="5323b955">2022.findings-acl.228</url>
      <attachment type="software" hash="038f36c2">2022.findings-acl.228.software.zip</attachment>
      <bibkey>inan-etal-2022-modeling</bibkey>
      <pwccode url="https://github.com/merterm/modeling-intensification-for-slg" additional="false">merterm/modeling-intensification-for-slg</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/phoenix14t">PHOENIX14T</pwcdataset>
    </paper>
    <paper id="229">
      <title>Controllable Natural Language Generation with Contrastive Prefixes</title>
      <author><first>Jing</first><last>Qian</last></author>
      <author><first>Li</first><last>Dong</last></author>
      <author><first>Yelong</first><last>Shen</last></author>
      <author><first>Furu</first><last>Wei</last></author>
      <author><first>Weizhu</first><last>Chen</last></author>
      <pages>2912-2924</pages>
      <abstract>To guide the generation of large pretrained language models (LM), previous work has focused on directly fine-tuning the language model or utilizing an attribute discriminator. In this work, we propose a novel lightweight framework for controllable GPT2 generation, which utilizes a set of small attribute-specific vectors, called prefixes (Li and Liang, 2021), to steer natural language generation. Different from Li and Liang (2021), where each prefix is trained independently, we take the relationship among prefixes into consideration and train multiple prefixes simultaneously. We propose a novel supervised method and also an unsupervised method to train the prefixes for single-aspect control while the combination of these two methods can achieve multi-aspect control. Experimental results on both single-aspect and multi-aspect control show that our methods can guide generation towards the desired attributes while keeping high linguistic quality.</abstract>
      <url hash="e92eac1f">2022.findings-acl.229</url>
      <bibkey>qian-etal-2022-controllable</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
    </paper>
    <paper id="230">
      <title>Revisiting the Effects of Leakage on Dependency Parsing</title>
      <author><first>Nathaniel</first><last>Krasner</last></author>
      <author><first>Miriam</first><last>Wanner</last></author>
      <author><first>Antonios</first><last>Anastasopoulos</last></author>
      <pages>2925-2934</pages>
      <abstract>Recent work by Søgaard (2020) showed that, treebank size aside, overlap between training and test graphs (termed <i>leakage</i>) explains more of the observed variation in dependency parsing performance than other explanations. In this work we revisit this claim, testing it on more models and languages. We find that it only holds for zero-shot cross-lingual settings. We then propose a more fine-grained measure of such leakage which, unlike the original measure, not only explains but also correlates with observed performance variation. Code and data are available here: https://github.com/miriamwanner/reu-nlp-project</abstract>
      <url hash="9bf61937">2022.findings-acl.230</url>
      <bibkey>krasner-etal-2022-revisiting</bibkey>
      <pwccode url="https://github.com/miriamwanner/reu-nlp-project" additional="false">miriamwanner/reu-nlp-project</pwccode>
    </paper>
    <paper id="231">
      <title>Learning to Describe Solutions for Bug Reports Based on Developer Discussions</title>
      <author><first>Sheena</first><last>Panthaplackel</last></author>
      <author><first>Junyi Jessy</first><last>Li</last></author>
      <author><first>Milos</first><last>Gligoric</last></author>
      <author><first>Ray</first><last>Mooney</last></author>
      <pages>2935-2952</pages>
      <abstract>When a software bug is reported, developers engage in a discussion to collaboratively resolve it. While the solution is likely formulated within the discussion, it is often buried in a large amount of text, making it difficult to comprehend and delaying its implementation. To expedite bug resolution, we propose generating a concise natural language description of the solution by synthesizing relevant content within the discussion, which encompasses both natural language and source code. We build a corpus for this task using a novel technique for obtaining noisy supervision from repository changes linked to bug reports, with which we establish benchmarks. We also design two systems for generating a description during an ongoing discussion by classifying when sufficient context for performing the task emerges in real-time. With automated and human evaluation, we find this task to form an ideal testbed for complex reasoning in long, bimodal dialogue context.</abstract>
      <url hash="c04754b3">2022.findings-acl.231</url>
      <bibkey>panthaplackel-etal-2022-learning</bibkey>
      <pwccode url="https://github.com/panthap2/describing-bug-report-solutions" additional="false">panthap2/describing-bug-report-solutions</pwccode>
    </paper>
    <paper id="232">
      <title>Perturbations in the Wild: Leveraging Human-Written Text Perturbations for Realistic Adversarial Attack and Defense</title>
      <author><first>Thai</first><last>Le</last></author>
      <author><first>Jooyoung</first><last>Lee</last></author>
      <author><first>Kevin</first><last>Yen</last></author>
      <author><first>Yifan</first><last>Hu</last></author>
      <author><first>Dongwon</first><last>Lee</last></author>
      <pages>2953-2965</pages>
      <abstract>We proposes a novel algorithm, ANTHRO, that inductively extracts over 600K human-written text perturbations in the wild and leverages them for realistic adversarial attack. Unlike existing character-based attacks which often deductively hypothesize a set of manipulation strategies, our work is grounded on actual observations from real-world texts. We find that adversarial texts generated by ANTHRO achieve the best trade-off between (1) attack success rate, (2) semantic preservation of the original text, and (3) stealthiness–i.e. indistinguishable from human writings hence harder to be flagged as suspicious. Specifically, our attacks accomplished around 83% and 91% attack success rates on BERT and RoBERTa, respectively. Moreover, it outperformed the TextBugger baseline with an increase of 50% and 40% in terms of semantic preservation and stealthiness when evaluated by both layperson and professional human workers. ANTHRO can further enhance a BERT classifier’s performance in understanding different variations of human-written toxic texts via adversarial training when compared to the Perspective API.</abstract>
      <url hash="9f9bbcca">2022.findings-acl.232</url>
      <bibkey>le-etal-2022-perturbations</bibkey>
      <pwccode url="https://github.com/lethaiq/perturbations-in-the-wild" additional="false">lethaiq/perturbations-in-the-wild</pwccode>
    </paper>
    <paper id="233">
      <title>Improving <fixed-case>C</fixed-case>hinese Grammatical Error Detection via Data augmentation by Conditional Error Generation</title>
      <author><first>Tianchi</first><last>Yue</last></author>
      <author><first>Shulin</first><last>Liu</last></author>
      <author><first>Huihui</first><last>Cai</last></author>
      <author><first>Tao</first><last>Yang</last></author>
      <author><first>Shengkang</first><last>Song</last></author>
      <author><first>TingHao</first><last>Yu</last></author>
      <pages>2966-2975</pages>
      <abstract>Chinese Grammatical Error Detection(CGED) aims at detecting grammatical errors in Chinese texts. One of the main challenges for CGED is the lack of annotated data. To alleviate this problem, previous studies proposed various methods to automatically generate more training samples, which can be roughly categorized into rule-based methods and model-based methods. The rule-based methods construct erroneous sentences by directly introducing noises into original sentences. However, the introduced noises are usually context-independent, which are quite different from those made by humans. The model-based methods utilize generative models to imitate human errors. The generative model may bring too many changes to the original sentences and generate semantically ambiguous sentences, so it is difficult to detect grammatical errors in these generated sentences. In addition, generated sentences may be error-free and thus become noisy data. To handle these problems, we propose CNEG, a novel Conditional Non-Autoregressive Error Generation model for generating Chinese grammatical errors. Specifically, in order to generate a context-dependent error, we first mask a span in a correct text, then predict an erroneous span conditioned on both the masked text and the correct span. Furthermore, we filter out error-free spans by measuring their perplexities in the original sentences. Experimental results show that our proposed method achieves better performance than all compared data augmentation methods on the CGED-2018 and CGED-2020 benchmarks.</abstract>
      <url hash="75d7da6e">2022.findings-acl.233</url>
      <bibkey>yue-etal-2022-improving</bibkey>
    </paper>
    <paper id="234">
      <title>Modular and Parameter-Efficient Multimodal Fusion with Prompting</title>
      <author><first>Sheng</first><last>Liang</last></author>
      <author><first>Mengjie</first><last>Zhao</last></author>
      <author><first>Hinrich</first><last>Schuetze</last></author>
      <pages>2976-2985</pages>
      <abstract>Recent research has made impressive progress in large-scale multimodal pre-training. In the context of the rapid growth of model size, it is necessary to seek efficient and flexible methods other than finetuning. In this paper, we propose to use prompt vectors to align the modalities. Our method achieves comparable performance to several other multimodal fusion methods in low-resource settings. We further show that our method is modular and parameter-efficient for processing tasks involving two or more data modalities.</abstract>
      <url hash="5cd11a03">2022.findings-acl.234</url>
      <attachment type="software" hash="8d479d2b">2022.findings-acl.234.software.zip</attachment>
      <bibkey>liang-etal-2022-modular</bibkey>
    </paper>
    <paper id="235">
      <title>Synchronous Refinement for Neural Machine Translation</title>
      <author><first>Kehai</first><last>Chen</last></author>
      <author><first>Masao</first><last>Utiyama</last></author>
      <author><first>Eiichiro</first><last>Sumita</last></author>
      <author><first>Rui</first><last>Wang</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <pages>2986-2996</pages>
      <abstract>Machine translation typically adopts an encoder-to-decoder framework, in which the decoder generates the target sentence word-by-word in an auto-regressive manner. However, the auto-regressive decoder faces a deep-rooted <tex-math>one</tex-math>-<tex-math>pass</tex-math> issue whereby each generated word is considered as one element of the final output regardless of whether it is correct or not. These generated wrong words further constitute the target historical context to affect the generation of subsequent target words. This paper proposes a novel synchronous refinement method to revise potential errors in the generated words by considering part of the target future context. Particularly, the proposed approach allows the auto-regressive decoder to refine the previously generated target words and generate the next target word synchronously. The experimental results on three widely-used machine translation tasks demonstrated the effectiveness of the proposed approach.</abstract>
      <url hash="7aa63181">2022.findings-acl.235</url>
      <bibkey>chen-etal-2022-synchronous</bibkey>
    </paper>
    <paper id="236">
      <title><fixed-case>HIE</fixed-case>-<fixed-case>SQL</fixed-case>: History Information Enhanced Network for Context-Dependent Text-to-<fixed-case>SQL</fixed-case> Semantic Parsing</title>
      <author><first>Yanzhao</first><last>Zheng</last></author>
      <author><first>Haibin</first><last>Wang</last></author>
      <author><first>Baohua</first><last>Dong</last></author>
      <author><first>Xingjun</first><last>Wang</last></author>
      <author><first>Changshan</first><last>Li</last></author>
      <pages>2997-3007</pages>
      <abstract>Recently, context-dependent text-to-SQL semantic parsing which translates natural language into SQL in an interaction process has attracted a lot of attentions. Previous works leverage context dependence information either from interaction history utterances or previous predicted queries but fail in taking advantage of both of them since of the mismatch between the natural language and logic-form SQL. In this work, we propose a History Information Enhanced text-to-SQL model (HIE-SQL) to exploit context dependence information from both history utterances and the last predicted SQL query. In view of the mismatch, we treat natural language and SQL as two modalities and propose a bimodal pre-trained model to bridge the gap between them. Besides, we design a schema-linking graph to enhance connections from utterances and the SQL query to database schema. We show our history information enhanced methods improve the performance of HIE-SQL by a significant margin, which achieves new state-of-the-art results on two context-dependent text-to-SQL benchmarks, the SparC and CoSQL datasets, at the writing time.</abstract>
      <url hash="16e2bb60">2022.findings-acl.236</url>
      <bibkey>zheng-etal-2022-hie</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cosql">CoSQL</pwcdataset>
    </paper>
    <paper id="237">
      <title><fixed-case>CRAS</fixed-case>pell: A Contextual Typo Robust Approach to Improve <fixed-case>C</fixed-case>hinese Spelling Correction</title>
      <author><first>Shulin</first><last>Liu</last></author>
      <author><first>Shengkang</first><last>Song</last></author>
      <author><first>Tianchi</first><last>Yue</last></author>
      <author><first>Tao</first><last>Yang</last></author>
      <author><first>Huihui</first><last>Cai</last></author>
      <author><first>TingHao</first><last>Yu</last></author>
      <author><first>Shengli</first><last>Sun</last></author>
      <pages>3008-3018</pages>
      <abstract>Recently, Bert-based models have dominated the research of Chinese spelling correction (CSC). These methods have two limitations: (1) they have poor performance on multi-typo texts. In such texts, the context of each typo contains at least one misspelled character, which brings noise information. Such noisy context leads to the declining performance on multi-typo texts. (2) they tend to overcorrect valid expressions to more frequent expressions due to the masked token recovering task of Bert. We attempt to address these limitations in this paper. To make our model robust to contextual noise brought by typos, our approach first constructs a noisy context for each training sample. Then the correction model is forced to yield similar outputs based on the noisy and original contexts. Moreover, to address the overcorrection problem, copy mechanism is incorporated to encourage our model to prefer to choose the input character when the miscorrected and input character are both valid according to the given context. Experiments are conducted on widely used benchmarks. Our model achieves superior performance against state-of-the-art methods by a remarkable gain.</abstract>
      <url hash="a3083293">2022.findings-acl.237</url>
      <attachment type="software" hash="5af34dbc">2022.findings-acl.237.software.zip</attachment>
      <bibkey>liu-etal-2022-craspell</bibkey>
      <pwccode url="https://github.com/liushulinle/craspell" additional="false">liushulinle/craspell</pwccode>
    </paper>
    <paper id="238">
      <title><fixed-case>G</fixed-case>aussian Multi-head Attention for Simultaneous Machine Translation</title>
      <author><first>Shaolei</first><last>Zhang</last></author>
      <author><first>Yang</first><last>Feng</last></author>
      <pages>3019-3030</pages>
      <abstract>Simultaneous machine translation (SiMT) outputs translation while receiving the streaming source inputs, and hence needs a policy to determine where to start translating. The alignment between target and source words often implies the most informative source word for each target word, and hence provides the unified control over translation quality and latency, but unfortunately the existing SiMT methods do not explicitly model the alignment to perform the control. In this paper, we propose Gaussian Multi-head Attention (GMA) to develop a new SiMT policy by modeling alignment and translation in a unified manner. For SiMT policy, GMA models the aligned source position of each target word, and accordingly waits until its aligned position to start translating. To integrate the learning of alignment into the translation model, a Gaussian distribution centered on predicted aligned position is introduced as an alignment-related prior, which cooperates with translation-related soft attention to determine the final attention. Experiments on En-Vi and De-En tasks show that our method outperforms strong baselines on the trade-off between translation and latency.</abstract>
      <url hash="6a1080cf">2022.findings-acl.238</url>
      <bibkey>zhang-feng-2022-gaussian</bibkey>
      <pwccode url="https://github.com/ictnlp/gma" additional="false">ictnlp/gma</pwccode>
    </paper>
    <paper id="239">
      <title>Composing Structure-Aware Batches for Pairwise Sentence Classification</title>
      <author><first>Andreas</first><last>Waldis</last></author>
      <author><first>Tilman</first><last>Beck</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <pages>3031-3045</pages>
      <abstract>Identifying the relation between two sentences requires datasets with pairwise annotations. In many cases, these datasets contain instances that are annotated multiple times as part of different pairs. They constitute a structure that contains additional helpful information about the inter-relatedness of the text instances based on the annotations. This paper investigates how this kind of structural dataset information can be exploited during training.We propose three batch composition strategies to incorporate such information and measure their performance over 14 heterogeneous pairwise sentence classification tasks. Our results show statistically significant improvements (up to 3.9%) - independent of the pre-trained language model - for most tasks compared to baselines that follow a standard training procedure. Further, we see that even this baseline procedure can profit from having such structural information in a low-resource setting.</abstract>
      <url hash="a9254fbb">2022.findings-acl.239</url>
      <bibkey>waldis-etal-2022-composing</bibkey>
      <pwccode url="https://github.com/ukplab/acl2022-structure-batches" additional="false">ukplab/acl2022-structure-batches</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
    </paper>
    <paper id="240">
      <title>Factual Consistency of Multilingual Pretrained Language Models</title>
      <author><first>Constanza</first><last>Fierro</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>3046-3052</pages>
      <abstract>Pretrained language models can be queried for factual knowledge, with potential applications in knowledge base acquisition and tasks that require inference. However, for that, we need to know how reliable this knowledge is, and recent work has shown that monolingual English language models lack consistency when predicting factual knowledge, that is, they fill-in-the-blank differently for paraphrases describing the same fact. In this paper, we extend the analysis of consistency to a multilingual setting. We introduce a resource, mParaRel, and investigate (i) whether multilingual language models such as mBERT and XLM-R are more consistent than their monolingual counterparts;and (ii) if such models are equally consistent across languages.We find that mBERT is as inconsistent as English BERT in English paraphrases, but that both mBERT and XLM-R exhibit a high degree of inconsistency in English and even more so for all the other 45 languages.</abstract>
      <url hash="ca0c5ebb">2022.findings-acl.240</url>
      <attachment type="software" hash="3f68fad9">2022.findings-acl.240.software.zip</attachment>
      <bibkey>fierro-sogaard-2022-factual</bibkey>
      <pwccode url="https://github.com/coastalcph/mpararel" additional="false">coastalcph/mpararel</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/lama">LAMA</pwcdataset>
    </paper>
    <paper id="241">
      <title>Selecting Stickers in Open-Domain Dialogue through Multitask Learning</title>
      <author><first>Zhexin</first><last>Zhang</last></author>
      <author><first>Yeshuang</first><last>Zhu</last></author>
      <author><first>Zhengcong</first><last>Fei</last></author>
      <author><first>Jinchao</first><last>Zhang</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>3053-3060</pages>
      <abstract>With the increasing popularity of online chatting, stickers are becoming important in our online communication. Selecting appropriate stickers in open-domain dialogue requires a comprehensive understanding of both dialogues and stickers, as well as the relationship between the two types of modalities. To tackle these challenges, we propose a multitask learning method comprised of three auxiliary tasks to enhance the understanding of dialogue history, emotion and semantic meaning of stickers. Extensive experiments conducted on a recent challenging dataset show that our model can better combine the multimodal information and achieve significantly higher accuracy over strong baselines. Ablation study further verifies the effectiveness of each auxiliary task. Our code is available at <url>https://github.com/nonstopfor/Sticker-Selection</url>.</abstract>
      <url hash="fac3840a">2022.findings-acl.241</url>
      <attachment type="software" hash="629403cd">2022.findings-acl.241.software.zip</attachment>
      <bibkey>zhang-etal-2022-selecting</bibkey>
      <pwccode url="https://github.com/nonstopfor/sticker-selection" additional="false">nonstopfor/sticker-selection</pwccode>
    </paper>
    <paper id="242">
      <title><fixed-case>Z</fixed-case>i<fixed-case>N</fixed-case>et: <fixed-case>L</fixed-case>inking <fixed-case>C</fixed-case>hinese Characters Spanning Three Thousand Years</title>
      <author><first>Yang</first><last>Chi</last></author>
      <author><first>Fausto</first><last>Giunchiglia</last></author>
      <author><first>Daqian</first><last>Shi</last></author>
      <author><first>Xiaolei</first><last>Diao</last></author>
      <author><first>Chuntao</first><last>Li</last></author>
      <author><first>Hao</first><last>Xu</last></author>
      <pages>3061-3070</pages>
      <abstract>Modern Chinese characters evolved from 3,000 years ago. Up to now, tens of thousands of glyphs of ancient characters have been discovered, which must be deciphered by experts to interpret unearthed documents. Experts usually need to compare each ancient character to be examined with similar known ones in whole historical periods. However, it is inevitably limited by human memory and experience, which often cost a lot of time but associations are limited to a small scope. To help researchers discover glyph similar characters, this paper introduces ZiNet, the first diachronic knowledge base describing relationships and evolution of Chinese characters and words. In addition, powered by the knowledge of radical systems in ZiNet, this paper introduces glyph similarity measurement between ancient Chinese characters, which could capture similar glyph pairs that are potentially related in origins or semantics. Results show strong positive correlations between scores from the method and from human experts. Finally, qualitative analysis and implicit future applications are presented.</abstract>
      <url hash="61a2cf4b">2022.findings-acl.242</url>
      <attachment type="software" hash="a027bd75">2022.findings-acl.242.software.zip</attachment>
      <bibkey>chi-etal-2022-zinet</bibkey>
      <pwccode url="https://github.com/yangchijlu/ancientchinesecharsim" additional="false">yangchijlu/ancientchinesecharsim</pwccode>
    </paper>
    <paper id="243">
      <title>How Can Cross-lingual Knowledge Contribute Better to Fine-Grained Entity Typing?</title>
      <author><first>Hailong</first><last>Jin</last></author>
      <author><first>Tiansi</first><last>Dong</last></author>
      <author><first>Lei</first><last>Hou</last></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <author><first>Hui</first><last>Chen</last></author>
      <author><first>Zelin</first><last>Dai</last></author>
      <author><first>Qu</first><last>Yincen</last></author>
      <pages>3071-3081</pages>
      <abstract>Cross-lingual Entity Typing (CLET) aims at improving the quality of entity type prediction by transferring semantic knowledge learned from rich-resourced languages to low-resourced languages. In this paper, by utilizing multilingual transfer learning via the mixture-of-experts approach, our model dynamically capture the relationship between target language and each source language, and effectively generalize to predict types of unseen entities in new languages. Extensive experiments on multi-lingual datasets show that our method significantly outperforms multiple baselines and can robustly handle negative transfer. We questioned the relationship between language similarity and the performance of CLET. A series of experiments refute the commonsense that the more source the better, and suggest the Similarity Hypothesis for CLET.</abstract>
      <url hash="abe5c240">2022.findings-acl.243</url>
      <bibkey>jin-etal-2022-cross</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/figer">FIGER</pwcdataset>
    </paper>
    <paper id="244">
      <title><fixed-case>AMR-DA</fixed-case>: <fixed-case>D</fixed-case>ata Augmentation by <fixed-case>A</fixed-case>bstract <fixed-case>M</fixed-case>eaning <fixed-case>R</fixed-case>epresentation</title>
      <author><first>Ziyi</first><last>Shou</last></author>
      <author><first>Yuxin</first><last>Jiang</last></author>
      <author><first>Fangzhen</first><last>Lin</last></author>
      <pages>3082-3098</pages>
      <abstract>Abstract Meaning Representation (AMR) is a semantic representation for NLP/NLU. In this paper, we propose to use it for data augmentation in NLP. Our proposed data augmentation technique, called AMR-DA, converts a sample sentence to an AMR graph, modifies the graph according to various data augmentation policies, and then generates augmentations from graphs. Our method combines both sentence-level techniques like back translation and token-level techniques like EDA (Easy Data Augmentation). To evaluate the effectiveness of our method, we apply it to the tasks of semantic textual similarity (STS) and text classification. For STS, our experiments show that AMR-DA boosts the performance of the state-of-the-art models on several STS benchmarks. For text classification, AMR-DA outperforms EDA and AEDA and leads to more robust improvements.</abstract>
      <url hash="c5cea47e">2022.findings-acl.244</url>
      <bibkey>shou-etal-2022-amr</bibkey>
      <pwccode url="https://github.com/zzshou/amr-data-augmentation" additional="false">zzshou/amr-data-augmentation</pwccode>
    </paper>
    <paper id="245">
      <title>Using Pre-Trained Language Models for Producing Counter Narratives Against Hate Speech: a Comparative Study</title>
      <author><first>Serra</first><last>Tekiroglu</last></author>
      <author><first>Helena</first><last>Bonaldi</last></author>
      <author><first>Margherita</first><last>Fanton</last></author>
      <author><first>Marco</first><last>Guerini</last></author>
      <pages>3099-3114</pages>
      <abstract>In this work, we present an extensive study on the use of pre-trained language models for the task of automatic Counter Narrative (CN) generation to fight online hate speech in English. We first present a comparative study to determine whether there is a particular Language Model (or class of LMs) and a particular decoding mechanism that are the most appropriate to generate CNs. Findings show that autoregressive models combined with stochastic decodings are the most promising. We then investigate how an LM performs in generating a CN with regard to an unseen target of hate. We find out that a key element for successful ‘out of target’ experiments is not an overall similarity with the training data but the presence of a specific subset of training data, i. e. a target that shares some commonalities with the test target that can be defined a-priori. We finally introduce the idea of a pipeline based on the addition of an automatic post-editing step to refine generated CNs.</abstract>
      <url hash="6eda3f4a">2022.findings-acl.245</url>
      <bibkey>tekiroglu-etal-2022-using</bibkey>
    </paper>
    <paper id="246">
      <title>Improving Robustness of Language Models from a Geometry-aware Perspective</title>
      <author><first>Bin</first><last>Zhu</last></author>
      <author><first>Zhaoquan</first><last>Gu</last></author>
      <author><first>Le</first><last>Wang</last></author>
      <author><first>Jinyin</first><last>Chen</last></author>
      <author><first>Qi</first><last>Xuan</last></author>
      <pages>3115-3125</pages>
      <abstract>Recent studies have found that removing the norm-bounded projection and increasing search steps in adversarial training can significantly improve robustness. However, we observe that a too large number of search steps can hurt accuracy. We aim to obtain strong robustness efficiently using fewer steps. Through a toy experiment, we find that perturbing the clean data to the decision boundary but not crossing it does not degrade the test accuracy. Inspired by this, we propose friendly adversarial data augmentation (FADA) to generate friendly adversarial data. On top of FADA, we propose geometry-aware adversarial training (GAT) to perform adversarial training on friendly adversarial data so that we can save a large number of search steps. Comprehensive experiments across two widely used datasets and three pre-trained language models demonstrate that GAT can obtain stronger robustness via fewer steps. In addition, we provide extensive empirical results and in-depth analyses on robustness to facilitate future studies.</abstract>
      <url hash="526ea1ce">2022.findings-acl.246</url>
      <attachment type="software" hash="cb33852b">2022.findings-acl.246.software.zip</attachment>
      <bibkey>zhu-etal-2022-improving</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="247">
      <title>Task-guided Disentangled Tuning for Pretrained Language Models</title>
      <author><first>Jiali</first><last>Zeng</last></author>
      <author><first>Yufan</first><last>Jiang</last></author>
      <author><first>Shuangzhi</first><last>Wu</last></author>
      <author><first>Yongjing</first><last>Yin</last></author>
      <author><first>Mu</first><last>Li</last></author>
      <pages>3126-3137</pages>
      <abstract>Pretrained language models (PLMs) trained on large-scale unlabeled corpus are typically fine-tuned on task-specific downstream datasets, which have produced state-of-the-art results on various NLP tasks. However, the data discrepancy issue in domain and scale makes fine-tuning fail to efficiently capture task-specific patterns, especially in low data regime. To address this issue, we propose Task-guided Disentangled Tuning (TDT) for PLMs, which enhances the generalization of representations by disentangling task-relevant signals from the entangled representations. For a given task, we introduce a learnable confidence model to detect indicative guidance from context, and further propose a disentangled regularization to mitigate the over-reliance problem. Experimental results on GLUE and CLUE benchmarks show that TDT gives consistently better results than fine-tuning with different PLMs, and extensive analysis demonstrates the effectiveness and robustness of our method. Code is available at https://github.com/lemon0830/TDT.</abstract>
      <url hash="96781d31">2022.findings-acl.247</url>
      <bibkey>zeng-etal-2022-task</bibkey>
      <pwccode url="https://github.com/lemon0830/tdt" additional="false">lemon0830/tdt</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/clue">CLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
    <paper id="248">
      <title>Exploring the Impact of Negative Samples of Contrastive Learning: A Case Study of Sentence Embedding</title>
      <author><first>Rui</first><last>Cao</last></author>
      <author><first>Yihao</first><last>Wang</last></author>
      <author><first>Yuxin</first><last>Liang</last></author>
      <author><first>Ling</first><last>Gao</last></author>
      <author><first>Jie</first><last>Zheng</last></author>
      <author><first>Jie</first><last>Ren</last></author>
      <author><first>Zheng</first><last>Wang</last></author>
      <pages>3138-3152</pages>
      <abstract>Contrastive learning is emerging as a powerful technique for extracting knowledge from unlabeled data. This technique requires a balanced mixture of two ingredients: positive (similar) and negative (dissimilar) samples. This is typically achieved by maintaining a queue of negative samples during training. Prior works in the area typically uses a fixed-length negative sample queue, but how the negative sample size affects the model performance remains unclear. The opaque impact of the number of negative samples on performance when employing contrastive learning aroused our in-depth exploration. This paper presents a momentum contrastive learning model with negative sample queue for sentence embedding, namely MoCoSE. We add the prediction layer to the online branch to make the model asymmetric and together with EMA update mechanism of the target branch to prevent the model from collapsing. We define a maximum traceable distance metric, through which we learn to what extent the text contrastive learning benefits from the historical information of negative samples. Our experiments find that the best results are obtained when the maximum traceable distance is at a certain range, demonstrating that there is an optimal range of historical information for a negative sample queue. We evaluate the proposed unsupervised MoCoSE on the semantic text similarity (STS) task and obtain an average Spearman’s correlation of 77.27%. Source code is available here.</abstract>
      <url hash="03859637">2022.findings-acl.248</url>
      <bibkey>cao-etal-2022-exploring</bibkey>
      <pwccode url="https://github.com/xbdxwyh/mocose" additional="false">xbdxwyh/mocose</pwccode>
    </paper>
    <paper id="249">
      <title>The Inefficiency of Language Models in Scholarly Retrieval: An Experimental Walk-through</title>
      <author><first>Shruti</first><last>Singh</last></author>
      <author><first>Mayank</first><last>Singh</last></author>
      <pages>3153-3173</pages>
      <abstract>Language models are increasingly becoming popular in AI-powered scientific IR systems. This paper evaluates popular scientific language models in handling (i) short-query texts and (ii) textual neighbors. Our experiments showcase the inability to retrieve relevant documents for a short-query text even under the most relaxed conditions. Additionally, we leverage textual neighbors, generated by small perturbations to the original text, to demonstrate that not all perturbations lead to close neighbors in the embedding space. Further, an exhaustive categorization yields several classes of orthographically and semantically related, partially related and completely unrelated neighbors. Retrieval performance turns out to be more influenced by the surface form rather than the semantics of the text.</abstract>
      <url hash="7e85d001">2022.findings-acl.249</url>
      <bibkey>singh-singh-2022-inefficiency</bibkey>
      <pwccode url="https://github.com/shruti-singh/scilm_exp" additional="false">shruti-singh/scilm_exp</pwccode>
    </paper>
    <paper id="250">
      <title>Fusing Heterogeneous Factors with Triaffine Mechanism for Nested Named Entity Recognition</title>
      <author><first>Zheng</first><last>Yuan</last></author>
      <author><first>Chuanqi</first><last>Tan</last></author>
      <author><first>Songfang</first><last>Huang</last></author>
      <author><first>Fei</first><last>Huang</last></author>
      <pages>3174-3186</pages>
      <abstract>Nested entities are observed in many domains due to their compositionality, which cannot be easily recognized by the widely-used sequence labeling framework.A natural solution is to treat the task as a span classification problem.To learn better span representation and increase classification performance, it is crucial to effectively integrate heterogeneous factors including inside tokens, boundaries, labels, and related spans which could be contributing to nested entities recognition.To fuse these heterogeneous factors, we propose a novel triaffine mechanism including triaffine attention and scoring.Triaffine attention uses boundaries and labels as queries and uses inside tokens and related spans as keys and values for span representations.Triaffine scoring interacts with boundaries and span representations for classification.Experiments show that our proposed method outperforms previous span-based methods, achieves the state-of-the-art <tex-math>F_1</tex-math> scores on nested NER datasets GENIA and KBP2017, and shows comparable results on ACE2004 and ACE2005.</abstract>
      <url hash="6ead26b7">2022.findings-acl.250</url>
      <attachment type="software" hash="6f88efa7">2022.findings-acl.250.software.zip</attachment>
      <bibkey>yuan-etal-2022-fusing</bibkey>
      <pwccode url="https://github.com/GanjinZero/Triaffine-nested-ner" additional="false">GanjinZero/Triaffine-nested-ner</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ace-2004">ACE 2004</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ace-2005">ACE 2005</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/genia">GENIA</pwcdataset>
    </paper>
    <paper id="251">
      <title><fixed-case>UNIMO</fixed-case>-2: End-to-End Unified Vision-Language Grounded Learning</title>
      <author><first>Wei</first><last>Li</last></author>
      <author><first>Can</first><last>Gao</last></author>
      <author><first>Guocheng</first><last>Niu</last></author>
      <author><first>Xinyan</first><last>Xiao</last></author>
      <author><first>Hao</first><last>Liu</last></author>
      <author><first>Jiachen</first><last>Liu</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <author><first>Haifeng</first><last>Wang</last></author>
      <pages>3187-3201</pages>
      <abstract>Vision-Language Pre-training (VLP) has achieved impressive performance on various cross-modal downstream tasks. However, most existing methods can only learn from aligned image-caption data and rely heavily on expensive regional features, which greatly limits their scalability and performance. In this paper, we propose an end-to-end unified-modal pre-training framework, namely UNIMO-2, for joint learning on both aligned image-caption data and unaligned image-only and text-only corpus. We build a unified Transformer model to jointly learn visual representations, textual representations and semantic alignment between images and texts. In particular, we propose to conduct grounded learning on both images and texts via a sharing grounded space, which helps bridge unaligned images and texts, and align the visual and textual semantic spaces on different types of corpora. The experiments show that our grounded learning method can improve textual and visual semantic alignment for improving performance on various cross-modal tasks. Moreover, benefiting from effective joint modeling of different types of corpora, our model also achieves impressive performance on single-modal visual and textual tasks. Our code and models are public at the UNIMO project page https://unimo-ptm.github.io/.</abstract>
      <url hash="9aa1ec50">2022.findings-acl.251</url>
      <bibkey>li-etal-2022-unimo</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imagenet">ImageNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli-ve">SNLI-VE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-genome">Visual Genome</pwcdataset>
    </paper>
    <paper id="252">
      <title>The Past Mistake is the Future Wisdom: Error-driven Contrastive Probability Optimization for <fixed-case>C</fixed-case>hinese Spell Checking</title>
      <author><first>Yinghui</first><last>Li</last></author>
      <author><first>Qingyu</first><last>Zhou</last></author>
      <author><first>Yangning</first><last>Li</last></author>
      <author><first>Zhongli</first><last>Li</last></author>
      <author><first>Ruiyang</first><last>Liu</last></author>
      <author><first>Rongyi</first><last>Sun</last></author>
      <author><first>Zizhen</first><last>Wang</last></author>
      <author><first>Chao</first><last>Li</last></author>
      <author><first>Yunbo</first><last>Cao</last></author>
      <author><first>Hai-Tao</first><last>Zheng</last></author>
      <pages>3202-3213</pages>
      <abstract>Chinese Spell Checking (CSC) aims to detect and correct Chinese spelling errors, which are mainly caused by the phonological or visual similarity. Recently, pre-trained language models (PLMs) promote the progress of CSC task. However, there exists a gap between the learned knowledge of PLMs and the goal of CSC task. PLMs focus on the semantics in text and tend to correct the erroneous characters to semantically proper or commonly used ones, but these aren’t the ground-truth corrections. To address this issue, we propose an Error-driven COntrastive Probability Optimization (ECOPO) framework for CSC task. ECOPO refines the knowledge representations of PLMs, and guides the model to avoid predicting these common characters through an error-driven way. Particularly, ECOPO is model-agnostic and it can be combined with existing CSC methods to achieve better performance. Extensive experiments and detailed analyses on SIGHAN datasets demonstrate that ECOPO is simple yet effective.</abstract>
      <url hash="1c40387b">2022.findings-acl.252</url>
      <attachment type="software" hash="a574d7f1">2022.findings-acl.252.software.zip</attachment>
      <bibkey>li-etal-2022-past</bibkey>
    </paper>
    <paper id="253">
      <title><fixed-case>XFUND</fixed-case>: A Benchmark Dataset for Multilingual Visually Rich Form Understanding</title>
      <author><first>Yiheng</first><last>Xu</last></author>
      <author><first>Tengchao</first><last>Lv</last></author>
      <author><first>Lei</first><last>Cui</last></author>
      <author><first>Guoxin</first><last>Wang</last></author>
      <author><first>Yijuan</first><last>Lu</last></author>
      <author><first>Dinei</first><last>Florencio</last></author>
      <author><first>Cha</first><last>Zhang</last></author>
      <author><first>Furu</first><last>Wei</last></author>
      <pages>3214-3224</pages>
      <abstract>Multimodal pre-training with text, layout, and image has achieved SOTA performance for visually rich document understanding tasks recently, which demonstrates the great potential for joint learning across different modalities. However, the existed research work has focused only on the English domain while neglecting the importance of multilingual generalization. In this paper, we introduce a human-annotated multilingual form understanding benchmark dataset named XFUND, which includes form understanding samples in 7 languages (Chinese, Japanese, Spanish, French, Italian, German, Portuguese). Meanwhile, we present LayoutXLM, a multimodal pre-trained model for multilingual document understanding, which aims to bridge the language barriers for visually rich document understanding. Experimental results show that the LayoutXLM model has significantly outperformed the existing SOTA cross-lingual pre-trained models on the XFUND dataset. The XFUND dataset and the pre-trained LayoutXLM model have been publicly available at https://aka.ms/layoutxlm.</abstract>
      <url hash="f1297ee0">2022.findings-acl.253</url>
      <attachment type="software" hash="c020263a">2022.findings-acl.253.software.zip</attachment>
      <bibkey>xu-etal-2022-xfund</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/funsd">FUNSD</pwcdataset>
    </paper>
    <paper id="254">
      <title>Type-Driven Multi-Turn Corrections for Grammatical Error Correction</title>
      <author><first>Shaopeng</first><last>Lai</last></author>
      <author><first>Qingyu</first><last>Zhou</last></author>
      <author><first>Jiali</first><last>Zeng</last></author>
      <author><first>Zhongli</first><last>Li</last></author>
      <author><first>Chao</first><last>Li</last></author>
      <author><first>Yunbo</first><last>Cao</last></author>
      <author><first>Jinsong</first><last>Su</last></author>
      <pages>3225-3236</pages>
      <abstract>Grammatical Error Correction (GEC) aims to automatically detect and correct grammatical errors. In this aspect, dominant models are trained by one-iteration learning while performing multiple iterations of corrections during inference. Previous studies mainly focus on the data augmentation approach to combat the exposure bias, which suffers from two drawbacks.First, they simply mix additionally-constructed training instances and original ones to train models, which fails to help models be explicitly aware of the procedure of gradual corrections. Second, they ignore the interdependence between different types of corrections.In this paper, we propose a Type-Driven Multi-Turn Corrections approach for GEC. Using this approach, from each training instance, we additionally construct multiple training instances, each of which involves the correction of a specific type of errors. Then, we use these additionally-constructed training instances and the original one to train the model in turn.Experimental results and in-depth analysis show that our approach significantly benefits the model training. Particularly, our enhanced model achieves state-of-the-art single-model performance on English GEC benchmarks. We release our code at Github.</abstract>
      <url hash="787655ca">2022.findings-acl.254</url>
      <attachment type="software" hash="2bcd9d5d">2022.findings-acl.254.software.zip</attachment>
      <bibkey>lai-etal-2022-type</bibkey>
      <pwccode url="https://github.com/deeplearnxmu/tmtc" additional="false">deeplearnxmu/tmtc</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fce">FCE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/locness-corpus">WI-LOCNESS</pwcdataset>
    </paper>
    <paper id="255">
      <title>Leveraging Knowledge in Multilingual Commonsense Reasoning</title>
      <author><first>Yuwei</first><last>Fang</last></author>
      <author><first>Shuohang</first><last>Wang</last></author>
      <author><first>Yichong</first><last>Xu</last></author>
      <author><first>Ruochen</first><last>Xu</last></author>
      <author><first>Siqi</first><last>Sun</last></author>
      <author><first>Chenguang</first><last>Zhu</last></author>
      <author><first>Michael</first><last>Zeng</last></author>
      <pages>3237-3246</pages>
      <abstract>Commonsense reasoning (CSR) requires models to be equipped with general world knowledge. While CSR is a language-agnostic process, most comprehensive knowledge sources are restricted to a small number of languages, especially English. Thus, it remains unclear how to effectively conduct multilingual commonsense reasoning (XCSR) for various languages. In this work, we propose to use English as a pivot language, utilizing English knowledge sources for our our commonsense reasoning framework via a translate-retrieve-translate (TRT) strategy. For multilingual commonsense questions and answer candidates, we collect related knowledge via translation and retrieval from the knowledge in the source language. The retrieved knowledge is then translated into the target language and integrated into a pre-trained multilingual language model via visible knowledge attention. Then we utilize a diverse of four English knowledge sources to provide more comprehensive coverage of knowledge in different formats. Extensive results on the XCSR benchmark demonstrate that TRT with external knowledge can significantly improve multilingual commonsense reasoning in both zero-shot and translate-train settings, consistently outperforming the state-of-the-art by more than 3% on the multilingual commonsense reasoning benchmark X-CSQA and X-CODAH.</abstract>
      <url hash="85d583b9">2022.findings-acl.255</url>
      <bibkey>fang-etal-2022-leveraging</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/copa">COPA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/commonsenseqa">CommonsenseQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/x-csqa">X-CSQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xcopa">XCOPA</pwcdataset>
    </paper>
    <paper id="256">
      <title>Encoding and Fusing Semantic Connection and Linguistic Evidence for Implicit Discourse Relation Recognition</title>
      <author><first>Wei</first><last>Xiang</last></author>
      <author><first>Bang</first><last>Wang</last></author>
      <author><first>Lu</first><last>Dai</last></author>
      <author><first>Yijun</first><last>Mo</last></author>
      <pages>3247-3257</pages>
      <abstract>Prior studies use one attention mechanism to improve contextual semantic representation learning for implicit discourse relation recognition (IDRR). However, diverse relation senses may benefit from different attention mechanisms. We also argue that some linguistic relation in between two words can be further exploited for IDRR. This paper proposes a Multi-Attentive Neural Fusion (MANF) model to encode and fuse both semantic connection and linguistic evidence for IDRR. In MANF, we design a Dual Attention Network (DAN) to learn and fuse two kinds of attentive representation for arguments as its semantic connection. We also propose an Offset Matrix Network (OMN) to encode the linguistic relations of word-pairs as linguistic evidence. Our MANF model achieves the state-of-the-art results on the PDTB 3.0 corpus.</abstract>
      <url hash="dc374fbb">2022.findings-acl.256</url>
      <bibkey>xiang-etal-2022-encoding</bibkey>
      <pwccode url="https://github.com/hustminslab/manf" additional="false">hustminslab/manf</pwccode>
    </paper>
    <paper id="257">
      <title>One Agent To Rule Them All: Towards Multi-agent Conversational <fixed-case>AI</fixed-case></title>
      <author><first>Christopher</first><last>Clarke</last></author>
      <author><first>Joseph</first><last>Peper</last></author>
      <author><first>Karthik</first><last>Krishnamurthy</last></author>
      <author><first>Walter</first><last>Talamonti</last></author>
      <author><first>Kevin</first><last>Leach</last></author>
      <author><first>Walter</first><last>Lasecki</last></author>
      <author><first>Yiping</first><last>Kang</last></author>
      <author><first>Lingjia</first><last>Tang</last></author>
      <author><first>Jason</first><last>Mars</last></author>
      <pages>3258-3267</pages>
      <abstract>The increasing volume of commercially available conversational agents (CAs) on the market has resulted in users being burdened with learning and adopting multiple agents to accomplish their tasks. Though prior work has explored supporting a multitude of domains within the design of a single agent, the interaction experience suffers due to the large action space of desired capabilities. To address these problems, we introduce a new task BBAI: Black-Box Agent Integration, focusing on combining the capabilities of multiple black-box CAs at scale. We explore two techniques: question agent pairing and question response pairing aimed at resolving this task. Leveraging these techniques, we design One For All (OFA), a scalable system that provides a unified interface to interact with multiple CAs. Additionally, we introduce MARS: Multi-Agent Response Selection, a new encoder model for question response pairing that jointly encodes user question and agent response pairs. We demonstrate that OFA is able to automatically and accurately integrate an ensemble of commercially available CAs spanning disparate domains. Specifically, using the MARS encoder we achieve the highest accuracy on our BBAI task, outperforming strong baselines.</abstract>
      <url hash="7a5806bc">2022.findings-acl.257</url>
      <bibkey>clarke-etal-2022-one</bibkey>
      <pwccode url="https://github.com/ChrisIsKing/black-box-multi-agent-integation" additional="false">ChrisIsKing/black-box-multi-agent-integation</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bbai-dataset">BBAI Dataset</pwcdataset>
    </paper>
    <paper id="258">
      <title>Word-level Perturbation Considering Word Length and Compositional Subwords</title>
      <author><first>Tatsuya</first><last>Hiraoka</last></author>
      <author><first>Sho</first><last>Takase</last></author>
      <author><first>Kei</first><last>Uchiumi</last></author>
      <author><first>Atsushi</first><last>Keyaki</last></author>
      <author><first>Naoaki</first><last>Okazaki</last></author>
      <pages>3268-3275</pages>
      <abstract>We present two simple modifications for word-level perturbation: Word Replacement considering Length (WR-L) and Compositional Word Replacement (CWR).In conventional word replacement, a word in an input is replaced with a word sampled from the entire vocabulary, regardless of the length and context of the target word.WR-L considers the length of a target word by sampling words from the Poisson distribution.CWR considers the compositional candidates by restricting the source of sampling to related words that appear in subword regularization.Experimental results showed that the combination of WR-L and CWR improved the performance of text classification and machine translation.</abstract>
      <url hash="eb299be3">2022.findings-acl.258</url>
      <bibkey>hiraoka-etal-2022-word</bibkey>
      <pwccode url="https://github.com/tathi/cwr" additional="false">tathi/cwr</pwccode>
    </paper>
    <paper id="259">
      <title>Bridging Pre-trained Language Models and Hand-crafted Features for Unsupervised <fixed-case>POS</fixed-case> Tagging</title>
      <author><first>Houquan</first><last>Zhou</last></author>
      <author><first>Yang</first><last>Li</last></author>
      <author><first>Zhenghua</first><last>Li</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <pages>3276-3290</pages>
      <abstract>In recent years, large-scale pre-trained language models (PLMs) have made extraordinary progress in most NLP tasks. But, in the unsupervised POS tagging task, works utilizing PLMs are few and fail to achieve state-of-the-art (SOTA) performance. The recent SOTA performance is yielded by a Guassian HMM variant proposed by He et al. (2018). However, as a generative model, HMM makes very strong independence assumptions, making it very challenging to incorporate contexualized word representations from PLMs. In this work, we for the first time propose a neural conditional random field autoencoder (CRF-AE) model for unsupervised POS tagging. The discriminative encoder of CRF-AE can straightforwardly incorporate ELMo word representations. Moreover, inspired by feature-rich HMM, we reintroduce hand-crafted features into the decoder of CRF-AE. Finally, experiments clearly show that our model outperforms previous state-of-the-art models by a large margin on Penn Treebank and multilingual Universal Dependencies treebank v2.0.</abstract>
      <url hash="aee67253">2022.findings-acl.259</url>
      <attachment type="software" hash="6b9cd4a0">2022.findings-acl.259.software.zip</attachment>
      <bibkey>zhou-etal-2022-bridging</bibkey>
      <pwccode url="https://github.com/Jacob-Zhou/FeatureCRFAE" additional="false">Jacob-Zhou/FeatureCRFAE</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="260">
      <title>Controlling the Focus of Pretrained Language Generation Models</title>
      <author><first>Jiabao</first><last>Ji</last></author>
      <author><first>Yoon</first><last>Kim</last></author>
      <author><first>James</first><last>Glass</last></author>
      <author><first>Tianxing</first><last>He</last></author>
      <pages>3291-3306</pages>
      <abstract>The finetuning of pretrained transformer-based language generation models are typically conducted in an end-to-end manner, where the model learns to attend to relevant parts of the input by itself. However, there does not exist a mechanism to directly control the model’s focus. This work aims to develop a control mechanism by which a user can select spans of context as “highlights” for the model to focus on, and generate relevant output. To achieve this goal, we augment a pretrained model with trainable “focus vectors” that are directly applied to the model’s embeddings, while the model itself is kept fixed. These vectors, trained on automatic annotations derived from attribution methods, act as indicators for context importance. We test our approach on two core generation tasks: dialogue response generation and abstractive summarization. We also collect evaluation data where the highlight-generation pairs are annotated by humans. Our experiments show that the trained focus vectors are effective in steering the model to generate outputs that are relevant to user-selected highlights.</abstract>
      <url hash="c5fe7303">2022.findings-acl.260</url>
      <bibkey>ji-etal-2022-controlling</bibkey>
      <pwccode url="https://github.com/question406/learningtofocus" additional="false">question406/learningtofocus</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/persona-chat-1">PERSONA-CHAT</pwcdataset>
    </paper>
    <paper id="261">
      <title>Comparative Opinion Summarization via Collaborative Decoding</title>
      <author><first>Hayate</first><last>Iso</last></author>
      <author><first>Xiaolan</first><last>Wang</last></author>
      <author><first>Stefanos</first><last>Angelidis</last></author>
      <author><first>Yoshihiko</first><last>Suhara</last></author>
      <pages>3307-3324</pages>
      <abstract>Opinion summarization focuses on generating summaries that reflect popular subjective information expressed in multiple online reviews.While generated summaries offer general and concise information about a particular hotel or product, the information may be insufficient to help the user compare multiple different choices.Thus, the user may still struggle with the question “Which one should I pick?” In this paper, we propose the comparative opinion summarization task, which aims at generating two contrastive summaries and one common summary from two different candidate sets of reviews.We develop a comparative summarization framework CoCoSum, which consists of two base summarization models that jointly generate contrastive and common summaries.Experimental results on a newly created benchmark CoCoTrip show that CoCoSum can produce higher-quality contrastive and common summaries than state-of-the-art opinion summarization models.The dataset and code are available at https://github.com/megagonlabs/cocosum</abstract>
      <url hash="547a496b">2022.findings-acl.261</url>
      <bibkey>iso-etal-2022-comparative</bibkey>
      <pwccode url="https://github.com/megagonlabs/cocosum" additional="false">megagonlabs/cocosum</pwccode>
    </paper>
    <paper id="262">
      <title><fixed-case>I</fixed-case>so<fixed-case>S</fixed-case>core: Measuring the Uniformity of Embedding Space Utilization</title>
      <author><first>William</first><last>Rudman</last></author>
      <author><first>Nate</first><last>Gillman</last></author>
      <author><first>Taylor</first><last>Rayne</last></author>
      <author><first>Carsten</first><last>Eickhoff</last></author>
      <pages>3325-3339</pages>
      <abstract>The recent success of distributed word representations has led to an increased interest in analyzing the properties of their spatial distribution. Several studies have suggested that contextualized word embedding models do not isotropically project tokens into vector space. However, current methods designed to measure isotropy, such as average random cosine similarity and the partition score, have not been thoroughly analyzed and are not appropriate for measuring isotropy. We propose IsoScore: a novel tool that quantifies the degree to which a point cloud uniformly utilizes the ambient vector space. Using rigorously designed tests, we demonstrate that IsoScore is the only tool available in the literature that accurately measures how uniformly distributed variance is across dimensions in vector space. Additionally, we use IsoScore to challenge a number of recent conclusions in the NLP literature that have been derived using brittle metrics of isotropy. We caution future studies from using existing tools to measure isotropy in contextualized embedding space as resulting conclusions will be misleading or altogether inaccurate.</abstract>
      <url hash="841e9bf1">2022.findings-acl.262</url>
      <bibkey>rudman-etal-2022-isoscore</bibkey>
      <pwccode url="https://github.com/bcbi-edu/p_eickhoff_isoscore" additional="false">bcbi-edu/p_eickhoff_isoscore</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-2">WikiText-2</pwcdataset>
    </paper>
    <paper id="263">
      <title>A Natural Diet: Towards Improving Naturalness of Machine Translation Output</title>
      <author><first>Markus</first><last>Freitag</last></author>
      <author><first>David</first><last>Vilar</last></author>
      <author><first>David</first><last>Grangier</last></author>
      <author><first>Colin</first><last>Cherry</last></author>
      <author><first>George</first><last>Foster</last></author>
      <pages>3340-3353</pages>
      <abstract>Machine translation (MT) evaluation often focuses on accuracy and fluency, without paying much attention to translation style. This means that, even when considered accurate and fluent, MT output can still sound less natural than high quality human translations or text originally written in the target language. Machine translation output notably exhibits lower lexical diversity, and employs constructs that mirror those in the source sentence. In this work we propose a method for training MT systems to achieve a more natural style, i.e. mirroring the style of text originally written in the target language. Our method tags parallel training data according to the naturalness of the target side by contrasting language models trained on natural and translated data. Tagging data allows us to put greater emphasis on target sentences originally written in the target language. Automatic metrics show that the resulting models achieve lexical richness on par with human translations, mimicking a style much closer to sentences originally written in the target language. Furthermore, we find that their output is preferred by human experts when compared to the baseline translations.</abstract>
      <url hash="4990c0d1">2022.findings-acl.263</url>
      <bibkey>freitag-etal-2022-natural</bibkey>
    </paper>
    <paper id="264">
      <title>From Stance to Concern: Adaptation of Propositional Analysis to New Tasks and Domains</title>
      <author><first>Brodie</first><last>Mather</last></author>
      <author><first>Bonnie</first><last>Dorr</last></author>
      <author><first>Adam</first><last>Dalton</last></author>
      <author><first>William</first><last>de Beaumont</last></author>
      <author><first>Owen</first><last>Rambow</last></author>
      <author><first>Sonja</first><last>Schmer-Galunder</last></author>
      <pages>3354-3367</pages>
      <abstract>We present a generalized paradigm for adaptation of propositional analysis (predicate-argument pairs) to new tasks and domains. We leverage an analogy between stances (belief-driven sentiment) and concerns (topical issues with moral dimensions/endorsements) to produce an explanatory representation. A key contribution is the combination of semi-automatic resource building for extraction of domain-dependent concern types (with 2-4 hours of human labor per domain) and an entirely automatic procedure for extraction of domain-independent moral dimensions and endorsement values. Prudent (automatic) selection of terms from propositional structures for lexical expansion (via semantic similarity) produces new moral dimension lexicons at three levels of granularity beyond a strong baseline lexicon. We develop a ground truth (GT) based on expert annotators and compare our concern detection output to GT, to yield 231% improvement in recall over baseline, with only a 10% loss in precision. F1 yields 66% improvement over baseline and 97.8% of human performance. Our lexically based approach yields large savings over approaches that employ costly human labor and model building. We provide to the community a newly expanded moral dimension/value lexicon, annotation guidelines, and GT.</abstract>
      <url hash="96902c8f">2022.findings-acl.264</url>
      <bibkey>mather-etal-2022-stance</bibkey>
      <pwccode url="https://github.com/ihmc/findings-of-acl-2022-concern-detection" additional="false">ihmc/findings-of-acl-2022-concern-detection</pwccode>
    </paper>
    <paper id="265">
      <title><fixed-case>CUE</fixed-case> Vectors: Modular Training of Language Models Conditioned on Diverse Contextual Signals</title>
      <author><first>Scott</first><last>Novotney</last></author>
      <author><first>Sreeparna</first><last>Mukherjee</last></author>
      <author><first>Zeeshan</first><last>Ahmed</last></author>
      <author><first>Andreas</first><last>Stolcke</last></author>
      <pages>3368-3379</pages>
      <abstract>We propose a framework to modularize the training of neural language models that use diverse forms of context by eliminating the need to jointly train context and within-sentence encoders. Our approach, contextual universal embeddings (CUE), trains LMs on one type of contextual data and adapts to novel context types. The model consists of a pretrained neural sentence LM, a BERT-based contextual encoder, and a masked transfomer decoder that estimates LM probabilities using sentence-internal and contextual evidence.When contextually annotated data is unavailable, our model learns to combine contextual and sentence-internal information using noisy oracle unigram embeddings as a proxy. Real context data can be introduced later and used to adapt a small number of parameters that map contextual data into the decoder’s embedding space. We validate the CUE framework on a NYTimes text corpus with multiple metadata types, for which the LM perplexity can be lowered from 36.6 to 27.4 by conditioning on context. Bootstrapping a contextual LM with only a subset of the metadata during training retains 85% of the achievable gain. Training the model initially with proxy context retains 67% of the perplexity gain after adapting to real context. Furthermore, we can swap one type of pretrained sentence LM for another without retraining the context encoders, by only adapting the decoder model. Overall, we obtain a modular framework that allows incremental, scalable training of context-enhanced LMs.</abstract>
      <url hash="23c93066">2022.findings-acl.265</url>
      <bibkey>novotney-etal-2022-cue</bibkey>
    </paper>
    <paper id="266">
      <title>Cross-Lingual <fixed-case>UMLS</fixed-case> Named Entity Linking using <fixed-case>UMLS</fixed-case> Dictionary Fine-Tuning</title>
      <author><first>Rina</first><last>Galperin</last></author>
      <author><first>Shachar</first><last>Schnapp</last></author>
      <author><first>Michael</first><last>Elhadad</last></author>
      <pages>3380-3390</pages>
      <abstract>We study cross-lingual UMLS named entity linking, where mentions in a given source language are mapped to UMLS concepts, most of which are labeled in English. Our cross-lingual framework includes an offline unsupervised construction of a translated UMLS dictionary and a per-document pipeline which identifies UMLS candidate mentions and uses a fine-tuned pretrained transformer language model to filter candidates according to context. Our method exploits a small dataset of manually annotated UMLS mentions in the source language and uses this supervised data in two ways: to extend the unsupervised UMLS dictionary and to fine-tune the contextual filtering of candidate mentions in full documents.We demonstrate results of our approach on both Hebrew and English. We achieve new state-of-the-art (SOTA) results on the Hebrew Camoni corpus, +8.9 F1 on average across three communities in the dataset. We also achieve new SOTA on the English dataset MedMentions with +7.3 F1.</abstract>
      <url hash="878ef369">2022.findings-acl.266</url>
      <attachment type="software" hash="70d7f6ef">2022.findings-acl.266.software.zip</attachment>
      <bibkey>galperin-etal-2022-cross</bibkey>
      <pwccode url="https://github.com/rinagalperin/biomedical_nel" additional="false">rinagalperin/biomedical_nel</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bc5cdr">BC5CDR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/medmentions">MedMentions</pwcdataset>
    </paper>
    <paper id="267">
      <title>Aligned Weight Regularizers for Pruning Pretrained Neural Networks</title>
      <author><first>James</first><last>O’ Neill</last></author>
      <author><first>Sourav</first><last>Dutta</last></author>
      <author><first>Haytham</first><last>Assem</last></author>
      <pages>3391-3401</pages>
      <abstract>Pruning aims to reduce the number of parameters while maintaining performance close to the original network. This work proposes a novel <i>self-distillation</i> based pruning strategy, whereby the representational similarity between the pruned and unpruned versions of the same network is maximized. Unlike previous approaches that treat distillation and pruning separately, we use distillation to inform the pruning criteria, without requiring a separate student network as in knowledge distillation. We show that the proposed <i>cross-correlation objective for self-distilled pruning</i> implicitly encourages sparse solutions, naturally complementing magnitude-based pruning criteria. Experiments on the GLUE and XGLUE benchmarks show that self-distilled pruning increases mono- and cross-lingual language model performance. Self-distilled pruned models also outperform smaller Transformers with an equal number of parameters and are competitive against (6 times) larger distilled networks. We also observe that self-distillation (1) maximizes class separability, (2) increases the signal-to-noise ratio, and (3) converges faster after pruning steps, providing further insights into why self-distilled pruning improves generalization. </abstract>
      <url hash="fdb3ff58">2022.findings-acl.267</url>
      <bibkey>o-neill-etal-2022-aligned</bibkey>
    </paper>
    <paper id="268">
      <title>Consistent Representation Learning for Continual Relation Extraction</title>
      <author><first>Kang</first><last>Zhao</last></author>
      <author><first>Hua</first><last>Xu</last></author>
      <author><first>Jiangong</first><last>Yang</last></author>
      <author><first>Kai</first><last>Gao</last></author>
      <pages>3402-3411</pages>
      <abstract>Continual relation extraction (CRE) aims to continuously train a model on data with new relations while avoiding forgetting old ones. Some previous work has proved that storing a few typical samples of old relations and replaying them when learning new relations can effectively avoid forgetting. However, these memory-based methods tend to overfit the memory samples and perform poorly on imbalanced datasets. To solve these challenges, a consistent representation learning method is proposed, which maintains the stability of the relation embedding by adopting contrastive learning and knowledge distillation when replaying memory. Specifically, supervised contrastive learning based on a memory bank is first used to train each new task so that the model can effectively learn the relation representation. Then, contrastive replay is conducted of the samples in memory and makes the model retain the knowledge of historical relations through memory knowledge distillation to prevent the catastrophic forgetting of the old task. The proposed method can better learn consistent representations to alleviate forgetting effectively. Extensive experiments on FewRel and TACRED datasets show that our method significantly outperforms state-of-the-art baselines and yield strong robustness on the imbalanced dataset.</abstract>
      <url hash="99b3b70e">2022.findings-acl.268</url>
      <bibkey>zhao-etal-2022-consistent</bibkey>
      <pwccode url="https://github.com/thuiar/CRL" additional="false">thuiar/CRL</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel">FewRel</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tacred">TACRED</pwcdataset>
    </paper>
    <paper id="269">
      <title>Event Transition Planning for Open-ended Text Generation</title>
      <author><first>Qintong</first><last>Li</last></author>
      <author><first>Piji</first><last>Li</last></author>
      <author><first>Wei</first><last>Bi</last></author>
      <author><first>Zhaochun</first><last>Ren</last></author>
      <author><first>Yuxuan</first><last>Lai</last></author>
      <author><first>Lingpeng</first><last>Kong</last></author>
      <pages>3412-3426</pages>
      <abstract>Open-ended text generation tasks, such as dialogue generation and story completion, require models to generate a coherent continuation given limited preceding context. The open-ended nature of these tasks brings new challenges to the neural auto-regressive text generators nowadays. Despite these neural models are good at producing human-like text, it is difficult for them to arrange causalities and relations between given facts and possible ensuing events. To bridge this gap, we propose a novel two-stage method which explicitly arranges the ensuing events in open-ended text generation. Our approach can be understood as a specially-trained coarse-to-fine algorithm, where an event transition planner provides a “coarse” plot skeleton and a text generator in the second stage refines the skeleton. Experiments on two open-ended text generation tasks demonstrate that our proposed method effectively improves the quality of the generated text, especially in coherence and diversity. We will release the codes to the community for further exploration.</abstract>
      <url hash="d61b35ed">2022.findings-acl.269</url>
      <bibkey>li-etal-2022-event</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/atomic">ATOMIC</pwcdataset>
    </paper>
    <paper id="270">
      <title>Comprehensive Multi-Modal Interactions for Referring Image Segmentation</title>
      <author><first>Kanishk</first><last>Jain</last></author>
      <author><first>Vineet</first><last>Gandhi</last></author>
      <pages>3427-3435</pages>
      <abstract>We investigate Referring Image Segmentation (RIS), which outputs a segmentation map corresponding to the natural language description. Addressing RIS efficiently requires considering the interactions happening across visual and linguistic modalities and the interactions within each modality. Existing methods are limited because they either compute different forms of interactions sequentially (leading to error propagation) or ignore intra-modal interactions. We address this limitation by performing all three interactions simultaneously through a Synchronous Multi-Modal Fusion Module (SFM). Moreover, to produce refined segmentation masks, we propose a novel Hierarchical Cross-Modal Aggregation Module (HCAM), where linguistic features facilitate the exchange of contextual information across the visual hierarchy. We present thorough ablation studies and validate our approach’s performance on four benchmark datasets, showing considerable performance gains over the existing state-of-the-art (SOTA) methods.</abstract>
      <url hash="bc704ead">2022.findings-acl.270</url>
      <attachment type="software" hash="12479973">2022.findings-acl.270.software.zip</attachment>
      <bibkey>jain-gandhi-2022-comprehensive</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/google-refexp">Google Refexp</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/refcoco">RefCOCO</pwcdataset>
    </paper>
    <paper id="271">
      <title><fixed-case>M</fixed-case>eta<fixed-case>W</fixed-case>eighting: Learning to Weight Tasks in Multi-Task Learning</title>
      <author><first>Yuren</first><last>Mao</last></author>
      <author><first>Zekai</first><last>Wang</last></author>
      <author><first>Weiwei</first><last>Liu</last></author>
      <author><first>Xuemin</first><last>Lin</last></author>
      <author><first>Pengtao</first><last>Xie</last></author>
      <pages>3436-3448</pages>
      <abstract>Task weighting, which assigns weights on the including tasks during training, significantly matters the performance of Multi-task Learning (MTL); thus, recently, there has been an explosive interest in it. However, existing task weighting methods assign weights only based on the training loss, while ignoring the gap between the training loss and generalization loss. It degenerates MTL’s performance. To address this issue, the present paper proposes a novel task weighting algorithm, which automatically weights the tasks via a learning-to-learn paradigm, referred to as MetaWeighting. Extensive experiments are conducted to validate the superiority of our proposed method in multi-task text classification.</abstract>
      <url hash="4a489e26">2022.findings-acl.271</url>
      <attachment type="software" hash="b2479293">2022.findings-acl.271.software.zip</attachment>
      <bibkey>mao-etal-2022-metaweighting</bibkey>
    </paper>
    <paper id="272">
      <title>Improving Controllable Text Generation with Position-Aware Weighted Decoding</title>
      <author><first>Yuxuan</first><last>Gu</last></author>
      <author><first>Xiaocheng</first><last>Feng</last></author>
      <author><first>Sicheng</first><last>Ma</last></author>
      <author><first>Jiaming</first><last>Wu</last></author>
      <author><first>Heng</first><last>Gong</last></author>
      <author><first>Bing</first><last>Qin</last></author>
      <pages>3449-3467</pages>
      <abstract>Weighted decoding methods composed of the pretrained language model (LM) and the controller have achieved promising results for controllable text generation. However, these models often suffer from a control strength/fluency trade-off problem as higher control strength is more likely to generate incoherent and repetitive text. In this paper, we illustrate this trade-off is arisen by the controller imposing the target attribute on the LM at improper positions. And we propose a novel framework based on existing weighted decoding methods called CAT-PAW, which introduces a lightweight regulator to adjust bias signals from the controller at different decoding positions. Experiments on positive sentiment control, topic control, and language detoxification show the effectiveness of our CAT-PAW upon 4 SOTA models.</abstract>
      <url hash="f21d317b">2022.findings-acl.272</url>
      <bibkey>gu-etal-2022-improving</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="273">
      <title>Prompt Tuning for Discriminative Pre-trained Language Models</title>
      <author><first>Yuan</first><last>Yao</last></author>
      <author><first>Bowen</first><last>Dong</last></author>
      <author><first>Ao</first><last>Zhang</last></author>
      <author><first>Zhengyan</first><last>Zhang</last></author>
      <author><first>Ruobing</first><last>Xie</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Leyu</first><last>Lin</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <author><first>Jianyong</first><last>Wang</last></author>
      <pages>3468-3473</pages>
      <abstract>Recent works have shown promising results of prompt tuning in stimulating pre-trained language models (PLMs) for natural language processing (NLP) tasks. However, to the best of our knowledge, existing works focus on prompt-tuning generative PLMs that are pre-trained to generate target tokens, such as BERT. It is still unknown whether and how discriminative PLMs, e.g., ELECTRA, can be effectively prompt-tuned. In this work, we present DPT, the first prompt tuning framework for discriminative PLMs, which reformulates NLP tasks into a discriminative language modeling problem. Comprehensive experiments on text classification and question answering show that, compared with vanilla fine-tuning, DPT achieves significantly higher performance, and also prevents the unstable problem in tuning large PLMs in both full-set and low-resource settings.</abstract>
      <url hash="5aea6728">2022.findings-acl.273</url>
      <attachment type="software" hash="584d5101">2022.findings-acl.273.software.zip</attachment>
      <bibkey>yao-etal-2022-prompt</bibkey>
      <pwccode url="https://github.com/thunlp/dpt" additional="false">thunlp/dpt</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/quoref">Quoref</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="274">
      <title>Two Birds with One Stone: Unified Model Learning for Both Recall and Ranking in News Recommendation</title>
      <author><first>Chuhan</first><last>Wu</last></author>
      <author><first>Fangzhao</first><last>Wu</last></author>
      <author><first>Tao</first><last>Qi</last></author>
      <author><first>Yongfeng</first><last>Huang</last></author>
      <pages>3474-3480</pages>
      <abstract>Recall and ranking are two critical steps in personalized news recommendation. Most existing news recommender systems conduct personalized news recall and ranking separately with different models. However, maintaining multiple models leads to high computational cost and poses great challenges to meeting the online latency requirement of news recommender systems. In order to handle this problem, in this paper we propose UniRec, a unified method for recall and ranking in news recommendation. In our method, we first infer user embedding for ranking from the historical news click behaviors of a user using a user encoder model. Then we derive the user embedding for recall from the obtained user embedding for ranking by using it as the attention query to select a set of basis user embeddings which encode different general user interests and synthesize them into a user embedding for recall. The extensive experiments on benchmark dataset demonstrate that our method can improve both efficiency and effectiveness for recall and ranking in news recommendation.</abstract>
      <url hash="6f8bb7c4">2022.findings-acl.274</url>
      <bibkey>wu-etal-2022-two</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mind">MIND</pwcdataset>
    </paper>
    <paper id="275">
      <title>What does it take to bake a cake? The <fixed-case>R</fixed-case>ecipe<fixed-case>R</fixed-case>ef corpus and anaphora resolution in procedural text</title>
      <author><first>Biaoyan</first><last>Fang</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <author><first>Karin</first><last>Verspoor</last></author>
      <pages>3481-3495</pages>
      <abstract>Procedural text contains rich anaphoric phenomena, yet has not received much attention in NLP. To fill this gap, we investigate the textual properties of two types of procedural text, recipes and chemical patents, and generalize an anaphora annotation framework developed for the chemical domain for modeling anaphoric phenomena in recipes. We apply this framework to annotate the RecipeRef corpus with both bridging and coreference relations. Through comparison to chemical patents, we show the complexity of anaphora resolution in recipes. We demonstrate empirically that transfer learning from the chemical domain improves resolution of anaphora in recipes, suggesting transferability of general procedural knowledge.</abstract>
      <url hash="74bab014">2022.findings-acl.275</url>
      <bibkey>fang-etal-2022-take</bibkey>
      <pwccode url="https://github.com/biaoyanf/reciperef" additional="false">biaoyanf/reciperef</pwccode>
    </paper>
    <paper id="276">
      <title><fixed-case>MERI</fixed-case>t: <fixed-case>M</fixed-case>eta-<fixed-case>P</fixed-case>ath <fixed-case>G</fixed-case>uided <fixed-case>C</fixed-case>ontrastive <fixed-case>L</fixed-case>earning for <fixed-case>L</fixed-case>ogical <fixed-case>R</fixed-case>easoning</title>
      <author><first>Fangkai</first><last>Jiao</last></author>
      <author><first>Yangyang</first><last>Guo</last></author>
      <author><first>Xuemeng</first><last>Song</last></author>
      <author><first>Liqiang</first><last>Nie</last></author>
      <pages>3496-3509</pages>
      <abstract>Logical reasoning is of vital importance to natural language understanding. Previous studies either employ graph-based models to incorporate prior knowledge about logical relations, or introduce symbolic logic into neural models through data augmentation. These methods, however, heavily depend on annotated training data, and thus suffer from over-fitting and poor generalization problems due to the dataset sparsity. To address these two problems, in this paper, we propose MERIt, a MEta-path guided contrastive learning method for logical ReasonIng of text, to perform self-supervised pre-training on abundant unlabeled text data. Two novel strategies serve as indispensable components of our method. In particular, a strategy based on meta-path is devised to discover the logical structure in natural texts, followed by a counterfactual data augmentation strategy to eliminate the information shortcut induced by pre-training. The experimental results on two challenging logical reasoning benchmarks, i.e., ReClor and LogiQA, demonstrate that our method outperforms the SOTA baselines with significant improvements.</abstract>
      <url hash="dbd58be1">2022.findings-acl.276</url>
      <bibkey>jiao-etal-2022-merit</bibkey>
      <pwccode url="https://github.com/sparkjiao/merit" additional="false">sparkjiao/merit</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/logiqa">LogiQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/reclor">ReClor</pwcdataset>
    </paper>
    <paper id="277">
      <title><fixed-case>THE</fixed-case>-<fixed-case>X</fixed-case>: Privacy-Preserving Transformer Inference with Homomorphic Encryption</title>
      <author><first>Tianyu</first><last>Chen</last></author>
      <author><first>Hangbo</first><last>Bao</last></author>
      <author><first>Shaohan</first><last>Huang</last></author>
      <author><first>Li</first><last>Dong</last></author>
      <author><first>Binxing</first><last>Jiao</last></author>
      <author><first>Daxin</first><last>Jiang</last></author>
      <author><first>Haoyi</first><last>Zhou</last></author>
      <author><first>Jianxin</first><last>Li</last></author>
      <author><first>Furu</first><last>Wei</last></author>
      <pages>3510-3520</pages>
      <abstract>As more and more pre-trained language models adopt on-cloud deployment, the privacy issues grow quickly, mainly for the exposure of plain-text user data (e.g., search history, medical record, bank account). Privacy-preserving inference of transformer models is on the demand of cloud service users. To protect privacy, it is an attractive choice to compute only with ciphertext in homomorphic encryption (HE). However, enabling pre-trained models inference on ciphertext data is difficult due to the complex computations in transformer blocks, which are not supported by current HE tools yet. In this work, we introduce THE-X, an approximation approach for transformers, which enables privacy-preserving inference of pre-trained models developed by popular frameworks. THE-X proposes a workflow to deal with complex computation in transformer networks, including all the non-polynomial functions like GELU, softmax, and LayerNorm. Experiments reveal our proposed THE-X can enable transformer inference on encrypted data for different downstream tasks, all with negligible performance drop but enjoying the theory-guaranteed privacy-preserving advantage.</abstract>
      <url hash="faf393bb">2022.findings-acl.277</url>
      <attachment type="software" hash="0978f6bd">2022.findings-acl.277.software.zip</attachment>
      <bibkey>chen-etal-2022-x</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="278">
      <title><fixed-case>HLDC</fixed-case>: <fixed-case>H</fixed-case>indi Legal Documents Corpus</title>
      <author><first>Arnav</first><last>Kapoor</last></author>
      <author><first>Mudit</first><last>Dhawan</last></author>
      <author><first>Anmol</first><last>Goel</last></author>
      <author><first>Arjun</first><last>T H</last></author>
      <author><first>Akshala</first><last>Bhatnagar</last></author>
      <author><first>Vibhu</first><last>Agrawal</last></author>
      <author><first>Amul</first><last>Agrawal</last></author>
      <author><first>Arnab</first><last>Bhattacharya</last></author>
      <author><first>Ponnurangam</first><last>Kumaraguru</last></author>
      <author><first>Ashutosh</first><last>Modi</last></author>
      <pages>3521-3536</pages>
      <abstract>Many populous countries including India are burdened with a considerable backlog of legal cases. Development of automated systems that could process legal documents and augment legal practitioners can mitigate this. However, there is a dearth of high-quality corpora that is needed to develop such data-driven systems. The problem gets even more pronounced in the case of low resource languages such as Hindi. In this resource paper, we introduce the Hindi Legal Documents Corpus (HLDC), a corpus of more than 900K legal documents in Hindi. Documents are cleaned and structured to enable the development of downstream applications. Further, as a use-case for the corpus, we introduce the task of bail prediction. We experiment with a battery of models and propose a Multi-Task Learning (MTL) based model for the same. MTL models use summarization as an auxiliary task along with bail prediction as the main task. Experiments with different models are indicative of the need for further research in this area.</abstract>
      <url hash="0e114075">2022.findings-acl.278</url>
      <attachment type="software" hash="c763b971">2022.findings-acl.278.software.zip</attachment>
      <bibkey>kapoor-etal-2022-hldc</bibkey>
      <pwccode url="https://github.com/exploration-lab/hldc" additional="false">exploration-lab/hldc</pwccode>
    </paper>
    <paper id="279">
      <title>Rethinking Document-level Neural Machine Translation</title>
      <author><first>Zewei</first><last>Sun</last></author>
      <author><first>Mingxuan</first><last>Wang</last></author>
      <author><first>Hao</first><last>Zhou</last></author>
      <author><first>Chengqi</first><last>Zhao</last></author>
      <author><first>Shujian</first><last>Huang</last></author>
      <author><first>Jiajun</first><last>Chen</last></author>
      <author><first>Lei</first><last>Li</last></author>
      <pages>3537-3548</pages>
      <abstract>This paper does not aim at introducing a novel model for document-level neural machine translation. Instead, we head back to the original Transformer model and hope to answer the following question: Is the capacity of current models strong enough for document-level translation? Interestingly, we observe that the original Transformer with appropriate training techniques can achieve strong results for document translation, even with a length of 2000 words. We evaluate this model and several recent approaches on nine document-level datasets and two sentence-level datasets across six languages. Experiments show that document-level Transformer models outperforms sentence-level ones and many previous methods in a comprehensive set of metrics, including BLEU, four lexical indices, three newly proposed assistant linguistic indicators, and human evaluation.</abstract>
      <url hash="621a01ad">2022.findings-acl.279</url>
      <bibkey>sun-etal-2022-rethinking</bibkey>
      <pwccode url="https://github.com/sunzewei2715/Doc2Doc_NMT" additional="false">sunzewei2715/Doc2Doc_NMT</pwccode>
    </paper>
    <paper id="280">
      <title>Incremental Intent Detection for Medical Domain with Contrast Replay Networks</title>
      <author><first>Guirong</first><last>Bai</last></author>
      <author><first>Shizhu</first><last>He</last></author>
      <author><first>Kang</first><last>Liu</last></author>
      <author><first>Jun</first><last>Zhao</last></author>
      <pages>3549-3556</pages>
      <abstract>Conventional approaches to medical intent detection require fixed pre-defined intent categories. However, due to the incessant emergence of new medical intents in the real world, such requirement is not practical. Considering that it is computationally expensive to store and re-train the whole data every time new data and intents come in, we propose to incrementally learn emerged intents while avoiding catastrophically forgetting old intents. We first formulate incremental learning for medical intent detection. Then, we employ a memory-based method to handle incremental learning. We further propose to enhance the method with contrast replay networks, which use multilevel distillation and contrast objective to address training data imbalance and medical rare words respectively. Experiments show that the proposed method outperforms the state-of-the-art model by 5.7% and 9.1% of accuracy on two benchmarks respectively.</abstract>
      <url hash="32ae01f5">2022.findings-acl.280</url>
      <bibkey>bai-etal-2022-incremental</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/kuake-qic">KUAKE-QIC</pwcdataset>
    </paper>
    <paper id="281">
      <title><fixed-case>L</fixed-case>a<fixed-case>P</fixed-case>ra<fixed-case>D</fixed-case>o<fixed-case>R</fixed-case>: Unsupervised Pretrained Dense Retriever for Zero-Shot Text Retrieval</title>
      <author><first>Canwen</first><last>Xu</last></author>
      <author><first>Daya</first><last>Guo</last></author>
      <author><first>Nan</first><last>Duan</last></author>
      <author><first>Julian</first><last>McAuley</last></author>
      <pages>3557-3569</pages>
      <abstract>In this paper, we propose LaPraDoR, a pretrained dual-tower dense retriever that does not require any supervised data for training. Specifically, we first present Iterative Contrastive Learning (ICoL) that iteratively trains the query and document encoders with a cache mechanism. ICoL not only enlarges the number of negative instances but also keeps representations of cached examples in the same hidden space. We then propose Lexicon-Enhanced Dense Retrieval (LEDR) as a simple yet effective way to enhance dense retrieval with lexical matching. We evaluate LaPraDoR on the recently proposed BEIR benchmark, including 18 datasets of 9 zero-shot text retrieval tasks. Experimental results show that LaPraDoR achieves state-of-the-art performance compared with supervised dense retrieval models, and further analysis reveals the effectiveness of our training strategy and objectives. Compared to re-ranking, our lexicon-enhanced approach can be run in milliseconds (22.5x faster) while achieving superior performance.</abstract>
      <url hash="3b59a182">2022.findings-acl.281</url>
      <bibkey>xu-etal-2022-laprador</bibkey>
      <pwccode url="https://github.com/jetrunner/laprador" additional="false">jetrunner/laprador</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/beir">BEIR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/c4">C4</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/climate-fever">CLIMATE-FEVER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/scifact">SciFact</pwcdataset>
    </paper>
    <paper id="282">
      <title>Do Pre-trained Models Benefit Knowledge Graph Completion? A Reliable Evaluation and a Reasonable Approach</title>
      <author><first>Xin</first><last>Lv</last></author>
      <author><first>Yankai</first><last>Lin</last></author>
      <author><first>Yixin</first><last>Cao</last></author>
      <author><first>Lei</first><last>Hou</last></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Peng</first><last>Li</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>3570-3581</pages>
      <abstract>In recent years, pre-trained language models (PLMs) have been shown to capture factual knowledge from massive texts, which encourages the proposal of PLM-based knowledge graph completion (KGC) models. However, these models are still quite behind the SOTA KGC models in terms of performance. In this work, we find two main reasons for the weak performance: (1) Inaccurate evaluation setting. The evaluation setting under the closed-world assumption (CWA) may underestimate the PLM-based KGC models since they introduce more external knowledge; (2) Inappropriate utilization of PLMs. Most PLM-based KGC models simply splice the labels of entities and relations as inputs, leading to incoherent sentences that do not take full advantage of the implicit knowledge in PLMs. To alleviate these problems, we highlight a more accurate evaluation setting under the open-world assumption (OWA), which manual checks the correctness of knowledge that is not in KGs. Moreover, motivated by prompt tuning, we propose a novel PLM-based KGC model named PKGC. The basic idea is to convert each triple and its support information into natural prompt sentences, which is further fed into PLMs for classification. Experiment results on two KGC datasets demonstrate OWA is more reliable for evaluating KGC, especially on the link prediction, and the effectiveness of our PKCG model on both CWA and OWA settings.</abstract>
      <url hash="5207eac2">2022.findings-acl.282</url>
      <bibkey>lv-etal-2022-pre</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/inferwiki">InferWiki</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/lama">LAMA</pwcdataset>
    </paper>
    <paper id="283">
      <title><fixed-case>EICO</fixed-case>: Improving Few-Shot Text Classification via Explicit and Implicit Consistency Regularization</title>
      <author><first>Lei</first><last>Zhao</last></author>
      <author><first>Cheng</first><last>Yao</last></author>
      <pages>3582-3587</pages>
      <abstract>While the prompt-based fine-tuning methods had advanced few-shot natural language understanding tasks, self-training methods are also being explored. This work revisits the consistency regularization in self-training and presents explicit and implicit consistency regularization enhanced language model (EICO). By employing both explicit and implicit consistency regularization, EICO advances the performance of prompt-based few-shot text classification. For implicit consistency regularization, we generate pseudo-label from the weakly-augmented view and predict pseudo-label from the strongly-augmented view. For explicit consistency regularization, we minimize the difference between the prediction of the augmentation view and the prediction of the original view. We conducted extensive experiments on six text classification datasets and found that with sixteen labeled examples, EICO achieves competitive performance compared to existing self-training few-shot learning methods.</abstract>
      <url hash="cb6afa30">2022.findings-acl.283</url>
      <bibkey>zhao-yao-2022-eico</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mpqa-opinion-corpus">MPQA Opinion Corpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="284">
      <title>Improving the Adversarial Robustness of <fixed-case>NLP</fixed-case> Models by Information Bottleneck</title>
      <author><first>Cenyuan</first><last>Zhang</last></author>
      <author><first>Xiang</first><last>Zhou</last></author>
      <author><first>Yixin</first><last>Wan</last></author>
      <author><first>Xiaoqing</first><last>Zheng</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <author><first>Cho-Jui</first><last>Hsieh</last></author>
      <pages>3588-3598</pages>
      <abstract>Existing studies have demonstrated that adversarial examples can be directly attributed to the presence of non-robust features, which are highly predictive, but can be easily manipulated by adversaries to fool NLP models. In this study, we explore the feasibility of capturing task-specific robust features, while eliminating the non-robust ones by using the information bottleneck theory. Through extensive experiments, we show that the models trained with our information bottleneck-based method are able to achieve a significant improvement in robust accuracy, exceeding performances of all the previously reported defense methods while suffering almost no performance drop in clean accuracy on SST-2, AGNEWS and IMDB datasets.</abstract>
      <url hash="cb62af9f">2022.findings-acl.284</url>
      <attachment type="software" hash="0a86dadd">2022.findings-acl.284.software.zip</attachment>
      <bibkey>zhang-etal-2022-improving</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="285">
      <title>Incorporating Dynamic Semantics into Pre-Trained Language Model for Aspect-based Sentiment Analysis</title>
      <author><first>Kai</first><last>Zhang</last></author>
      <author><first>Kun</first><last>Zhang</last></author>
      <author><first>Mengdi</first><last>Zhang</last></author>
      <author><first>Hongke</first><last>Zhao</last></author>
      <author><first>Qi</first><last>Liu</last></author>
      <author><first>Wei</first><last>Wu</last></author>
      <author><first>Enhong</first><last>Chen</last></author>
      <pages>3599-3610</pages>
      <abstract>Aspect-based sentiment analysis (ABSA) predicts sentiment polarity towards a specific aspect in the given sentence. While pre-trained language models such as BERT have achieved great success, incorporating dynamic semantic changes into ABSA remains challenging. To this end, in this paper, we propose to address this problem by Dynamic Re-weighting BERT (DR-BERT), a novel method designed to learn dynamic aspect-oriented semantics for ABSA. Specifically, we first take the Stack-BERT layers as a primary encoder to grasp the overall semantic of the sentence and then fine-tune it by incorporating a lightweight Dynamic Re-weighting Adapter (DRA). Note that the DRA can pay close attention to a small region of the sentences at each step and re-weigh the vitally important words for better aspect-aware sentiment understanding. Finally, experimental results on three benchmark datasets demonstrate the effectiveness and the rationality of our proposed model and provide good interpretable insights for future semantic modeling.</abstract>
      <url hash="38af9ebf">2022.findings-acl.285</url>
      <bibkey>zhang-etal-2022-incorporating</bibkey>
    </paper>
    <paper id="286">
      <title><fixed-case>DARER</fixed-case>: Dual-task Temporal Relational Recurrent Reasoning Network for Joint Dialog Sentiment Classification and Act Recognition</title>
      <author><first>Bowen</first><last>Xing</last></author>
      <author><first>Ivor</first><last>Tsang</last></author>
      <pages>3611-3621</pages>
      <abstract>The task of joint dialog sentiment classification (DSC) and act recognition (DAR) aims to simultaneously predict the sentiment label and act label for each utterance in a dialog.In this paper, we put forward a new framework which models the explicit dependencies via integrating <i>prediction-level interactions</i> other than semantics-level interactions, more consistent with human intuition.Besides, we propose a speaker-aware temporal graph (SATG) and a dual-task relational temporal graph (DRTG) to introduce <i>temporal relations</i> into dialog understanding and dual-task reasoning. To implement our framework, we propose a novel model dubbed DARER, which first generates the context-, speaker- and temporal-sensitive utterance representations via modeling SATG, then conducts recurrent dual-task relational reasoning on DRTG, in which process the estimated label distributions act as key clues in prediction-level interactions.Experiment results show that DARER outperforms existing models by large margins while requiring much less computation resource and costing less training time.Remarkably, on DSC task in Mastodon, DARER gains a relative improvement of about 25% over previous best model in terms of F1, with less than 50% parameters and about only 60% required GPU memory.</abstract>
      <url hash="85b0c945">2022.findings-acl.286</url>
      <bibkey>xing-tsang-2022-darer</bibkey>
      <pwccode url="https://github.com/xingbowen714/darer" additional="false">xingbowen714/darer</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog">DailyDialog</pwcdataset>
    </paper>
    <paper id="287">
      <title>Divide and Conquer: Text Semantic Matching with Disentangled Keywords and Intents</title>
      <author><first>Yicheng</first><last>Zou</last></author>
      <author><first>Hongwei</first><last>Liu</last></author>
      <author><first>Tao</first><last>Gui</last></author>
      <author><first>Junzhe</first><last>Wang</last></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <author><first>Meng</first><last>Tang</last></author>
      <author><first>Haixiang</first><last>Li</last></author>
      <author><first>Daniell</first><last>Wang</last></author>
      <pages>3622-3632</pages>
      <abstract>Text semantic matching is a fundamental task that has been widely used in various scenarios, such as community question answering, information retrieval, and recommendation. Most state-of-the-art matching models, e.g., BERT, directly perform text comparison by processing each word uniformly. However, a query sentence generally comprises content that calls for different levels of matching granularity. Specifically, keywords represent factual information such as action, entity, and event that should be strictly matched, while intents convey abstract concepts and ideas that can be paraphrased into various expressions. In this work, we propose a simple yet effective training strategy for text semantic matching in a divide-and-conquer manner by disentangling keywords from intents. Our approach can be easily combined with pre-trained language models (PLM) without influencing their inference efficiency, achieving stable performance improvements against a wide range of PLMs on three benchmarks.</abstract>
      <url hash="d0296395">2022.findings-acl.287</url>
      <bibkey>zou-etal-2022-divide</bibkey>
      <pwccode url="https://github.com/rowitzou/dc-match" additional="false">rowitzou/dc-match</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
    </paper>
    <paper id="288">
      <title>Modular Domain Adaptation</title>
      <author><first>Junshen</first><last>Chen</last></author>
      <author><first>Dallas</first><last>Card</last></author>
      <author><first>Dan</first><last>Jurafsky</last></author>
      <pages>3633-3655</pages>
      <abstract>Off-the-shelf models are widely used by computational social science researchers to measure properties of text, such as sentiment.However, without access to source data it is difficult to account for domain shift, which represents a threat to validity. Here, we treat domain adaptation as a modular process that involves separate model producers and model consumers, and show how they can independently cooperate to facilitate more accurate measurements of text. We introduce two lightweight techniques for this scenario, and demonstrate that they reliably increase out-of-domain accuracy on four multi-domain text classification datasets when used with linear and contextual embedding models. We conclude with recommendations for model producers and consumers, and release models and replication code to accompany this paper.</abstract>
      <url hash="5a6e17ee">2022.findings-acl.288</url>
      <bibkey>chen-etal-2022-modular</bibkey>
      <pwccode url="https://github.com/jkvc/modular-domain-adaptation" additional="false">jkvc/modular-domain-adaptation</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="289">
      <title>Detection of Adversarial Examples in Text Classification: Benchmark and Baseline via Robust Density Estimation</title>
      <author><first>KiYoon</first><last>Yoo</last></author>
      <author><first>Jangho</first><last>Kim</last></author>
      <author><first>Jiho</first><last>Jang</last></author>
      <author><first>Nojun</first><last>Kwak</last></author>
      <pages>3656-3672</pages>
      <abstract>Word-level adversarial attacks have shown success in NLP models, drastically decreasing the performance of transformer-based models in recent years. As a countermeasure, adversarial defense has been explored, but relatively few efforts have been made to detect adversarial examples. However, detecting adversarial examples may be crucial for automated tasks (e.g. review sentiment analysis) that wish to amass information about a certain population and additionally be a step towards a robust defense system. To this end, we release a dataset for four popular attack methods on four datasets and four models to encourage further research in this field. Along with it, we propose a competitive baseline based on density estimation that has the highest auc on 29 out of 30 dataset-attack-model combinations. The source code is released (https://github.com/bangawayoo/adversarial-examples-in-text-classification).</abstract>
      <url hash="1cbf832b">2022.findings-acl.289</url>
      <attachment type="software" hash="1dc5921e">2022.findings-acl.289.software.zip</attachment>
      <bibkey>yoo-etal-2022-detection</bibkey>
      <pwccode url="https://github.com/bangawayoo/adversarial-examples-in-text-classification" additional="false">bangawayoo/adversarial-examples-in-text-classification</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="290">
      <title><fixed-case>P</fixed-case>latt-Bin: Efficient Posterior Calibrated Training for <fixed-case>NLP</fixed-case> Classifiers</title>
      <author><first>Rishabh</first><last>Singh</last></author>
      <author><first>Shirin</first><last>Goshtasbpour</last></author>
      <pages>3673-3684</pages>
      <abstract>Modern NLP classifiers are known to return uncalibrated estimations of class posteriors. Existing methods for posterior calibration rescale the predicted probabilities but often have an adverse impact on final classification accuracy, thus leading to poorer generalization. We propose an end-to-end trained calibrator, Platt-Binning, that directly optimizes the objective while minimizing the difference between the predicted and empirical posterior probabilities. Our method leverages the sample efficiency of Platt scaling and the verification guarantees of histogram binning, thus not only reducing the calibration error but also improving task performance. In contrast to existing calibrators, we perform this efficient calibration during training. Empirical evaluation of benchmark NLP classification tasks echoes the efficacy of our proposal.</abstract>
      <url hash="0fd69edd">2022.findings-acl.290</url>
      <attachment type="software" hash="1ae094f8">2022.findings-acl.290.software.zip</attachment>
      <bibkey>singh-goshtasbpour-2022-platt</bibkey>
    </paper>
    <paper id="291">
      <title>Addressing Resource and Privacy Constraints in Semantic Parsing Through Data Augmentation</title>
      <author><first>Kevin</first><last>Yang</last></author>
      <author><first>Olivia</first><last>Deng</last></author>
      <author><first>Charles</first><last>Chen</last></author>
      <author><first>Richard</first><last>Shin</last></author>
      <author><first>Subhro</first><last>Roy</last></author>
      <author><first>Benjamin</first><last>Van Durme</last></author>
      <pages>3685-3695</pages>
      <abstract>We introduce a novel setup for low-resource task-oriented semantic parsing which incorporates several constraints that may arise in real-world scenarios: (1) lack of similar datasets/models from a related domain, (2) inability to sample useful logical forms directly from a grammar, and (3) privacy requirements for unlabeled natural utterances. Our goal is to improve a low-resource semantic parser using utterances collected through user interactions. In this highly challenging but realistic setting, we investigate data augmentation approaches involving generating a set of structured canonical utterances corresponding to logical forms, before simulating corresponding natural language and filtering the resulting pairs. We find that such approaches are effective despite our restrictive setup: in a low-resource setting on the complex SMCalFlow calendaring dataset (Andreas et al. 2020), we observe 33% relative improvement over a non-data-augmented baseline in top-1 match.</abstract>
      <url hash="750b654d">2022.findings-acl.291</url>
      <bibkey>yang-etal-2022-addressing</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/atis">ATIS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/break">BREAK</pwcdataset>
    </paper>
    <paper id="292">
      <title>Improving Candidate Retrieval with Entity Profile Generation for <fixed-case>W</fixed-case>ikidata Entity Linking</title>
      <author><first>Tuan</first><last>Lai</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <author><first>ChengXiang</first><last>Zhai</last></author>
      <pages>3696-3711</pages>
      <abstract>Entity linking (EL) is the task of linking entity mentions in a document to referent entities in a knowledge base (KB). Many previous studies focus on Wikipedia-derived KBs. There is little work on EL over Wikidata, even though it is the most extensive crowdsourced KB. The scale of Wikidata can open up many new real-world applications, but its massive number of entities also makes EL challenging. To effectively narrow down the search space, we propose a novel candidate retrieval paradigm based on entity profiling. Wikidata entities and their textual fields are first indexed into a text search engine (e.g., Elasticsearch). During inference, given a mention and its context, we use a sequence-to-sequence (seq2seq) model to generate the profile of the target entity, which consists of its title and description. We use the profile to query the indexed search engine to retrieve candidate entities. Our approach complements the traditional approach of using a Wikipedia anchor-text dictionary, enabling us to further design a highly effective hybrid method for candidate retrieval. Combined with a simple cross-attention reranker, our complete EL framework achieves state-of-the-art results on three Wikidata-based datasets and strong performance on TACKBP-2010.</abstract>
      <url hash="4b80b6e3">2022.findings-acl.292</url>
      <bibkey>lai-etal-2022-improving</bibkey>
      <pwccode url="https://github.com/laituan245/el-dockers" additional="false">laituan245/el-dockers</pwccode>
    </paper>
    <paper id="293">
      <title>Local Structure Matters Most: Perturbation Study in <fixed-case>NLU</fixed-case></title>
      <author><first>Louis</first><last>Clouatre</last></author>
      <author><first>Prasanna</first><last>Parthasarathi</last></author>
      <author><first>Amal</first><last>Zouaq</last></author>
      <author><first>Sarath</first><last>Chandar</last></author>
      <pages>3712-3731</pages>
      <abstract>Recent research analyzing the sensitivity of natural language understanding models to word-order perturbations has shown that neural models are surprisingly insensitive to the order of words.In this paper, we investigate this phenomenon by developing order-altering perturbations on the order of words, subwords, and characters to analyze their effect on neural models’ performance on language understanding tasks.We experiment with measuring the impact of perturbations to the local neighborhood of characters and global position of characters in the perturbed texts and observe that perturbation functions found in prior literature only affect the global ordering while the local ordering remains relatively unperturbed.We empirically show that neural models, invariant of their inductive biases, pretraining scheme, or the choice of tokenization, mostly rely on the local structure of text to build understanding and make limited use of the global structure.</abstract>
      <url hash="c6758cea">2022.findings-acl.293</url>
      <attachment type="software" hash="b5255e6e">2022.findings-acl.293.software.zip</attachment>
      <bibkey>clouatre-etal-2022-local</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
    <paper id="294">
      <title>Probing Factually Grounded Content Transfer with Factual Ablation</title>
      <author><first>Peter</first><last>West</last></author>
      <author><first>Chris</first><last>Quirk</last></author>
      <author><first>Michel</first><last>Galley</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <pages>3732-3746</pages>
      <abstract>Despite recent success, large neural models often generate factually incorrect text. Compounding this is the lack of a standard automatic evaluation for factuality–it cannot be meaningfully improved if it cannot be measured. Grounded generation promises a path to solving both of these problems: models draw on a reliable external document (grounding) for factual information, simplifying the challenge of factuality. Measuring factuality is also simplified–to factual consistency, testing whether the generation agrees with the grounding, rather than all facts. Yet, without a standard automatic metric for factual consistency, factually grounded generation remains an open problem. We study this problem for content transfer, in which generations extend a prompt, using information from factual grounding. Particularly, this domain allows us to introduce the notion of factual ablation for automatically measuring factual consistency: this captures the intuition that the model should be less likely to produce an output given a less relevant grounding document. In practice, we measure this by presenting a model with two grounding documents, and the model should prefer to use the more factually relevant one. We contribute two evaluation sets to measure this. Applying our new evaluation, we propose multiple novel methods improving over strong baselines.</abstract>
      <url hash="8ab8f758">2022.findings-acl.294</url>
      <bibkey>west-etal-2022-probing</bibkey>
    </paper>
    <paper id="295">
      <title><fixed-case>ED</fixed-case>2<fixed-case>LM</fixed-case>: Encoder-Decoder to Language Model for Faster Document Re-ranking Inference</title>
      <author><first>Kai</first><last>Hui</last></author>
      <author><first>Honglei</first><last>Zhuang</last></author>
      <author><first>Tao</first><last>Chen</last></author>
      <author><first>Zhen</first><last>Qin</last></author>
      <author><first>Jing</first><last>Lu</last></author>
      <author><first>Dara</first><last>Bahri</last></author>
      <author><first>Ji</first><last>Ma</last></author>
      <author><first>Jai</first><last>Gupta</last></author>
      <author><first>Cicero</first><last>Nogueira dos Santos</last></author>
      <author><first>Yi</first><last>Tay</last></author>
      <author><first>Donald</first><last>Metzler</last></author>
      <pages>3747-3758</pages>
      <abstract>State-of-the-art neural models typically encode document-query pairs using cross-attention for re-ranking. To this end, models generally utilize an encoder-only (like BERT) paradigm or an encoder-decoder (like T5) approach. These paradigms, however, are not without flaws, i.e., running the model on all query-document pairs at inference-time incurs a significant computational cost. This paper proposes a new training and inference paradigm for re-ranking. We propose to finetune a pretrained encoder-decoder model using in the form of document to query generation. Subsequently, we show that this encoder-decoder architecture can be decomposed into a decoder-only language model during inference. This results in significant inference time speedups since the decoder-only architecture only needs to learn to interpret static encoder embeddings during inference. Our experiments show that this new paradigm achieves results that are comparable to the more expensive cross-attention ranking approaches while being up to 6.8X faster. We believe this work paves the way for more efficient neural rankers that leverage large pretrained models.</abstract>
      <url hash="76584879">2022.findings-acl.295</url>
      <bibkey>hui-etal-2022-ed2lm</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
    </paper>
    <paper id="296">
      <title>Benchmarking Answer Verification Methods for Question Answering-Based Summarization Evaluation Metrics</title>
      <author><first>Daniel</first><last>Deutsch</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>3759-3765</pages>
      <abstract>Question answering-based summarization evaluation metrics must automatically determine whether the QA model’s prediction is correct or not, a task known as answer verification. In this work, we benchmark the lexical answer verification methods which have been used by current QA-based metrics as well as two more sophisticated text comparison methods, BERTScore and LERC. We find that LERC out-performs the other methods in some settings while remaining statistically indistinguishable from lexical overlap in others. However, our experiments reveal that improved verification performance does not necessarily translate to overall QA-based metric quality: In some scenarios, using a worse verification method — or using none at all — has comparable performance to using the best verification method, a result that we attribute to properties of the datasets.</abstract>
      <url hash="8b2f89e5">2022.findings-acl.296</url>
      <bibkey>deutsch-roth-2022-benchmarking</bibkey>
    </paper>
    <paper id="297">
      <title>Prior Knowledge and Memory Enriched Transformer for Sign Language Translation</title>
      <author><first>Tao</first><last>Jin</last></author>
      <author><first>Zhou</first><last>Zhao</last></author>
      <author><first>Meng</first><last>Zhang</last></author>
      <author><first>Xingshan</first><last>Zeng</last></author>
      <pages>3766-3775</pages>
      <abstract>This paper attacks the challenging problem of sign language translation (SLT), which involves not only visual and textual understanding but also additional prior knowledge learning (i.e. performing style, syntax). However, the majority of existing methods with vanilla encoder-decoder structures fail to sufficiently explore all of them. Based on this concern, we propose a novel method called Prior knowledge and memory Enriched Transformer (PET) for SLT, which incorporates the auxiliary information into vanilla transformer. Concretely, we develop gated interactive multi-head attention which associates the multimodal representation and global signing style with adaptive gated functions. One Part-of-Speech (POS) sequence generator relies on the associated information to predict the global syntactic structure, which is thereafter leveraged to guide the sentence generation. Besides, considering that the visual-textual context information, and additional auxiliary knowledge of a word may appear in more than one video, we design a multi-stream memory structure to obtain higher-quality translations, which stores the detailed correspondence between a word and its various relevant information, leading to a more comprehensive understanding for each word. We conduct extensive empirical studies on RWTH-PHOENIX-Weather-2014 dataset with both signer-dependent and signer-independent conditions. The quantitative and qualitative experimental results comprehensively reveal the effectiveness of PET.</abstract>
      <url hash="c4a410ce">2022.findings-acl.297</url>
      <bibkey>jin-etal-2022-prior</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/phoenix14t">PHOENIX14T</pwcdataset>
    </paper>
    <paper id="298">
      <title>Discontinuous Constituency and <fixed-case>BERT</fixed-case>: A Case Study of <fixed-case>D</fixed-case>utch</title>
      <author><first>Konstantinos</first><last>Kogkalidis</last></author>
      <author><first>Gijs</first><last>Wijnholds</last></author>
      <pages>3776-3785</pages>
      <abstract>In this paper, we set out to quantify the syntactic capacity of BERT in the evaluation regime of non-context free patterns, as occurring in Dutch. We devise a test suite based on a mildly context-sensitive formalism, from which we derive grammars that capture the linguistic phenomena of control verb nesting and verb raising. The grammars, paired with a small lexicon, provide us with a large collection of naturalistic utterances, annotated with verb-subject pairings, that serve as the evaluation test bed for an attention-based span selection probe. Our results, backed by extensive analysis, suggest that the models investigated fail in the implicit acquisition of the dependencies examined.</abstract>
      <url hash="48d8680d">2022.findings-acl.298</url>
      <attachment type="software" hash="d877a626">2022.findings-acl.298.software.zip</attachment>
      <bibkey>kogkalidis-wijnholds-2022-discontinuous</bibkey>
    </paper>
    <paper id="299">
      <title>Probing Multilingual Cognate Prediction Models</title>
      <author><first>Clémentine</first><last>Fourrier</last></author>
      <author><first>Benoît</first><last>Sagot</last></author>
      <pages>3786-3801</pages>
      <abstract>Character-based neural machine translation models have become the reference models for cognate prediction, a historical linguistics task. So far, all linguistic interpretations about latent information captured by such models have been based on external analysis (accuracy, raw results, errors). In this paper, we investigate what probing can tell us about both models and previous interpretations, and learn that though our models store linguistic and diachronic information, they do not achieve it in previously assumed ways.</abstract>
      <url hash="c1060f20">2022.findings-acl.299</url>
      <bibkey>fourrier-sagot-2022-probing</bibkey>
    </paper>
    <paper id="300">
      <title>A Neural Pairwise Ranking Model for Readability Assessment</title>
      <author><first>Justin</first><last>Lee</last></author>
      <author><first>Sowmya</first><last>Vajjala</last></author>
      <pages>3802-3813</pages>
      <abstract>Automatic Readability Assessment (ARA), the task of assigning a reading level to a text, is traditionally treated as a classification problem in NLP research. In this paper, we propose the first neural, pairwise ranking approach to ARA and compare it with existing classification, regression, and (non-neural) ranking methods. We establish the performance of our approach by conducting experiments with three English, one French and one Spanish datasets. We demonstrate that our approach performs well in monolingual single/cross corpus testing scenarios and achieves a zero-shot cross-lingual ranking accuracy of over 80% for both French and Spanish when trained on English data. Additionally, we also release a new parallel bilingual readability dataset, that could be useful for future research. To our knowledge, this paper proposes the first neural pairwise ranking model for ARA, and shows the first results of cross-lingual, zero-shot evaluation of ARA with neural models.</abstract>
      <url hash="f6a9931e">2022.findings-acl.300</url>
      <attachment type="software" hash="b8579279">2022.findings-acl.300.software.zip</attachment>
      <bibkey>lee-vajjala-2022-neural</bibkey>
      <pwccode url="https://github.com/jlee118/nprm" additional="false">jlee118/nprm</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/newsela">Newsela</pwcdataset>
    </paper>
    <paper id="301">
      <title>First the Worst: Finding Better Gender Translations During Beam Search</title>
      <author><first>Danielle</first><last>Saunders</last></author>
      <author><first>Rosie</first><last>Sallis</last></author>
      <author><first>Bill</first><last>Byrne</last></author>
      <pages>3814-3823</pages>
      <abstract>Generating machine translations via beam search seeks the most likely output under a model. However, beam search has been shown to amplify demographic biases exhibited by a model. We aim to address this, focusing on gender bias resulting from systematic errors in grammatical gender translation. Almost all prior work on this problem adjusts the training data or the model itself. By contrast, our approach changes only the inference procedure. We constrain beam search to improve gender diversity in n-best lists, and rerank n-best lists using gender features obtained from the source sentence. Combining these strongly improves WinoMT gender translation accuracy for three language pairs without additional bilingual data or retraining. We also demonstrate our approach’s utility for consistently gendering named entities, and its flexibility to handle new gendered language beyond the binary.</abstract>
      <url hash="3043d1dd">2022.findings-acl.301</url>
      <attachment type="software" hash="cbe41a52">2022.findings-acl.301.software.zip</attachment>
      <bibkey>saunders-etal-2022-first</bibkey>
      <pwccode url="https://github.com/dcsaunders/nmt-gender-rerank" additional="false">dcsaunders/nmt-gender-rerank</pwccode>
    </paper>
    <paper id="302">
      <title>Dialogue Summaries as Dialogue States (<fixed-case>DS</fixed-case>2), Template-Guided Summarization for Few-shot Dialogue State Tracking</title>
      <author><first>Jamin</first><last>Shin</last></author>
      <author><first>Hangyeol</first><last>Yu</last></author>
      <author><first>Hyeongdon</first><last>Moon</last></author>
      <author><first>Andrea</first><last>Madotto</last></author>
      <author><first>Juneyoung</first><last>Park</last></author>
      <pages>3824-3846</pages>
      <abstract>Annotating task-oriented dialogues is notorious for the expensive and difficult data collection process. Few-shot dialogue state tracking (DST) is a realistic solution to this problem. In this paper, we hypothesize that dialogue summaries are essentially unstructured dialogue states; hence, we propose to reformulate dialogue state tracking as a dialogue summarization problem. To elaborate, we train a text-to-text language model with synthetic template-based dialogue summaries, generated by a set of rules from the dialogue states. Then, the dialogue states can be recovered by inversely applying the summary generation rules. We empirically show that our method DS2 outperforms previous works on few-shot DST in MultiWoZ 2.0 and 2.1, in both cross-domain and multi-domain settings. Our method also exhibits vast speedup during both training and inference as it can generate all states at once.Finally, based on our analysis, we discover that the naturalness of the summary templates plays a key role for successful training.</abstract>
      <url hash="0fc736ce">2022.findings-acl.302</url>
      <attachment type="software" hash="e8b557c8">2022.findings-acl.302.software.zip</attachment>
      <bibkey>shin-etal-2022-dialogue</bibkey>
      <pwccode url="https://github.com/jshin49/ds2" additional="false">jshin49/ds2</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multiwoz">MultiWOZ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/samsum-corpus">SAMSum Corpus</pwcdataset>
    </paper>
    <paper id="303">
      <title>Unsupervised Preference-Aware Language Identification</title>
      <author><first>Xingzhang</first><last>Ren</last></author>
      <author><first>Baosong</first><last>Yang</last></author>
      <author><first>Dayiheng</first><last>Liu</last></author>
      <author><first>Haibo</first><last>Zhang</last></author>
      <author><first>Xiaoyu</first><last>Lv</last></author>
      <author><first>Liang</first><last>Yao</last></author>
      <author><first>Jun</first><last>Xie</last></author>
      <pages>3847-3852</pages>
      <abstract>Recognizing the language of ambiguous texts has become a main challenge in language identification (LID). When using multilingual applications, users have their own language preferences, which can be regarded as external knowledge for LID. Nevertheless, current studies do not consider the inter-personal variations due to the lack of user annotated training data. To fill this gap, we introduce preference-aware LID and propose a novel unsupervised learning strategy. Concretely, we construct pseudo training set for each user by extracting training samples from a standard LID corpus according to his/her historical language distribution. Besides, we contribute the first user labeled LID test set called “U-LID”. Experimental results reveal that our model can incarnate user traits and significantly outperforms existing LID systems on handling ambiguous texts. Our code and benchmark have been released.</abstract>
      <url hash="80e1c0a9">2022.findings-acl.303</url>
      <attachment type="software" hash="ad928da9">2022.findings-acl.303.software.zip</attachment>
      <bibkey>ren-etal-2022-unsupervised</bibkey>
      <pwccode url="https://github.com/xzhren/preferenceawarelid" additional="false">xzhren/preferenceawarelid</pwccode>
    </paper>
    <paper id="304">
      <title>Using <fixed-case>NLP</fixed-case> to quantify the environmental cost and diversity benefits of in-person <fixed-case>NLP</fixed-case> conferences</title>
      <author><first>Piotr</first><last>Przybyła</last></author>
      <author><first>Matthew</first><last>Shardlow</last></author>
      <pages>3853-3863</pages>
      <abstract>The environmental costs of research are progressively important to the NLP community and their associated challenges are increasingly debated. In this work, we analyse the carbon cost (measured as CO2-equivalent) associated with journeys made by researchers attending in-person NLP conferences. We obtain the necessary data by text-mining all publications from the ACL anthology available at the time of the study (n=60,572) and extracting information about an author’s affiliation, including their address. This allows us to estimate the corresponding carbon cost and compare it to previously known values for training large models. Further, we look at the benefits of in-person conferences by demonstrating that they can increase participation diversity by encouraging attendance from the region surrounding the host country. We show how the trade-off between carbon cost and diversity of an event depends on its location and type. Our aim is to foster further discussion on the best way to address the joint issue of emissions and diversity in the future.</abstract>
      <url hash="7a5738d0">2022.findings-acl.304</url>
      <bibkey>przybyla-shardlow-2022-using</bibkey>
      <pwccode url="https://github.com/piotrmp/nlp_geography" additional="false">piotrmp/nlp_geography</pwccode>
    </paper>
    <paper id="305">
      <title>Interpretable Research Replication Prediction via Variational Contextual Consistency Sentence Masking</title>
      <author><first>Tianyi</first><last>Luo</last></author>
      <author><first>Rui</first><last>Meng</last></author>
      <author><first>Xin</first><last>Wang</last></author>
      <author id="yang-liu-umich"><first>Yang</first><last>Liu</last></author>
      <pages>3864-3876</pages>
      <abstract>Research Replication Prediction (RRP) is the task of predicting whether a published research result can be replicated or not. Building an interpretable neural text classifier for RRP promotes the understanding of why a research paper is predicted as replicable or non-replicable and therefore makes its real-world application more reliable and trustworthy. However, the prior works on model interpretation mainly focused on improving the model interpretability at the word/phrase level, which are insufficient especially for long research papers in RRP. Furthermore, the existing methods cannot utilize a large size of unlabeled dataset to further improve the model interpretability. To address these limitations, we aim to build an interpretable neural model which can provide sentence-level explanations and apply weakly supervised approach to further leverage the large corpus of unlabeled datasets to boost the interpretability in addition to improving prediction performance as existing works have done. In this work, we propose the Variational Contextual Consistency Sentence Masking (VCCSM) method to automatically extract key sentences based on the context in the classifier, using both labeled and unlabeled datasets. Results of our experiments on RRP along with European Convention of Human Rights (ECHR) datasets demonstrate that VCCSM is able to improve the model interpretability for the long document classification tasks using the area over the perturbation curve and post-hoc accuracy as evaluation metrics.</abstract>
      <url hash="38677994">2022.findings-acl.305</url>
      <attachment type="software" hash="bfebc4b3">2022.findings-acl.305.software.zip</attachment>
      <bibkey>luo-etal-2022-interpretable</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/echr">ECHR</pwcdataset>
    </paper>
    <paper id="306">
      <title><fixed-case>C</fixed-case>hinese Synesthesia Detection: New Dataset and Models</title>
      <author><first>Xiaotong</first><last>Jiang</last></author>
      <author><first>Qingqing</first><last>Zhao</last></author>
      <author><first>Yunfei</first><last>Long</last></author>
      <author><first>Zhongqing</first><last>Wang</last></author>
      <pages>3877-3887</pages>
      <abstract>In this paper, we introduce a new task called synesthesia detection, which aims to extract the sensory word of a sentence, and to predict the original and synesthetic sensory modalities of the corresponding sensory word. Synesthesia refers to the description of perceptions in one sensory modality through concepts from other modalities. It involves not only a linguistic phenomenon, but also a cognitive phenomenon structuring human thought and action, which makes it become a bridge between figurative linguistic phenomenon and abstract cognition, and thus be helpful to understand the deep semantics. To address this, we construct a large-scale human-annotated Chinese synesthesia dataset, which contains 7,217 annotated sentences accompanied by 187 sensory words. Based on this dataset, we propose a family of strong and representative baseline models. Upon these baselines, we further propose a radical-based neural network model to identify the boundary of the sensory word, and to jointly detect the original and synesthetic sensory modalities for the word. Through extensive experiments, we observe that the importance of the proposed task and dataset can be verified by the statistics and progressive performances. In addition, our proposed model achieves state-of-the-art results on the synesthesia dataset.</abstract>
      <url hash="8c19f3a7">2022.findings-acl.306</url>
      <bibkey>jiang-etal-2022-chinese</bibkey>
    </paper>
    <paper id="307">
      <title>Rethinking Offensive Text Detection as a Multi-Hop Reasoning Problem</title>
      <author><first>Qiang</first><last>Zhang</last></author>
      <author><first>Jason</first><last>Naradowsky</last></author>
      <author><first>Yusuke</first><last>Miyao</last></author>
      <pages>3888-3905</pages>
      <abstract>We introduce the task of implicit offensive text detection in dialogues, where a statement may have either an offensive or non-offensive interpretation, depending on the listener and context. We argue that reasoning is crucial for understanding this broader class of offensive utterances, and release SLIGHT, a dataset to support research on this task. Experiments using the data show that state-of-the-art methods of offense detection perform poorly when asked to detect implicitly offensive statements, achieving only <tex-math>{\sim} 11\%</tex-math> accuracy. In contrast to existing offensive text detection datasets, SLIGHT features human-annotated chains of reasoning which describe the mental process by which an offensive interpretation can be reached from each ambiguous statement. We explore the potential for a multi-hop reasoning approach by utilizing existing entailment models to score the probability of these chains, and show that even naive reasoning models can yield improved performance in most situations. Analysis of the chains provides insight into the human interpretation process and emphasizes the importance of incorporating additional commonsense knowledge.</abstract>
      <url hash="f94c5eab">2022.findings-acl.307</url>
      <bibkey>zhang-etal-2022-rethinking</bibkey>
      <pwccode url="https://github.com/qzx7/slight" additional="false">qzx7/slight</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/olid">OLID</pwcdataset>
    </paper>
    <paper id="308">
      <title>On the Safety of Conversational Models: Taxonomy, Dataset, and Benchmark</title>
      <author><first>Hao</first><last>Sun</last></author>
      <author><first>Guangxuan</first><last>Xu</last></author>
      <author><first>Jiawen</first><last>Deng</last></author>
      <author><first>Jiale</first><last>Cheng</last></author>
      <author><first>Chujie</first><last>Zheng</last></author>
      <author><first>Hao</first><last>Zhou</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <author><first>Xiaoyan</first><last>Zhu</last></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <pages>3906-3923</pages>
      <abstract>Dialogue safety problems severely limit the real-world deployment of neural conversational models and have attracted great research interests recently. However, dialogue safety problems remain under-defined and the corresponding dataset is scarce. We propose a taxonomy for dialogue safety specifically designed to capture unsafe behaviors in human-bot dialogue settings, with focuses on context-sensitive unsafety, which is under-explored in prior works. To spur research in this direction, we compile DiaSafety, a dataset with rich context-sensitive unsafe examples. Experiments show that existing safety guarding tools fail severely on our dataset. As a remedy, we train a dialogue safety classifier to provide a strong baseline for context-sensitive dialogue unsafety detection. With our classifier, we perform safety evaluations on popular conversational models and show that existing dialogue systems still exhibit concerning context-sensitive safety problems.</abstract>
      <url hash="146e8b83">2022.findings-acl.308</url>
      <attachment type="software" hash="01939158">2022.findings-acl.308.software.zip</attachment>
      <bibkey>sun-etal-2022-safety</bibkey>
      <pwccode url="https://github.com/thu-coai/diasafety" additional="false">thu-coai/diasafety</pwccode>
    </paper>
    <paper id="309">
      <title>Word Segmentation by Separation Inference for <fixed-case>E</fixed-case>ast <fixed-case>A</fixed-case>sian Languages</title>
      <author><first>Yu</first><last>Tong</last></author>
      <author><first>Jingzhi</first><last>Guo</last></author>
      <author><first>Jizhe</first><last>Zhou</last></author>
      <author><first>Ge</first><last>Chen</last></author>
      <author><first>Guokai</first><last>Zheng</last></author>
      <pages>3924-3934</pages>
      <abstract>Chinese Word Segmentation (CWS) intends to divide a raw sentence into words through sequence labeling. Thinking in reverse, CWS can also be viewed as a process of grouping a sequence of characters into a sequence of words. In such a way, CWS is reformed as a separation inference task in every adjacent character pair. Since every character is either connected or not connected to the others, the tagging schema is simplified as two tags “Connection” (C) or “NoConnection” (NC). Therefore, bigram is specially tailored for “C-NC” to model the separation state of every two consecutive characters. Our Separation Inference (SpIn) framework is evaluated on five public datasets, is demonstrated to work for machine learning and deep learning models, and outperforms state-of-the-art performance for CWS in all experiments. Performance boosts on Japanese Word Segmentation (JWS) and Korean Word Segmentation (KWS) further prove the framework is universal and effective for East Asian Languages.</abstract>
      <url hash="73a0039b">2022.findings-acl.309</url>
      <bibkey>tong-etal-2022-word</bibkey>
      <pwccode url="https://github.com/um-nlper/spin-ws" additional="false">um-nlper/spin-ws</pwccode>
    </paper>
    <paper id="310">
      <title>Unsupervised <fixed-case>C</fixed-case>hinese Word Segmentation with <fixed-case>BERT</fixed-case> Oriented Probing and Transformation</title>
      <author><first>Wei</first><last>Li</last></author>
      <author><first>Yuhan</first><last>Song</last></author>
      <author><first>Qi</first><last>Su</last></author>
      <author><first>Yanqiu</first><last>Shao</last></author>
      <pages>3935-3940</pages>
      <abstract>Word Segmentation is a fundamental step for understanding Chinese language. Previous neural approaches for unsupervised Chinese Word Segmentation (CWS) only exploits shallow semantic information, which can miss important context. Large scale Pre-trained language models (PLM) have achieved great success in many areas because of its ability to capture the deep contextual semantic relation. In this paper, we propose to take advantage of the deep semantic information embedded in PLM (e.g., BERT) with a self-training manner, which iteratively probes and transforms the semantic information in PLM into explicit word segmentation ability. Extensive experiment results show that our proposed approach achieves state-of-the-art F1 score on two CWS benchmark datasets.</abstract>
      <url hash="7e14ec26">2022.findings-acl.310</url>
      <attachment type="software" hash="a9589624">2022.findings-acl.310.software.zip</attachment>
      <bibkey>li-etal-2022-unsupervised</bibkey>
      <pwccode url="https://github.com/liweitj47/bert_unsupervised_word_segmentation" additional="false">liweitj47/bert_unsupervised_word_segmentation</pwccode>
    </paper>
    <paper id="311">
      <title><fixed-case>E</fixed-case>-<fixed-case>KAR</fixed-case>: A Benchmark for Rationalizing Natural Language Analogical Reasoning</title>
      <author><first>Jiangjie</first><last>Chen</last></author>
      <author><first>Rui</first><last>Xu</last></author>
      <author><first>Ziquan</first><last>Fu</last></author>
      <author><first>Wei</first><last>Shi</last></author>
      <author><first>Zhongqiao</first><last>Li</last></author>
      <author><first>Xinbo</first><last>Zhang</last></author>
      <author><first>Changzhi</first><last>Sun</last></author>
      <author><first>Lei</first><last>Li</last></author>
      <author><first>Yanghua</first><last>Xiao</last></author>
      <author><first>Hao</first><last>Zhou</last></author>
      <pages>3941-3955</pages>
      <abstract>The ability to recognize analogies is fundamental to human cognition. Existing benchmarks to test word analogy do not reveal the underneath process of analogical reasoning of neural models. Holding the belief that models capable of reasoning should be right for the right reasons, we propose a first-of-its-kind Explainable Knowledge-intensive Analogical Reasoning benchmark (E-KAR). Our benchmark consists of 1,655 (in Chinese) and 1,251 (in English) problems sourced from the Civil Service Exams, which require intensive background knowledge to solve. More importantly, we design a free-text explanation scheme to explain whether an analogy should be drawn, and manually annotate them for each and every question and candidate answer. Empirical results suggest that this benchmark is very challenging for some state-of-the-art models for both explanation generation and analogical question answering tasks, which invites further research in this area.</abstract>
      <url hash="ed449cf0">2022.findings-acl.311</url>
      <bibkey>chen-etal-2022-e</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/e-kar">E-KAR</pwcdataset>
    </paper>
    <paper id="312">
      <title>Implicit Relation Linking for Question Answering over Knowledge Graph</title>
      <author><first>Yao</first><last>Zhao</last></author>
      <author><first>Jiacheng</first><last>Huang</last></author>
      <author><first>Wei</first><last>Hu</last></author>
      <author><first>Qijin</first><last>Chen</last></author>
      <author><first>XiaoXia</first><last>Qiu</last></author>
      <author><first>Chengfu</first><last>Huo</last></author>
      <author><first>Weijun</first><last>Ren</last></author>
      <pages>3956-3968</pages>
      <abstract>Relation linking (RL) is a vital module in knowledge-based question answering (KBQA) systems. It aims to link the relations expressed in natural language (NL) to the corresponding ones in knowledge graph (KG). Existing methods mainly rely on the textual similarities between NL and KG to build relation links. Due to the ambiguity of NL and the incompleteness of KG, many relations in NL are implicitly expressed, and may not link to a single relation in KG, which challenges the current methods. In this paper, we propose an implicit RL method called ImRL, which links relation phrases in NL to relation paths in KG. To find proper relation paths, we propose a novel path ranking model that aligns not only textual information in the word embedding space but also structural information in the KG embedding space between relation phrases in NL and relation paths in KG. Besides, we leverage a gated mechanism with attention to inject prior knowledge from external paraphrase dictionaries to address the relation phrases with vague meaning. Our experiments on two benchmark and a newly-created datasets show that ImRL significantly outperforms several state-of-the-art methods, especially for implicit RL.</abstract>
      <url hash="e41f85b5">2022.findings-acl.312</url>
      <attachment type="software" hash="c0d06122">2022.findings-acl.312.software.zip</attachment>
      <bibkey>zhao-etal-2022-implicit</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/dbpedia">DBpedia</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/simplequestions">SimpleQuestions</pwcdataset>
    </paper>
    <paper id="313">
      <title>Attention Mechanism with Energy-Friendly Operations</title>
      <author><first>Yu</first><last>Wan</last></author>
      <author><first>Baosong</first><last>Yang</last></author>
      <author><first>Dayiheng</first><last>Liu</last></author>
      <author><first>Rong</first><last>Xiao</last></author>
      <author><first>Derek</first><last>Wong</last></author>
      <author><first>Haibo</first><last>Zhang</last></author>
      <author><first>Boxing</first><last>Chen</last></author>
      <author><first>Lidia</first><last>Chao</last></author>
      <pages>3969-3976</pages>
      <abstract>Attention mechanism has become the dominant module in natural language processing models. It is computationally intensive and depends on massive power-hungry multiplications. In this paper, we rethink variants of attention mechanism from the energy consumption aspects. After reaching the conclusion that the energy costs of several energy-friendly operations are far less than their multiplication counterparts, we build a novel attention model by replacing multiplications with either selective operations or additions. Empirical results on three machine translation tasks demonstrate that the proposed model, against the vanilla one, achieves competitable accuracy while saving 99% and 66% energy during alignment calculation and the whole attention procedure. Our code will be released upon the acceptance.</abstract>
      <url hash="49853277">2022.findings-acl.313</url>
      <bibkey>wan-etal-2022-attention</bibkey>
      <pwccode url="https://github.com/nlp2ct/e-att" additional="false">nlp2ct/e-att</pwccode>
    </paper>
    <paper id="314">
      <title>Probing <fixed-case>BERT</fixed-case>’s priors with serial reproduction chains</title>
      <author><first>Takateru</first><last>Yamakoshi</last></author>
      <author><first>Thomas</first><last>Griffiths</last></author>
      <author><first>Robert</first><last>Hawkins</last></author>
      <pages>3977-3992</pages>
      <abstract>Sampling is a promising bottom-up method for exposing what generative models have learned about language, but it remains unclear how to generate representative samples from popular masked language models (MLMs) like BERT. The MLM objective yields a dependency network with no guarantee of consistent conditional distributions, posing a problem for naive approaches. Drawing from theories of iterated learning in cognitive science, we explore the use of <i>serial reproduction chains</i> to sample from BERT’s priors. In particular, we observe that a unique and consistent estimator of the ground-truth joint distribution is given by a Generative Stochastic Network (GSN) sampler, which randomly selects which token to mask and reconstruct on each step. We show that the lexical and syntactic statistics of sentences from GSN chains closely match the ground-truth corpus distribution and perform better than other methods in a large corpus of naturalness judgments. Our findings establish a firmer theoretical foundation for bottom-up probing and highlight richer deviations from human priors.</abstract>
      <url hash="0166889c">2022.findings-acl.314</url>
      <bibkey>yamakoshi-etal-2022-probing</bibkey>
    </paper>
    <paper id="315">
      <title>Interpreting the Robustness of Neural <fixed-case>NLP</fixed-case> Models to Textual Perturbations</title>
      <author><first>Yunxiang</first><last>Zhang</last></author>
      <author><first>Liangming</first><last>Pan</last></author>
      <author><first>Samson</first><last>Tan</last></author>
      <author><first>Min-Yen</first><last>Kan</last></author>
      <pages>3993-4007</pages>
      <abstract>Modern Natural Language Processing (NLP) models are known to be sensitive to input perturbations and their performance can decrease when applied to real-world, noisy data. However, it is still unclear why models are less robust to some perturbations than others. In this work, we test the hypothesis that the extent to which a model is affected by an unseen textual perturbation (robustness) can be explained by the learnability of the perturbation (defined as how well the model learns to identify the perturbation with a small amount of evidence). We further give a causal justification for the learnability metric. We conduct extensive experiments with four prominent NLP models — TextRNN, BERT, RoBERTa and XLNet — over eight types of textual perturbations on three datasets. We show that a model which is better at identifying a perturbation (higher learnability) becomes worse at ignoring such a perturbation at test time (lower robustness), providing empirical support for our hypothesis.</abstract>
      <url hash="47834ba0">2022.findings-acl.315</url>
      <bibkey>zhang-etal-2022-interpreting</bibkey>
    </paper>
    <paper id="316">
      <title>Zero-Shot Dense Retrieval with Momentum Adversarial Domain Invariant Representations</title>
      <author><first>Ji</first><last>Xin</last></author>
      <author><first>Chenyan</first><last>Xiong</last></author>
      <author><first>Ashwin</first><last>Srinivasan</last></author>
      <author><first>Ankita</first><last>Sharma</last></author>
      <author><first>Damien</first><last>Jose</last></author>
      <author><first>Paul</first><last>Bennett</last></author>
      <pages>4008-4020</pages>
      <abstract>Dense retrieval (DR) methods conduct text retrieval by first encoding texts in the embedding space and then matching them by nearest neighbor search. This requires strong locality properties from the representation space, e.g., close allocations of each small group of relevant texts, which are hard to generalize to domains without sufficient training data. In this paper, we aim to improve the generalization ability of DR models from source training domains with rich supervision signals to target domains without any relevance label, in the zero-shot setting. To achieve that, we propose Momentum adversarial Domain Invariant Representation learning (MoDIR), which introduces a momentum method to train a domain classifier that distinguishes source versus target domains, and then adversarially updates the DR encoder to learn domain invariant representations. Our experiments show that MoDIR robustly outperforms its baselines on 10+ ranking datasets collected in the BEIR benchmark in the zero-shot setup, with more than 10% relative gains on datasets with enough sensitivity for DR models’ evaluation. Source code is available at https://github.com/ji-xin/modir.</abstract>
      <url hash="51ee267a">2022.findings-acl.316</url>
      <bibkey>xin-etal-2022-zero</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/beir">BEIR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
    </paper>
    <paper id="317">
      <title>A Few-Shot Semantic Parser for <fixed-case>W</fixed-case>izard-of-<fixed-case>O</fixed-case>z Dialogues with the Precise <fixed-case>T</fixed-case>hing<fixed-case>T</fixed-case>alk Representation</title>
      <author><first>Giovanni</first><last>Campagna</last></author>
      <author><first>Sina</first><last>Semnani</last></author>
      <author><first>Ryan</first><last>Kearns</last></author>
      <author><first>Lucas Jun</first><last>Koba Sato</last></author>
      <author><first>Silei</first><last>Xu</last></author>
      <author><first>Monica</first><last>Lam</last></author>
      <pages>4021-4034</pages>
      <abstract>Previous attempts to build effective semantic parsers for Wizard-of-Oz (WOZ) conversations suffer from the difficulty in acquiring a high-quality, manually annotated training set. Approaches based only on dialogue synthesis are insufficient, as dialogues generated from state-machine based models are poor approximations of real-life conversations. Furthermore, previously proposed dialogue state representations are ambiguous and lack the precision necessary for building an effective agent.This paper proposes a new dialogue representation and a sample-efficient methodology that can predict precise dialogue states in WOZ conversations. We extended the ThingTalk representation to capture all information an agent needs to respond properly. Our training strategy is sample-efficient: we combine (1) few-shot data sparsely sampling the full dialogue space and (2) synthesized data covering a subset space of dialogues generated by a succinct state-based dialogue model. The completeness of the extended ThingTalk language is demonstrated with a fully operational agent, which is also used in training data synthesis. We demonstrate the effectiveness of our methodology on MultiWOZ 3.0, a reannotation of the MultiWOZ 2.1 dataset in ThingTalk. ThingTalk can represent 98% of the test turns, while the simulator can emulate 85% of the validation set. We train a contextual semantic parser using our strategy, and obtain 79% turn-by-turn exact match accuracy on the reannotated test set.</abstract>
      <url hash="4af43fd5">2022.findings-acl.317</url>
      <bibkey>campagna-etal-2022-shot</bibkey>
    </paper>
    <paper id="318">
      <title><fixed-case>GCPG</fixed-case>: A General Framework for Controllable Paraphrase Generation</title>
      <author><first>Kexin</first><last>Yang</last></author>
      <author><first>Dayiheng</first><last>Liu</last></author>
      <author><first>Wenqiang</first><last>Lei</last></author>
      <author><first>Baosong</first><last>Yang</last></author>
      <author><first>Haibo</first><last>Zhang</last></author>
      <author><first>Xue</first><last>Zhao</last></author>
      <author><first>Wenqing</first><last>Yao</last></author>
      <author><first>Boxing</first><last>Chen</last></author>
      <pages>4035-4047</pages>
      <abstract>Controllable paraphrase generation (CPG) incorporates various external conditions to obtain desirable paraphrases. However, existing works only highlight a special condition under two indispensable aspects of CPG (i.e., lexically and syntactically CPG) individually, lacking a unified circumstance to explore and analyze their effectiveness. In this paper, we propose a general controllable paraphrase generation framework (GCPG), which represents both lexical and syntactical conditions as text sequences and uniformly processes them in an encoder-decoder paradigm. Under GCPG, we reconstruct commonly adopted lexical condition (i.e., Keywords) and syntactical conditions (i.e., Part-Of-Speech sequence, Constituent Tree, Masked Template and Sentential Exemplar) and study the combination of the two types. In particular, for Sentential Exemplar condition, we propose a novel exemplar construction method — Syntax-Similarity based Exemplar (SSE). SSE retrieves a syntactically similar but lexically different sentence as the exemplar for each target sentence, avoiding exemplar-side words copying problem. Extensive experiments demonstrate that GCPG with SSE achieves state-of-the-art performance on two popular benchmarks. In addition, the combination of lexical and syntactical conditions shows the significant controllable ability of paraphrase generation, and these empirical results could provide novel insight to user-oriented paraphrasing.</abstract>
      <url hash="6037d737">2022.findings-acl.318</url>
      <attachment type="software" hash="8ba9c70a">2022.findings-acl.318.software.zip</attachment>
      <bibkey>yang-etal-2022-gcpg</bibkey>
    </paper>
    <paper id="319">
      <title><fixed-case>C</fixed-case>ross<fixed-case>A</fixed-case>ligner &amp; Co: Zero-Shot Transfer Methods for Task-Oriented Cross-lingual Natural Language Understanding</title>
      <author><first>Milan</first><last>Gritta</last></author>
      <author><first>Ruoyu</first><last>Hu</last></author>
      <author><first>Ignacio</first><last>Iacobacci</last></author>
      <pages>4048-4061</pages>
      <abstract>Task-oriented personal assistants enable people to interact with a host of devices and services using natural language. One of the challenges of making neural dialogue systems available to more users is the lack of training data for all but a few languages. Zero-shot methods try to solve this issue by acquiring task knowledge in a high-resource language such as English with the aim of transferring it to the low-resource language(s). To this end, we introduce CrossAligner, the principal method of a variety of effective approaches for zero-shot cross-lingual transfer based on learning alignment from unlabelled parallel data. We present a quantitative analysis of individual methods as well as their weighted combinations, several of which exceed state-of-the-art (SOTA) scores as evaluated across nine languages, fifteen test sets and three benchmark multilingual datasets. A detailed qualitative error analysis of the best methods shows that our fine-tuned language models can zero-shot transfer the task knowledge better than anticipated.</abstract>
      <url hash="fa4eb4b6">2022.findings-acl.319</url>
      <bibkey>gritta-etal-2022-crossaligner</bibkey>
      <pwccode url="https://github.com/huawei-noah/noah-research/tree/master/NLP/cross_aligner" additional="false">huawei-noah/noah-research</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mtop">MTOP</pwcdataset>
    </paper>
    <paper id="320">
      <title>Attention as Grounding: Exploring Textual and Cross-Modal Attention on Entities and Relations in Language-and-Vision Transformer</title>
      <author><first>Nikolai</first><last>Ilinykh</last></author>
      <author><first>Simon</first><last>Dobnik</last></author>
      <pages>4062-4073</pages>
      <abstract>We explore how a multi-modal transformer trained for generation of longer image descriptions learns syntactic and semantic representations about entities and relations grounded in objects at the level of masked self-attention (text generation) and cross-modal attention (information fusion). We observe that cross-attention learns the visual grounding of noun phrases into objects and high-level semantic information about spatial relations, while text-to-text attention captures low-level syntactic knowledge between words. This concludes that language models in a multi-modal task learn different semantic information about objects and relations cross-modally and uni-modally (text-only). Our code is available here: https://github.com/GU-CLASP/attention-as-grounding.</abstract>
      <url hash="b56b0921">2022.findings-acl.320</url>
      <bibkey>ilinykh-dobnik-2022-attention</bibkey>
      <pwccode url="https://github.com/gu-clasp/attention-as-grounding" additional="false">gu-clasp/attention-as-grounding</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/image-description-sequences">Image Description Sequences</pwcdataset>
    </paper>
    <paper id="321">
      <title>Improving Zero-Shot Cross-lingual Transfer Between Closely Related Languages by Injecting Character-Level Noise</title>
      <author><first>Noëmi</first><last>Aepli</last></author>
      <author><first>Rico</first><last>Sennrich</last></author>
      <pages>4074-4083</pages>
      <abstract>Cross-lingual transfer between a high-resource language and its dialects or closely related language varieties should be facilitated by their similarity. However, current approaches that operate in the embedding space do not take surface similarity into account. This work presents a simple yet effective strategy to improve cross-lingual transfer between closely related varieties. We propose to augment the data of the high-resource source language with character-level noise to make the model more robust towards spelling variations. Our strategy shows consistent improvements over several languages and tasks: Zero-shot transfer of POS tagging and topic identification between language varieties from the Finnic, West and North Germanic, and Western Romance language branches. Our work provides evidence for the usefulness of simple surface-level noise in improving transfer between language varieties.</abstract>
      <url hash="bf52bffc">2022.findings-acl.321</url>
      <bibkey>aepli-sennrich-2022-improving</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="322">
      <title>Structural Supervision for Word Alignment and Machine Translation</title>
      <author><first>Lei</first><last>Li</last></author>
      <author><first>Kai</first><last>Fan</last></author>
      <author><first>Hongjia</first><last>Li</last></author>
      <author><first>Chun</first><last>Yuan</last></author>
      <pages>4084-4094</pages>
      <abstract>Syntactic structure has long been argued to be potentially useful for enforcing accurate word alignment and improving generalization performance of machine translation. Unfortunately, existing wisdom demonstrates its significance by considering only the syntactic structure of source tokens, neglecting the rich structural information from target tokens and the structural similarity between the source and target sentences. In this work, we propose to incorporate the syntactic structure of both source and target tokens into the encoder-decoder framework, tightly correlating the internal logic of word alignment and machine translation for multi-task learning. Particularly, we won’t leverage any annotated syntactic graph of the target side during training, so we introduce Dynamic Graph Convolution Networks (DGCN) on observed target tokens to sequentially and simultaneously generate the target tokens and the corresponding syntactic graphs, and further guide the word alignment. On this basis, Hierarchical Graph Random Walks (HGRW) are performed on the syntactic graphs of both source and target sides, for incorporating structured constraints on machine translation outputs. Experiments on four publicly available language pairs verify that our method is highly effective in capturing syntactic structure in different languages, consistently outperforming baselines in alignment accuracy and demonstrating promising results in translation quality.</abstract>
      <url hash="6d0f7a70">2022.findings-acl.322</url>
      <bibkey>li-etal-2022-structural</bibkey>
    </paper>
    <paper id="323">
      <title>Focus on the Action: Learning to Highlight and Summarize Jointly for Email To-Do Items Summarization</title>
      <author><first>Kexun</first><last>Zhang</last></author>
      <author><first>Jiaao</first><last>Chen</last></author>
      <author><first>Diyi</first><last>Yang</last></author>
      <pages>4095-4106</pages>
      <abstract>Automatic email to-do item generation is the task of generating to-do items from a given email to help people overview emails and schedule daily work. Different from prior research on email summarization, to-do item generation focuses on generating action mentions to provide more structured summaries of email text.Prior work either requires large amount of annotation for key sentences with potential actions or fails to pay attention to nuanced actions from these unstructured emails, and thus often lead to unfaithful summaries. To fill these gaps, we propose a simple and effective learning to highlight and summarize framework (LHS) to learn to identify the most salient text and actions, and incorporate these structured representations to generate more faithful to-do items. Experiments show that our LHS model outperforms the baselines and achieves the state-of-the-art performance in terms of both quantitative evaluation and human judgement. We also discussed specific challenges that current models faced with email to-do summarization.</abstract>
      <url hash="e13d73b0">2022.findings-acl.323</url>
      <bibkey>zhang-etal-2022-focus</bibkey>
    </paper>
    <paper id="324">
      <title>Exploring the Capacity of a Large-scale Masked Language Model to Recognize Grammatical Errors</title>
      <author><first>Ryo</first><last>Nagata</last></author>
      <author><first>Manabu</first><last>Kimura</last></author>
      <author><first>Kazuaki</first><last>Hanawa</last></author>
      <pages>4107-4118</pages>
      <abstract>In this paper, we explore the capacity of a language model-based method for grammatical error detection in detail. We first show that 5 to 10% of training data are enough for a BERT-based error detection method to achieve performance equivalent to what a non-language model-based method can achieve with the full training data; recall improves much faster with respect to training data size in the BERT-based method than in the non-language model method. This suggests that (i) the BERT-based method should have a good knowledge of the grammar required to recognize certain types of error and that (ii) it can transform the knowledge into error detection rules by fine-tuning with few training samples, which explains its high generalization ability in grammatical error detection. We further show with pseudo error data that it actually exhibits such nice properties in learning rules for recognizing various types of error. Finally, based on these findings, we discuss a cost-effective method for detecting grammatical errors with feedback comments explaining relevant grammatical rules to learners.</abstract>
      <url hash="a254046f">2022.findings-acl.324</url>
      <bibkey>nagata-etal-2022-exploring</bibkey>
    </paper>
    <paper id="325">
      <title>Should We Trust This Summary? <fixed-case>B</fixed-case>ayesian Abstractive Summarization to The Rescue</title>
      <author><first>Alexios</first><last>Gidiotis</last></author>
      <author><first>Grigorios</first><last>Tsoumakas</last></author>
      <pages>4119-4131</pages>
      <abstract>We explore the notion of uncertainty in the context of modern abstractive summarization models, using the tools of Bayesian Deep Learning. Our approach approximates Bayesian inference by first extending state-of-the-art summarization models with Monte Carlo dropout and then using them to perform multiple stochastic forward passes. Based on Bayesian inference we are able to effectively quantify uncertainty at prediction time. Having a reliable uncertainty measure, we can improve the experience of the end user by filtering out generated summaries of high uncertainty. Furthermore, uncertainty estimation could be used as a criterion for selecting samples for annotation, and can be paired nicely with active learning and human-in-the-loop approaches. Finally, Bayesian inference enables us to find a Bayesian summary which performs better than a deterministic one and is more robust to uncertainty. In practice, we show that our Variational Bayesian equivalents of BART and PEGASUS can outperform their deterministic counterparts on multiple benchmark datasets.</abstract>
      <url hash="12a71fd6">2022.findings-acl.325</url>
      <bibkey>gidiotis-tsoumakas-2022-trust</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/aeslc">AESLC</pwcdataset>
    </paper>
    <paper id="326">
      <title>On the data requirements of probing</title>
      <author><first>Zining</first><last>Zhu</last></author>
      <author><first>Jixuan</first><last>Wang</last></author>
      <author><first>Bai</first><last>Li</last></author>
      <author><first>Frank</first><last>Rudzicz</last></author>
      <pages>4132-4147</pages>
      <abstract>As large and powerful neural language models are developed, researchers have been increasingly interested in developing diagnostic tools to probe them. There are many papers with conclusions of the form “observation <tex-math>X</tex-math> is found in model <tex-math>Y</tex-math>”, using their own datasets with varying sizes. Larger probing datasets bring more reliability, but are also expensive to collect. There is yet to be a quantitative method for estimating reasonable probing dataset sizes. We tackle this omission in the context of comparing two probing configurations: after we have collected a small dataset from a pilot study, how many additional data samples are sufficient to distinguish two different configurations? We present a novel method to estimate the required number of data samples in such experiments and, across several case studies, we verify that our estimations have sufficient statistical power. Our framework helps to systematically construct probing datasets to diagnose neural NLP models.</abstract>
      <url hash="3716d5cc">2022.findings-acl.326</url>
      <attachment type="software" hash="e92eb92a">2022.findings-acl.326.software.zip</attachment>
      <bibkey>zhu-etal-2022-data</bibkey>
      <pwccode url="https://github.com/spoclab-ca/probing_dataset" additional="false">spoclab-ca/probing_dataset</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/senteval">SentEval</pwcdataset>
    </paper>
    <paper id="327">
      <title>Translation Error Detection as Rationale Extraction</title>
      <author><first>Marina</first><last>Fomicheva</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <author><first>Nikolaos</first><last>Aletras</last></author>
      <pages>4148-4159</pages>
      <abstract>Recent Quality Estimation (QE) models based on multilingual pre-trained representations have achieved very competitive results in predicting the overall quality of translated sentences. However, detecting specifically which translated words are incorrect is a more challenging task, especially when dealing with limited amounts of training data. We hypothesize that, not unlike humans, successful QE models rely on translation errors to predict overall sentence quality. By exploring a set of feature attribution methods that assign relevance scores to the inputs to explain model predictions, we study the behaviour of state-of-the-art sentence-level QE models and show that explanations (i.e. rationales) extracted from these models can indeed be used to detect translation errors. We therefore (i) introduce a novel semi-supervised method for word-level QE; and (ii) propose to use the QE task as a new benchmark for evaluating the plausibility of feature attribution, i.e. how interpretable model explanations are to humans.</abstract>
      <url hash="35feecd8">2022.findings-acl.327</url>
      <bibkey>fomicheva-etal-2022-translation</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mlqe-pe">MLQE-PE</pwcdataset>
    </paper>
    <paper id="328">
      <title>Towards Collaborative Neural-Symbolic Graph Semantic Parsing via Uncertainty</title>
      <author><first>Zi</first><last>Lin</last></author>
      <author><first>Jeremiah Zhe</first><last>Liu</last></author>
      <author><first>Jingbo</first><last>Shang</last></author>
      <pages>4160-4173</pages>
      <abstract>Recent work in task-independent graph semantic parsing has shifted from grammar-based symbolic approaches to neural models, showing strong performance on different types of meaning representations. However, it is still unclear that what are the limitations of these neural parsers, and whether these limitations can be compensated by incorporating symbolic knowledge into model inference. In this paper, we address these questions by taking English Resource Grammar (ERG) parsing as a case study. Specifically, we first develop a state-of-the-art, T5-based neural ERG parser, and conduct detail analyses of parser performance within fine-grained linguistic categories.The neural parser attains superior performance on in-distribution test set, but degrades significantly on long-tail situations, while the symbolic parser performs more robustly. To address this, we further propose a simple yet principled collaborative framework for neural-symbolic semantic parsing, by designing a decision criterion for beam search that incorporates the prior knowledge from a symbolic parser and accounts for model uncertainty. Experimental results show that the proposed framework yields comprehensive improvement over neural baseline across long-tail categories, yielding the best known Smatch score (97.01) on the well-studied DeepBank benchmark.</abstract>
      <url hash="ca2c1494">2022.findings-acl.328</url>
      <bibkey>lin-etal-2022-towards</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/scan">SCAN</pwcdataset>
    </paper>
    <paper id="329">
      <title>Towards Few-shot Entity Recognition in Document Images: A Label-aware Sequence-to-Sequence Framework</title>
      <author><first>Zilong</first><last>Wang</last></author>
      <author><first>Jingbo</first><last>Shang</last></author>
      <pages>4174-4186</pages>
      <abstract>Entity recognition is a fundamental task in understanding document images. Traditional sequence labeling frameworks treat the entity types as class IDs and rely on extensive data and high-quality annotations to learn semantics which are typically expensive in practice. In this paper, we aim to build an entity recognition model requiring only a few shots of annotated document images. To overcome the data limitation, we propose to leverage the label surface names to better inform the model of the target entity type semantics and also embed the labels into the spatial embedding space to capture the spatial correspondence between regions and labels. Specifically, we go beyond sequence labeling and develop a novel label-aware seq2seq framework, LASER. The proposed model follows a new labeling scheme that generates the label surface names word-by-word explicitly after generating the entities. During training, LASER refines the label semantics by updating the label surface name representations and also strengthens the label-region correlation. In this way, LASER recognizes the entities from document images through both semantic and layout correspondence. Extensive experiments on two benchmark datasets demonstrate the superiority of LASER under the few-shot setting.</abstract>
      <url hash="055efb64">2022.findings-acl.329</url>
      <bibkey>wang-shang-2022-towards</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/funsd">FUNSD</pwcdataset>
    </paper>
    <paper id="330">
      <title>On Length Divergence Bias in Textual Matching Models</title>
      <author><first>Lan</first><last>Jiang</last></author>
      <author><first>Tianshu</first><last>Lyu</last></author>
      <author><first>Yankai</first><last>Lin</last></author>
      <author><first>Meng</first><last>Chong</last></author>
      <author><first>Xiaoyong</first><last>Lyu</last></author>
      <author><first>Dawei</first><last>Yin</last></author>
      <pages>4187-4193</pages>
      <abstract>Despite the remarkable success deep models have achieved in Textual Matching (TM) tasks, it still remains unclear whether they truly understand language or measure the semantic similarity of texts by exploiting statistical bias in datasets. In this work, we provide a new perspective to study this issue — via the length divergence bias. We find the length divergence heuristic widely exists in prevalent TM datasets, providing direct cues for prediction. To determine whether TM models have adopted such heuristic, we introduce an adversarial evaluation scheme which invalidates the heuristic. In this adversarial setting, all TM models perform worse, indicating they have indeed adopted this heuristic. Through a well-designed probing experiment, we empirically validate that the bias of TM models can be attributed in part to extracting the text length information during training. To alleviate the length divergence bias, we propose an adversarial training method. The results demonstrate we successfully improve the robustness and generalization ability of models at the same time.</abstract>
      <url hash="fdf34290">2022.findings-acl.330</url>
      <bibkey>jiang-etal-2022-length</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/trecqa">TrecQA</pwcdataset>
    </paper>
    <paper id="331">
      <title>What is wrong with you?: Leveraging User Sentiment for Automatic Dialog Evaluation</title>
      <author><first>Sarik</first><last>Ghazarian</last></author>
      <author><first>Behnam</first><last>Hedayatnia</last></author>
      <author><first>Alexandros</first><last>Papangelis</last></author>
      <author id="yang-liu-icsi"><first>Yang</first><last>Liu</last></author>
      <author><first>Dilek</first><last>Hakkani-Tur</last></author>
      <pages>4194-4204</pages>
      <abstract>Accurate automatic evaluation metrics for open-domain dialogs are in high demand. Existing model-based metrics for system response evaluation are trained on human annotated data, which is cumbersome to collect. In this work, we propose to use information that can be automatically extracted from the next user utterance, such as its sentiment or whether the user explicitly ends the conversation, as a proxy to measure the quality of the previous system response. This allows us to train on a massive set of dialogs with weak supervision, without requiring manual system turn quality annotations. Experiments show that our model is comparable to models trained on human annotated data. Furthermore, our model generalizes across both spoken and written open-domain dialog corpora collected from real and paid users.</abstract>
      <url hash="766b74d5">2022.findings-acl.331</url>
      <bibkey>ghazarian-etal-2022-wrong</bibkey>
      <pwccode url="https://github.com/alexa/conture" additional="false">alexa/conture</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fed">FED</pwcdataset>
    </paper>
  </volume>
</collection>
