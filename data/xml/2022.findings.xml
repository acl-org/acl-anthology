<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.findings">
  <volume id="acl" ingest-date="2022-05-15" type="proceedings">
    <meta>
      <booktitle>Findings of the Association for Computational Linguistics: ACL 2022</booktitle>
      <editor><first>Smaranda</first><last>Muresan</last></editor>
      <editor><first>Preslav</first><last>Nakov</last></editor>
      <editor><first>Aline</first><last>Villavicencio</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Dublin, Ireland</address>
      <month>May</month>
      <year>2022</year>
      <url hash="698a5d93">2022.findings-acl</url>
      <venue>findings</venue>
    </meta>
    <frontmatter>
      <url hash="eb6cc2a4">2022.findings-acl.0</url>
      <bibkey>findings-2022-findings</bibkey>
    </frontmatter>
    <paper id="1">
      <title>“Is Whole Word Masking Always Better for <fixed-case>C</fixed-case>hinese <fixed-case>BERT</fixed-case>?”: Probing on <fixed-case>C</fixed-case>hinese Grammatical Error Correction</title>
      <author><first>Yong</first><last>Dai</last></author>
      <author><first>Linyang</first><last>Li</last></author>
      <author><first>Cong</first><last>Zhou</last></author>
      <author><first>Zhangyin</first><last>Feng</last></author>
      <author><first>Enbo</first><last>Zhao</last></author>
      <author><first>Xipeng</first><last>Qiu</last></author>
      <author><first>Piji</first><last>Li</last></author>
      <author><first>Duyu</first><last>Tang</last></author>
      <pages>1-8</pages>
      <abstract>Whole word masking (WWM), which masks all subwords corresponding to a word at once, makes a better English BERT model. For the Chinese language, however, there is no subword because each token is an atomic character. The meaning of a word in Chinese is different in that a word is a compositional unit consisting of multiple characters. Such difference motivates us to investigate whether WWM leads to better context understanding ability for Chinese BERT. To achieve this, we introduce two probing tasks related to grammatical error correction and ask pretrained models to revise or insert tokens in a masked language modeling manner. We construct a dataset including labels for 19,075 tokens in 10,448 sentences. We train three Chinese BERT models with standard character-level masking (CLM), WWM, and a combination of CLM and WWM, respectively. Our major findings are as follows: First, when one character needs to be inserted or replaced, the model trained with CLM performs the best. Second, when more than one character needs to be handled, WWM is the key to better performance. Finally, when being fine-tuned on sentence-level downstream tasks, models trained with different masking strategies perform comparably.</abstract>
      <url hash="df716d18">2022.findings-acl.1</url>
      <bibkey>dai-etal-2022-whole</bibkey>
      <doi>10.18653/v1/2022.findings-acl.1</doi>
    </paper>
    <paper id="2">
      <title>Compilable Neural Code Generation with Compiler Feedback</title>
      <author><first>Xin</first><last>Wang</last></author>
      <author><first>Yasheng</first><last>Wang</last></author>
      <author><first>Yao</first><last>Wan</last></author>
      <author><first>Fei</first><last>Mi</last></author>
      <author><first>Yitong</first><last>Li</last></author>
      <author><first>Pingyi</first><last>Zhou</last></author>
      <author><first>Jin</first><last>Liu</last></author>
      <author><first>Hao</first><last>Wu</last></author>
      <author><first>Xin</first><last>Jiang</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <pages>9-19</pages>
      <abstract>Automatically generating compilable programs with (or without) natural language descriptions has always been a touchstone problem for computational linguistics and automated software engineering. Existing deep-learning approaches model code generation as text generation, either constrained by grammar structures in decoder, or driven by pre-trained language models on large-scale code corpus (e.g., CodeGPT, PLBART, and CodeT5). However, few of them account for compilability of the generated programs. To improve compilability of the generated programs, this paper proposes COMPCODER, a three-stage pipeline utilizing compiler feedback for compilable code generation, including language model fine-tuning, compilability reinforcement, and compilability discrimination. Comprehensive experiments on two code generation tasks demonstrate the effectiveness of our proposed approach, improving the success rate of compilation from 44.18 to 89.18 in code completion on average and from 70.3 to 96.2 in text-to-code generation, respectively, when comparing with the state-of-the-art CodeGPT.</abstract>
      <url hash="a411c1bf">2022.findings-acl.2</url>
      <bibkey>wang-etal-2022-compilable</bibkey>
      <doi>10.18653/v1/2022.findings-acl.2</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/codesearchnet">CodeSearchNet</pwcdataset>
    </paper>
    <paper id="3">
      <title>Towards Unifying the Label Space for Aspect- and Sentence-based Sentiment Analysis</title>
      <author><first>Yiming</first><last>Zhang</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <author><first>Sai</first><last>Wu</last></author>
      <author><first>Junbo</first><last>Zhao</last></author>
      <pages>20-30</pages>
      <abstract>The aspect-based sentiment analysis (ABSA) is a fine-grained task that aims to determine the sentiment polarity towards targeted aspect terms occurring in the sentence. The development of the ABSA task is very much hindered by the lack of annotated data. To tackle this, the prior works have studied the possibility of utilizing the sentiment analysis (SA) datasets to assist in training the ABSA model, primarily via pretraining or multi-task learning. In this article, we follow this line, and for the first time, we manage to apply the Pseudo-Label (PL) method to merge the two homogeneous tasks. While it seems straightforward to use generated pseudo labels to handle this case of label granularity unification for two highly related tasks, we identify its major challenge in this paper and propose a novel framework, dubbed as Dual-granularity Pseudo Labeling (DPL). Further, similar to PL, we regard the DPL as a general framework capable of combining other prior methods in the literature. Through extensive experiments, DPL has achieved state-of-the-art performance on standard benchmarks surpassing the prior work significantly.</abstract>
      <url hash="7ab13df3">2022.findings-acl.3</url>
      <bibkey>zhang-etal-2022-towards</bibkey>
      <doi>10.18653/v1/2022.findings-acl.3</doi>
      <pwccode url="https://github.com/yiming-zh/DPL" additional="false">yiming-zh/DPL</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/semeval-2014-task-4-sub-task-2">SemEval-2014 Task-4</pwcdataset>
    </paper>
    <paper id="4">
      <title>Input-specific Attention Subnetworks for Adversarial Detection</title>
      <author><first>Emil</first><last>Biju</last></author>
      <author><first>Anirudh</first><last>Sriram</last></author>
      <author><first>Pratyush</first><last>Kumar</last></author>
      <author><first>Mitesh</first><last>Khapra</last></author>
      <pages>31-44</pages>
      <abstract>Self-attention heads are characteristic of Transformer models and have been well studied for interpretability and pruning. In this work, we demonstrate an altogether different utility of attention heads, namely for adversarial detection. Specifically, we propose a method to construct input-specific attention subnetworks (IAS) from which we extract three features to discriminate between authentic and adversarial inputs. The resultant detector significantly improves (by over 7.5%) the state-of-the-art adversarial detection accuracy for the BERT encoder on 10 NLU datasets with 11 different adversarial attack types. We also demonstrate that our method (a) is more accurate for larger models which are likely to have more spurious correlations and thus vulnerable to adversarial attack, and (b) performs well even with modest training sets of adversarial examples.</abstract>
      <url hash="5581daa2">2022.findings-acl.4</url>
      <attachment type="software" hash="933945e1">2022.findings-acl.4.software.zip</attachment>
      <bibkey>biju-etal-2022-input</bibkey>
      <doi>10.18653/v1/2022.findings-acl.4</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst-2">SST-2</pwcdataset>
    </paper>
    <paper id="5">
      <title><fixed-case>R</fixed-case>elation<fixed-case>P</fixed-case>rompt: Leveraging Prompts to Generate Synthetic Data for Zero-Shot Relation Triplet Extraction</title>
      <author><first>Yew Ken</first><last>Chia</last></author>
      <author><first>Lidong</first><last>Bing</last></author>
      <author><first>Soujanya</first><last>Poria</last></author>
      <author><first>Luo</first><last>Si</last></author>
      <pages>45-57</pages>
      <abstract>Despite the importance of relation extraction in building and representing knowledge, less research is focused on generalizing to unseen relations types. We introduce the task setting of Zero-Shot Relation Triplet Extraction (ZeroRTE) to encourage further research in low-resource relation extraction methods. Given an input sentence, each extracted triplet consists of the head entity, relation label, and tail entity where the relation label is not seen at the training stage. To solve ZeroRTE, we propose to synthesize relation examples by prompting language models to generate structured texts. Concretely, we unify language model prompts and structured text approaches to design a structured prompt template for generating synthetic relation samples when conditioning on relation label prompts (RelationPrompt). To overcome the limitation for extracting multiple relation triplets in a sentence, we design a novel Triplet Search Decoding method. Experiments on FewRel and Wiki-ZSL datasets show the efficacy of RelationPrompt for the ZeroRTE task and zero-shot relation classification. Our code and data are available at github.com/declare-lab/RelationPrompt.</abstract>
      <url hash="8e567b7a">2022.findings-acl.5</url>
      <bibkey>chia-etal-2022-relationprompt</bibkey>
      <doi>10.18653/v1/2022.findings-acl.5</doi>
      <video href="2022.findings-acl.5.mp4"/>
      <pwccode url="https://github.com/declare-lab/relationprompt" additional="true">declare-lab/relationprompt</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel">FewRel</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wiki-zsl">Wiki-ZSL</pwcdataset>
    </paper>
    <paper id="6">
      <title>Pre-Trained Multilingual Sequence-to-Sequence Models: A Hope for Low-Resource Language Translation?</title>
      <author><first>En-Shiun</first><last>Lee</last></author>
      <author><first>Sarubi</first><last>Thillainathan</last></author>
      <author><first>Shravan</first><last>Nayak</last></author>
      <author><first>Surangika</first><last>Ranathunga</last></author>
      <author><first>David</first><last>Adelani</last></author>
      <author><first>Ruisi</first><last>Su</last></author>
      <author><first>Arya</first><last>McCarthy</last></author>
      <pages>58-67</pages>
      <abstract>What can pre-trained multilingual sequence-to-sequence models like mBART contribute to translating low-resource languages? We conduct a thorough empirical experiment in 10 languages to ascertain this, considering five factors: (1) the amount of fine-tuning data, (2) the noise in the fine-tuning data, (3) the amount of pre-training data in the model, (4) the impact of domain mismatch, and (5) language typology. In addition to yielding several heuristics, the experiments form a framework for evaluating the data sensitivities of machine translation systems. While mBART is robust to domain differences, its translations for unseen and typologically distant languages remain below 3.0 BLEU. In answer to our title’s question, mBART is not a low-resource panacea; we therefore encourage shifting the emphasis from new models to new data.</abstract>
      <url hash="57ee5031">2022.findings-acl.6</url>
      <attachment type="software" hash="a436de63">2022.findings-acl.6.software.zip</attachment>
      <bibkey>lee-etal-2022-pre</bibkey>
      <doi>10.18653/v1/2022.findings-acl.6</doi>
      <video href="2022.findings-acl.6.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/pmindia">PMIndia</pwcdataset>
    </paper>
    <paper id="7">
      <title>Multi-Scale Distribution Deep Variational Autoencoder for Explanation Generation</title>
      <author><first>ZeFeng</first><last>Cai</last></author>
      <author><first>Linlin</first><last>Wang</last></author>
      <author><first>Gerard</first><last>de Melo</last></author>
      <author><first>Fei</first><last>Sun</last></author>
      <author><first>Liang</first><last>He</last></author>
      <pages>68-78</pages>
      <abstract>Generating explanations for recommender systems is essential for improving their transparency, as users often wish to understand the reason for receiving a specified recommendation. Previous methods mainly focus on improving the generation quality, but often produce generic explanations that fail to incorporate user and item specific details. To resolve this problem, we present Multi-Scale Distribution Deep Variational Autoencoders (MVAE).These are deep hierarchical VAEs with a prior network that eliminates noise while retaining meaningful signals in the input, coupled with a recognition network serving as the source of information to guide the learning of the prior network. Further, the Multi-scale distribution Learning Framework (MLF) along with a Target Tracking Kullback-Leibler divergence (TKL) mechanism are proposed to employ multi KL divergences at different scales for more effective learning. Extensive empirical experiments demonstrate that our methods can generate explanations with concrete input-specific contents.</abstract>
      <url hash="d9aa8f0a">2022.findings-acl.7</url>
      <bibkey>cai-etal-2022-multi</bibkey>
      <doi>10.18653/v1/2022.findings-acl.7</doi>
    </paper>
    <paper id="8">
      <title>Dual Context-Guided Continuous Prompt Tuning for Few-Shot Learning</title>
      <author><first>Jie</first><last>Zhou</last></author>
      <author><first>Le</first><last>Tian</last></author>
      <author><first>Houjin</first><last>Yu</last></author>
      <author><first>Zhou</first><last>Xiao</last></author>
      <author><first>Hui</first><last>Su</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>79-84</pages>
      <abstract>Prompt-based paradigm has shown its competitive performance in many NLP tasks. However, its success heavily depends on prompt design, and the effectiveness varies upon the model and training data. In this paper, we propose a novel dual context-guided continuous prompt (DCCP) tuning method. To explore the rich contextual information in language structure and close the gap between discrete prompt tuning and continuous prompt tuning, DCCP introduces two auxiliary training objectives and constructs input in a pair-wise fashion. Experimental results demonstrate that our method is applicable to many NLP tasks, and can often outperform existing prompt tuning methods by a large margin in the few-shot setting.</abstract>
      <url hash="9656d26e">2022.findings-acl.8</url>
      <bibkey>zhou-etal-2022-dual</bibkey>
      <doi>10.18653/v1/2022.findings-acl.8</doi>
    </paper>
    <paper id="9">
      <title>Extract-Select: A Span Selection Framework for Nested Named Entity Recognition with Generative Adversarial Training</title>
      <author><first>Peixin</first><last>Huang</last></author>
      <author><first>Xiang</first><last>Zhao</last></author>
      <author><first>Minghao</first><last>Hu</last></author>
      <author><first>Yang</first><last>Fang</last></author>
      <author><first>Xinyi</first><last>Li</last></author>
      <author><first>Weidong</first><last>Xiao</last></author>
      <pages>85-96</pages>
      <abstract>Nested named entity recognition (NER) is a task in which named entities may overlap with each other. Span-based approaches regard nested NER as a two-stage span enumeration and classification task, thus having the innate ability to handle this task. However, they face the problems of error propagation, ignorance of span boundary, difficulty in long entity recognition and requirement on large-scale annotated data. In this paper, we propose Extract-Select, a span selection framework for nested NER, to tackle these problems. Firstly, we introduce a span selection framework in which nested entities with different input categories would be separately extracted by the extractor, thus naturally avoiding error propagation in two-stage span-based approaches. In the inference phase, the trained extractor selects final results specific to the given entity category. Secondly, we propose a hybrid selection strategy in the extractor, which not only makes full use of span boundary but also improves the ability of long entity recognition. Thirdly, we design a discriminator to evaluate the extraction result, and train both extractor and discriminator with generative adversarial training (GAT). The use of GAT greatly alleviates the stress on the dataset size. Experimental results on four benchmark datasets demonstrate that Extract-Select outperforms competitive nested NER models, obtaining state-of-the-art results. The proposed model also performs well when less labeled data are given, proving the effectiveness of GAT.</abstract>
      <url hash="ab6638a3">2022.findings-acl.9</url>
      <bibkey>huang-etal-2022-extract</bibkey>
      <doi>10.18653/v1/2022.findings-acl.9</doi>
    </paper>
    <paper id="10">
      <title>Controlled Text Generation Using Dictionary Prior in Variational Autoencoders</title>
      <author><first>Xianghong</first><last>Fang</last></author>
      <author><first>Jian</first><last>Li</last></author>
      <author><first>Lifeng</first><last>Shang</last></author>
      <author><first>Xin</first><last>Jiang</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <author><first>Dit-Yan</first><last>Yeung</last></author>
      <pages>97-111</pages>
      <abstract>While variational autoencoders (VAEs) have been widely applied in text generation tasks, they are troubled by two challenges: insufficient representation capacity and poor controllability. The former results from the posterior collapse and restrictive assumption, which impede better representation learning. The latter arises as continuous latent variables in traditional formulations hinder VAEs from interpretability and controllability. In this paper, we propose Dictionary Prior (DPrior), a new data-driven prior that enjoys the merits of expressivity and controllability. To facilitate controlled text generation with DPrior, we propose to employ contrastive learning to separate the latent space into several parts. Extensive experiments on both language modeling and controlled text generation demonstrate the effectiveness of the proposed approach.</abstract>
      <url hash="b4e105c5">2022.findings-acl.10</url>
      <bibkey>fang-etal-2022-controlled</bibkey>
      <doi>10.18653/v1/2022.findings-acl.10</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="11">
      <title>Challenges to Open-Domain Constituency Parsing</title>
      <author><first>Sen</first><last>Yang</last></author>
      <author><first>Leyang</first><last>Cui</last></author>
      <author><first>Ruoxi</first><last>Ning</last></author>
      <author><first>Di</first><last>Wu</last></author>
      <author><first>Yue</first><last>Zhang</last></author>
      <pages>112-127</pages>
      <abstract>Neural constituency parsers have reached practical performance on news-domain benchmarks. However, their generalization ability to other domains remains weak. Existing findings on cross-domain constituency parsing are only made on a limited number of domains. Tracking this, we manually annotate a high-quality constituency treebank containing five domains. We analyze challenges to open-domain constituency parsing using a set of linguistic features on various strong constituency parsers. Primarily, we find that 1) BERT significantly increases parsers’ cross-domain performance by reducing their sensitivity on the domain-variant features.2) Compared with single metrics such as unigram distribution and OOV rate, challenges to open-domain constituency parsing arise from complex features, including cross-domain lexical and constituent structure variations.</abstract>
      <url hash="037dfd3a">2022.findings-acl.11</url>
      <bibkey>yang-etal-2022-challenges</bibkey>
      <doi>10.18653/v1/2022.findings-acl.11</doi>
      <pwccode url="https://github.com/ringos/multi-domain-parsing-analysis" additional="true">ringos/multi-domain-parsing-analysis</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="12">
      <title>Going “Deeper”: Structured Sememe Prediction via Transformer with Tree Attention</title>
      <author><first>Yining</first><last>Ye</last></author>
      <author><first>Fanchao</first><last>Qi</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <pages>128-138</pages>
      <abstract>Sememe knowledge bases (SKBs), which annotate words with the smallest semantic units (i.e., sememes), have proven beneficial to many NLP tasks. Building an SKB is very time-consuming and labor-intensive. Therefore, some studies have tried to automate the building process by predicting sememes for the unannotated words. However, all existing sememe prediction studies ignore the hierarchical structures of sememes, which are important in the sememe-based semantic description system. In this work, we tackle the structured sememe prediction problem for the first time, which is aimed at predicting a sememe tree with hierarchical structures rather than a set of sememes. We design a sememe tree generation model based on Transformer with adjusted attention mechanism, which shows its superiority over the baselines in experiments. We also conduct a series of quantitative and qualitative analyses of the effectiveness of our model. All the code and data of this paper are available at <url>https://github.com/thunlp/STG</url>.</abstract>
      <url hash="1ec3124f">2022.findings-acl.12</url>
      <attachment type="software" hash="c59dd366">2022.findings-acl.12.software.zip</attachment>
      <bibkey>ye-etal-2022-going</bibkey>
      <doi>10.18653/v1/2022.findings-acl.12</doi>
      <pwccode url="https://github.com/thunlp/stg" additional="false">thunlp/stg</pwccode>
    </paper>
    <paper id="13">
      <title>Table-based Fact Verification with Self-adaptive Mixture of Experts</title>
      <author><first>Yuxuan</first><last>Zhou</last></author>
      <author><first>Xien</first><last>Liu</last></author>
      <author><first>Kaiyin</first><last>Zhou</last></author>
      <author><first>Ji</first><last>Wu</last></author>
      <pages>139-149</pages>
      <abstract>The table-based fact verification task has recently gained widespread attention and yet remains to be a very challenging problem. It inherently requires informative reasoning over natural language together with different numerical and logical reasoning on tables (e.g., count, superlative, comparative). Considering that, we exploit mixture-of-experts and present in this paper a new method: Self-adaptive Mixture-of-Experts Network (SaMoE). Specifically, we have developed a mixture-of-experts neural network to recognize and execute different types of reasoning—the network is composed of multiple experts, each handling a specific part of the semantics for reasoning, whereas a management module is applied to decide the contribution of each expert network to the verification result. A self-adaptive method is developed to teach the management module combining results of different experts more efficiently without external knowledge. The experimental results illustrate that our framework achieves 85.1% accuracy on the benchmark dataset TabFact, comparable with the previous state-of-the-art models. We hope our framework can serve as a new baseline for table-based verification. Our code is available at <url>https://github.com/THUMLP/SaMoE</url>.</abstract>
      <url hash="005f294b">2022.findings-acl.13</url>
      <bibkey>zhou-etal-2022-table</bibkey>
      <doi>10.18653/v1/2022.findings-acl.13</doi>
      <pwccode url="https://github.com/thumlp/samoe" additional="false">thumlp/samoe</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/tabfact">TabFact</pwcdataset>
    </paper>
    <paper id="14">
      <title>Investigating Data Variance in Evaluations of Automatic Machine Translation Metrics</title>
      <author><first>Jiannan</first><last>Xiang</last></author>
      <author><first>Huayang</first><last>Li</last></author>
      <author><first>Yahui</first><last>Liu</last></author>
      <author><first>Lemao</first><last>Liu</last></author>
      <author><first>Guoping</first><last>Huang</last></author>
      <author><first>Defu</first><last>Lian</last></author>
      <author><first>Shuming</first><last>Shi</last></author>
      <pages>150-157</pages>
      <abstract>Current practices in metric evaluation focus on one single dataset, e.g., Newstest dataset in each year’s WMT Metrics Shared Task. However, in this paper, we qualitatively and quantitatively show that the performances of metrics are sensitive to data. The ranking of metrics varies when the evaluation is conducted on different datasets. Then this paper further investigates two potential hypotheses, i.e., insignificant data points and the deviation of i.i.d assumption, which may take responsibility for the issue of data variance. In conclusion, our findings suggest that when evaluating automatic translation metrics, researchers should take data variance into account and be cautious to report the results on unreliable datasets, because it may leads to inconsistent results with most of the other datasets.</abstract>
      <url hash="de0daa53">2022.findings-acl.14</url>
      <bibkey>xiang-etal-2022-investigating</bibkey>
      <doi>10.18653/v1/2022.findings-acl.14</doi>
    </paper>
    <paper id="15">
      <title>Sememe Prediction for <fixed-case>B</fixed-case>abel<fixed-case>N</fixed-case>et Synsets using Multilingual and Multimodal Information</title>
      <author><first>Fanchao</first><last>Qi</last></author>
      <author><first>Chuancheng</first><last>Lv</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Xiaojun</first><last>Meng</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <author><first>Hai-Tao</first><last>Zheng</last></author>
      <pages>158-168</pages>
      <abstract>In linguistics, a sememe is defined as the minimum semantic unit of languages. Sememe knowledge bases (KBs), which are built by manually annotating words with sememes, have been successfully applied to various NLP tasks. However, existing sememe KBs only cover a few languages, which hinders the wide utilization of sememes. To address this issue, the task of sememe prediction for BabelNet synsets (SPBS) is presented, aiming to build a multilingual sememe KB based on BabelNet, a multilingual encyclopedia dictionary. By automatically predicting sememes for a BabelNet synset, the words in many languages in the synset would obtain sememe annotations simultaneously. However, previous SPBS methods have not taken full advantage of the abundant information in BabelNet. In this paper, we utilize the multilingual synonyms, multilingual glosses and images in BabelNet for SPBS. We design a multimodal information fusion model to encode and combine this information for sememe prediction. Experimental results show the substantial outperformance of our model over previous methods (about 10 MAP and F1 scores). All the code and data of this paper can be obtained at <url>https://github.com/thunlp/MSGI</url>.</abstract>
      <url hash="e4577237">2022.findings-acl.15</url>
      <attachment type="software" hash="7d6f2fd3">2022.findings-acl.15.software.zip</attachment>
      <bibkey>qi-etal-2022-sememe</bibkey>
      <doi>10.18653/v1/2022.findings-acl.15</doi>
      <pwccode url="https://github.com/thunlp/msgi" additional="false">thunlp/msgi</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imagenet">ImageNet</pwcdataset>
    </paper>
    <paper id="16">
      <title>Query and Extract: Refining Event Extraction as Type-oriented Binary Decoding</title>
      <author><first>Sijia</first><last>Wang</last></author>
      <author><first>Mo</first><last>Yu</last></author>
      <author><first>Shiyu</first><last>Chang</last></author>
      <author><first>Lichao</first><last>Sun</last></author>
      <author><first>Lifu</first><last>Huang</last></author>
      <pages>169-182</pages>
      <abstract>Event extraction is typically modeled as a multi-class classification problem where event types and argument roles are treated as atomic symbols. These approaches are usually limited to a set of pre-defined types. We propose a novel event extraction framework that uses event types and argument roles as natural language queries to extract candidate triggers and arguments from the input text. With the rich semantics in the queries, our framework benefits from the attention mechanisms to better capture the semantic correlation between the event types or argument roles and the input text. Furthermore, the query-and-extract formulation allows our approach to leverage all available event annotations from various ontologies as a unified model. Experiments on ACE and ERE demonstrate that our approach achieves state-of-the-art performance on each dataset and significantly outperforms existing methods on zero-shot event extraction.</abstract>
      <url hash="9299452c">2022.findings-acl.16</url>
      <bibkey>wang-etal-2022-query</bibkey>
      <doi>10.18653/v1/2022.findings-acl.16</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/maven">MAVEN</pwcdataset>
    </paper>
    <paper id="17">
      <title><fixed-case>LEVEN</fixed-case>: A Large-Scale <fixed-case>C</fixed-case>hinese Legal Event Detection Dataset</title>
      <author><first>Feng</first><last>Yao</last></author>
      <author><first>Chaojun</first><last>Xiao</last></author>
      <author><first>Xiaozhi</first><last>Wang</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Lei</first><last>Hou</last></author>
      <author><first>Cunchao</first><last>Tu</last></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <author><first>Yun</first><last>Liu</last></author>
      <author><first>Weixing</first><last>Shen</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <pages>183-201</pages>
      <abstract>Recognizing facts is the most fundamental step in making judgments, hence detecting events in the legal documents is important to legal case analysis tasks. However, existing Legal Event Detection (LED) datasets only concern incomprehensive event types and have limited annotated data, which restricts the development of LED methods and their downstream applications. To alleviate these issues, we present LEVEN a large-scale Chinese LEgal eVENt detection dataset, with 8,116 legal documents and 150,977 human-annotated event mentions in 108 event types. Not only charge-related events, LEVEN also covers general events, which are critical for legal case understanding but neglected in existing LED datasets. To our knowledge, LEVEN is the largest LED dataset and has dozens of times the data scale of others, which shall significantly promote the training and evaluation of LED methods. The results of extensive experiments indicate that LED is challenging and needs further effort. Moreover, we simply utilize legal events as side information to promote downstream applications. The method achieves improvements of average 2.2 points precision in low-resource judgment prediction, and 1.5 points mean average precision in unsupervised case retrieval, which suggests the fundamentality of LED. The source code and dataset can be obtained from <url>https://github.com/thunlp/LEVEN</url>.</abstract>
      <url hash="35ebebe5">2022.findings-acl.17</url>
      <attachment type="software" hash="4ef7c368">2022.findings-acl.17.software.zip</attachment>
      <bibkey>yao-etal-2022-leven</bibkey>
      <doi>10.18653/v1/2022.findings-acl.17</doi>
      <pwccode url="https://github.com/thunlp/leven" additional="false">thunlp/leven</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/leven">LEVEN</pwcdataset>
    </paper>
    <paper id="18">
      <title>Analyzing Dynamic Adversarial Training Data in the Limit</title>
      <author><first>Eric</first><last>Wallace</last></author>
      <author><first>Adina</first><last>Williams</last></author>
      <author><first>Robin</first><last>Jia</last></author>
      <author><first>Douwe</first><last>Kiela</last></author>
      <pages>202-217</pages>
      <abstract>To create models that are robust across a wide range of test inputs, training datasets should include diverse examples that span numerous phenomena. Dynamic adversarial data collection (DADC), where annotators craft examples that challenge continually improving models, holds promise as an approach for generating such diverse training sets. Prior work has shown that running DADC over 1-3 rounds can help models fix some error types, but it does not necessarily lead to better generalization beyond adversarial test data. We argue that running DADC over many rounds maximizes its training-time benefits, as the different rounds can together cover many of the task-relevant phenomena. We present the first study of longer-term DADC, where we collect 20 rounds of NLI examples for a small set of premise paragraphs, with both adversarial and non-adversarial approaches. Models trained on DADC examples make 26% fewer errors on our expert-curated test set compared to models trained on non-adversarial data. Our analysis shows that DADC yields examples that are more difficult, more lexically and syntactically diverse, and contain fewer annotation artifacts compared to non-adversarial examples.</abstract>
      <url hash="63428b6c">2022.findings-acl.18</url>
      <bibkey>wallace-etal-2022-analyzing</bibkey>
      <doi>10.18653/v1/2022.findings-acl.18</doi>
      <video href="2022.findings-acl.18.mp4"/>
      <pwccode url="https://github.com/facebookresearch/dadc-limit" additional="false">facebookresearch/dadc-limit</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="19">
      <title><fixed-case>A</fixed-case>bduction<fixed-case>R</fixed-case>ules: Training Transformers to Explain Unexpected Inputs</title>
      <author><first>Nathan</first><last>Young</last></author>
      <author><first>Qiming</first><last>Bao</last></author>
      <author><first>Joshua</first><last>Bensemann</last></author>
      <author><first>Michael</first><last>Witbrock</last></author>
      <pages>218-227</pages>
      <abstract>Transformers have recently been shown to be capable of reliably performing logical reasoning over facts and rules expressed in natural language, but abductive reasoning - inference to the best explanation of an unexpected observation - has been underexplored despite significant applications to scientific discovery, common-sense reasoning, and model interpretability. This paper presents AbductionRules, a group of natural language datasets designed to train and test generalisable abduction over natural-language knowledge bases. We use these datasets to finetune pretrained Transformers and discuss their performance, finding that our models learned generalisable abductive techniques but also learned to exploit the structure of our data. Finally, we discuss the viability of this approach to abductive reasoning and ways in which it may be improved in future work.</abstract>
      <url hash="16eed3fd">2022.findings-acl.19</url>
      <bibkey>young-etal-2022-abductionrules</bibkey>
      <doi>10.18653/v1/2022.findings-acl.19</doi>
      <pwccode url="https://github.com/strong-ai-lab/abductionrules" additional="false">strong-ai-lab/abductionrules</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/proofwriter">ProofWriter</pwcdataset>
    </paper>
    <paper id="20">
      <title>On the Importance of Data Size in Probing Fine-tuned Models</title>
      <author><first>Houman</first><last>Mehrafarin</last></author>
      <author><first>Sara</first><last>Rajaee</last></author>
      <author><first>Mohammad Taher</first><last>Pilehvar</last></author>
      <pages>228-238</pages>
      <abstract>Several studies have investigated the reasons behind the effectiveness of fine-tuning, usually through the lens of probing. However, these studies often neglect the role of the size of the dataset on which the model is fine-tuned. In this paper, we highlight the importance of this factor and its undeniable role in probing performance. We show that the extent of encoded linguistic knowledge depends on the number of fine-tuning samples. The analysis also reveals that larger training data mainly affects higher layers, and that the extent of this change is a factor of the number of iterations updating the model during fine-tuning rather than the diversity of the training samples. Finally, we show through a set of experiments that fine-tuning data size affects the recoverability of the changes made to the model’s linguistic knowledge.</abstract>
      <url hash="2f6fef4a">2022.findings-acl.20</url>
      <bibkey>mehrafarin-etal-2022-importance</bibkey>
      <doi>10.18653/v1/2022.findings-acl.20</doi>
      <pwccode url="https://github.com/hmehrafarin/data-size-analysis" additional="false">hmehrafarin/data-size-analysis</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst-2">SST-2</pwcdataset>
    </paper>
    <paper id="21">
      <title><fixed-case>R</fixed-case>u<fixed-case>CC</fixed-case>o<fixed-case>N</fixed-case>: Clinical Concept Normalization in <fixed-case>R</fixed-case>ussian</title>
      <author><first>Alexandr</first><last>Nesterov</last></author>
      <author><first>Galina</first><last>Zubkova</last></author>
      <author><first>Zulfat</first><last>Miftahutdinov</last></author>
      <author><first>Vladimir</first><last>Kokh</last></author>
      <author><first>Elena</first><last>Tutubalina</last></author>
      <author><first>Artem</first><last>Shelmanov</last></author>
      <author><first>Anton</first><last>Alekseev</last></author>
      <author><first>Manvel</first><last>Avetisian</last></author>
      <author><first>Andrey</first><last>Chertok</last></author>
      <author><first>Sergey</first><last>Nikolenko</last></author>
      <pages>239-245</pages>
      <abstract>We present RuCCoN, a new dataset for clinical concept normalization in Russian manually annotated by medical professionals. It contains over 16,028 entity mentions manually linked to over 2,409 unique concepts from the Russian language part of the UMLS ontology. We provide train/test splits for different settings (stratified, zero-shot, and CUI-less) and present strong baselines obtained with state-of-the-art models such as SapBERT. At present, Russian medical NLP is lacking in both datasets and trained models, and we view this work as an important step towards filling this gap. Our dataset and annotation guidelines are available at <url>https://github.com/sberbank-ai-lab/RuCCoN</url>.</abstract>
      <url hash="8f620f3a">2022.findings-acl.21</url>
      <bibkey>nesterov-etal-2022-ruccon</bibkey>
      <doi>10.18653/v1/2022.findings-acl.21</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/xl-bel">XL-BEL</pwcdataset>
    </paper>
    <paper id="22">
      <title>A Sentence is Worth 128 Pseudo Tokens: A Semantic-Aware Contrastive Learning Framework for Sentence Embeddings</title>
      <author><first>Haochen</first><last>Tan</last></author>
      <author><first>Wei</first><last>Shao</last></author>
      <author><first>Han</first><last>Wu</last></author>
      <author><first>Ke</first><last>Yang</last></author>
      <author><first>Linqi</first><last>Song</last></author>
      <pages>246-256</pages>
      <abstract>Contrastive learning has shown great potential in unsupervised sentence embedding tasks, e.g., SimCSE (CITATION).However, these existing solutions are heavily affected by superficial features like the length of sentences or syntactic structures. In this paper, we propose a semantic-aware contrastive learning framework for sentence embeddings, termed Pseudo-Token BERT (PT-BERT), which is able to explore the pseudo-token space (i.e., latent semantic space) representation of a sentence while eliminating the impact of superficial features such as sentence length and syntax. Specifically, we introduce an additional pseudo token embedding layer independent of the BERT encoder to map each sentence into a sequence of pseudo tokens in a fixed length. Leveraging these pseudo sequences, we are able to construct same-length positive and negative pairs based on the attention mechanism to perform contrastive learning. In addition, we utilize both the gradient-updating and momentum-updating encoders to encode instances while dynamically maintaining an additional queue to store the representation of sentence embeddings, enhancing the encoder’s learning performance for negative examples. Experiments show that our model outperforms the state-of-the-art baselines on six standard semantic textual similarity (STS) tasks. Furthermore, experiments on alignments and uniformity losses, as well as hard examples with different sentence lengths and syntax, consistently verify the effectiveness of our method.</abstract>
      <url hash="8b6b9cf4">2022.findings-acl.22</url>
      <bibkey>tan-etal-2022-sentence</bibkey>
      <doi>10.18653/v1/2022.findings-acl.22</doi>
      <pwccode url="https://github.com/namco0816/pt-bert" additional="false">namco0816/pt-bert</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/sick">SICK</pwcdataset>
    </paper>
    <paper id="23">
      <title>Eider: Empowering Document-level Relation Extraction with Efficient Evidence Extraction and Inference-stage Fusion</title>
      <author><first>Yiqing</first><last>Xie</last></author>
      <author><first>Jiaming</first><last>Shen</last></author>
      <author><first>Sha</first><last>Li</last></author>
      <author><first>Yuning</first><last>Mao</last></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <pages>257-268</pages>
      <abstract>Document-level relation extraction (DocRE) aims to extract semantic relations among entity pairs in a document. Typical DocRE methods blindly take the full document as input, while a subset of the sentences in the document, noted as the evidence, are often sufficient for humans to predict the relation of an entity pair. In this paper, we propose an evidence-enhanced framework, Eider, that empowers DocRE by efficiently extracting evidence and effectively fusing the extracted evidence in inference. We first jointly train an RE model with a lightweight evidence extraction model, which is efficient in both memory and runtime. Empirically, even training the evidence model on silver labels constructed by our heuristic rules can lead to better RE performance. We further design a simple yet effective inference process that makes RE predictions on both extracted evidence and the full document, then fuses the predictions through a blending layer. This allows Eider to focus on important sentences while still having access to the complete information in the document. Extensive experiments show that Eider outperforms state-of-the-art methods on three benchmark datasets (e.g., by 1.37/1.26 Ign F1/F1 on DocRED).</abstract>
      <url hash="b95e5be7">2022.findings-acl.23</url>
      <bibkey>xie-etal-2022-eider</bibkey>
      <doi>10.18653/v1/2022.findings-acl.23</doi>
      <pwccode url="https://github.com/veronicium/eider" additional="false">veronicium/eider</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/docred">DocRED</pwcdataset>
    </paper>
    <paper id="24">
      <title>Meta-X<tex-math>_{NLG}</tex-math>: A Meta-Learning Approach Based on Language Clustering for Zero-Shot Cross-Lingual Transfer and Generation</title>
      <author><first>Kaushal</first><last>Maurya</last></author>
      <author><first>Maunendra</first><last>Desarkar</last></author>
      <pages>269-284</pages>
      <abstract>Recently, the NLP community has witnessed a rapid advancement in multilingual and cross-lingual transfer research where the supervision is transferred from high-resource languages (HRLs) to low-resource languages (LRLs). However, the cross-lingual transfer is not uniform across languages, particularly in the zero-shot setting. Towards this goal, one promising research direction is to learn shareable structures across multiple tasks with limited annotated data. The downstream multilingual applications may benefit from such a learning setup as most of the languages across the globe are low-resource and share some structures with other languages. In this paper, we propose a novel meta-learning framework (called Meta-X<tex-math>_{NLG}</tex-math>) to learn shareable structures from typologically diverse languages based on meta-learning and language clustering. This is a step towards uniform cross-lingual transfer for unseen languages. We first cluster the languages based on language representations and identify the centroid language of each cluster. Then, a meta-learning algorithm is trained with all centroid languages and evaluated on the other languages in the zero-shot setting. We demonstrate the effectiveness of this modeling on two NLG tasks (Abstractive Text Summarization and Question Generation), 5 popular datasets and 30 typologically diverse languages. Consistent improvements over strong baselines demonstrate the efficacy of the proposed framework. The careful design of the model makes this end-to-end NLG setup less vulnerable to the accidental translation problem, which is a prominent concern in zero-shot cross-lingual NLG tasks.</abstract>
      <url hash="d679e8fd">2022.findings-acl.24</url>
      <attachment type="software" hash="5d215e2b">2022.findings-acl.24.software.zip</attachment>
      <bibkey>maurya-desarkar-2022-meta</bibkey>
      <doi>10.18653/v1/2022.findings-acl.24</doi>
      <video href="2022.findings-acl.24.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/mlqa">MLQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tydi-qa">TyDiQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikilingua">WikiLingua</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xquad">XQuAD</pwcdataset>
    </paper>
    <paper id="25">
      <title><fixed-case>MR</fixed-case>-<fixed-case>P</fixed-case>: A Parallel Decoding Algorithm for Iterative Refinement Non-Autoregressive Translation</title>
      <author><first>Hao</first><last>Cheng</last></author>
      <author><first>Zhihua</first><last>Zhang</last></author>
      <pages>285-296</pages>
      <abstract>Non-autoregressive translation (NAT) predicts all the target tokens in parallel and significantly speeds up the inference process. The Conditional Masked Language Model (CMLM) is a strong baseline of NAT. It decodes with the Mask-Predict algorithm which iteratively refines the output. Most works about CMLM focus on the model structure and the training objective. However, the decoding algorithm is equally important. We propose a simple, effective, and easy-to-implement decoding algorithm that we call MaskRepeat-Predict (MR-P). The MR-P algorithm gives higher priority to consecutive repeated tokens when selecting tokens to mask for the next iteration and stops the iteration after target tokens converge. We conduct extensive experiments on six translation directions with varying data sizes. The results show that MR-P significantly improves the performance with the same model parameters. Specifically, we achieve a BLEU increase of 1.39 points in the WMT’14 En-De translation task.</abstract>
      <url hash="9668043b">2022.findings-acl.25</url>
      <bibkey>cheng-zhang-2022-mr</bibkey>
      <doi>10.18653/v1/2022.findings-acl.25</doi>
    </paper>
    <paper id="26">
      <title>Open Relation Modeling: Learning to Define Relations between Entities</title>
      <author><first>Jie</first><last>Huang</last></author>
      <author><first>Kevin</first><last>Chang</last></author>
      <author><first>Jinjun</first><last>Xiong</last></author>
      <author><first>Wen-mei</first><last>Hwu</last></author>
      <pages>297-308</pages>
      <abstract>Relations between entities can be represented by different instances, e.g., a sentence containing both entities or a fact in a Knowledge Graph (KG). However, these instances may not well capture the general relations between entities, may be difficult to understand by humans, even may not be found due to the incompleteness of the knowledge source. In this paper, we introduce the Open Relation Modeling problem - given two entities, generate a coherent sentence describing the relation between them. To solve this problem, we propose to teach machines to generate definition-like relation descriptions by letting them learn from defining entities. Specifically, we fine-tune Pre-trained Language Models (PLMs) to produce definitions conditioned on extracted entity pairs. To help PLMs reason between entities and provide additional relational knowledge to PLMs for open relation modeling, we incorporate reasoning paths in KGs and include a reasoning path selection mechanism. Experimental results show that our model can generate concise but informative relation descriptions that capture the representative characteristics of entities.</abstract>
      <url hash="dc90219e">2022.findings-acl.26</url>
      <attachment type="software" hash="0120116c">2022.findings-acl.26.software.zip</attachment>
      <bibkey>huang-etal-2022-open</bibkey>
      <doi>10.18653/v1/2022.findings-acl.26</doi>
      <video href="2022.findings-acl.26.mp4"/>
      <pwccode url="https://github.com/jeffhj/open-relation-modeling" additional="false">jeffhj/open-relation-modeling</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/open-relation-modeling">Open Relation Modeling</pwcdataset>
    </paper>
    <paper id="27">
      <title>A Slot Is Not Built in One Utterance: Spoken Language Dialogs with Sub-Slots</title>
      <author><first>Sai</first><last>Zhang</last></author>
      <author><first>Yuwei</first><last>Hu</last></author>
      <author><first>Yuchuan</first><last>Wu</last></author>
      <author><first>Jiaman</first><last>Wu</last></author>
      <author><first>Yongbin</first><last>Li</last></author>
      <author><first>Jian</first><last>Sun</last></author>
      <author><first>Caixia</first><last>Yuan</last></author>
      <author><first>Xiaojie</first><last>Wang</last></author>
      <pages>309-321</pages>
      <abstract>A slot value might be provided segment by segment over multiple-turn interactions in a dialog, especially for some important information such as phone numbers and names. It is a common phenomenon in daily life, but little attention has been paid to it in previous work. To fill the gap, this paper defines a new task named Sub-Slot based Task-Oriented Dialog (SSTOD) and builds a Chinese dialog dataset SSD for boosting research on SSTOD. The dataset includes a total of 40K dialogs and 500K utterances from four different domains: Chinese names, phone numbers, ID numbers and license plate numbers. The data is well annotated with sub-slot values, slot values, dialog states and actions. We find some new linguistic phenomena and interactive manners in SSTOD which raise critical challenges of building dialog agents for the task. We test three state-of-the-art dialog models on SSTOD and find they cannot handle the task well on any of the four domains. We also investigate an improved model by involving slot knowledge in a plug-in manner. More work should be done to meet the new challenges raised from SSTOD which widely exists in real-life applications. The dataset and code are publicly available via <url>https://github.com/shunjiu/SSTOD</url>.</abstract>
      <url hash="21611972">2022.findings-acl.27</url>
      <bibkey>zhang-etal-2022-slot</bibkey>
      <doi>10.18653/v1/2022.findings-acl.27</doi>
      <pwccode url="https://github.com/shunjiu/sstod" additional="false">shunjiu/sstod</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ssd-1">SSD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ssd-id">SSD_ID</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ssd-name">SSD_NAME</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ssd-phone">SSD_PHONE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ssd-plate">SSD_PLATE</pwcdataset>
    </paper>
    <paper id="28">
      <title>Towards Transparent Interactive Semantic Parsing via Step-by-Step Correction</title>
      <author><first>Lingbo</first><last>Mo</last></author>
      <author><first>Ashley</first><last>Lewis</last></author>
      <author><first>Huan</first><last>Sun</last></author>
      <author><first>Michael</first><last>White</last></author>
      <pages>322-342</pages>
      <abstract>Existing studies on semantic parsing focus on mapping a natural-language utterance to a logical form (LF) in one turn. However, because natural language may contain ambiguity and variability, this is a difficult challenge. In this work, we investigate an interactive semantic parsing framework that explains the predicted LF step by step in natural language and enables the user to make corrections through natural-language feedback for individual steps. We focus on question answering over knowledge bases (KBQA) as an instantiation of our framework, aiming to increase the transparency of the parsing process and help the user trust the final answer. We construct INSPIRED, a crowdsourced dialogue dataset derived from the ComplexWebQuestions dataset. Our experiments show that this framework has the potential to greatly improve overall parse accuracy. Furthermore, we develop a pipeline for dialogue simulation to evaluate our framework w.r.t. a variety of state-of-the-art KBQA models without further crowdsourcing effort. The results demonstrate that our framework promises to be effective across such models.</abstract>
      <url hash="e6aed9e5">2022.findings-acl.28</url>
      <bibkey>mo-etal-2022-towards</bibkey>
      <doi>10.18653/v1/2022.findings-acl.28</doi>
      <video href="2022.findings-acl.28.mp4"/>
      <pwccode url="https://github.com/molingbo/inspired" additional="false">molingbo/inspired</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/break">BREAK</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/gem">GEM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/splash">SPLASH</pwcdataset>
    </paper>
    <paper id="29">
      <title><fixed-case>MINER</fixed-case>: Multi-Interest Matching Network for News Recommendation</title>
      <author><first>Jian</first><last>Li</last></author>
      <author><first>Jieming</first><last>Zhu</last></author>
      <author><first>Qiwei</first><last>Bi</last></author>
      <author><first>Guohao</first><last>Cai</last></author>
      <author><first>Lifeng</first><last>Shang</last></author>
      <author><first>Zhenhua</first><last>Dong</last></author>
      <author><first>Xin</first><last>Jiang</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <pages>343-352</pages>
      <abstract>Personalized news recommendation is an essential technique to help users find interested news. Accurately matching user’s interests and candidate news is the key to news recommendation. Most existing methods learn a single user embedding from user’s historical behaviors to represent the reading interest. However, user interest is usually diverse and may not be adequately modeled by a single user embedding. In this paper, we propose a poly attention scheme to learn multiple interest vectors for each user, which encodes the different aspects of user interest. We further propose a disagreement regularization to make the learned interests vectors more diverse. Moreover, we design a category-aware attention weighting strategy that incorporates the news category information as explicit interest signals into the attention mechanism. Extensive experiments on the MIND news recommendation benchmark demonstrate that our approach significantly outperforms existing state-of-the-art methods.</abstract>
      <url hash="92b4b110">2022.findings-acl.29</url>
      <bibkey>li-etal-2022-miner</bibkey>
      <doi>10.18653/v1/2022.findings-acl.29</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/mind">MIND</pwcdataset>
    </paper>
    <paper id="30">
      <title><fixed-case>KSAM</fixed-case>: Infusing Multi-Source Knowledge into Dialogue Generation via Knowledge Source Aware Multi-Head Decoding</title>
      <author><first>Sixing</first><last>Wu</last></author>
      <author><first>Ying</first><last>Li</last></author>
      <author><first>Dawei</first><last>Zhang</last></author>
      <author><first>Zhonghai</first><last>Wu</last></author>
      <pages>353-363</pages>
      <abstract>Knowledge-enhanced methods have bridged the gap between human beings and machines in generating dialogue responses. However, most previous works solely seek knowledge from a single source, and thus they often fail to obtain available knowledge because of the insufficient coverage of a single knowledge source. To this end, infusing knowledge from multiple sources becomes a trend. This paper proposes a novel approach Knowledge Source Aware Multi-Head Decoding, KSAM, to infuse multi-source knowledge into dialogue generation more efficiently. Rather than following the traditional single decoder paradigm, KSAM uses multiple independent source-aware decoder heads to alleviate three challenging problems in infusing multi-source knowledge, namely, the diversity among different knowledge sources, the indefinite knowledge alignment issue, and the insufficient flexibility/scalability in knowledge usage. Experiments on a Chinese multi-source knowledge-aligned dataset demonstrate the superior performance of KSAM against various competitive approaches.</abstract>
      <url hash="1aca8e3e">2022.findings-acl.30</url>
      <bibkey>wu-etal-2022-ksam</bibkey>
      <doi>10.18653/v1/2022.findings-acl.30</doi>
    </paper>
    <paper id="31">
      <title>Towards Responsible Natural Language Annotation for the Varieties of <fixed-case>A</fixed-case>rabic</title>
      <author><first>A.</first><last>Bergman</last></author>
      <author><first>Mona</first><last>Diab</last></author>
      <pages>364-371</pages>
      <abstract>When building NLP models, there is a tendency to aim for broader coverage, often overlooking cultural and (socio)linguistic nuance. In this position paper, we make the case for care and attention to such nuances, particularly in dataset annotation, as well as the inclusion of cultural and linguistic expertise in the process. We present a playbook for responsible dataset creation for polyglossic, multidialectal languages. This work is informed by a study on Arabic annotation of social media content.</abstract>
      <url hash="f48868e9">2022.findings-acl.31</url>
      <bibkey>bergman-diab-2022-towards</bibkey>
      <doi>10.18653/v1/2022.findings-acl.31</doi>
      <video href="2022.findings-acl.31.mp4"/>
    </paper>
    <paper id="32">
      <title>Dynamically Refined Regularization for Improving Cross-corpora Hate Speech Detection</title>
      <author><first>Tulika</first><last>Bose</last></author>
      <author><first>Nikolaos</first><last>Aletras</last></author>
      <author><first>Irina</first><last>Illina</last></author>
      <author><first>Dominique</first><last>Fohr</last></author>
      <pages>372-382</pages>
      <abstract>Hate speech classifiers exhibit substantial performance degradation when evaluated on datasets different from the source. This is due to learning spurious correlations between words that are not necessarily relevant to hateful language, and hate speech labels from the training corpus. Previous work has attempted to mitigate this problem by regularizing specific terms from pre-defined static dictionaries. While this has been demonstrated to improve the generalizability of classifiers, the coverage of such methods is limited and the dictionaries require regular manual updates from human experts. In this paper, we propose to automatically identify and reduce spurious correlations using attribution methods with dynamic refinement of the list of terms that need to be regularized during training. Our approach is flexible and improves the cross-corpora performance over previous work independently and in combination with pre-defined dictionaries.</abstract>
      <url hash="c6b773c3">2022.findings-acl.32</url>
      <bibkey>bose-etal-2022-dynamically</bibkey>
      <doi>10.18653/v1/2022.findings-acl.32</doi>
      <video href="2022.findings-acl.32.mp4"/>
      <pwccode url="https://github.com/tbose20/d-ref" additional="false">tbose20/d-ref</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hateval">HatEval</pwcdataset>
    </paper>
    <paper id="33">
      <title>Towards Large-Scale Interpretable Knowledge Graph Reasoning for Dialogue Systems</title>
      <author><first>Yi-Lin</first><last>Tuan</last></author>
      <author><first>Sajjad</first><last>Beygi</last></author>
      <author><first>Maryam</first><last>Fazel-Zarandi</last></author>
      <author><first>Qiaozi</first><last>Gao</last></author>
      <author><first>Alessandra</first><last>Cervone</last></author>
      <author><first>William Yang</first><last>Wang</last></author>
      <pages>383-395</pages>
      <abstract>Users interacting with voice assistants today need to phrase their requests in a very specific manner to elicit an appropriate response. This limits the user experience, and is partly due to the lack of reasoning capabilities of dialogue platforms and the hand-crafted rules that require extensive labor. One possible solution to improve user experience and relieve the manual efforts of designers is to build an end-to-end dialogue system that can do reasoning itself while perceiving user’s utterances. In this work, we propose a novel method to incorporate the knowledge reasoning capability into dialog systems in a more scalable and generalizable manner. Our proposed method allows a single transformer model to directly walk on a large-scale knowledge graph to generate responses. To the best of our knowledge, this is the first work to have transformer models generate responses by reasoning over differentiable knowledge graphs. We investigate the reasoning abilities of the proposed method on both task-oriented and domain-specific chit-chat dialogues. Empirical results show that this method can effectively and efficiently incorporate a knowledge graph into a dialogue system with fully-interpretable reasoning paths.</abstract>
      <url hash="53c555c0">2022.findings-acl.33</url>
      <bibkey>tuan-etal-2022-towards</bibkey>
      <doi>10.18653/v1/2022.findings-acl.33</doi>
      <video href="2022.findings-acl.33.mp4"/>
      <pwccode url="https://github.com/pascalson/diffkg-dialog" additional="false">pascalson/diffkg-dialog</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/opendialkg">OpenDialKG</pwcdataset>
    </paper>
    <paper id="34">
      <title><fixed-case>MDER</fixed-case>ank: A Masked Document Embedding Rank Approach for Unsupervised Keyphrase Extraction</title>
      <author><first>Linhan</first><last>Zhang</last></author>
      <author><first>Qian</first><last>Chen</last></author>
      <author><first>Wen</first><last>Wang</last></author>
      <author><first>Chong</first><last>Deng</last></author>
      <author><first>ShiLiang</first><last>Zhang</last></author>
      <author><first>Bing</first><last>Li</last></author>
      <author><first>Wei</first><last>Wang</last></author>
      <author><first>Xin</first><last>Cao</last></author>
      <pages>396-409</pages>
      <abstract>Keyphrase extraction (KPE) automatically extracts phrases in a document that provide a concise summary of the core content, which benefits downstream information retrieval and NLP tasks. Previous state-of-the-art methods select candidate keyphrases based on the similarity between learned representations of the candidates and the document. They suffer performance degradation on long documents due to discrepancy between sequence lengths which causes mismatch between representations of keyphrase candidates and the document. In this work, we propose a novel unsupervised embedding-based KPE approach, Masked Document Embedding Rank (MDERank), to address this problem by leveraging a mask strategy and ranking candidates by the similarity between embeddings of the source document and the masked document. We further develop a KPE-oriented BERT (KPEBERT) model by proposing a novel self-supervised contrastive learning method, which is more compatible to MDERank than vanilla BERT. Comprehensive evaluations on six KPE benchmarks demonstrate that the proposed MDERank outperforms state-of-the-art unsupervised KPE approach by average 1.80 <tex-math>F1@15</tex-math> improvement. MDERank further benefits from KPEBERT and overall achieves average 3.53 <tex-math>F1@15</tex-math> improvement over SIFRank.</abstract>
      <url hash="dc9b7e27">2022.findings-acl.34</url>
      <bibkey>zhang-etal-2022-mderank</bibkey>
      <doi>10.18653/v1/2022.findings-acl.34</doi>
      <pwccode url="https://github.com/linhanz/mderank" additional="false">linhanz/mderank</pwccode>
    </paper>
    <paper id="35">
      <title>Visualizing the Relationship Between Encoded Linguistic Information and Task Performance</title>
      <author><first>Jiannan</first><last>Xiang</last></author>
      <author><first>Huayang</first><last>Li</last></author>
      <author><first>Defu</first><last>Lian</last></author>
      <author><first>Guoping</first><last>Huang</last></author>
      <author><first>Taro</first><last>Watanabe</last></author>
      <author><first>Lemao</first><last>Liu</last></author>
      <pages>410-422</pages>
      <abstract>Probing is popular to analyze whether linguistic information can be captured by a well-trained deep neural model, but it is hard to answer how the change of the encoded linguistic information will affect task performance. To this end, we study the dynamic relationship between the encoded linguistic information and task performance from the viewpoint of Pareto Optimality. Its key idea is to obtain a set of models which are Pareto-optimal in terms of both objectives. From this viewpoint, we propose a method to optimize the Pareto-optimal models by formalizing it as a multi-objective optimization problem. We conduct experiments on two popular NLP tasks, i.e., machine translation and language modeling, and investigate the relationship between several kinds of linguistic information and task performances. Experimental results demonstrate that the proposed method is better than a baseline method. Our empirical findings suggest that some syntactic information is helpful for NLP tasks whereas encoding more syntactic information does not necessarily lead to better performance, because the model architecture is also an important factor.</abstract>
      <url hash="42104ce7">2022.findings-acl.35</url>
      <bibkey>xiang-etal-2022-visualizing</bibkey>
      <doi>10.18653/v1/2022.findings-acl.35</doi>
    </paper>
    <paper id="36">
      <title>Efficient Argument Structure Extraction with Transfer Learning and Active Learning</title>
      <author><first>Xinyu</first><last>Hua</last></author>
      <author><first>Lu</first><last>Wang</last></author>
      <pages>423-437</pages>
      <abstract>The automation of extracting argument structures faces a pair of challenges on (1) encoding long-term contexts to facilitate comprehensive understanding, and (2) improving data efficiency since constructing high-quality argument structures is time-consuming. In this work, we propose a novel context-aware Transformer-based argument structure prediction model which, on five different domains, significantly outperforms models that rely on features or only encode limited contexts. To tackle the difficulty of data annotation, we examine two complementary methods: (i) transfer learning to leverage existing annotated data to boost model performance in a new target domain, and (ii) active learning to strategically identify a small amount of samples for annotation. We further propose model-independent sample acquisition strategies, which can be generalized to diverse domains. With extensive experiments, we show that our simple-yet-effective acquisition strategies yield competitive results against three strong comparisons. Combined with transfer learning, substantial F1 score boost (5-25) can be further achieved during the early iterations of active learning across domains.</abstract>
      <url hash="52042a9a">2022.findings-acl.36</url>
      <bibkey>hua-wang-2022-efficient</bibkey>
      <doi>10.18653/v1/2022.findings-acl.36</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/cdcp">CDCP</pwcdataset>
    </paper>
    <paper id="37">
      <title>Plug-and-Play Adaptation for Continuously-updated <fixed-case>QA</fixed-case></title>
      <author><first>Kyungjae</first><last>Lee</last></author>
      <author><first>Wookje</first><last>Han</last></author>
      <author><first>Seung-won</first><last>Hwang</last></author>
      <author><first>Hwaran</first><last>Lee</last></author>
      <author><first>Joonsuk</first><last>Park</last></author>
      <author><first>Sang-Woo</first><last>Lee</last></author>
      <pages>438-447</pages>
      <abstract>Language models (LMs) have shown great potential as implicit knowledge bases (KBs). And for their practical use, knowledge in LMs need to be updated periodically. However, existing tasks to assess LMs’ efficacy as KBs do not adequately consider multiple large-scale updates. To this end, we first propose a novel task—Continuously-updated QA (CuQA)—in which multiple large-scale updates are made to LMs, and the performance is measured with respect to the success in adding and updating knowledge while retaining existing knowledge. We then present LMs with plug-in modules that effectively handle the updates. Experiments conducted on zsRE QA and NQ datasets show that our method outperforms existing approaches. We find that our method is 4x more effective in terms of updates/forgets ratio, compared to a fine-tuning baseline.</abstract>
      <url hash="5d1d8fdd">2022.findings-acl.37</url>
      <attachment type="software" hash="06685060">2022.findings-acl.37.software.zip</attachment>
      <bibkey>lee-etal-2022-plug</bibkey>
      <doi>10.18653/v1/2022.findings-acl.37</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/situatedqa">SituatedQA</pwcdataset>
    </paper>
    <paper id="38">
      <title>Reinforced Cross-modal Alignment for Radiology Report Generation</title>
      <author><first>Han</first><last>Qin</last></author>
      <author><first>Yan</first><last>Song</last></author>
      <pages>448-458</pages>
      <abstract>Medical images are widely used in clinical decision-making, where writing radiology reports is a potential application that can be enhanced by automatic solutions to alleviate physicians’ workload. In general, radiology report generation is an image-text task, where cross-modal mappings between images and texts play an important role in generating high-quality reports. Although previous studies attempt to facilitate the alignment via the co-attention mechanism under supervised settings, they suffer from lacking valid and accurate correspondences due to no annotation of such alignment. In this paper, we propose an approach with reinforcement learning (RL) over a cross-modal memory (CMM) to better align visual and textual features for radiology report generation. In detail, a shared memory is used to record the mappings between visual and textual information, and the proposed reinforced algorithm is performed to learn the signal from the reports to guide the cross-modal alignment even though such reports are not directly related to how images and texts are mapped. Experimental results on two English radiology report datasets, i.e., IU X-Ray and MIMIC-CXR, show the effectiveness of our approach, where the state-of-the-art results are achieved. We further conduct human evaluation and case study which confirm the validity of the reinforced algorithm in our approach.</abstract>
      <url hash="96e63023">2022.findings-acl.38</url>
      <attachment type="software" hash="4730ab3c">2022.findings-acl.38.software.zip</attachment>
      <bibkey>qin-song-2022-reinforced</bibkey>
      <doi>10.18653/v1/2022.findings-acl.38</doi>
      <pwccode url="https://github.com/cuhksz-nlp/r2genrl" additional="false">cuhksz-nlp/r2genrl</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/chexpert">CheXpert</pwcdataset>
    </paper>
    <paper id="39">
      <title>What Works and Doesn’t Work, A Deep Decoder for Neural Machine Translation</title>
      <author><first>Zuchao</first><last>Li</last></author>
      <author><first>Yiran</first><last>Wang</last></author>
      <author><first>Masao</first><last>Utiyama</last></author>
      <author><first>Eiichiro</first><last>Sumita</last></author>
      <author><first>Hai</first><last>Zhao</last></author>
      <author><first>Taro</first><last>Watanabe</last></author>
      <pages>459-471</pages>
      <abstract>Deep learning has demonstrated performance advantages in a wide range of natural language processing tasks, including neural machine translation (NMT). Transformer NMT models are typically strengthened by deeper encoder layers, but deepening their decoder layers usually results in failure. In this paper, we first identify the cause of the failure of the deep decoder in the Transformer model. Inspired by this discovery, we then propose approaches to improving it, with respect to model structure and model training, to make the deep decoder practical in NMT. Specifically, with respect to model structure, we propose a cross-attention drop mechanism to allow the decoder layers to perform their own different roles, to reduce the difficulty of deep-decoder learning. For model training, we propose a collapse reducing training approach to improve the stability and effectiveness of deep-decoder training. We experimentally evaluated our proposed Transformer NMT model structure modification and novel training methods on several popular machine translation benchmarks. The results showed that deepening the NMT model by increasing the number of decoder layers successfully prevented the deepened decoder from degrading to an unconditional language model. In contrast to prior work on deepening an NMT model on the encoder, our method can deepen the model on both the encoder and decoder at the same time, resulting in a deeper model and improved performance.</abstract>
      <url hash="a6d228a1">2022.findings-acl.39</url>
      <bibkey>li-etal-2022-works</bibkey>
      <doi>10.18653/v1/2022.findings-acl.39</doi>
    </paper>
    <paper id="40">
      <title><fixed-case>S</fixed-case>y<fixed-case>MC</fixed-case>o<fixed-case>M</fixed-case> - Syntactic Measure of Code Mixing A Study Of <fixed-case>E</fixed-case>nglish-<fixed-case>H</fixed-case>indi Code-Mixing</title>
      <author><first>Prashant</first><last>Kodali</last></author>
      <author><first>Anmol</first><last>Goel</last></author>
      <author><first>Monojit</first><last>Choudhury</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <author><first>Ponnurangam</first><last>Kumaraguru</last></author>
      <pages>472-480</pages>
      <abstract>Code mixing is the linguistic phenomenon where bilingual speakers tend to switch between two or more languages in conversations. Recent work on code-mixing in computational settings has leveraged social media code mixed texts to train NLP models. For capturing the variety of code mixing in, and across corpus, Language ID (LID) tags based measures (CMI) have been proposed. Syntactical variety/patterns of code-mixing and their relationship vis-a-vis computational model’s performance is under explored. In this work, we investigate a collection of English(en)-Hindi(hi) code-mixed datasets from a syntactic lens to propose, <tex-math>SyMCoM</tex-math>, an indicator of syntactic variety in code-mixed text, with intuitive theoretical bounds. We train SoTA en-hi PoS tagger, accuracy of 93.4%, to reliably compute PoS tags on a corpus, and demonstrate the utility of <tex-math>SyMCoM</tex-math> by applying it on various syntactical categories on a collection of datasets, and compare datasets using the measure.</abstract>
      <url hash="11f49965">2022.findings-acl.40</url>
      <bibkey>kodali-etal-2022-symcom</bibkey>
      <doi>10.18653/v1/2022.findings-acl.40</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/lince">LinCE</pwcdataset>
    </paper>
    <paper id="41">
      <title><fixed-case>H</fixed-case>ybri<fixed-case>D</fixed-case>ialogue: An Information-Seeking Dialogue Dataset Grounded on Tabular and Textual Data</title>
      <author><first>Kai</first><last>Nakamura</last></author>
      <author><first>Sharon</first><last>Levy</last></author>
      <author><first>Yi-Lin</first><last>Tuan</last></author>
      <author><first>Wenhu</first><last>Chen</last></author>
      <author><first>William Yang</first><last>Wang</last></author>
      <pages>481-492</pages>
      <abstract>A pressing challenge in current dialogue systems is to successfully converse with users on topics with information distributed across different modalities. Previous work in multiturn dialogue systems has primarily focused on either text or table information. In more realistic scenarios, having a joint understanding of both is critical as knowledge is typically distributed over both unstructured and structured forms. We present a new dialogue dataset, HybriDialogue, which consists of crowdsourced natural conversations grounded on both Wikipedia text and tables. The conversations are created through the decomposition of complex multihop questions into simple, realistic multiturn dialogue interactions. We propose retrieval, system state tracking, and dialogue response generation tasks for our dataset and conduct baseline experiments for each. Our results show that there is still ample opportunity for improvement, demonstrating the importance of building stronger dialogue systems that can reason over the complex setting of informationseeking dialogue grounded on tables and text.</abstract>
      <url hash="0a2a1c44">2022.findings-acl.41</url>
      <bibkey>nakamura-etal-2022-hybridialogue</bibkey>
      <doi>10.18653/v1/2022.findings-acl.41</doi>
      <video href="2022.findings-acl.41.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/coqa">CoQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/doqa">DoQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hybridqa">HybridQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ott-qa">OTT-QA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/recipeqa">RecipeQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sqa">SQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sharc">ShARC</pwcdataset>
    </paper>
    <paper id="42">
      <title><fixed-case>NEWTS</fixed-case>: A Corpus for News Topic-Focused Summarization</title>
      <author><first>Seyed Ali</first><last>Bahrainian</last></author>
      <author><first>Sheridan</first><last>Feucht</last></author>
      <author><first>Carsten</first><last>Eickhoff</last></author>
      <pages>493-503</pages>
      <abstract>Text summarization models are approaching human levels of fidelity. Existing benchmarking corpora provide concordant pairs of full and abridged versions of Web, news or professional content. To date, all summarization datasets operate under a one-size-fits-all paradigm that may not reflect the full range of organic summarization needs. Several recently proposed models (e.g., plug and play language models) have the capacity to condition the generated summaries on a desired range of themes. These capacities remain largely unused and unevaluated as there is no dedicated dataset that would support the task of topic-focused summarization. This paper introduces the first topical summarization corpus NEWTS, based on the well-known CNN/Dailymail dataset, and annotated via online crowd-sourcing. Each source article is paired with two reference summaries, each focusing on a different theme of the source document. We evaluate a representative range of existing techniques and analyze the effectiveness of different prompting methods.</abstract>
      <url hash="b277e1fe">2022.findings-acl.42</url>
      <bibkey>bahrainian-etal-2022-newts</bibkey>
      <doi>10.18653/v1/2022.findings-acl.42</doi>
      <video href="2022.findings-acl.42.mp4"/>
    </paper>
    <paper id="43">
      <title>Classification without (Proper) Representation: Political Heterogeneity in Social Media and Its Implications for Classification and Behavioral Analysis</title>
      <author><first>Kenan</first><last>Alkiek</last></author>
      <author><first>Bohan</first><last>Zhang</last></author>
      <author><first>David</first><last>Jurgens</last></author>
      <pages>504-522</pages>
      <abstract>Reddit is home to a broad spectrum of political activity, and users signal their political affiliations in multiple ways—from self-declarations to community participation. Frequently, computational studies have treated political users as a single bloc, both in developing models to infer political leaning and in studying political behavior. Here, we test this assumption of political users and show that commonly-used political-inference models do not generalize, indicating heterogeneous types of political users. The models remain imprecise at best for most users, regardless of which sources of data or methods are used. Across a 14-year longitudinal analysis, we demonstrate that the choice in definition of a political user has significant implications for behavioral analysis. Controlling for multiple factors, political users are more toxic on the platform and inter-party interactions are even more toxic—but not all political users behave this way. Last, we identify a subset of political users who repeatedly flip affiliations, showing that these users are the most controversial of all, acting as provocateurs by more frequently bringing up politics, and are more likely to be banned, suspended, or deleted.</abstract>
      <url hash="4c1bc63e">2022.findings-acl.43</url>
      <bibkey>alkiek-etal-2022-classification</bibkey>
      <doi>10.18653/v1/2022.findings-acl.43</doi>
      <video href="2022.findings-acl.43.mp4"/>
    </paper>
    <paper id="44">
      <title>Toward More Meaningful Resources for Lower-resourced Languages</title>
      <author><first>Constantine</first><last>Lignos</last></author>
      <author><first>Nolan</first><last>Holley</last></author>
      <author><first>Chester</first><last>Palen-Michel</last></author>
      <author><first>Jonne</first><last>Sälevä</last></author>
      <pages>523-532</pages>
      <abstract>In this position paper, we describe our perspective on how meaningful resources for lower-resourced languages should be developed in connection with the speakers of those languages. Before advancing that position, we first examine two massively multilingual resources used in language technology development, identifying shortcomings that limit their usefulness. We explore the contents of the names stored in Wikidata for a few lower-resourced languages and find that many of them are not in fact in the languages they claim to be, requiring non-trivial effort to correct. We discuss quality issues present in WikiAnn and evaluate whether it is a useful supplement to hand-annotated data. We then discuss the importance of creating annotations for lower-resourced languages in a thoughtful and ethical way that includes the language speakers as part of the development process. We conclude with recommended guidelines for resource development.</abstract>
      <url hash="5339d65b">2022.findings-acl.44</url>
      <bibkey>lignos-etal-2022-toward</bibkey>
      <doi>10.18653/v1/2022.findings-acl.44</doi>
      <video href="2022.findings-acl.44.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/masakhaner">MasakhaNER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikiann-1">WikiANN</pwcdataset>
    </paper>
    <paper id="45">
      <title>Better Quality Estimation for Low Resource Corpus Mining</title>
      <author><first>Muhammed</first><last>Kocyigit</last></author>
      <author><first>Jiho</first><last>Lee</last></author>
      <author><first>Derry</first><last>Wijaya</last></author>
      <pages>533-543</pages>
      <abstract>Quality Estimation (QE) models have the potential to change how we evaluate and maybe even train machine translation models. However, these models still lack the robustness to achieve general adoption. We show that Stateof-the-art QE models, when tested in a Parallel Corpus Mining (PCM) setting, perform unexpectedly bad due to a lack of robustness to out-of-domain examples. We propose a combination of multitask training, data augmentation and contrastive learning to achieve better and more robust QE performance. We show that our method improves QE performance significantly in the MLQE challenge and the robustness of QE models when tested in the Parallel Corpus Mining setup. We increase the accuracy in PCM by more than 0.80, making it on par with state-of-the-art PCM methods that use millions of sentence pairs to train their models. In comparison, we use a thousand times less data, 7K parallel sentences in total, and propose a novel low resource PCM method.</abstract>
      <url hash="168b2e37">2022.findings-acl.45</url>
      <attachment type="software" hash="82443e78">2022.findings-acl.45.software.zip</attachment>
      <bibkey>kocyigit-etal-2022-better</bibkey>
      <doi>10.18653/v1/2022.findings-acl.45</doi>
      <video href="2022.findings-acl.45.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mlqe">MLQE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
    </paper>
    <paper id="46">
      <title>End-to-End Segmentation-based News Summarization</title>
      <author id="yang-liu-microsoft"><first>Yang</first><last>Liu</last></author>
      <author><first>Chenguang</first><last>Zhu</last></author>
      <author><first>Michael</first><last>Zeng</last></author>
      <pages>544-554</pages>
      <abstract>In this paper, we bring a new way of digesting news content by introducing the task of segmenting a news article into multiple sections and generating the corresponding summary to each section. We make two contributions towards this new task. First, we create and make available a dataset, SegNews, consisting of 27k news articles with sections and aligned heading-style section summaries. Second, we propose a novel segmentation-based language generation model adapted from pre-trained language models that can jointly segment a document and produce the summary for each section. Experimental results on SegNews demonstrate that our model can outperform several state-of-the-art sequence-to-sequence generation models for this new task.</abstract>
      <url hash="4749987b">2022.findings-acl.46</url>
      <bibkey>liu-etal-2022-end</bibkey>
      <doi>10.18653/v1/2022.findings-acl.46</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/new-york-times-annotated-corpus">New York Times Annotated Corpus</pwcdataset>
    </paper>
    <paper id="47">
      <title>Fast Nearest Neighbor Machine Translation</title>
      <author><first>Yuxian</first><last>Meng</last></author>
      <author><first>Xiaoya</first><last>Li</last></author>
      <author><first>Xiayu</first><last>Zheng</last></author>
      <author><first>Fei</first><last>Wu</last></author>
      <author><first>Xiaofei</first><last>Sun</last></author>
      <author><first>Tianwei</first><last>Zhang</last></author>
      <author><first>Jiwei</first><last>Li</last></author>
      <pages>555-565</pages>
      <abstract>Though nearest neighbor Machine Translation (<tex-math>k</tex-math>NN-MT) (CITATION) has proved to introduce significant performance boosts over standard neural MT systems, it is prohibitively slow since it uses the entire reference corpus as the datastore for the nearest neighbor search. This means each step for each beam in the beam search has to search over the entire reference corpus. <tex-math>k</tex-math>NN-MT is thus two-orders slower than vanilla MT models, making it hard to be applied to real-world applications, especially online services. In this work, we propose Fast <tex-math>k</tex-math>NN-MT to address this issue. Fast <tex-math>k</tex-math>NN-MT constructs a significantly smaller datastore for the nearest neighbor search: for each word in a source sentence, Fast <tex-math>k</tex-math>NN-MT first selects its nearest token-level neighbors, which is limited to tokens that are the same as the query token. Then at each decoding step, in contrast to using the entire corpus as the datastore, the search space is limited to target tokens corresponding to the previously selected reference source tokens. This strategy avoids search through the whole datastore for nearest neighbors and drastically improves decoding efficiency. Without loss of performance, Fast <tex-math>k</tex-math>NN-MT is two-orders faster than <tex-math>k</tex-math>NN-MT, and is only two times slower than the standard NMT model. Fast <tex-math>k</tex-math>NN-MT enables the practical use of <tex-math>k</tex-math>NN-MT systems in real-world MT applications. The code is available at <url>https://github.com/ShannonAI/fast-knn-nmt</url>.</abstract>
      <url hash="6757763e">2022.findings-acl.47</url>
      <bibkey>meng-etal-2022-fast</bibkey>
      <doi>10.18653/v1/2022.findings-acl.47</doi>
      <pwccode url="https://github.com/ShannonAI/fast-knn-nmt" additional="false">ShannonAI/fast-knn-nmt</pwccode>
    </paper>
    <paper id="48">
      <title>Extracting Latent Steering Vectors from Pretrained Language Models</title>
      <author><first>Nishant</first><last>Subramani</last></author>
      <author><first>Nivedita</first><last>Suresh</last></author>
      <author><first>Matthew</first><last>Peters</last></author>
      <pages>566-581</pages>
      <abstract>Prior work on controllable text generation has focused on learning how to control language models through trainable decoding, smart-prompt design, or fine-tuning based on a desired objective. We hypothesize that the information needed to steer the model to generate a target sentence is already encoded within the model. Accordingly, we explore a different approach altogether: extracting latent vectors directly from pretrained language model decoders without fine-tuning. Experiments show that there exist steering vectors, which, when added to the hidden states of the language model, generate a target sentence nearly perfectly (&gt; 99 BLEU) for English sentences from a variety of domains. We show that vector arithmetic can be used for unsupervised sentiment transfer on the Yelp sentiment benchmark, with performance comparable to models tailored to this task. We find that distances between steering vectors reflect sentence similarity when evaluated on a textual similarity benchmark (STS-B), outperforming pooled hidden states of models. Finally, we present an analysis of the intrinsic properties of the steering vectors. Taken together, our results suggest that frozen LMs can be effectively controlled through their latent steering space.</abstract>
      <url hash="4d0c3e8a">2022.findings-acl.48</url>
      <bibkey>subramani-etal-2022-extracting</bibkey>
      <doi>10.18653/v1/2022.findings-acl.48</doi>
      <video href="2022.findings-acl.48.mp4"/>
      <pwccode url="https://github.com/nishantsubramani/steering_vectors" additional="false">nishantsubramani/steering_vectors</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/styleptb">StylePTB</pwcdataset>
    </paper>
    <paper id="49">
      <title>Domain Generalisation of <fixed-case>NMT</fixed-case>: Fusing Adapters with Leave-One-Domain-Out Training</title>
      <author><first>Thuy-Trang</first><last>Vu</last></author>
      <author><first>Shahram</first><last>Khadivi</last></author>
      <author><first>Dinh</first><last>Phung</last></author>
      <author><first>Gholamreza</first><last>Haffari</last></author>
      <pages>582-588</pages>
      <abstract>Generalising to unseen domains is under-explored and remains a challenge in neural machine translation. Inspired by recent research in parameter-efficient transfer learning from pretrained models, this paper proposes a fusion-based generalisation method that learns to combine domain-specific parameters. We propose a leave-one-domain-out training strategy to avoid information leaking to address the challenge of not knowing the test domain during training time. Empirical results on three language pairs show that our proposed fusion method outperforms other baselines up to +0.8 BLEU score on average.</abstract>
      <url hash="af446f61">2022.findings-acl.49</url>
      <bibkey>vu-etal-2022-domain</bibkey>
      <doi>10.18653/v1/2022.findings-acl.49</doi>
      <video href="2022.findings-acl.49.mp4"/>
      <pwccode url="https://github.com/trangvu/lodo-nmt" additional="false">trangvu/lodo-nmt</pwccode>
    </paper>
    <paper id="50">
      <title>Reframing Instructional Prompts to <fixed-case>GPT</fixed-case>k’s Language</title>
      <author><first>Swaroop</first><last>Mishra</last></author>
      <author><first>Daniel</first><last>Khashabi</last></author>
      <author><first>Chitta</first><last>Baral</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <author><first>Hannaneh</first><last>Hajishirzi</last></author>
      <pages>589-612</pages>
      <abstract>What kinds of instructional prompts are easier to follow for Language Models (LMs)? We study this question by conducting extensive empirical analysis that shed light on important features of successful instructional prompts. Specifically, we study several classes of reframing techniques for manual reformulation of prompts into more effective ones. Some examples include decomposing a complex task instruction into multiple simpler tasks or itemizing instructions into sequential steps. Our experiments compare the zero-shot and few-shot performance of LMs prompted with reframed instructions on 12 NLP tasks across 6 categories. Compared with original instructions, our reframed instructions lead to significant improvements across LMs with different sizes. For example, the same reframed prompts boost few-shot performance of GPT3-series and GPT2-series by 12.5% and 6.7% respectively averaged over all tasks. Furthermore, reframed instructions reduce the number of examples required to prompt LMs in the few-shot setting. We hope these empirically-driven techniques will pave the way towards more effective future prompting algorithms.</abstract>
      <url hash="d7d6729f">2022.findings-acl.50</url>
      <bibkey>mishra-etal-2022-reframing</bibkey>
      <doi>10.18653/v1/2022.findings-acl.50</doi>
      <video href="2022.findings-acl.50.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/cosmosqa">CosmosQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mc-taco">MC-TACO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qasc">QASC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/winogrande">WinoGrande</pwcdataset>
    </paper>
    <paper id="51">
      <title>Read Top News First: A Document Reordering Approach for Multi-Document News Summarization</title>
      <author><first>Chao</first><last>Zhao</last></author>
      <author><first>Tenghao</first><last>Huang</last></author>
      <author><first>Somnath</first><last>Basu Roy Chowdhury</last></author>
      <author><first>Muthu Kumar</first><last>Chandrasekaran</last></author>
      <author><first>Kathleen</first><last>McKeown</last></author>
      <author><first>Snigdha</first><last>Chaturvedi</last></author>
      <pages>613-621</pages>
      <abstract>A common method for extractive multi-document news summarization is to re-formulate it as a single-document summarization problem by concatenating all documents as a single meta-document. However, this method neglects the relative importance of documents. We propose a simple approach to reorder the documents according to their relative importance before concatenating and summarizing them. The reordering makes the salient content easier to learn by the summarization model. Experiments show that our approach outperforms previous state-of-the-art methods with more complex architectures.</abstract>
      <url hash="5491b6fd">2022.findings-acl.51</url>
      <bibkey>zhao-etal-2022-read</bibkey>
      <doi>10.18653/v1/2022.findings-acl.51</doi>
      <pwccode url="https://github.com/zhaochaocs/mds-dr" additional="false">zhaochaocs/mds-dr</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multi-news">Multi-News</pwcdataset>
    </paper>
    <paper id="52">
      <title>Human Language Modeling</title>
      <author><first>Nikita</first><last>Soni</last></author>
      <author><first>Matthew</first><last>Matero</last></author>
      <author><first>Niranjan</first><last>Balasubramanian</last></author>
      <author><first>H. Andrew</first><last>Schwartz</last></author>
      <pages>622-636</pages>
      <abstract>Natural language is generated by people, yet traditional language modeling views words or documents as if generated independently. Here, we propose human language modeling (HuLM), a hierarchical extension to the language modeling problem where by a human- level exists to connect sequences of documents (e.g. social media messages) and capture the notion that human language is moderated by changing human states. We introduce, HaRT, a large-scale transformer model for solving HuLM, pre-trained on approximately 100,000 social media users, and demonstrate it’s effectiveness in terms of both language modeling (perplexity) for social media and fine-tuning for 4 downstream tasks spanning document- and user-levels. Results on all tasks meet or surpass the current state-of-the-art.</abstract>
      <url hash="7ec3f959">2022.findings-acl.52</url>
      <bibkey>soni-etal-2022-human</bibkey>
      <doi>10.18653/v1/2022.findings-acl.52</doi>
      <video href="2022.findings-acl.52.mp4"/>
      <pwccode url="https://github.com/humanlab/hart" additional="false">humanlab/hart</pwccode>
    </paper>
    <paper id="53">
      <title>Inverse is Better! Fast and Accurate Prompt for Few-shot Slot Tagging</title>
      <author><first>Yutai</first><last>Hou</last></author>
      <author><first>Cheng</first><last>Chen</last></author>
      <author><first>Xianzhen</first><last>Luo</last></author>
      <author><first>Bohan</first><last>Li</last></author>
      <author><first>Wanxiang</first><last>Che</last></author>
      <pages>637-647</pages>
      <abstract>Prompting methods recently achieve impressive success in few-shot learning. These methods modify input samples with prompt sentence pieces, and decode label tokens to map samples to corresponding labels. However, such a paradigm is very inefficient for the task of slot tagging. Since slot tagging samples are multiple consecutive words in a sentence, the prompting methods have to enumerate all n-grams token spans to find all the possible slots, which greatly slows down the prediction. To tackle this, we introduce an inverse paradigm for prompting. Different from the classic prompts mapping tokens to labels, we reversely predict slot values given slot types. Such inverse prompting only requires a one-turn prediction for each slot type and greatly speeds up the prediction. Besides, we propose a novel Iterative Prediction Strategy, from which the model learns to refine predictions by considering the relations between different slot types. We find, somewhat surprisingly, the proposed method not only predicts faster but also significantly improves the effect (improve over 6.1 F1-scores on 10-shot setting) and achieves new state-of-the-art performance.</abstract>
      <url hash="bd9d02cc">2022.findings-acl.53</url>
      <bibkey>hou-etal-2022-inverse</bibkey>
      <doi>10.18653/v1/2022.findings-acl.53</doi>
      <pwccode url="https://github.com/atmahou/promptslottagging" additional="false">atmahou/promptslottagging</pwccode>
    </paper>
    <paper id="54">
      <title>Cross-Modal Cloze Task: A New Task to Brain-to-Word Decoding</title>
      <author><first>Shuxian</first><last>Zou</last></author>
      <author><first>Shaonan</first><last>Wang</last></author>
      <author><first>Jiajun</first><last>Zhang</last></author>
      <author><first>Chengqing</first><last>Zong</last></author>
      <pages>648-657</pages>
      <abstract>Decoding language from non-invasive brain activity has attracted increasing attention from both researchers in neuroscience and natural language processing. Due to the noisy nature of brain recordings, existing work has simplified brain-to-word decoding as a binary classification task which is to discriminate a brain signal between its corresponding word and a wrong one. This pairwise classification task, however, cannot promote the development of practical neural decoders for two reasons. First, it has to enumerate all pairwise combinations in the test set, so it is inefficient to predict a word in a large vocabulary. Second, a perfect pairwise decoder cannot guarantee the performance on direct classification. To overcome these and go a step further to a realistic neural decoder, we propose a novel Cross-Modal Cloze (CMC) task which is to predict the target word encoded in the neural image with a context as prompt. Furthermore, to address this task, we propose a general approach that leverages the pre-trained language model to predict the target word. To validate our method, we perform experiments on more than 20 participants from two brain imaging datasets. Our method achieves 28.91% top-1 accuracy and 54.19% top-5 accuracy on average across all participants, significantly outperforming several baselines. This result indicates that our model can serve as a state-of-the-art baseline for the CMC task. More importantly, it demonstrates that it is feasible to decode a certain word within a large vocabulary from its neural brain activity.</abstract>
      <url hash="3ea873e7">2022.findings-acl.54</url>
      <bibkey>zou-etal-2022-cross</bibkey>
      <doi>10.18653/v1/2022.findings-acl.54</doi>
      <pwccode url="https://github.com/littletreezou/cross-modal-cloze-task" additional="false">littletreezou/cross-modal-cloze-task</pwccode>
    </paper>
    <paper id="55">
      <title>Mitigating Gender Bias in Distilled Language Models via Counterfactual Role Reversal</title>
      <author><first>Umang</first><last>Gupta</last></author>
      <author><first>Jwala</first><last>Dhamala</last></author>
      <author><first>Varun</first><last>Kumar</last></author>
      <author><first>Apurv</first><last>Verma</last></author>
      <author><first>Yada</first><last>Pruksachatkun</last></author>
      <author><first>Satyapriya</first><last>Krishna</last></author>
      <author><first>Rahul</first><last>Gupta</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <author><first>Greg</first><last>Ver Steeg</last></author>
      <author><first>Aram</first><last>Galstyan</last></author>
      <pages>658-678</pages>
      <abstract>Language models excel at generating coherent text, and model compression techniques such as knowledge distillation have enabled their use in resource-constrained settings. However, these models can be biased in multiple ways, including the unfounded association of male and female genders with gender-neutral professions. Therefore, knowledge distillation without any fairness constraints may preserve or exaggerate the teacher model’s biases onto the distilled model. To this end, we present a novel approach to mitigate gender disparity in text generation by learning a fair model during knowledge distillation. We propose two modifications to the base knowledge distillation based on counterfactual role reversal—modifying teacher probabilities and augmenting the training set. We evaluate gender polarity across professions in open-ended text generated from the resulting distilled and finetuned GPT–2 models and demonstrate a substantial reduction in gender disparity with only a minor compromise in utility. Finally, we observe that language models that reduce gender polarity in language generation do not improve embedding fairness or downstream classification fairness.</abstract>
      <url hash="2406173b">2022.findings-acl.55</url>
      <bibkey>gupta-etal-2022-mitigating</bibkey>
      <doi>10.18653/v1/2022.findings-acl.55</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/webtext">WebText</pwcdataset>
    </paper>
    <paper id="56">
      <title>Domain Representative Keywords Selection: A Probabilistic Approach</title>
      <author><first>Pritom Saha</first><last>Akash</last></author>
      <author><first>Jie</first><last>Huang</last></author>
      <author><first>Kevin</first><last>Chang</last></author>
      <author><first>Yunyao</first><last>Li</last></author>
      <author><first>Lucian</first><last>Popa</last></author>
      <author><first>ChengXiang</first><last>Zhai</last></author>
      <pages>679-692</pages>
      <abstract>We propose a probabilistic approach to select a subset of a <i>target domain representative keywords</i> from a candidate set, contrasting with a context domain. Such a task is crucial for many downstream tasks in natural language processing. To contrast the target domain and the context domain, we adapt the <i>two-component mixture model</i> concept to generate a distribution of candidate keywords. It provides more importance to the <i>distinctive</i> keywords of the target domain than common keywords contrasting with the context domain. To support the <i>representativeness</i> of the selected keywords towards the target domain, we introduce an <i>optimization algorithm</i> for selecting the subset from the generated candidate distribution. We have shown that the optimization algorithm can be efficiently implemented with a near-optimal approximation guarantee. Finally, extensive experiments on multiple domains demonstrate the superiority of our approach over other baselines for the tasks of keyword summary generation and trending keywords selection.</abstract>
      <url hash="d905da42">2022.findings-acl.56</url>
      <bibkey>akash-etal-2022-domain</bibkey>
      <doi>10.18653/v1/2022.findings-acl.56</doi>
      <pwccode url="https://github.com/pritomsaha/keyword-selection" additional="false">pritomsaha/keyword-selection</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/aminer">AMiner</pwcdataset>
    </paper>
    <paper id="57">
      <title>Hierarchical Inductive Transfer for Continual Dialogue Learning</title>
      <author><first>Shaoxiong</first><last>Feng</last></author>
      <author><first>Xuancheng</first><last>Ren</last></author>
      <author><first>Kan</first><last>Li</last></author>
      <author><first>Xu</first><last>Sun</last></author>
      <pages>693-699</pages>
      <abstract>Pre-trained models have achieved excellent performance on the dialogue task. However, for the continual increase of online chit-chat scenarios, directly fine-tuning these models for each of the new tasks not only explodes the capacity of the dialogue system on the embedded devices but also causes knowledge forgetting on pre-trained models and knowledge interference among diverse dialogue tasks. In this work, we propose a hierarchical inductive transfer framework to learn and deploy the dialogue skills continually and efficiently. First, we introduce the adapter module into pre-trained models for learning new dialogue tasks. As the only trainable module, it is beneficial for the dialogue system on the embedded devices to acquire new dialogue skills with negligible additional parameters. Then, for alleviating knowledge interference between tasks yet benefiting the regularization between them, we further design hierarchical inductive transfer that enables new tasks to use general knowledge in the base adapter without being misled by diverse knowledge in task-specific adapters. Empirical evaluation and analysis indicate that our framework obtains comparable performance under deployment-friendly model capacity.</abstract>
      <url hash="d38f5210">2022.findings-acl.57</url>
      <bibkey>feng-etal-2022-hierarchical</bibkey>
      <doi>10.18653/v1/2022.findings-acl.57</doi>
    </paper>
    <paper id="58">
      <title>Why Exposure Bias Matters: An Imitation Learning Perspective of Error Accumulation in Language Generation</title>
      <author><first>Kushal</first><last>Arora</last></author>
      <author><first>Layla</first><last>El Asri</last></author>
      <author><first>Hareesh</first><last>Bahuleyan</last></author>
      <author><first>Jackie</first><last>Cheung</last></author>
      <pages>700-710</pages>
      <abstract>Current language generation models suffer from issues such as repetition, incoherence, and hallucinations. An often-repeated hypothesis for this brittleness of generation models is that it is caused by the training and the generation procedure mismatch, also referred to as exposure bias. In this paper, we verify this hypothesis by analyzing exposure bias from an imitation learning perspective. We show that exposure bias leads to an accumulation of errors during generation, analyze why perplexity fails to capture this accumulation of errors, and empirically show that this accumulation results in poor generation quality.</abstract>
      <url hash="c1bd655f">2022.findings-acl.58</url>
      <bibkey>arora-etal-2022-exposure</bibkey>
      <doi>10.18653/v1/2022.findings-acl.58</doi>
      <video href="2022.findings-acl.58.mp4"/>
      <pwccode url="https://github.com/kushalarora/quantifying_exposure_bias" additional="false">kushalarora/quantifying_exposure_bias</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-2">WikiText-2</pwcdataset>
    </paper>
    <paper id="59">
      <title>Question Answering Infused Pre-training of General-Purpose Contextualized Representations</title>
      <author><first>Robin</first><last>Jia</last></author>
      <author><first>Mike</first><last>Lewis</last></author>
      <author><first>Luke</first><last>Zettlemoyer</last></author>
      <pages>711-728</pages>
      <abstract>We propose a pre-training objective based on question answering (QA) for learning general-purpose contextual representations, motivated by the intuition that the representation of a phrase in a passage should encode all questions that the phrase can answer in context. To this end, we train a bi-encoder QA model, which independently encodes passages and questions, to match the predictions of a more accurate cross-encoder model on 80 million synthesized QA pairs. By encoding QA-relevant information, the bi-encoder’s token-level representations are useful for non-QA downstream tasks without extensive (or in some cases, any) fine-tuning. We show large improvements over both RoBERTa-large and previous state-of-the-art results on zero-shot and few-shot paraphrase detection on four datasets, few-shot named entity recognition on two datasets, and zero-shot sentiment analysis on three datasets.</abstract>
      <url hash="37aba859">2022.findings-acl.59</url>
      <bibkey>jia-etal-2022-question</bibkey>
      <doi>10.18653/v1/2022.findings-acl.59</doi>
      <pwccode url="https://github.com/facebookresearch/quip" additional="false">facebookresearch/quip</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrqa-2019">MRQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsqa">NewsQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paws">PAWS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst-2">SST-2</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/searchqa">SearchQA</pwcdataset>
    </paper>
    <paper id="60">
      <title>Automatic Song Translation for Tonal Languages</title>
      <author><first>Fenfei</first><last>Guo</last></author>
      <author><first>Chen</first><last>Zhang</last></author>
      <author><first>Zhirui</first><last>Zhang</last></author>
      <author><first>Qixin</first><last>He</last></author>
      <author><first>Kejun</first><last>Zhang</last></author>
      <author><first>Jun</first><last>Xie</last></author>
      <author><first>Jordan</first><last>Boyd-Graber</last></author>
      <pages>729-743</pages>
      <abstract>This paper develops automatic song translation (AST) for tonal languages and addresses the unique challenge of aligning words’ tones with melody of a song in addition to conveying the original meaning. We propose three criteria for effective AST—preserving meaning, singability and intelligibility—and design metrics for these criteria. We develop a new benchmark for English–Mandarin song translation and develop an unsupervised AST system, Guided AliGnment for Automatic Song Translation (GagaST), which combines pre-training with three decoding constraints. Both automatic and human evaluations show GagaST successfully balances semantics and singability.</abstract>
      <url hash="84bff581">2022.findings-acl.60</url>
      <bibkey>guo-etal-2022-automatic</bibkey>
      <doi>10.18653/v1/2022.findings-acl.60</doi>
      <video href="2022.findings-acl.60.mp4"/>
    </paper>
    <paper id="61">
      <title>Read before Generate! Faithful Long Form Question Answering with Machine Reading</title>
      <author><first>Dan</first><last>Su</last></author>
      <author><first>Xiaoguang</first><last>Li</last></author>
      <author><first>Jindi</first><last>Zhang</last></author>
      <author><first>Lifeng</first><last>Shang</last></author>
      <author><first>Xin</first><last>Jiang</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <pages>744-756</pages>
      <abstract>Long-form question answering (LFQA) aims to generate a paragraph-length answer for a given question. While current work on LFQA using large pre-trained model for generation are effective at producing fluent and somewhat relevant content, one primary challenge lies in how to generate a faithful answer that has less hallucinated content. We propose a new end-to-end framework that jointly models answer generation and machine reading. The key idea is to augment the generation model with fine-grained, answer-related salient information which can be viewed as an emphasis on faithful facts. State-of-the-art results on two LFQA datasets, ELI5 and MS MARCO, demonstrate the effectiveness of our method, in comparison with strong baselines on automatic and human evaluation metrics. A detailed analysis further proves the competency of our methods in generating fluent, relevant, and more faithful answers.</abstract>
      <url hash="7ed815bf">2022.findings-acl.61</url>
      <bibkey>su-etal-2022-read</bibkey>
      <doi>10.18653/v1/2022.findings-acl.61</doi>
      <video href="2022.findings-acl.61.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/eli5">ELI5</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/kilt">KILT</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
    </paper>
    <paper id="62">
      <title>A Simple yet Effective Relation Information Guided Approach for Few-Shot Relation Extraction</title>
      <author id="yang-liu-hk"><first>Yang</first><last>Liu</last></author>
      <author><first>Jinpeng</first><last>Hu</last></author>
      <author><first>Xiang</first><last>Wan</last></author>
      <author><first>Tsung-Hui</first><last>Chang</last></author>
      <pages>757-763</pages>
      <abstract>Few-Shot Relation Extraction aims at predicting the relation for a pair of entities in a sentence by training with a few labelled examples in each relation. Some recent works have introduced relation information (i.e., relation labels or descriptions) to assist model learning based on Prototype Network. However, most of them constrain the prototypes of each relation class implicitly with relation information, generally through designing complex network structures, like generating hybrid features, combining with contrastive learning or attention networks. We argue that relation information can be introduced more explicitly and effectively into the model. Thus, this paper proposes a direct addition approach to introduce relation information. Specifically, for each relation class, the relation representation is first generated by concatenating two views of relations (i.e., [CLS] token embedding and the mean value of embeddings of all tokens) and then directly added to the original prototype for both train and prediction. Experimental results on the benchmark dataset FewRel 1.0 show significant improvements and achieve comparable results to the state-of-the-art, which demonstrates the effectiveness of our proposed approach. Besides, further analyses verify that the direct addition is a much more effective way to integrate the relation representations and the original prototypes.</abstract>
      <url hash="ab7978e8">2022.findings-acl.62</url>
      <attachment type="software" hash="0e1fede5">2022.findings-acl.62.software.zip</attachment>
      <bibkey>liu-etal-2022-simple</bibkey>
      <doi>10.18653/v1/2022.findings-acl.62</doi>
      <pwccode url="https://github.com/lylylylylyly/simplefsre" additional="false">lylylylylyly/simplefsre</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel">FewRel</pwcdataset>
    </paper>
    <paper id="63">
      <title><fixed-case>MIMIC</fixed-case>ause: <fixed-case>R</fixed-case>epresentation and automatic extraction of causal relation types from clinical notes</title>
      <author><first>Vivek</first><last>Khetan</last></author>
      <author><first>Md Imbesat</first><last>Rizvi</last></author>
      <author><first>Jessica</first><last>Huber</last></author>
      <author><first>Paige</first><last>Bartusiak</last></author>
      <author><first>Bogdan</first><last>Sacaleanu</last></author>
      <author><first>Andrew</first><last>Fano</last></author>
      <pages>764-773</pages>
      <abstract>Understanding causal narratives communicated in clinical notes can help make strides towards personalized healthcare. Extracted causal information from clinical notes can be combined with structured EHR data such as patients’ demographics, diagnoses, and medications. This will enhance healthcare providers’ ability to identify aspects of a patient’s story communicated in the clinical notes and help make more informed decisions. In this work, we propose annotation guidelines, develop an annotated corpus and provide baseline scores to identify types and direction of causal relations between a pair of biomedical concepts in clinical notes; communicated implicitly or explicitly, identified either in a single sentence or across multiple sentences. We annotate a total of 2714 de-identified examples sampled from the 2018 n2c2 shared task dataset and train four different language model based architectures. Annotation based on our guidelines achieved a high inter-annotator agreement i.e. Fleiss’ kappa (<tex-math>\kappa</tex-math>) score of 0.72, and our model for identification of causal relations achieved a macro F1 score of 0.56 on the test data. The high inter-annotator agreement for clinical text shows the quality of our annotation guidelines while the provided baseline F1 score sets the direction for future research towards understanding narratives in clinical texts.</abstract>
      <url hash="36479e9c">2022.findings-acl.63</url>
      <bibkey>khetan-etal-2022-mimicause</bibkey>
      <doi>10.18653/v1/2022.findings-acl.63</doi>
      <video href="2022.findings-acl.63.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/mimic-iii">MIMIC-III</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/rocstories">ROCStories</pwcdataset>
    </paper>
    <paper id="64">
      <title>Compressing Sentence Representation for Semantic Retrieval via Homomorphic Projective Distillation</title>
      <author><first>Xuandong</first><last>Zhao</last></author>
      <author><first>Zhiguo</first><last>Yu</last></author>
      <author><first>Ming</first><last>Wu</last></author>
      <author><first>Lei</first><last>Li</last></author>
      <pages>774-781</pages>
      <abstract>How to learn highly compact yet effective sentence representation? Pre-trained language models have been effective in many NLP tasks. However, these models are often huge and produce large sentence embeddings. Moreover, there is a big performance gap between large and small models. In this paper, we propose Homomorphic Projective Distillation (HPD) to learn compressed sentence embeddings. Our method augments a small Transformer encoder model with learnable projection layers to produce compact representations while mimicking a large pre-trained language model to retain the sentence representation quality. We evaluate our method with different model sizes on both semantic textual similarity (STS) and semantic retrieval (SR) tasks. Experiments show that our method achieves 2.7-4.5 points performance gain on STS tasks compared with previous best representations of the same size. In SR tasks, our method improves retrieval speed (8.2×) and memory usage (8.0×) compared with state-of-the-art large models. Our implementation is available at <url>https://github.com/XuandongZhao/HPD</url>.</abstract>
      <url hash="3c74a3f8">2022.findings-acl.64</url>
      <bibkey>zhao-etal-2022-compressing</bibkey>
      <doi>10.18653/v1/2022.findings-acl.64</doi>
      <pwccode url="https://github.com/xuandongzhao/hpd" additional="false">xuandongzhao/hpd</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sick">SICK</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="65">
      <title>Debiasing Event Understanding for Visual Commonsense Tasks</title>
      <author><first>Minji</first><last>Seo</last></author>
      <author><first>YeonJoon</first><last>Jung</last></author>
      <author><first>Seungtaek</first><last>Choi</last></author>
      <author><first>Seung-won</first><last>Hwang</last></author>
      <author><first>Bei</first><last>Liu</last></author>
      <pages>782-787</pages>
      <abstract>We study event understanding as a critical step towards visual commonsense tasks. Meanwhile, we argue that current object-based event understanding is purely likelihood-based, leading to incorrect event prediction, due to biased correlation between events and objects. We propose to mitigate such biases with <tex-math>do</tex-math>-calculus, proposed in causality research, but overcoming its limited robustness, by an optimized aggregation with association-based prediction.We show the effectiveness of our approach, intrinsically by comparing our generated events with ground-truth event annotation, and extrinsically by downstream commonsense tasks.</abstract>
      <url hash="5e4bf4fd">2022.findings-acl.65</url>
      <attachment type="software" hash="c4f67a1e">2022.findings-acl.65.software.zip</attachment>
      <bibkey>seo-etal-2022-debiasing</bibkey>
      <doi>10.18653/v1/2022.findings-acl.65</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/vcr">VCR</pwcdataset>
    </paper>
    <paper id="66">
      <title>Fact-Tree Reasoning for N-ary Question Answering over Knowledge Graphs</title>
      <author><first>Yao</first><last>Zhang</last></author>
      <author><first>Peiyao</first><last>Li</last></author>
      <author><first>Hongru</first><last>Liang</last></author>
      <author><first>Adam</first><last>Jatowt</last></author>
      <author><first>Zhenglu</first><last>Yang</last></author>
      <pages>788-802</pages>
      <abstract>Current Question Answering over Knowledge Graphs (KGQA) task mainly focuses on performing answer reasoning upon KGs with binary facts. However, it neglects the n-ary facts, which contain more than two entities. In this work, we highlight a more challenging but under-explored task: n-ary KGQA, i.e., answering n-ary facts questions upon n-ary KGs. Nevertheless, the multi-hop reasoning framework popular in binary KGQA task is not directly applicable on n-ary KGQA. We propose two feasible improvements: 1) upgrade the basic reasoning unit from entity or relation to fact, and 2) upgrade the reasoning structure from chain to tree. Therefore, we propose a novel fact-tree reasoning framework, FacTree, which integrates the above two upgrades. FacTree transforms the question into a fact tree and performs iterative fact reasoning on the fact tree to infer the correct answer. Experimental results on the n-ary KGQA dataset we constructed and two binary KGQA benchmarks demonstrate the effectiveness of FacTree compared with state-of-the-art methods.</abstract>
      <url hash="248c692c">2022.findings-acl.66</url>
      <bibkey>zhang-etal-2022-fact</bibkey>
      <doi>10.18653/v1/2022.findings-acl.66</doi>
      <video href="2022.findings-acl.66.mp4"/>
    </paper>
    <paper id="67">
      <title><fixed-case>D</fixed-case>eep<fixed-case>S</fixed-case>truct: Pretraining of Language Models for Structure Prediction</title>
      <author><first>Chenguang</first><last>Wang</last></author>
      <author><first>Xiao</first><last>Liu</last></author>
      <author><first>Zui</first><last>Chen</last></author>
      <author><first>Haoyun</first><last>Hong</last></author>
      <author><first>Jie</first><last>Tang</last></author>
      <author><first>Dawn</first><last>Song</last></author>
      <pages>803-823</pages>
      <abstract>We introduce a method for improving the structural understanding abilities of language models. Unlike previous approaches that finetune the models with task-specific augmentation, we pretrain language models to generate structures from the text on a collection of task-agnostic corpora. Our structure pretraining enables zero-shot transfer of the learned knowledge that models have about the structure tasks. We study the performance of this approach on 28 datasets, spanning 10 structure prediction tasks including open information extraction, joint entity and relation extraction, named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, factual probe, intent detection, and dialogue state tracking. We further enhance the pretraining with the task-specific training sets. We show that a 10B parameter language model transfers non-trivially to most tasks and obtains state-of-the-art performance on 21 of 28 datasets that we evaluate. Our code and datasets will be made publicly available.</abstract>
      <url hash="658a9ec9">2022.findings-acl.67</url>
      <bibkey>wang-etal-2022-deepstruct</bibkey>
      <revision id="1" href="2022.findings-acl.67v1" hash="2f9acce6"/>
      <revision id="2" href="2022.findings-acl.67v2" hash="658a9ec9" date="2022-05-16">In the appendix, revises descriptions of certain datasets used in the experiments to provide more clarity and details.</revision>
      <doi>10.18653/v1/2022.findings-acl.67</doi>
      <pwccode url="https://github.com/cgraywang/deepstruct" additional="false">cgraywang/deepstruct</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ace-2005">ACE 2005</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/atis">ATIS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-1">CoNLL</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL 2003</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conll">CoNLL++</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conll04">CoNLL04</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel">FewRel</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/genia">GENIA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/kelm">KELM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multiwoz">MultiWOZ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/new-york-times-annotated-corpus">New York Times Annotated Corpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/oie2016">OIE2016</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/opiec">OPIEC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ontonotes-5-0">OntoNotes 5.0</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snips">SNIPS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/t-rex">T-REx</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tacred">TACRED</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tekgen">TekGen</pwcdataset>
    </paper>
    <paper id="68">
      <title>The Change that Matters in Discourse Parsing: Estimating the Impact of Domain Shift on Parser Error</title>
      <author><first>Katherine</first><last>Atwell</last></author>
      <author><first>Anthony</first><last>Sicilia</last></author>
      <author><first>Seong Jae</first><last>Hwang</last></author>
      <author><first>Malihe</first><last>Alikhani</last></author>
      <pages>824-845</pages>
      <abstract>Discourse analysis allows us to attain inferences of a text document that extend beyond the sentence-level. The current performance of discourse models is very low on texts outside of the training distribution’s coverage, diminishing the practical utility of existing models. There is need for a measure that can inform us to what extent our model generalizes from the training to the test sample when these samples may be drawn from distinct distributions. While this can be estimated via distribution shift, we argue that this does not directly correlate with change in the observed error of a classifier (i.e. error-gap). Thus, we propose to use a statistic from the theoretical domain adaptation literature which can be directly tied to error-gap. We study the bias of this statistic as an estimator of error-gap both theoretically and through a large-scale empirical study of over 2400 experiments on 6 discourse datasets from domains including, but not limited to: news, biomedical texts, TED talks, Reddit posts, and fiction. Our results not only motivate our proposal and help us to understand its limitations, but also provide insight on the properties of discourse models and datasets which improve performance in domain adaptation. For instance, we find that non-news datasets are slightly easier to transfer to than news datasets when the training and test sets are very different. Our code and an associated Python package are available to allow practitioners to make more informed model and dataset choices.</abstract>
      <url hash="65f823c8">2022.findings-acl.68</url>
      <bibkey>atwell-etal-2022-change</bibkey>
      <doi>10.18653/v1/2022.findings-acl.68</doi>
      <video href="2022.findings-acl.68.mp4"/>
      <pwccode url="https://github.com/anthonysicilia/change-that-matters-acl2022" additional="true">anthonysicilia/change-that-matters-acl2022</pwccode>
    </paper>
    <paper id="69">
      <title>Mukayese: <fixed-case>T</fixed-case>urkish <fixed-case>NLP</fixed-case> Strikes Back</title>
      <author><first>Ali</first><last>Safaya</last></author>
      <author><first>Emirhan</first><last>Kurtuluş</last></author>
      <author><first>Arda</first><last>Goktogan</last></author>
      <author><first>Deniz</first><last>Yuret</last></author>
      <pages>846-863</pages>
      <abstract>Having sufficient resources for language X lifts it from the under-resourced languages class, but not necessarily from the under-researched class. In this paper, we address the problem of the absence of organized benchmarks in the Turkish language. We demonstrate that languages such as Turkish are left behind the state-of-the-art in NLP applications. As a solution, we present Mukayese, a set of NLP benchmarks for the Turkish language that contains several NLP tasks. We work on one or more datasets for each benchmark and present two or more baselines. Moreover, we present four new benchmarking datasets in Turkish for language modeling, sentence segmentation, and spell checking. All datasets and baselines are available under: <url>https://github.com/alisafaya/mukayese</url></abstract>
      <url hash="e0c245a3">2022.findings-acl.69</url>
      <bibkey>safaya-etal-2022-mukayese</bibkey>
      <doi>10.18653/v1/2022.findings-acl.69</doi>
      <video href="2022.findings-acl.69.mp4"/>
      <pwccode url="https://github.com/alisafaya/mukayese" additional="false">alisafaya/mukayese</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
    <paper id="70">
      <title>Virtual Augmentation Supported Contrastive Learning of Sentence Representations</title>
      <author><first>Dejiao</first><last>Zhang</last></author>
      <author><first>Wei</first><last>Xiao</last></author>
      <author><first>Henghui</first><last>Zhu</last></author>
      <author><first>Xiaofei</first><last>Ma</last></author>
      <author><first>Andrew</first><last>Arnold</last></author>
      <pages>864-876</pages>
      <abstract>Despite profound successes, contrastive representation learning relies on carefully designed data augmentations using domain-specific knowledge. This challenge is magnified in natural language processing, where no general rules exist for data augmentation due to the discrete nature of natural language. We tackle this challenge by presenting a Virtual augmentation Supported Contrastive Learning of sentence representations (VaSCL). Originating from the interpretation that data augmentation essentially constructs the neighborhoods of each training instance, we, in turn, utilize the neighborhood to generate effective data augmentations. Leveraging the large training batch size of contrastive learning, we approximate the neighborhood of an instance via its K-nearest in-batch neighbors in the representation space. We then define an instance discrimination task regarding the neighborhood and generate the virtual augmentation in an adversarial training manner. We access the performance of VaSCL on a wide range of downstream tasks and set a new state-of-the-art for unsupervised sentence representation learning.</abstract>
      <url hash="308247c5">2022.findings-acl.70</url>
      <bibkey>zhang-etal-2022-virtual</bibkey>
      <doi>10.18653/v1/2022.findings-acl.70</doi>
      <pwccode url="https://github.com/amazon-research/sentence-representations" additional="false">amazon-research/sentence-representations</pwccode>
    </paper>
    <paper id="71">
      <title><fixed-case>M</fixed-case>o<fixed-case>E</fixed-case>fication: Transformer Feed-forward Layers are Mixtures of Experts</title>
      <author><first>Zhengyan</first><last>Zhang</last></author>
      <author><first>Yankai</first><last>Lin</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Peng</first><last>Li</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>877-890</pages>
      <abstract>Recent work has shown that feed-forward networks (FFNs) in pre-trained Transformers are a key component, storing various linguistic and factual knowledge. However, the computational patterns of FFNs are still unclear. In this work, we study the computational patterns of FFNs and observe that most inputs only activate a tiny ratio of neurons of FFNs. This phenomenon is similar to the sparsity of the human brain, which drives research on functional partitions of the human brain. To verify whether functional partitions also emerge in FFNs, we propose to convert a model into its MoE version with the same parameters, namely MoEfication. Specifically, MoEfication consists of two phases: (1) splitting the parameters of FFNs into multiple functional partitions as experts, and (2) building expert routers to decide which experts will be used for each input. Experimental results show that MoEfication can conditionally use 10% to 30% of FFN parameters while maintaining over 95% original performance for different models on various downstream tasks. Besides, MoEfication brings two advantages: (1) it significantly reduces the FLOPS of inference, i.e., 2x speedup with 25% of FFN parameters, and (2) it provides a fine-grained perspective to study the inner mechanism of FFNs. The source code of this paper can be obtained from <url>https://github.com/thunlp/MoEfication</url>.</abstract>
      <url hash="adc169ce">2022.findings-acl.71</url>
      <bibkey>zhang-etal-2022-moefication</bibkey>
      <doi>10.18653/v1/2022.findings-acl.71</doi>
      <pwccode url="https://github.com/thunlp/moefication" additional="false">thunlp/moefication</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst-2">SST-2</pwcdataset>
    </paper>
    <paper id="72">
      <title><fixed-case>DS</fixed-case>-<fixed-case>TOD</fixed-case>: Efficient Domain Specialization for Task-Oriented Dialog</title>
      <author><first>Chia-Chien</first><last>Hung</last></author>
      <author><first>Anne</first><last>Lauscher</last></author>
      <author><first>Simone</first><last>Ponzetto</last></author>
      <author><first>Goran</first><last>Glavaš</last></author>
      <pages>891-904</pages>
      <abstract>Recent work has shown that self-supervised dialog-specific pretraining on large conversational datasets yields substantial gains over traditional language modeling (LM) pretraining in downstream task-oriented dialog (TOD). These approaches, however, exploit general dialogic corpora (e.g., Reddit) and thus presumably fail to reliably embed domain-specific knowledge useful for concrete downstream TOD domains. In this work, we investigate the effects of domain specialization of pretrained language models (PLMs) for TOD. Within our DS-TOD framework, we first automatically extract salient domain-specific terms, and then use them to construct DomainCC and DomainReddit – resources that we leverage for domain-specific pretraining, based on (i) masked language modeling (MLM) and (ii) response selection (RS) objectives, respectively. We further propose a resource-efficient and modular domain specialization by means of domain adapters – additional parameter-light layers in which we encode the domain knowledge. Our experiments with prominent TOD tasks – dialog state tracking (DST) and response retrieval (RR) – encompassing five domains from the MultiWOZ benchmark demonstrate the effectiveness of DS-TOD. Moreover, we show that the light-weight adapter-based specialization (1) performs comparably to full fine-tuning in single domain setups and (2) is particularly suitable for multi-domain specialization, where besides advantageous computational footprint, it can offer better TOD performance.</abstract>
      <url hash="741450bc">2022.findings-acl.72</url>
      <attachment type="software" hash="39f682c7">2022.findings-acl.72.software.zip</attachment>
      <bibkey>hung-etal-2022-ds</bibkey>
      <doi>10.18653/v1/2022.findings-acl.72</doi>
      <video href="2022.findings-acl.72.mp4"/>
      <pwccode url="https://github.com/umanlp/ds-tod" additional="false">umanlp/ds-tod</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ccnet">CCNet</pwcdataset>
    </paper>
    <paper id="73">
      <title>Distinguishing Non-natural from Natural Adversarial Samples for More Robust Pre-trained Language Model</title>
      <author><first>Jiayi</first><last>Wang</last></author>
      <author><first>Rongzhou</first><last>Bao</last></author>
      <author><first>Zhuosheng</first><last>Zhang</last></author>
      <author><first>Hai</first><last>Zhao</last></author>
      <pages>905-915</pages>
      <abstract>Recently, the problem of robustness of pre-trained language models (PrLMs) has received increasing research interest. Latest studies on adversarial attacks achieve high attack success rates against PrLMs, claiming that PrLMs are not robust. However, we find that the adversarial samples that PrLMs fail are mostly non-natural and do not appear in reality. We question the validity of the current evaluation of robustness of PrLMs based on these non-natural adversarial samples and propose an anomaly detector to evaluate the robustness of PrLMs with more natural adversarial samples. We also investigate two applications of the anomaly detector: (1) In data augmentation, we employ the anomaly detector to force generating augmented data that are distinguished as non-natural, which brings larger gains to the accuracy of PrLMs. (2) We apply the anomaly detector to a defense framework to enhance the robustness of PrLMs. It can be used to defend all types of attacks and achieves higher accuracy on both adversarial samples and compliant samples than other defense frameworks.</abstract>
      <url hash="4d370eed">2022.findings-acl.73</url>
      <bibkey>wang-etal-2022-distinguishing</bibkey>
      <doi>10.18653/v1/2022.findings-acl.73</doi>
      <pwccode url="https://github.com/lilynlp/distinguishing-non-natural" additional="false">lilynlp/distinguishing-non-natural</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst-2">SST-2</pwcdataset>
    </paper>
    <paper id="74">
      <title>Learning Adaptive Axis Attentions in Fine-tuning: Beyond Fixed Sparse Attention Patterns</title>
      <author><first>Zihan</first><last>Wang</last></author>
      <author><first>Jiuxiang</first><last>Gu</last></author>
      <author><first>Jason</first><last>Kuen</last></author>
      <author><first>Handong</first><last>Zhao</last></author>
      <author><first>Vlad</first><last>Morariu</last></author>
      <author><first>Ruiyi</first><last>Zhang</last></author>
      <author><first>Ani</first><last>Nenkova</last></author>
      <author><first>Tong</first><last>Sun</last></author>
      <author><first>Jingbo</first><last>Shang</last></author>
      <pages>916-925</pages>
      <abstract>We present a comprehensive study of sparse attention patterns in Transformer models. We first question the need for pre-training with sparse attention and present experiments showing that an efficient fine-tuning only approach yields a slightly worse but still competitive model. Then we compare the widely used local attention pattern and the less-well-studied global attention pattern, demonstrating that global patterns have several unique advantages. We also demonstrate that a flexible approach to attention, with different patterns across different layers of the model, is beneficial for some tasks. Drawing on this insight, we propose a novel Adaptive Axis Attention method, which learns—during fine-tuning—different attention patterns for each Transformer layer depending on the downstream task. Rather than choosing a fixed attention pattern, the adaptive axis attention method identifies important tokens—for each task and model layer—and focuses attention on those. It does not require pre-training to accommodate the sparse patterns and demonstrates competitive and sometimes better performance against fixed sparse attention patterns that require resource-intensive pre-training.</abstract>
      <url hash="bd5e2a54">2022.findings-acl.74</url>
      <bibkey>wang-etal-2022-learning</bibkey>
      <doi>10.18653/v1/2022.findings-acl.74</doi>
      <video href="2022.findings-acl.74.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/lra">LRA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
    </paper>
    <paper id="75">
      <title>Using Interactive Feedback to Improve the Accuracy and Explainability of Question Answering Systems Post-Deployment</title>
      <author><first>Zichao</first><last>Li</last></author>
      <author><first>Prakhar</first><last>Sharma</last></author>
      <author><first>Xing Han</first><last>Lu</last></author>
      <author><first>Jackie</first><last>Cheung</last></author>
      <author><first>Siva</first><last>Reddy</last></author>
      <pages>926-937</pages>
      <abstract>Most research on question answering focuses on the pre-deployment stage; i.e., building an accurate model for deployment. In this paper, we ask the question: Can we improve QA systems further post-deployment based on user interactions? We focus on two kinds of improvements: 1) improving the QA system’s performance itself, and 2) providing the model with the ability to explain the correctness or incorrectness of an answer. We collect a retrieval-based QA dataset, FeedbackQA, which contains interactive feedback from users. We collect this dataset by deploying a base QA system to crowdworkers who then engage with the system and provide feedback on the quality of its answers. The feedback contains both structured ratings and unstructured natural language explanations. We train a neural model with this feedback data that can generate explanations and re-score answer candidates. We show that feedback data not only improves the accuracy of the deployed QA system but also other stronger non-deployed systems. The generated explanations also help users make informed decisions about the correctness of answers.</abstract>
      <url hash="6ba1dacd">2022.findings-acl.75</url>
      <bibkey>li-etal-2022-using</bibkey>
      <doi>10.18653/v1/2022.findings-acl.75</doi>
      <video href="2022.findings-acl.75.mp4"/>
      <pwccode url="https://github.com/McGill-NLP/feedbackqa" additional="false">McGill-NLP/feedbackqa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/feedbackqa">FeedbackQA</pwcdataset>
    </paper>
    <paper id="76">
      <title>To be or not to be an Integer? Encoding Variables for Mathematical Text</title>
      <author><first>Deborah</first><last>Ferreira</last></author>
      <author><first>Mokanarangan</first><last>Thayaparan</last></author>
      <author><first>Marco</first><last>Valentino</last></author>
      <author><first>Julia</first><last>Rozanova</last></author>
      <author><first>Andre</first><last>Freitas</last></author>
      <pages>938-948</pages>
      <abstract>The application of Natural Language Inference (NLI) methods over large textual corpora can facilitate scientific discovery, reducing the gap between current research and the available large-scale scientific knowledge. However, contemporary NLI models are still limited in interpreting mathematical knowledge written in Natural Language, even though mathematics is an integral part of scientific argumentation for many disciplines. One of the fundamental requirements towards mathematical language understanding, is the creation of models able to meaningfully represent variables. This problem is particularly challenging since the meaning of a variable should be assigned exclusively from its defining type, i.e., the representation of a variable should come from its context. Recent research has formalised the variable typing task, a benchmark for the understanding of abstract mathematical types and variables in a sentence. In this work, we propose VarSlot, a Variable Slot-based approach, which not only delivers state-of-the-art results in the task of variable typing, but is also able to create context-based representations for variables.</abstract>
      <url hash="e1f13195">2022.findings-acl.76</url>
      <attachment type="software" hash="04b2843f">2022.findings-acl.76.software.zip</attachment>
      <bibkey>ferreira-etal-2022-integer</bibkey>
      <doi>10.18653/v1/2022.findings-acl.76</doi>
    </paper>
    <paper id="77">
      <title><fixed-case>GRS</fixed-case>: Combining Generation and Revision in Unsupervised Sentence Simplification</title>
      <author><first>Mohammad</first><last>Dehghan</last></author>
      <author><first>Dhruv</first><last>Kumar</last></author>
      <author><first>Lukasz</first><last>Golab</last></author>
      <pages>949-960</pages>
      <abstract>We propose GRS: an unsupervised approach to sentence simplification that combines text generation and text revision. We start with an iterative framework in which an input sentence is revised using explicit edit operations, and add paraphrasing as a new edit operation. This allows us to combine the advantages of generative and revision-based approaches: paraphrasing captures complex edit operations, and the use of explicit edit operations in an iterative manner provides controllability and interpretability. We demonstrate these advantages of GRS compared to existing methods on the Newsela and ASSET datasets.</abstract>
      <url hash="d8045c78">2022.findings-acl.77</url>
      <bibkey>dehghan-etal-2022-grs</bibkey>
      <doi>10.18653/v1/2022.findings-acl.77</doi>
      <video href="2022.findings-acl.77.mp4"/>
      <pwccode url="https://github.com/imohammad12/grs" additional="false">imohammad12/grs</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/asset">ASSET</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsela">Newsela</pwcdataset>
    </paper>
    <paper id="78">
      <title><fixed-case>BPE</fixed-case> vs. Morphological Segmentation: A Case Study on Machine Translation of Four Polysynthetic Languages</title>
      <author><first>Manuel</first><last>Mager</last></author>
      <author><first>Arturo</first><last>Oncevay</last></author>
      <author><first>Elisabeth</first><last>Mager</last></author>
      <author><first>Katharina</first><last>Kann</last></author>
      <author><first>Thang</first><last>Vu</last></author>
      <pages>961-971</pages>
      <abstract>Morphologically-rich polysynthetic languages present a challenge for NLP systems due to data sparsity, and a common strategy to handle this issue is to apply subword segmentation. We investigate a wide variety of supervised and unsupervised morphological segmentation methods for four polysynthetic languages: Nahuatl, Raramuri, Shipibo-Konibo, and Wixarika. Then, we compare the morphologically inspired segmentation methods against Byte-Pair Encodings (BPEs) as inputs for machine translation (MT) when translating to and from Spanish. We show that for all language pairs except for Nahuatl, an unsupervised morphological segmentation algorithm outperforms BPEs consistently and that, although supervised methods achieve better segmentation scores, they under-perform in MT challenges. Finally, we contribute two new morphological segmentation datasets for Raramuri and Shipibo-Konibo, and a parallel corpus for Raramuri–Spanish.</abstract>
      <url hash="6cdee1b8">2022.findings-acl.78</url>
      <bibkey>mager-etal-2022-bpe</bibkey>
      <doi>10.18653/v1/2022.findings-acl.78</doi>
      <video href="2022.findings-acl.78.mp4"/>
    </paper>
    <paper id="79">
      <title>Distributed <fixed-case>NLI</fixed-case>: Learning to Predict Human Opinion Distributions for Language Reasoning</title>
      <author><first>Xiang</first><last>Zhou</last></author>
      <author><first>Yixin</first><last>Nie</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>972-987</pages>
      <abstract>We introduce distributed NLI, a new NLU task with a goal to predict the distribution of human judgements for natural language inference. We show that by applying additional distribution estimation methods, namely, Monte Carlo (MC) Dropout, Deep Ensemble, Re-Calibration, and Distribution Distillation, models can capture human judgement distribution more effectively than the softmax baseline. We show that MC Dropout is able to achieve decent performance without any distribution annotations while Re-Calibration can give further improvements with extra distribution annotations, suggesting the value of multiple annotations for one example in modeling the distribution of human judgements. Despite these improvements, the best results are still far below the estimated human upper-bound, indicating that predicting the distribution of human judgements is still an open, challenging problem with a large room for improvements. We showcase the common errors for MC Dropout and Re-Calibration. Finally, we give guidelines on the usage of these methods with different levels of data availability and encourage future work on modeling the human opinion distribution for language reasoning.</abstract>
      <url hash="ed25a73b">2022.findings-acl.79</url>
      <attachment type="software" hash="5a4914c7">2022.findings-acl.79.software.zip</attachment>
      <bibkey>zhou-etal-2022-distributed</bibkey>
      <doi>10.18653/v1/2022.findings-acl.79</doi>
      <pwccode url="https://github.com/easonnie/ChaosNLI" additional="false">easonnie/ChaosNLI</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/chaosnli">ChaosNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="80">
      <title>Morphological Processing of Low-Resource Languages: Where We Are and What’s Next</title>
      <author><first>Adam</first><last>Wiemerslage</last></author>
      <author><first>Miikka</first><last>Silfverberg</last></author>
      <author><first>Changbing</first><last>Yang</last></author>
      <author><first>Arya</first><last>McCarthy</last></author>
      <author><first>Garrett</first><last>Nicolai</last></author>
      <author><first>Eliana</first><last>Colunga</last></author>
      <author><first>Katharina</first><last>Kann</last></author>
      <pages>988-1007</pages>
      <abstract>Automatic morphological processing can aid downstream natural language processing applications, especially for low-resource languages, and assist language documentation efforts for endangered languages. Having long been multilingual, the field of computational morphology is increasingly moving towards approaches suitable for languages with minimal or no annotated resources. First, we survey recent developments in computational morphology with a focus on low-resource languages. Second, we argue that the field is ready to tackle the logical next challenge: understanding a language’s morphology from raw text alone. We perform an empirical study on a truly unsupervised version of the paradigm completion task and show that, while existing state-of-the-art models bridged by two newly proposed models we devise perform reasonably, there is still much room for improvement. The stakes are high: solving this task will increase the language coverage of morphological resources by a number of magnitudes.</abstract>
      <url hash="1d7407bb">2022.findings-acl.80</url>
      <bibkey>wiemerslage-etal-2022-morphological</bibkey>
      <doi>10.18653/v1/2022.findings-acl.80</doi>
      <video href="2022.findings-acl.80.mp4"/>
    </paper>
    <paper id="81">
      <title>Learning and Evaluating Character Representations in Novels</title>
      <author><first>Naoya</first><last>Inoue</last></author>
      <author><first>Charuta</first><last>Pethe</last></author>
      <author><first>Allen</first><last>Kim</last></author>
      <author><first>Steven</first><last>Skiena</last></author>
      <pages>1008-1019</pages>
      <abstract>We address the problem of learning fixed-length vector representations of characters in novels. Recent advances in word embeddings have proven successful in learning entity representations from short texts, but fall short on longer documents because they do not capture full book-level information. To overcome the weakness of such text-based embeddings, we propose two novel methods for representing characters: (i) graph neural network-based embeddings from a full corpus-based character network; and (ii) low-dimensional embeddings constructed from the occurrence pattern of characters in each novel. We test the quality of these character embeddings using a new benchmark suite to evaluate character representations, encompassing 12 different tasks. We show that our representation techniques combined with text-based embeddings lead to the best character representations, outperforming text-based embeddings in four tasks. Our dataset and evaluation script will be made publicly available to stimulate additional work in this area.</abstract>
      <url hash="7c807cab">2022.findings-acl.81</url>
      <bibkey>inoue-etal-2022-learning</bibkey>
      <doi>10.18653/v1/2022.findings-acl.81</doi>
      <video href="2022.findings-acl.81.mp4"/>
      <pwccode url="https://github.com/naoya-i/charembench" additional="false">naoya-i/charembench</pwccode>
    </paper>
    <paper id="82">
      <title>Answer Uncertainty and Unanswerability in Multiple-Choice Machine Reading Comprehension</title>
      <author><first>Vatsal</first><last>Raina</last></author>
      <author><first>Mark</first><last>Gales</last></author>
      <pages>1020-1034</pages>
      <abstract>Machine reading comprehension (MRC) has drawn a lot of attention as an approach for assessing the ability of systems to understand natural language. Usually systems focus on selecting the correct answer to a question given a contextual paragraph. However, for many applications of multiple-choice MRC systems there are two additional considerations. For multiple-choice exams there is often a negative marking scheme; there is a penalty for an incorrect answer. In terms of an MRC system this means that the system is required to have an idea of the uncertainty in the predicted answer. The second consideration is that many multiple-choice questions have the option of none-of-the-above (NOA) indicating that none of the answers is applicable, rather than there always being the correct answer in the list of choices. This paper investigates both of these issues by making use of predictive uncertainty. Whether the system should propose an answer is a direct application of answer uncertainty. There are two possibilities when considering the NOA option. The simplest is to explicitly build a system on data that includes this option. Alternatively uncertainty can be applied to detect whether the other options include the correct answer. If the system is not sufficiently confident it will select NOA. As there is no standard corpus available to investigate these topics, the ReClor corpus is modified by removing the correct answer from a subset of possible answers. A high-performance MRC system is used to evaluate whether answer uncertainty can be applied in these situations. It is shown that uncertainty does allow questions that the system is not confident about to be detected. Additionally it is shown that uncertainty outperforms a system explicitly built with an NOA option.</abstract>
      <url hash="b86f8bff">2022.findings-acl.82</url>
      <bibkey>raina-gales-2022-answer</bibkey>
      <doi>10.18653/v1/2022.findings-acl.82</doi>
      <video href="2022.findings-acl.82.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/reclor">ReClor</pwcdataset>
    </paper>
    <paper id="83">
      <title>Measuring the Language of Self-Disclosure across Corpora</title>
      <author><first>Ann-Katrin</first><last>Reuel</last></author>
      <author><first>Sebastian</first><last>Peralta</last></author>
      <author><first>João</first><last>Sedoc</last></author>
      <author><first>Garrick</first><last>Sherman</last></author>
      <author><first>Lyle</first><last>Ungar</last></author>
      <pages>1035-1047</pages>
      <abstract>Being able to reliably estimate self-disclosure – a key component of friendship and intimacy – from language is important for many psychology studies. We build single-task models on five self-disclosure corpora, but find that these models generalize poorly; the within-domain accuracy of predicted message-level self-disclosure of the best-performing model (mean Pearson’s r=0.69) is much higher than the respective across data set accuracy (mean Pearson’s r=0.32), due to both variations in the corpora (e.g., medical vs. general topics) and labeling instructions (target variables: self-disclosure, emotional disclosure, intimacy). However, some lexical features, such as expression of negative emotions and use of first person personal pronouns such as ‘I’ reliably predict self-disclosure across corpora. We develop a multi-task model that yields better results, with an average Pearson’s r of 0.37 for out-of-corpora prediction.</abstract>
      <url hash="0acc04dc">2022.findings-acl.83</url>
      <bibkey>reuel-etal-2022-measuring</bibkey>
      <doi>10.18653/v1/2022.findings-acl.83</doi>
    </paper>
    <paper id="84">
      <title>When Chosen Wisely, More Data Is What You Need: A Universal Sample-Efficient Strategy For Data Augmentation</title>
      <author><first>Ehsan</first><last>Kamalloo</last></author>
      <author><first>Mehdi</first><last>Rezagholizadeh</last></author>
      <author><first>Ali</first><last>Ghodsi</last></author>
      <pages>1048-1062</pages>
      <abstract>Data Augmentation (DA) is known to improve the generalizability of deep neural networks. Most existing DA techniques naively add a certain number of augmented samples without considering the quality and the added computational cost of these samples. To tackle this problem, a common strategy, adopted by several state-of-the-art DA methods, is to adaptively generate or re-weight augmented samples with respect to the task objective during training. However, these adaptive DA methods: (1) are computationally expensive and not sample-efficient, and (2) are designed merely for a specific setting. In this work, we present a universal DA technique, called Glitter, to overcome both issues. Glitter can be plugged into any DA method, making training sample-efficient without sacrificing performance. From a pre-generated pool of augmented samples, Glitter adaptively selects a subset of worst-case samples with maximal loss, analogous to adversarial DA. Without altering the training strategy, the task objective can be optimized on the selected subset. Our thorough experiments on the GLUE benchmark, SQuAD, and HellaSwag in three widely used training setups including consistency training, self-distillation and knowledge distillation reveal that Glitter is substantially faster to train and achieves a competitive performance, compared to strong baselines.</abstract>
      <url hash="6bded542">2022.findings-acl.84</url>
      <bibkey>kamalloo-etal-2022-chosen</bibkey>
      <doi>10.18653/v1/2022.findings-acl.84</doi>
      <pwccode url="https://github.com/huawei-noah/kd-nlp" additional="false">huawei-noah/kd-nlp</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/anli">ANLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hellaswag">HellaSwag</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sick">SICK</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="85">
      <title>Explaining Classes through Stable Word Attributions</title>
      <author><first>Samuel</first><last>Rönnqvist</last></author>
      <author><first>Aki-Juhani</first><last>Kyröläinen</last></author>
      <author><first>Amanda</first><last>Myntti</last></author>
      <author><first>Filip</first><last>Ginter</last></author>
      <author><first>Veronika</first><last>Laippala</last></author>
      <pages>1063-1074</pages>
      <abstract>Input saliency methods have recently become a popular tool for explaining predictions of deep learning models in NLP. Nevertheless, there has been little work investigating methods for aggregating prediction-level explanations to the class level, nor has a framework for evaluating such class explanations been established. We explore explanations based on XLM-R and the Integrated Gradients input attribution method, and propose 1) the Stable Attribution Class Explanation method (SACX) to extract keyword lists of classes in text classification tasks, and 2) a framework for the systematic evaluation of the keyword lists. We find that explanations of individual predictions are prone to noise, but that stable explanations can be effectively identified through repeated training and explanation. We evaluate on web register data and show that the class explanations are linguistically meaningful and distinguishing of the classes.</abstract>
      <url hash="244dcb03">2022.findings-acl.85</url>
      <attachment type="software" hash="69b60bdf">2022.findings-acl.85.software.tgz</attachment>
      <bibkey>ronnqvist-etal-2022-explaining</bibkey>
      <doi>10.18653/v1/2022.findings-acl.85</doi>
      <pwccode url="https://github.com/turkunlp/class-explainer" additional="false">turkunlp/class-explainer</pwccode>
    </paper>
    <paper id="86">
      <title>What to Learn, and How: <fixed-case>T</fixed-case>oward Effective Learning from Rationales</title>
      <author><first>Samuel</first><last>Carton</last></author>
      <author><first>Surya</first><last>Kanoria</last></author>
      <author><first>Chenhao</first><last>Tan</last></author>
      <pages>1075-1088</pages>
      <abstract>Learning from rationales seeks to augment model prediction accuracy using human-annotated rationales (i.e. subsets of input tokens) that justify their chosen labels, often in the form of intermediate or multitask supervision. While intuitive, this idea has proven elusive in practice. We make two observations about human rationales via empirical analyses:1) maximizing rationale supervision accuracy is not necessarily the optimal objective for improving model accuracy; 2) human rationales vary in whether they provide sufficient information for the model to exploit for prediction. Building on these insights, we propose several novel loss functions and learning strategies, and evaluate their effectiveness on three datasets with human rationales. Our results demonstrate consistent improvements over baselines in both label and rationale accuracy, including a 3% accuracy improvement on MultiRC. Our work highlights the importance of understanding properties of human explanations and exploiting them accordingly in model training.</abstract>
      <url hash="9ba00c79">2022.findings-acl.86</url>
      <bibkey>carton-etal-2022-learn</bibkey>
      <doi>10.18653/v1/2022.findings-acl.86</doi>
      <video href="2022.findings-acl.86.mp4"/>
      <pwccode url="https://github.com/chicagohai/learning-from-rationales" additional="false">chicagohai/learning-from-rationales</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multirc">MultiRC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/e-snli">e-SNLI</pwcdataset>
    </paper>
    <paper id="87">
      <title>Listening to Affected Communities to Define Extreme Speech: Dataset and Experiments</title>
      <author><first>Antonis</first><last>Maronikolakis</last></author>
      <author><first>Axel</first><last>Wisiorek</last></author>
      <author><first>Leah</first><last>Nann</last></author>
      <author><first>Haris</first><last>Jabbar</last></author>
      <author><first>Sahana</first><last>Udupa</last></author>
      <author><first>Hinrich</first><last>Schuetze</last></author>
      <pages>1089-1104</pages>
      <abstract>Building on current work on multilingual hate speech (e.g., Ousidhoum et al. (2019)) and hate speech reduction (e.g., Sap et al. (2020)), we present XTREMESPEECH, a new hate speech dataset containing 20,297 social media passages from Brazil, Germany, India and Kenya. The key novelty is that we directly involve the affected communities in collecting and annotating the data – as opposed to giving companies and governments control over defining and combatting hate speech. This inclusive approach results in datasets more representative of actually occurring online speech and is likely to facilitate the removal of the social media content that marginalized communities view as causing the most harm. Based on XTREMESPEECH, we establish novel tasks with accompanying baselines, provide evidence that cross-country training is generally not feasible due to cultural differences between countries and perform an interpretability analysis of BERT’s predictions.</abstract>
      <url hash="e8cf54ea">2022.findings-acl.87</url>
      <bibkey>maronikolakis-etal-2022-listening</bibkey>
      <doi>10.18653/v1/2022.findings-acl.87</doi>
      <video href="2022.findings-acl.87.mp4"/>
      <pwccode url="https://github.com/antmarakis/xtremespeech" additional="false">antmarakis/xtremespeech</pwccode>
    </paper>
    <paper id="88">
      <title>Entropy-based Attention Regularization Frees Unintended Bias Mitigation from Lists</title>
      <author><first>Giuseppe</first><last>Attanasio</last></author>
      <author><first>Debora</first><last>Nozza</last></author>
      <author><first>Dirk</first><last>Hovy</last></author>
      <author><first>Elena</first><last>Baralis</last></author>
      <pages>1105-1119</pages>
      <abstract>Natural Language Processing (NLP) models risk overfitting to specific terms in the training data, thereby reducing their performance, fairness, and generalizability. E.g., neural hate speech detection models are strongly influenced by identity terms like gay, or women, resulting in false positives, severe unintended bias, and lower performance. Most mitigation techniques use lists of identity terms or samples from the target domain during training. However, this approach requires a-priori knowledge and introduces further bias if important terms are neglected. Instead, we propose a knowledge-free Entropy-based Attention Regularization (EAR) to discourage overfitting to training-specific terms. An additional objective function penalizes tokens with low self-attention entropy. We fine-tune BERT via EAR: the resulting model matches or exceeds state-of-the-art performance for hate speech classification and bias metrics on three benchmark corpora in English and Italian.EAR also reveals overfitting terms, i.e., terms most likely to induce bias, to help identify their effect on the model, task, and predictions.</abstract>
      <url hash="9b36ea6e">2022.findings-acl.88</url>
      <attachment type="software" hash="527aa339">2022.findings-acl.88.software.zip</attachment>
      <bibkey>attanasio-etal-2022-entropy</bibkey>
      <doi>10.18653/v1/2022.findings-acl.88</doi>
      <video href="2022.findings-acl.88.mp4"/>
      <pwccode url="https://github.com/g8a9/ear" additional="false">g8a9/ear</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mlma-hate-speech">MLMA Hate Speech</pwcdataset>
    </paper>
    <paper id="89">
      <title>From <fixed-case>BERT</fixed-case>‘s <fixed-case>P</fixed-case>oint of <fixed-case>V</fixed-case>iew: <fixed-case>R</fixed-case>evealing the <fixed-case>P</fixed-case>revailing <fixed-case>C</fixed-case>ontextual <fixed-case>D</fixed-case>ifferences</title>
      <author><first>Carolin M.</first><last>Schuster</last></author>
      <author><first>Simon</first><last>Hegelich</last></author>
      <pages>1120-1138</pages>
      <abstract>Though successfully applied in research and industry large pretrained language models of the BERT family are not yet fully understood. While much research in the field of BERTology has tested whether specific knowledge can be extracted from layer activations, we invert the popular probing design to analyze the prevailing differences and clusters in BERT’s high dimensional space. By extracting coarse features from masked token representations and predicting them by probing models with access to only partial information we can apprehend the variation from ‘BERT’s point of view’. By applying our new methodology to different datasets we show how much the differences can be described by syntax but further how they are to a great extent shaped by the most simple positional information.</abstract>
      <url hash="f82c8085">2022.findings-acl.89</url>
      <attachment type="software" hash="bf9e3053">2022.findings-acl.89.software.zip</attachment>
      <bibkey>schuster-hegelich-2022-berts</bibkey>
      <doi>10.18653/v1/2022.findings-acl.89</doi>
    </paper>
    <paper id="90">
      <title>Learning Bias-reduced Word Embeddings Using Dictionary Definitions</title>
      <author><first>Haozhe</first><last>An</last></author>
      <author><first>Xiaojiang</first><last>Liu</last></author>
      <author><first>Donald</first><last>Zhang</last></author>
      <pages>1139-1152</pages>
      <abstract>Pre-trained word embeddings, such as GloVe, have shown undesirable gender, racial, and religious biases. To address this problem, we propose DD-GloVe, a train-time debiasing algorithm to learn word embeddings by leveraging <tex-math>\underline{d}</tex-math>ictionary <tex-math>\underline{d}</tex-math>efinitions. We introduce dictionary-guided loss functions that encourage word embeddings to be similar to their relatively neutral dictionary definition representations. Existing debiasing algorithms typically need a pre-compiled list of seed words to represent the bias direction, along which biased information gets removed. Producing this list involves subjective decisions and it might be difficult to obtain for some types of biases. We automate the process of finding seed words: our algorithm starts from a single pair of initial seed words and automatically finds more words whose definitions display similar attributes traits. We demonstrate the effectiveness of our approach with benchmark evaluations and empirical analyses. Our code is available at <url>https://github.com/haozhe-an/DD-GloVe</url>.</abstract>
      <url hash="71949aa4">2022.findings-acl.90</url>
      <bibkey>an-etal-2022-learning</bibkey>
      <doi>10.18653/v1/2022.findings-acl.90</doi>
      <pwccode url="https://github.com/haozhe-an/dd-glove" additional="false">haozhe-an/dd-glove</pwccode>
    </paper>
    <paper id="91">
      <title>Knowledge Graph Embedding by Adaptive Limit Scoring Loss Using Dynamic Weighting Strategy</title>
      <author><first>Jinfa</first><last>Yang</last></author>
      <author><first>Xianghua</first><last>Ying</last></author>
      <author><first>Yongjie</first><last>Shi</last></author>
      <author><first>Xin</first><last>Tong</last></author>
      <author><first>Ruibin</first><last>Wang</last></author>
      <author><first>Taiyan</first><last>Chen</last></author>
      <author><first>Bowei</first><last>Xing</last></author>
      <pages>1153-1163</pages>
      <abstract>Knowledge graph embedding aims to represent entities and relations as low-dimensional vectors, which is an effective way for predicting missing links in knowledge graphs. Designing a strong and effective loss framework is essential for knowledge graph embedding models to distinguish between correct and incorrect triplets. The classic margin-based ranking loss limits the scores of positive and negative triplets to have a suitable margin. The recently proposed Limit-based Scoring Loss independently limits the range of positive and negative triplet scores. However, these loss frameworks use equal or fixed penalty terms to reduce the scores of positive and negative sample pairs, which is inflexible in optimization. Our intuition is that if a triplet score deviates far from the optimum, it should be emphasized. To this end, we propose Adaptive Limit Scoring Loss, which simply re-weights each triplet to highlight the less-optimized triplet scores. We apply this loss framework to several knowledge graph embedding models such as TransE, TransH and ComplEx. The experimental results on link prediction and triplet classification show that our proposed method has achieved performance on par with the state of the art.</abstract>
      <url hash="6f749bd6">2022.findings-acl.91</url>
      <bibkey>yang-etal-2022-knowledge</bibkey>
      <doi>10.18653/v1/2022.findings-acl.91</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/fb15k-237">FB15k-237</pwcdataset>
    </paper>
    <paper id="92">
      <title><fixed-case>OCR</fixed-case> Improves Machine Translation for Low-Resource Languages</title>
      <author><first>Oana</first><last>Ignat</last></author>
      <author><first>Jean</first><last>Maillard</last></author>
      <author><first>Vishrav</first><last>Chaudhary</last></author>
      <author><first>Francisco</first><last>Guzmán</last></author>
      <pages>1164-1174</pages>
      <abstract>We aim to investigate the performance of current OCR systems on low resource languages and low resource scripts. We introduce and make publicly available a novel benchmark, OCR4MT, consisting of real and synthetic data, enriched with noise, for 60 low-resource languages in low resource scripts. We evaluate state-of-the-art OCR systems on our benchmark and analyse most common errors. We show that OCR monolingual data is a valuable resource that can increase performance of Machine Translation models, when used in backtranslation. We then perform an ablation study to investigate how OCR errors impact Machine Translation performance and determine what is the minimum level of OCR quality needed for the monolingual data to be useful for Machine Translation.</abstract>
      <url hash="8d03ec3c">2022.findings-acl.92</url>
      <bibkey>ignat-etal-2022-ocr</bibkey>
      <doi>10.18653/v1/2022.findings-acl.92</doi>
      <video href="2022.findings-acl.92.mp4"/>
    </paper>
    <paper id="93">
      <title><fixed-case>C</fixed-case>o<fixed-case>C</fixed-case>o<fixed-case>LM</fixed-case>: Complex Commonsense Enhanced Language Model with Discourse Relations</title>
      <author><first>Changlong</first><last>Yu</last></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Yangqiu</first><last>Song</last></author>
      <author><first>Wilfred</first><last>Ng</last></author>
      <pages>1175-1187</pages>
      <abstract>Large-scale pre-trained language models have demonstrated strong knowledge representation ability. However, recent studies suggest that even though these giant models contain rich simple commonsense knowledge (e.g., bird can fly and fish can swim.), they often struggle with complex commonsense knowledge that involves multiple eventualities (verb-centric phrases, e.g., identifying the relationship between “Jim yells at Bob” and “Bob is upset”). To address this issue, in this paper, we propose to help pre-trained language models better incorporate complex commonsense knowledge. Unlike direct fine-tuning approaches, we do not focus on a specific task and instead propose a general language model named CoCoLM. Through the careful training over a large-scale eventuality knowledge graph ASER, we successfully teach pre-trained language models (i.e., BERT and RoBERTa) rich multi-hop commonsense knowledge among eventualities. Experiments on multiple commonsense tasks that require the correct understanding of eventualities demonstrate the effectiveness of CoCoLM.</abstract>
      <url hash="3a8e6fdf">2022.findings-acl.93</url>
      <bibkey>yu-etal-2022-cocolm</bibkey>
      <doi>10.18653/v1/2022.findings-acl.93</doi>
      <pwccode url="https://github.com/hkust-knowcomp/co2lm" additional="false">hkust-knowcomp/co2lm</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/copa">COPA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/lama">LAMA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/rocstories">ROCStories</pwcdataset>
    </paper>
    <paper id="94">
      <title>Learning to Robustly Aggregate Labeling Functions for Semi-supervised Data Programming</title>
      <author><first>Ayush</first><last>Maheshwari</last></author>
      <author><first>Krishnateja</first><last>Killamsetty</last></author>
      <author><first>Ganesh</first><last>Ramakrishnan</last></author>
      <author><first>Rishabh</first><last>Iyer</last></author>
      <author><first>Marina</first><last>Danilevsky</last></author>
      <author><first>Lucian</first><last>Popa</last></author>
      <pages>1188-1202</pages>
      <abstract>A critical bottleneck in supervised machine learning is the need for large amounts of labeled data which is expensive and time-consuming to obtain. Although a small amount of labeled data cannot be used to train a model, it can be used effectively for the generation of humaninterpretable labeling functions (LFs). These LFs, in turn, have been used to generate a large amount of additional noisy labeled data in a paradigm that is now commonly referred to as data programming. Previous methods of generating LFs do not attempt to use the given labeled data further to train a model, thus missing opportunities for improving performance. Additionally, since the LFs are generated automatically, they are likely to be noisy, and naively aggregating these LFs can lead to suboptimal results. In this work, we propose an LF-based bi-level optimization framework WISDOM to solve these two critical limitations. WISDOM learns a joint model on the (same) labeled dataset used for LF induction along with any unlabeled data in a semi-supervised manner, and more critically, reweighs each LF according to its goodness, influencing its contribution to the semi-supervised loss using a robust bi-level optimization algorithm. We show that WISDOM significantly outperforms prior approaches on several text classification datasets.</abstract>
      <url hash="d59c3118">2022.findings-acl.94</url>
      <attachment type="software" hash="064c9fcf">2022.findings-acl.94.software.zip</attachment>
      <bibkey>maheshwari-etal-2022-learning</bibkey>
      <doi>10.18653/v1/2022.findings-acl.94</doi>
      <pwccode url="https://github.com/ayushbits/robust-aggregate-lfs" additional="false">ayushbits/robust-aggregate-lfs</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst-5">SST-5</pwcdataset>
    </paper>
    <paper id="95">
      <title>Multi-Granularity Semantic Aware Graph Model for Reducing Position Bias in Emotion Cause Pair Extraction</title>
      <author><first>Yinan</first><last>Bao</last></author>
      <author><first>Qianwen</first><last>Ma</last></author>
      <author><first>Lingwei</first><last>Wei</last></author>
      <author><first>Wei</first><last>Zhou</last></author>
      <author><first>Songlin</first><last>Hu</last></author>
      <pages>1203-1213</pages>
      <abstract>The emotion cause pair extraction (ECPE) task aims to extract emotions and causes as pairs from documents. We observe that the relative distance distribution of emotions and causes is extremely imbalanced in the typical ECPE dataset. Existing methods have set a fixed size window to capture relations between neighboring clauses. However, they neglect the effective semantic connections between distant clauses, leading to poor generalization ability towards position-insensitive data. To alleviate the problem, we propose a novel <tex-math>\textbf{M}</tex-math>ulti-<tex-math>\textbf{G}</tex-math>ranularity <tex-math>\textbf{S}</tex-math>emantic <tex-math>\textbf{A}</tex-math>ware <tex-math>\textbf{G}</tex-math>raph model (MGSAG) to incorporate fine-grained and coarse-grained semantic features jointly, without regard to distance limitation. In particular, we first explore semantic dependencies between clauses and keywords extracted from the document that convey fine-grained semantic features, obtaining keywords enhanced clause representations. Besides, a clause graph is also established to model coarse-grained semantic relations between clauses. Experimental results indicate that MGSAG surpasses the existing state-of-the-art ECPE models. Especially, MGSAG outperforms other models significantly in the condition of position-insensitive data.</abstract>
      <url hash="c858ac28">2022.findings-acl.95</url>
      <bibkey>bao-etal-2022-multi</bibkey>
      <doi>10.18653/v1/2022.findings-acl.95</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/ecpe-xia-and-ding-2019">Xia and Ding, 2019</pwcdataset>
    </paper>
    <paper id="96">
      <title>Cross-lingual Inference with A <fixed-case>C</fixed-case>hinese Entailment Graph</title>
      <author><first>Tianyi</first><last>Li</last></author>
      <author><first>Sabine</first><last>Weber</last></author>
      <author><first>Mohammad Javad</first><last>Hosseini</last></author>
      <author><first>Liane</first><last>Guillou</last></author>
      <author><first>Mark</first><last>Steedman</last></author>
      <pages>1214-1233</pages>
      <abstract>Predicate entailment detection is a crucial task for question-answering from text, where previous work has explored unsupervised learning of entailment graphs from typed open relation triples. In this paper, we present the first pipeline for building Chinese entailment graphs, which involves a novel high-recall open relation extraction (ORE) method and the first Chinese fine-grained entity typing dataset under the FIGER type ontology. Through experiments on the Levy-Holt dataset, we verify the strength of our Chinese entailment graph, and reveal the cross-lingual complementarity: on the parallel Levy-Holt dataset, an ensemble of Chinese and English entailment graphs outperforms both monolingual graphs, and raises unsupervised SOTA by 4.7 AUC points.</abstract>
      <url hash="65f04b84">2022.findings-acl.96</url>
      <attachment type="software" hash="b379bced">2022.findings-acl.96.software.zip</attachment>
      <bibkey>li-etal-2022-cross</bibkey>
      <doi>10.18653/v1/2022.findings-acl.96</doi>
      <video href="2022.findings-acl.96.mp4"/>
      <pwccode url="https://github.com/teddy-li/chineseentgraph" additional="false">teddy-li/chineseentgraph</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/clue">CLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/figer">FIGER</pwcdataset>
    </paper>
    <paper id="97">
      <title>Multi-task Learning for Paraphrase Generation With Keyword and Part-of-Speech Reconstruction</title>
      <author><first>Xuhang</first><last>Xie</last></author>
      <author><first>Xuesong</first><last>Lu</last></author>
      <author><first>Bei</first><last>Chen</last></author>
      <pages>1234-1243</pages>
      <abstract>Paraphrase generation using deep learning has been a research hotspot of natural language processing in the past few years. While previous studies tackle the problem from different aspects, the essence of paraphrase generation is to retain the key semantics of the source sentence and rewrite the rest of the content. Inspired by this observation, we propose a novel two-stage model, PGKPR, for paraphrase generation with keyword and part-of-speech reconstruction. The rationale is to capture simultaneously the possible keywords of a source sentence and the relations between them to facilitate the rewriting. In the first stage, we identify the possible keywords using a prediction attribution technique, where the words obtaining higher attribution scores are more likely to be the keywords. In the second stage, we train a transformer-based model via multi-task learning for paraphrase generation. The novel learning task is the reconstruction of the keywords and part-of-speech tags, respectively, from a perturbed sequence of the source sentence. The learned encodings are then decoded to generate the paraphrase. We conduct the experiments on two commonly-used datasets, and demonstrate the superior performance of PGKPR over comparative models on multiple evaluation metrics.</abstract>
      <url hash="d2d5d95b">2022.findings-acl.97</url>
      <attachment type="software" hash="6e514ed0">2022.findings-acl.97.software.zip</attachment>
      <bibkey>xie-etal-2022-multi</bibkey>
      <doi>10.18653/v1/2022.findings-acl.97</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">MS COCO</pwcdataset>
    </paper>
    <paper id="98">
      <title><fixed-case>MDCS</fixed-case>pell: A Multi-task Detector-Corrector Framework for <fixed-case>C</fixed-case>hinese Spelling Correction</title>
      <author><first>Chenxi</first><last>Zhu</last></author>
      <author><first>Ziqiang</first><last>Ying</last></author>
      <author><first>Boyu</first><last>Zhang</last></author>
      <author><first>Feng</first><last>Mao</last></author>
      <pages>1244-1253</pages>
      <abstract>Chinese Spelling Correction (CSC) is a task to detect and correct misspelled characters in Chinese texts. CSC is challenging since many Chinese characters are visually or phonologically similar but with quite different semantic meanings. Many recent works use BERT-based language models to directly correct each character of the input sentence. However, these methods can be sub-optimal since they correct every character of the sentence only by the context which is easily negatively affected by the misspelled characters. Some other works propose to use an error detector to guide the correction by masking the detected errors. Nevertheless, these methods dampen the visual or phonological features from the misspelled characters which could be critical for correction. In this work, we propose a novel general detector-corrector multi-task framework where the corrector uses BERT to capture the visual and phonological features from each character in the raw sentence and uses a late fusion strategy to fuse the hidden states of the corrector with that of the detector to minimize the negative impact from the misspelled characters. Comprehensive experiments on benchmarks demonstrate that our proposed method can significantly outperform the state-of-the-art methods in the CSC task.</abstract>
      <url hash="c16669e5">2022.findings-acl.98</url>
      <bibkey>zhu-etal-2022-mdcspell</bibkey>
      <doi>10.18653/v1/2022.findings-acl.98</doi>
    </paper>
    <paper id="99">
      <title><fixed-case>S</fixed-case><tex-math>^2</tex-math><fixed-case>SQL</fixed-case>: Injecting Syntax to Question-Schema Interaction Graph Encoder for Text-to-<fixed-case>SQL</fixed-case> Parsers</title>
      <author><first>Binyuan</first><last>Hui</last></author>
      <author><first>Ruiying</first><last>Geng</last></author>
      <author><first>Lihan</first><last>Wang</last></author>
      <author><first>Bowen</first><last>Qin</last></author>
      <author><first>Yanyang</first><last>Li</last></author>
      <author><first>Bowen</first><last>Li</last></author>
      <author><first>Jian</first><last>Sun</last></author>
      <author><first>Yongbin</first><last>Li</last></author>
      <pages>1254-1262</pages>
      <abstract>The task of converting a natural language question into an executable SQL query, known as text-to-SQL, is an important branch of semantic parsing. The state-of-the-art graph-based encoder has been successfully used in this task but does not model the question syntax well. In this paper, we propose S<tex-math>^2</tex-math>SQL, injecting Syntax to question-Schema graph encoder for Text-to-SQL parsers, which effectively leverages the syntactic dependency information of questions in text-to-SQL to improve the performance. We also employ the decoupling constraint to induce diverse relational edge embedding, which further improves the network’s performance. Experiments on the Spider and robustness setting Spider-Syn demonstrate that the proposed approach outperforms all existing methods when pre-training models are used, resulting in a performance ranks first on the Spider leaderboard.</abstract>
      <url hash="441c333e">2022.findings-acl.99</url>
      <attachment type="software" hash="a9f19b91">2022.findings-acl.99.software.zip</attachment>
      <bibkey>hui-etal-2022-s2sql</bibkey>
      <doi>10.18653/v1/2022.findings-acl.99</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/spider-realistic">Spider-Realistic</pwcdataset>
    </paper>
    <paper id="100">
      <title>Constructing Open Cloze Tests Using Generation and Discrimination Capabilities of Transformers</title>
      <author><first>Mariano</first><last>Felice</last></author>
      <author><first>Shiva</first><last>Taslimipoor</last></author>
      <author><first>Paula</first><last>Buttery</last></author>
      <pages>1263-1273</pages>
      <abstract>This paper presents the first multi-objective transformer model for generating open cloze tests that exploits generation and discrimination capabilities to improve performance. Our model is further enhanced by tweaking its loss function and applying a post-processing re-ranking algorithm that improves overall test structure. Experiments using automatic and human evaluation show that our approach can achieve up to 82% accuracy according to experts, outperforming previous work and baselines. We also release a collection of high-quality open cloze tests along with sample system output and human annotations that can serve as a future benchmark.</abstract>
      <url hash="340484a3">2022.findings-acl.100</url>
      <bibkey>felice-etal-2022-constructing</bibkey>
      <doi>10.18653/v1/2022.findings-acl.100</doi>
    </paper>
    <paper id="101">
      <title><fixed-case>C</fixed-case>o-training an <fixed-case>U</fixed-case>nsupervised <fixed-case>C</fixed-case>onstituency <fixed-case>P</fixed-case>arser with <fixed-case>W</fixed-case>eak <fixed-case>S</fixed-case>upervision</title>
      <author><first>Nickil</first><last>Maveli</last></author>
      <author><first>Shay</first><last>Cohen</last></author>
      <pages>1274-1291</pages>
      <abstract>We introduce a method for unsupervised parsing that relies on bootstrapping classifiers to identify if a node dominates a specific span in a sentence. There are two types of classifiers, an inside classifier that acts on a span, and an outside classifier that acts on everything outside of a given span. Through self-training and co-training with the two classifiers, we show that the interplay between them helps improve the accuracy of both, and as a result, effectively parse. A seed bootstrapping technique prepares the data to train these classifiers. Our analyses further validate that such an approach in conjunction with weak supervision using prior branching knowledge of a known language (left/right-branching) and minimal heuristics injects strong inductive bias into the parser, achieving 63.1 F<tex-math>_1</tex-math> on the English (PTB) test set. In addition, we show the effectiveness of our architecture by evaluating on treebanks for Chinese (CTB) and Japanese (KTB) and achieve new state-of-the-art results.</abstract>
      <url hash="6960059b">2022.findings-acl.101</url>
      <bibkey>maveli-cohen-2022-co</bibkey>
      <doi>10.18653/v1/2022.findings-acl.101</doi>
      <video href="2022.findings-acl.101.mp4"/>
      <pwccode url="https://github.com/Nickil21/weakly-supervised-parsing" additional="false">Nickil21/weakly-supervised-parsing</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/chinese-treebank">Chinese Treebank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ptb">PTB Diagnostic ECG Database</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="102">
      <title><fixed-case>H</fixed-case>i<fixed-case>S</fixed-case>truct+: Improving Extractive Text Summarization with Hierarchical Structure Information</title>
      <author><first>Qian</first><last>Ruan</last></author>
      <author><first>Malte</first><last>Ostendorff</last></author>
      <author><first>Georg</first><last>Rehm</last></author>
      <pages>1292-1308</pages>
      <abstract>Transformer-based language models usually treat texts as linear sequences. However, most texts also have an inherent hierarchical structure, i.e., parts of a text can be identified using their position in this hierarchy. In addition, section titles usually indicate the common topic of their respective sentences. We propose a novel approach to formulate, extract, encode and inject hierarchical structure information explicitly into an extractive summarization model based on a pre-trained, encoder-only Transformer language model (HiStruct+ model), which improves SOTA ROUGEs for extractive summarization on PubMed and arXiv substantially. Using various experimental settings on three datasets (i.e., CNN/DailyMail, PubMed and arXiv), our HiStruct+ model outperforms a strong baseline collectively, which differs from our model only in that the hierarchical structure information is not injected. It is also observed that the more conspicuous hierarchical structure the dataset has, the larger improvements our method gains. The ablation study demonstrates that the hierarchical position information is the main contributor to our model’s SOTA performance.</abstract>
      <url hash="35aee5d5">2022.findings-acl.102</url>
      <attachment type="software" hash="21541437">2022.findings-acl.102.software.zip</attachment>
      <bibkey>ruan-etal-2022-histruct</bibkey>
      <doi>10.18653/v1/2022.findings-acl.102</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/arxiv">Arxiv HEP-TH citation graph</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/pubmed">Pubmed</pwcdataset>
    </paper>
    <paper id="103">
      <title>An Isotropy Analysis in the Multilingual <fixed-case>BERT</fixed-case> Embedding Space</title>
      <author><first>Sara</first><last>Rajaee</last></author>
      <author><first>Mohammad Taher</first><last>Pilehvar</last></author>
      <pages>1309-1316</pages>
      <abstract>Several studies have explored various advantages of multilingual pre-trained models (such as multilingual BERT) in capturing shared linguistic knowledge. However, less attention has been paid to their limitations. In this paper, we investigate the multilingual BERT for two known issues of the monolingual models: anisotropic embedding space and outlier dimensions. We show that, unlike its monolingual counterpart, the multilingual BERT model exhibits no outlier dimension in its representations while it has a highly anisotropic space. There are a few dimensions in the monolingual BERT with high contributions to the anisotropic distribution. However, we observe no such dimensions in the multilingual BERT. Furthermore, our experimental results demonstrate that increasing the isotropy of multilingual space can significantly improve its representation power and performance, similarly to what had been observed for monolingual CWRs on semantic similarity tasks. Our analysis indicates that, despite having different degenerated directions, the embedding spaces in various languages tend to be partially similar with respect to their structures.</abstract>
      <url hash="4da36a67">2022.findings-acl.103</url>
      <bibkey>rajaee-pilehvar-2022-isotropy</bibkey>
      <doi>10.18653/v1/2022.findings-acl.103</doi>
      <pwccode url="https://github.com/sara-rajaee/multilingual-isotropy" additional="false">sara-rajaee/multilingual-isotropy</pwccode>
    </paper>
    <paper id="104">
      <title>Multi-Stage Prompting for Knowledgeable Dialogue Generation</title>
      <author><first>Zihan</first><last>Liu</last></author>
      <author><first>Mostofa</first><last>Patwary</last></author>
      <author><first>Ryan</first><last>Prenger</last></author>
      <author><first>Shrimai</first><last>Prabhumoye</last></author>
      <author><first>Wei</first><last>Ping</last></author>
      <author><first>Mohammad</first><last>Shoeybi</last></author>
      <author><first>Bryan</first><last>Catanzaro</last></author>
      <pages>1317-1337</pages>
      <abstract>Existing knowledge-grounded dialogue systems typically use finetuned versions of a pretrained language model (LM) and large-scale knowledge bases. These models typically fail to generalize on topics outside of the knowledge base, and require maintaining separate potentially large checkpoints each time finetuning is needed. In this paper, we aim to address these limitations by leveraging the inherent knowledge stored in the pretrained LM as well as its powerful generation ability. We propose a multi-stage prompting approach to generate knowledgeable responses from a single pretrained LM. We first prompt the LM to generate knowledge based on the dialogue context. Then, we further prompt it to generate responses based on the dialogue context and the previously generated knowledge. Results show that our knowledge generator outperforms the state-of-the-art retrieval-based model by 5.8% when combining knowledge relevance and correctness. In addition, our multi-stage prompting outperforms the finetuning-based dialogue model in terms of response knowledgeability and engagement by up to 10% and 5%, respectively. Furthermore, we scale our model up to 530 billion parameters and demonstrate that larger LMs improve the generation correctness score by up to 10%, and response relevance, knowledgeability and engagement by up to 10%. Our code is available at: <url>https://github.com/NVIDIA/Megatron-LM</url>.</abstract>
      <url hash="e9a1c9c8">2022.findings-acl.104</url>
      <bibkey>liu-etal-2022-multi</bibkey>
      <doi>10.18653/v1/2022.findings-acl.104</doi>
      <pwccode url="https://github.com/NVIDIA/Megatron-LM" additional="false">NVIDIA/Megatron-LM</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wizard-of-wikipedia">Wizard of Wikipedia</pwcdataset>
    </paper>
    <paper id="105">
      <title><tex-math>\textrm{DuReader}_{\textrm{vis}}</tex-math>: A <fixed-case>C</fixed-case>hinese Dataset for Open-domain Document Visual Question Answering</title>
      <author><first>Le</first><last>Qi</last></author>
      <author><first>Shangwen</first><last>Lv</last></author>
      <author><first>Hongyu</first><last>Li</last></author>
      <author><first>Jing</first><last>Liu</last></author>
      <author><first>Yu</first><last>Zhang</last></author>
      <author><first>Qiaoqiao</first><last>She</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <author><first>Haifeng</first><last>Wang</last></author>
      <author><first>Ting</first><last>Liu</last></author>
      <pages>1338-1351</pages>
      <abstract>Open-domain question answering has been used in a wide range of applications, such as web search and enterprise search, which usually takes clean texts extracted from various formats of documents (e.g., web pages, PDFs, or Word documents) as the information source. However, designing different text extraction approaches is time-consuming and not scalable. In order to reduce human cost and improve the scalability of QA systems, we propose and study an <tex-math>\textbf{Open-domain}</tex-math>
 <tex-math>\textbf{Doc}</tex-math>ument <tex-math>\textbf{V}</tex-math>isual <tex-math>\textbf{Q}</tex-math>uestion <tex-math>\textbf{A}</tex-math>nswering (Open-domain DocVQA) task, which requires answering questions based on a collection of document images directly instead of only document texts, utilizing layouts and visual features additionally. Towards this end, we introduce the first Chinese Open-domain DocVQA dataset called <tex-math>\textrm{DuReader}_{\textrm{vis}}</tex-math>, containing about 15K question-answering pairs and 158K document images from the Baidu search engine. There are three main challenges in <tex-math>\textrm{DuReader}_{\textrm{vis}}</tex-math>: (1) long document understanding, (2) noisy texts, and (3) multi-span answer extraction. The extensive experiments demonstrate that the dataset is challenging. Additionally, we propose a simple approach that incorporates the layout and visual features, and the experimental results show the effectiveness of the proposed approach. The dataset and code will be publicly available at <url>https://github.com/baidu/DuReader/tree/master/DuReader-vis</url>.</abstract>
      <url hash="efec9c15">2022.findings-acl.105</url>
      <bibkey>qi-etal-2022-dureadervis</bibkey>
      <doi>10.18653/v1/2022.findings-acl.105</doi>
      <pwccode url="https://github.com/baidu/DuReader" additional="false">baidu/DuReader</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/docvqa">DocVQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/infographicvqa">InfographicVQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visualmrc">VisualMRC</pwcdataset>
    </paper>
    <paper id="106">
      <title>Coloring the Blank Slate: Pre-training Imparts a Hierarchical Inductive Bias to Sequence-to-sequence Models</title>
      <author><first>Aaron</first><last>Mueller</last></author>
      <author><first>Robert</first><last>Frank</last></author>
      <author><first>Tal</first><last>Linzen</last></author>
      <author><first>Luheng</first><last>Wang</last></author>
      <author><first>Sebastian</first><last>Schuster</last></author>
      <pages>1352-1368</pages>
      <abstract>Relations between words are governed by hierarchical structure rather than linear ordering. Sequence-to-sequence (seq2seq) models, despite their success in downstream NLP applications, often fail to generalize in a hierarchy-sensitive manner when performing syntactic transformations—for example, transforming declarative sentences into questions. However, syntactic evaluations of seq2seq models have only observed models that were not pre-trained on natural language data before being trained to perform syntactic transformations, in spite of the fact that pre-training has been found to induce hierarchical linguistic generalizations in language models; in other words, the syntactic capabilities of seq2seq models may have been greatly understated. We address this gap using the pre-trained seq2seq models T5 and BART, as well as their multilingual variants mT5 and mBART. We evaluate whether they generalize hierarchically on two transformations in two languages: question formation and passivization in English and German. We find that pre-trained seq2seq models generalize hierarchically when performing syntactic transformations, whereas models trained from scratch on syntactic transformations do not. This result presents evidence for the learnability of hierarchical syntactic information from non-annotated natural language text while also demonstrating that seq2seq models are capable of syntactic generalization, though only after exposure to much more language data than human learners receive.</abstract>
      <url hash="9a53b9e5">2022.findings-acl.106</url>
      <bibkey>mueller-etal-2022-coloring</bibkey>
      <doi>10.18653/v1/2022.findings-acl.106</doi>
      <video href="2022.findings-acl.106.mp4"/>
      <pwccode url="https://github.com/sebschu/multilingual-transformations" additional="false">sebschu/multilingual-transformations</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mc4">mC4</pwcdataset>
    </paper>
    <paper id="107">
      <title><fixed-case>C</fixed-case><tex-math>^3</tex-math><fixed-case>KG</fixed-case>: A <fixed-case>C</fixed-case>hinese Commonsense Conversation Knowledge Graph</title>
      <author><first>Dawei</first><last>Li</last></author>
      <author><first>Yanran</first><last>Li</last></author>
      <author><first>Jiayi</first><last>Zhang</last></author>
      <author><first>Ke</first><last>Li</last></author>
      <author><first>Chen</first><last>Wei</last></author>
      <author><first>Jianwei</first><last>Cui</last></author>
      <author><first>Bin</first><last>Wang</last></author>
      <pages>1369-1383</pages>
      <abstract>Existing commonsense knowledge bases often organize tuples in an isolated manner, which is deficient for commonsense conversational models to plan the next steps. To fill the gap, we curate a large-scale multi-turn human-written conversation corpus, and create the first Chinese commonsense conversation knowledge graph which incorporates both social commonsense knowledge and dialog flow information. To show the potential of our graph, we develop a graph-conversation matching approach, and benchmark two graph-grounded conversational tasks. All the resources in this work will be released to foster future research.</abstract>
      <url hash="e140b04a">2022.findings-acl.107</url>
      <bibkey>li-etal-2022-c3kg</bibkey>
      <doi>10.18653/v1/2022.findings-acl.107</doi>
      <pwccode url="https://github.com/xiaomi/c3kg" additional="false">xiaomi/c3kg</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/atomic">ATOMIC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mod-1">MOD</pwcdataset>
    </paper>
    <paper id="108">
      <title>Graph Neural Networks for Multiparallel Word Alignment</title>
      <author><first>Ayyoob</first><last>Imani</last></author>
      <author><first>Lütfi Kerem</first><last>Senel</last></author>
      <author><first>Masoud</first><last>Jalili Sabet</last></author>
      <author><first>François</first><last>Yvon</last></author>
      <author><first>Hinrich</first><last>Schuetze</last></author>
      <pages>1384-1396</pages>
      <abstract>After a period of decrease, interest in word alignments is increasing again for their usefulness in domains such as typological research, cross-lingual annotation projection and machine translation. Generally, alignment algorithms only use bitext and do not make use of the fact that many parallel corpora are multiparallel. Here, we compute high-quality word alignments between multiple language pairs by considering all language pairs together. First, we create a multiparallel word alignment graph, joining all bilingual word alignment pairs in one graph. Next, we use graph neural networks (GNNs) to exploit the graph structure. Our GNN approach (i) utilizes information about the meaning, position and language of the input words, (ii) incorporates information from multiple parallel sentences, (iii) adds and removes edges from the initial alignments, and (iv) yields a prediction model that can generalize beyond the training sentences. We show that community detection algorithms can provide valuable information for multiparallel word alignment. Our method outperforms previous work on three word alignment datasets and on a downstream task.</abstract>
      <url hash="c8f2efd4">2022.findings-acl.108</url>
      <bibkey>imani-etal-2022-graph</bibkey>
      <doi>10.18653/v1/2022.findings-acl.108</doi>
      <video href="2022.findings-acl.108.mp4"/>
    </paper>
    <paper id="109">
      <title>Sentiment Word Aware Multimodal Refinement for Multimodal Sentiment Analysis with <fixed-case>ASR</fixed-case> Errors</title>
      <author><first>Yang</first><last>Wu</last></author>
      <author><first>Yanyan</first><last>Zhao</last></author>
      <author><first>Hao</first><last>Yang</last></author>
      <author><first>Song</first><last>Chen</last></author>
      <author><first>Bing</first><last>Qin</last></author>
      <author><first>Xiaohuan</first><last>Cao</last></author>
      <author><first>Wenting</first><last>Zhao</last></author>
      <pages>1397-1406</pages>
      <abstract>Multimodal sentiment analysis has attracted increasing attention and lots of models have been proposed. However, the performance of the state-of-the-art models decreases sharply when they are deployed in the real world. We find that the main reason is that real-world applications can only access the text outputs by the automatic speech recognition (ASR) models, which may be with errors because of the limitation of model capacity. Through further analysis of the ASR outputs, we find that in some cases the sentiment words, the key sentiment elements in the textual modality, are recognized as other words, which makes the sentiment of the text change and hurts the performance of multimodal sentiment analysis models directly. To address this problem, we propose the sentiment word aware multimodal refinement model (SWRM), which can dynamically refine the erroneous sentiment words by leveraging multimodal sentiment clues. Specifically, we first use the sentiment word position detection module to obtain the most possible position of the sentiment word in the text and then utilize the multimodal sentiment word refinement module to dynamically refine the sentiment word embeddings. The refined embeddings are taken as the textual inputs of the multimodal feature fusion module to predict the sentiment labels. We conduct extensive experiments on the real-world datasets including MOSI-Speechbrain, MOSI-IBM, and MOSI-iFlytek and the results demonstrate the effectiveness of our model, which surpasses the current state-of-the-art models on three datasets. Furthermore, our approach can be adapted for other multimodal feature fusion models easily.</abstract>
      <url hash="c8c28ec6">2022.findings-acl.109</url>
      <bibkey>wu-etal-2022-sentiment</bibkey>
      <doi>10.18653/v1/2022.findings-acl.109</doi>
      <pwccode url="https://github.com/albertwy/SWRM" additional="false">albertwy/SWRM</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multimodal-opinionlevel-sentiment-intensity">Multimodal Opinionlevel Sentiment Intensity</pwcdataset>
    </paper>
    <paper id="110">
      <title>A Novel Framework Based on Medical Concept Driven Attention for Explainable Medical Code Prediction via External Knowledge</title>
      <author><first>Tao</first><last>Wang</last></author>
      <author><first>Linhai</first><last>Zhang</last></author>
      <author><first>Chenchen</first><last>Ye</last></author>
      <author><first>Junxi</first><last>Liu</last></author>
      <author><first>Deyu</first><last>Zhou</last></author>
      <pages>1407-1416</pages>
      <abstract>Medical code prediction from clinical notes aims at automatically associating medical codes with the clinical notes. Rare code problem, the medical codes with low occurrences, is prominent in medical code prediction. Recent studies employ deep neural networks and the external knowledge to tackle it. However, such approaches lack interpretability which is a vital issue in medical application. Moreover, due to the lengthy and noisy clinical notes, such approaches fail to achieve satisfactory results. Therefore, in this paper, we propose a novel framework based on medical concept driven attention to incorporate external knowledge for explainable medical code prediction. In specific, both the clinical notes and Wikipedia documents are aligned into topic space to extract medical concepts using topic modeling. Then, the medical concept-driven attention mechanism is applied to uncover the medical code related concepts which provide explanations for medical code prediction. Experimental results on the benchmark dataset show the superiority of the proposed framework over several state-of-the-art baselines.</abstract>
      <url hash="ccce9501">2022.findings-acl.110</url>
      <bibkey>wang-etal-2022-novel</bibkey>
      <doi>10.18653/v1/2022.findings-acl.110</doi>
    </paper>
    <paper id="111">
      <title>Effective Unsupervised Constrained Text Generation based on Perturbed Masking</title>
      <author><first>Yingwen</first><last>Fu</last></author>
      <author><first>Wenjie</first><last>Ou</last></author>
      <author><first>Zhou</first><last>Yu</last></author>
      <author><first>Yue</first><last>Lin</last></author>
      <pages>1417-1427</pages>
      <abstract>Unsupervised constrained text generation aims to generate text under a given set of constraints without any supervised data. Current state-of-the-art methods stochastically sample edit positions and actions, which may cause unnecessary search steps. In this paper, we propose PMCTG to improve effectiveness by searching for the best edit position and action in each step. Specifically, PMCTG extends perturbed masking technique to effectively search for the most incongruent token to edit. Then it introduces four multi-aspect scoring functions to select edit action to further reduce search difficulty. Since PMCTG does not require supervised data, it could be applied to different generation tasks. We show that under the unsupervised setting, PMCTG achieves new state-of-the-art results in two representative tasks, namely keywords- to-sentence generation and paraphrasing.</abstract>
      <url hash="540266d4">2022.findings-acl.111</url>
      <bibkey>fu-etal-2022-effective</bibkey>
      <doi>10.18653/v1/2022.findings-acl.111</doi>
    </paper>
    <paper id="112">
      <title>Combining (Second-Order) Graph-Based and Headed-Span-Based Projective Dependency Parsing</title>
      <author><first>Songlin</first><last>Yang</last></author>
      <author><first>Kewei</first><last>Tu</last></author>
      <pages>1428-1434</pages>
      <abstract>Graph-based methods, which decompose the score of a dependency tree into scores of dependency arcs, are popular in dependency parsing for decades. Recently, (CITATION) propose a headed-span-based method that decomposes the score of a dependency tree into scores of headed spans. They show improvement over first-order graph-based methods. However, their method does not score dependency arcs at all, and dependency arcs are implicitly induced by their cubic-time algorithm, which is possibly sub-optimal since modeling dependency arcs is intuitively useful. In this work, we aim to combine graph-based and headed-span-based methods, incorporating both arc scores and headed span scores into our model. First, we show a direct way to combine with <tex-math>O(n^4)</tex-math> parsing complexity. To decrease complexity, inspired by the classical head-splitting trick, we show two <tex-math>O(n^3)</tex-math> dynamic programming algorithms to combine first- and second-order graph-based and headed-span-based methods. Our experiments on PTB, CTB, and UD show that combining first-order graph-based and headed-span-based methods is effective. We also confirm the effectiveness of second-order graph-based parsing in the deep learning age, however, we observe marginal or no improvement when combining second-order graph-based and headed-span-based methods .</abstract>
      <url hash="89dac78d">2022.findings-acl.112</url>
      <bibkey>yang-tu-2022-combining</bibkey>
      <doi>10.18653/v1/2022.findings-acl.112</doi>
      <pwccode url="https://github.com/sustcsonglin/span-based-dependency-parsing" additional="false">sustcsonglin/span-based-dependency-parsing</pwccode>
    </paper>
    <paper id="113">
      <title>End-to-End Speech Translation for Code Switched Speech</title>
      <author><first>Orion</first><last>Weller</last></author>
      <author><first>Matthias</first><last>Sperber</last></author>
      <author><first>Telmo</first><last>Pires</last></author>
      <author><first>Hendra</first><last>Setiawan</last></author>
      <author><first>Christian</first><last>Gollan</last></author>
      <author><first>Dominic</first><last>Telaar</last></author>
      <author><first>Matthias</first><last>Paulik</last></author>
      <pages>1435-1448</pages>
      <abstract>Code switching (CS) refers to the phenomenon of interchangeably using words and phrases from different languages. CS can pose significant accuracy challenges to NLP, due to the often monolingual nature of the underlying systems. In this work, we focus on CS in the context of English/Spanish conversations for the task of speech translation (ST), generating and evaluating both transcript and translation. To evaluate model performance on this task, we create a novel ST corpus derived from existing public data sets. We explore various ST architectures across two dimensions: cascaded (transcribe then translate) vs end-to-end (jointly transcribe and translate) and unidirectional (source -&gt; target) vs bidirectional (source &lt;-&gt; target). We show that our ST architectures, and especially our bidirectional end-to-end architecture, perform well on CS speech, even when no CS training data is used.</abstract>
      <url hash="630890be">2022.findings-acl.113</url>
      <bibkey>weller-etal-2022-end</bibkey>
      <doi>10.18653/v1/2022.findings-acl.113</doi>
      <video href="2022.findings-acl.113.mp4"/>
      <pwccode url="https://github.com/apple/ml-code-switched-speech-translation" additional="true">apple/ml-code-switched-speech-translation</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/covost">CoVoST</pwcdataset>
    </paper>
    <paper id="114">
      <title>A Transformational Biencoder with In-Domain Negative Sampling for Zero-Shot Entity Linking</title>
      <author><first>Kai</first><last>Sun</last></author>
      <author><first>Richong</first><last>Zhang</last></author>
      <author><first>Samuel</first><last>Mensah</last></author>
      <author><first>Yongyi</first><last>Mao</last></author>
      <author><first>Xudong</first><last>Liu</last></author>
      <pages>1449-1458</pages>
      <abstract>Recent interest in entity linking has focused in the zero-shot scenario, where at test time the entity mention to be labelled is never seen during training, or may belong to a different domain from the source domain. Current work leverage pre-trained BERT with the implicit assumption that it bridges the gap between the source and target domain distributions. However, fine-tuned BERT has a considerable underperformance at zero-shot when applied in a different domain. We solve this problem by proposing a Transformational Biencoder that incorporates a transformation into BERT to perform a zero-shot transfer from the source domain during training. As like previous work, we rely on negative entities to encourage our model to discriminate the golden entities during training. To generate these negative entities, we propose a simple but effective strategy that takes the domain of the golden entity into perspective. Our experimental results on the benchmark dataset Zeshel show effectiveness of our approach and achieve new state-of-the-art.</abstract>
      <url hash="034dd78c">2022.findings-acl.114</url>
      <attachment type="software" hash="eecc7c72">2022.findings-acl.114.software.zip</attachment>
      <bibkey>sun-etal-2022-transformational</bibkey>
      <doi>10.18653/v1/2022.findings-acl.114</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/zeshel">ZESHEL</pwcdataset>
    </paper>
    <paper id="115">
      <title>Finding the Dominant Winning Ticket in Pre-Trained Language Models</title>
      <author><first>Zhuocheng</first><last>Gong</last></author>
      <author><first>Di</first><last>He</last></author>
      <author><first>Yelong</first><last>Shen</last></author>
      <author><first>Tie-Yan</first><last>Liu</last></author>
      <author><first>Weizhu</first><last>Chen</last></author>
      <author><first>Dongyan</first><last>Zhao</last></author>
      <author><first>Ji-Rong</first><last>Wen</last></author>
      <author><first>Rui</first><last>Yan</last></author>
      <pages>1459-1472</pages>
      <abstract>The Lottery Ticket Hypothesis suggests that for any over-parameterized model, a small subnetwork exists to achieve competitive performance compared to the backbone architecture. In this paper, we study whether there is a winning lottery ticket for pre-trained language models, which allow the practitioners to fine-tune the parameters in the ticket but achieve good downstream performance. To achieve this, we regularize the fine-tuning process with L1 distance and explore the subnetwork structure (what we refer to as the “dominant winning ticket”). Empirically, we show that (a) the dominant winning ticket can achieve performance that is comparable with that of the full-parameter model, (b) the dominant winning ticket is transferable across different tasks, (c) and the dominant winning ticket has a natural structure within each parameter matrix. Strikingly, we find that a dominant winning ticket that takes up 0.05% of the parameters can already achieve satisfactory performance, indicating that the PLM is significantly reducible during fine-tuning.</abstract>
      <url hash="dc13167d">2022.findings-acl.115</url>
      <bibkey>gong-etal-2022-finding</bibkey>
      <doi>10.18653/v1/2022.findings-acl.115</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
    </paper>
    <paper id="116">
      <title><fixed-case>T</fixed-case>hai Nested Named Entity Recognition Corpus</title>
      <author><first>Weerayut</first><last>Buaphet</last></author>
      <author><first>Can</first><last>Udomcharoenchaikit</last></author>
      <author><first>Peerat</first><last>Limkonchotiwat</last></author>
      <author><first>Attapol</first><last>Rutherford</last></author>
      <author><first>Sarana</first><last>Nutanong</last></author>
      <pages>1473-1486</pages>
      <abstract>This paper presents the first Thai Nested Named Entity Recognition (N-NER) dataset. Thai N-NER consists of 264,798 mentions, 104 classes, and a maximum depth of 8 layers obtained from 4,894 documents in the domains of news articles and restaurant reviews. Our work, to the best of our knowledge, presents the largest non-English N-NER dataset and the first non-English one with fine-grained classes. To understand the new challenges our proposed dataset brings to the field, we conduct an experimental study on (i) cutting edge N-NER models with the state-of-the-art accuracy in English and (ii) baseline methods based on well-known language model architectures. From the experimental results, we obtained two key findings. First, all models produced poor F1 scores in the tail region of the class distribution. There is little or no performance improvement provided by these models with respect to the baseline methods with our Thai dataset. These findings suggest that further investigation is required to make a multilingual N-NER solution that works well across different languages.</abstract>
      <url hash="7bfeb1fc">2022.findings-acl.116</url>
      <bibkey>buaphet-etal-2022-thai</bibkey>
      <doi>10.18653/v1/2022.findings-acl.116</doi>
      <video href="2022.findings-acl.116.mp4"/>
      <pwccode url="https://github.com/vistec-ai/thai-nner" additional="false">vistec-ai/thai-nner</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL 2003</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dan">DaN+</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/nne">NNE</pwcdataset>
    </paper>
    <paper id="117">
      <title>Two-Step Question Retrieval for Open-Domain <fixed-case>QA</fixed-case></title>
      <author><first>Yeon</first><last>Seonwoo</last></author>
      <author><first>Juhee</first><last>Son</last></author>
      <author><first>Jiho</first><last>Jin</last></author>
      <author><first>Sang-Woo</first><last>Lee</last></author>
      <author><first>Ji-Hoon</first><last>Kim</last></author>
      <author><first>Jung-Woo</first><last>Ha</last></author>
      <author><first>Alice</first><last>Oh</last></author>
      <pages>1487-1492</pages>
      <abstract>The retriever-reader pipeline has shown promising performance in open-domain QA but suffers from a very slow inference speed. Recently proposed question retrieval models tackle this problem by indexing question-answer pairs and searching for similar questions. These models have shown a significant increase in inference speed, but at the cost of lower QA performance compared to the retriever-reader models. This paper proposes a two-step question retrieval model, SQuID (Sequential Question-Indexed Dense retrieval) and distant supervision for training. SQuID uses two bi-encoders for question retrieval. The first-step retriever selects top-k similar questions, and the second-step retriever finds the most similar question from the top-k questions. We evaluate the performance and the computational efficiency of SQuID. The results show that SQuID significantly increases the performance of existing question retrieval models with a negligible loss on inference speed.</abstract>
      <url hash="aecb05fb">2022.findings-acl.117</url>
      <attachment type="software" hash="6ed1ed63">2022.findings-acl.117.software.zip</attachment>
      <bibkey>seonwoo-etal-2022-two</bibkey>
      <doi>10.18653/v1/2022.findings-acl.117</doi>
      <video href="2022.findings-acl.117.mp4"/>
      <pwccode url="https://github.com/yeonsw/squid" additional="false">yeonsw/squid</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paq">PAQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
    </paper>
    <paper id="118">
      <title>Semantically Distributed Robust Optimization for Vision-and-Language Inference</title>
      <author><first>Tejas</first><last>Gokhale</last></author>
      <author><first>Abhishek</first><last>Chaudhary</last></author>
      <author><first>Pratyay</first><last>Banerjee</last></author>
      <author><first>Chitta</first><last>Baral</last></author>
      <author><first>Yezhou</first><last>Yang</last></author>
      <pages>1493-1513</pages>
      <abstract>Analysis of vision-and-language models has revealed their brittleness under linguistic phenomena such as paraphrasing, negation, textual entailment, and word substitutions with synonyms or antonyms. While data augmentation techniques have been designed to mitigate against these failure modes, methods that can integrate this knowledge into the training pipeline remain under-explored. In this paper, we present <b>SDRO</b>, a model-agnostic method that utilizes a set linguistic transformations in a distributed robust optimization setting, along with an ensembling technique to leverage these transformations during inference.Experiments on benchmark datasets with images (NLVR<tex-math>^2</tex-math>) and video (VIOLIN) demonstrate performance improvements as well as robustness to adversarial attacks.Experiments on binary VQA explore the generalizability of this method to other V&amp;L tasks.</abstract>
      <url hash="6b0de454">2022.findings-acl.118</url>
      <bibkey>gokhale-etal-2022-semantically</bibkey>
      <doi>10.18653/v1/2022.findings-acl.118</doi>
      <pwccode url="https://github.com/asu-apg/vli_sdro" additional="false">asu-apg/vli_sdro</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/violin">Violin</pwcdataset>
    </paper>
    <paper id="119">
      <title>Learning from Missing Relations: Contrastive Learning with Commonsense Knowledge Graphs for Commonsense Inference</title>
      <author><first>Yong-Ho</first><last>Jung</last></author>
      <author><first>Jun-Hyung</first><last>Park</last></author>
      <author><first>Joon-Young</first><last>Choi</last></author>
      <author><first>Mingyu</first><last>Lee</last></author>
      <author><first>Junho</first><last>Kim</last></author>
      <author><first>Kang-Min</first><last>Kim</last></author>
      <author><first>SangKeun</first><last>Lee</last></author>
      <pages>1514-1523</pages>
      <abstract>Commonsense inference poses a unique challenge to reason and generate the physical, social, and causal conditions of a given event. Existing approaches to commonsense inference utilize commonsense transformers, which are large-scale language models that learn commonsense knowledge graphs. However, they suffer from a lack of coverage and expressive diversity of the graphs, resulting in a degradation of the representation quality. In this paper, we focus on addressing missing relations in commonsense knowledge graphs, and propose a novel contrastive learning framework called SOLAR. Our framework contrasts sets of semantically similar and dissimilar events, learning richer inferential knowledge compared to existing approaches. Empirical results demonstrate the efficacy of SOLAR in commonsense inference of diverse commonsense knowledge graphs. Specifically, SOLAR outperforms the state-of-the-art commonsense transformer on commonsense inference with ConceptNet by 1.84% on average among 8 automatic evaluation metrics. In-depth analysis of SOLAR sheds light on the effects of the missing relations utilized in learning commonsense knowledge graphs.</abstract>
      <url hash="ce9fc9a2">2022.findings-acl.119</url>
      <bibkey>jung-etal-2022-learning</bibkey>
      <doi>10.18653/v1/2022.findings-acl.119</doi>
      <video href="2022.findings-acl.119.mp4"/>
      <pwccode url="https://github.com/yongho94/solar-framework_commonsense-inference" additional="false">yongho94/solar-framework_commonsense-inference</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/event2mind">Event2Mind</pwcdataset>
    </paper>
    <paper id="120">
      <title>Capture Human Disagreement Distributions by Calibrated Networks for Natural Language Inference</title>
      <author><first>Yuxia</first><last>Wang</last></author>
      <author><first>Minghan</first><last>Wang</last></author>
      <author><first>Yimeng</first><last>Chen</last></author>
      <author><first>Shimin</first><last>Tao</last></author>
      <author><first>Jiaxin</first><last>Guo</last></author>
      <author><first>Chang</first><last>Su</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <author><first>Hao</first><last>Yang</last></author>
      <pages>1524-1535</pages>
      <abstract>Natural Language Inference (NLI) datasets contain examples with highly ambiguous labels due to its subjectivity. Several recent efforts have been made to acknowledge and embrace the existence of ambiguity, and explore how to capture the human disagreement distribution. In contrast with directly learning from gold ambiguity labels, relying on special resource, we argue that the model has naturally captured the human ambiguity distribution as long as it’s calibrated, i.e. the predictive probability can reflect the true correctness likelihood. Our experiments show that when model is well-calibrated, either by label smoothing or temperature scaling, it can obtain competitive performance as prior work, on both divergence scores between predictive probability and the true human opinion distribution, and the accuracy. This reveals the overhead of collecting gold ambiguity labels can be cut, by broadly solving how to calibrate the NLI network.</abstract>
      <url hash="27f686db">2022.findings-acl.120</url>
      <bibkey>wang-etal-2022-capture</bibkey>
      <doi>10.18653/v1/2022.findings-acl.120</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/chaosnli">ChaosNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
    </paper>
    <paper id="121">
      <title>Efficient, Uncertainty-based Moderation of Neural Networks Text Classifiers</title>
      <author><first>Jakob Smedegaard</first><last>Andersen</last></author>
      <author><first>Walid</first><last>Maalej</last></author>
      <pages>1536-1546</pages>
      <abstract>To maximize the accuracy and increase the overall acceptance of text classifiers, we propose a framework for the efficient, in-operation moderation of classifiers’ output. Our framework focuses on use cases in which F1-scores of modern Neural Networks classifiers (ca. 90%) are still inapplicable in practice. We suggest a semi-automated approach that uses prediction uncertainties to pass unconfident, probably incorrect classifications to human moderators. To minimize the workload, we limit the human moderated data to the point where the accuracy gains saturate and further human effort does not lead to substantial improvements. A series of benchmarking experiments based on three different datasets and three state-of-the-art classifiers show that our framework can improve the classification F1-scores by 5.1 to 11.2% (up to approx. 98 to 99%), while reducing the moderation load up to 73.3% compared to a random moderation.</abstract>
      <url hash="2b0f4afc">2022.findings-acl.121</url>
      <attachment type="software" hash="fdb8e0b4">2022.findings-acl.121.software.zip</attachment>
      <bibkey>andersen-maalej-2022-efficient</bibkey>
      <doi>10.18653/v1/2022.findings-acl.121</doi>
      <video href="2022.findings-acl.121.mp4"/>
      <pwccode url="https://github.com/jsandersen/cmt" additional="false">jsandersen/cmt</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="122">
      <title>Revisiting Automatic Evaluation of Extractive Summarization Task: Can We Do Better than <fixed-case>ROUGE</fixed-case>?</title>
      <author><first>Mousumi</first><last>Akter</last></author>
      <author><first>Naman</first><last>Bansal</last></author>
      <author><first>Shubhra Kanti</first><last>Karmaker</last></author>
      <pages>1547-1560</pages>
      <abstract>It has been the norm for a long time to evaluate automated summarization tasks using the popular ROUGE metric. Although several studies in the past have highlighted the limitations of ROUGE, researchers have struggled to reach a consensus on a better alternative until today. One major limitation of the traditional ROUGE metric is the lack of semantic understanding (relies on direct overlap of n-grams). In this paper, we exclusively focus on the extractive summarization task and propose a semantic-aware nCG (normalized cumulative gain)-based evaluation metric (called Sem-nCG) for evaluating this task. One fundamental contribution of the paper is that it demonstrates how we can generate more reliable semantic-aware ground truths for evaluating extractive summarization tasks without any additional human intervention. To the best of our knowledge, this work is the first of its kind. We have conducted extensive experiments with this new metric using the widely used CNN/DailyMail dataset. Experimental results show that the new Sem-nCG metric is indeed semantic-aware, shows higher correlation with human judgement (more reliable) and yields a large number of disagreements with the original ROUGE metric (suggesting that ROUGE often leads to inaccurate conclusions also verified by humans).</abstract>
      <url hash="1da33b09">2022.findings-acl.122</url>
      <bibkey>akter-etal-2022-revisiting</bibkey>
      <doi>10.18653/v1/2022.findings-acl.122</doi>
      <video href="2022.findings-acl.122.mp4"/>
    </paper>
    <paper id="123">
      <title>Open Vocabulary Extreme Classification Using Generative Models</title>
      <author><first>Daniel</first><last>Simig</last></author>
      <author><first>Fabio</first><last>Petroni</last></author>
      <author><first>Pouya</first><last>Yanki</last></author>
      <author><first>Kashyap</first><last>Popat</last></author>
      <author><first>Christina</first><last>Du</last></author>
      <author><first>Sebastian</first><last>Riedel</last></author>
      <author><first>Majid</first><last>Yazdani</last></author>
      <pages>1561-1583</pages>
      <abstract>The extreme multi-label classification (XMC) task aims at tagging content with a subset of labels from an extremely large label set. The label vocabulary is typically defined in advance by domain experts and assumed to capture all necessary tags. However in real world scenarios this label set, although large, is often incomplete and experts frequently need to refine it. To develop systems that simplify this process, we introduce the task of open vocabulary XMC (OXMC): given a piece of content, predict a set of labels, some of which may be outside of the known tag set. Hence, in addition to not having training data for some labels–as is the case in zero-shot classification–models need to invent some labels on-thefly. We propose GROOV, a fine-tuned seq2seq model for OXMC that generates the set of labels as a flat sequence and is trained using a novel loss independent of predicted label order. We show the efficacy of the approach, experimenting with popular XMC datasets for which GROOV is able to predict meaningful labels outside the given vocabulary while performing on par with state-of-the-art solutions for known labels.</abstract>
      <url hash="33d9845e">2022.findings-acl.123</url>
      <bibkey>simig-etal-2022-open</bibkey>
      <doi>10.18653/v1/2022.findings-acl.123</doi>
      <video href="2022.findings-acl.123.mp4"/>
    </paper>
    <paper id="124">
      <title>Decomposed Meta-Learning for Few-Shot Named Entity Recognition</title>
      <author><first>Tingting</first><last>Ma</last></author>
      <author><first>Huiqiang</first><last>Jiang</last></author>
      <author><first>Qianhui</first><last>Wu</last></author>
      <author><first>Tiejun</first><last>Zhao</last></author>
      <author><first>Chin-Yew</first><last>Lin</last></author>
      <pages>1584-1596</pages>
      <abstract>Few-shot named entity recognition (NER) systems aim at recognizing novel-class named entities based on only a few labeled examples. In this paper, we present a decomposed meta-learning approach which addresses the problem of few-shot NER by sequentially tackling few-shot span detection and few-shot entity typing using meta-learning. In particular, we take the few-shot span detection as a sequence labeling problem and train the span detector by introducing the model-agnostic meta-learning (MAML) algorithm to find a good model parameter initialization that could fast adapt to new entity classes. For few-shot entity typing, we propose MAML-ProtoNet, i.e., MAML-enhanced prototypical networks to find a good embedding space that can better distinguish text span representations from different entity classes. Extensive experiments on various benchmarks show that our approach achieves superior performance over prior methods.</abstract>
      <url hash="cc1f91ee">2022.findings-acl.124</url>
      <attachment type="software" hash="f0183f32">2022.findings-acl.124.software.zip</attachment>
      <bibkey>ma-etal-2022-decomposed</bibkey>
      <doi>10.18653/v1/2022.findings-acl.124</doi>
      <pwccode url="https://github.com/microsoft/vert-papers" additional="false">microsoft/vert-papers</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2002">CoNLL 2002</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/few-nerd">Few-NERD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ontonotes-5-0">OntoNotes 5.0</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wnut-2017-emerging-and-rare-entity">WNUT 2017</pwcdataset>
    </paper>
    <paper id="125">
      <title><fixed-case>T</fixed-case>eg<fixed-case>T</fixed-case>ok: Augmenting Text Generation via Task-specific and Open-world Knowledge</title>
      <author><first>Chao-Hong</first><last>Tan</last></author>
      <author><first>Jia-Chen</first><last>Gu</last></author>
      <author><first>Chongyang</first><last>Tao</last></author>
      <author><first>Zhen-Hua</first><last>Ling</last></author>
      <author><first>Can</first><last>Xu</last></author>
      <author><first>Huang</first><last>Hu</last></author>
      <author><first>Xiubo</first><last>Geng</last></author>
      <author><first>Daxin</first><last>Jiang</last></author>
      <pages>1597-1609</pages>
      <abstract>Generating natural and informative texts has been a long-standing problem in NLP. Much effort has been dedicated into incorporating pre-trained language models (PLMs) with various open-world knowledge, such as knowledge graphs or wiki pages. However, their ability to access and manipulate the task-specific knowledge is still limited on downstream tasks, as this type of knowledge is usually not well covered in PLMs and is hard to acquire. To address the problem, we propose augmenting TExt Generation via Task-specific and Open-world Knowledge (TegTok) in a unified framework. Our model selects knowledge entries from two types of knowledge sources through dense retrieval and then injects them into the input encoding and output decoding stages respectively on the basis of PLMs. With the help of these two types of knowledge, our model can learn what and how to generate. Experiments on two text generation tasks of dialogue generation and question generation, and on two datasets show that our method achieves better performance than various baseline models.</abstract>
      <url hash="50290aea">2022.findings-acl.125</url>
      <bibkey>tan-etal-2022-tegtok</bibkey>
      <doi>10.18653/v1/2022.findings-acl.125</doi>
      <pwccode url="https://github.com/lxchtan/tegtok" additional="false">lxchtan/tegtok</pwccode>
    </paper>
    <paper id="126">
      <title><fixed-case>E</fixed-case>mo<fixed-case>C</fixed-case>aps: Emotion Capsule based Model for Conversational Emotion Recognition</title>
      <author><first>Zaijing</first><last>Li</last></author>
      <author><first>Fengxiao</first><last>Tang</last></author>
      <author><first>Ming</first><last>Zhao</last></author>
      <author><first>Yusen</first><last>Zhu</last></author>
      <pages>1610-1618</pages>
      <abstract>Emotion recognition in conversation (ERC) aims to analyze the speaker’s state and identify their emotion in the conversation. Recent works in ERC focus on context modeling but ignore the representation of contextual emotional tendency. In order to extract multi-modal information and the emotional tendency of the utterance effectively, we propose a new structure named Emoformer to extract multi-modal emotion vectors from different modalities and fuse them with sentence vector to be an emotion capsule. Furthermore, we design an end-to-end ERC model called EmoCaps, which extracts emotion vectors through the Emoformer structure and obtain the emotion classification results from a context analysis model. Through the experiments with two benchmark datasets, our model shows better performance than the existing state-of-the-art models.</abstract>
      <url hash="747bc2e6">2022.findings-acl.126</url>
      <attachment type="software" hash="d5d998e9">2022.findings-acl.126.software.zip</attachment>
      <bibkey>li-etal-2022-emocaps</bibkey>
      <doi>10.18653/v1/2022.findings-acl.126</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/iemocap">IEMOCAP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/meld">MELD</pwcdataset>
    </paper>
    <paper id="127">
      <title>Logic-Driven Context Extension and Data Augmentation for Logical Reasoning of Text</title>
      <author><first>Siyuan</first><last>Wang</last></author>
      <author><first>Wanjun</first><last>Zhong</last></author>
      <author><first>Duyu</first><last>Tang</last></author>
      <author><first>Zhongyu</first><last>Wei</last></author>
      <author><first>Zhihao</first><last>Fan</last></author>
      <author><first>Daxin</first><last>Jiang</last></author>
      <author><first>Ming</first><last>Zhou</last></author>
      <author><first>Nan</first><last>Duan</last></author>
      <pages>1619-1629</pages>
      <abstract>Logical reasoning of text requires identifying critical logical structures in the text and performing inference over them. Existing methods for logical reasoning mainly focus on contextual semantics of text while struggling to explicitly model the logical inference process. In this paper, we not only put forward a logic-driven context extension framework but also propose a logic-driven data augmentation algorithm. The former follows a three-step reasoning paradigm, and each step is respectively to extract logical expressions as elementary reasoning units, symbolically infer the implicit expressions following equivalence laws and extend the context to validate the options. The latter augments literally similar but logically different instances and incorporates contrastive learning to better capture logical information, especially logical negative and conditional relationships. We conduct experiments on two benchmark datasets, ReClor and LogiQA. The results show that our method achieves state-of-the-art performance on both datasets, and even surpasses human performance on the ReClor dataset.</abstract>
      <url hash="579b2036">2022.findings-acl.127</url>
      <attachment type="software" hash="d2fab7a1">2022.findings-acl.127.software.zip</attachment>
      <bibkey>wang-etal-2022-logic</bibkey>
      <doi>10.18653/v1/2022.findings-acl.127</doi>
      <pwccode url="https://github.com/WangsyGit/LReasoner" additional="true">WangsyGit/LReasoner</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/reclor">ReClor</pwcdataset>
    </paper>
    <paper id="128">
      <title>Transfer Learning and Prediction Consistency for Detecting Offensive Spans of Text</title>
      <author><first>Amir</first><last>Pouran Ben Veyseh</last></author>
      <author><first>Ning</first><last>Xu</last></author>
      <author><first>Quan</first><last>Tran</last></author>
      <author><first>Varun</first><last>Manjunatha</last></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Thien</first><last>Nguyen</last></author>
      <pages>1630-1637</pages>
      <abstract>Toxic span detection is the task of recognizing offensive spans in a text snippet. Although there has been prior work on classifying text snippets as offensive or not, the task of recognizing spans responsible for the toxicity of a text is not explored yet. In this work, we introduce a novel multi-task framework for toxic span detection in which the model seeks to simultaneously predict offensive words and opinion phrases to leverage their inter-dependencies and improve the performance. Moreover, we introduce a novel regularization mechanism to encourage the consistency of the model predictions across similar inputs for toxic span detection. Our extensive experiments demonstrate the effectiveness of the proposed model compared to strong baselines.</abstract>
      <url hash="555ea717">2022.findings-acl.128</url>
      <attachment type="software" hash="2cfb564e">2022.findings-acl.128.software.zip</attachment>
      <bibkey>pouran-ben-veyseh-etal-2022-transfer</bibkey>
      <doi>10.18653/v1/2022.findings-acl.128</doi>
    </paper>
    <paper id="129">
      <title>Learning Reasoning Patterns for Relational Triple Extraction with Mutual Generation of Text and Graph</title>
      <author><first>Yubo</first><last>Chen</last></author>
      <author><first>Yunqi</first><last>Zhang</last></author>
      <author><first>Yongfeng</first><last>Huang</last></author>
      <pages>1638-1647</pages>
      <abstract>Relational triple extraction is a critical task for constructing knowledge graphs. Existing methods focused on learning text patterns from explicit relational mentions. However, they usually suffered from ignoring relational reasoning patterns, thus failed to extract the implicitly implied triples. Fortunately, the graph structure of a sentence’s relational triples can help find multi-hop reasoning paths. Moreover, the type inference logic through the paths can be captured with the sentence’s supplementary relational expressions that represent the real-world conceptual meanings of the paths’ composite relations. In this paper, we propose a unified framework to learn the relational reasoning patterns for this task. To identify multi-hop reasoning paths, we construct a relational graph from the sentence (text-to-graph generation) and apply multi-layer graph convolutions to it. To capture the relation type inference logic of the paths, we propose to understand the unlabeled conceptual expressions by reconstructing the sentence from the relational graph (graph-to-text generation) in a self-supervised manner. Experimental results on several benchmark datasets demonstrate the effectiveness of our method.</abstract>
      <url hash="f200a93e">2022.findings-acl.129</url>
      <bibkey>chen-etal-2022-learning</bibkey>
      <doi>10.18653/v1/2022.findings-acl.129</doi>
    </paper>
    <paper id="130">
      <title>Document-Level Event Argument Extraction via Optimal Transport</title>
      <author><first>Amir</first><last>Pouran Ben Veyseh</last></author>
      <author><first>Minh Van</first><last>Nguyen</last></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Bonan</first><last>Min</last></author>
      <author><first>Thien</first><last>Nguyen</last></author>
      <pages>1648-1658</pages>
      <abstract>Event Argument Extraction (EAE) is one of the sub-tasks of event extraction, aiming to recognize the role of each entity mention toward a specific event trigger. Despite the success of prior works in sentence-level EAE, the document-level setting is less explored. In particular, whereas syntactic structures of sentences have been shown to be effective for sentence-level EAE, prior document-level EAE models totally ignore syntactic structures for documents. Hence, in this work, we study the importance of syntactic structures in document-level EAE. Specifically, we propose to employ Optimal Transport (OT) to induce structures of documents based on sentence-level syntactic structures and tailored to EAE task. Furthermore, we propose a novel regularization technique to explicitly constrain the contributions of unrelated context words in the final prediction for EAE. We perform extensive experiments on the benchmark document-level EAE dataset RAMS that leads to the state-of-the-art performance. Moreover, our experiments on the ACE 2005 dataset reveals the effectiveness of the proposed model in the sentence-level EAE by establishing new state-of-the-art results.</abstract>
      <url hash="d82688c1">2022.findings-acl.130</url>
      <attachment type="software" hash="e2c7df95">2022.findings-acl.130.software.zip</attachment>
      <bibkey>pouran-ben-veyseh-etal-2022-document</bibkey>
      <doi>10.18653/v1/2022.findings-acl.130</doi>
    </paper>
    <paper id="131">
      <title>N-Shot Learning for Augmenting Task-Oriented Dialogue State Tracking</title>
      <author><first>Ibrahim</first><last>Aksu</last></author>
      <author><first>Zhengyuan</first><last>Liu</last></author>
      <author><first>Min-Yen</first><last>Kan</last></author>
      <author><first>Nancy</first><last>Chen</last></author>
      <pages>1659-1671</pages>
      <abstract>Augmentation of task-oriented dialogues has followed standard methods used for plain-text such as back-translation, word-level manipulation, and paraphrasing despite its richly annotated structure. In this work, we introduce an augmentation framework that utilizes belief state annotations to match turns from various dialogues and form new synthetic dialogues in a bottom-up manner. Unlike other augmentation strategies, it operates with as few as five examples. Our augmentation strategy yields significant improvements when both adapting a DST model to a new domain, and when adapting a language model to the DST task, on evaluations with TRADE and TOD-BERT models. Further analysis shows that our model performs better on seen values during training, and it is also more robust to unseen values. We conclude that exploiting belief state annotations enhances dialogue augmentation and results in improved models in n-shot training scenarios.</abstract>
      <url hash="0e632aa9">2022.findings-acl.131</url>
      <bibkey>aksu-etal-2022-n</bibkey>
      <doi>10.18653/v1/2022.findings-acl.131</doi>
      <video href="2022.findings-acl.131.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/multiwoz">MultiWOZ</pwcdataset>
    </paper>
    <paper id="132">
      <title>Document-Level Relation Extraction with Adaptive Focal Loss and Knowledge Distillation</title>
      <author><first>Qingyu</first><last>Tan</last></author>
      <author><first>Ruidan</first><last>He</last></author>
      <author><first>Lidong</first><last>Bing</last></author>
      <author><first>Hwee Tou</first><last>Ng</last></author>
      <pages>1672-1681</pages>
      <abstract>Document-level Relation Extraction (DocRE) is a more challenging task compared to its sentence-level counterpart. It aims to extract relations from multiple sentences at once. In this paper, we propose a semi-supervised framework for DocRE with three novel components. Firstly, we use an axial attention module for learning the interdependency among entity-pairs, which improves the performance on two-hop relations. Secondly, we propose an adaptive focal loss to tackle the class imbalance problem of DocRE. Lastly, we use knowledge distillation to overcome the differences between human annotated data and distantly supervised data. We conducted experiments on two DocRE datasets. Our model consistently outperforms strong baselines and its performance exceeds the previous SOTA by 1.36 F1 and 1.46 Ign_F1 score on the DocRED leaderboard.</abstract>
      <url hash="2d20ba84">2022.findings-acl.132</url>
      <attachment type="software" hash="d9e434b3">2022.findings-acl.132.software.zip</attachment>
      <bibkey>tan-etal-2022-document</bibkey>
      <doi>10.18653/v1/2022.findings-acl.132</doi>
      <pwccode url="https://github.com/tonytan48/kd-docre" additional="false">tonytan48/kd-docre</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/docred">DocRED</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/re-docred">Re-DocRED</pwcdataset>
    </paper>
    <paper id="133">
      <title>Calibration of Machine Reading Systems at Scale</title>
      <author><first>Shehzaad</first><last>Dhuliawala</last></author>
      <author><first>Leonard</first><last>Adolphs</last></author>
      <author><first>Rajarshi</first><last>Das</last></author>
      <author><first>Mrinmaya</first><last>Sachan</last></author>
      <pages>1682-1693</pages>
      <abstract>In typical machine learning systems, an estimate of the probability of the prediction is used to assess the system’s confidence in the prediction. This confidence measure is usually uncalibrated; i.e. the system’s confidence in the prediction does not match the true probability of the predicted output. In this paper, we present an investigation into calibrating open setting machine reading systemssuch as open-domain question answering and claim verification systems. We show that calibrating such complex systems which contain discrete retrieval and deep reading components is challenging and current calibration techniques fail to scale to these settings. We propose simple extensions to existing calibration approaches that allows us to adapt them to these settings. Our experimental results reveal that the approach works well, and can be useful to selectively predict answers when question answering systems are posed with unanswerable or out-of-the-training distribution questions.</abstract>
      <url hash="b3b952b0">2022.findings-acl.133</url>
      <bibkey>dhuliawala-etal-2022-calibration</bibkey>
      <doi>10.18653/v1/2022.findings-acl.133</doi>
      <video href="2022.findings-acl.133.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
    </paper>
    <paper id="134">
      <title>Towards Adversarially Robust Text Classifiers by Learning to Reweight Clean Examples</title>
      <author><first>Jianhan</first><last>Xu</last></author>
      <author><first>Cenyuan</first><last>Zhang</last></author>
      <author><first>Xiaoqing</first><last>Zheng</last></author>
      <author><first>Linyang</first><last>Li</last></author>
      <author><first>Cho-Jui</first><last>Hsieh</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <author><first>Xuanjing</first><last>Huang</last></author>
      <pages>1694-1707</pages>
      <abstract>Most of the existing defense methods improve the adversarial robustness by making the models adapt to the training set augmented with some adversarial examples. However, the augmented adversarial examples may not be natural, which might distort the training distribution, resulting in inferior performance both in clean accuracy and adversarial robustness. In this study, we explore the feasibility of introducing a reweighting mechanism to calibrate the training distribution to obtain robust models. We propose to train text classifiers by a sample reweighting method in which the example weights are learned to minimize the loss of a validation set mixed with the clean examples and their adversarial ones in an online learning manner. Through extensive experiments, we show that there exists a reweighting mechanism to make the models more robust against adversarial attacks without the need to craft the adversarial examples for the entire training set.</abstract>
      <url hash="e4b68c0a">2022.findings-acl.134</url>
      <attachment type="software" hash="9a1d1e50">2022.findings-acl.134.software.zip</attachment>
      <bibkey>xu-etal-2022-towards</bibkey>
      <doi>10.18653/v1/2022.findings-acl.134</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst-2">SST-2</pwcdataset>
    </paper>
    <paper id="135">
      <title>Morphosyntactic Tagging with Pre-trained Language Models for <fixed-case>A</fixed-case>rabic and its Dialects</title>
      <author><first>Go</first><last>Inoue</last></author>
      <author><first>Salam</first><last>Khalifa</last></author>
      <author><first>Nizar</first><last>Habash</last></author>
      <pages>1708-1719</pages>
      <abstract>We present state-of-the-art results on morphosyntactic tagging across different varieties of Arabic using fine-tuned pre-trained transformer language models. Our models consistently outperform existing systems in Modern Standard Arabic and all the Arabic dialects we study, achieving 2.6% absolute improvement over the previous state-of-the-art in Modern Standard Arabic, 2.8% in Gulf, 1.6% in Egyptian, and 8.3% in Levantine. We explore different training setups for fine-tuning pre-trained transformer language models, including training data size, the use of external linguistic resources, and the use of annotated data from other dialects in a low-resource scenario. Our results show that strategic fine-tuning using datasets from other high-resource dialects is beneficial for a low-resource dialect. Additionally, we show that high-quality morphological analyzers as external linguistic resources are beneficial especially in low-resource settings.</abstract>
      <url hash="2d0842f4">2022.findings-acl.135</url>
      <bibkey>inoue-etal-2022-morphosyntactic</bibkey>
      <doi>10.18653/v1/2022.findings-acl.135</doi>
      <video href="2022.findings-acl.135.mp4"/>
      <pwccode url="https://github.com/camel-lab/camelbert_morphosyntactic_tagger" additional="false">camel-lab/camelbert_morphosyntactic_tagger</pwccode>
    </paper>
    <paper id="136">
      <title>How Pre-trained Language Models Capture Factual Knowledge? A Causal-Inspired Analysis</title>
      <author><first>Shaobo</first><last>Li</last></author>
      <author><first>Xiaoguang</first><last>Li</last></author>
      <author><first>Lifeng</first><last>Shang</last></author>
      <author><first>Zhenhua</first><last>Dong</last></author>
      <author><first>Chengjie</first><last>Sun</last></author>
      <author><first>Bingquan</first><last>Liu</last></author>
      <author><first>Zhenzhou</first><last>Ji</last></author>
      <author><first>Xin</first><last>Jiang</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <pages>1720-1732</pages>
      <abstract>Recently, there has been a trend to investigate the factual knowledge captured by Pre-trained Language Models (PLMs). Many works show the PLMs’ ability to fill in the missing factual words in cloze-style prompts such as ”Dante was born in [MASK].” However, it is still a mystery how PLMs generate the results correctly: relying on effective clues or shortcut patterns? We try to answer this question by a causal-inspired analysis that quantitatively measures and evaluates the word-level patterns that PLMs depend on to generate the missing words. We check the words that have three typical associations with the missing words: knowledge-dependent, positionally close, and highly co-occurred. Our analysis shows: (1) PLMs generate the missing factual words more by the positionally close and highly co-occurred words than the knowledge-dependent words; (2) the dependence on the knowledge-dependent words is more effective than the positionally close and highly co-occurred words. Accordingly, we conclude that the PLMs capture the factual knowledge ineffectively because of depending on the inadequate associations.</abstract>
      <url hash="2bc28acb">2022.findings-acl.136</url>
      <bibkey>li-etal-2022-pre</bibkey>
      <doi>10.18653/v1/2022.findings-acl.136</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/lama">LAMA</pwcdataset>
    </paper>
    <paper id="137">
      <title>Metadata Shaping: A Simple Approach for Knowledge-Enhanced Language Models</title>
      <author><first>Simran</first><last>Arora</last></author>
      <author><first>Sen</first><last>Wu</last></author>
      <author><first>Enci</first><last>Liu</last></author>
      <author><first>Christopher</first><last>Re</last></author>
      <pages>1733-1745</pages>
      <abstract>Popular language models (LMs) struggle to capture knowledge about rare tail facts and entities. Since widely used systems such as search and personal-assistants must support the long tail of entities that users ask about, there has been significant effort towards enhancing these base LMs with factual knowledge. We observe proposed methods typically start with a base LM and data that has been annotated with entity metadata, then change the model, by modifying the architecture or introducing auxiliary loss terms to better capture entity knowledge. In this work, we question this typical process and ask to what extent can we match the quality of model modifications, with a simple alternative: using a base LM and only changing the data. We propose metadata shaping, a method which inserts substrings corresponding to the readily available entity metadata, e.g. types and descriptions, into examples at train and inference time based on mutual information. Despite its simplicity, metadata shaping is quite effective. On standard evaluation benchmarks for knowledge-enhanced LMs, the method exceeds the base-LM baseline by an average of 4.3 F1 points and achieves state-of-the-art results. We further show the gains are on average 4.4x larger for the slice of examples containing tail vs. popular entities.</abstract>
      <url hash="be7d0b7a">2022.findings-acl.137</url>
      <bibkey>arora-etal-2022-metadata</bibkey>
      <doi>10.18653/v1/2022.findings-acl.137</doi>
      <video href="2022.findings-acl.137.mp4"/>
      <pwccode url="https://github.com/simran-arora/metadatashaping" additional="false">simran-arora/metadatashaping</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel">FewRel</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/open-entity-1">Open Entity</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tacred">TACRED</pwcdataset>
    </paper>
    <paper id="138">
      <title>Enhancing Natural Language Representation with Large-Scale Out-of-Domain Commonsense</title>
      <author><first>Wanyun</first><last>Cui</last></author>
      <author><first>Xingran</first><last>Chen</last></author>
      <pages>1746-1756</pages>
      <abstract>We study how to enhance text representation via textual commonsense. We point out that commonsense has the nature of domain discrepancy. Namely, commonsense has different data formats and is domain-independent from the downstream task. This nature brings challenges to introducing commonsense in general text understanding tasks. A typical method of introducing textual knowledge is continuing pre-training over the commonsense corpus. However, it will cause catastrophic forgetting to the downstream task due to the domain discrepancy. In addition, previous methods of directly using textual descriptions as extra input information cannot apply to large-scale commonsense. In this paper, we propose to use large-scale out-of-domain commonsense to enhance text representation. In order to effectively incorporate the commonsense, we proposed OK-Transformer (Out-of-domain Knowledge enhanced Transformer). OK-Transformer effectively integrates commonsense descriptions and enhances them to the target text representation. In addition, OK-Transformer can adapt to the Transformer-based language models (e.g. BERT, RoBERTa) for free, without pre-training on large-scale unsupervised corpora. We have verified the effectiveness of OK-Transformer in multiple applications such as commonsense reasoning, general text classification, and low-resource commonsense settings.</abstract>
      <url hash="de8ffff0">2022.findings-acl.138</url>
      <bibkey>cui-chen-2022-enhancing</bibkey>
      <doi>10.18653/v1/2022.findings-acl.138</doi>
      <pwccode url="https://github.com/chenxran/ok-transformer" additional="false">chenxran/ok-transformer</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/commonsenseqa">CommonsenseQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wsc">WSC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/winogrande">WinoGrande</pwcdataset>
    </paper>
    <paper id="139">
      <title>Weighted self Distillation for <fixed-case>C</fixed-case>hinese word segmentation</title>
      <author><first>Rian</first><last>He</last></author>
      <author><first>Shubin</first><last>Cai</last></author>
      <author><first>Zhong</first><last>Ming</last></author>
      <author><first>Jialei</first><last>Zhang</last></author>
      <pages>1757-1770</pages>
      <abstract>Recent researches show that multi-criteria resources and n-gram features are beneficial to Chinese Word Segmentation (CWS). However, these methods rely heavily on such additional information mentioned above and focus less on the model itself. We thus propose a novel neural framework, named Weighted self Distillation for Chinese word segmentation (WeiDC). The framework, which only requires unigram features, adopts self-distillation technology with four hand-crafted weight modules and two teacher models configurations. Experiment results show that WeiDC can make use of character features to learn contextual knowledge and successfully achieve state-of-the-art or competitive performance in terms of strictly closed test settings on SIGHAN Bakeoff benchmark datasets. Moreover, further experiments and analyses also demonstrate the robustness of WeiDC. Source codes of this paper are available on Github.</abstract>
      <url hash="201ddc2d">2022.findings-acl.139</url>
      <attachment type="software" hash="d5cb4ee0">2022.findings-acl.139.software.zip</attachment>
      <bibkey>he-etal-2022-weighted</bibkey>
      <doi>10.18653/v1/2022.findings-acl.139</doi>
      <pwccode url="https://github.com/anzi20/weidc" additional="false">anzi20/weidc</pwccode>
    </paper>
    <paper id="140">
      <title>Sibylvariant Transformations for Robust Text Classification</title>
      <author><first>Fabrice</first><last>Harel-Canada</last></author>
      <author><first>Muhammad Ali</first><last>Gulzar</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <author><first>Miryung</first><last>Kim</last></author>
      <pages>1771-1788</pages>
      <abstract>The vast majority of text transformation techniques in NLP are inherently limited in their ability to expand input space coverage due to an implicit constraint to preserve the original class label. In this work, we propose the notion of sibylvariance (SIB) to describe the broader set of transforms that relax the label-preserving constraint, knowably vary the expected class, and lead to significantly more diverse input distributions. We offer a unified framework to organize all data transformations, including two types of SIB: (1) Transmutations convert one discrete kind into another, (2) Mixture Mutations blend two or more classes together. To explore the role of sibylvariance within NLP, we implemented 41 text transformations, including several novel techniques like Concept2Sentence and SentMix. Sibylvariance also enables a unique form of adaptive training that generates new input mixtures for the most confused class pairs, challenging the learner to differentiate with greater nuance. Our experiments on six benchmark datasets strongly support the efficacy of sibylvariance for generalization performance, defect detection, and adversarial robustness.</abstract>
      <url hash="941e183d">2022.findings-acl.140</url>
      <bibkey>harel-canada-etal-2022-sibylvariant</bibkey>
      <doi>10.18653/v1/2022.findings-acl.140</doi>
      <video href="2022.findings-acl.140.mp4"/>
      <pwccode url="https://github.com/ucla-seal/sibyl" additional="false">ucla-seal/sibyl</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst-2">SST-2</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/yahoo-answers">Yahoo! Answers</pwcdataset>
    </paper>
    <paper id="141">
      <title><fixed-case>D</fixed-case>a<fixed-case>LC</fixed-case>: Domain Adaptation Learning Curve Prediction for Neural Machine Translation</title>
      <author><first>Cheonbok</first><last>Park</last></author>
      <author><first>Hantae</first><last>Kim</last></author>
      <author><first>Ioan</first><last>Calapodescu</last></author>
      <author><first>Hyun Chang</first><last>Cho</last></author>
      <author><first>Vassilina</first><last>Nikoulina</last></author>
      <pages>1789-1807</pages>
      <abstract>Domain Adaptation (DA) of Neural Machine Translation (NMT) model often relies on a pre-trained general NMT model which is adapted to the new domain on a sample of in-domain parallel data. Without parallel data, there is no way to estimate the potential benefit of DA, nor the amount of parallel samples it would require. It is however a desirable functionality that could help MT practitioners to make an informed decision before investing resources in dataset creation. We propose a Domain adaptation Learning Curve prediction (DaLC) model that predicts prospective DA performance based on in-domain monolingual samples in the source language. Our model relies on the NMT encoder representations combined with various instance and corpus-level features. We demonstrate that instance-level is better able to distinguish between different domains compared to corpus-level frameworks proposed in previous studies Finally, we perform in-depth analyses of the results highlighting the limitations of our approach, and provide directions for future research.</abstract>
      <url hash="1a30adf6">2022.findings-acl.141</url>
      <bibkey>park-etal-2022-dalc</bibkey>
      <doi>10.18653/v1/2022.findings-acl.141</doi>
    </paper>
    <paper id="142">
      <title>Hey <fixed-case>AI</fixed-case>, Can You Solve Complex Tasks by Talking to Agents?</title>
      <author><first>Tushar</first><last>Khot</last></author>
      <author><first>Kyle</first><last>Richardson</last></author>
      <author><first>Daniel</first><last>Khashabi</last></author>
      <author><first>Ashish</first><last>Sabharwal</last></author>
      <pages>1808-1823</pages>
      <abstract>Training giant models from scratch for each complex task is resource- and data-inefficient. To help develop models that can leverage existing systems, we propose a new challenge: Learning to solve complex tasks by communicating with existing agents (or models) in natural language. We design a synthetic benchmark, CommaQA, with three complex reasoning tasks (explicit, implicit, numeric) designed to be solved by communicating with existing QA agents. For instance, using text and table QA agents to answer questions such as “Who had the longest javelin throw from USA?”. We show that black-box models struggle to learn this task from scratch (accuracy under 50%) even with access to each agent’s knowledge and gold facts supervision. In contrast, models that learn to communicate with agents outperform black-box models, reaching scores of 100% when given gold decomposition supervision. However, we show that the challenge of learning to solve complex tasks by communicating with existing agents <i>without relying on any auxiliary supervision or data</i> still remains highly elusive. We will release CommaQA, along with a compositional generalization test split, to advance research in this direction.</abstract>
      <url hash="cf9af099">2022.findings-acl.142</url>
      <bibkey>khot-etal-2022-hey</bibkey>
      <doi>10.18653/v1/2022.findings-acl.142</doi>
      <video href="2022.findings-acl.142.mp4"/>
      <pwccode url="https://github.com/allenai/commaqa" additional="false">allenai/commaqa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mathqa">MathQA</pwcdataset>
    </paper>
    <paper id="143">
      <title>Modality-specific Learning Rates for Effective Multimodal Additive Late-fusion</title>
      <author><first>Yiqun</first><last>Yao</last></author>
      <author><first>Rada</first><last>Mihalcea</last></author>
      <pages>1824-1834</pages>
      <abstract>In multimodal machine learning, additive late-fusion is a straightforward approach to combine the feature representations from different modalities, in which the final prediction can be formulated as the sum of unimodal predictions. While it has been found that certain late-fusion models can achieve competitive performance with lower computational costs compared to complex multimodal interactive models, how to effectively search for a good late-fusion model is still an open question. Moreover, for different modalities, the best unimodal models may work under significantly different learning rates due to the nature of the modality and the computational flow of the model; thus, selecting a global learning rate for late-fusion models can result in a vanishing gradient for some modalities. To help address these issues, we propose a Modality-Specific Learning Rate (MSLR) method to effectively build late-fusion multimodal models from fine-tuned unimodal models. We investigate three different strategies to assign learning rates to different modalities. Our experiments show that MSLR outperforms global learning rates on multiple tasks and settings, and enables the models to effectively learn each modality.</abstract>
      <url hash="7b9dc305">2022.findings-acl.143</url>
      <bibkey>yao-mihalcea-2022-modality</bibkey>
      <doi>10.18653/v1/2022.findings-acl.143</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/meld">MELD</pwcdataset>
    </paper>
    <paper id="144">
      <title><fixed-case>B</fixed-case>i<fixed-case>S</fixed-case>yn-<fixed-case>GAT</fixed-case>+: Bi-Syntax Aware Graph Attention Network for Aspect-based Sentiment Analysis</title>
      <author><first>Shuo</first><last>Liang</last></author>
      <author><first>Wei</first><last>Wei</last></author>
      <author><first>Xian-Ling</first><last>Mao</last></author>
      <author><first>Fei</first><last>Wang</last></author>
      <author><first>Zhiyong</first><last>He</last></author>
      <pages>1835-1848</pages>
      <abstract>Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis task that aims to align aspects and corresponding sentiments for aspect-specific sentiment polarity inference. It is challenging because a sentence may contain multiple aspects or complicated (e.g., conditional, coordinating, or adversative) relations. Recently, exploiting dependency syntax information with graph neural networks has been the most popular trend. Despite its success, methods that heavily rely on the dependency tree pose challenges in accurately modeling the alignment of the aspects and their words indicative of sentiment, since the dependency tree may provide noisy signals of unrelated associations (e.g., the “conj” relation between “great” and “dreadful” in Figure 2). In this paper, to alleviate this problem, we propose a Bi-Syntax aware Graph Attention Network (BiSyn-GAT+). Specifically, BiSyn-GAT+ fully exploits the syntax information (e.g., phrase segmentation and hierarchical structure) of the constituent tree of a sentence to model the sentiment-aware context of every single aspect (called intra-context) and the sentiment relations across aspects (called inter-context) for learning. Experiments on four benchmark datasets demonstrate that BiSyn-GAT+ outperforms the state-of-the-art methods consistently.</abstract>
      <url hash="b49fda84">2022.findings-acl.144</url>
      <bibkey>liang-etal-2022-bisyn</bibkey>
      <doi>10.18653/v1/2022.findings-acl.144</doi>
      <pwccode url="https://github.com/CCIIPLab/BiSyn_GAT_plus" additional="false">CCIIPLab/BiSyn_GAT_plus</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mams">MAMS</pwcdataset>
    </paper>
    <paper id="145">
      <title><fixed-case>I</fixed-case>ndic<fixed-case>BART</fixed-case>: A Pre-trained Model for Indic Natural Language Generation</title>
      <author><first>Raj</first><last>Dabre</last></author>
      <author><first>Himani</first><last>Shrotriya</last></author>
      <author><first>Anoop</first><last>Kunchukuttan</last></author>
      <author><first>Ratish</first><last>Puduppully</last></author>
      <author><first>Mitesh</first><last>Khapra</last></author>
      <author><first>Pratyush</first><last>Kumar</last></author>
      <pages>1849-1863</pages>
      <abstract>In this paper, we study pre-trained sequence-to-sequence models for a group of related languages, with a focus on Indic languages. We present IndicBART, a multilingual, sequence-to-sequence pre-trained model focusing on 11 Indic languages and English. IndicBART utilizes the orthographic similarity between Indic scripts to improve transfer learning between similar Indic languages. We evaluate IndicBART on two NLG tasks: Neural Machine Translation (NMT) and extreme summarization. Our experiments on NMT and extreme summarization show that a model specific to related languages like IndicBART is competitive with large pre-trained models like mBART50 despite being significantly smaller. It also performs well on very low-resource translation scenarios where languages are not included in pre-training or fine-tuning. Script sharing, multilingual training, and better utilization of limited model capacity contribute to the good performance of the compact IndicBART model.</abstract>
      <url hash="ad155507">2022.findings-acl.145</url>
      <bibkey>dabre-etal-2022-indicbart</bibkey>
      <doi>10.18653/v1/2022.findings-acl.145</doi>
      <video href="2022.findings-acl.145.mp4"/>
      <pwccode url="https://github.com/AI4Bharat/indic-bart" additional="false">AI4Bharat/indic-bart</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/flores">FLoRes</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/flores-101">FLoRes-101</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/indiccorp">IndicCorp</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/samanantar">Samanantar</pwcdataset>
    </paper>
    <paper id="146">
      <title>Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models</title>
      <author><first>Jianmo</first><last>Ni</last></author>
      <author><first>Gustavo</first><last>Hernandez Abrego</last></author>
      <author><first>Noah</first><last>Constant</last></author>
      <author><first>Ji</first><last>Ma</last></author>
      <author><first>Keith</first><last>Hall</last></author>
      <author><first>Daniel</first><last>Cer</last></author>
      <author><first>Yinfei</first><last>Yang</last></author>
      <pages>1864-1874</pages>
      <abstract>We provide the first exploration of sentence embeddings from text-to-text transformers (T5) including the effects of scaling up sentence encoders to 11B parameters. Sentence embeddings are broadly useful for language processing tasks. While T5 achieves impressive performance on language tasks, it is unclear how to produce sentence embeddings from encoder-decoder models. We investigate three methods to construct Sentence-T5 (ST5) models: two utilize only the T5 encoder and one using the full T5 encoder-decoder. We establish a new sentence representation transfer benchmark, SentGLUE, which extends the SentEval toolkit to nine tasks from the GLUE benchmark. Our encoder-only models outperform the previous best models on both SentEval and SentGLUE transfer tasks, including semantic textual similarity (STS). Scaling up ST5 from millions to billions of parameters shown to consistently improve performance. Finally, our encoder-decoder method achieves a new state-of-the-art on STS when using sentence embeddings.</abstract>
      <url hash="bc7bd944">2022.findings-acl.146</url>
      <bibkey>ni-etal-2022-sentence</bibkey>
      <doi>10.18653/v1/2022.findings-acl.146</doi>
      <video href="2022.findings-acl.146.mp4"/>
      <pwccode url="" additional="true"/>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/reqa">ReQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/senteval">SentEval</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="147">
      <title>Improving Relation Extraction through Syntax-induced Pre-training with Dependency Masking</title>
      <author><first>Yuanhe</first><last>Tian</last></author>
      <author><first>Yan</first><last>Song</last></author>
      <author><first>Fei</first><last>Xia</last></author>
      <pages>1875-1886</pages>
      <abstract>Relation extraction (RE) is an important natural language processing task that predicts the relation between two given entities, where a good understanding of the contextual information is essential to achieve an outstanding model performance. Among different types of contextual information, the auto-generated syntactic information (namely, word dependencies) has shown its effectiveness for the task. However, most existing studies require modifications to the existing baseline architectures (e.g., adding new components, such as GCN, on the top of an encoder) to leverage the syntactic information. To offer an alternative solution, we propose to leverage syntactic information to improve RE by training a syntax-induced encoder on auto-parsed data through dependency masking. Specifically, the syntax-induced encoder is trained by recovering the masked dependency connections and types in first, second, and third orders, which significantly differs from existing studies that train language models or word embeddings by predicting the context words along the dependency paths. Experimental results on two English benchmark datasets, namely, ACE2005EN and SemEval 2010 Task 8 datasets, demonstrate the effectiveness of our approach for RE, where our approach outperforms strong baselines and achieve state-of-the-art results on both datasets.</abstract>
      <url hash="544e937d">2022.findings-acl.147</url>
      <attachment type="software" hash="473352cb">2022.findings-acl.147.software.zip</attachment>
      <bibkey>tian-etal-2022-improving</bibkey>
      <revision id="1" href="2022.findings-acl.147v1" hash="1490f450"/>
      <revision id="2" href="2022.findings-acl.147v2" hash="544e937d" date="2022-05-18">Updated code link in footnote.</revision>
      <doi>10.18653/v1/2022.findings-acl.147</doi>
      <pwccode url="https://github.com/synlp/RE-DMP" additional="false">synlp/RE-DMP</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/semeval-2010-task-8">SemEval-2010 Task-8</pwcdataset>
    </paper>
    <paper id="148">
      <title>Striking a Balance: Alleviating Inconsistency in Pre-trained Models for Symmetric Classification Tasks</title>
      <author><first>Ashutosh</first><last>Kumar</last></author>
      <author><first>Aditya</first><last>Joshi</last></author>
      <pages>1887-1895</pages>
      <abstract>While fine-tuning pre-trained models for downstream classification is the conventional paradigm in NLP, often task-specific nuances may not get captured in the resultant models. Specifically, for tasks that take two inputs and require the output to be invariant of the order of the inputs, inconsistency is often observed in the predicted labels or confidence scores. We highlight this model shortcoming and apply a consistency loss function to alleviate inconsistency in symmetric classification. Our results show an improved consistency in predictions for three paraphrase detection datasets without a significant drop in the accuracy scores. We examine the classification performance of six datasets (both symmetric and non-symmetric) to showcase the strengths and limitations of our approach.</abstract>
      <url hash="4063f841">2022.findings-acl.148</url>
      <attachment type="software" hash="610a5f56">2022.findings-acl.148.software.zip</attachment>
      <bibkey>kumar-joshi-2022-striking</bibkey>
      <doi>10.18653/v1/2022.findings-acl.148</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paws">PAWS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst-2">SST-2</pwcdataset>
    </paper>
    <paper id="149">
      <title>Diversifying Content Generation for Commonsense Reasoning with Mixture of Knowledge Graph Experts</title>
      <author><first>Wenhao</first><last>Yu</last></author>
      <author><first>Chenguang</first><last>Zhu</last></author>
      <author><first>Lianhui</first><last>Qin</last></author>
      <author><first>Zhihan</first><last>Zhang</last></author>
      <author><first>Tong</first><last>Zhao</last></author>
      <author><first>Meng</first><last>Jiang</last></author>
      <pages>1896-1906</pages>
      <abstract>Generative commonsense reasoning (GCR) in natural language is to reason about the commonsense while generating coherent text. Recent years have seen a surge of interest in improving the generation quality of commonsense reasoning tasks. Nevertheless, these approaches have seldom investigated diversity in the GCR tasks, which aims to generate alternative explanations for a real-world situation or predict all possible outcomes. Diversifying GCR is challenging as it expects to generate multiple outputs that are not only semantically different but also grounded in commonsense knowledge. In this paper, we propose MoKGE, a novel method that diversifies the generative reasoning by a mixture of expert (MoE) strategy on commonsense knowledge graphs (KG). A set of knowledge experts seek diverse reasoning on KG to encourage various generation outputs. Empirical experiments demonstrated that MoKGE can significantly improve the diversity while achieving on par performance on accuracy on two GCR benchmarks, based on both automatic and human evaluations.</abstract>
      <url hash="0f1f93d2">2022.findings-acl.149</url>
      <bibkey>yu-etal-2022-diversifying</bibkey>
      <doi>10.18653/v1/2022.findings-acl.149</doi>
    </paper>
    <paper id="150">
      <title>Dict-<fixed-case>BERT</fixed-case>: Enhancing Language Model Pre-training with Dictionary</title>
      <author><first>Wenhao</first><last>Yu</last></author>
      <author><first>Chenguang</first><last>Zhu</last></author>
      <author><first>Yuwei</first><last>Fang</last></author>
      <author><first>Donghan</first><last>Yu</last></author>
      <author><first>Shuohang</first><last>Wang</last></author>
      <author><first>Yichong</first><last>Xu</last></author>
      <author><first>Michael</first><last>Zeng</last></author>
      <author><first>Meng</first><last>Jiang</last></author>
      <pages>1907-1918</pages>
      <abstract>Pre-trained language models (PLMs) aim to learn universal language representations by conducting self-supervised training tasks on large-scale corpora. Since PLMs capture word semantics in different contexts, the quality of word representations highly depends on word frequency, which usually follows a heavy-tailed distributions in the pre-training corpus. Therefore, the embeddings of rare words on the tail are usually poorly optimized. In this work, we focus on enhancing language model pre-training by leveraging definitions of the rare words in dictionaries (e.g., Wiktionary). To incorporate a rare word definition as a part of input, we fetch its definition from the dictionary and append it to the end of the input text sequence. In addition to training with the masked language modeling objective, we propose two novel self-supervised pre-training tasks on word and sentence-level alignment between input text sequence and rare word definitions to enhance language modeling representation with dictionary. We evaluate the proposed Dict-BERT model on the language understanding benchmark GLUE and eight specialized domain benchmark datasets. Extensive experiments demonstrate that Dict-BERT can significantly improve the understanding of rare words and boost model performance on various NLP downstream tasks.</abstract>
      <url hash="c59a5f28">2022.findings-acl.150</url>
      <bibkey>yu-etal-2022-dict</bibkey>
      <doi>10.18653/v1/2022.findings-acl.150</doi>
      <pwccode url="https://huggingface.co/wyu1/DictBERT" additional="false">wyu1/DictBERT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wnlampro">WNLaMPro</pwcdataset>
    </paper>
    <paper id="151">
      <title>A Feasibility Study of Answer-Agnostic Question Generation for Education</title>
      <author><first>Liam</first><last>Dugan</last></author>
      <author><first>Eleni</first><last>Miltsakaki</last></author>
      <author><first>Shriyash</first><last>Upadhyay</last></author>
      <author><first>Etan</first><last>Ginsberg</last></author>
      <author><first>Hannah</first><last>Gonzalez</last></author>
      <author><first>DaHyeon</first><last>Choi</last></author>
      <author><first>Chuning</first><last>Yuan</last></author>
      <author><first>Chris</first><last>Callison-Burch</last></author>
      <pages>1919-1926</pages>
      <abstract>We conduct a feasibility study into the applicability of answer-agnostic question generation models to textbook passages. We show that a significant portion of errors in such systems arise from asking irrelevant or un-interpretable questions and that such errors can be ameliorated by providing summarized input. We find that giving these models human-written summaries instead of the original text results in a significant increase in acceptability of generated questions (33% <tex-math>\rightarrow</tex-math> 83%) as determined by expert annotators. We also find that, in the absence of human-written summaries, automatic summarization can serve as a good middle ground.</abstract>
      <url hash="c5c12bf2">2022.findings-acl.151</url>
      <bibkey>dugan-etal-2022-feasibility</bibkey>
      <doi>10.18653/v1/2022.findings-acl.151</doi>
      <pwccode url="https://github.com/liamdugan/summary-qg" additional="false">liamdugan/summary-qg</pwccode>
    </paper>
    <paper id="152">
      <title>Relevant <fixed-case>C</fixed-case>ommon<fixed-case>S</fixed-case>ense Subgraphs for “What if...” Procedural Reasoning</title>
      <author><first>Chen</first><last>Zheng</last></author>
      <author><first>Parisa</first><last>Kordjamshidi</last></author>
      <pages>1927-1933</pages>
      <abstract>We study the challenge of learning causal reasoning over procedural text to answer “What if...” questions when external commonsense knowledge is required. We propose a novel multi-hop graph reasoning model to 1) efficiently extract a commonsense subgraph with the most relevant information from a large knowledge graph; 2) predict the causal answer by reasoning over the representations obtained from the commonsense subgraph and the contextual interactions between the questions and context. We evaluate our model on WIQA benchmark and achieve state-of-the-art performance compared to the recent models.</abstract>
      <url hash="ebc5e748">2022.findings-acl.152</url>
      <bibkey>zheng-kordjamshidi-2022-relevant</bibkey>
      <doi>10.18653/v1/2022.findings-acl.152</doi>
      <video href="2022.findings-acl.152.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wiqa">WIQA</pwcdataset>
    </paper>
    <paper id="153">
      <title>Combining Feature and Instance Attribution to Detect Artifacts</title>
      <author><first>Pouya</first><last>Pezeshkpour</last></author>
      <author><first>Sarthak</first><last>Jain</last></author>
      <author><first>Sameer</first><last>Singh</last></author>
      <author><first>Byron</first><last>Wallace</last></author>
      <pages>1934-1946</pages>
      <abstract>Training the deep neural networks that dominate NLP requires large datasets. These are often collected automatically or via crowdsourcing, and may exhibit systematic biases or annotation artifacts. By the latter we mean spurious correlations between inputs and outputs that do not represent a generally held causal relationship between features and classes; models that exploit such correlations may appear to perform a given task well, but fail on out of sample data. In this paper, we evaluate use of different attribution methods for aiding identification of training data artifacts. We propose new hybrid approaches that combine saliency maps (which highlight important input features) with instance attribution methods (which retrieve training samples influential to a given prediction). We show that this proposed training-feature attribution can be used to efficiently uncover artifacts in training data when a challenging validation set is available. We also carry out a small user study to evaluate whether these methods are useful to NLP researchers in practice, with promising results. We make code for all methods and experiments in this paper available.</abstract>
      <url hash="648a8d6a">2022.findings-acl.153</url>
      <bibkey>pezeshkpour-etal-2022-combining</bibkey>
      <doi>10.18653/v1/2022.findings-acl.153</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/boolq">BoolQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="154">
      <title>Leveraging Expert Guided Adversarial Augmentation For Improving Generalization in Named Entity Recognition</title>
      <author><first>Aaron</first><last>Reich</last></author>
      <author><first>Jiaao</first><last>Chen</last></author>
      <author><first>Aastha</first><last>Agrawal</last></author>
      <author><first>Yanzhe</first><last>Zhang</last></author>
      <author><first>Diyi</first><last>Yang</last></author>
      <pages>1947-1955</pages>
      <abstract>Named Entity Recognition (NER) systems often demonstrate great performance on in-distribution data, but perform poorly on examples drawn from a shifted distribution. One way to evaluate the generalization ability of NER models is to use adversarial examples, on which the specific variations associated with named entities are rarely considered. To this end, we propose leveraging expert-guided heuristics to change the entity tokens and their surrounding contexts thereby altering their entity types as adversarial attacks. Using expert-guided heuristics, we augmented the CoNLL 2003 test set and manually annotated it to construct a high-quality challenging set. We found that state-of-the-art NER systems trained on CoNLL 2003 training data drop performance dramatically on our challenging set. By training on adversarial augmented training examples and using mixup for regularization, we were able to significantly improve the performance on the challenging set as well as improve out-of-domain generalization which we evaluated by using OntoNotes data. We have publicly released our dataset and code at <url>https://github.com/GT-SALT/Guided-Adversarial-Augmentation</url>.</abstract>
      <url hash="a1167047">2022.findings-acl.154</url>
      <attachment type="software" hash="1438e5c9">2022.findings-acl.154.software.zip</attachment>
      <bibkey>reich-etal-2022-leveraging</bibkey>
      <doi>10.18653/v1/2022.findings-acl.154</doi>
      <video href="2022.findings-acl.154.mp4"/>
      <pwccode url="https://github.com/gt-salt/guided-adversarial-augmentation" additional="false">gt-salt/guided-adversarial-augmentation</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL 2003</pwcdataset>
    </paper>
    <paper id="155">
      <title>Label Semantics for Few Shot Named Entity Recognition</title>
      <author><first>Jie</first><last>Ma</last></author>
      <author><first>Miguel</first><last>Ballesteros</last></author>
      <author><first>Srikanth</first><last>Doss</last></author>
      <author><first>Rishita</first><last>Anubhai</last></author>
      <author><first>Sunil</first><last>Mallya</last></author>
      <author><first>Yaser</first><last>Al-Onaizan</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>1956-1971</pages>
      <abstract>We study the problem of few shot learning for named entity recognition. Specifically, we leverage the semantic information in the names of the labels as a way of giving the model additional signal and enriched priors. We propose a neural architecture that consists of two BERT encoders, one to encode the document and its tokens and another one to encode each of the labels in natural language format. Our model learns to match the representations of named entities computed by the first encoder with label representations computed by the second encoder. The label semantics signal is shown to support improved state-of-the-art results in multiple few shot NER benchmarks and on-par performance in standard benchmarks. Our model is especially effective in low resource settings.</abstract>
      <url hash="a7cbbc10">2022.findings-acl.155</url>
      <bibkey>ma-etal-2022-label</bibkey>
      <doi>10.18653/v1/2022.findings-acl.155</doi>
      <video href="2022.findings-acl.155.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL 2003</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ncbi-disease-1">NCBI Disease</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wnut-2017-emerging-and-rare-entity">WNUT 2017</pwcdataset>
    </paper>
    <paper id="156">
      <title>Detection, Disambiguation, Re-ranking: Autoregressive Entity Linking as a Multi-Task Problem</title>
      <author><first>Khalil</first><last>Mrini</last></author>
      <author><first>Shaoliang</first><last>Nie</last></author>
      <author><first>Jiatao</first><last>Gu</last></author>
      <author><first>Sinong</first><last>Wang</last></author>
      <author><first>Maziar</first><last>Sanjabi</last></author>
      <author><first>Hamed</first><last>Firooz</last></author>
      <pages>1972-1983</pages>
      <abstract>We propose an autoregressive entity linking model, that is trained with two auxiliary tasks, and learns to re-rank generated samples at inference time. Our proposed novelties address two weaknesses in the literature. First, a recent method proposes to learn mention detection and then entity candidate selection, but relies on predefined sets of candidates. We use encoder-decoder autoregressive entity linking in order to bypass this need, and propose to train mention detection as an auxiliary task instead. Second, previous work suggests that re-ranking could help correct prediction errors. We add a new, auxiliary task, match prediction, to learn re-ranking. Without the use of a knowledge base or candidate sets, our model sets a new state of the art in two benchmark datasets of entity linking: COMETA in the biomedical domain, and AIDA-CoNLL in the news domain. We show through ablation studies that each of the two auxiliary tasks increases performance, and that re-ranking is an important factor to the increase. Finally, our low-resource experimental results suggest that performance on the main task benefits from the knowledge learned by the auxiliary tasks, and not just from the additional training data.</abstract>
      <url hash="f1136e7f">2022.findings-acl.156</url>
      <bibkey>mrini-etal-2022-detection</bibkey>
      <doi>10.18653/v1/2022.findings-acl.156</doi>
      <video href="2022.findings-acl.156.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/aida-conll-yago">AIDA CoNLL-YAGO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cometa">COMETA</pwcdataset>
    </paper>
    <paper id="157">
      <title><fixed-case>VISITRON</fixed-case>: Visual Semantics-Aligned Interactively Trained Object-Navigator</title>
      <author><first>Ayush</first><last>Shrivastava</last></author>
      <author><first>Karthik</first><last>Gopalakrishnan</last></author>
      <author id="yang-liu-icsi"><first>Yang</first><last>Liu</last></author>
      <author><first>Robinson</first><last>Piramuthu</last></author>
      <author><first>Gokhan</first><last>Tur</last></author>
      <author><first>Devi</first><last>Parikh</last></author>
      <author><first>Dilek</first><last>Hakkani-Tur</last></author>
      <pages>1984-1994</pages>
      <abstract>Interactive robots navigating photo-realistic environments need to be trained to effectively leverage and handle the dynamic nature of dialogue in addition to the challenges underlying vision-and-language navigation (VLN). In this paper, we present VISITRON, a multi-modal Transformer-based navigator better suited to the interactive regime inherent to Cooperative Vision-and-Dialog Navigation (CVDN). VISITRON is trained to: i) identify and associate object-level concepts and semantics between the environment and dialogue history, ii) identify when to interact vs. navigate via imitation learning of a binary classification head. We perform extensive pre-training and fine-tuning ablations with VISITRON to gain empirical insights and improve performance on CVDN. VISITRON’s ability to identify when to interact leads to a natural generalization of the game-play mode introduced by Roman et al. (2020) for enabling the use of such models in different environments. VISITRON is competitive with models on the static CVDN leaderboard and attains state-of-the-art performance on the Success weighted by Path Length (SPL) metric.</abstract>
      <url hash="2fe39c00">2022.findings-acl.157</url>
      <bibkey>shrivastava-etal-2022-visitron</bibkey>
      <doi>10.18653/v1/2022.findings-acl.157</doi>
      <video href="2022.findings-acl.157.mp4"/>
      <pwccode url="https://github.com/alexa/visitron" additional="false">alexa/visitron</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/matterport3d">Matterport3D</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/rxr">RxR</pwcdataset>
    </paper>
    <paper id="158">
      <title>Investigating Selective Prediction Approaches Across Several Tasks in <fixed-case>IID</fixed-case>, <fixed-case>OOD</fixed-case>, and Adversarial Settings</title>
      <author><first>Neeraj</first><last>Varshney</last></author>
      <author><first>Swaroop</first><last>Mishra</last></author>
      <author><first>Chitta</first><last>Baral</last></author>
      <pages>1995-2002</pages>
      <abstract>In order to equip NLP systems with ‘selective prediction’ capability, several task-specific approaches have been proposed. However, which approaches work best across tasks or even if they consistently outperform the simplest baseline MaxProb remains to be explored. To this end, we systematically study selective prediction in a large-scale setup of 17 datasets across several NLP tasks. Through comprehensive experiments under in-domain (IID), out-of-domain (OOD), and adversarial (ADV) settings, we show that despite leveraging additional resources (held-out data/computation), none of the existing approaches consistently and considerably outperforms MaxProb in all three settings. Furthermore, their performance does not translate well across tasks. For instance, Monte-Carlo Dropout outperforms all other approaches on Duplicate Detection datasets but does not fare well on NLI datasets, especially in the OOD setting. Thus, we recommend that future selective prediction approaches should be evaluated across tasks and settings for reliable estimation of their capabilities.</abstract>
      <url hash="81ccc0aa">2022.findings-acl.158</url>
      <attachment type="software" hash="64c1717d">2022.findings-acl.158.software.zip</attachment>
      <bibkey>varshney-etal-2022-investigating</bibkey>
      <doi>10.18653/v1/2022.findings-acl.158</doi>
      <video href="2022.findings-acl.158.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="159">
      <title>Unsupervised Natural Language Inference Using <fixed-case>PHL</fixed-case> Triplet Generation</title>
      <author><first>Neeraj</first><last>Varshney</last></author>
      <author><first>Pratyay</first><last>Banerjee</last></author>
      <author><first>Tejas</first><last>Gokhale</last></author>
      <author><first>Chitta</first><last>Baral</last></author>
      <pages>2003-2016</pages>
      <abstract>Transformer-based models achieve impressive performance on numerous Natural Language Inference (NLI) benchmarks when trained on respective training datasets. However, in certain cases, training samples may not be available or collecting them could be time-consuming and resource-intensive. In this work, we address the above challenge and present an explorative study on unsupervised NLI, a paradigm in which no human-annotated training samples are available. We investigate it under three settings: PH, P, and NPH that differ in the extent of unlabeled data available for learning. As a solution, we propose a procedural data generation approach that leverages a set of sentence transformations to collect PHL (Premise, Hypothesis, Label) triplets for training NLI models, bypassing the need for human-annotated training data. Comprehensive experiments with several NLI datasets show that the proposed approach results in accuracies of up to 66.75%, 65.9%, 65.39% in PH, P, and NPH settings respectively, outperforming all existing unsupervised baselines. Furthermore, fine-tuning our model with as little as ~0.1% of the human-annotated training dataset (500 instances) leads to 12.2% higher accuracy than the model trained from scratch on the same 500 instances. Supported by this superior performance, we conclude with a recommendation for collecting high-quality task-specific data.</abstract>
      <url hash="470bd790">2022.findings-acl.159</url>
      <attachment type="software" hash="983dbc30">2022.findings-acl.159.software.zip</attachment>
      <bibkey>varshney-etal-2022-unsupervised</bibkey>
      <doi>10.18653/v1/2022.findings-acl.159</doi>
      <video href="2022.findings-acl.159.mp4"/>
      <pwccode url="https://github.com/nrjvarshney/unsupervised_nli" additional="false">nrjvarshney/unsupervised_nli</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">MS COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="160">
      <title>Data Augmentation and Learned Layer Aggregation for Improved Multilingual Language Understanding in Dialogue</title>
      <author><first>Evgeniia</first><last>Razumovskaia</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <author><first>Anna</first><last>Korhonen</last></author>
      <pages>2017-2033</pages>
      <abstract>Scaling dialogue systems to a multitude of domains, tasks and languages relies on costly and time-consuming data annotation for different domain-task-language configurations. The annotation efforts might be substantially reduced by the methods that generalise well in zero- and few-shot scenarios, and also effectively leverage external unannotated data sources (e.g., Web-scale corpora). We propose two methods to this aim, offering improved dialogue natural language understanding (NLU) across multiple languages: 1) Multi-SentAugment, and 2) LayerAgg. Multi-SentAugment is a self-training method which augments available (typically few-shot) training data with similar (automatically labelled) in-domain sentences from large monolingual Web-scale corpora. LayerAgg learns to select and combine useful semantic information scattered across different layers of a Transformer model (e.g., mBERT); it is especially suited for zero-shot scenarios as semantically richer representations should strengthen the model’s cross-lingual capabilities. Applying the two methods with state-of-the-art NLU models obtains consistent improvements across two standard multilingual NLU datasets covering 16 diverse languages. The gains are observed in zero-shot, few-shot, and even in full-data scenarios. The results also suggest that the two methods achieve a synergistic effect: the best overall performance in few-shot setups is attained when the methods are used together.</abstract>
      <url hash="b76d6024">2022.findings-acl.160</url>
      <bibkey>razumovskaia-etal-2022-data</bibkey>
      <doi>10.18653/v1/2022.findings-acl.160</doi>
      <video href="2022.findings-acl.160.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/cc100">CC100</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xsid">xSID</pwcdataset>
    </paper>
    <paper id="161">
      <title>Ranking-Constrained Learning with Rationales for Text Classification</title>
      <author><first>Juanyan</first><last>Wang</last></author>
      <author><first>Manali</first><last>Sharma</last></author>
      <author><first>Mustafa</first><last>Bilgic</last></author>
      <pages>2034-2046</pages>
      <abstract>We propose a novel approach that jointly utilizes the labels and elicited rationales for text classification to speed up the training of deep learning models with limited training data. We define and optimize a ranking-constrained loss function that combines cross-entropy loss with ranking losses as rationale constraints. We evaluate our proposed rationale-augmented learning approach on three human-annotated datasets, and show that our approach provides significant improvements over classification approaches that do not utilize rationales as well as other state-of-the-art rationale-augmented baselines.</abstract>
      <url hash="ed9f5299">2022.findings-acl.161</url>
      <bibkey>wang-etal-2022-ranking</bibkey>
      <doi>10.18653/v1/2022.findings-acl.161</doi>
    </paper>
    <paper id="162">
      <title><fixed-case>C</fixed-case>a<fixed-case>M</fixed-case>-<fixed-case>G</fixed-case>en: <fixed-case>C</fixed-case>ausally Aware Metric-Guided Text Generation</title>
      <author><first>Navita</first><last>Goyal</last></author>
      <author><first>Roodram</first><last>Paneri</last></author>
      <author><first>Ayush</first><last>Agarwal</last></author>
      <author><first>Udit</first><last>Kalani</last></author>
      <author><first>Abhilasha</first><last>Sancheti</last></author>
      <author><first>Niyati</first><last>Chhaya</last></author>
      <pages>2047-2060</pages>
      <abstract>Content is created for a well-defined purpose, often described by a metric or signal represented in the form of structured information. The relationship between the goal (metrics) of target content and the content itself is non-trivial. While large-scale language models show promising text generation capabilities, guiding the generated text with external metrics is challenging. These metrics and content tend to have inherent relationships and not all of them may be of consequence. We introduce CaM-Gen: Causally aware Generative Networks guided by user-defined target metrics incorporating the causal relationships between the metric and content features. We leverage causal inference techniques to identify causally significant aspects of a text that lead to the target metric and then explicitly guide generative models towards these by a feedback mechanism. We propose this mechanism for variational autoencoder and Transformer-based generative models. The proposed models beat baselines in terms of the target metric control while maintaining fluency and language quality of the generated text. To the best of our knowledge, this is one of the early attempts at controlled generation incorporating a metric guide using causal inference.</abstract>
      <url hash="04a83ef0">2022.findings-acl.162</url>
      <bibkey>goyal-etal-2022-cam</bibkey>
      <doi>10.18653/v1/2022.findings-acl.162</doi>
    </paper>
    <paper id="163">
      <title>Training Dynamics for Text Summarization Models</title>
      <author><first>Tanya</first><last>Goyal</last></author>
      <author><first>Jiacheng</first><last>Xu</last></author>
      <author><first>Junyi Jessy</first><last>Li</last></author>
      <author><first>Greg</first><last>Durrett</last></author>
      <pages>2061-2073</pages>
      <abstract>Pre-trained language models (e.g. BART) have shown impressive results when fine-tuned on large summarization datasets. However, little is understood about this fine-tuning process, including what knowledge is retained from pre-training time or how content selection and generation strategies are learnt across iterations. In this work, we analyze the training dynamics for generation models, focusing on summarization. Across different datasets (CNN/DM, XSum, MediaSum) and summary properties, such as abstractiveness and hallucination, we study what the model learns at different stages of its fine-tuning process. We find that a propensity to copy the input is learned early in the training process consistently across all datasets studied. On the other hand, factual errors, such as hallucination of unsupported facts, are learnt in the later stages, though this behavior is more varied across domains. Based on these observations, we explore complementary approaches for modifying training: first, disregarding high-loss tokens that are challenging to learn and second, disregarding low-loss tokens that are learnt very quickly in the latter stages of the training process. We show that these simple training modifications allow us to configure our model to achieve different goals, such as improving factuality or improving abstractiveness.</abstract>
      <url hash="47a0b4e3">2022.findings-acl.163</url>
      <bibkey>goyal-etal-2022-training</bibkey>
      <doi>10.18653/v1/2022.findings-acl.163</doi>
      <video href="2022.findings-acl.163.mp4"/>
    </paper>
    <paper id="164">
      <title>Richer Countries and Richer Representations</title>
      <author><first>Kaitlyn</first><last>Zhou</last></author>
      <author><first>Kawin</first><last>Ethayarajh</last></author>
      <author><first>Dan</first><last>Jurafsky</last></author>
      <pages>2074-2085</pages>
      <abstract>We examine whether some countries are more richly represented in embedding space than others. We find that countries whose names occur with low frequency in training corpora are more likely to be tokenized into subwords, are less semantically distinct in embedding space, and are less likely to be correctly predicted: e.g., Ghana (the correct answer and in-vocabulary) is not predicted for, “The country producing the most cocoa is [MASK].”. Although these performance discrepancies and representational harms are due to frequency, we find that frequency is highly correlated with a country’s GDP; thus perpetuating historic power and wealth inequalities. We analyze the effectiveness of mitigation strategies; recommend that researchers report training word frequencies; and recommend future work for the community to define and design representational guarantees.</abstract>
      <url hash="e7c3f5c9">2022.findings-acl.164</url>
      <bibkey>zhou-etal-2022-richer</bibkey>
      <doi>10.18653/v1/2022.findings-acl.164</doi>
      <pwccode url="https://github.com/katezhou/country_distortions" additional="false">katezhou/country_distortions</pwccode>
    </paper>
    <paper id="165">
      <title><fixed-case>BBQ</fixed-case>: A hand-built bias benchmark for question answering</title>
      <author><first>Alicia</first><last>Parrish</last></author>
      <author><first>Angelica</first><last>Chen</last></author>
      <author><first>Nikita</first><last>Nangia</last></author>
      <author><first>Vishakh</first><last>Padmakumar</last></author>
      <author><first>Jason</first><last>Phang</last></author>
      <author><first>Jana</first><last>Thompson</last></author>
      <author><first>Phu Mon</first><last>Htut</last></author>
      <author><first>Samuel</first><last>Bowman</last></author>
      <pages>2086-2105</pages>
      <abstract>It is well documented that NLP models learn social biases, but little work has been done on how these biases manifest in model outputs for applied tasks like question answering (QA). We introduce the Bias Benchmark for QA (BBQ), a dataset of question-sets constructed by the authors that highlight attested social biases against people belonging to protected classes along nine social dimensions relevant for U.S. English-speaking contexts. Our task evaluate model responses at two levels: (i) given an under-informative context, we test how strongly responses reflect social biases, and (ii) given an adequately informative context, we test whether the model’s biases override a correct answer choice. We find that models often rely on stereotypes when the context is under-informative, meaning the model’s outputs consistently reproduce harmful biases in this setting. Though models are more accurate when the context provides an informative answer, they still rely on stereotypes and average up to 3.4 percentage points higher accuracy when the correct answer aligns with a social bias than when it conflicts, with this difference widening to over 5 points on examples targeting gender for most models tested.</abstract>
      <url hash="a354af36">2022.findings-acl.165</url>
      <bibkey>parrish-etal-2022-bbq</bibkey>
      <doi>10.18653/v1/2022.findings-acl.165</doi>
      <video href="2022.findings-acl.165.mp4"/>
      <pwccode url="https://github.com/nyu-mll/bbq" additional="false">nyu-mll/bbq</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bbq">BBQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/unqover">UnQover</pwcdataset>
    </paper>
    <paper id="166">
      <title>Zero-shot Learning for Grapheme to Phoneme Conversion with Language Ensemble</title>
      <author><first>Xinjian</first><last>Li</last></author>
      <author><first>Florian</first><last>Metze</last></author>
      <author><first>David</first><last>Mortensen</last></author>
      <author><first>Shinji</first><last>Watanabe</last></author>
      <author><first>Alan</first><last>Black</last></author>
      <pages>2106-2115</pages>
      <abstract>Grapheme-to-Phoneme (G2P) has many applications in NLP and speech fields. Most existing work focuses heavily on languages with abundant training datasets, which limits the scope of target languages to less than 100 languages. This work attempts to apply zero-shot learning to approximate G2P models for all low-resource and endangered languages in Glottolog (about 8k languages). For any unseen target language, we first build the phylogenetic tree (i.e. language family tree) to identify top-<tex-math>k</tex-math> nearest languages for which we have training sets. Then we run models of those languages to obtain a hypothesis set, which we combine into a confusion network to propose a most likely hypothesis as an approximation to the target language. We test our approach on over 600 unseen languages and demonstrate it significantly outperforms baselines.</abstract>
      <url hash="1ce8da67">2022.findings-acl.166</url>
      <bibkey>li-etal-2022-zero</bibkey>
      <doi>10.18653/v1/2022.findings-acl.166</doi>
      <video href="2022.findings-acl.166.mp4"/>
      <pwccode url="https://github.com/xinjli/transphone" additional="false">xinjli/transphone</pwccode>
    </paper>
    <paper id="167">
      <title>Dim Wihl Gat Tun: <fixed-case>T</fixed-case>he Case for Linguistic Expertise in <fixed-case>NLP</fixed-case> for Under-Documented Languages</title>
      <author><first>Clarissa</first><last>Forbes</last></author>
      <author><first>Farhan</first><last>Samir</last></author>
      <author><first>Bruce</first><last>Oliver</last></author>
      <author><first>Changbing</first><last>Yang</last></author>
      <author><first>Edith</first><last>Coates</last></author>
      <author><first>Garrett</first><last>Nicolai</last></author>
      <author><first>Miikka</first><last>Silfverberg</last></author>
      <pages>2116-2130</pages>
      <abstract>Recent progress in NLP is driven by pretrained models leveraging massive datasets and has predominantly benefited the world’s political and economic superpowers. Technologically underserved languages are left behind because they lack such resources. Hundreds of underserved languages, nevertheless, have available data sources in the form of interlinear glossed text (IGT) from language documentation efforts. IGT remains underutilized in NLP work, perhaps because its annotations are only semi-structured and often language-specific. With this paper, we make the case that IGT data can be leveraged successfully provided that target language expertise is available. We specifically advocate for collaboration with documentary linguists. Our paper provides a roadmap for successful projects utilizing IGT data: (1) It is essential to define which NLP tasks can be accomplished with the given IGT data and how these will benefit the speech community. (2) Great care and target language expertise is required when converting the data into structured formats commonly employed in NLP. (3) Task-specific and user-specific evaluation can help to ascertain that the tools which are created benefit the target language speech community. We illustrate each step through a case study on developing a morphological reinflection system for the Tsimchianic language Gitksan.</abstract>
      <url hash="32ecd05a">2022.findings-acl.167</url>
      <bibkey>forbes-etal-2022-dim</bibkey>
      <doi>10.18653/v1/2022.findings-acl.167</doi>
      <video href="2022.findings-acl.167.mp4"/>
    </paper>
    <paper id="168">
      <title>Question Generation for Reading Comprehension Assessment by Modeling How and What to Ask</title>
      <author><first>Bilal</first><last>Ghanem</last></author>
      <author><first>Lauren</first><last>Lutz Coleman</last></author>
      <author><first>Julia</first><last>Rivard Dexter</last></author>
      <author><first>Spencer</first><last>von der Ohe</last></author>
      <author><first>Alona</first><last>Fyshe</last></author>
      <pages>2131-2146</pages>
      <abstract>Reading is integral to everyday life, and yet learning to read is a struggle for many young learners. During lessons, teachers can use comprehension questions to increase engagement, test reading skills, and improve retention. Historically such questions were written by skilled teachers, but recently language models have been used to generate comprehension questions. However, many existing Question Generation (QG) systems focus on generating extractive questions from the text, and have no way to control the type of the generated question. In this paper, we study QG for reading comprehension where inferential questions are critical and extractive techniques cannot be used. We propose a two-step model (HTA-WTA) that takes advantage of previous datasets, and can generate questions for a specific targeted comprehension skill. We propose a new reading comprehension dataset that contains questions annotated with story-based reading comprehension skills (SBRCS), allowing for a more complete reader assessment. Across several experiments, our results show that HTA-WTA outperforms multiple strong baselines on this new dataset. We show that the HTA-WTA model tests for strong SCRS by asking deep inferential questions.</abstract>
      <url hash="797336f3">2022.findings-acl.168</url>
      <bibkey>ghanem-etal-2022-question</bibkey>
      <doi>10.18653/v1/2022.findings-acl.168</doi>
      <video href="2022.findings-acl.168.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/cosmosqa">CosmosQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="169">
      <title><fixed-case>TAB</fixed-case>i: <fixed-case>T</fixed-case>ype-Aware Bi-Encoders for Open-Domain Entity Retrieval</title>
      <author><first>Megan</first><last>Leszczynski</last></author>
      <author><first>Daniel</first><last>Fu</last></author>
      <author><first>Mayee</first><last>Chen</last></author>
      <author><first>Christopher</first><last>Re</last></author>
      <pages>2147-2166</pages>
      <abstract>Entity retrieval—retrieving information about entity mentions in a query—is a key step in open-domain tasks, such as question answering or fact checking. However, state-of-the-art entity retrievers struggle to retrieve rare entities for ambiguous mentions due to biases towards popular entities. Incorporating knowledge graph types during training could help overcome popularity biases, but there are several challenges: (1) existing type-based retrieval methods require mention boundaries as input, but open-domain tasks run on unstructured text, (2) type-based methods should not compromise overall performance, and (3) type-based methods should be robust to noisy and missing types. In this work, we introduce TABi, a method to jointly train bi-encoders on knowledge graph types and unstructured text for entity retrieval for open-domain tasks. TABi leverages a type-enforced contrastive loss to encourage entities and queries of similar types to be close in the embedding space. TABi improves retrieval of rare entities on the Ambiguous Entity Retrieval (AmbER) sets, while maintaining strong overall retrieval performance on open-domain tasks in the KILT benchmark compared to state-of-the-art retrievers. TABi is also robust to incomplete type systems, improving rare entity retrieval over baselines with only 5% type coverage of the training dataset. We make our code publicly available.</abstract>
      <url hash="65676acf">2022.findings-acl.169</url>
      <bibkey>leszczynski-etal-2022-tabi</bibkey>
      <doi>10.18653/v1/2022.findings-acl.169</doi>
      <video href="2022.findings-acl.169.mp4"/>
      <pwccode url="https://github.com/hazyresearch/tabi" additional="false">hazyresearch/tabi</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/figer">FIGER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/kilt">KILT</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
    </paper>
    <paper id="170">
      <title>Hierarchical Recurrent Aggregative Generation for Few-Shot <fixed-case>NLG</fixed-case></title>
      <author><first>Giulio</first><last>Zhou</last></author>
      <author><first>Gerasimos</first><last>Lampouras</last></author>
      <author><first>Ignacio</first><last>Iacobacci</last></author>
      <pages>2167-2181</pages>
      <abstract>Large pretrained models enable transfer learning to low-resource domains for language generation tasks. However, previous end-to-end approaches do not account for the fact that some generation sub-tasks, specifically aggregation and lexicalisation, can benefit from transfer learning in different extents. To exploit these varying potentials for transfer learning, we propose a new hierarchical approach for few-shot and zero-shot generation. Our approach consists of a three-moduled jointly trained architecture: the first module independently lexicalises the distinct units of information in the input as sentence sub-units (e.g. phrases), the second module recurrently aggregates these sub-units to generate a unified intermediate output, while the third module subsequently post-edits it to generate a coherent and fluent final text. We perform extensive empirical analysis and ablation studies on few-shot and zero-shot settings across 4 datasets. Automatic and human evaluation shows that the proposed hierarchical approach is consistently capable of achieving state-of-the-art results when compared to previous work.</abstract>
      <url hash="108609b2">2022.findings-acl.170</url>
      <bibkey>zhou-etal-2022-hierarchical</bibkey>
      <doi>10.18653/v1/2022.findings-acl.170</doi>
      <video href="2022.findings-acl.170.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/sgd">SGD</pwcdataset>
    </paper>
    <paper id="171">
      <title>Training Text-to-Text Transformers with Privacy Guarantees</title>
      <author><first>Natalia</first><last>Ponomareva</last></author>
      <author><first>Jasmijn</first><last>Bastings</last></author>
      <author><first>Sergei</first><last>Vassilvitskii</last></author>
      <pages>2182-2193</pages>
      <abstract>Recent advances in NLP often stem from large transformer-based pre-trained models, which rapidly grow in size and use more and more training data. Such models are often released to the public so that end users can fine-tune them on a task dataset. While it is common to treat pre-training data as public, it may still contain personally identifiable information (PII), such as names, phone numbers, and copyrighted material. Recent findings show that the capacity of these models allows them to memorize parts of the training data, and suggest differentially private (DP) training as a potential mitigation. While there is recent work on DP fine-tuning of NLP models, the effects of DP pre-training are less well understood: it is not clear how downstream performance is affected by DP pre-training, and whether DP pre-training mitigates some of the memorization concerns. We focus on T5 and show that by using recent advances in JAX and XLA we can train models with DP that do not suffer a large drop in pre-training utility, nor in training speed, and can still be fine-tuned to high accuracies on downstream tasks (e.g. GLUE). Moreover, we show that T5’s span corruption is a good defense against data memorization.</abstract>
      <url hash="cc2febb1">2022.findings-acl.171</url>
      <bibkey>ponomareva-etal-2022-training</bibkey>
      <doi>10.18653/v1/2022.findings-acl.171</doi>
    </paper>
    <paper id="172">
      <title>Revisiting Uncertainty-based Query Strategies for Active Learning with Transformers</title>
      <author><first>Christopher</first><last>Schröder</last></author>
      <author><first>Andreas</first><last>Niekler</last></author>
      <author><first>Martin</first><last>Potthast</last></author>
      <pages>2194-2203</pages>
      <abstract>Active learning is the iterative construction of a classification model through targeted labeling, enabling significant labeling cost savings. As most research on active learning has been carried out before transformer-based language models (“transformers”) became popular, despite its practical importance, comparably few papers have investigated how transformers can be combined with active learning to date. This can be attributed to the fact that using state-of-the-art query strategies for transformers induces a prohibitive runtime overhead, which effectively nullifies, or even outweighs the desired cost savings. For this reason, we revisit uncertainty-based query strategies, which had been largely outperformed before, but are particularly suited in the context of fine-tuning transformers. In an extensive evaluation, we connect transformers to experiments from previous research, assessing their performance on five widely used text classification benchmarks. For active learning with transformers, several other uncertainty-based approaches outperform the well-known prediction entropy query strategy, thereby challenging its status as most popular uncertainty baseline in active learning for text classification.</abstract>
      <url hash="a7a75b3f">2022.findings-acl.172</url>
      <bibkey>schroder-etal-2022-revisiting</bibkey>
      <doi>10.18653/v1/2022.findings-acl.172</doi>
      <video href="2022.findings-acl.172.mp4"/>
      <pwccode url="https://github.com/webis-de/acl-22" additional="false">webis-de/acl-22</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mr">MR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/subj">SUBJ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/trec-10">TREC-10</pwcdataset>
    </paper>
    <paper id="173">
      <title>The impact of lexical and grammatical processing on generating code from natural language</title>
      <author><first>Nathanaël</first><last>Beau</last></author>
      <author><first>Benoit</first><last>Crabbé</last></author>
      <pages>2204-2214</pages>
      <abstract>Considering the seq2seq architecture of Yin and Neubig (2018) for natural language to code translation, we identify four key components of importance: grammatical constraints, lexical preprocessing, input representations, and copy mechanisms. To study the impact of these components, we use a state-of-the-art architecture that relies on BERT encoder and a grammar-based decoder for which a formalization is provided. The paper highlights the importance of the lexical substitution component in the current natural language to code systems.</abstract>
      <url hash="28af5fc0">2022.findings-acl.173</url>
      <bibkey>beau-crabbe-2022-impact</bibkey>
      <doi>10.18653/v1/2022.findings-acl.173</doi>
      <video href="2022.findings-acl.173.mp4"/>
      <pwccode url="https://gitlab.com/codegenfactors/BertranX" additional="true">codegenfactors/BertranX</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conala">CoNaLa</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/django">Django</pwcdataset>
    </paper>
    <paper id="174">
      <title><fixed-case>S</fixed-case>eq2<fixed-case>P</fixed-case>ath: Generating Sentiment Tuples as Paths of a Tree</title>
      <author><first>Yue</first><last>Mao</last></author>
      <author><first>Yi</first><last>Shen</last></author>
      <author><first>Jingchao</first><last>Yang</last></author>
      <author><first>Xiaoying</first><last>Zhu</last></author>
      <author><first>Longjun</first><last>Cai</last></author>
      <pages>2215-2225</pages>
      <abstract>Aspect-based sentiment analysis (ABSA) tasks aim to extract sentiment tuples from a sentence. Recent generative methods such as Seq2Seq models have achieved good performance by formulating the output as a sequence of sentiment tuples. However, the orders between the sentiment tuples do not naturally exist and the generation of the current tuple should not condition on the previous ones. In this paper, we propose Seq2Path to generate sentiment tuples as paths of a tree. A tree can represent “1-to-n” relations (e.g., an aspect term may correspond to multiple opinion terms) and the paths of a tree are independent and do not have orders. For training, we treat each path as an independent target, and we calculate the average loss of the ordinary Seq2Seq model over paths. For inference, we apply beam search with constrained decoding. By introducing an additional discriminative token and applying a data augmentation technique, valid paths can be automatically selected. We conduct experiments on five tasks including AOPE, ASTE, TASD, UABSA, ACOS. We evaluate our method on four common benchmark datasets including Laptop14, Rest14, Rest15, Rest16. Our proposed method achieves state-of-the-art results in almost all cases.</abstract>
      <url hash="cce06e84">2022.findings-acl.174</url>
      <attachment type="software" hash="85d89133">2022.findings-acl.174.software.zip</attachment>
      <bibkey>mao-etal-2022-seq2path</bibkey>
      <doi>10.18653/v1/2022.findings-acl.174</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/aste">ASTE</pwcdataset>
    </paper>
    <paper id="175">
      <title>Mitigating the Inconsistency Between Word Saliency and Model Confidence with Pathological Contrastive Training</title>
      <author><first>Pengwei</first><last>Zhan</last></author>
      <author><first>Yang</first><last>Wu</last></author>
      <author><first>Shaolei</first><last>Zhou</last></author>
      <author><first>Yunjian</first><last>Zhang</last></author>
      <author><first>Liming</first><last>Wang</last></author>
      <pages>2226-2244</pages>
      <abstract>Neural networks are widely used in various NLP tasks for their remarkable performance. However, the complexity makes them difficult to interpret, i.e., they are not guaranteed right for the right reason. Besides the complexity, we reveal that the model pathology - the inconsistency between word saliency and model confidence, further hurts the interpretability. We show that the pathological inconsistency is caused by the representation collapse issue, which means that the representation of the sentences with tokens in different saliency reduced is somehow collapsed, and thus the important words cannot be distinguished from unimportant words in terms of model confidence changing. In this paper, to mitigate the pathology and obtain more interpretable models, we propose Pathological Contrastive Training (PCT) framework, which adopts contrastive learning and saliency-based samples augmentation to calibrate the sentences representation. Combined with qualitative analysis, we also conduct extensive quantitative experiments and measure the interpretability with eight reasonable metrics. Experiments show that our method can mitigate the model pathology and generate more interpretable models while keeping the model performance. Ablation study also shows the effectiveness.</abstract>
      <url hash="cd5c81fa">2022.findings-acl.175</url>
      <attachment type="software" hash="62374b66">2022.findings-acl.175.software.zip</attachment>
      <bibkey>zhan-etal-2022-mitigating</bibkey>
      <doi>10.18653/v1/2022.findings-acl.175</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="176">
      <title>Your fairness may vary: Pretrained language model fairness in toxic text classification</title>
      <author><first>Ioana</first><last>Baldini</last></author>
      <author><first>Dennis</first><last>Wei</last></author>
      <author><first>Karthikeyan</first><last>Natesan Ramamurthy</last></author>
      <author><first>Moninder</first><last>Singh</last></author>
      <author><first>Mikhail</first><last>Yurochkin</last></author>
      <pages>2245-2262</pages>
      <abstract>The popularity of pretrained language models in natural language processing systems calls for a careful evaluation of such models in down-stream tasks, which have a higher potential for societal impact. The evaluation of such systems usually focuses on accuracy measures. Our findings in this paper call for attention to be paid to fairness measures as well. Through the analysis of more than a dozen pretrained language models of varying sizes on two toxic text classification tasks (English), we demonstrate that focusing on accuracy measures alone can lead to models with wide variation in fairness characteristics. Specifically, we observe that fairness can vary even more than accuracy with increasing training data size and different random initializations. At the same time, we find that little of the fairness variation is explained by model size, despite claims in the literature. To improve model fairness without retraining, we show that two post-processing methods developed for structured, tabular data can be successfully applied to a range of pretrained language models. Warning: This paper contains samples of offensive text.</abstract>
      <url hash="08332a7e">2022.findings-acl.176</url>
      <bibkey>baldini-etal-2022-fairness</bibkey>
      <doi>10.18653/v1/2022.findings-acl.176</doi>
      <video href="2022.findings-acl.176.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/hatexplain">HateXplain</pwcdataset>
    </paper>
    <paper id="177">
      <title><fixed-case>C</fixed-case>hart<fixed-case>QA</fixed-case>: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning</title>
      <author><first>Ahmed</first><last>Masry</last></author>
      <author><first>Xuan Long</first><last>Do</last></author>
      <author><first>Jia Qing</first><last>Tan</last></author>
      <author><first>Shafiq</first><last>Joty</last></author>
      <author><first>Enamul</first><last>Hoque</last></author>
      <pages>2263-2279</pages>
      <abstract>Charts are very popular for analyzing data. When exploring charts, people often ask a variety of complex reasoning questions that involve several logical and arithmetic operations. They also commonly refer to visual features of a chart in their questions. However, most existing datasets do not focus on such complex reasoning questions as their questions are template-based and answers come from a fixed-vocabulary. In this work, we present a large-scale benchmark covering 9.6K human-written questions as well as 23.1K questions generated from human-written chart summaries. To address the unique challenges in our benchmark involving visual and logical reasoning over charts, we present two transformer-based models that combine visual features and the data table of the chart in a unified way to answer questions. While our models achieve the state-of-the-art results on the previous datasets as well as on our benchmark, the evaluation also reveals several challenges in answering complex reasoning questions.</abstract>
      <url hash="9d0a6c50">2022.findings-acl.177</url>
      <bibkey>masry-etal-2022-chartqa</bibkey>
      <doi>10.18653/v1/2022.findings-acl.177</doi>
      <video href="2022.findings-acl.177.mp4"/>
      <pwccode url="https://github.com/vis-nlp/chartqa" additional="false">vis-nlp/chartqa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/chartqa">ChartQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dvqa">DVQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/figureqa">FigureQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/leaf-qa">LEAF-QA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/plotqa">PlotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/realcqa">RealCQA</pwcdataset>
    </paper>
    <paper id="178">
      <title>A Novel Perspective to Look At Attention: Bi-level Attention-based Explainable Topic Modeling for News Classification</title>
      <author><first>Dairui</first><last>Liu</last></author>
      <author><first>Derek</first><last>Greene</last></author>
      <author><first>Ruihai</first><last>Dong</last></author>
      <pages>2280-2290</pages>
      <abstract>Many recent deep learning-based solutions have adopted the attention mechanism in various tasks in the field of NLP. However, the inherent characteristics of deep learning models and the flexibility of the attention mechanism increase the models’ complexity, thus leading to challenges in model explainability. To address this challenge, we propose a novel practical framework by utilizing a two-tier attention architecture to decouple the complexity of explanation and the decision-making process. We apply it in the context of a news article classification task. The experiments on two large-scaled news corpora demonstrate that the proposed model can achieve competitive performance with many state-of-the-art alternatives and illustrate its appropriateness from an explainability perspective. We release the source code here.</abstract>
      <url hash="dcddf4d9">2022.findings-acl.178</url>
      <bibkey>liu-etal-2022-novel</bibkey>
      <doi>10.18653/v1/2022.findings-acl.178</doi>
      <video href="2022.findings-acl.178.mp4"/>
      <pwccode url="https://github.com/ruixinhua/batm" additional="true">ruixinhua/batm</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mind">MIND</pwcdataset>
    </paper>
    <paper id="179">
      <title>Learn and Review: Enhancing Continual Named Entity Recognition via Reviewing Synthetic Samples</title>
      <author><first>Yu</first><last>Xia</last></author>
      <author><first>Quan</first><last>Wang</last></author>
      <author><first>Yajuan</first><last>Lyu</last></author>
      <author><first>Yong</first><last>Zhu</last></author>
      <author><first>Wenhao</first><last>Wu</last></author>
      <author><first>Sujian</first><last>Li</last></author>
      <author><first>Dai</first><last>Dai</last></author>
      <pages>2291-2300</pages>
      <abstract>Traditional methods for named entity recognition (NER) classify mentions into a fixed set of pre-defined entity types. However, in many real-world scenarios, new entity types are incrementally involved. To investigate this problem, continual learning is introduced for NER. However, the existing method depends on the relevance between tasks and is prone to inter-type confusion. In this paper, we propose a novel two-stage framework Learn-and-Review (L&amp;R) for continual NER under the type-incremental setting to alleviate the above issues. Specifically, for the learning stage, we distill the old knowledge from teacher to a student on the current dataset. For the reviewing stage, we first generate synthetic samples of old types to augment the dataset. Then, we further distill new knowledge from the above student and old knowledge from the teacher to get an enhanced student on the augmented dataset. This stage has the following advantages: (1) The synthetic samples mitigate the gap between the old and new task and thus enhance the further distillation; (2) Different types of entities are jointly seen during training which alleviates the inter-type confusion. Experimental results show that L&amp;R outperforms the state-of-the-art method on CoNLL-03 and OntoNotes-5.0.</abstract>
      <url hash="2b1f451d">2022.findings-acl.179</url>
      <bibkey>xia-etal-2022-learn</bibkey>
      <doi>10.18653/v1/2022.findings-acl.179</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL 2003</pwcdataset>
    </paper>
    <paper id="180">
      <title>Phoneme transcription of endangered languages: an evaluation of recent <fixed-case>ASR</fixed-case> architectures in the single speaker scenario</title>
      <author><first>Gilles</first><last>Boulianne</last></author>
      <pages>2301-2308</pages>
      <abstract>Transcription is often reported as the bottleneck in endangered language documentation, requiring large efforts from scarce speakers and transcribers. In general, automatic speech recognition (ASR) can be accurate enough to accelerate transcription only if trained on large amounts of transcribed data. However, when a single speaker is involved, several studies have reported encouraging results for phonetic transcription even with small amounts of training. Here we expand this body of work on speaker-dependent transcription by comparing four ASR approaches, notably recent transformer and pretrained multilingual models, on a common dataset of 11 languages. To automate data preparation, training and evaluation steps, we also developed a phoneme recognition setup which handles morphologically complex languages and writing systems for which no pronunciation dictionary exists. We find that fine-tuning a multilingual pretrained model yields an average phoneme error rate (PER) of 15% for 6 languages with 99 minutes or less of transcribed data for training. For the 5 languages with between 100 and 192 minutes of training, we achieved a PER of 8.4% or less. These results on a number of varied languages suggest that ASR can now significantly reduce transcription efforts in the speaker-dependent situation common in endangered language work.</abstract>
      <url hash="60fa5a43">2022.findings-acl.180</url>
      <bibkey>boulianne-2022-phoneme</bibkey>
      <doi>10.18653/v1/2022.findings-acl.180</doi>
    </paper>
    <paper id="181">
      <title>Does <fixed-case>BERT</fixed-case> really agree ? Fine-grained Analysis of Lexical Dependence on a Syntactic Task</title>
      <author><first>Karim</first><last>Lasri</last></author>
      <author><first>Alessandro</first><last>Lenci</last></author>
      <author><first>Thierry</first><last>Poibeau</last></author>
      <pages>2309-2315</pages>
      <abstract>Although transformer-based Neural Language Models demonstrate impressive performance on a variety of tasks, their generalization abilities are not well understood. They have been shown to perform strongly on subject-verb number agreement in a wide array of settings, suggesting that they learned to track syntactic dependencies during their training even without explicit supervision. In this paper, we examine the extent to which BERT is able to perform lexically-independent subject-verb number agreement (NA) on targeted syntactic templates. To do so, we disrupt the lexical patterns found in naturally occurring stimuli for each targeted structure in a novel fine-grained analysis of BERT’s behavior. Our results on nonce sentences suggest that the model generalizes well for simple templates, but fails to perform lexically-independent syntactic generalization when as little as one attractor is present.</abstract>
      <url hash="77797ea1">2022.findings-acl.181</url>
      <bibkey>lasri-etal-2022-bert</bibkey>
      <doi>10.18653/v1/2022.findings-acl.181</doi>
      <video href="2022.findings-acl.181.mp4"/>
    </paper>
    <paper id="182">
      <title>Combining Static and Contextualised Multilingual Embeddings</title>
      <author><first>Katharina</first><last>Hämmerl</last></author>
      <author><first>Jindřich</first><last>Libovický</last></author>
      <author><first>Alexander</first><last>Fraser</last></author>
      <pages>2316-2329</pages>
      <abstract>Static and contextual multilingual embeddings have complementary strengths. Static embeddings, while less expressive than contextual language models, can be more straightforwardly aligned across multiple languages. We combine the strengths of static and contextual models to improve multilingual representations. We extract static embeddings for 40 languages from XLM-R, validate those embeddings with cross-lingual word retrieval, and then align them using VecMap. This results in high-quality, highly multilingual static embeddings. Then we apply a novel continued pre-training approach to XLM-R, leveraging the high quality alignment of our static embeddings to better align the representation space of XLM-R. We show positive results for multiple complex semantic tasks. We release the static embeddings and the continued pre-training code. Unlike most previous work, our continued pre-training approach does not require parallel text.</abstract>
      <url hash="3bea225f">2022.findings-acl.182</url>
      <attachment type="software" hash="f8f1075d">2022.findings-acl.182.software.zip</attachment>
      <bibkey>hammerl-etal-2022-combining</bibkey>
      <doi>10.18653/v1/2022.findings-acl.182</doi>
      <video href="2022.findings-acl.182.mp4"/>
      <pwccode url="https://github.com/kathyhaem/combining-static-contextual" additional="false">kathyhaem/combining-static-contextual</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/tydi-qa">TyDiQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xquad">XQuAD</pwcdataset>
    </paper>
    <paper id="183">
      <title>An Accurate Unsupervised Method for Joint Entity Alignment and Dangling Entity Detection</title>
      <author><first>Shengxuan</first><last>Luo</last></author>
      <author><first>Sheng</first><last>Yu</last></author>
      <pages>2330-2339</pages>
      <abstract>Knowledge graph integration typically suffers from the widely existing dangling entities that cannot find alignment cross knowledge graphs (KGs). The dangling entity set is unavailable in most real-world scenarios, and manually mining the entity pairs that consist of entities with the same meaning is labor-consuming. In this paper, we propose a novel accurate Unsupervised method for joint Entity alignment (EA) and Dangling entity detection (DED), called UED. The UED mines the literal semantic information to generate pseudo entity pairs and globally guided alignment information for EA and then utilizes the EA results to assist the DED. We construct a medical cross-lingual knowledge graph dataset, MedED, providing data for both the EA and DED tasks. Extensive experiments demonstrate that in the EA task, UED achieves EA results comparable to those of state-of-the-art supervised EA baselines and outperforms the current state-of-the-art EA methods by combining supervised EA data. For the DED task, UED obtains high-quality results without supervision.</abstract>
      <url hash="f72e3ebf">2022.findings-acl.183</url>
      <attachment type="software" hash="68be3234">2022.findings-acl.183.software.zip</attachment>
      <bibkey>luo-yu-2022-accurate</bibkey>
      <doi>10.18653/v1/2022.findings-acl.183</doi>
      <pwccode url="https://github.com/luosx18/ued" additional="false">luosx18/ued</pwccode>
    </paper>
    <paper id="184">
      <title>Square One Bias in <fixed-case>NLP</fixed-case>: Towards a Multi-Dimensional Exploration of the Research Manifold</title>
      <author><first>Sebastian</first><last>Ruder</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>2340-2354</pages>
      <abstract>The prototypical NLP experiment trains a standard architecture on labeled English data and optimizes for accuracy, without accounting for other dimensions such as fairness, interpretability, or computational efficiency. We show through a manual classification of recent NLP research papers that this is indeed the case and refer to it as the square one experimental setup. We observe that NLP research often goes beyond the square one setup, e.g, focusing not only on accuracy, but also on fairness or interpretability, but typically only along a single dimension. Most work targeting multilinguality, for example, considers only accuracy; most work on fairness or interpretability considers only English; and so on. Such one-dimensionality of most research means we are only exploring a fraction of the NLP research search space. We provide historical and recent examples of how the square one bias has led researchers to draw false conclusions or make unwise choices, point to promising yet unexplored directions on the research manifold, and make practical recommendations to enable more multi-dimensional research. We open-source the results of our annotations to enable further analysis.</abstract>
      <url hash="291713f7">2022.findings-acl.184</url>
      <bibkey>ruder-etal-2022-square</bibkey>
      <doi>10.18653/v1/2022.findings-acl.184</doi>
      <pwccode url="https://github.com/google-research/url-nlp" additional="false">google-research/url-nlp</pwccode>
    </paper>
    <paper id="185">
      <title>Systematicity, Compositionality and Transitivity of Deep <fixed-case>NLP</fixed-case> Models: a Metamorphic Testing Perspective</title>
      <author><first>Edoardo</first><last>Manino</last></author>
      <author><first>Julia</first><last>Rozanova</last></author>
      <author><first>Danilo</first><last>Carvalho</last></author>
      <author><first>Andre</first><last>Freitas</last></author>
      <author><first>Lucas</first><last>Cordeiro</last></author>
      <pages>2355-2366</pages>
      <abstract>Metamorphic testing has recently been used to check the safety of neural NLP models. Its main advantage is that it does not rely on a ground truth to generate test cases. However, existing studies are mostly concerned with robustness-like metamorphic relations, limiting the scope of linguistic properties they can test. We propose three new classes of metamorphic relations, which address the properties of systematicity, compositionality and transitivity. Unlike robustness, our relations are defined over multiple source inputs, thus increasing the number of test cases that we can produce by a polynomial factor. With them, we test the internal consistency of state-of-the-art NLP models, and show that they do not always behave according to their expected linguistic properties. Lastly, we introduce a novel graphical notation that efficiently summarises the inner structure of metamorphic relations.</abstract>
      <url hash="93456b9e">2022.findings-acl.185</url>
      <attachment type="software" hash="c85e0fb0">2022.findings-acl.185.software.zip</attachment>
      <bibkey>manino-etal-2022-systematicity</bibkey>
      <doi>10.18653/v1/2022.findings-acl.185</doi>
      <video href="2022.findings-acl.185.mp4"/>
    </paper>
    <paper id="186">
      <title>Improving Neural Political Statement Classification with Class Hierarchical Information</title>
      <author><first>Erenay</first><last>Dayanik</last></author>
      <author><first>Andre</first><last>Blessing</last></author>
      <author><first>Nico</first><last>Blokker</last></author>
      <author><first>Sebastian</first><last>Haunss</last></author>
      <author><first>Jonas</first><last>Kuhn</last></author>
      <author><first>Gabriella</first><last>Lapesa</last></author>
      <author><first>Sebastian</first><last>Pado</last></author>
      <pages>2367-2382</pages>
      <abstract>Many tasks in text-based computational social science (CSS) involve the classification of political statements into categories based on a domain-specific codebook. In order to be useful for CSS analysis, these categories must be fine-grained. The typically skewed distribution of fine-grained categories, however, results in a challenging classification problem on the NLP side. This paper proposes to make use of the hierarchical relations among categories typically present in such codebooks:e.g., markets and taxation are both subcategories of economy, while borders is a subcategory of security. We use these ontological relations as prior knowledge to establish additional constraints on the learned model, thusimproving performance overall and in particular for infrequent categories. We evaluate several lightweight variants of this intuition by extending state-of-the-art transformer-based textclassifiers on two datasets and multiple languages. We find the most consistent improvement for an approach based on regularization.</abstract>
      <url hash="04b06c85">2022.findings-acl.186</url>
      <attachment type="software" hash="3affc077">2022.findings-acl.186.software.zip</attachment>
      <bibkey>dayanik-etal-2022-improving</bibkey>
      <doi>10.18653/v1/2022.findings-acl.186</doi>
    </paper>
    <paper id="187">
      <title>Enabling Multimodal Generation on <fixed-case>CLIP</fixed-case> via Vision-Language Knowledge Distillation</title>
      <author><first>Wenliang</first><last>Dai</last></author>
      <author><first>Lu</first><last>Hou</last></author>
      <author><first>Lifeng</first><last>Shang</last></author>
      <author><first>Xin</first><last>Jiang</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <pages>2383-2395</pages>
      <abstract>The recent large-scale vision-language pre-training (VLP) of dual-stream architectures (e.g., CLIP) with a tremendous amount of image-text pair data, has shown its superiority on various multimodal alignment tasks. Despite its success, the resulting models are not capable of multimodal generative tasks due to the weak text encoder. To tackle this problem, we propose to augment the dual-stream VLP model with a textual pre-trained language model (PLM) via vision-language knowledge distillation (VLKD), enabling the capability for multimodal generation. VLKD is pretty data- and computation-efficient compared to the pre-training from scratch. Experimental results show that the resulting model has strong zero-shot performance on multimodal generation tasks, such as open-ended visual question answering and image captioning. For example, it achieves 44.5% zero-shot accuracy on the VQAv2 dataset, surpassing the previous state-of-the-art zero-shot model with <tex-math>7\times</tex-math> fewer parameters. Furthermore, the original textual language understanding and generation ability of the PLM is maintained after VLKD, which makes our model versatile for both multimodal and unimodal tasks.</abstract>
      <url hash="1fb59ca9">2022.findings-acl.187</url>
      <bibkey>dai-etal-2022-enabling</bibkey>
      <doi>10.18653/v1/2022.findings-acl.187</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">MS COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/nocaps">NoCaps</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ok-vqa">OK-VQA</pwcdataset>
    </paper>
    <paper id="188">
      <title>Co-<fixed-case>VQA</fixed-case> : Answering by Interactive Sub Question Sequence</title>
      <author><first>Ruonan</first><last>Wang</last></author>
      <author><first>Yuxi</first><last>Qian</last></author>
      <author><first>Fangxiang</first><last>Feng</last></author>
      <author><first>Xiaojie</first><last>Wang</last></author>
      <author><first>Huixing</first><last>Jiang</last></author>
      <pages>2396-2408</pages>
      <abstract>Most existing approaches to Visual Question Answering (VQA) answer questions directly, however, people usually decompose a complex question into a sequence of simple sub questions and finally obtain the answer to the original question after answering the sub question sequence(SQS). By simulating the process, this paper proposes a conversation-based VQA (Co-VQA) framework, which consists of three components: Questioner, Oracle, and Answerer. Questioner raises the sub questions using an extending HRED model, and Oracle answers them one-by-one. An Adaptive Chain Visual Reasoning Model (ACVRM) for Answerer is also proposed, where the question-answer pair is used to update the visual representation sequentially. To perform supervised learning for each model, we introduce a well-designed method to build a SQS for each question on VQA 2.0 and VQA-CP v2 datasets. Experimental results show that our method achieves state-of-the-art on VQA-CP v2. Further analyses show that SQSs help build direct semantic connections between questions and images, provide question-adaptive variable-length reasoning chains, and with explicit interpretability as well as error traceability.</abstract>
      <url hash="43bde968">2022.findings-acl.188</url>
      <bibkey>wang-etal-2022-co</bibkey>
      <doi>10.18653/v1/2022.findings-acl.188</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/guesswhat">GuessWhat?!</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-genome">Visual Genome</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering-v2-0">Visual Question Answering v2.0</pwcdataset>
    </paper>
    <paper id="189">
      <title>A Simple Hash-Based Early Exiting Approach For Language Understanding and Generation</title>
      <author><first>Tianxiang</first><last>Sun</last></author>
      <author><first>Xiangyang</first><last>Liu</last></author>
      <author><first>Wei</first><last>Zhu</last></author>
      <author><first>Zhichao</first><last>Geng</last></author>
      <author><first>Lingling</first><last>Wu</last></author>
      <author><first>Yilong</first><last>He</last></author>
      <author><first>Yuan</first><last>Ni</last></author>
      <author><first>Guotong</first><last>Xie</last></author>
      <author><first>Xuanjing</first><last>Huang</last></author>
      <author><first>Xipeng</first><last>Qiu</last></author>
      <pages>2409-2421</pages>
      <abstract>Early exiting allows instances to exit at different layers according to the estimation of difficulty. Previous works usually adopt heuristic metrics such as the entropy of internal outputs to measure instance difficulty, which suffers from generalization and threshold-tuning. In contrast, learning to exit, or learning to predict instance difficulty is a more appealing way. Though some effort has been devoted to employing such “learn-to-exit” modules, it is still unknown whether and how well the instance difficulty can be learned. As a response, we first conduct experiments on the learnability of instance difficulty, which demonstrates that modern neural models perform poorly on predicting instance difficulty. Based on this observation, we propose a simple-yet-effective Hash-based Early Exiting approach HashEE) that replaces the learn-to-exit modules with hash functions to assign each token to a fixed exiting layer. Different from previous methods, HashEE requires no internal classifiers nor extra parameters, and therefore is more efficient. HashEE can be used in various tasks (including language understanding and generation) and model architectures such as seq2seq models. Experimental results on classification, regression, and generation tasks demonstrate that HashEE can achieve higher performance with fewer FLOPs and inference time compared with previous state-of-the-art early exiting methods.</abstract>
      <url hash="9dc7392f">2022.findings-acl.189</url>
      <bibkey>sun-etal-2022-simple</bibkey>
      <doi>10.18653/v1/2022.findings-acl.189</doi>
      <pwccode url="https://github.com/txsun1997/hashee" additional="false">txsun1997/hashee</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst-2">SST-2</pwcdataset>
    </paper>
    <paper id="190">
      <title>Auxiliary tasks to boost Biaffine Semantic Dependency Parsing</title>
      <author><first>Marie</first><last>Candito</last></author>
      <pages>2422-2429</pages>
      <abstract>The biaffine parser of (CITATION) was successfully extended to semantic dependency parsing (SDP) (CITATION). Its performance on graphs is surprisingly high given that, without the constraint of producing a tree, all arcs for a given sentence are predicted independently from each other (modulo a shared representation of tokens).To circumvent such an independence of decision, while retaining the <tex-math>O(n^2)</tex-math> complexity and highly parallelizable architecture, we propose to use simple auxiliary tasks that introduce some form of interdependence between arcs. Experiments on the three English acyclic datasets of SemEval-2015 task 18 (CITATION), and on French deep syntactic cyclic graphs (CITATION) show modest but systematic performance gains on a near-state-of-the-art baseline using transformer-based contextualized representations. This provides a simple and robust method to boost SDP performance.</abstract>
      <url hash="1c1bbd82">2022.findings-acl.190</url>
      <attachment type="software" hash="fc9f57cf">2022.findings-acl.190.software.tgz</attachment>
      <bibkey>candito-2022-auxiliary</bibkey>
      <doi>10.18653/v1/2022.findings-acl.190</doi>
      <pwccode url="https://github.com/mcandito/aux-tasks-biaffine-graph-parser-findingsacl22" additional="false">mcandito/aux-tasks-biaffine-graph-parser-findingsacl22</pwccode>
    </paper>
    <paper id="191">
      <title>Syntax-guided Contrastive Learning for Pre-trained Language Model</title>
      <author><first>Shuai</first><last>Zhang</last></author>
      <author><first>Wang</first><last>Lijie</last></author>
      <author><first>Xinyan</first><last>Xiao</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <pages>2430-2440</pages>
      <abstract>Syntactic information has been proved to be useful for transformer-based pre-trained language models. Previous studies often rely on additional syntax-guided attention components to enhance the transformer, which require more parameters and additional syntactic parsing in downstream tasks. This increase in complexity severely limits the application of syntax-enhanced language model in a wide range of scenarios. In order to inject syntactic knowledge effectively and efficiently into pre-trained language models, we propose a novel syntax-guided contrastive learning method which does not change the transformer architecture. Based on constituency and dependency structures of syntax trees, we design phrase-guided and tree-guided contrastive objectives, and optimize them in the pre-training stage, so as to help the pre-trained language model to capture rich syntactic knowledge in its representations. Experimental results show that our contrastive method achieves consistent improvements in a variety of tasks, including grammatical error detection, entity tasks, structural probing and GLUE. Detailed analysis further verifies that the improvements come from the utilization of syntactic information, and the learned attention weights are more explainable in terms of linguistics.</abstract>
      <url hash="fd00729d">2022.findings-acl.191</url>
      <bibkey>zhang-etal-2022-syntax</bibkey>
      <doi>10.18653/v1/2022.findings-acl.191</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/blimp">BLiMP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL 2003</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fce">FCE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/open-entity-1">Open Entity</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
    </paper>
    <paper id="192">
      <title>Improved Multi-label Classification under Temporal Concept Drift: Rethinking Group-Robust Algorithms in a Label-Wise Setting</title>
      <author><first>Ilias</first><last>Chalkidis</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>2441-2454</pages>
      <abstract>In document classification for, e.g., legal and biomedical text, we often deal with hundreds of classes, including very infrequent ones, as well as temporal concept drift caused by the influence of real world events, e.g., policy changes, conflicts, or pandemics. Class imbalance and drift can sometimes be mitigated by resampling the training data to simulate (or compensate for) a known target distribution, but what if the target distribution is determined by unknown future events? Instead of simply resampling uniformly to hedge our bets, we focus on the underlying optimization algorithms used to train such document classifiers and evaluate several group-robust optimization algorithms, initially proposed to mitigate group-level disparities. Reframing group-robust algorithms as adaptation algorithms under concept drift, we find that Invariant Risk Minimization and Spectral Decoupling outperform sampling-based approaches to class imbalance and concept drift, and lead to much better performance on minority classes. The effect is more pronounced the larger the label set.</abstract>
      <url hash="4bcf8dd4">2022.findings-acl.192</url>
      <attachment type="software" hash="03ea652c">2022.findings-acl.192.software.zip</attachment>
      <bibkey>chalkidis-sogaard-2022-improved</bibkey>
      <doi>10.18653/v1/2022.findings-acl.192</doi>
      <video href="2022.findings-acl.192.mp4"/>
      <pwccode url="https://github.com/coastalcph/lw-robust" additional="false">coastalcph/lw-robust</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bioasq">BioASQ</pwcdataset>
    </paper>
    <paper id="193">
      <title><fixed-case>ASCM</fixed-case>: An Answer Space Clustered Prompting Method without Answer Engineering</title>
      <author><first>Zhen</first><last>Wang</last></author>
      <author><first>Yating</first><last>Yang</last></author>
      <author><first>Zhou</first><last>Xi</last></author>
      <author><first>Bo</first><last>Ma</last></author>
      <author><first>Lei</first><last>Wang</last></author>
      <author><first>Rui</first><last>Dong</last></author>
      <author><first>Azmat</first><last>Anwar</last></author>
      <pages>2455-2469</pages>
      <abstract>Prompt-based learning, which exploits knowledge from pre-trained language models by providing textual prompts and designing appropriate answer-category mapping methods, has achieved impressive successes on few-shot text classification and natural language inference (NLI). Because of the diverse linguistic expression, there exist many answer tokens for the same category. However, both manual answer design and automatic answer search constrain answer space and therefore hardly achieve ideal performance. To address this issue, we propose an answer space clustered prompting model (ASCM) together with a synonym initialization method (SI) which automatically categorizes all answer tokens in a semantic-clustered embedding space. We also propose a stable semi-supervised method named stair learning (SL) that orderly distills knowledge from better models to weaker models. Extensive experiments demonstrate that our ASCM+SL significantly outperforms existing state-of-the-art techniques in few-shot settings.</abstract>
      <url hash="27a6ba4f">2022.findings-acl.193</url>
      <bibkey>wang-etal-2022-ascm</bibkey>
      <doi>10.18653/v1/2022.findings-acl.193</doi>
      <pwccode url="https://github.com/miaomiao1215/ascm" additional="false">miaomiao1215/ascm</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
    </paper>
    <paper id="194">
      <title>Why don’t people use character-level machine translation?</title>
      <author><first>Jindřich</first><last>Libovický</last></author>
      <author><first>Helmut</first><last>Schmid</last></author>
      <author><first>Alexander</first><last>Fraser</last></author>
      <pages>2470-2485</pages>
      <abstract>We present a literature and empirical survey that critically assesses the state of the art in character-level modeling for machine translation (MT). Despite evidence in the literature that character-level systems are comparable with subword systems, they are virtually never used in competitive setups in WMT competitions. We empirically show that even with recent modeling innovations in character-level natural language processing, character-level MT systems still struggle to match their subword-based counterparts. Character-level MT systems show neither better domain robustness, nor better morphological generalization, despite being often so motivated. However, we are able to show robustness towards source side noise and that translation quality does not degrade with increasing beam size at decoding time.</abstract>
      <url hash="cb530acb">2022.findings-acl.194</url>
      <attachment type="software" hash="d50242f4">2022.findings-acl.194.software.tgz</attachment>
      <bibkey>libovicky-etal-2022-dont</bibkey>
      <doi>10.18653/v1/2022.findings-acl.194</doi>
      <video href="2022.findings-acl.194.mp4"/>
    </paper>
    <paper id="195">
      <title>Seeking Patterns, Not just Memorizing Procedures: Contrastive Learning for Solving Math Word Problems</title>
      <author><first>Zhongli</first><last>Li</last></author>
      <author><first>Wenxuan</first><last>Zhang</last></author>
      <author><first>Chao</first><last>Yan</last></author>
      <author><first>Qingyu</first><last>Zhou</last></author>
      <author><first>Chao</first><last>Li</last></author>
      <author><first>Hongzhi</first><last>Liu</last></author>
      <author><first>Yunbo</first><last>Cao</last></author>
      <pages>2486-2496</pages>
      <abstract>Math Word Problem (MWP) solving needs to discover the quantitative relationships over natural language narratives. Recent work shows that existing models memorize procedures from context and rely on shallow heuristics to solve MWPs. In this paper, we look at this issue and argue that the cause is a lack of overall understanding of MWP patterns. We first investigate how a neural network understands patterns only from semantics, and observe that, if the prototype equations are the same, most problems get closer representations and those representations apart from them or close to other prototypes tend to produce wrong solutions. Inspired by it, we propose a contrastive learning approach, where the neural network perceives the divergence of patterns. We collect contrastive examples by converting the prototype equation into a tree and seeking similar tree structures. The solving model is trained with an auxiliary objective on the collected examples, resulting in the representations of problems with similar prototypes being pulled closer. We conduct experiments on the Chinese dataset Math23k and the English dataset MathQA. Our method greatly improves the performance in monolingual and multilingual settings.</abstract>
      <url hash="c8f2f130">2022.findings-acl.195</url>
      <bibkey>li-etal-2022-seeking</bibkey>
      <doi>10.18653/v1/2022.findings-acl.195</doi>
      <pwccode url="https://github.com/zwx980624/mwp-cl" additional="false">zwx980624/mwp-cl</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/math23k">Math23K</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mathqa">MathQA</pwcdataset>
    </paper>
    <paper id="196">
      <title>x<fixed-case>GQA</fixed-case>: Cross-Lingual Visual Question Answering</title>
      <author><first>Jonas</first><last>Pfeiffer</last></author>
      <author><first>Gregor</first><last>Geigle</last></author>
      <author><first>Aishwarya</first><last>Kamath</last></author>
      <author><first>Jan-Martin</first><last>Steitz</last></author>
      <author><first>Stefan</first><last>Roth</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <pages>2497-2511</pages>
      <abstract>Recent advances in multimodal vision and language modeling have predominantly focused on the English language, mostly due to the lack of multilingual multimodal datasets to steer modeling efforts. In this work, we address this gap and provide xGQA, a new multilingual evaluation benchmark for the visual question answering task. We extend the established English GQA dataset to 7 typologically diverse languages, enabling us to detect and explore crucial challenges in cross-lingual visual question answering. We further propose new adapter-based approaches to adapt multimodal transformer-based models to become multilingual, and—vice versa—multilingual models to become multimodal. Our proposed methods outperform current state-of-the-art multilingual multimodal models (e.g., M3P) in zero-shot cross-lingual settings, but the accuracy remains low across the board; a performance drop of around 38 accuracy points in target languages showcases the difficulty of zero-shot cross-lingual transfer for this task. Our results suggest that simple cross-lingual transfer of multimodal models yields latent multilingual multimodal misalignment, calling for more sophisticated methods for vision and multilingual language modeling.</abstract>
      <url hash="8f6fc9c4">2022.findings-acl.196</url>
      <bibkey>pfeiffer-etal-2022-xgqa</bibkey>
      <doi>10.18653/v1/2022.findings-acl.196</doi>
      <video href="2022.findings-acl.196.mp4"/>
      <pwccode url="https://github.com/adapter-hub/xgqa" additional="false">adapter-hub/xgqa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/gqa">GQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/iglue">IGLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multisubs">MultiSubs</pwcdataset>
    </paper>
    <paper id="197">
      <title>Automatic Speech Recognition and Query By Example for Creole Languages Documentation</title>
      <author><first>Cécile</first><last>Macaire</last></author>
      <author><first>Didier</first><last>Schwab</last></author>
      <author><first>Benjamin</first><last>Lecouteux</last></author>
      <author><first>Emmanuel</first><last>Schang</last></author>
      <pages>2512-2520</pages>
      <abstract>We investigate the exploitation of self-supervised models for two Creole languages with few resources: Gwadloupéyen and Morisien. Automatic language processing tools are almost non-existent for these two languages. We propose to use about one hour of annotated data to design an automatic speech recognition system for each language. We evaluate how much data is needed to obtain a query-by-example system that is usable by linguists. Moreover, our experiments show that multilingual self-supervised models are not necessarily the most efficient for Creole languages.</abstract>
      <url hash="f8c16e05">2022.findings-acl.197</url>
      <bibkey>macaire-etal-2022-automatic</bibkey>
      <doi>10.18653/v1/2022.findings-acl.197</doi>
      <video href="2022.findings-acl.197.mp4"/>
      <pwccode url="https://github.com/macairececile/asr-qbe-creole" additional="false">macairececile/asr-qbe-creole</pwccode>
    </paper>
    <paper id="198">
      <title><fixed-case>MR</fixed-case>e<fixed-case>D</fixed-case>: A Meta-Review Dataset for Structure-Controllable Text Generation</title>
      <author><first>Chenhui</first><last>Shen</last></author>
      <author><first>Liying</first><last>Cheng</last></author>
      <author><first>Ran</first><last>Zhou</last></author>
      <author><first>Lidong</first><last>Bing</last></author>
      <author><first>Yang</first><last>You</last></author>
      <author><first>Luo</first><last>Si</last></author>
      <pages>2521-2535</pages>
      <abstract>When directly using existing text generation datasets for controllable generation, we are facing the problem of not having the domain knowledge and thus the aspects that could be controlled are limited. A typical example is when using CNN/Daily Mail dataset for controllable text summarization, there is no guided information on the emphasis of summary sentences. A more useful text generator should leverage both the input text and the control signal to guide the generation, which can only be built with deep understanding of the domain knowledge. Motivated by this vision, our paper introduces a new text generation dataset, named MReD. Our new dataset consists of 7,089 meta-reviews and all its 45k meta-review sentences are manually annotated with one of the 9 carefully defined categories, including abstract, strength, decision, etc. We present experimental results on start-of-the-art summarization models, and propose methods for structure-controlled generation with both extractive and abstractive models using our annotated data. By exploring various settings and analyzing the model behavior with respect to the control signal, we demonstrate the challenges of our proposed task and the values of our dataset MReD. Meanwhile, MReD also allows us to have a better understanding of the meta-review domain.</abstract>
      <url hash="6f1cec49">2022.findings-acl.198</url>
      <bibkey>shen-etal-2022-mred</bibkey>
      <doi>10.18653/v1/2022.findings-acl.198</doi>
      <pwccode url="https://github.com/shen-chenhui/mred" additional="false">shen-chenhui/mred</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
    </paper>
    <paper id="199">
      <title>Single Model Ensemble for Subword Regularized Models in Low-Resource Machine Translation</title>
      <author><first>Sho</first><last>Takase</last></author>
      <author><first>Tatsuya</first><last>Hiraoka</last></author>
      <author><first>Naoaki</first><last>Okazaki</last></author>
      <pages>2536-2541</pages>
      <abstract>Subword regularizations use multiple subword segmentations during training to improve the robustness of neural machine translation models. In previous subword regularizations, we use multiple segmentations in the training process but use only one segmentation in the inference. In this study, we propose an inference strategy to address this discrepancy. The proposed strategy approximates the marginalized likelihood by using multiple segmentations including the most plausible segmentation and several sampled segmentations. Because the proposed strategy aggregates predictions from several segmentations, we can regard it as a single model ensemble that does not require any additional cost for training. Experimental results show that the proposed strategy improves the performance of models trained with subword regularization in low-resource machine translation tasks.</abstract>
      <url hash="16e62edb">2022.findings-acl.199</url>
      <bibkey>takase-etal-2022-single</bibkey>
      <doi>10.18653/v1/2022.findings-acl.199</doi>
    </paper>
    <paper id="200">
      <title>Detecting Various Types of Noise for Neural Machine Translation</title>
      <author><first>Christian</first><last>Herold</last></author>
      <author><first>Jan</first><last>Rosendahl</last></author>
      <author><first>Joris</first><last>Vanvinckenroye</last></author>
      <author><first>Hermann</first><last>Ney</last></author>
      <pages>2542-2551</pages>
      <abstract>The filtering and/or selection of training data is one of the core aspects to be considered when building a strong machine translation system. In their influential work, Khayrallah and Koehn (2018) investigated the impact of different types of noise on the performance of machine translation systems. In the same year the WMT introduced a shared task on parallel corpus filtering, which went on to be repeated in the following years, and resulted in many different filtering approaches being proposed. In this work we aim to combine the recent achievements in data filtering with the original analysis of Khayrallah and Koehn (2018) and investigate whether state-of-the-art filtering systems are capable of removing all the suggested noise types. We observe that most of these types of noise can be detected with an accuracy of over 90% by modern filtering systems when operating in a well studied high resource setting. However, we also find that when confronted with more refined noise categories or when working with a less common language pair, the performance of the filtering systems is far from optimal, showing that there is still room for improvement in this area of research.</abstract>
      <url hash="e5c38443">2022.findings-acl.200</url>
      <bibkey>herold-etal-2022-detecting</bibkey>
      <doi>10.18653/v1/2022.findings-acl.200</doi>
      <video href="2022.findings-acl.200.mp4"/>
    </paper>
    <paper id="201">
      <title><fixed-case>DU</fixed-case>-<fixed-case>VLG</fixed-case>: Unifying Vision-and-Language Generation via Dual Sequence-to-Sequence Pre-training</title>
      <author><first>Luyang</first><last>Huang</last></author>
      <author><first>Guocheng</first><last>Niu</last></author>
      <author><first>Jiachen</first><last>Liu</last></author>
      <author><first>Xinyan</first><last>Xiao</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <pages>2552-2566</pages>
      <abstract>Due to the limitations of the model structure and pre-training objectives, existing vision-and-language generation models cannot utilize pair-wise images and text through bi-directional generation. In this paper, we propose DU-VLG, a framework which unifies vision-and-language generation as sequence generation problems. DU-VLG is trained with novel dual pre-training tasks: multi-modal denoising autoencoder tasks and modality translation tasks. To bridge the gap between image understanding and generation, we further design a novel commitment loss. We compare pre-training objectives on image captioning and text-to-image generation datasets. Results show that DU-VLG yields better performance than variants trained with uni-directional generation objectives or the variant without the commitment loss. We also obtain higher scores compared to previous state-of-the-art systems on three vision-and-language generation tasks. In addition, human judges further confirm that our model generates real and relevant images as well as faithful and informative captions.</abstract>
      <url hash="a6c16797">2022.findings-acl.201</url>
      <bibkey>huang-etal-2022-du</bibkey>
      <doi>10.18653/v1/2022.findings-acl.201</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">MS COCO</pwcdataset>
    </paper>
    <paper id="202">
      <title><fixed-case>H</fixed-case>i<fixed-case>CLRE</fixed-case>: A Hierarchical Contrastive Learning Framework for Distantly Supervised Relation Extraction</title>
      <author><first>Dongyang</first><last>Li</last></author>
      <author><first>Taolin</first><last>Zhang</last></author>
      <author><first>Nan</first><last>Hu</last></author>
      <author><first>Chengyu</first><last>Wang</last></author>
      <author><first>Xiaofeng</first><last>He</last></author>
      <pages>2567-2578</pages>
      <abstract>Distant supervision assumes that any sentence containing the same entity pairs reflects identical relationships. Previous works of distantly supervised relation extraction (DSRE) task generally focus on sentence-level or bag-level de-noising techniques independently, neglecting the explicit interaction with cross levels. In this paper, we propose a hierarchical contrastive learning Framework for Distantly Supervised relation extraction (HiCLRE) to reduce noisy sentences, which integrate the global structural information and local fine-grained interaction. Specifically, we propose a three-level hierarchical learning framework to interact with cross levels, generating the de-noising context-aware representations via adapting the existing multi-head self-attention, named Multi-Granularity Recontextualization. Meanwhile, pseudo positive samples are also provided in the specific level for contrastive learning via a dynamic gradient-based data augmentation strategy, named Dynamic Gradient Adversarial Perturbation. Experiments demonstrate that HiCLRE significantly outperforms strong baselines in various mainstream DSRE datasets.</abstract>
      <url hash="49c2116c">2022.findings-acl.202</url>
      <bibkey>li-etal-2022-hiclre</bibkey>
      <doi>10.18653/v1/2022.findings-acl.202</doi>
      <pwccode url="https://github.com/matnlp/hiclre" additional="false">matnlp/hiclre</pwccode>
    </paper>
    <paper id="203">
      <title>Prompt-Driven Neural Machine Translation</title>
      <author><first>Yafu</first><last>Li</last></author>
      <author><first>Yongjing</first><last>Yin</last></author>
      <author><first>Jing</first><last>Li</last></author>
      <author><first>Yue</first><last>Zhang</last></author>
      <pages>2579-2590</pages>
      <abstract>Neural machine translation (NMT) has obtained significant performance improvement over the recent years. However, NMT models still face various challenges including fragility and lack of style flexibility. Moreover, current methods for instance-level constraints are limited in that they are either constraint-specific or model-specific. To this end, we propose prompt-driven neural machine translation to incorporate prompts for enhancing translation control and enriching flexibility. Empirical results demonstrate the effectiveness of our method in both prompt responding and translation quality. Through human evaluation, we further show the flexibility of prompt control and the efficiency in human-in-the-loop translation.</abstract>
      <url hash="2e9cfdd3">2022.findings-acl.203</url>
      <bibkey>li-etal-2022-prompt</bibkey>
      <doi>10.18653/v1/2022.findings-acl.203</doi>
      <pwccode url="https://github.com/yafuly/promptnmt" additional="false">yafuly/promptnmt</pwccode>
    </paper>
    <paper id="204">
      <title>On Controlling Fallback Responses for Grounded Dialogue Generation</title>
      <author><first>Hongyuan</first><last>Lu</last></author>
      <author><first>Wai</first><last>Lam</last></author>
      <author><first>Hong</first><last>Cheng</last></author>
      <author><first>Helen</first><last>Meng</last></author>
      <pages>2591-2601</pages>
      <abstract>Dialogue agents can leverage external textual knowledge to generate responses of a higher quality. To our best knowledge, most existing works on knowledge grounded dialogue settings assume that the user intention is always answerable. Unfortunately, this is impractical as there is no guarantee that the knowledge retrievers could always retrieve the desired knowledge. Therefore, this is crucial to incorporate fallback responses to respond to unanswerable contexts appropriately while responding to the answerable contexts in an informative manner. We propose a novel framework that automatically generates a control token with the generator to bias the succeeding response towards informativeness for answerable contexts and fallback for unanswerable contexts in an end-to-end manner. Since no existing knowledge grounded dialogue dataset considers this aim, we augment the existing dataset with unanswerable contexts to conduct our experiments. Automatic and human evaluation results indicate that naively incorporating fallback responses with controlled text generation still hurts informativeness for answerable context. In contrast, our proposed framework effectively mitigates this problem while still appropriately presenting fallback responses to unanswerable contexts. Such a framework also reduces the extra burden of the additional classifier and the overheads introduced in the previous works, which operates in a pipeline manner.</abstract>
      <url hash="5c880298">2022.findings-acl.204</url>
      <attachment type="software" hash="66939f03">2022.findings-acl.204.software.zip</attachment>
      <bibkey>lu-etal-2022-controlling</bibkey>
      <doi>10.18653/v1/2022.findings-acl.204</doi>
    </paper>
    <paper id="205">
      <title><fixed-case>CRAFT</fixed-case>: A Benchmark for Causal Reasoning About Forces and in<fixed-case>T</fixed-case>eractions</title>
      <author><first>Tayfun</first><last>Ates</last></author>
      <author><first>M.</first><last>Ateşoğlu</last></author>
      <author><first>Çağatay</first><last>Yiğit</last></author>
      <author><first>Ilker</first><last>Kesen</last></author>
      <author><first>Mert</first><last>Kobas</last></author>
      <author><first>Erkut</first><last>Erdem</last></author>
      <author><first>Aykut</first><last>Erdem</last></author>
      <author><first>Tilbe</first><last>Goksun</last></author>
      <author><first>Deniz</first><last>Yuret</last></author>
      <pages>2602-2627</pages>
      <abstract>Humans are able to perceive, understand and reason about causal events. Developing models with similar physical and causal understanding capabilities is a long-standing goal of artificial intelligence. As a step towards this direction, we introduce CRAFT, a new video question answering dataset that requires causal reasoning about physical forces and object interactions. It contains 58K video and question pairs that are generated from 10K videos from 20 different virtual environments, containing various objects in motion that interact with each other and the scene. Two question categories in CRAFT include previously studied descriptive and counterfactual questions. Additionally, inspired by the Force Dynamics Theory in cognitive linguistics, we introduce a new causal question category that involves understanding the causal interactions between objects through notions like cause, enable, and prevent. Our results show that even though the questions in CRAFT are easy for humans, the tested baseline models, including existing state-of-the-art methods, do not yet deal with the challenges posed in our benchmark.</abstract>
      <url hash="b5c5714a">2022.findings-acl.205</url>
      <bibkey>ates-etal-2022-craft</bibkey>
      <doi>10.18653/v1/2022.findings-acl.205</doi>
      <video href="2022.findings-acl.205.mp4"/>
      <pwccode url="https://github.com/hucvl/craft" additional="false">hucvl/craft</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/clevr">CLEVR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/phyre">PHYRE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tvqa">TVQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tvqa-1">TVQA+</pwcdataset>
    </paper>
    <paper id="206">
      <title>A Graph Enhanced <fixed-case>BERT</fixed-case> Model for Event Prediction</title>
      <author><first>Li</first><last>Du</last></author>
      <author><first>Xiao</first><last>Ding</last></author>
      <author><first>Yue</first><last>Zhang</last></author>
      <author><first>Ting</first><last>Liu</last></author>
      <author><first>Bing</first><last>Qin</last></author>
      <pages>2628-2638</pages>
      <abstract>Predicting the subsequent event for an existing event context is an important but challenging task, as it requires understanding the underlying relationship between events. Previous methods propose to retrieve relational features from event graph to enhance the modeling of event correlation. However, the sparsity of event graph may restrict the acquisition of relevant graph information, and hence influence the model performance. To address this issue, we consider automatically building of event graph using a BERT model. To this end, we incorporate an additional structured variable into BERT to learn to predict the event connections in the training process. Hence, in the test process, the connection relationship for unseen events can be predicted by the structured variable. Results on two event prediction tasks: script event prediction and story ending prediction, show that our approach can outperform state-of-the-art baseline methods.</abstract>
      <url hash="49b9d68c">2022.findings-acl.206</url>
      <attachment type="software" hash="03b914e7">2022.findings-acl.206.software.zip</attachment>
      <bibkey>du-etal-2022-graph</bibkey>
      <doi>10.18653/v1/2022.findings-acl.206</doi>
      <revision id="1" href="2022.findings-acl.206v1" hash="e8511dac"/>
      <revision id="2" href="2022.findings-acl.206v2" hash="49b9d68c" date="2022-09-29">Author info correction.</revision>
    </paper>
    <paper id="207">
      <title>Long Time No See! Open-Domain Conversation with Long-Term Persona Memory</title>
      <author><first>Xinchao</first><last>Xu</last></author>
      <author><first>Zhibin</first><last>Gou</last></author>
      <author><first>Wenquan</first><last>Wu</last></author>
      <author><first>Zheng-Yu</first><last>Niu</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <author><first>Haifeng</first><last>Wang</last></author>
      <author><first>Shihang</first><last>Wang</last></author>
      <pages>2639-2650</pages>
      <abstract>Most of the open-domain dialogue models tend to perform poorly in the setting of long-term human-bot conversations. The possible reason is that they lack the capability of understanding and memorizing long-term dialogue history information. To address this issue, we present a novel task of Long-term Memory Conversation (LeMon) and then build a new dialogue dataset DuLeMon and a dialogue generation framework with Long-Term Memory (LTM) mechanism (called PLATO-LTM). This LTM mechanism enables our system to accurately extract and continuously update long-term persona memory without requiring multiple-session dialogue datasets for model training. To our knowledge, this is the first attempt to conduct real-time dynamic management of persona information of both parties, including the user and the bot. Results on DuLeMon indicate that PLATO-LTM can significantly outperform baselines in terms of long-term dialogue consistency, leading to better dialogue engagingness.</abstract>
      <url hash="c29ec385">2022.findings-acl.207</url>
      <bibkey>xu-etal-2022-long</bibkey>
      <doi>10.18653/v1/2022.findings-acl.207</doi>
      <pwccode url="https://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2022-DuLeMon" additional="false">PaddlePaddle/Research</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/delemon">DuLeMon</pwcdataset>
    </paper>
    <paper id="208">
      <title>Lacking the Embedding of a Word? Look it up into a Traditional Dictionary</title>
      <author><first>Elena Sofia</first><last>Ruzzetti</last></author>
      <author><first>Leonardo</first><last>Ranaldi</last></author>
      <author><first>Michele</first><last>Mastromattei</last></author>
      <author><first>Francesca</first><last>Fallucchi</last></author>
      <author><first>Noemi</first><last>Scarpato</last></author>
      <author><first>Fabio Massimo</first><last>Zanzotto</last></author>
      <pages>2651-2662</pages>
      <abstract>Word embeddings are powerful dictionaries, which may easily capture language variations. However, these dictionaries fail to give sense to rare words, which are surprisingly often covered by traditional dictionaries. In this paper, we propose to use definitions retrieved in traditional dictionaries to produce word embeddings for rare words. For this purpose, we introduce two methods: Definition Neural Network (DefiNNet) and Define BERT (DefBERT). In our experiments, DefiNNet and DefBERT significantly outperform state-of-the-art as well as baseline methods devised for producing embeddings of unknown words. In fact, DefiNNet significantly outperforms FastText, which implements a method for the same task-based on n-grams, and DefBERT significantly outperforms the BERT method for OOV words. Then, definitions in traditional dictionaries are useful to build word embeddings for rare words.</abstract>
      <url hash="a0d5de34">2022.findings-acl.208</url>
      <attachment type="software" hash="86b8a2fa">2022.findings-acl.208.software.zip</attachment>
      <bibkey>ruzzetti-etal-2022-lacking</bibkey>
      <doi>10.18653/v1/2022.findings-acl.208</doi>
      <video href="2022.findings-acl.208.mp4"/>
    </paper>
    <paper id="209">
      <title><fixed-case>MTR</fixed-case>ec: Multi-Task Learning over <fixed-case>BERT</fixed-case> for News Recommendation</title>
      <author><first>Qiwei</first><last>Bi</last></author>
      <author><first>Jian</first><last>Li</last></author>
      <author><first>Lifeng</first><last>Shang</last></author>
      <author><first>Xin</first><last>Jiang</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <author><first>Hanfang</first><last>Yang</last></author>
      <pages>2663-2669</pages>
      <abstract>Existing news recommendation methods usually learn news representations solely based on news titles. To sufficiently utilize other fields of news information such as category and entities, some methods treat each field as an additional feature and combine different feature vectors with attentive pooling. With the adoption of large pre-trained models like BERT in news recommendation, the above way to incorporate multi-field information may encounter challenges: the shallow feature encoding to compress the category and entity information is not compatible with the deep BERT encoding. In this paper, we propose a multi-task method to incorporate the multi-field information into BERT, which improves its news encoding capability. Besides, we modify the gradients of auxiliary tasks based on their gradient conflicts with the main task, which further boosts the model performance. Extensive experiments on the MIND news recommendation benchmark show the effectiveness of our approach.</abstract>
      <url hash="632dfd18">2022.findings-acl.209</url>
      <bibkey>bi-etal-2022-mtrec</bibkey>
      <doi>10.18653/v1/2022.findings-acl.209</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/mind">MIND</pwcdataset>
    </paper>
    <paper id="210">
      <title>Cross-domain Named Entity Recognition via Graph Matching</title>
      <author><first>Junhao</first><last>Zheng</last></author>
      <author><first>Haibin</first><last>Chen</last></author>
      <author><first>Qianli</first><last>Ma</last></author>
      <pages>2670-2680</pages>
      <abstract>Cross-domain NER is a practical yet challenging problem since the data scarcity in the real-world scenario. A common practice is first to learn a NER model in a rich-resource general domain and then adapt the model to specific domains. Due to the mismatch problem between entity types across domains, the wide knowledge in the general domain can not effectively transfer to the target domain NER model. To this end, we model the label relationship as a probability distribution and construct label graphs in both source and target label spaces. To enhance the contextual representation with label structures, we fuse the label graph into the word embedding output by BERT. By representing label relationships as graphs, we formulate cross-domain NER as a graph matching problem. Furthermore, the proposed method has good applicability with pre-training methods and is potentially capable of other cross-domain prediction tasks. Empirical results on four datasets show that our method outperforms a series of transfer learning, multi-task learning, and few-shot learning methods.</abstract>
      <url hash="2350fdc4">2022.findings-acl.210</url>
      <attachment type="software" hash="4c4335d1">2022.findings-acl.210.software.zip</attachment>
      <bibkey>zheng-etal-2022-cross</bibkey>
      <doi>10.18653/v1/2022.findings-acl.210</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/crossner">CrossNER</pwcdataset>
    </paper>
    <paper id="211">
      <title>Assessing Multilingual Fairness in Pre-trained Multimodal Representations</title>
      <author><first>Jialu</first><last>Wang</last></author>
      <author id="yang-liu-umich"><first>Yang</first><last>Liu</last></author>
      <author><first>Xin</first><last>Wang</last></author>
      <pages>2681-2695</pages>
      <abstract>Recently pre-trained multimodal models, such as CLIP, have shown exceptional capabilities towards connecting images and natural language. The textual representations in English can be desirably transferred to multilingualism and support downstream multimodal tasks for different languages. Nevertheless, the principle of multilingual fairness is rarely scrutinized: do multilingual multimodal models treat languages equally? Are their performances biased towards particular languages? To answer these questions, we view language as the fairness recipient and introduce two new fairness notions, multilingual individual fairness and multilingual group fairness, for pre-trained multimodal models. Multilingual individual fairness requires that text snippets expressing similar semantics in different languages connect similarly to images, while multilingual group fairness requires equalized predictive performance across languages. We characterize the extent to which pre-trained multilingual vision-and-language representations are individually fair across languages. However, extensive experiments demonstrate that multilingual representations do not satisfy group fairness: (1) there is a severe multilingual accuracy disparity issue; (2) the errors exhibit biases across languages conditioning the group of people in the images, including race, gender and age.</abstract>
      <url hash="d74051dc">2022.findings-acl.211</url>
      <attachment type="software" hash="5844056e">2022.findings-acl.211.software.tgz</attachment>
      <bibkey>wang-etal-2022-assessing</bibkey>
      <doi>10.18653/v1/2022.findings-acl.211</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/fairface">FairFace</pwcdataset>
    </paper>
    <paper id="212">
      <title>More Than Words: Collocation Retokenization for <fixed-case>L</fixed-case>atent <fixed-case>D</fixed-case>irichlet <fixed-case>A</fixed-case>llocation Models</title>
      <author><first>Jin</first><last>Cheevaprawatdomrong</last></author>
      <author><first>Alexandra</first><last>Schofield</last></author>
      <author><first>Attapol</first><last>Rutherford</last></author>
      <pages>2696-2704</pages>
      <abstract>Traditionally, Latent Dirichlet Allocation (LDA) ingests words in a collection of documents to discover their latent topics using word-document co-occurrences. Previous studies show that representing bigrams collocations in the input can improve topic coherence in English. However, it is unclear how to achieve the best results for languages without marked word boundaries such as Chinese and Thai. Here, we explore the use of retokenization based on chi-squared measures, <tex-math>t</tex-math>-statistics, and raw frequency to merge frequent token ngrams into collocations when preparing input to the LDA model. Based on the goodness of fit and the coherence metric, we show that topics trained with merged tokens result in topic keys that are clearer, more coherent, and more effective at distinguishing topics than those of unmerged models.</abstract>
      <url hash="6887b674">2022.findings-acl.212</url>
      <bibkey>cheevaprawatdomrong-etal-2022-words</bibkey>
      <doi>10.18653/v1/2022.findings-acl.212</doi>
      <video href="2022.findings-acl.212.mp4"/>
    </paper>
    <paper id="213">
      <title><i>Generalized but not Robust?</i> Comparing the Effects of Data Modification Methods on Out-of-Domain Generalization and Adversarial Robustness</title>
      <author><first>Tejas</first><last>Gokhale</last></author>
      <author><first>Swaroop</first><last>Mishra</last></author>
      <author><first>Man</first><last>Luo</last></author>
      <author><first>Bhavdeep</first><last>Sachdeva</last></author>
      <author><first>Chitta</first><last>Baral</last></author>
      <pages>2705-2718</pages>
      <abstract>Data modification, either via additional training datasets, data augmentation, debiasing, and dataset filtering, has been proposed as an effective solution for generalizing to out-of-domain (OOD) inputs, in both natural language processing and computer vision literature. However, the effect of data modification on adversarial robustness remains unclear. In this work, we conduct a comprehensive study of common data modification strategies and evaluate not only their in-domain and OOD performance, but also their adversarial robustness (AR).We also present results on a two-dimensional synthetic dataset to visualize the effect of each method on the training distribution. This work serves as an empirical study towards understanding the relationship between generalizing to unseen domains and defending against adversarial perturbations. Our findings suggest that more data (either via additional datasets or data augmentation) benefits both OOD accuracy and AR.However, data filtering (previously shown to improve OOD accuracy on natural language inference) hurts OOD accuracy on other tasks such as question answering and image classification. We provide insights from our experiments to inform future work in this direction.</abstract>
      <url hash="62e863c3">2022.findings-acl.213</url>
      <bibkey>gokhale-etal-2022-generalized</bibkey>
      <doi>10.18653/v1/2022.findings-acl.213</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/mnist">MNIST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mnist-m">MNIST-M</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrqa-2019">MRQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/svhn">SVHN</pwcdataset>
    </paper>
    <paper id="214">
      <title><fixed-case>ASSIST</fixed-case>: Towards Label Noise-Robust Dialogue State Tracking</title>
      <author><first>Fanghua</first><last>Ye</last></author>
      <author><first>Yue</first><last>Feng</last></author>
      <author><first>Emine</first><last>Yilmaz</last></author>
      <pages>2719-2731</pages>
      <abstract>The MultiWOZ 2.0 dataset has greatly boosted the research on dialogue state tracking (DST). However, substantial noise has been discovered in its state annotations. Such noise brings about huge challenges for training DST models robustly. Although several refined versions, including MultiWOZ 2.1-2.4, have been published recently, there are still lots of noisy labels, especially in the training set. Besides, it is costly to rectify all the problematic annotations. In this paper, instead of improving the annotation quality further, we propose a general framework, named ASSIST (lAbel noiSe-robuSt dIalogue State Tracking), to train DST models robustly from noisy labels. ASSIST first generates pseudo labels for each sample in the training set by using an auxiliary model trained on a small clean dataset, then puts the generated pseudo labels and vanilla noisy labels together to train the primary model. We show the validity of ASSIST theoretically. Experimental results also demonstrate that ASSIST improves the joint goal accuracy of DST by up to 28.16% on MultiWOZ 2.0 and 8.41% on MultiWOZ 2.4, compared to using only the vanilla noisy labels.</abstract>
      <url hash="288defd2">2022.findings-acl.214</url>
      <attachment type="software" hash="c0a77648">2022.findings-acl.214.software.zip</attachment>
      <bibkey>ye-etal-2022-assist</bibkey>
      <doi>10.18653/v1/2022.findings-acl.214</doi>
      <video href="2022.findings-acl.214.mp4"/>
      <pwccode url="https://github.com/smartyfh/dst-assist" additional="false">smartyfh/dst-assist</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multiwoz">MultiWOZ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sgd">SGD</pwcdataset>
    </paper>
    <paper id="215">
      <title>Graph Refinement for Coreference Resolution</title>
      <author><first>Lesly</first><last>Miculicich</last></author>
      <author><first>James</first><last>Henderson</last></author>
      <pages>2732-2742</pages>
      <abstract>The state-of-the-art models for coreference resolution are based on independent mention pair-wise decisions. We propose a modelling approach that learns coreference at the document-level and takes global decisions. For this purpose, we model coreference links in a graph structure where the nodes are tokens in the text, and the edges represent the relationship between them. Our model predicts the graph in a non-autoregressive manner, then iteratively refines it based on previous predictions, allowing global dependencies between decisions. The experimental results show improvements over various baselines, reinforcing the hypothesis that document-level information improves conference resolution.</abstract>
      <url hash="df6ef574">2022.findings-acl.215</url>
      <bibkey>miculicich-henderson-2022-graph</bibkey>
      <doi>10.18653/v1/2022.findings-acl.215</doi>
      <video href="2022.findings-acl.215.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/ontonotes-5-0">OntoNotes 5.0</pwcdataset>
    </paper>
    <paper id="216">
      <title><fixed-case>ECO</fixed-case> v1: Towards Event-Centric Opinion Mining</title>
      <author><first>Ruoxi</first><last>Xu</last></author>
      <author><first>Hongyu</first><last>Lin</last></author>
      <author><first>Meng</first><last>Liao</last></author>
      <author><first>Xianpei</first><last>Han</last></author>
      <author><first>Jin</first><last>Xu</last></author>
      <author><first>Wei</first><last>Tan</last></author>
      <author><first>Yingfei</first><last>Sun</last></author>
      <author><first>Le</first><last>Sun</last></author>
      <pages>2743-2753</pages>
      <abstract>Events are considered as the fundamental building blocks of the world. Mining event-centric opinions can benefit decision making, people communication, and social good. Unfortunately, there is little literature addressing event-centric opinion mining, although which significantly diverges from the well-studied entity-centric opinion mining in connotation, structure, and expression. In this paper, we propose and formulate the task of event-centric opinion mining based on event-argument structure and expression categorizing theory. We also benchmark this task by constructing a pioneer corpus and designing a two-step benchmark framework. Experiment results show that event-centric opinion mining is feasible and challenging, and the proposed task, dataset, and baselines are beneficial for future studies.</abstract>
      <url hash="323fc93a">2022.findings-acl.216</url>
      <attachment type="software" hash="f04afa7c">2022.findings-acl.216.software.zip</attachment>
      <bibkey>xu-etal-2022-eco</bibkey>
      <doi>10.18653/v1/2022.findings-acl.216</doi>
      <video href="2022.findings-acl.216.mp4"/>
    </paper>
    <paper id="217">
      <title>Deep Reinforcement Learning for Entity Alignment</title>
      <author><first>Lingbing</first><last>Guo</last></author>
      <author><first>Yuqiang</first><last>Han</last></author>
      <author><first>Qiang</first><last>Zhang</last></author>
      <author><first>Huajun</first><last>Chen</last></author>
      <pages>2754-2765</pages>
      <abstract>Embedding-based methods have attracted increasing attention in recent entity alignment (EA) studies. Although great promise they can offer, there are still several limitations. The most notable is that they identify the aligned entities based on cosine similarity, ignoring the semantics underlying the embeddings themselves. Furthermore, these methods are shortsighted, heuristically selecting the closest entity as the target and allowing multiple entities to match the same candidate. To address these limitations, we model entity alignment as a sequential decision-making task, in which an agent sequentially decides whether two entities are matched or mismatched based on their representation vectors. The proposed reinforcement learning (RL)-based entity alignment framework can be flexibly adapted to most embedding-based EA methods. The experimental results demonstrate that it consistently advances the performance of several state-of-the-art methods, with a maximum improvement of 31.1% on Hits@1.</abstract>
      <url hash="d56eb2b6">2022.findings-acl.217</url>
      <attachment type="software" hash="b2de8f78">2022.findings-acl.217.software.zip</attachment>
      <bibkey>guo-etal-2022-deep</bibkey>
      <doi>10.18653/v1/2022.findings-acl.217</doi>
      <pwccode url="https://github.com/guolingbing/rlea" additional="false">guolingbing/rlea</pwccode>
    </paper>
    <paper id="218">
      <title>Breaking Down Multilingual Machine Translation</title>
      <author><first>Ting-Rui</first><last>Chiang</last></author>
      <author><first>Yi-Pei</first><last>Chen</last></author>
      <author><first>Yi-Ting</first><last>Yeh</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <pages>2766-2780</pages>
      <abstract>While multilingual training is now an essential ingredient in machine translation (MT) systems, recent work has demonstrated that it has different effects in different multilingual settings, such as many-to-one, one-to-many, and many-to-many learning. These training settings expose the encoder and the decoder in a machine translation model with different data distributions. In this paper, we examine how different varieties of multilingual training contribute to learning these two components of the MT model. Specifically, we compare bilingual models with encoders and/or decoders initialized by multilingual training. We show that multilingual training is beneficial to encoders in general, while it only benefits decoders for low-resource languages (LRLs). We further find the important attention heads for each language pair and compare their correlations during inference. Our analysis sheds light on how multilingual translation models work and also enables us to propose methods to improve performance by training with highly related languages. Our many-to-one models for high-resource languages and one-to-many models for LRL outperform the best results reported by Aharoni et al. (2019).</abstract>
      <url hash="72ed420c">2022.findings-acl.218</url>
      <bibkey>chiang-etal-2022-breaking</bibkey>
      <doi>10.18653/v1/2022.findings-acl.218</doi>
    </paper>
    <paper id="219">
      <title>Mitigating Contradictions in Dialogue Based on Contrastive Learning</title>
      <author><first>Weizhao</first><last>Li</last></author>
      <author><first>Junsheng</first><last>Kong</last></author>
      <author><first>Ben</first><last>Liao</last></author>
      <author><first>Yi</first><last>Cai</last></author>
      <pages>2781-2788</pages>
      <abstract>Chatbot models have achieved remarkable progress in recent years but tend to yield contradictory responses. In this paper, we exploit the advantage of contrastive learning technique to mitigate this issue. To endow the model with the ability of discriminating contradictory patterns, we minimize the similarity between the target response and contradiction related negative example. The negative example is generated with learnable latent noise, which receives contradiction related feedback from the pretrained critic. Experimental results show that our method helps to avoid contradictions in response generation while preserving response fluency, outperforming existing methods on both automatic and human evaluation.</abstract>
      <url hash="24ff9057">2022.findings-acl.219</url>
      <attachment type="software" hash="8c6b78e0">2022.findings-acl.219.software.zip</attachment>
      <bibkey>li-etal-2022-mitigating</bibkey>
      <doi>10.18653/v1/2022.findings-acl.219</doi>
    </paper>
    <paper id="220">
      <title><fixed-case>ELLE</fixed-case>: Efficient Lifelong Pre-training for Emerging Data</title>
      <author><first>Yujia</first><last>Qin</last></author>
      <author><first>Jiajie</first><last>Zhang</last></author>
      <author><first>Yankai</first><last>Lin</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Peng</first><last>Li</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>2789-2810</pages>
      <abstract>Current pre-trained language models (PLM) are typically trained with static data, ignoring that in real-world scenarios, streaming data of various sources may continuously grow. This requires PLMs to integrate the information from all the sources in a lifelong manner. Although this goal could be achieved by exhaustive pre-training on all the existing data, such a process is known to be computationally expensive. To this end, we propose ELLE, aiming at efficient lifelong pre-training for emerging data. Specifically, ELLE consists of (1) function preserved model expansion, which flexibly expands an existing PLM’s width and depth to improve the efficiency of knowledge acquisition; and (2) pre-trained domain prompts, which disentangle the versatile knowledge learned during pre-training and stimulate the proper knowledge for downstream tasks. We experiment ELLE with streaming data from 5 domains on BERT and GPT. The results show the superiority of ELLE over various lifelong learning baselines in both pre-training efficiency and downstream performances. The codes are publicly available at <url>https://github.com/thunlp/ELLE</url>.</abstract>
      <url hash="767b6e04">2022.findings-acl.220</url>
      <attachment type="software" hash="37e6acdc">2022.findings-acl.220.software.zip</attachment>
      <bibkey>qin-etal-2022-elle</bibkey>
      <doi>10.18653/v1/2022.findings-acl.220</doi>
      <pwccode url="https://github.com/thunlp/elle" additional="false">thunlp/elle</pwccode>
    </paper>
    <paper id="221">
      <title><fixed-case>E</fixed-case>n<fixed-case>CBP</fixed-case>: A New Benchmark Dataset for Finer-Grained Cultural Background Prediction in <fixed-case>E</fixed-case>nglish</title>
      <author><first>Weicheng</first><last>Ma</last></author>
      <author><first>Samiha</first><last>Datta</last></author>
      <author><first>Lili</first><last>Wang</last></author>
      <author><first>Soroush</first><last>Vosoughi</last></author>
      <pages>2811-2823</pages>
      <abstract>While cultural backgrounds have been shown to affect linguistic expressions, existing natural language processing (NLP) research on culture modeling is overly coarse-grained and does not examine cultural differences among speakers of the same language. To address this problem and augment NLP models with cultural background features, we collect, annotate, manually validate, and benchmark EnCBP, a finer-grained news-based cultural background prediction dataset in English. Through language modeling (LM) evaluations and manual analyses, we confirm that there are noticeable differences in linguistic expressions among five English-speaking countries and across four states in the US. Additionally, our evaluations on nine syntactic (CoNLL-2003), semantic (PAWS-Wiki, QNLI, STS-B, and RTE), and psycholinguistic tasks (SST-5, SST-2, Emotion, and Go-Emotions) show that, while introducing cultural background information does not benefit the Go-Emotions task due to text domain conflicts, it noticeably improves deep learning (DL) model performance on other tasks. Our findings strongly support the importance of cultural background modeling to a wide variety of NLP tasks and demonstrate the applicability of EnCBP in culture-related research.</abstract>
      <url hash="0ae6be5f">2022.findings-acl.221</url>
      <bibkey>ma-etal-2022-encbp</bibkey>
      <doi>10.18653/v1/2022.findings-acl.221</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/emotion">CARER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL 2003</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/goemotions">GoEmotions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst-2">SST-2</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst-5">SST-5</pwcdataset>
    </paper>
    <paper id="222">
      <title>Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models</title>
      <author><first>Robert</first><last>Logan IV</last></author>
      <author><first>Ivana</first><last>Balazevic</last></author>
      <author><first>Eric</first><last>Wallace</last></author>
      <author><first>Fabio</first><last>Petroni</last></author>
      <author><first>Sameer</first><last>Singh</last></author>
      <author><first>Sebastian</first><last>Riedel</last></author>
      <pages>2824-2835</pages>
      <abstract>Prompting language models (LMs) with training examples and task descriptions has been seen as critical to recent successes in few-shot learning. In this work, we show that finetuning LMs in the few-shot setting can considerably reduce the need for prompt engineering. In fact, one can use null prompts, prompts that contain neither task-specific templates nor training examples, and achieve competitive accuracy to manually-tuned prompts across a wide range of tasks. While finetuning LMs does introduce new parameters for each downstream task, we show that this memory overhead can be substantially reduced: finetuning only the bias terms can achieve comparable or better accuracy than standard finetuning while only updating 0.1% of the parameters. All in all, we recommend finetuning LMs for few-shot learning as it is more accurate, robust to different prompts, and can be made nearly as efficient as using frozen LMs.</abstract>
      <url hash="d4d7e92e">2022.findings-acl.222</url>
      <attachment type="software" hash="076748db">2022.findings-acl.222.software.zip</attachment>
      <bibkey>logan-iv-etal-2022-cutting</bibkey>
      <doi>10.18653/v1/2022.findings-acl.222</doi>
      <video href="2022.findings-acl.222.mp4"/>
      <pwccode url="https://github.com/ucinlp/null-prompts" additional="true">ucinlp/null-prompts</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
    </paper>
    <paper id="223">
      <title>u<fixed-case>FACT</fixed-case>: Unfaithful Alien-Corpora Training for Semantically Consistent Data-to-Text Generation</title>
      <author><first>Tisha</first><last>Anders</last></author>
      <author><first>Alexandru</first><last>Coca</last></author>
      <author id="bill-byrne"><first>Bill</first><last>Byrne</last></author>
      <pages>2836-2841</pages>
      <abstract>We propose uFACT (Un-Faithful Alien Corpora Training), a training corpus construction method for data-to-text (d2t) generation models. We show that d2t models trained on uFACT datasets generate utterances which represent the semantic content of the data sources more accurately compared to models trained on the target corpus alone. Our approach is to augment the training set of a given target corpus with alien corpora which have different semantic representations. We show that while it is important to have faithful data from the target corpus, the faithfulness of additional corpora only plays a minor role. Consequently, uFACT datasets can be constructed with large quantities of unfaithful data. We show how uFACT can be leveraged to obtain state-of-the-art results on the WebNLG benchmark using METEOR as our performance metric. Furthermore, we investigate the sensitivity of the generation faithfulness to the training corpus structure using the PARENT metric, and provide a baseline for this metric on the WebNLG (Gardent et al., 2017) benchmark to facilitate comparisons with future work.</abstract>
      <url hash="b9cccf42">2022.findings-acl.223</url>
      <bibkey>anders-etal-2022-ufact</bibkey>
      <doi>10.18653/v1/2022.findings-acl.223</doi>
      <video href="2022.findings-acl.223.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/viggo">ViGGO</pwcdataset>
    </paper>
    <paper id="224">
      <title>Good Night at 4 pm?! Time Expressions in Different Cultures</title>
      <author><first>Vered</first><last>Shwartz</last></author>
      <pages>2842-2853</pages>
      <abstract>We propose the task of culture-specific time expression grounding, i.e. mapping from expressions such as “morning” in English or “Manhã” in Portuguese to specific hours in the day. We propose 3 language-agnostic methods, one of which achieves promising results on gold standard annotations that we collected for a small number of languages. We then apply this method to 27 languages and analyze the similarities across languages in the grounding of time expressions.</abstract>
      <url hash="97bb54df">2022.findings-acl.224</url>
      <bibkey>shwartz-2022-good</bibkey>
      <doi>10.18653/v1/2022.findings-acl.224</doi>
      <video href="2022.findings-acl.224.mp4"/>
      <pwccode url="https://github.com/vered1986/time_expressions" additional="false">vered1986/time_expressions</pwccode>
    </paper>
    <paper id="225">
      <title>Extracting Person Names from User Generated Text: Named-Entity Recognition for Combating Human Trafficking</title>
      <author><first>Yifei</first><last>Li</last></author>
      <author><first>Pratheeksha</first><last>Nair</last></author>
      <author><first>Kellin</first><last>Pelrine</last></author>
      <author><first>Reihaneh</first><last>Rabbany</last></author>
      <pages>2854-2868</pages>
      <abstract>Online escort advertisement websites are widely used for advertising victims of human trafficking. Domain experts agree that advertising multiple people in the same ad is a strong indicator of trafficking. Thus, extracting person names from the text of these ads can provide valuable clues for further analysis. However, Named-Entity Recognition (NER) on escort ads is challenging because the text can be noisy, colloquial and often lacking proper grammar and punctuation. Most existing state-of-the-art NER models fail to demonstrate satisfactory performance in this task. In this paper, we propose NEAT (Name Extraction Against Trafficking) for extracting person names. It effectively combines classic rule-based and dictionary extractors with a contextualized language model to capture ambiguous names (e.g penny, hazel) and adapts to adversarial changes in the text by expanding its dictionary. NEAT shows 19% improvement on average in the F1 classification score for name extraction compared to previous state-of-the-art in two domain-specific datasets.</abstract>
      <url hash="d03852e0">2022.findings-acl.225</url>
      <bibkey>li-etal-2022-extracting</bibkey>
      <doi>10.18653/v1/2022.findings-acl.225</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/wnut-2017-emerging-and-rare-entity">WNUT 2017</pwcdataset>
    </paper>
    <paper id="226">
      <title><fixed-case>O</fixed-case>ne<fixed-case>A</fixed-case>ligner: Zero-shot Cross-lingual Transfer with One Rich-Resource Language Pair for Low-Resource Sentence Retrieval</title>
      <author><first>Tong</first><last>Niu</last></author>
      <author><first>Kazuma</first><last>Hashimoto</last></author>
      <author><first>Yingbo</first><last>Zhou</last></author>
      <author><first>Caiming</first><last>Xiong</last></author>
      <pages>2869-2882</pages>
      <abstract>Aligning parallel sentences in multilingual corpora is essential to curating data for downstream applications such as Machine Translation. In this work, we present OneAligner, an alignment model specially designed for sentence retrieval tasks. This model is able to train on only one language pair and transfers, in a cross-lingual fashion, to low-resource language pairs with negligible degradation in performance. When trained with all language pairs of a large-scale parallel multilingual corpus (OPUS-100), this model achieves the state-of-the-art result on the Tateoba dataset, outperforming an equally-sized previous model by 8.0 points in accuracy while using less than 0.6% of their parallel data. When finetuned on a single rich-resource language pair, be it English-centered or not, our model is able to match the performance of the ones finetuned on all language pairs under the same data budget with less than 2.0 points decrease in accuracy. Furthermore, with the same setup, scaling up the number of rich-resource language pairs monotonically improves the performance, reaching a minimum of 0.4 points discrepancy in accuracy, making it less mandatory to collect any low-resource parallel data. Finally, we conclude through empirical results and analyses that the performance of the sentence alignment task depends mostly on the monolingual and parallel data size, up to a certain size threshold, rather than on what language pairs are used for training or evaluation.</abstract>
      <url hash="4cbfa809">2022.findings-acl.226</url>
      <bibkey>niu-etal-2022-onealigner</bibkey>
      <doi>10.18653/v1/2022.findings-acl.226</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/cc100">CC100</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikimatrix">WikiMatrix</pwcdataset>
    </paper>
    <paper id="227">
      <title>Suum Cuique: Studying Bias in Taboo Detection with a Community Perspective</title>
      <author><first>Osama</first><last>Khalid</last></author>
      <author><first>Jonathan</first><last>Rusert</last></author>
      <author><first>Padmini</first><last>Srinivasan</last></author>
      <pages>2883-2896</pages>
      <abstract>Prior research has discussed and illustrated the need to consider linguistic norms at the community level when studying taboo (hateful/offensive/toxic etc.) language. However, a methodology for doing so, that is firmly founded on community language norms is still largely absent. This can lead both to biases in taboo text classification and limitations in our understanding of the causes of bias. We propose a method to study bias in taboo classification and annotation where a community perspective is front and center. This is accomplished by using special classifiers tuned for each community’s language. In essence, these classifiers represent community level language norms. We use these to study bias and find, for example, biases are largest against African Americans (7/10 datasets and all 3 classifiers examined). In contrast to previous papers we also study other communities and find, for example, strong biases against South Asians. In a small scale user study we illustrate our key idea which is that common utterances, i.e., those with high alignment scores with a community (community classifier confidence scores) are unlikely to be regarded taboo. Annotators who are community members contradict taboo classification decisions and annotations in a majority of instances. This paper is a significant step toward reducing false positive taboo decisions that over time harm minority communities.</abstract>
      <url hash="7a5cf3cb">2022.findings-acl.227</url>
      <attachment type="software" hash="c11ad0a0">2022.findings-acl.227.software.zip</attachment>
      <bibkey>khalid-etal-2022-suum</bibkey>
      <doi>10.18653/v1/2022.findings-acl.227</doi>
      <video href="2022.findings-acl.227.mp4"/>
      <pwccode url="https://github.com/jonrusert/suumcuique" additional="false">jonrusert/suumcuique</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/olid">OLID</pwcdataset>
    </paper>
    <paper id="228">
      <title>Modeling Intensification for Sign Language Generation: A Computational Approach</title>
      <author><first>Mert</first><last>Inan</last></author>
      <author><first>Yang</first><last>Zhong</last></author>
      <author><first>Sabit</first><last>Hassan</last></author>
      <author><first>Lorna</first><last>Quandt</last></author>
      <author><first>Malihe</first><last>Alikhani</last></author>
      <pages>2897-2911</pages>
      <abstract>End-to-end sign language generation models do not accurately represent the prosody in sign language. A lack of temporal and spatial variations leads to poor-quality generated presentations that confuse human interpreters. In this paper, we aim to improve the prosody in generated sign languages by modeling intensification in a data-driven manner. We present different strategies grounded in linguistics of sign language that inform how intensity modifiers can be represented in gloss annotations. To employ our strategies, we first annotate a subset of the benchmark PHOENIX-14T, a German Sign Language dataset, with different levels of intensification. We then use a supervised intensity tagger to extend the annotated dataset and obtain labels for the remaining portion of it. This enhanced dataset is then used to train state-of-the-art transformer models for sign language generation. We find that our efforts in intensification modeling yield better results when evaluated with automatic metrics. Human evaluation also indicates a higher preference of the videos generated using our model.</abstract>
      <url hash="5323b955">2022.findings-acl.228</url>
      <attachment type="software" hash="038f36c2">2022.findings-acl.228.software.zip</attachment>
      <bibkey>inan-etal-2022-modeling</bibkey>
      <doi>10.18653/v1/2022.findings-acl.228</doi>
      <video href="2022.findings-acl.228.mp4"/>
      <pwccode url="https://github.com/merterm/modeling-intensification-for-slg" additional="false">merterm/modeling-intensification-for-slg</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/phoenix14t">PHOENIX14T</pwcdataset>
    </paper>
    <paper id="229">
      <title>Controllable Natural Language Generation with Contrastive Prefixes</title>
      <author><first>Jing</first><last>Qian</last></author>
      <author><first>Li</first><last>Dong</last></author>
      <author><first>Yelong</first><last>Shen</last></author>
      <author><first>Furu</first><last>Wei</last></author>
      <author><first>Weizhu</first><last>Chen</last></author>
      <pages>2912-2924</pages>
      <abstract>To guide the generation of large pretrained language models (LM), previous work has focused on directly fine-tuning the language model or utilizing an attribute discriminator. In this work, we propose a novel lightweight framework for controllable GPT2 generation, which utilizes a set of small attribute-specific vectors, called prefixes (Li and Liang, 2021), to steer natural language generation. Different from Li and Liang (2021), where each prefix is trained independently, we take the relationship among prefixes into consideration and train multiple prefixes simultaneously. We propose a novel supervised method and also an unsupervised method to train the prefixes for single-aspect control while the combination of these two methods can achieve multi-aspect control. Experimental results on both single-aspect and multi-aspect control show that our methods can guide generation towards the desired attributes while keeping high linguistic quality.</abstract>
      <url hash="e92eac1f">2022.findings-acl.229</url>
      <bibkey>qian-etal-2022-controllable</bibkey>
      <doi>10.18653/v1/2022.findings-acl.229</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
    </paper>
    <paper id="230">
      <title>Revisiting the Effects of Leakage on Dependency Parsing</title>
      <author><first>Nathaniel</first><last>Krasner</last></author>
      <author><first>Miriam</first><last>Wanner</last></author>
      <author><first>Antonios</first><last>Anastasopoulos</last></author>
      <pages>2925-2934</pages>
      <abstract>Recent work by Søgaard (2020) showed that, treebank size aside, overlap between training and test graphs (termed <i>leakage</i>) explains more of the observed variation in dependency parsing performance than other explanations. In this work we revisit this claim, testing it on more models and languages. We find that it only holds for zero-shot cross-lingual settings. We then propose a more fine-grained measure of such leakage which, unlike the original measure, not only explains but also correlates with observed performance variation. Code and data are available here: <url>https://github.com/miriamwanner/reu-nlp-project</url></abstract>
      <url hash="9bf61937">2022.findings-acl.230</url>
      <bibkey>krasner-etal-2022-revisiting</bibkey>
      <doi>10.18653/v1/2022.findings-acl.230</doi>
      <video href="2022.findings-acl.230.mp4"/>
      <pwccode url="https://github.com/miriamwanner/reu-nlp-project" additional="false">miriamwanner/reu-nlp-project</pwccode>
    </paper>
    <paper id="231">
      <title>Learning to Describe Solutions for Bug Reports Based on Developer Discussions</title>
      <author><first>Sheena</first><last>Panthaplackel</last></author>
      <author><first>Junyi Jessy</first><last>Li</last></author>
      <author><first>Milos</first><last>Gligoric</last></author>
      <author><first>Ray</first><last>Mooney</last></author>
      <pages>2935-2952</pages>
      <abstract>When a software bug is reported, developers engage in a discussion to collaboratively resolve it. While the solution is likely formulated within the discussion, it is often buried in a large amount of text, making it difficult to comprehend and delaying its implementation. To expedite bug resolution, we propose generating a concise natural language description of the solution by synthesizing relevant content within the discussion, which encompasses both natural language and source code. We build a corpus for this task using a novel technique for obtaining noisy supervision from repository changes linked to bug reports, with which we establish benchmarks. We also design two systems for generating a description during an ongoing discussion by classifying when sufficient context for performing the task emerges in real-time. With automated and human evaluation, we find this task to form an ideal testbed for complex reasoning in long, bimodal dialogue context.</abstract>
      <url hash="c04754b3">2022.findings-acl.231</url>
      <bibkey>panthaplackel-etal-2022-learning</bibkey>
      <doi>10.18653/v1/2022.findings-acl.231</doi>
      <video href="2022.findings-acl.231.mp4"/>
      <pwccode url="https://github.com/panthap2/describing-bug-report-solutions" additional="false">panthap2/describing-bug-report-solutions</pwccode>
    </paper>
    <paper id="232">
      <title>Perturbations in the Wild: Leveraging Human-Written Text Perturbations for Realistic Adversarial Attack and Defense</title>
      <author><first>Thai</first><last>Le</last></author>
      <author><first>Jooyoung</first><last>Lee</last></author>
      <author><first>Kevin</first><last>Yen</last></author>
      <author><first>Yifan</first><last>Hu</last></author>
      <author><first>Dongwon</first><last>Lee</last></author>
      <pages>2953-2965</pages>
      <abstract>We proposes a novel algorithm, ANTHRO, that inductively extracts over 600K human-written text perturbations in the wild and leverages them for realistic adversarial attack. Unlike existing character-based attacks which often deductively hypothesize a set of manipulation strategies, our work is grounded on actual observations from real-world texts. We find that adversarial texts generated by ANTHRO achieve the best trade-off between (1) attack success rate, (2) semantic preservation of the original text, and (3) stealthiness–i.e. indistinguishable from human writings hence harder to be flagged as suspicious. Specifically, our attacks accomplished around 83% and 91% attack success rates on BERT and RoBERTa, respectively. Moreover, it outperformed the TextBugger baseline with an increase of 50% and 40% in terms of semantic preservation and stealthiness when evaluated by both layperson and professional human workers. ANTHRO can further enhance a BERT classifier’s performance in understanding different variations of human-written toxic texts via adversarial training when compared to the Perspective API.</abstract>
      <url hash="9f9bbcca">2022.findings-acl.232</url>
      <bibkey>le-etal-2022-perturbations</bibkey>
      <doi>10.18653/v1/2022.findings-acl.232</doi>
      <video href="2022.findings-acl.232.mp4"/>
      <pwccode url="https://github.com/lethaiq/perturbations-in-the-wild" additional="false">lethaiq/perturbations-in-the-wild</pwccode>
    </paper>
    <paper id="233">
      <title>Improving <fixed-case>C</fixed-case>hinese Grammatical Error Detection via Data augmentation by Conditional Error Generation</title>
      <author><first>Tianchi</first><last>Yue</last></author>
      <author><first>Shulin</first><last>Liu</last></author>
      <author><first>Huihui</first><last>Cai</last></author>
      <author><first>Tao</first><last>Yang</last></author>
      <author><first>Shengkang</first><last>Song</last></author>
      <author><first>TingHao</first><last>Yu</last></author>
      <pages>2966-2975</pages>
      <abstract>Chinese Grammatical Error Detection(CGED) aims at detecting grammatical errors in Chinese texts. One of the main challenges for CGED is the lack of annotated data. To alleviate this problem, previous studies proposed various methods to automatically generate more training samples, which can be roughly categorized into rule-based methods and model-based methods. The rule-based methods construct erroneous sentences by directly introducing noises into original sentences. However, the introduced noises are usually context-independent, which are quite different from those made by humans. The model-based methods utilize generative models to imitate human errors. The generative model may bring too many changes to the original sentences and generate semantically ambiguous sentences, so it is difficult to detect grammatical errors in these generated sentences. In addition, generated sentences may be error-free and thus become noisy data. To handle these problems, we propose CNEG, a novel Conditional Non-Autoregressive Error Generation model for generating Chinese grammatical errors. Specifically, in order to generate a context-dependent error, we first mask a span in a correct text, then predict an erroneous span conditioned on both the masked text and the correct span. Furthermore, we filter out error-free spans by measuring their perplexities in the original sentences. Experimental results show that our proposed method achieves better performance than all compared data augmentation methods on the CGED-2018 and CGED-2020 benchmarks.</abstract>
      <url hash="75d7da6e">2022.findings-acl.233</url>
      <bibkey>yue-etal-2022-improving</bibkey>
      <doi>10.18653/v1/2022.findings-acl.233</doi>
    </paper>
    <paper id="234">
      <title>Modular and Parameter-Efficient Multimodal Fusion with Prompting</title>
      <author><first>Sheng</first><last>Liang</last></author>
      <author><first>Mengjie</first><last>Zhao</last></author>
      <author><first>Hinrich</first><last>Schuetze</last></author>
      <pages>2976-2985</pages>
      <abstract>Recent research has made impressive progress in large-scale multimodal pre-training. In the context of the rapid growth of model size, it is necessary to seek efficient and flexible methods other than finetuning. In this paper, we propose to use prompt vectors to align the modalities. Our method achieves comparable performance to several other multimodal fusion methods in low-resource settings. We further show that our method is modular and parameter-efficient for processing tasks involving two or more data modalities.</abstract>
      <url hash="5cd11a03">2022.findings-acl.234</url>
      <attachment type="software" hash="8d479d2b">2022.findings-acl.234.software.zip</attachment>
      <bibkey>liang-etal-2022-modular</bibkey>
      <doi>10.18653/v1/2022.findings-acl.234</doi>
    </paper>
    <paper id="235">
      <title>Synchronous Refinement for Neural Machine Translation</title>
      <author><first>Kehai</first><last>Chen</last></author>
      <author><first>Masao</first><last>Utiyama</last></author>
      <author><first>Eiichiro</first><last>Sumita</last></author>
      <author><first>Rui</first><last>Wang</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <pages>2986-2996</pages>
      <abstract>Machine translation typically adopts an encoder-to-decoder framework, in which the decoder generates the target sentence word-by-word in an auto-regressive manner. However, the auto-regressive decoder faces a deep-rooted <tex-math>one</tex-math>-<tex-math>pass</tex-math> issue whereby each generated word is considered as one element of the final output regardless of whether it is correct or not. These generated wrong words further constitute the target historical context to affect the generation of subsequent target words. This paper proposes a novel synchronous refinement method to revise potential errors in the generated words by considering part of the target future context. Particularly, the proposed approach allows the auto-regressive decoder to refine the previously generated target words and generate the next target word synchronously. The experimental results on three widely-used machine translation tasks demonstrated the effectiveness of the proposed approach.</abstract>
      <url hash="7aa63181">2022.findings-acl.235</url>
      <bibkey>chen-etal-2022-synchronous</bibkey>
      <doi>10.18653/v1/2022.findings-acl.235</doi>
    </paper>
    <paper id="236">
      <title><fixed-case>HIE</fixed-case>-<fixed-case>SQL</fixed-case>: History Information Enhanced Network for Context-Dependent Text-to-<fixed-case>SQL</fixed-case> Semantic Parsing</title>
      <author><first>Yanzhao</first><last>Zheng</last></author>
      <author><first>Haibin</first><last>Wang</last></author>
      <author><first>Baohua</first><last>Dong</last></author>
      <author><first>Xingjun</first><last>Wang</last></author>
      <author><first>Changshan</first><last>Li</last></author>
      <pages>2997-3007</pages>
      <abstract>Recently, context-dependent text-to-SQL semantic parsing which translates natural language into SQL in an interaction process has attracted a lot of attentions. Previous works leverage context dependence information either from interaction history utterances or previous predicted queries but fail in taking advantage of both of them since of the mismatch between the natural language and logic-form SQL. In this work, we propose a History Information Enhanced text-to-SQL model (HIE-SQL) to exploit context dependence information from both history utterances and the last predicted SQL query. In view of the mismatch, we treat natural language and SQL as two modalities and propose a bimodal pre-trained model to bridge the gap between them. Besides, we design a schema-linking graph to enhance connections from utterances and the SQL query to database schema. We show our history information enhanced methods improve the performance of HIE-SQL by a significant margin, which achieves new state-of-the-art results on two context-dependent text-to-SQL benchmarks, the SparC and CoSQL datasets, at the writing time.</abstract>
      <url hash="16e2bb60">2022.findings-acl.236</url>
      <bibkey>zheng-etal-2022-hie</bibkey>
      <doi>10.18653/v1/2022.findings-acl.236</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/cosql">CoSQL</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/spider-realistic">Spider-Realistic</pwcdataset>
    </paper>
    <paper id="237">
      <title><fixed-case>CRAS</fixed-case>pell: A Contextual Typo Robust Approach to Improve <fixed-case>C</fixed-case>hinese Spelling Correction</title>
      <author><first>Shulin</first><last>Liu</last></author>
      <author><first>Shengkang</first><last>Song</last></author>
      <author><first>Tianchi</first><last>Yue</last></author>
      <author><first>Tao</first><last>Yang</last></author>
      <author><first>Huihui</first><last>Cai</last></author>
      <author><first>TingHao</first><last>Yu</last></author>
      <author><first>Shengli</first><last>Sun</last></author>
      <pages>3008-3018</pages>
      <abstract>Recently, Bert-based models have dominated the research of Chinese spelling correction (CSC). These methods have two limitations: (1) they have poor performance on multi-typo texts. In such texts, the context of each typo contains at least one misspelled character, which brings noise information. Such noisy context leads to the declining performance on multi-typo texts. (2) they tend to overcorrect valid expressions to more frequent expressions due to the masked token recovering task of Bert. We attempt to address these limitations in this paper. To make our model robust to contextual noise brought by typos, our approach first constructs a noisy context for each training sample. Then the correction model is forced to yield similar outputs based on the noisy and original contexts. Moreover, to address the overcorrection problem, copy mechanism is incorporated to encourage our model to prefer to choose the input character when the miscorrected and input character are both valid according to the given context. Experiments are conducted on widely used benchmarks. Our model achieves superior performance against state-of-the-art methods by a remarkable gain.</abstract>
      <url hash="a3083293">2022.findings-acl.237</url>
      <attachment type="software" hash="5af34dbc">2022.findings-acl.237.software.zip</attachment>
      <bibkey>liu-etal-2022-craspell</bibkey>
      <doi>10.18653/v1/2022.findings-acl.237</doi>
      <pwccode url="https://github.com/liushulinle/craspell" additional="false">liushulinle/craspell</pwccode>
    </paper>
    <paper id="238">
      <title><fixed-case>G</fixed-case>aussian Multi-head Attention for Simultaneous Machine Translation</title>
      <author><first>Shaolei</first><last>Zhang</last></author>
      <author><first>Yang</first><last>Feng</last></author>
      <pages>3019-3030</pages>
      <abstract>Simultaneous machine translation (SiMT) outputs translation while receiving the streaming source inputs, and hence needs a policy to determine where to start translating. The alignment between target and source words often implies the most informative source word for each target word, and hence provides the unified control over translation quality and latency, but unfortunately the existing SiMT methods do not explicitly model the alignment to perform the control. In this paper, we propose Gaussian Multi-head Attention (GMA) to develop a new SiMT policy by modeling alignment and translation in a unified manner. For SiMT policy, GMA models the aligned source position of each target word, and accordingly waits until its aligned position to start translating. To integrate the learning of alignment into the translation model, a Gaussian distribution centered on predicted aligned position is introduced as an alignment-related prior, which cooperates with translation-related soft attention to determine the final attention. Experiments on En-Vi and De-En tasks show that our method outperforms strong baselines on the trade-off between translation and latency.</abstract>
      <url hash="6a1080cf">2022.findings-acl.238</url>
      <bibkey>zhang-feng-2022-gaussian</bibkey>
      <doi>10.18653/v1/2022.findings-acl.238</doi>
      <pwccode url="https://github.com/ictnlp/gma" additional="false">ictnlp/gma</pwccode>
    </paper>
    <paper id="239">
      <title>Composing Structure-Aware Batches for Pairwise Sentence Classification</title>
      <author><first>Andreas</first><last>Waldis</last></author>
      <author><first>Tilman</first><last>Beck</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <pages>3031-3045</pages>
      <abstract>Identifying the relation between two sentences requires datasets with pairwise annotations. In many cases, these datasets contain instances that are annotated multiple times as part of different pairs. They constitute a structure that contains additional helpful information about the inter-relatedness of the text instances based on the annotations. This paper investigates how this kind of structural dataset information can be exploited during training. We propose three batch composition strategies to incorporate such information and measure their performance over 14 heterogeneous pairwise sentence classification tasks. Our results show statistically significant improvements (up to 3.9%) - independent of the pre-trained language model - for most tasks compared to baselines that follow a standard training procedure. Further, we see that even this baseline procedure can profit from having such structural information in a low-resource setting.</abstract>
      <url hash="a9254fbb">2022.findings-acl.239</url>
      <bibkey>waldis-etal-2022-composing</bibkey>
      <doi>10.18653/v1/2022.findings-acl.239</doi>
      <video href="2022.findings-acl.239.mp4"/>
      <pwccode url="https://github.com/ukplab/acl2022-structure-batches" additional="false">ukplab/acl2022-structure-batches</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
    </paper>
    <paper id="240">
      <title>Factual Consistency of Multilingual Pretrained Language Models</title>
      <author><first>Constanza</first><last>Fierro</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>3046-3052</pages>
      <abstract>Pretrained language models can be queried for factual knowledge, with potential applications in knowledge base acquisition and tasks that require inference. However, for that, we need to know how reliable this knowledge is, and recent work has shown that monolingual English language models lack consistency when predicting factual knowledge, that is, they fill-in-the-blank differently for paraphrases describing the same fact. In this paper, we extend the analysis of consistency to a multilingual setting. We introduce a resource, mParaRel, and investigate (i) whether multilingual language models such as mBERT and XLM-R are more consistent than their monolingual counterparts;and (ii) if such models are equally consistent across languages. We find that mBERT is as inconsistent as English BERT in English paraphrases, but that both mBERT and XLM-R exhibit a high degree of inconsistency in English and even more so for all the other 45 languages.</abstract>
      <url hash="ca0c5ebb">2022.findings-acl.240</url>
      <attachment type="software" hash="3f68fad9">2022.findings-acl.240.software.zip</attachment>
      <bibkey>fierro-sogaard-2022-factual</bibkey>
      <doi>10.18653/v1/2022.findings-acl.240</doi>
      <video href="2022.findings-acl.240.mp4"/>
      <pwccode url="https://github.com/coastalcph/mpararel" additional="false">coastalcph/mpararel</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/lama">LAMA</pwcdataset>
    </paper>
    <paper id="241">
      <title>Selecting Stickers in Open-Domain Dialogue through Multitask Learning</title>
      <author><first>Zhexin</first><last>Zhang</last></author>
      <author><first>Yeshuang</first><last>Zhu</last></author>
      <author><first>Zhengcong</first><last>Fei</last></author>
      <author><first>Jinchao</first><last>Zhang</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>3053-3060</pages>
      <abstract>With the increasing popularity of online chatting, stickers are becoming important in our online communication. Selecting appropriate stickers in open-domain dialogue requires a comprehensive understanding of both dialogues and stickers, as well as the relationship between the two types of modalities. To tackle these challenges, we propose a multitask learning method comprised of three auxiliary tasks to enhance the understanding of dialogue history, emotion and semantic meaning of stickers. Extensive experiments conducted on a recent challenging dataset show that our model can better combine the multimodal information and achieve significantly higher accuracy over strong baselines. Ablation study further verifies the effectiveness of each auxiliary task. Our code is available at <url>https://github.com/nonstopfor/Sticker-Selection</url>.</abstract>
      <url hash="fac3840a">2022.findings-acl.241</url>
      <attachment type="software" hash="629403cd">2022.findings-acl.241.software.zip</attachment>
      <bibkey>zhang-etal-2022-selecting</bibkey>
      <doi>10.18653/v1/2022.findings-acl.241</doi>
      <pwccode url="https://github.com/nonstopfor/sticker-selection" additional="false">nonstopfor/sticker-selection</pwccode>
    </paper>
    <paper id="242">
      <title><fixed-case>Z</fixed-case>i<fixed-case>N</fixed-case>et: <fixed-case>L</fixed-case>inking <fixed-case>C</fixed-case>hinese Characters Spanning Three Thousand Years</title>
      <author><first>Yang</first><last>Chi</last></author>
      <author><first>Fausto</first><last>Giunchiglia</last></author>
      <author><first>Daqian</first><last>Shi</last></author>
      <author><first>Xiaolei</first><last>Diao</last></author>
      <author><first>Chuntao</first><last>Li</last></author>
      <author><first>Hao</first><last>Xu</last></author>
      <pages>3061-3070</pages>
      <abstract>Modern Chinese characters evolved from 3,000 years ago. Up to now, tens of thousands of glyphs of ancient characters have been discovered, which must be deciphered by experts to interpret unearthed documents. Experts usually need to compare each ancient character to be examined with similar known ones in whole historical periods. However, it is inevitably limited by human memory and experience, which often cost a lot of time but associations are limited to a small scope. To help researchers discover glyph similar characters, this paper introduces ZiNet, the first diachronic knowledge base describing relationships and evolution of Chinese characters and words. In addition, powered by the knowledge of radical systems in ZiNet, this paper introduces glyph similarity measurement between ancient Chinese characters, which could capture similar glyph pairs that are potentially related in origins or semantics. Results show strong positive correlations between scores from the method and from human experts. Finally, qualitative analysis and implicit future applications are presented.</abstract>
      <url hash="61a2cf4b">2022.findings-acl.242</url>
      <attachment type="software" hash="a027bd75">2022.findings-acl.242.software.zip</attachment>
      <bibkey>chi-etal-2022-zinet</bibkey>
      <doi>10.18653/v1/2022.findings-acl.242</doi>
      <pwccode url="https://github.com/yangchijlu/ancientchinesecharsim" additional="false">yangchijlu/ancientchinesecharsim</pwccode>
    </paper>
    <paper id="243">
      <title>How Can Cross-lingual Knowledge Contribute Better to Fine-Grained Entity Typing?</title>
      <author><first>Hailong</first><last>Jin</last></author>
      <author><first>Tiansi</first><last>Dong</last></author>
      <author><first>Lei</first><last>Hou</last></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <author><first>Hui</first><last>Chen</last></author>
      <author><first>Zelin</first><last>Dai</last></author>
      <author><first>Qu</first><last>Yincen</last></author>
      <pages>3071-3081</pages>
      <abstract>Cross-lingual Entity Typing (CLET) aims at improving the quality of entity type prediction by transferring semantic knowledge learned from rich-resourced languages to low-resourced languages. In this paper, by utilizing multilingual transfer learning via the mixture-of-experts approach, our model dynamically capture the relationship between target language and each source language, and effectively generalize to predict types of unseen entities in new languages. Extensive experiments on multi-lingual datasets show that our method significantly outperforms multiple baselines and can robustly handle negative transfer. We questioned the relationship between language similarity and the performance of CLET. A series of experiments refute the commonsense that the more source the better, and suggest the Similarity Hypothesis for CLET.</abstract>
      <url hash="abe5c240">2022.findings-acl.243</url>
      <bibkey>jin-etal-2022-cross</bibkey>
      <doi>10.18653/v1/2022.findings-acl.243</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/figer">FIGER</pwcdataset>
    </paper>
    <paper id="244">
      <title><fixed-case>AMR-DA</fixed-case>: <fixed-case>D</fixed-case>ata Augmentation by <fixed-case>A</fixed-case>bstract <fixed-case>M</fixed-case>eaning <fixed-case>R</fixed-case>epresentation</title>
      <author><first>Ziyi</first><last>Shou</last></author>
      <author><first>Yuxin</first><last>Jiang</last></author>
      <author><first>Fangzhen</first><last>Lin</last></author>
      <pages>3082-3098</pages>
      <abstract>Abstract Meaning Representation (AMR) is a semantic representation for NLP/NLU. In this paper, we propose to use it for data augmentation in NLP. Our proposed data augmentation technique, called AMR-DA, converts a sample sentence to an AMR graph, modifies the graph according to various data augmentation policies, and then generates augmentations from graphs. Our method combines both sentence-level techniques like back translation and token-level techniques like EDA (Easy Data Augmentation). To evaluate the effectiveness of our method, we apply it to the tasks of semantic textual similarity (STS) and text classification. For STS, our experiments show that AMR-DA boosts the performance of the state-of-the-art models on several STS benchmarks. For text classification, AMR-DA outperforms EDA and AEDA and leads to more robust improvements.</abstract>
      <url hash="c5cea47e">2022.findings-acl.244</url>
      <bibkey>shou-etal-2022-amr</bibkey>
      <doi>10.18653/v1/2022.findings-acl.244</doi>
      <pwccode url="https://github.com/zzshou/amr-data-augmentation" additional="false">zzshou/amr-data-augmentation</pwccode>
    </paper>
    <paper id="245">
      <title>Using Pre-Trained Language Models for Producing Counter Narratives Against Hate Speech: a Comparative Study</title>
      <author><first>Serra Sinem</first><last>Tekiroğlu</last></author>
      <author><first>Helena</first><last>Bonaldi</last></author>
      <author><first>Margherita</first><last>Fanton</last></author>
      <author><first>Marco</first><last>Guerini</last></author>
      <pages>3099-3114</pages>
      <abstract>In this work, we present an extensive study on the use of pre-trained language models for the task of automatic Counter Narrative (CN) generation to fight online hate speech in English. We first present a comparative study to determine whether there is a particular Language Model (or class of LMs) and a particular decoding mechanism that are the most appropriate to generate CNs. Findings show that autoregressive models combined with stochastic decodings are the most promising. We then investigate how an LM performs in generating a CN with regard to an unseen target of hate. We find out that a key element for successful ‘out of target’ experiments is not an overall similarity with the training data but the presence of a specific subset of training data, i. e. a target that shares some commonalities with the test target that can be defined a-priori. We finally introduce the idea of a pipeline based on the addition of an automatic post-editing step to refine generated CNs.</abstract>
      <url hash="6eda3f4a">2022.findings-acl.245</url>
      <bibkey>tekiroglu-etal-2022-using</bibkey>
      <doi>10.18653/v1/2022.findings-acl.245</doi>
    </paper>
    <paper id="246">
      <title>Improving Robustness of Language Models from a Geometry-aware Perspective</title>
      <author><first>Bin</first><last>Zhu</last></author>
      <author><first>Zhaoquan</first><last>Gu</last></author>
      <author><first>Le</first><last>Wang</last></author>
      <author><first>Jinyin</first><last>Chen</last></author>
      <author><first>Qi</first><last>Xuan</last></author>
      <pages>3115-3125</pages>
      <abstract>Recent studies have found that removing the norm-bounded projection and increasing search steps in adversarial training can significantly improve robustness. However, we observe that a too large number of search steps can hurt accuracy. We aim to obtain strong robustness efficiently using fewer steps. Through a toy experiment, we find that perturbing the clean data to the decision boundary but not crossing it does not degrade the test accuracy. Inspired by this, we propose friendly adversarial data augmentation (FADA) to generate friendly adversarial data. On top of FADA, we propose geometry-aware adversarial training (GAT) to perform adversarial training on friendly adversarial data so that we can save a large number of search steps. Comprehensive experiments across two widely used datasets and three pre-trained language models demonstrate that GAT can obtain stronger robustness via fewer steps. In addition, we provide extensive empirical results and in-depth analyses on robustness to facilitate future studies.</abstract>
      <url hash="526ea1ce">2022.findings-acl.246</url>
      <attachment type="software" hash="cb33852b">2022.findings-acl.246.software.zip</attachment>
      <bibkey>zhu-etal-2022-improving</bibkey>
      <doi>10.18653/v1/2022.findings-acl.246</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst-2">SST-2</pwcdataset>
    </paper>
    <paper id="247">
      <title>Task-guided Disentangled Tuning for Pretrained Language Models</title>
      <author><first>Jiali</first><last>Zeng</last></author>
      <author><first>Yufan</first><last>Jiang</last></author>
      <author><first>Shuangzhi</first><last>Wu</last></author>
      <author><first>Yongjing</first><last>Yin</last></author>
      <author><first>Mu</first><last>Li</last></author>
      <pages>3126-3137</pages>
      <abstract>Pretrained language models (PLMs) trained on large-scale unlabeled corpus are typically fine-tuned on task-specific downstream datasets, which have produced state-of-the-art results on various NLP tasks. However, the data discrepancy issue in domain and scale makes fine-tuning fail to efficiently capture task-specific patterns, especially in low data regime. To address this issue, we propose Task-guided Disentangled Tuning (TDT) for PLMs, which enhances the generalization of representations by disentangling task-relevant signals from the entangled representations. For a given task, we introduce a learnable confidence model to detect indicative guidance from context, and further propose a disentangled regularization to mitigate the over-reliance problem. Experimental results on GLUE and CLUE benchmarks show that TDT gives consistently better results than fine-tuning with different PLMs, and extensive analysis demonstrates the effectiveness and robustness of our method. Code is available at <url>https://github.com/lemon0830/TDT</url>.</abstract>
      <url hash="96781d31">2022.findings-acl.247</url>
      <bibkey>zeng-etal-2022-task</bibkey>
      <doi>10.18653/v1/2022.findings-acl.247</doi>
      <pwccode url="https://github.com/lemon0830/tdt" additional="false">lemon0830/tdt</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/clue">CLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cmnli">CMNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
    <paper id="248">
      <title>Exploring the Impact of Negative Samples of Contrastive Learning: A Case Study of Sentence Embedding</title>
      <author><first>Rui</first><last>Cao</last></author>
      <author><first>Yihao</first><last>Wang</last></author>
      <author><first>Yuxin</first><last>Liang</last></author>
      <author><first>Ling</first><last>Gao</last></author>
      <author><first>Jie</first><last>Zheng</last></author>
      <author><first>Jie</first><last>Ren</last></author>
      <author><first>Zheng</first><last>Wang</last></author>
      <pages>3138-3152</pages>
      <abstract>Contrastive learning is emerging as a powerful technique for extracting knowledge from unlabeled data. This technique requires a balanced mixture of two ingredients: positive (similar) and negative (dissimilar) samples. This is typically achieved by maintaining a queue of negative samples during training. Prior works in the area typically uses a fixed-length negative sample queue, but how the negative sample size affects the model performance remains unclear. The opaque impact of the number of negative samples on performance when employing contrastive learning aroused our in-depth exploration. This paper presents a momentum contrastive learning model with negative sample queue for sentence embedding, namely MoCoSE. We add the prediction layer to the online branch to make the model asymmetric and together with EMA update mechanism of the target branch to prevent the model from collapsing. We define a maximum traceable distance metric, through which we learn to what extent the text contrastive learning benefits from the historical information of negative samples. Our experiments find that the best results are obtained when the maximum traceable distance is at a certain range, demonstrating that there is an optimal range of historical information for a negative sample queue. We evaluate the proposed unsupervised MoCoSE on the semantic text similarity (STS) task and obtain an average Spearman’s correlation of 77.27%. Source code is available here.</abstract>
      <url hash="03859637">2022.findings-acl.248</url>
      <bibkey>cao-etal-2022-exploring</bibkey>
      <doi>10.18653/v1/2022.findings-acl.248</doi>
      <pwccode url="https://github.com/xbdxwyh/mocose" additional="false">xbdxwyh/mocose</pwccode>
    </paper>
    <paper id="249">
      <title>The Inefficiency of Language Models in Scholarly Retrieval: An Experimental Walk-through</title>
      <author><first>Shruti</first><last>Singh</last></author>
      <author><first>Mayank</first><last>Singh</last></author>
      <pages>3153-3173</pages>
      <abstract>Language models are increasingly becoming popular in AI-powered scientific IR systems. This paper evaluates popular scientific language models in handling (i) short-query texts and (ii) textual neighbors. Our experiments showcase the inability to retrieve relevant documents for a short-query text even under the most relaxed conditions. Additionally, we leverage textual neighbors, generated by small perturbations to the original text, to demonstrate that not all perturbations lead to close neighbors in the embedding space. Further, an exhaustive categorization yields several classes of orthographically and semantically related, partially related and completely unrelated neighbors. Retrieval performance turns out to be more influenced by the surface form rather than the semantics of the text.</abstract>
      <url hash="7e85d001">2022.findings-acl.249</url>
      <bibkey>singh-singh-2022-inefficiency</bibkey>
      <doi>10.18653/v1/2022.findings-acl.249</doi>
      <pwccode url="https://github.com/shruti-singh/scilm_exp" additional="false">shruti-singh/scilm_exp</pwccode>
    </paper>
    <paper id="250">
      <title>Fusing Heterogeneous Factors with Triaffine Mechanism for Nested Named Entity Recognition</title>
      <author><first>Zheng</first><last>Yuan</last></author>
      <author><first>Chuanqi</first><last>Tan</last></author>
      <author><first>Songfang</first><last>Huang</last></author>
      <author><first>Fei</first><last>Huang</last></author>
      <pages>3174-3186</pages>
      <abstract>Nested entities are observed in many domains due to their compositionality, which cannot be easily recognized by the widely-used sequence labeling framework.A natural solution is to treat the task as a span classification problem. To learn better span representation and increase classification performance, it is crucial to effectively integrate heterogeneous factors including inside tokens, boundaries, labels, and related spans which could be contributing to nested entities recognition. To fuse these heterogeneous factors, we propose a novel triaffine mechanism including triaffine attention and scoring.Triaffine attention uses boundaries and labels as queries and uses inside tokens and related spans as keys and values for span representations.Triaffine scoring interacts with boundaries and span representations for classification. Experiments show that our proposed method outperforms previous span-based methods, achieves the state-of-the-art <tex-math>F_1</tex-math> scores on nested NER datasets GENIA and KBP2017, and shows comparable results on ACE2004 and ACE2005.</abstract>
      <url hash="6ead26b7">2022.findings-acl.250</url>
      <attachment type="software" hash="6f88efa7">2022.findings-acl.250.software.zip</attachment>
      <bibkey>yuan-etal-2022-fusing</bibkey>
      <doi>10.18653/v1/2022.findings-acl.250</doi>
      <pwccode url="https://github.com/GanjinZero/Triaffine-nested-ner" additional="false">GanjinZero/Triaffine-nested-ner</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ace-2004">ACE 2004</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ace-2005">ACE 2005</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/genia">GENIA</pwcdataset>
    </paper>
    <paper id="251">
      <title><fixed-case>UNIMO</fixed-case>-2: End-to-End Unified Vision-Language Grounded Learning</title>
      <author><first>Wei</first><last>Li</last></author>
      <author><first>Can</first><last>Gao</last></author>
      <author><first>Guocheng</first><last>Niu</last></author>
      <author><first>Xinyan</first><last>Xiao</last></author>
      <author><first>Hao</first><last>Liu</last></author>
      <author><first>Jiachen</first><last>Liu</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <author><first>Haifeng</first><last>Wang</last></author>
      <pages>3187-3201</pages>
      <abstract>Vision-Language Pre-training (VLP) has achieved impressive performance on various cross-modal downstream tasks. However, most existing methods can only learn from aligned image-caption data and rely heavily on expensive regional features, which greatly limits their scalability and performance. In this paper, we propose an end-to-end unified-modal pre-training framework, namely UNIMO-2, for joint learning on both aligned image-caption data and unaligned image-only and text-only corpus. We build a unified Transformer model to jointly learn visual representations, textual representations and semantic alignment between images and texts. In particular, we propose to conduct grounded learning on both images and texts via a sharing grounded space, which helps bridge unaligned images and texts, and align the visual and textual semantic spaces on different types of corpora. The experiments show that our grounded learning method can improve textual and visual semantic alignment for improving performance on various cross-modal tasks. Moreover, benefiting from effective joint modeling of different types of corpora, our model also achieves impressive performance on single-modal visual and textual tasks. Our code and models are public at the UNIMO project page <url>https://unimo-ptm.github.io/</url>.</abstract>
      <url hash="9aa1ec50">2022.findings-acl.251</url>
      <bibkey>li-etal-2022-unimo</bibkey>
      <doi>10.18653/v1/2022.findings-acl.251</doi>
      <pwccode url="https://github.com/PaddlePaddle/Research/tree/master/NLP/UNIMO-2" additional="false">PaddlePaddle/Research</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imagenet">ImageNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">MS COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli-ve">SNLI-VE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst-2">SST-2</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-genome">Visual Genome</pwcdataset>
    </paper>
    <paper id="252">
      <title>The Past Mistake is the Future Wisdom: Error-driven Contrastive Probability Optimization for <fixed-case>C</fixed-case>hinese Spell Checking</title>
      <author><first>Yinghui</first><last>Li</last></author>
      <author><first>Qingyu</first><last>Zhou</last></author>
      <author><first>Yangning</first><last>Li</last></author>
      <author><first>Zhongli</first><last>Li</last></author>
      <author><first>Ruiyang</first><last>Liu</last></author>
      <author><first>Rongyi</first><last>Sun</last></author>
      <author><first>Zizhen</first><last>Wang</last></author>
      <author><first>Chao</first><last>Li</last></author>
      <author><first>Yunbo</first><last>Cao</last></author>
      <author><first>Hai-Tao</first><last>Zheng</last></author>
      <pages>3202-3213</pages>
      <abstract>Chinese Spell Checking (CSC) aims to detect and correct Chinese spelling errors, which are mainly caused by the phonological or visual similarity. Recently, pre-trained language models (PLMs) promote the progress of CSC task. However, there exists a gap between the learned knowledge of PLMs and the goal of CSC task. PLMs focus on the semantics in text and tend to correct the erroneous characters to semantically proper or commonly used ones, but these aren’t the ground-truth corrections. To address this issue, we propose an Error-driven COntrastive Probability Optimization (ECOPO) framework for CSC task. ECOPO refines the knowledge representations of PLMs, and guides the model to avoid predicting these common characters through an error-driven way. Particularly, ECOPO is model-agnostic and it can be combined with existing CSC methods to achieve better performance. Extensive experiments and detailed analyses on SIGHAN datasets demonstrate that ECOPO is simple yet effective.</abstract>
      <url hash="1c40387b">2022.findings-acl.252</url>
      <attachment type="software" hash="a574d7f1">2022.findings-acl.252.software.zip</attachment>
      <bibkey>li-etal-2022-past</bibkey>
      <doi>10.18653/v1/2022.findings-acl.252</doi>
    </paper>
    <paper id="253">
      <title><fixed-case>XFUND</fixed-case>: A Benchmark Dataset for Multilingual Visually Rich Form Understanding</title>
      <author><first>Yiheng</first><last>Xu</last></author>
      <author><first>Tengchao</first><last>Lv</last></author>
      <author><first>Lei</first><last>Cui</last></author>
      <author><first>Guoxin</first><last>Wang</last></author>
      <author><first>Yijuan</first><last>Lu</last></author>
      <author><first>Dinei</first><last>Florencio</last></author>
      <author><first>Cha</first><last>Zhang</last></author>
      <author><first>Furu</first><last>Wei</last></author>
      <pages>3214-3224</pages>
      <abstract>Multimodal pre-training with text, layout, and image has achieved SOTA performance for visually rich document understanding tasks recently, which demonstrates the great potential for joint learning across different modalities. However, the existed research work has focused only on the English domain while neglecting the importance of multilingual generalization. In this paper, we introduce a human-annotated multilingual form understanding benchmark dataset named XFUND, which includes form understanding samples in 7 languages (Chinese, Japanese, Spanish, French, Italian, German, Portuguese). Meanwhile, we present LayoutXLM, a multimodal pre-trained model for multilingual document understanding, which aims to bridge the language barriers for visually rich document understanding. Experimental results show that the LayoutXLM model has significantly outperformed the existing SOTA cross-lingual pre-trained models on the XFUND dataset. The XFUND dataset and the pre-trained LayoutXLM model have been publicly available at <url>https://aka.ms/layoutxlm</url>.</abstract>
      <url hash="f1297ee0">2022.findings-acl.253</url>
      <attachment type="software" hash="c020263a">2022.findings-acl.253.software.zip</attachment>
      <bibkey>xu-etal-2022-xfund</bibkey>
      <doi>10.18653/v1/2022.findings-acl.253</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/funsd">FUNSD</pwcdataset>
    </paper>
    <paper id="254">
      <title>Type-Driven Multi-Turn Corrections for Grammatical Error Correction</title>
      <author><first>Shaopeng</first><last>Lai</last></author>
      <author><first>Qingyu</first><last>Zhou</last></author>
      <author><first>Jiali</first><last>Zeng</last></author>
      <author><first>Zhongli</first><last>Li</last></author>
      <author><first>Chao</first><last>Li</last></author>
      <author><first>Yunbo</first><last>Cao</last></author>
      <author><first>Jinsong</first><last>Su</last></author>
      <pages>3225-3236</pages>
      <abstract>Grammatical Error Correction (GEC) aims to automatically detect and correct grammatical errors. In this aspect, dominant models are trained by one-iteration learning while performing multiple iterations of corrections during inference. Previous studies mainly focus on the data augmentation approach to combat the exposure bias, which suffers from two drawbacks. First, they simply mix additionally-constructed training instances and original ones to train models, which fails to help models be explicitly aware of the procedure of gradual corrections. Second, they ignore the interdependence between different types of corrections. In this paper, we propose a Type-Driven Multi-Turn Corrections approach for GEC. Using this approach, from each training instance, we additionally construct multiple training instances, each of which involves the correction of a specific type of errors. Then, we use these additionally-constructed training instances and the original one to train the model in turn. Experimental results and in-depth analysis show that our approach significantly benefits the model training. Particularly, our enhanced model achieves state-of-the-art single-model performance on English GEC benchmarks. We release our code at Github.</abstract>
      <url hash="787655ca">2022.findings-acl.254</url>
      <attachment type="software" hash="2bcd9d5d">2022.findings-acl.254.software.zip</attachment>
      <bibkey>lai-etal-2022-type</bibkey>
      <doi>10.18653/v1/2022.findings-acl.254</doi>
      <pwccode url="https://github.com/deeplearnxmu/tmtc" additional="false">deeplearnxmu/tmtc</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fce">FCE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/locness-corpus">WI-LOCNESS</pwcdataset>
    </paper>
    <paper id="255">
      <title>Leveraging Knowledge in Multilingual Commonsense Reasoning</title>
      <author><first>Yuwei</first><last>Fang</last></author>
      <author><first>Shuohang</first><last>Wang</last></author>
      <author><first>Yichong</first><last>Xu</last></author>
      <author><first>Ruochen</first><last>Xu</last></author>
      <author><first>Siqi</first><last>Sun</last></author>
      <author><first>Chenguang</first><last>Zhu</last></author>
      <author><first>Michael</first><last>Zeng</last></author>
      <pages>3237-3246</pages>
      <abstract>Commonsense reasoning (CSR) requires models to be equipped with general world knowledge. While CSR is a language-agnostic process, most comprehensive knowledge sources are restricted to a small number of languages, especially English. Thus, it remains unclear how to effectively conduct multilingual commonsense reasoning (XCSR) for various languages. In this work, we propose to use English as a pivot language, utilizing English knowledge sources for our our commonsense reasoning framework via a translate-retrieve-translate (TRT) strategy. For multilingual commonsense questions and answer candidates, we collect related knowledge via translation and retrieval from the knowledge in the source language. The retrieved knowledge is then translated into the target language and integrated into a pre-trained multilingual language model via visible knowledge attention. Then we utilize a diverse of four English knowledge sources to provide more comprehensive coverage of knowledge in different formats. Extensive results on the XCSR benchmark demonstrate that TRT with external knowledge can significantly improve multilingual commonsense reasoning in both zero-shot and translate-train settings, consistently outperforming the state-of-the-art by more than 3% on the multilingual commonsense reasoning benchmark X-CSQA and X-CODAH.</abstract>
      <url hash="85d583b9">2022.findings-acl.255</url>
      <bibkey>fang-etal-2022-leveraging</bibkey>
      <doi>10.18653/v1/2022.findings-acl.255</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/copa">COPA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/commonsenseqa">CommonsenseQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/x-csqa">X-CSQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xcopa">XCOPA</pwcdataset>
    </paper>
    <paper id="256">
      <title>Encoding and Fusing Semantic Connection and Linguistic Evidence for Implicit Discourse Relation Recognition</title>
      <author><first>Wei</first><last>Xiang</last></author>
      <author><first>Bang</first><last>Wang</last></author>
      <author><first>Lu</first><last>Dai</last></author>
      <author><first>Yijun</first><last>Mo</last></author>
      <pages>3247-3257</pages>
      <abstract>Prior studies use one attention mechanism to improve contextual semantic representation learning for implicit discourse relation recognition (IDRR). However, diverse relation senses may benefit from different attention mechanisms. We also argue that some linguistic relation in between two words can be further exploited for IDRR. This paper proposes a Multi-Attentive Neural Fusion (MANF) model to encode and fuse both semantic connection and linguistic evidence for IDRR. In MANF, we design a Dual Attention Network (DAN) to learn and fuse two kinds of attentive representation for arguments as its semantic connection. We also propose an Offset Matrix Network (OMN) to encode the linguistic relations of word-pairs as linguistic evidence. Our MANF model achieves the state-of-the-art results on the PDTB 3.0 corpus.</abstract>
      <url hash="dc374fbb">2022.findings-acl.256</url>
      <bibkey>xiang-etal-2022-encoding</bibkey>
      <doi>10.18653/v1/2022.findings-acl.256</doi>
      <pwccode url="https://github.com/hustminslab/manf" additional="false">hustminslab/manf</pwccode>
    </paper>
    <paper id="257">
      <title>One Agent To Rule Them All: Towards Multi-agent Conversational <fixed-case>AI</fixed-case></title>
      <author><first>Christopher</first><last>Clarke</last></author>
      <author><first>Joseph</first><last>Peper</last></author>
      <author><first>Karthik</first><last>Krishnamurthy</last></author>
      <author><first>Walter</first><last>Talamonti</last></author>
      <author><first>Kevin</first><last>Leach</last></author>
      <author><first>Walter</first><last>Lasecki</last></author>
      <author><first>Yiping</first><last>Kang</last></author>
      <author><first>Lingjia</first><last>Tang</last></author>
      <author><first>Jason</first><last>Mars</last></author>
      <pages>3258-3267</pages>
      <abstract>The increasing volume of commercially available conversational agents (CAs) on the market has resulted in users being burdened with learning and adopting multiple agents to accomplish their tasks. Though prior work has explored supporting a multitude of domains within the design of a single agent, the interaction experience suffers due to the large action space of desired capabilities. To address these problems, we introduce a new task BBAI: Black-Box Agent Integration, focusing on combining the capabilities of multiple black-box CAs at scale. We explore two techniques: question agent pairing and question response pairing aimed at resolving this task. Leveraging these techniques, we design One For All (OFA), a scalable system that provides a unified interface to interact with multiple CAs. Additionally, we introduce MARS: Multi-Agent Response Selection, a new encoder model for question response pairing that jointly encodes user question and agent response pairs. We demonstrate that OFA is able to automatically and accurately integrate an ensemble of commercially available CAs spanning disparate domains. Specifically, using the MARS encoder we achieve the highest accuracy on our BBAI task, outperforming strong baselines.</abstract>
      <url hash="7a5806bc">2022.findings-acl.257</url>
      <bibkey>clarke-etal-2022-one</bibkey>
      <doi>10.18653/v1/2022.findings-acl.257</doi>
      <video href="2022.findings-acl.257.mp4"/>
      <pwccode url="https://github.com/ChrisIsKing/black-box-multi-agent-integation" additional="false">ChrisIsKing/black-box-multi-agent-integation</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bbai-dataset">BBAI Dataset</pwcdataset>
    </paper>
    <paper id="258">
      <title>Word-level Perturbation Considering Word Length and Compositional Subwords</title>
      <author><first>Tatsuya</first><last>Hiraoka</last></author>
      <author><first>Sho</first><last>Takase</last></author>
      <author><first>Kei</first><last>Uchiumi</last></author>
      <author><first>Atsushi</first><last>Keyaki</last></author>
      <author><first>Naoaki</first><last>Okazaki</last></author>
      <pages>3268-3275</pages>
      <abstract>We present two simple modifications for word-level perturbation: Word Replacement considering Length (WR-L) and Compositional Word Replacement (CWR).In conventional word replacement, a word in an input is replaced with a word sampled from the entire vocabulary, regardless of the length and context of the target word.WR-L considers the length of a target word by sampling words from the Poisson distribution.CWR considers the compositional candidates by restricting the source of sampling to related words that appear in subword regularization. Experimental results showed that the combination of WR-L and CWR improved the performance of text classification and machine translation.</abstract>
      <url hash="eb299be3">2022.findings-acl.258</url>
      <bibkey>hiraoka-etal-2022-word</bibkey>
      <doi>10.18653/v1/2022.findings-acl.258</doi>
      <pwccode url="https://github.com/tathi/cwr" additional="false">tathi/cwr</pwccode>
    </paper>
    <paper id="259">
      <title>Bridging Pre-trained Language Models and Hand-crafted Features for Unsupervised <fixed-case>POS</fixed-case> Tagging</title>
      <author><first>Houquan</first><last>Zhou</last></author>
      <author><first>Yang</first><last>Li</last></author>
      <author><first>Zhenghua</first><last>Li</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <pages>3276-3290</pages>
      <abstract>In recent years, large-scale pre-trained language models (PLMs) have made extraordinary progress in most NLP tasks. But, in the unsupervised POS tagging task, works utilizing PLMs are few and fail to achieve state-of-the-art (SOTA) performance. The recent SOTA performance is yielded by a Guassian HMM variant proposed by He et al. (2018). However, as a generative model, HMM makes very strong independence assumptions, making it very challenging to incorporate contexualized word representations from PLMs. In this work, we for the first time propose a neural conditional random field autoencoder (CRF-AE) model for unsupervised POS tagging. The discriminative encoder of CRF-AE can straightforwardly incorporate ELMo word representations. Moreover, inspired by feature-rich HMM, we reintroduce hand-crafted features into the decoder of CRF-AE. Finally, experiments clearly show that our model outperforms previous state-of-the-art models by a large margin on Penn Treebank and multilingual Universal Dependencies treebank v2.0.</abstract>
      <url hash="aee67253">2022.findings-acl.259</url>
      <attachment type="software" hash="6b9cd4a0">2022.findings-acl.259.software.zip</attachment>
      <bibkey>zhou-etal-2022-bridging</bibkey>
      <doi>10.18653/v1/2022.findings-acl.259</doi>
      <pwccode url="https://github.com/Jacob-Zhou/FeatureCRFAE" additional="false">Jacob-Zhou/FeatureCRFAE</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="260">
      <title>Controlling the Focus of Pretrained Language Generation Models</title>
      <author><first>Jiabao</first><last>Ji</last></author>
      <author><first>Yoon</first><last>Kim</last></author>
      <author><first>James</first><last>Glass</last></author>
      <author><first>Tianxing</first><last>He</last></author>
      <pages>3291-3306</pages>
      <abstract>The finetuning of pretrained transformer-based language generation models are typically conducted in an end-to-end manner, where the model learns to attend to relevant parts of the input by itself. However, there does not exist a mechanism to directly control the model’s focus. This work aims to develop a control mechanism by which a user can select spans of context as “highlights” for the model to focus on, and generate relevant output. To achieve this goal, we augment a pretrained model with trainable “focus vectors” that are directly applied to the model’s embeddings, while the model itself is kept fixed. These vectors, trained on automatic annotations derived from attribution methods, act as indicators for context importance. We test our approach on two core generation tasks: dialogue response generation and abstractive summarization. We also collect evaluation data where the highlight-generation pairs are annotated by humans. Our experiments show that the trained focus vectors are effective in steering the model to generate outputs that are relevant to user-selected highlights.</abstract>
      <url hash="c5fe7303">2022.findings-acl.260</url>
      <bibkey>ji-etal-2022-controlling</bibkey>
      <doi>10.18653/v1/2022.findings-acl.260</doi>
      <pwccode url="https://github.com/question406/learningtofocus" additional="false">question406/learningtofocus</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
    </paper>
    <paper id="261">
      <title>Comparative Opinion Summarization via Collaborative Decoding</title>
      <author><first>Hayate</first><last>Iso</last></author>
      <author><first>Xiaolan</first><last>Wang</last></author>
      <author><first>Stefanos</first><last>Angelidis</last></author>
      <author><first>Yoshihiko</first><last>Suhara</last></author>
      <pages>3307-3324</pages>
      <abstract>Opinion summarization focuses on generating summaries that reflect popular subjective information expressed in multiple online reviews. While generated summaries offer general and concise information about a particular hotel or product, the information may be insufficient to help the user compare multiple different choices. Thus, the user may still struggle with the question “Which one should I pick?” In this paper, we propose the comparative opinion summarization task, which aims at generating two contrastive summaries and one common summary from two different candidate sets of reviews. We develop a comparative summarization framework CoCoSum, which consists of two base summarization models that jointly generate contrastive and common summaries. Experimental results on a newly created benchmark CoCoTrip show that CoCoSum can produce higher-quality contrastive and common summaries than state-of-the-art opinion summarization models. The dataset and code are available at <url>https://github.com/megagonlabs/cocosum</url></abstract>
      <url hash="547a496b">2022.findings-acl.261</url>
      <bibkey>iso-etal-2022-comparative</bibkey>
      <doi>10.18653/v1/2022.findings-acl.261</doi>
      <pwccode url="https://github.com/megagonlabs/cocosum" additional="false">megagonlabs/cocosum</pwccode>
    </paper>
    <paper id="262">
      <title><fixed-case>I</fixed-case>so<fixed-case>S</fixed-case>core: Measuring the Uniformity of Embedding Space Utilization</title>
      <author><first>William</first><last>Rudman</last></author>
      <author><first>Nate</first><last>Gillman</last></author>
      <author><first>Taylor</first><last>Rayne</last></author>
      <author><first>Carsten</first><last>Eickhoff</last></author>
      <pages>3325-3339</pages>
      <abstract>The recent success of distributed word representations has led to an increased interest in analyzing the properties of their spatial distribution. Several studies have suggested that contextualized word embedding models do not isotropically project tokens into vector space. However, current methods designed to measure isotropy, such as average random cosine similarity and the partition score, have not been thoroughly analyzed and are not appropriate for measuring isotropy. We propose IsoScore: a novel tool that quantifies the degree to which a point cloud uniformly utilizes the ambient vector space. Using rigorously designed tests, we demonstrate that IsoScore is the only tool available in the literature that accurately measures how uniformly distributed variance is across dimensions in vector space. Additionally, we use IsoScore to challenge a number of recent conclusions in the NLP literature that have been derived using brittle metrics of isotropy. We caution future studies from using existing tools to measure isotropy in contextualized embedding space as resulting conclusions will be misleading or altogether inaccurate.</abstract>
      <url hash="841e9bf1">2022.findings-acl.262</url>
      <bibkey>rudman-etal-2022-isoscore</bibkey>
      <doi>10.18653/v1/2022.findings-acl.262</doi>
      <video href="2022.findings-acl.262.mp4"/>
      <pwccode url="https://github.com/bcbi-edu/p_eickhoff_isoscore" additional="false">bcbi-edu/p_eickhoff_isoscore</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-2">WikiText-2</pwcdataset>
    </paper>
    <paper id="263">
      <title>A Natural Diet: Towards Improving Naturalness of Machine Translation Output</title>
      <author><first>Markus</first><last>Freitag</last></author>
      <author><first>David</first><last>Vilar</last></author>
      <author><first>David</first><last>Grangier</last></author>
      <author><first>Colin</first><last>Cherry</last></author>
      <author><first>George</first><last>Foster</last></author>
      <pages>3340-3353</pages>
      <abstract>Machine translation (MT) evaluation often focuses on accuracy and fluency, without paying much attention to translation style. This means that, even when considered accurate and fluent, MT output can still sound less natural than high quality human translations or text originally written in the target language. Machine translation output notably exhibits lower lexical diversity, and employs constructs that mirror those in the source sentence. In this work we propose a method for training MT systems to achieve a more natural style, i.e. mirroring the style of text originally written in the target language. Our method tags parallel training data according to the naturalness of the target side by contrasting language models trained on natural and translated data. Tagging data allows us to put greater emphasis on target sentences originally written in the target language. Automatic metrics show that the resulting models achieve lexical richness on par with human translations, mimicking a style much closer to sentences originally written in the target language. Furthermore, we find that their output is preferred by human experts when compared to the baseline translations.</abstract>
      <url hash="4990c0d1">2022.findings-acl.263</url>
      <bibkey>freitag-etal-2022-natural</bibkey>
      <doi>10.18653/v1/2022.findings-acl.263</doi>
      <video href="2022.findings-acl.263.mp4"/>
    </paper>
    <paper id="264">
      <title>From Stance to Concern: Adaptation of Propositional Analysis to New Tasks and Domains</title>
      <author><first>Brodie</first><last>Mather</last></author>
      <author><first>Bonnie</first><last>Dorr</last></author>
      <author><first>Adam</first><last>Dalton</last></author>
      <author><first>William</first><last>de Beaumont</last></author>
      <author><first>Owen</first><last>Rambow</last></author>
      <author><first>Sonja</first><last>Schmer-Galunder</last></author>
      <pages>3354-3367</pages>
      <abstract>We present a generalized paradigm for adaptation of propositional analysis (predicate-argument pairs) to new tasks and domains. We leverage an analogy between stances (belief-driven sentiment) and concerns (topical issues with moral dimensions/endorsements) to produce an explanatory representation. A key contribution is the combination of semi-automatic resource building for extraction of domain-dependent concern types (with 2-4 hours of human labor per domain) and an entirely automatic procedure for extraction of domain-independent moral dimensions and endorsement values. Prudent (automatic) selection of terms from propositional structures for lexical expansion (via semantic similarity) produces new moral dimension lexicons at three levels of granularity beyond a strong baseline lexicon. We develop a ground truth (GT) based on expert annotators and compare our concern detection output to GT, to yield 231% improvement in recall over baseline, with only a 10% loss in precision. F1 yields 66% improvement over baseline and 97.8% of human performance. Our lexically based approach yields large savings over approaches that employ costly human labor and model building. We provide to the community a newly expanded moral dimension/value lexicon, annotation guidelines, and GT.</abstract>
      <url hash="96902c8f">2022.findings-acl.264</url>
      <bibkey>mather-etal-2022-stance</bibkey>
      <doi>10.18653/v1/2022.findings-acl.264</doi>
      <pwccode url="https://github.com/ihmc/findings-of-acl-2022-concern-detection" additional="false">ihmc/findings-of-acl-2022-concern-detection</pwccode>
    </paper>
    <paper id="265">
      <title><fixed-case>CUE</fixed-case> Vectors: Modular Training of Language Models Conditioned on Diverse Contextual Signals</title>
      <author><first>Scott</first><last>Novotney</last></author>
      <author><first>Sreeparna</first><last>Mukherjee</last></author>
      <author><first>Zeeshan</first><last>Ahmed</last></author>
      <author><first>Andreas</first><last>Stolcke</last></author>
      <pages>3368-3379</pages>
      <abstract>We propose a framework to modularize the training of neural language models that use diverse forms of context by eliminating the need to jointly train context and within-sentence encoders. Our approach, contextual universal embeddings (CUE), trains LMs on one type of contextual data and adapts to novel context types. The model consists of a pretrained neural sentence LM, a BERT-based contextual encoder, and a masked transfomer decoder that estimates LM probabilities using sentence-internal and contextual evidence. When contextually annotated data is unavailable, our model learns to combine contextual and sentence-internal information using noisy oracle unigram embeddings as a proxy. Real context data can be introduced later and used to adapt a small number of parameters that map contextual data into the decoder’s embedding space. We validate the CUE framework on a NYTimes text corpus with multiple metadata types, for which the LM perplexity can be lowered from 36.6 to 27.4 by conditioning on context. Bootstrapping a contextual LM with only a subset of the metadata during training retains 85% of the achievable gain. Training the model initially with proxy context retains 67% of the perplexity gain after adapting to real context. Furthermore, we can swap one type of pretrained sentence LM for another without retraining the context encoders, by only adapting the decoder model. Overall, we obtain a modular framework that allows incremental, scalable training of context-enhanced LMs.</abstract>
      <url hash="23c93066">2022.findings-acl.265</url>
      <bibkey>novotney-etal-2022-cue</bibkey>
      <doi>10.18653/v1/2022.findings-acl.265</doi>
      <video href="2022.findings-acl.265.mp4"/>
    </paper>
    <paper id="266">
      <title>Cross-Lingual <fixed-case>UMLS</fixed-case> Named Entity Linking using <fixed-case>UMLS</fixed-case> Dictionary Fine-Tuning</title>
      <author><first>Rina</first><last>Galperin</last></author>
      <author><first>Shachar</first><last>Schnapp</last></author>
      <author><first>Michael</first><last>Elhadad</last></author>
      <pages>3380-3390</pages>
      <abstract>We study cross-lingual UMLS named entity linking, where mentions in a given source language are mapped to UMLS concepts, most of which are labeled in English. Our cross-lingual framework includes an offline unsupervised construction of a translated UMLS dictionary and a per-document pipeline which identifies UMLS candidate mentions and uses a fine-tuned pretrained transformer language model to filter candidates according to context. Our method exploits a small dataset of manually annotated UMLS mentions in the source language and uses this supervised data in two ways: to extend the unsupervised UMLS dictionary and to fine-tune the contextual filtering of candidate mentions in full documents. We demonstrate results of our approach on both Hebrew and English. We achieve new state-of-the-art (SOTA) results on the Hebrew Camoni corpus, +8.9 F1 on average across three communities in the dataset. We also achieve new SOTA on the English dataset MedMentions with +7.3 F1.</abstract>
      <url hash="878ef369">2022.findings-acl.266</url>
      <attachment type="software" hash="70d7f6ef">2022.findings-acl.266.software.zip</attachment>
      <bibkey>galperin-etal-2022-cross</bibkey>
      <doi>10.18653/v1/2022.findings-acl.266</doi>
      <pwccode url="https://github.com/rinagalperin/biomedical_nel" additional="false">rinagalperin/biomedical_nel</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bc5cdr">BC5CDR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/medmentions">MedMentions</pwcdataset>
    </paper>
    <paper id="267">
      <title>Aligned Weight Regularizers for Pruning Pretrained Neural Networks</title>
      <author><first>James</first><last>O’ Neill</last></author>
      <author><first>Sourav</first><last>Dutta</last></author>
      <author><first>Haytham</first><last>Assem</last></author>
      <pages>3391-3401</pages>
      <abstract>Pruning aims to reduce the number of parameters while maintaining performance close to the original network. This work proposes a novel <i>self-distillation</i> based pruning strategy, whereby the representational similarity between the pruned and unpruned versions of the same network is maximized. Unlike previous approaches that treat distillation and pruning separately, we use distillation to inform the pruning criteria, without requiring a separate student network as in knowledge distillation. We show that the proposed <i>cross-correlation objective for self-distilled pruning</i> implicitly encourages sparse solutions, naturally complementing magnitude-based pruning criteria. Experiments on the GLUE and XGLUE benchmarks show that self-distilled pruning increases mono- and cross-lingual language model performance. Self-distilled pruned models also outperform smaller Transformers with an equal number of parameters and are competitive against (6 times) larger distilled networks. We also observe that self-distillation (1) maximizes class separability, (2) increases the signal-to-noise ratio, and (3) converges faster after pruning steps, providing further insights into why self-distilled pruning improves generalization.</abstract>
      <url hash="fdb3ff58">2022.findings-acl.267</url>
      <bibkey>o-neill-etal-2022-aligned</bibkey>
      <doi>10.18653/v1/2022.findings-acl.267</doi>
      <video href="2022.findings-acl.267.mp4"/>
    </paper>
    <paper id="268">
      <title>Consistent Representation Learning for Continual Relation Extraction</title>
      <author><first>Kang</first><last>Zhao</last></author>
      <author><first>Hua</first><last>Xu</last></author>
      <author><first>Jiangong</first><last>Yang</last></author>
      <author><first>Kai</first><last>Gao</last></author>
      <pages>3402-3411</pages>
      <abstract>Continual relation extraction (CRE) aims to continuously train a model on data with new relations while avoiding forgetting old ones. Some previous work has proved that storing a few typical samples of old relations and replaying them when learning new relations can effectively avoid forgetting. However, these memory-based methods tend to overfit the memory samples and perform poorly on imbalanced datasets. To solve these challenges, a consistent representation learning method is proposed, which maintains the stability of the relation embedding by adopting contrastive learning and knowledge distillation when replaying memory. Specifically, supervised contrastive learning based on a memory bank is first used to train each new task so that the model can effectively learn the relation representation. Then, contrastive replay is conducted of the samples in memory and makes the model retain the knowledge of historical relations through memory knowledge distillation to prevent the catastrophic forgetting of the old task. The proposed method can better learn consistent representations to alleviate forgetting effectively. Extensive experiments on FewRel and TACRED datasets show that our method significantly outperforms state-of-the-art baselines and yield strong robustness on the imbalanced dataset.</abstract>
      <url hash="99b3b70e">2022.findings-acl.268</url>
      <bibkey>zhao-etal-2022-consistent</bibkey>
      <doi>10.18653/v1/2022.findings-acl.268</doi>
      <pwccode url="https://github.com/thuiar/CRL" additional="false">thuiar/CRL</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel">FewRel</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tacred">TACRED</pwcdataset>
    </paper>
    <paper id="269">
      <title>Event Transition Planning for Open-ended Text Generation</title>
      <author><first>Qintong</first><last>Li</last></author>
      <author><first>Piji</first><last>Li</last></author>
      <author><first>Wei</first><last>Bi</last></author>
      <author><first>Zhaochun</first><last>Ren</last></author>
      <author><first>Yuxuan</first><last>Lai</last></author>
      <author><first>Lingpeng</first><last>Kong</last></author>
      <pages>3412-3426</pages>
      <abstract>Open-ended text generation tasks, such as dialogue generation and story completion, require models to generate a coherent continuation given limited preceding context. The open-ended nature of these tasks brings new challenges to the neural auto-regressive text generators nowadays. Despite these neural models are good at producing human-like text, it is difficult for them to arrange causalities and relations between given facts and possible ensuing events. To bridge this gap, we propose a novel two-stage method which explicitly arranges the ensuing events in open-ended text generation. Our approach can be understood as a specially-trained coarse-to-fine algorithm, where an event transition planner provides a “coarse” plot skeleton and a text generator in the second stage refines the skeleton. Experiments on two open-ended text generation tasks demonstrate that our proposed method effectively improves the quality of the generated text, especially in coherence and diversity. We will release the codes to the community for further exploration.</abstract>
      <url hash="d61b35ed">2022.findings-acl.269</url>
      <bibkey>li-etal-2022-event</bibkey>
      <doi>10.18653/v1/2022.findings-acl.269</doi>
      <pwccode url="https://github.com/qtli/eventplanfortextgen" additional="false">qtli/eventplanfortextgen</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/atomic">ATOMIC</pwcdataset>
    </paper>
    <paper id="270">
      <title>Comprehensive Multi-Modal Interactions for Referring Image Segmentation</title>
      <author><first>Kanishk</first><last>Jain</last></author>
      <author><first>Vineet</first><last>Gandhi</last></author>
      <pages>3427-3435</pages>
      <abstract>We investigate Referring Image Segmentation (RIS), which outputs a segmentation map corresponding to the natural language description. Addressing RIS efficiently requires considering the interactions happening across visual and linguistic modalities and the interactions within each modality. Existing methods are limited because they either compute different forms of interactions sequentially (leading to error propagation) or ignore intra-modal interactions. We address this limitation by performing all three interactions simultaneously through a Synchronous Multi-Modal Fusion Module (SFM). Moreover, to produce refined segmentation masks, we propose a novel Hierarchical Cross-Modal Aggregation Module (HCAM), where linguistic features facilitate the exchange of contextual information across the visual hierarchy. We present thorough ablation studies and validate our approach’s performance on four benchmark datasets, showing considerable performance gains over the existing state-of-the-art (SOTA) methods.</abstract>
      <url hash="bc704ead">2022.findings-acl.270</url>
      <attachment type="software" hash="12479973">2022.findings-acl.270.software.zip</attachment>
      <bibkey>jain-gandhi-2022-comprehensive</bibkey>
      <doi>10.18653/v1/2022.findings-acl.270</doi>
      <video href="2022.findings-acl.270.mp4"/>
      <pwccode url="https://github.com/kanji95/SHNET" additional="false">kanji95/SHNET</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/google-refexp">Google Refexp</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">MS COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/refcoco">RefCOCO</pwcdataset>
    </paper>
    <paper id="271">
      <title><fixed-case>M</fixed-case>eta<fixed-case>W</fixed-case>eighting: Learning to Weight Tasks in Multi-Task Learning</title>
      <author><first>Yuren</first><last>Mao</last></author>
      <author><first>Zekai</first><last>Wang</last></author>
      <author><first>Weiwei</first><last>Liu</last></author>
      <author><first>Xuemin</first><last>Lin</last></author>
      <author><first>Pengtao</first><last>Xie</last></author>
      <pages>3436-3448</pages>
      <abstract>Task weighting, which assigns weights on the including tasks during training, significantly matters the performance of Multi-task Learning (MTL); thus, recently, there has been an explosive interest in it. However, existing task weighting methods assign weights only based on the training loss, while ignoring the gap between the training loss and generalization loss. It degenerates MTL’s performance. To address this issue, the present paper proposes a novel task weighting algorithm, which automatically weights the tasks via a learning-to-learn paradigm, referred to as MetaWeighting. Extensive experiments are conducted to validate the superiority of our proposed method in multi-task text classification.</abstract>
      <url hash="4a489e26">2022.findings-acl.271</url>
      <attachment type="software" hash="b2479293">2022.findings-acl.271.software.zip</attachment>
      <bibkey>mao-etal-2022-metaweighting</bibkey>
      <doi>10.18653/v1/2022.findings-acl.271</doi>
    </paper>
    <paper id="272">
      <title>Improving Controllable Text Generation with Position-Aware Weighted Decoding</title>
      <author><first>Yuxuan</first><last>Gu</last></author>
      <author><first>Xiaocheng</first><last>Feng</last></author>
      <author><first>Sicheng</first><last>Ma</last></author>
      <author><first>Jiaming</first><last>Wu</last></author>
      <author><first>Heng</first><last>Gong</last></author>
      <author><first>Bing</first><last>Qin</last></author>
      <pages>3449-3467</pages>
      <abstract>Weighted decoding methods composed of the pretrained language model (LM) and the controller have achieved promising results for controllable text generation. However, these models often suffer from a control strength/fluency trade-off problem as higher control strength is more likely to generate incoherent and repetitive text. In this paper, we illustrate this trade-off is arisen by the controller imposing the target attribute on the LM at improper positions. And we propose a novel framework based on existing weighted decoding methods called CAT-PAW, which introduces a lightweight regulator to adjust bias signals from the controller at different decoding positions. Experiments on positive sentiment control, topic control, and language detoxification show the effectiveness of our CAT-PAW upon 4 SOTA models.</abstract>
      <url hash="f21d317b">2022.findings-acl.272</url>
      <bibkey>gu-etal-2022-improving</bibkey>
      <doi>10.18653/v1/2022.findings-acl.272</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst-5">SST-5</pwcdataset>
    </paper>
    <paper id="273">
      <title>Prompt Tuning for Discriminative Pre-trained Language Models</title>
      <author><first>Yuan</first><last>Yao</last></author>
      <author><first>Bowen</first><last>Dong</last></author>
      <author><first>Ao</first><last>Zhang</last></author>
      <author><first>Zhengyan</first><last>Zhang</last></author>
      <author><first>Ruobing</first><last>Xie</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Leyu</first><last>Lin</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <author><first>Jianyong</first><last>Wang</last></author>
      <pages>3468-3473</pages>
      <abstract>Recent works have shown promising results of prompt tuning in stimulating pre-trained language models (PLMs) for natural language processing (NLP) tasks. However, to the best of our knowledge, existing works focus on prompt-tuning generative PLMs that are pre-trained to generate target tokens, such as BERT. It is still unknown whether and how discriminative PLMs, e.g., ELECTRA, can be effectively prompt-tuned. In this work, we present DPT, the first prompt tuning framework for discriminative PLMs, which reformulates NLP tasks into a discriminative language modeling problem. Comprehensive experiments on text classification and question answering show that, compared with vanilla fine-tuning, DPT achieves significantly higher performance, and also prevents the unstable problem in tuning large PLMs in both full-set and low-resource settings.</abstract>
      <url hash="5aea6728">2022.findings-acl.273</url>
      <attachment type="software" hash="584d5101">2022.findings-acl.273.software.zip</attachment>
      <bibkey>yao-etal-2022-prompt</bibkey>
      <doi>10.18653/v1/2022.findings-acl.273</doi>
      <pwccode url="https://github.com/thunlp/dpt" additional="false">thunlp/dpt</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/quoref">Quoref</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst-2">SST-2</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst-5">SST-5</pwcdataset>
    </paper>
    <paper id="274">
      <title>Two Birds with One Stone: Unified Model Learning for Both Recall and Ranking in News Recommendation</title>
      <author><first>Chuhan</first><last>Wu</last></author>
      <author><first>Fangzhao</first><last>Wu</last></author>
      <author><first>Tao</first><last>Qi</last></author>
      <author><first>Yongfeng</first><last>Huang</last></author>
      <pages>3474-3480</pages>
      <abstract>Recall and ranking are two critical steps in personalized news recommendation. Most existing news recommender systems conduct personalized news recall and ranking separately with different models. However, maintaining multiple models leads to high computational cost and poses great challenges to meeting the online latency requirement of news recommender systems. In order to handle this problem, in this paper we propose UniRec, a unified method for recall and ranking in news recommendation. In our method, we first infer user embedding for ranking from the historical news click behaviors of a user using a user encoder model. Then we derive the user embedding for recall from the obtained user embedding for ranking by using it as the attention query to select a set of basis user embeddings which encode different general user interests and synthesize them into a user embedding for recall. The extensive experiments on benchmark dataset demonstrate that our method can improve both efficiency and effectiveness for recall and ranking in news recommendation.</abstract>
      <url hash="6f8bb7c4">2022.findings-acl.274</url>
      <bibkey>wu-etal-2022-two</bibkey>
      <doi>10.18653/v1/2022.findings-acl.274</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/mind">MIND</pwcdataset>
    </paper>
    <paper id="275">
      <title>What does it take to bake a cake? The <fixed-case>R</fixed-case>ecipe<fixed-case>R</fixed-case>ef corpus and anaphora resolution in procedural text</title>
      <author><first>Biaoyan</first><last>Fang</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <author><first>Karin</first><last>Verspoor</last></author>
      <pages>3481-3495</pages>
      <abstract>Procedural text contains rich anaphoric phenomena, yet has not received much attention in NLP. To fill this gap, we investigate the textual properties of two types of procedural text, recipes and chemical patents, and generalize an anaphora annotation framework developed for the chemical domain for modeling anaphoric phenomena in recipes. We apply this framework to annotate the RecipeRef corpus with both bridging and coreference relations. Through comparison to chemical patents, we show the complexity of anaphora resolution in recipes. We demonstrate empirically that transfer learning from the chemical domain improves resolution of anaphora in recipes, suggesting transferability of general procedural knowledge.</abstract>
      <url hash="74bab014">2022.findings-acl.275</url>
      <bibkey>fang-etal-2022-take</bibkey>
      <doi>10.18653/v1/2022.findings-acl.275</doi>
      <video href="2022.findings-acl.275.mp4"/>
      <pwccode url="https://github.com/biaoyanf/reciperef" additional="false">biaoyanf/reciperef</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bashi">BASHI</pwcdataset>
    </paper>
    <paper id="276">
      <title><fixed-case>MERI</fixed-case>t: <fixed-case>M</fixed-case>eta-<fixed-case>P</fixed-case>ath <fixed-case>G</fixed-case>uided <fixed-case>C</fixed-case>ontrastive <fixed-case>L</fixed-case>earning for <fixed-case>L</fixed-case>ogical <fixed-case>R</fixed-case>easoning</title>
      <author><first>Fangkai</first><last>Jiao</last></author>
      <author><first>Yangyang</first><last>Guo</last></author>
      <author><first>Xuemeng</first><last>Song</last></author>
      <author><first>Liqiang</first><last>Nie</last></author>
      <pages>3496-3509</pages>
      <abstract>Logical reasoning is of vital importance to natural language understanding. Previous studies either employ graph-based models to incorporate prior knowledge about logical relations, or introduce symbolic logic into neural models through data augmentation. These methods, however, heavily depend on annotated training data, and thus suffer from over-fitting and poor generalization problems due to the dataset sparsity. To address these two problems, in this paper, we propose MERIt, a MEta-path guided contrastive learning method for logical ReasonIng of text, to perform self-supervised pre-training on abundant unlabeled text data. Two novel strategies serve as indispensable components of our method. In particular, a strategy based on meta-path is devised to discover the logical structure in natural texts, followed by a counterfactual data augmentation strategy to eliminate the information shortcut induced by pre-training. The experimental results on two challenging logical reasoning benchmarks, i.e., ReClor and LogiQA, demonstrate that our method outperforms the SOTA baselines with significant improvements.</abstract>
      <url hash="dbd58be1">2022.findings-acl.276</url>
      <bibkey>jiao-etal-2022-merit</bibkey>
      <doi>10.18653/v1/2022.findings-acl.276</doi>
      <pwccode url="https://github.com/sparkjiao/merit" additional="false">sparkjiao/merit</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/logiqa">LogiQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/reclor">ReClor</pwcdataset>
    </paper>
    <paper id="277">
      <title><fixed-case>THE</fixed-case>-<fixed-case>X</fixed-case>: Privacy-Preserving Transformer Inference with Homomorphic Encryption</title>
      <author><first>Tianyu</first><last>Chen</last></author>
      <author><first>Hangbo</first><last>Bao</last></author>
      <author><first>Shaohan</first><last>Huang</last></author>
      <author><first>Li</first><last>Dong</last></author>
      <author><first>Binxing</first><last>Jiao</last></author>
      <author><first>Daxin</first><last>Jiang</last></author>
      <author><first>Haoyi</first><last>Zhou</last></author>
      <author><first>Jianxin</first><last>Li</last></author>
      <author><first>Furu</first><last>Wei</last></author>
      <pages>3510-3520</pages>
      <abstract>As more and more pre-trained language models adopt on-cloud deployment, the privacy issues grow quickly, mainly for the exposure of plain-text user data (e.g., search history, medical record, bank account). Privacy-preserving inference of transformer models is on the demand of cloud service users. To protect privacy, it is an attractive choice to compute only with ciphertext in homomorphic encryption (HE). However, enabling pre-trained models inference on ciphertext data is difficult due to the complex computations in transformer blocks, which are not supported by current HE tools yet. In this work, we introduce THE-X, an approximation approach for transformers, which enables privacy-preserving inference of pre-trained models developed by popular frameworks. THE-X proposes a workflow to deal with complex computation in transformer networks, including all the non-polynomial functions like GELU, softmax, and LayerNorm. Experiments reveal our proposed THE-X can enable transformer inference on encrypted data for different downstream tasks, all with negligible performance drop but enjoying the theory-guaranteed privacy-preserving advantage.</abstract>
      <url hash="faf393bb">2022.findings-acl.277</url>
      <attachment type="software" hash="0978f6bd">2022.findings-acl.277.software.zip</attachment>
      <bibkey>chen-etal-2022-x</bibkey>
      <doi>10.18653/v1/2022.findings-acl.277</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL 2003</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst-2">SST-2</pwcdataset>
    </paper>
    <paper id="278">
      <title><fixed-case>HLDC</fixed-case>: <fixed-case>H</fixed-case>indi Legal Documents Corpus</title>
      <author><first>Arnav</first><last>Kapoor</last></author>
      <author><first>Mudit</first><last>Dhawan</last></author>
      <author><first>Anmol</first><last>Goel</last></author>
      <author><first>Arjun</first><last>T H</last></author>
      <author><first>Akshala</first><last>Bhatnagar</last></author>
      <author><first>Vibhu</first><last>Agrawal</last></author>
      <author><first>Amul</first><last>Agrawal</last></author>
      <author><first>Arnab</first><last>Bhattacharya</last></author>
      <author><first>Ponnurangam</first><last>Kumaraguru</last></author>
      <author><first>Ashutosh</first><last>Modi</last></author>
      <pages>3521-3536</pages>
      <abstract>Many populous countries including India are burdened with a considerable backlog of legal cases. Development of automated systems that could process legal documents and augment legal practitioners can mitigate this. However, there is a dearth of high-quality corpora that is needed to develop such data-driven systems. The problem gets even more pronounced in the case of low resource languages such as Hindi. In this resource paper, we introduce the Hindi Legal Documents Corpus (HLDC), a corpus of more than 900K legal documents in Hindi. Documents are cleaned and structured to enable the development of downstream applications. Further, as a use-case for the corpus, we introduce the task of bail prediction. We experiment with a battery of models and propose a Multi-Task Learning (MTL) based model for the same. MTL models use summarization as an auxiliary task along with bail prediction as the main task. Experiments with different models are indicative of the need for further research in this area.</abstract>
      <url hash="0e114075">2022.findings-acl.278</url>
      <attachment type="software" hash="c763b971">2022.findings-acl.278.software.zip</attachment>
      <bibkey>kapoor-etal-2022-hldc</bibkey>
      <doi>10.18653/v1/2022.findings-acl.278</doi>
      <pwccode url="https://github.com/exploration-lab/hldc" additional="false">exploration-lab/hldc</pwccode>
    </paper>
    <paper id="279">
      <title>Rethinking Document-level Neural Machine Translation</title>
      <author><first>Zewei</first><last>Sun</last></author>
      <author><first>Mingxuan</first><last>Wang</last></author>
      <author><first>Hao</first><last>Zhou</last></author>
      <author><first>Chengqi</first><last>Zhao</last></author>
      <author><first>Shujian</first><last>Huang</last></author>
      <author><first>Jiajun</first><last>Chen</last></author>
      <author><first>Lei</first><last>Li</last></author>
      <pages>3537-3548</pages>
      <abstract>This paper does not aim at introducing a novel model for document-level neural machine translation. Instead, we head back to the original Transformer model and hope to answer the following question: Is the capacity of current models strong enough for document-level translation? Interestingly, we observe that the original Transformer with appropriate training techniques can achieve strong results for document translation, even with a length of 2000 words. We evaluate this model and several recent approaches on nine document-level datasets and two sentence-level datasets across six languages. Experiments show that document-level Transformer models outperforms sentence-level ones and many previous methods in a comprehensive set of metrics, including BLEU, four lexical indices, three newly proposed assistant linguistic indicators, and human evaluation.</abstract>
      <url hash="621a01ad">2022.findings-acl.279</url>
      <bibkey>sun-etal-2022-rethinking</bibkey>
      <doi>10.18653/v1/2022.findings-acl.279</doi>
      <pwccode url="https://github.com/sunzewei2715/Doc2Doc_NMT" additional="false">sunzewei2715/Doc2Doc_NMT</pwccode>
    </paper>
    <paper id="280">
      <title>Incremental Intent Detection for Medical Domain with Contrast Replay Networks</title>
      <author><first>Guirong</first><last>Bai</last></author>
      <author><first>Shizhu</first><last>He</last></author>
      <author><first>Kang</first><last>Liu</last></author>
      <author><first>Jun</first><last>Zhao</last></author>
      <pages>3549-3556</pages>
      <abstract>Conventional approaches to medical intent detection require fixed pre-defined intent categories. However, due to the incessant emergence of new medical intents in the real world, such requirement is not practical. Considering that it is computationally expensive to store and re-train the whole data every time new data and intents come in, we propose to incrementally learn emerged intents while avoiding catastrophically forgetting old intents. We first formulate incremental learning for medical intent detection. Then, we employ a memory-based method to handle incremental learning. We further propose to enhance the method with contrast replay networks, which use multilevel distillation and contrast objective to address training data imbalance and medical rare words respectively. Experiments show that the proposed method outperforms the state-of-the-art model by 5.7% and 9.1% of accuracy on two benchmarks respectively.</abstract>
      <url hash="32ae01f5">2022.findings-acl.280</url>
      <bibkey>bai-etal-2022-incremental</bibkey>
      <doi>10.18653/v1/2022.findings-acl.280</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/kuake-qic">KUAKE-QIC</pwcdataset>
    </paper>
    <paper id="281">
      <title><fixed-case>L</fixed-case>a<fixed-case>P</fixed-case>ra<fixed-case>D</fixed-case>o<fixed-case>R</fixed-case>: Unsupervised Pretrained Dense Retriever for Zero-Shot Text Retrieval</title>
      <author><first>Canwen</first><last>Xu</last></author>
      <author><first>Daya</first><last>Guo</last></author>
      <author><first>Nan</first><last>Duan</last></author>
      <author><first>Julian</first><last>McAuley</last></author>
      <pages>3557-3569</pages>
      <abstract>In this paper, we propose LaPraDoR, a pretrained dual-tower dense retriever that does not require any supervised data for training. Specifically, we first present Iterative Contrastive Learning (ICoL) that iteratively trains the query and document encoders with a cache mechanism. ICoL not only enlarges the number of negative instances but also keeps representations of cached examples in the same hidden space. We then propose Lexicon-Enhanced Dense Retrieval (LEDR) as a simple yet effective way to enhance dense retrieval with lexical matching. We evaluate LaPraDoR on the recently proposed BEIR benchmark, including 18 datasets of 9 zero-shot text retrieval tasks. Experimental results show that LaPraDoR achieves state-of-the-art performance compared with supervised dense retrieval models, and further analysis reveals the effectiveness of our training strategy and objectives. Compared to re-ranking, our lexicon-enhanced approach can be run in milliseconds (22.5x faster) while achieving superior performance.</abstract>
      <url hash="3b59a182">2022.findings-acl.281</url>
      <bibkey>xu-etal-2022-laprador</bibkey>
      <doi>10.18653/v1/2022.findings-acl.281</doi>
      <video href="2022.findings-acl.281.mp4"/>
      <pwccode url="https://github.com/jetrunner/laprador" additional="false">jetrunner/laprador</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/beir">BEIR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/c4">C4</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/climate-fever">CLIMATE-FEVER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/scifact">SciFact</pwcdataset>
    </paper>
    <paper id="282">
      <title>Do Pre-trained Models Benefit Knowledge Graph Completion? A Reliable Evaluation and a Reasonable Approach</title>
      <author><first>Xin</first><last>Lv</last></author>
      <author><first>Yankai</first><last>Lin</last></author>
      <author><first>Yixin</first><last>Cao</last></author>
      <author><first>Lei</first><last>Hou</last></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Peng</first><last>Li</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>3570-3581</pages>
      <abstract>In recent years, pre-trained language models (PLMs) have been shown to capture factual knowledge from massive texts, which encourages the proposal of PLM-based knowledge graph completion (KGC) models. However, these models are still quite behind the SOTA KGC models in terms of performance. In this work, we find two main reasons for the weak performance: (1) Inaccurate evaluation setting. The evaluation setting under the closed-world assumption (CWA) may underestimate the PLM-based KGC models since they introduce more external knowledge; (2) Inappropriate utilization of PLMs. Most PLM-based KGC models simply splice the labels of entities and relations as inputs, leading to incoherent sentences that do not take full advantage of the implicit knowledge in PLMs. To alleviate these problems, we highlight a more accurate evaluation setting under the open-world assumption (OWA), which manual checks the correctness of knowledge that is not in KGs. Moreover, motivated by prompt tuning, we propose a novel PLM-based KGC model named PKGC. The basic idea is to convert each triple and its support information into natural prompt sentences, which is further fed into PLMs for classification. Experiment results on two KGC datasets demonstrate OWA is more reliable for evaluating KGC, especially on the link prediction, and the effectiveness of our PKCG model on both CWA and OWA settings.</abstract>
      <url hash="5207eac2">2022.findings-acl.282</url>
      <bibkey>lv-etal-2022-pre</bibkey>
      <doi>10.18653/v1/2022.findings-acl.282</doi>
      <pwccode url="https://github.com/thu-keg/pkgc" additional="false">thu-keg/pkgc</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/inferwiki">InferWiki</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/lama">LAMA</pwcdataset>
    </paper>
    <paper id="283">
      <title><fixed-case>EICO</fixed-case>: Improving Few-Shot Text Classification via Explicit and Implicit Consistency Regularization</title>
      <author><first>Lei</first><last>Zhao</last></author>
      <author><first>Cheng</first><last>Yao</last></author>
      <pages>3582-3587</pages>
      <abstract>While the prompt-based fine-tuning methods had advanced few-shot natural language understanding tasks, self-training methods are also being explored. This work revisits the consistency regularization in self-training and presents explicit and implicit consistency regularization enhanced language model (EICO). By employing both explicit and implicit consistency regularization, EICO advances the performance of prompt-based few-shot text classification. For implicit consistency regularization, we generate pseudo-label from the weakly-augmented view and predict pseudo-label from the strongly-augmented view. For explicit consistency regularization, we minimize the difference between the prediction of the augmentation view and the prediction of the original view. We conducted extensive experiments on six text classification datasets and found that with sixteen labeled examples, EICO achieves competitive performance compared to existing self-training few-shot learning methods.</abstract>
      <url hash="cb6afa30">2022.findings-acl.283</url>
      <bibkey>zhao-yao-2022-eico</bibkey>
      <doi>10.18653/v1/2022.findings-acl.283</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/mpqa-opinion-corpus">MPQA Opinion Corpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst-5">SST-5</pwcdataset>
    </paper>
    <paper id="284">
      <title>Improving the Adversarial Robustness of <fixed-case>NLP</fixed-case> Models by Information Bottleneck</title>
      <author><first>Cenyuan</first><last>Zhang</last></author>
      <author><first>Xiang</first><last>Zhou</last></author>
      <author><first>Yixin</first><last>Wan</last></author>
      <author><first>Xiaoqing</first><last>Zheng</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <author><first>Cho-Jui</first><last>Hsieh</last></author>
      <pages>3588-3598</pages>
      <abstract>Existing studies have demonstrated that adversarial examples can be directly attributed to the presence of non-robust features, which are highly predictive, but can be easily manipulated by adversaries to fool NLP models. In this study, we explore the feasibility of capturing task-specific robust features, while eliminating the non-robust ones by using the information bottleneck theory. Through extensive experiments, we show that the models trained with our information bottleneck-based method are able to achieve a significant improvement in robust accuracy, exceeding performances of all the previously reported defense methods while suffering almost no performance drop in clean accuracy on SST-2, AGNEWS and IMDB datasets.</abstract>
      <url hash="cb62af9f">2022.findings-acl.284</url>
      <attachment type="software" hash="0a86dadd">2022.findings-acl.284.software.zip</attachment>
      <bibkey>zhang-etal-2022-improving</bibkey>
      <doi>10.18653/v1/2022.findings-acl.284</doi>
      <pwccode url="https://github.com/zhangcen456/ib" additional="false">zhangcen456/ib</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst-2">SST-2</pwcdataset>
    </paper>
    <paper id="285">
      <title>Incorporating Dynamic Semantics into Pre-Trained Language Model for Aspect-based Sentiment Analysis</title>
      <author><first>Kai</first><last>Zhang</last></author>
      <author><first>Kun</first><last>Zhang</last></author>
      <author><first>Mengdi</first><last>Zhang</last></author>
      <author><first>Hongke</first><last>Zhao</last></author>
      <author><first>Qi</first><last>Liu</last></author>
      <author><first>Wei</first><last>Wu</last></author>
      <author><first>Enhong</first><last>Chen</last></author>
      <pages>3599-3610</pages>
      <abstract>Aspect-based sentiment analysis (ABSA) predicts sentiment polarity towards a specific aspect in the given sentence. While pre-trained language models such as BERT have achieved great success, incorporating dynamic semantic changes into ABSA remains challenging. To this end, in this paper, we propose to address this problem by Dynamic Re-weighting BERT (DR-BERT), a novel method designed to learn dynamic aspect-oriented semantics for ABSA. Specifically, we first take the Stack-BERT layers as a primary encoder to grasp the overall semantic of the sentence and then fine-tune it by incorporating a lightweight Dynamic Re-weighting Adapter (DRA). Note that the DRA can pay close attention to a small region of the sentences at each step and re-weigh the vitally important words for better aspect-aware sentiment understanding. Finally, experimental results on three benchmark datasets demonstrate the effectiveness and the rationality of our proposed model and provide good interpretable insights for future semantic modeling.</abstract>
      <url hash="38af9ebf">2022.findings-acl.285</url>
      <bibkey>zhang-etal-2022-incorporating</bibkey>
      <doi>10.18653/v1/2022.findings-acl.285</doi>
    </paper>
    <paper id="286">
      <title><fixed-case>DARER</fixed-case>: Dual-task Temporal Relational Recurrent Reasoning Network for Joint Dialog Sentiment Classification and Act Recognition</title>
      <author><first>Bowen</first><last>Xing</last></author>
      <author><first>Ivor</first><last>Tsang</last></author>
      <pages>3611-3621</pages>
      <abstract>The task of joint dialog sentiment classification (DSC) and act recognition (DAR) aims to simultaneously predict the sentiment label and act label for each utterance in a dialog. In this paper, we put forward a new framework which models the explicit dependencies via integrating <i>prediction-level interactions</i> other than semantics-level interactions, more consistent with human intuition.Besides, we propose a speaker-aware temporal graph (SATG) and a dual-task relational temporal graph (DRTG) to introduce <i>temporal relations</i> into dialog understanding and dual-task reasoning. To implement our framework, we propose a novel model dubbed DARER, which first generates the context-, speaker- and temporal-sensitive utterance representations via modeling SATG, then conducts recurrent dual-task relational reasoning on DRTG, in which process the estimated label distributions act as key clues in prediction-level interactions.Experiment results show that DARER outperforms existing models by large margins while requiring much less computation resource and costing less training time.Remarkably, on DSC task in Mastodon, DARER gains a relative improvement of about 25% over previous best model in terms of F1, with less than 50% parameters and about only 60% required GPU memory.</abstract>
      <url hash="85b0c945">2022.findings-acl.286</url>
      <bibkey>xing-tsang-2022-darer</bibkey>
      <doi>10.18653/v1/2022.findings-acl.286</doi>
      <pwccode url="https://github.com/xingbowen714/darer" additional="false">xingbowen714/darer</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog">DailyDialog</pwcdataset>
    </paper>
    <paper id="287">
      <title>Divide and Conquer: Text Semantic Matching with Disentangled Keywords and Intents</title>
      <author><first>Yicheng</first><last>Zou</last></author>
      <author><first>Hongwei</first><last>Liu</last></author>
      <author><first>Tao</first><last>Gui</last></author>
      <author><first>Junzhe</first><last>Wang</last></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <author><first>Meng</first><last>Tang</last></author>
      <author><first>Haixiang</first><last>Li</last></author>
      <author><first>Daniell</first><last>Wang</last></author>
      <pages>3622-3632</pages>
      <abstract>Text semantic matching is a fundamental task that has been widely used in various scenarios, such as community question answering, information retrieval, and recommendation. Most state-of-the-art matching models, e.g., BERT, directly perform text comparison by processing each word uniformly. However, a query sentence generally comprises content that calls for different levels of matching granularity. Specifically, keywords represent factual information such as action, entity, and event that should be strictly matched, while intents convey abstract concepts and ideas that can be paraphrased into various expressions. In this work, we propose a simple yet effective training strategy for text semantic matching in a divide-and-conquer manner by disentangling keywords from intents. Our approach can be easily combined with pre-trained language models (PLM) without influencing their inference efficiency, achieving stable performance improvements against a wide range of PLMs on three benchmarks.</abstract>
      <url hash="d0296395">2022.findings-acl.287</url>
      <bibkey>zou-etal-2022-divide</bibkey>
      <doi>10.18653/v1/2022.findings-acl.287</doi>
      <pwccode url="https://github.com/rowitzou/dc-match" additional="false">rowitzou/dc-match</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
    </paper>
    <paper id="288">
      <title>Modular Domain Adaptation</title>
      <author><first>Junshen</first><last>Chen</last></author>
      <author><first>Dallas</first><last>Card</last></author>
      <author><first>Dan</first><last>Jurafsky</last></author>
      <pages>3633-3655</pages>
      <abstract>Off-the-shelf models are widely used by computational social science researchers to measure properties of text, such as sentiment. However, without access to source data it is difficult to account for domain shift, which represents a threat to validity. Here, we treat domain adaptation as a modular process that involves separate model producers and model consumers, and show how they can independently cooperate to facilitate more accurate measurements of text. We introduce two lightweight techniques for this scenario, and demonstrate that they reliably increase out-of-domain accuracy on four multi-domain text classification datasets when used with linear and contextual embedding models. We conclude with recommendations for model producers and consumers, and release models and replication code to accompany this paper.</abstract>
      <url hash="5a6e17ee">2022.findings-acl.288</url>
      <bibkey>chen-etal-2022-modular</bibkey>
      <doi>10.18653/v1/2022.findings-acl.288</doi>
      <pwccode url="https://github.com/jkvc/modular-domain-adaptation" additional="false">jkvc/modular-domain-adaptation</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="289">
      <title>Detection of Adversarial Examples in Text Classification: Benchmark and Baseline via Robust Density Estimation</title>
      <author><first>KiYoon</first><last>Yoo</last></author>
      <author><first>Jangho</first><last>Kim</last></author>
      <author><first>Jiho</first><last>Jang</last></author>
      <author><first>Nojun</first><last>Kwak</last></author>
      <pages>3656-3672</pages>
      <abstract>Word-level adversarial attacks have shown success in NLP models, drastically decreasing the performance of transformer-based models in recent years. As a countermeasure, adversarial defense has been explored, but relatively few efforts have been made to detect adversarial examples. However, detecting adversarial examples may be crucial for automated tasks (e.g. review sentiment analysis) that wish to amass information about a certain population and additionally be a step towards a robust defense system. To this end, we release a dataset for four popular attack methods on four datasets and four models to encourage further research in this field. Along with it, we propose a competitive baseline based on density estimation that has the highest auc on 29 out of 30 dataset-attack-model combinations. The source code is released (<url>https://github.com/bangawayoo/adversarial-examples-in-text-classification</url>).</abstract>
      <url hash="1cbf832b">2022.findings-acl.289</url>
      <attachment type="software" hash="1dc5921e">2022.findings-acl.289.software.zip</attachment>
      <bibkey>yoo-etal-2022-detection</bibkey>
      <doi>10.18653/v1/2022.findings-acl.289</doi>
      <video href="2022.findings-acl.289.mp4"/>
      <pwccode url="https://github.com/bangawayoo/adversarial-examples-in-text-classification" additional="false">bangawayoo/adversarial-examples-in-text-classification</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst-2">SST-2</pwcdataset>
    </paper>
    <paper id="290">
      <title><fixed-case>P</fixed-case>latt-Bin: Efficient Posterior Calibrated Training for <fixed-case>NLP</fixed-case> Classifiers</title>
      <author><first>Rishabh</first><last>Singh</last></author>
      <author><first>Shirin</first><last>Goshtasbpour</last></author>
      <pages>3673-3684</pages>
      <abstract>Modern NLP classifiers are known to return uncalibrated estimations of class posteriors. Existing methods for posterior calibration rescale the predicted probabilities but often have an adverse impact on final classification accuracy, thus leading to poorer generalization. We propose an end-to-end trained calibrator, Platt-Binning, that directly optimizes the objective while minimizing the difference between the predicted and empirical posterior probabilities. Our method leverages the sample efficiency of Platt scaling and the verification guarantees of histogram binning, thus not only reducing the calibration error but also improving task performance. In contrast to existing calibrators, we perform this efficient calibration during training. Empirical evaluation of benchmark NLP classification tasks echoes the efficacy of our proposal.</abstract>
      <url hash="0fd69edd">2022.findings-acl.290</url>
      <attachment type="software" hash="1ae094f8">2022.findings-acl.290.software.zip</attachment>
      <bibkey>singh-goshtasbpour-2022-platt</bibkey>
      <doi>10.18653/v1/2022.findings-acl.290</doi>
    </paper>
    <paper id="291">
      <title>Addressing Resource and Privacy Constraints in Semantic Parsing Through Data Augmentation</title>
      <author><first>Kevin</first><last>Yang</last></author>
      <author><first>Olivia</first><last>Deng</last></author>
      <author><first>Charles</first><last>Chen</last></author>
      <author><first>Richard</first><last>Shin</last></author>
      <author><first>Subhro</first><last>Roy</last></author>
      <author><first>Benjamin</first><last>Van Durme</last></author>
      <pages>3685-3695</pages>
      <abstract>We introduce a novel setup for low-resource task-oriented semantic parsing which incorporates several constraints that may arise in real-world scenarios: (1) lack of similar datasets/models from a related domain, (2) inability to sample useful logical forms directly from a grammar, and (3) privacy requirements for unlabeled natural utterances. Our goal is to improve a low-resource semantic parser using utterances collected through user interactions. In this highly challenging but realistic setting, we investigate data augmentation approaches involving generating a set of structured canonical utterances corresponding to logical forms, before simulating corresponding natural language and filtering the resulting pairs. We find that such approaches are effective despite our restrictive setup: in a low-resource setting on the complex SMCalFlow calendaring dataset (Andreas et al. 2020), we observe 33% relative improvement over a non-data-augmented baseline in top-1 match.</abstract>
      <url hash="750b654d">2022.findings-acl.291</url>
      <bibkey>yang-etal-2022-addressing</bibkey>
      <doi>10.18653/v1/2022.findings-acl.291</doi>
      <video href="2022.findings-acl.291.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/atis">ATIS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/break">BREAK</pwcdataset>
    </paper>
    <paper id="292">
      <title>Improving Candidate Retrieval with Entity Profile Generation for <fixed-case>W</fixed-case>ikidata Entity Linking</title>
      <author><first>Tuan</first><last>Lai</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <author><first>ChengXiang</first><last>Zhai</last></author>
      <pages>3696-3711</pages>
      <abstract>Entity linking (EL) is the task of linking entity mentions in a document to referent entities in a knowledge base (KB). Many previous studies focus on Wikipedia-derived KBs. There is little work on EL over Wikidata, even though it is the most extensive crowdsourced KB. The scale of Wikidata can open up many new real-world applications, but its massive number of entities also makes EL challenging. To effectively narrow down the search space, we propose a novel candidate retrieval paradigm based on entity profiling. Wikidata entities and their textual fields are first indexed into a text search engine (e.g., Elasticsearch). During inference, given a mention and its context, we use a sequence-to-sequence (seq2seq) model to generate the profile of the target entity, which consists of its title and description. We use the profile to query the indexed search engine to retrieve candidate entities. Our approach complements the traditional approach of using a Wikipedia anchor-text dictionary, enabling us to further design a highly effective hybrid method for candidate retrieval. Combined with a simple cross-attention reranker, our complete EL framework achieves state-of-the-art results on three Wikidata-based datasets and strong performance on TACKBP-2010.</abstract>
      <url hash="4b80b6e3">2022.findings-acl.292</url>
      <bibkey>lai-etal-2022-improving</bibkey>
      <doi>10.18653/v1/2022.findings-acl.292</doi>
      <pwccode url="https://github.com/laituan245/el-dockers" additional="false">laituan245/el-dockers</pwccode>
    </paper>
    <paper id="293">
      <title>Local Structure Matters Most: Perturbation Study in <fixed-case>NLU</fixed-case></title>
      <author><first>Louis</first><last>Clouatre</last></author>
      <author><first>Prasanna</first><last>Parthasarathi</last></author>
      <author><first>Amal</first><last>Zouaq</last></author>
      <author><first>Sarath</first><last>Chandar</last></author>
      <pages>3712-3731</pages>
      <abstract>Recent research analyzing the sensitivity of natural language understanding models to word-order perturbations has shown that neural models are surprisingly insensitive to the order of words. In this paper, we investigate this phenomenon by developing order-altering perturbations on the order of words, subwords, and characters to analyze their effect on neural models’ performance on language understanding tasks. We experiment with measuring the impact of perturbations to the local neighborhood of characters and global position of characters in the perturbed texts and observe that perturbation functions found in prior literature only affect the global ordering while the local ordering remains relatively unperturbed. We empirically show that neural models, invariant of their inductive biases, pretraining scheme, or the choice of tokenization, mostly rely on the local structure of text to build understanding and make limited use of the global structure.</abstract>
      <url hash="c6758cea">2022.findings-acl.293</url>
      <attachment type="software" hash="b5255e6e">2022.findings-acl.293.software.zip</attachment>
      <bibkey>clouatre-etal-2022-local</bibkey>
      <doi>10.18653/v1/2022.findings-acl.293</doi>
      <video href="2022.findings-acl.293.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
    <paper id="294">
      <title>Probing Factually Grounded Content Transfer with Factual Ablation</title>
      <author><first>Peter</first><last>West</last></author>
      <author><first>Chris</first><last>Quirk</last></author>
      <author><first>Michel</first><last>Galley</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <pages>3732-3746</pages>
      <abstract>Despite recent success, large neural models often generate factually incorrect text. Compounding this is the lack of a standard automatic evaluation for factuality–it cannot be meaningfully improved if it cannot be measured. Grounded generation promises a path to solving both of these problems: models draw on a reliable external document (grounding) for factual information, simplifying the challenge of factuality. Measuring factuality is also simplified–to factual consistency, testing whether the generation agrees with the grounding, rather than all facts. Yet, without a standard automatic metric for factual consistency, factually grounded generation remains an open problem. We study this problem for content transfer, in which generations extend a prompt, using information from factual grounding. Particularly, this domain allows us to introduce the notion of factual ablation for automatically measuring factual consistency: this captures the intuition that the model should be less likely to produce an output given a less relevant grounding document. In practice, we measure this by presenting a model with two grounding documents, and the model should prefer to use the more factually relevant one. We contribute two evaluation sets to measure this. Applying our new evaluation, we propose multiple novel methods improving over strong baselines.</abstract>
      <url hash="8ab8f758">2022.findings-acl.294</url>
      <bibkey>west-etal-2022-probing</bibkey>
      <doi>10.18653/v1/2022.findings-acl.294</doi>
      <video href="2022.findings-acl.294.mp4"/>
    </paper>
    <paper id="295">
      <title><fixed-case>ED</fixed-case>2<fixed-case>LM</fixed-case>: Encoder-Decoder to Language Model for Faster Document Re-ranking Inference</title>
      <author><first>Kai</first><last>Hui</last></author>
      <author><first>Honglei</first><last>Zhuang</last></author>
      <author><first>Tao</first><last>Chen</last></author>
      <author><first>Zhen</first><last>Qin</last></author>
      <author><first>Jing</first><last>Lu</last></author>
      <author><first>Dara</first><last>Bahri</last></author>
      <author><first>Ji</first><last>Ma</last></author>
      <author><first>Jai</first><last>Gupta</last></author>
      <author><first>Cicero</first><last>Nogueira dos Santos</last></author>
      <author><first>Yi</first><last>Tay</last></author>
      <author><first>Donald</first><last>Metzler</last></author>
      <pages>3747-3758</pages>
      <abstract>State-of-the-art neural models typically encode document-query pairs using cross-attention for re-ranking. To this end, models generally utilize an encoder-only (like BERT) paradigm or an encoder-decoder (like T5) approach. These paradigms, however, are not without flaws, i.e., running the model on all query-document pairs at inference-time incurs a significant computational cost. This paper proposes a new training and inference paradigm for re-ranking. We propose to finetune a pretrained encoder-decoder model using in the form of document to query generation. Subsequently, we show that this encoder-decoder architecture can be decomposed into a decoder-only language model during inference. This results in significant inference time speedups since the decoder-only architecture only needs to learn to interpret static encoder embeddings during inference. Our experiments show that this new paradigm achieves results that are comparable to the more expensive cross-attention ranking approaches while being up to 6.8X faster. We believe this work paves the way for more efficient neural rankers that leverage large pretrained models.</abstract>
      <url hash="76584879">2022.findings-acl.295</url>
      <bibkey>hui-etal-2022-ed2lm</bibkey>
      <doi>10.18653/v1/2022.findings-acl.295</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
    </paper>
    <paper id="296">
      <title>Benchmarking Answer Verification Methods for Question Answering-Based Summarization Evaluation Metrics</title>
      <author><first>Daniel</first><last>Deutsch</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>3759-3765</pages>
      <abstract>Question answering-based summarization evaluation metrics must automatically determine whether the QA model’s prediction is correct or not, a task known as answer verification. In this work, we benchmark the lexical answer verification methods which have been used by current QA-based metrics as well as two more sophisticated text comparison methods, BERTScore and LERC. We find that LERC out-performs the other methods in some settings while remaining statistically indistinguishable from lexical overlap in others. However, our experiments reveal that improved verification performance does not necessarily translate to overall QA-based metric quality: In some scenarios, using a worse verification method — or using none at all — has comparable performance to using the best verification method, a result that we attribute to properties of the datasets.</abstract>
      <url hash="8b2f89e5">2022.findings-acl.296</url>
      <bibkey>deutsch-roth-2022-benchmarking</bibkey>
      <doi>10.18653/v1/2022.findings-acl.296</doi>
      <video href="2022.findings-acl.296.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/summeval">SummEval</pwcdataset>
    </paper>
    <paper id="297">
      <title>Prior Knowledge and Memory Enriched Transformer for Sign Language Translation</title>
      <author><first>Tao</first><last>Jin</last></author>
      <author><first>Zhou</first><last>Zhao</last></author>
      <author><first>Meng</first><last>Zhang</last></author>
      <author><first>Xingshan</first><last>Zeng</last></author>
      <pages>3766-3775</pages>
      <abstract>This paper attacks the challenging problem of sign language translation (SLT), which involves not only visual and textual understanding but also additional prior knowledge learning (i.e. performing style, syntax). However, the majority of existing methods with vanilla encoder-decoder structures fail to sufficiently explore all of them. Based on this concern, we propose a novel method called Prior knowledge and memory Enriched Transformer (PET) for SLT, which incorporates the auxiliary information into vanilla transformer. Concretely, we develop gated interactive multi-head attention which associates the multimodal representation and global signing style with adaptive gated functions. One Part-of-Speech (POS) sequence generator relies on the associated information to predict the global syntactic structure, which is thereafter leveraged to guide the sentence generation. Besides, considering that the visual-textual context information, and additional auxiliary knowledge of a word may appear in more than one video, we design a multi-stream memory structure to obtain higher-quality translations, which stores the detailed correspondence between a word and its various relevant information, leading to a more comprehensive understanding for each word. We conduct extensive empirical studies on RWTH-PHOENIX-Weather-2014 dataset with both signer-dependent and signer-independent conditions. The quantitative and qualitative experimental results comprehensively reveal the effectiveness of PET.</abstract>
      <url hash="c4a410ce">2022.findings-acl.297</url>
      <bibkey>jin-etal-2022-prior</bibkey>
      <doi>10.18653/v1/2022.findings-acl.297</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/phoenix14t">PHOENIX14T</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/rwth-phoenix-weather-2014-t">RWTH-PHOENIX-Weather 2014 T</pwcdataset>
    </paper>
    <paper id="298">
      <title>Discontinuous Constituency and <fixed-case>BERT</fixed-case>: A Case Study of <fixed-case>D</fixed-case>utch</title>
      <author><first>Konstantinos</first><last>Kogkalidis</last></author>
      <author><first>Gijs</first><last>Wijnholds</last></author>
      <pages>3776-3785</pages>
      <abstract>In this paper, we set out to quantify the syntactic capacity of BERT in the evaluation regime of non-context free patterns, as occurring in Dutch. We devise a test suite based on a mildly context-sensitive formalism, from which we derive grammars that capture the linguistic phenomena of control verb nesting and verb raising. The grammars, paired with a small lexicon, provide us with a large collection of naturalistic utterances, annotated with verb-subject pairings, that serve as the evaluation test bed for an attention-based span selection probe. Our results, backed by extensive analysis, suggest that the models investigated fail in the implicit acquisition of the dependencies examined.</abstract>
      <url hash="48d8680d">2022.findings-acl.298</url>
      <attachment type="software" hash="d877a626">2022.findings-acl.298.software.zip</attachment>
      <bibkey>kogkalidis-wijnholds-2022-discontinuous</bibkey>
      <doi>10.18653/v1/2022.findings-acl.298</doi>
      <pwccode url="https://github.com/gijswijnholds/discontinuous-probing" additional="false">gijswijnholds/discontinuous-probing</pwccode>
    </paper>
    <paper id="299">
      <title>Probing Multilingual Cognate Prediction Models</title>
      <author><first>Clémentine</first><last>Fourrier</last></author>
      <author><first>Benoît</first><last>Sagot</last></author>
      <pages>3786-3801</pages>
      <abstract>Character-based neural machine translation models have become the reference models for cognate prediction, a historical linguistics task. So far, all linguistic interpretations about latent information captured by such models have been based on external analysis (accuracy, raw results, errors). In this paper, we investigate what probing can tell us about both models and previous interpretations, and learn that though our models store linguistic and diachronic information, they do not achieve it in previously assumed ways.</abstract>
      <url hash="c1060f20">2022.findings-acl.299</url>
      <bibkey>fourrier-sagot-2022-probing</bibkey>
      <doi>10.18653/v1/2022.findings-acl.299</doi>
      <video href="2022.findings-acl.299.mp4"/>
    </paper>
    <paper id="300">
      <title>A Neural Pairwise Ranking Model for Readability Assessment</title>
      <author><first>Justin</first><last>Lee</last></author>
      <author><first>Sowmya</first><last>Vajjala</last></author>
      <pages>3802-3813</pages>
      <abstract>Automatic Readability Assessment (ARA), the task of assigning a reading level to a text, is traditionally treated as a classification problem in NLP research. In this paper, we propose the first neural, pairwise ranking approach to ARA and compare it with existing classification, regression, and (non-neural) ranking methods. We establish the performance of our approach by conducting experiments with three English, one French and one Spanish datasets. We demonstrate that our approach performs well in monolingual single/cross corpus testing scenarios and achieves a zero-shot cross-lingual ranking accuracy of over 80% for both French and Spanish when trained on English data. Additionally, we also release a new parallel bilingual readability dataset, that could be useful for future research. To our knowledge, this paper proposes the first neural pairwise ranking model for ARA, and shows the first results of cross-lingual, zero-shot evaluation of ARA with neural models.</abstract>
      <url hash="f6a9931e">2022.findings-acl.300</url>
      <attachment type="software" hash="b8579279">2022.findings-acl.300.software.zip</attachment>
      <bibkey>lee-vajjala-2022-neural</bibkey>
      <doi>10.18653/v1/2022.findings-acl.300</doi>
      <pwccode url="https://github.com/jlee118/nprm" additional="false">jlee118/nprm</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/newsela">Newsela</pwcdataset>
    </paper>
    <paper id="301">
      <title>First the Worst: Finding Better Gender Translations During Beam Search</title>
      <author><first>Danielle</first><last>Saunders</last></author>
      <author><first>Rosie</first><last>Sallis</last></author>
      <author id="bill-byrne"><first>Bill</first><last>Byrne</last></author>
      <pages>3814-3823</pages>
      <abstract>Generating machine translations via beam search seeks the most likely output under a model. However, beam search has been shown to amplify demographic biases exhibited by a model. We aim to address this, focusing on gender bias resulting from systematic errors in grammatical gender translation. Almost all prior work on this problem adjusts the training data or the model itself. By contrast, our approach changes only the inference procedure. We constrain beam search to improve gender diversity in n-best lists, and rerank n-best lists using gender features obtained from the source sentence. Combining these strongly improves WinoMT gender translation accuracy for three language pairs without additional bilingual data or retraining. We also demonstrate our approach’s utility for consistently gendering named entities, and its flexibility to handle new gendered language beyond the binary.</abstract>
      <url hash="3043d1dd">2022.findings-acl.301</url>
      <attachment type="software" hash="cbe41a52">2022.findings-acl.301.software.zip</attachment>
      <bibkey>saunders-etal-2022-first</bibkey>
      <doi>10.18653/v1/2022.findings-acl.301</doi>
      <video href="2022.findings-acl.301.mp4"/>
      <pwccode url="https://github.com/dcsaunders/nmt-gender-rerank" additional="false">dcsaunders/nmt-gender-rerank</pwccode>
    </paper>
    <paper id="302">
      <title>Dialogue Summaries as Dialogue States (<fixed-case>DS</fixed-case>2), Template-Guided Summarization for Few-shot Dialogue State Tracking</title>
      <author><first>Jamin</first><last>Shin</last></author>
      <author><first>Hangyeol</first><last>Yu</last></author>
      <author><first>Hyeongdon</first><last>Moon</last></author>
      <author><first>Andrea</first><last>Madotto</last></author>
      <author><first>Juneyoung</first><last>Park</last></author>
      <pages>3824-3846</pages>
      <abstract>Annotating task-oriented dialogues is notorious for the expensive and difficult data collection process. Few-shot dialogue state tracking (DST) is a realistic solution to this problem. In this paper, we hypothesize that dialogue summaries are essentially unstructured dialogue states; hence, we propose to reformulate dialogue state tracking as a dialogue summarization problem. To elaborate, we train a text-to-text language model with synthetic template-based dialogue summaries, generated by a set of rules from the dialogue states. Then, the dialogue states can be recovered by inversely applying the summary generation rules. We empirically show that our method DS2 outperforms previous works on few-shot DST in MultiWoZ 2.0 and 2.1, in both cross-domain and multi-domain settings. Our method also exhibits vast speedup during both training and inference as it can generate all states at once. Finally, based on our analysis, we discover that the naturalness of the summary templates plays a key role for successful training.</abstract>
      <url hash="0fc736ce">2022.findings-acl.302</url>
      <attachment type="software" hash="e8b557c8">2022.findings-acl.302.software.zip</attachment>
      <bibkey>shin-etal-2022-dialogue</bibkey>
      <doi>10.18653/v1/2022.findings-acl.302</doi>
      <video href="2022.findings-acl.302.mp4"/>
      <pwccode url="https://github.com/jshin49/ds2" additional="false">jshin49/ds2</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multiwoz">MultiWOZ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/samsum-corpus">SAMSum</pwcdataset>
    </paper>
    <paper id="303">
      <title>Unsupervised Preference-Aware Language Identification</title>
      <author><first>Xingzhang</first><last>Ren</last></author>
      <author><first>Baosong</first><last>Yang</last></author>
      <author><first>Dayiheng</first><last>Liu</last></author>
      <author><first>Haibo</first><last>Zhang</last></author>
      <author><first>Xiaoyu</first><last>Lv</last></author>
      <author><first>Liang</first><last>Yao</last></author>
      <author><first>Jun</first><last>Xie</last></author>
      <pages>3847-3852</pages>
      <abstract>Recognizing the language of ambiguous texts has become a main challenge in language identification (LID). When using multilingual applications, users have their own language preferences, which can be regarded as external knowledge for LID. Nevertheless, current studies do not consider the inter-personal variations due to the lack of user annotated training data. To fill this gap, we introduce preference-aware LID and propose a novel unsupervised learning strategy. Concretely, we construct pseudo training set for each user by extracting training samples from a standard LID corpus according to his/her historical language distribution. Besides, we contribute the first user labeled LID test set called “U-LID”. Experimental results reveal that our model can incarnate user traits and significantly outperforms existing LID systems on handling ambiguous texts. Our code and benchmark have been released.</abstract>
      <url hash="80e1c0a9">2022.findings-acl.303</url>
      <attachment type="software" hash="ad928da9">2022.findings-acl.303.software.zip</attachment>
      <bibkey>ren-etal-2022-unsupervised</bibkey>
      <doi>10.18653/v1/2022.findings-acl.303</doi>
      <pwccode url="https://github.com/xzhren/preferenceawarelid" additional="false">xzhren/preferenceawarelid</pwccode>
    </paper>
    <paper id="304">
      <title>Using <fixed-case>NLP</fixed-case> to quantify the environmental cost and diversity benefits of in-person <fixed-case>NLP</fixed-case> conferences</title>
      <author><first>Piotr</first><last>Przybyła</last></author>
      <author><first>Matthew</first><last>Shardlow</last></author>
      <pages>3853-3863</pages>
      <abstract>The environmental costs of research are progressively important to the NLP community and their associated challenges are increasingly debated. In this work, we analyse the carbon cost (measured as CO2-equivalent) associated with journeys made by researchers attending in-person NLP conferences. We obtain the necessary data by text-mining all publications from the ACL anthology available at the time of the study (n=60,572) and extracting information about an author’s affiliation, including their address. This allows us to estimate the corresponding carbon cost and compare it to previously known values for training large models. Further, we look at the benefits of in-person conferences by demonstrating that they can increase participation diversity by encouraging attendance from the region surrounding the host country. We show how the trade-off between carbon cost and diversity of an event depends on its location and type. Our aim is to foster further discussion on the best way to address the joint issue of emissions and diversity in the future.</abstract>
      <url hash="7a5738d0">2022.findings-acl.304</url>
      <bibkey>przybyla-shardlow-2022-using</bibkey>
      <doi>10.18653/v1/2022.findings-acl.304</doi>
      <video href="2022.findings-acl.304.mp4"/>
      <pwccode url="https://github.com/piotrmp/nlp_geography" additional="false">piotrmp/nlp_geography</pwccode>
    </paper>
    <paper id="305">
      <title>Interpretable Research Replication Prediction via Variational Contextual Consistency Sentence Masking</title>
      <author><first>Tianyi</first><last>Luo</last></author>
      <author><first>Rui</first><last>Meng</last></author>
      <author><first>Xin</first><last>Wang</last></author>
      <author id="yang-liu-umich"><first>Yang</first><last>Liu</last></author>
      <pages>3864-3876</pages>
      <abstract>Research Replication Prediction (RRP) is the task of predicting whether a published research result can be replicated or not. Building an interpretable neural text classifier for RRP promotes the understanding of why a research paper is predicted as replicable or non-replicable and therefore makes its real-world application more reliable and trustworthy. However, the prior works on model interpretation mainly focused on improving the model interpretability at the word/phrase level, which are insufficient especially for long research papers in RRP. Furthermore, the existing methods cannot utilize a large size of unlabeled dataset to further improve the model interpretability. To address these limitations, we aim to build an interpretable neural model which can provide sentence-level explanations and apply weakly supervised approach to further leverage the large corpus of unlabeled datasets to boost the interpretability in addition to improving prediction performance as existing works have done. In this work, we propose the Variational Contextual Consistency Sentence Masking (VCCSM) method to automatically extract key sentences based on the context in the classifier, using both labeled and unlabeled datasets. Results of our experiments on RRP along with European Convention of Human Rights (ECHR) datasets demonstrate that VCCSM is able to improve the model interpretability for the long document classification tasks using the area over the perturbation curve and post-hoc accuracy as evaluation metrics.</abstract>
      <url hash="38677994">2022.findings-acl.305</url>
      <attachment type="software" hash="bfebc4b3">2022.findings-acl.305.software.zip</attachment>
      <bibkey>luo-etal-2022-interpretable</bibkey>
      <doi>10.18653/v1/2022.findings-acl.305</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/echr">ECHR</pwcdataset>
    </paper>
    <paper id="306">
      <title><fixed-case>C</fixed-case>hinese Synesthesia Detection: New Dataset and Models</title>
      <author><first>Xiaotong</first><last>Jiang</last></author>
      <author><first>Qingqing</first><last>Zhao</last></author>
      <author><first>Yunfei</first><last>Long</last></author>
      <author><first>Zhongqing</first><last>Wang</last></author>
      <pages>3877-3887</pages>
      <abstract>In this paper, we introduce a new task called synesthesia detection, which aims to extract the sensory word of a sentence, and to predict the original and synesthetic sensory modalities of the corresponding sensory word. Synesthesia refers to the description of perceptions in one sensory modality through concepts from other modalities. It involves not only a linguistic phenomenon, but also a cognitive phenomenon structuring human thought and action, which makes it become a bridge between figurative linguistic phenomenon and abstract cognition, and thus be helpful to understand the deep semantics. To address this, we construct a large-scale human-annotated Chinese synesthesia dataset, which contains 7,217 annotated sentences accompanied by 187 sensory words. Based on this dataset, we propose a family of strong and representative baseline models. Upon these baselines, we further propose a radical-based neural network model to identify the boundary of the sensory word, and to jointly detect the original and synesthetic sensory modalities for the word. Through extensive experiments, we observe that the importance of the proposed task and dataset can be verified by the statistics and progressive performances. In addition, our proposed model achieves state-of-the-art results on the synesthesia dataset.</abstract>
      <url hash="8c19f3a7">2022.findings-acl.306</url>
      <bibkey>jiang-etal-2022-chinese</bibkey>
      <doi>10.18653/v1/2022.findings-acl.306</doi>
    </paper>
    <paper id="307">
      <title>Rethinking Offensive Text Detection as a Multi-Hop Reasoning Problem</title>
      <author><first>Qiang</first><last>Zhang</last></author>
      <author><first>Jason</first><last>Naradowsky</last></author>
      <author><first>Yusuke</first><last>Miyao</last></author>
      <pages>3888-3905</pages>
      <abstract>We introduce the task of implicit offensive text detection in dialogues, where a statement may have either an offensive or non-offensive interpretation, depending on the listener and context. We argue that reasoning is crucial for understanding this broader class of offensive utterances, and release SLIGHT, a dataset to support research on this task. Experiments using the data show that state-of-the-art methods of offense detection perform poorly when asked to detect implicitly offensive statements, achieving only <tex-math>{\sim} 11\%</tex-math> accuracy. In contrast to existing offensive text detection datasets, SLIGHT features human-annotated chains of reasoning which describe the mental process by which an offensive interpretation can be reached from each ambiguous statement. We explore the potential for a multi-hop reasoning approach by utilizing existing entailment models to score the probability of these chains, and show that even naive reasoning models can yield improved performance in most situations. Analysis of the chains provides insight into the human interpretation process and emphasizes the importance of incorporating additional commonsense knowledge.</abstract>
      <url hash="f94c5eab">2022.findings-acl.307</url>
      <bibkey>zhang-etal-2022-rethinking</bibkey>
      <doi>10.18653/v1/2022.findings-acl.307</doi>
      <pwccode url="https://github.com/qzx7/slight" additional="false">qzx7/slight</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/olid">OLID</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tweeteval">TweetEval</pwcdataset>
    </paper>
    <paper id="308">
      <title>On the Safety of Conversational Models: Taxonomy, Dataset, and Benchmark</title>
      <author><first>Hao</first><last>Sun</last></author>
      <author><first>Guangxuan</first><last>Xu</last></author>
      <author><first>Jiawen</first><last>Deng</last></author>
      <author><first>Jiale</first><last>Cheng</last></author>
      <author><first>Chujie</first><last>Zheng</last></author>
      <author><first>Hao</first><last>Zhou</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <author><first>Xiaoyan</first><last>Zhu</last></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <pages>3906-3923</pages>
      <abstract>Dialogue safety problems severely limit the real-world deployment of neural conversational models and have attracted great research interests recently. However, dialogue safety problems remain under-defined and the corresponding dataset is scarce. We propose a taxonomy for dialogue safety specifically designed to capture unsafe behaviors in human-bot dialogue settings, with focuses on context-sensitive unsafety, which is under-explored in prior works. To spur research in this direction, we compile DiaSafety, a dataset with rich context-sensitive unsafe examples. Experiments show that existing safety guarding tools fail severely on our dataset. As a remedy, we train a dialogue safety classifier to provide a strong baseline for context-sensitive dialogue unsafety detection. With our classifier, we perform safety evaluations on popular conversational models and show that existing dialogue systems still exhibit concerning context-sensitive safety problems.</abstract>
      <url hash="146e8b83">2022.findings-acl.308</url>
      <attachment type="software" hash="01939158">2022.findings-acl.308.software.zip</attachment>
      <bibkey>sun-etal-2022-safety</bibkey>
      <doi>10.18653/v1/2022.findings-acl.308</doi>
      <video href="2022.findings-acl.308.mp4"/>
      <pwccode url="https://github.com/thu-coai/diasafety" additional="false">thu-coai/diasafety</pwccode>
    </paper>
    <paper id="309">
      <title>Word Segmentation by Separation Inference for <fixed-case>E</fixed-case>ast <fixed-case>A</fixed-case>sian Languages</title>
      <author><first>Yu</first><last>Tong</last></author>
      <author><first>Jingzhi</first><last>Guo</last></author>
      <author><first>Jizhe</first><last>Zhou</last></author>
      <author><first>Ge</first><last>Chen</last></author>
      <author><first>Guokai</first><last>Zheng</last></author>
      <pages>3924-3934</pages>
      <abstract>Chinese Word Segmentation (CWS) intends to divide a raw sentence into words through sequence labeling. Thinking in reverse, CWS can also be viewed as a process of grouping a sequence of characters into a sequence of words. In such a way, CWS is reformed as a separation inference task in every adjacent character pair. Since every character is either connected or not connected to the others, the tagging schema is simplified as two tags “Connection” (C) or “NoConnection” (NC). Therefore, bigram is specially tailored for “C-NC” to model the separation state of every two consecutive characters. Our Separation Inference (SpIn) framework is evaluated on five public datasets, is demonstrated to work for machine learning and deep learning models, and outperforms state-of-the-art performance for CWS in all experiments. Performance boosts on Japanese Word Segmentation (JWS) and Korean Word Segmentation (KWS) further prove the framework is universal and effective for East Asian Languages.</abstract>
      <url hash="73a0039b">2022.findings-acl.309</url>
      <bibkey>tong-etal-2022-word</bibkey>
      <doi>10.18653/v1/2022.findings-acl.309</doi>
      <pwccode url="https://github.com/um-nlper/spin-ws" additional="false">um-nlper/spin-ws</pwccode>
    </paper>
    <paper id="310">
      <title>Unsupervised <fixed-case>C</fixed-case>hinese Word Segmentation with <fixed-case>BERT</fixed-case> Oriented Probing and Transformation</title>
      <author><first>Wei</first><last>Li</last></author>
      <author><first>Yuhan</first><last>Song</last></author>
      <author><first>Qi</first><last>Su</last></author>
      <author><first>Yanqiu</first><last>Shao</last></author>
      <pages>3935-3940</pages>
      <abstract>Word Segmentation is a fundamental step for understanding Chinese language. Previous neural approaches for unsupervised Chinese Word Segmentation (CWS) only exploits shallow semantic information, which can miss important context. Large scale Pre-trained language models (PLM) have achieved great success in many areas because of its ability to capture the deep contextual semantic relation. In this paper, we propose to take advantage of the deep semantic information embedded in PLM (e.g., BERT) with a self-training manner, which iteratively probes and transforms the semantic information in PLM into explicit word segmentation ability. Extensive experiment results show that our proposed approach achieves state-of-the-art F1 score on two CWS benchmark datasets.</abstract>
      <url hash="7e14ec26">2022.findings-acl.310</url>
      <attachment type="software" hash="a9589624">2022.findings-acl.310.software.zip</attachment>
      <bibkey>li-etal-2022-unsupervised</bibkey>
      <doi>10.18653/v1/2022.findings-acl.310</doi>
      <pwccode url="https://github.com/liweitj47/bert_unsupervised_word_segmentation" additional="false">liweitj47/bert_unsupervised_word_segmentation</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/10000-people-human-pose-recognition-data">10,000 People - Human Pose Recognition Data</pwcdataset>
    </paper>
    <paper id="311">
      <title><fixed-case>E</fixed-case>-<fixed-case>KAR</fixed-case>: A Benchmark for Rationalizing Natural Language Analogical Reasoning</title>
      <author><first>Jiangjie</first><last>Chen</last></author>
      <author><first>Rui</first><last>Xu</last></author>
      <author><first>Ziquan</first><last>Fu</last></author>
      <author><first>Wei</first><last>Shi</last></author>
      <author><first>Zhongqiao</first><last>Li</last></author>
      <author><first>Xinbo</first><last>Zhang</last></author>
      <author><first>Changzhi</first><last>Sun</last></author>
      <author><first>Lei</first><last>Li</last></author>
      <author><first>Yanghua</first><last>Xiao</last></author>
      <author><first>Hao</first><last>Zhou</last></author>
      <pages>3941-3955</pages>
      <abstract>The ability to recognize analogies is fundamental to human cognition. Existing benchmarks to test word analogy do not reveal the underneath process of analogical reasoning of neural models. Holding the belief that models capable of reasoning should be right for the right reasons, we propose a first-of-its-kind Explainable Knowledge-intensive Analogical Reasoning benchmark (E-KAR). Our benchmark consists of 1,655 (in Chinese) and 1,251 (in English) problems sourced from the Civil Service Exams, which require intensive background knowledge to solve. More importantly, we design a free-text explanation scheme to explain whether an analogy should be drawn, and manually annotate them for each and every question and candidate answer. Empirical results suggest that this benchmark is very challenging for some state-of-the-art models for both explanation generation and analogical question answering tasks, which invites further research in this area.</abstract>
      <url hash="ed449cf0">2022.findings-acl.311</url>
      <bibkey>chen-etal-2022-e</bibkey>
      <doi>10.18653/v1/2022.findings-acl.311</doi>
      <video href="2022.findings-acl.311.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/e-kar">E-KAR</pwcdataset>
    </paper>
    <paper id="312">
      <title>Implicit Relation Linking for Question Answering over Knowledge Graph</title>
      <author><first>Yao</first><last>Zhao</last></author>
      <author><first>Jiacheng</first><last>Huang</last></author>
      <author><first>Wei</first><last>Hu</last></author>
      <author><first>Qijin</first><last>Chen</last></author>
      <author><first>XiaoXia</first><last>Qiu</last></author>
      <author><first>Chengfu</first><last>Huo</last></author>
      <author><first>Weijun</first><last>Ren</last></author>
      <pages>3956-3968</pages>
      <abstract>Relation linking (RL) is a vital module in knowledge-based question answering (KBQA) systems. It aims to link the relations expressed in natural language (NL) to the corresponding ones in knowledge graph (KG). Existing methods mainly rely on the textual similarities between NL and KG to build relation links. Due to the ambiguity of NL and the incompleteness of KG, many relations in NL are implicitly expressed, and may not link to a single relation in KG, which challenges the current methods. In this paper, we propose an implicit RL method called ImRL, which links relation phrases in NL to relation paths in KG. To find proper relation paths, we propose a novel path ranking model that aligns not only textual information in the word embedding space but also structural information in the KG embedding space between relation phrases in NL and relation paths in KG. Besides, we leverage a gated mechanism with attention to inject prior knowledge from external paraphrase dictionaries to address the relation phrases with vague meaning. Our experiments on two benchmark and a newly-created datasets show that ImRL significantly outperforms several state-of-the-art methods, especially for implicit RL.</abstract>
      <url hash="e41f85b5">2022.findings-acl.312</url>
      <attachment type="software" hash="c0d06122">2022.findings-acl.312.software.zip</attachment>
      <bibkey>zhao-etal-2022-implicit</bibkey>
      <doi>10.18653/v1/2022.findings-acl.312</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/dbpedia">DBpedia</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/simplequestions">SimpleQuestions</pwcdataset>
    </paper>
    <paper id="313">
      <title>Attention Mechanism with Energy-Friendly Operations</title>
      <author><first>Yu</first><last>Wan</last></author>
      <author><first>Baosong</first><last>Yang</last></author>
      <author><first>Dayiheng</first><last>Liu</last></author>
      <author><first>Rong</first><last>Xiao</last></author>
      <author><first>Derek</first><last>Wong</last></author>
      <author><first>Haibo</first><last>Zhang</last></author>
      <author><first>Boxing</first><last>Chen</last></author>
      <author><first>Lidia</first><last>Chao</last></author>
      <pages>3969-3976</pages>
      <abstract>Attention mechanism has become the dominant module in natural language processing models. It is computationally intensive and depends on massive power-hungry multiplications. In this paper, we rethink variants of attention mechanism from the energy consumption aspects. After reaching the conclusion that the energy costs of several energy-friendly operations are far less than their multiplication counterparts, we build a novel attention model by replacing multiplications with either selective operations or additions. Empirical results on three machine translation tasks demonstrate that the proposed model, against the vanilla one, achieves competitable accuracy while saving 99% and 66% energy during alignment calculation and the whole attention procedure. Our code will be released upon the acceptance.</abstract>
      <url hash="49853277">2022.findings-acl.313</url>
      <bibkey>wan-etal-2022-attention</bibkey>
      <doi>10.18653/v1/2022.findings-acl.313</doi>
      <pwccode url="https://github.com/nlp2ct/e-att" additional="false">nlp2ct/e-att</pwccode>
    </paper>
    <paper id="314">
      <title>Probing <fixed-case>BERT</fixed-case>’s priors with serial reproduction chains</title>
      <author><first>Takateru</first><last>Yamakoshi</last></author>
      <author><first>Thomas</first><last>Griffiths</last></author>
      <author><first>Robert</first><last>Hawkins</last></author>
      <pages>3977-3992</pages>
      <abstract>Sampling is a promising bottom-up method for exposing what generative models have learned about language, but it remains unclear how to generate representative samples from popular masked language models (MLMs) like BERT. The MLM objective yields a dependency network with no guarantee of consistent conditional distributions, posing a problem for naive approaches. Drawing from theories of iterated learning in cognitive science, we explore the use of <i>serial reproduction chains</i> to sample from BERT’s priors. In particular, we observe that a unique and consistent estimator of the ground-truth joint distribution is given by a Generative Stochastic Network (GSN) sampler, which randomly selects which token to mask and reconstruct on each step. We show that the lexical and syntactic statistics of sentences from GSN chains closely match the ground-truth corpus distribution and perform better than other methods in a large corpus of naturalness judgments. Our findings establish a firmer theoretical foundation for bottom-up probing and highlight richer deviations from human priors.</abstract>
      <url hash="0166889c">2022.findings-acl.314</url>
      <bibkey>yamakoshi-etal-2022-probing</bibkey>
      <doi>10.18653/v1/2022.findings-acl.314</doi>
    </paper>
    <paper id="315">
      <title>Interpreting the Robustness of Neural <fixed-case>NLP</fixed-case> Models to Textual Perturbations</title>
      <author><first>Yunxiang</first><last>Zhang</last></author>
      <author><first>Liangming</first><last>Pan</last></author>
      <author><first>Samson</first><last>Tan</last></author>
      <author><first>Min-Yen</first><last>Kan</last></author>
      <pages>3993-4007</pages>
      <abstract>Modern Natural Language Processing (NLP) models are known to be sensitive to input perturbations and their performance can decrease when applied to real-world, noisy data. However, it is still unclear why models are less robust to some perturbations than others. In this work, we test the hypothesis that the extent to which a model is affected by an unseen textual perturbation (robustness) can be explained by the learnability of the perturbation (defined as how well the model learns to identify the perturbation with a small amount of evidence). We further give a causal justification for the learnability metric. We conduct extensive experiments with four prominent NLP models — TextRNN, BERT, RoBERTa and XLNet — over eight types of textual perturbations on three datasets. We show that a model which is better at identifying a perturbation (higher learnability) becomes worse at ignoring such a perturbation at test time (lower robustness), providing empirical support for our hypothesis.</abstract>
      <url hash="47834ba0">2022.findings-acl.315</url>
      <bibkey>zhang-etal-2022-interpreting</bibkey>
      <doi>10.18653/v1/2022.findings-acl.315</doi>
    </paper>
    <paper id="316">
      <title>Zero-Shot Dense Retrieval with Momentum Adversarial Domain Invariant Representations</title>
      <author><first>Ji</first><last>Xin</last></author>
      <author><first>Chenyan</first><last>Xiong</last></author>
      <author><first>Ashwin</first><last>Srinivasan</last></author>
      <author><first>Ankita</first><last>Sharma</last></author>
      <author><first>Damien</first><last>Jose</last></author>
      <author><first>Paul</first><last>Bennett</last></author>
      <pages>4008-4020</pages>
      <abstract>Dense retrieval (DR) methods conduct text retrieval by first encoding texts in the embedding space and then matching them by nearest neighbor search. This requires strong locality properties from the representation space, e.g., close allocations of each small group of relevant texts, which are hard to generalize to domains without sufficient training data. In this paper, we aim to improve the generalization ability of DR models from source training domains with rich supervision signals to target domains without any relevance label, in the zero-shot setting. To achieve that, we propose Momentum adversarial Domain Invariant Representation learning (MoDIR), which introduces a momentum method to train a domain classifier that distinguishes source versus target domains, and then adversarially updates the DR encoder to learn domain invariant representations. Our experiments show that MoDIR robustly outperforms its baselines on 10+ ranking datasets collected in the BEIR benchmark in the zero-shot setup, with more than 10% relative gains on datasets with enough sensitivity for DR models’ evaluation. Source code is available at <url>https://github.com/ji-xin/modir</url>.</abstract>
      <url hash="51ee267a">2022.findings-acl.316</url>
      <bibkey>xin-etal-2022-zero</bibkey>
      <doi>10.18653/v1/2022.findings-acl.316</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/beir">BEIR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/trec-covid">TREC-COVID</pwcdataset>
    </paper>
    <paper id="317">
      <title>A Few-Shot Semantic Parser for <fixed-case>W</fixed-case>izard-of-<fixed-case>O</fixed-case>z Dialogues with the Precise <fixed-case>T</fixed-case>hing<fixed-case>T</fixed-case>alk Representation</title>
      <author><first>Giovanni</first><last>Campagna</last></author>
      <author><first>Sina</first><last>Semnani</last></author>
      <author><first>Ryan</first><last>Kearns</last></author>
      <author><first>Lucas Jun</first><last>Koba Sato</last></author>
      <author><first>Silei</first><last>Xu</last></author>
      <author><first>Monica</first><last>Lam</last></author>
      <pages>4021-4034</pages>
      <abstract>Previous attempts to build effective semantic parsers for Wizard-of-Oz (WOZ) conversations suffer from the difficulty in acquiring a high-quality, manually annotated training set. Approaches based only on dialogue synthesis are insufficient, as dialogues generated from state-machine based models are poor approximations of real-life conversations. Furthermore, previously proposed dialogue state representations are ambiguous and lack the precision necessary for building an effective agent. This paper proposes a new dialogue representation and a sample-efficient methodology that can predict precise dialogue states in WOZ conversations. We extended the ThingTalk representation to capture all information an agent needs to respond properly. Our training strategy is sample-efficient: we combine (1) few-shot data sparsely sampling the full dialogue space and (2) synthesized data covering a subset space of dialogues generated by a succinct state-based dialogue model. The completeness of the extended ThingTalk language is demonstrated with a fully operational agent, which is also used in training data synthesis. We demonstrate the effectiveness of our methodology on MultiWOZ 3.0, a reannotation of the MultiWOZ 2.1 dataset in ThingTalk. ThingTalk can represent 98% of the test turns, while the simulator can emulate 85% of the validation set. We train a contextual semantic parser using our strategy, and obtain 79% turn-by-turn exact match accuracy on the reannotated test set.</abstract>
      <url hash="4af43fd5">2022.findings-acl.317</url>
      <bibkey>campagna-etal-2022-shot</bibkey>
      <doi>10.18653/v1/2022.findings-acl.317</doi>
    </paper>
    <paper id="318">
      <title><fixed-case>GCPG</fixed-case>: A General Framework for Controllable Paraphrase Generation</title>
      <author><first>Kexin</first><last>Yang</last></author>
      <author><first>Dayiheng</first><last>Liu</last></author>
      <author><first>Wenqiang</first><last>Lei</last></author>
      <author><first>Baosong</first><last>Yang</last></author>
      <author><first>Haibo</first><last>Zhang</last></author>
      <author><first>Xue</first><last>Zhao</last></author>
      <author><first>Wenqing</first><last>Yao</last></author>
      <author><first>Boxing</first><last>Chen</last></author>
      <pages>4035-4047</pages>
      <abstract>Controllable paraphrase generation (CPG) incorporates various external conditions to obtain desirable paraphrases. However, existing works only highlight a special condition under two indispensable aspects of CPG (i.e., lexically and syntactically CPG) individually, lacking a unified circumstance to explore and analyze their effectiveness. In this paper, we propose a general controllable paraphrase generation framework (GCPG), which represents both lexical and syntactical conditions as text sequences and uniformly processes them in an encoder-decoder paradigm. Under GCPG, we reconstruct commonly adopted lexical condition (i.e., Keywords) and syntactical conditions (i.e., Part-Of-Speech sequence, Constituent Tree, Masked Template and Sentential Exemplar) and study the combination of the two types. In particular, for Sentential Exemplar condition, we propose a novel exemplar construction method — Syntax-Similarity based Exemplar (SSE). SSE retrieves a syntactically similar but lexically different sentence as the exemplar for each target sentence, avoiding exemplar-side words copying problem. Extensive experiments demonstrate that GCPG with SSE achieves state-of-the-art performance on two popular benchmarks. In addition, the combination of lexical and syntactical conditions shows the significant controllable ability of paraphrase generation, and these empirical results could provide novel insight to user-oriented paraphrasing.</abstract>
      <url hash="6037d737">2022.findings-acl.318</url>
      <attachment type="software" hash="8ba9c70a">2022.findings-acl.318.software.zip</attachment>
      <bibkey>yang-etal-2022-gcpg</bibkey>
      <doi>10.18653/v1/2022.findings-acl.318</doi>
    </paper>
    <paper id="319">
      <title><fixed-case>C</fixed-case>ross<fixed-case>A</fixed-case>ligner &amp; Co: Zero-Shot Transfer Methods for Task-Oriented Cross-lingual Natural Language Understanding</title>
      <author><first>Milan</first><last>Gritta</last></author>
      <author><first>Ruoyu</first><last>Hu</last></author>
      <author><first>Ignacio</first><last>Iacobacci</last></author>
      <pages>4048-4061</pages>
      <abstract>Task-oriented personal assistants enable people to interact with a host of devices and services using natural language. One of the challenges of making neural dialogue systems available to more users is the lack of training data for all but a few languages. Zero-shot methods try to solve this issue by acquiring task knowledge in a high-resource language such as English with the aim of transferring it to the low-resource language(s). To this end, we introduce CrossAligner, the principal method of a variety of effective approaches for zero-shot cross-lingual transfer based on learning alignment from unlabelled parallel data. We present a quantitative analysis of individual methods as well as their weighted combinations, several of which exceed state-of-the-art (SOTA) scores as evaluated across nine languages, fifteen test sets and three benchmark multilingual datasets. A detailed qualitative error analysis of the best methods shows that our fine-tuned language models can zero-shot transfer the task knowledge better than anticipated.</abstract>
      <url hash="fa4eb4b6">2022.findings-acl.319</url>
      <bibkey>gritta-etal-2022-crossaligner</bibkey>
      <doi>10.18653/v1/2022.findings-acl.319</doi>
      <pwccode url="https://github.com/huawei-noah/noah-research/tree/master/NLP/cross_aligner" additional="false">huawei-noah/noah-research</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mtop">MTOP</pwcdataset>
    </paper>
    <paper id="320">
      <title>Attention as Grounding: Exploring Textual and Cross-Modal Attention on Entities and Relations in Language-and-Vision Transformer</title>
      <author><first>Nikolai</first><last>Ilinykh</last></author>
      <author><first>Simon</first><last>Dobnik</last></author>
      <pages>4062-4073</pages>
      <abstract>We explore how a multi-modal transformer trained for generation of longer image descriptions learns syntactic and semantic representations about entities and relations grounded in objects at the level of masked self-attention (text generation) and cross-modal attention (information fusion). We observe that cross-attention learns the visual grounding of noun phrases into objects and high-level semantic information about spatial relations, while text-to-text attention captures low-level syntactic knowledge between words. This concludes that language models in a multi-modal task learn different semantic information about objects and relations cross-modally and uni-modally (text-only). Our code is available here: <url>https://github.com/GU-CLASP/attention-as-grounding</url>.</abstract>
      <url hash="b56b0921">2022.findings-acl.320</url>
      <bibkey>ilinykh-dobnik-2022-attention</bibkey>
      <doi>10.18653/v1/2022.findings-acl.320</doi>
      <video href="2022.findings-acl.320.mp4"/>
      <pwccode url="https://github.com/gu-clasp/attention-as-grounding" additional="false">gu-clasp/attention-as-grounding</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/image-description-sequences">Image Description Sequences</pwcdataset>
    </paper>
    <paper id="321">
      <title>Improving Zero-Shot Cross-lingual Transfer Between Closely Related Languages by Injecting Character-Level Noise</title>
      <author><first>Noëmi</first><last>Aepli</last></author>
      <author><first>Rico</first><last>Sennrich</last></author>
      <pages>4074-4083</pages>
      <abstract>Cross-lingual transfer between a high-resource language and its dialects or closely related language varieties should be facilitated by their similarity. However, current approaches that operate in the embedding space do not take surface similarity into account. This work presents a simple yet effective strategy to improve cross-lingual transfer between closely related varieties. We propose to augment the data of the high-resource source language with character-level noise to make the model more robust towards spelling variations. Our strategy shows consistent improvements over several languages and tasks: Zero-shot transfer of POS tagging and topic identification between language varieties from the Finnic, West and North Germanic, and Western Romance language branches. Our work provides evidence for the usefulness of simple surface-level noise in improving transfer between language varieties.</abstract>
      <url hash="bf52bffc">2022.findings-acl.321</url>
      <bibkey>aepli-sennrich-2022-improving</bibkey>
      <doi>10.18653/v1/2022.findings-acl.321</doi>
      <video href="2022.findings-acl.321.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="322">
      <title>Structural Supervision for Word Alignment and Machine Translation</title>
      <author><first>Lei</first><last>Li</last></author>
      <author><first>Kai</first><last>Fan</last></author>
      <author><first>Hongjia</first><last>Li</last></author>
      <author><first>Chun</first><last>Yuan</last></author>
      <pages>4084-4094</pages>
      <abstract>Syntactic structure has long been argued to be potentially useful for enforcing accurate word alignment and improving generalization performance of machine translation. Unfortunately, existing wisdom demonstrates its significance by considering only the syntactic structure of source tokens, neglecting the rich structural information from target tokens and the structural similarity between the source and target sentences. In this work, we propose to incorporate the syntactic structure of both source and target tokens into the encoder-decoder framework, tightly correlating the internal logic of word alignment and machine translation for multi-task learning. Particularly, we won’t leverage any annotated syntactic graph of the target side during training, so we introduce Dynamic Graph Convolution Networks (DGCN) on observed target tokens to sequentially and simultaneously generate the target tokens and the corresponding syntactic graphs, and further guide the word alignment. On this basis, Hierarchical Graph Random Walks (HGRW) are performed on the syntactic graphs of both source and target sides, for incorporating structured constraints on machine translation outputs. Experiments on four publicly available language pairs verify that our method is highly effective in capturing syntactic structure in different languages, consistently outperforming baselines in alignment accuracy and demonstrating promising results in translation quality.</abstract>
      <url hash="6d0f7a70">2022.findings-acl.322</url>
      <bibkey>li-etal-2022-structural</bibkey>
      <doi>10.18653/v1/2022.findings-acl.322</doi>
    </paper>
    <paper id="323">
      <title>Focus on the Action: Learning to Highlight and Summarize Jointly for Email To-Do Items Summarization</title>
      <author><first>Kexun</first><last>Zhang</last></author>
      <author><first>Jiaao</first><last>Chen</last></author>
      <author><first>Diyi</first><last>Yang</last></author>
      <pages>4095-4106</pages>
      <abstract>Automatic email to-do item generation is the task of generating to-do items from a given email to help people overview emails and schedule daily work. Different from prior research on email summarization, to-do item generation focuses on generating action mentions to provide more structured summaries of email text. Prior work either requires large amount of annotation for key sentences with potential actions or fails to pay attention to nuanced actions from these unstructured emails, and thus often lead to unfaithful summaries. To fill these gaps, we propose a simple and effective learning to highlight and summarize framework (LHS) to learn to identify the most salient text and actions, and incorporate these structured representations to generate more faithful to-do items. Experiments show that our LHS model outperforms the baselines and achieves the state-of-the-art performance in terms of both quantitative evaluation and human judgement. We also discussed specific challenges that current models faced with email to-do summarization.</abstract>
      <url hash="e13d73b0">2022.findings-acl.323</url>
      <bibkey>zhang-etal-2022-focus</bibkey>
      <doi>10.18653/v1/2022.findings-acl.323</doi>
    </paper>
    <paper id="324">
      <title>Exploring the Capacity of a Large-scale Masked Language Model to Recognize Grammatical Errors</title>
      <author><first>Ryo</first><last>Nagata</last></author>
      <author><first>Manabu</first><last>Kimura</last></author>
      <author><first>Kazuaki</first><last>Hanawa</last></author>
      <pages>4107-4118</pages>
      <abstract>In this paper, we explore the capacity of a language model-based method for grammatical error detection in detail. We first show that 5 to 10% of training data are enough for a BERT-based error detection method to achieve performance equivalent to what a non-language model-based method can achieve with the full training data; recall improves much faster with respect to training data size in the BERT-based method than in the non-language model method. This suggests that (i) the BERT-based method should have a good knowledge of the grammar required to recognize certain types of error and that (ii) it can transform the knowledge into error detection rules by fine-tuning with few training samples, which explains its high generalization ability in grammatical error detection. We further show with pseudo error data that it actually exhibits such nice properties in learning rules for recognizing various types of error. Finally, based on these findings, we discuss a cost-effective method for detecting grammatical errors with feedback comments explaining relevant grammatical rules to learners.</abstract>
      <url hash="a254046f">2022.findings-acl.324</url>
      <bibkey>nagata-etal-2022-exploring</bibkey>
      <doi>10.18653/v1/2022.findings-acl.324</doi>
      <video href="2022.findings-acl.324.mp4"/>
    </paper>
    <paper id="325">
      <title>Should We Trust This Summary? <fixed-case>B</fixed-case>ayesian Abstractive Summarization to The Rescue</title>
      <author><first>Alexios</first><last>Gidiotis</last></author>
      <author><first>Grigorios</first><last>Tsoumakas</last></author>
      <pages>4119-4131</pages>
      <abstract>We explore the notion of uncertainty in the context of modern abstractive summarization models, using the tools of Bayesian Deep Learning. Our approach approximates Bayesian inference by first extending state-of-the-art summarization models with Monte Carlo dropout and then using them to perform multiple stochastic forward passes. Based on Bayesian inference we are able to effectively quantify uncertainty at prediction time. Having a reliable uncertainty measure, we can improve the experience of the end user by filtering out generated summaries of high uncertainty. Furthermore, uncertainty estimation could be used as a criterion for selecting samples for annotation, and can be paired nicely with active learning and human-in-the-loop approaches. Finally, Bayesian inference enables us to find a Bayesian summary which performs better than a deterministic one and is more robust to uncertainty. In practice, we show that our Variational Bayesian equivalents of BART and PEGASUS can outperform their deterministic counterparts on multiple benchmark datasets.</abstract>
      <url hash="12a71fd6">2022.findings-acl.325</url>
      <bibkey>gidiotis-tsoumakas-2022-trust</bibkey>
      <doi>10.18653/v1/2022.findings-acl.325</doi>
      <video href="2022.findings-acl.325.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/aeslc">AESLC</pwcdataset>
    </paper>
    <paper id="326">
      <title>On the data requirements of probing</title>
      <author><first>Zining</first><last>Zhu</last></author>
      <author><first>Jixuan</first><last>Wang</last></author>
      <author><first>Bai</first><last>Li</last></author>
      <author><first>Frank</first><last>Rudzicz</last></author>
      <pages>4132-4147</pages>
      <abstract>As large and powerful neural language models are developed, researchers have been increasingly interested in developing diagnostic tools to probe them. There are many papers with conclusions of the form “observation <tex-math>X</tex-math> is found in model <tex-math>Y</tex-math>”, using their own datasets with varying sizes. Larger probing datasets bring more reliability, but are also expensive to collect. There is yet to be a quantitative method for estimating reasonable probing dataset sizes. We tackle this omission in the context of comparing two probing configurations: after we have collected a small dataset from a pilot study, how many additional data samples are sufficient to distinguish two different configurations? We present a novel method to estimate the required number of data samples in such experiments and, across several case studies, we verify that our estimations have sufficient statistical power. Our framework helps to systematically construct probing datasets to diagnose neural NLP models.</abstract>
      <url hash="3716d5cc">2022.findings-acl.326</url>
      <attachment type="software" hash="e92eb92a">2022.findings-acl.326.software.zip</attachment>
      <bibkey>zhu-etal-2022-data</bibkey>
      <doi>10.18653/v1/2022.findings-acl.326</doi>
      <video href="2022.findings-acl.326.mp4"/>
      <pwccode url="https://github.com/spoclab-ca/probing_dataset" additional="false">spoclab-ca/probing_dataset</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/senteval">SentEval</pwcdataset>
    </paper>
    <paper id="327">
      <title>Translation Error Detection as Rationale Extraction</title>
      <author><first>Marina</first><last>Fomicheva</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <author><first>Nikolaos</first><last>Aletras</last></author>
      <pages>4148-4159</pages>
      <abstract>Recent Quality Estimation (QE) models based on multilingual pre-trained representations have achieved very competitive results in predicting the overall quality of translated sentences. However, detecting specifically which translated words are incorrect is a more challenging task, especially when dealing with limited amounts of training data. We hypothesize that, not unlike humans, successful QE models rely on translation errors to predict overall sentence quality. By exploring a set of feature attribution methods that assign relevance scores to the inputs to explain model predictions, we study the behaviour of state-of-the-art sentence-level QE models and show that explanations (i.e. rationales) extracted from these models can indeed be used to detect translation errors. We therefore (i) introduce a novel semi-supervised method for word-level QE; and (ii) propose to use the QE task as a new benchmark for evaluating the plausibility of feature attribution, i.e. how interpretable model explanations are to humans.</abstract>
      <url hash="35feecd8">2022.findings-acl.327</url>
      <bibkey>fomicheva-etal-2022-translation</bibkey>
      <doi>10.18653/v1/2022.findings-acl.327</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/mlqe-pe">MLQE-PE</pwcdataset>
    </paper>
    <paper id="328">
      <title>Towards Collaborative Neural-Symbolic Graph Semantic Parsing via Uncertainty</title>
      <author><first>Zi</first><last>Lin</last></author>
      <author><first>Jeremiah Zhe</first><last>Liu</last></author>
      <author><first>Jingbo</first><last>Shang</last></author>
      <pages>4160-4173</pages>
      <abstract>Recent work in task-independent graph semantic parsing has shifted from grammar-based symbolic approaches to neural models, showing strong performance on different types of meaning representations. However, it is still unclear that what are the limitations of these neural parsers, and whether these limitations can be compensated by incorporating symbolic knowledge into model inference. In this paper, we address these questions by taking English Resource Grammar (ERG) parsing as a case study. Specifically, we first develop a state-of-the-art, T5-based neural ERG parser, and conduct detail analyses of parser performance within fine-grained linguistic categories. The neural parser attains superior performance on in-distribution test set, but degrades significantly on long-tail situations, while the symbolic parser performs more robustly. To address this, we further propose a simple yet principled collaborative framework for neural-symbolic semantic parsing, by designing a decision criterion for beam search that incorporates the prior knowledge from a symbolic parser and accounts for model uncertainty. Experimental results show that the proposed framework yields comprehensive improvement over neural baseline across long-tail categories, yielding the best known Smatch score (97.01) on the well-studied DeepBank benchmark.</abstract>
      <url hash="ca2c1494">2022.findings-acl.328</url>
      <bibkey>lin-etal-2022-towards</bibkey>
      <doi>10.18653/v1/2022.findings-acl.328</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/scan">SCAN</pwcdataset>
    </paper>
    <paper id="329">
      <title>Towards Few-shot Entity Recognition in Document Images: A Label-aware Sequence-to-Sequence Framework</title>
      <author><first>Zilong</first><last>Wang</last></author>
      <author><first>Jingbo</first><last>Shang</last></author>
      <pages>4174-4186</pages>
      <abstract>Entity recognition is a fundamental task in understanding document images. Traditional sequence labeling frameworks treat the entity types as class IDs and rely on extensive data and high-quality annotations to learn semantics which are typically expensive in practice. In this paper, we aim to build an entity recognition model requiring only a few shots of annotated document images. To overcome the data limitation, we propose to leverage the label surface names to better inform the model of the target entity type semantics and also embed the labels into the spatial embedding space to capture the spatial correspondence between regions and labels. Specifically, we go beyond sequence labeling and develop a novel label-aware seq2seq framework, LASER. The proposed model follows a new labeling scheme that generates the label surface names word-by-word explicitly after generating the entities. During training, LASER refines the label semantics by updating the label surface name representations and also strengthens the label-region correlation. In this way, LASER recognizes the entities from document images through both semantic and layout correspondence. Extensive experiments on two benchmark datasets demonstrate the superiority of LASER under the few-shot setting.</abstract>
      <url hash="055efb64">2022.findings-acl.329</url>
      <bibkey>wang-shang-2022-towards</bibkey>
      <doi>10.18653/v1/2022.findings-acl.329</doi>
      <pwccode url="https://github.com/zlwang-cs/laser-release" additional="false">zlwang-cs/laser-release</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/funsd">FUNSD</pwcdataset>
    </paper>
    <paper id="330">
      <title>On Length Divergence Bias in Textual Matching Models</title>
      <author><first>Lan</first><last>Jiang</last></author>
      <author><first>Tianshu</first><last>Lyu</last></author>
      <author><first>Yankai</first><last>Lin</last></author>
      <author><first>Meng</first><last>Chong</last></author>
      <author><first>Xiaoyong</first><last>Lyu</last></author>
      <author><first>Dawei</first><last>Yin</last></author>
      <pages>4187-4193</pages>
      <abstract>Despite the remarkable success deep models have achieved in Textual Matching (TM) tasks, it still remains unclear whether they truly understand language or measure the semantic similarity of texts by exploiting statistical bias in datasets. In this work, we provide a new perspective to study this issue — via the length divergence bias. We find the length divergence heuristic widely exists in prevalent TM datasets, providing direct cues for prediction. To determine whether TM models have adopted such heuristic, we introduce an adversarial evaluation scheme which invalidates the heuristic. In this adversarial setting, all TM models perform worse, indicating they have indeed adopted this heuristic. Through a well-designed probing experiment, we empirically validate that the bias of TM models can be attributed in part to extracting the text length information during training. To alleviate the length divergence bias, we propose an adversarial training method. The results demonstrate we successfully improve the robustness and generalization ability of models at the same time.</abstract>
      <url hash="fdf34290">2022.findings-acl.330</url>
      <bibkey>jiang-etal-2022-length</bibkey>
      <doi>10.18653/v1/2022.findings-acl.330</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/trecqa">TrecQA</pwcdataset>
    </paper>
    <paper id="331">
      <title>What is wrong with you?: Leveraging User Sentiment for Automatic Dialog Evaluation</title>
      <author><first>Sarik</first><last>Ghazarian</last></author>
      <author><first>Behnam</first><last>Hedayatnia</last></author>
      <author><first>Alexandros</first><last>Papangelis</last></author>
      <author id="yang-liu-icsi"><first>Yang</first><last>Liu</last></author>
      <author><first>Dilek</first><last>Hakkani-Tur</last></author>
      <pages>4194-4204</pages>
      <abstract>Accurate automatic evaluation metrics for open-domain dialogs are in high demand. Existing model-based metrics for system response evaluation are trained on human annotated data, which is cumbersome to collect. In this work, we propose to use information that can be automatically extracted from the next user utterance, such as its sentiment or whether the user explicitly ends the conversation, as a proxy to measure the quality of the previous system response. This allows us to train on a massive set of dialogs with weak supervision, without requiring manual system turn quality annotations. Experiments show that our model is comparable to models trained on human annotated data. Furthermore, our model generalizes across both spoken and written open-domain dialog corpora collected from real and paid users.</abstract>
      <url hash="766b74d5">2022.findings-acl.331</url>
      <bibkey>ghazarian-etal-2022-wrong</bibkey>
      <doi>10.18653/v1/2022.findings-acl.331</doi>
      <video href="2022.findings-acl.331.mp4"/>
      <pwccode url="https://github.com/alexa/conture" additional="false">alexa/conture</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fed">FED</pwcdataset>
    </paper>
  </volume>
  <volume id="naacl" ingest-date="2022-07-05" type="proceedings">
    <meta>
      <booktitle>Findings of the Association for Computational Linguistics: NAACL 2022</booktitle>
      <editor><first>Marine</first><last>Carpuat</last></editor>
      <editor><first>Marie-Catherine</first><last>de Marneffe</last></editor>
      <editor><first>Ivan Vladimir</first><last>Meza Ruiz</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Seattle, United States</address>
      <month>July</month>
      <year>2022</year>
      <url hash="9f890147">2022.findings-naacl</url>
      <venue>findings</venue>
    </meta>
    <frontmatter>
      <url hash="6e6bf21a">2022.findings-naacl.0</url>
      <bibkey>findings-2022-findings-association</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>P</fixed-case>ub<fixed-case>H</fixed-case>ealth<fixed-case>T</fixed-case>ab: <fixed-case>A</fixed-case> Public Health Table-based Dataset for Evidence-based Fact Checking</title>
      <author><first>Mubashara</first><last>Akhtar</last></author>
      <author><first>Oana</first><last>Cocarascu</last></author>
      <author><first>Elena</first><last>Simperl</last></author>
      <pages>1-16</pages>
      <abstract>Inspired by human fact checkers, who use different types of evidence (e.g. tables, images, audio) in addition to text, several datasets with tabular evidence data have been released in recent years. Whilst the datasets encourage research on table fact-checking, they rely on information from restricted data sources, such as Wikipedia for creating claims and extracting evidence data, making the fact-checking process different from the real-world process used by fact checkers. In this paper, we introduce PubHealthTab, a table fact-checking dataset based on real world public health claims and noisy evidence tables from sources similar to those used by real fact checkers. We outline our approach for collecting evidence data from various websites and present an in-depth analysis of our dataset. Finally, we evaluate state-of-the-art table representation and pre-trained models fine-tuned on our dataset, achieving an overall <tex-math>F_1</tex-math> score of 0.73.</abstract>
      <url hash="2d2fc5bf">2022.findings-naacl.1</url>
      <attachment type="software" hash="4189927d">2022.findings-naacl.1.software.zip</attachment>
      <bibkey>akhtar-etal-2022-pubhealthtab</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.1</doi>
      <video href="2022.findings-naacl.1.mp4"/>
      <pwccode url="https://github.com/mubasharaak/pubhealthtab" additional="false">mubasharaak/pubhealthtab</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/blue">BLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/feverous">FEVEROUS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/infotabs">InfoTabS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/pubhealth">PUBHEALTH</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="2">
      <title>Masked Measurement Prediction: Learning to Jointly Predict Quantities and Units from Textual Context</title>
      <author><first>Daniel</first><last>Spokoyny</last></author>
      <author><first>Ivan</first><last>Lee</last></author>
      <author><first>Zhao</first><last>Jin</last></author>
      <author><first>Taylor</first><last>Berg-Kirkpatrick</last></author>
      <pages>17-29</pages>
      <abstract>Physical measurements constitute a large portion of numbers in academic papers, engineering reports, and web tables. Current benchmarks fall short of properly evaluating numeracy of pretrained language models on measurements, hindering research on developing new methods and applying them to numerical tasks. To that end, we introduce a novel task, Masked Measurement Prediction (MMP), where a model learns to reconstruct a number together with its associated unit given masked text. MMP is useful for both training new numerically informed models as well as evaluating numeracy of existing systems. To address this task, we introduce a new Generative Masked Measurement (GeMM) model that jointly learns to predict numbers along with their units. We perform fine-grained analyses comparing our model with various ablations and baselines. We use linear probing of traditional pretrained transformer models (RoBERTa) to show that they significantly underperform jointly trained number-unit models, highlighting the difficulty of this new task and the benefits of our proposed pretraining approach. We hope this framework accelerates the progress towards building more robust numerical reasoning systems in the future.</abstract>
      <url hash="cd9c229d">2022.findings-naacl.2</url>
      <bibkey>spokoyny-etal-2022-masked</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.2</doi>
      <video href="2022.findings-naacl.2.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/wikiconvert">WikiConvert</pwcdataset>
    </paper>
    <paper id="3">
      <title><fixed-case>P</fixed-case>rompt<fixed-case>G</fixed-case>en: Automatically Generate Prompts using Generative Models</title>
      <author><first>Yue</first><last>Zhang</last></author>
      <author><first>Hongliang</first><last>Fei</last></author>
      <author><first>Dingcheng</first><last>Li</last></author>
      <author><first>Ping</first><last>Li</last></author>
      <pages>30-37</pages>
      <abstract>Recently, prompt learning has received significant attention, where the downstream tasks are reformulated to the mask-filling task with the help of a textual prompt. The key point of prompt learning is finding the most appropriate prompt. This paper proposes a novel model PromptGen, which can automatically generate prompts conditional on the input sentence. PromptGen is the first work considering dynamic prompt generation for knowledge probing, based on a pre-trained generative model. To mitigate any label information leaking from the pre-trained generative model, when given a generated prompt, we replace the query input with “None”. We pursue that this perturbed context-free prompt cannot trigger the correct label. We evaluate our model on the knowledge probing LAMA benchmark, and show that PromptGen significantly outperforms other baselines.</abstract>
      <url hash="c07dcdc4">2022.findings-naacl.3</url>
      <attachment type="software" hash="24214412">2022.findings-naacl.3.software.zip</attachment>
      <bibkey>zhang-etal-2022-promptgen</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.3</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/lama">LAMA</pwcdataset>
    </paper>
    <paper id="4">
      <title>Improving Conversational Recommendation Systems’ Quality with Context-Aware Item Meta-Information</title>
      <author><first>Bowen</first><last>Yang</last></author>
      <author><first>Cong</first><last>Han</last></author>
      <author><first>Yu</first><last>Li</last></author>
      <author><first>Lei</first><last>Zuo</last></author>
      <author><first>Zhou</first><last>Yu</last></author>
      <pages>38-48</pages>
      <abstract>A key challenge of Conversational Recommendation Systems (CRS) is to integrate the recommendation function and the dialog generation function smoothly. Previous works employ graph neural networks with external knowledge graphs (KG) to model individual recommendation items and integrate KGs with language models through attention mechanism for response generation. Although previous approaches prove effective, there is still room for improvement. For example, KG-based approaches only rely on entity relations and bag-of-words to recommend items and neglect the information in the conversational context. We propose to improve the usage of dialog context for both recommendation and response generation using an encoding architecture along with the self-attention mechanism of transformers. In this paper, we propose a simple yet effective architecture comprising a pre-trained language model (PLM) and an item metadata encoder to integrate the recommendation and the dialog generation better. The proposed item encoder learns to map item metadata to embeddings reflecting the rich information of the item, which can be matched with dialog context. The PLM then consumes the context-aware item embeddings and dialog context to generate high-quality recommendations and responses. Experimental results on the benchmark dataset ReDial show that our model obtains state-of-the-art results on both recommendation and response generation tasks.</abstract>
      <url hash="d8cf2d3d">2022.findings-naacl.4</url>
      <bibkey>yang-etal-2022-improving</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.4</doi>
      <video href="2022.findings-naacl.4.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/inspired">Inspired</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/redial">ReDial</pwcdataset>
    </paper>
    <paper id="5">
      <title><fixed-case>SEQZERO</fixed-case>: Few-shot Compositional Semantic Parsing with Sequential Prompts and Zero-shot Models</title>
      <author><first>Jingfeng</first><last>Yang</last></author>
      <author><first>Haoming</first><last>Jiang</last></author>
      <author><first>Qingyu</first><last>Yin</last></author>
      <author><first>Danqing</first><last>Zhang</last></author>
      <author><first>Bing</first><last>Yin</last></author>
      <author><first>Diyi</first><last>Yang</last></author>
      <pages>49-60</pages>
      <abstract>Recent research showed promising results on combining pretrained language models (LMs) with canonical utterance for few-shot semantic parsing. The canonical utterance is often lengthy and complex due to the compositional structure of formal languages. Learning to generate such canonical utterance requires significant amount of data to reach high performance. Fine-tuning with only few-shot samples, the LMs can easily forget pretrained knowledge, overfit spurious biases, and suffer from compositionally out-of-distribution generalization errors. To tackle these issues, we propose a novel few-shot semantic parsing method – SEQZERO. SEQZERO decomposes the problem into a sequence of sub-problems, which corresponds to the sub-clauses of the formal language. Based on the decomposition, the LMs only need to generate short answers using prompts for predicting sub-clauses. Thus, SEQZERO avoids generating a long canonical utterance at once. Moreover, SEQZERO employs not only a few-shot model but also a zero-shot model to alleviate the overfitting.In particular, SEQZERO brings out the merits from both models via ensemble equipped with our proposed constrained rescaling.SEQZERO achieves SOTA performance of BART-based models on GeoQuery and EcommerceQuery, which are two few-shot datasets with compositional data split.</abstract>
      <url hash="ed57d22e">2022.findings-naacl.5</url>
      <bibkey>yang-etal-2022-seqzero</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.5</doi>
      <video href="2022.findings-naacl.5.mp4"/>
      <pwccode url="https://github.com/amzn/seqzero" additional="false">amzn/seqzero</pwccode>
    </paper>
    <paper id="6">
      <title><fixed-case>M</fixed-case>ulti<fixed-case>V</fixed-case>er<fixed-case>S</fixed-case>: Improving scientific claim verification with weak supervision and full-document context</title>
      <author><first>David</first><last>Wadden</last></author>
      <author><first>Kyle</first><last>Lo</last></author>
      <author><first>Lucy Lu</first><last>Wang</last></author>
      <author><first>Arman</first><last>Cohan</last></author>
      <author><first>Iz</first><last>Beltagy</last></author>
      <author><first>Hannaneh</first><last>Hajishirzi</last></author>
      <pages>61-76</pages>
      <abstract>The scientific claim verification task requires an NLP system to label scientific documents which Support or Refute an input claim, and to select evidentiary sentences (or rationales) justifying each predicted label. In this work, we present MultiVerS, which predicts a fact-checking label and identifies rationales in a multitask fashion based on a shared encoding of the claim and full document context. This approach accomplishes two key modeling goals. First, it ensures that all relevant contextual information is incorporated into each labeling decision. Second, it enables the model to learn from instances annotated with a document-level fact-checking label, but lacking sentence-level rationales. This allows MultiVerS to perform weakly-supervised domain adaptation by training on scientific documents labeled using high-precision heuristics. Our approach outperforms two competitive baselines on three scientific claim verification datasets, with particularly strong performance in zero / few-shot domain adaptation experiments. Our code and data are available at <url>https://github.com/dwadden/multivers</url>.</abstract>
      <url hash="d436301a">2022.findings-naacl.6</url>
      <bibkey>wadden-etal-2022-multivers</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.6</doi>
      <video href="2022.findings-naacl.6.mp4"/>
      <pwccode url="https://github.com/dwadden/longchecker" additional="true">dwadden/longchecker</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cord-19">CORD-19</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/covid-fact">COVID-Fact</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/trec-covid">TREC-COVID</pwcdataset>
    </paper>
    <paper id="7">
      <title>An Item Response Theory Framework for Persuasion</title>
      <author><first>Anastassia</first><last>Kornilova</last></author>
      <author><first>Vladimir</first><last>Eidelman</last></author>
      <author><first>Daniel</first><last>Douglass</last></author>
      <pages>77-86</pages>
      <abstract>In this paper, we apply Item Response Theory, popular in education and political science research, to the analysis of argument persuasiveness in language. We empirically evaluate the model’s performance on three datasets, including a novel dataset in the area of political advocacy. We show the advantages of separating these components under several style and content representations, including evaluating the ability of the speaker embeddings generated by the model to parallel real-world observations about persuadability.</abstract>
      <url hash="de24eafe">2022.findings-naacl.7</url>
      <bibkey>kornilova-etal-2022-item</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.7</doi>
      <video href="2022.findings-naacl.7.mp4"/>
    </paper>
    <paper id="8">
      <title>Self-Supervised Contrastive Learning with Adversarial Perturbations for Defending Word Substitution-based Attacks</title>
      <author><first>Zhao</first><last>Meng</last></author>
      <author><first>Yihan</first><last>Dong</last></author>
      <author><first>Mrinmaya</first><last>Sachan</last></author>
      <author><first>Roger</first><last>Wattenhofer</last></author>
      <pages>87-101</pages>
      <abstract>In this paper, we present an approach to improve the robustness of BERT language models against word substitution-based adversarial attacks by leveraging adversarial perturbations for self-supervised contrastive learning. We create a word-level adversarial attack generating hard positives on-the-fly as adversarial examples during contrastive learning. In contrast to previous works, our method improves model robustness without using any labeled data. Experimental results show that our method improves robustness of BERT against four different word substitution-based adversarial attacks, and combining our method with adversarial training gives higher robustness than adversarial training alone. As our method improves the robustness of BERT purely with unlabeled data, it opens up the possibility of using large text datasets to train robust language models against word substitution-based adversarial attacks.</abstract>
      <url hash="03e7b594">2022.findings-naacl.8</url>
      <bibkey>meng-etal-2022-self</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.8</doi>
      <video href="2022.findings-naacl.8.mp4"/>
      <pwccode url="https://github.com/LotusDYH/ssl_robust" additional="false">LotusDYH/ssl_robust</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="9">
      <title>Quiz Design Task: Helping Teachers Create Quizzes with Automated Question Generation</title>
      <author><first>Philippe</first><last>Laban</last></author>
      <author><first>Chien-Sheng</first><last>Wu</last></author>
      <author><first>Lidiya</first><last>Murakhovs’ka</last></author>
      <author><first>Wenhao</first><last>Liu</last></author>
      <author><first>Caiming</first><last>Xiong</last></author>
      <pages>102-111</pages>
      <abstract>Question generation (QGen) models are often evaluated with standardized NLG metrics that are based on n-gram overlap. In this paper, we measure whether these metric improvements translate to gains in a practical setting, focusing on the use case of helping teachers automate the generation of reading comprehension quizzes. In our study, teachers building a quiz receive question suggestions, which they can either accept or refuse with a reason. Even though we find that recent progress in QGen leads to a significant increase in question acceptance rates, there is still large room for improvement, with the best model having only 68.4% of its questions accepted by the ten teachers who participated in our study. We then leverage the annotations we collected to analyze standard NLG metrics and find that model performance has reached projected upper-bounds, suggesting new automatic metrics are needed to guide QGen research forward.</abstract>
      <url hash="77b10473">2022.findings-naacl.9</url>
      <bibkey>laban-etal-2022-quiz</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.9</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="10">
      <title>In-<fixed-case>B</fixed-case>o<fixed-case>XBART</fixed-case>: Get Instructions into Biomedical Multi-Task Learning</title>
      <author><first>Mihir</first><last>Parmar</last></author>
      <author><first>Swaroop</first><last>Mishra</last></author>
      <author><first>Mirali</first><last>Purohit</last></author>
      <author><first>Man</first><last>Luo</last></author>
      <author><first>Murad</first><last>Mohammad</last></author>
      <author><first>Chitta</first><last>Baral</last></author>
      <pages>112-128</pages>
      <abstract>Single-task models have proven pivotal in solving specific tasks; however, they have limitations in real-world applications where multi-tasking is necessary and domain shifts are exhibited. Recently, instructional prompts have shown significant improvement towards multi-task generalization; however, the effect of instructional prompts and Multi-Task Learning (MTL) has not been systematically studied in the biomedical domain. Motivated by this, this paper explores the impact of instructional prompts for biomedical MTL. We introduce the BoX, a collection of 32 instruction tasks for Biomedical NLP across (X) various categories. Using this meta-dataset, we propose a unified model termed as In-BoXBART, that can jointly learn all tasks of the BoX without any task-specific modules. To the best of our knowledge, this is the first attempt to propose a unified model in the biomedical domain and use instructions to achieve generalization across several biomedical tasks. Experimental results indicate that the proposed model: 1) outperforms single-task baseline by ~3% and multi-task (without instruction) baseline by ~18% on an average, and 2) shows ~23% improvement compared to single-task baseline in few-shot learning (i.e., 32 instances per task) on an average. Our analysis indicates that there is significant room for improvement across tasks in the BoX, implying the scope for future research direction.</abstract>
      <url hash="d897dde0">2022.findings-naacl.10</url>
      <bibkey>parmar-etal-2022-boxbart</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.10</doi>
      <video href="2022.findings-naacl.10.mp4"/>
      <pwccode url="https://github.com/mihir3009/in-boxbart" additional="true">mihir3009/in-boxbart</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hoc-1">HOC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-instructions">Natural Instructions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/pubmedqa">PubMedQA</pwcdataset>
    </paper>
    <paper id="11">
      <title>How to Translate Your Samples and Choose Your Shots? Analyzing Translate-train &amp; Few-shot Cross-lingual Transfer</title>
      <author><first>Iman</first><last>Jundi</last></author>
      <author><first>Gabriella</first><last>Lapesa</last></author>
      <pages>129-150</pages>
      <abstract>Translate-train or few-shot cross-lingual transfer can be used to improve the zero-shot performance of multilingual pretrained language models. Few-shot utilizes high-quality low-quantity samples (often manually translated from the English corpus ). Translate-train employs a machine translation of the English corpus, resulting in samples with lower quality that could be scaled to high quantity. Given the lower cost and higher availability of machine translation compared to manual professional translation, it is important to systematically compare few-shot and translate-train, understand when each has an advantage, and investigate how to choose the shots to translate in order to increase the few-shot gain. This work aims to fill this gap: we compare and quantify the performance gain of few-shot vs. translate-train using three different base models and a varying number of samples for three tasks/datasets (XNLI, PAWS-X, XQuAD) spanning 17 languages. We show that scaling up the training data using machine translation gives a larger gain compared to using the small-scale (higher-quality) few-shot data. When few-shot is beneficial, we show that there are random sets of samples that perform better across languages and that the performance on English and on the machine-translation of the samples can both be used to choose the shots to manually translate for an increased few-shot gain.</abstract>
      <url hash="5fe782e7">2022.findings-naacl.11</url>
      <bibkey>jundi-lapesa-2022-translate</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.11</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/paws-x">PAWS-X</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xquad">XQuAD</pwcdataset>
    </paper>
    <paper id="12">
      <title>Multi-Hop Open-Domain Question Answering over Structured and Unstructured Knowledge</title>
      <author><first>Yue</first><last>Feng</last></author>
      <author><first>Zhen</first><last>Han</last></author>
      <author><first>Mingming</first><last>Sun</last></author>
      <author><first>Ping</first><last>Li</last></author>
      <pages>151-156</pages>
      <abstract>Open-domain question answering systems need to answer question of our interests with structured and unstructured information. However, existing approaches only select one source to generate answer or only conduct reasoning on structured information. In this paper, we pro- pose a Document-Entity Heterogeneous Graph Network, referred to as DEHG, to effectively integrate different sources of information, and conduct reasoning on heterogeneous information. DEHG employs a graph constructor to integrate structured and unstructured information, a context encoder to represent nodes and question, a heterogeneous information reasoning layer to conduct multi-hop reasoning on both information sources, and an answer decoder to generate answers for the question. Experimental results on HybirdQA dataset show that DEHG outperforms the state-of-the-art methods.</abstract>
      <url hash="ef38ce90">2022.findings-naacl.12</url>
      <bibkey>feng-etal-2022-multi</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.12</doi>
      <video href="2022.findings-naacl.12.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/hybridqa">HybridQA</pwcdataset>
    </paper>
    <paper id="13">
      <title><fixed-case>F</fixed-case>ed<fixed-case>NLP</fixed-case>: Benchmarking Federated Learning Methods for Natural Language Processing Tasks</title>
      <author><first>Bill Yuchen</first><last>Lin</last></author>
      <author><first>Chaoyang</first><last>He</last></author>
      <author><first>Zihang</first><last>Ze</last></author>
      <author><first>Hulin</first><last>Wang</last></author>
      <author><first>Yufen</first><last>Hua</last></author>
      <author><first>Christophe</first><last>Dupuy</last></author>
      <author><first>Rahul</first><last>Gupta</last></author>
      <author><first>Mahdi</first><last>Soltanolkotabi</last></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <author><first>Salman</first><last>Avestimehr</last></author>
      <pages>157-175</pages>
      <abstract>Increasing concerns and regulations about data privacy and sparsity necessitate the study of privacy-preserving, decentralized learning methods for natural language processing (NLP) tasks. Federated learning (FL) provides promising approaches for a large number of clients (e.g., personal devices or organizations) to collaboratively learn a shared global model to benefit all clients while allowing users to keep their data locally. Despite interest in studying FL methods for NLP tasks, a systematic comparison and analysis is lacking in the literature. Herein, we present the FedNLP, a benchmarking framework for evaluating federated learning methods on four different task formulations: text classification, sequence tagging, question answering, and seq2seq. We propose a universal interface between Transformer-based language models (e.g., BERT, BART) and FL methods (e.g., FedAvg, FedOPT, etc.) under various non-IID partitioning strategies. Our extensive experiments with FedNLP provide empirical comparisons between FL methods and help us better understand the inherent challenges of this direction. The comprehensive analysis points to intriguing and exciting future research aimed at developing FL methods for NLP tasks.</abstract>
      <url hash="c3bf6b88">2022.findings-naacl.13</url>
      <bibkey>lin-etal-2022-fednlp</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.13</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/mrqa-2019">MRQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ontonotes-5-0">OntoNotes 5.0</pwcdataset>
    </paper>
    <paper id="14">
      <title><fixed-case>S</fixed-case>em<fixed-case>A</fixed-case>ttack: Natural Textual Attacks via Different Semantic Spaces</title>
      <author><first>Boxin</first><last>Wang</last></author>
      <author><first>Chejian</first><last>Xu</last></author>
      <author><first>Xiangyu</first><last>Liu</last></author>
      <author><first>Yu</first><last>Cheng</last></author>
      <author id="bo-li-vanderbilt"><first>Bo</first><last>Li</last></author>
      <pages>176-205</pages>
      <abstract>Recent studies show that pre-trained language models (LMs) are vulnerable to textual adversarial attacks. However, existing attack methods either suffer from low attack success rates or fail to search efficiently in the exponentially large perturbation space. We propose an efficient and effective framework SemAttack to generate natural adversarial text by constructing different semantic perturbation functions. In particular, SemAttack optimizes the generated perturbations constrained on generic semantic spaces, including typo space, knowledge space (e.g., WordNet), contextualized semantic space (e.g., the embedding space of BERT clusterings), or the combination of these spaces. Thus, the generated adversarial texts are more semantically close to the original inputs. Extensive experiments reveal that state-of-the-art (SOTA) large-scale LMs (e.g., DeBERTa-v2) and defense strategies (e.g., FreeLB) are still vulnerable to SemAttack. We further demonstrate that SemAttack is general and able to generate natural adversarial texts for different languages (e.g., English and Chinese) with high attack success rates. Human evaluations also confirm that our generated adversarial texts are natural and barely affect human performance. Our code is publicly available at <url>https://github.com/AI-secure/SemAttack</url>.</abstract>
      <url hash="40b47248">2022.findings-naacl.14</url>
      <bibkey>wang-etal-2022-semattack</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.14</doi>
      <video href="2022.findings-naacl.14.mp4"/>
      <pwccode url="https://github.com/ai-secure/semattack" additional="false">ai-secure/semattack</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="15">
      <title>Lacuna Reconstruction: Self-Supervised Pre-Training for Low-Resource Historical Document Transcription</title>
      <author><first>Nikolai</first><last>Vogler</last></author>
      <author><first>Jonathan</first><last>Allen</last></author>
      <author><first>Matthew</first><last>Miller</last></author>
      <author><first>Taylor</first><last>Berg-Kirkpatrick</last></author>
      <pages>206-216</pages>
      <abstract>We present a self-supervised pre-training approach for learning rich visual language representations for both handwritten and printed historical document transcription. After supervised fine-tuning of our pre-trained encoder representations for low-resource document transcription on two languages, (1) a heterogeneous set of handwritten Islamicate manuscript images and (2) early modern English printed documents, we show a meaningful improvement in recognition accuracy over the same supervised model trained from scratch with as few as 30 line image transcriptions for training. Our masked language model-style pre-training strategy, where the model is trained to be able to identify the true masked visual representation from distractors sampled from within the same line, encourages learning robust contextualized language representations invariant to scribal writing style and printing noise present across documents.</abstract>
      <url hash="1191c339">2022.findings-naacl.15</url>
      <bibkey>vogler-etal-2022-lacuna</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.15</doi>
      <video href="2022.findings-naacl.15.mp4"/>
    </paper>
    <paper id="16">
      <title><fixed-case>F</fixed-case>ree<fixed-case>T</fixed-case>ransfer-<fixed-case>X</fixed-case>: Safe and Label-Free Cross-Lingual Transfer from Off-the-Shelf Models</title>
      <author><first>Yinpeng</first><last>Guo</last></author>
      <author><first>Liangyou</first><last>Li</last></author>
      <author><first>Xin</first><last>Jiang</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <pages>217-228</pages>
      <abstract>Cross-lingual transfer (CLT) is of various applications. However, labeled cross-lingual corpus is expensive or even inaccessible, especially in the fields where labels are private, such as diagnostic results of symptoms in medicine and user profiles in business. Nevertheless, there are off-the-shelf models in these sensitive fields. Instead of pursuing the original labels, a workaround for CLT is to transfer knowledge from the off-the-shelf models without labels. To this end, we define a novel CLT problem named FreeTransfer-X that aims to achieve knowledge transfer from the off-the-shelf models in rich-resource languages. To address the problem, we propose a 2-step knowledge distillation (KD, Hinton et al., 2015) framework based on multilingual pre-trained language models (mPLM). The significant improvement over strong neural machine translation (NMT) baselines demonstrates the effectiveness of the proposed method. In addition to reducing annotation cost and protecting private labels, the proposed method is compatible with different networks and easy to be deployed. Finally, a range of analyses indicate the great potential of the proposed method.</abstract>
      <url hash="75455d77">2022.findings-naacl.16</url>
      <bibkey>guo-etal-2022-freetransfer</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.16</doi>
      <video href="2022.findings-naacl.16.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/mtop">MTOP</pwcdataset>
    </paper>
    <paper id="17">
      <title>Opportunities for Human-centered Evaluation of Machine Translation Systems</title>
      <author><first>Daniel</first><last>Liebling</last></author>
      <author><first>Katherine</first><last>Heller</last></author>
      <author><first>Samantha</first><last>Robertson</last></author>
      <author><first>Wesley</first><last>Deng</last></author>
      <pages>229-240</pages>
      <abstract>Machine translation models are embedded in larger user-facing systems. Although model evaluation has matured, evaluation at the systems level is still lacking. We review literature from both the translation studies and HCI communities about who uses machine translation and for what purposes. We emphasize an important difference in evaluating machine translation models versus the physical and cultural systems in which they are embedded. We then propose opportunities for improved measurement of user-facing translation systems. We pay particular attention to the need for design and evaluation to aid engendering trust and enhancing user agency in future machine translation systems.</abstract>
      <url hash="0d8b6793">2022.findings-naacl.17</url>
      <bibkey>liebling-etal-2022-opportunities</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.17</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/mtop">MTOP</pwcdataset>
    </paper>
    <paper id="18">
      <title>Aligning Generative Language Models with Human Values</title>
      <author><first>Ruibo</first><last>Liu</last></author>
      <author><first>Ge</first><last>Zhang</last></author>
      <author><first>Xinyu</first><last>Feng</last></author>
      <author><first>Soroush</first><last>Vosoughi</last></author>
      <pages>241-252</pages>
      <abstract>Although current large-scale generative language models (LMs) can show impressive insights about factual knowledge, they do not exhibit similar success with respect to human values judgements (e.g., whether or not the generations of an LM are moral). Existing methods learn human values either by directly mimicking the behavior of human data, or rigidly constraining the generation space to human-chosen tokens. These methods are inherently limited in that they do not consider the contextual and abstract nature of human values and as a result often fail when dealing with out-of-domain context or sophisticated and abstract human values. This paper proposes SENSEI, a new reinforcement learning based method that can embed human values judgements into each step of language generation. SENSEI deploys an Actor-Critic framework, where the Critic is a reward distributor that simulates the reward assignment procedure of humans, while the Actor guides the generation towards the maximum reward direction. Compared with five existing methods in three human values alignment datasets, SENSEI not only achieves higher alignment performance in terms of both automatic and human evaluations, but also shows improvements on robustness and transfer learning on unseen human values.</abstract>
      <url hash="3c93d668">2022.findings-naacl.18</url>
      <bibkey>liu-etal-2022-aligning</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.18</doi>
      <video href="2022.findings-naacl.18.mp4"/>
    </paper>
    <paper id="19">
      <title><fixed-case>P</fixed-case>er<fixed-case>KGQA</fixed-case>: Question Answering over Personalized Knowledge Graphs</title>
      <author><first>Ritam</first><last>Dutt</last></author>
      <author><first>Kasturi</first><last>Bhattacharjee</last></author>
      <author><first>Rashmi</first><last>Gangadharaiah</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <author><first>Carolyn</first><last>Rose</last></author>
      <pages>253-268</pages>
      <abstract>Previous studies on question answering over knowledge graphs have typically operated over a single knowledge graph (KG). This KG is assumed to be known a priori and is lever- aged similarly for all users’ queries during inference. However, such an assumption is not applicable to real-world settings, such as health- care, where one needs to handle queries of new users over unseen KGs during inference. Furthermore, privacy concerns and high computational costs render it infeasible to query the single KG that has information about all users while answering a specific user’s query. The above concerns motivate our question answer- ing setting over personalized knowledge graphs (PERKGQA) where each user has restricted access to their KG. We observe that current state-of-the-art KGQA methods that require learning prior node representations fare poorly. We propose two complementary approaches, PATHCBR and PATHRGCN for PERKGQA. The former is a simple non-parametric technique that employs case-based reasoning, while the latter is a parametric approach using graph neural networks. Our proposed methods circumvent learning prior representations, can generalize to unseen KGs, and outperform strong baselines on an academic and an internal dataset by 6.5% and 10.5%.</abstract>
      <url hash="4f3df8bd">2022.findings-naacl.19</url>
      <bibkey>dutt-etal-2022-perkgqa</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.19</doi>
      <video href="2022.findings-naacl.19.mp4"/>
    </paper>
    <paper id="20">
      <title>Zero-shot Cross-lingual Conversational Semantic Role Labeling</title>
      <author><first>Han</first><last>Wu</last></author>
      <author><first>Haochen</first><last>Tan</last></author>
      <author><first>Kun</first><last>Xu</last></author>
      <author><first>Shuqi</first><last>Liu</last></author>
      <author><first>Lianwei</first><last>Wu</last></author>
      <author><first>Linqi</first><last>Song</last></author>
      <pages>269-281</pages>
      <abstract>While conversational semantic role labeling (CSRL) has shown its usefulness on Chinese conversational tasks, it is still under-explored in non-Chinese languages due to the lack of multilingual CSRL annotations for the parser training. To avoid expensive data collection and error-propagation of translation-based methods, we present a simple but effective approach to perform zero-shot cross-lingual CSRL.Our model implicitly learns language-agnostic, conversational structure-aware and semantically rich representations with the hierarchical encoders and elaborately designed pre-training objectives. Experimental results show that our model outperforms all baselines by large margins on two newly collected English CSRL test sets. More importantly, we confirm the usefulness of CSRL to non-Chinese conversational tasks such as the question-in-context rewriting task in English and the multi-turn dialogue response generation tasks in English, German and Japanese by incorporating the CSRL information into the downstream conversation-based models. We believe this finding is significant and will facilitate the research of non-Chinese dialogue tasks which suffer the problems of ellipsis and anaphora.</abstract>
      <url hash="3b6e3e5e">2022.findings-naacl.20</url>
      <bibkey>wu-etal-2022-zero-shot</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.20</doi>
      <video href="2022.findings-naacl.20.mp4"/>
      <pwccode url="https://github.com/hahahawu/zero-shot-xcsrl" additional="false">hahahawu/zero-shot-xcsrl</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/canard">CANARD</pwcdataset>
    </paper>
    <paper id="21">
      <title>A Framework to Generate High-Quality Datapoints for Multiple Novel Intent Detection</title>
      <author><first>Ankan</first><last>Mullick</last></author>
      <author><first>Sukannya</first><last>Purkayastha</last></author>
      <author><first>Pawan</first><last>Goyal</last></author>
      <author><first>Niloy</first><last>Ganguly</last></author>
      <pages>282-292</pages>
      <abstract>Systems like Voice-command based conversational agents are characterized by a pre-defined set of skills or intents to perform user specified tasks. In the course of time, newer intents may emerge requiring retraining. However, the newer intents may not be explicitly announced and need to be inferred dynamically. Thus, there are two important tasks at hand (a). identifying emerging new intents, (b). annotating data of the new intents so that the underlying classifier can be retrained efficiently. The tasks become specially challenging when a large number of new intents emerge simultaneously and there is a limited budget of manual annotation. In this paper, we propose MNID (Multiple Novel Intent Detection) which is a cluster based framework to detect multiple novel intents with budgeted human annotation cost. Empirical results on various benchmark datasets (of different sizes) demonstrate that MNID, by intelligently using the budget for annotation, outperforms the baseline methods in terms of accuracy and F1-score.</abstract>
      <url hash="b110a469">2022.findings-naacl.21</url>
      <bibkey>mullick-etal-2022-framework</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.21</doi>
      <video href="2022.findings-naacl.21.mp4"/>
      <pwccode url="https://github.com/sukannyapurkayastha/mnid" additional="false">sukannyapurkayastha/mnid</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/banking77">BANKING77</pwcdataset>
    </paper>
    <paper id="22">
      <title>Design Challenges for a Multi-Perspective Search Engine</title>
      <author><first>Sihao</first><last>Chen</last></author>
      <author><first>Siyi</first><last>Liu</last></author>
      <author><first>Xander</first><last>Uyttendaele</last></author>
      <author><first>Yi</first><last>Zhang</last></author>
      <author><first>William</first><last>Bruno</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>293-303</pages>
      <abstract>Many users turn to document retrieval systems (e.g. search engines) to seek answers to controversial or open-ended questions. However, classical document retrieval systems fall short at delivering users a set of direct and diverse responses in such cases, which requires identifying responses within web documents in the context of the query, and aggregating the responses based on their different perspectives. The goal of this work is to survey and study the user information needs for building a multi-perspective search engine of such. We examine the challenges of synthesizing such language understanding objectives with document retrieval, and study a new <i>perspective-oriented</i> document retrieval paradigm. We discuss and assess the inherent natural language understanding challenges one needs to address in order to achieve the goal. Following the design challenges and principles, we propose and evaluate a practical prototype pipeline system. We use the prototype system to conduct a user survey in order to assess the utility of our paradigm, as well as understanding the user information needs when issuing controversial and open-ended queries to a search engine.</abstract>
      <url hash="e8132fa1">2022.findings-naacl.22</url>
      <bibkey>chen-etal-2022-design</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.22</doi>
      <video href="2022.findings-naacl.22.mp4"/>
      <pwccode url="https://github.com/cogcomp/multi-persp-search-engine" additional="false">cogcomp/multi-persp-search-engine</pwccode>
    </paper>
    <paper id="23">
      <title>Exploring the Value of Multi-View Learning for Session-Aware Query Representation</title>
      <author><first>Diego</first><last>Ortiz</last></author>
      <author><first>Jose</first><last>Moreno</last></author>
      <author><first>Gilles</first><last>Hubert</last></author>
      <author><first>Karen</first><last>Pinel-Sauvagnat</last></author>
      <author><first>Lynda</first><last>Tamine</last></author>
      <pages>304-315</pages>
      <abstract>Recent years have witnessed a growing interest towards learning distributed query representations that are able to capture search intent semantics. Most existing approaches learn query embeddings using relevance supervision making them suited only to document ranking tasks. Besides, they generally consider either user’s query reformulations or system’s rankings whereas previous findings show that user’s query behavior and knowledge change depending on the system’s results, intertwine and affect each other during the completion of a search task. In this paper, we explore the value of multi-view learning for generic and unsupervised session-aware query representation learning. First, single-view query embeddings are obtained in separate spaces from query reformulations and document ranking representations using transformers. Then, we investigate the use of linear (CCA) and non linear (UMAP) multi-view learning methods, to align those spaces with the aim of revealing similarity traits in the multi-view shared space. Experimental evaluation is carried out in a query classification and session-based retrieval downstream tasks using respectively the KDD and TREC session datasets. The results show that multi-view learning is an effective and controllable approach for unsupervised learning of generic query representations and can reflect search behavior patterns.</abstract>
      <url hash="bca1d0e9">2022.findings-naacl.23</url>
      <attachment type="software" hash="1fa7a6b0">2022.findings-naacl.23.software.zip</attachment>
      <bibkey>ortiz-etal-2022-exploring</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.23</doi>
    </paper>
    <paper id="24">
      <title>Hierarchical Relation-Guided Type-Sentence Alignment for Long-Tail Relation Extraction with Distant Supervision</title>
      <author><first>Yang</first><last>Li</last></author>
      <author><first>Guodong</first><last>Long</last></author>
      <author><first>Tao</first><last>Shen</last></author>
      <author><first>Jing</first><last>Jiang</last></author>
      <pages>316-326</pages>
      <abstract>Distant supervision uses triple facts in knowledge graphs to label a corpus for relation extraction, leading to wrong labeling and long-tail problems. Some works use the hierarchy of relations for knowledge transfer to long-tail relations. However, a coarse-grained relation often implies only an attribute (e.g., domain or topic) of the distant fact, making it hard to discriminate relations based solely on sentence semantics. One solution is resorting to entity types, but open questions remain about how to fully leverage the information of entity types and how to align multi-granular entity types with sentences. In this work, we propose a novel model to enrich distantly-supervised sentences with entity types. It consists of (1) a pairwise type-enriched sentence encoding module injecting both context-free and -related backgrounds to alleviate sentence-level wrong labeling, and (2) a hierarchical type-sentence alignment module enriching a sentence with the triple fact’s basic attributes to support long-tail relations. Our model achieves new state-of-the-art results in overall and long-tail performance on benchmarks.</abstract>
      <url hash="49c6c202">2022.findings-naacl.24</url>
      <bibkey>li-etal-2022-hierarchical</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.24</doi>
      <video href="2022.findings-naacl.24.mp4"/>
    </paper>
    <paper id="25">
      <title><fixed-case>PCEE</fixed-case>-<fixed-case>BERT</fixed-case>: Accelerating <fixed-case>BERT</fixed-case> Inference via Patient and Confident Early Exiting</title>
      <author><first>Zhen</first><last>Zhang</last></author>
      <author><first>Wei</first><last>Zhu</last></author>
      <author><first>Jinfan</first><last>Zhang</last></author>
      <author><first>Peng</first><last>Wang</last></author>
      <author><first>Rize</first><last>Jin</last></author>
      <author><first>Tae-Sun</first><last>Chung</last></author>
      <pages>327-338</pages>
      <abstract>BERT and other pretrained language models (PLMs) are ubiquitous in modern NLP. Even though PLMs are the state-of-the-art (SOTA) models for almost every NLP task (CITATION), the significant latency during inference prohibits wider industrial usage. In this work, we propose Patient and Confident Early Exiting BERT (PCEE-BERT), an off-the-shelf sample-dependent early exiting method that can work with different PLMs and can also work along with popular model compression methods. With a multi-exit BERT as the backbone model, PCEE-BERT will make the early exiting decision if enough numbers (patience parameter) of consecutive intermediate layers are confident about their predictions. The entropy value measures the confidence level of an intermediate layer’s prediction. Experiments on the GLUE benchmark demonstrate that our method outperforms previous SOTA early exiting methods. Ablation studies show that: (a) our method performs consistently well on other PLMs, such as ALBERT and TinyBERT; (b) PCEE-BERT can achieve different speed-up ratios by adjusting the patience parameter and the confidence threshold. The code for PCEE-BERT can be found at <url>https://github.com/michael-wzhu/PCEE-BERT</url>.</abstract>
      <url hash="75249766">2022.findings-naacl.25</url>
      <bibkey>zhang-etal-2022-pcee</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.25</doi>
      <pwccode url="https://github.com/michael-wzhu/pcee-bert" additional="false">michael-wzhu/pcee-bert</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cifar-10">CIFAR-10</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cifar-100">CIFAR-100</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
    </paper>
    <paper id="26">
      <title>Learning to repair: Repairing model output errors after deployment using a dynamic memory of feedback</title>
      <author><first>Niket</first><last>Tandon</last></author>
      <author><first>Aman</first><last>Madaan</last></author>
      <author><first>Peter</first><last>Clark</last></author>
      <author><first>Yiming</first><last>Yang</last></author>
      <pages>339-352</pages>
      <abstract>Large language models (LMs), while powerful, are not immune to mistakes, but can be difficult to retrain. Our goal is for an LM to continue to improve after deployment, without retraining, using feedback from the user. Our approach pairs an LM with (i) a growing memory of cases where the user identified an output error and provided general feedback on how to correct it (ii) a <i>corrector model</i>, trained to translate this general feedback into specific edits to repair the model output. Given a new, unseen input, our model can then use feedback from similar, past cases to repair output errors that may occur. We instantiate our approach using an existing, fixed model for <i>script generation</i>, that takes a goal (e.g., “bake a cake”) and generates a partially ordered sequence of actions to achieve that goal, sometimes containing errors. Our memory-enhanced system, , learns to apply user feedback to repair such errors (up to 30 points improvement), while making a start at avoiding similar past mistakes on new, unseen examples (up to 7 points improvement in a controlled setting). This is a first step towards strengthening deployed models, potentially broadening their utility. Our code and data is available at <url>https://github.com/allenai/interscript</url></abstract>
      <url hash="918ed01e">2022.findings-naacl.26</url>
      <bibkey>tandon-etal-2022-learning</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.26</doi>
      <video href="2022.findings-naacl.26.mp4"/>
      <pwccode url="https://github.com/allenai/interscript" additional="false">allenai/interscript</pwccode>
    </paper>
    <paper id="27">
      <title>One Size Does Not Fit All: The Case for Personalised Word Complexity Models</title>
      <author><first>Sian</first><last>Gooding</last></author>
      <author><first>Manuel</first><last>Tragut</last></author>
      <pages>353-365</pages>
      <abstract>Complex Word Identification (CWI) aims to detect words within a text that a reader may find difficult to understand. It has been shown that CWI systems can improve text simplification, readability prediction and vocabulary acquisition modelling. However, the difficulty of a word is a highly idiosyncratic notion that depends on a reader’s first language, proficiency and reading experience. In this paper, we show that personal models are best when predicting word complexity for individual readers. We use a novel active learning framework that allows models to be tailored to individuals and release a dataset of complexity annotations and models as a benchmark for further research.</abstract>
      <url hash="34797866">2022.findings-naacl.27</url>
      <bibkey>gooding-tragut-2022-one</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.27</doi>
    </paper>
    <paper id="28">
      <title><fixed-case>TEAM</fixed-case>: A multitask learning based Taxonomy Expansion approach for Attach and Merge</title>
      <author><first>Bornali</first><last>Phukon</last></author>
      <author><first>Anasua</first><last>Mitra</last></author>
      <author><first>Ranbir</first><last>Sanasam</last></author>
      <author><first>Priyankoo</first><last>Sarmah</last></author>
      <pages>366-378</pages>
      <abstract>Taxonomy expansion is a crucial task. Most of Automatic expansion of taxonomy are of two types, attach and merge. In a taxonomy like WordNet, both merge and attach are integral parts of the expansion operations but majority of study consider them separately. This paper proposes a novel mult-task learning-based deep learning method known as Taxonomy Expansion with Attach and Merge (TEAM) that performs both the merge and attach operations. To the best of our knowledge this is the first study which integrates both merge and attach operations in a single model. The proposed models have been evaluated on three separate WordNet taxonomies, viz., Assamese, Bangla, and Hindi. From the various experimental setups, it is shown that TEAM outperforms its state-of-the-art counterparts for attach operation, and also provides highly encouraging performance for the merge operation.</abstract>
      <url hash="59341166">2022.findings-naacl.28</url>
      <bibkey>phukon-etal-2022-team</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.28</doi>
      <video href="2022.findings-naacl.28.mp4"/>
    </paper>
    <paper id="29">
      <title>Extracting Temporal Event Relation with Syntax-guided Graph Transformer</title>
      <author><first>Shuaicheng</first><last>Zhang</last></author>
      <author><first>Qiang</first><last>Ning</last></author>
      <author><first>Lifu</first><last>Huang</last></author>
      <pages>379-390</pages>
      <abstract>Extracting temporal relations (e.g., before, after, and simultaneous) among events is crucial to natural language understanding. One of the key challenges of this problem is that when the events of interest are far away in text, the context in-between often becomes complicated, making it challenging to resolve the temporal relationship between them. This paper thus proposes a new Syntax-guided Graph Transformer network (SGT) to mitigate this issue, by (1) explicitly exploiting the connection between two events based on their dependency parsing trees, and (2) automatically locating temporal cues between two events via a novel syntax-guided attention mechanism. Experiments on two benchmark datasets, MATRES and TB-DENSE, show that our approach significantly outperforms previous state-of-the-art methods on both end-to-end temporal relation extraction and temporal relation classification with up to 7.9% absolute F-score gain; This improvement also proves to be robust on the contrast set of MATRES. We will make all the programs publicly available once the paper is accepted.</abstract>
      <url hash="422143f7">2022.findings-naacl.29</url>
      <bibkey>zhang-etal-2022-extracting</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.29</doi>
      <video href="2022.findings-naacl.29.mp4"/>
      <pwccode url="https://github.com/vt-nlp/syntax-guided-graph-transformer" additional="false">vt-nlp/syntax-guided-graph-transformer</pwccode>
    </paper>
    <paper id="30">
      <title>From Cognitive to Computational Modeling: <fixed-case>T</fixed-case>ext-based Risky Decision-Making Guided by Fuzzy Trace Theory</title>
      <author><first>Jaron</first><last>Mar</last></author>
      <author><first>Jiamou</first><last>Liu</last></author>
      <pages>391-409</pages>
      <abstract>Understanding, modelling and predicting human risky decision-making is challenging due to intrinsic individual differences and irrationality. Fuzzy trace theory (FTT) is a powerful paradigm that explains human decision-making by incorporating gists, i.e., fuzzy representations of information which capture only its quintessential meaning. Inspired by Broniatowski and Reyna’s FTT cognitive model, we propose a computational framework which combines the effects of the underlying semantics and sentiments on text-based decision-making. In particular, we introduce Category-2-Vector to learn categorical gists and categorical sentiments, and demonstrate how our computational model can be optimised to predict risky decision-making in groups and individuals.</abstract>
      <url hash="fa961af2">2022.findings-naacl.30</url>
      <attachment type="software" hash="108d47c5">2022.findings-naacl.30.software.zip</attachment>
      <bibkey>mar-liu-2022-cognitive</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.30</doi>
    </paper>
    <paper id="31">
      <title>Few-Shot Self-Rationalization with Natural Language Prompts</title>
      <author><first>Ana</first><last>Marasovic</last></author>
      <author><first>Iz</first><last>Beltagy</last></author>
      <author><first>Doug</first><last>Downey</last></author>
      <author><first>Matthew</first><last>Peters</last></author>
      <pages>410-424</pages>
      <abstract>Self-rationalization models that predict task labels and generate free-text elaborations for their predictions could enable more intuitive interaction with NLP systems. These models are, however, currently trained with a large amount of human-written free-text explanations for each task which hinders their broader usage. We propose to study a more realistic setting of self-rationalization using few training examples. We present FEB—a standardized collection of four existing English-language datasets and associated metrics. We identify the right prompting approach by extensively exploring natural language prompts on FEB. Then, by using this prompt and scaling the model size, we demonstrate that making progress on few-shot self-rationalization is possible. We show there is still ample room for improvement in this task: the average plausibility of generated explanations assessed by human annotators is at most 51% (with GPT-3), while plausibility of human explanations is 76%. We hope that FEB and our proposed approach will spur the community to take on the few-shot self-rationalization challenge.</abstract>
      <url hash="536d2a81">2022.findings-naacl.31</url>
      <bibkey>marasovic-etal-2022-shot</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.31</doi>
      <video href="2022.findings-naacl.31.mp4"/>
      <pwccode url="https://github.com/allenai/feb" additional="false">allenai/feb</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ecqa">ECQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sbic">SBIC</pwcdataset>
    </paper>
    <paper id="32">
      <title><fixed-case>DOC</fixed-case>m<fixed-case>T</fixed-case>5: Document-Level Pretraining of Multilingual Language Models</title>
      <author><first>Chia-Hsuan</first><last>Lee</last></author>
      <author><first>Aditya</first><last>Siddhant</last></author>
      <author><first>Viresh</first><last>Ratnakar</last></author>
      <author><first>Melvin</first><last>Johnson</last></author>
      <pages>425-437</pages>
      <abstract>In this paper, we introduce DOCmT5, a multilingual sequence-to-sequence language model pretrained with large-scale parallel documents. While previous approaches have focused on leveraging sentence-level parallel data, we try to build a general-purpose pretrained model that can understand and generate long documents. We propose a simple and effective pretraining objective - Document reordering Machine Translation (DrMT), in which the input documents that are shuffled and masked need to be translated. DrMT brings consistent improvements over strong baselines on a variety of document-level generation tasks, including over 12 BLEU points for seen-language pair document-level MT, over 7 BLEU points for unseen-language-pair document-level MT and over 3 ROUGE-1 points for seen-language pair cross-lingual summarization. We achieve state-of-the-art (SOTA) on WMT20 De-En and IWSLT15 Zh-En document translation tasks. We also conduct extensive analysis on various factors for document pretraining, including (1) the effects of pretraining data quality and (2) The effects of combining mono-lingual and cross-lingual pretraining. We plan to make our model checkpoints publicly available.</abstract>
      <url hash="9fff3769">2022.findings-naacl.32</url>
      <bibkey>lee-etal-2022-docmt5</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.32</doi>
      <video href="2022.findings-naacl.32.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/c4">C4</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/iwslt2015">IWSLT2015</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2020">WMT 2020</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikilingua">WikiLingua</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mc4">mC4</pwcdataset>
    </paper>
    <paper id="33">
      <title>Literature-Augmented Clinical Outcome Prediction</title>
      <author><first>Aakanksha</first><last>Naik</last></author>
      <author><first>Sravanthi</first><last>Parasa</last></author>
      <author><first>Sergey</first><last>Feldman</last></author>
      <author><first>Lucy Lu</first><last>Wang</last></author>
      <author><first>Tom</first><last>Hope</last></author>
      <pages>438-453</pages>
      <abstract>We present BEEP (Biomedical Evidence-Enhanced Predictions), a novel approach for clinical outcome prediction that retrieves patient-specific medical literature and incorporates it into predictive models. Based on each individual patient’s clinical notes, we train language models (LMs) to find relevant papers and fuse them with information from notes to predict outcomes such as in-hospital mortality. We develop methods to retrieve literature based on noisy, information-dense patient notes, and to augment existing outcome prediction models with retrieved papers in a manner that maximizes predictive accuracy. Our approach boosts predictive performance on three important clinical tasks in comparison to strong recent LM baselines, increasing F1 by up to 5 points and precision@Top-K by a large margin of over 25%.</abstract>
      <url hash="351baae6">2022.findings-naacl.33</url>
      <bibkey>naik-etal-2022-literature</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.33</doi>
      <video href="2022.findings-naacl.33.mp4"/>
      <pwccode url="https://github.com/allenai/beep" additional="false">allenai/beep</pwccode>
    </paper>
    <paper id="34">
      <title>Improving Few-Shot Relation Classification by Prototypical Representation Learning with Definition Text</title>
      <author><first>Li</first><last>Zhenzhen</last></author>
      <author><first>Yuyang</first><last>Zhang</last></author>
      <author><first>Jian-Yun</first><last>Nie</last></author>
      <author><first>Dongsheng</first><last>Li</last></author>
      <pages>454-464</pages>
      <abstract>Few-shot relation classification is difficult because the few instances available may not represent well the relation patterns. Some existing approaches explored extra information such as relation definition, in addition to the instances, to learn a better relation representation. However, the encoding of the extra information has been performed independently from the labeled instances. In this paper, we propose to learn a prototype encoder from relation definition in a way that is useful for relation instance classification. To this end, we use a joint training approach to train both a prototype encoder from definition and an instance encoder. Extensive experiments on several datasets demonstrate the effectiveness and usefulness of our prototype encoder from definition text, enabling us to outperform state-of-the-art approaches.</abstract>
      <url hash="fd8ed972">2022.findings-naacl.34</url>
      <bibkey>zhenzhen-etal-2022-improving</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.34</doi>
      <video href="2022.findings-naacl.34.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel">FewRel</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel-2-0">FewRel 2.0</pwcdataset>
    </paper>
    <paper id="35">
      <title>Entailment Tree Explanations via Iterative Retrieval-Generation Reasoner</title>
      <author><first>Danilo</first><last>Neves Ribeiro</last></author>
      <author><first>Shen</first><last>Wang</last></author>
      <author><first>Xiaofei</first><last>Ma</last></author>
      <author><first>Rui</first><last>Dong</last></author>
      <author><first>Xiaokai</first><last>Wei</last></author>
      <author><first>Henghui</first><last>Zhu</last></author>
      <author><first>Xinchi</first><last>Chen</last></author>
      <author><first>Peng</first><last>Xu</last></author>
      <author><first>Zhiheng</first><last>Huang</last></author>
      <author><first>Andrew</first><last>Arnold</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>465-475</pages>
      <abstract>Large language models have achieved high performance on various question answering (QA) benchmarks, but the explainability of their output remains elusive. Structured explanations, called entailment trees, were recently suggested as a way to explain the reasoning behind a QA system’s answer. In order to better generate such entailment trees, we propose an architecture called Iterative Retrieval-Generation Reasoner (IRGR). Our model is able to explain a given hypothesis by systematically generating a step-by-step explanation from textual premises. The IRGR model iteratively searches for suitable premises, constructing a single entailment step at a time. Contrary to previous approaches, our method combines generation steps and retrieval of premises, allowing the model to leverage intermediate conclusions, and mitigating the input size limit of baseline encoder-decoder models. We conduct experiments using the EntailmentBank dataset, where we outperform existing benchmarks on premise retrieval and entailment tree generation, with around 300% gain in overall correctness.</abstract>
      <url hash="1c61fd18">2022.findings-naacl.35</url>
      <bibkey>neves-ribeiro-etal-2022-entailment</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.35</doi>
      <video href="2022.findings-naacl.35.mp4"/>
      <pwccode url="https://github.com/amazon-research/irgr" additional="false">amazon-research/irgr</pwccode>
    </paper>
    <paper id="36">
      <title>Multimodal Intent Discovery from Livestream Videos</title>
      <author><first>Adyasha</first><last>Maharana</last></author>
      <author><first>Quan</first><last>Tran</last></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Seunghyun</first><last>Yoon</last></author>
      <author><first>Trung</first><last>Bui</last></author>
      <author><first>Walter</first><last>Chang</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>476-489</pages>
      <abstract>Individuals, educational institutions, and businesses are prolific at generating instructional video content such as “how-to” and tutorial guides. While significant progress has been made in basic video understanding tasks, identifying procedural intent within these instructional videos is a challenging and important task that remains unexplored but essential to video summarization, search, and recommendations. This paper introduces the problem of instructional intent identification and extraction from software instructional livestreams. We construct and present a new multimodal dataset consisting of software instructional livestreams and containing manual annotations for both detailed and abstract procedural intent that enable training and evaluation of joint video and text understanding models. We then introduce a multimodal cascaded cross-attention model to efficiently combine the weaker and noisier video signal with the more discriminative text signal. Our experiments show that our proposed model brings significant gains compared to strong baselines, including large-scale pretrained multimodal models. Our analysis further identifies that the task benefits from spatial as well as motion features extracted from videos, and provides insight on how the video signal is preferentially used for intent discovery. We also show that current models struggle to comprehend the nature of abstract intents, revealing important gaps in multimodal understanding and paving the way for future work.</abstract>
      <url hash="34a34803">2022.findings-naacl.36</url>
      <attachment type="software" hash="10806c79">2022.findings-naacl.36.software.zip</attachment>
      <bibkey>maharana-etal-2022-multimodal</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.36</doi>
      <video href="2022.findings-naacl.36.mp4"/>
    </paper>
    <paper id="37">
      <title>A Question-Answer Driven Approach to Reveal Affirmative Interpretations from Verbal Negations</title>
      <author><first>Md Mosharaf</first><last>Hossain</last></author>
      <author><first>Luke</first><last>Holman</last></author>
      <author><first>Anusha</first><last>Kakileti</last></author>
      <author><first>Tiffany</first><last>Kao</last></author>
      <author><first>Nathan</first><last>Brito</last></author>
      <author><first>Aaron</first><last>Mathews</last></author>
      <author><first>Eduardo</first><last>Blanco</last></author>
      <pages>490-503</pages>
      <abstract>This paper explores a question-answer driven approach to reveal affirmative interpretations from verbal negations (i.e., when a negation cue grammatically modifies a verb). We create a new corpus consisting of 4,472 verbal negations and discover that 67.1% of them convey that an event actually occurred. Annotators generate and answer 7,277 questions % converted for 4,000 for the 3,001 negations that convey an affirmative interpretation. We first cast the problem of revealing affirmative interpretations from negations as a natural language inference (NLI) classification task. Experimental results show that state-of-the-art transformers trained with existing NLI corpora are insufficient to reveal affirmative interpretations. We also observe, however, that fine-tuning brings substantial improvements. In addition to NLI classification, we also explore the more realistic task of generating affirmative interpretations directly from negations with the T5 transformer. We conclude that the generation task remains a challenge as T5 substantially underperforms humans.</abstract>
      <url hash="ce68721e">2022.findings-naacl.37</url>
      <attachment type="software" hash="f283d636">2022.findings-naacl.37.software.zip</attachment>
      <bibkey>hossain-etal-2022-question</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.37</doi>
      <video href="2022.findings-naacl.37.mp4"/>
      <pwccode url="https://github.com/mosharafhossain/afin" additional="false">mosharafhossain/afin</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qa-srl">QA-SRL</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="38">
      <title>Harmless Transfer Learning for Item Embeddings</title>
      <author><first>Chengyue</first><last>Gong</last></author>
      <author><first>Xiaocong</first><last>Du</last></author>
      <author><first>Dhruv</first><last>Choudhary</last></author>
      <author><first>Bhargav</first><last>Bhushanam</last></author>
      <author><first>Qiang</first><last>Liu</last></author>
      <author><first>Arun</first><last>Kejariwal</last></author>
      <pages>504-516</pages>
      <abstract>Learning embedding layers (for classes, words, items, etc.) is a key component of lots of applications, ranging from natural language processing, recommendation systems to electronic health records, etc. However, the frequency of real-world items follows a long-tail distribution in these applications, causing naive training methods perform poorly on the rare items. A line of previous works address this problem by transferring the knowledge from the frequent items to rare items by introducing an auxiliary transfer loss. However, when defined improperly, the transfer loss may introduce harmful biases and deteriorate the performance. In this work, we propose a harmless transfer learning framework that limits the impact of the potential biases in both the definition and optimization of the transfer loss. On the definition side, we reduce the bias in transfer loss by focusing on the items to which information from high-frequency items can be efficiently transferred. On the optimization side, we leverage a lexicographic optimization framework to efficiently incorporate the information of the transfer loss without hurting the minimization of the main prediction loss function. Our method serves as a plug-in module and significantly boosts the performance on a variety of NLP and recommendation system tasks.</abstract>
      <url hash="4d15266f">2022.findings-naacl.38</url>
      <bibkey>gong-etal-2022-harmless</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.38</doi>
      <video href="2022.findings-naacl.38.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL 2003</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/movielens">MovieLens</pwcdataset>
    </paper>
    <paper id="39">
      <title>Fine-grained Image Captioning with <fixed-case>CLIP</fixed-case> Reward</title>
      <author><first>Jaemin</first><last>Cho</last></author>
      <author><first>Seunghyun</first><last>Yoon</last></author>
      <author><first>Ajinkya</first><last>Kale</last></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Trung</first><last>Bui</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>517-527</pages>
      <abstract>Modern image captioning models are usually trained with text similarity objectives. However, since reference captions in public datasets often describe the most salient common objects, models trained with the text similarity objectives tend to ignore specific and detailed aspects of an image that distinguish it from others. Towards more descriptive and distinctive caption generation, we propose to use CLIP, a multimodal encoder trained on huge image-text pairs from the web, to calculate multi-modal similarity and use it as a reward function. We also propose a simple finetuning strategy of CLIP text encoder to improve grammar that does not require extra text annotation. This completely eliminates the need for reference captions during the reward computation. To comprehensively evaluate descriptive captions, we introduce FineCapEval, a new dataset for caption evaluation with fine-grained criteria: overall, background, object, relations. In our experiments on text-to-image retrieval and FineCapEval, the proposed CLIP-guided model generates more distinctive captions than the CIDEroptimized model. We also show that our unsupervised grammar finetuning of the CLIP text encoder alleviates the degeneration problem of the naive CLIP reward. Lastly, we show human analysis where the annotators strongly prefer CLIP reward to CIDEr and MLE objectives on diverse criteria.</abstract>
      <url hash="a0672dd2">2022.findings-naacl.39</url>
      <bibkey>cho-etal-2022-fine</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.39</doi>
      <video href="2022.findings-naacl.39.mp4"/>
      <pwccode url="https://github.com/j-min/clip-caption-reward" additional="false">j-min/clip-caption-reward</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/coco-captions">COCO Captions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">MS COCO</pwcdataset>
    </paper>
    <paper id="40">
      <title>Improving the Faithfulness of Abstractive Summarization via Entity Coverage Control</title>
      <author><first>Haopeng</first><last>Zhang</last></author>
      <author><first>Semih</first><last>Yavuz</last></author>
      <author><first>Wojciech</first><last>Kryscinski</last></author>
      <author><first>Kazuma</first><last>Hashimoto</last></author>
      <author><first>Yingbo</first><last>Zhou</last></author>
      <pages>528-535</pages>
      <abstract>Abstractive summarization systems leveraging pre-training language models have achieved superior results on benchmark datasets. However, such models have been shown to be more prone to hallucinate facts that are unfaithful to the input context. In this paper, we propose a method to remedy entity-level extrinsic hallucinations with Entity Coverage Control (ECC). We first compute entity coverage precision and prepend the corresponding control code for each training example, which implicitly guides the model to recognize faithfulness contents in the training phase. We further extend our method via intermediate fine-tuning on large but noisy data extracted from Wikipedia to unlock zero-shot summarization. We show that the proposed method leads to more faithful and salient abstractive summarization in supervised fine-tuning and zero-shot settings according to our experimental results on three benchmark datasets XSum, Pubmed, and SAMSum of very different domains and styles.</abstract>
      <url hash="b23a8b57">2022.findings-naacl.40</url>
      <bibkey>zhang-etal-2022-improving-faithfulness</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.40</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/samsum-corpus">SAMSum</pwcdataset>
    </paper>
    <paper id="41">
      <title>Modeling Ideological Salience and Framing in Polarized Online Groups with Graph Neural Networks and Structured Sparsity</title>
      <author><first>Valentin</first><last>Hofmann</last></author>
      <author><first>Xiaowen</first><last>Dong</last></author>
      <author><first>Janet</first><last>Pierrehumbert</last></author>
      <author><first>Hinrich</first><last>Schuetze</last></author>
      <pages>536-550</pages>
      <abstract>The increasing polarization of online political discourse calls for computational tools that automatically detect and monitor ideological divides in social media. We introduce a minimally supervised method that leverages the network structure of online discussion forums, specifically Reddit, to detect polarized concepts. We model polarization along the dimensions of salience and framing, drawing upon insights from moral psychology. Our architecture combines graph neural networks with structured sparsity learning and results in representations for concepts and subreddits that capture temporal ideological dynamics such as right-wing and left-wing radicalization.</abstract>
      <url hash="b10e6a8a">2022.findings-naacl.41</url>
      <attachment type="software" hash="55ede551">2022.findings-naacl.41.software.zip</attachment>
      <bibkey>hofmann-etal-2022-modeling</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.41</doi>
      <video href="2022.findings-naacl.41.mp4"/>
      <pwccode url="https://github.com/valentinhofmann/slap4slip" additional="false">valentinhofmann/slap4slip</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/pushshift-reddit">Pushshift Reddit</pwcdataset>
    </paper>
    <paper id="42">
      <title>On Measuring Social Biases in Prompt-Based Multi-Task Learning</title>
      <author><first>Afra Feyza</first><last>Akyürek</last></author>
      <author><first>Sejin</first><last>Paik</last></author>
      <author><first>Muhammed</first><last>Kocyigit</last></author>
      <author><first>Seda</first><last>Akbiyik</last></author>
      <author><first>Serife Leman</first><last>Runyun</last></author>
      <author><first>Derry</first><last>Wijaya</last></author>
      <pages>551-564</pages>
      <abstract>Large language models trained on a mixture of NLP tasks that are converted into a text-to-text format using prompts, can generalize into novel forms of language and handle novel tasks. A large body of work within prompt engineering attempts to understand the effects of input forms and prompts in achieving superior performance. We consider an alternative measure and inquire whether the way in which an input is encoded affects social biases promoted in outputs. In this paper, we study T0, a large-scale multi-task text-to-text language model trained using prompt-based learning. We consider two different forms of semantically equivalent inputs: question-answer format and premise-hypothesis format. We use an existing bias benchmark for the former BBQ and create the first bias benchmark in natural language inference BBNLI with hand-written hypotheses while also converting each benchmark into the other form. The results on two benchmarks suggest that given two different formulations of essentially the same input, T0 conspicuously acts more biased in question answering form, which is seen during training, compared to premise-hypothesis form which is unlike its training examples. Code and data are released under <url>https://github.com/feyzaakyurek/bbnli</url>.</abstract>
      <url hash="1ac22f65">2022.findings-naacl.42</url>
      <bibkey>akyurek-etal-2022-measuring</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.42</doi>
      <video href="2022.findings-naacl.42.mp4"/>
      <pwccode url="https://github.com/feyzaakyurek/bbnli" additional="false">feyzaakyurek/bbnli</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bbq">BBQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/crows-pairs">CrowS-Pairs</pwcdataset>
    </paper>
    <paper id="43">
      <title>Anti-Overestimation Dialogue Policy Learning for Task-Completion Dialogue System</title>
      <author><first>Chang</first><last>Tian</last></author>
      <author><first>Wenpeng</first><last>Yin</last></author>
      <author><first>Marie-Francine</first><last>Moens</last></author>
      <pages>565-577</pages>
      <abstract>A dialogue policy module is an essential part of task-completion dialogue systems. Recently, increasing interest has focused on reinforcement learning (RL)-based dialogue policy. Its favorable performance and wise action decisions rely on an accurate estimation of action values. The overestimation problem is a widely known issue of RL since its estimate of the maximum action value is larger than the ground truth, which results in an unstable learning process and suboptimal policy. This problem is detrimental to RL-based dialogue policy learning. To mitigate this problem, this paper proposes a dynamic partial average estimator (DPAV) of the ground truth maximum action value. DPAV calculates the partial average between the predicted maximum action value and minimum action value, where the weights are dynamically adaptive and problem-dependent. We incorporate DPAV into a deep Q-network as the dialogue policy and show that our method can achieve better or comparable results compared to top baselines on three dialogue datasets of different domains with a lower computational load. In addition, we also theoretically prove the convergence and derive the upper and lower bounds of the bias compared with those of other methods.</abstract>
      <url hash="6e89e096">2022.findings-naacl.43</url>
      <bibkey>tian-etal-2022-anti</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.43</doi>
      <video href="2022.findings-naacl.43.mp4"/>
    </paper>
    <paper id="44">
      <title><fixed-case>P</fixed-case>enn-<fixed-case>H</fixed-case>elsinki Parsed Corpus of Early <fixed-case>M</fixed-case>odern <fixed-case>E</fixed-case>nglish: First Parsing Results and Analysis</title>
      <author><first>Seth</first><last>Kulick</last></author>
      <author><first>Neville</first><last>Ryant</last></author>
      <author><first>Beatrice</first><last>Santorini</last></author>
      <pages>578-593</pages>
      <abstract>The Penn-Helsinki Parsed Corpus of Early Modern English (PPCEME), a 1.7-million-word treebank that is an important resource for research in syntactic change, has several properties that present potential challenges for NLP technologies. We describe these key features of PPCEME that make it challenging for parsing, including a larger and more varied set of function tags than in the Penn Treebank, and present results for this corpus using a modified version of the Berkeley Neural Parser and the approach to function tag recovery of Gabbard et al. (2006). While this approach to function tag recovery gives reasonable results, it is in some ways inappropriate for span-based parsers. We also present further evidence of the importance of in-domain pretraining for contextualized word representations. The resulting parser will be used to parse Early English Books Online, a 1.5 billion word corpus whose utility for the study of syntactic change will be greatly increased with the addition of accurate parse trees.</abstract>
      <url hash="3131ccc8">2022.findings-naacl.44</url>
      <bibkey>kulick-etal-2022-penn</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.44</doi>
      <video href="2022.findings-naacl.44.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="45">
      <title>Instilling Type Knowledge in Language Models via Multi-Task <fixed-case>QA</fixed-case></title>
      <author><first>Shuyang</first><last>Li</last></author>
      <author><first>Mukund</first><last>Sridhar</last></author>
      <author><first>Chandana</first><last>Satya Prakash</last></author>
      <author><first>Jin</first><last>Cao</last></author>
      <author><first>Wael</first><last>Hamza</last></author>
      <author><first>Julian</first><last>McAuley</last></author>
      <pages>594-603</pages>
      <abstract>Understanding human language often necessitates understanding entities and their place in a taxonomy of knowledge—their <i>types</i>.Previous methods to learn entity types rely on training classifiers on datasets with coarse, noisy, and incomplete labels. We introduce a method to instill fine-grained type knowledge in language models with text-to-text pre-training on type-centric questions leveraging knowledge base documents and knowledge graphs.We create the <b>WikiWiki</b> dataset: entities and passages from 10M Wikipedia articles linked to the Wikidata knowledge graph with 41K types.Models trained on WikiWiki achieve state-of-the-art performance in zero-shot dialog state tracking benchmarks, accurately infer entity types in Wikipedia articles, and can discover new types deemed useful by human judges.</abstract>
      <url hash="5f3b0ed8">2022.findings-naacl.45</url>
      <bibkey>li-etal-2022-instilling</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.45</doi>
      <video href="2022.findings-naacl.45.mp4"/>
      <pwccode url="https://github.com/amazon-research/wikiwiki-dataset" additional="false">amazon-research/wikiwiki-dataset</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikiwiki">WikiWiki</pwcdataset>
    </paper>
    <paper id="46">
      <title><fixed-case>S</fixed-case>t<fixed-case>ATIK</fixed-case>: Structure and Text for Inductive Knowledge Graph Completion</title>
      <author><first>Elan</first><last>Markowitz</last></author>
      <author><first>Keshav</first><last>Balasubramanian</last></author>
      <author><first>Mehrnoosh</first><last>Mirtaheri</last></author>
      <author><first>Murali</first><last>Annavaram</last></author>
      <author><first>Aram</first><last>Galstyan</last></author>
      <author><first>Greg</first><last>Ver Steeg</last></author>
      <pages>604-615</pages>
      <abstract>Knowledge graphs (KGs) often represent knowledge bases that are incomplete. Machine learning models can alleviate this by helping automate graph completion. Recently, there has been growing interest in completing knowledge bases that are dynamic, where previously unseen entities may be added to the KG with many missing links. In this paper, we present <b>StATIK</b>–<b>St</b>ructure <b>A</b>nd <b>T</b>ext for <b>I</b>nductive <b>K</b>nowledge Completion. StATIK uses Language Models to extract the semantic information from text descriptions, while using Message Passing Neural Networks to capture the structural information. StATIK achieves state of the art results on three challenging inductive baselines. We further analyze our hybrid model through detailed ablation studies.</abstract>
      <url hash="ddbffff2">2022.findings-naacl.46</url>
      <attachment type="software" hash="6a5ae84f">2022.findings-naacl.46.software.zip</attachment>
      <bibkey>markowitz-etal-2022-statik</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.46</doi>
      <video href="2022.findings-naacl.46.mp4"/>
      <pwccode url="https://github.com/elanmarkowitz/statik" additional="false">elanmarkowitz/statik</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikidata5m">Wikidata5M</pwcdataset>
    </paper>
    <paper id="47">
      <title><fixed-case>C</fixed-case>o<fixed-case>C</fixed-case>o<fixed-case>A</fixed-case>-<fixed-case>MT</fixed-case>: A Dataset and Benchmark for Contrastive Controlled <fixed-case>MT</fixed-case> with Application to Formality</title>
      <author><first>Maria</first><last>Nadejde</last></author>
      <author><first>Anna</first><last>Currey</last></author>
      <author><first>Benjamin</first><last>Hsu</last></author>
      <author><first>Xing</first><last>Niu</last></author>
      <author><first>Marcello</first><last>Federico</last></author>
      <author><first>Georgiana</first><last>Dinu</last></author>
      <pages>616-632</pages>
      <abstract>The machine translation (MT) task is typically formulated as that of returning a single translation for an input segment. However, in many cases, multiple different translations are valid and the appropriate translation may depend on the intended target audience, characteristics of the speaker, or even the relationship between speakers. Specific problems arise when dealing with honorifics, particularly translating from English into languages with formality markers. For example, the sentence “Are you sure?” can be translated in German as “Sind Sie sich sicher?” (formal register) or “Bist du dir sicher?” (informal). Using wrong or inconsistent tone may be perceived as inappropriate or jarring for users of certain cultures and demographics. This work addresses the problem of learning to control target language attributes, in this case formality, from a small amount of labeled contrastive data. We introduce an annotated dataset (CoCoA-MT) and an associated evaluation metric for training and evaluating formality-controlled MT models for six diverse target languages. We show that we can train formality-controlled models by fine-tuning on labeled contrastive data, achieving high accuracy (82% in-domain and 73% out-of-domain) while maintaining overall quality.</abstract>
      <url hash="769bc50a">2022.findings-naacl.47</url>
      <bibkey>nadejde-etal-2022-cocoa</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.47</doi>
      <video href="2022.findings-naacl.47.mp4"/>
      <pwccode url="https://github.com/awslabs/sockeye" additional="true">awslabs/sockeye</pwccode>
    </paper>
    <paper id="48">
      <title><fixed-case>CLEAR</fixed-case>: Improving Vision-Language Navigation with Cross-Lingual, Environment-Agnostic Representations</title>
      <author><first>Jialu</first><last>Li</last></author>
      <author><first>Hao</first><last>Tan</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>633-649</pages>
      <abstract>Vision-and-Language Navigation (VLN) tasks require an agent to navigate through the environment based on language instructions. In this paper, we aim to solve two key challenges in this task: utilizing multilingual instructions for improved instruction-path grounding and navigating through new environments that are unseen during training. To address these challenges, first, our agent learns a shared and visually-aligned cross-lingual language representation for the three languages (English, Hindi and Telugu) in the Room-Across-Room dataset. Our language representation learning is guided by text pairs that are aligned by visual information. Second, our agent learns an environment-agnostic visual representation by maximizing the similarity between semantically-aligned image pairs (with constraints on object-matching) from different environments. Our environment agnostic visual representation can mitigate the environment bias induced by low-level visual information. Empirically, on the Room-Across-Room dataset, we show that our multi-lingual agent gets large improvements in all metrics over the strong baseline model when generalizing to unseen environments with the cross-lingual language representation and the environment-agnostic visual representation. Furthermore, we show that our learned language and visual representations can be successfully transferred to the Room-to-Room and Cooperative Vision-and-Dialogue Navigation task, and present detailed qualitative and quantitative generalization and grounding analysis.</abstract>
      <url hash="fb750b0d">2022.findings-naacl.48</url>
      <attachment type="software" hash="72db7f74">2022.findings-naacl.48.software.zip</attachment>
      <bibkey>li-etal-2022-clear</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.48</doi>
      <video href="2022.findings-naacl.48.mp4"/>
      <pwccode url="https://github.com/jialuli-luka/clear" additional="false">jialuli-luka/clear</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/rxr">RxR</pwcdataset>
    </paper>
    <paper id="49">
      <title>Language Models for Code-switch Detection of te reo <fixed-case>M</fixed-case>āori and <fixed-case>E</fixed-case>nglish in a Low-resource Setting</title>
      <author><first>Jesin</first><last>James</last></author>
      <author><first>Vithya</first><last>Yogarajan</last></author>
      <author><first>Isabella</first><last>Shields</last></author>
      <author><first>Catherine</first><last>Watson</last></author>
      <author><first>Peter</first><last>Keegan</last></author>
      <author><first>Keoni</first><last>Mahelona</last></author>
      <author><first>Peter-Lucas</first><last>Jones</last></author>
      <pages>650-660</pages>
      <abstract>Te reo Māori, New Zealand’s only indigenous language, is code-switched with English. Māori speakers are atleast bilingual, and the use of Māori is increasing in New Zealand English. Unfortunately, due to the minimal availability of resources, including digital data, Māori is under-represented in technological advances. Cloud-based multilingual systems such as Google and Microsoft Azure support Māori language detection. However, we provide experimental evidence to show that the accuracy of such systems is low when detecting Māori. Hence, with the support of Māori community, we collect Māori and bilingual data to use natural language processing (NLP) to improve Māori language detection. We train bilingual sub-word embeddings and provide evidence to show that our bilingual embeddings improve overall accuracy compared to the publicly-available monolingual embeddings. This improvement has been verified for various NLP tasks using three bilingual databases containing formal transcripts and informal social media data. We also show that BiLSTM with pre-trained Māori-English sub-word embeddings outperforms large-scale contextual language models such as BERT on down streaming tasks of detecting Māori language. However, this research uses large models ‘as is’ for transfer learning, where no further training was done on Māori-English data. The best accuracy of 87% was obtained using BiLSTM with bilingual embeddings to detect Māori-English code-switching points.</abstract>
      <url hash="701b1e45">2022.findings-naacl.49</url>
      <bibkey>james-etal-2022-language</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.49</doi>
      <video href="2022.findings-naacl.49.mp4"/>
    </paper>
    <paper id="50">
      <title>Opponent Modeling in Negotiation Dialogues by Related Data Adaptation</title>
      <author><first>Kushal</first><last>Chawla</last></author>
      <author><first>Gale</first><last>Lucas</last></author>
      <author><first>Jonathan</first><last>May</last></author>
      <author><first>Jonathan</first><last>Gratch</last></author>
      <pages>661-674</pages>
      <abstract>Opponent modeling is the task of inferring another party’s mental state within the context of social interactions. In a multi-issue negotiation, it involves inferring the relative importance that the opponent assigns to each issue under discussion, which is crucial for finding high-value deals. A practical model for this task needs to infer these priorities of the opponent on the fly based on partial dialogues as input, without needing additional annotations for training. In this work, we propose a ranker for identifying these priorities from negotiation dialogues. The model takes in a partial dialogue as input and predicts the priority order of the opponent. We further devise ways to adapt related data sources for this task to provide more explicit supervision for incorporating the opponent’s preferences and offers, as a proxy to relying on granular utterance-level annotations. We show the utility of our proposed approach through extensive experiments based on two dialogue datasets. We find that the proposed data adaptations lead to strong performance in zero-shot and few-shot scenarios. Moreover, they allow the model to perform better than baselines while accessing fewer utterances from the opponent. We release our code to support future work in this direction.</abstract>
      <url hash="84b0b3a6">2022.findings-naacl.50</url>
      <bibkey>chawla-etal-2022-opponent</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.50</doi>
      <video href="2022.findings-naacl.50.mp4"/>
      <pwccode url="https://github.com/kushalchawla/opponent-modeling" additional="false">kushalchawla/opponent-modeling</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/casino">CaSiNo</pwcdataset>
    </paper>
    <paper id="51">
      <title><fixed-case>LMT</fixed-case>urk: Few-Shot Learners as Crowdsourcing Workers in a Language-Model-as-a-Service Framework</title>
      <author><first>Mengjie</first><last>Zhao</last></author>
      <author><first>Fei</first><last>Mi</last></author>
      <author><first>Yasheng</first><last>Wang</last></author>
      <author><first>Minglei</first><last>Li</last></author>
      <author><first>Xin</first><last>Jiang</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <author><first>Hinrich</first><last>Schuetze</last></author>
      <pages>675-692</pages>
      <abstract>Vast efforts have been devoted to creating high-performance few-shot learners, i.e., large-scale pretrained language models (PLMs) that perform well with little downstream task training data. Training PLMs has incurred significant cost, but utilizing the few-shot learners is still challenging due to their enormous size. This work focuses on a crucial question: How to make effective use of these few-shot learners? We propose LMTurk, a novel approach that treats few-shotlearners as crowdsourcing workers. The rationale is that crowdsourcing workers are in fact few-shot learners: They are shown a few illustrative examples to learn about a task and then start annotating. LMTurk employs few-shot learners built upon PLMs as workers. We show that the resulting annotations can be utilized to train models that solve the task well and are small enough to be deployable in practical scenarios. Active learning is integrated into LMTurk to reduce the amount of queries made to PLMs, minimizing the computational cost of running PLM inference passes. Altogether, LMTurk is an important step towards making effective use of current PLMs.</abstract>
      <url hash="b95005d7">2022.findings-naacl.51</url>
      <bibkey>zhao-etal-2022-lmturk</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.51</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst-2">SST-2</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst-5">SST-5</pwcdataset>
    </paper>
    <paper id="52">
      <title>Entity Cloze By Date: What <fixed-case>LM</fixed-case>s Know About Unseen Entities</title>
      <author><first>Yasumasa</first><last>Onoe</last></author>
      <author><first>Michael</first><last>Zhang</last></author>
      <author><first>Eunsol</first><last>Choi</last></author>
      <author><first>Greg</first><last>Durrett</last></author>
      <pages>693-702</pages>
      <abstract>Language models (LMs) are typically trained once on a large-scale corpus and used for years without being updated. However, in a dynamic world, new entities constantly arise. We propose a framework to analyze what LMs can infer about new entities that did not exist when the LMs were pretrained. We derive a dataset of entities indexed by their origination date and paired with their English Wikipedia articles, from which we can find sentences about each entity. We evaluate LMs’ perplexity on masked spans within these sentences. We show that models more informed about the entities, such as those with access to a textual definition of them, achieve lower perplexity on this benchmark. Our experimental results demonstrate that making inferences about new entities remains difficult for LMs. Given its wide coverage on entity knowledge and temporal indexing, our dataset can be used to evaluate LMs and techniques designed to modify or extend their knowledge. Our automatic data collection pipeline can be easily used to continually update our benchmark.</abstract>
      <url hash="884e9ac2">2022.findings-naacl.52</url>
      <bibkey>onoe-etal-2022-entity</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.52</doi>
      <video href="2022.findings-naacl.52.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/lama">LAMA</pwcdataset>
    </paper>
    <paper id="53">
      <title>Data Augmentation for Low-Resource Dialogue Summarization</title>
      <author><first>Yongtai</first><last>Liu</last></author>
      <author><first>Joshua</first><last>Maynez</last></author>
      <author><first>Gonçalo</first><last>Simões</last></author>
      <author><first>Shashi</first><last>Narayan</last></author>
      <pages>703-710</pages>
      <abstract>We present DADS, a novel Data Augmentation technique for low-resource Dialogue Summarization. Our method generates synthetic examples by replacing sections of text from both the input dialogue and summary while preserving the augmented summary to correspond to a viable summary for the augmented dialogue. We utilize pretrained language models that produce highly likely dialogue alternatives while still being free to generate diverse alternatives. We applied our data augmentation method to the SAMSum dataset in low resource scenarios, mimicking real world problems such as chat, thread, and meeting summarization where large scale supervised datasets with human-written summaries are scarce. Through both automatic and human evaluations, we show that DADS shows strong improvements for low resource scenarios while generating topically diverse summaries without introducing additional hallucinations to the summaries.</abstract>
      <url hash="01612648">2022.findings-naacl.53</url>
      <bibkey>liu-etal-2022-data</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.53</doi>
      <video href="2022.findings-naacl.53.mp4"/>
    </paper>
    <paper id="54">
      <title>A Versatile Adaptive Curriculum Learning Framework for Task-oriented Dialogue Policy Learning</title>
      <author><first>Yang</first><last>Zhao</last></author>
      <author><first>Hua</first><last>Qin</last></author>
      <author><first>Wang</first><last>Zhenyu</last></author>
      <author><first>Changxi</first><last>Zhu</last></author>
      <author><first>Shihan</first><last>Wang</last></author>
      <pages>711-723</pages>
      <abstract>Training a deep reinforcement learning-based dialogue policy with brute-force random sampling is costly. A new training paradigm was proposed to improve learning performance and efficiency by combining curriculum learning. However, attempts in the field of dialogue policy are very limited due to the lack of reliable evaluation of difficulty scores of dialogue tasks and the high sensitivity to the mode of progression through dialogue tasks. In this paper, we present a novel versatile adaptive curriculum learning (VACL) framework, which presents a substantial step toward applying automatic curriculum learning on dialogue policy tasks. It supports evaluating the difficulty of dialogue tasks only using the learning experiences of dialogue policy and skip-level selection according to their learning needs to maximize the learning efficiency. Moreover, an attractive feature of VACL is the construction of a generic, elastic global curriculum while training a good dialogue policy that could guide different dialogue policy learning without extra effort on re-training. The superiority and versatility of VACL are validated on three public dialogue datasets.</abstract>
      <url hash="0e8b2449">2022.findings-naacl.54</url>
      <bibkey>zhao-etal-2022-versatile</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.54</doi>
      <video href="2022.findings-naacl.54.mp4"/>
    </paper>
    <paper id="55">
      <title><fixed-case>L</fixed-case>ong<fixed-case>T</fixed-case>5: <fixed-case>E</fixed-case>fficient Text-To-Text Transformer for Long Sequences</title>
      <author><first>Mandy</first><last>Guo</last></author>
      <author><first>Joshua</first><last>Ainslie</last></author>
      <author><first>David</first><last>Uthus</last></author>
      <author><first>Santiago</first><last>Ontanon</last></author>
      <author><first>Jianmo</first><last>Ni</last></author>
      <author><first>Yun-Hsuan</first><last>Sung</last></author>
      <author><first>Yinfei</first><last>Yang</last></author>
      <pages>724-736</pages>
      <abstract>Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the performance of Transformer-based neural models. In this paper, we present LongT5, a new model that explores the effects of scaling both the input length and model size at the same time. Specifically, we integrate attention ideas from long-input transformers (ETC), and adopt pre-training strategies from summarization pre-training (PEGASUS) into the scalable T5 architecture. The result is a new attention mechanism we call Transient Global (TGlobal), which mimics ETC’s local/global attention mechanism, but without requiring additional side-inputs. We are able to achieve state-of-the-art results on several summarization and question answering tasks, as well as outperform the original T5 models on these tasks. We have open sourced our architecture and training code, as well as our pre-trained model checkpoints.</abstract>
      <url hash="2f435c2b">2022.findings-naacl.55</url>
      <bibkey>guo-etal-2022-longt5</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.55</doi>
      <video href="2022.findings-naacl.55.mp4"/>
      <pwccode url="https://github.com/google-research/longt5" additional="false">google-research/longt5</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/arxiv">Arxiv HEP-TH citation graph</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/bigpatent">BigPatent</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multi-news">Multi-News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/pubmed">Pubmed</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/scrolls">SCROLLS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/arxiv-summarization-dataset">arXiv Summarization Dataset</pwcdataset>
    </paper>
    <paper id="56">
      <title>Challenging <fixed-case>A</fixed-case>merica: Modeling language in longer time scales</title>
      <author><first>Jakub</first><last>Pokrywka</last></author>
      <author><first>Filip</first><last>Graliński</last></author>
      <author><first>Krzysztof</first><last>Jassem</last></author>
      <author><first>Karol</first><last>Kaczmarek</last></author>
      <author><first>Krzysztof</first><last>Jurkiewicz</last></author>
      <author><first>Piotr</first><last>Wierzchon</last></author>
      <pages>737-749</pages>
      <abstract>The aim of the paper is to apply, for historical texts, the methodology used commonly to solve various NLP tasks defined for contemporary data, i.e. pre-train and fine-tune large Transformer models. This paper introduces an ML challenge, named Challenging America (ChallAm), based on OCR-ed excerpts from historical newspapers collected from the Chronicling America portal. ChallAm provides a dataset of clippings, labeled with metadata on their origin, and paired with their textual contents retrieved by an OCR tool. Three, publicly available, ML tasks are defined in the challenge: to determine the article date, to detect the location of the issue, and to deduce a word in a text gap (cloze test). Strong baselines are provided for all three ChallAm tasks. In particular, we pre-trained a RoBERTa model from scratch from the historical texts. We also discuss the issues of discrimination and hate-speech present in the historical American texts.</abstract>
      <url hash="5731b1a8">2022.findings-naacl.56</url>
      <bibkey>pokrywka-etal-2022-challenging</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.56</doi>
      <video href="2022.findings-naacl.56.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="57">
      <title><fixed-case>LM</fixed-case>-<fixed-case>CORE</fixed-case>: Language Models with Contextually Relevant External Knowledge</title>
      <author><first>Jivat</first><last>Kaur</last></author>
      <author><first>Sumit</first><last>Bhatia</last></author>
      <author><first>Milan</first><last>Aggarwal</last></author>
      <author><first>Rachit</first><last>Bansal</last></author>
      <author><first>Balaji</first><last>Krishnamurthy</last></author>
      <pages>750-769</pages>
      <abstract>Large transformer-based pre-trained language models have achieved impressive performance on a variety of knowledge-intensive tasks and can capture factual knowledge in their parameters. We argue that storing large amounts of knowledge in the model parameters is sub-optimal given the ever-growing amounts of knowledge and resource requirements. We posit that a more efficient alternative is to provide explicit access to contextually relevant structured knowledge to the model and train it to use that knowledge. We present LM-CORE – a general framework to achieve this– that allows <i>decoupling</i> of the language model training from the external knowledge source and allows the latter to be updated without affecting the already trained model. Experimental results show that LM-CORE, having access to external knowledge, achieves significant and robust outperformance over state-of-the-art knowledge-enhanced language models on knowledge probing tasks; can effectively handle knowledge updates; and performs well on two downstream tasks. We also present a thorough error analysis highlighting the successes and failures of LM-CORE. Our code and model checkpoints are publicly available.</abstract>
      <url hash="d38924e8">2022.findings-naacl.57</url>
      <bibkey>kaur-etal-2022-lm</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.57</doi>
      <video href="2022.findings-naacl.57.mp4"/>
      <pwccode url="https://github.com/sumit-research/lmcore" additional="false">sumit-research/lmcore</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/lama">LAMA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webquestions">WebQuestions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikidata5m">Wikidata5M</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/yago">YAGO</pwcdataset>
    </paper>
    <paper id="58">
      <title>A Generative Language Model for Few-shot Aspect-Based Sentiment Analysis</title>
      <author><first>Ehsan</first><last>Hosseini-Asl</last></author>
      <author><first>Wenhao</first><last>Liu</last></author>
      <author><first>Caiming</first><last>Xiong</last></author>
      <pages>770-787</pages>
      <abstract>Sentiment analysis is an important task in natural language processing. In recent works, pre-trained language models are often used to achieve state-of-the-art results, especially when training data is scarce. It is common to fine-tune on the downstream task, usually by adding task-specific layers on top of the model. In this paper, we focus on aspect-based sentiment analysis, which involves extracting aspect term, category, and predicting their corresponding polarities. In particular, we are interested in few-shot settings. We propose to reformulate the extraction and prediction tasks into the sequence generation task, using a generative language model with unidirectional attention (GPT2 is used unless stated otherwise). This way, the model learns to accomplish the tasks via language generation without the need of training task-specific layers. Our evaluation results on the single-task polarity prediction show that our approach outperforms the previous state-of-the-art (based on BERT) on average performance by a large margins in few-shot and full-shot settings. More importantly, our generative approach significantly reduces the model variance caused by low-resource data. We further demonstrate that the proposed generative language model can handle joint and multi-task settings, unlike previous work. We observe that the proposed sequence generation method achieves further improved performances on polarity prediction when the model is trained via joint and multi-task settings. Further evaluation on similar sentiment analysis datasets, SST-2, SST-5 and OOS intent detection validates the superiority and noise robustness of generative language model in few-shot settings.</abstract>
      <url hash="64774850">2022.findings-naacl.58</url>
      <bibkey>hosseini-asl-etal-2022-generative</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.58</doi>
      <video href="2022.findings-naacl.58.mp4"/>
      <pwccode url="https://github.com/salesforce/fewshot_absa" additional="false">salesforce/fewshot_absa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst-2">SST-2</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst-5">SST-5</pwcdataset>
    </paper>
    <paper id="59">
      <title>Permutation Invariant Strategy Using Transformer Encoders for Table Understanding</title>
      <author><first>Sarthak</first><last>Dash</last></author>
      <author><first>Sugato</first><last>Bagchi</last></author>
      <author><first>Nandana</first><last>Mihindukulasooriya</last></author>
      <author><first>Alfio</first><last>Gliozzo</last></author>
      <pages>788-800</pages>
      <abstract>Representing text in tables is essential for many business intelligence tasks such as semantic retrieval, data exploration and visualization, and question answering. Existing methods that leverage pretrained Transformer encoders range from a simple construction of pseudo-sentences by concatenating text across rows or columns to complex parameter-intensive models that encode table structure and require additional pretraining. In this work, we introduce a novel encoding strategy for Transformer encoders that preserves the critical property of permutation invariance across rows or columns. Unlike existing state-of-the-art methods for Table Understanding, our proposed approach does not require any additional pretraining and still substantially outperforms existing methods in almost all instances. We demonstrate the effectiveness of our proposed approach on three table interpretation tasks: column type annotation, relation extraction, and entity linking through extensive experiments on existing tabular datasets.</abstract>
      <url hash="2aa7f298">2022.findings-naacl.59</url>
      <bibkey>dash-etal-2022-permutation</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.59</doi>
      <video href="2022.findings-naacl.59.mp4"/>
    </paper>
    <paper id="60">
      <title><fixed-case>M</fixed-case>ulti<fixed-case>NERD</fixed-case>: A Multilingual, Multi-Genre and Fine-Grained Dataset for Named Entity Recognition (and Disambiguation)</title>
      <author><first>Simone</first><last>Tedeschi</last></author>
      <author><first>Roberto</first><last>Navigli</last></author>
      <pages>801-812</pages>
      <abstract>Named Entity Recognition (NER) is the task of identifying named entities in texts and classifying them through specific semantic categories, a process which is crucial for a wide range of NLP applications. Current datasets for NER focus mainly on coarse-grained entity types, tend to consider a single textual genre and to cover a narrow set of languages, thus limiting the general applicability of NER systems. In this work, we design a new methodology for automatically producing NER annotations, and address the aforementioned limitations by introducing a novel dataset that covers 10 languages, 15 NER categories and 2 textual genres. We also introduce a manually-annotated test set, and extensively evaluate the quality of our novel dataset on both this new test set and standard benchmarks for NER.In addition, in our dataset, we include: i) disambiguation information to enable the development of multilingual entity linking systems, and ii) image URLs to encourage the creation of multimodal systems. We release our dataset at <url>https://github.com/Babelscape/multinerd</url>.</abstract>
      <url hash="ac0cda9b">2022.findings-naacl.60</url>
      <bibkey>tedeschi-navigli-2022-multinerd</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.60</doi>
      <video href="2022.findings-naacl.60.mp4"/>
      <pwccode url="https://github.com/babelscape/multinerd" additional="false">babelscape/multinerd</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2002">CoNLL 2002</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikiann-1">WikiANN</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikineural">WikiNEuRal</pwcdataset>
    </paper>
    <paper id="61">
      <title>Learning to Embed Multi-Modal Contexts for Situated Conversational Agents</title>
      <author><first>Haeju</first><last>Lee</last></author>
      <author><first>Oh Joon</first><last>Kwon</last></author>
      <author><first>Yunseon</first><last>Choi</last></author>
      <author><first>Minho</first><last>Park</last></author>
      <author><first>Ran</first><last>Han</last></author>
      <author><first>Yoonhyung</first><last>Kim</last></author>
      <author><first>Jinhyeon</first><last>Kim</last></author>
      <author><first>Youngjune</first><last>Lee</last></author>
      <author><first>Haebin</first><last>Shin</last></author>
      <author><first>Kangwook</first><last>Lee</last></author>
      <author><first>Kee-Eung</first><last>Kim</last></author>
      <pages>813-830</pages>
      <abstract>The Situated Interactive Multi-Modal Conversations (SIMMC) 2.0 aims to create virtual shopping assistants that can accept complex multi-modal inputs, i.e. visual appearances of objects and user utterances. It consists of four subtasks, multi-modal disambiguation (MM-Disamb), multi-modal coreference resolution (MM-Coref), multi-modal dialog state tracking (MM-DST), and response retrieval and generation. While many task-oriented dialog systems usually tackle each subtask separately, we propose a jointly learned multi-modal encoder-decoder that incorporates visual inputs and performs all four subtasks at once for efficiency. This approach won the MM-Coref and response retrieval subtasks and nominated runner-up for the remaining subtasks using a single unified model at the 10th Dialog Systems Technology Challenge (DSTC10), setting a high bar for the novel task of multi-modal task-oriented dialog systems.</abstract>
      <url hash="221693ec">2022.findings-naacl.61</url>
      <bibkey>lee-etal-2022-learning</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.61</doi>
      <video href="2022.findings-naacl.61.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/simmc">SIMMC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/simmc2-0">SIMMC2.0</pwcdataset>
    </paper>
    <paper id="62">
      <title>Measuring and Improving Compositional Generalization in Text-to-<fixed-case>SQL</fixed-case> via Component Alignment</title>
      <author><first>Yujian</first><last>Gan</last></author>
      <author><first>Xinyun</first><last>Chen</last></author>
      <author><first>Qiuping</first><last>Huang</last></author>
      <author><first>Matthew</first><last>Purver</last></author>
      <pages>831-843</pages>
      <abstract>In text-to-SQL tasks — as in much of NLP — <i>compositional generalization</i> is a major challenge: neural networks struggle with compositional generalization where training and test distributions differ. However, most recent attempts to improve this are based on word-level synthetic data or specific dataset splits to generate compositional biases. In this work, we propose a clause-level compositional example generation method. We first split the sentences in the Spider text-to-SQL dataset into sub-sentences, annotating each sub-sentence with its corresponding SQL clause, resulting in a new dataset Spider-SS. We then construct a further dataset, Spider-CG, by composing Spider-SS sub-sentences in different combinations, to test the ability of models to generalize compositionally. Experiments show that existing models suffer significant performance degradation when evaluated on Spider-CG, even though every sub-sentence is seen during training. To deal with this problem, we modify a number of state-of-the-art models to train on the segmented data of Spider-SS, and we show that this method improves the generalization performance.</abstract>
      <url hash="44e5d057">2022.findings-naacl.62</url>
      <attachment type="software" hash="0683d9fa">2022.findings-naacl.62.software.zip</attachment>
      <bibkey>gan-etal-2022-measuring</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.62</doi>
      <video href="2022.findings-naacl.62.mp4"/>
      <pwccode url="https://github.com/ygan/spiderss-spidercg" additional="false">ygan/spiderss-spidercg</pwccode>
    </paper>
    <paper id="63">
      <title>Empathetic Persuasion: Reinforcing Empathy and Persuasiveness in Dialogue Systems</title>
      <author><first>Azlaan Mustafa</first><last>Samad</last></author>
      <author><first>Kshitij</first><last>Mishra</last></author>
      <author><first>Mauajama</first><last>Firdaus</last></author>
      <author><first>Asif</first><last>Ekbal</last></author>
      <pages>844-856</pages>
      <abstract>Persuasion is an intricate process involving empathetic connection between two individuals. Plain persuasive responses may make a conversation non-engaging. Even the most well-intended and reasoned persuasive conversations can fall through in the absence of empathetic connection between the speaker and listener. In this paper, we propose a novel task of incorporating empathy when generating persuasive responses. We develop an empathetic persuasive dialogue system by fine-tuning a maximum likelihood Estimation (MLE)-based language model in a reinforcement learning (RL) framework. To design feedbacks for our RL-agent, we define an effective and efficient reward function considering consistency, repetitiveness, emotion and persuasion rewards to ensure consistency, non-repetitiveness, empathy and persuasiveness in the generated responses. Due to lack of emotion annotated persuasive data, we first annotate the existing Persuaion For Good dataset with emotions, then build transformer based classifiers to provide emotion based feedbacks to our RL agent. Experimental results confirm that our proposed model increases the rate of generating persuasive responses as compared to the available state-of-the-art dialogue models while making the dialogues empathetically more engaging and retaining the language quality in responses.</abstract>
      <url hash="8f790820">2022.findings-naacl.63</url>
      <attachment type="software" hash="af6ca1c9">2022.findings-naacl.63.software.zip</attachment>
      <bibkey>samad-etal-2022-empathetic</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.63</doi>
      <video href="2022.findings-naacl.63.mp4"/>
    </paper>
    <paper id="64">
      <title>Attention Fusion: a light yet efficient late fusion mechanism for task adaptation in <fixed-case>NLU</fixed-case></title>
      <author><first>Jin</first><last>Cao</last></author>
      <author><first>Chandana</first><last>Satya Prakash</last></author>
      <author><first>Wael</first><last>Hamza</last></author>
      <pages>857-866</pages>
      <abstract>Fine-tuning a pre-trained language model using annotated data has become the de-facto standard for adapting general-purpose pre-trained models like BERT to downstream tasks. However, given the trend of larger pre-trained models, fine-tuning these models for each downstream task is parameter-inefficient and computationally-expensive deeming this approach sub-optimal for adoption by NLU systems. In recent years, various approaches have been proposed for parameter efficient task adaptation such as Adaptor, Bitfit, Prompt tuning, Prefix tuning etc. However, most of these efforts propose to insert task specific parameters in-between or inside intermediate layers of the pre-trained encoder resulting in higher computational cost due to back-propagation of errors to all layers. To mitigate this issue, we propose a light but efficient, attention based fusion module which computes task-attuned token representations by aggregating intermediate layer representations from a pre-trained network. Our proposed fusion module trains only 0.0009% of total parameters and achieves competitive performance to the standard fine-tuning approach on various tasks. It is also decoupled from the pre-trained network making it efficient during computation and scalable during deployment. Last but not the least, we demonstrate that our proposed attention-fusion mechanism can transfer effectively to different languages for further re-use and expansion.</abstract>
      <url hash="abcaf9b4">2022.findings-naacl.64</url>
      <bibkey>cao-etal-2022-attention</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.64</doi>
      <video href="2022.findings-naacl.64.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL 2003</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
    </paper>
    <paper id="65">
      <title>The Limits of Word Level Differential Privacy</title>
      <author><first>Justus</first><last>Mattern</last></author>
      <author><first>Benjamin</first><last>Weggenmann</last></author>
      <author><first>Florian</first><last>Kerschbaum</last></author>
      <pages>867-881</pages>
      <abstract>As the issues of privacy and trust are receiving increasing attention within the research community, various attempts have been made to anonymize textual data. A significant subset of these approaches incorporate differentially private mechanims to perturb word embeddings, thus replacing individual words in a sentence. While these methods represent very important contributions, have various advantages over other techniques and do show anonymization capabilities,they have several shortcomings. In this paper, we investigate these weaknesses and demonstrate significant mathematical constraints diminishing the theoretical privacy guaranteeas well as major practical shortcomings with regard to the protection against deanonymization attacks, the preservation of content of the original sentences as well as the quality of the language output. Finally, we propose a new method for text anonymization based on transformer based language models fine-tuned for paraphrasing that circumvents most of the identified weaknesses and also offers a formal privacy guarantee. We evaluate the performance of our method via thourough experimentation and demonstrate superior performance over the discussed mechanisms.</abstract>
      <url hash="1bf19c6b">2022.findings-naacl.65</url>
      <attachment type="software" hash="30819f7f">2022.findings-naacl.65.software.zip</attachment>
      <bibkey>mattern-etal-2022-limits</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.65</doi>
      <video href="2022.findings-naacl.65.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="66">
      <title>Efficient Learning of Multiple <fixed-case>NLP</fixed-case> Tasks via Collective Weight Factorization on <fixed-case>BERT</fixed-case></title>
      <author><first>Christos</first><last>Papadopoulos</last></author>
      <author><first>Yannis</first><last>Panagakis</last></author>
      <author><first>Manolis</first><last>Koubarakis</last></author>
      <author><first>Mihalis</first><last>Nicolaou</last></author>
      <pages>882-890</pages>
      <abstract>The Transformer architecture continues to show remarkable performance gains in many Natural Language Processing tasks. However, obtaining such state-of-the-art performance in different tasks requires fine-tuning the same model separately for each task. Clearly, such an approach is demanding in terms of both memory requirements and computing power. In this paper, aiming to improve training efficiency across multiple tasks, we propose to collectively factorize the weighs of the multi-head attention module of a pre-trained Transformer. We test our proposed method on finetuning multiple natural language understanding tasks by employing BERT-Large as an instantiation of the Transformer and the GLUE as the evaluation benchmark. Experimental results show that our method requires training and storing only 1% of the initial model parameters for each task and matches or improves the original fine-tuned model’s performance for each task while effectively decreasing the parameter requirements by two orders of magnitude. Furthermore, compared to well-known adapter-based alternatives on the GLUE benchmark, our method consistently reaches the same levels of performance while requiring approximately four times fewer total and trainable parameters per task.</abstract>
      <url hash="e2064bc6">2022.findings-naacl.66</url>
      <bibkey>papadopoulos-etal-2022-efficient</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.66</doi>
      <video href="2022.findings-naacl.66.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst-2">SST-2</pwcdataset>
    </paper>
    <paper id="67">
      <title>Learning Rich Representation of Keyphrases from Text</title>
      <author><first>Mayank</first><last>Kulkarni</last></author>
      <author><first>Debanjan</first><last>Mahata</last></author>
      <author><first>Ravneet</first><last>Arora</last></author>
      <author><first>Rajarshi</first><last>Bhowmik</last></author>
      <pages>891-906</pages>
      <abstract>In this work, we explore how to train task-specific language models aimed towards learning rich representation of keyphrases from text documents. We experiment with different masking strategies for pre-training transformer language models (LMs) in discriminative as well as generative settings. In the discriminative setting, we introduce a new pre-training objective - Keyphrase Boundary Infilling with Replacement (KBIR), showing large gains in performance (upto 8.16 points in F1) over SOTA, when the LM pre-trained using KBIR is fine-tuned for the task of keyphrase extraction. In the generative setting, we introduce a new pre-training setup for BART - KeyBART, that reproduces the keyphrases related to the input text in the CatSeq format, instead of the denoised original input. This also led to gains in performance (upto 4.33 points in F1@M) over SOTA for keyphrase generation. Additionally, we also fine-tune the pre-trained language models on named entity recognition (NER), question answering (QA), relation extraction (RE), abstractive summarization and achieve comparable performance with that of the SOTA, showing that learning rich representation of keyphrases is indeed beneficial for many other fundamental NLP tasks.</abstract>
      <url hash="adbb11c4">2022.findings-naacl.67</url>
      <bibkey>kulkarni-etal-2022-learning</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.67</doi>
      <video href="2022.findings-naacl.67.mp4"/>
      <pwccode url="https://github.com/bloomberg/kbir_keybart" additional="false">bloomberg/kbir_keybart</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/inspec">Inspec</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/kp20k">KP20k</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="68">
      <title>Improving Contextual Representation with Gloss Regularized Pre-training</title>
      <author><first>Yu</first><last>Lin</last></author>
      <author><first>Zhecheng</first><last>An</last></author>
      <author><first>Peihao</first><last>Wu</last></author>
      <author><first>Zejun</first><last>Ma</last></author>
      <pages>907-920</pages>
      <abstract>Though achieving impressive results on many NLP tasks, the BERT-like masked language models (MLM) encounter the discrepancy between pre-training and inference. In light of this gap, we investigate the contextual representation of pre-training and inference from the perspective of word probability distribution. We discover that BERT risks neglecting the contextual word similarity in pre-training. To tackle this issue, we propose an auxiliary gloss regularizer module to BERT pre-training (GR-BERT), to enhance word semantic similarity. By predicting masked words and aligning contextual embeddings to corresponding glosses simultaneously, the word similarity can be explicitly modeled. We design two architectures for GR-BERT and evaluate our model in downstream tasks. Experimental results show that the gloss regularizer benefits BERT in word-level and sentence-level semantic representation. The GR-BERT achieves new state-of-the-art in lexical substitution task and greatly promotes BERT sentence representation in both unsupervised and supervised STS tasks.</abstract>
      <url hash="c4b7efef">2022.findings-naacl.68</url>
      <attachment type="software" hash="6bdd87f9">2022.findings-naacl.68.software.zip</attachment>
      <bibkey>lin-etal-2022-improving</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.68</doi>
      <video href="2022.findings-naacl.68.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/sick">SICK</pwcdataset>
    </paper>
    <paper id="69">
      <title>An Information-Theoretic Approach and Dataset for Probing Gender Stereotypes in Multilingual Masked Language Models</title>
      <author><first>Victor</first><last>Steinborn</last></author>
      <author><first>Philipp</first><last>Dufter</last></author>
      <author><first>Haris</first><last>Jabbar</last></author>
      <author><first>Hinrich</first><last>Schuetze</last></author>
      <pages>921-932</pages>
      <abstract>Bias research in NLP is a rapidly growing and developing field. Similar to CrowS-Pairs (Nangia et al., 2020), we assess gender bias in masked-language models (MLMs) by studying pairs of sentences with gender swapped person references. Most bias research focuses on and often is specific to English.Using a novel methodology for creating sentence pairs that is applicable across languages, we create, based on CrowS-Pairs, a multilingual dataset for English, Finnish, German, Indonesian and Thai.Additionally, we propose <tex-math>S_{JSD}</tex-math>, a new bias measure based on Jensen–Shannon divergence, which we argue retains more information from the model output probabilities than other previously proposed bias measures for MLMs.Using multilingual MLMs, we find that <tex-math>S_{JSD}</tex-math> diagnoses the same systematic biased behavior for non-English that previous studies have found for monolingual English pre-trained MLMs. <tex-math>S_{JSD}</tex-math> outperforms the CrowS-Pairs measure, which struggles to find such biases for smaller non-English datasets.</abstract>
      <url hash="f0aabad2">2022.findings-naacl.69</url>
      <attachment type="software" hash="bfebafe5">2022.findings-naacl.69.software.zip</attachment>
      <bibkey>steinborn-etal-2022-information</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.69</doi>
      <video href="2022.findings-naacl.69.mp4"/>
      <pwccode url="https://github.com/vsteinborn/s_jsd-multilingual-bias" additional="false">vsteinborn/s_jsd-multilingual-bias</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/crows-pairs">CrowS-Pairs</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/stereoset">StereoSet</pwcdataset>
    </paper>
    <paper id="70">
      <title>Self-Training with Differentiable Teacher</title>
      <author><first>Simiao</first><last>Zuo</last></author>
      <author><first>Yue</first><last>Yu</last></author>
      <author><first>Chen</first><last>Liang</last></author>
      <author><first>Haoming</first><last>Jiang</last></author>
      <author><first>Siawpeng</first><last>Er</last></author>
      <author><first>Chao</first><last>Zhang</last></author>
      <author><first>Tuo</first><last>Zhao</last></author>
      <author><first>Hongyuan</first><last>Zha</last></author>
      <pages>933-949</pages>
      <abstract>Self-training achieves enormous success in various semi-supervised and weakly-supervised learning tasks. The method can be interpreted as a teacher-student framework, where the teacher generates pseudo-labels, and the student makes predictions. The two models are updated alternatingly. However, such a straightforward alternating update rule leads to training instability. This is because a small change in the teacher may result in a significant change in the student. To address this issue, we propose DRIFT, short for differentiable self-training, that treats teacher-student as a Stackelberg game. In this game, a leader is always in a more advantageous position than a follower. In self-training, the student contributes to the prediction performance, and the teacher controls the training process by generating pseudo-labels. Therefore, we treat the student as the leader and the teacher as the follower. The leader procures its advantage by acknowledging the follower’s strategy, which involves differentiable pseudo-labels and differentiable sample weights. Consequently, the leader-follower interaction can be effectively captured via Stackelberg gradient, obtained by differentiating the follower’s strategy. Experimental results on semi- and weakly-supervised classification and named entity recognition tasks show that our model outperforms existing approaches by large margins.</abstract>
      <url hash="6380c403">2022.findings-naacl.70</url>
      <bibkey>zuo-etal-2022-self</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.70</doi>
      <video href="2022.findings-naacl.70.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/bc5cdr">BC5CDR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="71">
      <title><fixed-case>SHARP</fixed-case>: Search-Based Adversarial Attack for Structured Prediction</title>
      <author><first>Liwen</first><last>Zhang</last></author>
      <author><first>Zixia</first><last>Jia</last></author>
      <author><first>Wenjuan</first><last>Han</last></author>
      <author><first>Zilong</first><last>Zheng</last></author>
      <author><first>Kewei</first><last>Tu</last></author>
      <pages>950-961</pages>
      <abstract>Adversarial attack of structured prediction models faces various challenges such as the difficulty of perturbing discrete words, the sentence quality issue, and the sensitivity of outputs to small perturbations. In this work, we introduce SHARP, a new attack method that formulates the black-box adversarial attack as a search-based optimization problem with a specially designed objective function considering sentence fluency, meaning preservation and attacking effectiveness. Additionally, three different searching strategies are analyzed and compared, i.e., Beam Search, Metropolis-Hastings Sampling, and Hybrid Search. We demonstrate the effectiveness of our attacking strategies on two challenging structured prediction tasks: Pos-tagging and dependency parsing. Through automatic and human evaluations, we show that our method performs a more potent attack compared with pioneer arts. Moreover, the generated adversarial examples can be used to successfully boost the robustness and performance of the victim model via adversarial training.</abstract>
      <url hash="f0fabde3">2022.findings-naacl.71</url>
      <bibkey>zhang-etal-2022-sharp</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.71</doi>
      <video href="2022.findings-naacl.71.mp4"/>
    </paper>
    <paper id="72">
      <title><fixed-case>MM</fixed-case>-Claims: A Dataset for Multimodal Claim Detection in Social Media</title>
      <author><first>Gullal Singh</first><last>Cheema</last></author>
      <author><first>Sherzod</first><last>Hakimov</last></author>
      <author><first>Abdul</first><last>Sittar</last></author>
      <author><first>Eric</first><last>Müller-Budack</last></author>
      <author><first>Christian</first><last>Otto</last></author>
      <author><first>Ralph</first><last>Ewerth</last></author>
      <pages>962-979</pages>
      <abstract>In recent years, the problem of misinformation on the web has become widespread across languages, countries, and various social media platforms. Although there has been much work on automated fake news detection, the role of images and their variety are not well explored. In this paper, we investigate the roles of image and text at an earlier stage of the fake news detection pipeline, called claim detection. For this purpose, we introduce a novel dataset, MM-Claims, which consists of tweets and corresponding images over three topics: COVID-19, Climate Change and broadly Technology. The dataset contains roughly 86000 tweets, out of which 3400 are labeled manually by multiple annotators for the training and evaluation of multimodal models. We describe the dataset in detail, evaluate strong unimodal and multimodal baselines, and analyze the potential and drawbacks of current models.</abstract>
      <url hash="4840126b">2022.findings-naacl.72</url>
      <bibkey>cheema-etal-2022-mm</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.72</doi>
      <video href="2022.findings-naacl.72.mp4"/>
      <pwccode url="https://github.com/tibhannover/mm_claims" additional="false">tibhannover/mm_claims</pwccode>
    </paper>
    <paper id="73">
      <title><fixed-case>QLEVR</fixed-case>: A Diagnostic Dataset for Quantificational Language and Elementary Visual Reasoning</title>
      <author><first>Zechen</first><last>Li</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>980-996</pages>
      <abstract>Synthetic datasets have successfully been used to probe visual question-answering datasets for their reasoning abilities. CLEVR (John- son et al., 2017), for example, tests a range of visual reasoning abilities. The questions in CLEVR focus on comparisons of shapes, colors, and sizes, numerical reasoning, and existence claims. This paper introduces a minimally biased, diagnostic visual question-answering dataset, QLEVR, that goes beyond existential and numerical quantification and focus on more complex quantifiers and their combinations, e.g., asking whether there are more than two red balls that are smaller than at least three blue balls in an image. We describe how the dataset was created and present a first evaluation of state-of-the-art visual question-answering models, showing that QLEVR presents a formidable challenge to our current models. Code and Dataset are available at <url>https://github.com/zechenli03/QLEVR</url></abstract>
      <url hash="6ce0846b">2022.findings-naacl.73</url>
      <bibkey>li-sogaard-2022-qlevr</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.73</doi>
      <pwccode url="https://github.com/zechenli03/qlevr" additional="false">zechenli03/qlevr</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/qlevr">QLEVR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/clevr">CLEVR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/shapes-1">SHAPES</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
    </paper>
    <paper id="74">
      <title><fixed-case>MWP</fixed-case>-<fixed-case>BERT</fixed-case>: Numeracy-Augmented Pre-training for Math Word Problem Solving</title>
      <author><first>Zhenwen</first><last>Liang</last></author>
      <author><first>Jipeng</first><last>Zhang</last></author>
      <author><first>Lei</first><last>Wang</last></author>
      <author><first>Wei</first><last>Qin</last></author>
      <author><first>Yunshi</first><last>Lan</last></author>
      <author><first>Jie</first><last>Shao</last></author>
      <author><first>Xiangliang</first><last>Zhang</last></author>
      <pages>997-1009</pages>
      <abstract>Math word problem (MWP) solving faces a dilemma in number representation learning. In order to avoid the number representation issue and reduce the search space of feasible solutions, existing works striving for MWP solving usually replace real numbers with symbolic placeholders to focus on logic reasoning. However, different from common symbolic reasoning tasks like program synthesis and knowledge graph reasoning, MWP solving has extra requirements in numerical reasoning. In other words, instead of the number value itself, it is the reusable numerical property that matters more in numerical reasoning. Therefore, we argue that injecting numerical properties into symbolic placeholders with contextualized representation learning schema can provide a way out of the dilemma in the number representation issue here. In this work, we introduce this idea to the popular pre-training language model (PLM) techniques and build MWP-BERT, an effective contextual number representation PLM. We demonstrate the effectiveness of our MWP-BERT on MWP solving and several MWP-specific understanding tasks on both English and Chinese benchmarks.</abstract>
      <url hash="72642964">2022.findings-naacl.74</url>
      <bibkey>liang-etal-2022-mwp</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.74</doi>
      <video href="2022.findings-naacl.74.mp4"/>
      <pwccode url="https://github.com/lzhenwen/mwp-bert" additional="false">lzhenwen/mwp-bert</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/math23k">Math23K</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mathqa">MathQA</pwcdataset>
    </paper>
    <paper id="75">
      <title>Restoring <fixed-case>H</fixed-case>ebrew Diacritics Without a Dictionary</title>
      <author><first>Elazar</first><last>Gershuni</last></author>
      <author><first>Yuval</first><last>Pinter</last></author>
      <pages>1010-1018</pages>
      <abstract>We demonstrate that it is feasible to accurately diacritize Hebrew script without any human-curated resources other than plain diacritized text. We present Nakdimon, a two-layer character-level LSTM, that performs on par with much more complicated curation-dependent systems, across a diverse array of modern Hebrew sources. The model is accompanied by a training set and a test set, collected from diverse sources.</abstract>
      <url hash="66e2c56f">2022.findings-naacl.75</url>
      <attachment type="software" hash="1af020f0">2022.findings-naacl.75.software.zip</attachment>
      <bibkey>gershuni-pinter-2022-restoring</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.75</doi>
      <video href="2022.findings-naacl.75.mp4"/>
      <pwccode url="https://github.com/elazarg/nakdimon" additional="false">elazarg/nakdimon</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/nakdimon-test">Nakdimon-test</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/nakdimon-train">Nakdimon-train</pwcdataset>
    </paper>
    <paper id="76">
      <title>Masked Summarization to Generate Factually Inconsistent Summaries for Improved Factual Consistency Checking</title>
      <author><first>Hwanhee</first><last>Lee</last></author>
      <author><first>Kang Min</first><last>Yoo</last></author>
      <author><first>Joonsuk</first><last>Park</last></author>
      <author><first>Hwaran</first><last>Lee</last></author>
      <author><first>Kyomin</first><last>Jung</last></author>
      <pages>1019-1030</pages>
      <abstract>Despite the recent advances in abstractive summarization systems, it is still difficult to determine whether a generated summary is factual consistent with the source text. To this end, the latest approach is to train a factual consistency classifier on factually consistent and inconsistent summaries. Luckily, the former is readily available as reference summaries in existing summarization datasets. However, generating the latter remains a challenge, as they need to be factually inconsistent, yet closely relevant to the source text to be effective. In this paper, we propose to generate factually inconsistent summaries using source texts and reference summaries with key information masked. Experiments on seven benchmark datasets demonstrate that factual consistency classifiers trained on summaries generated using our method generally outperform existing models and show a competitive correlation with human judgments. We also analyze the characteristics of the summaries generated using our method. We will release the pre-trained model and the code at <url>https://github.com/hwanheelee1993/MFMA</url>.</abstract>
      <url hash="f566e457">2022.findings-naacl.76</url>
      <attachment type="software" hash="f1755a9b">2022.findings-naacl.76.software.tgz</attachment>
      <bibkey>lee-etal-2022-masked</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.76</doi>
      <video href="2022.findings-naacl.76.mp4"/>
      <pwccode url="https://github.com/hwanheelee1993/mfma" additional="false">hwanheelee1993/mfma</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/summeval">SummEval</pwcdataset>
    </paper>
    <paper id="77">
      <title>Probing the Role of Positional Information in Vision-Language Models</title>
      <author><first>Philipp J.</first><last>Rösch</last></author>
      <author><first>Jindřich</first><last>Libovický</last></author>
      <pages>1031-1041</pages>
      <abstract>In most Vision-Language models (VL), the understanding of the image structure is enabled by injecting the position information (PI) about objects in the image. In our case study of LXMERT, a state-of-the-art VL model, we probe the use of the PI in the representation and study its effect on Visual Question Answering. We show that the model is not capable of leveraging the PI for the image-text matching task on a challenge set where only position differs. Yet, our experiments with probing confirm that the PI is indeed present in the representation. We introduce two strategies to tackle this: (i) Positional Information Pre-training and (ii) Contrastive Learning on PI using Cross-Modality Matching. Doing so, the model can correctly classify if images with detailed PI statements match. Additionally to the 2D information from bounding boxes, we introduce the object’s depth as new feature for a better object localization in the space. Even though we were able to improve the model properties as defined by our probes, it only has a negligible effect on the downstream performance. Our results thus highlight an important issue of multimodal modeling: the mere presence of information detectable by a probing classifier is not a guarantee that the information is available in a cross-modal setup.</abstract>
      <url hash="216c13a0">2022.findings-naacl.77</url>
      <bibkey>rosch-libovicky-2022-probing</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.77</doi>
      <video href="2022.findings-naacl.77.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/gqa">GQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">MS COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-genome">Visual Genome</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
    </paper>
    <paper id="78">
      <title>”Diversity and Uncertainty in Moderation” are the Key to Data Selection for Multilingual Few-shot Transfer</title>
      <author><first>Shanu</first><last>Kumar</last></author>
      <author><first>Sandipan</first><last>Dandapat</last></author>
      <author><first>Monojit</first><last>Choudhury</last></author>
      <pages>1042-1055</pages>
      <abstract>Few-shot transfer often shows substantial gain over zero-shot transfer (CITATION), which is a practically useful trade-off between fully supervised and unsupervised learning approaches for multilingual pretained model-based systems. This paper explores various strategies for selecting data for annotation that can result in a better few-shot transfer. The proposed approaches rely on multiple measures such as data entropy using <tex-math>n</tex-math>-gram language model, predictive entropy, and gradient embedding. We propose a loss embedding method for sequence labeling tasks, which induces diversity and uncertainty sampling similar to gradient embedding. The proposed data selection strategies are evaluated and compared for POS tagging, NER, and NLI tasks for up to 20 languages. Our experiments show that the gradient and loss embedding-based strategies consistently outperform random data selection baselines, with gains varying with the initial performance of the zero-shot transfer. Furthermore, the proposed method shows similar trends in improvement even when the model is fine-tuned using a lower proportion of the original task-specific labeled training data for zero-shot transfer.</abstract>
      <url hash="e544911a">2022.findings-naacl.78</url>
      <bibkey>kumar-etal-2022-diversity</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.78</doi>
      <video href="2022.findings-naacl.78.mp4"/>
    </paper>
    <paper id="79">
      <title>A Self-supervised Joint Training Framework for Document Reranking</title>
      <author><first>Xiaozhi</first><last>Zhu</last></author>
      <author><first>Tianyong</first><last>Hao</last></author>
      <author><first>Sijie</first><last>Cheng</last></author>
      <author><first>Fu Lee</first><last>Wang</last></author>
      <author><first>Hai</first><last>Liu</last></author>
      <pages>1056-1065</pages>
      <abstract>Pretrained language models such as BERT have been successfully applied to a wide range of natural language processing tasks and also achieved impressive performance in document reranking tasks. Recent works indicate that further pretraining the language models on the task-specific datasets before fine-tuning helps improve reranking performance. However, the pre-training tasks like masked language model and next sentence prediction were based on the context of documents instead of encouraging the model to understand the content of queries in document reranking task. In this paper, we propose a new self-supervised joint training framework (SJTF) with a self-supervised method called Masked Query Prediction (MQP) to establish semantic relations between given queries and positive documents. The framework randomly masks a token of query and encodes the masked query paired with positive documents, and uses a linear layer as a decoder to predict the masked token. In addition, the MQP is used to jointly optimize the models with supervised ranking objective during fine-tuning stage without an extra further pre-training stage. Extensive experiments on the MS MARCO passage ranking and TREC Robust datasets show that models trained with our framework obtain significant improvements compared to original models.</abstract>
      <url hash="6de6c3b6">2022.findings-naacl.79</url>
      <bibkey>zhu-etal-2022-self</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.79</doi>
    </paper>
    <paper id="80">
      <title><fixed-case>CODE</fixed-case>-<fixed-case>MVP</fixed-case>: Learning to Represent Source Code from Multiple Views with Contrastive Pre-Training</title>
      <author><first>Xin</first><last>Wang</last></author>
      <author><first>Yasheng</first><last>Wang</last></author>
      <author><first>Yao</first><last>Wan</last></author>
      <author><first>Jiawei</first><last>Wang</last></author>
      <author><first>Pingyi</first><last>Zhou</last></author>
      <author><first>Li</first><last>Li</last></author>
      <author><first>Hao</first><last>Wu</last></author>
      <author><first>Jin</first><last>Liu</last></author>
      <pages>1066-1077</pages>
      <abstract>Recent years have witnessed increasing interest in code representation learning, which aims to represent the semantics of source code into distributed vectors. Currently, various works have been proposed to represent the complex semantics of source code from different views, including plain text, Abstract Syntax Tree (AST), and several kinds of code graphs (e.g., Control/Data Flow Graph). However, most of them only consider a single view of source code independently, ignoring the correspondences among different views. In this paper, we propose to integrate different views with the natural-language description of source code into a unified framework with Multi-View contrastive Pre-training, and name our model as CODE-MVP. Specifically, we first extract multiple code views using compiler tools, and learn the complementary information among them under a contrastive learning framework. Inspired by the type checking in compilation, we also design a fine-grained type inference objective in the pre-training. Experiments on three downstream tasks over five datasets demonstrate the superiority of CODE-MVP when compared with several state-of-the-art baselines. For example, we achieve 2.4/2.3/1.1 gain in terms of MRR/MAP/Accuracy metrics on natural language code retrieval, code similarity, and code defect detection tasks, respectively.</abstract>
      <url hash="542b7232">2022.findings-naacl.80</url>
      <bibkey>wang-etal-2022-code</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.80</doi>
      <video href="2022.findings-naacl.80.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/conala">CoNaLa</pwcdataset>
    </paper>
    <paper id="81">
      <title><fixed-case>RGL</fixed-case>: A Simple yet Effective Relation Graph Augmented Prompt-based Tuning Approach for Few-Shot Learning</title>
      <author><first>Yaqing</first><last>Wang</last></author>
      <author><first>Xin</first><last>Tian</last></author>
      <author><first>Haoyi</first><last>Xiong</last></author>
      <author><first>Yueyang</first><last>Li</last></author>
      <author><first>Zeyu</first><last>Chen</last></author>
      <author><first>Sheng</first><last>Guo</last></author>
      <author><first>Dejing</first><last>Dou</last></author>
      <pages>1078-1084</pages>
      <abstract>Pre-trained language models (PLMs) can provide a good starting point for downstream applications. However, it is difficult to generalize PLMs to new tasks given a few labeled samples. In this work, we show that Relation Graph augmented Learning (RGL) can improve the performance of few-shot natural language understanding tasks. During learning, RGL constructs a relation graph based on the label consistency between samples in the same batch, and learns to solve the resultant node classification and link prediction problems on the relation graph. In this way, RGL fully exploits the limited supervised information, which can boost the tuning effectiveness. Extensive experimental results show that RGL consistently improves the performance of prompt-based tuning strategies.</abstract>
      <url hash="b9149794">2022.findings-naacl.81</url>
      <bibkey>wang-etal-2022-rgl</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.81</doi>
      <video href="2022.findings-naacl.81.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
    <paper id="82">
      <title>Seeing the wood for the trees: a contrastive regularization method for the low-resource Knowledge Base Question Answering</title>
      <author><first>Jpliu@wtu.edu.cn</first><last>Jpliu@wtu.edu.cn</last></author>
      <author><first>Shijie</first><last>Mei</last></author>
      <author><first>Xinrong</first><last>Hu</last></author>
      <author><first>Xun</first><last>Yao</last></author>
      <author><first>Jack</first><last>Yang</last></author>
      <author><first>Yi</first><last>Guo</last></author>
      <pages>1085-1094</pages>
      <abstract>Given a context knowledge base (KB) and a corresponding question, the Knowledge Base Question Answering task aims to retrieve correct answer entities from this KB. Despite sophisticated retrieval algorithms, the impact of the low-resource (incomplete) KB is not fully exploited, where contributing components (. key entities and/or relations) may be absent for question answering. To effectively address this problem, we propose a contrastive regularization based method, which is motivated by the learn-by-analogy capability from human readers. Specifically, the proposed work includes two major modules: the knowledge extension and sMoCo module. The former aims at exploiting the latent knowledge from the context KB and generating auxiliary information in the form of question-answer pairs. The later module utilizes those additional pairs and applies the contrastive regularization to learn informative representations, that making hard positive pairs attracted and hard negative pairs separated. Empirically, we achieved the state-of-the-art performance on the WebQuestionsSP dataset and the effectiveness of proposed modules is also evaluated.</abstract>
      <url hash="b87962c4">2022.findings-naacl.82</url>
      <bibkey>jpliu-wtu-edu-cn-etal-2022-seeing</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.82</doi>
      <video href="2022.findings-naacl.82.mp4"/>
      <pwccode url="https://github.com/jakeymei/smoco" additional="false">jakeymei/smoco</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/webquestionssp">WebQuestionsSP</pwcdataset>
    </paper>
    <paper id="83">
      <title>Phrase-level Textual Adversarial Attack with Label Preservation</title>
      <author><first>Yibin</first><last>Lei</last></author>
      <author><first>Yu</first><last>Cao</last></author>
      <author><first>Dianqi</first><last>Li</last></author>
      <author><first>Tianyi</first><last>Zhou</last></author>
      <author><first>Meng</first><last>Fang</last></author>
      <author><first>Mykola</first><last>Pechenizkiy</last></author>
      <pages>1095-1112</pages>
      <abstract>Generating high-quality textual adversarial examples is critical for investigating the pitfalls of natural language processing (NLP) models and further promoting their robustness. Existing attacks are usually realized through word-level or sentence-level perturbations, which either limit the perturbation space or sacrifice fluency and textual quality, both affecting the attack effectiveness. In this paper, we propose Phrase-Level Textual Adversarial ATtack (PLAT) that generates adversarial samples through phrase-level perturbations. PLAT first extracts the vulnerable phrases as attack targets by a syntactic parser, and then perturbs them by a pre-trained blank-infilling model. Such flexible perturbation design substantially expands the search space for more effective attacks without introducing too many modifications, and meanwhile maintaining the textual fluency and grammaticality via contextualized generation using surrounding texts. Moreover, we develop a label preservation filter leveraging the likelihoods of language models fine-tuned on each class, rather than textual similarity, to rule out those perturbations that potentially alter the original class label for humans. Extensive experiments and human evaluation demonstrate that PLAT has a superior attack effectiveness as well as a better label consistency than strong baselines.</abstract>
      <url hash="94df2822">2022.findings-naacl.83</url>
      <bibkey>lei-etal-2022-phrase</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.83</doi>
      <video href="2022.findings-naacl.83.mp4"/>
      <pwccode url="https://github.com/yibin-lei/plat" additional="false">yibin-lei/plat</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
    </paper>
    <paper id="84">
      <title>Prompt Augmented Generative Replay via Supervised Contrastive Learning for Lifelong Intent Detection</title>
      <author><first>Vaibhav</first><last>Varshney</last></author>
      <author><first>Mayur</first><last>Patidar</last></author>
      <author><first>Rajat</first><last>Kumar</last></author>
      <author><first>Lovekesh</first><last>Vig</last></author>
      <author><first>Gautam</first><last>Shroff</last></author>
      <pages>1113-1127</pages>
      <abstract>Identifying all possible user intents for a dialog system at design time is challenging even for skilled domain experts. For practical applications, novel intents may have to be inferred incrementally on the fly. This typically entails repeated retraining of the intent detector on both the existing and novel intents which can be expensive and would require storage of all past data corresponding to prior intents. In this paper, the objective is to continually train an intent detector on new intents while maintaining performance on prior intents without mandating access to prior intent data. Several data replay-based approaches have been introduced to avoid catastrophic forgetting during continual learning, including exemplar and generative replay. Current generative replay approaches struggle to generate representative samples because the generation is conditioned solely on the class/task label. Motivated by the recent work around prompt-based generation via pre-trained language models (PLMs), we employ generative replay using PLMs for incremental intent detection. Unlike exemplar replay, we only store the relevant contexts per intent in memory and use these stored contexts (with the class label) as prompts for generating intent-specific utterances. We use a common model for both generation and classification to promote optimal sharing of knowledge across both tasks. To further improve generation, we employ supervised contrastive fine-tuning of the PLM. Our proposed approach achieves state-of-the-art (SOTA) for lifelong intent detection on four public datasets and even outperforms exemplar replay-based approaches. The technique also achieves SOTA on a lifelong relation extraction task, suggesting that the approach is extendable to other continual learning tasks beyond intent detection.</abstract>
      <url hash="27fc8fc1">2022.findings-naacl.84</url>
      <bibkey>varshney-etal-2022-prompt</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.84</doi>
      <video href="2022.findings-naacl.84.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/banking77">BANKING77</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel">FewRel</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hwu64">HWU64</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sgd">SGD</pwcdataset>
    </paper>
    <paper id="85">
      <title><fixed-case>OTE</fixed-case>xt<fixed-case>S</fixed-case>um: <fixed-case>E</fixed-case>xtractive <fixed-case>T</fixed-case>ext <fixed-case>S</fixed-case>ummarisation with <fixed-case>O</fixed-case>ptimal <fixed-case>T</fixed-case>ransport</title>
      <author><first>Peggy</first><last>Tang</last></author>
      <author><first>Kun</first><last>Hu</last></author>
      <author><first>Rui</first><last>Yan</last></author>
      <author><first>Lei</first><last>Zhang</last></author>
      <author><first>Junbin</first><last>Gao</last></author>
      <author><first>Zhiyong</first><last>Wang</last></author>
      <pages>1128-1141</pages>
      <abstract>Extractive text summarisation aims to select salient sentences from a document to form a short yet informative summary. While learning-based methods have achieved promising results, they have several limitations, such as dependence on expensive training and lack of interpretability. Therefore, in this paper, we propose a novel non-learning-based method by for the first time formulating text summarisation as an Optimal Transport (OT) problem, namely Optimal Transport Extractive Summariser (OTExtSum). Optimal sentence extraction is conceptualised as obtaining an optimal summary that minimises the transportation cost to a given document regarding their semantic distributions. Such a cost is defined by the Wasserstein distance and used to measure the summary’s semantic coverage of the original document. Comprehensive experiments on four challenging and widely used datasets - MultiNews, PubMed, BillSum, and CNN/DM demonstrate that our proposed method outperforms the state-of-the-art non-learning-based methods and several recent learning-based methods in terms of the ROUGE metric.</abstract>
      <url hash="9e6261c0">2022.findings-naacl.85</url>
      <attachment type="software" hash="23df0894">2022.findings-naacl.85.software.zip</attachment>
      <bibkey>tang-etal-2022-otextsum</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.85</doi>
      <video href="2022.findings-naacl.85.mp4"/>
      <pwccode url="https://github.com/peggypytang/otextsum" additional="false">peggypytang/otextsum</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/billsum">BillSum</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multi-news">Multi-News</pwcdataset>
    </paper>
    <paper id="86">
      <title>Speeding Up Entmax</title>
      <author><first>Maxat</first><last>Tezekbayev</last></author>
      <author><first>Vassilina</first><last>Nikoulina</last></author>
      <author><first>Matthias</first><last>Gallé</last></author>
      <author><first>Zhenisbek</first><last>Assylbekov</last></author>
      <pages>1142-1158</pages>
      <abstract>Softmax is the de facto standard for normalizing logits in modern neural networks for language processing. However, by producing a dense probability distribution each token in the vocabulary has a nonzero chance of being selected at each generation step, leading to a variety of reported problems in text generation. <tex-math>\alpha</tex-math>-entmax of Peters et al. (2019) solves this problem, but is unfortunately slower than softmax. In this paper, we propose an alternative to <tex-math>\alpha</tex-math>-entmax, which keeps its virtuous characteristics, but is as fast as optimized softmax and achieves on par or better performance in machine translation task.</abstract>
      <url hash="5e384ad3">2022.findings-naacl.86</url>
      <bibkey>tezekbayev-etal-2022-speeding</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.86</doi>
      <video href="2022.findings-naacl.86.mp4"/>
      <pwccode url="https://github.com/maxattezekbayev/alpha-relu" additional="false">maxattezekbayev/alpha-relu</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2014">WMT 2014</pwcdataset>
    </paper>
    <paper id="87">
      <title>Improving Code-Switching Dependency Parsing with Semi-Supervised Auxiliary Tasks</title>
      <author><first>Şaziye Betül</first><last>Özateş</last></author>
      <author><first>Arzucan</first><last>Özgür</last></author>
      <author><first>Tunga</first><last>Gungor</last></author>
      <author><first>Özlem</first><last>Çetinoğlu</last></author>
      <pages>1159-1171</pages>
      <abstract>Code-switching dependency parsing stands as a challenging task due to both the scarcity of necessary resources and the structural difficulties embedded in code-switched languages. In this study, we introduce novel sequence labeling models to be used as auxiliary tasks for dependency parsing of code-switched text in a semi-supervised scheme. We show that using auxiliary tasks enhances the performance of an LSTM-based dependency parsing model and leads to better results compared to an XLM-R-based model with significantly less computational and time complexity. As the first study that focuses on multiple code-switching language pairs for dependency parsing, we acquire state-of-the-art scores on all of the studied languages. Our best models outperform the previous work by 7.4 LAS points on average.</abstract>
      <url hash="7a3b552e">2022.findings-naacl.87</url>
      <bibkey>ozates-etal-2022-improving</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.87</doi>
      <pwccode url="https://github.com/sb-b/ss-cs-depparser" additional="false">sb-b/ss-cs-depparser</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/lince">LinCE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="88">
      <title>Dangling-Aware Entity Alignment with Mixed High-Order Proximities</title>
      <author><first>Juncheng</first><last>Liu</last></author>
      <author><first>Zequn</first><last>Sun</last></author>
      <author><first>Bryan</first><last>Hooi</last></author>
      <author><first>Yiwei</first><last>Wang</last></author>
      <author><first>Dayiheng</first><last>Liu</last></author>
      <author><first>Baosong</first><last>Yang</last></author>
      <author><first>Xiaokui</first><last>Xiao</last></author>
      <author><first>Muhao</first><last>Chen</last></author>
      <pages>1172-1184</pages>
      <abstract>We study dangling-aware entity alignment in knowledge graphs (KGs), which is an underexplored but important problem. As different KGs are naturally constructed by different sets of entities, a KG commonly contains some dangling entities that cannot find counterparts in other KGs. Therefore, dangling-aware entity alignment is more realistic than the conventional entity alignment where prior studies simply ignore dangling entities. We propose a framework using mixed high-order proximities on dangling-aware entity alignment. Our framework utilizes both the local high-order proximity in a nearest neighbor subgraph and the global high-order proximity in an embedding space for both dangling detection and entity alignment. Extensive experiments with two evaluation settings shows that our method more precisely detects dangling entities, and better aligns matchable entities. Further investigations demonstrate that our framework can mitigate the hubness problem on dangling-aware entity alignment.</abstract>
      <url hash="9d8739df">2022.findings-naacl.88</url>
      <attachment type="software" hash="10c4425d">2022.findings-naacl.88.software.zip</attachment>
      <bibkey>liu-etal-2022-dangling</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.88</doi>
      <video href="2022.findings-naacl.88.mp4"/>
    </paper>
    <paper id="89">
      <title><fixed-case>D</fixed-case>ec<fixed-case>BERT</fixed-case>: Enhancing the Language Understanding of <fixed-case>BERT</fixed-case> with Causal Attention Masks</title>
      <author><first>Ziyang</first><last>Luo</last></author>
      <author><first>Yadong</first><last>Xi</last></author>
      <author><first>Jing</first><last>Ma</last></author>
      <author><first>Zhiwei</first><last>Yang</last></author>
      <author><first>Xiaoxi</first><last>Mao</last></author>
      <author><first>Changjie</first><last>Fan</last></author>
      <author><first>Rongsheng</first><last>Zhang</last></author>
      <pages>1185-1197</pages>
      <abstract>Since 2017, the Transformer-based models play critical roles in various downstream Natural Language Processing tasks. However, a common limitation of the attention mechanism utilized in Transformer Encoder is that it cannot automatically capture the information of word order, so explicit position embeddings are generally required to be fed into the target model. In contrast, Transformer Decoder with the causal attention masks is naturally sensitive to the word order. In this work, we focus on improving the position encoding ability of BERT with the causal attention masks. Furthermore, we propose a new pre-trained language model <i>DecBERT</i> and evaluate it on the GLUE benchmark. Experimental results show that (1) the causal attention mask is effective for BERT on the language understanding tasks; (2) our <i>DecBERT</i> model without position embeddings achieve comparable performance on the GLUE benchmark; and (3) our modification accelerates the pre-training process and <i>DecBERT w/ PE</i> achieves better overall performance than the baseline systems when pre-training with the same amount of computational resources.</abstract>
      <url hash="d8d7af5b">2022.findings-naacl.89</url>
      <bibkey>luo-etal-2022-decbert</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.89</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst-2">SST-2</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-2">WikiText-2</pwcdataset>
    </paper>
    <paper id="90">
      <title>Towards Computationally Feasible Deep Active Learning</title>
      <author><first>Akim</first><last>Tsvigun</last></author>
      <author><first>Artem</first><last>Shelmanov</last></author>
      <author><first>Gleb</first><last>Kuzmin</last></author>
      <author><first>Leonid</first><last>Sanochkin</last></author>
      <author><first>Daniil</first><last>Larionov</last></author>
      <author><first>Gleb</first><last>Gusev</last></author>
      <author><first>Manvel</first><last>Avetisian</last></author>
      <author><first>Leonid</first><last>Zhukov</last></author>
      <pages>1198-1218</pages>
      <abstract>Active learning (AL) is a prominent technique for reducing the annotation effort required for training machine learning models. Deep learning offers a solution for several essential obstacles to deploying AL in practice but introduces many others. One of such problems is the excessive computational resources required to train an acquisition model and estimate its uncertainty on instances in the unlabeled pool. We propose two techniques that tackle this issue for text classification and tagging tasks, offering a substantial reduction of AL iteration duration and the computational overhead introduced by deep acquisition models in AL. We also demonstrate that our algorithm that leverages pseudo-labeling and distilled models overcomes one of the essential obstacles revealed previously in the literature. Namely, it was shown that due to differences between an acquisition model used to select instances during AL and a successor model trained on the labeled data, the benefits of AL can diminish. We show that our algorithm, despite using a smaller and faster acquisition model, is capable of training a more expressive successor model with higher performance.</abstract>
      <url hash="10eabe59">2022.findings-naacl.90</url>
      <bibkey>tsvigun-etal-2022-towards</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.90</doi>
      <video href="2022.findings-naacl.90.mp4"/>
      <pwccode url="https://github.com/airi-institute/al_nlp_feasible" additional="false">airi-institute/al_nlp_feasible</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL 2003</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ontonotes-5-0">OntoNotes 5.0</pwcdataset>
    </paper>
    <paper id="91">
      <title>End-to-end Spoken Conversational Question Answering: Task, Dataset and Model</title>
      <author><first>Chenyu</first><last>You</last></author>
      <author><first>Nuo</first><last>Chen</last></author>
      <author><first>Fenglin</first><last>Liu</last></author>
      <author><first>Shen</first><last>Ge</last></author>
      <author><first>Xian</first><last>Wu</last></author>
      <author><first>Yuexian</first><last>Zou</last></author>
      <pages>1219-1232</pages>
      <abstract>In spoken question answering, the systems are designed to answer questions from contiguous text spans within the related speech transcripts. However, the most natural way that human seek or test their knowledge is via human conversations. Therefore, we propose a new Spoken Conversational Question Answering task (SCQA), aiming at enabling the systems to model complex dialogues flow given the speech documents. In this task, our main objective is to build the system to deal with conversational questions based on the audio recordings, and to explore the plausibility of providing more cues from different modalities with systems in information gathering. To this end, instead of directly adopting automatically generated speech transcripts with highly noisy data, we propose a novel unified data distillation approach, DDNet, which effectively ingests cross-modal information to achieve fine-grained representations of the speech and language modalities. Moreover, we propose a simple and novel mechanism, termed Dual Attention, by encouraging better alignments between audio and text to ease the process of knowledge transfer. To evaluate the capacity of SCQA systems in a dialogue-style interaction, we assemble a Spoken Conversational Question Answering (Spoken-CoQA) dataset with more than 40k question-answer pairs from 4k conversations. We first show that the performance of the existing state-of-the-art methods significantly degrade on our dataset, hence demonstrating the necessity of incorporating cross-modal information to achieve good performance gains. Our experimental results demonstrate that our proposed method achieves superior performance in spoken conversational question answering. Codes and datasets will be made publicly available.</abstract>
      <url hash="c4e0bb73">2022.findings-naacl.91</url>
      <bibkey>you-etal-2022-end</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.91</doi>
      <video href="2022.findings-naacl.91.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/coqa">CoQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/spoken-squad">Spoken-SQuAD</pwcdataset>
    </paper>
    <paper id="92">
      <title>Retrieval-Augmented Multilingual Keyphrase Generation with Retriever-Generator Iterative Training</title>
      <author><first>Yifan</first><last>Gao</last></author>
      <author><first>Qingyu</first><last>Yin</last></author>
      <author><first>Zheng</first><last>Li</last></author>
      <author><first>Rui</first><last>Meng</last></author>
      <author><first>Tong</first><last>Zhao</last></author>
      <author><first>Bing</first><last>Yin</last></author>
      <author><first>Irwin</first><last>King</last></author>
      <author><first>Michael</first><last>Lyu</last></author>
      <pages>1233-1246</pages>
      <abstract>Keyphrase generation is the task of automatically predicting keyphrases given a piece of long text. Despite its recent flourishing, keyphrase generation on non-English languages haven’t been vastly investigated. In this paper, we call attention to a new setting named multilingual keyphrase generation and we contribute two new datasets, EcommerceMKP and AcademicMKP, covering six languages. Technically, we propose a retrieval-augmented method for multilingual keyphrase generation to mitigate the data shortage problem in non-English languages. The retrieval-augmented model leverages keyphrase annotations in English datasets to facilitate generating keyphrases in low-resource languages. Given a non-English passage, a cross-lingual dense passage retrieval module finds relevant English passages. Then the associated English keyphrases serve as external knowledge for keyphrase generation in the current language. Moreover, we develop a retriever-generator iterative training algorithm to mine pseudo parallel passage pairs to strengthen the cross-lingual passage retriever. Comprehensive experiments and ablations show that the proposed approach outperforms all baselines.</abstract>
      <url hash="521cef94">2022.findings-naacl.92</url>
      <bibkey>gao-etal-2022-retrieval</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.92</doi>
      <video href="2022.findings-naacl.92.mp4"/>
      <pwccode url="https://github.com/yifan-gao/multilingual_keyphrase_generation" additional="false">yifan-gao/multilingual_keyphrase_generation</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/kp20k">KP20k</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/microsoft-academic-graph">Microsoft Academic Graph</pwcdataset>
    </paper>
    <paper id="93">
      <title><fixed-case>FA</fixed-case>t<fixed-case>N</fixed-case>et: Cost-Effective Approach Towards Mitigating the Linguistic Bias in Speaker Verification Systems</title>
      <author><first>Divya</first><last>Sharma</last></author>
      <author><first>Arun Balaji</first><last>Buduru</last></author>
      <pages>1247-1258</pages>
      <abstract>Linguistic bias in Deep Neural Network (DNN) based Natural Language Processing (NLP) systems is a critical problem that needs attention. The problem further intensifies in the case of security systems, such as speaker verification, where fairness is essential. Speaker verification systems are intelligent systems that determine if two speech recordings belong to the same speaker. Such human-oriented security systems should be usable by diverse people speaking varied languages. Thus, a speaker verification system trained on speech in one language should generalize when tested for other languages. However, DNN-based models are often language-dependent. Previous works explore domain adaptation to fine-tune the pre-trained model for out-of-domain languages. Fine-tuning the model individually for each existing language is expensive. Hence, it limits the usability of the system. This paper proposes the cost-effective idea of integrating a lightweight embedding with existing speaker verification systems to mitigate linguistic bias without adaptation. This work is motivated by the theoretical hypothesis that attentive-frames could help generate language-agnostic embeddings. For scientific validation of this hypothesis, we propose two frame-attentive networks and investigate the effect of their integration with baselines for twelve languages. Empirical results suggest that frame-attentive embedding can cost-effectively reduce linguistic bias and enhance the usability of baselines.</abstract>
      <url hash="2778c7aa">2022.findings-naacl.93</url>
      <attachment type="software" hash="867d3788">2022.findings-naacl.93.software.zip</attachment>
      <bibkey>sharma-buduru-2022-fatnet</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.93</doi>
      <video href="2022.findings-naacl.93.mp4"/>
      <pwccode url="https://github.com/vdivyas/fatnet" additional="false">vdivyas/fatnet</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/aishell-1">AISHELL-1</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/librispeech">LibriSpeech</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/voxceleb1">VoxCeleb1</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/voxceleb2">VoxCeleb2</pwcdataset>
    </paper>
    <paper id="94">
      <title>A Survey on Stance Detection for Mis- and Disinformation Identification</title>
      <author><first>Momchil</first><last>Hardalov</last></author>
      <author><first>Arnav</first><last>Arora</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <author><first>Isabelle</first><last>Augenstein</last></author>
      <pages>1259-1277</pages>
      <abstract>Understanding attitudes expressed in texts, also known as stance detection, plays an important role in systems for detecting false information online, be it misinformation (unintentionally false) or disinformation (intentionally false information). Stance detection has been framed in different ways, including (a) as a component of fact-checking, rumour detection, and detecting previously fact-checked claims, or (b) as a task in its own right. While there have been prior efforts to contrast stance detection with other related tasks such as argumentation mining and sentiment analysis, there is no existing survey on examining the relationship between stance detection and mis- and disinformation detection. Here, we aim to bridge this gap by reviewing and analysing existing work in this area, with mis- and disinformation in focus, and discussing lessons learnt and future challenges.</abstract>
      <url hash="0cdd2c48">2022.findings-naacl.94</url>
      <bibkey>hardalov-etal-2022-survey</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.94</doi>
      <video href="2022.findings-naacl.94.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snopes">Snopes</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tabfact">TabFact</pwcdataset>
    </paper>
    <paper id="95">
      <title>Syntax Controlled Knowledge Graph-to-Text Generation with Order and Semantic Consistency</title>
      <author><first>Jin</first><last>Liu</last></author>
      <author><first>Chongfeng</first><last>Fan</last></author>
      <author><first>Zhou</first><last>Fengyu</last></author>
      <author><first>Huijuan</first><last>Xu</last></author>
      <pages>1278-1291</pages>
      <abstract>The knowledge graph (KG) stores a large amount of structural knowledge, while it is not easy for direct human understanding. Knowledge graph-to-text (KG-to-text) generation aims to generate easy-to-understand sentences from the KG, and at the same time, maintains semantic consistency between generated sentences and the KG. Existing KG-to-text generation methods phrase this task as a sequence-to-sequence generation task with linearized KG as input and consider the consistency issue of the generated texts and KG through a simple selection between decoded sentence word and KG node word at each time step. However, the linearized KG order is obtained through a heuristic search without data-driven optimization. In this paper, we optimize the knowledge description order prediction under the order supervision extracted from the caption and further enhance the consistency of the generated sentences and KG through syntactic and semantic regularization. We incorporate the Part-of-Speech (POS) syntactic tags to constrain the positions to copy words from the KG and employ a semantic context scoring function to evaluate the semantic fitness for each word in its local context when decoding each word in the generated sentence. Extensive experiments are conducted on two datasets, WebNLG and DART, and achieve state-of-the-art performances. Our code is now public available.</abstract>
      <url hash="5653c33e">2022.findings-naacl.95</url>
      <bibkey>liu-etal-2022-syntax</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.95</doi>
      <video href="2022.findings-naacl.95.mp4"/>
      <pwccode url="https://github.com/lemonqc/kg2text" additional="false">lemonqc/kg2text</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/dart">DART</pwcdataset>
    </paper>
    <paper id="96">
      <title>To Answer or Not To Answer? Improving Machine Reading Comprehension Model with Span-based Contrastive Learning</title>
      <author><first>Yunjie</first><last>Ji</last></author>
      <author><first>Liangyu</first><last>Chen</last></author>
      <author><first>Chenxiao</first><last>Dou</last></author>
      <author><first>Baochang</first><last>Ma</last></author>
      <author><first>Xiangang</first><last>Li</last></author>
      <pages>1292-1300</pages>
      <abstract>Machine Reading Comprehension with Unanswerable Questions is a difficult NLP task, challenged by the questions which can not be answered from passages. It is observed that subtle literal changes often make an answerable question unanswerable, however, most MRC models fail to recognize such changes. To address this problem, in this paper, we propose a span-based method of Contrastive Learning (spanCL) which explicitly contrast answerable questions with their answerable and unanswerable counterparts at the answer span level. With spanCL, MRC models are forced to perceive crucial semantic changes from slight literal differences. Experiments on SQuAD 2.0 dataset show that spanCL can improve baselines significantly, yielding 0.86 2.14 absolute EM improvements. Additional experiments also show that spanCL is an effective way to utilize generated questions.</abstract>
      <url hash="3876833b">2022.findings-naacl.96</url>
      <bibkey>ji-etal-2022-answer</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.96</doi>
      <video href="2022.findings-naacl.96.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="97">
      <title>Target-Guided Dialogue Response Generation Using Commonsense and Data Augmentation</title>
      <author><first>Prakhar</first><last>Gupta</last></author>
      <author><first>Harsh</first><last>Jhamtani</last></author>
      <author><first>Jeffrey</first><last>Bigham</last></author>
      <pages>1301-1317</pages>
      <abstract>Target-guided response generation enables dialogue systems to smoothly transition a conversation from a dialogue context toward a target sentence. Such control is useful for designing dialogue systems that direct a conversation toward specific goals, such as creating non-obtrusive recommendations or introducing new topics in the conversation. In this paper, we introduce a new technique for target-guided response generation, which first finds a bridging path of commonsense knowledge concepts between the source and the target, and then uses the identified bridging path to generate transition responses. Additionally, we propose techniques to re-purpose existing dialogue datasets for target-guided generation. Experiments reveal that the proposed techniques outperform various baselines on this task. Finally, we observe that the existing automated metrics for this task correlate poorly with human judgement ratings. We propose a novel evaluation metric that we demonstrate is more reliable for target-guided response evaluation. Our work generally enables dialogue system designers to exercise more control over the conversations that their systems produce.</abstract>
      <url hash="74fd28e3">2022.findings-naacl.97</url>
      <bibkey>gupta-etal-2022-target</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.97</doi>
      <video href="2022.findings-naacl.97.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog">DailyDialog</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/otters">OTTers</pwcdataset>
    </paper>
    <paper id="98">
      <title><fixed-case>B</fixed-case>angla<fixed-case>BERT</fixed-case>: Language Model Pretraining and Benchmarks for Low-Resource Language Understanding Evaluation in <fixed-case>B</fixed-case>angla</title>
      <author><first>Abhik</first><last>Bhattacharjee</last></author>
      <author><first>Tahmid</first><last>Hasan</last></author>
      <author><first>Wasi</first><last>Ahmad</last></author>
      <author><first>Kazi Samin</first><last>Mubasshir</last></author>
      <author><first>Md Saiful</first><last>Islam</last></author>
      <author><first>Anindya</first><last>Iqbal</last></author>
      <author><first>M. Sohel</first><last>Rahman</last></author>
      <author><first>Rifat</first><last>Shahriyar</last></author>
      <pages>1318-1327</pages>
      <abstract>In this work, we introduce BanglaBERT, a BERT-based Natural Language Understanding (NLU) model pretrained in Bangla, a widely spoken yet low-resource language in the NLP literature. To pretrain BanglaBERT, we collect 27.5 GB of Bangla pretraining data (dubbed ‘Bangla2B+’) by crawling 110 popular Bangla sites. We introduce two downstream task datasets on natural language inference and question answering and benchmark on four diverse NLU tasks covering text classification, sequence labeling, and span prediction. In the process, we bring them under the first-ever Bangla Language Understanding Benchmark (BLUB). BanglaBERT achieves state-of-the-art results outperforming multilingual and monolingual models. We are making the models, datasets, and a leaderboard publicly available at <url>https://github.com/csebuetnlp/banglabert</url> to advance Bangla NLP.</abstract>
      <url hash="95056464">2022.findings-naacl.98</url>
      <bibkey>bhattacharjee-etal-2022-banglabert</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.98</doi>
      <video href="2022.findings-naacl.98.mp4"/>
      <pwccode url="https://github.com/csebuetnlp/banglabert" additional="false">csebuetnlp/banglabert</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multiconer">MultiCoNER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sentnob">SentNoB</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tydi-qa">TyDiQA</pwcdataset>
    </paper>
    <paper id="99">
      <title><fixed-case>ALLSH</fixed-case>: Active Learning Guided by Local Sensitivity and Hardness</title>
      <author><first>Shujian</first><last>Zhang</last></author>
      <author><first>Chengyue</first><last>Gong</last></author>
      <author><first>Xingchao</first><last>Liu</last></author>
      <author><first>Pengcheng</first><last>He</last></author>
      <author><first>Weizhu</first><last>Chen</last></author>
      <author><first>Mingyuan</first><last>Zhou</last></author>
      <pages>1328-1342</pages>
      <abstract>Active learning, which effectively collects informative unlabeled data for annotation, reduces the demand for labeled data. In this work, we propose to retrieve unlabeled samples with a local sensitivity and hardness-aware acquisition function. The proposed method generates data copies through local perturbations and selects data points whose predictive likelihoods diverge the most from their copies. We further empower our acquisition function by injecting the select-worst case perturbation. Our method achieves consistent gains over the commonly used active learning strategies in various classification tasks. Furthermore, we observe consistent improvements over the baselines on the study of prompt selection in prompt-based few-shot learning. These experiments demonstrate that our acquisition guided by local sensitivity and hardness can be effective and beneficial for many NLP tasks.</abstract>
      <url hash="2d6187e6">2022.findings-naacl.99</url>
      <bibkey>zhang-etal-2022-allsh</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.99</doi>
      <video href="2022.findings-naacl.99.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst-2">SST-2</pwcdataset>
    </paper>
    <paper id="100">
      <title>Low-resource Entity Set Expansion: A Comprehensive Study on User-generated Text</title>
      <author><first>Yutong</first><last>Shao</last></author>
      <author><first>Nikita</first><last>Bhutani</last></author>
      <author><first>Sajjadur</first><last>Rahman</last></author>
      <author><first>Estevam</first><last>Hruschka</last></author>
      <pages>1343-1353</pages>
      <abstract>Entity set expansion (ESE) aims at obtaining a more complete set of entities given a textual corpus and a seed set of entities of a concept. Although it is a critical task in many NLP applications, existing benchmarks are limited to well-formed text (e.g., Wikipedia) and well-defined concepts (e.g., countries and diseases). Furthermore, only a small number of predictions are evaluated compared to the actual size of an entity set. A rigorous assessment of ESE methods warrants more comprehensive benchmarks and evaluation. In this paper, we consider user-generated text to understand the generalizability of ESE methods. We develop new benchmarks and propose more rigorous evaluation metrics for assessing the performance of ESE methods. Additionally, we identify phenomena such as non-named entities, multifaceted entities, vague concepts that are more prevalent in user-generated text than well-formed text, and use them to profile ESE methods. We observe that the strong performance of state-of-the-art ESE methods does not generalize well to user-generated text. We conduct comprehensive empirical analysis and draw insights from the findings.</abstract>
      <url hash="c3a9670e">2022.findings-naacl.100</url>
      <bibkey>shao-etal-2022-low</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.100</doi>
      <video href="2022.findings-naacl.100.mp4"/>
      <pwccode url="https://github.com/megagonlabs/esebench" additional="false">megagonlabs/esebench</pwccode>
    </paper>
    <paper id="101">
      <title><fixed-case>POLITICS</fixed-case>: Pretraining with Same-story Article Comparison for Ideology Prediction and Stance Detection</title>
      <author><first>Yujian</first><last>Liu</last></author>
      <author><first>Xinliang Frederick</first><last>Zhang</last></author>
      <author><first>David</first><last>Wegsman</last></author>
      <author><first>Nicholas</first><last>Beauchamp</last></author>
      <author><first>Lu</first><last>Wang</last></author>
      <pages>1354-1374</pages>
      <abstract>Ideology is at the core of political science research. Yet, there still does not exist general-purpose tools to characterize and predict ideology across different genres of text. To this end, we study Pretrained Language Models using novel ideology-driven pretraining objectives that rely on the comparison of articles on the same story written by media of different ideologies. We further collect a large-scale dataset, consisting of more than 3.6M political news articles, for pretraining. Our model POLITICS outperforms strong baselines and the previous state-of-the-art models on ideology prediction and stance detection tasks. Further analyses show that POLITICS is especially good at understanding long or formally written texts, and is also robust in few-shot learning scenarios.</abstract>
      <url hash="90cecfcf">2022.findings-naacl.101</url>
      <bibkey>liu-etal-2022-politics</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.101</doi>
      <video href="2022.findings-naacl.101.mp4"/>
      <pwccode url="https://github.com/launchnlp/politics" additional="false">launchnlp/politics</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bignews">BigNews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/basil">BASIL</pwcdataset>
    </paper>
    <paper id="102">
      <title>Empowering parameter-efficient transfer learning by recognizing the kernel structure in self-attention</title>
      <author><first>Yifan</first><last>Chen</last></author>
      <author><first>Devamanyu</first><last>Hazarika</last></author>
      <author><first>Mahdi</first><last>Namazifar</last></author>
      <author id="yang-liu-icsi"><first>Yang</first><last>Liu</last></author>
      <author><first>Di</first><last>Jin</last></author>
      <author><first>Dilek</first><last>Hakkani-Tur</last></author>
      <pages>1375-1388</pages>
      <abstract>The massive amount of trainable parameters in the pre-trained language models (PLMs) makes them hard to be deployed to multiple downstream tasks. To address this issue, parameter-efficient transfer learning methods have been proposed to tune only a few parameters during fine-tuning while freezing the rest. This paper looks at existing methods along this line through the <i>kernel lens</i>. Motivated by the connection between self-attention in transformer-based PLMs and kernel learning, we propose <i>kernel-wise adapters</i>, namely <i>Kernel-mix</i>, that utilize the kernel structure in self-attention to guide the assignment of the tunable parameters. These adapters use guidelines found in classical kernel learning and enable separate parameter tuning for each attention head. Our empirical results, over a diverse set of natural language generation and understanding tasks, show that our proposed adapters can attain or improve the strong performance of existing baselines.</abstract>
      <url hash="178b23ca">2022.findings-naacl.102</url>
      <bibkey>chen-etal-2022-empowering</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.102</doi>
      <video href="2022.findings-naacl.102.mp4"/>
      <pwccode url="https://github.com/ychen-stat-ml/kernel-adapters" additional="false">ychen-stat-ml/kernel-adapters</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/coqa">CoQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst-2">SST-2</pwcdataset>
    </paper>
    <paper id="103">
      <title><fixed-case>RAIL</fixed-case>-<fixed-case>KD</fixed-case>: <fixed-case>RA</fixed-case>ndom Intermediate Layer Mapping for Knowledge Distillation</title>
      <author><first>Md Akmal</first><last>Haidar</last></author>
      <author><first>Nithin</first><last>Anchuri</last></author>
      <author><first>Mehdi</first><last>Rezagholizadeh</last></author>
      <author><first>Abbas</first><last>Ghaddar</last></author>
      <author><first>Philippe</first><last>Langlais</last></author>
      <author><first>Pascal</first><last>Poupart</last></author>
      <pages>1389-1400</pages>
      <abstract>Intermediate layer knowledge distillation (KD) can improve the standard KD technique (which only targets the output of teacher and student models) especially over large pre-trained language models. However, intermediate layer distillation suffers from excessive computational burdens and engineering efforts required for setting up a proper layer mapping. To address these problems, we propose a RAndom Intermediate Layer Knowledge Distillation (RAIL-KD) approach in which, intermediate layers from the teacher model are selected randomly to be distilled into the intermediate layers of the student model. This randomized selection enforces that all teacher layers are taken into account in the training process, while reducing the computational cost of intermediate layer distillation. Also, we show that it acts as a regularizer for improving the generalizability of the student model. We perform extensive experiments on GLUE tasks as well as on out-of-domain test sets. We show that our proposed RAIL-KD approach outperforms other state-of-the-art intermediate layer KD methods considerably in both performance and training-time.</abstract>
      <url hash="49f4358b">2022.findings-naacl.103</url>
      <bibkey>haidar-etal-2022-rail</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.103</doi>
      <video href="2022.findings-naacl.103.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paws">PAWS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
    </paper>
    <paper id="104">
      <title>Unbiased Math Word Problems Benchmark for Mitigating Solving Bias</title>
      <author><first>Zhicheng</first><last>Yang</last></author>
      <author><first>Jinghui</first><last>Qin</last></author>
      <author><first>Jiaqi</first><last>Chen</last></author>
      <author><first>Xiaodan</first><last>Liang</last></author>
      <pages>1401-1408</pages>
      <abstract>In this paper, we revisit the solving bias when evaluating models on current Math Word Problem (MWP) benchmarks. However, current solvers exist solving bias which consists of data bias and learning bias due to biased dataset and improper training strategy. Our experiments verify MWP solvers are easy to be biased by the biased training datasets which do not cover diverse questions for each problem narrative of all MWPs, thus a solver can only learn shallow heuristics rather than deep semantics for understanding problems. Besides, an MWP can be naturally solved by multiple equivalent equations while current datasets take only one of the equivalent equations as ground truth, forcing the model to match the labeled ground truth and ignoring other equivalent equations. Here, we first introduce a novel MWP dataset named UnbiasedMWP which is constructed by varying the grounded expressions in our collected data and annotating them with corresponding multiple new questions manually. Then, to further mitigate learning bias, we propose a Dynamic Target Selection (DTS) Strategy to dynamically select more suitable target expressions according to the longest prefix match between the current model output and candidate equivalent equations which are obtained by applying commutative law during training. The results show that our UnbiasedMWP has significantly fewer biases than its original data and other datasets, posing a promising benchmark for fairly evaluating the solvers’ reasoning skills rather than matching nearest neighbors. And the solvers trained with our DTS achieve higher accuracies on multiple MWP benchmarks. The source code is available at <url>https://github.com/yangzhch6/UnbiasedMWP</url>.</abstract>
      <url hash="b46ea7ed">2022.findings-naacl.104</url>
      <bibkey>yang-etal-2022-unbiased</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.104</doi>
      <video href="2022.findings-naacl.104.mp4"/>
      <pwccode url="https://github.com/yangzhch6/unbiasedmwp" additional="true">yangzhch6/unbiasedmwp</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/math23k">Math23K</pwcdataset>
    </paper>
    <paper id="105">
      <title>Learn To Remember: Transformer with Recurrent Memory for Document-Level Machine Translation</title>
      <author><first>Yukun</first><last>Feng</last></author>
      <author><first>Feng</first><last>Li</last></author>
      <author><first>Ziang</first><last>Song</last></author>
      <author><first>Boyuan</first><last>Zheng</last></author>
      <author><first>Philipp</first><last>Koehn</last></author>
      <pages>1409-1420</pages>
      <abstract>The Transformer architecture has led to significant gains in machine translation. However, most studies focus on only sentence-level translation without considering the context dependency within documents, leading to the inadequacy of document-level coherence. Some recent research tried to mitigate this issue by introducing an additional context encoder or translating with multiple sentences or even the entire document. Such methods may lose the information on the target side or have an increasing computational complexity as documents get longer. To address such problems, we introduce a recurrent memory unit to the vanilla Transformer, which supports the information exchange between the sentence and previous context. The memory unit is recurrently updated by acquiring information from sentences, and passing the aggregated knowledge back to subsequent sentence states. We follow a two-stage training strategy, in which the model is first trained at the sentence level and then finetuned for document-level translation. We conduct experiments on three popular datasets for document-level machine translation and our model has an average improvement of 0.91 s-BLEU over the sentence-level baseline. We also achieve state-of-the-art results on TED and News, outperforming the previous work by 0.36 s-BLEU and 1.49 d-BLEU on average.</abstract>
      <url hash="c097822a">2022.findings-naacl.105</url>
      <attachment type="software" hash="575cb903">2022.findings-naacl.105.software.zip</attachment>
      <bibkey>feng-etal-2022-learn</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.105</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/europarl">Europarl</pwcdataset>
    </paper>
    <paper id="106">
      <title>Improving Few-Shot Image Classification Using Machine- and User-Generated Natural Language Descriptions</title>
      <author><first>Kosuke</first><last>Nishida</last></author>
      <author><first>Kyosuke</first><last>Nishida</last></author>
      <author><first>Shuichi</first><last>Nishioka</last></author>
      <pages>1421-1430</pages>
      <abstract>Humans can obtain the knowledge of novel visual concepts from language descriptions, and we thus use the few-shot image classification task to investigate whether a machine learning model can have this capability. Our proposed model, LIDE (Learning from Image and DEscription), has a text decoder to generate the descriptions and a text encoder to obtain the text representations of machine- or user-generated descriptions. We confirmed that LIDE with machine-generated descriptions outperformed baseline models. Moreover, the performance was improved further with high-quality user-generated descriptions. The generated descriptions can be viewed as the explanations of the model’s predictions, and we observed that such explanations were consistent with prediction results. We also investigated why the language description improves the few-shot image classification performance by comparing the image representations and the text representations in the feature spaces.</abstract>
      <url hash="6f149962">2022.findings-naacl.106</url>
      <bibkey>nishida-etal-2022-improving</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.106</doi>
      <video href="2022.findings-naacl.106.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/cub-200-2011">CUB-200-2011</pwcdataset>
    </paper>
    <paper id="107">
      <title>All Information is Valuable: Question Matching over Full Information Transmission Network</title>
      <author><first>Le</first><last>Qi</last></author>
      <author><first>Yu</first><last>Zhang</last></author>
      <author><first>Qingyu</first><last>Yin</last></author>
      <author><first>Guidong</first><last>Zheng</last></author>
      <author><first>Wen</first><last>Junjie</last></author>
      <author><first>Jinlong</first><last>Li</last></author>
      <author><first>Ting</first><last>Liu</last></author>
      <pages>1431-1440</pages>
      <abstract>Question matching is the task of identifying whether two questions have the same intent. For better reasoning the relationship between questions, existing studies adopt multiple interaction modules and perform multi-round reasoning via deep neural networks. In this process, there are two kinds of critical information that are commonly employed: the representation information of original questions and the interactive information between pairs of questions. However, previous studies tend to transmit only one kind of information, while failing to utilize both kinds of information simultaneously. To address this problem, in this paper, we propose a Full Information Transmission Network (FITN) that can transmit both representation and interactive information together in a simultaneous fashion. More specifically, we employ a novel memory-based attention for keeping and transmitting the interactive information through a global interaction matrix. Besides, we apply an original-average mixed connection method to effectively transmit the representation information between different reasoning rounds, which helps to preserve the original representation features of questions along with the historical hidden features. Experiments on two standard benchmarks demonstrate that our approach outperforms strong baseline models.</abstract>
      <url hash="33ffc238">2022.findings-naacl.107</url>
      <attachment type="software" hash="d63585d3">2022.findings-naacl.107.software.zip</attachment>
      <bibkey>qi-etal-2022-information</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.107</doi>
      <video href="2022.findings-naacl.107.mp4"/>
    </paper>
    <paper id="108">
      <title><fixed-case>P</fixed-case>athway2<fixed-case>T</fixed-case>ext: Dataset and Method for Biomedical Pathway Description Generation</title>
      <author><first>Junwei</first><last>Yang</last></author>
      <author><first>Zequn</first><last>Liu</last></author>
      <author><first>Ming</first><last>Zhang</last></author>
      <author><first>Sheng</first><last>Wang</last></author>
      <pages>1441-1454</pages>
      <abstract>Biomedical pathways have been extensively used to characterize the mechanism of complex diseases. One essential step in biomedical pathway analysis is to curate the description of a pathway based on its graph structure and node features. Neural text generation could be a plausible technique to circumvent the tedious manual curation. In this paper, we propose a new dataset Pathway2Text, which contains 2,367 pairs of biomedical pathways and textual descriptions. All pathway graphs are experimentally derived or manually curated. All textual descriptions are written by domain experts. We form this problem as a Graph2Text task and propose a novel graph-based text generation approach <tex-math>k</tex-math>NN-Graph2Text, which explicitly exploited descriptions of similar graphs to generate new descriptions. We observed substantial improvement of our method on both Graph2Text and the reverse task of Text2Graph. We further illustrated how our dataset can be used as a novel benchmark for biomedical named entity recognition. Collectively, we envision our method will become an important benchmark for evaluating Graph2Text methods and advance biomedical research for complex diseases.</abstract>
      <url hash="7ae02c41">2022.findings-naacl.108</url>
      <bibkey>yang-etal-2022-pathway2text</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.108</doi>
      <video href="2022.findings-naacl.108.mp4"/>
      <pwccode url="https://github.com/yjwtheonly/pathway2text" additional="false">yjwtheonly/pathway2text</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bio">Bio</pwcdataset>
    </paper>
    <paper id="109">
      <title>Exploring Neural Models for Query-Focused Summarization</title>
      <author><first>Jesse</first><last>Vig</last></author>
      <author><first>Alexander</first><last>Fabbri</last></author>
      <author><first>Wojciech</first><last>Kryscinski</last></author>
      <author><first>Chien-Sheng</first><last>Wu</last></author>
      <author><first>Wenhao</first><last>Liu</last></author>
      <pages>1455-1468</pages>
      <abstract>Query-focused summarization (QFS) aims to produce summaries that answer particular questions of interest, enabling greater user control and personalization. While recently released datasets, such as QMSum or AQuaMuSe, facilitate research efforts in QFS, the field lacks a comprehensive study of the broad space of applicable modeling methods. In this paper we conduct a systematic exploration of neural approaches to QFS, considering two general classes of methods: two-stage extractive-abstractive solutions and end-to-end models. Within those categories, we investigate existing models and explore strategies for transfer learning. We also present two modeling extensions that achieve state-of-the-art performance on the QMSum dataset, up to a margin of 3.38 ROUGE-1, 3.72 ROUGE2, and 3.28 ROUGE-L when combined with transfer learning strategies. Results from human evaluation suggest that the best models produce more comprehensive and factually consistent summaries compared to a baseline model. Code and checkpoints are made publicly available: <url>https://github.com/salesforce/query-focused-sum</url>.</abstract>
      <url hash="e9bef4dc">2022.findings-naacl.109</url>
      <bibkey>vig-etal-2022-exploring</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.109</doi>
      <video href="2022.findings-naacl.109.mp4"/>
      <pwccode url="https://github.com/salesforce/query-focused-sum" additional="false">salesforce/query-focused-sum</pwccode>
    </paper>
    <paper id="110">
      <title><fixed-case>B</fixed-case>itext<fixed-case>E</fixed-case>dit: Automatic Bitext Editing for Improved Low-Resource Machine Translation</title>
      <author><first>Eleftheria</first><last>Briakou</last></author>
      <author><first>Sida</first><last>Wang</last></author>
      <author><first>Luke</first><last>Zettlemoyer</last></author>
      <author><first>Marjan</first><last>Ghazvininejad</last></author>
      <pages>1469-1485</pages>
      <abstract>Mined bitexts can contain imperfect translations that yield unreliable training signals for Neural Machine Translation (NMT). While filtering such pairs out is known to improve final model quality, we argue that it is suboptimal in low-resource conditions where even mined data can be limited. In our work, we propose instead, to refine the mined bitexts via automatic editing: given a sentence in a language <tex-math>x_f</tex-math>, and a possibly imperfect translation of it <tex-math>\mathbf{x_e}</tex-math>, our model generates a revised version <tex-math>x_f'</tex-math> or <tex-math>x_e'</tex-math> that yields a more equivalent translation pair (i.e., &lt;<tex-math>x_f, x_e'</tex-math>&gt; or &lt;<tex-math>x_f', x_e</tex-math>&gt;). We use a simple editing strategy by (1) mining potentially imperfect translations for each sentence in a given bitext, (2) learning a model to reconstruct the original translations and translate, in a multi-task fashion. Experiments demonstrate that our approach successfully improves the quality of CCMatrix mined bitext for 5 low-resource language-pairs and 10 translation directions by up to 8 BLEU points, in most cases improving upon a competitive translation-based baseline.</abstract>
      <url hash="0f0c0a3a">2022.findings-naacl.110</url>
      <bibkey>briakou-etal-2022-bitextedit</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.110</doi>
      <video href="2022.findings-naacl.110.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/ccmatrix">CCMatrix</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/flores">FLoRes</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paracrawl">ParaCrawl</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikimatrix">WikiMatrix</pwcdataset>
    </paper>
    <paper id="111">
      <title><fixed-case>M</fixed-case>ix<fixed-case>QG</fixed-case>: Neural Question Generation with Mixed Answer Types</title>
      <author><first>Lidiya</first><last>Murakhovs’ka</last></author>
      <author><first>Chien-Sheng</first><last>Wu</last></author>
      <author><first>Philippe</first><last>Laban</last></author>
      <author><first>Tong</first><last>Niu</last></author>
      <author><first>Wenhao</first><last>Liu</last></author>
      <author><first>Caiming</first><last>Xiong</last></author>
      <pages>1486-1497</pages>
      <abstract>Asking good questions is an essential ability for both human and machine intelligence. However, existing neural question generation approaches mainly focus on short factoid type of answers. In this paper, we introduce a neural question generator, MixQG, to bridge this gap. We combine nine question answering datasets with diverse answer types, including yes/no, multiple-choice, extractive, and abstractive answers, to train a single generative model. We show with empirical results that our model outperforms existing work in both seen and unseen domains, and can generate questions with different cognitive levels when conditioned on different answer types. We run a human evaluation study to assess the quality of generated questions and find that MixQG outperforms the next best model by 10%. Our code and model checkpoints will be released and integrated with the HuggingFace library to facilitate various downstream applications.</abstract>
      <url hash="137b4b53">2022.findings-naacl.111</url>
      <bibkey>murakhovska-etal-2022-mixqg</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.111</doi>
      <video href="2022.findings-naacl.111.mp4"/>
      <pwccode url="https://github.com/salesforce/qgen" additional="false">salesforce/qgen</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/boolq">BoolQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mctest">MCTest</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/narrativeqa">NarrativeQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/quoref">Quoref</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="112">
      <title>Temporal Attention for Language Models</title>
      <author><first>Guy D.</first><last>Rosin</last></author>
      <author><first>Kira</first><last>Radinsky</last></author>
      <pages>1498-1508</pages>
      <abstract>Pretrained language models based on the transformer architecture have shown great success in NLP.Textual training data often comes from the web and is thus tagged with time-specific information, but most language models ignore this information. They are trained on the textual data alone, limiting their ability to generalize temporally. In this work, we extend the key component of the transformer architecture, i.e., the self-attention mechanism, and propose temporal attention - a time-aware self-attention mechanism. Temporal attention can be applied to any transformer model and requires the input texts to be accompanied with their relevant time points. This mechanism allows the transformer to capture this temporal information and create time-specific contextualized word representations. We leverage these representations for the task of semantic change detection; we apply our proposed mechanism to BERT and experiment on three datasets in different languages (English, German, and Latin) that also vary in time, size, and genre. Our proposed model achieves state-of-the-art results on all the datasets.</abstract>
      <url hash="d015808e">2022.findings-naacl.112</url>
      <attachment type="software" hash="7b2fc025">2022.findings-naacl.112.software.tgz</attachment>
      <bibkey>rosin-radinsky-2022-temporal</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.112</doi>
      <video href="2022.findings-naacl.112.mp4"/>
      <pwccode url="https://github.com/guyrosin/temporal_attention" additional="false">guyrosin/temporal_attention</pwccode>
    </paper>
    <paper id="113">
      <title>Efficient Few-Shot Fine-Tuning for Opinion Summarization</title>
      <author><first>Arthur</first><last>Brazinskas</last></author>
      <author><first>Ramesh</first><last>Nallapati</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <author><first>Markus</first><last>Dreyer</last></author>
      <pages>1509-1523</pages>
      <abstract>Abstractive summarization models are typically pre-trained on large amounts of generic texts, then fine-tuned on tens or hundreds of thousands of annotated samples. However, in opinion summarization, large annotated datasets of reviews paired with reference summaries are not available and would be expensive to create. This calls for fine-tuning methods robust to overfitting on small datasets. In addition, generically pre-trained models are often not accustomed to the specifics of customer reviews and, after fine-tuning, yield summaries with disfluencies and semantic mistakes. To address these problems, we utilize an efficient few-shot method based on adapters which, as we show, can easily store in-domain knowledge. Instead of fine-tuning the entire model, we add adapters and pre-train them in a task-specific way on a large corpus of unannotated customer reviews, using held-out reviews as pseudo summaries. Then, fine-tune the adapters on the small available human-annotated dataset. We show that this self-supervised adapter pre-training improves summary quality over standard fine-tuning by 2.0 and 1.3 ROUGE-L points on the Amazon and Yelp datasets, respectively. Finally, for summary personalization, we condition on aspect keyword queries, automatically created from generic datasets. In the same vein, we pre-train the adapters in a query-based manner on customer reviews and then fine-tune them on annotated datasets. This results in better-organized summary content reflected in improved coherence and fewer redundancies.</abstract>
      <url hash="7f4ed86c">2022.findings-naacl.113</url>
      <bibkey>brazinskas-etal-2022-efficient</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.113</doi>
      <video href="2022.findings-naacl.113.mp4"/>
      <pwccode url="https://github.com/amazon-research/adasum" additional="false">amazon-research/adasum</pwccode>
    </paper>
    <paper id="114">
      <title>Domain-matched Pre-training Tasks for Dense Retrieval</title>
      <author><first>Barlas</first><last>Oguz</last></author>
      <author><first>Kushal</first><last>Lakhotia</last></author>
      <author><first>Anchit</first><last>Gupta</last></author>
      <author><first>Patrick</first><last>Lewis</last></author>
      <author><first>Vladimir</first><last>Karpukhin</last></author>
      <author><first>Aleksandra</first><last>Piktus</last></author>
      <author><first>Xilun</first><last>Chen</last></author>
      <author><first>Sebastian</first><last>Riedel</last></author>
      <author><first>Scott</first><last>Yih</last></author>
      <author><first>Sonal</first><last>Gupta</last></author>
      <author><first>Yashar</first><last>Mehdad</last></author>
      <pages>1524-1534</pages>
      <abstract>Pre-training on larger datasets with ever increasing model size isnow a proven recipe for increased performance across almost all NLP tasks.A notable exception is information retrieval, where additional pre-traininghas so far failed to produce convincing results. We show that, with theright pre-training setup, this barrier can be overcome. We demonstrate thisby pre-training large bi-encoder models on 1) a recently released set of 65 millionsynthetically generated questions, and 2) 200 million post-comment pairs from a preexisting dataset of Reddit conversations made available by pushshift.io. We evaluate on a set of information retrieval and dialogue retrieval benchmarks, showing substantial improvements over supervised baselines.</abstract>
      <url hash="28d52db8">2022.findings-naacl.114</url>
      <bibkey>oguz-etal-2022-domain</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.114</doi>
      <video href="2022.findings-naacl.114.mp4"/>
      <pwccode url="https://github.com/facebookresearch/dpr-scale" additional="false">facebookresearch/dpr-scale</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/convai2">ConvAI2</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dstc7-task-1">DSTC7 Task 1</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/kilt">KILT</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paq">PAQ</pwcdataset>
    </paper>
    <paper id="115">
      <title><fixed-case>U</fixed-case>ni<fixed-case>K</fixed-case>-<fixed-case>QA</fixed-case>: Unified Representations of Structured and Unstructured Knowledge for Open-Domain Question Answering</title>
      <author><first>Barlas</first><last>Oguz</last></author>
      <author><first>Xilun</first><last>Chen</last></author>
      <author><first>Vladimir</first><last>Karpukhin</last></author>
      <author><first>Stan</first><last>Peshterliev</last></author>
      <author><first>Dmytro</first><last>Okhonko</last></author>
      <author><first>Michael</first><last>Schlichtkrull</last></author>
      <author><first>Sonal</first><last>Gupta</last></author>
      <author><first>Yashar</first><last>Mehdad</last></author>
      <author><first>Scott</first><last>Yih</last></author>
      <pages>1535-1546</pages>
      <abstract>We study open-domain question answering with structured, unstructured and semi-structured knowledge sources, including text, tables, lists and knowledge bases. Departing from prior work, we propose a unifying approach that homogenizes all sources by reducing them to text and applies the retriever-reader model which has so far been limited to text sources only. Our approach greatly improves the results on knowledge-base QA tasks by 11 points, compared to latest graph-based methods. More importantly, we demonstrate that our unified knowledge (UniK-QA) model is a simple and yet effective way to combine heterogeneous sources of knowledge, advancing the state-of-the-art results on two popular question answering benchmarks, NaturalQuestions and WebQuestions, by 3.5 and 2.6 points, respectively. The code of UniK-QA is available at: <url>https://github.com/facebookresearch/UniK-QA</url>.</abstract>
      <url hash="a9ac680f">2022.findings-naacl.115</url>
      <bibkey>oguz-etal-2022-unik</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.115</doi>
      <video href="2022.findings-naacl.115.mp4"/>
      <pwccode url="https://github.com/facebookresearch/UniK-QA" additional="false">facebookresearch/UniK-QA</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tqa">TQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webquestions">WebQuestions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webquestionssp">WebQuestionsSP</pwcdataset>
    </paper>
    <paper id="116">
      <title>White-box Testing of <fixed-case>NLP</fixed-case> models with Mask Neuron Coverage</title>
      <author><first>Arshdeep</first><last>Sekhon</last></author>
      <author><first>Yangfeng</first><last>Ji</last></author>
      <author><first>Matthew</first><last>Dwyer</last></author>
      <author><first>Yanjun</first><last>Qi</last></author>
      <pages>1547-1558</pages>
      <abstract>Recent literature has seen growing interest in using black-box strategies like for testing the behavior of NLP models. Research on white-box testing has developed a number of methods for evaluatinghow thoroughly the internal behavior of deep models is tested, but they are not applicableto NLP models. We propose a set of white-box testing methods that are customized for transformer-based NLP models. These include MASK NEURON COVERAGE (MNCOVER) that measures how thoroughlythe attention layers in models are exercised during testing. We show that MNCOVER can refine testing suites generated by CheckList by substantiallyreduce them in size, for more than 60% on average, while retaining failing tests – thereby concentrating the faultdetection power of the test suite. Further we show how can be used to guide CheckList input generation,evaluate alternative NLP testing methods, and drive data augmentation to improve accuracy.</abstract>
      <url hash="3099b26e">2022.findings-naacl.116</url>
      <bibkey>sekhon-etal-2022-white</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.116</doi>
      <video href="2022.findings-naacl.116.mp4"/>
    </paper>
    <paper id="117">
      <title>Hierarchical Transformers Are More Efficient Language Models</title>
      <author><first>Piotr</first><last>Nawrot</last></author>
      <author><first>Szymon</first><last>Tworkowski</last></author>
      <author><first>Michał</first><last>Tyrolski</last></author>
      <author><first>Lukasz</first><last>Kaiser</last></author>
      <author><first>Yuhuai</first><last>Wu</last></author>
      <author><first>Christian</first><last>Szegedy</last></author>
      <author><first>Henryk</first><last>Michalewski</last></author>
      <pages>1559-1571</pages>
      <abstract>Transformer models yield impressive results on many NLP and sequence modeling tasks. Remarkably, Transformers can handle long sequences, which allows them to produce long coherent outputs: entire paragraphs produced by GPT-3 or well-structured images produced by DALL-E. These large language models are impressive but also very inefficient and costly, which limits their applications and accessibility. We postulate that having an explicit hierarchical architecture is the key to Transformers that efficiently handle long sequences. To verify this claim, we first study different ways to downsample and upsample activations in Transformers so as to make them hierarchical. We use the best performing upsampling and downsampling layers to create Hourglass - a hierarchical Transformer language model. Hourglass improves upon the Transformer baseline given the same amount of computation and can yield the same results as Transformers more efficiently. In particular, Hourglass sets new state-of-the-art for Transformer models on the ImageNet32 generation task and improves language modeling efficiency on the widely studied enwik8 benchmark.</abstract>
      <url hash="727c3d73">2022.findings-naacl.117</url>
      <attachment type="software" hash="0e30bdcc">2022.findings-naacl.117.software.zip</attachment>
      <bibkey>nawrot-etal-2022-hierarchical</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.117</doi>
      <video href="2022.findings-naacl.117.mp4"/>
      <pwccode url="https://github.com/google/trax/blob/master/trax/models/research/hourglass.py" additional="true">google/trax</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cifar-10">CIFAR-10</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hutter-prize">Hutter Prize</pwcdataset>
    </paper>
    <paper id="118">
      <title><fixed-case>DISARM</fixed-case>: Detecting the Victims Targeted by Harmful Memes</title>
      <author><first>Shivam</first><last>Sharma</last></author>
      <author><first>Md Shad</first><last>Akhtar</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <author><first>Tanmoy</first><last>Chakraborty</last></author>
      <pages>1572-1588</pages>
      <abstract>Internet memes have emerged as an increasingly popular means of communication on the web. Although memes are typically intended to elicit humour, they have been increasingly used to spread hatred, trolling, and cyberbullying, as well as to target specific individuals, communities, or society on political, socio-cultural, and psychological grounds. While previous work has focused on detecting harmful, hateful, and offensive memes in general, identifying whom these memes attack (i.e., the ‘victims’) remains a challenging and underexplored area. We attempt to address this problem in this paper. To this end, we create a dataset in which we annotate each meme with its victim(s) such as the name of the targeted person(s), organization(s), and community(ies). We then propose DISARM (Detecting vIctimS targeted by hARmful Memes), a framework that uses named-entity recognition and person identification to detect all entities a meme is referring to, and then, incorporates a novel contextualized multimodal deep neural network to classify whether the meme intends to harm these entities. We perform several systematic experiments on three different test sets, corresponding to entities that are (i) all seen while training, (ii) not seen as a harmful target while training, and (iii) not seen at all while training. The evaluation shows that DISARM significantly outperforms 10 unimodal and multimodal systems. Finally, we demonstrate that DISARM is interpretable and comparatively more generalizable and that it can reduce the relative error rate of harmful target identification by up to 9 % absolute over multimodal baseline systems.</abstract>
      <url hash="90a8bb20">2022.findings-naacl.118</url>
      <bibkey>sharma-etal-2022-disarm</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.118</doi>
      <video href="2022.findings-naacl.118.mp4"/>
      <pwccode url="https://github.com/lcs2-iiitd/disarm" additional="false">lcs2-iiitd/disarm</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hateful-memes">Hateful Memes</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">MS COCO</pwcdataset>
    </paper>
    <paper id="119">
      <title><fixed-case>KD</fixed-case>-<fixed-case>VLP</fixed-case>: Improving End-to-End Vision-and-Language Pretraining with Object Knowledge Distillation</title>
      <author><first>Yongfei</first><last>Liu</last></author>
      <author><first>Chenfei</first><last>Wu</last></author>
      <author><first>Shao-Yen</first><last>Tseng</last></author>
      <author><first>Vasudev</first><last>Lal</last></author>
      <author><first>Xuming</first><last>He</last></author>
      <author><first>Nan</first><last>Duan</last></author>
      <pages>1589-1600</pages>
      <abstract>Self-supervised vision-and-language pretraining (VLP) aims to learn transferable multi-modal representations from large-scale image-text data and to achieve strong performances on a broad scope of vision-language tasks after finetuning. Previous mainstream VLP approaches typically adopt a two-step strategy relying on external object detectors to encode images in a multi-modal Transformer framework, which suffer from restrictive object concept space, limited image context and inefficient computation. In this paper, we propose an object-aware end-to-end VLP framework, which directly feeds image grid features from CNNs into the Transformer and learns the multi-modal representations jointly. More importantly, we propose to perform object knowledge distillation to facilitate learning cross-modal alignment at different semantic levels. To achieve that, we design two novel pretext tasks by taking object features and their semantic labels from external detectors as supervision: 1.) Object-guided masked vision modeling task focuses on enforcing object-aware representation learning in the multi-modal Transformer; 2.) Phrase-region alignment task aims to improve cross-modal alignment by utilizing the similarities between noun phrases and object labels in the linguistic space. Extensive experiments on a wide range of vision-language tasks demonstrate the efficacy of our proposed framework, and we achieve competitive or superior performances over the existing pretraining strategies.</abstract>
      <url hash="c8866904">2022.findings-naacl.119</url>
      <bibkey>liu-etal-2022-kd</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.119</doi>
      <video href="2022.findings-naacl.119.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">MS COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli-ve">SNLI-VE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
    </paper>
    <paper id="120">
      <title>Dependency Position Encoding for Relation Extraction</title>
      <author><first>Qiushi</first><last>Guo</last></author>
      <author><first>Xin</first><last>Wang</last></author>
      <author><first>Dehong</first><last>Gao</last></author>
      <pages>1601-1606</pages>
      <abstract>Leveraging the dependency tree of the input sentence is able to improve the model performance for relation extraction. A challenging issue is how to remove confusions from the tree. Efforts have been made to utilize the dependency connections between words to selectively emphasize target-relevant information. However, these approaches are limited in focusing on exploiting dependency types. In this paper, we propose dependency position encoding (DPE), an efficient way of incorporating both dependency connections and dependency types into the self-attention mechanism to distinguish the importance of different word dependencies for the task. In contrast to previous studies that process input sentence and dependency information in separate streams, DPE can be seamlessly incorporated into the Transformer and makes it possible to use an one-stream scheme to extract relations between entity pairs. Extensive experiments show that models with our DPE significantly outperform the previous methods on SemEval 2010 Task 8, KBP37, and TACRED.</abstract>
      <url hash="6a0011bd">2022.findings-naacl.120</url>
      <bibkey>guo-etal-2022-dependency</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.120</doi>
      <video href="2022.findings-naacl.120.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/tacred">TACRED</pwcdataset>
    </paper>
    <paper id="121">
      <title>Good Visual Guidance Make A Better Extractor: Hierarchical Visual Prefix for Multimodal Entity and Relation Extraction</title>
      <author><first>Xiang</first><last>Chen</last></author>
      <author><first>Ningyu</first><last>Zhang</last></author>
      <author><first>Lei</first><last>Li</last></author>
      <author><first>Yunzhi</first><last>Yao</last></author>
      <author><first>Shumin</first><last>Deng</last></author>
      <author><first>Chuanqi</first><last>Tan</last></author>
      <author><first>Fei</first><last>Huang</last></author>
      <author><first>Luo</first><last>Si</last></author>
      <author><first>Huajun</first><last>Chen</last></author>
      <pages>1607-1618</pages>
      <abstract>Multimodal named entity recognition and relation extraction (MNER and MRE) is a fundamental and crucial branch in information extraction. However, existing approaches for MNER and MRE usually suffer from error sensitivity when irrelevant object images incorporated in texts. To deal with these issues, we propose a novel Hierarchical Visual Prefix fusion NeTwork (HVPNeT) for visual-enhanced entity and relation extraction, aiming to achieve more effective and robust performance. Specifically, we regard visual representation as pluggable visual prefix to guide the textual representation for error insensitive forecasting decision. We further propose a dynamic gated aggregation strategy to achieve hierarchical multi-scaled visual features as visual prefix for fusion. Extensive experiments on three benchmark datasets demonstrate the effectiveness of our method, and achieve state-of-the-art performance.</abstract>
      <url hash="2c0f6ab6">2022.findings-naacl.121</url>
      <attachment type="software" hash="e3f4d95d">2022.findings-naacl.121.software.zip</attachment>
      <bibkey>chen-etal-2022-good</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.121</doi>
      <video href="2022.findings-naacl.121.mp4"/>
      <pwccode url="https://github.com/zjunlp/HVPNeT" additional="false">zjunlp/HVPNeT</pwccode>
    </paper>
    <paper id="122">
      <title>The Role of Context in Detecting Previously Fact-Checked Claims</title>
      <author><first>Shaden</first><last>Shaar</last></author>
      <author><first>Firoj</first><last>Alam</last></author>
      <author><first>Giovanni</first><last>Da San Martino</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <pages>1619-1631</pages>
      <abstract>Recent years have seen the proliferation of disinformation and fake news online. Traditional approaches to mitigate these issues is to use manual or automatic fact-checking. Recently, another approach has emerged: checking whether the input claim has previously been fact-checked, which can be done automatically, and thus fast, while also offering credibility and explainability, thanks to the human fact-checking and explanations in the associated fact-checking article. Here, we focus on claims made in a political debate and we study the impact of modeling the context of the claim: both on the source side, i.e., in the debate, as well as on the target side, i.e., in the fact-checking explanation document. We do this by modeling the local context, the global context, as well as by means of co-reference resolution, and multi-hop reasoning over the sentences of the document describing the fact-checked claim. The experimental results show that each of these represents a valuable information source, but that modeling the source-side context is most important, and can yield 10+ points of absolute improvement over a state-of-the-art model.</abstract>
      <url hash="bb855f41">2022.findings-naacl.122</url>
      <bibkey>shaar-etal-2022-role</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.122</doi>
      <video href="2022.findings-naacl.122.mp4"/>
      <pwccode url="https://github.com/firojalam/detecting-previously-fact-checked-claims" additional="false">firojalam/detecting-previously-fact-checked-claims</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/politifact">PolitiFact</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snopes">Snopes</pwcdataset>
    </paper>
    <paper id="123">
      <title>Pruning Adatperfusion with Lottery Ticket Hypothesis</title>
      <author><first>Jiarun</first><last>Wu</last></author>
      <author><first>Qingliang</first><last>Chen</last></author>
      <author><first>Zeguan</first><last>Xiao</last></author>
      <author><first>Yuliang</first><last>Gu</last></author>
      <author><first>Mengsi</first><last>Sun</last></author>
      <pages>1632-1646</pages>
      <abstract>Pre-trained language models have shown great success in multiple downstream tasks. However, they are computationally expensive to fine-tune. Thus, transfer learning with adapter modules has been introduced to alleviate this problem, helping to extract knowledge of the downstream tasks. Adapterfusion models are an example of the transformers-with-adapter-modules, which merge multiple adapters to incorporate knowledge from different tasks. However, merging multiple adapters will inevitably cause redundancies, increasing the training and inference time massively. Therefore, in this paper, we propose an approach to identify the influence of each adapter module and a novel way to prune adapters based on the prestigious Lottery Ticket Hypothesis. Experiments on GLUE datasets show that the pruned Adapterfusion model with our scheme can achieve state-of-the-art results, reducing sizes significantly while keeping performance intact.</abstract>
      <url hash="0e990a4a">2022.findings-naacl.123</url>
      <attachment type="software" hash="dece4a2f">2022.findings-naacl.123.software.zip</attachment>
      <bibkey>wu-etal-2022-pruning</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.123</doi>
      <video href="2022.findings-naacl.123.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
    </paper>
    <paper id="124">
      <title><fixed-case>EVI</fixed-case>: Multilingual Spoken Dialogue Tasks and Dataset for Knowledge-Based Enrolment, Verification, and Identification</title>
      <author><first>Georgios</first><last>Spithourakis</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <author><first>Michał</first><last>Lis</last></author>
      <author><first>Inigo</first><last>Casanueva</last></author>
      <author><first>Paweł</first><last>Budzianowski</last></author>
      <pages>1647-1659</pages>
      <abstract>Knowledge-based authentication is crucial for task-oriented spoken dialogue systems that offer personalised and privacy-focused services. Such systems should be able to enrol (E), verify (V), and identify (I) new and recurring users based on their personal information, e.g. postcode, name, and date of birth. In this work, we formalise the three authentication tasks and their evaluation protocols, and we present EVI, a challenging spoken multilingual dataset with 5,506 dialogues in English, Polish, and French. Our proposed models set the first competitive benchmarks, explore the challenges of multilingual natural language processing of spoken dialogue, and set directions for future research.</abstract>
      <url hash="9308bfe8">2022.findings-naacl.124</url>
      <bibkey>spithourakis-etal-2022-evi</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.124</doi>
      <video href="2022.findings-naacl.124.mp4"/>
      <pwccode url="https://github.com/PolyAI-LDN/evi-paper" additional="false">PolyAI-LDN/evi-paper</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/evi">EVI</pwcdataset>
    </paper>
    <paper id="125">
      <title>Post-Training Dialogue Summarization using Pseudo-Paraphrasing</title>
      <author><first>Qi</first><last>Jia</last></author>
      <author><first>Yizhu</first><last>Liu</last></author>
      <author><first>Haifeng</first><last>Tang</last></author>
      <author><first>Kenny</first><last>Zhu</last></author>
      <pages>1660-1669</pages>
      <abstract>Previous dialogue summarization techniques adapt large language models pretrained on the narrative text by injecting dialogue-specific features into the models. These features either require additional knowledge to recognize or make the resulting models harder to tune. To bridge the format gap between dialogues and narrative summaries in dialogue summarization tasks, we propose to post-train pretrained language models (PLMs) to rephrase from dialogue to narratives. After that, the model is fine-tuned for dialogue summarization as usual. Comprehensive experiments show that our approach significantly improves vanilla PLMs on dialogue summarization and outperforms other SOTA models by the summary quality and implementation costs.</abstract>
      <url hash="6cf9704e">2022.findings-naacl.125</url>
      <attachment type="software" hash="32737ab1">2022.findings-naacl.125.software.zip</attachment>
      <bibkey>jia-etal-2022-post</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.125</doi>
      <video href="2022.findings-naacl.125.mp4"/>
      <pwccode url="https://github.com/jiaqisjtu/dialsent-pgg" additional="false">jiaqisjtu/dialsent-pgg</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/samsum-corpus">SAMSum</pwcdataset>
    </paper>
    <paper id="126">
      <title>A Dual-Channel Framework for Sarcasm Recognition by Detecting Sentiment Conflict</title>
      <author><first>Yiyi</first><last>Liu</last></author>
      <author><first>Yequan</first><last>Wang</last></author>
      <author><first>Aixin</first><last>Sun</last></author>
      <author><first>Xuying</first><last>Meng</last></author>
      <author><first>Jing</first><last>Li</last></author>
      <author><first>Jiafeng</first><last>Guo</last></author>
      <pages>1670-1680</pages>
      <abstract>Sarcasm employs ambivalence, where one says something positive but actually means negative, and vice versa. The essence of sarcasm, which is also a sufficient and necessary condition, is the conflict between literal and implied sentiments expressed in one sentence. However, it is difficult to recognize such sentiment conflict because the sentiments are mixed or even implicit. As a result, the recognition of sophisticated and obscure sentiment brings in a great challenge to sarcasm detection. In this paper, we propose a Dual-Channel Framework by modeling both literal and implied sentiments separately. Based on this dual-channel framework, we design the Dual-Channel Network (DC-Net) to recognize sentiment conflict. Experiments on political debates (i.e. IAC-V1 and IAC-V2) and Twitter datasets show that our proposed DC-Net achieves state-of-the-art performance on sarcasm recognition. Our code is released to support research.</abstract>
      <url hash="3a4621da">2022.findings-naacl.126</url>
      <bibkey>liu-etal-2022-dual</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.126</doi>
      <video href="2022.findings-naacl.126.mp4"/>
      <pwccode url="https://github.com/yiyi-ict/dual-channel-for-sarcasm" additional="false">yiyi-ict/dual-channel-for-sarcasm</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/sarc">SARC</pwcdataset>
    </paper>
    <paper id="127">
      <title>Zero-shot Entity Linking with Less Data</title>
      <author><first>G P Shrivatsa</first><last>Bhargav</last></author>
      <author><first>Dinesh</first><last>Khandelwal</last></author>
      <author><first>Saswati</first><last>Dana</last></author>
      <author><first>Dinesh</first><last>Garg</last></author>
      <author><first>Pavan</first><last>Kapanipathi</last></author>
      <author><first>Salim</first><last>Roukos</last></author>
      <author><first>Alexander</first><last>Gray</last></author>
      <author><first>L Venkata</first><last>Subramaniam</last></author>
      <pages>1681-1697</pages>
      <abstract>Entity Linking (EL) maps an entity mention in a natural language sentence to an entity in a knowledge base (KB). The Zero-shot Entity Linking (ZEL) extends the scope of EL to unseen entities at the test time without requiring new labeled data. BLINK (BERT-based) is one of the SOTA models for ZEL. Interestingly, we discovered that BLINK exhibits diminishing returns, i.e., it reaches 98% of its performance with just 1% of the training data and the remaining 99% of the data yields only a marginal increase of 2% in the performance. While this extra 2% gain makes a huge difference for downstream tasks, training BLINK on large amounts of data is very resource-intensive and impractical. In this paper, we propose a neuro-symbolic, multi-task learning approach to bridge this gap. Our approach boosts the BLINK’s performance with much less data by exploiting an auxiliary information about entity types. Specifically, we train our model on two tasks simultaneously - entity linking (primary task) and hierarchical entity type prediction (auxiliary task). The auxiliary task exploits the hierarchical structure of entity types. Our approach achieves superior performance on ZEL task with significantly less training data. On four different benchmark datasets, we show that our approach achieves significantly higher performance than SOTA models when they are trained with just 0.01%, 0.1%, or 1% of the original training data. Our code is available at <url>https://github.com/IBM/NeSLET</url>.</abstract>
      <url hash="c5f87c80">2022.findings-naacl.127</url>
      <bibkey>bhargav-etal-2022-zero</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.127</doi>
      <revision id="1" href="2022.findings-naacl.127v1" hash="80d7c748"/>
      <revision id="2" href="2022.findings-naacl.127v2" hash="c5f87c80" date="2022-08-21">Personally identifiable information has been masked in Tables 12 and 13 of the appendix.</revision>
      <video href="2022.findings-naacl.127.mp4"/>
      <pwccode url="https://github.com/facebookresearch/BLINK" additional="true">facebookresearch/BLINK</pwccode>
    </paper>
    <paper id="128">
      <title><fixed-case>G</fixed-case>raph<fixed-case>C</fixed-case>ache: Message Passing as Caching for Sentence-Level Relation Extraction</title>
      <author><first>Yiwei</first><last>Wang</last></author>
      <author><first>Muhao</first><last>Chen</last></author>
      <author><first>Wenxuan</first><last>Zhou</last></author>
      <author><first>Yujun</first><last>Cai</last></author>
      <author><first>Yuxuan</first><last>Liang</last></author>
      <author><first>Bryan</first><last>Hooi</last></author>
      <pages>1698-1708</pages>
      <abstract>Entity types and textual context are essential properties for sentence-level relation extraction (RE). Existing work only encodes these properties within individual instances, which limits the performance of RE given the insufficient features in a single sentence. In contrast, we model these properties from the whole dataset and use the dataset-level information to enrich the semantics of every instance. We propose the GraphCache (Graph Neural Network as Caching) module, that propagates the features across sentences to learn better representations for RE. GraphCache aggregates the features from sentences in the whole dataset to learn global representations of properties, and use them to augment the local features within individual sentences. The global property features act as dataset-level prior knowledge for RE, and a complement to the sentence-level features. Inspired by the classical caching technique in computer systems, we develop GraphCache to update the property representations in an online manner. Overall, GraphCache yields significant effectiveness gains on RE and enables efficient message passing across all sentences in the dataset.</abstract>
      <url hash="1c21e2ea">2022.findings-naacl.128</url>
      <bibkey>wang-etal-2022-graphcache</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.128</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/semeval-2010-task-8">SemEval-2010 Task-8</pwcdataset>
    </paper>
    <paper id="129">
      <title>Revisiting Generative Commonsense Reasoning: A Pre-Ordering Approach</title>
      <author><first>Chao</first><last>Zhao</last></author>
      <author><first>Faeze</first><last>Brahman</last></author>
      <author><first>Tenghao</first><last>Huang</last></author>
      <author><first>Snigdha</first><last>Chaturvedi</last></author>
      <pages>1709-1718</pages>
      <abstract>Pre-trained models (PTMs) have lead to great improvements in natural language generation (NLG). However, it is still unclear how much commonsense knowledge they possess. With the goal of evaluating commonsense knowledge of NLG models, recent work has proposed the problem of generative commonsense reasoning, e.g., to compose a logical sentence given a set of unordered concepts. Existing approaches to this problem hypothesize that PTMs lack sufficient parametric knowledge for this task, which can be overcome by introducing external knowledge or task-specific pre-training objectives. Different from this trend, we argue that PTM’s inherent ability for generative commonsense reasoning is underestimated due to the order-agnostic property of its input. In particular, we hypothesize that the order of the input concepts can affect the PTM’s ability to utilize its commonsense knowledge. To this end, we propose a pre-ordering approach to elaborately manipulate the order of the given concepts before generation. Experiments show that our approach can outperform the more sophisticated models that have access to a lot of external data and resources.</abstract>
      <url hash="6fe1f046">2022.findings-naacl.129</url>
      <bibkey>zhao-etal-2022-revisiting</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.129</doi>
      <video href="2022.findings-naacl.129.mp4"/>
      <pwccode url="https://github.com/zhaochaocs/planned-ptm" additional="false">zhaochaocs/planned-ptm</pwccode>
    </paper>
    <paper id="130">
      <title>Identifying and Mitigating Spurious Correlations for Improving Robustness in <fixed-case>NLP</fixed-case> Models</title>
      <author><first>Tianlu</first><last>Wang</last></author>
      <author><first>Rohit</first><last>Sridhar</last></author>
      <author><first>Diyi</first><last>Yang</last></author>
      <author><first>Xuezhi</first><last>Wang</last></author>
      <pages>1719-1729</pages>
      <abstract>Recently, NLP models have achieved remarkable progress across a variety of tasks; however, they have also been criticized for being not robust. Many robustness problems can be attributed to models exploiting “spurious correlations”, or “shortcuts” between the training data and the task labels. Most existing work identifies a limited set of task-specific shortcuts via human priors or error analyses, which requires extensive expertise and efforts. In this paper, we aim to automatically identify such spurious correlations in NLP models at scale. We first leverage existing interpretability methods to extract tokens that significantly affect model’s decision process from the input text. We then distinguish “genuine” tokens and “spurious” tokens by analyzing model predictions across multiple corpora and further verify them through knowledge-aware perturbations. We show that our proposed method can effectively and efficiently identify a scalable set of “shortcuts”, and mitigating these leads to more robust models in multiple applications.</abstract>
      <url hash="1826f1f3">2022.findings-naacl.130</url>
      <bibkey>wang-etal-2022-identifying</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.130</doi>
      <video href="2022.findings-naacl.130.mp4"/>
      <pwccode url="https://github.com/tianlu-wang/Identifying-and-Mitigating-Spurious-Correlations-for-Improving-Robustness-in-NLP-Models" additional="false">tianlu-wang/Identifying-and-Mitigating-Spurious-Correlations-for-Improving-Robustness-in-NLP-Models</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/dbpedia">DBpedia</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst-2">SST-2</pwcdataset>
    </paper>
    <paper id="131">
      <title><tex-math>Great~Truths~are ~Always ~Simple:</tex-math> A Rather Simple Knowledge Encoder for Enhancing the Commonsense Reasoning Capacity of Pre-Trained Models</title>
      <author><first>Jinhao</first><last>Jiang</last></author>
      <author><first>Kun</first><last>Zhou</last></author>
      <author><first>Ji-Rong</first><last>Wen</last></author>
      <author><first>Xin</first><last>Zhao</last></author>
      <pages>1730-1741</pages>
      <abstract>Commonsense reasoning in natural language is a desired ability of artificial intelligent systems. For solving complex commonsense reasoning tasks, a typical solution is to enhance pre-trained language models (PTMs) with a knowledge-aware graph neural network (GNN) encoder that models a commonsense knowledge graph (CSKG).Despite the effectiveness, these approaches are built on heavy architectures, and can’t clearly explain how external knowledge resources improve the reasoning capacity of PTMs. Considering this issue, we conduct a deep empirical analysis, and find that it is indeed <i>relation features</i> from CSKGs (but not <i>node features</i>) that mainly contribute to the performance improvement of PTMs. Based on this finding, we design a simple MLP-based knowledge encoder that utilizes statistical relation paths as features. Extensive experiments conducted on five benchmarks demonstrate the effectiveness of our approach, which also largely reduces the parameters for encoding CSKGs.Our codes and data are publicly available at <url>https://github.com/RUCAIBox/SAFE</url>.</abstract>
      <url hash="932cb689">2022.findings-naacl.131</url>
      <bibkey>jiang-etal-2022-great</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.131</doi>
      <video href="2022.findings-naacl.131.mp4"/>
      <pwccode url="https://github.com/rucaibox/safe" additional="false">rucaibox/safe</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/copa">COPA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/commonsenseqa">CommonsenseQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/openbookqa">OpenBookQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/piqa">PIQA</pwcdataset>
    </paper>
    <paper id="132">
      <title>Analyzing the Intensity of Complaints on Social Media</title>
      <author><first>Ming</first><last>Fang</last></author>
      <author><first>Shi</first><last>Zong</last></author>
      <author><first>Jing</first><last>Li</last></author>
      <author><first>Xinyu</first><last>Dai</last></author>
      <author><first>Shujian</first><last>Huang</last></author>
      <author><first>Jiajun</first><last>Chen</last></author>
      <pages>1742-1754</pages>
      <abstract>Complaining is a speech act that expresses a negative inconsistency between reality and human’s expectations. While prior studies mostly focus on identifying the existence or the type of complaints, in this work, we present the first study in computational linguistics of measuring the intensity of complaints from text. Analyzing complaints from such perspective is particularly useful, as complaints of certain degrees may cause severe consequences for companies or organizations. We first collect 3,103 posts about complaints in education domain from Weibo, a popular Chinese social media platform. These posts are then annotated with complaints intensity scores using Best-Worst Scaling (BWS) method. We show that complaints intensity can be accurately estimated by computational models with best mean square error achieving 0.11. Furthermore, we conduct a comprehensive linguistic analysis around complaints, including the connections between complaints and sentiment, and a cross-lingual comparison for complaints expressions used by Chinese and English speakers. We finally show that our complaints intensity scores can be incorporated for better estimating the popularity of posts on social media.</abstract>
      <url hash="fceca6c0">2022.findings-naacl.132</url>
      <bibkey>fang-etal-2022-analyzing</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.132</doi>
      <video href="2022.findings-naacl.132.mp4"/>
      <pwccode url="https://github.com/nlpfang/complaint_intensity" additional="false">nlpfang/complaint_intensity</pwccode>
    </paper>
    <paper id="133">
      <title>Detecting Narrative Elements in Informational Text</title>
      <author><first>Effi</first><last>Levi</last></author>
      <author><first>Guy</first><last>Mor</last></author>
      <author><first>Tamir</first><last>Sheafer</last></author>
      <author><first>Shaul</first><last>Shenhav</last></author>
      <pages>1755-1765</pages>
      <abstract>Automatic extraction of narrative elements from text, combining narrative theories with computational models, has been receiving increasing attention over the last few years. Previous works have utilized the oral narrative theory by Labov and Waletzky to identify various narrative elements in personal stories texts. Instead, we direct our focus to informational texts, specifically news stories. We introduce NEAT (Narrative Elements AnnoTation) – a novel NLP task for detecting narrative elements in raw text. For this purpose, we designed a new multi-label narrative annotation scheme, better suited for informational text (e.g. news media), by adapting elements from the narrative theory of Labov and Waletzky (Complication and Resolution) and adding a new narrative element of our own (Success). We then used this scheme to annotate a new dataset of 2,209 sentences, compiled from 46 news articles from various category domains. We trained a number of supervised models in several different setups over the annotated dataset to identify the different narrative elements, achieving an average <tex-math>F_1</tex-math> score of up to 0.77. The results demonstrate the holistic nature of our annotation scheme as well as its robustness to domain category.</abstract>
      <url hash="687b1ef2">2022.findings-naacl.133</url>
      <bibkey>levi-etal-2022-detecting</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.133</doi>
      <video href="2022.findings-naacl.133.mp4"/>
      <pwccode url="https://github.com/efle/neat" additional="false">efle/neat</pwccode>
    </paper>
    <paper id="134">
      <title>When do Contrastive Word Alignments Improve Many-to-many Neural Machine Translation?</title>
      <author><first>Zhuoyuan</first><last>Mao</last></author>
      <author><first>Chenhui</first><last>Chu</last></author>
      <author><first>Raj</first><last>Dabre</last></author>
      <author><first>Haiyue</first><last>Song</last></author>
      <author><first>Zhen</first><last>Wan</last></author>
      <author><first>Sadao</first><last>Kurohashi</last></author>
      <pages>1766-1775</pages>
      <abstract>Word alignment has proven to benefit many-to-many neural machine translation (NMT). However, high-quality ground-truth bilingual dictionaries were used for pre-editing in previous methods, which are unavailable for most language pairs. Meanwhile, the contrastive objective can implicitly utilize automatically learned word alignment, which has not been explored in many-to-many NMT. This work proposes a word-level contrastive objective to leverage word alignments for many-to-many NMT. Empirical results show that this leads to 0.8 BLEU gains for several language pairs. Analyses reveal that in many-to-many NMT, the encoder’s sentence retrieval performance highly correlates with the translation quality, which explains when the proposed method impacts translation. This motivates future exploration for many-to-many NMT to improve the encoder’s sentence retrieval performance.</abstract>
      <url hash="c0404af7">2022.findings-naacl.134</url>
      <bibkey>mao-etal-2022-contrastive</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.134</doi>
      <video href="2022.findings-naacl.134.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/word2word">word2word</pwcdataset>
    </paper>
    <paper id="135">
      <title>Minimally-Supervised Relation Induction from Pre-trained Language Model</title>
      <author><first>Lu</first><last>Sun</last></author>
      <author><first>Yongliang</first><last>Shen</last></author>
      <author><first>Weiming</first><last>Lu</last></author>
      <pages>1776-1786</pages>
      <abstract>Relation Induction is a very practical task in Natural Language Processing (NLP) area. In practical application scenarios, people want to induce more entity pairs having the same relation from only a few seed entity pairs. Thus, instead of the laborious supervised setting, in this paper, we focus on the minimally-supervised setting where only a couple of seed entity pairs per relation are provided. Although the conventional relation induction methods have made some success, their performance depends heavily on the quality of word embeddings. The great success of Pre-trained Language Models, such as BERT, changes the NLP area a lot, and they are proven to be able to better capture relation knowledge. In this paper, we propose a novel method to induce relation with BERT under the minimally-supervised setting. Specifically, we firstly extract proper templates from the corpus by using the mask-prediction task in BERT to build pseudo-sentences as the context of entity pairs. Then we use BERT attention weights to better represent the pseudo-sentences. In addition, We also use the IntegratedGradient of entity pairs to iteratively select better templates further. Finally, with the high-quality pseudo-sentences, we can train a better classifier for relation induction. Experiments onGoogle Analogy Test Sets (GATS), Bigger Analogy TestSet (BATS) and DiffVec demonstrate that our proposed method achieves state-of-the-art performance.</abstract>
      <url hash="de66f894">2022.findings-naacl.135</url>
      <bibkey>sun-etal-2022-minimally</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.135</doi>
    </paper>
    <paper id="136">
      <title>Crake: Causal-Enhanced Table-Filler for Question Answering over Large Scale Knowledge Base</title>
      <author><first>Minhao</first><last>Zhang</last></author>
      <author><first>Ruoyu</first><last>Zhang</last></author>
      <author><first>Yanzeng</first><last>Li</last></author>
      <author><first>Lei</first><last>Zou</last></author>
      <pages>1787-1798</pages>
      <abstract>Semantic parsing solves knowledge base (KB) question answering (KBQA) by composing a KB query, which generally involves node extraction (NE) and graph composition (GC) to detect and connect related nodes in a query. Despite the strong causal effects between NE and GC, previous works fail to directly model such causalities in their pipeline, hindering the learning of subtask correlations. Also, the sequence-generation process for GC in previous works induces ambiguity and exposure bias, which further harms accuracy. In this work, we formalize semantic parsing into two stages. In the first stage (graph structure generation), we propose a causal-enhanced table-filler to overcome the issues in sequence-modelling and to learn the internal causalities. In the second stage (relation extraction), an efficient beam-search algorithm is presented to scale complex queries on large-scale KBs. Experiments on LC-QuAD 1.0 indicate that our method surpasses previous state-of-the-arts by a large margin (17%) while remaining time and space efficiency.</abstract>
      <url hash="af34b92d">2022.findings-naacl.136</url>
      <attachment type="software" hash="6d48c806">2022.findings-naacl.136.software.zip</attachment>
      <bibkey>zhang-etal-2022-crake</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.136</doi>
      <video href="2022.findings-naacl.136.mp4"/>
      <pwccode url="https://github.com/aozmh/crake" additional="false">aozmh/crake</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/dbpedia">DBpedia</pwcdataset>
    </paper>
    <paper id="137">
      <title>Exploring the Universal Vulnerability of Prompt-based Learning Paradigm</title>
      <author><first>Lei</first><last>Xu</last></author>
      <author><first>Yangyi</first><last>Chen</last></author>
      <author><first>Ganqu</first><last>Cui</last></author>
      <author><first>Hongcheng</first><last>Gao</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <pages>1799-1810</pages>
      <abstract>Prompt-based learning paradigm bridges the gap between pre-training and fine-tuning, and works effectively under the few-shot setting. However, we find that this learning paradigm inherits the vulnerability from the pre-training stage, where model predictions can be misled by inserting certain triggers into the text. In this paper, we explore this universal vulnerability by either injecting backdoor triggers or searching for adversarial triggers on pre-trained language models using only plain text. In both scenarios, we demonstrate that our triggers can totally control or severely decrease the performance of prompt-based models fine-tuned on arbitrary downstream tasks, reflecting the universal vulnerability of the prompt-based learning paradigm. Further experiments show that adversarial triggers have good transferability among language models. We also find conventional fine-tuning models are not vulnerable to adversarial triggers constructed from pre-trained language models. We conclude by proposing a potential solution to mitigate our attack methods. Code and data are publicly available.</abstract>
      <url hash="81943ddc">2022.findings-naacl.137</url>
      <bibkey>xu-etal-2022-exploring</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.137</doi>
      <video href="2022.findings-naacl.137.mp4"/>
      <pwccode url="https://github.com/leix28/prompt-universal-vulnerability" additional="false">leix28/prompt-universal-vulnerability</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="138">
      <title>Exploiting Numerical-Contextual Knowledge to Improve Numerical Reasoning in Question Answering</title>
      <author><first>Jeonghwan</first><last>Kim</last></author>
      <author><first>Junmo</first><last>Kang</last></author>
      <author><first>Kyung-min</first><last>Kim</last></author>
      <author><first>Giwon</first><last>Hong</last></author>
      <author><first>Sung-Hyon</first><last>Myaeng</last></author>
      <pages>1811-1821</pages>
      <abstract>Numerical reasoning over text is a challenging subtask in question answering (QA) that requires both the understanding of texts and numbers. However, existing language models in these numerical reasoning QA models tend to overly rely on the pre-existing parametric knowledge at inference time, which commonly causes hallucination in interpreting numbers. Our work proposes a novel attention masked reasoning model, the NC-BERT, that learns to leverage the number-related contextual knowledge to alleviate the over-reliance on parametric knowledge and enhance the numerical reasoning capabilities of the QA model. The empirical results suggest that understanding of numbers in their context by reducing the parametric knowledge influence, and refining numerical information in the number embeddings lead to improved numerical reasoning accuracy and performance in DROP, a numerical QA dataset.</abstract>
      <url hash="0665b4bb">2022.findings-naacl.138</url>
      <attachment type="software" hash="20dad746">2022.findings-naacl.138.software.zip</attachment>
      <bibkey>kim-etal-2022-exploiting</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.138</doi>
      <video href="2022.findings-naacl.138.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
    </paper>
    <paper id="139">
      <title>Learn from Relation Information: Towards Prototype Representation Rectification for Few-Shot Relation Extraction</title>
      <author id="yang-liu-hk"><first>Yang</first><last>Liu</last></author>
      <author><first>Jinpeng</first><last>Hu</last></author>
      <author><first>Xiang</first><last>Wan</last></author>
      <author><first>Tsung-Hui</first><last>Chang</last></author>
      <pages>1822-1831</pages>
      <abstract>Few-shot Relation Extraction refers to fast adaptation to novel relation classes with few samples through training on the known relation classes. Most existing methods focus on implicitly introducing relation information (i.e., relation label or relation description) to constrain the prototype representation learning, such as contrastive learning, graphs, and specifically designed attentions, which may bring useless and even harmful parameters. Besides, these approaches are limited in handing outlier samples far away from the class center due to the weakly implicit constraint. In this paper, we propose an effective and parameter-less Prototype Rectification Method (PRM) to promote few-shot relation extraction, where we utilize a prototype rectification module to rectify original prototypes explicitly by the relation information. Specifically, PRM is composed of two gate mechanisms. One gate decides how much of the original prototype remains, and another one updates the remained prototype with relation information. In doing so, better and stabler global relation information can be captured for guiding prototype representations, and thus PRM can robustly deal with outliers. Moreover, we also extend PRM to both none-of-the-above (NOTA) and domain adaptation scenarios. Experimental results on FewRel 1.0 and 2.0 datasets demonstrate the effectiveness of our proposed method, which achieves state-of-the-art performance.</abstract>
      <url hash="12d4765e">2022.findings-naacl.139</url>
      <attachment type="software" hash="4040bb52">2022.findings-naacl.139.software.zip</attachment>
      <bibkey>liu-etal-2022-learn</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.139</doi>
      <pwccode url="https://github.com/lylylylylyly/prm-fsre" additional="false">lylylylylyly/prm-fsre</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel">FewRel</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel-2-0">FewRel 2.0</pwcdataset>
    </paper>
    <paper id="140">
      <title><fixed-case>HUE</fixed-case>: Pretrained Model and Dataset for Understanding Hanja Documents of <fixed-case>A</fixed-case>ncient <fixed-case>K</fixed-case>orea</title>
      <author><first>Haneul</first><last>Yoo</last></author>
      <author><first>Jiho</first><last>Jin</last></author>
      <author><first>Juhee</first><last>Son</last></author>
      <author><first>JinYeong</first><last>Bak</last></author>
      <author><first>Kyunghyun</first><last>Cho</last></author>
      <author><first>Alice</first><last>Oh</last></author>
      <pages>1832-1844</pages>
      <abstract>Historical records in Korea before the 20th century were primarily written in Hanja, an extinct language based on Chinese characters and not understood by modern Korean or Chinese speakers. Historians with expertise in this time period have been analyzing the documents, but that process is very difficult and time-consuming, and language models would significantly speed up the process. Toward building and evaluating language models for Hanja, we release the Hanja Understanding Evaluation dataset consisting of chronological attribution, topic classification, named entity recognition, and summary retrieval tasks. We also present BERT-based models continued training on the two major corpora from the 14th to the 19th centuries: the Annals of the Joseon Dynasty and Diaries of the Royal Secretariats. We compare the models with several baselines on all tasks and show there are significant improvements gained by training on the two corpora. Additionally, we run zero-shot experiments on the Daily Records of the Royal Court and Important Officials (DRRI). The DRRI dataset has not been studied much by the historians, and not at all by the NLP community.</abstract>
      <url hash="511488ca">2022.findings-naacl.140</url>
      <bibkey>yoo-etal-2022-hue</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.140</doi>
      <video href="2022.findings-naacl.140.mp4"/>
      <pwccode url="https://github.com/haneul-yoo/hue" additional="false">haneul-yoo/hue</pwccode>
    </paper>
    <paper id="141">
      <title><fixed-case>S</fixed-case>ea<fixed-case>D</fixed-case>: End-to-end Text-to-<fixed-case>SQL</fixed-case> Generation with Schema-aware Denoising</title>
      <author><first>Kuan</first><last>Xu</last></author>
      <author><first>Yongbo</first><last>Wang</last></author>
      <author><first>Yongliang</first><last>Wang</last></author>
      <author><first>Zihao</first><last>Wang</last></author>
      <author><first>Zujie</first><last>Wen</last></author>
      <author><first>Yang</first><last>Dong</last></author>
      <pages>1845-1853</pages>
      <abstract>On the WikiSQL benchmark, most methods tackle the challenge of text-to-SQL with predefined sketch slots and build sophisticated sub-tasks to fill these slots. Though achieving promising results, these methods suffer from over-complex model structure. In this paper, we present a simple yet effective approach that enables auto-regressive sequence-to-sequence model to robust text-to-SQL generation. Instead of formulating the task of text-to-SQL as slot-filling, we propose to train sequence-to-sequence model with Schema-aware Denoising (SeaD), which consists of two denoising objectives that train model to either recover input or predict output from two novel erosion and shuffle noises. These model-agnostic denoising objectives act as the auxiliary tasks for structural data modeling during sequence-to-sequence generation. In addition, we propose a clause-sensitive execution guided (EG) decoding strategy to overcome the limitation of EG decoding for generative model. The experiments show that the proposed method improves the performance of sequence-to-sequence model in both schema linking and grammar correctness and establishes new state-of-the-art on WikiSQL benchmark. Our work indicates that the capacity of sequence-to-sequence model for text-to-SQL may have been under-estimated and could be enhanced by specialized denoising task.</abstract>
      <url hash="d93b1d10">2022.findings-naacl.141</url>
      <bibkey>xu-etal-2022-sead</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.141</doi>
      <video href="2022.findings-naacl.141.mp4"/>
      <pwccode url="https://github.com/salesforce/WikiSQL" additional="false">salesforce/WikiSQL</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikisql">WikiSQL</pwcdataset>
    </paper>
    <paper id="142">
      <title>Cross-Lingual Cross-Modal Consolidation for Effective Multilingual Video Corpus Moment Retrieval</title>
      <author><first>Jiaheng</first><last>Liu</last></author>
      <author><first>Tan</first><last>Yu</last></author>
      <author><first>Hanyu</first><last>Peng</last></author>
      <author><first>Mingming</first><last>Sun</last></author>
      <author><first>Ping</first><last>Li</last></author>
      <pages>1854-1862</pages>
      <abstract>Existing multilingual video corpus moment retrieval (mVCMR) methods are mainly based on a two-stream structure. The visual stream utilizes the visual content in the video to estimate the query-visual similarity, and the subtitle stream exploits the query-subtitle similarity. The final query-video similarity ensembles similarities from two streams. In our work, we pro- pose a simple and effective strategy termed as Cross-lingual Cross-modal Consolidation (C3 ) to improve mVCMR accuracy. We adopt the ensemble similarity as the teacher to guide the training of each stream, leading to a more powerful ensemble similarity. Meanwhile, we use the teacher for a specific language to guide the student for another language to exploit the complementary knowledge across languages. Ex- tensive experiments on mTVR dataset demonstrate the effectiveness of our C3 method.</abstract>
      <url hash="98bad28a">2022.findings-naacl.142</url>
      <bibkey>liu-etal-2022-cross-lingual</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.142</doi>
      <video href="2022.findings-naacl.142.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/tvr">TVR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mtvr">mTVR</pwcdataset>
    </paper>
    <paper id="143">
      <title>Delving Deep into Regularity: A Simple but Effective Method for <fixed-case>C</fixed-case>hinese Named Entity Recognition</title>
      <author><first>Yingjie</first><last>Gu</last></author>
      <author><first>Xiaoye</first><last>Qu</last></author>
      <author><first>Zhefeng</first><last>Wang</last></author>
      <author><first>Yi</first><last>Zheng</last></author>
      <author><first>Baoxing</first><last>Huai</last></author>
      <author><first>Nicholas Jing</first><last>Yuan</last></author>
      <pages>1863-1873</pages>
      <abstract>Recent years have witnessed the improving performance of Chinese Named Entity Recognition (NER) from proposing new frameworks or incorporating word lexicons. However, the inner composition of entity mentions in character-level Chinese NER has been rarely studied. Actually, most mentions of regular types have strong name regularity. For example, entities end with indicator words such as “公司 (company) ” or “银行 (bank)” usually belong to organization. In this paper, we propose a simple but effective method for investigating the regularity of entity spans in Chinese NER, dubbed as Regularity-Inspired reCOgnition Network (RICON). Specifically, the proposed model consists of two branches: a regularity-aware module and a regularity-agnostic module. The regularity-aware module captures the internal regularity of each span for better entity type prediction, while the regularity-agnostic module is employed to locate the boundary of entities and relieve the excessive attention to span regularity. An orthogonality space is further constructed to encourage two modules to extract different aspects of regularity features. To verify the effectiveness of our method, we conduct extensive experiments on three benchmark datasets and a practical medical dataset. The experimental results show that our RICON significantly outperforms previous state-of-the-art methods, including various lexicon-based methods.</abstract>
      <url hash="01cdf614">2022.findings-naacl.143</url>
      <bibkey>gu-etal-2022-delving</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.143</doi>
      <video href="2022.findings-naacl.143.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/ontonotes-4-0">OntoNotes 4.0</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ontonotes-5-0">OntoNotes 5.0</pwcdataset>
    </paper>
    <paper id="144">
      <title><fixed-case>CRUSH</fixed-case>: Contextually Regularized and User anchored Self-supervised Hate speech Detection</title>
      <author><first>Souvic</first><last>Chakraborty</last></author>
      <author><first>Parag</first><last>Dutta</last></author>
      <author><first>Sumegh</first><last>Roychowdhury</last></author>
      <author><first>Animesh</first><last>Mukherjee</last></author>
      <pages>1874-1886</pages>
      <abstract>The last decade has witnessed a surge in the interaction of people through social networking platforms. While there are several positive aspects of these social platforms, their proliferation has led them to become the breeding ground for cyber-bullying and hate speech. Recent advances in NLP have often been used to mitigate the spread of such hateful content. Since the task of hate speech detection is usually applicable in the context of social networks, we introduce CRUSH, a framework for hate speech detection using User Anchored self-supervision and contextual regularization. Our proposed approach secures ~1-12% improvement in test set metrics over best performing previous approaches on two types of tasks and multiple popular English language social networking datasets.</abstract>
      <url hash="79afbbcb">2022.findings-naacl.144</url>
      <attachment type="software" hash="d6029fcf">2022.findings-naacl.144.software.zip</attachment>
      <bibkey>chakraborty-etal-2022-crush</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.144</doi>
      <video href="2022.findings-naacl.144.mp4"/>
      <pwccode url="https://github.com/parag1604/crush" additional="false">parag1604/crush</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hatexplain">HateXplain</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ruddit">Ruddit</pwcdataset>
    </paper>
    <paper id="145">
      <title><fixed-case>METGEN</fixed-case>: A Module-Based Entailment Tree Generation Framework for Answer Explanation</title>
      <author><first>Ruixin</first><last>Hong</last></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Xintong</first><last>Yu</last></author>
      <author><first>Changshui</first><last>Zhang</last></author>
      <pages>1887-1905</pages>
      <abstract>Knowing the reasoning chains from knowledge to the predicted answers can help construct an explainable question answering (QA) system. Advances on QA explanation propose to explain the answers with entailment trees composed of multiple entailment steps. While current work proposes to generate entailment trees with end-to-end generative models, the steps in the generated trees are not constrained and could be unreliable. In this paper, we propose METGEN, a Module-based Entailment Tree GENeration framework that has multiple modules and a reasoning controller. Given a question and several supporting knowledge, METGEN can iteratively generate the entailment tree by conducting single-step entailment with separate modules and selecting the reasoning flow with the controller. As each module is guided to perform a specific type of entailment reasoning, the steps generated by METGEN are more reliable and valid. Experiment results on the standard benchmark show that METGEN can outperform previous state-of-the-art models with only 9% of the parameters.</abstract>
      <url hash="2906e5fd">2022.findings-naacl.145</url>
      <bibkey>hong-etal-2022-metgen</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.145</doi>
      <video href="2022.findings-naacl.145.mp4"/>
      <pwccode url="https://github.com/Raising-hrx/MetGen" additional="true">Raising-hrx/MetGen</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/entailmentbank">EntailmentBank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/openbookqa">OpenBookQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qasc">QASC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/eqasc">eQASC</pwcdataset>
    </paper>
    <paper id="146">
      <title>A Timestep aware Sentence Embedding and Acme Coverage for Brief but Informative Title Generation</title>
      <author><first>Quanbin</first><last>Wang</last></author>
      <author><first>XieXiong</first><last>Lin</last></author>
      <author><first>Feng</first><last>Wang</last></author>
      <pages>1906-1918</pages>
      <abstract>The title generation task that summarizes article content in recapitulatory words relies heavily on utilizing the corresponding key context. To generate a title with appropriate information in the content and avoid repetition, we propose a title generation framework with two complementary components in this paper. First, we propose a Timestep aware Sentence Embedding (TSE) mechanism, which updates the sentences’ representations by re-locating the critical words in the corresponding sentence for each decoding step. Then, we present an Acme Coverage (AC) mechanism to solve the repetition problem and preserve the remaining valuable keywords after each decoding step according to the final vocabulary distribution. We conduct comprehensive experiments on various title generation tasks with different backbones, the evaluation scores of ROUGE and METEOR in varying degrees are significantly outperforming most of the existing state-of-the-art approaches. The experimental results demonstrate the effectiveness and generality of our novel generation framework TSE-AC.</abstract>
      <url hash="65b0e2ee">2022.findings-naacl.146</url>
      <bibkey>wang-etal-2022-timestep</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.146</doi>
      <pwccode url="https://github.com/alipay/timestep-aware-sentenceembedding-and-acmecoverage" additional="false">alipay/timestep-aware-sentenceembedding-and-acmecoverage</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/aeslc">AESLC</pwcdataset>
    </paper>
    <paper id="147">
      <title>Make The Most of Prior Data: A Solution for Interactive Text Summarization with Preference Feedback</title>
      <author><first>Duy-Hung</first><last>Nguyen</last></author>
      <author><first>Nguyen Viet Dung</first><last>Nghiem</last></author>
      <author><first>Bao-Sinh</first><last>Nguyen</last></author>
      <author><first>Dung Tien</first><last>Tien Le</last></author>
      <author><first>Shahab</first><last>Sabahi</last></author>
      <author><first>Minh-Tien</first><last>Nguyen</last></author>
      <author><first>Hung</first><last>Le</last></author>
      <pages>1919-1930</pages>
      <abstract>For summarization, human preferences is critical to tame outputs of the summarizer in favor of human interests, as ground-truth summaries are scarce and ambiguous. Practical settings require dynamic exchanges between humans and AI agents wherein feedback is provided in an online manner, a few at a time. In this paper, we introduce a new framework to train summarization models with preference feedback interactively. By properly leveraging offline data and a novel reward model, we improve the performance regarding ROUGE scores and sample-efficiency. Our experiments on three various datasets confirm the benefit of the proposed framework in active, few-shot and online settings of preference learning.</abstract>
      <url hash="bd0cd1bb">2022.findings-naacl.147</url>
      <bibkey>nguyen-etal-2022-make</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.147</doi>
      <video href="2022.findings-naacl.147.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/billsum">BillSum</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/reddit-tifu">Reddit TIFU</pwcdataset>
    </paper>
    <paper id="148">
      <title><fixed-case>XLT</fixed-case>ime: A Cross-Lingual Knowledge Transfer Framework for Temporal Expression Extraction</title>
      <author><first>Yuwei</first><last>Cao</last></author>
      <author><first>William</first><last>Groves</last></author>
      <author><first>Tanay Kumar</first><last>Saha</last></author>
      <author><first>Joel</first><last>Tetreault</last></author>
      <author><first>Alejandro</first><last>Jaimes</last></author>
      <author><first>Hao</first><last>Peng</last></author>
      <author><first>Philip</first><last>Yu</last></author>
      <pages>1931-1942</pages>
      <abstract>Temporal Expression Extraction (TEE) is essential for understanding time in natural language. It has applications in Natural Language Processing (NLP) tasks such as question answering, information retrieval, and causal inference. To date, work in this area has mostly focused on English as there is a scarcity of labeled data for other languages. We propose XLTime, a novel framework for multilingual TEE. XLTime works on top of pre-trained language models and leverages multi-task learning to prompt cross-language knowledge transfer both from English and within the non-English languages. XLTime alleviates problems caused by a shortage of data in the target language. We apply XLTime with different language models and show that it outperforms the previous automatic SOTA methods on French, Spanish, Portuguese, and Basque, by large margins. XLTime also closes the gap considerably on the handcrafted HeidelTime method.</abstract>
      <url hash="1c95732f">2022.findings-naacl.148</url>
      <bibkey>cao-etal-2022-xltime</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.148</doi>
      <video href="2022.findings-naacl.148.mp4"/>
      <pwccode url="https://github.com/yuweicao-uic/xltime" additional="false">yuweicao-uic/xltime</pwccode>
    </paper>
    <paper id="149">
      <title><fixed-case>B</fixed-case>ehance<fixed-case>PR</fixed-case>: A Punctuation Restoration Dataset for Livestreaming Video Transcript</title>
      <author><first>Viet</first><last>Lai</last></author>
      <author><first>Amir</first><last>Pouran Ben Veyseh</last></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Thien</first><last>Nguyen</last></author>
      <pages>1943-1951</pages>
      <abstract>Given the increasing number of livestreaming videos, automatic speech recognition and post-processing for livestreaming video transcripts are crucial for efficient data management as well as knowledge mining. A key step in this process is punctuation restoration which restores fundamental text structures such as phrase and sentence boundaries from the video transcripts. This work presents a new human-annotated corpus, called BehancePR, for punctuation restoration in livestreaming video transcripts. Our experiments on BehancePR demonstrate the challenges of punctuation restoration for this domain. Furthermore, we show that popular natural language processing toolkits like Stanford Stanza, Spacy, and Trankit underperform on detecting sentence boundary on non-punctuated transcripts of livestreaming videos. The dataset is publicly accessible at <url>http://github.com/nlp-uoregon/behancepr</url>.</abstract>
      <url hash="4a3f74b3">2022.findings-naacl.149</url>
      <bibkey>lai-etal-2022-behancepr</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.149</doi>
      <video href="2022.findings-naacl.149.mp4"/>
      <pwccode url="https://github.com/nlp-uoregon/behancepr" additional="false">nlp-uoregon/behancepr</pwccode>
    </paper>
    <paper id="150">
      <title>Event Detection for Suicide Understanding</title>
      <author><first>Luis</first><last>Guzman-Nateras</last></author>
      <author><first>Viet</first><last>Lai</last></author>
      <author><first>Amir</first><last>Pouran Ben Veyseh</last></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Thien</first><last>Nguyen</last></author>
      <pages>1952-1961</pages>
      <abstract>Suicide is a serious problem in every society. Understanding life events of a potential patient is essential for successful suicide-risk assessment and prevention. In this work, we focus on the Event Detection (ED) task to identify event trigger words of suicide-related events in public posts of discussion forums. In particular, we introduce SuicideED: a new dataset for the ED task that features seven suicidal event types to comprehensively capture suicide actions and ideation, and general risk and protective factors. Our experiments with current state-of-the-art ED systems suggest that this domain poses meaningful challenges as there is significant room for improvement of ED models. We will release SuicideED to support future research in this important area.</abstract>
      <url hash="97f602f3">2022.findings-naacl.150</url>
      <bibkey>guzman-nateras-etal-2022-event</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.150</doi>
      <video href="2022.findings-naacl.150.mp4"/>
    </paper>
    <paper id="151">
      <title>Great Power, Great Responsibility: Recommendations for Reducing Energy for Training Language Models</title>
      <author><first>Joseph</first><last>McDonald</last></author>
      <author><first>Baolin</first><last>Li</last></author>
      <author><first>Nathan</first><last>Frey</last></author>
      <author><first>Devesh</first><last>Tiwari</last></author>
      <author><first>Vijay</first><last>Gadepally</last></author>
      <author><first>Siddharth</first><last>Samsi</last></author>
      <pages>1962-1970</pages>
      <abstract>The energy requirements of current natural language processing models continue to grow at a rapid, unsustainable pace. Recent works highlighting this problem conclude there is an urgent need for methods that reduce the energy needs of NLP and machine learning more broadly. In this article, we investigate techniques that can be used to reduce the energy consumption of common NLP applications. In particular, we focus on techniques to measure energy usage and different hardware and datacenter-oriented settings that can be tuned to reduce energy consumption for training and inference for language models. We characterize the impact of these settings on metrics such as computational performance and energy consumption through experiments conducted on a high performance computing system as well as popular cloud computing platforms. These techniques can lead to significant reduction in energy consumption when training language models or their use for inference. For example, power-capping, which limits the maximum power a GPU can consume, can enable a 15% decrease in energy usage with marginal increase in overall computation time when training a transformer-based language model.</abstract>
      <url hash="a7c728d9">2022.findings-naacl.151</url>
      <bibkey>mcdonald-etal-2022-great</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.151</doi>
      <video href="2022.findings-naacl.151.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-2">WikiText-2</pwcdataset>
    </paper>
    <paper id="152">
      <title>What kinds of errors do reference resolution models make and what can we learn from them?</title>
      <author><first>Jorge</first><last>Sánchez</last></author>
      <author><first>Mauricio</first><last>Mazuecos</last></author>
      <author><first>Hernán</first><last>Maina</last></author>
      <author><first>Luciana</first><last>Benotti</last></author>
      <pages>1971-1986</pages>
      <abstract>Referring resolution is the task of identifying the referent of a natural language expression, for example “the woman behind the other woman getting a massage”. In this paper we investigate which are the kinds of referring expressions on which current transformer based models fail. Motivated by this analysis we identify the weakening of the spatial natural constraints as one of its causes and propose a model that aims to restore it. We evaluate our proposed model on different datasets for the task showing improved performance on the most challenging kinds of referring expressions. Finally we present a thorough analysis of the kinds errors that are improved by the new model and those that are not and remain future challenges for the task.</abstract>
      <url hash="a0fd6237">2022.findings-naacl.152</url>
      <bibkey>sanchez-etal-2022-kinds</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.152</doi>
      <video href="2022.findings-naacl.152.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">MS COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/refcoco">RefCOCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/referitgame">ReferItGame</pwcdataset>
    </paper>
    <paper id="153">
      <title>Uncertainty-Aware Cross-Lingual Transfer with Pseudo Partial Labels</title>
      <author><first>Shuo</first><last>Lei</last></author>
      <author><first>Xuchao</first><last>Zhang</last></author>
      <author><first>Jianfeng</first><last>He</last></author>
      <author><first>Fanglan</first><last>Chen</last></author>
      <author><first>Chang-Tien</first><last>Lu</last></author>
      <pages>1987-1997</pages>
      <abstract>Large-scale multilingual pre-trained language models have achieved remarkable performance in zero-shot cross-lingual tasks. A recent study has demonstrated the effectiveness of self-learning-based approach on cross-lingual transfer, where only unlabeled data of target languages are required, without any efforts to annotate gold labels for target languages. However, it suffers from noisy training due to the incorrectly pseudo-labeled samples. In this work, we propose an uncertainty-aware Cross-Lingual Transfer framework with Pseudo-Partial-Label (CLTP)1 to maximize the utilization of unlabeled data by reducing the noise introduced in the training phase. To estimate pseudo-partial-label for each unlabeled data, we propose a novel estimation method, considering both prediction confidence and the limitation to the number of similar labels. Extensive experiments are conducted on two cross-lingual tasks, including Named Entity Recognition (NER) and Natural Language Inference (NLI) across 40 languages, which shows our method can outperform the baselines on both high-resource and low-resource languages, such as 6.9 on Kazakh (kk) and 5.2 Marathi (mr) for NER.</abstract>
      <url hash="c37658da">2022.findings-naacl.153</url>
      <attachment type="software" hash="4b6a1dd0">2022.findings-naacl.153.software.zip</attachment>
      <bibkey>lei-etal-2022-uncertainty</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.153</doi>
      <video href="2022.findings-naacl.153.mp4"/>
      <pwccode url="https://github.com/slei109/cltp" additional="false">slei109/cltp</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/xtreme">XTREME</pwcdataset>
    </paper>
    <paper id="154">
      <title><fixed-case>NLU</fixed-case>++: A Multi-Label, Slot-Rich, Generalisable Dataset for Natural Language Understanding in Task-Oriented Dialogue</title>
      <author><first>Inigo</first><last>Casanueva</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <author><first>Georgios</first><last>Spithourakis</last></author>
      <author><first>Paweł</first><last>Budzianowski</last></author>
      <pages>1998-2013</pages>
      <abstract>We present NLU++, a novel dataset for natural language understanding (NLU) in task-oriented dialogue (ToD) systems, with the aim to provide a much more challenging evaluation environment for dialogue NLU models, up to date with the current application and industry requirements. NLU++ is divided into two domains (BANKING and HOTELS) and brings several crucial improvements over current commonly used NLU datasets. 1) NLU++ provides fine-grained domain ontologies with a large set of challenging multi-intent sentences combined with finer-grained and thus more challenging slot sets. 2) The ontology is divided into domain-specific and generic (i.e., domain-universal) intents that overlap across domains, promoting cross-domain reusability of annotated examples. 3) The dataset design has been inspired by the problems observed in industrial ToD systems, and 4) it has been collected, filtered and carefully annotated by dialogue NLU experts, yielding high-quality annotated data. Finally, we benchmark a series of current state-of-the-art NLU models on NLU++; the results demonstrate the challenging nature of the dataset, especially in low-data regimes, and call for further research on ToD NLU.</abstract>
      <url hash="663f73e8">2022.findings-naacl.154</url>
      <bibkey>casanueva-etal-2022-nlu</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.154</doi>
      <video href="2022.findings-naacl.154.mp4"/>
      <pwccode url="https://github.com/PolyAI-LDN/task-specific-datasets" additional="false">PolyAI-LDN/task-specific-datasets</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/nlu">NLU++</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/atis">ATIS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/banking77">BANKING77</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snips">SNIPS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="155">
      <title>Challenges in Generalization in Open Domain Question Answering</title>
      <author><first>Linqing</first><last>Liu</last></author>
      <author><first>Patrick</first><last>Lewis</last></author>
      <author><first>Sebastian</first><last>Riedel</last></author>
      <author><first>Pontus</first><last>Stenetorp</last></author>
      <pages>2014-2029</pages>
      <abstract>Recent work on Open Domain Question Answering has shown that there is a large discrepancy in model performance between novel test questions and those that largely overlap with training questions. However, it is unclear which aspects of novel questions make them challenging. Drawing upon studies on systematic generalization, we introduce and annotate questions according to three categories that measure different levels and kinds of generalization: training set overlap, compositional generalization (comp-gen), and novel-entity generalization (novel-entity). When evaluating six popular parametric and non-parametric models, we find that for the established Natural Questions and TriviaQA datasets, even the strongest model performance for comp-gen/novel-entity is 13.1/5.4% and 9.6/1.5% lower compared to that for the full test set – indicating the challenge posed by these types of questions. Furthermore, we show that whilst non-parametric models can handle questions containing novel entities relatively well, they struggle with those requiring compositional generalization. Lastly, we find that key question difficulty factors are: cascading errors from the retrieval component, frequency of question pattern, and frequency of the entity.</abstract>
      <url hash="d0fb4aee">2022.findings-naacl.155</url>
      <bibkey>liu-etal-2022-challenges</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.155</doi>
      <video href="2022.findings-naacl.155.mp4"/>
      <pwccode url="https://github.com/likicode/QA-generalize" additional="false">likicode/QA-generalize</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webquestions">WebQuestions</pwcdataset>
    </paper>
    <paper id="156">
      <title>Beyond Distributional Hypothesis: Let Language Models Learn Meaning-Text Correspondence</title>
      <author><first>Myeongjun</first><last>Jang</last></author>
      <author><first>Frank</first><last>Mtumbuka</last></author>
      <author><first>Thomas</first><last>Lukasiewicz</last></author>
      <pages>2030-2042</pages>
      <abstract>The logical negation property (LNP), which implies generating different predictions for semantically opposite inputs (p is true iff ¬p is false), is an important property that a trustworthy language model must satisfy. However, much recent evidence shows that large-size pre-trained language models (PLMs) do not satisfy this property. In this paper, we perform experiments using probing tasks to assess PLMs’ LNP understanding. Unlike previous studies that only examined negation expressions, we expand the boundary of the investigation to lexical semantics. Through experiments, we observe that PLMs violate the LNP frequently. To alleviate the issue, we propose a novel intermediate training task, named meaning-matching, designed to directly learn a meaning text correspondence, instead of relying on the distributional hypothesis. Through multiple experiments, we find that the task enables PLMs to learn lexical semantic information. Also, through fine-tuning experiments on 7 GLUE tasks, we confirm that it is a safe intermediate task that guarantees a similar or better performance of downstream tasks. Finally, we observe that our proposed approach outperforms our previous counterparts despite its time and resource efficiency.</abstract>
      <url hash="32e9461a">2022.findings-naacl.156</url>
      <bibkey>jang-etal-2022-beyond</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.156</doi>
      <video href="2022.findings-naacl.156.mp4"/>
      <pwccode url="https://github.com/mj-jang/beyond-distributional" additional="false">mj-jang/beyond-distributional</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/lama">LAMA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="157">
      <title>Por Qué Não Utiliser Alla Språk? Mixed Training with Gradient Optimization in Few-Shot Cross-Lingual Transfer</title>
      <author><first>Haoran</first><last>Xu</last></author>
      <author><first>Kenton</first><last>Murray</last></author>
      <pages>2043-2059</pages>
      <abstract>The current state-of-the-art for few-shot cross-lingual transfer learning first trains on abundant labeled data in the source language and then fine-tunes with a few examples on the target language, termed target-adapting. Though this has been demonstrated to work on a variety of tasks, in this paper we show some deficiencies of this approach and propose a one-step mixed training method that trains on both source and target data with stochastic gradient surgery, a novel gradient-level optimization. Unlike the previous studies that focus on one language at a time when target-adapting, we use one model to handle all target languages simultaneously to avoid excessively language-specific models. Moreover, we discuss the unreality of utilizing large target development sets for model selection in previous literature. We further show that our method is both development-free for target languages, and is also able to escape from overfitting issues. We conduct a large-scale experiment on 4 diverse NLP tasks across up to 48 languages. Our proposed method achieves state-of-the-art performance on all tasks and outperforms target-adapting by a large margin, especially for languages that are linguistically distant from the source language, e.g., 7.36% F1 absolute gain on average for the NER task, up to 17.60% on Punjabi.</abstract>
      <url hash="fa8e5e99">2022.findings-naacl.157</url>
      <bibkey>xu-murray-2022-por</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.157</doi>
      <video href="2022.findings-naacl.157.mp4"/>
      <pwccode url="https://github.com/fe1ixxu/mixed-gradient-few-shot" additional="false">fe1ixxu/mixed-gradient-few-shot</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/tydi-qa">TyDiQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
    </paper>
    <paper id="158">
      <title>Learning to Execute Actions or Ask Clarification Questions</title>
      <author><first>Zhengxiang</first><last>Shi</last></author>
      <author><first>Yue</first><last>Feng</last></author>
      <author><first>Aldo</first><last>Lipani</last></author>
      <pages>2060-2070</pages>
      <abstract>Collaborative tasks are ubiquitous activities where a form of communication is required in order to reach a joint goal. Collaborative building is one of such tasks. We wish to develop an intelligent builder agent in a simulated building environment (Minecraft) that can build whatever users wish to build by just talking to the agent. In order to achieve this goal, such agents need to be able to take the initiative by asking clarification questions when further information is needed. Existing works on Minecraft Corpus Dataset only learn to execute instructions neglecting the importance of asking for clarifications. In this paper, we extend the Minecraft Corpus Dataset by annotating all builder utterances into eight types, including clarification questions, and propose a new builder agent model capable of determining when to ask or execute instructions. Experimental results show that our model achieves state-of-the-art performance on the collaborative building task with a substantial improvement. We also define two new tasks, the learning to ask task and the joint learning task. The latter consists of solving both collaborating building and learning to ask tasks jointly.</abstract>
      <url hash="0d519cfe">2022.findings-naacl.158</url>
      <bibkey>shi-etal-2022-learning</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.158</doi>
      <video href="2022.findings-naacl.158.mp4"/>
      <pwccode url="https://github.com/zhengxiangshi/learntoask" additional="false">zhengxiangshi/learntoask</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/extended-minecraft-corpus-dataset">Extended Minecraft Corpus dataset</pwcdataset>
    </paper>
    <paper id="159">
      <title>Capturing Conversational Interaction for Question Answering via Global History Reasoning</title>
      <author><first>Jin</first><last>Qian</last></author>
      <author><first>Bowei</first><last>Zou</last></author>
      <author><first>Mengxing</first><last>Dong</last></author>
      <author><first>Xiao</first><last>Li</last></author>
      <author><first>AiTi</first><last>Aw</last></author>
      <author><first>Yu</first><last>Hong</last></author>
      <pages>2071-2078</pages>
      <abstract>Conversational Question Answering (ConvQA) is required to answer the current question, conditioned on the observable paragraph-level context and conversation history. Previous works have intensively studied history-dependent reasoning. They perceive and absorb topic-related information of prior utterances in the interactive encoding stage. It yielded significant improvement compared to history-independent reasoning. This paper further strengthens the ConvQA encoder by establishing long-distance dependency among global utterances in multi-turn conversation. We use multi-layer transformers to resolve long-distance relationships, which potentially contribute to the reweighting of attentive information in historical utterances. Experiments on QuAC show that our method obtains a substantial improvement (1%), yielding the F1 score of 73.7%. All source codes are available at <url>https://github.com/jaytsien/GHR</url>.</abstract>
      <url hash="4cd077d6">2022.findings-naacl.159</url>
      <bibkey>qian-etal-2022-capturing</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.159</doi>
      <video href="2022.findings-naacl.159.mp4"/>
      <pwccode url="https://github.com/jaytsien/ghr" additional="false">jaytsien/ghr</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/quac">QuAC</pwcdataset>
    </paper>
    <paper id="160">
      <title>Learning Structural Information for Syntax-Controlled Paraphrase Generation</title>
      <author><first>Erguang</first><last>Yang</last></author>
      <author><first>Chenglin</first><last>Bai</last></author>
      <author><first>Deyi</first><last>Xiong</last></author>
      <author><first>Yujie</first><last>Zhang</last></author>
      <author><first>Yao</first><last>Meng</last></author>
      <author><first>Jinan</first><last>Xu</last></author>
      <author><first>Yufeng</first><last>Chen</last></author>
      <pages>2079-2090</pages>
      <abstract>Syntax-controlled paraphrase generation aims to produce paraphrase conform to given syntactic patterns. To address this task, recent works have started to use parse trees (or syntactic templates) to guide generation.A constituency parse tree contains abundant structural information, such as parent-child relation, sibling relation, and the alignment relation between words and nodes. Previous works have only utilized parent-child and alignment relations, which may affect the generation quality. To address this limitation, we propose a Structural Information-augmented Syntax-Controlled Paraphrasing (SI-SCP) model. Particularly, we design a syntax encoder based on tree-transformer to capture parent-child and sibling relations. To model the alignment relation between words and nodes, we propose an attention regularization objective, which makes the decoder accurately select corresponding syntax nodes to guide the generation of words. Experiments show that SI-SCP achieves state-of-the-art performances in terms of semantic and syntactic quality on two popular benchmark datasets. Additionally, we propose a Syntactic Template Retriever (STR) to retrieve compatible syntactic structures. We validate that STR is capable of retrieving compatible syntactic structures. We further demonstrate the effectiveness of SI-SCP to generate diverse paraphrases with retrieved syntactic structures.</abstract>
      <url hash="6926c7b3">2022.findings-naacl.160</url>
      <bibkey>yang-etal-2022-learning-structural</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.160</doi>
      <video href="2022.findings-naacl.160.mp4"/>
    </paper>
    <paper id="161">
      <title>Controllable Sentence Simplification via Operation Classification</title>
      <author><first>Liam</first><last>Cripwell</last></author>
      <author><first>Joël</first><last>Legrand</last></author>
      <author><first>Claire</first><last>Gardent</last></author>
      <pages>2091-2103</pages>
      <abstract>Different types of transformations have been used to model sentence simplification ranging from mainly local operations such as phrasal or lexical rewriting, deletion and re-ordering to the more global affecting the whole input sentence such as sentence rephrasing, copying and splitting. In this paper, we propose a novel approach to sentence simplification which encompasses four global operations: whether to rephrase or copy and whether to split based on syntactic or discourse structure. We create a novel dataset that can be used to train highly accurate classification systems for these four operations. We propose a controllable-simplification model that tailors simplifications to these operations and show that it outperforms both end-to-end, non-controllable approaches and previous controllable approaches.</abstract>
      <url hash="fda45001">2022.findings-naacl.161</url>
      <bibkey>cripwell-etal-2022-controllable</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.161</doi>
      <video href="2022.findings-naacl.161.mp4"/>
      <pwccode url="https://github.com/liamcripwell/control_simp" additional="false">liamcripwell/control_simp</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/asset">ASSET</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsela">Newsela</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikisplit">WikiSplit</pwcdataset>
    </paper>
    <paper id="162">
      <title>Balancing Multi-Domain Corpora Learning for Open-Domain Response Generation</title>
      <author><first>Yujie</first><last>Xing</last></author>
      <author><first>Jinglun</first><last>Cai</last></author>
      <author><first>Nils</first><last>Barlaug</last></author>
      <author><first>Peng</first><last>Liu</last></author>
      <author><first>Jon Atle</first><last>Gulla</last></author>
      <pages>2104-2120</pages>
      <abstract>Open-domain conversational systems are assumed to generate equally good responses on multiple domains. Previous work achieved good performance on the single corpus, but training and evaluating on multiple corpora from different domains are less studied. This paper explores methods of generating relevant responses for each of multiple multi-domain corpora. We first examine interleaved learning which intermingles multiple corpora as the baseline. We then investigate two multi-domain learning methods, labeled learning and multi-task labeled learning, which encode each corpus through a unique corpus embedding. Furthermore, we propose Domain-specific Frequency (DF), a novel word-level importance weight that measures the relative importance of a word for a specific corpus compared to other corpora. Based on DF, we propose weighted learning, a method that integrates DF to the loss function. We also adopt DF as a new evaluation metric. Extensive experiments show that our methods gain significant improvements on both automatic and human evaluation. We share our code and data for reproducibility.</abstract>
      <url hash="172215f4">2022.findings-naacl.162</url>
      <attachment type="software" hash="6c133b7d">2022.findings-naacl.162.software.zip</attachment>
      <bibkey>xing-etal-2022-balancing</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.162</doi>
      <video href="2022.findings-naacl.162.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/convai2">ConvAI2</pwcdataset>
    </paper>
    <paper id="163">
      <title>Semantic-Preserving Abstractive Text Summarization with <fixed-case>S</fixed-case>iamese Generative Adversarial Net</title>
      <author><first>Xin</first><last>Sheng</last></author>
      <author><first>Linli</first><last>Xu</last></author>
      <author><first>Yinlong</first><last>Xu</last></author>
      <author><first>Deqiang</first><last>Jiang</last></author>
      <author><first>Bo</first><last>Ren</last></author>
      <pages>2121-2132</pages>
      <abstract>We propose a novel siamese generative adversarial net for abstractive text summarization (SSPGAN), which can preserve the main semantics of the source text. Different from previous generative adversarial net based methods, SSPGAN is equipped with a siamese semantic-preserving discriminator, which can not only be trained to discriminate the machine-generated summaries from the human-summarized ones, but also ensure the semantic consistency between the source text and target summary. As a consequence of the min-max game between the generator and the siamese semantic-preserving discriminator, the generator can generate a summary that conveys the key content of the source text more accurately. Extensive experiments on several text summarization benchmarks in different languages demonstrate that the proposed model can achieve significant improvements over the state-of-the-art methods.</abstract>
      <url hash="63f3bd9b">2022.findings-naacl.163</url>
      <bibkey>sheng-etal-2022-semantic</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.163</doi>
    </paper>
    <paper id="164">
      <title>Towards Job-Transition-Tag Graph for a Better Job Title Representation Learning</title>
      <author><first>Jun</first><last>Zhu</last></author>
      <author><first>Celine</first><last>Hudelot</last></author>
      <pages>2133-2140</pages>
      <abstract>Works on learning job title representation are mainly based on <i>Job-Transition Graph</i>, built from the working history of talents. However, since these records are usually messy, this graph is very sparse, which affects the quality of the learned representation and hinders further analysis. To address this specific issue, we propose to enrich the graph with additional nodes that improve the quality of job title representation. Specifically, we construct <i>Job-Transition-Tag Graph</i>, a heterogeneous graph containing two types of nodes, i.e., job titles and tags (i.e., words related to job responsibilities or functionalities). Along this line, we reformulate job title representation learning as the task of learning node embedding on the <i>Job-Transition-Tag Graph</i>. Experiments on two datasets show the interest of our approach.</abstract>
      <url hash="cfa32fa9">2022.findings-naacl.164</url>
      <bibkey>zhu-hudelot-2022-towards</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.164</doi>
      <video href="2022.findings-naacl.164.mp4"/>
      <pwccode url="https://github.com/zhujun81/job_title_representation" additional="false">zhujun81/job_title_representation</pwccode>
    </paper>
    <paper id="165">
      <title><fixed-case>CL</fixed-case>-<fixed-case>R</fixed-case>e<fixed-case>LKT</fixed-case>: Cross-lingual Language Knowledge Transfer for Multilingual Retrieval Question Answering</title>
      <author><first>Peerat</first><last>Limkonchotiwat</last></author>
      <author><first>Wuttikorn</first><last>Ponwitayarat</last></author>
      <author><first>Can</first><last>Udomcharoenchaikit</last></author>
      <author><first>Ekapol</first><last>Chuangsuwanich</last></author>
      <author><first>Sarana</first><last>Nutanong</last></author>
      <pages>2141-2155</pages>
      <abstract>Cross-Lingual Retrieval Question Answering (CL-ReQA) is concerned with retrieving answer documents or passages to a question written in a different language. A common approach to CL-ReQA is to create a multilingual sentence embedding space such that question-answer pairs across different languages are close to each other. In this paper, we propose a novel CL-ReQA method utilizing the concept of language knowledge transfer and a new cross-lingual consistency training technique to create a multilingual embedding space for ReQA. To assess the effectiveness of our work, we conducted comprehensive experiments on CL-ReQA and a downstream task, machine reading QA. We compared our proposed method with the current state-of-the-art solutions across three public CL-ReQA corpora. Our method outperforms competitors in 19 out of 21 settings of CL-ReQA. When used with a downstream machine reading QA task, our method outperforms the best existing language-model-based method by 10% in F1 while being 10 times faster in sentence embedding computation. The code and models are available at <url>https://github.com/mrpeerat/CL-ReLKT</url>.</abstract>
      <url hash="e87e1624">2022.findings-naacl.165</url>
      <bibkey>limkonchotiwat-etal-2022-cl</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.165</doi>
      <video href="2022.findings-naacl.165.mp4"/>
      <pwccode url="https://github.com/mrpeerat/cl-relkt" additional="false">mrpeerat/cl-relkt</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mlqa">MLQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xquad">XQuAD</pwcdataset>
    </paper>
    <paper id="166">
      <title><fixed-case>BORT</fixed-case>: Back and Denoising Reconstruction for End-to-End Task-Oriented Dialog</title>
      <author><first>Haipeng</first><last>Sun</last></author>
      <author><first>Junwei</first><last>Bao</last></author>
      <author><first>Youzheng</first><last>Wu</last></author>
      <author><first>Xiaodong</first><last>He</last></author>
      <pages>2156-2170</pages>
      <abstract>A typical end-to-end task-oriented dialog system transfers context into dialog state, and upon which generates a response, which usually faces the problem of error propagation from both previously generated inaccurate dialog states and responses, especially in low-resource scenarios. To alleviate these issues, we propose BORT, a back and denoising reconstruction approach for end-to-end task-oriented dialog system. Squarely, to improve the accuracy of dialog states, back reconstruction is used to reconstruct the original input context from the generated dialog states since inaccurate dialog states cannot recover the corresponding input context. To enhance the denoising capability of the model to reduce the impact of error propagation, denoising reconstruction is used to reconstruct the corrupted dialog state and response. Extensive experiments conducted on MultiWOZ 2.0 and CamRest676 show the effectiveness of BORT. Furthermore, BORT demonstrates its advanced capabilities in the zero-shot domain and low-resource scenarios.</abstract>
      <url hash="6482fc9f">2022.findings-naacl.166</url>
      <attachment type="software" hash="2ed1190a">2022.findings-naacl.166.software.zip</attachment>
      <bibkey>sun-etal-2022-bort</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.166</doi>
      <video href="2022.findings-naacl.166.mp4"/>
      <pwccode url="https://github.com/jd-ai-research-nlp/bort" additional="false">jd-ai-research-nlp/bort</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multiwoz">MultiWOZ</pwcdataset>
    </paper>
    <paper id="167">
      <title>Multi-stage Distillation Framework for Cross-Lingual Semantic Similarity Matching</title>
      <author><first>Kunbo</first><last>Ding</last></author>
      <author><first>Weijie</first><last>Liu</last></author>
      <author><first>Yuejian</first><last>Fang</last></author>
      <author><first>Zhe</first><last>Zhao</last></author>
      <author><first>Qi</first><last>Ju</last></author>
      <author><first>Xuefeng</first><last>Yang</last></author>
      <author><first>Rong</first><last>Tian</last></author>
      <author><first>Zhu</first><last>Tao</last></author>
      <author><first>Haoyan</first><last>Liu</last></author>
      <author><first>Han</first><last>Guo</last></author>
      <author><first>Xingyu</first><last>Bai</last></author>
      <author><first>Weiquan</first><last>Mao</last></author>
      <author><first>Yudong</first><last>Li</last></author>
      <author><first>Weigang</first><last>Guo</last></author>
      <author><first>Taiqiang</first><last>Wu</last></author>
      <author><first>Ningyuan</first><last>Sun</last></author>
      <pages>2171-2181</pages>
      <abstract>Previous studies have proved that cross-lingual knowledge distillation can significantly improve the performance of pre-trained models for cross-lingual similarity matching tasks. However, the student model needs to be large in this operation. Otherwise, its performance will drop sharply, thus making it impractical to be deployed to memory-limited devices. To address this issue, we delve into cross-lingual knowledge distillation and propose a multi-stage distillation framework for constructing a small-size but high-performance cross-lingual model. In our framework, contrastive learning, bottleneck, and parameter recurrent strategies are delicately combined to prevent performance from being compromised during the compression process. The experimental results demonstrate that our method can compress the size of XLM-R and MiniLM by more than 50%, while the performance is only reduced by about 1%.</abstract>
      <url hash="428e8964">2022.findings-naacl.167</url>
      <bibkey>ding-etal-2022-multi</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.167</doi>
      <video href="2022.findings-naacl.167.mp4"/>
      <pwccode url="https://github.com/KB-Ding/Multi-stage-Distillaton-Framework" additional="false">KB-Ding/Multi-stage-Distillaton-Framework</pwccode>
    </paper>
    <paper id="168">
      <title>On the Limitations of Dataset Balancing: The Lost Battle Against Spurious Correlations</title>
      <author><first>Roy</first><last>Schwartz</last></author>
      <author><first>Gabriel</first><last>Stanovsky</last></author>
      <pages>2182-2194</pages>
      <abstract>Recent work has shown that deep learning models in NLP are highly sensitive to low-level correlations between simple features and specific output labels, leading to over-fitting and lack of generalization. To mitigate this problem, a common practice is to balance datasets by adding new instances or by filtering out “easy” instances (Sakaguchi et al., 2020), culminating in a recent proposal to eliminate single-word correlations altogether (Gardner et al., 2021). In this opinion paper, we identify that despite these efforts, increasingly-powerful models keep exploiting ever-smaller spurious correlations, and as a result even balancing all single-word features is insufficient for mitigating all of these correlations. In parallel, a truly balanced dataset may be bound to “throw the baby out with the bathwater” and miss important signal encoding common sense and world knowledge. We highlight several alternatives to dataset balancing, focusing on enhancing datasets with richer contexts, allowing models to abstain and interact with users, and turning from large-scale fine-tuning to zero- or few-shot setups.</abstract>
      <url hash="492c6626">2022.findings-naacl.168</url>
      <bibkey>schwartz-stanovsky-2022-limitations</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.168</doi>
      <video href="2022.findings-naacl.168.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hellaswag">HellaSwag</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/record">ReCoRD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/swag">SWAG</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering-v2-0">Visual Question Answering v2.0</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/winogrande">WinoGrande</pwcdataset>
    </paper>
    <paper id="169">
      <title>Specializing Pre-trained Language Models for Better Relational Reasoning via Network Pruning</title>
      <author><first>Siyu</first><last>Ren</last></author>
      <author><first>Kenny</first><last>Zhu</last></author>
      <pages>2195-2207</pages>
      <abstract>Pretrained masked language models (PLMs) were shown to be inheriting a considerable amount of relational knowledge from the source corpora. In this paper, we present an in-depth and comprehensive study concerning specializing PLMs into relational models from the perspective of network pruning. We show that it is possible to find subnetworks capable of representing grounded commonsense relations at non-trivial sparsity while being more generalizable than original PLMs in scenarios requiring knowledge of single or multiple commonsense relations.</abstract>
      <url hash="79d46bff">2022.findings-naacl.169</url>
      <attachment type="software" hash="4cb51ea2">2022.findings-naacl.169.software.zip</attachment>
      <bibkey>ren-zhu-2022-specializing</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.169</doi>
      <pwccode url="https://github.com/drsy/lamp" additional="false">drsy/lamp</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/copa">COPA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/commonsenseqa">CommonsenseQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cosmosqa">CosmosQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/lama">LAMA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/swag">SWAG</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wsc">WSC</pwcdataset>
    </paper>
    <paper id="170">
      <title><fixed-case>D</fixed-case>2<fixed-case>GCLF</fixed-case>: Document-to-Graph Classifier for Legal Document Classification</title>
      <author><first>Qiqi</first><last>Wang</last></author>
      <author><first>Kaiqi</first><last>Zhao</last></author>
      <author><first>Robert</first><last>Amor</last></author>
      <author><first>Benjamin</first><last>Liu</last></author>
      <author><first>Ruofan</first><last>Wang</last></author>
      <pages>2208-2221</pages>
      <abstract>Legal document classification is an essential task in law intelligence to automate the labor-intensive law case filing process. Unlike traditional document classification problems, legal documents should be classified by reasons and facts instead of topics. We propose a Document-to-Graph Classifier (D2GCLF), which extracts facts as relations between key participants in the law case and represents a legal document with four relation graphs. Each graph is responsible for capturing different relations between the litigation participants. We further develop a graph attention network on top of the four relation graphs to classify the legal documents. Experiments on a real-world legal document dataset show that D2GCLF outperforms the state-of-the-art methods in terms of accuracy.</abstract>
      <url hash="24b6d77e">2022.findings-naacl.170</url>
      <bibkey>wang-etal-2022-d2gclf</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.170</doi>
      <video href="2022.findings-naacl.170.mp4"/>
    </paper>
    <paper id="171">
      <title>A Label-Aware Autoregressive Framework for Cross-Domain <fixed-case>NER</fixed-case></title>
      <author><first>Jinpeng</first><last>Hu</last></author>
      <author><first>He</first><last>Zhao</last></author>
      <author><first>Dan</first><last>Guo</last></author>
      <author><first>Xiang</first><last>Wan</last></author>
      <author><first>Tsung-Hui</first><last>Chang</last></author>
      <pages>2222-2232</pages>
      <abstract>Cross-domain named entity recognition (NER) aims to borrow the entity information from the source domain to help the entity recognition in the target domain with limited labeled data. Despite the promising performance of existing approaches, most of them focus on reducing the discrepancy of token representation between source and target domains, while the transfer of the valuable label information is often not explicitly considered or even ignored. Therefore, we propose a novel autoregressive framework to advance cross-domain NER by first enhancing the relationship between labels and tokens and then further improving the transferability of label information. Specifically, we associate each label with an embedding vector, and for each token, we utilize a bidirectional LSTM (Bi-LSTM) to encode the labels of its previous tokens for modeling internal context information and label dependence. Afterward, we propose a Bi-Attention module that merges the token representation from a pre-trained model and the label features from the Bi-LSTM as the label-aware information, which is concatenated to the token representation to facilitate cross-domain NER. In doing so, label information contained in the embedding vectors can be effectively transferred to the target domain, and Bi-LSTM can further model the label relationship among different domains by pre-train and then fine-tune setting. Experimental results on several datasets confirm the effectiveness of our model, where our model achieves significant improvements over the state of the arts.</abstract>
      <url hash="109f2fca">2022.findings-naacl.171</url>
      <attachment type="software" hash="e06d148a">2022.findings-naacl.171.software.zip</attachment>
      <bibkey>hu-etal-2022-label</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.171</doi>
      <video href="2022.findings-naacl.171.mp4"/>
      <pwccode url="https://github.com/jinpeng01/laner" additional="false">jinpeng01/laner</pwccode>
    </paper>
    <paper id="172">
      <title>A Dog Is Passing Over The Jet? A Text-Generation Dataset for <fixed-case>K</fixed-case>orean Commonsense Reasoning and Evaluation</title>
      <author><first>Jaehyung</first><last>Seo</last></author>
      <author><first>Seounghoon</first><last>Lee</last></author>
      <author><first>Chanjun</first><last>Park</last></author>
      <author><first>Yoonna</first><last>Jang</last></author>
      <author><first>Hyeonseok</first><last>Moon</last></author>
      <author><first>Sugyeong</first><last>Eo</last></author>
      <author><first>Seonmin</first><last>Koo</last></author>
      <author><first>Heuiseok</first><last>Lim</last></author>
      <pages>2233-2249</pages>
      <abstract>Recent natural language understanding (NLU) research on the Korean language has been vigorously maturing with the advancements of pretrained language models and datasets. However, Korean pretrained language models still struggle to generate a short sentence with a given condition based on compositionality and commonsense reasoning (i.e., generative commonsense reasoning). The two major challenges are inadequate data resources to develop generative commonsense reasoning regarding Korean linguistic features and to evaluate language models which are necessary for natural language generation (NLG). To solve these problems, we propose a text-generation dataset for Korean generative commonsense reasoning and language model evaluation. In this work, a semi-automatic dataset construction approach filters out contents inexplicable to commonsense, ascertains quality, and reduces the cost of building the dataset. We also present an in-depth analysis of the generation results of language models with various evaluation metrics along with human-annotated scores. The whole dataset is publicly available at (<url>https://aihub.or.kr/opendata/korea-university</url>).</abstract>
      <url hash="1a666ddb">2022.findings-naacl.172</url>
      <attachment type="software" hash="1132701a">2022.findings-naacl.172.software.zip</attachment>
      <bibkey>seo-etal-2022-dog</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.172</doi>
      <video href="2022.findings-naacl.172.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/commongen">CommonGen</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/commonsenseqa">CommonsenseQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/korsts">KorSTS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">MS COCO</pwcdataset>
    </paper>
    <paper id="173">
      <title>Improve Discourse Dependency Parsing with Contextualized Representations</title>
      <author><first>Yifei</first><last>Zhou</last></author>
      <author><first>Yansong</first><last>Feng</last></author>
      <pages>2250-2261</pages>
      <abstract>Previous works show that discourse analysis benefits from modeling intra- and inter-sentential levels separately, where proper representations for text units of different granularities are desired to capture both the information of the text units and their relation to the context. In this paper, we propose to take advantage of transformers to encode different contextualized representations of units of different levels to dynamically capture the information required for discourse dependency analysis on intra- and inter-sentential levels. Motivated by the observation of writing patterns shared across articles to improve discourse analysis, we propose to design sequence labeling methods to take advantage of such structural information from the context that substantially outperforms traditional direct classification methods. Experiments show that our model achieves state-of-the-art results on both English and Chinese datasets.</abstract>
      <url hash="203e4a41">2022.findings-naacl.173</url>
      <bibkey>zhou-feng-2022-improve</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.173</doi>
      <video href="2022.findings-naacl.173.mp4"/>
    </paper>
    <paper id="174">
      <title><fixed-case>L</fixed-case>i<fixed-case>ST</fixed-case>: Lite Prompted Self-training Makes Parameter-efficient Few-shot Learners</title>
      <author><first>Yaqing</first><last>Wang</last></author>
      <author><first>Subhabrata</first><last>Mukherjee</last></author>
      <author><first>Xiaodong</first><last>Liu</last></author>
      <author><first>Jing</first><last>Gao</last></author>
      <author><first>Ahmed</first><last>Awadallah</last></author>
      <author><first>Jianfeng</first><last>Gao</last></author>
      <pages>2262-2281</pages>
      <abstract>We present a new method LiST for efficient fine-tuning of large pre-trained language models (PLMs) in few-shot learning settings. LiST improves over recent methods that adopt prompt-based fine-tuning (FN) using two key techniques. The first is the use of self-training to leverage large amounts of unlabeled data for prompt-based FN in few-shot settings. We use self-training in conjunction with meta-learning for re-weighting noisy pseudo-prompt labels. Traditionally, self-training is expensive as it requires updating all the model parameters repetitively. Therefore, we use a second technique for light-weight fine-tuning where we introduce a small number of task-specific parameters that are fine-tuned during self-training while keeping the PLM encoder frozen. Our experiments show that LiST can effectively leverage unlabeled data to improve the model performance for few-shot learning. Additionally, the finetuning process is efficient as it only updates a small percentage of the parameters and the overall model footprint is reduced since several tasks can share a common PLM encoder as backbone. We present a comprehensive study on six NLU tasks to validate the effectiveness of LiST. The results show that LiST improves by 35% over classic fine-tuning methods and 6% over prompt-based FN with 96% reduction in number of trainable parameters when fine-tuned with no more than 30 labeled examples from each task. With only 14M tunable parameters, LiST outperforms GPT-3 in-context learning by 33% on few-shot NLU tasks</abstract>
      <url hash="090a2551">2022.findings-naacl.174</url>
      <attachment type="software" hash="5093f91c">2022.findings-naacl.174.software.zip</attachment>
      <bibkey>wang-etal-2022-list</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.174</doi>
      <pwccode url="https://github.com/microsoft/list" additional="false">microsoft/list</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mpqa-opinion-corpus">MPQA Opinion Corpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst-2">SST-2</pwcdataset>
    </paper>
    <paper id="175">
      <title><fixed-case>CLMLF</fixed-case>:A Contrastive Learning and Multi-Layer Fusion Method for Multimodal Sentiment Detection</title>
      <author><first>Zhen</first><last>Li</last></author>
      <author><first>Bing</first><last>Xu</last></author>
      <author><first>Conghui</first><last>Zhu</last></author>
      <author><first>Tiejun</first><last>Zhao</last></author>
      <pages>2282-2294</pages>
      <abstract>Compared with unimodal data, multimodal data can provide more features to help the model analyze the sentiment of data. Previous research works rarely consider token-level feature fusion, and few works explore learning the common features related to sentiment in multimodal data to help the model fuse multimodal features. In this paper, we propose a Contrastive Learning and Multi-Layer Fusion (CLMLF) method for multimodal sentiment detection. Specifically, we first encode text and image to obtain hidden representations, and then use a multi-layer fusion module to align and fuse the token-level features of text and image. In addition to the sentiment analysis task, we also designed two contrastive learning tasks, label based contrastive learning and data based contrastive learning tasks, which will help the model learn common features related to sentiment in multimodal data. Extensive experiments conducted on three publicly available multimodal datasets demonstrate the effectiveness of our approach for multimodal sentiment detection compared with existing methods. The codes are available for use at https: //github.com/Link-Li/CLMLF</abstract>
      <url hash="4a08350c">2022.findings-naacl.175</url>
      <bibkey>li-etal-2022-clmlf</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.175</doi>
      <video href="2022.findings-naacl.175.mp4"/>
      <pwccode url="https://github.com/link-li/clmlf" additional="false">link-li/clmlf</pwccode>
    </paper>
    <paper id="176">
      <title>Weakly Supervised Text Classification using Supervision Signals from a Language Model</title>
      <author><first>Ziqian</first><last>Zeng</last></author>
      <author><first>Weimin</first><last>Ni</last></author>
      <author><first>Tianqing</first><last>Fang</last></author>
      <author><first>Xiang</first><last>Li</last></author>
      <author><first>Xinran</first><last>Zhao</last></author>
      <author><first>Yangqiu</first><last>Song</last></author>
      <pages>2295-2305</pages>
      <abstract>Solving text classification in a weakly supervised manner is important for real-world applications where human annotations are scarce. In this paper, we propose to query a masked language model with cloze style prompts to obtain supervision signals. We design a prompt which combines the document itself and “this article is talking about [MASK].” A masked language model can generate words for the [MASK] token. The generated words which summarize the content of a document can be utilized as supervision signals. We propose a latent variable model to learn a word distribution learner which associates generated words to pre-defined categories and a document classifier simultaneously without using any annotated data. Evaluation on three datasets, AGNews, 20Newsgroups, and UCINews, shows that our method can outperform baselines by 2%, 4%, and 3%.</abstract>
      <url hash="2e15f44c">2022.findings-naacl.176</url>
      <bibkey>zeng-etal-2022-weakly</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.176</doi>
      <video href="2022.findings-naacl.176.mp4"/>
      <pwccode url="https://github.com/hkust-knowcomp/wddc" additional="false">hkust-knowcomp/wddc</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
    </paper>
    <paper id="177">
      <title>Analytical Reasoning of Text</title>
      <author><first>Wanjun</first><last>Zhong</last></author>
      <author><first>Siyuan</first><last>Wang</last></author>
      <author><first>Duyu</first><last>Tang</last></author>
      <author><first>Zenan</first><last>Xu</last></author>
      <author><first>Daya</first><last>Guo</last></author>
      <author><first>Yining</first><last>Chen</last></author>
      <author><first>Jiahai</first><last>Wang</last></author>
      <author><first>Jian</first><last>Yin</last></author>
      <author><first>Ming</first><last>Zhou</last></author>
      <author><first>Nan</first><last>Duan</last></author>
      <pages>2306-2319</pages>
      <abstract>Analytical reasoning is an essential and challenging task that requires a system to analyze a scenario involving a set of particular circumstances and perform reasoning over it to make conclusions. However, current neural models with implicit reasoning ability struggle to solve this task. In this paper, we study the challenge of analytical reasoning of text and collect a new dataset consisting of questions from the Law School Admission Test from 1991 to 2016. We analyze what knowledge understanding and reasoning abilities are required to do well on this task, and present an approach dubbed ARM. It extracts knowledge such as participants and facts from the context. Such knowledge are applied to an inference engine to deduce legitimate solutions for drawing conclusions. In our experiments, we find that ubiquitous pre-trained models struggle to deal with this task as their performance is close to random guess. Results show that ARM outperforms pre-trained models significantly. Moreover, we demonstrate that ARM has better explicit interpretable reasoning ability.</abstract>
      <url hash="5d020df6">2022.findings-naacl.177</url>
      <bibkey>zhong-etal-2022-analytical</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.177</doi>
      <video href="2022.findings-naacl.177.mp4"/>
      <pwccode url="https://github.com/zhongwanjun/AR-LSAT" additional="false">zhongwanjun/AR-LSAT</pwccode>
    </paper>
    <paper id="178">
      <title>Denoising Neural Network for News Recommendation with Positive and Negative Implicit Feedback</title>
      <author><first>Yunfan</first><last>Hu</last></author>
      <author><first>Zhaopeng</first><last>Qiu</last></author>
      <author><first>Xian</first><last>Wu</last></author>
      <pages>2320-2329</pages>
      <abstract>News recommendation is different from movie or e-commercial recommendation as people usually do not grade the news. Therefore, user feedback for news is always implicit (click behavior, reading time, etc). Inevitably, there are noises in implicit feedback. On one hand, the user may exit immediately after clicking the news as he dislikes the news content, leaving the noise in his positive implicit feedback; on the other hand, the user may be recommended multiple interesting news at the same time and only click one of them, producing the noise in his negative implicit feedback. Opposite implicit feedback could construct more integrated user preferences and help each other to minimize the noise influence. Previous works on news recommendation only used positive implicit feedback and suffered from the noise impact. In this paper, we propose a denoising neural network for news recommendation with positive and negative implicit feedback, named DRPN. DRPN utilizes both feedback for recommendation with a module to denoise both positive and negative implicit feedback to further enhance the performance. Experiments on the real-world large-scale dataset demonstrate the state-of-the-art performance of DRPN.</abstract>
      <url hash="8e92db53">2022.findings-naacl.178</url>
      <attachment type="software" hash="62a30a45">2022.findings-naacl.178.software.zip</attachment>
      <bibkey>hu-etal-2022-denoising</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.178</doi>
      <video href="2022.findings-naacl.178.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/mind">MIND</pwcdataset>
    </paper>
    <paper id="179">
      <title>Continual Machine Reading Comprehension via Uncertainty-aware Fixed Memory and Adversarial Domain Adaptation</title>
      <author><first>Zhijing</first><last>Wu</last></author>
      <author><first>Hua</first><last>Xu</last></author>
      <author><first>Jingliang</first><last>Fang</last></author>
      <author><first>Kai</first><last>Gao</last></author>
      <pages>2330-2339</pages>
      <abstract>Continual Machine Reading Comprehension aims to incrementally learn from a continuous data stream across time without access the previous seen data, which is crucial for the development of real-world MRC systems. However, it is a great challenge to learn a new domain incrementally without catastrophically forgetting previous knowledge. In this paper, MA-MRC, a continual MRC model with uncertainty-aware fixed Memory and Adversarial domain adaptation, is proposed. In MA-MRC, a fixed size memory stores a small number of samples in previous domain data along with an uncertainty-aware updating strategy when new domain data arrives. For incremental learning, MA-MRC not only keeps a stable understanding by learning both memory and new domain data, but also makes full use of the domain adaptation relationship between them by adversarial learning strategy. The experimental results show that MA-MRC is superior to strong baselines and has a substantial incremental learning ability without catastrophically forgetting under two different continual MRC settings.</abstract>
      <url hash="5ec69ee1">2022.findings-naacl.179</url>
      <bibkey>wu-etal-2022-continual</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.179</doi>
      <video href="2022.findings-naacl.179.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsqa">NewsQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
    </paper>
    <paper id="180">
      <title>Jointly Learning Guidance Induction and Faithful Summary Generation via Conditional Variational Autoencoders</title>
      <author><first>Wang</first><last>Xu</last></author>
      <author><first>Tiejun</first><last>Zhao</last></author>
      <pages>2340-2350</pages>
      <abstract>Abstractive summarization can generate high quality results with the development of the neural network. However, generating factual consistency summaries is a challenging task for abstractive summarization. Recent studies extract the additional information with off-the-shelf tools from the source document as a clue to guide the summary generation, which shows effectiveness to improve the faithfulness. Unlike these work, we present a novel framework based on conditional variational autoencoders, which induces the guidance information and generates the summary equipment with the guidance synchronously. Experiments on XSUM and CNNDM dataset show that our approach can generate relevant and fluent summaries which is more faithful than the existing state-of-the-art approaches, according to multiple factual consistency metrics.</abstract>
      <url hash="a5ce3be4">2022.findings-naacl.180</url>
      <bibkey>xu-zhao-2022-jointly</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.180</doi>
    </paper>
    <paper id="181">
      <title>Context-Aware Language Modeling for Goal-Oriented Dialogue Systems</title>
      <author><first>Charlie</first><last>Snell</last></author>
      <author><first>Sherry</first><last>Yang</last></author>
      <author><first>Justin</first><last>Fu</last></author>
      <author><first>Yi</first><last>Su</last></author>
      <author><first>Sergey</first><last>Levine</last></author>
      <pages>2351-2366</pages>
      <abstract>Goal-oriented dialogue systems face a trade-off between fluent language generation and task-specific control. While supervised learning with large language models is capable of producing realistic text, how to steer such responses towards completing a specific task without sacrificing language quality remains an open question. In this work, we formulate goal-oriented dialogue as a partially observed Markov decision process, interpreting the language model as a representation of both the dynamics and the policy. This view allows us to extend techniques from learning-based control, such as task relabeling, to derive a simple and effective method to finetune language models in a goal-aware way, leading to significantly improved task performance. We additionally introduce a number of training strategies that serve to better focus the model on the task at hand. We evaluate our method, Context-Aware Language Models (CALM), on a practical flight-booking task using AirDialogue. Empirically, CALM outperforms the state-of-the-art method by 7% in terms of task success, matching human-level task performance.</abstract>
      <url hash="bef8cbe1">2022.findings-naacl.181</url>
      <bibkey>snell-etal-2022-context</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.181</doi>
      <video href="2022.findings-naacl.181.mp4"/>
    </paper>
    <paper id="182">
      <title>Am <fixed-case>I</fixed-case> Me or You? State-of-the-Art Dialogue Models Cannot Maintain an Identity</title>
      <author><first>Kurt</first><last>Shuster</last></author>
      <author><first>Jack</first><last>Urbanek</last></author>
      <author><first>Arthur</first><last>Szlam</last></author>
      <author><first>Jason</first><last>Weston</last></author>
      <pages>2367-2387</pages>
      <abstract>State-of-the-art dialogue models still often stumble with regards to factual accuracy and self-contradiction. Anecdotally, they have been observed to fail to maintain character identity throughout discourse; and more specifically, may take on the role of their interlocutor. In this work we formalize and quantify this deficiency, and show experimentally through human evaluations that this is indeed a problem. In contrast, we show that discriminative models trained specifically to recognize who is speaking can perform well; and further, these can be used as automated metrics. Finally, we evaluate a wide variety of mitigation methods, including changes to model architecture, training protocol, and decoding strategy. Our best models reduce mistaken identity issues by nearly 65% according to human annotators, while simultaneously improving engagingness. Despite these results, we find that maintaining character identity still remains a challenging problem.</abstract>
      <url hash="66900e4c">2022.findings-naacl.182</url>
      <bibkey>shuster-etal-2022-state</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.182</doi>
      <video href="2022.findings-naacl.182.mp4"/>
    </paper>
    <paper id="183">
      <title>Unsupervised Domain Adaptation for Question Generation with <fixed-case>D</fixed-case>omain<fixed-case>D</fixed-case>ata Selection and Self-training</title>
      <author><first>Peide</first><last>Zhu</last></author>
      <author><first>Claudia</first><last>Hauff</last></author>
      <pages>2388-2401</pages>
      <abstract>Question generation (QG) approaches based on large neural models require (i) large-scale and (ii) high-quality training data. These two requirements pose difficulties for specific application domains where training data is expensive and difficult to obtain. The trained QG models’ effectiveness can degrade significantly when they are applied on a different domain due to domain shift. In this paper, we explore an <i>unsupervised domain adaptation</i> approach to combat the lack of training data and domain shift issue with domain data selection and self-training. We first present a novel answer-aware strategy for domain data selection to select data with the most similarity to a new domain. The selected data are then used as pseudo-in-domain data to retrain the QG model. We then present generation confidence guided self-training with two generation confidence modeling methods (i) generated questions’ perplexity and (ii) the fluency score. We test our approaches on three large public datasets with different domain similarities, using a transformer-based pre-trained QG model. The results show that our proposed approaches outperform the baselines, and show the viability of unsupervised domain adaptation with answer-aware data selection and self-training on the QG task.</abstract>
      <url hash="dc971234">2022.findings-naacl.183</url>
      <bibkey>zhu-hauff-2022-unsupervised</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.183</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sciq">SciQ</pwcdataset>
    </paper>
    <paper id="184">
      <title><fixed-case>CCQA</fixed-case>: A New Web-Scale Question Answering Dataset for Model Pre-Training</title>
      <author><first>Patrick</first><last>Huber</last></author>
      <author><first>Armen</first><last>Aghajanyan</last></author>
      <author><first>Barlas</first><last>Oguz</last></author>
      <author><first>Dmytro</first><last>Okhonko</last></author>
      <author><first>Scott</first><last>Yih</last></author>
      <author><first>Sonal</first><last>Gupta</last></author>
      <author><first>Xilun</first><last>Chen</last></author>
      <pages>2402-2420</pages>
      <abstract>We propose a novel open-domain question-answering dataset based on the Common Crawl project. With a previously unseen number of around 130 million multilingual question-answer pairs (including about 60 million English data-points), we use our large-scale, natural, diverse and high-quality corpus to in-domain pre-train popular language models for the task of question-answering. In our experiments, we find that our Common Crawl Question Answering dataset (CCQA) achieves promising results in zero-shot, low resource and fine-tuned settings across multiple tasks, models and benchmarks.</abstract>
      <url hash="ce92cb51">2022.findings-naacl.184</url>
      <bibkey>huber-etal-2022-ccqa</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.184</doi>
      <video href="2022.findings-naacl.184.mp4"/>
      <pwccode url="https://github.com/facebookresearch/CCQA" additional="false">facebookresearch/CCQA</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ccqa">CCQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cc100">CC100</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ccnet">CCNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/eli5">ELI5</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/gooaq">GooAQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
    </paper>
    <paper id="185">
      <title>The Case for a Single Model that can Both Generate Continuations and Fill-in-the-Blank</title>
      <author><first>Daphne</first><last>Ippolito</last></author>
      <author><first>Liam</first><last>Dugan</last></author>
      <author><first>Emily</first><last>Reif</last></author>
      <author><first>Ann</first><last>Yuan</last></author>
      <author><first>Andy</first><last>Coenen</last></author>
      <author><first>Chris</first><last>Callison-Burch</last></author>
      <pages>2421-2432</pages>
      <abstract>The task of inserting text into a specified position in a passage, known as fill in the blank (FitB), is useful for a variety of applications where writers interact with a natural language generation (NLG) system to craft text. While previous work has tackled this problem with models trained specifically to do fill in the blank, a more useful model is one that can effectively perform _both_ FitB and continuation tasks. In this work, we evaluate the feasibility of using a single model to do both tasks. We show that models pre-trained with a FitB-style objective are capable of both tasks, while models pre-trained for continuation are not. Finally, we show how these models can be easily finetuned to allow for fine-grained control over the length and word choice of the generation.</abstract>
      <url hash="b9e39c3f">2022.findings-naacl.185</url>
      <bibkey>ippolito-etal-2022-case</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.185</doi>
      <video href="2022.findings-naacl.185.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/writingprompts">WritingPrompts</pwcdataset>
    </paper>
    <paper id="186">
      <title>Learning Discriminative Representations for Open Relation Extraction with Instance Ranking and Label Calibration</title>
      <author><first>Shusen</first><last>Wang</last></author>
      <author><first>Bin</first><last>Duan</last></author>
      <author><first>Yanan</first><last>Wu</last></author>
      <author><first>Yajing</first><last>Xu</last></author>
      <pages>2433-2438</pages>
      <abstract>Open relation extraction is the task to extract relational facts without pre-defined relation types from open-domain corpora. However, since there are some hard or semi-hard instances sharing similar context and entity information but belonging to different underlying relation, current OpenRE methods always cluster them into the same relation type. In this paper, we propose a novel method based on Instance Ranking and Label Calibration strategies (IRLC) to learn discriminative representations for open relation extraction. Due to lacking the original instance label, we provide three surrogate strategies to generate the positive, hard negative, and semi-hard negative instances for the original instance. Instance ranking aims to refine the relational feature space by pushing the hard and semi-hard negative instances apart from the original instance with different margins and pulling the original instance and its positive instance together. To refine the cluster probability distributions of these instances, we introduce a label calibration strategy to model the constraint relationship between instances. Experimental results on two public datasets demonstrate that our proposed method can significantly outperform the previous state-of-the-art methods.</abstract>
      <url hash="26f43692">2022.findings-naacl.186</url>
      <attachment type="software" hash="f0f874dd">2022.findings-naacl.186.software.zip</attachment>
      <bibkey>wang-etal-2022-learning-discriminative</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.186</doi>
      <video href="2022.findings-naacl.186.mp4"/>
      <pwccode url="https://github.com/shusenwang/naacl2022-irlc" additional="false">shusenwang/naacl2022-irlc</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/t-rex">T-REx</pwcdataset>
    </paper>
    <paper id="187">
      <title>Textual Entailment for Event Argument Extraction: Zero- and Few-Shot with Multi-Source Learning</title>
      <author><first>Oscar</first><last>Sainz</last></author>
      <author><first>Itziar</first><last>Gonzalez-Dios</last></author>
      <author><first>Oier</first><last>Lopez de Lacalle</last></author>
      <author><first>Bonan</first><last>Min</last></author>
      <author><first>Eneko</first><last>Agirre</last></author>
      <pages>2439-2455</pages>
      <abstract>Recent work has shown that NLP tasks such as Relation Extraction (RE) can be recasted as a Textual Entailment tasks using verbalizations, with strong performance in zero-shot and few-shot settings thanks to pre-trained entailment models. The fact that relations in current RE datasets are easily verbalized casts doubts on whether entailment would be effective in more complex tasks. In this work we show that entailment is also effective in Event Argument Extraction (EAE), reducing the need of manual annotation to 50% and 20% in ACE and WikiEvents, respectively, while achieving the same performance as with full training. More importantly, we show that recasting EAE as entailment alleviates the dependency on schemas, which has been a roadblock for transferring annotations between domains. Thanks to entailment, the multi-source transfer between ACE and WikiEvents further reduces annotation down to 10% and 5% (respectively) of the full training without transfer. Our analysis shows that key to good results is the use of several entailment datasets to pre-train the entailment model. Similar to previous approaches, our method requires a small amount of effort for manual verbalization: only less than 15 minutes per event argument types is needed; comparable results can be achieved from users of different level of expertise.</abstract>
      <url hash="4e026391">2022.findings-naacl.187</url>
      <bibkey>sainz-etal-2022-textual</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.187</doi>
      <video href="2022.findings-naacl.187.mp4"/>
      <pwccode url="https://github.com/osainz59/Ask2Transformers" additional="false">osainz59/Ask2Transformers</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ace-2005">ACE 2005</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/anli">ANLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikievents">WikiEvents</pwcdataset>
    </paper>
    <paper id="188">
      <title><fixed-case>RCL</fixed-case>: Relation Contrastive Learning for Zero-Shot Relation Extraction</title>
      <author><first>Shusen</first><last>Wang</last></author>
      <author><first>Bosen</first><last>Zhang</last></author>
      <author><first>Yajing</first><last>Xu</last></author>
      <author><first>Yanan</first><last>Wu</last></author>
      <author><first>Bo</first><last>Xiao</last></author>
      <pages>2456-2468</pages>
      <abstract>Zero-shot relation extraction aims to identify novel relations which cannot be observed at the training stage. However, it still faces some challenges since the unseen relations of instances are similar or the input sentences have similar entities, the unseen relation representations from different categories tend to overlap and lead to errors. In this paper, we propose a novel Relation Contrastive Learning framework (RCL) to mitigate above two types of similar problems: Similar Relations and Similar Entities. By jointly optimizing a contrastive instance loss with a relation classification loss on seen relations, RCL can learn subtle difference between instances and achieve better separation between different relation categories in the representation space simultaneously. Especially in contrastive instance learning, the dropout noise as data augmentation is adopted to amplify the semantic difference between similar instances without breaking relation representation, so as to promote model to learn more effective representations. Experiments conducted on two well-known datasets show that RCL can significantly outperform previous state-of-the-art methods. Moreover, if the seen relations are insufficient, RCL can also obtain comparable results with the model trained on the full training set, showing the robustness of our approach.</abstract>
      <url hash="3d06103b">2022.findings-naacl.188</url>
      <bibkey>wang-etal-2022-rcl</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.188</doi>
      <video href="2022.findings-naacl.188.mp4"/>
      <pwccode url="https://github.com/shusenwang/naacl2022-rcl" additional="false">shusenwang/naacl2022-rcl</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel">FewRel</pwcdataset>
    </paper>
    <paper id="189">
      <title>Latent Group Dropout for Multilingual and Multidomain Machine Translation</title>
      <author><first>Minh-Quang</first><last>Pham</last></author>
      <author><first>François</first><last>Yvon</last></author>
      <author><first>Josep</first><last>Crego</last></author>
      <pages>2469-2481</pages>
      <abstract>Multidomain and multilingual machine translation often rely on parameter sharing strategies, where large portions of the network are meant to capture the commonalities of the tasks at hand, while smaller parts are reserved to model the peculiarities of a language or a domain. In adapter-based approaches, these strategies are hardcoded in the network architecture, independent of the similarities between tasks. In this work, we propose a new method to better take advantage of these similarities, using a latent-variable model. We also develop new techniques to train this model end-to-end and report experimental results showing that the learned patterns are both meaningful and yield improved translation performance without any increase of the model size.</abstract>
      <url hash="275a004e">2022.findings-naacl.189</url>
      <bibkey>pham-etal-2022-latent</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.189</doi>
      <video href="2022.findings-naacl.189.mp4"/>
    </paper>
    <paper id="190">
      <title><fixed-case>ATP</fixed-case>: <fixed-case>AMR</fixed-case>ize Then Parse! Enhancing <fixed-case>AMR</fixed-case> Parsing with <fixed-case>P</fixed-case>seudo<fixed-case>AMR</fixed-case>s</title>
      <author><first>Liang</first><last>Chen</last></author>
      <author><first>Peiyi</first><last>Wang</last></author>
      <author><first>Runxin</first><last>Xu</last></author>
      <author><first>Tianyu</first><last>Liu</last></author>
      <author><first>Zhifang</first><last>Sui</last></author>
      <author><first>Baobao</first><last>Chang</last></author>
      <pages>2482-2496</pages>
      <abstract>As Abstract Meaning Representation (AMR) implicitly involves compound semantic annotations, we hypothesize auxiliary tasks which are semantically or formally related can better enhance AMR parsing. We find that 1) Semantic role labeling (SRL) and dependency parsing (DP), would bring more performance gain than other tasks e.g. MT and summarization in the text-to-AMR transition even with much less data. 2) To make a better fit for AMR, data from auxiliary tasks should be properly “AMRized” to PseudoAMR before training. Knowledge from shallow level parsing tasks can be better transferred to AMR Parsing with structure transform. 3) Intermediate-task learning is a better paradigm to introduce auxiliary tasks to AMR parsing, compared to multitask learning. From an empirical perspective, we propose a principled method to involve auxiliary tasks to boost AMR parsing. Extensive experiments show that our method achieves new state-of-the-art performance on different benchmarks especially in topology-related scores. Code and models are released at <url>https://github.com/PKUnlp-icler/ATP</url>.</abstract>
      <url hash="b7717204">2022.findings-naacl.190</url>
      <bibkey>chen-etal-2022-atp</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.190</doi>
      <pwccode url="https://github.com/chenllliang/atp" additional="true">chenllliang/atp</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bio">Bio</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ldc2017t10">LDC2017T10</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ldc2020t02">LDC2020T02</pwcdataset>
    </paper>
    <paper id="191">
      <title><fixed-case>T</fixed-case>a<fixed-case>CL</fixed-case>: Improving <fixed-case>BERT</fixed-case> Pre-training with Token-aware Contrastive Learning</title>
      <author><first>Yixuan</first><last>Su</last></author>
      <author><first>Fangyu</first><last>Liu</last></author>
      <author><first>Zaiqiao</first><last>Meng</last></author>
      <author><first>Tian</first><last>Lan</last></author>
      <author><first>Lei</first><last>Shu</last></author>
      <author><first>Ehsan</first><last>Shareghi</last></author>
      <author><first>Nigel</first><last>Collier</last></author>
      <pages>2497-2507</pages>
      <abstract>Masked language models (MLMs) such as BERT have revolutionized the field of Natural Language Understanding in the past few years. However, existing pre-trained MLMs often output an anisotropic distribution of token representations that occupies a narrow subset of the entire representation space. Such token representations are not ideal, especially for tasks that demand discriminative semantic meanings of distinct tokens. In this work, we propose TaCL (Token-aware Contrastive Learning), a novel continual pre-training approach that encourages BERT to learn an isotropic and discriminative distribution of token representations. TaCL is fully unsupervised and requires no additional data. We extensively test our approach on a wide range of English and Chinese benchmarks. The results show that TaCL brings consistent and notable improvements over the original BERT model. Furthermore, we conduct detailed analysis to reveal the merits and inner-workings of our approach.</abstract>
      <url hash="58f555bf">2022.findings-naacl.191</url>
      <bibkey>su-etal-2022-tacl</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.191</doi>
      <video href="2022.findings-naacl.191.mp4"/>
      <pwccode url="https://github.com/yxuansu/tacl" additional="true">yxuansu/tacl</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
    </paper>
    <paper id="192">
      <title><fixed-case>MTG</fixed-case>: A Benchmark Suite for Multilingual Text Generation</title>
      <author><first>Yiran</first><last>Chen</last></author>
      <author><first>Zhenqiao</first><last>Song</last></author>
      <author><first>Xianze</first><last>Wu</last></author>
      <author><first>Danqing</first><last>Wang</last></author>
      <author><first>Jingjing</first><last>Xu</last></author>
      <author><first>Jiaze</first><last>Chen</last></author>
      <author><first>Hao</first><last>Zhou</last></author>
      <author><first>Lei</first><last>Li</last></author>
      <pages>2508-2527</pages>
      <abstract>We introduce MTG, a new benchmark suite for training and evaluating multilingual text generation. It is the first-proposed multilingual multiway text generation dataset with the largest human-annotated data (400k). It includes four generation tasks (story generation, question generation, title generation and text summarization) across five languages (English, German, French, Spanish and Chinese). The multiway setup enables testing knowledge transfer capabilities for a model across languages and tasks. Using MTG, we train and analyze several popular multilingual generation models from different aspects. Our benchmark suite fosters model performance enhancement with more human-annotated parallel data. It provides comprehensive evaluations with diverse generation scenarios. Code and data are available at <url>https://github.com/zide05/MTG</url>.</abstract>
      <url hash="dad0ea74">2022.findings-naacl.192</url>
      <bibkey>chen-etal-2022-mtg</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.192</doi>
      <video href="2022.findings-naacl.192.mp4"/>
      <pwccode url="https://github.com/zide05/mtg" additional="false">zide05/mtg</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/gem">GEM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/rocstories">ROCStories</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="193">
      <title>Weakly Supervised Text-to-<fixed-case>SQL</fixed-case> Parsing through Question Decomposition</title>
      <author><first>Tomer</first><last>Wolfson</last></author>
      <author><first>Daniel</first><last>Deutch</last></author>
      <author><first>Jonathan</first><last>Berant</last></author>
      <pages>2528-2542</pages>
      <abstract>Text-to-SQL parsers are crucial in enabling non-experts to effortlessly query relational data. Training such parsers, by contrast, generally requires expertise in annotating natural language (NL) utterances with corresponding SQL queries. In this work, we propose a weak supervision approach for training text-to-SQL parsers. We take advantage of the recently proposed question meaning representation called QDMR, an intermediate between NL and formal query languages. Given questions, their QDMR structures (annotated by non-experts or automatically predicted), and the answers, we are able to automatically synthesize SQL queries that are used to train text-to-SQL models. We test our approach by experimenting on five benchmark datasets. Our results show that the weakly supervised models perform competitively with those trained on annotated NL-SQL data. Overall, we effectively train text-to-SQL parsers, while using zero SQL annotations.</abstract>
      <url hash="a8fe45ee">2022.findings-naacl.193</url>
      <bibkey>wolfson-etal-2022-weakly</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.193</doi>
      <video href="2022.findings-naacl.193.mp4"/>
      <pwccode url="https://github.com/tomerwolgithub/question-decomposition-to-sql" additional="false">tomerwolgithub/question-decomposition-to-sql</pwccode>
    </paper>
    <paper id="194">
      <title>Detect Rumors in Microblog Posts for Low-Resource Domains via Adversarial Contrastive Learning</title>
      <author><first>Hongzhan</first><last>Lin</last></author>
      <author><first>Jing</first><last>Ma</last></author>
      <author><first>Liangliang</first><last>Chen</last></author>
      <author><first>Zhiwei</first><last>Yang</last></author>
      <author><first>Mingfei</first><last>Cheng</last></author>
      <author><first>Chen</first><last>Guang</last></author>
      <pages>2543-2556</pages>
      <abstract>Massive false rumors emerging along with breaking news or trending topics severely hinder the truth. Existing rumor detection approaches achieve promising performance on the yesterday’s news, since there is enough corpus collected from the same domain for model training. However, they are poor at detecting rumors about unforeseen events especially those propagated in minority languages due to the lack of training data and prior knowledge (i.e., low-resource regimes). In this paper, we propose an adversarial contrastive learning framework to detect rumors by adapting the features learned from well-resourced rumor data to that of the low-resourced. Our model explicitly overcomes the restriction of domain and/or language usage via language alignment and a novel supervised contrastive training paradigm. Moreover, we develop an adversarial augmentation mechanism to further enhance the robustness of low-resource rumor representation. Extensive experiments conducted on two low-resource datasets collected from real-world microblog platforms demonstrate that our framework achieves much better performance than state-of-the-art methods and exhibits a superior capacity for detecting rumors at early stages.</abstract>
      <url hash="98eb4394">2022.findings-naacl.194</url>
      <bibkey>lin-etal-2022-detect</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.194</doi>
      <video href="2022.findings-naacl.194.mp4"/>
      <pwccode url="https://github.com/daniellin97/aclr4rumor-naacl2022" additional="false">daniellin97/aclr4rumor-naacl2022</pwccode>
    </paper>
    <paper id="195">
      <title><fixed-case>D</fixed-case>ialo<fixed-case>KG</fixed-case>: Knowledge-Structure Aware Task-Oriented Dialogue Generation</title>
      <author><first>Md Rashad Al Hasan</first><last>Rony</last></author>
      <author><first>Ricardo</first><last>Usbeck</last></author>
      <author><first>Jens</first><last>Lehmann</last></author>
      <pages>2557-2571</pages>
      <abstract>Task-oriented dialogue generation is challenging since the underlying knowledge is often dynamic and effectively incorporating knowledge into the learning process is hard. It is particularly challenging to generate both human-like and informative responses in this setting. Recent research primarily focused on various knowledge distillation methods where the underlying relationship between the facts in a knowledge base is not effectively captured. In this paper, we go one step further and demonstrate how the structural information of a knowledge graph can improve the system’s inference capabilities. Specifically, we propose DialoKG, a novel task-oriented dialogue system that effectively incorporates knowledge into a language model. Our proposed system views relational knowledge as a knowledge graph and introduces (1) a structure-aware knowledge embedding technique, and (2) a knowledge graph-weighted attention masking strategy to facilitate the system selecting relevant information during the dialogue generation. An empirical evaluation demonstrates the effectiveness of DialoKG over state-of-the-art methods on several standard benchmark datasets.</abstract>
      <url hash="0479ec28">2022.findings-naacl.195</url>
      <bibkey>rony-etal-2022-dialokg</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.195</doi>
      <video href="2022.findings-naacl.195.mp4"/>
      <pwccode url="https://github.com/rashad101/dialokg" additional="false">rashad101/dialokg</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multiwoz">MultiWOZ</pwcdataset>
    </paper>
    <paper id="196">
      <title>Zero-Shot Event Detection Based on Ordered Contrastive Learning and Prompt-Based Prediction</title>
      <author><first>Senhui</first><last>Zhang</last></author>
      <author><first>Tao</first><last>Ji</last></author>
      <author><first>Wendi</first><last>Ji</last></author>
      <author><first>Xiaoling</first><last>Wang</last></author>
      <pages>2572-2580</pages>
      <abstract>Event detection is a classic natural language processing task. However, the constantly emerging new events make supervised methods not applicable to unseen types. Previous zero-shot event detection methods either require predefined event types as heuristic rules or resort to external semantic analyzing tools. To overcome this weakness, we propose an end-to-end framework named Zero-Shot Event Detection Based on Ordered Contrastive Learning and Prompt-Based Prediction (ZEOP). By creatively introducing multiple contrastive samples with ordered similarities, the encoder can learn event representations from both instance-level and class-level, which makes the distinctions between different unseen types more significant. Meanwhile, we utilize the prompt-based prediction to identify trigger words without relying on external resources. Experiments demonstrate that our model detects events more effectively and accurately than state-of-the-art methods.</abstract>
      <url hash="c5002df5">2022.findings-naacl.196</url>
      <bibkey>zhang-etal-2022-zero</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.196</doi>
      <video href="2022.findings-naacl.196.mp4"/>
      <pwccode url="https://github.com/kindroach/naacl-zeop" additional="false">kindroach/naacl-zeop</pwccode>
    </paper>
    <paper id="197">
      <title><fixed-case>KETOD</fixed-case>: Knowledge-Enriched Task-Oriented Dialogue</title>
      <author><first>Zhiyu</first><last>Chen</last></author>
      <author><first>Bing</first><last>Liu</last></author>
      <author><first>Seungwhan</first><last>Moon</last></author>
      <author><first>Chinnadhurai</first><last>Sankar</last></author>
      <author><first>Paul</first><last>Crook</last></author>
      <author><first>William Yang</first><last>Wang</last></author>
      <pages>2581-2593</pages>
      <abstract>Existing studies in dialogue system research mostly treat task-oriented dialogue and chit-chat as separate domains. Towards building a human-like assistant that can converse naturally and seamlessly with users, it is important to build a dialogue system that conducts both types of conversations effectively. In this work, we investigate how task-oriented dialogue and knowledge-grounded chit-chat can be effectively integrated into a single model. To this end, we create a new dataset, KETOD (Knowledge-Enriched Task-Oriented Dialogue), where we naturally enrich task-oriented dialogues with chit-chat based on relevant entity knowledge. We also propose two new models, SimpleToDPlus and Combiner, for the proposed task. Experimental results on both automatic and human evaluations show that the proposed methods can significantly improve the performance in knowledge-enriched response generation while maintaining a competitive task-oriented dialog performance. We believe our new dataset will be a valuable resource for future studies. Our dataset and code are publicly available at <url>https://github.com/facebookresearch/ketod</url>.</abstract>
      <url hash="5cdabf2e">2022.findings-naacl.197</url>
      <bibkey>chen-etal-2022-ketod</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.197</doi>
      <video href="2022.findings-naacl.197.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/ketod">KETOD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sgd">SGD</pwcdataset>
    </paper>
    <paper id="198">
      <title><fixed-case>TAN</fixed-case>et: <fixed-case>T</fixed-case>hread-<fixed-case>A</fixed-case>ware <fixed-case>P</fixed-case>retraining for <fixed-case>A</fixed-case>bstractive <fixed-case>C</fixed-case>onversational <fixed-case>S</fixed-case>ummarization</title>
      <author><first>Ze</first><last>Yang</last></author>
      <author><first>Christian</first><last>Wang</last></author>
      <author><first>Zhoujin</first><last>Tian</last></author>
      <author><first>Wei</first><last>Wu</last></author>
      <author><first>Zhoujun</first><last>Li</last></author>
      <pages>2594-2607</pages>
      <abstract>Although pre-trained language models (PLMs) have achieved great success and become a milestone in NLP, abstractive conversational summarization remains a challenging but less studied task. The difficulty lies in two aspects. One is the lack of large-scale conversational summary data. Another is that applying the existing pre-trained models to this task is tricky because of the structural dependence within the conversation and its informal expression, etc. In this work, we first build a large-scale (11M) pretraining dataset called RCSum, based on the multi-person discussions in the Reddit community. We then present TANet, a thread-aware Transformer-based network. Unlike the existing pre-trained models that treat a conversation as a sequence of sentences, we argue that the inherent contextual dependency among the utterances plays an essential role in understanding the entire conversation and thus propose two new techniques to incorporate the structural information into our model. The first is thread-aware attention which is computed by taking into account the contextual dependency within utterances. Second, we apply thread prediction loss to predict the relations between utterances. We evaluate our model on four datasets of real conversations, covering types of meeting transcripts, customer-service records, and forum threads. Experimental results demonstrate that TANet achieves a new state-of-the-art in terms of both automatic evaluation and human judgment.</abstract>
      <url hash="cd59ebea">2022.findings-naacl.198</url>
      <bibkey>yang-etal-2022-tanet</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.198</doi>
      <video href="2022.findings-naacl.198.mp4"/>
    </paper>
    <paper id="199">
      <title><fixed-case>A</fixed-case>dapter<fixed-case>B</fixed-case>ias: Parameter-efficient Token-dependent Representation Shift for Adapters in <fixed-case>NLP</fixed-case> Tasks</title>
      <author><first>Chin-Lun</first><last>Fu</last></author>
      <author><first>Zih-Ching</first><last>Chen</last></author>
      <author><first>Yun-Ru</first><last>Lee</last></author>
      <author><first>Hung-yi</first><last>Lee</last></author>
      <pages>2608-2621</pages>
      <abstract>Transformer-based pre-trained models with millions of parameters require large storage. Recent approaches tackle this shortcoming by training adapters, but these approaches still require a relatively large number of parameters. In this study, AdapterBias, a surprisingly simple yet effective adapter architecture, is proposed. AdapterBias adds a token-dependent shift to the hidden output of transformer layers to adapt to downstream tasks with only a vector and a linear layer. Extensive experiments are conducted to demonstrate the effectiveness of AdapterBias. The experiments show that our proposed method can dramatically reduce the trainable parameters compared to the previous works with a minimal decrease in task performances compared with fine-tuned pre-trained models. We further find that AdapterBias automatically learns to assign more significant representation shifts to the tokens related to the task in consideration.</abstract>
      <url hash="425cf6db">2022.findings-naacl.199</url>
      <bibkey>fu-etal-2022-adapterbias</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.199</doi>
      <video href="2022.findings-naacl.199.mp4"/>
      <pwccode url="https://github.com/Allen0307/AdapterBias" additional="false">Allen0307/AdapterBias</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst-2">SST-2</pwcdataset>
    </paper>
    <paper id="200">
      <title>Bridging the Gap between Training and Inference: Multi-Candidate Optimization for Diverse Neural Machine Translation</title>
      <author><first>Huan</first><last>Lin</last></author>
      <author><first>Baosong</first><last>Yang</last></author>
      <author><first>Liang</first><last>Yao</last></author>
      <author><first>Dayiheng</first><last>Liu</last></author>
      <author><first>Haibo</first><last>Zhang</last></author>
      <author><first>Jun</first><last>Xie</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <author><first>Jinsong</first><last>Su</last></author>
      <pages>2622-2632</pages>
      <abstract>Diverse NMT aims at generating multiple diverse yet faithful translations given a source sentence. In this paper, we investigate a common shortcoming in existing diverse NMT studies: the model is usually trained with single reference, while expected to generate multiple candidate translations in inference. The discrepancy between training and inference enlarges the confidence variance and quality gap among candidate translations and thus hinders model performance. To deal with this defect, we propose a multi-candidate optimization framework for diverse NMT. Specifically, we define assessments to score the diversity and the quality of candidate translations during training, and optimize the diverse NMT model with two strategies based on reinforcement learning, namely hard constrained training and soft constrained training. We conduct experiments on NIST Chinese-English and WMT14 English-German translation tasks. The results illustrate that our framework is transparent to basic diverse NMT models, and universally makes better trade-off between diversity and quality. Our source codeis available at <url>https://github.com/DeepLearnXMU/MultiCanOptim</url>.</abstract>
      <url hash="f5914256">2022.findings-naacl.200</url>
      <attachment type="software" hash="c478c355">2022.findings-naacl.200.software.zip</attachment>
      <bibkey>lin-etal-2022-bridging</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.200</doi>
      <pwccode url="https://github.com/deeplearnxmu/multicanoptim" additional="false">deeplearnxmu/multicanoptim</pwccode>
    </paper>
    <paper id="201">
      <title>Learning from Bootstrapping and Stepwise Reinforcement Reward: A Semi-Supervised Framework for Text Style Transfer</title>
      <author><first>Zhengyuan</first><last>Liu</last></author>
      <author><first>Nancy</first><last>Chen</last></author>
      <pages>2633-2648</pages>
      <abstract>Text style transfer is an important task in controllable language generation. Supervised approaches have pushed performance improvement on style-oriented rewriting such as formality conversion. However, challenges remain due to the scarcity of large-scale parallel data in many domains. While unsupervised approaches do not rely on annotated sentence pairs for each style, they are often plagued with instability issues such as mode collapse or quality degradation. To take advantage of both supervised and unsupervised paradigms and tackle the challenges, in this work, we propose a semi-supervised framework for text style transfer. First, the learning process is bootstrapped with supervision guided by automatically constructed pseudo-parallel pairs using lexical and semantic-based methods. Then the model learns from unlabeled data via reinforcement rewards. Specifically, we propose to improve the sequence-to-sequence policy gradient via stepwise reward optimization, providing fine-grained learning signals and stabilizing the reinforced learning process. Experimental results show that the proposed approach achieves state-of-the-art performance on multiple datasets, and produces effective generation with as minimal as 10% of training data.</abstract>
      <url hash="e2c9ac34">2022.findings-naacl.201</url>
      <bibkey>liu-chen-2022-learning</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.201</doi>
      <video href="2022.findings-naacl.201.mp4"/>
      <pwccode url="https://github.com/seq-to-mind/semi-style-transfer" additional="false">seq-to-mind/semi-style-transfer</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/gyafc">GYAFC</pwcdataset>
    </paper>
    <paper id="202">
      <title><fixed-case>EA</fixed-case><tex-math>^2</tex-math><fixed-case>E</fixed-case>: Improving Consistency with Event Awareness for Document-Level Argument Extraction</title>
      <author><first>Qi</first><last>Zeng</last></author>
      <author><first>Qiusi</first><last>Zhan</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <pages>2649-2655</pages>
      <abstract>Events are inter-related in documents. Motivated by the one-sense-per-discourse theory, we hypothesize that a participant tends to play consistent roles across multiple events in the same document. However recent work on document-level event argument extraction models each individual event in isolation and therefore causes inconsistency among extracted arguments across events, which will further cause discrepancy for downstream applications such as event knowledge base population, question answering, and hypothesis generation. In this work, we formulate event argument consistency as the constraints from event-event relations under the document-level setting. To improve consistency we introduce the Event-Aware Argument Extraction (EA<tex-math>^2</tex-math>E) model with augmented context for training and inference. Experiment results on WIKIEVENTS and ACE2005 datasets demonstrate the effectiveness of EA<tex-math>^2</tex-math>E compared to baseline methods.</abstract>
      <url hash="73b74f15">2022.findings-naacl.202</url>
      <attachment type="software" hash="942f8e23">2022.findings-naacl.202.software.zip</attachment>
      <bibkey>zeng-etal-2022-ea2e</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.202</doi>
      <video href="2022.findings-naacl.202.mp4"/>
      <pwccode url="https://github.com/zqs1943/docie" additional="false">zqs1943/docie</pwccode>
    </paper>
    <paper id="203">
      <title>Label Refinement via Contrastive Learning for Distantly-Supervised Named Entity Recognition</title>
      <author><first>Huaiyuan</first><last>Ying</last></author>
      <author><first>Shengxuan</first><last>Luo</last></author>
      <author><first>Tiantian</first><last>Dang</last></author>
      <author><first>Sheng</first><last>Yu</last></author>
      <pages>2656-2666</pages>
      <abstract>Distantly-supervised named entity recognition (NER) locates and classifies entities using only knowledge bases and unlabeled corpus to mitigate the reliance on human-annotated labels. The distantly annotated data suffer from the noise in labels, and previous works on DSNER have proved the importance of pre-refining distant labels with hand-crafted rules and extra existing semantic information. In this work, we explore the way to directly learn the distant label refinement knowledge by imitating annotations of different qualities and comparing these annotations in contrastive learning frameworks. the proposed distant label refinement model can give modified suggestions on distant data without additional supervised labels, and thus reduces the requirement on the quality of the knowledge bases. We perform extensive experiments and observe that recent and state-of-the-art DSNER methods gain evident benefits with our method.</abstract>
      <url hash="9f47d420">2022.findings-naacl.203</url>
      <attachment type="software" hash="19c45228">2022.findings-naacl.203.software.zip</attachment>
      <bibkey>ying-etal-2022-label</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.203</doi>
      <video href="2022.findings-naacl.203.mp4"/>
      <pwccode url="https://github.com/yinghy18/credel" additional="false">yinghy18/credel</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ncbi-disease-1">NCBI Disease</pwcdataset>
    </paper>
    <paper id="204">
      <title>Negative Sample is Negative in Its Own Way: Tailoring Negative Sentences for Image-Text Retrieval</title>
      <author><first>Zhihao</first><last>Fan</last></author>
      <author><first>Zhongyu</first><last>Wei</last></author>
      <author><first>Zejun</first><last>Li</last></author>
      <author><first>Siyuan</first><last>Wang</last></author>
      <author><first>Xuanjing</first><last>Huang</last></author>
      <author><first>Jianqing</first><last>Fan</last></author>
      <pages>2667-2678</pages>
      <abstract>Matching model is essential for Image-Text Retrieval framework. Existing research usually train the model with a triplet loss and explore various strategy to retrieve hard negative sentences in the dataset. We argue that current retrieval-based negative sample construction approach is limited in the scale of the dataset thus fail to identify negative sample of high difficulty for every image. We propose our TAiloring neGative Sentences with Discrimination and Correction (TAGS-DC) to generate synthetic sentences automatically as negative samples. TAGS-DC is composed of masking and refilling to generate synthetic negative sentences with higher difficulty. To keep the difficulty during training, we mutually improve the retrieval and generation through parameter sharing. To further utilize fine-grained semantic of mismatch in the negative sentence, we propose two auxiliary tasks, namely word discrimination and word correction to improve the training. In experiments, we verify the effectiveness of our model on MS-COCO and Flickr30K compared with current state-of-the-art models and demonstrates its robustness and faithfulness in the further analysis.</abstract>
      <url hash="6713dd37">2022.findings-naacl.204</url>
      <bibkey>fan-etal-2022-negative</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.204</doi>
      <video href="2022.findings-naacl.204.mp4"/>
      <pwccode url="https://github.com/libertfan/tags" additional="false">libertfan/tags</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">MS COCO</pwcdataset>
    </paper>
    <paper id="205">
      <title>Explore More Guidance: A Task-aware Instruction Network for Sign Language Translation Enhanced with Data Augmentation</title>
      <author><first>Yong</first><last>Cao</last></author>
      <author><first>Wei</first><last>Li</last></author>
      <author><first>Xianzhi</first><last>Li</last></author>
      <author><first>Min</first><last>Chen</last></author>
      <author><first>Guangyong</first><last>Chen</last></author>
      <author><first>Long</first><last>Hu</last></author>
      <author><first>Zhengdao</first><last>Li</last></author>
      <author><first>Kai</first><last>Hwang</last></author>
      <pages>2679-2690</pages>
      <abstract>Sign language recognition and translation first uses a recognition module to generate glosses from sign language videos and then employs a translation module to translate glosses into spoken sentences. Most existing works focus on the recognition step, while paying less attention to sign language translation. In this work, we propose a task-aware instruction network, namely TIN-SLT, for sign language translation, by introducing the isntruction module and the learning-based feature fuse strategy into a Transformer network. In this way, the pre-trained model’s language ability can be well explored and utilized to further boost the translation performance. Moreover, by exploring the representation space of sign language glosses and target spoken language, we propose a multi-level data augmentation scheme to adjust the data distribution of the training set. We conduct extensive experiments on two challenging benchmark datasets, PHOENIX-2014-T and ASLG-PC12, on which our method outperforms former best solutions by 1.65 and 1.42 in terms of BLEU-4. Our code and trained networks will be available upon the publication of this work.</abstract>
      <url hash="ba33976f">2022.findings-naacl.205</url>
      <bibkey>cao-etal-2022-explore</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.205</doi>
      <video href="2022.findings-naacl.205.mp4"/>
      <pwccode url="https://github.com/yongcaoplus/tin-slt" additional="false">yongcaoplus/tin-slt</pwccode>
    </paper>
    <paper id="206">
      <title><fixed-case>R</fixed-case>o<fixed-case>V</fixed-case>i<fixed-case>ST</fixed-case>: Learning Robust Metrics for Visual Storytelling</title>
      <author><first>Eileen</first><last>Wang</last></author>
      <author><first>Caren</first><last>Han</last></author>
      <author><first>Josiah</first><last>Poon</last></author>
      <pages>2691-2702</pages>
      <abstract>Visual storytelling (VST) is the task of generating a story paragraph that describes a given image sequence. Most existing storytelling approaches have evaluated their models using traditional natural language generation metrics like BLEU or CIDEr. However, such metrics based on <tex-math>n</tex-math>-gram matching tend to have poor correlation with human evaluation scores and do not explicitly consider other criteria necessary for storytelling such as sentence structure or topic coherence. Moreover, a single score is not enough to assess a story as it does not inform us about what specific errors were made by the model. In this paper, we propose 3 evaluation metrics sets that analyses which aspects we would look for in a good story: 1) visual grounding, 2) coherence, and 3) non-redundancy. We measure the reliability of our metric sets by analysing its correlation with human judgement scores on a sample of machine stories obtained from 4 state-of-the-arts models trained on the Visual Storytelling Dataset (VIST). Our metric sets outperforms other metrics on human correlation, and could be served as a learning based evaluation metric set that is complementary to existing rule-based metrics.</abstract>
      <url hash="65022a83">2022.findings-naacl.206</url>
      <bibkey>wang-etal-2022-rovist</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.206</doi>
      <video href="2022.findings-naacl.206.mp4"/>
      <pwccode url="https://github.com/usydnlp/rovist" additional="false">usydnlp/rovist</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/flickr30k">Flickr30k</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/rocstories">ROCStories</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/vist">VIST</pwcdataset>
    </paper>
    <paper id="207">
      <title><fixed-case>Q</fixed-case>uery2<fixed-case>P</fixed-case>articles: Knowledge Graph Reasoning with Particle Embeddings</title>
      <author><first>Jiaxin</first><last>Bai</last></author>
      <author><first>Zihao</first><last>Wang</last></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Yangqiu</first><last>Song</last></author>
      <pages>2703-2714</pages>
      <abstract>Answering complex logical queries on incomplete knowledge graphs (KGs) with missing edges is a fundamental and important task for knowledge graph reasoning. The query embedding method is proposed to answer these queries by jointly encoding queries and entities to the same embedding space. Then the answer entities are selected according to the similarities between the entity embeddings and the query embedding. As the answers to a complex query are obtained from a combination of logical operations over sub-queries, the embeddings of the answer entities may not always follow a uni-modal distribution in the embedding space. Thus, it is challenging to simultaneously retrieve a set of diverse answers from the embedding space using a single and concentrated query representation such as a vector or a hyper-rectangle. To better cope with queries with diversified answers, we propose Query2Particles (Q2P), a complex KG query answering method. Q2P encodes each query into multiple vectors, named particle embeddings. By doing so, the candidate answers can be retrieved from different areas over the embedding space using the maximal similarities between the entity embeddings and any of the particle embeddings. Meanwhile, the corresponding neural logic operations are defined to support its reasoning over arbitrary first-order logic queries. The experiments show that Query2Particles achieves state-of-the-art performance on the complex query answering tasks on FB15k, FB15K-237, and NELL knowledge graphs.</abstract>
      <url hash="e95c19aa">2022.findings-naacl.207</url>
      <bibkey>bai-etal-2022-query2particles</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.207</doi>
      <video href="2022.findings-naacl.207.mp4"/>
      <pwccode url="https://github.com/hkust-knowcomp/query2particles" additional="false">hkust-knowcomp/query2particles</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fb15k-237">FB15k-237</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/nell">NELL</pwcdataset>
    </paper>
    <paper id="208">
      <title><fixed-case>ID</fixed-case>10<fixed-case>M</fixed-case>: Idiom Identification in 10 Languages</title>
      <author><first>Simone</first><last>Tedeschi</last></author>
      <author><first>Federico</first><last>Martelli</last></author>
      <author><first>Roberto</first><last>Navigli</last></author>
      <pages>2715-2726</pages>
      <abstract>Idioms are phrases which present a figurative meaning that cannot be (completely) derived by looking at the meaning of their individual components. Identifying and understanding idioms in context is a crucial goal and a key challenge in a wide range of Natural Language Understanding tasks. Although efforts have been undertaken in this direction, the automatic identification and understanding of idioms is still a largely under-investigated area, especially when operating in a multilingual scenario. In this paper, we address such limitations and put forward several new contributions: we propose a novel multilingual Transformer-based system for the identification of idioms; we produce a high-quality automatically-created training dataset in 10 languages, along with a novel manually-curated evaluation benchmark; finally, we carry out a thorough performance analysis and release our evaluation suite at <url>https://github.com/Babelscape/ID10M</url>.</abstract>
      <url hash="2733e5e5">2022.findings-naacl.208</url>
      <bibkey>tedeschi-etal-2022-id10m</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.208</doi>
      <video href="2022.findings-naacl.208.mp4"/>
      <pwccode url="https://github.com/babelscape/id10m" additional="false">babelscape/id10m</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikimatrix">WikiMatrix</pwcdataset>
    </paper>
    <paper id="209">
      <title>Cross-Domain Classification of Moral Values</title>
      <author><first>Enrico</first><last>Liscio</last></author>
      <author><first>Alin</first><last>Dondera</last></author>
      <author><first>Andrei</first><last>Geadau</last></author>
      <author><first>Catholijn</first><last>Jonker</last></author>
      <author><first>Pradeep</first><last>Murukannaiah</last></author>
      <pages>2727-2745</pages>
      <abstract>Moral values influence how we interpret and act upon the information we receive. Identifying human moral values is essential for artificially intelligent agents to co-exist with humans. Recent progress in natural language processing allows the identification of moral values in textual discourse. However, domain-specific moral rhetoric poses challenges for transferring knowledge from one domain to another. We provide the first extensive investigation on the effects of cross-domain classification of moral values from text. We compare a state-of-the-art deep learning model (BERT) in seven domains and four cross-domain settings. We show that a value classifier can generalize and transfer knowledge to novel domains, but it can introduce catastrophic forgetting. We also highlight the typical classification errors in cross-domain value classification and compare the model predictions to the annotators agreement. Our results provide insights to computer and social scientists that seek to identify moral rhetoric specific to a domain of discourse.</abstract>
      <url hash="73bdbd8c">2022.findings-naacl.209</url>
      <attachment type="software" hash="0cf9d64e">2022.findings-naacl.209.software.zip</attachment>
      <bibkey>liscio-etal-2022-cross</bibkey>
      <doi>10.18653/v1/2022.findings-naacl.209</doi>
      <video href="2022.findings-naacl.209.mp4"/>
      <pwccode url="https://github.com/adondera/transferability-of-values" additional="false">adondera/transferability-of-values</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ethics-1">ETHICS</pwcdataset>
    </paper>
  </volume>
  <volume id="aacl" ingest-date="2022-11-21" type="proceedings">
    <meta>
      <booktitle>Findings of the Association for Computational Linguistics: AACL-IJCNLP 2022</booktitle>
      <editor><first>Yulan</first><last>He</last></editor>
      <editor><first>Heng</first><last>Ji</last></editor>
      <editor><first>Sujian</first><last>Li</last></editor>
      <editor id="yang-liu-ict"><first>Yang</first><last>Liu</last></editor>
      <editor><first>Chua-Hui</first><last>Chang</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online only</address>
      <month>November</month>
      <year>2022</year>
      <url hash="0e9549b3">2022.findings-aacl</url>
      <venue>findings</venue>
    </meta>
    <frontmatter>
      <url hash="35d4fe70">2022.findings-aacl.0</url>
      <bibkey>findings-2022-findings-association-linguistics</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Efficient Entity Embedding Construction from Type Knowledge for <fixed-case>BERT</fixed-case></title>
      <author><first>Yukun</first><last>Feng</last></author>
      <author><first>Amir</first><last>Fayazi</last></author>
      <author><first>Abhinav</first><last>Rastogi</last></author>
      <author><first>Manabu</first><last>Okumura</last></author>
      <pages>1–10</pages>
      <abstract>Recent work has shown advantages of incorporating knowledge graphs (KGs) into BERT for various NLP tasks. One common way is to feed entity embeddings as an additional input during pre-training. There are two limitations to such a method. First, to train the entity embeddings to include rich information of factual knowledge, it typically requires access to the entire KG. This is challenging for KGs with daily changes (e.g., Wikidata). Second, it requires a large scale pre-training corpus with entity annotations and high computational cost during pre-training. In this work, we efficiently construct entity embeddings only from the type knowledge, that does not require access to the entire KG. Although the entity embeddings contain only local information, they perform very well when combined with context. Furthermore, we show that our entity embeddings, constructed from BERT’s input embeddings, can be directly incorporated into the fine-tuning phase without requiring any specialized pre-training. In addition, these entity embeddings can also be constructed on the fly without requiring a large memory footprint to store them. Finally, we propose task-specific models that incorporate our entity embeddings for entity linking, entity typing, and relation classification. Experiments show that our models have comparable or superior performance to existing models while being more resource efficient.</abstract>
      <url hash="5669a333">2022.findings-aacl.1</url>
      <bibkey>feng-etal-2022-efficient</bibkey>
    </paper>
    <paper id="2">
      <title>Spa: On the Sparsity of Virtual Adversarial Training for Dependency Parsing</title>
      <author><first>Chao</first><last>Lou</last></author>
      <author><first>Wenjuan</first><last>Han</last></author>
      <author><first>Kewei</first><last>Tu</last></author>
      <pages>11–21</pages>
      <abstract>Virtual adversarial training (VAT) is a powerful approach to improving robustness and performance, leveraging both labeled and unlabeled data to compensate for the scarcity of labeled data. It is adopted on lots of vision and language classification tasks. However, for tasks with structured output (e.g., dependency parsing), the application of VAT is nontrivial due to the intrinsic proprieties of structures: (1) the non-sparse problem and (2) exponential complexity. Against this background, we propose the Sparse Parse Adjustment (spa) algorithm and successfully applied VAT to the dependency parsing task. spa refers to the learning algorithm which combines the graph-based dependency parsing model with VAT in an exact computational manner and enhances the dependency parser with controllable and adjustable sparsity. Empirical studies show that the TreeCRF parser optimized using outperforms other methods without sparsity regularization.</abstract>
      <url hash="463453cb">2022.findings-aacl.2</url>
      <attachment type="Software" hash="322a0217">2022.findings-aacl.2.Software.zip</attachment>
      <bibkey>lou-etal-2022-spa</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>K</fixed-case>reol<fixed-case>M</fixed-case>orisien<fixed-case>MT</fixed-case>: A Dataset for Mauritian Creole Machine Translation</title>
      <author><first>Raj</first><last>Dabre</last></author>
      <author><first>Aneerav</first><last>Sukhoo</last></author>
      <pages>22–29</pages>
      <abstract>In this paper, we describe KreolMorisienMT, a dataset for benchmarking machine translation quality of Mauritian Creole. Mauritian Creole (Kreol Morisien) is a French-based creole and a lingua franca of the Republic of Mauritius. KreolMorisienMT consists of a parallel corpus between English and Kreol Morisien, French and Kreol Morisien and a monolingual corpus for Kreol Morisien. We first give an overview of Kreol Morisien and then describe the steps taken to create the corpora. Thereafter, we benchmark Kreol Morisien ↔ English and Kreol Morisien ↔ French models leveraging pre-trained models and multilingual transfer learning. Human evaluation reveals our systems’ high translation quality.</abstract>
      <url hash="43e41764">2022.findings-aacl.3</url>
      <attachment type="Dataset" hash="8b2e0adb">2022.findings-aacl.3.Dataset.zip</attachment>
      <bibkey>dabre-sukhoo-2022-kreolmorisienmt</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>LEATHER</fixed-case>: A Framework for Learning to Generate Human-like Text in Dialogue</title>
      <author><first>Anthony</first><last>Sicilia</last></author>
      <author><first>Malihe</first><last>Alikhani</last></author>
      <pages>30–53</pages>
      <abstract>Algorithms for text-generation in dialogue can be misguided. For example, in task-oriented settings, reinforcement learning that optimizes only task-success can lead to abysmal lexical diversity. We hypothesize this is due to poor theoretical understanding of the objectives in text-generation and their relation to the learning process (i.e., model training). To this end, we propose a new theoretical framework for learning to generate text in dialogue. Compared to existing theories of learning, our framework allows for analysis of the multi-faceted goals inherent to text-generation. We use our framework to develop theoretical guarantees for learners that adapt to unseen data. As an example, we apply our theory to study data-shift within a cooperative learning algorithm proposed for the GuessWhat?! visual dialogue game. From this insight, we propose a new algorithm, and empirically, we demonstrate our proposal improves both task-success and human-likeness of the generated text. Finally, we show statistics from our theory are empirically predictive of multiple qualities of the generated dialogue, suggesting our theory is useful for model-selection when human evaluations are not available.</abstract>
      <url hash="a23c3135">2022.findings-aacl.4</url>
      <bibkey>sicilia-alikhani-2022-leather</bibkey>
    </paper>
    <paper id="5">
      <title>Conceptual Similarity for Subjective Tags</title>
      <author><first>Yacine</first><last>Gaci</last></author>
      <author><first>Boualem</first><last>Benatallah</last></author>
      <author><first>Fabio</first><last>Casati</last></author>
      <author><first>Khalid</first><last>Benabdeslem</last></author>
      <pages>54–66</pages>
      <abstract>Tagging in the context of online resources is a fundamental addition to search systems. Tags assist with the indexing, management, and retrieval of online products and services to answer complex user queries. Traditional methods of matching user queries with tags either rely on cosine similarity, or employ semantic similarity models that fail to recognize conceptual connections between tags, e.g. ambiance and music. In this work, we focus on subjective tags which characterize subjective aspects of a product or service. We propose conceptual similarity to leverage conceptual awareness when assessing similarity between tags. We also provide a simple cost-effective pipeline to automatically generate data in order to train the conceptual similarity model. We show that our pipeline generates high-quality datasets, and evaluate the similarity model both systematically and on a downstream application. Experiments show that conceptual similarity outperforms existing work when using subjective tags.</abstract>
      <url hash="49b84d6d">2022.findings-aacl.5</url>
      <attachment type="Software" hash="73753f54">2022.findings-aacl.5.Software.zip</attachment>
      <bibkey>gaci-etal-2022-conceptual</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>T</fixed-case>ask<fixed-case>M</fixed-case>ix: Data Augmentation for Meta-Learning of Spoken Intent Understanding</title>
      <author><first>Surya Kant</first><last>Sahu</last></author>
      <pages>67–72</pages>
      <abstract>Meta-Learning has emerged as a research direction to better transfer knowledge from related tasks to unseen but related tasks. However, Meta-Learning requires many training tasks to learn representations that transfer well to unseen tasks; otherwise, it leads to overfitting, and the performance degenerates to worse than Multi-task Learning. We show that a state-of-the-art data augmentation method worsens this problem of overfitting when the task diversity is low. We propose a simple method, TaskMix, which synthesizes new tasks by linearly interpolating existing tasks. We compare TaskMix against many baselines on an in-house multilingual intent classification dataset of N-Best ASR hypotheses derived from real-life human-machine telephony utterances and two datasets derived from MTOP. We show that TaskMix outperforms baselines, alleviates overfitting when task diversity is low, and does not degrade performance even when it is high.</abstract>
      <url hash="6dee78ec">2022.findings-aacl.6</url>
      <bibkey>sahu-2022-taskmix</bibkey>
    </paper>
    <paper id="7">
      <title>Understanding the Use of Quantifiers in <fixed-case>M</fixed-case>andarin</title>
      <author><first>Guanyi</first><last>Chen</last></author>
      <author><first>Kees</first><last>van Deemter</last></author>
      <pages>73–80</pages>
      <abstract>We introduce a corpus of short texts in Mandarin, in which quantified expressions figure prominently. We illustrate the significance of the corpus by examining the hypothesis (known as Huang’s “coolness” hypothesis) that speakers of East Asian Languages tend to speak more briefly but less informatively than, for example, speakers of West-European languages. The corpus results from an elicitation experiment in which participants were asked to describe abstract visual scenes. We compare the resulting corpus, called MQTUNA, with an English corpus that was collected using the same experimental paradigm. The comparison reveals that some, though not all, aspects of quantifier use support the above-mentioned hypothesis. Implications of these findings for the generation of quantified noun phrases are discussed.</abstract>
      <url hash="4d4cf3f8">2022.findings-aacl.7</url>
      <bibkey>chen-van-deemter-2022-understanding</bibkey>
    </paper>
    <paper id="8">
      <title>Does Representational Fairness Imply Empirical Fairness?</title>
      <author><first>Aili</first><last>Shen</last></author>
      <author><first>Xudong</first><last>Han</last></author>
      <author><first>Trevor</first><last>Cohn</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <author><first>Lea</first><last>Frermann</last></author>
      <pages>81–95</pages>
      <abstract>NLP technologies can cause unintended harms if learned representations encode sensitive attributes of the author, or predictions systematically vary in quality across groups. Popular debiasing approaches, like adversarial training, remove sensitive information from representations in order to reduce disparate performance, however the relation between representational fairness and empirical (performance) fairness has not been systematically studied. This paper fills this gap, and proposes a novel debiasing method building on contrastive learning to encourage a latent space that separates instances based on target label, while mixing instances that share protected attributes. Our results show the effectiveness of our new method and, more importantly, show across a set of diverse debiasing methods that <i>representational fairness does not imply empirical fairness</i>. This work highlights the importance of aligning and understanding the relation of the optimization objective and final fairness target.</abstract>
      <url hash="36b4db36">2022.findings-aacl.8</url>
      <bibkey>shen-etal-2022-representational</bibkey>
    </paper>
    <paper id="9">
      <title><fixed-case>SEHY</fixed-case>: A Simple yet Effective Hybrid Model for Summarization of Long Scientific Documents</title>
      <author><first>Zhihua</first><last>Jiang</last></author>
      <author><first>Junzhan</first><last>Yang</last></author>
      <author><first>Dongning</first><last>Rao</last></author>
      <pages>96–106</pages>
      <abstract>Long document (e.g., scientific papers) summarization is obtaining more and more attention in recent years. Extractive approaches attempt to choose salient sentences via understanding the whole document, but long documents cover numerous subjects with varying details and will not ease content understanding. Instead, abstractive approaches elaborate to generate related tokens while suffering from truncating the source document due to their input sizes. To this end, we propose a Simple yet Effective HYbrid approach, which we call SEHY, that exploits the discourse information of a document to select salient sections instead sentences for summary generation. On the one hand, SEHY avoids the full-text understanding; on the other hand, it retains salient information given the length limit. In particular, we design two simple strategies for training the extractor: extracting sections incrementally and based on salience-analysis. Then, we use strong abstractive models to generate the final summary. We evaluate our approach on a large-scale scientific paper dataset: arXiv. Further, we discuss how the disciplinary class (e.g., computer science, math or physics) of a scientific paper affects the performance of SEHY as its writing style indicates, which is unexplored yet in existing works. Experimental results show the effectiveness of our approach and interesting findings on arXiv and its subsets generated in this paper.</abstract>
      <url hash="ec19e28c">2022.findings-aacl.9</url>
      <attachment type="Software" hash="6643101d">2022.findings-aacl.9.Software.rar</attachment>
      <bibkey>jiang-etal-2022-sehy</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>PLATO</fixed-case>-<fixed-case>XL</fixed-case>: Exploring the Large-scale Pre-training of Dialogue Generation</title>
      <author><first>Siqi</first><last>Bao</last></author>
      <author><first>Huang</first><last>He</last></author>
      <author><first>Fan</first><last>Wang</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <author><first>Haifeng</first><last>Wang</last></author>
      <author><first>Wenquan</first><last>Wu</last></author>
      <author><first>Zhihua</first><last>Wu</last></author>
      <author><first>Zhen</first><last>Guo</last></author>
      <author><first>Hua</first><last>Lu</last></author>
      <author><first>Xinxian</first><last>Huang</last></author>
      <author><first>Xin</first><last>Tian</last></author>
      <author><first>Xinchao</first><last>Xu</last></author>
      <author><first>Yingzhan</first><last>Lin</last></author>
      <author><first>Zheng-Yu</first><last>Niu</last></author>
      <pages>107–118</pages>
      <abstract>To explore the limit of dialogue generation pre-training, we present the models of PLATO-XL with up to 11 billion parameters, trained on both Chinese and English social media conversations. To train such large models, we adopt the architecture of unified transformer with high computation and parameter efficiency. In addition, we carry out multi-party aware pre-training to better distinguish the characteristic information in social media conversations. With such designs, PLATO-XL successfully achieves superior performances as compared to other approaches in both Chinese and English chitchat. We further explore the capacity of PLATO-XL on other conversational tasks, such as knowledge grounded dialogue and task-oriented conversation. The experimental results indicate that PLATO-XL obtains state-of-the-art results across multiple conversational tasks, verifying its potential as a foundation model of conversational AI.</abstract>
      <url hash="f08866a7">2022.findings-aacl.10</url>
      <bibkey>bao-etal-2022-plato</bibkey>
    </paper>
    <paper id="11">
      <title>A Hybrid Architecture for Labelling Bilingual <fixed-case>M</fixed-case>āori-<fixed-case>E</fixed-case>nglish Tweets</title>
      <author><first>David</first><last>Trye</last></author>
      <author><first>Vithya</first><last>Yogarajan</last></author>
      <author><first>Jemma</first><last>König</last></author>
      <author><first>Te Taka</first><last>Keegan</last></author>
      <author><first>David</first><last>Bainbridge</last></author>
      <author><first>Mark</first><last>Apperley</last></author>
      <pages>119–130</pages>
      <abstract>Most large-scale language detection tools perform poorly at identifying Māori text. Moreover, rule-based and machine learning-based techniques devised specifically for the Māori-English language pair struggle with interlingual homographs. We develop a hybrid architecture that couples Māori-language orthography with machine learning models in order to annotate mixed Māori-English text. This architecture is used to label a new bilingual Twitter corpus at both the token (word) and tweet (sentence) levels. We use the collected tweets to show that the hybrid approach outperforms existing systems with respect to language detection of interlingual homographs and overall accuracy. We also evaluate its performance on out-of-domain data. Two interactive visualisations are provided for exploring the Twitter corpus and comparing errors across the new and existing techniques. The architecture code and visualisations are available online, and the corpus is available on request.</abstract>
      <url hash="bb897ee1">2022.findings-aacl.11</url>
      <attachment type="Software" hash="f0da16e1">2022.findings-aacl.11.Software.zip</attachment>
      <bibkey>trye-etal-2022-hybrid</bibkey>
    </paper>
    <paper id="12">
      <title>Meta-Learning Adaptive Knowledge Distillation for Efficient Biomedical Natural Language Processing</title>
      <author><first>Abiola</first><last>Obamuyide</last></author>
      <author><first>Blair</first><last>Johnston</last></author>
      <pages>131–137</pages>
      <abstract>There has been an increase in the number of large and high-performing models made available for various biomedical natural language processing tasks. While these models have demonstrated impressive performance on various biomedical tasks, their training and run-time costs can be computationally prohibitive. This work investigates the use of knowledge distillation, a common model compression method, to reduce the size of large models for biomedical natural language processing. We further improve the performance of knowledge distillation methods for biomedical natural language by proposing a meta-learning approach which adaptively learns parameters that enable the optimal rate of knowledge exchange between the teacher and student models from the distillation data during knowledge distillation. Experiments on two biomedical natural language processing tasks demonstrate that our proposed adaptive meta-learning approach to knowledge distillation delivers improved predictive performance over previous and recent state-of-the-art knowledge distillation methods.</abstract>
      <url hash="510c744f">2022.findings-aacl.12</url>
      <bibkey>obamuyide-johnston-2022-meta</bibkey>
    </paper>
    <paper id="13">
      <title>The Effects of Surprisal across Languages: Results from Native and Non-native Reading</title>
      <author><first>Andrea</first><last>de Varda</last></author>
      <author><first>Marco</first><last>Marelli</last></author>
      <pages>138–144</pages>
      <abstract>It is well known that the surprisal of an upcoming word, as estimated by language models, is a solid predictor of reading times (Smith and Levy, 2013). However, most of the studies that support this view are based on English and few other Germanic languages, leaving an open question as to the cross-lingual generalizability of such findings. Moreover, they tend to consider only the best-performing eye-tracking measure, which might conflate the effects of predictive and integrative processing. Furthermore, it is not clear whether prediction plays a role in non-native language processing in bilingual individuals (Grüter et al., 2014). We approach these problems at large scale, extracting surprisal estimates from mBERT, and assessing their psychometric predictive power on the MECO corpus, a cross-linguistic dataset of eye movement behavior in reading (Siegelman et al., 2022; Kuperman et al., 2020). We show that surprisal is a strong predictor of reading times across languages and fixation measurements, and that its effects in L2 are weaker with respect to L1.</abstract>
      <url hash="eb8e39af">2022.findings-aacl.13</url>
      <bibkey>de-varda-marelli-2022-effects</bibkey>
    </paper>
    <paper id="14">
      <title>Assessing How Users Display Self-Disclosure and Authenticity in Conversation with Human-Like Agents: A Case Study of Luda Lee</title>
      <author><first>Won Ik</first><last>Cho</last></author>
      <author><first>Soomin</first><last>Kim</last></author>
      <author><first>Eujeong</first><last>Choi</last></author>
      <author><first>Younghoon</first><last>Jeong</last></author>
      <pages>145–152</pages>
      <abstract>There is an ongoing discussion on what makes humans more engaged when interacting with conversational agents. However, in the area of language processing, there has been a paucity of studies on how people react to agents and share interactions with others. We attack this issue by investigating the user dialogues with human-like agents posted online and aim to analyze the dialogue patterns. We construct a taxonomy to discern the users’ self-disclosure in the dialogue and the communication authenticity displayed in the user posting. We annotate the in-the-wild data, examine the reliability of the proposed scheme, and discuss how the categorization can be utilized for future research and industrial development.</abstract>
      <url hash="0cb53785">2022.findings-aacl.14</url>
      <bibkey>cho-etal-2022-assessing</bibkey>
    </paper>
    <paper id="15">
      <title>Block Diagram-to-Text: Understanding Block Diagram Images by Generating Natural Language Descriptors</title>
      <author><first>Shreyanshu</first><last>Bhushan</last></author>
      <author><first>Minho</first><last>Lee</last></author>
      <pages>153–168</pages>
      <abstract>Block diagrams are very popular for representing a workflow or process of a model. Understanding block diagrams by generating summaries can be extremely useful in document summarization. It can also assist people in inferring key insights from block diagrams without requiring a lot of perceptual and cognitive effort. In this paper, we propose a novel task of converting block diagram images into text by presenting a framework called “BloSum”. This framework extracts the contextual meaning from the images in the form of triplets that help the language model in summary generation. We also introduce a new dataset for complex computerized block diagrams, explain the dataset preparation process, and later analyze it. Additionally, to showcase the generalization of the model, we test our method with publicly available handwritten block diagram datasets. Our evaluation with different metrics demonstrates the effectiveness of our approach that outperforms other methods and techniques.</abstract>
      <url hash="403757f6">2022.findings-aacl.15</url>
      <bibkey>bhushan-lee-2022-block</bibkey>
    </paper>
    <paper id="16">
      <title>Multi-Domain Dialogue State Tracking By Neural-Retrieval Augmentation</title>
      <author><first>Lohith</first><last>Ravuru</last></author>
      <author><first>Seonghan</first><last>Ryu</last></author>
      <author><first>Hyungtak</first><last>Choi</last></author>
      <author><first>Haehun</first><last>Yang</last></author>
      <author><first>Hyeonmok</first><last>Ko</last></author>
      <pages>169–175</pages>
      <abstract>Dialogue State Tracking (DST) is a very complex task that requires precise understanding and information tracking of multi-domain conversations between users and dialogue systems. Many task-oriented dialogue systems use dialogue state tracking technology to infer users’ goals from the history of the conversation. Existing approaches for DST are usually conditioned on previous dialogue states. However, the dependency on previous dialogues makes it very challenging to prevent error propagation to subsequent turns of a dialogue. In this paper, we propose Neural Retrieval Augmentation to alleviate this problem by creating a Neural Index based on dialogue context. Our NRA-DST framework efficiently retrieves dialogue context from the index built using a combination of unstructured dialogue state and structured user/system utterances. We explore a simple pipeline resulting in a retrieval-guided generation approach for training a DST model. Experiments on different retrieval methods for augmentation show that neural retrieval augmentation is the best performing retrieval method for DST. Our evaluations on the large-scale MultiWOZ dataset show that our model outperforms the baseline approaches.</abstract>
      <url hash="e10289c1">2022.findings-aacl.16</url>
      <bibkey>ravuru-etal-2022-multi</bibkey>
    </paper>
    <paper id="17">
      <title><fixed-case>T</fixed-case>a<fixed-case>KG</fixed-case>: A New Dataset for Paragraph-level Table-to-Text Generation Enhanced with Knowledge Graphs</title>
      <author><first>Qianqian</first><last>Qi</last></author>
      <author><first>Zhenyun</first><last>Deng</last></author>
      <author><first>Yonghua</first><last>Zhu</last></author>
      <author><first>Lia Jisoo</first><last>Lee</last></author>
      <author><first>Michael</first><last>Witbrock</last></author>
      <author><first>Jiamou</first><last>Liu</last></author>
      <pages>176–187</pages>
      <abstract>We introduce TaKG, a new table-to-text generation dataset with the following highlights: (1) TaKG defines a long-text (paragraph-level) generation task as opposed to well-established short-text (sentence-level) generation datasets. (2) TaKG is the first large-scale dataset for this task, containing three application domains and ~750,000 samples. (3) To address the divergence phenomenon, TaKG enhances table input using external knowledge graphs, extracted by a new Wikidata-based method. We then propose a new Transformer-based multimodal sequence-to-sequence architecture for TaKG that integrates two pretrained language models RoBERTa and GPT-2. Our model shows reliable performance on long-text generation across a variety of metrics, and outperforms existing models for short-text generation tasks.</abstract>
      <url hash="6fac3aff">2022.findings-aacl.17</url>
      <bibkey>qi-etal-2022-takg</bibkey>
    </paper>
    <paper id="18">
      <title>Revisiting Checkpoint Averaging for Neural Machine Translation</title>
      <author><first>Yingbo</first><last>Gao</last></author>
      <author><first>Christian</first><last>Herold</last></author>
      <author><first>Zijian</first><last>Yang</last></author>
      <author><first>Hermann</first><last>Ney</last></author>
      <pages>188–196</pages>
      <abstract>Checkpoint averaging is a simple and effective method to boost the performance of converged neural machine translation models. The calculation is cheap to perform and the fact that the translation improvement almost comes for free, makes it widely adopted in neural machine translation research. Despite the popularity, the method itself simply takes the mean of the model parameters from several checkpoints, the selection of which is mostly based on empirical recipes without many justifications. In this work, we revisit the concept of checkpoint averaging and consider several extensions. Specifically, we experiment with ideas such as using different checkpoint selection strategies, calculating weighted average instead of simple mean, making use of gradient information and fine-tuning the interpolation weights on development data. Our results confirm the necessity of applying checkpoint averaging for optimal performance, but also suggest that the landscape between the converged checkpoints is rather flat and not much further improvement compared to simple averaging is to be obtained.</abstract>
      <url hash="42289108">2022.findings-aacl.18</url>
      <bibkey>gao-etal-2022-revisiting</bibkey>
    </paper>
    <paper id="19">
      <title>Modeling Referential Gaze in Task-oriented Settings of Varying Referential Complexity</title>
      <author><first>Özge</first><last>Alacam</last></author>
      <author><first>Eugen</first><last>Ruppert</last></author>
      <author><first>Sina</first><last>Zarrieß</last></author>
      <author><first>Ganeshan</first><last>Malhotra</last></author>
      <author><first>Chris</first><last>Biemann</last></author>
      <author><first>Sina</first><last>Zarrieß</last></author>
      <pages>197–210</pages>
      <abstract>Referential gaze is a fundamental phenomenon for psycholinguistics and human-human communication. However, modeling referential gaze for real-world scenarios, e.g. for task-oriented communication, is lacking the well-deserved attention from the NLP community. In this paper, we address this challenging issue by proposing a novel multimodal NLP task; namely predicting when the gaze is referential. We further investigate how to model referential gaze and transfer gaze features to adapt to unseen situated settings that target different referential complexities than the training environment. We train (i) a sequential attention-based LSTM model and (ii) a multivariate transformer encoder architecture to predict whether the gaze is on a referent object. The models are evaluated on the three complexity datasets. The results indicate that the gaze features can be transferred not only among various similar tasks and scenes but also across various complexity levels. Taking the referential complexity of a scene into account is important for successful target prediction using gaze parameters especially when there is not much data for fine-tuning.</abstract>
      <url hash="13d02d84">2022.findings-aacl.19</url>
      <bibkey>alacam-etal-2022-modeling</bibkey>
    </paper>
    <paper id="20">
      <title>Automating Interlingual Homograph Recognition with Parallel Sentences</title>
      <author><first>Yi</first><last>Han</last></author>
      <author><first>Ryohei</first><last>Sasano</last></author>
      <author><first>Koichi</first><last>Takeda</last></author>
      <pages>211–216</pages>
      <abstract>Interlingual homographs are words that spell the same but possess different meanings across languages. Recognizing interlingual homographs from form-identical words generally needs linguistic knowledge and massive annotation work. In this paper, we propose an automatic interlingual homograph recognition method based on the cross-lingual word embedding similarity and co-occurrence of form-identical words in parallel sentences. We conduct experiments with various off-the-shelf language models coordinating with cross-lingual alignment operations and co-occurrence metrics on the Chinese-Japanese and English-Dutch language pairs. Experimental results demonstrate that our proposed method is able to make accurate and consistent predictions across languages.</abstract>
      <url hash="1c366a77">2022.findings-aacl.20</url>
      <bibkey>han-etal-2022-automating</bibkey>
    </paper>
    <paper id="21">
      <title><fixed-case>C</fixed-case>o<fixed-case>RAL</fixed-case>: a Context-aware <fixed-case>C</fixed-case>roatian Abusive Language Dataset</title>
      <author><first>Ravi</first><last>Shekhar</last></author>
      <author><first>Mladen</first><last>Karan</last></author>
      <author><first>Matthew</first><last>Purver</last></author>
      <pages>217–225</pages>
      <abstract>In light of unprecedented increases in the popularity of the internet and social media, comment moderation has never been a more relevant task. Semi-automated comment moderation systems greatly aid human moderators by either automatically classifying the examples or allowing the moderators to prioritize which comments to consider first. However, the concept of inappropriate content is often subjective, and such content can be conveyed in many subtle and indirect ways. In this work, we propose CoRAL – a language and culturally aware Croatian Abusive dataset covering phenomena of implicitness and reliance on local and global context. We show experimentally that current models degrade when comments are not explicit and further degrade when language skill and context knowledge are required to interpret the comment.</abstract>
      <url hash="2e94e643">2022.findings-aacl.21</url>
      <bibkey>shekhar-etal-2022-coral</bibkey>
      <video href="2022.findings-aacl.21.mp4"/>
    </paper>
    <paper id="22">
      <title>A Copy Mechanism for Handling Knowledge Base Elements in <fixed-case>SPARQL</fixed-case> Neural Machine Translation</title>
      <author><first>Rose</first><last>Hirigoyen</last></author>
      <author><first>Amal</first><last>Zouaq</last></author>
      <author><first>Samuel</first><last>Reyd</last></author>
      <pages>226–236</pages>
      <abstract>Neural Machine Translation (NMT) models from English to SPARQL are a promising development for SPARQL query generation. However, current architectures are unable to integrate the knowledge base (KB) schema and handle questions on knowledge resources, classes, and properties unseen during training, rendering them unusable outside the scope of topics covered in the training set. Inspired by the performance gains in natural language processing tasks, we propose to integrate a copy mechanism for neural SPARQL query generation as a way to tackle this issue. We illustrate our proposal by adding a copy layer and a dynamic knowledge base vocabulary to two Seq2Seq architectures (CNNs and Transformers). This layer makes the models copy KB elements directly from the questions, instead of generating them. We evaluate our approach on state-of-the-art datasets, including datasets referencing unknown KB elements and measure the accuracy of the copy-augmented architectures. Our results show a considerable increase in performance on all datasets compared to non-copy architectures.</abstract>
      <url hash="b88d4377">2022.findings-aacl.22</url>
      <bibkey>hirigoyen-etal-2022-copy</bibkey>
    </paper>
    <paper id="23">
      <title>A Multilingual Multiway Evaluation Data Set for Structured Document Translation of <fixed-case>A</fixed-case>sian Languages</title>
      <author><first>Bianka</first><last>Buschbeck</last></author>
      <author><first>Raj</first><last>Dabre</last></author>
      <author><first>Miriam</first><last>Exel</last></author>
      <author><first>Matthias</first><last>Huck</last></author>
      <author><first>Patrick</first><last>Huy</last></author>
      <author><first>Raphael</first><last>Rubino</last></author>
      <author><first>Hideki</first><last>Tanaka</last></author>
      <pages>237–245</pages>
      <abstract>Translation of structured content is an important application of machine translation, but the scarcity of evaluation data sets, especially for Asian languages, limits progress. In this paper we present a novel multilingual multiway evaluation data set for the translation of structured documents of the Asian languages Japanese, Korean and Chinese. We describe the data set, its creation process and important characteristics, followed by establishing and evaluating baselines using the direct translation as well as detag-project approaches. Our data set is well suited for multilingual evaluation, and it contains richer annotation tag sets than existing data sets. Our results show that massively multilingual translation models like M2M-100 and mBART-50 perform surprisingly well despite not being explicitly trained to handle structured content. The data set described in this paper and used in our experiments is released publicly.</abstract>
      <url hash="65a1be60">2022.findings-aacl.23</url>
      <bibkey>buschbeck-etal-2022-multilingual</bibkey>
    </paper>
    <paper id="24">
      <title>On Measures of Biases and Harms in <fixed-case>NLP</fixed-case></title>
      <author><first>Sunipa</first><last>Dev</last></author>
      <author><first>Emily</first><last>Sheng</last></author>
      <author><first>Jieyu</first><last>Zhao</last></author>
      <author><first>Aubrie</first><last>Amstutz</last></author>
      <author><first>Jiao</first><last>Sun</last></author>
      <author><first>Yu</first><last>Hou</last></author>
      <author><first>Mattie</first><last>Sanseverino</last></author>
      <author><first>Jiin</first><last>Kim</last></author>
      <author><first>Akihiro</first><last>Nishi</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <pages>246–267</pages>
      <abstract>Recent studies show that Natural Language Processing (NLP) technologies propagate societal biases about demographic groups associated with attributes such as gender, race, and nationality. To create interventions and mitigate these biases and associated harms, it is vital to be able to detect and measure such biases. While existing works propose bias evaluation and mitigation methods for various tasks, there remains a need to cohesively understand the biases and the specific harms they measure, and how different measures compare with each other. To address this gap, this work presents a practical framework of harms and a series of questions that practitioners can answer to guide the development of bias measures. As a validation of our framework and documentation questions, we also present several case studies of how existing bias measures in NLP—both intrinsic measures of bias in representations and extrinsic measures of bias of downstream applications—can be aligned with different harms and how our proposed documentation questions facilitates more holistic understanding of what bias measures are measuring.</abstract>
      <url hash="5f4fae86">2022.findings-aacl.24</url>
      <bibkey>dev-etal-2022-measures</bibkey>
    </paper>
    <paper id="25">
      <title>Logographic Information Aids Learning Better Representations for Natural Language Inference</title>
      <author><first>Zijian</first><last>Jin</last></author>
      <author><first>Duygu</first><last>Ataman</last></author>
      <pages>268–273</pages>
      <abstract>Statistical language models conventionally implement representation learning based on the contextual distribution of words or other formal units, whereas any information related to the logographic features of written text are often ignored, assuming they should be retrieved relying on the cooccurence statistics. On the other hand, as language models become larger and require more data to learn reliable representations, such assumptions may start to fall back, especially under conditions of data sparsity. Many languages, including Chinese and Vietnamese, use logographic writing systems where surface forms are represented as a visual organization of smaller graphemic units, which often contain many semantic cues. In this paper, we present a novel study which explores the benefits of providing language models with logographic information in learning better semantic representations. We test our hypothesis in the natural language inference (NLI) task by evaluating the benefit of computing multi-modal representations that combine contextual information with glyph information. Our evaluation results in six languages with different typology and writing systems suggest significant benefits of using multi-modal embeddings in languages with logograhic systems, especially for words with less occurence statistics.</abstract>
      <url hash="05ebe9fc">2022.findings-aacl.25</url>
      <bibkey>jin-ataman-2022-logographic</bibkey>
    </paper>
    <paper id="26">
      <title>Cross-domain Analysis on <fixed-case>J</fixed-case>apanese Legal Pretrained Language Models</title>
      <author><first>Keisuke</first><last>Miyazaki</last></author>
      <author><first>Hiroaki</first><last>Yamada</last></author>
      <author><first>Takenobu</first><last>Tokunaga</last></author>
      <pages>274–281</pages>
      <abstract>This paper investigates the pretrained language model (PLM) specialised in the Japanese legal domain. We create PLMs using different pretraining strategies and investigate their performance across multiple domains. Our findings are (i) the PLM built with general domain data can be improved by further pretraining with domain-specific data, (ii) domain-specific PLMs can learn domain-specific and general word meanings simultaneously and can distinguish them, (iii) domain-specific PLMs work better on its target domain; still, the PLMs retain the information learnt in the original PLM even after being further pretrained with domain-specific data, (iv) the PLMs sequentially pretrained with corpora of different domains show high performance for the later learnt domains.</abstract>
      <url hash="b3b66943">2022.findings-aacl.26</url>
      <bibkey>miyazaki-etal-2022-cross</bibkey>
    </paper>
    <paper id="27">
      <title>Multilingual <fixed-case>C</fixed-case>heck<fixed-case>L</fixed-case>ist: Generation and Evaluation</title>
      <author><first>Karthikeyan</first><last>K</last></author>
      <author><first>Shaily</first><last>Bhatt</last></author>
      <author><first>Pankaj</first><last>Singh</last></author>
      <author><first>Somak</first><last>Aditya</last></author>
      <author><first>Sandipan</first><last>Dandapat</last></author>
      <author><first>Sunayana</first><last>Sitaram</last></author>
      <author><first>Monojit</first><last>Choudhury</last></author>
      <pages>282–295</pages>
      <abstract>Multilingual evaluation benchmarks usually contain limited high-resource languages and do not test models for specific linguistic capabilities. CheckList is a template-based evaluation approach that tests models for specific capabilities. The CheckList template creation process requires native speakers, posing a challenge in scaling to hundreds of languages. In this work, we explore multiple approaches to generate Multilingual CheckLists. We device an algorithm –Template Extraction Algorithm (TEA) for automatically extracting target language CheckList templates from machine translated instances of a source language templates. We compare the TEA CheckLists with CheckLists created with different levels of human intervention. We further introduce metrics along the dimensions of cost, diversity, utility, and correctness to compare the CheckLists. We thoroughly analyze different approaches to creating CheckLists in Hindi. Furthermore, we experiment with 9 more different languages. We find that TEA followed by human verification is ideal for scaling Checklist-based evaluation to multiple languages while TEA gives a good estimates of model performance. We release the code of TEA and the CheckLists created at aka.ms/multilingualchecklist</abstract>
      <url hash="0a36532b">2022.findings-aacl.27</url>
      <bibkey>k-etal-2022-multilingual</bibkey>
    </paper>
    <paper id="28">
      <title>Part Represents Whole: Improving the Evaluation of Machine Translation System Using Entropy Enhanced Metrics</title>
      <author><first>Yilun</first><last>Liu</last></author>
      <author><first>Shimin</first><last>Tao</last></author>
      <author><first>Chang</first><last>Su</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <author><first>Yanqing</first><last>Zhao</last></author>
      <author><first>Hao</first><last>Yang</last></author>
      <pages>296–307</pages>
      <abstract>Machine translation (MT) metrics often experience poor correlations with human assessments. In terms of MT system evaluation, most metrics pay equal attentions to every sample in an evaluation set, while in human evaluation, difficult sentences often make candidate systems distinguishable via notable fluctuations in human scores, especially when systems are competitive. We find that samples with high entropy values, which though usually count less than 5%, tend to play a key role in MT evaluation: when the evaluation set is shrunk to only the high-entropy portion, correlations with human assessments are actually improved. Thus, in this paper, we propose a fast and unsupervised approach to enhance MT metrics using entropy, expanding the dimension of evaluation by introducing sentence-level difficulty. A translation hypothesis with a significantly high entropy value is considered difficult and receives a large weight in aggregation of system-level scores. Experimental results on five sub-tracks in the WMT19 Metrics shared tasks show that our proposed method significantly enhanced the performance of commonly-used MT metrics in terms of system-level correlations with human assessments, even outperforming existing SOTA metrics. In particular, all enhanced metrics exhibit overall stability in correlations with human assessments in circumstances where only competitive MT systems are included, while the corresponding vanilla metrics fail to correlate with human assessments.</abstract>
      <url hash="c5bedc49">2022.findings-aacl.28</url>
      <bibkey>liu-etal-2022-part</bibkey>
    </paper>
    <paper id="29">
      <title>Memformer: A Memory-Augmented Transformer for Sequence Modeling</title>
      <author><first>Qingyang</first><last>Wu</last></author>
      <author><first>Zhenzhong</first><last>Lan</last></author>
      <author><first>Kun</first><last>Qian</last></author>
      <author><first>Jing</first><last>Gu</last></author>
      <author><first>Alborz</first><last>Geramifard</last></author>
      <author><first>Zhou</first><last>Yu</last></author>
      <pages>308–318</pages>
      <abstract>Transformers have reached remarkable success in sequence modeling. However, these models have efficiency issues as they need to store all the history token-level representations as memory. We present Memformer, an efficient neural network for sequence modeling, that utilizes an external dynamic memory to encode and retrieve past information. Our model achieves linear time complexity and constant memory space complexity when processing long sequences. We also propose a new optimization scheme, memory replay back-propagation (MRBP), which promotes long-range back-propagation through time with a significantly reduced memory requirement. Experimental results show that Memformer has achieved comparable performance compared against the baselines by using 8.1x less memory space and 3.2x faster on inference. Analysis of the attention pattern shows that our external memory slots can encode and retain important information through timesteps.</abstract>
      <url hash="b2c2b7a4">2022.findings-aacl.29</url>
      <bibkey>wu-etal-2022-memformer</bibkey>
    </paper>
    <paper id="30">
      <title>Open-Domain Conversational Question Answering with Historical Answers</title>
      <author><first>Hung-Chieh</first><last>Fang</last></author>
      <author><first>Kuo-Han</first><last>Hung</last></author>
      <author><first>Chen-Wei</first><last>Huang</last></author>
      <author><first>Yun-Nung</first><last>Chen</last></author>
      <pages>319–326</pages>
      <abstract>Open-domain conversational question answering can be viewed as two tasks: passage retrieval and conversational question answering, where the former relies on selecting candidate passages from a large corpus and the latter requires better understanding of a question with contexts to predict the answers. This paper proposes ConvADR-QA that leverages historical answers to boost retrieval performance and further achieves better answering performance. Our experiments on the benchmark dataset, OR-QuAC, demonstrate that our model outperforms existing baselines in both extractive and generative reader settings, well justifying the effectiveness of historical answers for open-domain conversational question answering.</abstract>
      <url hash="a15cd604">2022.findings-aacl.30</url>
      <bibkey>fang-etal-2022-open</bibkey>
    </paper>
    <paper id="31">
      <title>Robustness Evaluation of Text Classification Models Using Mathematical Optimization and Its Application to Adversarial Training</title>
      <author><first>Hikaru</first><last>Tomonari</last></author>
      <author><first>Masaaki</first><last>Nishino</last></author>
      <author><first>Akihiro</first><last>Yamamoto</last></author>
      <pages>327–333</pages>
      <abstract>Neural networks are known to be vulnerable to adversarial examples due to slightly perturbed input data. In practical applications of neural network models, the robustness of the models against perturbations must be evaluated. However, no method can strictly evaluate their robustness in natural language domains. We therefore propose a method that evaluates the robustness of text classification models using an integer linear programming (ILP) solver by an optimization problem that identifies a minimum synonym swap that changes the classification result. Our method allows us to compare the robustness of various models in realistic time. It can also be used for obtaining adversarial examples. Because of the minimal impact on the altered sentences, adversarial examples with our method obtained high scores in human evaluations of grammatical correctness and semantic similarity for an IMDb dataset. In addition, we implemented adversarial training with the IMDb and SST2 datasets and found that our adversarial training method makes the model robust.</abstract>
      <url hash="28a6d757">2022.findings-aacl.31</url>
      <bibkey>tomonari-etal-2022-robustness</bibkey>
    </paper>
    <paper id="32">
      <title><fixed-case>HERB</fixed-case>: Measuring Hierarchical Regional Bias in Pre-trained Language Models</title>
      <author><first>Yizhi</first><last>Li</last></author>
      <author><first>Ge</first><last>Zhang</last></author>
      <author><first>Bohao</first><last>Yang</last></author>
      <author><first>Chenghua</first><last>Lin</last></author>
      <author><first>Anton</first><last>Ragni</last></author>
      <author><first>Shi</first><last>Wang</last></author>
      <author><first>Jie</first><last>Fu</last></author>
      <pages>334–346</pages>
      <abstract>Fairness has become a trending topic in natural language processing (NLP) and covers biases targeting certain social groups such as genders and religions. Yet regional bias, another long-standing global discrimination problem, remains unexplored still. Consequently, we intend to provide a study to analyse the regional bias learned by the pre-trained language models (LMs) that are broadly used in NLP tasks. While verifying the existence of regional bias in LMs, we find that the biases on regional groups can be largely affected by the corresponding geographical clustering. We accordingly propose a hierarchical regional bias evaluation method (HERB) utilising the information from the sub-region clusters to quantify the bias in the pre-trained LMs. Experiments show that our hierarchical metric can effectively evaluate the regional bias with regard to comprehensive topics and measure the potential regional bias that can be propagated to downstream tasks. Our codes are available at <url>https://github.com/Bernard-Yang/HERB</url>.</abstract>
      <url hash="f5d310c5">2022.findings-aacl.32</url>
      <bibkey>li-etal-2022-herb</bibkey>
    </paper>
    <paper id="33">
      <title>Multilingual Auxiliary Tasks Training: Bridging the Gap between Languages for Zero-Shot Transfer of Hate Speech Detection Models</title>
      <author><first>Syrielle</first><last>Montariol</last></author>
      <author><first>Arij</first><last>Riabi</last></author>
      <author><first>Djamé</first><last>Seddah</last></author>
      <pages>347–363</pages>
      <abstract>Zero-shot cross-lingual transfer learning has been shown to be highly challenging for tasks involving a lot of linguistic specificities or when a cultural gap is present between lan- guages, such as in hate speech detection. In this paper, we highlight this limitation for hate speech detection in several domains and languages using strict experimental settings. Then, we propose to train on multilingual auxiliary tasks – sentiment analysis, named entity recognition, and tasks relying on syntactic information – to improve zero-shot transfer of hate speech detection models across languages. We show how hate speech detection models benefit from a cross-lingual knowledge proxy brought by auxiliary tasks fine-tuning and highlight these tasks’ positive impact on bridging the hate speech linguistic and cultural gap between languages.</abstract>
      <url hash="7ab68fe7">2022.findings-aacl.33</url>
      <bibkey>montariol-etal-2022-multilingual</bibkey>
    </paper>
    <paper id="34">
      <title>Chop and Change: Anaphora Resolution in Instructional Cooking Videos</title>
      <author><first>Cennet</first><last>Oguz</last></author>
      <author><first>Ivana</first><last>Kruijff-Korbayova</last></author>
      <author><first>Emmanuel</first><last>Vincent</last></author>
      <author><first>Pascal</first><last>Denis</last></author>
      <author><first>Josef</first><last>van Genabith</last></author>
      <pages>364–374</pages>
      <abstract>Linguistic ambiguities arising from changes in entities in action flows are a key challenge in instructional cooking videos. In particular, temporally evolving entities present rich and to date understudied challenges for anaphora resolution. For example “oil” mixed with “salt” is later referred to as a “mixture”. In this paper we propose novel annotation guidelines to annotate recipes for the anaphora resolution task, reflecting change in entities. Moreover, we present experimental results for end-to-end multimodal anaphora resolution with the new annotation scheme and propose the use of temporal features for performance improvement.</abstract>
      <url hash="83df2f20">2022.findings-aacl.34</url>
      <bibkey>oguz-etal-2022-chop</bibkey>
    </paper>
    <paper id="35">
      <title>“#<fixed-case>D</fixed-case>isabled<fixed-case>O</fixed-case>n<fixed-case>I</fixed-case>ndian<fixed-case>T</fixed-case>witter” : A Dataset towards Understanding the Expression of People with Disabilities on <fixed-case>I</fixed-case>ndian <fixed-case>T</fixed-case>witter</title>
      <author><first>Ishani</first><last>Mondal</last></author>
      <author><first>Sukhnidh</first><last>Kaur</last></author>
      <author><first>Kalika</first><last>Bali</last></author>
      <author><first>Aditya</first><last>Vashistha</last></author>
      <author><first>Manohar</first><last>Swaminathan</last></author>
      <pages>375–386</pages>
      <abstract>Twitter serves as a powerful tool for self-expression among the disabled people. To understand how disabled people in India use Twitter, we introduce a manually annotated corpus #DisabledOnIndianTwitter comprising of 2,384 tweets posted by 27 female and 15 male users. These users practice diverse professions and engage in varied online discourses on disability in India. To examine patterns in their Twitter use, we propose a novel hierarchical annotation taxonomy to classify the tweets into various themes including discrimination, advocacy, and self-identification. Using these annotations, we benchmark the corpus leveraging state-of-the-art classifiers. Finally through a mixed-methods analysis on our annotated corpus, we reveal stark differences in self-expression between male and female disabled users on Indian Twitter.</abstract>
      <url hash="ed67555e">2022.findings-aacl.35</url>
      <bibkey>mondal-etal-2022-disabledonindiantwitter</bibkey>
    </paper>
    <paper id="36">
      <title>Topic-aware Multimodal Summarization</title>
      <author><first>Sourajit</first><last>Mukherjee</last></author>
      <author><first>Anubhav</first><last>Jangra</last></author>
      <author><first>Sriparna</first><last>Saha</last></author>
      <author><first>Adam</first><last>Jatowt</last></author>
      <pages>387–398</pages>
      <abstract>Multimodal Summarization (MS) has attracted research interest in the past few years due to the ease with which users perceive multimodal summaries. It is important for MS models to consider the topic a given target content belongs to. In the current paper, we propose a topic-aware MS system which performs two tasks simultaneously: differentiating the images into “on-topic” and “off-topic” categories and further utilizing the “on-topic” images to generate multimodal summaries. The hypothesis is that, the proposed topic similarity classifier will help in generating better multimodal summary by focusing on important components of images and text which are specific to a particular topic. To develop the topic similarity classifier, we have augmented the existing popular MS data set, MSMO, with similar “on-topic” and dissimilar “off-topic” images for each sample. Our experimental results establish that the focus on “on-topic” features helps in generating topic-aware multimodal summaries, which outperforms the state of the art approach by 1.7 % in ROUGE-L metric.</abstract>
      <url hash="20979116">2022.findings-aacl.36</url>
      <attachment type="Software" hash="5a8d9152">2022.findings-aacl.36.Software.zip</attachment>
      <attachment type="Dataset" hash="44f09028">2022.findings-aacl.36.Dataset.zip</attachment>
      <bibkey>mukherjee-etal-2022-topic</bibkey>
    </paper>
    <paper id="37">
      <title><fixed-case>A</fixed-case>rg<fixed-case>G</fixed-case>en: Prompting Text Generation Models for Document-Level Event-Argument Aggregation</title>
      <author><first>Debanjana</first><last>Kar</last></author>
      <author><first>Sudeshna</first><last>Sarkar</last></author>
      <author><first>Pawan</first><last>Goyal</last></author>
      <pages>399–404</pages>
      <abstract>Most of the existing discourse-level Information Extraction tasks have been modeled to be extractive in nature. However, we argue that extracting information from larger bodies of discourse-like documents requires more natural language understanding and reasoning capabilities. In our work, we propose the novel task of document-level event argument aggregation which generates consolidated event-arguments at a document-level with minimal loss of information. More specifically, we focus on generating precise document-level information frames in a multilingual setting using prompt-based methods. In this paper, we show the effectiveness of u prompt-based text generation approach to generate document-level argument spans in a low-resource and zero-shot setting. We also release the first of its kind multilingual event argument aggregation dataset that can be leveraged in other related multilingual text generation tasks as well: <url>https://github.com/DebanjanaKar/ArgGen</url>.</abstract>
      <url hash="159b7527">2022.findings-aacl.37</url>
      <bibkey>kar-etal-2022-arggen</bibkey>
    </paper>
    <paper id="38">
      <title>Hierarchical Processing of Visual and Language Information in the Brain</title>
      <author><first>Haruka</first><last>Kawasaki</last></author>
      <author><first>Satoshi</first><last>Nishida</last></author>
      <author><first>Ichiro</first><last>Kobayashi</last></author>
      <pages>405–410</pages>
      <abstract>In recent years, many studies using deep learning have been conducted to elucidate the mechanism of information representation in the brain under stimuli evoked by various modalities. On the other hand, it has not yet been clarified how we humans link information of different modalities in the brain. In this study, to elucidate the relationship between visual and language information in the brain, we constructed encoding models that predict brain activity based on features extracted from the hidden layers of VGG16 for visual information and BERT for language information. We investigated the hierarchical characteristics of cortical localization and representational content of visual and semantic information in the cortex based on the brain activity predicted by the encoding model. The results showed that the cortical localization modeled by VGG16 is getting close to that of BERT as VGG16 moves to higher layers, while the representational contents differ significantly between the two modalities.</abstract>
      <url hash="51839194">2022.findings-aacl.38</url>
      <bibkey>kawasaki-etal-2022-hierarchical</bibkey>
    </paper>
    <paper id="39">
      <title>Differential Bias: On the Perceptibility of Stance Imbalance in Argumentation</title>
      <author><first>Alonso</first><last>Palomino</last></author>
      <author><first>Khalid</first><last>Al Khatib</last></author>
      <author><first>Martin</first><last>Potthast</last></author>
      <author><first>Benno</first><last>Stein</last></author>
      <pages>411–421</pages>
      <abstract>Most research on natural language processing treats bias as an absolute concept: Based on a (probably complex) algorithmic analysis, a sentence, an article, or a text is classified as biased or not. Given the fact that for humans the question of whether a text is biased can be difficult to answer or is answered contradictory, we ask whether an “absolute bias classification” is a promising goal at all. We see the problem not in the complexity of interpreting language phenomena but in the diversity of sociocultural backgrounds of the readers, which cannot be handled uniformly: To decide whether a text has crossed the proverbial line between non-biased and biased is subjective. By asking “Is text X more [less, equally] biased than text Y?” we propose to analyze a simpler problem, which, by its construction, is rather independent of standpoints, views, or sociocultural aspects. In such a model, bias becomes a preference relation that induces a partial ordering from least biased to most biased texts without requiring a decision on where to draw the line. A prerequisite for this kind of bias model is the ability of humans to perceive relative bias differences in the first place. In our research, we selected a specific type of bias in argumentation, the stance bias, and designed a crowdsourcing study showing that differences in stance bias are perceptible when (light) support is provided through training or visual aid.</abstract>
      <url hash="35134d6b">2022.findings-aacl.39</url>
      <bibkey>palomino-etal-2022-differential</bibkey>
    </paper>
    <paper id="40">
      <title><fixed-case>B</fixed-case>eam<fixed-case>R</fixed-case>: Beam Reweighing with Attribute Discriminators for Controllable Text Generation</title>
      <author><first>David</first><last>Landsman</last></author>
      <author><first>Jerry Zikun</first><last>Chen</last></author>
      <author><first>Hussain</first><last>Zaidi</last></author>
      <pages>422–437</pages>
      <abstract>Recent advances in natural language processing have led to the availability of large pre-trained language models (LMs), with rich generative capabilities. Although these models are able to produce fluent and coherent text, it remains a challenge to control various attributes of the generation, including sentiment, formality, topic and many others. We propose a Beam Reweighing (BeamR) method, building on top of standard beam search, in order to control different attributes. BeamR combines any generative LM with any attribute discriminator, offering full flexibility of generation style and attribute, while the beam search backbone maintains fluency across different domains. Notably, BeamR allows practitioners to leverage pre-trained models without the need to train generative LMs together with discriminators. We evaluate BeamR in two diverse tasks: sentiment steering, and machine translation formality. Our results show that BeamR performs on par with or better than existing state-of-the-art approaches (including fine-tuned methods), and highlight the flexiblity of BeamR in both causal and seq2seq language modeling tasks.</abstract>
      <url hash="18372907">2022.findings-aacl.40</url>
      <bibkey>landsman-etal-2022-beamr</bibkey>
    </paper>
    <paper id="41">
      <title><fixed-case>R</fixed-case>&amp;<fixed-case>R</fixed-case>: Metric-guided Adversarial Sentence Generation</title>
      <author><first>Lei</first><last>Xu</last></author>
      <author><first>Alfredo</first><last>Cuesta-Infante</last></author>
      <author><first>Laure</first><last>Berti-Equille</last></author>
      <author><first>Kalyan</first><last>Veeramachaneni</last></author>
      <pages>438–452</pages>
      <abstract>Adversarial examples are helpful for analyzing and improving the robustness of text classifiers. Generating high-quality adversarial examples is a challenging task as it requires generating fluent adversarial sentences that are semantically similar to the original sentences and preserve the original labels, while causing the classifier to misclassify them. Existing methods prioritize misclassification by maximizing each perturbation’s effectiveness at misleading a text classifier; thus, the generated adversarial examples fall short in terms of fluency and similarity. In this paper, we propose a rewrite and rollback (R&amp;R) framework for adversarial attack. It improves the quality of adversarial examples by optimizing a critique score which combines the fluency, similarity, and misclassification metrics. R&amp;R generates high-quality adversarial examples by allowing exploration of perturbations that do not have immediate impact on the misclassification metric but can improve fluency and similarity metrics. We evaluate our method on 5 representative datasets and 3 classifier architectures. Our method outperforms current state-of-the-art in attack success rate by +16.2%, +12.8%, and +14.0% on the classifiers respectively. Code is available at <url>https://github.com/DAI-Lab/fibber</url></abstract>
      <url hash="901d8f84">2022.findings-aacl.41</url>
      <bibkey>xu-etal-2022-r</bibkey>
    </paper>
    <paper id="42">
      <title>A Simple yet Effective Learnable Positional Encoding Method for Improving Document Transformer Model</title>
      <author><first>Guoxin</first><last>Wang</last></author>
      <author><first>Yijuan</first><last>Lu</last></author>
      <author><first>Lei</first><last>Cui</last></author>
      <author><first>Tengchao</first><last>Lv</last></author>
      <author><first>Dinei</first><last>Florencio</last></author>
      <author><first>Cha</first><last>Zhang</last></author>
      <pages>453–463</pages>
      <abstract>Positional encoding plays a key role in Transformer-based architecture, which is to indicate and embed token sequential order information. Understanding documents with unreliable reading order information is a real challenge for document Transformer models. This paper proposes a simple and effective positional encoding method, learnable sinusoidal positional encoding (LSPE), by building a learnable sinusoidal positional encoding feed-forward network. We apply LSPE to document Transformer models and pretrain them on document datasets. Then we finetune and evaluate the model performance on document understanding tasks in form, receipt, and invoice domains. Experimental results show our proposed method not only outperforms other baselines, but also demonstrates its robustness and stability on handling noisy data with incorrect order information.</abstract>
      <url hash="1f335532">2022.findings-aacl.42</url>
      <bibkey>wang-etal-2022-simple-yet</bibkey>
    </paper>
    <paper id="43">
      <title><fixed-case>MMM</fixed-case>: An Emotion and Novelty-aware Approach for Multilingual Multimodal Misinformation Detection</title>
      <author><first>Vipin</first><last>Gupta</last></author>
      <author><first>Rina</first><last>Kumari</last></author>
      <author><first>Nischal</first><last>Ashok</last></author>
      <author><first>Tirthankar</first><last>Ghosal</last></author>
      <author><first>Asif</first><last>Ekbal</last></author>
      <pages>464–477</pages>
      <abstract>The growth of multilingual web content in low-resource languages is becoming an emerging challenge to detect misinformation. One particular hindrance to research on this problem is the non-availability of resources and tools. Majority of the earlier works in misinformation detection are based on English content which confines the applicability of the research to a specific language only. Increasing presence of multimedia content on the web has promoted misinformation in which real multimedia content (images, videos) are used in different but related contexts with manipulated texts to mislead the readers. Detecting this category of misleading information is almost impossible without any prior knowledge. Studies say that emotion-invoking and highly novel content accelerates the dissemination of false information. To counter this problem, here in this paper, we first introduce a novel multilingual multimodal misinformation dataset that includes background knowledge (from authentic sources) of the misleading articles. Second, we propose an effective neural model leveraging novelty detection and emotion recognition to detect fabricated information. We perform extensive experiments to justify that our proposed model outperforms the state-of-the-art (SOTA) on the concerned task.</abstract>
      <url hash="05b4cd7a">2022.findings-aacl.43</url>
      <bibkey>gupta-etal-2022-mmm</bibkey>
    </paper>
    <paper id="44">
      <title>Adversarial Sample Generation for Aspect based Sentiment Classification</title>
      <author><first/><last>Mamta</last></author>
      <author><first>Asif</first><last>Ekbal</last></author>
      <pages>478–492</pages>
      <abstract>Deep learning models have been proven vulnerable towards small imperceptible perturbed input, known as adversarial samples, which are indiscernible by humans. Initial attacks in Natural Language Processing perturb characters or words in sentences using heuristics and synonyms-based strategies, resulting in grammatical incorrect or out-of-context sentences. Recent works attempt to generate contextual adversarial samples using a masked language model, capturing word relevance using leave-one-out (LOO). However, they lack the design to maintain the semantic coherency for aspect based sentiment analysis (ABSA) tasks. Moreover, they focused on resource-rich languages like English. We present an attack algorithm for the ABSA task by exploiting model explainability techniques to address these limitations. It does not require access to the training data, raw access to the model, or calibrating a new model. Our proposed method generates adversarial samples for a given aspect, maintaining more semantic coherency. In addition, it can be generalized to low-resource languages, which are at high risk due to resource scarcity. We show the effectiveness of the proposed attack using automatic and human evaluation. Our method outperforms the state-of-art methods in perturbation ratio, success rate, and semantic coherence.</abstract>
      <url hash="5a216bde">2022.findings-aacl.44</url>
      <bibkey>-ekbal-2022-adversarial</bibkey>
    </paper>
  </volume>
  <volume id="emnlp" ingest-date="2022-12-08" type="proceedings">
    <meta>
      <booktitle>Findings of the Association for Computational Linguistics: EMNLP 2022</booktitle>
      <editor><first>Yoav</first><last>Goldberg</last></editor>
      <editor><first>Zornitsa</first><last>Kozareva</last></editor>
      <editor><first>Yue</first><last>Zhang</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Abu Dhabi, United Arab Emirates</address>
      <month>December</month>
      <year>2022</year>
      <url hash="d56d4453">2022.findings-emnlp</url>
      <venue>findings</venue>
    </meta>
    <frontmatter>
      <url hash="d56d4453">2022.findings-emnlp.0</url>
      <bibkey>findings-2022-findings-association-linguistics-emnlp</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>L</fixed-case>ogic<fixed-case>S</fixed-case>olver: Towards Interpretable Math Word Problem Solving with Logical Prompt-enhanced Learning</title>
      <author><first>Zhicheng</first><last>Yang</last></author>
      <author><first>Jinghui</first><last>Qin</last></author>
      <author><first>Jiaqi</first><last>Chen</last></author>
      <author><first>Liang</first><last>Lin</last></author>
      <author><first>Xiaodan</first><last>Liang</last></author>
      <pages>1-13</pages>
      <abstract>Recently, deep learning models have made great progress in MWP solving on answer accuracy. However, they are uninterpretable since they mainly rely on shallow heuristics to achieve high performance without understanding and reasoning the grounded math logic. To address this issue and make a step towards interpretable MWP solving, we first construct a high-quality MWP dataset named InterMWP which consists of 11,495 MWPs and annotates interpretable logical formulas based on algebraic knowledge as the grounded linguistic logic of each solution equation. Different from existing MWP datasets, our InterMWP benchmark asks for a solver to not only output the solution expressions but also predict the corresponding logical formulas. We further propose a novel approach with logical prompt and interpretation generation, called LogicSolver. For each MWP, our LogicSolver first retrieves some highly-correlated algebraic knowledge and then passes them to the backbone model as prompts to improve the semantic representations of MWPs. With these improved semantic representations, our LogicSolver generates corresponding solution expressions and interpretable knowledge formulas in accord with the generated solution expressions, simultaneously. Experimental results show that our LogicSolver has stronger logical formula-based interpretability than baselines while achieving higher answer accuracy with the help of logical prompts, simultaneously. The source code and dataset will be available at <url>https://github.com/yangzhch6/InterMWP</url>.</abstract>
      <url hash="43db3335">2022.findings-emnlp.1</url>
      <bibkey>yang-etal-2022-logicsolver</bibkey>
      <video href="2022.findings-emnlp.1.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.1</doi>
    </paper>
    <paper id="2">
      <title>Commonsense Knowledge Salience Evaluation with a Benchmark Dataset in <fixed-case>E</fixed-case>-commerce</title>
      <author><first>Yincen</first><last>Qu</last></author>
      <author><first>Ningyu</first><last>Zhang</last></author>
      <author><first>Hui</first><last>Chen</last></author>
      <author><first>Zelin</first><last>Dai</last></author>
      <author><first>Chengming</first><last>Wang</last></author>
      <author><first>Xiaoyu</first><last>Wang</last></author>
      <author><first>Qiang</first><last>Chen</last></author>
      <author><first>Huajun</first><last>Chen</last></author>
      <pages>14-27</pages>
      <abstract>In e-commerce, the salience of commonsense knowledge (CSK) is beneficial for widespread applications such as product search and recommendation. For example, when users search for “running” in e-commerce, they would like to find products highly related to running, such as “running shoes” rather than “shoes”. Nevertheless, many existing CSK collections rank statements solely by confidence scores, and there is no information about which ones are salient from a human perspective. In this work, we define the task of supervised salience evaluation, where given a CSK triple, the model is required to learn whether the triple is salient or not. In addition to formulating the new task, we also release a new Benchmark dataset of Salience Evaluation in E-commerce (BSEE) and hope to promote related research on commonsense knowledge salience evaluation. We conduct experiments in the dataset with several representative baseline models. The experimental results show that salience evaluation is a hard task where models perform poorly on our evaluation set. We further propose a simple but effective approach, PMI-tuning, which shows promise for solving this novel problem. Code is available in <url>https://github.com/OpenBGBenchmark/OpenBG-CSK</url>.</abstract>
      <url hash="9400ac83">2022.findings-emnlp.2</url>
      <bibkey>qu-etal-2022-commonsense</bibkey>
      <video href="2022.findings-emnlp.2.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.2</doi>
    </paper>
    <paper id="3">
      <title>Automatic Rule Induction for Efficient Semi-Supervised Learning</title>
      <author><first>Reid</first><last>Pryzant</last></author>
      <author><first>Ziyi</first><last>Yang</last></author>
      <author><first>Yichong</first><last>Xu</last></author>
      <author><first>Chenguang</first><last>Zhu</last></author>
      <author><first>Michael</first><last>Zeng</last></author>
      <pages>28-44</pages>
      <abstract>Semi-supervised learning has shown promise in allowing NLP models to generalize from small amounts of labeled data. Meanwhile, pretrained transformer models act as black-box correlation engines that are difficult to explain and sometimes behave unreliably. In this paper, we propose tackling both of these challenges via Automatic Rule Induction (ARI), a simple and general-purpose framework for the automatic discovery and integration of symbolic rules into pretrained transformer models. First, we extract weak symbolic rules from low-capacity machine learning models trained on small amounts of labeled data. Next, we use an attention mechanism to integrate these rules into high-capacity pretrained transformer models. Last, the rule-augmented system becomes part of a self-training framework to boost supervision signal on unlabeled data. These steps can be layered beneath a variety of existing weak supervision and semi-supervised NLP algorithms in order to improve performance and interpretability. Experiments across nine sequence classification and relation extraction tasks suggest that ARI can improve state-of-the-art methods with no manual effort and minimal computational overhead.</abstract>
      <url hash="51dfcc60">2022.findings-emnlp.3</url>
      <bibkey>pryzant-etal-2022-automatic</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.3</doi>
    </paper>
    <paper id="4">
      <title>Improving Semantic Matching through Dependency-Enhanced Pre-trained Model with Adaptive Fusion</title>
      <author><first>Jian</first><last>Song</last></author>
      <author><first>Di</first><last>Liang</last></author>
      <author><first>Rumei</first><last>Li</last></author>
      <author><first>Yuntao</first><last>Li</last></author>
      <author><first>Sirui</first><last>Wang</last></author>
      <author><first>Minlong</first><last>Peng</last></author>
      <author><first>Wei</first><last>Wu</last></author>
      <author><first>Yongxin</first><last>Yu</last></author>
      <pages>45-57</pages>
      <abstract>Transformer-based pre-trained models like BERT have achieved great progress on Semantic Sentence Matching. Meanwhile, dependency prior knowledge has also shown general benefits in multiple NLP tasks. However, how to efficiently integrate dependency prior structure into pre-trained models to better model complex semantic matching relations is still unsettled. In this paper, we propose the Dependency-Enhanced Adaptive Fusion Attention (DAFA), which explicitly introduces dependency structure into pre-trained models and adaptively fuses it with semantic information. Specifically, (i) DAFA first proposes a structure-sensitive paradigm to construct a dependency matrix for calibrating attention weights. (ii) It adopts an adaptive fusion module to integrate the obtained dependency information and the original semantic signals. Moreover, DAFA reconstructs the attention calculation flow and provides better interpretability. By applying it on BERT, our method achieves state-of-the-art or competitive performance on 10 public datasets, demonstrating the benefits of adaptively fusing dependency structure in semantic matching task.</abstract>
      <url hash="8db91fab">2022.findings-emnlp.4</url>
      <bibkey>song-etal-2022-improving-semantic</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.4</doi>
    </paper>
    <paper id="5">
      <title>Sparse Mixers: Combining <fixed-case>M</fixed-case>o<fixed-case>E</fixed-case> and Mixing to build a more efficient <fixed-case>BERT</fixed-case></title>
      <author><first>James</first><last>Lee-Thorp</last></author>
      <author><first>Joshua</first><last>Ainslie</last></author>
      <pages>58-75</pages>
      <abstract>We combine the capacity of sparsely gated Mixture-of-Experts (MoE) with the speed and stability of linear, mixing transformations to design the Sparse Mixer encoder model. Sparse Mixer slightly outperforms BERT on GLUE and SuperGLUE, but more importantly trains 65% faster and runs inference 61% faster. We also present a faster variant, prosaically named Fast Sparse Mixer, that marginally underperforms BERT on SuperGLUE, but trains and runs nearly twice as fast. We justify the design of these two models by carefully ablating through various mixing mechanisms, MoE configurations, and hyperparameters. Sparse Mixer overcomes many of the latency and stability concerns of MoE models and offers the prospect of serving sparse student models, without resorting to distilling them to dense variants.</abstract>
      <url hash="50d6aee5">2022.findings-emnlp.5</url>
      <bibkey>lee-thorp-ainslie-2022-sparse</bibkey>
      <video href="2022.findings-emnlp.5.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.5</doi>
    </paper>
    <paper id="6">
      <title><fixed-case>KE</fixed-case>-<fixed-case>GCL</fixed-case>: Knowledge Enhanced Graph Contrastive Learning for Commonsense Question Answering</title>
      <author><first>Lihui</first><last>Zhang</last></author>
      <author><first>Ruifan</first><last>Li</last></author>
      <pages>76-87</pages>
      <abstract>Commonsense question answering (CQA) aims to choose the correct answers for commonsense questions. Most existing works focus on extracting and reasoning over external knowledge graphs (KG). However, the noise in KG prevents these models from learning effective representations. In this paper, we propose a Knowledge Enhanced Graph Contrastive Learning model (KE-GCL) by incorporating the contextual descriptions of entities and adopting a graph contrastive learning scheme. Specifically, for QA pairs we represent the knowledge from KG and contextual descriptions. Then, the representations of contextual descriptions as context nodes are inserted into KG, forming the knowledge-enhanced graphs. Moreover, we design a contrastive learning method on graphs. For knowledge-enhanced graphs, we build their augmented views with an adaptive sampling strategy. After that, we reason over graphs to update their representations by scattering edges and aggregating nodes. To further improve GCL, hard graph negatives are chosen based on incorrect answers. Extensive experiments on two benchmark datasets demonstrate the effectiveness of our proposed KE-GCL, which outperforms previous methods consistently.</abstract>
      <url hash="1145676e">2022.findings-emnlp.6</url>
      <bibkey>zhang-li-2022-ke</bibkey>
      <video href="2022.findings-emnlp.6.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.6</doi>
    </paper>
    <paper id="7">
      <title>Acceptability Judgements via Examining the Topology of Attention Maps</title>
      <author><first>Daniil</first><last>Cherniavskii</last></author>
      <author><first>Eduard</first><last>Tulchinskii</last></author>
      <author><first>Vladislav</first><last>Mikhailov</last></author>
      <author><first>Irina</first><last>Proskurina</last></author>
      <author><first>Laida</first><last>Kushnareva</last></author>
      <author><first>Ekaterina</first><last>Artemova</last></author>
      <author><first>Serguei</first><last>Barannikov</last></author>
      <author><first>Irina</first><last>Piontkovskaya</last></author>
      <author><first>Dmitri</first><last>Piontkovski</last></author>
      <author><first>Evgeny</first><last>Burnaev</last></author>
      <pages>88-107</pages>
      <abstract>The role of the attention mechanism in encoding linguistic knowledge has received special interest in NLP. However, the ability of the attention heads to judge the grammatical acceptability of a sentence has been underexplored. This paper approaches the paradigm of acceptability judgments with topological data analysis (TDA), showing that the geometric properties of the attention graph can be efficiently exploited for two standard practices in linguistics: binary judgments and linguistic minimal pairs. Topological features enhance the BERT-based acceptability classifier scores by 8%-24% on CoLA in three languages (English, Italian, and Swedish). By revealing the topological discrepancy between attention maps of minimal pairs, we achieve the human-level performance on the BLiMP benchmark, outperforming nine statistical and Transformer LM baselines. At the same time, TDA provides the foundation for analyzing the linguistic functions of attention heads and interpreting the correspondence between the graph features and grammatical phenomena. We publicly release the code and other materials used in the experiments.</abstract>
      <url hash="86a930d5">2022.findings-emnlp.7</url>
      <bibkey>cherniavskii-etal-2022-acceptability</bibkey>
      <video href="2022.findings-emnlp.7.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.7</doi>
    </paper>
    <paper id="8">
      <title>Clip-Tuning: Towards Derivative-free Prompt Learning with a Mixture of Rewards</title>
      <author><first>Yekun</first><last>Chai</last></author>
      <author><first>Shuohuan</first><last>Wang</last></author>
      <author><first>Yu</first><last>Sun</last></author>
      <author><first>Hao</first><last>Tian</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <author><first>Haifeng</first><last>Wang</last></author>
      <pages>108-117</pages>
      <abstract>Derivative-free prompt learning has emerged as a lightweight alternative to prompt tuning, which only requires model inference to optimize the prompts. However, existing work did not take full advantage of the over-parameterized characteristics of large pre-trained language models (PLMs). In this paper, we propose Clip-Tuning, a simple yet effective method that adopts diverse frozen “thinned” networks of PLMs to obtain *a mixture of rewards* and thus advance the derivative-free prompt learning. The thinned networks consist of all the hidden units that survive a stationary dropout strategy, whose inference predictions reflect an ensemble of partial views over prompted training samples. Our method outperforms previous gradient-free prompt learning methods and achieves parity with gradient-based counterparts on seven language understanding benchmarks under few-shot settings.</abstract>
      <url hash="a831433f">2022.findings-emnlp.8</url>
      <bibkey>chai-etal-2022-clip</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.8</doi>
    </paper>
    <paper id="9">
      <title>Soft-Labeled Contrastive Pre-Training for Function-Level Code Representation</title>
      <author><first>Xiaonan</first><last>Li</last></author>
      <author><first>Daya</first><last>Guo</last></author>
      <author><first>Yeyun</first><last>Gong</last></author>
      <author><first>Yun</first><last>Lin</last></author>
      <author><first>Yelong</first><last>Shen</last></author>
      <author><first>Xipeng</first><last>Qiu</last></author>
      <author><first>Daxin</first><last>Jiang</last></author>
      <author><first>Weizhu</first><last>Chen</last></author>
      <author><first>Nan</first><last>Duan</last></author>
      <pages>118-129</pages>
      <abstract>Code contrastive pre-training has recently achieved significant progress on code-related tasks. In this paper, we present <b>SCodeR</b>, a <b>S</b>oft-labeled contrastive pre-training framework with two positive sample construction methods to learn functional-level <b>Code</b>
 <b>R</b>epresentation. Considering the relevance between codes in a large-scale code corpus, the soft-labeled contrastive pre-training can obtain fine-grained soft-labels through an iterative adversarial manner and use them to learn better code representation. The positive sample construction is another key for contrastive pre-training. Previous works use transformation-based methods like variable renaming to generate semantically equal positive codes. However, they usually result in the generated code with a highly similar surface form, and thus mislead the model to focus on superficial code structure instead of code semantics. To encourage SCodeR to capture semantic information from the code, we utilize code comments and abstract syntax sub-trees of the code to build positive samples. We conduct experiments on four code-related tasks over seven datasets. Extensive experimental results show that SCodeR achieves new state-of-the-art performance on all of them, which illustrates the effectiveness of the proposed pre-training method.</abstract>
      <url hash="6b6c258a">2022.findings-emnlp.9</url>
      <bibkey>li-etal-2022-soft</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.9</doi>
    </paper>
    <paper id="10">
      <title>Conditioned Masked Language and Image Modeling for Image-Text Dense Retrieval</title>
      <author><first>Ziyang</first><last>Luo</last></author>
      <author><first>Yadong</first><last>Xi</last></author>
      <author><first>Rongsheng</first><last>Zhang</last></author>
      <author><first>GongZheng</first><last>Li</last></author>
      <author><first>Zeng</first><last>Zhao</last></author>
      <author><first>Jing</first><last>Ma</last></author>
      <pages>130-140</pages>
      <abstract>Image-text retrieval is a fundamental cross-modal task that takes image/text as a query to retrieve relevant data of another type. The large-scale two-stream pre-trained models like CLIP have achieved tremendous success in this area. They embed the images and texts into instance representations with two separate encoders, aligning them on the instance-level with contrastive learning. Beyond this, the following works adopt the fine-grained token-level interaction (Masked Language and Image Modeling) to boost performance further. However, the vanilla token-level objectives are not designed to aggregate the image-text alignment information into the instance representations, but the token representations, causing a gap between pre-training and application. To address this issue, we carefully design two novel conditioned token-level pre-training objectives, Conditioned Masked Language and Image Modeling (ConMLM and ConMIM), forcing models to aggregate the token-level alignment information into the instance representations. Combing with the instance-level contrastive learning, we propose our cross-modal dense retrieval framework, Conditioned Language-Image Pre-training (ConLIP). Experimental results on two popular cross-modal retrieval benchmarks (MSCOCO and Flickr30k) reveal the effectiveness of our methods.</abstract>
      <url hash="9cdf3ff2">2022.findings-emnlp.10</url>
      <bibkey>luo-etal-2022-conditioned</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.10</doi>
    </paper>
    <paper id="11">
      <title>Does Simultaneous Speech Translation need Simultaneous Models?</title>
      <author><first>Sara</first><last>Papi</last></author>
      <author><first>Marco</first><last>Gaido</last></author>
      <author><first>Matteo</first><last>Negri</last></author>
      <author><first>Marco</first><last>Turchi</last></author>
      <pages>141-153</pages>
      <abstract>In simultaneous speech translation (SimulST), finding the best trade-off between high output quality and low latency is a challenging task. To meet the latency constraints posed by different application scenarios, multiple dedicated SimulST models are usually trained and maintained, generating high computational costs. In this paper, also motivated by the increased sensitivity towards sustainable AI, we investigate whether a single model trained offline can serve both offline and simultaneous applications under different latency regimes without additional training or adaptation. Experiments on en-&gt;de, es show that, aside from facilitating the adoption of well-established offline architectures and training strategies without affecting latency, offline training achieves similar or better quality compared to the standard SimulST training protocol, also being competitive with the state-of-the-art system.</abstract>
      <url hash="b8b99762">2022.findings-emnlp.11</url>
      <bibkey>papi-etal-2022-simultaneous</bibkey>
      <video href="2022.findings-emnlp.11.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.11</doi>
    </paper>
    <paper id="12">
      <title>Utilizing Language-Image Pretraining for Efficient and Robust Bilingual Word Alignment</title>
      <author><first>Tuan</first><last>Dinh</last></author>
      <author><first>Jy-yong</first><last>Sohn</last></author>
      <author><first>Shashank</first><last>Rajput</last></author>
      <author><first>Timothy</first><last>Ossowski</last></author>
      <author><first>Yifei</first><last>Ming</last></author>
      <author><first>Junjie</first><last>Hu</last></author>
      <author><first>Dimitris</first><last>Papailiopoulos</last></author>
      <author><first>Kangwook</first><last>Lee</last></author>
      <pages>154-168</pages>
      <abstract>Word translation without parallel corpora has become feasible, rivaling the performance of supervised methods. Recent findings have shown the improvement in accuracy and robustness of unsupervised word translation (UWT) by utilizing visual observations, which are universal representations across languages. Our work investigates the potential of using not only visual observations but also pretrained language-image models for enabling a more efficient and robust UWT. We develop a novel UWT method dubbed Word Alignment using Language-Image Pretraining (WALIP), leveraging visual observations via the shared image-text embedding space of CLIPs (Radford et al., 2021). WALIP has a two-step procedure. First, we retrieve word pairs with high confidences of similarity, computed using our proposed image-based fingerprints, which define the initial pivot for the alignment. Second, we apply our robust Procrustes algorithm to estimate the linear mapping between two embedding spaces, which iteratively corrects and refines the estimated alignment. Our extensive experiments show that WALIP improves upon the state-of-the-art performance of bilingual word alignment for a few language pairs across different word embeddings and displays great robustness to the dissimilarity of language pairs or training corpora for two word embeddings.</abstract>
      <url hash="f090ea1b">2022.findings-emnlp.12</url>
      <bibkey>dinh-etal-2022-utilizing</bibkey>
      <video href="2022.findings-emnlp.12.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.12</doi>
    </paper>
    <paper id="13">
      <title>Grape: Knowledge Graph Enhanced Passage Reader for Open-domain Question Answering</title>
      <author><first>Mingxuan</first><last>Ju</last></author>
      <author><first>Wenhao</first><last>Yu</last></author>
      <author><first>Tong</first><last>Zhao</last></author>
      <author><first>Chuxu</first><last>Zhang</last></author>
      <author><first>Yanfang</first><last>Ye</last></author>
      <pages>169-181</pages>
      <abstract>A common thread of open-domain question answering (QA) models employs a retriever-reader pipeline that first retrieves a handful of relevant passages from Wikipedia and then peruses the passages to produce an answer. However, even state-of-the-art readers fail to capture the complex relationships between entities appearing in questions and retrieved passages, leading to answers that contradict the facts. In light of this, we propose a novel knowledge graph enhanced passage reader, namely Grape, to improve the reader performance for open-domain QA. Specifically, for each pair of question and retrieved passage, we first construct a localized bipartite graph, attributed to entity embeddings extracted from the intermediate layer of the reader model. Then, a graph neural network learns relational knowledge while fusing graph and contextual representations into the hidden states of the reader model. Experiments on three open-domain QA benchmarks show Grape can improve the state-of-the-art performance by up to 2.2 exact match score with a negligible overhead increase, with the same retriever and retrieved passages. Our code is publicly available at <url>https://github.com/jumxglhf/GRAPE</url>.</abstract>
      <url hash="58fe228c">2022.findings-emnlp.13</url>
      <bibkey>ju-etal-2022-grape</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.13</doi>
    </paper>
    <paper id="14">
      <title><fixed-case>N</fixed-case>arra<fixed-case>S</fixed-case>um: A Large-Scale Dataset for Abstractive Narrative Summarization</title>
      <author><first>Chao</first><last>Zhao</last></author>
      <author><first>Faeze</first><last>Brahman</last></author>
      <author><first>Kaiqiang</first><last>Song</last></author>
      <author><first>Wenlin</first><last>Yao</last></author>
      <author><first>Dian</first><last>Yu</last></author>
      <author><first>Snigdha</first><last>Chaturvedi</last></author>
      <pages>182-197</pages>
      <abstract>Narrative summarization aims to produce a distilled version of a narrative to describe its most salient events and characters. Writing a summary for a narrative is challenging as it requires an understanding of event causality and character behaviors. To encourage research in this direction, we propose NarraSum, a large-scale narrative summarization dataset. It contains 122K narratives, which are collected from the synopses of movies and TV episodes with diverse genres, and their corresponding abstractive summaries. Experiments show that there is a large performance gap between humans and the state-of-the-art summarization models on NarraSum. We hope that this dataset will promote future research in summarization, as well as broader studies of natural language understanding and generation. The dataset is available at <url>https://github.com/zhaochaocs/narrasum</url>.</abstract>
      <url hash="fa0f9846">2022.findings-emnlp.14</url>
      <bibkey>zhao-etal-2022-narrasum</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.14</doi>
    </paper>
    <paper id="15">
      <title><fixed-case>NMTS</fixed-case>core: A Multilingual Analysis of Translation-based Text Similarity Measures</title>
      <author><first>Jannis</first><last>Vamvas</last></author>
      <author><first>Rico</first><last>Sennrich</last></author>
      <pages>198-213</pages>
      <abstract>Being able to rank the similarity of short text segments is an interesting bonus feature of neural machine translation. Translation-based similarity measures include direct and pivot translation probability, as well as translation cross-likelihood, which has not been studied so far. We analyze these measures in the common framework of multilingual NMT, releasing the NMTScore library. Compared to baselines such as sentence embeddings, translation-based measures prove competitive in paraphrase identification and are more robust against adversarial or multilingual input, especially if proper normalization is applied. When used for reference-based evaluation of data-to-text generation in 2 tasks and 17 languages, translation-based measures show a relatively high correlation to human judgments.</abstract>
      <url hash="16e71593">2022.findings-emnlp.15</url>
      <bibkey>vamvas-sennrich-2022-nmtscore</bibkey>
      <video href="2022.findings-emnlp.15.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.15</doi>
    </paper>
    <paper id="16">
      <title>Language Models Understand Us, Poorly</title>
      <author><first>Jared</first><last>Moore</last></author>
      <pages>214-222</pages>
      <abstract>Some claim language models understand us. Others won’t hear it. To clarify, I investigate three views of human language understanding: as-mapping, as-reliability and as-representation. I argue that while behavioral reliability is necessary for understanding, internal representations are sufficient; they climb the right hill. I review state-of-the-art language and multi-modal models: they are pragmatically challenged by under-specification of form. I question the Scaling Paradigm: limits on resources may prohibit scaled-up models from approaching understanding. Last, I describe how as-representation advances a science of understanding. We need work which probes model internals, adds more of human language, and measures what models can learn.</abstract>
      <url hash="efaf7bdf">2022.findings-emnlp.16</url>
      <bibkey>moore-2022-language</bibkey>
      <video href="2022.findings-emnlp.16.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.16</doi>
    </paper>
    <paper id="17">
      <title>Dialogue Meaning Representation for Task-Oriented Dialogue Systems</title>
      <author><first>Xiangkun</first><last>Hu</last></author>
      <author><first>Junqi</first><last>Dai</last></author>
      <author><first>Hang</first><last>Yan</last></author>
      <author><first>Yi</first><last>Zhang</last></author>
      <author><first>Qipeng</first><last>Guo</last></author>
      <author><first>Xipeng</first><last>Qiu</last></author>
      <author><first>Zheng</first><last>Zhang</last></author>
      <pages>223-237</pages>
      <abstract>Dialogue meaning representation formulates natural language utterance semantics in their conversational context in an explicit and machine-readable form. Previous work typically follows the intent-slot framework, which is easy for annotation yet limited in scalability for complex linguistic expressions. A line of works alleviates the representation issue by introducing hierarchical structures but challenging to express complex compositional semantics, such as negation and coreference. We propose Dialogue Meaning Representation (DMR), a pliable and easily extendable representation for task-oriented dialogue. Our representation contains a set of nodes and edges to represent rich compositional semantics. Moreover, we propose an inheritance hierarchy mechanism focusing on domain extensibility. Additionally, we annotated DMR-FastFood, a multi-turn dialogue dataset with more than 70k utterances, with DMR. We propose two evaluation tasks to evaluate different dialogue models and a novel coreference resolution model GNNCoref for the graph-based coreference resolution task. Experiments show that DMR can be parsed well with pre-trained Seq2Seq models, and GNNCoref outperforms the baseline models by a large margin. The dataset and code are available at <url>https://github.com/amazon-research/dialogue-meaning-representation</url></abstract>
      <url hash="e010d2f4">2022.findings-emnlp.17</url>
      <bibkey>hu-etal-2022-dialogue</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.17</doi>
    </paper>
    <paper id="18">
      <title>Learning from the Dictionary: Heterogeneous Knowledge Guided Fine-tuning for <fixed-case>C</fixed-case>hinese Spell Checking</title>
      <author><first>Yinghui</first><last>Li</last></author>
      <author><first>Shirong</first><last>Ma</last></author>
      <author><first>Qingyu</first><last>Zhou</last></author>
      <author><first>Zhongli</first><last>Li</last></author>
      <author><first>Li</first><last>Yangning</last></author>
      <author><first>Shulin</first><last>Huang</last></author>
      <author><first>Ruiyang</first><last>Liu</last></author>
      <author><first>Chao</first><last>Li</last></author>
      <author><first>Yunbo</first><last>Cao</last></author>
      <author><first>Haitao</first><last>Zheng</last></author>
      <pages>238-249</pages>
      <abstract>Chinese Spell Checking (CSC) aims to detect and correct Chinese spelling errors. Recent researches start from the pretrained knowledge of language models and take multimodal information into CSC models to improve the performance. However, they overlook the rich knowledge in the dictionary, the reference book where one can learn how one character should be pronounced, written, and used. In this paper, we propose the LEAD framework, which renders the CSC model to learn heterogeneous knowledge from the dictionary in terms of phonetics, vision, and meaning. LEAD first constructs positive and negative samples according to the knowledge of character phonetics, glyphs, and definitions in the dictionary. Then a unified contrastive learning-based training scheme is employed to refine the representations of the CSC models. Extensive experiments and detailed analyses on the SIGHAN benchmark datasets demonstrate the effectiveness of our proposed methods.</abstract>
      <url hash="6465e2c8">2022.findings-emnlp.18</url>
      <bibkey>li-etal-2022-learning-dictionary</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.18</doi>
    </paper>
    <paper id="19">
      <title>Salient Phrase Aware Dense Retrieval: Can a Dense Retriever Imitate a Sparse One?</title>
      <author><first>Xilun</first><last>Chen</last></author>
      <author><first>Kushal</first><last>Lakhotia</last></author>
      <author><first>Barlas</first><last>Oguz</last></author>
      <author><first>Anchit</first><last>Gupta</last></author>
      <author><first>Patrick</first><last>Lewis</last></author>
      <author><first>Stan</first><last>Peshterliev</last></author>
      <author><first>Yashar</first><last>Mehdad</last></author>
      <author><first>Sonal</first><last>Gupta</last></author>
      <author><first>Wen-tau</first><last>Yih</last></author>
      <pages>250-262</pages>
      <abstract>Despite their recent popularity and well-known advantages, dense retrievers still lag behind sparse methods such as BM25 in their ability to reliably match salient phrases and rare entities in the query and to generalize to out-of-domain data. It has been argued that this is an inherent limitation of dense models. We rebut this claim by introducing the Salient Phrase Aware Retriever (SPAR), a dense retriever with the lexical matching capacity of a sparse model. We show that a dense Lexical Model Λ can be trained to imitate a sparse one, and SPAR is built by augmenting a standard dense retriever with Λ. Empirically, SPAR shows superior performance on a range of tasks including five question answering datasets, MS MARCO passage retrieval, as well as the EntityQuestions and BEIR benchmarks for out-of-domain evaluation, exceeding the performance of state-of-the-art dense and sparse retrievers. The code and models of SPAR are available at: <url>https://github.com/facebookresearch/dpr-scale/tree/main/spar</url></abstract>
      <url hash="a671d688">2022.findings-emnlp.19</url>
      <bibkey>chen-etal-2022-salient</bibkey>
      <video href="2022.findings-emnlp.19.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.19</doi>
    </paper>
    <paper id="20">
      <title><fixed-case>SMARTAVE</fixed-case>: Structured Multimodal Transformer for Product Attribute Value Extraction</title>
      <author><first>Qifan</first><last>Wang</last></author>
      <author><first>Li</first><last>Yang</last></author>
      <author><first>Jingang</first><last>Wang</last></author>
      <author><first>Jitin</first><last>Krishnan</last></author>
      <author><first>Bo</first><last>Dai</last></author>
      <author><first>Sinong</first><last>Wang</last></author>
      <author><first>Zenglin</first><last>Xu</last></author>
      <author><first>Madian</first><last>Khabsa</last></author>
      <author><first>Hao</first><last>Ma</last></author>
      <pages>263-276</pages>
      <abstract>Automatic product attribute value extraction refers to the task of identifying values of an attribute from the product information. Product attributes are essential in improving online shopping experience for customers. Most existing methods focus on extracting attribute values from product title and description. However, in many real-world applications, a product is usually represented by multiple modalities beyond title and description, such as product specifications, text and visual information from the product image, etc. In this paper, we propose SMARTAVE, a Structure Mltimodal trAnsformeR for producT Attribute Value Extraction, which jointly encodes the structured product information from multiple modalities. Specifically, in SMARTAVE encoder, we introduce hyper-tokens to represent the modality-level information, and local-tokens to represent the original text and visual inputs. Structured attention patterns are designed among the hyper-tokens and local-tokens for learning effective product representation. The attribute values are then extracted based on the learned embeddings. We conduct extensive experiments on two multimodal product datasets. Experimental results demonstrate the superior performance of the proposed approach over several state-of-the-art methods. Ablation studies validate the effectiveness of the structured attentions in modeling the multimodal product information.</abstract>
      <url hash="c73403fc">2022.findings-emnlp.20</url>
      <bibkey>wang-etal-2022-smartave</bibkey>
      <video href="2022.findings-emnlp.20.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.20</doi>
    </paper>
    <paper id="21">
      <title>When Language Model Meets Private Library</title>
      <author><first>Daoguang</first><last>Zan</last></author>
      <author><first>Bei</first><last>Chen</last></author>
      <author><first>Zeqi</first><last>Lin</last></author>
      <author><first>Bei</first><last>Guan</last></author>
      <author><first>Wang</first><last>Yongji</last></author>
      <author><first>Jian-Guang</first><last>Lou</last></author>
      <pages>277-288</pages>
      <abstract>With the rapid development of pre-training techniques, a number of language models have been pre-trained on large-scale code corpora and perform well in code generation. In this paper, we investigate how to equip pre-trained language models with the ability of code generation for private libraries. In practice, it is common for programmers to write code using private libraries. However, this is a challenge for language models since they have never seen private APIs during training. Motivated by the fact that private libraries usually come with elaborate API documentation, we propose a novel framework with two modules: the APIRetriever finds useful APIs, and then the APICoder generates code using these APIs. For APIRetriever, we present a dense retrieval system and also design a friendly interaction to involve uses. For APICoder, we can directly use off-the-shelf language models, or continually pre-train the base model on a code corpus containing API information. Both modules are trained with data from public libraries and can be generalized to private ones. Furthermore, we craft three benchmarks for private libraries, named TorchDataEval, MonkeyEval, and BeatNumEval. Experimental results demonstrate the impressive performance of our framework.</abstract>
      <url hash="562dac83">2022.findings-emnlp.21</url>
      <bibkey>zan-etal-2022-language</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.21</doi>
    </paper>
    <paper id="22">
      <title>Cross-Domain Sentiment Classification using Semantic Representation</title>
      <author><first>Shichen</first><last>Li</last></author>
      <author><first>Zhongqing</first><last>Wang</last></author>
      <author><first>Xiaotong</first><last>Jiang</last></author>
      <author><first>Guodong</first><last>Zhou</last></author>
      <pages>289-299</pages>
      <abstract>Previous studies on cross-domain sentiment classification depend on the pivot features or utilize the target data for representation learning, which ignore the semantic relevance between different domains. To this end, we exploit Abstract Meaning Representation (AMR) to help with cross-domain sentiment classification. Compared with the textual input, AMR reduces data sparsity and explicitly provides core semantic knowledge and correlations between different domains. In particular, we develop an algorithm to construct a sentiment-driven semantic graph from sentence-level AMRs. We further design two strategies to linearize the semantic graph and propose a text-graph interaction model to fuse the text and semantic graph representations for cross-domain sentiment classification. Empirical studies show the effectiveness of our proposed model over several strong baselines. The results also indicate the importance of the proposed sentiment-driven semantic graph for cross-domain sentiment classification.</abstract>
      <url hash="2ff663f9">2022.findings-emnlp.22</url>
      <bibkey>li-etal-2022-cross-domain</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.22</doi>
    </paper>
    <paper id="23">
      <title>Yes-Yes-Yes: Proactive Data Collection for <fixed-case>ACL</fixed-case> Rolling Review and Beyond</title>
      <author><first>Nils</first><last>Dycke</last></author>
      <author><first>Ilia</first><last>Kuznetsov</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <pages>300-318</pages>
      <abstract>The shift towards publicly available text sources has enabled language processing at unprecedented scale, yet leaves under-serviced the domains where public and openly licensed data is scarce. Proactively collecting text data for research is a viable strategy to address this scarcity, but lacks systematic methodology taking into account the many ethical, legal and confidentiality-related aspects of data collection. Our work presents a case study on proactive data collection in peer review – a challenging and under-resourced NLP domain. We outline ethical and legal desiderata for proactive data collection and introduce “Yes-Yes-Yes”, the first donation-based peer reviewing data collection workflow that meets these requirements. We report on the implementation of Yes-Yes-Yes at ACL Rolling Review and empirically study the implications of proactive data collection for the dataset size and the biases induced by the donation behavior on the peer reviewing platform.</abstract>
      <url hash="62ee181b">2022.findings-emnlp.23</url>
      <bibkey>dycke-etal-2022-yes</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.23</doi>
    </paper>
    <paper id="24">
      <title><fixed-case>A</fixed-case>ssist<fixed-case>SR</fixed-case>: Task-oriented Video Segment Retrieval for Personal <fixed-case>AI</fixed-case> Assistant</title>
      <author><first>Weixian</first><last>Lei</last></author>
      <author><first>Difei</first><last>Gao</last></author>
      <author><first>Yuxuan</first><last>Wang</last></author>
      <author><first>Dongxing</first><last>Mao</last></author>
      <author><first>Zihan</first><last>Liang</last></author>
      <author><first>Lingmin</first><last>Ran</last></author>
      <author><first>Mike Zheng</first><last>Shou</last></author>
      <pages>319-338</pages>
      <abstract>It is still a pipe dream that personal AI assistants on the phone and AR glasses can assist our daily life in addressing our questions like “how to adjust the date for this watch?” and “how to set its heating duration? (while pointing at an oven)”. The queries used in conventional tasks (i.e. Video Question Answering, Video Retrieval, Moment Localization) are often factoid and based on pure text. In contrast, we present a new task called Task-oriented Question-driven Video Segment Retrieval (TQVSR). Each of our questions is an image-box-text query that focuses on affordance of items in our daily life and expects relevant answer segments to be retrieved from a corpus of instructional video-transcript segments. To support the study of this TQVSR task, we construct a new dataset called AssistSR. We design novel guidelines to create high-quality samples. This dataset contains 3.2k multimodal questions on 1.6k video segments from instructional videos on diverse daily-used items. To address TQVSR, we develop a simple yet effective model called Dual Multimodal Encoders (DME) that significantly outperforms several baseline methods while still having large room for improvement in the future. Moreover, we present detailed ablation analyses. Code and data are available at <url>https://github.com/StanLei52/TQVSR</url>.</abstract>
      <url hash="5c306f10">2022.findings-emnlp.24</url>
      <bibkey>lei-etal-2022-assistsr</bibkey>
      <video href="2022.findings-emnlp.24.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.24</doi>
    </paper>
    <paper id="25">
      <title>Dim-Krum: Backdoor-Resistant Federated Learning for <fixed-case>NLP</fixed-case> with Dimension-wise Krum-Based Aggregation</title>
      <author><first>Zhiyuan</first><last>Zhang</last></author>
      <author><first>Qi</first><last>Su</last></author>
      <author><first>Xu</first><last>Sun</last></author>
      <pages>339-354</pages>
      <abstract>Despite the potential of federated learning, it is known to be vulnerable to backdoor attacks. Many robust federated aggregation methods are proposed to reduce the potential backdoor risk. However, they are mainly validated in the CV field. In this paper, we find that NLP backdoors are hard to defend against than CV, and we provide a theoretical analysis that the malicious update detection error probabilities are determined by the relative backdoor strengths. NLP attacks tend to have small relative backdoor strengths, which may result in the failure of robust federated aggregation methods for NLP attacks. Inspired by the theoretical results, we can choose some dimensions with higher backdoor strengths to settle this issue. We propose a novel federated aggregation algorithm, Dim-Krum, for NLP tasks, and experimental results validate its effectiveness.</abstract>
      <url hash="65fc338a">2022.findings-emnlp.25</url>
      <bibkey>zhang-etal-2022-dim</bibkey>
      <video href="2022.findings-emnlp.25.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.25</doi>
    </paper>
    <paper id="26">
      <title>Fine-mixing: Mitigating Backdoors in Fine-tuned Language Models</title>
      <author><first>Zhiyuan</first><last>Zhang</last></author>
      <author><first>Lingjuan</first><last>Lyu</last></author>
      <author><first>Xingjun</first><last>Ma</last></author>
      <author><first>Chenguang</first><last>Wang</last></author>
      <author><first>Xu</first><last>Sun</last></author>
      <pages>355-372</pages>
      <abstract>Deep Neural Networks (DNNs) are known to be vulnerable to backdoor attacks. In Natural Language Processing (NLP), DNNs are often backdoored during the fine-tuning process of a large-scale Pre-trained Language Model (PLM) with poisoned samples. Although the clean weights of PLMs are readily available, existing methods have ignored this information in defending NLP models against backdoor attacks. In this work, we take the first step to exploit the pre-trained (unfine-tuned) weights to mitigate backdoors in fine-tuned language models. Specifically, we leverage the clean pre-trained weights via two complementary techniques: (1) a two-step Fine-mixing technique, which first mixes the backdoored weights (fine-tuned on poisoned data) with the pre-trained weights, then fine-tunes the mixed weights on a small subset of clean data; (2) an Embedding Purification (E-PUR) technique, which mitigates potential backdoors existing in the word embeddings. We compare Fine-mixing with typical backdoor mitigation methods on three single-sentence sentiment classification tasks and two sentence-pair classification tasks and show that it outperforms the baselines by a considerable margin in all scenarios. We also show that our E-PUR method can benefit existing mitigation methods. Our work establishes a simple but strong baseline defense for secure fine-tuned NLP models against backdoor attacks.</abstract>
      <url hash="df08abfa">2022.findings-emnlp.26</url>
      <bibkey>zhang-etal-2022-fine-mixing</bibkey>
      <video href="2022.findings-emnlp.26.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.26</doi>
    </paper>
    <paper id="27">
      <title>Language Models that Seek for Knowledge: Modular Search &amp; Generation for Dialogue and Prompt Completion</title>
      <author><first>Kurt</first><last>Shuster</last></author>
      <author><first>Mojtaba</first><last>Komeili</last></author>
      <author><first>Leonard</first><last>Adolphs</last></author>
      <author><first>Stephen</first><last>Roller</last></author>
      <author><first>Arthur</first><last>Szlam</last></author>
      <author><first>Jason</first><last>Weston</last></author>
      <pages>373-393</pages>
      <abstract>Language models (LMs) have recently been shown to generate more factual responses by employing modularity (Zhou et al., 2022) in combination with retrieval (Adolphs et al., 2021). We extend the recent approach of Adolphs et al. (2021) to include internet search as a module. Our SeeKeR (Search engine-&gt;Knowledge-&gt;Response) method thus applies a single LM to three modular tasks in succession: search, generating knowledge, and generating a final response. We show that, when using SeeKeR as a dialogue model, it outperforms the state-of-the-art model BlenderBot 2 (Chen et al., 2021) on open-domain knowledge-grounded conversations for the same number of parameters, in terms of consistency, knowledge and per-turn engagingness. SeeKeR applied to topical prompt completions as a standard language model outperforms GPT2 (Radford et al., 2019) and GPT3 (Brown et al., 2020) in terms of factuality and topicality, despite GPT3 being a vastly larger model. Our code and models are made publicly available.</abstract>
      <url hash="072060fb">2022.findings-emnlp.27</url>
      <bibkey>shuster-etal-2022-language</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.27</doi>
    </paper>
    <paper id="28">
      <title>Stretching Sentence-pair <fixed-case>NLI</fixed-case> Models to Reason over Long Documents and Clusters</title>
      <author><first>Tal</first><last>Schuster</last></author>
      <author><first>Sihao</first><last>Chen</last></author>
      <author><first>Senaka</first><last>Buthpitiya</last></author>
      <author><first>Alex</first><last>Fabrikant</last></author>
      <author><first>Donald</first><last>Metzler</last></author>
      <pages>394-412</pages>
      <abstract>Natural Language Inference (NLI) has been extensively studied by the NLP community as a framework for estimating the semantic relation between sentence pairs. While early work identified certain biases in NLI models, recent advancements in modeling and datasets demonstrated promising performance. In this work, we further explore the direct zero-shot applicability of NLI models to real applications, beyond the sentence-pair setting they were trained on. First, we analyze the robustness of these models to longer and out-of-domain inputs. Then, we develop new aggregation methods to allow operating over full documents, reaching state-of-the-art performance on the ContractNLI dataset. Interestingly, we find NLI scores to provide strong retrieval signals, leading to more relevant evidence extractions compared to common similarity-based methods. Finally, we go further and investigate whole document clusters to identify both discrepancies and consensus among sources. In a test case, we find real inconsistencies between Wikipedia pages in different languages about the same topic.</abstract>
      <url hash="b0959690">2022.findings-emnlp.28</url>
      <bibkey>schuster-etal-2022-stretching</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.28</doi>
    </paper>
    <paper id="29">
      <title>Towards Realistic Low-resource Relation Extraction: A Benchmark with Empirical Baseline Study</title>
      <author><first>Xin</first><last>Xu</last></author>
      <author><first>Xiang</first><last>Chen</last></author>
      <author><first>Ningyu</first><last>Zhang</last></author>
      <author><first>Xin</first><last>Xie</last></author>
      <author><first>Xi</first><last>Chen</last></author>
      <author><first>Huajun</first><last>Chen</last></author>
      <pages>413-427</pages>
      <abstract>This paper presents an empirical study to build relation extraction systems in low-resource settings. Based upon recent pre-trained language models, we comprehensively investigate three schemes to evaluate the performance in low-resource settings: (i) different types of prompt-based methods with few-shot labeled data; (ii) diverse balancing methods to address the long-tailed distribution issue; (iii) data augmentation technologies and self-training to generate more labeled in-domain data. We create a benchmark with 8 relation extraction (RE) datasets covering different languages, domains and contexts and perform extensive comparisons over the proposed schemes with combinations. Our experiments illustrate: (i) Though prompt-based tuning is beneficial in low-resource RE, there is still much potential for improvement, especially in extracting relations from cross-sentence contexts with multiple relational triples; (ii) Balancing methods are not always helpful for RE with long-tailed distribution; (iii) Data augmentation complements existing baselines and can bring much performance gain, while self-training may not consistently achieve advancement to low-resource RE. Code and datasets are in <url>https://github.com/zjunlp/LREBench</url>.</abstract>
      <url hash="a2471fb3">2022.findings-emnlp.29</url>
      <bibkey>xu-etal-2022-towards-realistic</bibkey>
      <video href="2022.findings-emnlp.29.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.29</doi>
    </paper>
    <paper id="30">
      <title><fixed-case>CLLE</fixed-case>: A Benchmark for Continual Language Learning Evaluation in Multilingual Machine Translation</title>
      <author><first>Han</first><last>Zhang</last></author>
      <author><first>Sheng</first><last>Zhang</last></author>
      <author><first>Yang</first><last>Xiang</last></author>
      <author><first>Bin</first><last>Liang</last></author>
      <author><first>Jinsong</first><last>Su</last></author>
      <author><first>Zhongjian</first><last>Miao</last></author>
      <author><first>Hui</first><last>Wang</last></author>
      <author><first>Ruifeng</first><last>Xu</last></author>
      <pages>428-443</pages>
      <abstract>Continual Language Learning (CLL) in multilingual translation is inevitable when new languages are required to be translated. Due to the lack of unified and generalized benchmarks, the evaluation of existing methods is greatly influenced by experimental design which usually has a big gap from the industrial demands. In this work, we propose the first Continual Language Learning Evaluation benchmark CLLE in multilingual translation. CLLE consists of a Chinese-centric corpus — CN-25 and two CLL tasks — the close-distance language continual learning task and the language family continual learning task designed for real and disparate demands. Different from existing translation benchmarks, CLLE considers several restrictions for CLL, including domain distribution alignment, content overlap, language diversity, and the balance of corpus. Furthermore, we propose a novel framework COMETA based on Constrained Optimization and META-learning to alleviate catastrophic forgetting and dependency on history training data by using a meta-model to retain the important parameters for old languages. Our experiments prove that CLLE is a challenging CLL benchmark and that our proposed method is effective when compared with other strong baselines. Due to the construction of the corpus, the task designing and the evaluation method are independent of the centric language, we also construct and release the English-centric corpus EN-25 to facilitate academic research.</abstract>
      <url hash="4efd90f2">2022.findings-emnlp.30</url>
      <bibkey>zhang-etal-2022-clle</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.30</doi>
    </paper>
    <paper id="31">
      <title>Lexicon-Enhanced Self-Supervised Training for Multilingual Dense Retrieval</title>
      <author><first>Houxing</first><last>Ren</last></author>
      <author><first>Linjun</first><last>Shou</last></author>
      <author><first>Jian</first><last>Pei</last></author>
      <author><first>Ning</first><last>Wu</last></author>
      <author><first>Ming</first><last>Gong</last></author>
      <author><first>Daxin</first><last>Jiang</last></author>
      <pages>444-459</pages>
      <abstract>Recent multilingual pre-trained models have shown better performance in various multilingual tasks. However, these models perform poorly on multilingual retrieval tasks due to lacking multilingual training data. In this paper, we propose to mine and generate self-supervised training data based on a large-scale unlabeled corpus. We carefully design a mining method which combines the sparse and dense models to mine the relevance of unlabeled queries and passages. And we introduce a query generator to generate more queries in target languages for unlabeled passages. Through extensive experiments on Mr. TYDI dataset and an industrial dataset from a commercial search engine, we demonstrate that our method performs better than baselines based on various pre-trained multilingual models. Our method even achieves on-par performance with the supervised method on the latter dataset.</abstract>
      <url hash="ec040b3a">2022.findings-emnlp.31</url>
      <bibkey>ren-etal-2022-lexicon</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.31</doi>
    </paper>
    <paper id="32">
      <title>Improve Interpretability of Neural Networks via Sparse Contrastive Coding</title>
      <author><first>Junhong</first><last>Liu</last></author>
      <author><first>Yijie</first><last>Lin</last></author>
      <author><first>Liang</first><last>Jiang</last></author>
      <author><first>Jia</first><last>Liu</last></author>
      <author><first>Zujie</first><last>Wen</last></author>
      <author><first>Xi</first><last>Peng</last></author>
      <pages>460-470</pages>
      <abstract>Although explainable artificial intelligence (XAI) has achieved remarkable developments in recent years, there are few efforts have been devoted to the following problems, namely, i) how to develop an explainable method that could explain the black-box in a model-agnostic way? and ii) how to improve the performance and interpretability of the black-box using such explanations instead of pre-collected important attributions? To explore the potential solution, we propose a model-agnostic explanation method termed as Sparse Contrastive Coding (SCC) and verify its effectiveness in text classification and natural language inference. In brief, SCC explains the feature attributions which characterize the importance of words based on the hidden states of each layer of the model. With such word-level explainability, SCC adaptively divides the input sentences into foregrounds and backgrounds in terms of task relevance. Through maximizing the similarity between the foregrounds and input sentences while minimizing the similarity between the backgrounds and input sentences, SSC employs a supervised contrastive learning loss to boost the interpretability and performance of the model. Extensive experiments show the superiority of our method over five state-of-the-art methods in terms of interpretability and classification measurements. The code is available at <url>https://pengxi.me</url>.</abstract>
      <url hash="a65b07a0">2022.findings-emnlp.32</url>
      <bibkey>liu-etal-2022-improve</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.32</doi>
    </paper>
    <paper id="33">
      <title><fixed-case>LEMON</fixed-case>: Language-Based Environment Manipulation via Execution-Guided Pre-training</title>
      <author><first>Qi</first><last>Shi</last></author>
      <author><first>Qian</first><last>Liu</last></author>
      <author><first>Bei</first><last>Chen</last></author>
      <author><first>Yu</first><last>Zhang</last></author>
      <author><first>Ting</first><last>Liu</last></author>
      <author><first>Jian-Guang</first><last>Lou</last></author>
      <pages>471-485</pages>
      <abstract>Language-based environment manipulation requires agents to manipulate the environment following natural language instructions, which is challenging due to the huge space of the environments. To address this challenge, various approaches have been proposed in recent work. Although these approaches work well for their intended environments, they are difficult to generalize across environments. In this work, we propose LEMON, a general framework for language-based environment manipulation tasks. Specifically, we first specify a general approach for language-based environment manipulation tasks, which can deal with various environments using the same generative language model. Then we propose an execution-guided pre-training strategy to inject prior knowledge of environments to the language model with a pure synthetic pre-training corpus. Experimental results on tasks including Alchemy, Scene, Tangrams, ProPara and Recipes demonstrate the effectiveness of LEMON: it achieves new state-of-the-art results on four of the tasks, and the execution-guided pre-training strategy brings remarkable improvements on all experimental tasks.</abstract>
      <url hash="bda6eb3f">2022.findings-emnlp.33</url>
      <bibkey>shi-etal-2022-lemon</bibkey>
      <video href="2022.findings-emnlp.33.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.33</doi>
    </paper>
    <paper id="34">
      <title><fixed-case>CROP</fixed-case>: Zero-shot Cross-lingual Named Entity Recognition with Multilingual Labeled Sequence Translation</title>
      <author><first>Jian</first><last>Yang</last></author>
      <author><first>Shaohan</first><last>Huang</last></author>
      <author><first>Shuming</first><last>Ma</last></author>
      <author><first>Yuwei</first><last>Yin</last></author>
      <author><first>Li</first><last>Dong</last></author>
      <author><first>Dongdong</first><last>Zhang</last></author>
      <author><first>Hongcheng</first><last>Guo</last></author>
      <author><first>Zhoujun</first><last>Li</last></author>
      <author><first>Furu</first><last>Wei</last></author>
      <pages>486-496</pages>
      <abstract>Named entity recognition (NER) suffers from the scarcity of annotated training data, especially for low-resource languages without labeled data. Cross-lingual NER has been proposed to alleviate this issue by transferring knowledge from high-resource languages to low-resource languages via aligned cross-lingual representations or machine translation results. However, the performance of cross-lingual NER methods is severely affected by the unsatisfactory quality of translation or label projection. To address these problems, we propose a Cross-lingual Entity Projection framework (CROP) to enable zero-shot cross-lingual NER with the help of a multilingual labeled sequence translation model. Specifically, the target sequence is first translated into the source language and then tagged by a source NER model. We further adopt a labeled sequence translation model to project the tagged sequence back to the target language and label the target raw sentence. Ultimately, the whole pipeline is integrated into an end-to-end model by the way of self-training. Experimental results on two benchmarks demonstrate that our method substantially outperforms the previous strong baseline by a large margin of +3 7 F1 scores and achieves state-of-the-art performance.</abstract>
      <url hash="8c06d158">2022.findings-emnlp.34</url>
      <bibkey>yang-etal-2022-crop</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.34</doi>
    </paper>
    <paper id="35">
      <title>Handling and Presenting Harmful Text in <fixed-case>NLP</fixed-case> Research</title>
      <author><first>Hannah</first><last>Kirk</last></author>
      <author><first>Abeba</first><last>Birhane</last></author>
      <author><first>Bertie</first><last>Vidgen</last></author>
      <author><first>Leon</first><last>Derczynski</last></author>
      <pages>497-510</pages>
      <abstract>Text data can pose a risk of harm. However, the risks are not fully understood, and how to handle, present, and discuss harmful text in a safe way remains an unresolved issue in the NLP community. We provide an analytical framework categorising harms on three axes: (1) the harm type (e.g., misinformation, hate speech or racial stereotypes); (2) whether a harm is sought as a feature of the research design if explicitly studying harmful content (e.g., training a hate speech classifier), versus unsought if harmful content is encountered when working on unrelated problems (e.g., language generation or part-of-speech tagging); and (3) who it affects, from people (mis)represented in the data to those handling the data and those publishing on the data. We provide advice for practitioners, with concrete steps for mitigating harm in research and in publication. To assist implementation we introduce HarmCheck – a documentation standard for handling and presenting harmful text in research.</abstract>
      <url hash="4ec22f86">2022.findings-emnlp.35</url>
      <bibkey>kirk-etal-2022-handling</bibkey>
      <video href="2022.findings-emnlp.35.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.35</doi>
    </paper>
    <paper id="36">
      <title>Multimodal Contrastive Learning via Uni-Modal Coding and Cross-Modal Prediction for Multimodal Sentiment Analysis</title>
      <author><first>Ronghao</first><last>Lin</last></author>
      <author><first>Haifeng</first><last>Hu</last></author>
      <pages>511-523</pages>
      <abstract>Multimodal representation learning is a challenging task in which previous work mostly focus on either uni-modality pre-training or cross-modality fusion. In fact, we regard modeling multimodal representation as building a skyscraper, where laying stable foundation and designing the main structure are equally essential. The former is like encoding robust uni-modal representation while the later is like integrating interactive information among different modalities, both of which are critical to learning an effective multimodal representation. Recently, contrastive learning has been successfully applied in representation learning, which can be utilized as the pillar of the skyscraper and benefit the model to extract the most important features contained in the multimodal data. In this paper, we propose a novel framework named MultiModal Contrastive Learning (MMCL) for multimodal representation to capture intra- and inter-modality dynamics simultaneously. Specifically, we devise uni-modal contrastive coding with an efficient uni-modal feature augmentation strategy to filter inherent noise contained in acoustic and visual modality and acquire more robust uni-modality representations. Besides, a pseudo siamese network is presented to predict representation across different modalities, which successfully captures cross-modal dynamics. Moreover, we design two contrastive learning tasks, instance- and sentiment-based contrastive learning, to promote the process of prediction and learn more interactive information related to sentiment. Extensive experiments conducted on two public datasets demonstrate that our method surpasses the state-of-the-art methods.</abstract>
      <url hash="6844c246">2022.findings-emnlp.36</url>
      <bibkey>lin-hu-2022-multimodal</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.36</doi>
    </paper>
    <paper id="37">
      <title>Towards Unified Prompt Tuning for Few-shot Text Classification</title>
      <author><first>Jianing</first><last>Wang</last></author>
      <author><first>Chengyu</first><last>Wang</last></author>
      <author><first>Fuli</first><last>Luo</last></author>
      <author><first>Chuanqi</first><last>Tan</last></author>
      <author><first>Minghui</first><last>Qiu</last></author>
      <author><first>Fei</first><last>Yang</last></author>
      <author><first>Qiuhui</first><last>Shi</last></author>
      <author><first>Songfang</first><last>Huang</last></author>
      <author><first>Ming</first><last>Gao</last></author>
      <pages>524-536</pages>
      <abstract>Prompt-based fine-tuning has boosted the performance of Pre-trained Language Models (PLMs) on few-shot text classification by employing task-specific prompts. Yet, PLMs are unfamiliar with prompt-style expressions during pre-training, which limits the few-shot learning performance on downstream tasks. It would be desirable if the models can acquire some prompting knowledge before adapting to specific NLP tasks. We present the Unified Prompt Tuning (UPT) framework, leading to better few-shot text classification for BERT-style models by explicitly capturing prompting semantics from non-target NLP datasets. In UPT, a novel paradigm Prompt-Options-Verbalizer is proposed for joint prompt learning across different NLP tasks, forcing PLMs to capture task-invariant prompting knowledge. We further design a self-supervised task named Knowledge-enhanced Selective Masked Language Modeling to improve the PLM’s generalization abilities for accurate adaptation to previously unseen tasks. After multi-task learning across multiple tasks, the PLM can be better prompt-tuned towards any dissimilar target tasks in low-resourced settings. Experiments over a variety of NLP tasks show that UPT consistently outperforms state-of-the-arts for prompt-based fine-tuning.</abstract>
      <url hash="ede4fde8">2022.findings-emnlp.37</url>
      <bibkey>wang-etal-2022-towards-unified</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.37</doi>
    </paper>
    <paper id="38">
      <title>Can language models learn from explanations in context?</title>
      <author><first>Andrew</first><last>Lampinen</last></author>
      <author><first>Ishita</first><last>Dasgupta</last></author>
      <author><first>Stephanie</first><last>Chan</last></author>
      <author><first>Kory</first><last>Mathewson</last></author>
      <author><first>Mh</first><last>Tessler</last></author>
      <author><first>Antonia</first><last>Creswell</last></author>
      <author><first>James</first><last>McClelland</last></author>
      <author><first>Jane</first><last>Wang</last></author>
      <author><first>Felix</first><last>Hill</last></author>
      <pages>537-563</pages>
      <abstract>Language Models (LMs) can perform new tasks by adapting to a few in-context examples. For humans, explanations that connect examples to task principles can improve learning. We therefore investigate whether explanations of few-shot examples can help LMs. We annotate questions from 40 challenging tasks with answer explanations, and various matched control explanations. We evaluate how different types of explanations, instructions, and controls affect zero- and few-shot performance. We analyze these results using statistical multilevel modeling techniques that account for the nested dependencies among conditions, tasks, prompts, and models. We find that explanations can improve performance—even without tuning. Furthermore, explanations hand-tuned for performance on a small validation set offer substantially larger benefits, and building a prompt by selecting examples and explanations together substantially improves performance over selecting examples alone. Finally, even untuned explanations outperform carefully matched controls, suggesting that the benefits are due to the link between an example and its explanation, rather than lower-level features. However, only large models benefit. In summary, explanations can support the in-context learning of large LMs on challenging tasks.</abstract>
      <url hash="abfe3e8a">2022.findings-emnlp.38</url>
      <bibkey>lampinen-etal-2022-language</bibkey>
      <video href="2022.findings-emnlp.38.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.38</doi>
    </paper>
    <paper id="39">
      <title><fixed-case>GNN</fixed-case>-encoder: Learning a Dual-encoder Architecture via Graph Neural Networks for Dense Passage Retrieval</title>
      <author><first>Jiduan</first><last>Liu</last></author>
      <author><first>Jiahao</first><last>Liu</last></author>
      <author><first>Yang</first><last>Yang</last></author>
      <author><first>Jingang</first><last>Wang</last></author>
      <author><first>Wei</first><last>Wu</last></author>
      <author><first>Dongyan</first><last>Zhao</last></author>
      <author><first>Rui</first><last>Yan</last></author>
      <pages>564-575</pages>
      <abstract>Recently, retrieval models based on dense representations are dominant in passage retrieval tasks, due to their outstanding ability in terms of capturing semantics of input text compared to the traditional sparse vector space models. A common practice of dense retrieval models is to exploit a dual-encoder architecture to represent a query and a passage independently. Though efficient, such a structure loses interaction between the query-passage pair, resulting in inferior accuracy. To enhance the performance of dense retrieval models without loss of efficiency, we propose a GNN-encoder model in which query (passage) information is fused into passage (query) representations via graph neural networks that are constructed by queries and their top retrieved passages. By this means, we maintain a dual-encoder structure, and retain some interaction information between query-passage pairs in their representations, which enables us to achieve both efficiency and efficacy in passage retrieval. Evaluation results indicate that our method significantly outperforms the existing models on MSMARCO, Natural Questions and TriviaQA datasets, and achieves the new state-of-the-art on these datasets.</abstract>
      <url hash="977e4918">2022.findings-emnlp.39</url>
      <bibkey>liu-etal-2022-gnn</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.39</doi>
    </paper>
    <paper id="40">
      <title>Linguistic Rules-Based Corpus Generation for Native <fixed-case>C</fixed-case>hinese Grammatical Error Correction</title>
      <author><first>Shirong</first><last>Ma</last></author>
      <author><first>Yinghui</first><last>Li</last></author>
      <author><first>Rongyi</first><last>Sun</last></author>
      <author><first>Qingyu</first><last>Zhou</last></author>
      <author><first>Shulin</first><last>Huang</last></author>
      <author><first>Ding</first><last>Zhang</last></author>
      <author><first>Li</first><last>Yangning</last></author>
      <author><first>Ruiyang</first><last>Liu</last></author>
      <author><first>Zhongli</first><last>Li</last></author>
      <author><first>Yunbo</first><last>Cao</last></author>
      <author><first>Haitao</first><last>Zheng</last></author>
      <author><first>Ying</first><last>Shen</last></author>
      <pages>576-589</pages>
      <abstract>Chinese Grammatical Error Correction (CGEC) is both a challenging NLP task and a common application in human daily life. Recently, many data-driven approaches are proposed for the development of CGEC research. However, there are two major limitations in the CGEC field: First, the lack of high-quality annotated training corpora prevents the performance of existing CGEC models from being significantly improved. Second, the grammatical errors in widely used test sets are not made by native Chinese speakers, resulting in a significant gap between the CGEC models and the real application. In this paper, we propose a linguistic rules-based approach to construct large-scale CGEC training corpora with automatically generated grammatical errors. Additionally, we present a challenging CGEC benchmark derived entirely from errors made by native Chinese speakers in real-world scenarios. Extensive experiments and detailed analyses not only demonstrate that the training data constructed by our method effectively improves the performance of CGEC models, but also reflect that our benchmark is an excellent resource for further development of the CGEC field.</abstract>
      <url hash="a961953a">2022.findings-emnlp.40</url>
      <bibkey>ma-etal-2022-linguistic</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.40</doi>
    </paper>
    <paper id="41">
      <title>Rethinking the Video Sampling and Reasoning Strategies for Temporal Sentence Grounding</title>
      <author><first>Jiahao</first><last>Zhu</last></author>
      <author><first>Daizong</first><last>Liu</last></author>
      <author><first>Pan</first><last>Zhou</last></author>
      <author><first>Xing</first><last>Di</last></author>
      <author><first>Yu</first><last>Cheng</last></author>
      <author><first>Song</first><last>Yang</last></author>
      <author><first>Wenzheng</first><last>Xu</last></author>
      <author><first>Zichuan</first><last>Xu</last></author>
      <author><first>Yao</first><last>Wan</last></author>
      <author><first>Lichao</first><last>Sun</last></author>
      <author><first>Zeyu</first><last>Xiong</last></author>
      <pages>590-600</pages>
      <abstract>Temporal sentence grounding (TSG) aims to identify the temporal boundary of a specific segment from an untrimmed video by a sentence query. All existing works first utilize a sparse sampling strategy to extract a fixed number of video frames and then interact them with query for reasoning. However, we argue that these methods have overlooked two indispensable issues:1) Boundary-bias: The annotated target segment generally refers to two specific frames as corresponding start and end timestamps. The video downsampling process may lose these two frames and take the adjacent irrelevant frames as new boundaries.2) Reasoning-bias: Such incorrect new boundary frames also lead to the reasoning bias during frame-query interaction, reducing the generalization ability of model. To alleviate above limitations, in this paper, we propose a novel Siamese Sampling and Reasoning Network (SSRN) for TSG, which introduces a siamese sampling mechanism to generate additional contextual frames to enrich and refine the new boundaries. Specifically, a reasoning strategy is developed to learn the inter-relationship among these frames and generate soft labels on boundaries for more accurate frame-query reasoning. Such mechanism is also able to supplement the absent consecutive visual semantics to the sampled sparse frames for fine-grained activity understanding. Extensive experiments demonstrate the effectiveness of SSRN on three challenging datasets.</abstract>
      <url hash="12c35089">2022.findings-emnlp.41</url>
      <bibkey>zhu-etal-2022-rethinking</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.41</doi>
    </paper>
    <paper id="42">
      <title>System 1 + System 2 = Better World: Neural-Symbolic Chain of Logic Reasoning</title>
      <author><first>Wenyue</first><last>Hua</last></author>
      <author><first>Yongfeng</first><last>Zhang</last></author>
      <pages>601-612</pages>
      <abstract>Logical reasoning is a challenge for many current NLP neural network models since it requires more than the ability of learning informative representations from data. Inspired by the Dual Process Theory in cognitive science — which proposes that human cognition process involves two stages: an intuitive, unconscious and fast process relying on perception calledSystem 1, and a logical, conscious and slow process performing complex reasoning called System 2 — we leverage neural logic reasoning (System 2) on top of the representation learning models (System 1), which conducts explicit neural-based differentiable logical reasoning on top of the representations learned by the base neural models. Based on experiments on the commonsense knowledge graph completion task, we show that the two-system architecture always improves from its System 1 model alone. Experiments also show that both the rule-driven logical regularizer and the data-driven value regularizer are important and the performance improvement is marginal without the two regularizers, which indicates that learning from both logical prior and training data is important for reasoning tasks.</abstract>
      <url hash="2df78214">2022.findings-emnlp.42</url>
      <bibkey>hua-zhang-2022-system</bibkey>
      <video href="2022.findings-emnlp.42.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.42</doi>
    </paper>
    <paper id="43">
      <title>Efficient Federated Learning on Knowledge Graphs via Privacy-preserving Relation Embedding Aggregation</title>
      <author><first>Kai</first><last>Zhang</last></author>
      <author><first>Yu</first><last>Wang</last></author>
      <author><first>Hongyi</first><last>Wang</last></author>
      <author><first>Lifu</first><last>Huang</last></author>
      <author><first>Carl</first><last>Yang</last></author>
      <author><first>Xun</first><last>Chen</last></author>
      <author><first>Lichao</first><last>Sun</last></author>
      <pages>613-621</pages>
      <abstract>Federated learning (FL) can be essential in knowledge representation, reasoning, and data mining applications over multi-source knowledge graphs (KGs). A recent study FedE first proposes an FL framework that shares entity embeddings of KGs across all clients. However, entity embedding sharing from FedE would incur a severe privacy leakage. Specifically, the known entity embedding can be used to infer whether a specific relation between two entities exists in a private client. In this paper, we introduce a novel attack method that aims to recover the original data based on the embedding information, which is further used to evaluate the vulnerabilities of FedE. Furthermore, we propose a Federated learning paradigm with privacy-preserving Relation embedding aggregation (FedR) to tackle the privacy issue in FedE. Besides, relation embedding sharing can significantly reduce the communication cost due to its smaller size of queries. We conduct extensive experiments to evaluate FedR with five different KG embedding models and three datasets. Compared to FedE, FedR achieves similar utility and significant improvements regarding privacy-preserving effect and communication efficiency on the link prediction task.</abstract>
      <url hash="1149bc65">2022.findings-emnlp.43</url>
      <bibkey>zhang-etal-2022-efficient-federated</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.43</doi>
    </paper>
    <paper id="44">
      <title><fixed-case>T</fixed-case>ext<fixed-case>H</fixed-case>acker: Learning based Hybrid Local Search Algorithm for Text Hard-label Adversarial Attack</title>
      <author><first>Zhen</first><last>Yu</last></author>
      <author><first>Xiaosen</first><last>Wang</last></author>
      <author><first>Wanxiang</first><last>Che</last></author>
      <author><first>Kun</first><last>He</last></author>
      <pages>622-637</pages>
      <abstract>Existing textual adversarial attacks usually utilize the gradient or prediction confidence to generate adversarial examples, making it hard to be deployed in real-world applications. To this end, we consider a rarely investigated but more rigorous setting, namely hard-label attack, in which the attacker can only access the prediction label. In particular, we find we can learn the importance of different words via the change on prediction label caused by word substitutions on the adversarial examples. Based on this observation, we propose a novel adversarial attack, termed Text Hard-label attacker (TextHacker). TextHacker randomly perturbs lots of words to craft an adversarial example. Then, TextHacker adopts a hybrid local search algorithm with the estimation of word importance from the attack history to minimize the adversarial perturbation. Extensive evaluations for text classification and textual entailment show that TextHacker significantly outperforms existing hard-label attacks regarding the attack performance as well as adversary quality.</abstract>
      <url hash="6c25d4fe">2022.findings-emnlp.44</url>
      <bibkey>yu-etal-2022-texthacker</bibkey>
      <video href="2022.findings-emnlp.44.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.44</doi>
    </paper>
    <paper id="45">
      <title>Visualizing the Obvious: A Concreteness-based Ensemble Model for Noun Property Prediction</title>
      <author><first>Yue</first><last>Yang</last></author>
      <author><first>Artemis</first><last>Panagopoulou</last></author>
      <author><first>Marianna</first><last>Apidianaki</last></author>
      <author><first>Mark</first><last>Yatskar</last></author>
      <author><first>Chris</first><last>Callison-Burch</last></author>
      <pages>638-655</pages>
      <abstract>Neural language models encode rich knowledge about entities and their relationships which can be extracted from their representations using probing. Common properties of nouns (e.g., red strawberries, small ant) are, however, more challenging to extract compared to other types of knowledge because they are rarely explicitly stated in texts. We hypothesize this to mainly be the case for perceptual properties which are obvious to the participants in the communication. We propose to extract these properties from images and use them in an ensemble model, in order to complement the information that is extracted from language models. We consider perceptual properties to be more concrete than abstract properties (e.g., interesting, flawless). We propose to use the adjectives’ concreteness score as a lever to calibrate the contribution of each source (text vs. images). We evaluate our ensemble model in a ranking task where the actual properties of a noun need to be ranked higher than other non-relevant properties. Our results show that the proposed combination of text and images greatly improves noun property prediction compared to powerful text-based language models.</abstract>
      <url hash="0953167e">2022.findings-emnlp.45</url>
      <bibkey>yang-etal-2022-visualizing</bibkey>
      <video href="2022.findings-emnlp.45.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.45</doi>
    </paper>
    <paper id="46">
      <title>It’s Better to Teach Fishing than Giving a Fish: An Auto-Augmented Structure-aware Generative Model for Metaphor Detection</title>
      <author><first>Huawen</first><last>Feng</last></author>
      <author><first>Qianli</first><last>Ma</last></author>
      <pages>656-667</pages>
      <abstract>Metaphor Detection aims to identify the metaphorical meaning of words in the sentence. Most existing work is discriminant models, which use the contextual semantic information extracted by transformers for classifications directly. Due to insufficient training data and corresponding paraphrases, recent methods focus on how to get external resources and utilize them to introduce more knowledge. Currently, contextual modeling and external data are two key issues in the field. In this paper, we propose **A**n **A**uto-**A**ugmented **S**tructure-aware generative model (**AAAS**) for metaphor detection, which transforms the classification task into a keywords-extraction task. Specifically, we propose the task of structure information extraction to allow the model to use the ‘structural language’ to describe the whole sentence. Furthermore, without any other external resources, we design a simple but effective auto-augmented method to expand the limited datasets. Experimental results show that **AAAS** obtains competitive results compared with state-of-the-art methods.</abstract>
      <url hash="c40d9c7a">2022.findings-emnlp.46</url>
      <bibkey>feng-ma-2022-better</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.46</doi>
    </paper>
    <paper id="47">
      <title>Expose Backdoors on the Way: A Feature-Based Efficient Defense against Textual Backdoor Attacks</title>
      <author><first>Sishuo</first><last>Chen</last></author>
      <author><first>Wenkai</first><last>Yang</last></author>
      <author><first>Zhiyuan</first><last>Zhang</last></author>
      <author><first>Xiaohan</first><last>Bi</last></author>
      <author><first>Xu</first><last>Sun</last></author>
      <pages>668-683</pages>
      <abstract>Natural language processing (NLP) models are known to be vulnerable to backdoor attacks, which poses a newly arisen threat to NLP models. Prior online backdoor defense methods for NLP models only focus on the anomalies at either the input or output level, still suffering from fragility to adaptive attacks and high computational cost. In this work, we take the first step to investigate the unconcealment of textual poisoned samples at the intermediate-feature level and propose a feature-based efficient online defense method. Through extensive experiments on existing attacking methods, we find that the poisoned samples are far away from clean samples in the intermediate feature space of a poisoned NLP model. Motivated by this observation, we devise a distance-based anomaly score (DAN) to distinguish poisoned samples from clean samples at the feature level. Experiments on sentiment analysis and offense detection tasks demonstrate the superiority of DAN, as it substantially surpasses existing online defense methods in terms of defending performance and enjoys lower inference costs. Moreover, we show that DAN is also resistant to adaptive attacks based on feature-level regularization. Our code is available at <url>https://github.com/lancopku/DAN</url>.</abstract>
      <url hash="a3473763">2022.findings-emnlp.47</url>
      <bibkey>chen-etal-2022-expose</bibkey>
      <video href="2022.findings-emnlp.47.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.47</doi>
    </paper>
    <paper id="48">
      <title>Diving Deep into Modes of Fact Hallucinations in Dialogue Systems</title>
      <author><first>Souvik</first><last>Das</last></author>
      <author><first>Sougata</first><last>Saha</last></author>
      <author><first>Rohini</first><last>Srihari</last></author>
      <pages>684-699</pages>
      <abstract>Knowledge Graph(KG) grounded conversations often use large pre-trained models and usually suffer from fact hallucination. Frequently entities with no references in knowledge sources and conversation history are introduced into responses, thus hindering the flow of the conversation—existing work attempt to overcome this issue by tweaking the training procedure or using a multi-step refining method. However, minimal effort is put into constructing an entity-level hallucination detection system, which would provide fine-grained signals that control fallacious content while generating responses. As a first step to address this issue, we dive deep to identify various modes of hallucination in KG-grounded chatbots through human feedback analysis. Secondly, we propose a series of perturbation strategies to create a synthetic dataset named FADE (FActual Dialogue Hallucination DEtection Dataset). Finally, we conduct comprehensive data analyses and create multiple baseline models for hallucination detection to compare against human-verified data and already established benchmarks.</abstract>
      <url hash="4a178274">2022.findings-emnlp.48</url>
      <bibkey>das-etal-2022-diving</bibkey>
      <video href="2022.findings-emnlp.48.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.48</doi>
    </paper>
    <paper id="49">
      <title>Representation Learning for Resource-Constrained Keyphrase Generation</title>
      <author><first>Di</first><last>Wu</last></author>
      <author><first>Wasi</first><last>Ahmad</last></author>
      <author><first>Sunipa</first><last>Dev</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <pages>700-716</pages>
      <abstract>State-of-the-art keyphrase generation methods generally depend on large annotated datasets, limiting their performance in domains with limited annotated data. To overcome this challenge, we design a data-oriented approach that first identifies salient information using retrieval-based corpus-level statistics, and then learns a task-specific intermediate representation based on a pre-trained language model using large-scale unlabeled documents. We introduce salient span recovery and salient span prediction as denoising training objectives that condense the intra-article and inter-article knowledge essential for keyphrase generation. Through experiments on multiple keyphrase generation benchmarks, we show the effectiveness of the proposed approach for facilitating low-resource keyphrase generation and zero-shot domain adaptation. Our method especially benefits the generation of absent keyphrases, approaching the performance of models trained with large training sets.</abstract>
      <url hash="a82ad57d">2022.findings-emnlp.49</url>
      <bibkey>wu-etal-2022-representation</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.49</doi>
    </paper>
    <paper id="50">
      <title>Systematicity in <fixed-case>GPT</fixed-case>-3’s Interpretation of Novel <fixed-case>E</fixed-case>nglish Noun Compounds</title>
      <author><first>Siyan</first><last>Li</last></author>
      <author><first>Riley</first><last>Carlson</last></author>
      <author><first>Christopher</first><last>Potts</last></author>
      <pages>717-728</pages>
      <abstract>Levin et al. (2019) show experimentally that the interpretations of novel English noun compounds (e.g., stew skillet), while not fully compositional, are highly predictable based on whether the modifier and head refer to artifacts or natural kinds. Is the large language model GPT-3 governed by the same interpretive principles? To address this question, we first compare Levin et al.’s experimental data with GPT-3 generations, finding a high degree of similarity. However, this evidence is consistent with GPT-3 reasoning only about specific lexical items rather than the more abstract conceptual categories of Levin et al.’s theory. To probe more deeply, we construct prompts that require the relevant kind of conceptual reasoning. Here, we fail to find convincing evidence that GPT-3 is reasoning about more than just individual lexical items. These results highlight the importance of controlling for low-level distributional regularities when assessing whether a large language model latently encodes a deeper theory.</abstract>
      <url hash="64cb7c7f">2022.findings-emnlp.50</url>
      <bibkey>li-etal-2022-systematicity</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.50</doi>
    </paper>
    <paper id="51">
      <title><fixed-case>CARE</fixed-case>: Causality Reasoning for Empathetic Responses by Conditional Graph Generation</title>
      <author><first>Jiashuo</first><last>Wang</last></author>
      <author><first>Yi</first><last>Cheng</last></author>
      <author><first>Wenjie</first><last>Li</last></author>
      <pages>729-741</pages>
      <abstract>Recent approaches to empathetic response generation incorporate emotion causalities to enhance comprehension of both the user’s feelings and experiences. However, these approaches suffer from two critical issues. First, they only consider causalities between the user’s emotion and the user’s experiences, and ignore those between the user’s experiences. Second, they neglect interdependence among causalities and reason them independently. To solve the above problems, we expect to reason all plausible causalities interdependently and simultaneously, given the user’s emotion, dialogue history, and future dialogue content. Then, we infuse these causalities into response generation for empathetic responses. Specifically, we design a new model, i.e., the Conditional Variational Graph Auto-Encoder (CVGAE), for the causality reasoning, and adopt a multi-source attention mechanism in the decoder for the causality infusion. We name the whole framework as CARE, abbreviated for CAusality Reasoning for Empathetic conversation. Experimental results indicate that our method achieves state-of-the-art performance.</abstract>
      <url hash="05cbf5b0">2022.findings-emnlp.51</url>
      <bibkey>wang-etal-2022-care</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.51</doi>
    </paper>
    <paper id="52">
      <title><fixed-case>T</fixed-case>rans<fixed-case>A</fixed-case>dv: A Translation-based Adversarial Learning Framework for Zero-Resource Cross-Lingual Named Entity Recognition</title>
      <author><first>Yichun</first><last>Zhao</last></author>
      <author><first>Jintao</first><last>Du</last></author>
      <author><first>Gongshen</first><last>Liu</last></author>
      <author><first>Huijia</first><last>Zhu</last></author>
      <pages>742-749</pages>
      <abstract>Zero-Resource Cross-Lingual Named Entity Recognition aims at training an NER model of the target language using only labeled source language data and unlabeled target language data. Existing methods are mainly divided into three categories: model transfer based, data transfer based and knowledge transfer based. Each method has its own disadvantages, and combining more than one of them often leads to better performance. However, the performance of data transfer based methods is often limited by inevitable noise in the translation process. To handle the problem, we propose a framework named TransAdv to mitigate lexical and syntactic errors of word-by-word translated data, better utilizing the data by multi-level adversarial learning and multi-model knowledge distillation. Extensive experiments are conducted over 6 target languages with English as the source language, and the results show that TransAdv achieves competitive performance to the state-of-the-art models.</abstract>
      <url hash="144a4137">2022.findings-emnlp.52</url>
      <bibkey>zhao-etal-2022-transadv</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.52</doi>
    </paper>
    <paper id="53">
      <title><fixed-case>BARLE</fixed-case>: Background-Aware Representation Learning for Background Shift Out-of-Distribution Detection</title>
      <author><first>Hanyu</first><last>Duan</last></author>
      <author><first>Yi</first><last>Yang</last></author>
      <author><first>Ahmed</first><last>Abbasi</last></author>
      <author><first>Kar Yan</first><last>Tam</last></author>
      <pages>750-764</pages>
      <abstract>Machine learning models often suffer from a performance drop when they are applied to out-of-distribution (OOD) samples, i.e., those drawn far away from the training data distribution. Existing OOD detection work mostly focuses on identifying semantic-shift OOD samples, e.g., instances from unseen new classes. However, background-shift OOD detection, which identifies samples with domain or style-change, represents a more practical yet challenging task. In this paper, we propose Background-Aware Representation Learning (BARLE) for background-shift OOD detection in NLP. Specifically, we generate semantics-preserving background-shifted pseudo OOD samples from pretrained masked language models. We then contrast the in-distribution (ID) samples with their pseudo OOD counterparts. Unlike prior semantic-shift OOD detection work that often leverages an external text corpus, BARLE only uses ID data, which is more flexible and cost-efficient. In experiments across several text classification tasks, we demonstrate that BARLE is capable of improving background-shift OOD detection performance while maintaining ID classification accuracy. We further investigate the properties of the generated pseudo OOD samples, uncovering the working mechanism of BARLE.</abstract>
      <url hash="80345b14">2022.findings-emnlp.53</url>
      <bibkey>duan-etal-2022-barle</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.53</doi>
    </paper>
    <paper id="54">
      <title>What Language Model to Train if You Have One Million <fixed-case>GPU</fixed-case> Hours?</title>
      <author><first>Teven</first><last>Le Scao</last></author>
      <author><first>Thomas</first><last>Wang</last></author>
      <author><first>Daniel</first><last>Hesslow</last></author>
      <author><first>Stas</first><last>Bekman</last></author>
      <author><first>M Saiful</first><last>Bari</last></author>
      <author><first>Stella</first><last>Biderman</last></author>
      <author><first>Hady</first><last>Elsahar</last></author>
      <author><first>Niklas</first><last>Muennighoff</last></author>
      <author><first>Jason</first><last>Phang</last></author>
      <author><first>Ofir</first><last>Press</last></author>
      <author><first>Colin</first><last>Raffel</last></author>
      <author><first>Victor</first><last>Sanh</last></author>
      <author><first>Sheng</first><last>Shen</last></author>
      <author><first>Lintang</first><last>Sutawika</last></author>
      <author><first>Jaesung</first><last>Tae</last></author>
      <author><first>Zheng Xin</first><last>Yong</last></author>
      <author><first>Julien</first><last>Launay</last></author>
      <author><first>Iz</first><last>Beltagy</last></author>
      <pages>765-782</pages>
      <abstract>The crystallization of modeling methods around the Transformer architecture has been a boon for practitioners. Simple, well-motivated architectural variations can transfer across tasks and scale, increasing the impact of modeling research. However, with the emergence of state-of-the-art 100B+ parameters models, large language models are increasingly expensive to accurately design and train. Notably, it can be difficult to evaluate how modeling decisions may impact emergent capabilities, given that these capabilities arise mainly from sheer scale alone. In the process of building BLOOM–the Big Science Large Open-science Open-access Multilingual language model–our goal is to identify an architecture and training setup that makes the best use of our 1,000,000 A100-GPU-hours budget. Specifically, we perform an ablation study at the billion-parameter scale comparing different modeling practices and their impact on zero-shot generalization. In addition, we study the impact of various popular pre-training corpora on zero-shot generalization. We also study the performance of a multilingual model and how it compares to the English-only one. Finally, we consider the scaling behaviour of Transformers to choose the target model size, shape, and training setup. All our models and code are open-sourced at <url>https://huggingface.co/bigscience</url>.</abstract>
      <url hash="a63c5ef1">2022.findings-emnlp.54</url>
      <bibkey>le-scao-etal-2022-language</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.54</doi>
    </paper>
    <paper id="55">
      <title>Enhancing Out-of-Distribution Detection in Natural Language Understanding via Implicit Layer Ensemble</title>
      <author><first>Hyunsoo</first><last>Cho</last></author>
      <author><first>Choonghyun</first><last>Park</last></author>
      <author><first>Jaewook</first><last>Kang</last></author>
      <author><first>Kang Min</first><last>Yoo</last></author>
      <author><first>Taeuk</first><last>Kim</last></author>
      <author><first>Sang-goo</first><last>Lee</last></author>
      <pages>783-798</pages>
      <abstract>Out-of-distribution (OOD) detection aims to discern outliers from the intended data distribution, which is crucial to maintaining high reliability and a good user experience. Most recent studies in OOD detection utilize the information from a single representation that resides in the penultimate layer to determine whether the input is anomalous or not. Although such a method is straightforward, the potential of diverse information in the intermediate layers is overlooked. In this paper, we propose a novel framework based on contrastive learning that encourages intermediate features to learn layer-specialized representations and assembles them implicitly into a single representation to absorb rich information in the pre-trained language model. Extensive experiments in various intent classification and OOD datasets demonstrate that our approach is significantly more effective than other works.</abstract>
      <url hash="c22e5710">2022.findings-emnlp.55</url>
      <bibkey>cho-etal-2022-enhancing</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.55</doi>
    </paper>
    <paper id="56">
      <title>Contrastive Demonstration Tuning for Pre-trained Language Models</title>
      <author><first>Xiaozhuan</first><last>Liang</last></author>
      <author><first>Ningyu</first><last>Zhang</last></author>
      <author><first>Siyuan</first><last>Cheng</last></author>
      <author><first>Zhenru</first><last>Zhang</last></author>
      <author><first>Chuanqi</first><last>Tan</last></author>
      <author><first>Huajun</first><last>Chen</last></author>
      <pages>799-811</pages>
      <abstract>Pretrained language models can be effectively stimulated by textual prompts or demonstrations, especially in low-data scenarios. Recent works have focused on automatically searching discrete or continuous prompts or optimized verbalizers, yet studies for the demonstration are still limited. Concretely, the demonstration examples are crucial for an excellent final performance of prompt-tuning. In this paper, we propose a novel pluggable, extensible, and efficient approach named contrastive demonstration tuning, which is free of demonstration sampling. Furthermore, the proposed approach can be: (i) Plugged into any previous prompt-tuning approaches; (ii) Extended to widespread classification tasks with a large number of categories. Experimental results on 16 datasets illustrate that our method integrated with previous approaches LM-BFF and P-tuning can yield better performance. Code is available in <url>https://github.com/zjunlp/PromptKG/tree/main/research/Demo-Tuning</url>.</abstract>
      <url hash="a0649e57">2022.findings-emnlp.56</url>
      <bibkey>liang-etal-2022-contrastive</bibkey>
      <video href="2022.findings-emnlp.56.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.56</doi>
    </paper>
    <paper id="57">
      <title>Detect-Localize-Repair: A Unified Framework for Learning to Debug with <fixed-case>C</fixed-case>ode<fixed-case>T</fixed-case>5</title>
      <author><first>Nghi</first><last>Bui</last></author>
      <author><first>Yue</first><last>Wang</last></author>
      <author><first>Steven C.H.</first><last>Hoi</last></author>
      <pages>812-823</pages>
      <abstract>Automated software debugging is a crucial task for improving the productivity of software developers. Many neural-based techniques have been proven effective for debugging-related tasks such as bug localization and program repair (or bug fixing). However, these techniques often focus only on either one of them or approach them in a stage-wise manner, ignoring the mutual benefits between them. In this work, we propose a novel unified Detect-Localize-Repair framework based on a pretrained programming language model CodeT5 to seamlessly address these tasks, named CodeT5-DLR. Specifically, we propose three objectives to adapt the generic CodeT5 for debugging: a bug detection objective to determine whether a given code snippet is buggy or not, a bug localization objective to identify the buggy lines, and a program repair objective to translate the buggy code to its fixed version. We evaluate it on each of these tasks and their combined setting on two newly collected line-level debugging datasets in Java and Python. Extensive results show that our model significantly outperforms existing baselines from both NLP and software engineering domains.</abstract>
      <url hash="85389d5b">2022.findings-emnlp.57</url>
      <bibkey>bui-etal-2022-detect</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.57</doi>
    </paper>
    <paper id="58">
      <title>Influence Functions for Sequence Tagging Models</title>
      <author><first>Sarthak</first><last>Jain</last></author>
      <author><first>Varun</first><last>Manjunatha</last></author>
      <author><first>Byron</first><last>Wallace</last></author>
      <author><first>Ani</first><last>Nenkova</last></author>
      <pages>824-839</pages>
      <abstract>Many standard tasks in NLP (e.g., Named Entity Recognition, Part-of-Speech tagging, and Semantic Role Labeling) are naturally framed as sequence tagging problems. However, there has been comparatively little work on interpretability methods for sequence tagging models. In this paper, we extend influence functions — which aim to trace predictions back to the training points that informed them — to sequence tagging tasks. We define the influence of a training instance segment as the effect that perturbing the labels within this segment has on a test segment level prediction. We provide an efficient approximation to compute this, and show that it tracks with the “true” segment influence (measured empirically). We show the practical utility of segment influence by using the method to identify noisy annotations in NER corpora.</abstract>
      <url hash="403f973c">2022.findings-emnlp.58</url>
      <bibkey>jain-etal-2022-influence</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.58</doi>
    </paper>
    <paper id="59">
      <title>Impact of Pretraining Term Frequencies on Few-Shot Numerical Reasoning</title>
      <author><first>Yasaman</first><last>Razeghi</last></author>
      <author><first>Robert L</first><last>Logan IV</last></author>
      <author><first>Matt</first><last>Gardner</last></author>
      <author><first>Sameer</first><last>Singh</last></author>
      <pages>840-854</pages>
      <abstract>Pretrained Language Models (LMs) have demonstrated ability to perform numerical reasoning by extrapolating from a few examples in few-shot settings. However, the extent to which this extrapolation relies on robust reasoning is unclear. In this paper, we investigate how well these models reason with terms that are less frequent in the pretraining data. In particular, we examine the correlations between the model performance on test instances and the frequency of terms from those instances in the pretraining data. We measure the strength of this correlation for a number of GPT-based language models (pretrained on the Pile dataset) on various numerical deduction tasks (e.g., arithmetic and unit conversion). Our results consistently demonstrate that models are more accurate on instances whose terms are more prevalent, in some cases above 70% (absolute) more accurate on the top 10% frequent terms in comparison to the bottom 10%. Overall, although LMs appear successful at few-shot numerical reasoning, our results raise the question of how much models actually generalize beyond pretraining data, and we encourage researchers to take the pretraining data into account when interpreting evaluation results.</abstract>
      <url hash="230ddef6">2022.findings-emnlp.59</url>
      <bibkey>razeghi-etal-2022-impact</bibkey>
      <video href="2022.findings-emnlp.59.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.59</doi>
    </paper>
    <paper id="60">
      <title>Syntactic and Semantic Uniformity for Semantic Parsing and Task-Oriented Dialogue Systems</title>
      <author><first>Bowen</first><last>Chen</last></author>
      <author><first>Yusuke</first><last>Miyao</last></author>
      <pages>855-867</pages>
      <abstract>This paper proposes a data representation framework for semantic parsing and task-oriented dialogue systems, aiming to achieve a uniform representation for syntactically and semantically diverse machine-readable formats. Current NLP systems heavily rely on adapting pre-trained language models to specific tasks, and this approach has been proven effective for modeling natural language texts. However, little attention has been paid to the representation of machine-readable formats, such as database queries and dialogue states. We present a method for converting original machine-readable formats of semantic parsing and task-oriented dialogue datasets into a syntactically and semantically uniform representation. We define a meta grammar for syntactically uniform representations and translate semantically equivalent functions into a uniform vocabulary. Empirical experiments on 13 datasets show that accuracy consistently improves over original formats, revealing the advantage of the proposed representation. Additionally, we show that the proposed representation allows for transfer learning across datasets.</abstract>
      <url hash="b0f6e602">2022.findings-emnlp.60</url>
      <bibkey>chen-miyao-2022-syntactic</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.60</doi>
    </paper>
    <paper id="61">
      <title>Knowledge-Rich Self-Supervision for Biomedical Entity Linking</title>
      <author><first>Sheng</first><last>Zhang</last></author>
      <author><first>Hao</first><last>Cheng</last></author>
      <author><first>Shikhar</first><last>Vashishth</last></author>
      <author><first>Cliff</first><last>Wong</last></author>
      <author><first>Jinfeng</first><last>Xiao</last></author>
      <author><first>Xiaodong</first><last>Liu</last></author>
      <author><first>Tristan</first><last>Naumann</last></author>
      <author><first>Jianfeng</first><last>Gao</last></author>
      <author><first>Hoifung</first><last>Poon</last></author>
      <pages>868-880</pages>
      <abstract>Entity linking faces significant challenges such as prolific variations and prevalent ambiguities, especially in high-value domains with myriad entities. Standard classification approaches suffer from the annotation bottleneck and cannot effectively handle unseen entities. Zero-shot entity linking has emerged as a promising direction for generalizing to new entities, but it still requires example gold entity mentions during training and canonical descriptions for all entities, both of which are rarely available outside of Wikipedia. In this paper, we explore Knowledge-RIch Self-Supervision (KRISS) for biomedical entity linking, by leveraging readily available domain knowledge. In training, it generates self-supervised mention examples on unlabeled text using a domain ontology and trains a contextual encoder using contrastive learning. For inference, it samples self-supervised mentions as prototypes for each entity and conducts linking by mapping the test mention to the most similar prototype. Our approach can easily incorporate entity descriptions and gold mention labels if available. We conducted extensive experiments on seven standard datasets spanning biomedical literature and clinical notes. Without using any labeled information, our method produces KRISSBERT, a universal entity linker for four million UMLS entities that attains new state of the art, outperforming prior self-supervised methods by as much as 20 absolute points in accuracy. We released KRISSBERT at <url>https://aka.ms/krissbert</url>.</abstract>
      <url hash="1c89563e">2022.findings-emnlp.61</url>
      <bibkey>zhang-etal-2022-knowledge</bibkey>
      <video href="2022.findings-emnlp.61.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.61</doi>
    </paper>
    <paper id="62">
      <title><fixed-case>ARTIST</fixed-case>: A Transformer-based <fixed-case>C</fixed-case>hinese Text-to-Image Synthesizer Digesting Linguistic and World Knowledge</title>
      <author><first>Tingting</first><last>Liu</last></author>
      <author><first>Chengyu</first><last>Wang</last></author>
      <author><first>Xiangru</first><last>Zhu</last></author>
      <author><first>Lei</first><last>Li</last></author>
      <author><first>Minghui</first><last>Qiu</last></author>
      <author><first>Jun</first><last>Huang</last></author>
      <author><first>Ming</first><last>Gao</last></author>
      <author><first>Yanghua</first><last>Xiao</last></author>
      <pages>881-888</pages>
      <abstract>Text-to-Image Synthesis (TIS) is a popular task to convert natural language texts into realistic images. Recently, transformer-based TIS models (such as DALL-E) have been proposed using the encoder-decoder architectures. Yet, these billion-scale TIS models are difficult to tune and deploy in resource-constrained environments. In addition, there is a lack of language-specific TIS benchmarks for Chinese, together with high-performing models with moderate sizes. In this work, we present ARTIST, A tRansformer-based Chinese Text-to-Image SynThesizer for high-resolution image generation. In ARTIST, the rich linguistic and relational knowledge facts are injected into the model to ensure better model performance without the usage of ultra-large models. We further establish a large-scale Chinese TIS benchmark with the re-production results of state-of-the-art transformer-based TIS models. Results show ARTIST outperforms previous approaches.</abstract>
      <url hash="d0b39bc6">2022.findings-emnlp.62</url>
      <bibkey>liu-etal-2022-artist</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.62</doi>
    </paper>
    <paper id="63">
      <title>From Spelling to Grammar: A New Framework for <fixed-case>C</fixed-case>hinese Grammatical Error Correction</title>
      <author><first>Xiuyu</first><last>Wu</last></author>
      <author><first>Yunfang</first><last>Wu</last></author>
      <pages>889-902</pages>
      <abstract>Chinese Grammatical Error Correction (CGEC) aims to generate a correct sentence from an erroneous sequence, where different kinds of errors are mixed. This paper divides the CGEC task into two steps, namely spelling error correction and grammatical error correction. We firstly propose a novel zero-shot approach for spelling error correction, which is simple but effective, obtaining a high precision to avoid error accumulation of the pipeline structure. To handle grammatical error correction, we design part-of-speech (POS) features and semantic class features to enhance the neural network model, and propose an auxiliary task to predict the POS sequence of the target sentence. Our proposed framework achieves a 42.11 F-0.5 score on CGEC dataset without using any synthetic data or data augmentation methods, which outperforms the previous state-of-the-art by a wide margin of 1.30 points. Moreover, our model produces meaningful POS representations that capture different POS words and convey reasonable POS transition rules.</abstract>
      <url hash="0ef649f3">2022.findings-emnlp.63</url>
      <bibkey>wu-wu-2022-spelling</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.63</doi>
    </paper>
    <paper id="64">
      <title>Language Models Are Poor Learners of Directional Inference</title>
      <author><first>Tianyi</first><last>Li</last></author>
      <author><first>Mohammad Javad</first><last>Hosseini</last></author>
      <author><first>Sabine</first><last>Weber</last></author>
      <author><first>Mark</first><last>Steedman</last></author>
      <pages>903-921</pages>
      <abstract>We examine LMs’ competence of directional predicate entailments by supervised fine-tuning with prompts. Our analysis shows that contrary to their apparent success on standard NLI, LMs show limited ability to learn such directional inference; moreover, existing datasets fail to test directionality, and/or are infested by artefacts that can be learnt as proxy for entailments, yielding over-optimistic results. In response, we present BoOQA (Boolean Open QA), a robust multi-lingual evaluation benchmark for directional predicate entailments, extrinsic to existing training sets. On BoOQA, we establish baselines and show evidence of existing LM-prompting models being incompetent directional entailment learners, in contrast to entailment graphs, however limited by sparsity.</abstract>
      <url hash="5c9ab2f9">2022.findings-emnlp.64</url>
      <bibkey>li-etal-2022-language</bibkey>
      <video href="2022.findings-emnlp.64.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.64</doi>
    </paper>
    <paper id="65">
      <title>Wish <fixed-case>I</fixed-case> Can Feel What You Feel: A Neural Approach for Empathetic Response Generation</title>
      <author><first>Yangbin</first><last>Chen</last></author>
      <author><first>Chunfeng</first><last>Liang</last></author>
      <pages>922-933</pages>
      <abstract>Expressing empathy is important in everyday conversations, and exploring how empathy arises is crucial in automatic response generation. Most previous approaches consider only a single factor that affects empathy. However, in practice, empathy generation and expression is a very complex and dynamic psychological process. A listener needs to find out events which cause a speaker’s emotions (emotion cause extraction), project the events into some experience (knowledge extension), and express empathy in the most appropriate way (communication mechanism).To this end, we propose a novel approach, which integrates the three components - emotion cause, knowledge graph, and communication mechanism for empathetic response generation. Experimental results on the benchmark dataset demonstrate the effectiveness of our method and show that incorporating the key components generates more informative and empathetic responses.</abstract>
      <url hash="57f6412f">2022.findings-emnlp.65</url>
      <bibkey>chen-liang-2022-wish</bibkey>
      <video href="2022.findings-emnlp.65.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.65</doi>
    </paper>
    <paper id="66">
      <title>Measuring and Improving Semantic Diversity of Dialogue Generation</title>
      <author><first>Seungju</first><last>Han</last></author>
      <author><first>Beomsu</first><last>Kim</last></author>
      <author><first>Buru</first><last>Chang</last></author>
      <pages>934-950</pages>
      <abstract>Response diversity has become an important criterion for evaluating the quality of open-domain dialogue generation models. However, current evaluation metrics for response diversity often fail to capture the semantic diversity of generated responses, as they mainly consider lexical aspects of the generated responses. In this paper, we introduce a new automatic evaluation metric to measure the semantic diversity of generated responses. Through human evaluation, we demonstrate that our proposed metric captures human judgments on response diversity better than existing lexical-level diversity metrics. Furthermore, motivated by analyzing an existing dialogue dataset, we propose a simple yet effective learning method that improves the semantic diversity of generated responses. Our learning method weights training samples based on the semantic distribution of the training set. We show that our learning method improves response diversity and coherency better than other baseline methods through automatic and human evaluation.</abstract>
      <url hash="4d252a7f">2022.findings-emnlp.66</url>
      <bibkey>han-etal-2022-measuring</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.66</doi>
    </paper>
    <paper id="67">
      <title>Plug-and-Play <fixed-case>VQA</fixed-case>: Zero-shot <fixed-case>VQA</fixed-case> by Conjoining Large Pretrained Models with Zero Training</title>
      <author><first>Anthony Meng Huat</first><last>Tiong</last></author>
      <author><first>Junnan</first><last>Li</last></author>
      <author><first>Boyang</first><last>Li</last></author>
      <author><first>Silvio</first><last>Savarese</last></author>
      <author><first>Steven C.H.</first><last>Hoi</last></author>
      <pages>951-967</pages>
      <abstract>Visual question answering (VQA) is a hallmark of vision and language reasoningand a challenging task under the zero-shot setting. We propose Plug-and-Play VQA (PNP-VQA),a modular framework for zero-shot VQA.In contrast to most existing works, which require substantial adaptation of pretrained language models (PLMs) for the vision modality,PNP-VQA requires no additional training of the PLMs.Instead, we propose to use natural language and network interpretation as an intermediate representation that glues pretrained models together. We first generate question-guided informative image captions,and pass the captions to a PLM as context for question answering. Surpassing end-to-end trained baselines, PNP-VQA achieves state-of-the-art results on zero-shot VQAv2 and GQA. With 11B parameters, it outperforms the 80B-parameter Flamingo model by 8.5% on VQAv2. With 738M PLM parameters, PNP-VQA achieves an improvement of 9.1% on GQA over FewVLM with 740M PLM parameters.</abstract>
      <url hash="fd7b1f13">2022.findings-emnlp.67</url>
      <bibkey>tiong-etal-2022-plug</bibkey>
      <video href="2022.findings-emnlp.67.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.67</doi>
    </paper>
    <paper id="68">
      <title><fixed-case>TSGP</fixed-case>: Two-Stage Generative Prompting for Unsupervised Commonsense Question Answering</title>
      <author><first>Yueqing</first><last>Sun</last></author>
      <author><first>Yu</first><last>Zhang</last></author>
      <author><first>Le</first><last>Qi</last></author>
      <author><first>Qi</first><last>Shi</last></author>
      <pages>968-980</pages>
      <abstract>Without training on labeled task data, unsupervised commonsense question answering seems challenging since it requires commonsense knowledge beyond the context of questions. Previous methods typically retrieved from traditional knowledge bases or used pre-trained language models (PrLMs) to generate fixed types of knowledge, which have poor generalization ability. In this paper, we aim to address the above limitation by leveraging the implicit knowledge stored in PrLMs and propose a two-stage prompt-based unsupervised commonsense question answering framework (TSGP). We first use knowledge generation prompts to generate the knowledge required for questions with unlimited types and possible candidate answers independent of specified choices. Then, we further utilize answer generation prompts to generate possible candidate answers independent of specified choices. Experimental results and analysis on three different commonsense reasoning tasks, CommonsenseQA, OpenBookQA, and SocialIQA, demonstrate that TSGP significantly improves the reasoning ability of language models in unsupervised settings.</abstract>
      <url hash="332eb2c7">2022.findings-emnlp.68</url>
      <bibkey>sun-etal-2022-tsgp</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.68</doi>
    </paper>
    <paper id="69">
      <title>Subword-Delimited Downsampling for Better Character-Level Translation</title>
      <author><first>Lukas</first><last>Edman</last></author>
      <author><first>Antonio</first><last>Toral</last></author>
      <author><first>Gertjan</first><last>van Noord</last></author>
      <pages>981-992</pages>
      <abstract>Subword-level models have been the dominant paradigm in NLP. However, character-level models have the benefit of seeing each character individually, providing the model with more detailed information that ultimately could lead to better models. Recent works have shown character-level models to be competitive with subword models, but costly in terms of time and computation. Character-level models with a downsampling component alleviate this, but at the cost of quality, particularly for machine translation. This work analyzes the problems of previous downsampling methods and introduces a novel downsampling method which is informed by subwords.This new downsampling method not only outperforms existing downsampling methods, showing that downsampling characters can be done without sacrificing quality, but also leads to promising performance compared to subword models for translation.</abstract>
      <url hash="54df033c">2022.findings-emnlp.69</url>
      <bibkey>edman-etal-2022-subword</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.69</doi>
    </paper>
    <paper id="70">
      <title>Autoregressive Structured Prediction with Language Models</title>
      <author><first>Tianyu</first><last>Liu</last></author>
      <author><first>Yuchen Eleanor</first><last>Jiang</last></author>
      <author><first>Nicholas</first><last>Monath</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <author><first>Mrinmaya</first><last>Sachan</last></author>
      <pages>993-1005</pages>
      <abstract>Recent years have seen a paradigm shift in NLP towards using pretrained language models (PLM) for a wide range of tasks. However, there are many difficult design decisions to represent structures (e.g. tagged text, coreference chains) in a way such that they can be captured by PLMs. Prior work on structured prediction with PLMs typically flattens the structured output into a sequence, which limits the quality of structural information being learned and leads to inferior performance compared to classic discriminative models. In this work, we describe an approach to model structures as sequences of actions in an autoregressive manner with PLMs, allowing in-structure dependencies to be learned without any loss. Our approach achieves the new state-of-the-art on all the structured prediction tasks we looked at, namely, named entity recognition, end-to-end relation extraction, and coreference resolution.</abstract>
      <url hash="5c5af427">2022.findings-emnlp.70</url>
      <bibkey>liu-etal-2022-autoregressive</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.70</doi>
    </paper>
    <paper id="71">
      <title><fixed-case>XD</fixed-case>oc: Unified Pre-training for Cross-Format Document Understanding</title>
      <author><first>Jingye</first><last>Chen</last></author>
      <author><first>Tengchao</first><last>Lv</last></author>
      <author><first>Lei</first><last>Cui</last></author>
      <author><first>Cha</first><last>Zhang</last></author>
      <author><first>Furu</first><last>Wei</last></author>
      <pages>1006-1016</pages>
      <abstract>The surge of pre-training has witnessed the rapid development of document understanding recently. Pre-training and fine-tuning framework has been effectively used to tackle texts in various formats, including plain texts, document texts, and web texts. Despite achieving promising performance, existing pre-trained models usually target one specific document format at one time, making it difficult to combine knowledge from multiple document formats. To address this, we propose XDoc, a unified pre-trained model which deals with different document formats in a single model. For parameter efficiency, we share backbone parameters for different formats such as the word embedding layer and the Transformer layers. Meanwhile, we introduce adaptive layers with lightweight parameters to enhance the distinction across different formats. Experimental results have demonstrated that with only 36.7% parameters, XDoc achieves comparable or even better performance on a variety of downstream tasks compared with the individual pre-trained models, which is cost effective for real-world deployment. The code and pre-trained models are publicly available at <url>https://aka.ms/xdoc</url>.</abstract>
      <url hash="7ceec6bf">2022.findings-emnlp.71</url>
      <bibkey>chen-etal-2022-xdoc</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.71</doi>
    </paper>
    <paper id="72">
      <title>A Few More Examples May Be Worth Billions of Parameters</title>
      <author><first>Yuval</first><last>Kirstain</last></author>
      <author><first>Patrick</first><last>Lewis</last></author>
      <author><first>Sebastian</first><last>Riedel</last></author>
      <author><first>Omer</first><last>Levy</last></author>
      <pages>1017-1029</pages>
      <abstract>We investigate the dynamics of increasing the number of model parameters versus the number of labeled examples across a wide variety of tasks. Our exploration reveals that while scaling parameters consistently yields performance improvements, the contribution of additional examples highly depends on the task’s format. Specifically, in open question answering tasks, enlarging the training set does not improve performance. In contrast, classification, extractive question answering, and multiple choice tasks benefit so much from additional examples that collecting a few hundred examples is often “worth” billions of parameters. We hypothesize that unlike open question answering, which involves recalling specific information, solving strategies for tasks with a more restricted output space transfer across examples, and can therefore be learned with small amounts of labeled data.</abstract>
      <url hash="7be2f11f">2022.findings-emnlp.72</url>
      <bibkey>kirstain-etal-2022-examples</bibkey>
      <video href="2022.findings-emnlp.72.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.72</doi>
    </paper>
    <paper id="73">
      <title><fixed-case>MCP</fixed-case>: Self-supervised Pre-training for Personalized Chatbots with Multi-level Contrastive Sampling</title>
      <author><first>Zhaoheng</first><last>Huang</last></author>
      <author><first>Zhicheng</first><last>Dou</last></author>
      <author><first>Yutao</first><last>Zhu</last></author>
      <author><first>Zhengyi</first><last>Ma</last></author>
      <pages>1030-1042</pages>
      <abstract>Personalized chatbots focus on endowing the chatbots with a consistent personality to behave like real users and further act as personal assistants. Previous studies have explored generating implicit user profiles from the user’s dialogue history for building personalized chatbots. However, these studies only use the response generation loss to train the entire model, thus it is prone to suffer from the problem of data sparsity. Besides, they overemphasize the final generated response’s quality while ignoring the correlations and fusions between the user’s dialogue history, leading to rough data representations and performance degradation. To tackle these problems, we propose a self-supervised learning framework MCP for capturing better representations from users’ dialogue history for personalized chatbots. Specifically, we apply contrastive sampling methods to leverage the supervised signals hidden in user dialog history, and generate the pre-training samples for enhancing the model. We design three pre-training tasks based on three types of contrastive pairs from user dialogue history, namely response pairs, sequence augmentation pairs, and user pairs. We pre-train the utterance encoder and the history encoder towards the contrastive objectives and use these pre-trained encoders for generating user profiles while personalized response generation. Experimental results on two real-world datasets show a significant improvement in our proposed model MCP compared with the existing methods.</abstract>
      <url hash="296f93f8">2022.findings-emnlp.73</url>
      <bibkey>huang-etal-2022-mcp</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.73</doi>
    </paper>
    <paper id="74">
      <title><fixed-case>E</fixed-case>xpert<fixed-case>PLM</fixed-case>: Pre-training Expert Representation for Expert Finding</title>
      <author><first>Qiyao</first><last>Peng</last></author>
      <author><first>Hongtao</first><last>Liu</last></author>
      <pages>1043-1052</pages>
      <abstract>Expert Finding is an important task in Community Question Answering (CQA) platforms, which could help route questions to potential users to answer. The key is to learn representations of experts based on their historical answered questions accurately. In this paper, inspired by the strong text understanding ability of Pretrained Language modelings (PLMs), we propose a pre-training and fine-tuning expert finding framework. The core is that we design an expert-level pre-training paradigm, that effectively integrates expert interest and expertise simultaneously. Specifically different from the typical corpus-level pre-training, we treat each expert as the basic pre-training unit including all the historical answered question titles of the expert, which could fully indicate the expert interests for questions. Besides, we integrate the vote score information along with each answer of the expert into the pre-training phrase to model the expert ability explicitly. Finally, we propose a novel reputation-augmented Masked Language Model (MLM) pre-training strategy to capture the expert reputation information. In this way, our method could learn expert representation comprehensively, which then will be adopted and fine-tuned in the down-streaming expert-finding task. Extensive experimental results on six real-world CQA datasets demonstrate the effectiveness of our method.</abstract>
      <url hash="e7a1e315">2022.findings-emnlp.74</url>
      <bibkey>peng-liu-2022-expertplm</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.74</doi>
    </paper>
    <paper id="75">
      <title>You Truly Understand What <fixed-case>I</fixed-case> Need : Intellectual and Friendly Dialog Agents grounding Persona and Knowledge</title>
      <author><first>Jungwoo</first><last>Lim</last></author>
      <author><first>Myunghoon</first><last>Kang</last></author>
      <author><first>Yuna</first><last>Hur</last></author>
      <author><first>Seung Won</first><last>Jeong</last></author>
      <author><first>Jinsung</first><last>Kim</last></author>
      <author><first>Yoonna</first><last>Jang</last></author>
      <author><first>Dongyub</first><last>Lee</last></author>
      <author><first>Hyesung</first><last>Ji</last></author>
      <author><first>DongHoon</first><last>Shin</last></author>
      <author><first>Seungryong</first><last>Kim</last></author>
      <author><first>Heuiseok</first><last>Lim</last></author>
      <pages>1053-1066</pages>
      <abstract>To build a conversational agent that interacts fluently with humans, previous studies blend knowledge or personal profile into the pre-trained language model. However, the model that considers knowledge and persona at the same time is still limited, leading to hallucination and a passive way of using personas. We propose an effective dialogue agent that grounds external knowledge and persona simultaneously. The agent selects the proper knowledge and persona to use for generating the answers with our candidate scoring implemented with a poly-encoder. Then, our model generates the utterance with lesser hallucination and more engagingness utilizing retrieval augmented generation with knowledge-persona enhanced query. We conduct experiments on the persona-knowledge chat and achieve state-of-the-art performance in grounding and generation tasks on the automatic metrics. Moreover, we validate the answers from the models regarding hallucination and engagingness through human evaluation and qualitative results. We show our retriever’s effectiveness in extracting relevant documents compared to the other previous retrievers, along with the comparison of multiple candidate scoring methods. Code is available at <url>https://github.com/dlawjddn803/INFO</url></abstract>
      <url hash="c2d2c774">2022.findings-emnlp.75</url>
      <bibkey>lim-etal-2022-truly</bibkey>
      <video href="2022.findings-emnlp.75.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.75</doi>
    </paper>
    <paper id="76">
      <title>Faithful to the Document or to the World? Mitigating Hallucinations via Entity-Linked Knowledge in Abstractive Summarization</title>
      <author><first>Yue</first><last>Dong</last></author>
      <author><first>John</first><last>Wieting</last></author>
      <author><first>Pat</first><last>Verga</last></author>
      <pages>1067-1082</pages>
      <abstract>Existing abstractive summarization systems are hampered by content hallucinations in which models generate text that is not directly inferable from the source alone. Annotations from prior work have shown that some of these hallucinations, while being ‘unfaithful’ to the source, are nonetheless factual. Our analysis in this paper suggests that these factual hallucinations occur as a result of the prevalence of factual yet unfaithful entities in summarization datasets. We find that these entities are not aberrations, but instead examples of additional world knowledge being readily used to latently connect entities and concepts – in this case connecting entities in the source document to those in the target summary. In our analysis and experiments, we demonstrate that connecting entities to an external knowledge base can lend provenance to many of these unfaithful yet factual entities, and further, this knowledge can be used to improve the factuality of summaries without simply making them more extractive.</abstract>
      <url hash="cde3479a">2022.findings-emnlp.76</url>
      <bibkey>dong-etal-2022-faithful</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.76</doi>
    </paper>
    <paper id="77">
      <title><fixed-case>RL</fixed-case> with <fixed-case>KL</fixed-case> penalties is better viewed as <fixed-case>B</fixed-case>ayesian inference</title>
      <author><first>Tomasz</first><last>Korbak</last></author>
      <author><first>Ethan</first><last>Perez</last></author>
      <author><first>Christopher</first><last>Buckley</last></author>
      <pages>1083-1091</pages>
      <abstract>Reinforcement learning (RL) is frequently employed in fine-tuning large language models (LMs), such as GPT-3, to penalize them for undesirable features of generated sequences, such as offensiveness, social bias, harmfulness or falsehood. The RL formulation involves treating the LM as a policy and updating it to maximise the expected value of a reward function which captures human preferences, such as non-offensiveness. In this paper, we analyze challenges associated with treating a language model as an RL policy and show how avoiding those challenges requires moving beyond the RL paradigm. We start by observing that the standard RL approach is flawed as an objective for fine-tuning LMs because it leads to distribution collapse: turning the LM into a degenerate distribution. Then, we analyze KL-regularised RL, a widely used recipe for fine-tuning LMs, which additionally constrains the fine-tuned LM to stay close to its original distribution in terms of Kullback-Leibler (KL) divergence. We show that KL-regularised RL is equivalent to variational inference: approximating a Bayesian posterior which specifies how to update a prior LM to conform with evidence provided by the reward function. We argue that this Bayesian inference view of KL-regularised RL is more insightful than the typically employed RL perspective. The Bayesian inference view explains how KL-regularised RL avoids the distribution collapse problem and offers a first-principles derivation for its objective. While this objective happens to be equivalent to RL (with a particular choice of parametric reward), there exist other objectives for fine-tuning LMs which are no longer equivalent to RL. That observation leads to a more general point: RL is not an adequate formal framework for problems such as fine-tuning language models. These problems are best viewed as Bayesian inference: approximating a pre-defined target distribution.</abstract>
      <url hash="32a4d376">2022.findings-emnlp.77</url>
      <bibkey>korbak-etal-2022-rl</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.77</doi>
    </paper>
    <paper id="78">
      <title>Evaluating Token-Level and Passage-Level Dense Retrieval Models for Math Information Retrieval</title>
      <author><first>Wei</first><last>Zhong</last></author>
      <author><first>Jheng-Hong</first><last>Yang</last></author>
      <author><first>Yuqing</first><last>Xie</last></author>
      <author><first>Jimmy</first><last>Lin</last></author>
      <pages>1092-1102</pages>
      <abstract>With the recent success of dense retrieval methods based on bi-encoders, studies have applied this approach to various interesting downstream retrieval tasks with good efficiency and in-domain effectiveness. Recently, we have also seen the presence of dense retrieval models in Math Information Retrieval (MIR) tasks,but the most effective systems remain classic retrieval methods that consider hand-crafted structure features. In this work, we try to combine the best of both worlds: a well-defined structure search method for effective formula search and efficient bi-encoder dense retrieval models to capture contextual similarities. Specifically, we have evaluated two representative bi-encoder models for token-level and passage-level dense retrieval on recent MIR tasks. Our results show that bi-encoder models are highly complementary to existing structure search methods, and we are able to advance the state-of-the-art on MIR datasets.</abstract>
      <url hash="9b8e6d89">2022.findings-emnlp.78</url>
      <bibkey>zhong-etal-2022-evaluating</bibkey>
      <video href="2022.findings-emnlp.78.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.78</doi>
    </paper>
    <paper id="79">
      <title>Multi-View Reasoning: Consistent Contrastive Learning for Math Word Problem</title>
      <author><first>Wenqi</first><last>Zhang</last></author>
      <author><first>Yongliang</first><last>Shen</last></author>
      <author><first>Yanna</first><last>Ma</last></author>
      <author><first>Xiaoxia</first><last>Cheng</last></author>
      <author><first>Zeqi</first><last>Tan</last></author>
      <author><first>Qingpeng</first><last>Nong</last></author>
      <author><first>Weiming</first><last>Lu</last></author>
      <pages>1103-1116</pages>
      <abstract>Math word problem solver requires both precise relation reasoning about quantities in the text and reliable generation for the diverse equation. Current sequence-to-tree or relation extraction methods regard this only from a fixed view, struggling to simultaneously handle complex semantics and diverse equations. However, human solving naturally involves two consistent reasoning views: top-down and bottom-up, just as math equations also can be expressed in multiple equivalent forms: pre-order and post-order. We propose a multi-view consistent contrastive learning for a more complete semantics-to-equation mapping. The entire process is decoupled into two independent but consistent views: top-down decomposition and bottom-up construction, and the two reasoning views are aligned in multi-granularity for consistency, enhancing global generation and precise reasoning. Experiments on multiple datasets across two languages show our approach significantly outperforms the existing baselines, especially on complex problems. We also show after consistent alignment, multi-view can absorb the merits of both views and generate more diverse results consistent with the mathematical laws.</abstract>
      <url hash="fb6c3cbc">2022.findings-emnlp.79</url>
      <bibkey>zhang-etal-2022-multi-view</bibkey>
      <video href="2022.findings-emnlp.79.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.79</doi>
    </paper>
    <paper id="80">
      <title>Few-shot initializing of Active Learner via Meta-Learning</title>
      <author><first>Zi Long</first><last>Zhu</last></author>
      <author><first>Vikrant</first><last>Yadav</last></author>
      <author><first>Zubair</first><last>Afzal</last></author>
      <author><first>George</first><last>Tsatsaronis</last></author>
      <pages>1117-1133</pages>
      <abstract>Despite the important evolutions in few-shot and zero-shot learning techniques, domain specific applications still require expert knowledge and significant effort in annotating and labeling a large volume of unstructured textual data. To mitigate this problem, active learning, and meta-learning attempt to reach a high performance with the least amount of labeled data. In this paper, we introduce a novel approach to combine both lines of work by initializing an active learner with meta-learned parameters obtained through meta-training on tasks similar to the target task during active learning. In this approach we use the pre-trained BERT as our text-encoder and meta-learn its parameters with LEOPARD, which extends the model-agnostic meta-learning method by generating task dependent softmax weights to enable learning across tasks with different number of classes. We demonstrate the effectiveness of our method by performing active learning on five natural language understanding tasks and six datasets with five different acquisition functions. We train two different meta-initializations, and we use the pre-trained BERT base initialization as baseline. We observe that our approach performs better than the baseline at low budget, especially when closely related tasks were present during meta-learning. Moreover, our results show that better performance in the initial phase, i.e., with fewer labeled samples, leads to better performance when larger acquisition batches are used. We also perform an ablation study of the proposed method, showing that active learning with only the meta-learned weights is beneficial and adding the meta-learned learning rates and generating the softmax have negative consequences for the performance.</abstract>
      <url hash="462819ee">2022.findings-emnlp.80</url>
      <bibkey>zhu-etal-2022-shot</bibkey>
      <video href="2022.findings-emnlp.80.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.80</doi>
    </paper>
    <paper id="81">
      <title>Bootstrapping meaning through listening: Unsupervised learning of spoken sentence embeddings</title>
      <author><first>Jian</first><last>Zhu</last></author>
      <author><first>Zuoyu</first><last>Tian</last></author>
      <author><first>Yadong</first><last>Liu</last></author>
      <author><first>Cong</first><last>Zhang</last></author>
      <author><first>Chia-Wen</first><last>Lo</last></author>
      <pages>1134-1154</pages>
      <abstract>Inducing semantic representations directly from speech signals is a highly challenging task but has many useful applications in speech mining and spoken language understanding. This study tackles the unsupervised learning of semantic representations for spoken utterances. Through converting speech signals into hidden units generated from acoustic unit discovery, we propose WavEmbed, a multimodal sequential autoencoder that predicts hidden units from a dense representation of speech. Secondly, we also propose S-HuBERT to induce meaning through knowledge distillation, in which a sentence embedding model is first trained on hidden units and passes its knowledge to a speech encoder through contrastive learning. The best performing model achieves a moderate correlation (0.5 0.6) with human judgments, without relying on any labels or transcriptions. Furthermore, these models can also be easily extended to leverage textual transcriptions of speech to learn much better speech embeddings that are strongly correlated with human annotations. Our proposed methods are applicable to the development of purely data-driven systems for speech mining, indexing and search.</abstract>
      <url hash="1e5882f8">2022.findings-emnlp.81</url>
      <bibkey>zhu-etal-2022-bootstrapping</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.81</doi>
    </paper>
    <paper id="82">
      <title>Progressive Sentiment Analysis for Code-Switched Text Data</title>
      <author><first>Sudhanshu</first><last>Ranjan</last></author>
      <author><first>Dheeraj</first><last>Mekala</last></author>
      <author><first>Jingbo</first><last>Shang</last></author>
      <pages>1155-1167</pages>
      <abstract>Multilingual transformer language models have recently attracted much attention from researchers and are used in cross-lingual transfer learning for many NLP tasks such as text classification and named entity recognition. However, similar methods for transfer learning from monolingual text to code-switched text have not been extensively explored mainly due to the following challenges:(1) Code-switched corpus, unlike monolingual corpus, consists of more than one language and existing methods can’t be applied efficiently,(2) Code-switched corpus is usually made of resource-rich and low-resource languages and upon using multilingual pre-trained language models, the final model might bias towards resource-rich language. In this paper, we focus on code-switched sentiment analysis where we have a labelled resource-rich language dataset and unlabelled code-switched data. We propose a framework that takes the distinction between resource-rich and low-resource language into account. Instead of training on the entire code-switched corpus at once, we create buckets based on the fraction of words in the resource-rich language and progressively train from resource-rich language dominated samples to low-resource language dominated samples. Extensive experiments across multiple language pairs demonstrate that progressive training helps low-resource language dominated samples.</abstract>
      <url hash="a7ea442a">2022.findings-emnlp.82</url>
      <bibkey>ranjan-etal-2022-progressive</bibkey>
      <video href="2022.findings-emnlp.82.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.82</doi>
    </paper>
    <paper id="83">
      <title>Knowledge Stimulated Contrastive Prompting for Low-Resource Stance Detection</title>
      <author><first>Kai</first><last>Zheng</last></author>
      <author><first>Qingfeng</first><last>Sun</last></author>
      <author><first>Yaming</first><last>Yang</last></author>
      <author><first>Fei</first><last>Xu</last></author>
      <pages>1168-1178</pages>
      <abstract>Stance Detection Task (SDT) aims at identifying the stance of the sentence towards a specific target and is usually modeled as a classification problem. Backgound knowledge is often necessary for stance detection with respect to a specific target, especially when there is no target explicitly mentioned in text. This paper focuses on the knowledge stimulation for low-resource stance detection tasks. We firstly explore to formalize stance detection as a prompt based contrastive learning task. At the same time, to make prompt learning suit to stance detection, we design a template mechanism to incorporate corresponding target into instance representation. Furthermore, we propose a masked language prompt joint contrastive learning approach to stimulate the knowledge inherit from the pre-trained model. The experimental results on three benchmarks show that knowledge stimulation is effective in stance detection accompanied with our proposed mechanism.</abstract>
      <url hash="302dd97a">2022.findings-emnlp.83</url>
      <bibkey>zheng-etal-2022-knowledge</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.83</doi>
    </paper>
    <paper id="84">
      <title><fixed-case>WS</fixed-case>peller: Robust Word Segmentation for Enhancing <fixed-case>C</fixed-case>hinese Spelling Check</title>
      <author><first>Fangfang</first><last>Li</last></author>
      <author><first>Youran</first><last>Shan</last></author>
      <author><first>Junwen</first><last>Duan</last></author>
      <author><first>Xingliang</first><last>Mao</last></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <pages>1179-1188</pages>
      <abstract>Chinese spelling check (CSC) detects and corrects spelling errors in Chinese texts. Previous approaches have combined character-level phonetic and graphic information, ignoring the importance of segment-level information. According to our pilot study, spelling errors are always associated with incorrect word segmentation. When appropriate word boundaries are provided, CSC performance is greatly enhanced. Based on these findings, we present WSpeller, a CSC model that takes into account word segmentation. A fundamental component of WSpeller is a W-MLM, which is trained by predicting visually and phonetically similar words. Through modification of the embedding layer’s input, word segmentation information can be incorporated. Additionally, a robust module is trained to assist the W-MLM-based correction module by predicting the correct word segmentations from sentences containing spelling errors. We evaluate WSpeller on the widely used benchmark datasets SIGHAN13, SIGHAN14, and SIGHAN15. Our model is superior to state-of-the-art baselines on SIGHAN13 and SIGHAN15 and maintains equal performance on SIGHAN14.</abstract>
      <url hash="eefabbd3">2022.findings-emnlp.84</url>
      <bibkey>li-etal-2022-wspeller</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.84</doi>
    </paper>
    <paper id="85">
      <title>Extracting Trigger-sharing Events via an Event Matrix</title>
      <author><first>Jun</first><last>Xu</last></author>
      <author><first>Weidi</first><last>Xu</last></author>
      <author><first>Mengshu</first><last>Sun</last></author>
      <author><first>Taifeng</first><last>Wang</last></author>
      <author><first>Wei</first><last>Chu</last></author>
      <pages>1189-1201</pages>
      <abstract>A growing interest emerges in event extraction which aims to extract multiple events with triggers and arguments. Previous methods mitigate the problem of multiple events extraction by predicting the arguments conditioned on the event trigger and event type, assuming that these arguments belong to a single event. However, the assumption is invalid in general as there may be multiple events. Therefore, we present a unified framework called MatEE for trigger-sharing events extraction. It resolves the kernel bottleneck by effectively modeling the relations between arguments by an event matrix, where trigger-sharing events are represented by multiple cliques. We verify the proposed method on 3 widely-used benchmark datasets of event extraction. The experimental results show that it beats all the advanced competitors, significantly improving the state-of-the-art performances in event extraction.</abstract>
      <url hash="da469347">2022.findings-emnlp.85</url>
      <bibkey>xu-etal-2022-extracting</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.85</doi>
    </paper>
    <paper id="86">
      <title><fixed-case>T</fixed-case>ran<fixed-case>S</fixed-case>: Transition-based Knowledge Graph Embedding with Synthetic Relation Representation</title>
      <author><first>Xuanyu</first><last>Zhang</last></author>
      <author><first>Qing</first><last>Yang</last></author>
      <author><first>Dongliang</first><last>Xu</last></author>
      <pages>1202-1208</pages>
      <abstract>Knowledge graph embedding (KGE) aims to learn continuous vector representations of relations and entities in knowledge graph (KG). Recently, transition-based KGE methods have become popular and achieved promising performance. However, scoring patterns like TransE are not suitable for complex scenarios where the same entity pair has different relations. Although some models attempt to employ entity-relation interaction or projection to improve entity representation for one-to-many/many-to-one/many-to-many complex relations, they still continue the traditional scoring pattern, where only a single relation vector in the relation part is used to translate the head entity to the tail entity or their variants. And recent research shows that entity representation only needs to consider entities and their interactions to achieve better performance. Thus, in this paper, we propose a novel transition-based method, TranS, for KGE. The single relation vector of the relation part in the traditional scoring pattern is replaced by the synthetic relation representation with entity-relation interactions to solve these issues. And the entity part still retains its independence through entity-entity interactions. Experiments on a large KG dataset, ogbl-wikikg2, show that our model achieves state-of-the-art results.</abstract>
      <url hash="b46a0e91">2022.findings-emnlp.86</url>
      <bibkey>zhang-etal-2022-trans</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.86</doi>
    </paper>
    <paper id="87">
      <title>Sequential Topic Selection Model with Latent Variable for Topic-Grounded Dialogue</title>
      <author><first>Xiao-Fei</first><last>Wen</last></author>
      <author><first>Wei</first><last>Wei</last></author>
      <author><first>Xian-Ling</first><last>Mao</last></author>
      <pages>1209-1219</pages>
      <abstract>Recently, topic-grounded dialogue system has attracted significant attention due to its effectiveness in predicting the next topic to yield better responses via the historical context and given topic sequence. However, almost all existing topic prediction solutions focus on only the current conversation and corresponding topic sequence to predict the next conversation topic, without exploiting other topic-guided conversations which may contain relevant topic-transitions to current conversation. To address the problem, in this paper we propose a novel approach, named Sequential Global Topic Attention (SGTA) to exploit topic transition over all conversations in a subtle way for better modeling post-to-response topic-transition and guiding the response generation to the current conversation. Specifically, we introduce a latent space modeled as a Multivariate Skew-Normal distribution with hybrid kernel functions to flexibly integrate the global-level information with sequence-level information, and predict the topic based on the distribution sampling results. We also leverage a topic-aware prior-posterior approach for secondary selection of predicted topics, which is utilized to optimize the response generation task. Extensive experiments demonstrate that our model outperforms competitive baselines on prediction and generation tasks.</abstract>
      <url hash="37e6b8a2">2022.findings-emnlp.87</url>
      <bibkey>wen-etal-2022-sequential</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.87</doi>
    </paper>
    <paper id="88">
      <title>Robust Task-Oriented Dialogue Generation with Contrastive Pre-training and Adversarial Filtering</title>
      <author><first>Shiquan</first><last>Yang</last></author>
      <author><first>Xinting</first><last>Huang</last></author>
      <author><first>Jey Han</first><last>Lau</last></author>
      <author><first>Sarah</first><last>Erfani</last></author>
      <pages>1220-1234</pages>
      <abstract>Data artifacts incentivize machine learning models to learn non-transferable generalizations by taking advantage of shortcuts in the data, andthere is growing evidence that data artifacts play a role for the strong results that deep learning models achieve in recent natural language processing benchmarks. In this paper, we focus on task-oriented dialogue and investigate whether popular datasets such as MultiWOZ contain such data artifacts. We found that by only keeping frequent phrases in the trainingexamples, state-of-the-art models perform similarly compared to the variant trained with full data, suggesting they exploit these spurious correlationsto solve the task. Motivated by this, we propose a contrastive learning based framework to encourage the model to ignore these cues and focus on learning generalisable patterns. We also experiment with adversarial filtering to remove easy training instances so that the model would focus on learning from the harder instances. We conduct a number of generalization experiments — e.g., cross-domain/dataset and adversarial tests — to assess the robustness of our approach and found that it works exceptionally well.</abstract>
      <url hash="cbce7d14">2022.findings-emnlp.88</url>
      <bibkey>yang-etal-2022-robust</bibkey>
      <video href="2022.findings-emnlp.88.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.88</doi>
    </paper>
    <paper id="89">
      <title><fixed-case>STAR</fixed-case>: <fixed-case>SQL</fixed-case> Guided Pre-Training for Context-dependent Text-to-<fixed-case>SQL</fixed-case> Parsing</title>
      <author><first>Zefeng</first><last>Cai</last></author>
      <author><first>Xiangyu</first><last>Li</last></author>
      <author><first>Binyuan</first><last>Hui</last></author>
      <author><first>Min</first><last>Yang</last></author>
      <author><first>Bowen</first><last>Li</last></author>
      <author><first>Binhua</first><last>Li</last></author>
      <author><first>Zheng</first><last>Cao</last></author>
      <author><first>Weijie</first><last>Li</last></author>
      <author><first>Fei</first><last>Huang</last></author>
      <author><first>Luo</first><last>Si</last></author>
      <author><first>Yongbin</first><last>Li</last></author>
      <pages>1235-1247</pages>
      <abstract>In this paper, we propose a novel SQL guided pre-training framework STAR for context-dependent text-to-SQL parsing, which leverages contextual information to enrich natural language (NL) utterance and table schema representations for text-to-SQL conversations. Concretely, we propose two novel pre-training objectives which respectively explore the context-dependent interactions of NL utterances and SQL queries within each text-to-SQL conversation: (i) schema state tracking (SST) objective that tracks and explores the schema states of context-dependent SQL queries in the form of schema-states by predicting and updating the value of each schema slot during interaction; (ii) utterance dependency tracking (UDT) objective that employs weighted contrastive learning to pull together two semantically similar NL utterances and push away the representations of semantically dissimilar NL utterances within each conversation. In addition, we construct a high-quality large-scale context-dependent text-to-SQL conversation corpus to pre-train STAR. Extensive experiments show that STAR achieves new state-of-the-art performance on two downstream benchmarks (SParC and CoSQL), significantly outperforming previous pre-training methods and ranking first on the leaderboard. We believe the release of the constructed corpus, codebase and pre-trained STAR checkpoints would push forward the research in this area.</abstract>
      <url hash="0594848f">2022.findings-emnlp.89</url>
      <bibkey>cai-etal-2022-star</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.89</doi>
    </paper>
    <paper id="90">
      <title>Is <fixed-case>M</fixed-case>ulti<fixed-case>WOZ</fixed-case> a Solved Task? An Interactive <fixed-case>TOD</fixed-case> Evaluation Framework with User Simulator</title>
      <author><first>Qinyuan</first><last>Cheng</last></author>
      <author><first>Linyang</first><last>Li</last></author>
      <author><first>Guofeng</first><last>Quan</last></author>
      <author><first>Feng</first><last>Gao</last></author>
      <author><first>Xiaofeng</first><last>Mou</last></author>
      <author><first>Xipeng</first><last>Qiu</last></author>
      <pages>1248-1259</pages>
      <abstract>Task-Oriented Dialogue (TOD) systems are drawing more and more attention in recent studies. Current methods focus on constructing pre-trained models or fine-tuning strategies while the evaluation of TOD is limited by a policy mismatch problem. That is, during evaluation, the user utterances are from the annotated dataset while these utterances should interact with previous responses which can have many alternatives besides annotated texts. Therefore, in this work, we propose an interactive evaluation framework for TOD. We first build a goal-oriented user simulator based on pre-trained models and then use the user simulator to interact with the dialogue system to generate dialogues. Besides, we introduce a sentence-level and a session-level score to measure the sentence fluency and session coherence in the interactive evaluation. Experimental results show that RL-based TOD systems trained by our proposed user simulator can achieve nearly 98% inform and success rates in the interactive evaluation of MultiWOZ dataset and the proposed scores measure the response quality besides the inform and success rates. We are hoping that our work will encourage simulator-based interactive evaluations in the TOD task.</abstract>
      <url hash="cecb1956">2022.findings-emnlp.90</url>
      <bibkey>cheng-etal-2022-multiwoz</bibkey>
      <video href="2022.findings-emnlp.90.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.90</doi>
    </paper>
    <paper id="91">
      <title>Translating Hanja Historical Documents to Contemporary <fixed-case>K</fixed-case>orean and <fixed-case>E</fixed-case>nglish</title>
      <author><first>Juhee</first><last>Son</last></author>
      <author><first>Jiho</first><last>Jin</last></author>
      <author><first>Haneul</first><last>Yoo</last></author>
      <author><first>JinYeong</first><last>Bak</last></author>
      <author><first>Kyunghyun</first><last>Cho</last></author>
      <author><first>Alice</first><last>Oh</last></author>
      <pages>1260-1272</pages>
      <abstract>The Annals of Joseon Dynasty (AJD) contain the daily records of the Kings of Joseon, the 500-year kingdom preceding the modern nation of Korea.The Annals were originally written in an archaic Korean writing system, ‘Hanja’, and were translated into Korean from 1968 to 1993.The resulting translation was however too literal and contained many archaic Korean words; thus, a new expert translation effort began in 2012. Since then, the records of only one king have been completed in a decade. In parallel, expert translators are working on English translation, also at a slow pace and produced only one king’s records in English so far. Thus, we propose H2KE, a neural machine translation model, that translates historical documents in Hanja to more easily understandable Korean and to English.Built on top of multilingual neural machine translation, H2KE learns to translate a historical document written in Hanja, from both a full dataset of outdated Korean translation and a small dataset of more recently translated contemporary Korean and English.We compare our method against two baselines:a recent model that simultaneously learns to restore and translate Hanja historical documentand a Transformer based model trained only on newly translated corpora. The experiments reveal that our method significantly outperforms the baselines in terms of BLEU scores for both contemporary Korean and English translations. We further conduct extensive human evaluation which shows that our translation is preferred over the original expert translations by both experts and non-expert Korean speakers.</abstract>
      <url hash="bef67522">2022.findings-emnlp.91</url>
      <bibkey>son-etal-2022-translating</bibkey>
      <video href="2022.findings-emnlp.91.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.91</doi>
    </paper>
    <paper id="92">
      <title>Exploring Compositional Image Retrieval with Hybrid Compositional Learning and Heuristic Negative Mining</title>
      <author><first>Chao</first><last>Wang</last></author>
      <author><first>Ehsan</first><last>Nezhadarya</last></author>
      <author><first>Tanmana</first><last>Sadhu</last></author>
      <author><first>Shengdong</first><last>Zhang</last></author>
      <pages>1273-1285</pages>
      <abstract>Compositional image retrieval (CIR) is a challenging retrieval task, where the query is composed of a reference image and a modification text, and the target is another image reflecting the modification to the reference image. Due to the great success of the pre-trained vision-and-language model CLIP and its favorable applicability to large-scale retrieval tasks, we propose a CIR model HyCoLe-HNM with CLIP as the backbone. In HyCoLe-HNM, we follow the contrastive pre-training method of CLIP to perform cross-modal representation learning. On this basis, we propose a hybrid compositional learning mechanism, which includes both image compositional learning and text compositional learning. In hybrid compositional learning, we borrow a gated fusion mechanism from a question answering model to perform compositional fusion, and propose a heuristic negative mining method to filter negative samples. Privileged information in the form of image-related texts is utilized in cross-modal representation learning and hybrid compositional learning. Experimental results show that HyCoLe-HNM achieves state-of-the-art performance on three CIR datasets, namely FashionIQ, Fashion200K, and MIT-States.</abstract>
      <url hash="49ab8329">2022.findings-emnlp.92</url>
      <bibkey>wang-etal-2022-exploring</bibkey>
      <video href="2022.findings-emnlp.92.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.92</doi>
    </paper>
    <paper id="93">
      <title>Outlier Dimensions that Disrupt Transformers are Driven by Frequency</title>
      <author><first>Giovanni</first><last>Puccetti</last></author>
      <author><first>Anna</first><last>Rogers</last></author>
      <author><first>Aleksandr</first><last>Drozd</last></author>
      <author><first>Felice</first><last>Dell’Orletta</last></author>
      <pages>1286-1304</pages>
      <abstract>While Transformer-based language models are generally very robust to pruning, there is the recently discovered outlier phenomenon: disabling only 48 out of 110M parameters in BERT-base drops its performance by nearly 30% on MNLI. We replicate the original evidence for the outlier phenomenon and we link it to the geometry of the embedding space. We find that in both BERT and RoBERTa the magnitude of hidden state coefficients corresponding to outlier dimensions correlate with the frequencies of encoded tokens in pre-training data, and they also contribute to the “vertical” self-attention pattern enabling the model to focus on the special tokens. This explains the drop in performance from disabling the outliers, and it suggests that to decrease anisotopicity in future models we need pre-training schemas that would better take into account the skewed token distributions.</abstract>
      <url hash="325f3a67">2022.findings-emnlp.93</url>
      <bibkey>puccetti-etal-2022-outlier</bibkey>
      <video href="2022.findings-emnlp.93.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.93</doi>
    </paper>
    <paper id="94">
      <title><fixed-case>M</fixed-case>i<fixed-case>ST</fixed-case>: a Large-Scale Annotated Resource and Neural Models for Functions of Modal Verbs in <fixed-case>E</fixed-case>nglish Scientific Text</title>
      <author><first>Sophie</first><last>Henning</last></author>
      <author><first>Nicole</first><last>Macher</last></author>
      <author><first>Stefan</first><last>Grünewald</last></author>
      <author><first>Annemarie</first><last>Friedrich</last></author>
      <pages>1305-1324</pages>
      <abstract>Modal verbs (e.g., can, should or must) occur highly frequently in scientific articles. Decoding their function is not straightforward: they are often used for hedging, but they may also denote abilities and restrictions. Understanding their meaning is important for accurate information extraction from scientific text. To foster research on the usage of modals in this genre, we introduce the MIST (Modals In Scientific Text) dataset, which contains 3737 modal instances in five scientific domains annotated for their semantic, pragmatic, or rhetorical function. We systematically evaluate a set of competitive neural architectures on MIST. Transfer experiments reveal that leveraging non-scientific data is of limited benefit for modeling the distinctions in MIST. Our corpus analysis provides evidence that scientific communities differ in their usage of modal verbs, yet, classifiers trained on scientific data generalize to some extent to unseen scientific domains.</abstract>
      <url hash="a7293bc9">2022.findings-emnlp.94</url>
      <bibkey>henning-etal-2022-mist</bibkey>
      <video href="2022.findings-emnlp.94.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.94</doi>
    </paper>
    <paper id="95">
      <title>Late Prompt Tuning: A Late Prompt Could Be Better Than Many Prompts</title>
      <author><first>Xiangyang</first><last>Liu</last></author>
      <author><first>Tianxiang</first><last>Sun</last></author>
      <author><first>Xuanjing</first><last>Huang</last></author>
      <author><first>Xipeng</first><last>Qiu</last></author>
      <pages>1325-1338</pages>
      <abstract>Prompt tuning is a parameter-efficient tuning (PETuning) method for utilizing pre-trained models (PTMs) that simply prepends a soft prompt to the input and only optimizes the prompt to adapt PTMs to downstream tasks. Although it is parameter- and deployment-efficient, its performance still lags behind other state-of-the-art PETuning methods. Besides, the training cost of prompt tuning is not significantly reduced due to the back-propagation through the entire model. Through empirical analyses, we shed some light on the lagging performance of prompt tuning and recognize a trade-off between the propagation distance from label signals to the inserted prompt and the influence of the prompt on model outputs. Further, we present Late Prompt Tuning (LPT) that inserts a late prompt into an intermediate layer of the PTM instead of the input layer or all layers. The late prompt is obtained by a neural prompt generator conditioned on the hidden states before the prompt insertion layer and therefore is instance-dependent. Through extensive experimental results across various tasks and PTMs, we show that LPT can achieve competitive performance to full model tuning and other PETuning methods under both full-data and few-shot scenarios while possessing faster training speed and lower memory cost.</abstract>
      <url hash="d5d6c5f0">2022.findings-emnlp.95</url>
      <bibkey>liu-etal-2022-late</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.95</doi>
    </paper>
    <paper id="96">
      <title><fixed-case>MICO</fixed-case>: A Multi-alternative Contrastive Learning Framework for Commonsense Knowledge Representation</title>
      <author><first>Ying</first><last>Su</last></author>
      <author><first>Zihao</first><last>Wang</last></author>
      <author><first>Tianqing</first><last>Fang</last></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Yangqiu</first><last>Song</last></author>
      <author><first>Tong</first><last>Zhang</last></author>
      <pages>1339-1351</pages>
      <abstract>Commonsense reasoning tasks such as commonsense knowledge graph completion and commonsense question answering require powerful representation learning. In this paper, we propose to learn commonsense knowledge representation by MICO, a Multi-alternative contrastIve learning framework on COmmonsense knowledge graphs (MICO). MICO generates the commonsense knowledge representation by contextual interaction between entity nodes and relations with multi-alternative contrastive learning. In MICO, the head and tail entities in an <tex-math>(h,r,t)</tex-math> knowledge triple are converted to two relation-aware sequence pairs (a premise and an alternative) in the form of natural language. Semantic representations generated by MICO can benefit the following two tasks by simply comparing the similarity score between the representations: 1) zero-shot commonsense question answering tasks; 2) inductive commonsense knowledge graph completion tasks. Extensive experiments show the effectiveness of our method.</abstract>
      <url hash="afd238bc">2022.findings-emnlp.96</url>
      <bibkey>su-etal-2022-mico</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.96</doi>
    </paper>
    <paper id="97">
      <title>Leveraging Only the Category Name for Aspect Detection through Prompt-based Constrained Clustering</title>
      <author><first>Yazheng</first><last>Li</last></author>
      <author><first>Pengyun</first><last>Wang</last></author>
      <author><first>Yasheng</first><last>Wang</last></author>
      <author><first>Yong</first><last>Dai</last></author>
      <author><first>Yadao</first><last>Wang</last></author>
      <author><first>Lujia</first><last>Pan</last></author>
      <author><first>Zenglin</first><last>Xu</last></author>
      <pages>1352-1364</pages>
      <abstract>Aspect category detection (ACD) aims to automatically identify user-concerned aspects from online reviews, which is of great value for evaluating the fine-grained performance of a product. The most recent solutions tackle this problem via weakly supervised methods, achieving remarkable improvement over unsupervised methods. However, a closer look at these methods reveals that the required human efforts are nontrivial and can sometimes be hard to obtain. In this study, we explore the possibility of minimizing human guidance while improving detection performance, with a deep clustering method that relies merely on the category name of each aspect and a pretrained language model (LM). The LM, combined with prompt techniques, is employed as a knowledge base to automatically generate constraints for clustering, as well as to provide a representation space to perform the clustering. Our method (1) extracts extensive keywords to expand our understanding of each aspect, (2) automatically generates instance-level and concept-level constraints for clustering, and (3) trains the clustering model with the above constraints. We demonstrate the capability of the proposed framework through extensive experiments on nine benchmark datasets. Our model not only performs noticeably better than existing unsupervised approaches but also considerably surpasses weakly supervised methods that require more human efforts.</abstract>
      <url hash="24b83752">2022.findings-emnlp.97</url>
      <bibkey>li-etal-2022-leveraging</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.97</doi>
    </paper>
    <paper id="98">
      <title>Controllable Factuality in Document-Grounded Dialog Systems Using a Noisy Channel Model</title>
      <author><first>Nico</first><last>Daheim</last></author>
      <author><first>David</first><last>Thulke</last></author>
      <author><first>Christian</first><last>Dugast</last></author>
      <author><first>Hermann</first><last>Ney</last></author>
      <pages>1365-1381</pages>
      <abstract>In this work, we present a model for document-grounded response generation in dialog that is decomposed into two components according to Bayes’ theorem. One component is a traditional ungrounded response generation model and the other component models the reconstruction of the grounding document based on the dialog context and generated response. We propose different approximate decoding schemes and evaluate our approach on multiple open-domain and task-oriented document-grounded dialog datasets. Our experiments show that the model is more factual in terms of automatic factuality metrics than the baseline model. Furthermore, we outline how introducing scaling factors between the components allows for controlling the tradeoff between factuality and fluency in the model output. Finally, we compare our approach to a recently proposed method to control factuality in grounded dialog, CTRL (Rashkin et al., 2021), and show that both approaches can be combined to achieve additional improvements.</abstract>
      <url hash="c3637eab">2022.findings-emnlp.98</url>
      <bibkey>daheim-etal-2022-controllable</bibkey>
      <video href="2022.findings-emnlp.98.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.98</doi>
    </paper>
    <paper id="99">
      <title>Transformer Language Models without Positional Encodings Still Learn Positional Information</title>
      <author><first>Adi</first><last>Haviv</last></author>
      <author><first>Ori</first><last>Ram</last></author>
      <author><first>Ofir</first><last>Press</last></author>
      <author><first>Peter</first><last>Izsak</last></author>
      <author><first>Omer</first><last>Levy</last></author>
      <pages>1382-1390</pages>
      <abstract>Causal transformer language models (LMs), such as GPT-3, typically require some form of positional encoding, such as positional embeddings. However, we show that LMs without any explicit positional encoding are still competitive with standard models and that this phenomenon is robust across different datasets, model sizes, and sequence lengths. Probing experiments reveal that such models acquire an implicit notion of absolute positions throughout the network, effectively compensating for the missing information. We conjecture that causal attention enables the model to infer the number of predecessors that each token can attend to, thereby approximating its absolute position. Our findings indicate that causal LMs might derive positional awareness not only from the explicit positioning mechanism but also from the effects of the causal mask.</abstract>
      <url hash="62cef91a">2022.findings-emnlp.99</url>
      <bibkey>haviv-etal-2022-transformer</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.99</doi>
    </paper>
    <paper id="100">
      <title>Beyond Model Interpretability: On the Faithfulness and Adversarial Robustness of Contrastive Textual Explanations</title>
      <author><first>Julia</first><last>El Zini</last></author>
      <author><first>Mariette</first><last>Awad</last></author>
      <pages>1391-1402</pages>
      <abstract>Contrastive explanation methods go beyond transparency and address the contrastive aspect of explanations. Such explanations are emerging as an attractive option to provide actionable change to scenarios adversely impacted by classifiers’ decisions. However, their extension to textual data is under-explored and there is little investigation on their vulnerabilities and limitations. This work motivates textual counterfactuals by highlighting the social limitations of non-contrastive explainability. We also lay the ground for a novel evaluation scheme inspired by the faithfulness of explanations. Accordingly, we extend the computation of three metrics, proximity, connectedness and stability, to textual data and we benchmark two successful contrastive methods, POLYJUICE and MiCE, on our suggested metrics. Experiments on sentiment analysis data show that the connectedness of counterfactuals to their original counterparts is not obvious in both models. More interestingly, the generated contrastive texts are more attainable with POLYJUICE which highlights the significance of latent representations in counterfactual search. Finally, we perform the first semantic adversarial attack on textual recourse methods. The results demonstrate the robustness of POLYJUICE and the role that latent input representations play in robustness and reliability.</abstract>
      <url hash="5279ac0b">2022.findings-emnlp.100</url>
      <bibkey>el-zini-awad-2022-beyond</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.100</doi>
    </paper>
    <paper id="101">
      <title>How Much Does Attention Actually Attend? Questioning the Importance of Attention in Pretrained Transformers</title>
      <author><first>Michael</first><last>Hassid</last></author>
      <author><first>Hao</first><last>Peng</last></author>
      <author><first>Daniel</first><last>Rotem</last></author>
      <author><first>Jungo</first><last>Kasai</last></author>
      <author><first>Ivan</first><last>Montero</last></author>
      <author><first>Noah A.</first><last>Smith</last></author>
      <author><first>Roy</first><last>Schwartz</last></author>
      <pages>1403-1416</pages>
      <abstract>The attention mechanism is considered the backbone of the widely-used Transformer architecture. It contextualizes the input by computing input-specific attention matrices. We find that this mechanism, while powerful and elegant, is not as important as typically thought for pretrained language models. We introduce PAPA, a new probing method that replaces the input-dependent attention matrices with constant ones—the average attention weights over multiple inputs. We use PAPA to analyze several established pretrained Transformers on six downstream tasks. We find that without any input-dependent attention, all models achieve competitive performance—an average relative drop of only 8% from the probing baseline. Further, little or no performance drop is observed when replacing half of the input-dependent attention matrices with constant (input-independent) ones. Interestingly, we show that better-performing models lose more from applying our method than weaker models, suggesting that the utilization of the input-dependent attention mechanism might be a factor in their success. Our results motivate research on simpler alternatives to input-dependent attention, as well as on methods for better utilization of this mechanism in the Transformer architecture.</abstract>
      <url hash="125f54a9">2022.findings-emnlp.101</url>
      <bibkey>hassid-etal-2022-much</bibkey>
      <video href="2022.findings-emnlp.101.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.101</doi>
    </paper>
    <paper id="102">
      <title>What Has Been Enhanced in my Knowledge-Enhanced Language Model?</title>
      <author><first>Yifan</first><last>Hou</last></author>
      <author><first>Guoji</first><last>Fu</last></author>
      <author><first>Mrinmaya</first><last>Sachan</last></author>
      <pages>1417-1438</pages>
      <abstract>A number of knowledge integration (KI) methods have recently been proposed to incorporate external knowledge into pretrained language models (LMs). Even though knowledge-enhanced LMs (KELMs) outperform base LMs on knowledge-intensive tasks, the inner-workings of these KI methods are not well-understood. For instance, it is unclear which knowledge is effectively integrated into KELMs and which is not; and if such integration led to catastrophic forgetting of already learned knowledge. We show that existing model interpretation methods such as linear probes and prompts have some key limitations in answering these questions. Then, we revisit KI from an information-theoretic view and propose a new theoretically sound probe model called Graph Convolution Simulator (GCS) for KI interpretation. GCS is eventually quite simple – it uses graph attention on the corresponding knowledge graph for interpretation. We conduct various experiments to verify that GCS provides reasonable interpretation results for two well-known KELMs: ERNIE and K-Adapter. Our experiments reveal that only little knowledge is successfully integrated in these models, and simply increasing the size of the KI corpus may not lead to better KELMs.</abstract>
      <url hash="d4e57728">2022.findings-emnlp.102</url>
      <bibkey>hou-etal-2022-enhanced</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.102</doi>
    </paper>
    <paper id="103">
      <title>Towards Generalized Open Information Extraction</title>
      <author><first>Bowen</first><last>Yu</last></author>
      <author><first>Zhenyu</first><last>Zhang</last></author>
      <author><first>Jingyang</first><last>Li</last></author>
      <author><first>Haiyang</first><last>Yu</last></author>
      <author><first>Tingwen</first><last>Sun</last></author>
      <author><first>Jian</first><last>Liu</last></author>
      <author><first>Yongbin</first><last>Li</last></author>
      <author><first>Bin</first><last>Wang</last></author>
      <pages>1439-1453</pages>
      <abstract>Open Information Extraction (OpenIE) facilitates the open-domain discovery of textual facts. However, the prevailing solutions evaluate OpenIE models on in-domain test sets aside from the training corpus, which certainly violates the initial task principle of domain-independence. In this paper, we propose to advance OpenIE towards a more realistic scenario: generalizing over unseen target domains with different data distributions from the source training domains, termed Generalized OpenIE. For this purpose, we first introduce GLOBE, a large-scale human-annotated multi-domain OpenIE benchmark, to examine the robustness of recent OpenIE models to domain shifts, and the relative performance degradation of up to 70% implies the challenges of generalized OpenIE. Then, we propose DragonIE, which explores a minimalist expression of textual fact: directed acyclic graph, to improve the OpenIE generalization ability. Extensive experiments demonstrate that DragonIE beats the previous methods in both in-domain and out-of-domain settings by as much as 6.0% in F1 score absolutely, but there is still ample room for improvement.</abstract>
      <url hash="2aa8272c">2022.findings-emnlp.103</url>
      <bibkey>nn-2022-towards</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.103</doi>
    </paper>
    <paper id="104">
      <title><fixed-case>B</fixed-case>io<fixed-case>LORD</fixed-case>: Learning Ontological Representations from Definitions for Biomedical Concepts and their Textual Descriptions</title>
      <author><first>François</first><last>Remy</last></author>
      <author><first>Kris</first><last>Demuynck</last></author>
      <author><first>Thomas</first><last>Demeester</last></author>
      <pages>1454-1465</pages>
      <abstract>This work introduces BioLORD, a new pre-training strategy for producing meaningful representations for clinical sentences and biomedical concepts. State-of-the-art methodologies operate by maximizing the similarity in representation of names referring to the same concept, and preventing collapse through contrastive learning. However, because biomedical names are not always self-explanatory, it sometimes results in non-semantic representations. BioLORD overcomes this issue by grounding its concept representations using definitions, as well as short descriptions derived from a multi-relational knowledge graph consisting of biomedical ontologies. Thanks to this grounding, our model produces more semantic concept representations that match more closely the hierarchical structure of ontologies. BioLORD establishes a new state of the art for text similarity on both clinical sentences (MedSTS) and biomedical concepts (MayoSRS).</abstract>
      <url hash="89edb938">2022.findings-emnlp.104</url>
      <bibkey>remy-etal-2022-biolord</bibkey>
      <video href="2022.findings-emnlp.104.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.104</doi>
    </paper>
    <paper id="105">
      <title>Improving the Extraction of Supertags for Constituency Parsing with Linear Context-Free Rewriting Systems</title>
      <author><first>Thomas</first><last>Ruprecht</last></author>
      <pages>1466-1477</pages>
      <abstract>In parsing phrase structures, supertagging achieves a symbiosis between the interpretability of formal grammars and the accuracy and speed of more recent neural models. The approach was only recently transferred to parsing discontinuous constituency structures with linear context-free rewriting systems (LCFRS).We reformulate and parameterize the previously fixed extraction process for LCFRS supertags with the aim to improve the overall parsing quality. These parameters are set in the context of several steps in the extraction process and are used to control the granularity of extracted grammar rules as well as the association of lexical symbols with each supertag.We evaluate the influence of the parameters on the sets of extracted supertags and the parsing quality using three treebanks in the English and German language, and we compare the best-performing configurations to recent state-of-the-art parsers in the area. Our results show that some of our configurations and the slightly modified parsing process improve the quality and speed of parsing with our supertags over the previous approach. Moreover, we achieve parsing scores that either surpass or are among the state-of-the-art in discontinuous constituent parsing.</abstract>
      <url hash="e4fa3b22">2022.findings-emnlp.105</url>
      <bibkey>ruprecht-2022-improving</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.105</doi>
    </paper>
    <paper id="106">
      <title>Mask More and Mask Later: Efficient Pre-training of Masked Language Models by Disentangling the [<fixed-case>MASK</fixed-case>] Token</title>
      <author><first>Baohao</first><last>Liao</last></author>
      <author><first>David</first><last>Thulke</last></author>
      <author><first>Sanjika</first><last>Hewavitharana</last></author>
      <author><first>Hermann</first><last>Ney</last></author>
      <author><first>Christof</first><last>Monz</last></author>
      <pages>1478-1492</pages>
      <abstract>The pre-training of masked language models (MLMs) consumes massive computation to achieve good results on downstream NLP tasks, resulting in a large carbon footprint. In the vanilla MLM, the virtual tokens, [MASK]s, act as placeholders and gather the contextualized information from unmasked tokens to restore the corrupted information. It raises the question of whether we can append [MASK]s at a later layer, to reduce the sequence length for earlier layers and make the pre-training more efficient. We show: (1) [MASK]s can indeed be appended at a later layer, being disentangled from the word embedding; (2) The gathering of contextualized information from unmasked tokens can be conducted with a few layers. By further increasing the masking rate from 15% to 50%, we can pre-train RoBERTa-base and RoBERTa-large from scratch with only 78% and 68% of the original computational budget without any degradation on the GLUE benchmark. When pre-training with the original budget, our method outperforms RoBERTa for 6 out of 8 GLUE tasks, on average by 0.4%.</abstract>
      <url hash="a31f1e5d">2022.findings-emnlp.106</url>
      <bibkey>liao-etal-2022-mask</bibkey>
      <video href="2022.findings-emnlp.106.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.106</doi>
    </paper>
    <paper id="107">
      <title><fixed-case>SMSM</fixed-case>ix: Sense-Maintained Sentence Mixup for Word Sense Disambiguation</title>
      <author><first>Hee Suk</first><last>Yoon</last></author>
      <author><first>Eunseop</first><last>Yoon</last></author>
      <author><first>John</first><last>Harvill</last></author>
      <author><first>Sunjae</first><last>Yoon</last></author>
      <author><first>Mark</first><last>Hasegawa-Johnson</last></author>
      <author><first>Chang</first><last>Yoo</last></author>
      <pages>1493-1502</pages>
      <abstract>Word Sense Disambiguation (WSD) is an NLP task aimed at determining the correct sense of a word in a sentence from discrete sense choices. Although current systems have attained unprecedented performances for such tasks, the nonuniform distribution of word senses during training generally results in systems performing poorly on rare senses. To this end, we consider data augmentation to increase the frequency of these least frequent senses (LFS) to reduce the distributional bias of senses during training. We propose Sense-Maintained Sentence Mixup (SMSMix), a novel word-level mixup method that maintains the sense of a target word. SMSMix smoothly blends two sentences using mask prediction while preserving the relevant span determined by saliency scores to maintain a specific word’s sense. To the best of our knowledge, this is the first attempt to apply mixup in NLP while preserving the meaning of a specific word. With extensive experiments, we validate that our augmentation method can effectively give more information about rare senses during training with maintained target sense label.</abstract>
      <url hash="2a04ddca">2022.findings-emnlp.107</url>
      <bibkey>yoon-etal-2022-smsmix</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.107</doi>
    </paper>
    <paper id="108">
      <title>On the Effectiveness of Automated Metrics for Text Generation Systems</title>
      <author><first>Pius</first><last>von Däniken</last></author>
      <author><first>Jan</first><last>Deriu</last></author>
      <author><first>Don</first><last>Tuggener</last></author>
      <author><first>Mark</first><last>Cieliebak</last></author>
      <pages>1503-1522</pages>
      <abstract>A major challenge in the field of Text Generation is evaluation, because we lack a sound theory that can be leveraged to extract guidelines for evaluation campaigns. In this work, we propose a first step towards such a theory that incorporates different sources of uncertainty, such as imperfect automated metrics and insufficiently sized test sets. The theory has practical applications, such as determining the number of samples needed to reliably distinguish the performance of a set of Text Generation systems in a given setting. We showcase the application of the theory on the WMT 21 and Spot-The-Bot evaluation data and outline how it can be leveraged to improve the evaluation protocol regarding the reliability, robustness, and significance of the evaluation outcome.</abstract>
      <url hash="f175fdc3">2022.findings-emnlp.108</url>
      <bibkey>von-daniken-etal-2022-effectiveness</bibkey>
      <video href="2022.findings-emnlp.108.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.108</doi>
    </paper>
    <paper id="109">
      <title>Residual Learning of Neural Text Generation with n-gram Language Model</title>
      <author><first>Huayang</first><last>Li</last></author>
      <author><first>Deng</first><last>Cai</last></author>
      <author><first>Jin</first><last>Xu</last></author>
      <author><first>Taro</first><last>Watanabe</last></author>
      <pages>1523-1533</pages>
      <abstract>N-gram language models (LM) has been largely superseded by neural LMs as the latter exhibits better performance. However, we find that n-gram models can achieve satisfactory performance on a large proportion of testing cases, indicating they have already captured abundant knowledge of the language with relatively low computational cost. With this observation, we propose to learn a neural LM that fits the residual between an n-gram LM and the real-data distribution. The combination of n-gram LMs and neural LMs not only allows the neural part to focus on deeper understanding of the language, but also provides a flexible way to customize a LM by switching the underlying n-gram model without changing the neural model. Experimental results on three typical language tasks (i.e., language modeling, machine translation, and summarization) demonstrate that our approach attains additional performance gains over popular standalone neural models consistently. We also show that our approach allows for effective domain adaptation by simply switching to a domain-specific n-gram model, without any extra training.</abstract>
      <url hash="236f4904">2022.findings-emnlp.109</url>
      <bibkey>li-etal-2022-residual</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.109</doi>
    </paper>
    <paper id="110">
      <title><fixed-case>D</fixed-case>iff<fixed-case>G</fixed-case>-<fixed-case>RL</fixed-case>: Leveraging Difference between Environment State and Common Sense</title>
      <author><first>Tsunehiko</first><last>Tanaka</last></author>
      <author><first>Daiki</first><last>Kimura</last></author>
      <author><first>Michiaki</first><last>Tatsubori</last></author>
      <pages>1534-1546</pages>
      <abstract>Taking into account background knowledge as the context has always been an important part of solving tasks that involve natural language. One representative example of such tasks is text-based games, where players need to make decisions based on both description text previously shown in the game, and their own background knowledge about the language and common sense. In this work, we investigate not simply giving common sense, as can be seen in prior research, but also its effective usage. We assume that a part of the environment states different from common sense should constitute one of the grounds for action selection. We propose a novel agent, DiffG-RL, which constructs a Difference Graph that organizes the environment states and common sense by means of interactive objects with a dedicated graph encoder. DiffG-RL also contains a framework for extracting the appropriate amount and representation of common sense from the source to support the construction of the graph. We validate DiffG-RL in experiments with text-based games that require common sense and show that it outperforms baselines by 17% of scores. We will make our code publicly available.</abstract>
      <url hash="a64e4cd2">2022.findings-emnlp.110</url>
      <bibkey>tanaka-etal-2022-diffg</bibkey>
      <video href="2022.findings-emnlp.110.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.110</doi>
    </paper>
    <paper id="111">
      <title>Unsupervised Syntactically Controlled Paraphrase Generation with <fixed-case>A</fixed-case>bstract <fixed-case>M</fixed-case>eaning <fixed-case>R</fixed-case>epresentations</title>
      <author><first>Kuan-Hao</first><last>Huang</last></author>
      <author><first>Varun</first><last>Iyer</last></author>
      <author><first>Anoop</first><last>Kumar</last></author>
      <author><first>Sriram</first><last>Venkatapathy</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <author><first>Aram</first><last>Galstyan</last></author>
      <pages>1547-1554</pages>
      <abstract>Syntactically controlled paraphrase generation has become an emerging research direction in recent years. Most existing approaches require annotated paraphrase pairs for training and are thus costly to extend to new domains. Unsupervised approaches, on the other hand, do not need paraphrase pairs but suffer from relatively poor performance in terms of syntactic control and quality of generated paraphrases. In this paper, we demonstrate that leveraging Abstract Meaning Representations (AMR) can greatly improve the performance of unsupervised syntactically controlled paraphrase generation. Our proposed model, AMR-enhanced Paraphrase Generator (AMRPG), separately encodes the AMR graph and the constituency parse of the input sentence into two disentangled semantic and syntactic embeddings. A decoder is then learned to reconstruct the input sentence from the semantic and syntactic embeddings. Our experiments show that AMRPG generates more accurate syntactically controlled paraphrases, both quantitatively and qualitatively, compared to the existing unsupervised approaches. We also demonstrate that the paraphrases generated by AMRPG can be used for data augmentation to improve the robustness of NLP models.</abstract>
      <url hash="f1133483">2022.findings-emnlp.111</url>
      <bibkey>huang-etal-2022-unsupervised-syntactically</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.111</doi>
    </paper>
    <paper id="112">
      <title>Can <fixed-case>AMR</fixed-case> Assist Legal and Logical Reasoning?</title>
      <author><first>Nikolaus</first><last>Schrack</last></author>
      <author><first>Ruixiang</first><last>Cui</last></author>
      <author><first>Hugo</first><last>López</last></author>
      <author><first>Daniel</first><last>Hershcovich</last></author>
      <pages>1555-1568</pages>
      <abstract>Abstract Meaning Representation (AMR) has been shown to be useful for many downstream tasks. In this work, we explore the use of AMR for legal and logical reasoning. Specifically, we investigate if AMR can help capture logical relationships on multiple choice question answering (MCQA) tasks. We propose neural architectures that utilize linearised AMR graphs in combination with pre-trained language models. While these models are not able to outperform text-only baselines, they correctly solve different instances than the text models, suggesting complementary abilities. Error analysis further reveals that AMR parsing quality is the most prominent challenge, especially regarding inputs with multiple sentences. We conduct a theoretical analysis of how logical relations are represented in AMR and conclude it might be helpful in some logical statements but not for others.</abstract>
      <url hash="bd9e209a">2022.findings-emnlp.112</url>
      <bibkey>schrack-etal-2022-amr</bibkey>
      <video href="2022.findings-emnlp.112.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.112</doi>
    </paper>
    <paper id="113">
      <title>Data Selection Curriculum for Neural Machine Translation</title>
      <author><first>Tasnim</first><last>Mohiuddin</last></author>
      <author><first>Philipp</first><last>Koehn</last></author>
      <author><first>Vishrav</first><last>Chaudhary</last></author>
      <author><first>James</first><last>Cross</last></author>
      <author><first>Shruti</first><last>Bhosale</last></author>
      <author><first>Shafiq</first><last>Joty</last></author>
      <pages>1569-1582</pages>
      <abstract>Neural Machine Translation (NMT) models are typically trained on heterogeneous data that are concatenated and randomly shuffled. However, not all of the training data are equally useful to the model. Curriculum training aims to present the data to the NMT models in a meaningful order. In this work, we introduce a two-stage training framework for NMT where we fine-tune a base NMT model on subsets of data, selected by both deterministic scoring using pre-trained methods and online scoring that considers prediction scores of the emerging NMT model. Through comprehensive experiments on six language pairs comprising low- and high-resource languages from WMT’21, we have shown that our curriculum strategies consistently demonstrate better quality (up to +2.2 BLEU improvement) and faster convergence (approximately 50% fewer updates).</abstract>
      <url hash="867bf2fa">2022.findings-emnlp.113</url>
      <bibkey>mohiuddin-etal-2022-data</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.113</doi>
    </paper>
    <paper id="114">
      <title>Text Editing as Imitation Game</title>
      <author><first>Ning</first><last>Shi</last></author>
      <author><first>Bin</first><last>Tang</last></author>
      <author><first>Bo</first><last>Yuan</last></author>
      <author><first>Longtao</first><last>Huang</last></author>
      <author><first>Yewen</first><last>Pu</last></author>
      <author><first>Jie</first><last>Fu</last></author>
      <author><first>Zhouhan</first><last>Lin</last></author>
      <pages>1583-1594</pages>
      <abstract>Text editing, such as grammatical error correction, arises naturally from imperfect textual data. Recent works frame text editing as a multi-round sequence tagging task, where operations – such as insertion and substitution – are represented as a sequence of tags. While achieving good results, this encoding is limited in flexibility as all actions are bound to token-level tags. In this work, we reformulate text editing as an imitation game using behavioral cloning. Specifically, we convert conventional sequence-to-sequence data into state-to-action demonstrations, where the action space can be as flexible as needed. Instead of generating the actions one at a time, we introduce a dual decoders structure to parallel the decoding while retaining the dependencies between action tokens, coupled with trajectory augmentation to alleviate the distribution shift that imitation learning often suffers. In experiments on a suite of Arithmetic Equation benchmarks, our model consistently outperforms the autoregressive baselines in terms of performance, efficiency, and robustness. We hope our findings will shed light on future studies in reinforcement learning applying sequence-level action generation to natural language processing.</abstract>
      <url hash="0f1e8a50">2022.findings-emnlp.114</url>
      <bibkey>shi-etal-2022-text</bibkey>
      <video href="2022.findings-emnlp.114.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.114</doi>
    </paper>
    <paper id="115">
      <title>Seeded Hierarchical Clustering for Expert-Crafted Taxonomies</title>
      <author><first>Anish</first><last>Saha</last></author>
      <author><first>Amith</first><last>Ananthram</last></author>
      <author><first>Emily</first><last>Allaway</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <author><first>Kathleen</first><last>McKeown</last></author>
      <pages>1595-1609</pages>
      <abstract>Practitioners from many disciplines (e.g., political science) use expert-crafted taxonomies to make sense of large, unlabeled corpora. In this work, we study Seeded Hierarchical Clustering (SHC): the task of automatically fitting unlabeled data to such taxonomies using a small set of labeled examples. We propose HierSeed, a novel weakly supervised algorithm for this task that uses only a small set of labeled seed examples in a computation and data efficient manner. HierSeed assigns documents to topics by weighing document density against topic hierarchical structure. It outperforms unsupervised and supervised baselines for the SHC task on three real-world datasets.</abstract>
      <url hash="be507f01">2022.findings-emnlp.115</url>
      <bibkey>saha-etal-2022-seeded</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.115</doi>
    </paper>
    <paper id="116">
      <title>Knowledge Graph Generation From Text</title>
      <author><first>Igor</first><last>Melnyk</last></author>
      <author><first>Pierre</first><last>Dognin</last></author>
      <author><first>Payel</first><last>Das</last></author>
      <pages>1610-1622</pages>
      <abstract>In this work we propose a novel end-to-end multi-stage Knowledge Graph (KG) generation system from textual inputs, separating the overall process into two stages. The graph nodes are generated first using pretrained language model, followed by a simple edge construction head, enabling efficient KG extraction from the text. For each stage we consider several architectural choices that can be used depending on the available training resources. We evaluated the model on a recent WebNLG 2020 Challenge dataset, matching the state-of-the-art performance on text-to-RDF generation task, as well as on New York Times (NYT) and a large-scale TekGen datasets, showing strong overall performance, outperforming the existing baselines. We believe that the proposed system can serve as a viable KG construction alternative to the existing linearization or sampling-based graph generation approaches.</abstract>
      <url hash="985875e5">2022.findings-emnlp.116</url>
      <bibkey>melnyk-etal-2022-knowledge</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.116</doi>
    </paper>
    <paper id="117">
      <title><fixed-case>D</fixed-case>ialogue<fixed-case>GAT</fixed-case>: A Graph Attention Network for Financial Risk Prediction by Modeling the Dialogues in Earnings Conference Calls</title>
      <author><first>Yunxin</first><last>Sang</last></author>
      <author><first>Yang</first><last>Bao</last></author>
      <pages>1623-1633</pages>
      <abstract>Financial risk prediction is an essential task for risk management in capital markets. While traditional prediction models are built based on the hard information of numerical data, recent studies have shown that the soft information of verbal cues in earnings conference calls is significant for predicting market risk due to its less constrained fashion and direct interaction between managers and analysts. However, most existing models mainly focus on extracting useful semantic information from the textual conference call transcripts but ignore their subtle yet important information of dialogue structures. To bridge this gap, we develop a graph attention network called DialogueGAT for financial risk prediction by simultaneously modeling the speakers and their utterances in dialogues in conference calls. Different from previous studies, we propose a new method for constructing the graph of speakers and utterances in a dialogue, and design contextual attention at both speaker and utterance levels for disentangling their effects on the downstream prediction task. For model evaluation, we extend an existing dataset of conference call transcripts by adding the dialogue structure and speaker information. Empirical results on our dataset of S&amp;P1500 companies demonstrate the superiority of our proposed model over competitive baselines from the extant literature.</abstract>
      <url hash="b768d79f">2022.findings-emnlp.117</url>
      <bibkey>sang-bao-2022-dialoguegat</bibkey>
      <video href="2022.findings-emnlp.117.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.117</doi>
    </paper>
    <paper id="118">
      <title>Investigating Ensemble Methods for Model Robustness Improvement of Text Classifiers</title>
      <author><first>Jieyu</first><last>Zhao</last></author>
      <author><first>Xuezhi</first><last>Wang</last></author>
      <author><first>Yao</first><last>Qin</last></author>
      <author><first>Jilin</first><last>Chen</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <pages>1634-1640</pages>
      <abstract>Large pre-trained language models have shown remarkable performance over the past few years. These models, however, sometimes learn superficial features from the dataset and cannot generalize to the distributions that are dissimilar to the training scenario. There have been several approaches proposed to reduce model’s reliance on these bias features which can improve model robustness in the out-of-distribution setting. However, existing methods usually use a fixed low-capacity model to deal with various bias features, which ignore the learnability of those features. In this paper, we analyze a set of existing bias features and demonstrate there is no single model that works best for all the cases. We further show that by choosing an appropriate bias model, we can obtain a better robustness result than baselines with a more sophisticated model design.</abstract>
      <url hash="ee3dce82">2022.findings-emnlp.118</url>
      <bibkey>zhao-etal-2022-investigating</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.118</doi>
    </paper>
    <paper id="119">
      <title>Adaptive Ranking-based Sample Selection for Weakly Supervised Class-imbalanced Text Classification</title>
      <author><first>Linxin</first><last>Song</last></author>
      <author><first>Jieyu</first><last>Zhang</last></author>
      <author><first>Tianxiang</first><last>Yang</last></author>
      <author><first>Masayuki</first><last>Goto</last></author>
      <pages>1641-1655</pages>
      <abstract>To obtain a large amount of training labels inexpensively, researchers have recently adopted the weak supervision (WS) paradigm, which leverages labeling rules to synthesize training labels rather than using individual annotations to achieve competitive results for natural language processing (NLP) tasks. However, data imbalance is often overlooked in applying the WS paradigm, despite being a common issue in a variety of NLP tasks. To address this challenge, we propose Adaptive Ranking-based Sample Selection (ARS2), a model-agnostic framework to alleviate the data imbalance issue in the WS paradigm. Specifically, it calculates a probabilistic margin score based on the output of the current model to measure and rank the cleanliness of each data point. Then, the ranked data are sampled based on both class-wise and rule-aware ranking. In particular, the two sample strategies corresponds to our motivations: (1) to train the model with balanced data batches to reduce the data imbalance issue and (2) to exploit the expertise of each labeling rule for collecting clean samples. Experiments on four text classification datasets with four different imbalance ratios show that ARS2 outperformed the state-of-the-art imbalanced learning and WS methods, leading to a 2%-57.8% improvement on their F1-score.</abstract>
      <url hash="757a2b25">2022.findings-emnlp.119</url>
      <bibkey>song-etal-2022-adaptive</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.119</doi>
    </paper>
    <paper id="120">
      <title><fixed-case>C</fixed-case>om<fixed-case>F</fixed-case>act: A Benchmark for Linking Contextual Commonsense Knowledge</title>
      <author><first>Silin</first><last>Gao</last></author>
      <author><first>Jena D.</first><last>Hwang</last></author>
      <author><first>Saya</first><last>Kanno</last></author>
      <author><first>Hiromi</first><last>Wakaki</last></author>
      <author><first>Yuki</first><last>Mitsufuji</last></author>
      <author><first>Antoine</first><last>Bosselut</last></author>
      <pages>1656-1675</pages>
      <abstract>Understanding rich narratives, such as dialogues and stories, often requires natural language processing systems to access relevant knowledge from commonsense knowledge graphs. However, these systems typically retrieve facts from KGs using simple heuristics that disregard the complex challenges of identifying situationally-relevant commonsense knowledge (e.g., contextualization, implicitness, ambiguity).In this work, we propose the new task of commonsense fact linking, where models are given contexts and trained to identify situationally-relevant commonsense knowledge from KGs. Our novel benchmark, ComFact, contains ~293k in-context relevance annotations for commonsense triplets across four stylistically diverse dialogue and storytelling datasets. Experimental results confirm that heuristic fact linking approaches are imprecise knowledge extractors. Learned fact linking models demonstrate across-the-board performance improvements (~34.6% F1) over these heuristics. Furthermore, improved knowledge retrieval yielded average downstream improvements of 9.8% for a dialogue response generation task. However, fact linking models still significantly underperform humans, suggesting our benchmark is a promising testbed for research in commonsense augmentation of NLP systems.</abstract>
      <url hash="7d4579c8">2022.findings-emnlp.120</url>
      <bibkey>gao-etal-2022-comfact</bibkey>
      <video href="2022.findings-emnlp.120.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.120</doi>
    </paper>
    <paper id="121">
      <title>Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models</title>
      <author><first>Victor</first><last>Bursztyn</last></author>
      <author><first>David</first><last>Demeter</last></author>
      <author><first>Doug</first><last>Downey</last></author>
      <author><first>Larry</first><last>Birnbaum</last></author>
      <pages>1676-1686</pages>
      <abstract>How to usefully encode compositional task structure has long been a core challenge in AI. Recent work in chain of thought prompting has shown that for very large neural language models (LMs), explicitly demonstrating the inferential steps involved in a target task may improve performance over end-to-end learning that focuses on the target task alone. However, chain of thought prompting has significant limitations due to its dependency on huge pretrained LMs. In this work, we present compositional fine-tuning (CFT): an approach based on explicitly decomposing a target task into component tasks, and then fine-tuning smaller LMs on a curriculum of such component tasks. We apply CFT to recommendation tasks in two domains, world travel and local dining, as well as a previously studied inferential task (sports understanding). We show that CFT outperforms end-to-end learning even with equal amounts of data, and gets consistently better as more component tasks are modeled via fine-tuning. Compared with chain of thought prompting, CFT performs at least as well using LMs only 7.4% of the size, and is moreover applicable to task domains for which data are not available during pretraining.</abstract>
      <url hash="634f8312">2022.findings-emnlp.121</url>
      <bibkey>bursztyn-etal-2022-learning</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.121</doi>
    </paper>
    <paper id="122">
      <title>Topic Taxonomy Expansion via Hierarchy-Aware Topic Phrase Generation</title>
      <author><first>Dongha</first><last>Lee</last></author>
      <author><first>Jiaming</first><last>Shen</last></author>
      <author><first>Seonghyeon</first><last>Lee</last></author>
      <author><first>Susik</first><last>Yoon</last></author>
      <author><first>Hwanjo</first><last>Yu</last></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <pages>1687-1700</pages>
      <abstract>Topic taxonomies display hierarchical topic structures of a text corpus and provide topical knowledge to enhance various NLP applications. To dynamically incorporate new topic information, several recent studies have tried to expand (or complete) a topic taxonomy by inserting emerging topics identified in a set of new documents. However, existing methods focus only on frequent terms in documents and the local topic-subtopic relations in a taxonomy, which leads to limited topic term coverage and fails to model the global taxonomy structure. In this work, we propose a novel framework for topic taxonomy expansion, named TopicExpan, which directly generates topic-related terms belonging to new topics. Specifically, TopicExpan leverages the hierarchical relation structure surrounding a new topic and the textual content of an input document for topic term generation. This approach encourages newly-inserted topics to further cover important but less frequent terms as well as to keep their relation consistency within the taxonomy. Experimental results on two real-world text corpora show that TopicExpan significantly outperforms other baseline methods in terms of the quality of output taxonomies.</abstract>
      <url hash="5d57b9f7">2022.findings-emnlp.122</url>
      <bibkey>lee-etal-2022-topic</bibkey>
      <video href="2022.findings-emnlp.122.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.122</doi>
    </paper>
    <paper id="123">
      <title>Language as a fingerprint: Self-supervised learning of user encodings using transformers</title>
      <author><first>Roberta</first><last>Rocca</last></author>
      <author><first>Tal</first><last>Yarkoni</last></author>
      <pages>1701-1714</pages>
      <abstract>The way we talk carries information about who we are. Demographics, personality, clinical conditions, political preferences influence what we speak about and how, suggesting that many individual attributes could be inferred from adequate encodings of linguistic behavior. Conversely, conditioning text representations on author attributes has been shown to improve model performance in many NLP tasks. Previous research on individual differences and language representations has mainly focused on predicting selected attributes from text, or on conditioning text representations on such attributes for author-based contextualization. Here, we present a self-supervised approach to learning language-based user encodings using transformers. Using a large corpus of Reddit submissions, we fine-tune DistilBERT on user-based triplet loss. We show that fine-tuned models can pick up on complex linguistic signatures of users, and that they are able to infer rich information about them. Through a series of intrinsic analyses and probing tasks, we provide evidence that fine-tuning enhances models’ ability to abstract generalizable user information, which yields performance advantages for user-based downstream tasks. We discuss applications in language-based assessment and contextualized and personalized NLP.</abstract>
      <url hash="ad9d061d">2022.findings-emnlp.123</url>
      <bibkey>rocca-yarkoni-2022-language</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.123</doi>
    </paper>
    <paper id="124">
      <title>Hyperdecoders: Instance-specific decoders for multi-task <fixed-case>NLP</fixed-case></title>
      <author><first>Hamish</first><last>Ivison</last></author>
      <author><first>Matthew</first><last>Peters</last></author>
      <pages>1715-1730</pages>
      <abstract>We investigate input-conditioned hypernetworks for multi-tasking in NLP, generating parameter-efficient adaptations for a decoder using a hypernetwork conditioned on the output of an encoder. This approach produces a unique decoder adaptation for every input instance, allowing the network a larger degree of flexibility than prior work that only produces one decoder adaptation per task. We apply our method to sequence classification tasks, extractive QA, and summarisation and find that it surpasses previous parameter efficient fine-tuning methods and often outperforms fully finetuning the underlying model. An analysis of the embeddings used by our hypernetwork shows that they are sensitive to output label and type, suggesting that our approach better maps from encoder representations to output labels. Our code is publicly available at <url>https://github.com/allenai/hyperdecoders</url>.</abstract>
      <url hash="54705daf">2022.findings-emnlp.124</url>
      <bibkey>ivison-peters-2022-hyperdecoders</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.124</doi>
    </paper>
    <paper id="125">
      <title>Evaluating the Faithfulness of Importance Measures in <fixed-case>NLP</fixed-case> by Recursively Masking Allegedly Important Tokens and Retraining</title>
      <author><first>Andreas</first><last>Madsen</last></author>
      <author><first>Nicholas</first><last>Meade</last></author>
      <author><first>Vaibhav</first><last>Adlakha</last></author>
      <author><first>Siva</first><last>Reddy</last></author>
      <pages>1731-1751</pages>
      <abstract>To explain NLP models a popular approach is to use importance measures, such as attention, which inform input tokens are important for making a prediction. However, an open question is how well these explanations accurately reflect a model’s logic, a property called faithfulness. To answer this question, we propose Recursive ROAR, a new faithfulness metric. This works by recursively masking allegedly important tokens and then retraining the model. The principle is that this should result in worse model performance compared to masking random tokens. The result is a performance curve given a masking-ratio. Furthermore, we propose a summarizing metric using area-between-curves (ABC), which allows for easy comparison across papers, models, and tasks. We evaluate 4 different importance measures on 8 different datasets, using both LSTM-attention models and RoBERTa models. We find that the faithfulness of importance measures is both model-dependent and task-dependent. This conclusion contradicts previous evaluations in both computer vision and faithfulness of attention literature.</abstract>
      <url hash="8c112d60">2022.findings-emnlp.125</url>
      <bibkey>madsen-etal-2022-evaluating</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.125</doi>
    </paper>
    <paper id="126">
      <title>Towards Explaining Subjective Ground of Individuals on Social Media</title>
      <author><first>Younghun</first><last>Lee</last></author>
      <author><first>Dan</first><last>Goldwasser</last></author>
      <pages>1752-1766</pages>
      <abstract>Large-scale language models have been reducing the gap between machines and humans in understanding the real world, yet understanding an individual’s theory of mind and behavior from text is far from being resolved. This research proposes a neural model—Subjective Ground Attention—that learns subjective grounds of individuals and accounts for their judgments on situations of others posted on social media. Using simple attention modules as well as taking one’s previous activities into consideration, we empirically show that our model provides human-readable explanations of an individual’s subjective preference in judging social situations. We further qualitatively evaluate the explanations generated by the model and claim that our model learns an individual’s subjective orientation towards abstract moral concepts.</abstract>
      <url hash="c1b5a78c">2022.findings-emnlp.126</url>
      <bibkey>lee-goldwasser-2022-towards</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.126</doi>
    </paper>
    <paper id="127">
      <title>Knowledge Injected Prompt Based Fine-tuning for Multi-label Few-shot <fixed-case>ICD</fixed-case> Coding</title>
      <author><first>Zhichao</first><last>Yang</last></author>
      <author><first>Shufan</first><last>Wang</last></author>
      <author><first>Bhanu Pratap Singh</first><last>Rawat</last></author>
      <author><first>Avijit</first><last>Mitra</last></author>
      <author><first>Hong</first><last>Yu</last></author>
      <pages>1767-1781</pages>
      <abstract>Automatic International Classification of Diseases (ICD) coding aims to assign multiple ICD codes to a medical note with average length of 3,000+ tokens. This task is challenging due to a high-dimensional space of multi-label assignment (tens of thousands of ICD codes) and the long-tail challenge: only a few codes (common diseases) are frequently assigned while most codes (rare diseases) are infrequently assigned. This study addresses the long-tail challenge by adapting a prompt-based fine-tuning technique with label semantics, which has been shown to be effective under few-shot setting. To further enhance the performance in medical domain, we propose a knowledge-enhanced longformer by injecting three domain-specific knowledge: hierarchy, synonym, and abbreviation with additional pretraining using contrastive learning. Experiments on MIMIC-III-full, a benchmark dataset of code assignment, show that our proposed method outperforms previous state-of-the-art method in 14.5% in marco F1 (from 10.3 to 11.8, P&lt;0.001). To further test our model on few-shot setting, we created a new rare diseases coding dataset, MIMIC-III-rare50, on which our model improves marco F1 from 17.1 to 30.4 and micro F1 from 17.2 to 32.6 compared to previous method.</abstract>
      <url hash="c2424f3a">2022.findings-emnlp.127</url>
      <bibkey>yang-etal-2022-knowledge-injected</bibkey>
      <video href="2022.findings-emnlp.127.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.127</doi>
    </paper>
    <paper id="128">
      <title>Do Language Models Understand Measurements?</title>
      <author><first>Sungjin</first><last>Park</last></author>
      <author><first>Seungwoo</first><last>Ryu</last></author>
      <author><first>Edward</first><last>Choi</last></author>
      <pages>1782-1792</pages>
      <abstract>Recent success of pre-trained language models (PLMs) has stimulated interest in their ability to understand and work with numbers. Yet, the numerical reasoning over measurements has not been formally studied despite their importance. In this study, we show that PLMs lack the capability required for reasoning over measurements. Furthermore, we find that a language model trained on a measurement-rich corpus shows better performance on understanding measurements. We propose a simple embedding strategy to better distinguish between numbers and units, which leads to a significant improvement in the probing tasks.</abstract>
      <url hash="4858d8b0">2022.findings-emnlp.128</url>
      <bibkey>park-etal-2022-language</bibkey>
      <video href="2022.findings-emnlp.128.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.128</doi>
    </paper>
    <paper id="129">
      <title>Reconciliation of Pre-trained Models and Prototypical Neural Networks in Few-shot Named Entity Recognition</title>
      <author><first>Youcheng</first><last>Huang</last></author>
      <author><first>Wenqiang</first><last>Lei</last></author>
      <author><first>Jie</first><last>Fu</last></author>
      <author><first>Jiancheng</first><last>Lv</last></author>
      <pages>1793-1807</pages>
      <abstract>Incorporating large-scale pre-trained models with the prototypical neural networks is a de-facto paradigm in few-shot named entity recognition. Existing methods, unfortunately, are not aware of the fact that embeddings from pre-trained models contain a prominently large amount of information regarding word frequencies, biasing prototypical neural networks against learning word entities. This discrepancy constrains the two models’ synergy. Thus, we propose a one-line-code normalization method to reconcile such a mismatch with empirical and theoretical grounds. Our experiments based on nine benchmark datasets show the superiority of our method over the counterpart models and are comparable to the state-of-the-art methods. In addition to the model enhancement, our work also provides an analytical viewpoint for addressing the general problems in few-shot name entity recognition or other tasks that rely on pre-trained models or prototypical neural networks.</abstract>
      <url hash="fc22abcc">2022.findings-emnlp.129</url>
      <bibkey>huang-etal-2022-reconciliation</bibkey>
      <video href="2022.findings-emnlp.129.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.129</doi>
    </paper>
    <paper id="130">
      <title><fixed-case>HCL</fixed-case>-<fixed-case>TAT</fixed-case>: A Hybrid Contrastive Learning Method for Few-shot Event Detection with Task-Adaptive Threshold</title>
      <author><first>Ruihan</first><last>Zhang</last></author>
      <author><first>Wei</first><last>Wei</last></author>
      <author><first>Xian-Ling</first><last>Mao</last></author>
      <author><first>Rui</first><last>Fang</last></author>
      <author><first>Dangyang</first><last>Chen</last></author>
      <pages>1808-1819</pages>
      <abstract>Event detection has been suffering from constantly emerging event types with lack of sufficient data. Existing works formulate the new problem as few-shot event detection (FSED), and employ two-stage or unified models based on meta-learning to address the problem. However, these methods fall far short of expectations due to: (i) insufficient learning of discriminative representations in low-resource scenarios, and (ii) representation overlap between triggers and non-triggers. To resolve the above issues, in this paper, we propose a novel Hybrid Contrastive Learning method with a Task-Adaptive Threshold (abbreviated as HCL-TAT), which enables discriminative representation learning with a two-view contrastive loss (support-support and prototype-query), and devises an easily-adapted threshold to alleviate misidentification of triggers. Extensive experiments on the benchmark dataset FewEvent demonstrate the superiority of our method to achieve better results compared to the state-of-the-arts. All the data and codes will be available to facilitate future research.</abstract>
      <url hash="7eed6812">2022.findings-emnlp.130</url>
      <bibkey>zhang-etal-2022-hcl</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.130</doi>
    </paper>
    <paper id="131">
      <title><fixed-case>D</fixed-case>oc2<fixed-case>B</fixed-case>ot: Accessing Heterogeneous Documents via Conversational Bots</title>
      <author><first>Haomin</first><last>Fu</last></author>
      <author><first>Yeqin</first><last>Zhang</last></author>
      <author><first>Haiyang</first><last>Yu</last></author>
      <author><first>Jian</first><last>Sun</last></author>
      <author><first>Fei</first><last>Huang</last></author>
      <author><first>Luo</first><last>Si</last></author>
      <author><first>Yongbin</first><last>Li</last></author>
      <author><first>Cam Tu</first><last>Nguyen</last></author>
      <pages>1820-1836</pages>
      <abstract>This paper introduces Doc2Bot, a novel dataset for building machines that help users seek information via conversations. This is of particular interest for companies and organizations that own a large number of manuals or instruction books. Despite its potential, the nature of our task poses several challenges: (1) documents contain various structures that hinder the ability of machines to comprehend, and (2) user information needs are often underspecified. Compared to prior datasets that either focus on a single structural type or overlook the role of questioning to uncover user needs, the Doc2Bot dataset is developed to target such challenges systematically. Our dataset contains over 100,000 turns based on Chinese documents from five domains, larger than any prior document-grounded dialog dataset for information seeking. We propose three tasks in Doc2Bot: (1) dialog state tracking to track user intentions, (2) dialog policy learning to plan system actions and contents, and (3) response generation which generates responses based on the outputs of the dialog policy. Baseline methods based on the latest deep learning models are presented, indicating that our proposed tasks are challenging and worthy of further research.</abstract>
      <url hash="40e9059b">2022.findings-emnlp.131</url>
      <bibkey>fu-etal-2022-doc2bot</bibkey>
      <video href="2022.findings-emnlp.131.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.131</doi>
    </paper>
    <paper id="132">
      <title><fixed-case>D</fixed-case>ual<fixed-case>NER</fixed-case>: A Dual-Teaching framework for Zero-shot Cross-lingual Named Entity Recognition</title>
      <author><first>Jiali</first><last>Zeng</last></author>
      <author><first>Yufan</first><last>Jiang</last></author>
      <author><first>Yongjing</first><last>Yin</last></author>
      <author><first>Xu</first><last>Wang</last></author>
      <author><first>Binghuai</first><last>Lin</last></author>
      <author><first>Yunbo</first><last>Cao</last></author>
      <pages>1837-1843</pages>
      <abstract>We present DualNER, a simple and effective framework to make full use of both annotated source language corpus and unlabeled target language text for zero-shot cross-lingual named entity recognition (NER). In particular, we combine two complementary learning paradigms of NER, i.e., sequence labeling and span prediction, into a unified multi-task framework. After obtaining a sufficient NER model trained on the source data, we further train it on the target data in a <i>dual-teaching</i> manner, in which the pseudo-labels for one task are constructed from the prediction of the other task. Moreover, based on the span prediction, an entity-aware regularization is proposed to enhance the intrinsic cross-lingual alignment between the same entities in different languages. Experiments and analysis demonstrate the effectiveness of our DualNER.</abstract>
      <url hash="4034c0a9">2022.findings-emnlp.132</url>
      <bibkey>zeng-etal-2022-dualner</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.132</doi>
    </paper>
    <paper id="133">
      <title>Knowledge-augmented Self-training of A Question Rewriter for Conversational Knowledge Base Question Answering</title>
      <author><first>Xirui</first><last>Ke</last></author>
      <author><first>Jing</first><last>Zhang</last></author>
      <author><first>Xin</first><last>Lv</last></author>
      <author><first>Yiqi</first><last>Xu</last></author>
      <author><first>Shulin</first><last>Cao</last></author>
      <author><first>Cuiping</first><last>Li</last></author>
      <author><first>Hong</first><last>Chen</last></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <pages>1844-1856</pages>
      <abstract>The recent rise of conversational applications such as online customer service systems and intelligent personal assistants has promoted the development of conversational knowledge base question answering (ConvKBQA). Different from the traditional single-turn KBQA, ConvKBQA usually explores multi-turn questions around a topic, where ellipsis and coreference pose great challenges to the single-turn KBQA systems which require self-contained questions. In this paper, we propose a rewrite-and-reason framework to first produce a full-fledged rewritten question based on the conversation history and then reason the answer by existing single-turn KBQA models. To overcome the absence of the rewritten supervision signals, we introduce a knowledge-augmented self-training mechanism to transfer the question rewriter from another dataset to adapt to the current knowledge base. Our question rewriter is decoupled from the subsequent QA process, which makes it easy to be united with either retrieval-based or semantic parsing-based KBQA models. Experiment results demonstrate the effectiveness of our method and a new state-of-the-art result is achieved. The code and dataset are available online now.</abstract>
      <url hash="fba22b95">2022.findings-emnlp.133</url>
      <bibkey>ke-etal-2022-knowledge</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.133</doi>
    </paper>
    <paper id="134">
      <title>Extractive Summarization of Legal Decisions using Multi-task Learning and Maximal Marginal Relevance</title>
      <author><first>Abhishek</first><last>Agarwal</last></author>
      <author><first>Shanshan</first><last>Xu</last></author>
      <author><first>Matthias</first><last>Grabmair</last></author>
      <pages>1857-1872</pages>
      <abstract>Summarizing legal decisions requires the expertise of law practitioners, which is both time- and cost-intensive. This paper presents techniques for extractive summarization of legal decisions in a low-resource setting using limited expert annotated data. We test a set of models that locate relevant content using a sequential model and tackle redundancy by leveraging maximal marginal relevance to compose summaries. We also demonstrate an implicit approach to help train our proposed models generate more informative summaries. Our multi-task learning model variant leverages rhetorical role identification as an auxiliary task to further improve the summarizer. We perform extensive experiments on datasets containing legal decisions from the US Board of Veterans’ Appeals and conduct quantitative and expert-ranked evaluations of our models. Our results show that the proposed approaches can achieve ROUGE scores vis-à-vis expert extracted summaries that match those achieved by inter-annotator comparison.</abstract>
      <url hash="545006b7">2022.findings-emnlp.134</url>
      <bibkey>agarwal-etal-2022-extractive</bibkey>
      <video href="2022.findings-emnlp.134.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.134</doi>
    </paper>
    <paper id="135">
      <title><fixed-case>M</fixed-case>ovie<fixed-case>UN</fixed-case>: A Dataset for Movie Understanding and Narrating</title>
      <author><first>Qi</first><last>Zhang</last></author>
      <author><first>Zihao</first><last>Yue</last></author>
      <author><first>Anwen</first><last>Hu</last></author>
      <author><first>Ziheng</first><last>Wang</last></author>
      <author><first>Qin</first><last>Jin</last></author>
      <pages>1873-1885</pages>
      <abstract>Automatic movie narration generation and narration grounding are very important to provide a true movie experience for the blind and visually impaired. To tell the movie story well, it is necessary to mention plot-related details (such as character names) and keep the narrations in a plot coherent. Taking these two points into consideration, we construct a Chinese large-scale video benchmark from 101 movies for Movie Understanding and Narrating (MovieUN) to support the Movie Clip Narrating (MCN) task and Temporal Narration Grounding (TNG) task. We split movies in MovieUN into movie clips according to plots, and pair them with corresponding narrations provided by the movie narrators. Ultimately, the TNG task involves 3,253 long video clips totaling 179 hours. The MCN task contains 33,060 video clips totaling 105 hours. We benchmark state-of-the-art video captioning models and temporal grounding models in MCN and TNG tasks, respectively. Furthermore, to accurately comprehend plots of different characters, we propose methods to incorporate portraits of actors as external knowledge in both tasks. The experiment results demonstrate the effectiveness of our proposed methods. The dataset and codes are released at <url>https://github.com/yuezih/MovieUN</url>.</abstract>
      <url hash="22b255d2">2022.findings-emnlp.135</url>
      <bibkey>zhang-etal-2022-movieun</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.135</doi>
    </paper>
    <paper id="136">
      <title><fixed-case>ASDOT</fixed-case>: Any-Shot Data-to-Text Generation with Pretrained Language Models</title>
      <author><first>Jiannan</first><last>Xiang</last></author>
      <author><first>Zhengzhong</first><last>Liu</last></author>
      <author><first>Yucheng</first><last>Zhou</last></author>
      <author><first>Eric</first><last>Xing</last></author>
      <author><first>Zhiting</first><last>Hu</last></author>
      <pages>1886-1899</pages>
      <abstract>Data-to-text generation is challenging due to the great variety of the input data in terms of domains (e.g., finance vs sports) or schemata (e.g., diverse predicates). Recent end-to-end neural methods thus require substantial training examples to learn to disambiguate and describe the data. Yet, real-world data-to-text problems often suffer from various data-scarce issues: one may have access to only a handful of or no training examples, and/or have to rely on examples in a different domain or schema. To fill this gap, we propose Any-Shot Data-to-Text (ASDOT), a new approach flexibly applicable to diverse settings by making efficient use of any given (or no) examples. ASDOT consists of two steps, data disambiguation and sentence fusion, both of which are amenable to be solved with off-the-shelf pretrained language models (LMs) with optional finetuning. In the data disambiguation stage, we employ the prompted GPT-3 model to understand possibly ambiguous triples from the input data and convert each into a short sentence with reduced ambiguity. The sentence fusion stage then uses an LM like T5 to fuse all the resulting sentences into a coherent paragraph as the final description. We evaluate extensively on various datasets in different scenarios, including the zero-/few-/full-shot settings, and generalization to unseen predicates and out-of-domain data. Experimental results show that ASDOT consistently achieves significant improvement over baselines, e.g., a 30.81 BLEU gain on the DART dataset under the zero-shot setting.</abstract>
      <url hash="083bded5">2022.findings-emnlp.136</url>
      <bibkey>xiang-etal-2022-asdot</bibkey>
      <video href="2022.findings-emnlp.136.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.136</doi>
    </paper>
    <paper id="137">
      <title><fixed-case>FCGEC</fixed-case>: Fine-Grained Corpus for <fixed-case>C</fixed-case>hinese Grammatical Error Correction</title>
      <author><first>Lvxiaowei</first><last>Xu</last></author>
      <author><first>Jianwang</first><last>Wu</last></author>
      <author><first>Jiawei</first><last>Peng</last></author>
      <author><first>Jiayu</first><last>Fu</last></author>
      <author><first>Ming</first><last>Cai</last></author>
      <pages>1900-1918</pages>
      <abstract>Grammatical Error Correction (GEC) has been broadly applied in automatic correction and proofreading system recently. However, it is still immature in Chinese GEC due to limited high-quality data from native speakers in terms of category and scale. In this paper, we present FCGEC, a fine-grained corpus to detect, identify and correct the grammatical errors. FCGEC is a human-annotated corpus with multiple references, consisting of 41,340 sentences collected mainly from multi-choice questions in public school Chinese examinations. Furthermore, we propose a Switch-Tagger-Generator (STG) baseline model to correct the grammatical errors in low-resource settings. Compared to other GEC benchmark models, experimental results illustrate that STG outperforms them on our FCGEC. However, there exists a significant gap between benchmark models and humans that encourages future models to bridge it.</abstract>
      <url hash="8b4b3a2e">2022.findings-emnlp.137</url>
      <bibkey>xu-etal-2022-fcgec</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.137</doi>
    </paper>
    <paper id="138">
      <title>Audience-Centric Natural Language Generation via Style Infusion</title>
      <author><first>Samraj</first><last>Moorjani</last></author>
      <author><first>Adit</first><last>Krishnan</last></author>
      <author><first>Hari</first><last>Sundaram</last></author>
      <author><first>Ewa</first><last>Maslowska</last></author>
      <author><first>Aravind</first><last>Sankar</last></author>
      <pages>1919-1932</pages>
      <abstract>Adopting contextually appropriate, audience-tailored linguistic styles is critical to the success of user-centric language generation systems (e.g., chatbots, computer-aided writing, dialog systems). While existing approaches demonstrate text style transfer (TST) with large volumes of parallel or non-parallel data, we argue that grounding style on audience-independent external factors is innately limiting for two reasons. First, it is difficult to collect large volumes of audience-specific stylistic data. Second, some stylistic objectives (e.g., persuasiveness, memorability, empathy) are hard to define without audience feedback. In this paper, we propose the novel task of style infusion - infusing the stylistic preferences of audiences in pretrained language generation models. Since humans are better at pairwise comparisons than direct scoring - i.e., is Sample-A more persuasive/polite/empathic than Sample-B - we leverage limited pairwise human judgments to bootstrap a style analysis model and augment our seed set of judgments. We then infuse the learned textual style in a GPT-2 based text generator while balancing fluency and style adoption. With quantitative and qualitative assessments, we show that our infusion approach can generate compelling stylized examples with generic text prompts. We make the anonymized code and data accessible.</abstract>
      <url hash="218afb19">2022.findings-emnlp.138</url>
      <bibkey>moorjani-etal-2022-audience</bibkey>
      <video href="2022.findings-emnlp.138.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.138</doi>
    </paper>
    <paper id="139">
      <title><fixed-case>D</fixed-case>oc<fixed-case>F</fixed-case>in: Multimodal Financial Prediction and Bias Mitigation using Semi-structured Documents</title>
      <author><first>Puneet</first><last>Mathur</last></author>
      <author><first>Mihir</first><last>Goyal</last></author>
      <author><first>Ramit</first><last>Sawhney</last></author>
      <author><first>Ritik</first><last>Mathur</last></author>
      <author><first>Jochen</first><last>Leidner</last></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Dinesh</first><last>Manocha</last></author>
      <pages>1933-1940</pages>
      <abstract>Financial prediction is complex due to the stochastic nature of the stock market. Semi-structured financial documents present comprehensive financial data in tabular formats, such as earnings, profit-loss statements, and balance sheets, and can often contain rich technical analysis along with a textual discussion of corporate history, and management analysis, compliance, and risks. Existing research focuses on the textual and audio modalities of financial disclosures from company conference calls to forecast stock volatility and price movement, but ignores the rich tabular data available in financial reports. Moreover, the economic realm is still plagued with a severe under-representation of various communities spanning diverse demographics, gender, and native speakers. In this work, we show that combining tabular data from financial semi-structured documents with text transcripts and audio recordings not only improves stock volatility and price movement prediction by 5-12% but also reduces gender bias caused due to audio-based neural networks by over 30%.</abstract>
      <url hash="4a98caa5">2022.findings-emnlp.139</url>
      <bibkey>mathur-etal-2022-docfin</bibkey>
      <video href="2022.findings-emnlp.139.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.139</doi>
    </paper>
    <paper id="140">
      <title>Not Just Plain Text! Fuel Document-Level Relation Extraction with Explicit Syntax Refinement and Subsentence Modeling</title>
      <author><first>Zhichao</first><last>Duan</last></author>
      <author><first>Xiuxing</first><last>Li</last></author>
      <author><first>Zhenyu</first><last>Li</last></author>
      <author><first>Zhuo</first><last>Wang</last></author>
      <author><first>Jianyong</first><last>Wang</last></author>
      <pages>1941-1951</pages>
      <abstract>Document-level relation extraction (DocRE) aims to identify semantic labels among entities within a single document. One major challenge of DocRE is to dig decisive details regarding a specific entity pair from long text. However, in many cases, only a fraction of text carries required information, even in the manually labeled supporting evidence. To better capture and exploit instructive information, we propose a novel expLicit syntAx Refinement and Subsentence mOdeliNg based framework (LARSON). By introducing extra syntactic information, LARSON can model subsentences of arbitrary granularity and efficiently screen instructive ones. Moreover, we incorporate refined syntax into text representations which further improves the performance of LARSON. Experimental results on three benchmark datasets (DocRED, CDR, and GDA) demonstrate that LARSON significantly outperforms existing methods.</abstract>
      <url hash="4dada5cd">2022.findings-emnlp.140</url>
      <bibkey>duan-etal-2022-just</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.140</doi>
    </paper>
    <paper id="141">
      <title>Self-supervised Rewiring of Pre-trained Speech Encoders:

Towards Faster Fine-tuning with Less Labels in Speech Processing</title>
      <author><first>Hao</first><last>Yang</last></author>
      <author><first>Jinming</first><last>Zhao</last></author>
      <author><first>Gholamreza</first><last>Haffari</last></author>
      <author><first>Ehsan</first><last>Shareghi</last></author>
      <pages>1952-1959</pages>
      <abstract>Pre-trained speech Transformers have facilitated great success across various speech processing tasks. However, fine-tuning these encoders for downstream tasks require sufficiently large training data to converge or to achieve state-of-the-art. In text domain this has been partly attributed to sub-optimality of the representation space in pre-trained Transformers. In this work, we take a sober look into pre-trained speech encoders and rewire their representation space without requiring any task-specific labels. Our method utilises neutrally synthesised version of audio inputs along with frame masking to construct positive pairs for contrastive self-supervised learning. When used for augmenting the wav2vec 2 encoder, we observe consistent improvement of isotropy in the representation space. Our experiments on 6 speech processing tasks, exhibit a significant convergence speedup during task fine-tuning as well as consistent task improvement, specially in low-resource settings.</abstract>
      <url hash="368c212c">2022.findings-emnlp.141</url>
      <bibkey>yang-etal-2022-self</bibkey>
      <video href="2022.findings-emnlp.141.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.141</doi>
    </paper>
    <paper id="142">
      <title><fixed-case>R</fixed-case>ed<fixed-case>A</fixed-case>pt: An Adaptor for wav2vec 2 Encoding

Faster and Smaller Speech Translation without Quality Compromise</title>
      <author><first>Jinming</first><last>Zhao</last></author>
      <author><first>Hao</first><last>Yang</last></author>
      <author><first>Gholamreza</first><last>Haffari</last></author>
      <author><first>Ehsan</first><last>Shareghi</last></author>
      <pages>1960-1967</pages>
      <abstract>Pre-trained speech Transformers in speech translation (ST) have facilitated state-of-the-art (SotA) results; yet, using such encoders is computationally expensive. To improve this, we present a novel Reducer Adaptor block, RedApt, that could be seamlessly integrated within any Transformer-based speech encoding architecture. Integrating the pretrained wav2vec 2 speech encoder with RedAptbrings 41% speedup, 33% memory reduction with 24% fewer FLOPs at inference. To our positive surprise, our ST model with RedApt outperforms the SotA architecture by an average of 0.68 BLEU score on 8 language pairs from Must-C.</abstract>
      <url hash="6a095382">2022.findings-emnlp.142</url>
      <bibkey>zhao-etal-2022-redapt</bibkey>
      <video href="2022.findings-emnlp.142.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.142</doi>
    </paper>
    <paper id="143">
      <title>How sensitive are translation systems to extra contexts? Mitigating gender bias in Neural Machine Translation models through relevant contexts.</title>
      <author><first>Shanya</first><last>Sharma</last></author>
      <author><first>Manan</first><last>Dey</last></author>
      <author><first>Koustuv</first><last>Sinha</last></author>
      <pages>1968-1984</pages>
      <abstract>Neural Machine Translation systems built on top of Transformer-based architectures are routinely improving the state-of-the-art in translation quality according to word-overlap metrics. However, a growing number of studies also highlight the inherent gender bias that these models incorporate during training, which reflects poorly in their translations. In this work, we investigate whether these models can be instructed to fix their bias during inference using targeted, guided instructions as contexts. By translating relevant contextual sentences during inference along with the input, we observe large improvements in reducing the gender bias in translations, across three popular test suites (WinoMT, BUG, SimpleGen). We further propose a novel metric to assess several large pre-trained models (OPUS-MT, M2M-100) on their sensitivity towards using contexts during translation to correct their biases. Our approach requires no fine-tuning, and thus can be used easily in production systems to de-bias translations from stereotypical gender-occupation bias. We hope our method, along with our metric, can be used to build better, bias-free translation systems.</abstract>
      <url hash="c291a179">2022.findings-emnlp.143</url>
      <bibkey>sharma-etal-2022-sensitive</bibkey>
      <video href="2022.findings-emnlp.143.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.143</doi>
    </paper>
    <paper id="144">
      <title>P<tex-math>\text{M}^2\text{F}^2</tex-math><fixed-case>N</fixed-case>: Patient Multi-view Multi-modal Feature Fusion Networks for Clinical Outcome Prediction</title>
      <author><first>Ying</first><last>Zhang</last></author>
      <author><first>Baohang</first><last>Zhou</last></author>
      <author><first>Kehui</first><last>Song</last></author>
      <author><first>Xuhui</first><last>Sui</last></author>
      <author><first>Guoqing</first><last>Zhao</last></author>
      <author><first>Ning</first><last>Jiang</last></author>
      <author><first>Xiaojie</first><last>Yuan</last></author>
      <pages>1985-1994</pages>
      <abstract>Clinical outcome prediction is critical to the condition prediction of patients and management of hospital capacities. There are two kinds of medical data, including time series signals recorded by various devices and clinical notes in electronic health records (EHR), which are used for two common prediction targets: mortality and length of stay. Traditional methods focused on utilizing time series data but ignored clinical notes. With the development of deep learning, natural language processing (NLP) and multi-modal learning methods are exploited to jointly model the time series and clinical notes with different modals. However, the existing methods failed to fuse the multi-modal features of patients from different views. Therefore, we propose the patient multi-view multi-modal feature fusion networks for clinical outcome prediction. Firstly, from patient inner view, we propose to utilize the co-attention module to enhance the fine-grained feature interaction between time series and clinical notes from each patient. Secondly, the patient outer view is the correlation between patients, which can be reflected by the structural knowledge in clinical notes. We exploit the structural information extracted from clinical notes to construct the patient correlation graph, and fuse patients’ multi-modal features by graph neural networks (GNN). The experimental results on MIMIC-III benchmark demonstrate the superiority of our method.</abstract>
      <url hash="76314cac">2022.findings-emnlp.144</url>
      <bibkey>zhang-etal-2022-pm2f2n</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.144</doi>
    </paper>
    <paper id="145">
      <title>Long Text and Multi-Table Summarization: Dataset and Method</title>
      <author><first>Shuaiqi</first><last>Liu</last></author>
      <author><first>Jiannong</first><last>Cao</last></author>
      <author><first>Ruosong</first><last>Yang</last></author>
      <author><first>Zhiyuan</first><last>Wen</last></author>
      <pages>1995-2010</pages>
      <abstract>Automatic document summarization aims to produce a concise summary covering the input document’s salient information. Within a report document, the salient information can be scattered in the textual and non-textual content. However, existing document summarization datasets and methods usually focus on the text and filter out the non-textual content. Missing tabular data can limit produced summaries’ informativeness, especially when summaries require covering quantitative descriptions of critical metrics in tables. Existing datasets and methods cannot meet the requirements of summarizing long text and multiple tables in each report. To deal with the scarcity of available data, we propose FINDSum, the first large-scale dataset for long text and multi-table summarization. Built on 21,125 annual reports from 3,794 companies, it has two subsets for summarizing each company’s results of operations and liquidity. To summarize the long text and dozens of tables in each report, we present three types of summarization methods. Besides, we propose a set of evaluation metrics to assess the usage of numerical information in produced summaries. Dataset analyses and experimental results indicate the importance of jointly considering input textual and tabular data when summarizing report documents.</abstract>
      <url hash="38f8670d">2022.findings-emnlp.145</url>
      <bibkey>liu-etal-2022-long</bibkey>
      <video href="2022.findings-emnlp.145.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.145</doi>
    </paper>
    <paper id="146">
      <title><fixed-case>M</fixed-case>at<fixed-case>R</fixed-case>ank: Text Re-ranking by Latent Preference Matrix</title>
      <author><first>Jinwen</first><last>Luo</last></author>
      <author><first>Jiuding</first><last>Yang</last></author>
      <author><first>Weidong</first><last>Guo</last></author>
      <author><first>Chenglin</first><last>Li</last></author>
      <author><first>Di</first><last>Niu</last></author>
      <author><first>Yu</first><last>Xu</last></author>
      <pages>2011-2023</pages>
      <abstract>Text ranking plays a key role in providing content that best answers user queries. It is usually divided into two sub-tasks to perform efficient information retrieval given a query: text retrieval and text re-ranking. Recent research on pretrained language models (PLM) has demonstrated efficiency and gain on both sub-tasks. However, while existing methods have benefited from pre-trained language models and achieved high recall rates on passage retrieval, the ranking performance still demands further improvement. In this paper, we propose MatRank, which learns to re-rank the text retrieved for a given query by learning to predict the most relevant passage based on a latent preference matrix. Specifically, MatRank uses a PLM to generate an asymmetric latent matrix of relative preference scores between all pairs of retrieved passages. Then, the latent matrix is aggregated row-wise and column-wise to obtain global preferences and predictions of the most relevant passage in two of these directions, respectively. We conduct extensive experiments on MS MACRO, WikiAQ, and SemEval datasets. Experimental results show that MatRank has achieved new state-of-the-art results on these datasets, outperforming all prior methods on ranking performance metrics.</abstract>
      <url hash="63f652e4">2022.findings-emnlp.146</url>
      <bibkey>luo-etal-2022-matrank</bibkey>
      <video href="2022.findings-emnlp.146.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.146</doi>
    </paper>
    <paper id="147">
      <title>Can Language Models Serve as Temporal Knowledge Bases?</title>
      <author><first>Ruilin</first><last>Zhao</last></author>
      <author><first>Feng</first><last>Zhao</last></author>
      <author><first>Guandong</first><last>Xu</last></author>
      <author><first>Sixiao</first><last>Zhang</last></author>
      <author><first>Hai</first><last>Jin</last></author>
      <pages>2024-2037</pages>
      <abstract>Recent progress regarding the use of language models (LMs) as knowledge bases (KBs) has shown that language models can act as structured knowledge bases for storing relational facts. However, most existing works only considered the LM-as-KB paradigm in a static setting, which ignores the analysis of temporal dynamics of world knowledge. Furthermore, a basic function of KBs, i.e., the ability to store conflicting information (i.e., 1-N, N-1, and N-M relations), is underexplored. In this paper, we formulate two practical requirements for treating LMs as temporal KBs: (i) The capacity to store temporally-scoped knowledge that contains conflicting information and (ii) the ability to use stored knowledge for temporally-scoped knowledge queries. We introduce a new dataset called LAMA-TK which is aimed at probing temporally-scoped knowledge, and investigate the two above requirements to explore the LM-as-KB paradigm in the temporal domain. On the one hand, experiments show that LMs can memorize millions of temporally-scoped facts with relatively high accuracy and transfer stored knowledge to temporal knowledge queries, thereby expanding the LM-as-KB paradigm to the temporal domain. On the other hand, we show that memorizing conflicting information, which has been neglected by previous works, is still challenging for LMs and hinders the memorization of other unrelated one-to-one relationships.</abstract>
      <url hash="f42875e5">2022.findings-emnlp.147</url>
      <bibkey>zhao-etal-2022-language</bibkey>
      <video href="2022.findings-emnlp.147.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.147</doi>
    </paper>
    <paper id="148">
      <title>Are Large Pre-Trained Language Models Leaking Your Personal Information?</title>
      <author><first>Jie</first><last>Huang</last></author>
      <author><first>Hanyin</first><last>Shao</last></author>
      <author><first>Kevin Chen-Chuan</first><last>Chang</last></author>
      <pages>2038-2047</pages>
      <abstract>Are Large Pre-Trained Language Models Leaking Your Personal Information? In this paper, we analyze whether Pre-Trained Language Models (PLMs) are prone to leaking personal information. Specifically, we query PLMs for email addresses with contexts of the email address or prompts containing the owner’s name. We find that PLMs do leak personal information due to memorization. However, since the models are weak at association, the risk of specific personal information being extracted by attackers is low. We hope this work could help the community to better understand the privacy risk of PLMs and bring new insights to make PLMs safe.</abstract>
      <url hash="52496d8e">2022.findings-emnlp.148</url>
      <bibkey>huang-etal-2022-large</bibkey>
      <video href="2022.findings-emnlp.148.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.148</doi>
    </paper>
    <paper id="149">
      <title>Self-Distillation with Meta Learning for Knowledge Graph Completion</title>
      <author><first>Yunshui</first><last>Li</last></author>
      <author><first>Junhao</first><last>Liu</last></author>
      <author><first>Min</first><last>Yang</last></author>
      <author><first>Chengming</first><last>Li</last></author>
      <pages>2048-2054</pages>
      <abstract>In this paper, we propose a self-distillation framework with meta learning (MetaSD) for knowledge graph completion with dynamic pruning, which aims to learn compressed graph embeddings and tackle the long-tail samples. Specifically, we first propose a dynamic pruning technique to obtain a small pruned model from a large source model, where the pruning mask of the pruned model could be updated adaptively per epoch after the model weights are updated. The pruned model is supposed to be more sensitive to difficult-to-memorize samples (e.g., long-tail samples) than the source model. Then, we propose a one-step meta self-distillation method for distilling comprehensive knowledge from the source model to the pruned model, where the two models co-evolve in a dynamic manner during training. In particular, we exploit the performance of the pruned model, which is trained alongside the source model in one iteration, to improve the source model’s knowledge transfer ability for the next iteration via meta learning. Extensive experiments show that MetaSD achieves competitive performance compared to strong baselines, while being 10x smaller than baselines.</abstract>
      <url hash="65dd8b6f">2022.findings-emnlp.149</url>
      <bibkey>li-etal-2022-self</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.149</doi>
      <pwccode url="https://github.com/pldlgb/MetaSD" additional="false">pldlgb/MetaSD</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fb15k">FB15k</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fb15k-237">FB15k-237</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wn18">WN18</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wn18rr">WN18RR</pwcdataset>
    </paper>
    <paper id="150">
      <title><fixed-case>CQR</fixed-case>-<fixed-case>SQL</fixed-case>: Conversational Question Reformulation Enhanced Context-Dependent Text-to-<fixed-case>SQL</fixed-case> Parsers</title>
      <author><first>Dongling</first><last>Xiao</last></author>
      <author><first>LinZheng</first><last>Chai</last></author>
      <author><first>Qian-Wen</first><last>Zhang</last></author>
      <author><first>Zhao</first><last>Yan</last></author>
      <author><first>Zhoujun</first><last>Li</last></author>
      <author><first>Yunbo</first><last>Cao</last></author>
      <pages>2055-2068</pages>
      <abstract>Context-dependent text-to-SQL is the task of translating multi-turn questions into database-related SQL queries. Existing methods typically focus on making full use of history context or previously predicted SQL for currently SQL parsing, while neglecting to explicitly comprehend the schema and conversational dependency, such as co-reference, ellipsis and user focus change. In this paper, we propose CQR-SQL, which uses auxiliary Conversational Question Reformulation (CQR) learning to explicitly exploit schema and decouple contextual dependency for multi-turn SQL parsing. Specifically, we first present a schema enhanced recursive CQR method to produce domain-relevant self-contained questions. Secondly, we train CQR-SQL models to map the semantics of multi-turn questions and auxiliary self-contained questions into the same latent space through schema grounding consistency task and tree-structured SQL parsing consistency task, which enhances the abilities of SQL parsing by adequately contextual understanding. At the time of writing, our CQR-SQL achieves new state-of-the-art results on two context-dependent text-to-SQL benchmarks SParC and CoSQL.</abstract>
      <url hash="7fd9b41e">2022.findings-emnlp.150</url>
      <bibkey>xiao-etal-2022-cqr</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.150</doi>
    </paper>
    <paper id="151">
      <title>Assisting the Human Fact-Checkers: Detecting All Previously Fact-Checked Claims in a Document</title>
      <author><first>Shaden</first><last>Shaar</last></author>
      <author><first>Nikola</first><last>Georgiev</last></author>
      <author><first>Firoj</first><last>Alam</last></author>
      <author><first>Giovanni</first><last>Da San Martino</last></author>
      <author><first>Aisha</first><last>Mohamed</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <pages>2069-2080</pages>
      <abstract>Given the recent proliferation of false claims online, there has been a lot of manual fact-checking effort. As this is very time-consuming, human fact-checkers can benefit from tools that can support them and make them more efficient. Here, we focus on building a system that could provide such support. Given an input document, it aims to detect all sentences that contain a claim that can be verified by some previously fact-checked claims (from a given database). The output is a re-ranked list of the document sentences, so that those that can be verified are ranked as high as possible, together with corresponding evidence. Unlike previous work, which has looked into claim retrieval, here we take a document-level perspective. We create a new manually annotated dataset for the task, and we propose suitable evaluation measures. We further experiment with a learning-to-rank approach, achieving sizable performance gains over several strong baselines. Our analysis demonstrates the importance of modeling text similarity and stance, while also taking into account the veracity of the retrieved previously fact-checked claims. We believe that this research would be of interest to fact-checkers, journalists, media, and regulatory authorities.</abstract>
      <url hash="2ebf0f2f">2022.findings-emnlp.151</url>
      <bibkey>shaar-etal-2022-assisting</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.151</doi>
    </paper>
    <paper id="152">
      <title>No Word Embedding Model Is Perfect: Evaluating the Representation Accuracy for Social Bias in the Media</title>
      <author><first>Maximilian</first><last>Spliethöver</last></author>
      <author><first>Maximilian</first><last>Keiff</last></author>
      <author><first>Henning</first><last>Wachsmuth</last></author>
      <pages>2081-2093</pages>
      <abstract>News articles both shape and reflect public opinion across the political spectrum. Analyzing them for social bias can thus provide valuable insights, such as prevailing stereotypes in society and the media, which are often adopted by NLP models trained on respective data. Recent work has relied on word embedding bias measures, such as WEAT. However, several representation issues of embeddings can harm the measures’ accuracy, including low-resource settings and token frequency differences. In this work, we study what kind of embedding algorithm serves best to accurately measure types of social bias known to exist in US online news articles. To cover the whole spectrum of political bias in the US, we collect 500k articles and review psychology literature with respect to expected social bias. We then quantify social bias using WEAT along with embedding algorithms that account for the aforementioned issues. We compare how models trained with the algorithms on news articles represent the expected social bias. Our results suggest that the standard way to quantify bias does not align well with knowledge from psychology. While the proposed algorithms reduce the gap, they still do not fully match the literature.</abstract>
      <url hash="fdf3a602">2022.findings-emnlp.152</url>
      <bibkey>spliethover-etal-2022-word</bibkey>
      <video href="2022.findings-emnlp.152.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.152</doi>
    </paper>
    <paper id="153">
      <title>Scientific and Creative Analogies in Pretrained Language Models</title>
      <author><first>Tamara</first><last>Czinczoll</last></author>
      <author><first>Helen</first><last>Yannakoudakis</last></author>
      <author><first>Pushkar</first><last>Mishra</last></author>
      <author><first>Ekaterina</first><last>Shutova</last></author>
      <pages>2094-2100</pages>
      <abstract>This paper examines the encoding of analogy in large-scale pretrained language models, such as BERT and GPT-2. Existing analogy datasets typically focus on a limited set of analogical relations, with a high similarity of the two domains between which the analogy holds. As a more realistic setup, we introduce the Scientific and Creative Analogy dataset (SCAN), a novel analogy dataset containing systematic mappings of multiple attributes and relational structures across dissimilar domains. Using this dataset, we test the analogical reasoning capabilities of several widely-used pretrained language models (LMs). We find that state-of-the-art LMs achieve low performance on these complex analogy tasks, highlighting the challenges still posed by analogy understanding.</abstract>
      <url hash="dad23d99">2022.findings-emnlp.153</url>
      <bibkey>czinczoll-etal-2022-scientific</bibkey>
      <video href="2022.findings-emnlp.153.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.153</doi>
    </paper>
    <paper id="154">
      <title>Bitext Mining Using Distilled Sentence Representations for Low-Resource Languages</title>
      <author><first>Kevin</first><last>Heffernan</last></author>
      <author><first>Onur</first><last>Çelebi</last></author>
      <author><first>Holger</first><last>Schwenk</last></author>
      <pages>2101-2112</pages>
      <abstract>Scaling multilingual representation learning beyond the hundred most frequent languages is challenging, in particular to cover the long tail of low-resource languages. We move away from the popular one-for-all multilingual models and focus on training multiple language (family) specific representations, but most prominently enable all languages to still be encoded in the same representational space. We focus on teacher-student training, allowing all encoders to be mutually compatible for bitext mining, and enabling fast learning of new languages. We also combine supervised and self-supervised training, allowing encoders to take advantage of monolingual training data. Our approach significantly outperforms the original LASER encoder. We study very low-resource languages and handle 44 African languages, many of which are not covered by any other model. For these languages, we train sentence encoders and mine bitexts. Adding these mined bitexts yielded an improvement of 3.8 BLEU for NMT into English.</abstract>
      <url hash="a64d1d05">2022.findings-emnlp.154</url>
      <bibkey>heffernan-etal-2022-bitext</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.154</doi>
    </paper>
    <paper id="155">
      <title>Towards Generalizable and Robust Text-to-<fixed-case>SQL</fixed-case> Parsing</title>
      <author><first>Chang</first><last>Gao</last></author>
      <author><first>Bowen</first><last>Li</last></author>
      <author><first>Wenxuan</first><last>Zhang</last></author>
      <author><first>Wai</first><last>Lam</last></author>
      <author><first>Binhua</first><last>Li</last></author>
      <author><first>Fei</first><last>Huang</last></author>
      <author><first>Luo</first><last>Si</last></author>
      <author><first>Yongbin</first><last>Li</last></author>
      <pages>2113-2125</pages>
      <abstract>Text-to-SQL parsing tackles the problem of mapping natural language questions to executable SQL queries. In practice, text-to-SQL parsers often encounter various challenging scenarios, requiring them to be generalizable and robust. While most existing work addresses a particular generalization or robustness challenge, we aim to study it in a more comprehensive manner. In specific, we believe that text-to-SQL parsers should be (1) generalizable at three levels of generalization, namely i.i.d., zero-shot, and compositional, and (2) robust against input perturbations. To enhance these capabilities of the parser, we propose a novel TKK framework consisting of Task decomposition, Knowledge acquisition, and Knowledge composition to learn text-to-SQL parsing in stages. By dividing the learning process into multiple stages, our framework improves the parser’s ability to acquire general SQL knowledge instead of capturing spurious patterns, making it more generalizable and robust. Experimental results under various generalization and robustness settings show that our framework is effective in all scenarios and achieves state-of-the-art performance on the Spider, SParC, and CoSQL datasets.</abstract>
      <url hash="6d72e32a">2022.findings-emnlp.155</url>
      <bibkey>gao-etal-2022-towards-generalizable</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.155</doi>
    </paper>
    <paper id="156">
      <title><fixed-case>E</fixed-case>di<fixed-case>T</fixed-case>5: Semi-Autoregressive Text Editing with T5 Warm-Start</title>
      <author><first>Jonathan</first><last>Mallinson</last></author>
      <author><first>Jakub</first><last>Adamek</last></author>
      <author><first>Eric</first><last>Malmi</last></author>
      <author><first>Aliaksei</first><last>Severyn</last></author>
      <pages>2126-2138</pages>
      <abstract>We present EdiT5 - a novel semi-autoregressive text-editing approach designed to combine the strengths of non-autoregressive text-editing and autoregressive decoding. EdiT5 is faster at inference times than conventional sequence-to-sequence (seq2seq) models, while being capable of modeling flexible input-output transformations. This is achieved by decomposing the generation process into three sub-tasks: (1) tagging to decide on the subset of input tokens to be preserved in the output, (2) re-ordering to define their order in the output text, and (3) insertion to infill the missing tokens that are not present in the input. The tagging and re-ordering steps, which are responsible for generating the largest portion of the output, are non-autoregressive, while the insertion uses an autoregressive decoder. Depending on the task, EdiT5 requires significantly fewer autoregressive steps demonstrating speedups of up to 25x when compared to classic seq2seq models. Quality-wise, EdiT5 is initialized with a pre-trained T5 checkpoint yielding comparable performance to T5 in high-resource settings and clearly outperforms it on low-resource settings when evaluated on three NLG tasks: Sentence Fusion, Grammatical Error Correction, and Decontextualization.</abstract>
      <url hash="654c2cb8">2022.findings-emnlp.156</url>
      <bibkey>mallinson-etal-2022-edit5</bibkey>
      <video href="2022.findings-emnlp.156.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.156</doi>
    </paper>
    <paper id="157">
      <title>A Critical Reflection and Forward Perspective on Empathy and Natural Language Processing</title>
      <author><first>Allison</first><last>Lahnala</last></author>
      <author><first>Charles</first><last>Welch</last></author>
      <author><first>David</first><last>Jurgens</last></author>
      <author><first>Lucie</first><last>Flek</last></author>
      <pages>2139-2158</pages>
      <abstract>We review the state of research on empathy in natural language processing and identify the following issues: (1) empathy definitions are absent or abstract, which (2) leads to low construct validity and reproducibility. Moreover, (3) emotional empathy is overemphasized, skewing our focus to a narrow subset of simplified tasks. We believe these issues hinder research progress and argue that current directions will benefit from a clear conceptualization that includes operationalizing cognitive empathy components. Our main objectives are to provide insight and guidance on empathy conceptualization for NLP research objectives and to encourage researchers to pursue the overlooked opportunities in this area, highly relevant, e.g., for clinical and educational sectors.</abstract>
      <url hash="afae992c">2022.findings-emnlp.157</url>
      <bibkey>lahnala-etal-2022-critical</bibkey>
      <video href="2022.findings-emnlp.157.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.157</doi>
    </paper>
    <paper id="158">
      <title>A Neural-Symbolic Approach to Natural Language Understanding</title>
      <author><first>Zhixuan</first><last>Liu</last></author>
      <author><first>Zihao</first><last>Wang</last></author>
      <author><first>Yuan</first><last>Lin</last></author>
      <author><first>Hang</first><last>Li</last></author>
      <pages>2159-2172</pages>
      <abstract>Deep neural networks, empowered by pre-trained language models, have achieved remarkable results in natural language understanding (NLU) tasks. However, their performances can drastically deteriorate when logical reasoning is needed. This is because NLU in principle depends on not only analogical reasoning, which deep neural networks are good at, but also logical reasoning. According to the dual-process theory, analogical reasoning and logical reasoning are respectively carried out by System 1 and System 2 in the human brain. Inspired by the theory, we present a novel framework for NLU called Neural-Symbolic Processor (NSP), which performs analogical reasoning based on neural processing and logical reasoning based on both neural and symbolic processing. As a case study, we conduct experiments on two NLU tasks, question answering (QA) and natural language inference (NLI), when numerical reasoning (a type of logical reasoning) is necessary. The experimental results show that our method significantly outperforms state-of-the-art methods in both tasks.</abstract>
      <url hash="c7b534d9">2022.findings-emnlp.158</url>
      <bibkey>liu-etal-2022-neural</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.158</doi>
    </paper>
    <paper id="159">
      <title>Social-aware Sparse Attention Network for Session-based Social Recommendation</title>
      <author><first>Kai</first><last>Ouyang</last></author>
      <author><first>Xianghong</first><last>Xu</last></author>
      <author><first>Chen</first><last>Tang</last></author>
      <author><first>Wang</first><last>Chen</last></author>
      <author><first>Haitao</first><last>Zheng</last></author>
      <pages>2173-2183</pages>
      <abstract>Session-based Social Recommendation (SSR) aims to use users’ social networks and historical sessions to provide more personalized recommendations for the current session. Unfortunately, existing SSR methods have two limitations. First, they do not screen users’ useless social relationships and noisy irrelevant interactions. However, user preferences are mainly affected by several close friends and key interactions. Second, when modeling the current session, they do not take full advantage of user preference information. To tackle these issues, we propose a novel Social-aware Sparse Attention Network for SSR, abbreviated as SSAN.It mainly consists of the Heterogeneous Graph Embedding (HGE) module and the Social-aware Encoder-decoder Network (SEN) module. In the HGE module, we adopt a modified heterogeneous graph neural network, which focuses more on close friends and key historical interactions, to enhance user/item representations. In the SEN module, we use the user representation as a bridge between the Encoder and Decoder to incorporate user preferences when modeling the current session. Extensive experiments on two benchmark datasets demonstrate the superiority of SSAN over the state-of-the-art models.</abstract>
      <url hash="6db14583">2022.findings-emnlp.159</url>
      <bibkey>ouyang-etal-2022-social</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.159</doi>
    </paper>
    <paper id="160">
      <title><fixed-case>S</fixed-case>parse<fixed-case>A</fixed-case>dapter: An Easy Approach for Improving the Parameter-Efficiency of Adapters</title>
      <author><first>Shwai</first><last>He</last></author>
      <author><first>Liang</first><last>Ding</last></author>
      <author><first>Daize</first><last>Dong</last></author>
      <author><first>Jeremy</first><last>Zhang</last></author>
      <author><first>Dacheng</first><last>Tao</last></author>
      <pages>2184-2190</pages>
      <abstract>Adapter Tuning, which freezes the pretrained language models (PLMs) and only fine-tunes a few extra modules, becomes an appealing efficient alternative to the full model fine-tuning. Although computationally efficient, the recent Adapters often increase parameters (e.g. bottleneck dimension) for matching the performance of full model fine-tuning, which we argue goes against their original intention. In this work, we re-examine the parameter-efficiency of Adapter through the lens of network pruning (we name such plug-in concept as SparseAdapter) and find that SparseAdapter can achieve comparable or better performance than standard Adapters when the sparse ratio reaches up to 80%. Based on our findings, we introduce an easy but effective setting “Large-Sparse” to improve the model capacity of Adapters under the same parameter budget. Experiments on five competitive Adapters upon three advanced PLMs show that with proper sparse method (e.g. SNIP) and ratio (e.g. 40%) SparseAdapter can consistently outperform their corresponding counterpart. Encouragingly, with the Large-Sparse setting, we can obtain further appealing gains, even outperforming the full fine-tuning by a large margin.</abstract>
      <url hash="120df0c0">2022.findings-emnlp.160</url>
      <bibkey>he-etal-2022-sparseadapter</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.160</doi>
    </paper>
    <paper id="161">
      <title>Measurement Extraction with Natural Language Processing: A Review</title>
      <author><first>Jan</first><last>Göpfert</last></author>
      <author><first>Patrick</first><last>Kuckertz</last></author>
      <author><first>Jann</first><last>Weinand</last></author>
      <author><first>Leander</first><last>Kotzur</last></author>
      <author><first>Detlef</first><last>Stolten</last></author>
      <pages>2191-2215</pages>
      <abstract>Quantitative data is important in many domains. Information extraction methods draw structured data from documents. However, the extraction of quantities and their contexts has received little attention in the history of information extraction. In this review, an overview of prior work on measurement extraction is presented. We describe different approaches to measurement extraction and outline the challenges posed by this task. The review concludes with an outline of potential future research. Research strains in measurement extraction tend to be isolated and lack a common terminology. Improvements in numerical reasoning, more extensive datasets, and the consideration of wider contexts may lead to significant improvements in measurement extraction.</abstract>
      <url hash="e1ff0733">2022.findings-emnlp.161</url>
      <bibkey>gopfert-etal-2022-measurement</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.161</doi>
    </paper>
    <paper id="162">
      <title>Summarizing Procedural Text: Data and Approach</title>
      <author><first>Shen</first><last>Gao</last></author>
      <author><first>Haotong</first><last>Zhang</last></author>
      <author><first>Xiuying</first><last>Chen</last></author>
      <author><first>Rui</first><last>Yan</last></author>
      <author><first>Dongyan</first><last>Zhao</last></author>
      <pages>2216-2225</pages>
      <abstract>Procedural text is a widely used genre that contains many steps of instructions of how to cook a dish or how to conduct a chemical experiment and analyze the procedural text has become a popular task in the NLP field. Since the procedural text can be very long and contains many details, summarizing the whole procedural text or giving an overview for each complicated procedure step can save time for readers and help them to capture the core information in the text. In this paper, we propose the procedural text summarization task with two summarization granularity: step-view and global-view, which summarizes each step in the procedural text separately or gives an overall summary for all steps respectively. To tackle this task, we propose an Entity-State Graph-based Summarizer (ESGS) which is based on state-of-the-art entity state tracking methods and constructs a heterogeneous graph to aggregate contextual information for each procedure. In order to help the summarization model focus on the salient entities, we propose to use the contextualized procedure graph representation to predict the salient entities. Experiments conducted on two datasets verify the effectiveness of our proposed model. Our code and datasets will be released on <url>https://github.com/gsh199449/procedural-summ</url>.</abstract>
      <url hash="d0c1f7b5">2022.findings-emnlp.162</url>
      <bibkey>gao-etal-2022-summarizing-procedural</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.162</doi>
    </paper>
    <paper id="163">
      <title>Snapshot-Guided Domain Adaptation for <fixed-case>ELECTRA</fixed-case></title>
      <author><first>Daixuan</first><last>Cheng</last></author>
      <author><first>Shaohan</first><last>Huang</last></author>
      <author><first>Jianfeng</first><last>Liu</last></author>
      <author><first>Yuefeng</first><last>Zhan</last></author>
      <author><first>Hao</first><last>Sun</last></author>
      <author><first>Furu</first><last>Wei</last></author>
      <author><first>Denvy</first><last>Deng</last></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <pages>2226-2232</pages>
      <abstract>Discriminative pre-trained language models, such as ELECTRA, have achieved promising performances in a variety of general tasks. However, these generic pre-trained models struggle to capture domain-specific knowledge of domain-related tasks. In this work, we propose a novel domain-adaptation method for ELECTRA, which can dynamically select domain-specific tokens and guide the discriminator to emphasize them, without introducing new training parameters. We show that by re-weighting the losses of domain-specific tokens, ELECTRA can be effectively adapted to different domains. The experimental results in both computer science and biomedical domains show that the proposed method can achieve state-of-the-art results on the domain-related tasks.</abstract>
      <url hash="9413539c">2022.findings-emnlp.163</url>
      <bibkey>cheng-etal-2022-snapshot</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.163</doi>
    </paper>
    <paper id="164">
      <title>Exploiting Labeled and Unlabeled Data via Transformer Fine-tuning for Peer-Review Score Prediction</title>
      <author><first>Panitan</first><last>Muangkammuen</last></author>
      <author><first>Fumiyo</first><last>Fukumoto</last></author>
      <author><first>Jiyi</first><last>Li</last></author>
      <author><first>Yoshimi</first><last>Suzuki</last></author>
      <pages>2233-2240</pages>
      <abstract>Automatic Peer-review Aspect Score Prediction (PASP) of academic papers can be a helpful assistant tool for both reviewers and authors. Most existing works on PASP utilize supervised learning techniques. However, the limited number of peer-review data deteriorates the performance of PASP. This paper presents a novel semi-supervised learning (SSL) method that incorporates the Transformer fine-tuning into the Γ-model, a variant of the Ladder network, to leverage contextual features from unlabeled data. Backpropagation simultaneously minimizes the sum of supervised and unsupervised cost functions, avoiding the need for layer-wise pre-training. The experimental results show that our model outperforms the supervised and naive semi-supervised learning baselines. Our source codes are available online.</abstract>
      <url hash="07a26bb0">2022.findings-emnlp.164</url>
      <bibkey>muangkammuen-etal-2022-exploiting</bibkey>
      <video href="2022.findings-emnlp.164.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.164</doi>
    </paper>
    <paper id="165">
      <title><fixed-case>HARALD</fixed-case>: Augmenting Hate Speech Data Sets with Real Data</title>
      <author><first>Tal</first><last>Ilan</last></author>
      <author><first>Dan</first><last>Vilenchik</last></author>
      <pages>2241-2248</pages>
      <abstract>The successful completion of the hate speech detection task hinges upon the availability of rich and variable labeled data, which is hard to obtain. In this work, we present a new approach for data augmentation that uses as input real unlabelled data, which is carefully selected from online platforms where invited hate speech is abundant. We show that by harvesting and processing this data (in an automatic manner), one can augment existing manually-labeled datasets to improve the classification performance of hate speech classification models. We observed an improvement in F1-score ranging from 2.7% and up to 9.5%, depending on the task (in- or cross-domain) and the model used.</abstract>
      <url hash="cb18f516">2022.findings-emnlp.165</url>
      <bibkey>ilan-vilenchik-2022-harald</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.165</doi>
    </paper>
    <paper id="166">
      <title>Wait-info Policy: Balancing Source and Target at Information Level for Simultaneous Machine Translation</title>
      <author><first>Shaolei</first><last>Zhang</last></author>
      <author><first>Shoutao</first><last>Guo</last></author>
      <author><first>Yang</first><last>Feng</last></author>
      <pages>2249-2263</pages>
      <abstract>Simultaneous machine translation (SiMT) outputs the translation while receiving the source inputs, and hence needs to balance the received source information and translated target information to make a reasonable decision between waiting for inputs or outputting translation. Previous methods always balance source and target information at the token level, either directly waiting for a fixed number of tokens or adjusting the waiting based on the current token. In this paper, we propose a Wait-info Policy to balance source and target at the information level. We first quantify the amount of information contained in each token, named info. Then during simultaneous translation, the decision of waiting or outputting is made based on the comparison results between the total info of previous target outputs and received source inputs. Experiments show that our method outperforms strong baselines under and achieves better balance via the proposed info.</abstract>
      <url hash="71517e2b">2022.findings-emnlp.166</url>
      <bibkey>zhang-etal-2022-wait</bibkey>
      <video href="2022.findings-emnlp.166.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.166</doi>
    </paper>
    <paper id="167">
      <title>Turning Fixed to Adaptive: Integrating Post-Evaluation into Simultaneous Machine Translation</title>
      <author><first>Shoutao</first><last>Guo</last></author>
      <author><first>Shaolei</first><last>Zhang</last></author>
      <author><first>Yang</first><last>Feng</last></author>
      <pages>2264-2278</pages>
      <abstract>Simultaneous machine translation (SiMT) starts its translation before reading the whole source sentence and employs either fixed or adaptive policy to generate the target sentence. Compared to the fixed policy, the adaptive policy achieves better latency-quality tradeoffs by adopting a flexible translation policy. If the policy can evaluate rationality before taking action, the probability of incorrect actions will also decrease. However, previous methods lack evaluation of actions before taking them. In this paper, we propose a method of performing the adaptive policy via integrating post-evaluation into the fixed policy. Specifically, whenever a candidate token is generated, our model will evaluate the rationality of the next action by measuring the change in the source content. Our model will then take different actions based on the evaluation results. Experiments on three translation tasks show that our method can exceed strong baselines under all latency.</abstract>
      <url hash="dce73fd1">2022.findings-emnlp.167</url>
      <bibkey>guo-etal-2022-turning</bibkey>
      <video href="2022.findings-emnlp.167.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.167</doi>
    </paper>
    <paper id="168">
      <title>Alleviating Sparsity of Open Knowledge Graphs with Ternary Contrastive Learning</title>
      <author><first>Qian</first><last>Li</last></author>
      <author><first>Shafiq</first><last>Joty</last></author>
      <author><first>Daling</first><last>Wang</last></author>
      <author><first>Shi</first><last>Feng</last></author>
      <author><first>Yifei</first><last>Zhang</last></author>
      <pages>2279-2291</pages>
      <abstract>Sparsity of formal knowledge and roughness of non-ontological construction make sparsity problem particularly prominent in Open Knowledge Graphs (OpenKGs). Due to sparse links, learning effective representation for few-shot entities becomes difficult. We hypothesize that by introducing negative samples, a contrastive learning (CL) formulation could be beneficial in such scenarios. However, existing CL methods model KG triplets as binary objects of entities ignoring the relation-guided ternary propagation patterns and they are too generic, i.e., they ignore zero-shot, few-shot and synonymity problems that appear in OpenKGs. To address this, we propose TernaryCL, a CL framework based on ternary propagation patterns among head, relation and tail. TernaryCL designs Contrastive Entity and Contrastive Relation to mine ternary discriminative features with both negative entities and relations, introduces Contrastive Self to help zero- and few-shot entities learn discriminative features, Contrastive Synonym to model synonymous entities, and Contrastive Fusion to aggregate graph features from multiple paths. Extensive experiments on benchmarks demonstrate the superiority of TernaryCL over state-of-the-art models.</abstract>
      <url hash="a55bfca5">2022.findings-emnlp.168</url>
      <bibkey>li-etal-2022-alleviating</bibkey>
      <video href="2022.findings-emnlp.168.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.168</doi>
    </paper>
    <paper id="169">
      <title>Using Developer Discussions to Guide Fixing Bugs in Software</title>
      <author><first>Sheena</first><last>Panthaplackel</last></author>
      <author><first>Milos</first><last>Gligoric</last></author>
      <author><first>Junyi Jessy</first><last>Li</last></author>
      <author><first>Raymond</first><last>Mooney</last></author>
      <pages>2292-2301</pages>
      <abstract>Automatically fixing software bugs is a challenging task. While recent work showed that natural language context is useful in guiding bug-fixing models, the approach required prompting developers to provide this context, which was simulated through commit messages written after the bug-fixing code changes were made. We instead propose using bug report discussions, which are available before the task is performed and are also naturally occurring, avoiding the need for any additional information from developers. For this, we augment standard bug-fixing datasets with bug report discussions. Using these newly compiled datasets, we demonstrate that various forms of natural language context derived from such discussions can aid bug-fixing, even leading to improved performance over using commit messages corresponding to the oracle bug-fixing commits.</abstract>
      <url hash="e82cdf9f">2022.findings-emnlp.169</url>
      <bibkey>panthaplackel-etal-2022-using</bibkey>
      <video href="2022.findings-emnlp.169.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.169</doi>
    </paper>
    <paper id="170">
      <title><fixed-case>A</fixed-case>uto<fixed-case>CAD</fixed-case>: Automatically Generate Counterfactuals for Mitigating Shortcut Learning</title>
      <author><first>Jiaxin</first><last>Wen</last></author>
      <author><first>Yeshuang</first><last>Zhu</last></author>
      <author><first>Jinchao</first><last>Zhang</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <pages>2302-2317</pages>
      <abstract>Recent studies have shown the impressive efficacy of counterfactually augmented data (CAD) for reducing NLU models’ reliance on spurious features and improving their generalizability. However, current methods still heavily rely on human efforts or task-specific designs to generate counterfactuals, thereby impeding CAD’s applicability to a broad range of NLU tasks. In this paper, we present AutoCAD, a fully automatic and task-agnostic CAD generation framework. AutoCAD first leverages a classifier to unsupervisedly identify rationales as spans to be intervened, which disentangles spurious and causal features. Then, AutoCAD performs controllable generation enhanced by unlikelihood training to produce diverse counterfactuals. Extensive evaluations on multiple out-of-domain and challenge benchmarks demonstrate that AutoCAD consistently and significantly boosts the out-of-distribution performance of powerful pre-trained models across different NLU tasks, which is comparable or even better than previous state-of-the-art human-in-the-loop or task-specific CAD methods.</abstract>
      <url hash="c7f13cfb">2022.findings-emnlp.170</url>
      <bibkey>wen-etal-2022-autocad</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.170</doi>
    </paper>
    <paper id="171">
      <title>A Multi-Modal Knowledge Graph for Classical <fixed-case>C</fixed-case>hinese Poetry</title>
      <author><first>Yuqing</first><last>Li</last></author>
      <author><first>Yuxin</first><last>Zhang</last></author>
      <author><first>Bin</first><last>Wu</last></author>
      <author><first>Ji-Rong</first><last>Wen</last></author>
      <author><first>Ruihua</first><last>Song</last></author>
      <author><first>Ting</first><last>Bai</last></author>
      <pages>2318-2326</pages>
      <abstract>Classical Chinese poetry has a long history and is a precious cultural heritage of humankind. Displaying the classical Chinese poetry in a visual way, helps to cross cultural barriers in different countries, making it enjoyable for all the people. In this paper, we construct a multi-modal knowledge graph for classical Chinese poetry (PKG), in which the visual information of words in the poetry are incorporated. Then a multi-modal pre-training language model, PKG-Bert, is proposed to obtain the poetry representation with visual information, which bridges the semantic gap between different modalities. PKG-Bert achieves the state-of-the-art performance on the poetry-image retrieval task, showing the effectiveness of incorporating the multi-modal knowledge. The large-scale multi-modal knowledge graph of classical Chinese poetry will be released to promote the researches in classical Chinese culture area.</abstract>
      <url hash="ab70053e">2022.findings-emnlp.171</url>
      <bibkey>li-etal-2022-multi-modal</bibkey>
      <video href="2022.findings-emnlp.171.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.171</doi>
    </paper>
    <paper id="172">
      <title>Assessing Non-autoregressive Alignment in Neural Machine Translation via Word Reordering</title>
      <author><first>Chun-Hin</first><last>Tse</last></author>
      <author><first>Ester</first><last>Leung</last></author>
      <author><first>William K.</first><last>Cheung</last></author>
      <pages>2327-2333</pages>
      <abstract>Recent work on non-autoregressive neural machine translation (NAT) that leverages alignment information to explicitly reduce the modality of target distribution has reported comparable performance with counterparts that tackle multi-modality problem by implicitly modeling dependencies. Effectiveness in handling alignment is vital for models that follow this approach, where a token reordering mechanism is typically involved and plays a vital role. We review the reordering capability of the respective mechanisms in recent NAT models, and our experimental results show that their performance is sub-optimal. We propose to learn a non-autoregressive language model (NALM) based on transformer which can be combined with Viterbi decoding to achieve better reordering performance. We evaluate the proposed NALM using the PTB dataset where sentences with words permuted in different ways are expected to have their ordering recovered. Our empirical results show that the proposed method can outperform the state-of-the-art reordering mechanisms under different word permutation settings, with a 2-27 BLEU improvement, suggesting high potential for word alignment in NAT.</abstract>
      <url hash="2111a7b9">2022.findings-emnlp.172</url>
      <bibkey>tse-etal-2022-assessing</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.172</doi>
    </paper>
    <paper id="173">
      <title>Syntax-guided Localized Self-attention by Constituency Syntactic Distance</title>
      <author><first>Shengyuan</first><last>Hou</last></author>
      <author><first>Jushi</first><last>Kai</last></author>
      <author><first>Haotian</first><last>Xue</last></author>
      <author><first>Bingyu</first><last>Zhu</last></author>
      <author><first>Bo</first><last>Yuan</last></author>
      <author><first>Longtao</first><last>Huang</last></author>
      <author><first>Xinbing</first><last>Wang</last></author>
      <author><first>Zhouhan</first><last>Lin</last></author>
      <pages>2334-2341</pages>
      <abstract>Recent works have revealed that Transformers are implicitly learning the syntactic information in its lower layers from data, albeit is highly dependent on the quality and scale of the training data. However, learning syntactic information from data is not necessary if we can leverage an external syntactic parser, which provides better parsing quality with well-defined syntactic structures. This could potentially improve Transformer’s performance and sample efficiency. In this work, we propose a syntax-guided localized self-attention for Transformer that allows directly incorporating grammar structures from an external constituency parser. It prohibits the attention mechanism to overweight the grammatically distant tokens over close ones. Experimental results show that our model could consistently improve translation performance on a variety of machine translation datasets, ranging from small to large dataset sizes, and with different source languages.</abstract>
      <url hash="25d71ce1">2022.findings-emnlp.173</url>
      <bibkey>hou-etal-2022-syntax</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.173</doi>
    </paper>
    <paper id="174">
      <title><fixed-case>C</fixed-case>ode<fixed-case>E</fixed-case>xp: Explanatory Code Document Generation</title>
      <author><first>Haotian</first><last>Cui</last></author>
      <author><first>Chenglong</first><last>Wang</last></author>
      <author><first>Junjie</first><last>Huang</last></author>
      <author><first>Jeevana Priya</first><last>Inala</last></author>
      <author><first>Todd</first><last>Mytkowicz</last></author>
      <author><first>Bo</first><last>Wang</last></author>
      <author><first>Jianfeng</first><last>Gao</last></author>
      <author><first>Nan</first><last>Duan</last></author>
      <pages>2342-2354</pages>
      <abstract>Developing models that can automatically generate detailed code explanation can greatly benefit software maintenance and programming education. However, existing code-to-text generation models often produce only high-level summaries of code that do not capture implementation-level choices essential for these scenarios. To fill in this gap, we propose the code explanation generation task. We first conducted a human study to identify the criteria for high-quality explanatory docstring for code. Based on that, we collected and refined a large-scale code docstring corpus and formulated automatic evaluation metrics that best match human assessments. Finally, we present a multi-stage fine-tuning strategy and baseline models for the task. Our experiments show that (1) our refined training dataset lets models achieve better performance in the explanation generation tasks compared to larger-scale unrefined data (15x larger), and (2) fine-tuned models can generate well-structured long docstrings comparable to human-written ones. We envision our training dataset, human-evaluation protocol, recommended metrics, and fine-tuning strategy can boost future code explanation research. The code and annotated data are available at <url>https://github.com/subercui/CodeExp</url>.</abstract>
      <url hash="dd18d299">2022.findings-emnlp.174</url>
      <bibkey>cui-etal-2022-codeexp</bibkey>
      <video href="2022.findings-emnlp.174.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.174</doi>
    </paper>
    <paper id="175">
      <title><fixed-case>PAUQ</fixed-case>: Text-to-<fixed-case>SQL</fixed-case> in <fixed-case>R</fixed-case>ussian</title>
      <author><first>Daria</first><last>Bakshandaeva</last></author>
      <author><first>Oleg</first><last>Somov</last></author>
      <author><first>Ekaterina</first><last>Dmitrieva</last></author>
      <author><first>Vera</first><last>Davydova</last></author>
      <author><first>Elena</first><last>Tutubalina</last></author>
      <pages>2355-2376</pages>
      <abstract>Semantic parsing is an important task that allows to democratize human-computer interaction. One of the most popular text-to-SQL datasets with complex and diverse natural language (NL) questions and SQL queries is Spider. We construct and complement a Spider dataset for Russian, thus creating the first publicly available text-to-SQL dataset for this language. While examining its components - NL questions, SQL queries and databases content - we identify limitations of the existing database structure, fill out missing values for tables and add new requests for underrepresented categories. We select thirty functional test sets with different features that can be used for the evaluation of neural models’ abilities. To conduct the experiments, we adapt baseline architectures RAT-SQL and BRIDGE and provide in-depth query component analysis. On the target language, both models demonstrate strong results with monolingual training and improved accuracy in multilingual scenario. In this paper, we also study trade-offs between machine-translated and manually-created NL queries. At present, Russian text-to-SQL is lacking in datasets as well as trained models, and we view this work as an important step towards filling this gap.</abstract>
      <url hash="1d89e5c6">2022.findings-emnlp.175</url>
      <bibkey>bakshandaeva-etal-2022-pauq</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.175</doi>
    </paper>
    <paper id="176">
      <title>Event-Centric Question Answering via Contrastive Learning and Invertible Event Transformation</title>
      <author><first>Junru</first><last>Lu</last></author>
      <author><first>Xingwei</first><last>Tan</last></author>
      <author><first>Gabriele</first><last>Pergola</last></author>
      <author><first>Lin</first><last>Gui</last></author>
      <author><first>Yulan</first><last>He</last></author>
      <pages>2377-2389</pages>
      <abstract>Human reading comprehension often requires reasoning of event semantic relations in narratives, represented by Event-centric Question-Answering (QA). To address event-centric QA, we propose a novel QA model with contrastive learning and invertible event transformation, call TranCLR. Our proposed model utilizes an invertible transformation matrix to project semantic vectors of events into a common event embedding space, trained with contrastive learning, and thus naturally inject event semantic knowledge into mainstream QA pipelines. The transformation matrix is fine-tuned with the annotated event relation types between events that occurred in questions and those in answers, using event-aware question vectors. Experimental results on the Event Semantic Relation Reasoning (ESTER) dataset show significant improvements in both generative and extractive settings compared to the existing strong baselines, achieving over 8.4% gain in the token-level F1 score and 3.0% gain in Exact Match (EM) score under the multi-answer setting. Qualitative analysis reveals the high quality of the generated answers by TranCLR, demonstrating the feasibility of injecting event knowledge into QA model learning. Our code and models can be found at <url>https://github.com/LuJunru/TranCLR</url>.</abstract>
      <url hash="aa4230ab">2022.findings-emnlp.176</url>
      <bibkey>lu-etal-2022-event</bibkey>
      <video href="2022.findings-emnlp.176.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.176</doi>
    </paper>
    <paper id="177">
      <title>Label-Driven Denoising Framework for Multi-Label Few-Shot Aspect Category Detection</title>
      <author><first>Fei</first><last>Zhao</last></author>
      <author><first>Yuchen</first><last>Shen</last></author>
      <author><first>Zhen</first><last>Wu</last></author>
      <author><first>Xinyu</first><last>Dai</last></author>
      <pages>2390-2402</pages>
      <abstract>Multi-Label Few-Shot Aspect Category Detection (FS-ACD) is a new sub-task of aspect-based sentiment analysis, which aims to detect aspect categories accurately with limited training instances. Recently, dominant works use the prototypical network to accomplish this task, and employ the attention mechanism to extract keywords of aspect category from the sentences to produce the prototype for each aspect. However, they still suffer from serious noise problems: (1) due to lack of sufficient supervised data, the previous methods easily catch noisy words irrelevant to the current aspect category, which largely affects the quality of the generated prototype; (2) the semantically-close aspect categories usually generate similar prototypes, which are mutually noisy and confuse the classifier seriously. In this paper, we resort to the label information of each aspect to tackle the above problems, along with proposing a novel Label-Driven Denoising Framework (LDF). Extensive experimental results show that our framework achieves better performance than other state-of-the-art methods.</abstract>
      <url hash="4caeb64a">2022.findings-emnlp.177</url>
      <bibkey>zhao-etal-2022-label</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.177</doi>
    </paper>
    <paper id="178">
      <title>Visual Named Entity Linking: A New Dataset and A Baseline</title>
      <author><first>Wen</first><last>Sun</last></author>
      <author><first>Yixing</first><last>Fan</last></author>
      <author><first>Jiafeng</first><last>Guo</last></author>
      <author><first>Ruqing</first><last>Zhang</last></author>
      <author><first>Xueqi</first><last>Cheng</last></author>
      <pages>2403-2415</pages>
      <abstract>Visual Entity Linking (VEL) is a task to link regions of images with their corresponding entities in Knowledge Bases (KBs), which is beneficial for many computer vision tasks such as image retrieval, image caption, and visual question answering. While existing tasks in VEL either rely on textual data to complement a multi-modal linking or only link objects with general entities, which fails to perform named entity linking on large amounts of image data. In this paper, we consider a purely Visual-based Named Entity Linking (VNEL) task, where the input only consists of an image. The task is to identify objects of interest (i.e., visual entity mentions) in images and link them to corresponding named entities in KBs. Since each entity often contains rich visual and textual information in KBs, we thus propose three different sub-tasks, i.e., visual to visual entity linking (V2VEL), visual to textual entity linking (V2TEL), and visual to visual-textual entity linking (V2VTEL). In addition, we present a high-quality human-annotated visual person linking dataset, named WIKIPerson. Based on WIKIPerson, we establish a series of baseline algorithms for the solution of each sub-task, and conduct experiments to verify the quality of the proposed datasets and the effectiveness of baseline methods. We envision this work to be helpful for soliciting more works regarding VNEL in the future. The codes and datasets are publicly available at https: //github.com/ict-bigdatalab/VNEL.</abstract>
      <url hash="5e10fb05">2022.findings-emnlp.178</url>
      <bibkey>sun-etal-2022-visual</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.178</doi>
    </paper>
    <paper id="179">
      <title><fixed-case>MAGMA</fixed-case> – Multimodal Augmentation of Generative Models through Adapter-based Finetuning</title>
      <author><first>Constantin</first><last>Eichenberg</last></author>
      <author><first>Sidney</first><last>Black</last></author>
      <author><first>Samuel</first><last>Weinbach</last></author>
      <author><first>Letitia</first><last>Parcalabescu</last></author>
      <author><first>Anette</first><last>Frank</last></author>
      <pages>2416-2428</pages>
      <abstract>Large-scale pretraining is fast becoming the norm in Vision-Language (VL) modeling. However, prevailing VL approaches are limited by the requirement for labeled data and the use of complex multi-step pretraining objectives. We present MAGMA - a simple method for augmenting generative language models with additional modalities using adapter-based finetuning. Building on Frozen, we train a series of VL models that autoregressively generate text from arbitrary combinations of visual and textual input. The pretraining is entirely end-to-end using a single language modeling objective, simplifying optimization compared to previous approaches. Importantly, the language model weights remain unchanged during training, allowing for transfer of encyclopedic knowledge and in-context learning abilities from language pretraining. MAGMA outperforms Frozen on open-ended generative tasks, achieving state of the art results on the OKVQA benchmark and competitive results on a range of other popular VL benchmarks, while pretraining on 0.2 % of the number of samples used to train SimVLM.</abstract>
      <url hash="1b1a790e">2022.findings-emnlp.179</url>
      <bibkey>eichenberg-etal-2022-magma</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.179</doi>
    </paper>
    <paper id="180">
      <title>Towards Tracing Knowledge in Language Models Back to the Training Data</title>
      <author><first>Ekin</first><last>Akyurek</last></author>
      <author><first>Tolga</first><last>Bolukbasi</last></author>
      <author><first>Frederick</first><last>Liu</last></author>
      <author><first>Binbin</first><last>Xiong</last></author>
      <author><first>Ian</first><last>Tenney</last></author>
      <author><first>Jacob</first><last>Andreas</last></author>
      <author><first>Kelvin</first><last>Guu</last></author>
      <pages>2429-2446</pages>
      <abstract>Language models (LMs) have been shown to memorize a great deal of factual knowledge contained in their training data. But when an LM generates an assertion, it is often difficult to determine where it learned this information and whether it is true. In this paper, we propose the problem of fact tracing: identifying which training examples taught an LM to generate a particular factual assertion. Prior work on training data attribution (TDA) may offer effective tools for identifying such examples, known as “proponents”. We present the first quantitative benchmark to evaluate this. We compare two popular families of TDA methods — gradient-based and embedding-based — and find that much headroom remains. For example, both methods have lower proponent-retrieval precision than an information retrieval baseline (BM25) that does not have access to the LM at all. We identify key challenges that may be necessary for further improvement such as overcoming the problem of gradient saturation, and also show how several nuanced implementation details of existing neural TDA methods can significantly improve overall fact tracing performance.</abstract>
      <url hash="24e9c1dc">2022.findings-emnlp.180</url>
      <bibkey>akyurek-etal-2022-towards</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.180</doi>
    </paper>
    <paper id="181">
      <title><fixed-case>R</fixed-case>ea<fixed-case>R</fixed-case>ev: Adaptive Reasoning for Question Answering over Knowledge Graphs</title>
      <author><first>Costas</first><last>Mavromatis</last></author>
      <author><first>George</first><last>Karypis</last></author>
      <pages>2447-2458</pages>
      <abstract>Knowledge Graph Question Answering (KGQA) involves retrieving entities as answers from a Knowledge Graph (KG) using natural language queries. The challenge is to learn to reason over question-relevant KG facts that traverse KG entities and lead to the question answers. To facilitate reasoning, the question is decoded into instructions, which are dense question representations used to guide the KG traversals. However, if the derived instructions do not exactly match the underlying KG information, they may lead to reasoning under irrelevant context. Our method, termed ReaRev, introduces a new way to KGQA reasoning with respectto both instruction decoding and execution. To improve instruction decoding, we perform reasoning in an adaptive manner, where KG-aware information is used to iteratively update the initial instructions. To improve instruction execution, we emulate breadth-first search (BFS) with graph neural networks (GNNs). The BFS strategy treats the instructions as a set and allows our method to decide on their execution order on the fly. Experimental results on three KGQA benchmarks demonstrate the ReaRev’s effectiveness compared with previous state-of-the-art, especially when the KG is incomplete or when we tackle complex questions. Our code is publicly available at <url>https://github.com/cmavro/ReaRev_KGQA</url>.</abstract>
      <url hash="15560e49">2022.findings-emnlp.181</url>
      <bibkey>mavromatis-karypis-2022-rearev</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.181</doi>
    </paper>
    <paper id="182">
      <title>Understanding Social Media Cross-Modality Discourse in Linguistic Space</title>
      <author><first>Chunpu</first><last>Xu</last></author>
      <author><first>Hanzhuo</first><last>Tan</last></author>
      <author><first>Jing</first><last>Li</last></author>
      <author><first>Piji</first><last>Li</last></author>
      <pages>2459-2471</pages>
      <abstract>The multimedia communications with texts and images are popular on social media. However, limited studies concern how images are structured with texts to form coherent meanings in human cognition. To fill in the gap, we present a novel concept of cross-modality discourse, reflecting how human readers couple image and text understandings. Text descriptions are first derived from images (named as subtitles) in the multimedia contexts. Five labels – entity-level insertion, projection and concretization and scene-level restatement and extension — are further employed to shape the structure of subtitles and texts and present their joint meanings. As a pilot study, we also build the very first dataset containing over 16K multimedia tweets with manually annotated discourse labels. The experimental results show that trendy multimedia encoders based on multi-head attention (with captions) are unable to well understand cross-modality discourse and additionally modeling texts at the output layer helps yield the-state-of-the-art results.</abstract>
      <url hash="607da3a0">2022.findings-emnlp.182</url>
      <bibkey>xu-etal-2022-understanding</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.182</doi>
    </paper>
    <paper id="183">
      <title><fixed-case>TAPE</fixed-case>: Assessing Few-shot <fixed-case>R</fixed-case>ussian Language Understanding</title>
      <author><first>Ekaterina</first><last>Taktasheva</last></author>
      <author><first>Tatiana</first><last>Shavrina</last></author>
      <author><first>Alena</first><last>Fenogenova</last></author>
      <author><first>Denis</first><last>Shevelev</last></author>
      <author><first>Nadezhda</first><last>Katricheva</last></author>
      <author><first>Maria</first><last>Tikhonova</last></author>
      <author><first>Albina</first><last>Akhmetgareeva</last></author>
      <author><first>Oleg</first><last>Zinkevich</last></author>
      <author><first>Anastasiia</first><last>Bashmakova</last></author>
      <author><first>Svetlana</first><last>Iordanskaia</last></author>
      <author><first>Alena</first><last>Spiridonova</last></author>
      <author><first>Valentina</first><last>Kurenshchikova</last></author>
      <author><first>Ekaterina</first><last>Artemova</last></author>
      <author><first>Vladislav</first><last>Mikhailov</last></author>
      <pages>2472-2497</pages>
      <abstract>Recent advances in zero-shot and few-shot learning have shown promise for a scope of research and practical purposes. However, this fast-growing area lacks standardized evaluation suites for non-English languages, hindering progress outside the Anglo-centric paradigm. To address this line of research, we propose TAPE (Text Attack and Perturbation Evaluation), a novel benchmark that includes six more complex NLU tasks for Russian, covering multi-hop reasoning, ethical concepts, logic and commonsense knowledge. The TAPE’s design focuses on systematic zero-shot and few-shot NLU evaluation: (i) linguistic-oriented adversarial attacks and perturbations for analyzing robustness, and (ii) subpopulations for nuanced interpretation. The detailed analysis of testing the autoregressive baselines indicates that simple spelling-based perturbations affect the performance the most, while paraphrasing the input has a more negligible effect. At the same time, the results demonstrate a significant gap between the neural and human baselines for most tasks. We publicly release TAPE (<url>https://tape-benchmark.com</url>) to foster research on robust LMs that can generalize to new tasks when little to no supervision is available.</abstract>
      <url hash="30aa2220">2022.findings-emnlp.183</url>
      <bibkey>taktasheva-etal-2022-tape</bibkey>
      <video href="2022.findings-emnlp.183.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.183</doi>
    </paper>
    <paper id="184">
      <title>A Hierarchical N-Gram Framework for Zero-Shot Link Prediction</title>
      <author><first>Mingchen</first><last>Li</last></author>
      <author><first>Junfan</first><last>Chen</last></author>
      <author><first>Samuel</first><last>Mensah</last></author>
      <author><first>Nikolaos</first><last>Aletras</last></author>
      <author><first>Xiulong</first><last>Yang</last></author>
      <author><first>Yang</first><last>Ye</last></author>
      <pages>2498-2509</pages>
      <abstract>Knowledge graphs typically contain a large number of entities but often cover only a fraction of all relations between them (i.e., incompleteness). Zero-shot link prediction (ZSLP) is a popular way to tackle the problem by automatically identifying unobserved relations between entities. Most recent approaches use textual features of relations (e.g., surface name or textual descriptions) as auxiliary information to improve the encoded representation. These methods lack robustness as they are bound to support only tokens from a fixed vocabulary and unable to model out-of-vocabulary (OOV) words. Subword units such as character n-grams have the capability of generating more expressive representations for OOV words. Hence, in this paper, we propose a <b>H</b>ierarchical <b>N</b>-gram framework for <b>Z</b>ero-<b>S</b>hot <b>L</b>ink <b>P</b>rediction (HNZSLP) that leverages character n-gram information for ZSLP. Our approach works by first constructing a hierarchical n-gram graph from the surface name of relations. Subsequently, a new Transformer-based network models the hierarchical n-gram graph to learn a relation embedding for ZSLP. Experimental results show that our proposed HNZSLP method achieves state-of-the-art performance on two standard ZSLP datasets.</abstract>
      <url hash="c7d1cf9f">2022.findings-emnlp.184</url>
      <bibkey>li-etal-2022-hierarchical-n</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.184</doi>
    </paper>
    <paper id="185">
      <title>Quadapter: Adapter for <fixed-case>GPT</fixed-case>-2 Quantization</title>
      <author><first>Minseop</first><last>Park</last></author>
      <author><first>Jaeseong</first><last>You</last></author>
      <author><first>Markus</first><last>Nagel</last></author>
      <author><first>Simyung</first><last>Chang</last></author>
      <pages>2510-2517</pages>
      <abstract>Transformer language models such as GPT-2 are difficult to quantize because of outliers in the activations leading to a large quantization error. To adapt to the error, one must use quantization-aware training, which entails a fine-tuning process based on the dataset and the training pipeline identical to those for the original model. Pretrained language models, however, often do not grant access to their datasets and training pipelines, forcing us to rely on arbitrary ones for fine-tuning. In that case, it is observed that quantization-aware training overfits the model to the fine-tuning data. To this end introduced is a quantization adapter (Quadapter), a small set of parameters that are learned to make activations quantization-friendly by scaling them channel-wise. For quantization without overfitting, we introduce a quantization adapter (Quadapter), a small set of parameters that are learned to make activations quantization-friendly by scaling them channel-wise. It keeps the model parameters unchanged. By applying our method to the challenging task of quantizing GPT-2, we demonstrate that it effectively prevents the overfitting and improves the quantization performance.</abstract>
      <url hash="2000ce42">2022.findings-emnlp.185</url>
      <bibkey>park-etal-2022-quadapter</bibkey>
      <revision id="1" href="2022.findings-emnlp.185v1" hash="dd8efa58"/>
      <revision id="2" href="2022.findings-emnlp.185v2" hash="2000ce42" date="2023-02-15">Author info correction.</revision>
      <video href="2022.findings-emnlp.185.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.185</doi>
    </paper>
    <paper id="186">
      <title><fixed-case>B</fixed-case>angla<fixed-case>RQA</fixed-case>: A Benchmark Dataset for Under-resourced <fixed-case>B</fixed-case>angla Language Reading Comprehension-based Question Answering with Diverse Question-Answer Types</title>
      <author><first>Syed Mohammed Sartaj</first><last>Ekram</last></author>
      <author><first>Adham Arik</first><last>Rahman</last></author>
      <author><first>Md. Sajid</first><last>Altaf</last></author>
      <author><first>Mohammed Saidul</first><last>Islam</last></author>
      <author><first>Mehrab Mustafy</first><last>Rahman</last></author>
      <author><first>Md Mezbaur</first><last>Rahman</last></author>
      <author><first>Md Azam</first><last>Hossain</last></author>
      <author><first>Abu Raihan Mostofa</first><last>Kamal</last></author>
      <pages>2518-2532</pages>
      <abstract>High-resource languages, such as English, have access to a plethora of datasets with various question-answer types resembling real-world reading comprehension. However, there is a severe lack of diverse and comprehensive question-answering datasets in under-resourced languages like Bangla. The ones available are either translated versions of English datasets with a niche answer format or created by human annotations focusing on a specific domain, question type, or answer type. To address these limitations, this paper introduces BanglaRQA, a reading comprehension-based Bangla question-answering dataset with various question-answer types. BanglaRQA consists of 3,000 context passages and 14,889 question-answer pairs created from those passages. The dataset comprises answerable and unanswerable questions covering four unique categories of questions and three types of answers. In addition, this paper also implemented four different Transformer models for question-answering on the proposed dataset. The best-performing model achieved an overall 62.42% EM and 78.11% F1 score. However, detailed analyses showed that the performance varies across question-answer types, leaving room for substantial improvement of the model performance. Furthermore, we demonstrated the effectiveness of BanglaRQA as a training resource by showing strong results on the bn_squad dataset. Therefore, BanglaRQA has the potential to contribute to the advancement of future research by enhancing the capability of language models. The dataset and codes are available at <url>https://github.com/sartajekram419/BanglaRQA</url></abstract>
      <url hash="4afe19c2">2022.findings-emnlp.186</url>
      <bibkey>ekram-etal-2022-banglarqa</bibkey>
      <video href="2022.findings-emnlp.186.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.186</doi>
    </paper>
    <paper id="187">
      <title>Chaining Simultaneous Thoughts for Numerical Reasoning</title>
      <author><first>Zhihong</first><last>Shao</last></author>
      <author><first>Fei</first><last>Huang</last></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <pages>2533-2547</pages>
      <abstract>Given that rich information is hidden behind ubiquitous numbers in text, numerical reasoning over text should be an essential skill of AI systems. To derive precise equations to solve numerical reasoning problems, previous work focused on modeling the structures of equations, and has proposed various structured decoders. Though structure modeling proves to be effective, these structured decoders construct a single equation in a pre-defined autoregressive order, potentially placing an unnecessary restriction on how a model should grasp the reasoning process. Intuitively, humans may have numerous pieces of thoughts popping up in no pre-defined order; thoughts are not limited to the problem at hand, and can even be concerned with other related problems. By comparing diverse thoughts and chaining relevant pieces, humans are less prone to errors. In this paper, we take this inspiration and propose CANTOR, a numerical reasoner that models reasoning steps using a directed acyclic graph where we produce diverse reasoning steps simultaneously without pre-defined decoding dependencies, and compare and chain relevant ones to reach a solution. Extensive experiments demonstrated the effectiveness of CANTOR under both fully-supervised and weakly-supervised settings.</abstract>
      <url hash="d4b04ebc">2022.findings-emnlp.187</url>
      <bibkey>shao-etal-2022-chaining</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.187</doi>
    </paper>
    <paper id="188">
      <title>Inferring Implicit Relations in Complex Questions with Language Models</title>
      <author><first>Uri</first><last>Katz</last></author>
      <author><first>Mor</first><last>Geva</last></author>
      <author><first>Jonathan</first><last>Berant</last></author>
      <pages>2548-2566</pages>
      <abstract>A prominent challenge for modern language understanding systems is the ability to answer implicit reasoning questions, where the required reasoning steps for answering the question are not mentioned in the text explicitly. In this work, we investigate why current models struggle with implicit reasoning question answering (QA) tasks, by decoupling inference of reasoning steps from their execution. We define a new task of implicit relation inference and construct a benchmark, IMPLICITRELATIONS, where given a question, a model should output a list of concept-relation pairs, where the relations describe the implicit reasoning steps required for answering the question. Using IMPLICITRELATIONS, we evaluate models from the GPT-3 family and find that, while these models struggle on the implicit reasoning QA task, they often succeed at inferring implicit relations. This suggests that the challenge in implicit reasoning questions does not stem from the need to plan a reasoning strategy alone, but to do it while also retrieving and reasoning over relevant information.</abstract>
      <url hash="db828827">2022.findings-emnlp.188</url>
      <bibkey>katz-etal-2022-inferring</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.188</doi>
    </paper>
    <paper id="189">
      <title>Eliciting and Understanding Cross-task Skills with Task-level Mixture-of-Experts</title>
      <author><first>Qinyuan</first><last>Ye</last></author>
      <author><first>Juan</first><last>Zha</last></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <pages>2567-2592</pages>
      <abstract>Recent works suggest that transformer models are capable of multi-tasking on diverse NLP tasks and adapt to new tasks efficiently. However, the potential of these multi-task models may be limited as they use the same set of parameters for all tasks. In contrast, humans tackle tasks in a more flexible way, by making proper presumptions on what skills and knowledge are relevant and executing only the necessary computations. Inspired by this, we propose to use task-level mixture-of-expert models, which has a collection of transformer layers (i.e., experts) and a router component to choose among these experts dynamically and flexibly. We find that these models help improve the average performance gain (ARG) metric by 2.6% when adapting to unseen tasks in few-shot settings, and by 5.6% in zero-shot generalization settings. Further, we show that the learned routing decisions and experts partly rediscover human categorization of NLP tasks – certain experts are strongly associated with extractive tasks, some with classification tasks, and some with tasks requiring world knowledge.</abstract>
      <url hash="b0b9f7de">2022.findings-emnlp.189</url>
      <bibkey>ye-etal-2022-eliciting</bibkey>
      <video href="2022.findings-emnlp.189.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.189</doi>
    </paper>
    <paper id="190">
      <title>On the Curious Case of l2 norm of Sense Embeddings</title>
      <author><first>Yi</first><last>Zhou</last></author>
      <author><first>Danushka</first><last>Bollegala</last></author>
      <pages>2593-2602</pages>
      <abstract>We show that the l2 norm of a static sense embedding encodes information related to the frequency of that sense in the training corpus used to learn the sense embeddings. This finding can be seen as an extension of a previously known relationship for word embeddings to sense embeddings. Our experimental results show that in spite of its simplicity, the l2 norm of sense embeddings is a surprisingly effective feature for several word sense related tasks such as (a) most frequent sense prediction, (b) word-in-context (WiC), and (c) word sense disambiguation (WSD). In particular, by simply including the l2 norm of a sense embedding as a feature in a classifier, we show that we can improve WiC and WSD methods that use static sense embeddings.</abstract>
      <url hash="cec3186b">2022.findings-emnlp.190</url>
      <bibkey>zhou-bollegala-2022-curious</bibkey>
      <video href="2022.findings-emnlp.190.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.190</doi>
    </paper>
    <paper id="191">
      <title>Partially-Random Initialization: A Smoking Gun for Binarization Hypothesis of <fixed-case>BERT</fixed-case></title>
      <author><first>Arash</first><last>Ardakani</last></author>
      <pages>2603-2612</pages>
      <abstract>In the past few years, pre-trained BERT has become one of the most popular deep-learning language models due to their remarkable performance in natural language processing (NLP) tasks. However, the superior performance of BERT comes at the cost of high computational and memory complexity, hindering its envisioned widespread deployment in edge devices with limited computing resources. Binarization can alleviate these limitations by reducing storage requirements and improving computing performance. However, obtaining a comparable accuracy performance for binary BERT w.r.t. its full-precision counterpart is still a difficult task. We observe that direct binarization of pre-trained BERT provides a poor initialization during the fine-tuning phase, making the model incapable of achieving a decent accuracy on downstream tasks. Based on this observation, we put forward the following <i>hypothesis</i>: partially randomly-initialized BERT with binary weights and activations can reach to a decent accuracy performance by distilling knowledge from the its full-precision counterpart. We show that BERT with pre-trained embedding layer and randomly-initialized encoder is a smoking gun for this hypothesis. We identify the smoking gun through a series of experiments and show that it yields a new set of state-of-the-art results on the GLUE and SQuAD benchmarks.</abstract>
      <url hash="d5e0ba44">2022.findings-emnlp.191</url>
      <bibkey>ardakani-2022-partially</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.191</doi>
    </paper>
    <paper id="192">
      <title>Prompt Consistency for Zero-Shot Task Generalization</title>
      <author><first>Chunting</first><last>Zhou</last></author>
      <author><first>Junxian</first><last>He</last></author>
      <author><first>Xuezhe</first><last>Ma</last></author>
      <author><first>Taylor</first><last>Berg-Kirkpatrick</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <pages>2613-2626</pages>
      <abstract>One of the most impressive results of recent NLP history is the ability of pre-trained language models to solve new tasks in a zero-shot setting. To achieve this, NLP tasks are framed as natural language prompts, generating a response indicating the predicted output. Nonetheless, the performance in such settings often lags far behind its supervised counterpart, suggesting a large space for potential improvement. In this paper, we explore methods to utilize unlabeled data to improve zero-shot performance. Specifically, we take advantage of the fact that multiple prompts can be used to specify a single task, and propose to regularize prompt consistency, encouraging consistent predictions over this diverse set of prompts. Our method makes it possible to fine-tune the model either with extra unlabeled training data, or directly on test input at inference time in an unsupervised manner. In experiments, our approach outperforms the state-of-the-art zero-shot learner, T0, on 9 out of 11 datasets across 4 NLP tasks by up to 10.6 absolute points in terms of accuracy. The gains are often attained with a small number of unlabeled examples.</abstract>
      <url hash="75ba4bb3">2022.findings-emnlp.192</url>
      <bibkey>zhou-etal-2022-prompt</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.192</doi>
    </paper>
    <paper id="193">
      <title>In-Context Learning for Few-Shot Dialogue State Tracking</title>
      <author><first>Yushi</first><last>Hu</last></author>
      <author><first>Chia-Hsuan</first><last>Lee</last></author>
      <author><first>Tianbao</first><last>Xie</last></author>
      <author><first>Tao</first><last>Yu</last></author>
      <author><first>Noah A.</first><last>Smith</last></author>
      <author><first>Mari</first><last>Ostendorf</last></author>
      <pages>2627-2643</pages>
      <abstract>Collecting and annotating task-oriented dialogues is time-consuming and costly. Thus, zero and few shot learning for dialogue tasks presents an exciting opportunity. In this work, we propose an in-context (IC) learning framework for zero-shot and few-shot learning dialogue state tracking (DST), where a large pretrained language model (LM) takes a test instance and a few exemplars as input, and directly decodes the dialogue state without any parameter updates. This approach is more flexible and scalable than prior DST work when adapting to new domains and scenarios. To better leverage a tabular domain description in the LM prompt, we reformulate DST into a text-to-SQL problem. We also propose a novel approach to retrieve annotated dialogues as exemplars. Empirical results on MultiWOZ show that our method IC-DST substantially outperforms previous fine-tuned state-of-the-art models in few-shot settings. In addition, we test IC-DST in zero-shot settings, in which the model only takes a fixed task instruction as input, finding that it outperforms previous zero-shot methods by a large margin.</abstract>
      <url hash="29d1516c">2022.findings-emnlp.193</url>
      <bibkey>hu-etal-2022-context</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.193</doi>
    </paper>
    <paper id="194">
      <title>On Advances in Text Generation from Images Beyond Captioning: A Case Study in Self-Rationalization</title>
      <author><first>Shruti</first><last>Palaskar</last></author>
      <author><first>Akshita</first><last>Bhagia</last></author>
      <author><first>Yonatan</first><last>Bisk</last></author>
      <author><first>Florian</first><last>Metze</last></author>
      <author><first>Alan W</first><last>Black</last></author>
      <author><first>Ana</first><last>Marasovic</last></author>
      <pages>2644-2657</pages>
      <abstract>Combining the visual modality with pretrained language models has been surprisingly effective for simple descriptive tasks such as image captioning. More general text generation however remains elusive. We take a step back and ask: How do these models work for more complex generative tasks, i.e. conditioning on both text and images? Are multimodal models simply visually adapted language models, or do they combine they reason jointly over modalities?We investigate these questions in the context of self-rationalization (jointly generating task labels/answers and free-text explanations) of three tasks: (i) visual question answering in VQA-X, (ii) visual commonsense reasoning in VCR, and (iii) visual-textual entailment in E-SNLI-VE. We show that recent unimodal advances, CLIP image representations and scaling of language models, do not consistently improveself-rationalization in multimodal tasks. We find that no single model type works universally best across tasks, datasets, and finetuning data sizes. Our findings motivate the need for novel general backbones that move text generation from images and text beyond image captioning.</abstract>
      <url hash="0e11d168">2022.findings-emnlp.194</url>
      <bibkey>palaskar-etal-2022-advances</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.194</doi>
    </paper>
    <paper id="195">
      <title>The challenges of temporal alignment on <fixed-case>T</fixed-case>witter during crises</title>
      <author><first>Aniket</first><last>Pramanick</last></author>
      <author><first>Tilman</first><last>Beck</last></author>
      <author><first>Kevin</first><last>Stowe</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <pages>2658-2672</pages>
      <abstract>Language use changes over time, and this impacts the effectiveness of NLP systems. This phenomenon is even more prevalent in social media data during crisis events where meaning and frequency of word usage may change over the course of days. Contextual language models fail to adapt temporally, emphasizing the need for temporal adaptation in models which need to be deployed over an extended period of time. While existing approaches consider data spanning large periods of time (from years to decades), shorter time spans are critical for crisis data. We quantify temporal degradation for this scenario and propose methods to cope with performance loss by leveraging techniques from domain adaptation. To the best of our knowledge, this is the first effort to explore effects of rapid language change driven by adversarial adaptations, particularly during natural and human-induced disasters. Through extensive experimentation on diverse crisis datasets, we analyze under what conditions our approaches outperform strong baselines while highlighting the current limitations of temporal adaptation methods in scenarios where access to unlabeled data is scarce.</abstract>
      <url hash="3f26e81b">2022.findings-emnlp.195</url>
      <bibkey>pramanick-etal-2022-challenges</bibkey>
      <video href="2022.findings-emnlp.195.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.195</doi>
    </paper>
    <paper id="196">
      <title>Experimental Standards for Deep Learning in Natural Language Processing Research</title>
      <author><first>Dennis</first><last>Ulmer</last></author>
      <author><first>Elisa</first><last>Bassignana</last></author>
      <author><first>Max</first><last>Müller-Eberstein</last></author>
      <author><first>Daniel</first><last>Varab</last></author>
      <author><first>Mike</first><last>Zhang</last></author>
      <author><first>Rob</first><last>van der Goot</last></author>
      <author><first>Christian</first><last>Hardmeier</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <pages>2673-2692</pages>
      <abstract>The field of Deep Learning (DL) has undergone explosive growth during the last decade, with a substantial impact on Natural Language Processing (NLP) as well. Yet, compared to more established disciplines, a lack of common experimental standards remains an open challenge to the field at large. Starting from fundamental scientific principles, we distill ongoing discussions on experimental standards in NLP into a single, widely-applicable methodology. Following these best practices is crucial to strengthen experimental evidence, improve reproducibility and enable scientific progress. These standards are further collected in a public repository to help them transparently adapt to future needs.</abstract>
      <url hash="97da8361">2022.findings-emnlp.196</url>
      <bibkey>ulmer-etal-2022-experimental</bibkey>
      <video href="2022.findings-emnlp.196.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.196</doi>
    </paper>
    <paper id="197">
      <title>Few-Shot Anaphora Resolution in Scientific Protocols via Mixtures of In-Context Experts</title>
      <author><first>Nghia T.</first><last>Le</last></author>
      <author><first>Fan</first><last>Bai</last></author>
      <author><first>Alan</first><last>Ritter</last></author>
      <pages>2693-2706</pages>
      <abstract>Anaphora resolution is an important task for information extraction across a range of languages, text genres, and domains, motivating the need for methods that do not require large annotated datasets. In-context learning has emerged as a promising approach, yet there are a number of challenges in applying in-context learning to resolve anaphora. For example, encoding a single in-context demonstration that consists of: an anaphor, a paragraph-length context, and a list of corresponding antecedents, requires conditioning a language model on a long sequence of tokens, limiting the number of demonstrations per prompt. In this paper, we present Mice (Mixtures of In-Context Experts), which we demonstrate is effective for few-shot anaphora resolution in scientific protocols. Given only a handful of training examples, Mice combines the predictions of hundreds of in-context experts, yielding a 30% increase in F1 score over a competitive prompt retrieval baseline. Furthermore, we show Mice can be used to train compact student models without sacrificing performance. As far as we are aware, this is the first work to present experimental results demonstrating the effectiveness of in-context learning on the task of few-shot anaphora resolution in scientific protocols.</abstract>
      <url hash="fc961209">2022.findings-emnlp.197</url>
      <bibkey>le-etal-2022-shot</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.197</doi>
    </paper>
    <paper id="198">
      <title>Exploring Predictive Uncertainty and Calibration in <fixed-case>NLP</fixed-case>: A Study on the Impact of Method &amp; Data Scarcity</title>
      <author><first>Dennis</first><last>Ulmer</last></author>
      <author><first>Jes</first><last>Frellsen</last></author>
      <author><first>Christian</first><last>Hardmeier</last></author>
      <pages>2707-2735</pages>
      <abstract>We investigate the problem of determining the predictive confidence (or, conversely, uncertainty) of a neural classifier through the lens of low-resource languages. By training models on sub-sampled datasets in three different languages, we assess the quality of estimates from a wide array of approaches and their dependence on the amount of available data. We find that while approaches based on pre-trained models and ensembles achieve the best results overall, the quality of uncertainty estimates can surprisingly suffer with more data. We also perform a qualitative analysis of uncertainties on sequences, discovering that a model’s total uncertainty seems to be influenced to a large degree by its data uncertainty, not model uncertainty. All model implementations are open-sourced in a software package.</abstract>
      <url hash="af7fbd40">2022.findings-emnlp.198</url>
      <bibkey>ulmer-etal-2022-exploring</bibkey>
      <video href="2022.findings-emnlp.198.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.198</doi>
    </paper>
    <paper id="199">
      <title>Conditional Supervised Contrastive Learning for Fair Text Classification</title>
      <author><first>Jianfeng</first><last>Chi</last></author>
      <author><first>William</first><last>Shand</last></author>
      <author><first>Yaodong</first><last>Yu</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <author><first>Han</first><last>Zhao</last></author>
      <author><first>Yuan</first><last>Tian</last></author>
      <pages>2736-2756</pages>
      <abstract>Contrastive representation learning has gained much attention due to its superior performance in learning representations from both image and sequential data. However, the learned representations could potentially lead to performance disparities in downstream tasks, such as increased silencing of underrepresented groups in toxicity comment classification. In light of this challenge, in this work, we study learning fair representations that satisfy a notion of fairness known as equalized odds for text classification via contrastive learning. Specifically, we first theoretically analyze the connections between learning representations with a fairness constraint and conditional supervised contrastive objectives, and then propose to use conditional supervised contrastive objectives to learn fair representations for text classification. We conduct experiments on two text datasets to demonstrate the effectiveness of our approaches in balancing the trade-offs between task performance and bias mitigation among existing baselines for text classification. Furthermore, we also show that the proposed methods are stable in different hyperparameter settings.</abstract>
      <url hash="4b0b3455">2022.findings-emnlp.199</url>
      <bibkey>chi-etal-2022-conditional</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.199</doi>
    </paper>
    <paper id="200">
      <title><fixed-case>S</fixed-case>pa<fixed-case>BERT</fixed-case>: A Pretrained Language Model from Geographic Data for Geo-Entity Representation</title>
      <author><first>Zekun</first><last>Li</last></author>
      <author><first>Jina</first><last>Kim</last></author>
      <author><first>Yao-Yi</first><last>Chiang</last></author>
      <author><first>Muhao</first><last>Chen</last></author>
      <pages>2757-2769</pages>
      <abstract>Named geographic entities (geo-entities for short) are the building blocks of many geographic datasets. Characterizing geo-entities is integral to various application domains, such as geo-intelligence and map comprehension, while a key challenge is to capture the spatial-varying context of an entity. We hypothesize that we shall know the characteristics of a geo-entity by its surrounding entities, similar to knowing word meanings by their linguistic context. Accordingly, we propose a novel spatial language model, SpaBERT, which provides a general-purpose geo-entity representation based on neighboring entities in geospatial data. SpaBERT extends BERT to capture linearized spatial context, while incorporating a spatial coordinate embedding mechanism to preserve spatial relations of entities in the 2-dimensional space. SpaBERT is pretrained with masked language modeling and masked entity prediction tasks to learn spatial dependencies. We apply SpaBERT to two downstream tasks: geo-entity typing and geo-entity linking. Compared with the existing language models that do not use spatial context, SpaBERT shows significant performance improvement on both tasks. We also analyze the entity representation from SpaBERT in various settings and the effect of spatial coordinate embedding.</abstract>
      <url hash="39ca7642">2022.findings-emnlp.200</url>
      <bibkey>li-etal-2022-spabert</bibkey>
      <video href="2022.findings-emnlp.200.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.200</doi>
    </paper>
    <paper id="201">
      <title>Self-training with Two-phase Self-augmentation for Few-shot Dialogue Generation</title>
      <author><first>Wanyu</first><last>Du</last></author>
      <author><first>Hanjie</first><last>Chen</last></author>
      <author><first>Yangfeng</first><last>Ji</last></author>
      <pages>2770-2784</pages>
      <abstract>In task-oriented dialogue systems, response generation from meaning representations (MRs) often suffers from limited training examples, due to the high cost of annotating MR-to-Text pairs. Previous works on self-training leverage fine-tuned conversational models to automatically generate pseudo-labeled MR-to-Text pairs for further fine-tuning. However, some self-augmented data may be noisy or uninformative for the model to learn from. In this work, we propose a two-phase self-augmentation procedure to generate high-quality pseudo-labeled MR-to-Text pairs: the first phase selects the most informative MRs based on model’s prediction uncertainty; with the selected MRs, the second phase generates accurate responses by aggregating multiple perturbed latent representations from each MR. Empirical experiments on two benchmark datasets, FewShotWOZ and FewShotSGD, show that our method generally outperforms existing self-training methods on both automatic and human evaluations.</abstract>
      <url hash="a6b7f98e">2022.findings-emnlp.201</url>
      <bibkey>du-etal-2022-self</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.201</doi>
    </paper>
    <paper id="202">
      <title>Is <fixed-case>NLP</fixed-case> Ready for Standardization?</title>
      <author><first>Lauriane</first><last>Aufrant</last></author>
      <pages>2785-2800</pages>
      <abstract>While standardization is a well-established activity in other scientific fields such as telecommunications, networks or multimedia, in the field of AI and more specifically NLP it is still at its dawn. In this paper, we explore how various aspects of NLP (evaluation, data, tasks...) lack standards and how that can impact science, but also the society, the industry, and regulations. We argue that the numerous initiatives to rationalize the field and establish good practices are only the first step, and developing formal standards remains needed to bring further clarity to NLP research and industry, at a time where this community faces various crises regarding ethics or reproducibility. We thus encourage NLP researchers to contribute to existing and upcoming standardization projects, so that they can express their needs and concerns, while sharing their expertise.</abstract>
      <url hash="cfa2f850">2022.findings-emnlp.202</url>
      <bibkey>aufrant-2022-nlp</bibkey>
      <video href="2022.findings-emnlp.202.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.202</doi>
    </paper>
    <paper id="203">
      <title>Probing for Incremental Parse States in Autoregressive Language Models</title>
      <author><first>Tiwalayo</first><last>Eisape</last></author>
      <author><first>Vineet</first><last>Gangireddy</last></author>
      <author><first>Roger</first><last>Levy</last></author>
      <author><first>Yoon</first><last>Kim</last></author>
      <pages>2801-2813</pages>
      <abstract>Next-word predictions from autoregressive neural language models show remarkable sensitivity to syntax. This work evaluates the extent to which this behavior arises as a result of a learned ability to maintain implicit representations of incremental syntactic structures. We extend work in syntactic probing to the incremental setting and present several probes for extracting incomplete syntactic structure (operationalized through parse states from a stack-based parser) from autoregressive language models. We find that our probes can be used to predict model preferences on ambiguous sentence prefixes and causally intervene on model representations and steer model behavior. This suggests implicit incremental syntactic inferences underlie next-word predictions in autoregressive neural language models.</abstract>
      <url hash="6a72182e">2022.findings-emnlp.203</url>
      <bibkey>eisape-etal-2022-probing</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.203</doi>
    </paper>
    <paper id="204">
      <title>Re-Examining Calibration: The Case of Question Answering</title>
      <author><first>Chenglei</first><last>Si</last></author>
      <author><first>Chen</first><last>Zhao</last></author>
      <author><first>Sewon</first><last>Min</last></author>
      <author><first>Jordan</first><last>Boyd-Graber</last></author>
      <pages>2814-2829</pages>
      <abstract>For users to trust model predictions, they need to understand model outputs, particularly their confidence — calibration aims to adjust (calibrate) models’ confidence to match expected accuracy. We argue that the traditional calibration evaluation does not promote effective calibrations: for example, it can encourage always assigning a mediocre confidence score to all predictions, which does not help users distinguish correct predictions from wrong ones. Building on those observations, we propose a new calibration metric, MacroCE, that better captures whether the model assigns low confidence to wrong predictions and high confidence to correct predictions. Focusing on the practical application of open-domain question answering, we examine conventional calibration methods applied on the widely-used retriever-reader pipeline, all of which do not bring significant gains under our new MacroCE metric. Toward better calibration, we propose a new calibration method (ConsCal) that uses not just final model predictions but whether multiple model checkpoints make consistent predictions. Altogether, we provide an alternative view of calibration along with a new metric, re-evaluation of existing calibration methods on our metric, and proposal of a more effective calibration method.</abstract>
      <url hash="85beafa3">2022.findings-emnlp.204</url>
      <bibkey>si-etal-2022-examining</bibkey>
      <video href="2022.findings-emnlp.204.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.204</doi>
    </paper>
    <paper id="205">
      <title>Accelerating Learned Sparse Indexes Via Term Impact Decomposition</title>
      <author><first>Joel</first><last>Mackenzie</last></author>
      <author><first>Antonio</first><last>Mallia</last></author>
      <author><first>Alistair</first><last>Moffat</last></author>
      <author><first>Matthias</first><last>Petri</last></author>
      <pages>2830-2842</pages>
      <abstract>Novel inverted index-based learned sparse ranking models provide more effective, but less efficient, retrieval performance compared to traditional ranking models like BM25. In this paper, we introduce a technique we call postings clipping to improve the query efficiency of learned representations. Our technique amplifies the benefit of dynamic pruning query processing techniques by accounting for changes in term importance distributions of learned ranking models. The new clipping mechanism accelerates top-k retrieval by up to 9.6X without any loss in effectiveness.</abstract>
      <url hash="d1323d07">2022.findings-emnlp.205</url>
      <bibkey>mackenzie-etal-2022-accelerating</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.205</doi>
    </paper>
    <paper id="206">
      <title>Do Text-to-Text Multi-Task Learners Suffer from Task Conflict?</title>
      <author><first>David</first><last>Mueller</last></author>
      <author><first>Nicholas</first><last>Andrews</last></author>
      <author><first>Mark</first><last>Dredze</last></author>
      <pages>2843-2858</pages>
      <abstract>Traditional multi-task learning architectures learn a single model across multiple tasks through a shared encoder followed by task-specific decoders. Learning these models often requires specialized training algorithms that address task-conflict in the shared parameter updates, which otherwise can lead to negative transfer. A new type of multi-task learning within NLP homogenizes multi-task architectures as a shared encoder and language model decoder, which does surprisingly well across a range of diverse tasks. Does this new architecture suffer from task-conflicts that require specialized training algorithms? We study how certain factors in the shift towards text-to-text models affects multi-task conflict and negative transfer, finding that both directional conflict and transfer are surprisingly constant across architectures.</abstract>
      <url hash="251ac40c">2022.findings-emnlp.206</url>
      <bibkey>mueller-etal-2022-text</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.206</doi>
    </paper>
    <paper id="207">
      <title><fixed-case>MANT</fixed-case>a: Efficient Gradient-Based Tokenization for End-to-End Robust Language Modeling</title>
      <author><first>Nathan</first><last>Godey</last></author>
      <author><first>Roman</first><last>Castagné</last></author>
      <author><first>Éric</first><last>de la Clergerie</last></author>
      <author><first>Benoît</first><last>Sagot</last></author>
      <pages>2859-2870</pages>
      <abstract>Static subword tokenization algorithms have been an essential component of recent works on language modeling. However, their static nature results in important flaws that degrade the models’ downstream performance and robustness. In this work, we propose MANTa, a Module for Adaptive Neural TokenizAtion. MANTa is a differentiable tokenizer trained end-to-end with the language model. The resulting system offers a trade-off between the expressiveness of byte-level models and the speed of models trained using subword tokenization. In addition, our tokenizer is highly explainable since it produces an explicit segmentation of sequences into blocks. We evaluate our pre-trained model on several English datasets from different domains as well as on synthetic noise. We find that MANTa improves robustness to character perturbations and out-of-domain data. We then show that MANTa performs comparably to other models on the general-domain GLUE benchmark. Finally, we show that it is considerably faster than strictly byte-level models.</abstract>
      <url hash="9ea98b29">2022.findings-emnlp.207</url>
      <bibkey>godey-etal-2022-manta</bibkey>
      <video href="2022.findings-emnlp.207.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.207</doi>
    </paper>
    <paper id="208">
      <title>Towards Intelligent Clinically-Informed Language Analyses of People with Bipolar Disorder and Schizophrenia</title>
      <author><first>Ankit</first><last>Aich</last></author>
      <author><first>Avery</first><last>Quynh</last></author>
      <author><first>Varsha</first><last>Badal</last></author>
      <author><first>Amy</first><last>Pinkham</last></author>
      <author><first>Philip</first><last>Harvey</last></author>
      <author><first>Colin</first><last>Depp</last></author>
      <author><first>Natalie</first><last>Parde</last></author>
      <pages>2871-2887</pages>
      <abstract>NLP offers a myriad of opportunities to support mental health research. However, prior work has almost exclusively focused on social media data, for which diagnoses are difficult or impossible to validate. We present a first-of-its-kind dataset of manually transcribed interactions with people clinically diagnosed with bipolar disorder and schizophrenia, as well as healthy controls. Data was collected through validated clinical tasks and paired with diagnostic measures. We extract 100+ temporal, sentiment, psycholinguistic, emotion, and lexical features from the data and establish classification validity using a variety of models to study language differences between diagnostic groups. Our models achieve strong classification performance (maximum F1=0.93-0.96), and lead to the discovery of interesting associations between linguistic features and diagnostic class. It is our hope that this dataset will offer high value to clinical and NLP researchers, with potential for widespread broader impacts.</abstract>
      <url hash="1b2876ec">2022.findings-emnlp.208</url>
      <bibkey>aich-etal-2022-towards</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.208</doi>
    </paper>
    <paper id="209">
      <title>Calibrating Trust of Multi-Hop Question Answering Systems with Decompositional Probes</title>
      <author><first>Kaige</first><last>Xie</last></author>
      <author><first>Sarah</first><last>Wiegreffe</last></author>
      <author><first>Mark</first><last>Riedl</last></author>
      <pages>2888-2902</pages>
      <abstract>Multi-hop Question Answering (QA) is a challenging task since it requires an accurate aggregation of information from multiple context paragraphs and a thorough understanding of the underlying reasoning chains. Recent work in multi-hop QA has shown that performance can be boosted by first decomposing the questions into simpler, single-hop questions. In this paper, we explore one additional utility of the multi-hop decomposition from the perspective of explainable NLP: to create explanation by probing a neural QA model with them. We hypothesize that in doing so, users will be better able to predict when the underlying QA system will give the correct answer. Through human participant studies, we verify that exposing the decomposition probes and answers to the probes to users can increase their ability to predict system performance on a question instance basis. We show that decomposition is an effective form of probing QA systems as well as a promising approach to explanation generation. In-depth analyses show the need for improvements in decomposition systems.</abstract>
      <url hash="343062d5">2022.findings-emnlp.209</url>
      <bibkey>xie-etal-2022-calibrating</bibkey>
      <video href="2022.findings-emnlp.209.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.209</doi>
    </paper>
    <paper id="210">
      <title><fixed-case>C</fixed-case>heck<fixed-case>HARD</fixed-case>: Checking Hard Labels for Adversarial Text Detection, Prediction Correction, and Perturbed Word Suggestion</title>
      <author><first>Hoang-Quoc</first><last>Nguyen-Son</last></author>
      <author><first>Huy Quang</first><last>Ung</last></author>
      <author><first>Seira</first><last>Hidano</last></author>
      <author><first>Kazuhide</first><last>Fukushima</last></author>
      <author><first>Shinsaku</first><last>Kiyomoto</last></author>
      <pages>2903-2913</pages>
      <abstract>An adversarial attack generates harmful text that fools a target model. More dangerously, this text is unrecognizable by humans. Existing work detects adversarial text and corrects a target’s prediction by identifying perturbed words and changing them into their synonyms, but many benign words are also changed. In this paper, we directly detect adversarial text, correct the prediction, and suggest perturbed words by checking the change in the hard labels from the target’s predictions after replacing a word with its transformation using a model that we call CheckHARD. The experiments demonstrate that CheckHARD outperforms existing work on various attacks, models, and datasets.</abstract>
      <url hash="53eeff72">2022.findings-emnlp.210</url>
      <bibkey>nguyen-son-etal-2022-checkhard</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.210</doi>
    </paper>
    <paper id="211">
      <title>Mitigating Covertly Unsafe Text within Natural Language Systems</title>
      <author><first>Alex</first><last>Mei</last></author>
      <author><first>Anisha</first><last>Kabir</last></author>
      <author><first>Sharon</first><last>Levy</last></author>
      <author><first>Melanie</first><last>Subbiah</last></author>
      <author><first>Emily</first><last>Allaway</last></author>
      <author><first>John</first><last>Judge</last></author>
      <author><first>Desmond</first><last>Patton</last></author>
      <author><first>Bruce</first><last>Bimber</last></author>
      <author><first>Kathleen</first><last>McKeown</last></author>
      <author><first>William Yang</first><last>Wang</last></author>
      <pages>2914-2926</pages>
      <abstract>An increasingly prevalent problem for intelligent technologies is text safety, as uncontrolled systems may generate recommendations to their users that lead to injury or life-threatening consequences. However, the degree of explicitness of a generated statement that can cause physical harm varies. In this paper, we distinguish types of text that can lead to physical harm and establish one particularly underexplored category: covertly unsafe text. Then, we further break down this category with respect to the system’s information and discuss solutions to mitigate the generation of text in each of these subcategories. Ultimately, our work defines the problem of covertly unsafe language that causes physical harm and argues that this subtle yet dangerous issue needs to be prioritized by stakeholders and regulators. We highlight mitigation strategies to inspire future researchers to tackle this challenging problem and help improve safety within smart systems.</abstract>
      <url hash="599c7e4b">2022.findings-emnlp.211</url>
      <bibkey>mei-etal-2022-mitigating</bibkey>
      <video href="2022.findings-emnlp.211.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.211</doi>
    </paper>
    <paper id="212">
      <title>“<fixed-case>I</fixed-case> Know Who You Are”: Character-Based Features for Conversational Humor Recognition in <fixed-case>C</fixed-case>hinese</title>
      <author><first>Wenbo</first><last>Shang</last></author>
      <author><first>Jiangjiang</first><last>Zhao</last></author>
      <author><first>Zezhong</first><last>Wang</last></author>
      <author><first>Binyang</first><last>Li</last></author>
      <author><first>Fangchun</first><last>Yang</last></author>
      <author><first>Kam-Fai</first><last>Wong</last></author>
      <pages>2927-2932</pages>
      <abstract>Humor plays an important role in our daily life, as it is an essential and fascinating element in the communication between persons. Therefore, how to recognize punchlines from the dialogue, i.e. conversational humor recognition, has attracted much interest of computational linguistics communities. However, most existing work attempted to understand the conversational humor by analyzing the contextual information of the dialogue, but neglected the character of the interlocutor, such as age, gender, occupation, and so on. For instance, the same utterance could bring out humorous from a serious person, but may be a plain expression from a naive person. To this end, this paper proposes a Character Fusion Conversational Humor Recognition model (CFCHR) to explore character information to recognize conversational humor. CFCHR utilizes a multi-task learning framework that unifies two highly pertinent tasks, i.e., character extraction and punchline identification. Based on deep neural networks, we trained both tasks jointly by sharing weight to extract the common and task-invariant features while each task could still learn its task-specific features. Experiments were conducted on Chinese sitcoms corpus, which consisted of 12,677 utterances from 22 characters. The experimental results demonstrated that CFCHR could achieve 33.08% improvements in terms of F1-score over some strong baselines, and proved the effectiveness of the character information to identify the punchlines.</abstract>
      <url hash="e5134467">2022.findings-emnlp.212</url>
      <bibkey>shang-etal-2022-know</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.212</doi>
    </paper>
    <paper id="213">
      <title><fixed-case>D</fixed-case>ebias<fixed-case>GAN</fixed-case>: Eliminating Position Bias in News Recommendation with Adversarial Learning</title>
      <author><first>Chuhan</first><last>Wu</last></author>
      <author><first>Fangzhao</first><last>Wu</last></author>
      <author><first>Xiangnan</first><last>He</last></author>
      <author><first>Yongfeng</first><last>Huang</last></author>
      <pages>2933-2938</pages>
      <abstract>Click behaviors are widely used for learning news recommendation models, but they are heavily affected by the biases brought by the news display positions. It is important to remove position biases to train unbiased recommendation model and capture unbiased user interest. In this paper, we propose a news recommendation method named DebiasGAN that can effectively alleviate position biases via adversarial learning. The core idea is modeling the personalized effect of position bias on click behaviors in a candidate-aware way, and learning debiased candidate-aware user embeddings from which the position information cannot be discriminated. More specifically, we use a bias-aware click model to capture the effect of position bias on click behaviors, and use a bias-invariant click model with random candidate positions to estimate the ideally unbiased click scores. We apply adversarial learning to the embeddings learned by the two models to help the bias-invariant click model capture debiased user interest. Experimental results on two real-world datasets show that DebiasGAN effectively improves news recommendation by eliminating position biases.</abstract>
      <url hash="5e37e151">2022.findings-emnlp.213</url>
      <bibkey>wu-etal-2022-debiasgan</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.213</doi>
    </paper>
    <paper id="214">
      <title>Generating Multiple-Length Summaries via Reinforcement Learning for Unsupervised Sentence Summarization</title>
      <author><first>Dongmin</first><last>Hyun</last></author>
      <author><first>Xiting</first><last>Wang</last></author>
      <author><first>Chayoung</first><last>Park</last></author>
      <author><first>Xing</first><last>Xie</last></author>
      <author><first>Hwanjo</first><last>Yu</last></author>
      <pages>2939-2951</pages>
      <abstract>Sentence summarization shortens given texts while maintaining core contents of the texts. Unsupervised approaches have been studied to summarize texts without ground-truth summaries. However, recent unsupervised models are extractive, which remove words from texts and thus they are less flexible than abstractive summarization. In this work, we devise an abstractive model based on reinforcement learning without ground-truth summaries. We formulate the unsupervised summarization based on the Markov decision process with rewards representing the summary quality. To further enhance the summary quality, we develop a multi-summary learning mechanism that generates multiple summaries with varying lengths for a given text, while making the summaries mutually enhance each other. Experimental results show that the proposed model substantially outperforms both abstractive and extractive models, yet frequently generating new words not contained in input texts.</abstract>
      <url hash="b8b5d76b">2022.findings-emnlp.214</url>
      <bibkey>hyun-etal-2022-generating</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.214</doi>
    </paper>
    <paper id="215">
      <title>Multilingual Sentence Transformer as A Multilingual Word Aligner</title>
      <author><first>Weikang</first><last>Wang</last></author>
      <author><first>Guanhua</first><last>Chen</last></author>
      <author><first>Hanqing</first><last>Wang</last></author>
      <author><first>Yue</first><last>Han</last></author>
      <author><first>Yun</first><last>Chen</last></author>
      <pages>2952-2963</pages>
      <abstract>Multilingual pretrained language models (mPLMs) have shown their effectiveness in multilingual word alignment induction. However, these methods usually start from mBERT or XLM-R. In this paper, we investigate whether multilingual sentence Transformer LaBSE is a strong multilingual word aligner. This idea is non-trivial as LaBSE is trained to learn language-agnostic sentence-level embeddings, while the alignment extraction task requires the more fine-grained word-level embeddings to be language-agnostic. We demonstrate that the vanilla LaBSE outperforms other mPLMs currently used in the alignment task, and then propose to finetune LaBSE on parallel corpus for further improvement. Experiment results on seven language pairs show that our best aligner outperforms previous state-of-the-art models of all varieties. In addition, our aligner supports different language pairs in a single model, and even achieves new state-of-the-art on zero-shot language pairs that does not appear in the finetuning process.</abstract>
      <url hash="4f567763">2022.findings-emnlp.215</url>
      <bibkey>wang-etal-2022-multilingual</bibkey>
      <video href="2022.findings-emnlp.215.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.215</doi>
    </paper>
    <paper id="216">
      <title><fixed-case>CORE</fixed-case>: A Retrieve-then-Edit Framework for Counterfactual Data Generation</title>
      <author><first>Tanay</first><last>Dixit</last></author>
      <author><first>Bhargavi</first><last>Paranjape</last></author>
      <author><first>Hannaneh</first><last>Hajishirzi</last></author>
      <author><first>Luke</first><last>Zettlemoyer</last></author>
      <pages>2964-2984</pages>
      <abstract>Counterfactual data augmentation (CDA) – i.e., adding minimally perturbed inputs during training – helps reduce model reliance on spurious correlations and improves generalization to out-of-distribution (OOD) data. Prior work on generating counterfactuals only considered restricted classes of perturbations, limiting their effectiveness. We present Counterfactual Generation via Retrieval and Editing (CORE), a retrieval-augmented generation framework for creating diverse counterfactual perturbations for CDA. For each training example, CORE first performs a dense retrieval over a task-related unlabeled text corpus using a learned bi-encoder and extracts relevant counterfactual excerpts. CORE then incorporates these into prompts to a large language model with few-shot learning capabilities, for counterfactual editing. Conditioning language model edits on naturally occurring data results in more diverse perturbations. Experiments on natural language inference and sentiment analysis benchmarks show that CORE counterfactuals are more effective at improving generalization to OOD data compared to other DA approaches. We also show that the CORE retrieval framework can be used to encourage diversity in manually authored perturbations.</abstract>
      <url hash="dfe889bb">2022.findings-emnlp.216</url>
      <bibkey>dixit-etal-2022-core</bibkey>
      <video href="2022.findings-emnlp.216.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.216</doi>
    </paper>
    <paper id="217">
      <title>Conversation Disentanglement with Bi-Level Contrastive Learning</title>
      <author><first>Chengyu</first><last>Huang</last></author>
      <author><first>Zheng</first><last>Zhang</last></author>
      <author><first>Hao</first><last>Fei</last></author>
      <author><first>Lizi</first><last>Liao</last></author>
      <pages>2985-2996</pages>
      <abstract>Conversation disentanglement aims to group utterances into detached sessions, which is a fundamental task in processing multi-party conversations. Existing methods have two main drawbacks. First, they overemphasize pairwise utterance relations but pay inadequate attention to the utterance-to-context relation modeling. Second, huge amount of human annotated data is required for training, which is expensive to obtain in practice. To address these issues, we propose a general disentangle model based on bi-level contrastive learning. It brings closer utterances in the same session while encourages each utterance to be near its clustered session prototypes in the representation space. Unlike existing approaches, our disentangle model works in both supervised setting with labeled data and unsupervised setting when no such data is available. The proposed method achieves new state-of-the-art performance on both settings across several public datasets.</abstract>
      <url hash="37c26c9e">2022.findings-emnlp.217</url>
      <bibkey>huang-etal-2022-conversation</bibkey>
      <video href="2022.findings-emnlp.217.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.217</doi>
    </paper>
    <paper id="218">
      <title>You can’t pick your neighbors, or can you? When and How to Rely on Retrieval in the k<fixed-case>NN</fixed-case>-<fixed-case>LM</fixed-case></title>
      <author><first>Andrew</first><last>Drozdov</last></author>
      <author><first>Shufan</first><last>Wang</last></author>
      <author><first>Razieh</first><last>Rahimi</last></author>
      <author><first>Andrew</first><last>McCallum</last></author>
      <author><first>Hamed</first><last>Zamani</last></author>
      <author><first>Mohit</first><last>Iyyer</last></author>
      <pages>2997-3007</pages>
      <abstract>Retrieval-enhanced language models (LMs), which condition their predictions on text retrieved from large external datastores, have recently shown significant perplexity improvements compared to standard LMs. One such approach, the kNN-LM, interpolates any existing LM’s predictions with the output of a k-nearest neighbors model and requires no additional training. In this paper, we explore the importance of lexical and semantic matching in the context of items retrieved by kNN-LM. We find two trends: (1) the presence of large overlapping n-grams between the datastore and evaluation set plays an important factor in strong performance, even when the datastore is derived from the training data; and (2) the kNN-LM is most beneficial when retrieved items have high semantic similarity with the query. Based on our analysis, we define a new formulation of the kNN-LM that uses retrieval quality to assign the interpolation coefficient. We empirically measure the effectiveness of our approach on two English language modeling datasets, Wikitext-103 and PG-19. Our re-formulation of the kNN-LM is beneficial in both cases, and leads to nearly 4% improvement in perplexity on the Wikitext-103 test set.</abstract>
      <url hash="bf8b80d0">2022.findings-emnlp.218</url>
      <bibkey>drozdov-etal-2022-cant</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.218</doi>
    </paper>
    <paper id="219">
      <title><fixed-case>S</fixed-case>tu<fixed-case>B</fixed-case>ot: Learning by Teaching a Conversational Agent Through Machine Reading Comprehension</title>
      <author><first>Nayoung</first><last>Jin</last></author>
      <author><first>Hana</first><last>Lee</last></author>
      <pages>3008-3020</pages>
      <abstract>This paper proposes StuBot, a text-based conversational agent that provides adaptive feedback for learning by teaching. StuBot first asks the users to teach the learning content by summarizing and explaining it in their own words. After the users inputted the explanation text for teaching, StuBot uses a machine reading comprehension (MRC) engine to provide adaptive feedback with further questions about the insufficient parts of the explanation text. We conducted a within-subject study to evaluate the effectiveness of adaptive feedback by StuBot. Both the quantitative and qualitative results showed that learning by teaching with adaptive feedback can improve learning performance, immersion, and overall experience.</abstract>
      <url hash="313333a2">2022.findings-emnlp.219</url>
      <bibkey>jin-lee-2022-stubot</bibkey>
      <video href="2022.findings-emnlp.219.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.219</doi>
    </paper>
    <paper id="220">
      <title>Improved Universal Sentence Embeddings with Prompt-based Contrastive Learning and Energy-based Learning</title>
      <author><first>Yuxin</first><last>Jiang</last></author>
      <author><first>Linhan</first><last>Zhang</last></author>
      <author><first>Wei</first><last>Wang</last></author>
      <pages>3021-3035</pages>
      <abstract>Contrastive learning has been demonstrated to be effective in enhancing pre-trained language models (PLMs) to derive superior universal sentence embeddings. However, existing contrastive methods still have two limitations. Firstly, previous works may acquire poor performance under domain shift settings, thus hindering the application of sentence representations in practice. We attribute this low performance to the over-parameterization of PLMs with millions of parameters. To alleviate it, we propose PromCSE (Prompt-based Contrastive Learning for Sentence Embeddings), which only trains small-scale Soft Prompt (i.e., a set of trainable vectors) while keeping PLMs fixed. Secondly, the commonly used NT-Xent loss function of contrastive learning does not fully exploit hard negatives in supervised learning settings. To this end, we propose to integrate an Energy-based Hinge loss to enhance the pairwise discriminative power, inspired by the connection between the NT-Xent loss and the Energy-based Learning paradigm. Empirical results on seven standard semantic textual similarity (STS) tasks and a domain-shifted STS task both show the effectiveness of our method compared with the current state-of-the-art sentence embedding models.</abstract>
      <url hash="4992548c">2022.findings-emnlp.220</url>
      <bibkey>jiang-etal-2022-improved</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.220</doi>
    </paper>
    <paper id="221">
      <title><fixed-case>R</fixed-case>a<fixed-case>P</fixed-case>: Redundancy-aware Video-language Pre-training for Text-Video Retrieval</title>
      <author><first>Xing</first><last>Wu</last></author>
      <author><first>Chaochen</first><last>Gao</last></author>
      <author><first>Zijia</first><last>Lin</last></author>
      <author><first>Zhongyuan</first><last>Wang</last></author>
      <author><first>Jizhong</first><last>Han</last></author>
      <author><first>Songlin</first><last>Hu</last></author>
      <pages>3036-3047</pages>
      <abstract>Video language pre-training methods have mainly adopted sparse sampling techniques to alleviate the temporal redundancy of videos. Though effective, sparse sampling still suffers inter-modal redundancy: visual redundancy and textual redundancy. Compared with highly generalized text, sparsely sampled frames usually contain text-independent portions, called visual redundancy. Sparse sampling is also likely to miss important frames corresponding to some text portions, resulting in textual redundancy. Inter-modal redundancy leads to a mismatch of video and text information, hindering the model from better learning the shared semantics across modalities. To alleviate it, we propose Redundancy-aware Video-language Pre-training. We design a redundancy measurement of video patches and text tokens by calculating the cross-modal minimum dis-similarity. Then, we penalize the high-redundant video patches and text tokens through a proposed redundancy-aware contrastive learning. We evaluate our method on four benchmark datasets, MSRVTT, MSVD, DiDeMo, and LSMDC, achieving a significant improvement over the previous state-of-the-art results.</abstract>
      <url hash="c7864a2c">2022.findings-emnlp.221</url>
      <bibkey>wu-etal-2022-rap</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.221</doi>
    </paper>
    <paper id="222">
      <title><fixed-case>FCGCL</fixed-case>: Fine- and Coarse-Granularity Contrastive Learning for Speech Translation</title>
      <author><first>Hao</first><last>Zhang</last></author>
      <author><first>Nianwen</first><last>Si</last></author>
      <author><first>Yaqi</first><last>Chen</last></author>
      <author><first>Zhen</first><last>Li</last></author>
      <author><first>Tong</first><last>Niu</last></author>
      <author><first>Xukui</first><last>Yang</last></author>
      <author><first>Dan</first><last>Qu</last></author>
      <pages>3048-3059</pages>
      <abstract>It is notoriously difficult to implement end-to-end speech translation (E2E-ST) model because of the task complexity and data scarcity. Existing techniques often attempt to carry out implicit knowledge transfer from machine translation (MT) to ST model by imposing various constraints. However, in this transfer scenario, a significant problem is that the performance of the MT will drop significantly and the final transfer effect is also restricted. In this article, we recommend Fine and Coarse Granularity Contrastive Learning (FCGCL), which conduct explicit knowledge transfer from MT to ST model. Specially, we ensure through multi granularity contrastive learning that inputs with similar semantic between different modalities are encoded closely in the shared semantic space while inputs with different semantics are kept apart. Experiments on the MuST-C datasets on all 8 languages and further analysis show that our method can effectively improve the E2E-ST performance and achieves an average BLEU of 29.0.</abstract>
      <url hash="fdfa6152">2022.findings-emnlp.222</url>
      <bibkey>zhang-etal-2022-fcgcl</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.222</doi>
    </paper>
    <paper id="223">
      <title><fixed-case>I</fixed-case>nfo<fixed-case>CSE</fixed-case>: Information-aggregated Contrastive Learning of Sentence Embeddings</title>
      <author><first>Xing</first><last>Wu</last></author>
      <author><first>Chaochen</first><last>Gao</last></author>
      <author><first>Zijia</first><last>Lin</last></author>
      <author><first>Jizhong</first><last>Han</last></author>
      <author><first>Zhongyuan</first><last>Wang</last></author>
      <author><first>Songlin</first><last>Hu</last></author>
      <pages>3060-3070</pages>
      <abstract>Contrastive learning has been extensively studied in sentence embedding learning, which assumes that the embeddings of different views of the same sentence are closer. The constraint brought by this assumption is weak, and a good sentence representation should also be able to reconstruct the original sentence fragments. Therefore, this paper proposes an information-aggregated contrastive learning framework for learning unsupervised sentence embeddings, termed InfoCSE.InfoCSE forces the representation of [CLS] positions to aggregate denser sentence information by introducing an additional Masked language model task and a well-designed network. We evaluate the proposed InfoCSE on several benchmark datasets w.r.t the semantic text similarity (STS) task. Experimental results show that InfoCSE outperforms SimCSE by an average Spearman correlation of 2.60% on BERT-base, and 1.77% on BERT-large, achieving state-of-the-art results among unsupervised sentence representation learning methods.</abstract>
      <url hash="33839143">2022.findings-emnlp.223</url>
      <bibkey>wu-etal-2022-infocse</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.223</doi>
    </paper>
    <paper id="224">
      <title>Benchmarking Language Models for Code Syntax Understanding</title>
      <author><first>Da</first><last>Shen</last></author>
      <author><first>Xinyun</first><last>Chen</last></author>
      <author><first>Chenguang</first><last>Wang</last></author>
      <author><first>Koushik</first><last>Sen</last></author>
      <author><first>Dawn</first><last>Song</last></author>
      <pages>3071-3093</pages>
      <abstract>Pre-trained language models have demonstrated impressive performance in both natural language processing and program understanding, which represent the input as a token sequence without explicitly modeling its structure. Some prior works show that pre-trained language models can capture the syntactic rules of natural languages without finetuning on syntax understanding tasks. However, there is limited understanding of how well pre-trained models understand the code structure so far. In this work, we perform the first thorough benchmarking of the state-of-the-art pre-trained models for identifying the syntactic structures of programs. Specifically, we introduce CodeSyntax, a large-scale dataset of programs annotated with the syntactic relationships in their corresponding abstract syntax trees. Our key observation is that pre-training on massive code data does not result in decent code syntax understanding. In fact, these pre-trained programming language models fail to match the performance of naive baselines based on positional offsets and keywords. We also present a natural language benchmark to highlight the differences between natural languages and programming languages in terms of understanding corresponding syntactic structures. Our findings point out key limitations of existing pre-training methods and suggest the importance of modeling syntactic structures for the programming language.</abstract>
      <url hash="59c3c501">2022.findings-emnlp.224</url>
      <bibkey>shen-etal-2022-benchmarking</bibkey>
      <video href="2022.findings-emnlp.224.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.224</doi>
    </paper>
    <paper id="225">
      <title>Learning When and What to Quote: A Quotation Recommender System with Mutual Promotion of Recommendation and Generation</title>
      <author><first>Lingzhi</first><last>Wang</last></author>
      <author><first>Xingshan</first><last>Zeng</last></author>
      <author><first>Kam-Fai</first><last>Wong</last></author>
      <pages>3094-3105</pages>
      <abstract>This work extends the current quotation recommendation task to a more realistic quotation recommender system that learns to predict when to quote and what to quote jointly. The system consists of three modules (tasks), a prediction module to predict whether to quote given conversation contexts, a recommendation module to recommend suitable quotations and a generation module generating quotations or sentences in ordinary language to continue the conversation. We benchmark several competitive models for the two newly introduced tasks (i.e., when-to-quote and what-to-continue). For quotation recommendation, compared with previous work that is either generation-based or ranking-based recommendation, we propose a novel framework with mutual promotion of generation module and ranking-based recommendation module. Experiments show that our framework achieves significantly better performance than baselines on two datasets. Further experiments and analyses validate the effectiveness of the proposed mechanisms and get a better understanding of the quotation recommendation task.</abstract>
      <url hash="f388b642">2022.findings-emnlp.225</url>
      <bibkey>wang-etal-2022-learning-quote</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.225</doi>
    </paper>
    <paper id="226">
      <title>Think Beyond Words: Exploring Context-Relevant Visual Commonsense for Diverse Dialogue Generation</title>
      <author><first>Yiting</first><last>Liu</last></author>
      <author><first>Liang</first><last>Li</last></author>
      <author><first>Beichen</first><last>Zhang</last></author>
      <author><first>Qingming</first><last>Huang</last></author>
      <pages>3106-3117</pages>
      <abstract>Commonsense knowledge has been widely considered for building intelligent open-domain dialogue agents, aiming to generate meaningful and diverse responses. Previous works in this field usually lack the ability to effectively obtain and utilize auxiliary commonsense from the external visual world. In this paper, we argue that exploiting logical information in images related to context can be effective to enrich and steer the generation process. In view of this, we propose VICTOR, a context-relevant VIsual Commonsense enhanced dialogue generaTOR for generating coherent and informative responses. To obtain the associated visual commonsense, we devise a novel approach that expands topic words on the knowledge graph and maps them into daily scenarios. During the generation, the model adopts multimodal fusion mechanism to integrate visual and textual information, and adaptively combine their decoding distributions for better response generation. The experimental results on two public datasets show that our proposed method outperforms the latest competitive methods in terms of coherence and diversity.</abstract>
      <url hash="43ba2757">2022.findings-emnlp.226</url>
      <bibkey>liu-etal-2022-think</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.226</doi>
    </paper>
    <paper id="227">
      <title>Gender Bias in Meta-Embeddings</title>
      <author><first>Masahiro</first><last>Kaneko</last></author>
      <author><first>Danushka</first><last>Bollegala</last></author>
      <author><first>Naoaki</first><last>Okazaki</last></author>
      <pages>3118-3133</pages>
      <abstract>Different methods have been proposed to develop meta-embeddings from a given set of source embeddings. However, the source embeddings can contain unfair gender-related biases, and how these influence the meta-embeddings has not been studied yet. We study the gender bias in meta-embeddings created under three different settings:(1) meta-embedding multiple sources without performing any debiasing (Multi-Source No-Debiasing),(2) meta-embedding multiple sources debiased by a single method (Multi-Source Single-Debiasing), and(3) meta-embedding a single source debiased by different methods (Single-Source Multi-Debiasing).Our experimental results show that meta-embedding amplifies the gender biases compared to input source embeddings.We find that debiasing not only the sources but also their meta-embedding is needed to mitigate those biases. Moreover, we propose a novel debiasing method based on meta-embedding learning where we use multiple debiasing methods on a single source embedding and then create a single unbiased meta-embedding.</abstract>
      <url hash="3a1fd6fd">2022.findings-emnlp.227</url>
      <bibkey>kaneko-etal-2022-gender-bias</bibkey>
      <video href="2022.findings-emnlp.227.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.227</doi>
    </paper>
    <paper id="228">
      <title>Third-Party Aligner for Neural Word Alignments</title>
      <author><first>Jinpeng</first><last>Zhang</last></author>
      <author><first>Chuanqi</first><last>Dong</last></author>
      <author><first>Xiangyu</first><last>Duan</last></author>
      <author><first>Yuqi</first><last>Zhang</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <pages>3134-3145</pages>
      <abstract>Word alignment is to find translationally equivalent words between source and target sentences. Previous work has demonstrated that self-training can achieve competitive word alignment results. In this paper, we propose to use word alignments generated by a third-party word aligner to supervise the neural word alignment training. Specifically, source word and target word of each word pair aligned by the third-party aligner are trained to be close neighbors to each other in the contextualized embedding space when fine-tuning a pre-trained cross-lingual language model. Experiments on the benchmarks of various language pairs show that our approach can surprisingly do self-correction over the third-party supervision by finding more accurate word alignments and deleting wrong word alignments, leading to better performance than various third-party word aligners, including the currently best one. When we integrate all supervisions from various third-party aligners, we achieve state-of-the-art word alignment performances, with averagely more than two points lower alignment error rates than the best third-party aligner. We released our code at <url>https://github.com/sdongchuanqi/Third-Party-Supervised-Aligner</url>.</abstract>
      <url hash="9c79d943">2022.findings-emnlp.228</url>
      <bibkey>zhang-etal-2022-third</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.228</doi>
    </paper>
    <paper id="229">
      <title><fixed-case>Q</fixed-case>a<fixed-case>D</fixed-case>ial<fixed-case>M</fixed-case>o<fixed-case>E</fixed-case>: Question-answering Dialogue based Fact Verification with Mixture of Experts</title>
      <author><first>Longzheng</first><last>Wang</last></author>
      <author><first>Peng</first><last>Zhang</last></author>
      <author><first>Xiaoyu</first><last>Lu</last></author>
      <author><first>Lei</first><last>Zhang</last></author>
      <author><first>Chaoyang</first><last>Yan</last></author>
      <author><first>Chuang</first><last>Zhang</last></author>
      <pages>3146-3159</pages>
      <abstract>Fact verification is an essential tool to mitigate the spread of false information online, which has gained a widespread attention recently. However, a fact verification in the question-answering dialogue is still underexplored. In this paper, we propose a neural network based approach called question-answering dialogue based fact verification with mixture of experts (QaDialMoE). It exploits questions and evidence effectively in the verification process and can significantly improve the performance of fact verification. Specifically, we exploit the mixture of experts to focus on various interactions among responses, questions and evidence. A manager with an attention guidance module is implemented to guide the training of experts and assign a reasonable attention score to each expert. A prompt module is developed to generate synthetic questions that make our approach more generalizable. Finally, we evaluate the QaDialMoE and conduct a comparative study on three benchmark datasets. The experimental results demonstrate that our QaDialMoE outperforms previous approaches by a large margin and achieves new state-of-the-art results on all benchmarks. This includes the accuracy improvements on the HEALTHVER as 84.26%, the FAVIQ A dev set as 78.7%, the FAVIQ R dev set as 86.1%, test set as 86.0%, and the COLLOQUIAL as 89.5%. To our best knowledge, this is the first work to investigate a question-answering dialogue based fact verification, and achieves new state-of-the-art results on various benchmark datasets.</abstract>
      <url hash="40507e99">2022.findings-emnlp.229</url>
      <bibkey>wang-etal-2022-qadialmoe</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.229</doi>
    </paper>
    <paper id="230">
      <title>Multimodal Knowledge Learning for Named Entity Disambiguation</title>
      <author><first>Zhang</first><last>Dongjie</last></author>
      <author><first>Longtao</first><last>Huang</last></author>
      <pages>3160-3169</pages>
      <abstract>With the popularity of online social media, massive-scale multimodal information has brought new challenges to traditional Named Entity Disambiguation (NED) tasks. Recently, Multimodal Named Entity Disambiguation (MNED) has been proposed to link ambiguous mentions with the textual and visual contexts to a predefined knowledge graph. Existing attempts usually perform MNED by annotating multimodal mentions and adding multimodal features to traditional NED models. However, these studies may suffer from 1) failing to model multimodal information at the knowledge level, and 2) lacking multimodal annotation data against the large-scale unlabeled corpus. In this paper, we explore a pioneer study on leveraging multimodal knowledge learning to address the MNED task. Specifically, we first harvest multimodal knowledge in the Meta-Learning way, which is much easier than collecting ambiguous mention corpus. Then we design a knowledge-guided transfer learning strategy to extract unified representation from different modalities. Finally, we propose an Interactive Multimodal Learning Network (IMN) to fully utilize the multimodal information on both the mention and knowledge sides. Extensive experiments conducted on two public MNED datasets demonstrate that the proposed method achieves improvements over the state-of-the-art multimodal methods.</abstract>
      <url hash="08d5bdf7">2022.findings-emnlp.230</url>
      <bibkey>dongjie-huang-2022-multimodal</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.230</doi>
    </paper>
    <paper id="231">
      <title>Generative Prompt Tuning for Relation Classification</title>
      <author><first>Jiale</first><last>Han</last></author>
      <author><first>Shuai</first><last>Zhao</last></author>
      <author><first>Bo</first><last>Cheng</last></author>
      <author><first>Shengkun</first><last>Ma</last></author>
      <author><first>Wei</first><last>Lu</last></author>
      <pages>3170-3185</pages>
      <abstract>Using prompts to explore the knowledge contained within pre-trained language models for downstream tasks has now become an active topic. Current prompt tuning methods mostly convert the downstream tasks to masked language modeling problems by adding cloze-style phrases and mapping all labels to verbalizations with fixed length, which has proven effective for tasks with simple label spaces. However, when applied to relation classification exhibiting complex label spaces, vanilla prompt tuning methods may struggle with label verbalizations with arbitrary lengths due to rigid prompt restrictions. Inspired by the text infilling task for pre-training generative models that can flexibly predict missing spans, we propose a novel generative prompt tuning method to reformulate relation classification as an infilling problem, which frees our approach from limitations of current prompt based approaches and thus fully exploits rich semantics of entity and relation types. In addition, we design entity-guided decoding and discriminative relation scoring to generate and align relations effectively and efficiently during inference. Extensive experiments under fully supervised settings and low-resource settings demonstrate the effectiveness of our approach.</abstract>
      <url hash="3cd54f24">2022.findings-emnlp.231</url>
      <bibkey>han-etal-2022-generative</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.231</doi>
    </paper>
    <paper id="232">
      <title>Formulating Few-shot Fine-tuning Towards Language Model Pre-training: A Pilot Study on Named Entity Recognition</title>
      <author><first>Zihan</first><last>Wang</last></author>
      <author><first>Kewen</first><last>Zhao</last></author>
      <author><first>Zilong</first><last>Wang</last></author>
      <author><first>Jingbo</first><last>Shang</last></author>
      <pages>3186-3199</pages>
      <abstract>Fine-tuning pre-trained language models is a common practice in building NLP models for various tasks, including the case with less supervision. We argue that under the few-shot setting, formulating fine-tuning closer to the pre-training objective shall be able to unleash more benefits from the pre-trained language models. In this work, we take few-shot named entity recognition (NER) for a pilot study, where existing fine-tuning strategies are much different from pre-training. We propose a novel few-shot fine-tuning framework for NER, FFF-NER. Specifically, we introduce three new types of tokens, “is-entity”, “which-type” and “bracket”, so we can formulate the NER fine-tuning as (masked) token prediction or generation, depending on the choice of the pre-training objective. In our experiments, we apply to fine-tune both BERT and BART for few-shot NER on several benchmark datasets and observe significant improvements over existing fine-tuning strategies, including sequence labeling, prototype meta-learning, and prompt-based approaches. We further perform a series of ablation studies, showing few-shot NER performance is strongly correlated with the similarity between fine-tuning and pre-training.</abstract>
      <url hash="74f3a06c">2022.findings-emnlp.232</url>
      <bibkey>wang-etal-2022-formulating</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.232</doi>
    </paper>
    <paper id="233">
      <title>Masked Language Models Know Which are Popular: A Simple Ranking Strategy for Commonsense Question Answering</title>
      <author><first>Xuan</first><last>Luo</last></author>
      <author><first>Chuang</first><last>Fan</last></author>
      <author><first>Yice</first><last>Zhang</last></author>
      <author><first>Wanguo</first><last>Jiang</last></author>
      <author><first>Bing</first><last>Qin</last></author>
      <author><first>Ruifeng</first><last>Xu</last></author>
      <pages>3200-3213</pages>
      <abstract>We propose a simple ranking strategy to solve a generative commonsense question answering (QA) problem. Compared with multiple-choice QA, it is challenging because the answers to a question are not unique and they are supposed to be popular and diverse. Our strategy exploits the dataset itself and negative samples that we collect from WordNet to train a ranker that picks out the most popular answers for commonsense questions. The effectiveness of our strategy is verified on different pre-trained masked language models (MLMs) in a pipeline framework, where an MLM reranks the generated answers. Further, we explore an end-to-end framework where MLMs are utilized to guide the generation of generative language models (GLMs). Taking advantage of reinforcement learning, we apply policy gradient to train a GLM with the rewards fed back by an MLM. Empirical results on ProtoQA dataset demonstrate that MLMs can acquire the ability to distinguish the popular answers and improve the typical answer generation of GLMs as well.</abstract>
      <url hash="879bd4e0">2022.findings-emnlp.233</url>
      <bibkey>luo-etal-2022-masked</bibkey>
      <video href="2022.findings-emnlp.233.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.233</doi>
    </paper>
    <paper id="234">
      <title><fixed-case>D</fixed-case>ialog<fixed-case>USR</fixed-case>: Complex Dialogue Utterance Splitting and Reformulation for Multiple Intent Detection</title>
      <author><first>Haoran</first><last>Meng</last></author>
      <author><first>Zheng</first><last>Xin</last></author>
      <author><first>Tianyu</first><last>Liu</last></author>
      <author><first>Zizhen</first><last>Wang</last></author>
      <author><first>He</first><last>Feng</last></author>
      <author><first>Binghuai</first><last>Lin</last></author>
      <author><first>Xuemin</first><last>Zhao</last></author>
      <author><first>Yunbo</first><last>Cao</last></author>
      <author><first>Zhifang</first><last>Sui</last></author>
      <pages>3214-3229</pages>
      <abstract>While interacting with chatbots, users may elicit multiple intents in a single dialogue utterance. Instead of training a dedicated multi-intent detection model, we propose DialogUSR, a dialogue utterance splitting and reformulation task that first splits multi-intent user query into several single-intent sub-queries and then recovers all the coreferred and omitted information in the sub-queries. DialogUSR can serve as a plug-in and domain-agnostic module that empowers the multi-intent detection for the deployed chatbots with minimal efforts. We collect a high-quality naturally occurring dataset that covers 23 domains with a multi-step crowd-souring procedure. To benchmark the proposed dataset, we propose multiple action-based generative models that involve end-to-end and two-stage training, and conduct in-depth analyses on the pros and cons of the proposed baselines.</abstract>
      <url hash="232c04bd">2022.findings-emnlp.234</url>
      <bibkey>meng-etal-2022-dialogusr</bibkey>
      <video href="2022.findings-emnlp.234.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.234</doi>
    </paper>
    <paper id="235">
      <title>Low-resource Interactive Active Labeling for Fine-tuning Language Models</title>
      <author><first>Seiji</first><last>Maekawa</last></author>
      <author><first>Dan</first><last>Zhang</last></author>
      <author><first>Hannah</first><last>Kim</last></author>
      <author><first>Sajjadur</first><last>Rahman</last></author>
      <author><first>Estevam</first><last>Hruschka</last></author>
      <pages>3230-3242</pages>
      <abstract>Recently, active learning (AL) methods have been used to effectively fine-tune pre-trained language models for various NLP tasks such as sentiment analysis and document classification. However, given the task of fine-tuning language models, understanding the impact of different aspects on AL methods such as labeling cost, sample acquisition latency, and the diversity of the datasets necessitates a deeper investigation. This paper examines the performance of existing AL methods within a low-resource, interactive labeling setting. We observe that existing methods often underperform in such a setting while exhibiting higher latency and a lack of generalizability. To overcome these challenges, we propose a novel active learning method TYROUGE that employs a hybrid sampling strategy to minimize labeling cost and acquisition latency while providing a framework for adapting to dataset diversity via user guidance. Through our experiments, we observe that compared to SOTA methods, TYROUGE reduces the labeling cost by up to 43% and the acquisition latency by as much as 11X, while achieving comparable accuracy. Finally, we discuss the strengths and weaknesses of TYROUGE by exploring the impact of dataset characteristics.</abstract>
      <url hash="55a23379">2022.findings-emnlp.235</url>
      <bibkey>maekawa-etal-2022-low</bibkey>
      <video href="2022.findings-emnlp.235.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.235</doi>
    </paper>
    <paper id="236">
      <title>Getting the Most out of Simile Recognition</title>
      <author><first>Xiaoyue</first><last>Wang</last></author>
      <author><first>Linfeng</first><last>Song</last></author>
      <author><first>Xin</first><last>Liu</last></author>
      <author><first>Chulun</first><last>Zhou</last></author>
      <author><first>Hualin</first><last>Zeng</last></author>
      <author><first>Jinsong</first><last>Su</last></author>
      <pages>3243-3252</pages>
      <abstract>Simile recognition involves two subtasks: simile sentence classification that discriminates whether a sentence contains simile, and simile component extraction that locates the corresponding objects (i.e., tenors and vehicles).Recent work ignores features other than surface strings and suffers from the data hunger issue. We explore expressive features for this task to help achieve more effective data utilization. In particular, we study two types of features: 1) input-side features that include POS tags, dependency trees and word definitions, and 2) decoding features that capture the interdependence among various decoding decisions. We further construct a model named HGSR, which merges the input-side features as a heterogeneous graph and leverages decoding features via distillation. Experiments show that HGSR significantly outperforms the current state-of-the-art systems and carefully designed baselines, verifying the effectiveness of introduced features. We will release our code upon paper acceptance.</abstract>
      <url hash="750de3fe">2022.findings-emnlp.236</url>
      <bibkey>wang-etal-2022-getting</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.236</doi>
    </paper>
    <paper id="237">
      <title>A Unified Framework for Pun Generation with Humor Principles</title>
      <author><first>Yufei</first><last>Tian</last></author>
      <author><first>Divyanshu</first><last>Sheth</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <pages>3253-3261</pages>
      <abstract>We propose a unified framework to generate both homophonic and homographic puns to resolve the split-up in existing works. Specifically, we incorporate three linguistic attributes of puns to the language models: ambiguity, distinctiveness, and surprise. Our framework consists of three parts: 1) a context words/phrases selector to promote the aforementioned attributes, 2) a generation model trained on non-pun sentences to incorporate the context words/phrases into the generation output, and 3) a label predictor that learns the structure of puns which is used to steer the generation model at inference time. Evaluation results on both pun types demonstrate the efficacy of our model over strong baselines.</abstract>
      <url hash="553d6088">2022.findings-emnlp.237</url>
      <bibkey>tian-etal-2022-unified</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.237</doi>
    </paper>
    <paper id="238">
      <title>Improving <fixed-case>E</fixed-case>nglish-<fixed-case>A</fixed-case>rabic Transliteration with Phonemic Memories</title>
      <author><first>Yuanhe</first><last>Tian</last></author>
      <author><first>Renze</first><last>Lou</last></author>
      <author><first>Xiangyu</first><last>Pang</last></author>
      <author><first>Lianxi</first><last>Wang</last></author>
      <author><first>Shengyi</first><last>Jiang</last></author>
      <author><first>Yan</first><last>Song</last></author>
      <pages>3262-3272</pages>
      <abstract>Transliteration is an important task in natural language processing (NLP) which aims to convert a name in the source language to the target language without changing its pronunciation. Particularly, transliteration from English to Arabic is highly needed in many applications, especially in countries (e.g., United Arab Emirates (UAE)) whose most citizens are foreigners but the official language is Arabic. In such a task-oriented scenario, namely transliterating the English names to the corresponding Arabic ones, the performance of the transliteration model is highly important. However, most existing neural approaches mainly apply a universal transliteration model with advanced encoders and decoders to the task, where limited attention is paid to leveraging the phonemic association between English and Arabic to further improve model performance. In this paper, we focus on transliteration of people’s names from English to Arabic for the general public. In doing so, we collect a corpus named EANames by extracting high quality name pairs from online resources which better represent the names in the general public than linked Wikipedia entries that are always names of famous people). We propose a model for English-Arabic transliteration, where a memory module modeling the phonemic association between English and Arabic is used to guide the transliteration process. We run experiments on the collected data and the results demonstrate the effectiveness of our approach for English-Arabic transliteration.</abstract>
      <url hash="90517969">2022.findings-emnlp.238</url>
      <bibkey>tian-etal-2022-improving-english</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.238</doi>
    </paper>
    <paper id="239">
      <title>Mix-and-Match: Scalable Dialog Response Retrieval using <fixed-case>G</fixed-case>aussian Mixture Embeddings</title>
      <author><first>Gaurav</first><last>Pandey</last></author>
      <author><first>Danish</first><last>Contractor</last></author>
      <author><first>Sachindra</first><last>Joshi</last></author>
      <pages>3273-3287</pages>
      <abstract>Embedding-based approaches for dialog response retrieval embed the context-response pairs as points in the embedding space. These approaches are scalable, but fail to account for the complex, many-to-many relationships that exist between context-response pairs. On the other end of the spectrum, there are approaches that feed the context-response pairs jointly through multiple layers of neural networks. These approaches can model the complex relationships between context-response pairs, but fail to scale when the set of responses is moderately large (&gt;1000). In this paper, we propose a scalable model that can learn complex relationships between context-response pairs. Specifically, the model maps the contexts as well as responses to probability distributions over the embedding space. We train the models by optimizing the Kullback-Leibler divergence between the distributions induced by context-response pairs in the training data. We show that the resultant model achieves better performance as compared to other embedding-based approaches on publicly available conversation data.</abstract>
      <url hash="ff20b80b">2022.findings-emnlp.239</url>
      <bibkey>pandey-etal-2022-mix</bibkey>
      <video href="2022.findings-emnlp.239.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.239</doi>
    </paper>
    <paper id="240">
      <title><fixed-case>A</fixed-case>lpha<fixed-case>T</fixed-case>uning: Quantization-Aware Parameter-Efficient Adaptation of Large-Scale Pre-Trained Language Models</title>
      <author><first>Se Jung</first><last>Kwon</last></author>
      <author><first>Jeonghoon</first><last>Kim</last></author>
      <author><first>Jeongin</first><last>Bae</last></author>
      <author><first>Kang Min</first><last>Yoo</last></author>
      <author><first>Jin-Hwa</first><last>Kim</last></author>
      <author><first>Baeseong</first><last>Park</last></author>
      <author><first>Byeongwook</first><last>Kim</last></author>
      <author><first>Jung-Woo</first><last>Ha</last></author>
      <author><first>Nako</first><last>Sung</last></author>
      <author><first>Dongsoo</first><last>Lee</last></author>
      <pages>3288-3305</pages>
      <abstract>There are growing interests in adapting large-scale language models using parameter-efficient fine-tuning methods. However, accelerating the model itself and achieving better inference efficiency through model compression has not been thoroughly explored yet. Model compression could provide the benefits of reducing memory footprints, enabling low-precision computations, and ultimately achieving cost-effective inference. To combine parameter-efficient adaptation and model compression, we propose AlphaTuning consisting of post-training quantization of the pre-trained language model and fine-tuning only some parts of quantized parameters for a target task. Specifically, AlphaTuning works by employing binary-coding quantization, which factorizes the full-precision parameters into binary parameters and a separate set of scaling factors. During the adaptation phase, the binary values are frozen for all tasks, while the scaling factors are fine-tuned for the downstream task. We demonstrate that AlphaTuning, when applied to GPT-2 and OPT, performs competitively with full fine-tuning on a variety of downstream tasks while achieving &gt;10x compression ratio under 4-bit quantization and &gt;1,000x reduction in the number of trainable parameters.</abstract>
      <url hash="7f069b03">2022.findings-emnlp.240</url>
      <bibkey>kwon-etal-2022-alphatuning</bibkey>
      <video href="2022.findings-emnlp.240.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.240</doi>
    </paper>
    <paper id="241">
      <title>Learning Invariant Representation Improves Robustness for <fixed-case>MRC</fixed-case> Models</title>
      <author><first>Yu</first><last>Hai</last></author>
      <author><first>Liang</first><last>Wen</last></author>
      <author><first>Haoran</first><last>Meng</last></author>
      <author><first>Tianyu</first><last>Liu</last></author>
      <author><first>Houfeng</first><last>Wang</last></author>
      <pages>3306-3314</pages>
      <abstract>The prosperity of Pretrained Language Models(PLM) has greatly promoted the development of Machine Reading Comprehension (MRC). However, these models are vulnerable and not robust to adversarial examples. In this paper, we propose Stable and Contrastive Question Answering (SCQA) to improve invariance of representation to alleviate these robustness issues. Specifically, we first construct positive example pairs which have same answer through data augmentation. Then SCQA learns enhanced representations with better alignment between positive pairs by introducing stability and contrastive loss. Experimental results show that our approach can boost the robustness of QA models cross different MRC tasks and attack sets significantly and consistently.</abstract>
      <url hash="22ffec72">2022.findings-emnlp.241</url>
      <bibkey>hai-etal-2022-learning</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.241</doi>
    </paper>
    <paper id="242">
      <title><fixed-case>ER</fixed-case>-Test: Evaluating Explanation Regularization Methods for Language Models</title>
      <author><first>Brihi</first><last>Joshi</last></author>
      <author><first>Aaron</first><last>Chan</last></author>
      <author><first>Ziyi</first><last>Liu</last></author>
      <author><first>Shaoliang</first><last>Nie</last></author>
      <author><first>Maziar</first><last>Sanjabi</last></author>
      <author><first>Hamed</first><last>Firooz</last></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <pages>3315-3336</pages>
      <abstract>By explaining how humans would solve a given task, human rationales can provide strong learning signal for neural language models (NLMs). Explanation regularization (ER) aims to improve NLM generalization by pushing the NLM’s machine rationales (Which input tokens did the NLM focus on?) to align with human rationales (Which input tokens would humans focus on). Though prior works primarily study ER via in-distribution (ID) evaluation, out-of-distribution (OOD) generalization is often more critical in real-world scenarios, yet ER’s effect on OOD generalization has been underexplored.In this paper, we introduce ER-Test, a framework for evaluating ER models’ OOD generalization along three dimensions: unseen datasets, contrast set tests, and functional tests. Using ER-Test, we comprehensively analyze how ER models’ OOD generalization varies with the rationale alignment criterion (loss function), human rationale type (instance-level v/s task-level), number and choice of rationale-annotated instances, and time budget for rationale annotation. Across two tasks and six datasets, we show that ER has little impact on ID performance but yields large OOD performance gains, with the best ER criterion being task-dependent. Also, ER can improve OOD performance even with task-level or few human rationales. Finally, we find that rationale annotation is more time-efficient than label annotation for improving OOD performance. Our results with ER-Test help demonstrate ER’s utility and establish best practices for using ER effectively.</abstract>
      <url hash="1801974f">2022.findings-emnlp.242</url>
      <bibkey>joshi-etal-2022-er</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.242</doi>
    </paper>
    <paper id="243">
      <title>Learning Cooperative Interactions for Multi-Overlap Aspect Sentiment Triplet Extraction</title>
      <author><first>Shiman</first><last>Zhao</last></author>
      <author><first>Wei</first><last>Chen</last></author>
      <author><first>Tengjiao</first><last>Wang</last></author>
      <pages>3337-3347</pages>
      <abstract>Aspect sentiment triplet extraction (ASTE) is an essential task, which aims to extract triplets(aspect, opinion, sentiment). However, overlapped triplets, especially multi-overlap triplets,make ASTE a challenge. Most existing methods suffer from multi-overlap triplets becausethey focus on the single interactions between an aspect and an opinion. To solve the aboveissues, we propose a novel multi-overlap triplet extraction method, which decodes the complexrelations between multiple aspects and opinions by learning their cooperative interactions. Overall, the method is based on an encoder-decoder architecture. During decoding, we design ajoint decoding mechanism, which employs a multi-channel strategy to generate aspects andopinions through the cooperative interactions between them jointly. Furthermore, we constructa correlation-enhanced network to reinforce the interactions between related aspectsand opinions for sentiment prediction. Besides, a relation-wise calibration scheme is adoptedto further improve performance. Experiments show that our method outperforms baselines,especially multi-overlap triplets.</abstract>
      <url hash="0ffd62b5">2022.findings-emnlp.243</url>
      <bibkey>zhao-etal-2022-learning-cooperative</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.243</doi>
    </paper>
    <paper id="244">
      <title>Different Tunes Played with Equal Skill: Exploring a Unified Optimization Subspace for Parameter-Efficient Tuning</title>
      <author><first>Jing</first><last>Yi</last></author>
      <author><first>Weize</first><last>Chen</last></author>
      <author><first>Yujia</first><last>Qin</last></author>
      <author><first>Yankai</first><last>Lin</last></author>
      <author><first>Ning</first><last>Ding</last></author>
      <author><first>Xu</first><last>Han</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>3348-3366</pages>
      <abstract>Delta tuning (DET, also known as parameter-efficient tuning) is deemed as the new paradigm for using pre-trained language models (PLMs). Up to now, various DETs with distinct design elements have been proposed, achieving performance on par with fine-tuning. However, the mechanisms behind the above success are still under-explored, especially the connections among various DETs. To fathom the mystery, we hypothesize that the adaptations of different DETs could all be reparameterized as low-dimensional optimizations in a unified optimization subspace, which could be found by jointly decomposing independent solutions of different DETs. Then we explore the connections among different DETs by conducting optimization within the subspace. In experiments, we find that, for a certain DET, conducting optimization simply in the subspace could achieve comparable performance to its original space, and the found solution in the subspace could be transferred to another DET and achieve non-trivial performance. We also visualize the performance landscape of the subspace, and find that, there exists a substantial region where different DETs all perform well. Finally, we extend our analysis and show the strong connections between fine-tuning and DETs. The codes are publicly available at <url>https://github.com/thunlp/Unified-DeltaTuning</url>.</abstract>
      <url hash="8dcce807">2022.findings-emnlp.244</url>
      <bibkey>yi-etal-2022-different</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.244</doi>
    </paper>
    <paper id="245">
      <title>Explainable Slot Type Attentions to Improve Joint Intent Detection and Slot Filling</title>
      <author><first>Kalpa</first><last>Gunaratna</last></author>
      <author><first>Vijay</first><last>Srinivasan</last></author>
      <author><first>Akhila</first><last>Yerukola</last></author>
      <author><first>Hongxia</first><last>Jin</last></author>
      <pages>3367-3378</pages>
      <abstract>Joint intent detection and slot filling is a key research topic in natural language understanding (NLU). Existing joint intent and slot filling systems analyze and compute features collectively for all slot types, and importantly, have no way to explain the slot filling model decisions. In this work, we propose a novel approach that: (i) learns to generate additional slot type specific features in order to improve accuracy and (ii) provides explanations for slot filling decisions for the first time in a joint NLU model. We perform an additional constrained supervision using a set of binary classifiers for the slot type specific feature learning, thus ensuring appropriate attention weights are learned in the process to explain slot filling decisions for utterances. Our model is inherently explainable and does not need any post-hoc processing. We evaluate our approach on two widely used datasets and show accuracy improvements. Moreover, a detailed analysis is also provided for the exclusive slot explainability.</abstract>
      <url hash="97a3da2a">2022.findings-emnlp.245</url>
      <bibkey>gunaratna-etal-2022-explainable</bibkey>
      <video href="2022.findings-emnlp.245.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.245</doi>
    </paper>
    <paper id="246">
      <title><fixed-case>P</fixed-case>seudo<fixed-case>R</fixed-case>easoner: Leveraging Pseudo Labels for Commonsense Knowledge Base Population</title>
      <author><first>Tianqing</first><last>Fang</last></author>
      <author><first>Quyet V.</first><last>Do</last></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Yangqiu</first><last>Song</last></author>
      <author><first>Ginny Y.</first><last>Wong</last></author>
      <author><first>Simon</first><last>See</last></author>
      <pages>3379-3394</pages>
      <abstract>Commonsense Knowledge Base (CSKB) Population aims at reasoning over unseen entities and assertions on CSKBs, and is an important yet hard commonsense reasoning task. One challenge is that it requires out-of-domain generalization ability as the source CSKB for training is of a relatively smaller scale (1M) while the whole candidate space for population is way larger (200M). We propose PseudoReasoner, a semi-supervised learning framework for CSKB population that uses a teacher model pre-trained on CSKBs to provide pseudo labels on the unlabeled candidate dataset for a student model to learn from. The teacher can be a generative model rather than restricted to discriminative models as previous works. In addition, we design a new filtering procedure for pseudo labels based on influence function and the student model’s prediction to further improve the performance. The framework can improve the backbone model KG-BERT (RoBERTa-large) by 3.3 points on the overall performance and especially, 5.3 points on the out-of-domain performance, and achieves the state-of-the-art. The codes will be made public on acceptance. Codes and data are available at <url>https://github.com/HKUST-KnowComp/PseudoReasoner</url>.</abstract>
      <url hash="6682b72a">2022.findings-emnlp.246</url>
      <bibkey>fang-etal-2022-pseudoreasoner</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.246</doi>
    </paper>
    <paper id="247">
      <title>History-Aware Hierarchical Transformer for Multi-session Open-domain Dialogue System</title>
      <author><first>Tong</first><last>Zhang</last></author>
      <author><first>Yong</first><last>Liu</last></author>
      <author><first>Boyang</first><last>Li</last></author>
      <author><first>Zhiwei</first><last>Zeng</last></author>
      <author><first>Pengwei</first><last>Wang</last></author>
      <author><first>Yuan</first><last>You</last></author>
      <author><first>Chunyan</first><last>Miao</last></author>
      <author><first>Lizhen</first><last>Cui</last></author>
      <pages>3395-3407</pages>
      <abstract>With the evolution of pre-trained language models, current open-domain dialogue systems have achieved great progress in conducting one-session conversations. In contrast, Multi-Session Conversation (MSC), which consists of multiple sessions over a long term with the same user, is under-investigated. In this paper, we propose History-Aware Hierarchical Transformer (HAHT) for multi-session open-domain dialogue. HAHT maintains a long-term memory of history conversations and utilizes history information to understand current conversation context and generate well-informed and context-relevant responses. Specifically, HAHT first encodes history conversation sessions hierarchically into a history memory. Then, HAHT leverages historical information to facilitate the understanding of the current conversation context by encoding the history memory together with the current context with attention-based mechanisms. Finally, to explicitly utilize historical information, HAHT uses a history-aware response generator that switches between a generic vocabulary and a history-aware vocabulary. Experimental results on a large-scale MSC dataset suggest that the proposed HAHT model consistently outperforms baseline models. Human evaluation results support that HAHT generates more human-like, context-relevant, and history-relevant responses than baseline models.</abstract>
      <url hash="a52f5398">2022.findings-emnlp.247</url>
      <bibkey>zhang-etal-2022-history</bibkey>
      <video href="2022.findings-emnlp.247.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.247</doi>
    </paper>
    <paper id="248">
      <title>Guiding Abstractive Dialogue Summarization with Content Planning</title>
      <author><first>Ye</first><last>Wang</last></author>
      <author><first>Xiaojun</first><last>Wan</last></author>
      <author><first>Zhiping</first><last>Cai</last></author>
      <pages>3408-3413</pages>
      <abstract>Abstractive dialogue summarization has recently been receiving more attention. We propose a coarse-to-fine model for generating abstractive dialogue summaries, and introduce a fact-aware reinforcement learning (RL) objective that improves the fact consistency between the dialogue and the generated summary. Initially, the model generates the predicate-argument spans of the dialogue, and then generates the final summary through a fact-aware RL objective. Extensive experiments and analysis on two benchmark datasets demonstrate that our proposed method effectively improves the quality of the generated summary, especially in coherence and consistency.</abstract>
      <url hash="51131456">2022.findings-emnlp.248</url>
      <bibkey>wang-etal-2022-guiding</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.248</doi>
    </paper>
    <paper id="249">
      <title>Truncation Sampling as Language Model Desmoothing</title>
      <author><first>John</first><last>Hewitt</last></author>
      <author><first>Christopher</first><last>Manning</last></author>
      <author><first>Percy</first><last>Liang</last></author>
      <pages>3414-3427</pages>
      <abstract>Long samples of text from neural language models can be of poor quality. Truncation sampling algorithms–like top-p or top-k—address this by setting some words’ probabilities to zero at each step. This work investigates why these methods are important, and how to improve them. We propose thinking of a neural language model as a mixture of a true distribution and a smoothing distribution that avoids infinite perplexity. In this light, truncation algorithms aim to perform desmoothing, estimating a subset of the support of the true distribution. Finding a good subset is crucial: we show that top-p unnecessarily truncates high-probability words, for example causing it to truncate all words but Trump for a document that starts with Donald. We introduce eta-sampling, which truncates words below an entropy-dependent probability threshold. Compared to previous algorithms, our eta-sampling generates more plausible long documents according to humans, is better at breaking out of repetition, and behaves more reasonably on a battery of test distributions.</abstract>
      <url hash="6a2dc1ac">2022.findings-emnlp.249</url>
      <bibkey>hewitt-etal-2022-truncation</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.249</doi>
    </paper>
    <paper id="250">
      <title>Knowledge-grounded Dialog State Tracking</title>
      <author><first>Dian</first><last>Yu</last></author>
      <author><first>Mingqiu</first><last>Wang</last></author>
      <author><first>Yuan</first><last>Cao</last></author>
      <author><first>Laurent</first><last>El Shafey</last></author>
      <author><first>Izhak</first><last>Shafran</last></author>
      <author><first>Hagen</first><last>Soltau</last></author>
      <pages>3428-3435</pages>
      <abstract>Knowledge (including structured knowledge such as schema and ontology and unstructured knowledge such as web corpus) is a critical part of dialog understanding, especially for unseen tasks and domains. Traditionally, such domain-specific knowledge is encoded implicitly into model parameters for the execution of downstream tasks, which makes training inefficient. In addition , such models are not easily transferable to new tasks with different schemas. In this work, we propose to perform dialog state tracking grounded on knowledge encoded externally. We query relevant knowledge of various forms based on the dialog context where such information can grounds the prediction of dialog states. We demonstrate superior performance of our proposed method over strong baselines, especially in the few-shot learning setting.</abstract>
      <url hash="6d61635a">2022.findings-emnlp.250</url>
      <bibkey>yu-etal-2022-knowledge</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.250</doi>
    </paper>
    <paper id="251">
      <title>Context-aware Information-theoretic Causal De-biasing for Interactive Sequence Labeling</title>
      <author><first>Junda</first><last>Wu</last></author>
      <author><first>Rui</first><last>Wang</last></author>
      <author><first>Tong</first><last>Yu</last></author>
      <author><first>Ruiyi</first><last>Zhang</last></author>
      <author><first>Handong</first><last>Zhao</last></author>
      <author><first>Shuai</first><last>Li</last></author>
      <author><first>Ricardo</first><last>Henao</last></author>
      <author><first>Ani</first><last>Nenkova</last></author>
      <pages>3436-3448</pages>
      <abstract>Supervised training of existing deep learning models for sequence labeling relies on large scale labeled datasets. Such datasets are generally created with crowd-source labeling. However, crowd-source labeling for tasks of sequence labeling can be expensive and time-consuming. Further, crowd-source labeling by external annotators may not be appropriate for data that contains user private information. Considering the above limitations of crowd-source labeling, we study interactive sequence labeling that allows training directly with the user feedback, which alleviates the annotation cost and maintains the user privacy. We identify two bias, namely, context bias and feedback bias, by formulating interactive sequence labeling via a Structural Causal Model (SCM). To alleviate the context and feedback bias based on the SCM, we identify the frequent context tokens as confounders in the backdoor adjustment and further propose an entropy-based modulation that is inspired by information theory. entities more sample-efficiently. With extensive experiments, we validate that our approach can effectively alleviate the biases and our models can be efficiently learnt with the user feedback.</abstract>
      <url hash="8240706e">2022.findings-emnlp.251</url>
      <bibkey>wu-etal-2022-context</bibkey>
      <video href="2022.findings-emnlp.251.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.251</doi>
    </paper>
    <paper id="252">
      <title>Simple but Challenging: Natural Language Inference Models Fail on Simple Sentences</title>
      <author><first>Cheng</first><last>Luo</last></author>
      <author><first>Wei</first><last>Liu</last></author>
      <author><first>Jieyu</first><last>Lin</last></author>
      <author><first>Jiajie</first><last>Zou</last></author>
      <author><first>Ming</first><last>Xiang</last></author>
      <author><first>Nai</first><last>Ding</last></author>
      <pages>3449-3462</pages>
      <abstract>Natural language inference (NLI) is a task to infer the relationship between a premise and a hypothesis (e.g., entailment, neutral, or contradiction), and transformer-based models perform well on current NLI datasets such as MNLI and SNLI. Nevertheless, given the linguistic complexity of the large-scale datasets, it remains controversial whether these models can truly infer the relationship between sentences or they simply guess the answer via shallow heuristics. Here, we introduce a controlled evaluation set called Simple Pair to test the basic sentence inference ability of NLI models using sentences with syntactically simple structures. Three popular transformer-based models, i.e., BERT, RoBERTa, and DeBERTa, are employed. We find that these models fine-tuned on MNLI or SNLI perform very poorly on Simple Pair (&lt; 35.4% accuracy). Further analyses reveal event coreference and compositional binding problems in these models. To improve the model performance, we augment the training set, i.e., MNLI or SNLI, with a few examples constructed based on Simple Pair ( 1% of the size of the original SNLI/MNLI training sets). Models fine-tuned on the augmented training set maintain high performance on MNLI/SNLI and perform very well on Simple Pair (~100% accuracy). Furthermore, the positive performance of the augmented training models can transfer to more complex examples constructed based on sentences from MNLI and SNLI. Taken together, the current work shows that (1) models achieving high accuracy on mainstream large-scale datasets still lack the capacity to draw accurate inferences on simple sentences, and (2) augmenting mainstream datasets with a small number of target simple sentences can effectively improve model performance.</abstract>
      <url hash="ea454bbe">2022.findings-emnlp.252</url>
      <bibkey>luo-etal-2022-simple-challenging</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.252</doi>
    </paper>
    <paper id="253">
      <title><fixed-case>DORE</fixed-case>: Document Ordered Relation Extraction based on Generative Framework</title>
      <author><first>Qipeng</first><last>Guo</last></author>
      <author><first>Yuqing</first><last>Yang</last></author>
      <author><first>Hang</first><last>Yan</last></author>
      <author><first>Xipeng</first><last>Qiu</last></author>
      <author><first>Zheng</first><last>Zhang</last></author>
      <pages>3463-3474</pages>
      <abstract>In recent years, there is a surge of generation-based information extraction work, which allows a more direct use of pre-trained language models and efficiently captures output dependencies. However, previous generative methods using lexical representation do not naturally fit document-level relation extraction (DocRE) where there are multiple entities and relational facts. In this paper, we investigate the root cause of the underwhelming performance of the existing generative DocRE models and discover that the culprit is the inadequacy of the training paradigm, instead of the capacities of the models. We propose to generate a symbolic and ordered sequence from the relation matrix which is deterministic and easier for model to learn. Moreover, we design a parallel row generation method to process overlong target sequences. Besides, we introduce several negative sampling strategies to improve the performance with balanced signals. Experimental results on four datasets show that our proposed method can improve the performance of the generative DocRE models.</abstract>
      <url hash="bc3d6e6e">2022.findings-emnlp.253</url>
      <bibkey>guo-etal-2022-dore</bibkey>
      <video href="2022.findings-emnlp.253.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.253</doi>
    </paper>
    <paper id="254">
      <title>Explicit Role Interaction Network for Event Argument Extraction</title>
      <author><first>Nan</first><last>Ding</last></author>
      <author><first>Chunming</first><last>Hu</last></author>
      <author><first>Kai</first><last>Sun</last></author>
      <author><first>Samuel</first><last>Mensah</last></author>
      <author><first>Richong</first><last>Zhang</last></author>
      <pages>3475-3485</pages>
      <abstract>Event argument extraction is a challenging subtask of event extraction, aiming to identify and assign roles to arguments under a certain event. Existing methods extract arguments of each role independently, ignoring the relationship between different roles. Such an approach hinders the model from learning explicit interactions between different roles to improve the performance of individual argument extraction. As a solution, we design a neural model that we refer to as the Explicit Role Interaction Network (ERIN) which allows for dynamically capturing the correlations between different argument roles within an event. Extensive experiments on the benchmark dataset ACE2005 demonstrate the superiority of our proposed model to existing approaches.</abstract>
      <url hash="4efa9e50">2022.findings-emnlp.254</url>
      <bibkey>ding-etal-2022-explicit</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.254</doi>
    </paper>
    <paper id="255">
      <title>Few-Shot Out-of-Domain Transfer Learning of Natural Language Explanations in a Label-Abundant Setup</title>
      <author><first>Yordan</first><last>Yordanov</last></author>
      <author><first>Vid</first><last>Kocijan</last></author>
      <author><first>Thomas</first><last>Lukasiewicz</last></author>
      <author><first>Oana-Maria</first><last>Camburu</last></author>
      <pages>3486-3501</pages>
      <abstract>Training a model to provide natural language explanations (NLEs) for its predictions usually requires the acquisition of task-specific NLEs, which is time- and resource-consuming. A potential solution is the few-shot out-of-domain transfer of NLEs from a parent task with many NLEs to a child task. In this work, we examine the setup in which the child task has few NLEs but abundant labels. We establish four few-shot transfer learning methods that cover the possible fine-tuning combinations of the labels and NLEs for the parent and child tasks. We transfer explainability from a large natural language inference dataset (e-SNLI) separately to two child tasks: (1) hard cases of pronoun resolution, where we introduce the small-e-WinoGrande dataset of NLEs on top of the WinoGrande dataset, and (2) commonsense validation (ComVE). Our results demonstrate that the parent task helps with NLE generation and we establish the best methods for this setup.</abstract>
      <url hash="d64291bd">2022.findings-emnlp.255</url>
      <bibkey>yordanov-etal-2022-shot</bibkey>
      <video href="2022.findings-emnlp.255.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.255</doi>
    </paper>
    <paper id="256">
      <title><fixed-case>R</fixed-case>o<fixed-case>C</fixed-case>h<fixed-case>B</fixed-case>ert: Towards Robust <fixed-case>BERT</fixed-case> Fine-tuning for <fixed-case>C</fixed-case>hinese</title>
      <author><first>Zihan</first><last>Zhang</last></author>
      <author><first>Jinfeng</first><last>Li</last></author>
      <author><first>Ning</first><last>Shi</last></author>
      <author><first>Bo</first><last>Yuan</last></author>
      <author><first>Xiangyu</first><last>Liu</last></author>
      <author><first>Rong</first><last>Zhang</last></author>
      <author><first>Hui</first><last>Xue</last></author>
      <author><first>Donghong</first><last>Sun</last></author>
      <author><first>Chao</first><last>Zhang</last></author>
      <pages>3502-3516</pages>
      <abstract>Despite of the superb performance on a wide range of tasks, pre-trained language models (e.g., BERT) have been proved vulnerable to adversarial texts. In this paper, we present RoChBERT, a framework to build more Robust BERT-based models by utilizing a more comprehensive adversarial graph to fuse Chinese phonetic and glyph features into pre-trained representations during fine-tuning. Inspired by curriculum learning, we further propose to augment the training dataset with adversarial texts in combination with intermediate samples. Extensive experiments demonstrate that RoChBERT outperforms previous methods in significant ways: (i) robust – RoChBERT greatly improves the model robustness without sacrificing accuracy on benign texts. Specifically, the defense lowers the success rates of unlimited and limited attacks by 59.43% and 39.33% respectively, while remaining accuracy of 93.30%; (ii) flexible – RoChBERT can easily extend to various language models to solve different downstream tasks with excellent performance; and (iii) efficient – RoChBERT can be directly applied to the fine-tuning stage without pre-training language model from scratch, and the proposed data augmentation method is also low-cost.</abstract>
      <url hash="caaae904">2022.findings-emnlp.256</url>
      <bibkey>zhang-etal-2022-rochbert</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.256</doi>
    </paper>
    <paper id="257">
      <title>Lexical Entailment with Hierarchy Representations by Deep Metric Learning</title>
      <author><first>Naomi</first><last>Sato</last></author>
      <author><first>Masaru</first><last>Isonuma</last></author>
      <author><first>Kimitaka</first><last>Asatani</last></author>
      <author><first>Shoya</first><last>Ishizuka</last></author>
      <author><first>Aori</first><last>Shimizu</last></author>
      <author><first>Ichiro</first><last>Sakata</last></author>
      <pages>3517-3522</pages>
      <abstract>In this paper, we introduce a novel method for lexical entailment tasks, which detects a hyponym-hypernym relation among words. Existing lexical entailment studies are lacking in generalization performance, as they cannot be applied to words that are not included in the training dataset. Moreover, existing work evaluates the performance by using the dataset that contains words used for training. This study proposes a method that learns a mapping from word embeddings to the hierarchical embeddings in order to predict the hypernymy relations of any input words. To validate the generalization performance, we conduct experiments using a train dataset that does not overlap with the evaluation dataset. As a result, our method achieved state-of-the-art performance and showed robustness for unknown words.</abstract>
      <url hash="145820c7">2022.findings-emnlp.257</url>
      <bibkey>sato-etal-2022-lexical</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.257</doi>
    </paper>
    <paper id="258">
      <title>Improving the Sample Efficiency of Prompt Tuning with Domain Adaptation</title>
      <author><first>Xu</first><last>Guo</last></author>
      <author><first>Boyang</first><last>Li</last></author>
      <author><first>Han</first><last>Yu</last></author>
      <pages>3523-3537</pages>
      <abstract>Prompt tuning, or the conditioning of a frozen pretrained language model (PLM) with soft prompts learned from data, has demonstrated impressive performance on a wide range of NLP tasks. However, prompt tuning requires a large training dataset to be effective and is outperformed by finetuning the entire PLM in data-scarce regimes. Previous work (Gu et al., 2022, Vu et al., 2022) proposed to transfer soft prompts pretrained on the source domain to the target domain. In this paper, we explore domain adaptation for prompt tuning, a problem setting where unlabeled data from the target domain are available during pretraining. We propose bOosting Prompt TunIng with doMain Adaptation (OPTIMA), which regularizes the decision boundary to be smooth around regions where source and target data distributions are similar. Extensive experiments demonstrate that OPTIMA significantly enhances the transferability and sample-efficiency of prompt tuning compared to strong baselines. Moreover, in few-shot settings, OPTIMA exceeds full-model tuning by a large margin.</abstract>
      <url hash="c207b8f6">2022.findings-emnlp.258</url>
      <bibkey>guo-etal-2022-improving</bibkey>
      <video href="2022.findings-emnlp.258.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.258</doi>
    </paper>
    <paper id="259">
      <title><fixed-case>M</fixed-case>c<fixed-case>P</fixed-case>hra<fixed-case>S</fixed-case>y: Multi-Context Phrase Similarity and Clustering</title>
      <author><first>Amir</first><last>Cohen</last></author>
      <author><first>Hila</first><last>Gonen</last></author>
      <author><first>Ori</first><last>Shapira</last></author>
      <author><first>Ran</first><last>Levy</last></author>
      <author><first>Yoav</first><last>Goldberg</last></author>
      <pages>3538-3550</pages>
      <abstract>Phrase similarity is a key component of many NLP applications. Current phrase similarity methods focus on embedding the phrase itself and use the phrase context only during training of the pretrained model. To better leverage the information in the context, we propose McPhraSy (Multi-context Phrase Similarity), a novel algorithm for estimating the similarity of phrases based on multiple contexts. At inference time, McPhraSy represents each phrase by considering multiple contexts in which it appears and computes the similarity of two phrases by aggregating the pairwise similarities between the contexts of the phrases. Incorporating context during inference enables McPhraSy to outperform current state-of-the-art models on two phrase similarity datasets by up to 13.3%. Finally, we also present a new downstream task that relies on phrase similarity – keyphrase clustering – and create a new benchmark for it in the product reviews domain. We show that McPhraSy surpasses all other baselines for this task.</abstract>
      <url hash="fb79109f">2022.findings-emnlp.259</url>
      <bibkey>cohen-etal-2022-mcphrasy</bibkey>
      <video href="2022.findings-emnlp.259.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.259</doi>
    </paper>
    <paper id="260">
      <title><fixed-case>CAN</fixed-case>ar<fixed-case>E</fixed-case>x: Contextually Aware Narrative Extraction for Semantically Rich Text-as-data Applications</title>
      <author><first>Nandini</first><last>Anantharama</last></author>
      <author><first>Simon</first><last>Angus</last></author>
      <author><first>Lachlan</first><last>O’Neill</last></author>
      <pages>3551-3564</pages>
      <abstract>Narrative modelling is an area of active research, motivated by the acknowledgement of narratives as drivers of societal decision making. These research efforts conceptualize narratives as connected entity chains, and modeling typically focuses on the identification of entities and their connections within a text. An emerging approach to narrative modelling is the use of semantic role labeling (SRL) to extract Entity-Verb-Entity (E-V-Es) tuples from a text, followed by dimensionality reduction to reduce the space of entities and connections separately. This process penalises the semantic richness of narratives and discards much contextual information along the way. Here, we propose an alternate narrative extraction approach - CANarEx, incorporating a pipeline of common contextual constructs through co-reference resolution, micro-narrative generation and clustering of these narratives through sentence embeddings. We evaluate our approach through testing the recovery of “narrative time-series clusters”, mimicking a desirable text-as-data task. The evaluation framework leverages synthetic data generated using a GPT-3 model. The GPT-3 model is trained to generate similar sentences using a large dataset of news articles. The synthetic data maps to three topics in the news dataset. We then generate narrative time-series document cluster representations by mapping the synthetic data to three distinct signals synthetically injected into the testing corpus. Evaluation results demonstrate the superior ability of CANarEx to recover narrative time-series through reduced MSE and improved precision/recall relative to existing methods. The validity is further reinforced through ablation studies and qualitative analysis.</abstract>
      <url hash="33bb49db">2022.findings-emnlp.260</url>
      <bibkey>anantharama-etal-2022-canarex</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.260</doi>
    </paper>
    <paper id="261">
      <title>Narrate Dialogues for Better Summarization</title>
      <author><first>Ruochen</first><last>Xu</last></author>
      <author><first>Chenguang</first><last>Zhu</last></author>
      <author><first>Michael</first><last>Zeng</last></author>
      <pages>3565-3575</pages>
      <abstract>Dialogue summarization models aim to generate a concise and accurate summary for multi-party dialogue. The complexity of dialogue, including coreference, dialogue acts, and inter-speaker interactions bring unique challenges to dialogue summarization. Most recent neural models achieve state-of-art performance following the pretrain-then-finetune recipe, where the large-scale language model (LLM) is pretrained on large-scale single-speaker written text, but later finetuned on multi-speaker dialogue text. To mitigate the gap between pretraining and finetuning, we propose several approaches to convert the dialogue into a third-person narrative style and show that the narration serves as a valuable annotation for LLMs. Empirical results on three benchmark datasets show our simple approach achieves higher scores on the ROUGE and a factual correctness metric.</abstract>
      <url hash="ce7496c2">2022.findings-emnlp.261</url>
      <bibkey>xu-etal-2022-narrate</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.261</doi>
    </paper>
    <paper id="262">
      <title>Towards Identifying Social Bias in Dialog Systems: Framework, Dataset, and Benchmark</title>
      <author><first>Jingyan</first><last>Zhou</last></author>
      <author><first>Jiawen</first><last>Deng</last></author>
      <author><first>Fei</first><last>Mi</last></author>
      <author><first>Yitong</first><last>Li</last></author>
      <author><first>Yasheng</first><last>Wang</last></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <author><first>Xin</first><last>Jiang</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <author><first>Helen</first><last>Meng</last></author>
      <pages>3576-3591</pages>
      <abstract>Among all the safety concerns that hinder the deployment of open-domain dialog systems (e.g., offensive languages, biases, and toxic behaviors), social bias presents an insidious challenge. Addressing this challenge requires rigorous analyses and normative reasoning. In this paper, we focus our investigation on social bias measurement to facilitate the development of unbiased dialog systems. We first propose a novel Dial-Bias Framework for analyzing the social bias in conversations using a holistic method beyond bias lexicons or dichotomous annotations. Leveraging the proposed framework, we further introduce the CDial-Bias Dataset which is, to the best of our knowledge, the first annotated Chinese social bias dialog dataset. We also establish a fine-grained dialog bias measurement benchmark and conduct in-depth ablation studies to shed light on the utility of the detailed annotations in the proposed dataset. Finally, we evaluate representative Chinese generative models with our classifiers to unveil the presence of social bias in these systems.</abstract>
      <url hash="aec4224c">2022.findings-emnlp.262</url>
      <bibkey>zhou-etal-2022-towards-identifying</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.262</doi>
    </paper>
    <paper id="263">
      <title><fixed-case>C</fixed-case>ross<fixed-case>RE</fixed-case>: A Cross-Domain Dataset for Relation Extraction</title>
      <author><first>Elisa</first><last>Bassignana</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <pages>3592-3604</pages>
      <abstract>Relation Extraction (RE) has attracted increasing attention, but current RE evaluation is limited to in-domain evaluation setups. Little is known on how well a RE system fares in challenging, but realistic out-of-distribution evaluation setups. To address this gap, we propose CrossRE, a new, freely-available cross-domain benchmark for RE, which comprises six distinct text domains and includes multi-label annotations. An additional innovation is that we release meta-data collected during annotation, to include explanations and flags of difficult instances. We provide an empirical evaluation with a state-of-the-art model for relation classification. As the meta-data enables us to shed new light on the state-of-the-art model, we provide a comprehensive analysis on the impact of difficult cases and find correlations between model and human annotations. Overall, our empirical investigation highlights the difficulty of cross-domain RE. We release our dataset, to spur more research in this direction.</abstract>
      <url hash="63f4009e">2022.findings-emnlp.263</url>
      <bibkey>bassignana-plank-2022-crossre</bibkey>
      <video href="2022.findings-emnlp.263.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.263</doi>
    </paper>
    <paper id="264">
      <title>Probing Structural Knowledge from Pre-trained Language Model for Argumentation Relation Classification</title>
      <author><first>Yang</first><last>Sun</last></author>
      <author><first>Bin</first><last>Liang</last></author>
      <author><first>Jianzhu</first><last>Bao</last></author>
      <author><first>Min</first><last>Yang</last></author>
      <author><first>Ruifeng</first><last>Xu</last></author>
      <pages>3605-3615</pages>
      <abstract>Extracting fine-grained structural information between argumentation component (AC) pairs is essential for argumentation relation classification (ARC). However, most previous studies attempt to model the relationship between AC pairs using AC level similarity or semantically relevant features. They ignore the complex interaction between AC pairs and cannot effectively reason the argumentation relation deeply. Therefore, in this paper, we propose a novel dual prior graph neural network (DPGNN) to jointly explore the probing knowledge derived from pre-trained language models (PLMs) and the syntactical information for comprehensively modeling the relationship between AC pairs. Specifically, we construct a probing graph by using probing knowledge derived from PLMs to recognize and align the relational information within and across the argumentation components. In addition, we propose a mutual dependency graph for the AC pair to reason the fine-grained syntactic structural information, in which the syntactical correlation between words is set by the dependency information within AC and mutual attention mechanism across ACs. The knowledge learned from the probing graph and the dependency graph are combined to comprehensively capture the aligned relationships of AC pairs for improving the results of ARC. Experimental results on three public datasets show that DPGNN outperforms the state-of-the-art baselines by a noticeable margin.</abstract>
      <url hash="61c3d464">2022.findings-emnlp.264</url>
      <bibkey>sun-etal-2022-probing</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.264</doi>
    </paper>
    <paper id="265">
      <title><fixed-case>L</fixed-case>ogic<fixed-case>NMR</fixed-case>: Probing the Non-monotonic Reasoning Ability of Pre-trained Language Models</title>
      <author><first>Yeliang</first><last>Xiu</last></author>
      <author><first>Zhanhao</first><last>Xiao</last></author>
      <author><first>Yongmei</first><last>Liu</last></author>
      <pages>3616-3626</pages>
      <abstract>The logical reasoning capabilities of pre-trained language models have recently received much attention. As one of the vital reasoning paradigms, non-monotonic reasoning refers to the fact that conclusions may be invalidated with new information. Existing work has constructed a non-monotonic inference dataset <tex-math>\delta</tex-math>-NLI and explored the performance of language models on it. However, the <tex-math>\delta</tex-math>-NLI dataset is entangled with commonsense reasoning. In this paper, we explore the pure non-monotonic reasoning ability of pre-trained language models. We build a non-monotonic reasoning benchmark, named LogicNMR, with explicit default rules and iterative updates. In the experimental part, the performance of popular language models on LogicNMR is explored from the perspectives of accuracy, generalization, proof-based traceability and robustness. The experimental results show that even though the fine-tuned language models achieve an accuracy of more than 94.4% on LogicNMR, they perform unsatisfactorily, with a significant drop, in generalization and proof-based traceability.</abstract>
      <url hash="5010bcb2">2022.findings-emnlp.265</url>
      <bibkey>xiu-etal-2022-logicnmr</bibkey>
      <video href="2022.findings-emnlp.265.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.265</doi>
    </paper>
    <paper id="266">
      <title>Cheater’s Bowl: Human vs. Computer Search Strategies for Open-Domain <fixed-case>QA</fixed-case></title>
      <author><first>Wanrong</first><last>He</last></author>
      <author><first>Andrew</first><last>Mao</last></author>
      <author><first>Jordan</first><last>Boyd-Graber</last></author>
      <pages>3627-3639</pages>
      <abstract>For humans and computers, the first step in answering an open-domain question is retrieving a set of relevant documents from a large corpus. However, the strategies that computers use fundamentally differ from those of humans. To better understand these differences, we design a gamified interface for data collection—Cheater’s Bowl—where a human answers complex questions with access to both traditional and modern search tools. We collect a dataset of human search sessions, analyze human search strategies, and compare them to state-of-the-art multi-hop QA models. Humans query logically, apply dynamic search chains, and use world knowledge to boost searching. We demonstrate how human queries can improve the accuracy of existing systems and propose improving the future design of QA models.</abstract>
      <url hash="a710b2e3">2022.findings-emnlp.266</url>
      <bibkey>he-etal-2022-cheaters</bibkey>
      <video href="2022.findings-emnlp.266.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.266</doi>
    </paper>
    <paper id="267">
      <title><fixed-case>FRSUM</fixed-case>: Towards Faithful Abstractive Summarization via Enhancing Factual Robustness</title>
      <author><first>Wenhao</first><last>Wu</last></author>
      <author><first>Wei</first><last>Li</last></author>
      <author><first>Jiachen</first><last>Liu</last></author>
      <author><first>Xinyan</first><last>Xiao</last></author>
      <author><first>Ziqiang</first><last>Cao</last></author>
      <author><first>Sujian</first><last>Li</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <pages>3640-3654</pages>
      <abstract>Despite being able to generate fluent and grammatical text, current Seq2Seq summarization models still suffering from the unfaithful generation problem. In this paper, we study the faithfulness of existing systems from a new perspective of factual robustness which is the ability to correctly generate factual information over adversarial unfaithful information. We first measure a model’sfactual robustness by its success rate to defend against adversarial attacks when generating factual information. The factual robustness analysis on a wide range of current systems shows its good consistency with human judgments on faithfulness. Inspired by these findings, we propose to improve the faithfulness of a model by enhancing its factual robustness. Specifically, we propose a novel training strategy, namely FRSUM, which teaches the model to defend against both explicit adversarial samples and implicit factual adversarial perturbations. Extensive automatic and human evaluation results show that FRSUM consistently improves the faithfulness of various Seq2Seq models, such as T5, BART.</abstract>
      <url hash="fca62fb6">2022.findings-emnlp.267</url>
      <bibkey>wu-etal-2022-frsum</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.267</doi>
    </paper>
    <paper id="268">
      <title><fixed-case>P</fixed-case>oe<fixed-case>LM</fixed-case>: A Meter- and Rhyme-Controllable Language Model for Unsupervised Poetry Generation</title>
      <author><first>Aitor</first><last>Ormazabal</last></author>
      <author><first>Mikel</first><last>Artetxe</last></author>
      <author><first>Manex</first><last>Agirrezabal</last></author>
      <author><first>Aitor</first><last>Soroa</last></author>
      <author><first>Eneko</first><last>Agirre</last></author>
      <pages>3655-3670</pages>
      <abstract>Formal verse poetry imposes strict constraints on the meter and rhyme scheme of poems. Most prior work on generating this type of poetry uses existing poems for supervision, which are difficult to obtain for most languages and poetic forms. In this work, we propose an unsupervised approach to generate poems that follow any given meter and rhyme scheme, without requiring any poetic text for training. Our method works by splitting a regular, non-poetic corpus into phrases, prepending control codes that describe the length and end rhyme of each phrase, and training a transformer language model in the augmented corpus. The transformer learns to link the structure descriptor with the control codes to the number of lines, their length and their end rhyme. During inference, we build control codes for the desired meter and rhyme scheme, and condition our language model on them to generate formal verse poetry. Experiments in Spanish and Basque show that our approach is able to generate valid poems, which are often comparable in quality to those written by humans.</abstract>
      <url hash="0421e5ed">2022.findings-emnlp.268</url>
      <bibkey>ormazabal-etal-2022-poelm</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.268</doi>
    </paper>
    <paper id="269">
      <title><fixed-case>P</fixed-case>ro<fixed-case>G</fixed-case>en: Progressive Zero-shot Dataset Generation via In-context Feedback</title>
      <author><first>Jiacheng</first><last>Ye</last></author>
      <author><first>Jiahui</first><last>Gao</last></author>
      <author><first>Zhiyong</first><last>Wu</last></author>
      <author><first>Jiangtao</first><last>Feng</last></author>
      <author><first>Tao</first><last>Yu</last></author>
      <author><first>Lingpeng</first><last>Kong</last></author>
      <pages>3671-3683</pages>
      <abstract>Recently, dataset-generation-based zero-shot learning has shown promising results by training a task-specific model with a dataset synthesized from large pre-trained language models (PLMs). The final task-specific model often achieves compatible or even better performance than PLMs under the zero-shot setting, with orders of magnitude fewer parameters. However, synthetic datasets have their drawbacks. They have long being suffering from the low-quality issue (e.g., low informativeness, redundancy). This explains why the massive synthetic data does not lead to better performance – a scenario we would expect in the human-labeled data. To improve the quality in dataset synthesis, we propose a progressive zero-shot dataset generation framework, ProGen, which leverages the feedback from the task-specific model to guide the generation of new training data via in-context examples. Extensive experiments on five text classification datasets demonstrate the effectiveness of the proposed approach. We also show ProGen achieves on-par or superior performance with only 1% synthetic dataset size, when comparing to baseline methods without in-context feedback.</abstract>
      <url hash="7bc2f0e5">2022.findings-emnlp.269</url>
      <bibkey>ye-etal-2022-progen</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.269</doi>
    </paper>
    <paper id="270">
      <title>Constructing Highly Inductive Contexts for Dialogue Safety through Controllable Reverse Generation</title>
      <author><first>Zhexin</first><last>Zhang</last></author>
      <author><first>Jiale</first><last>Cheng</last></author>
      <author><first>Hao</first><last>Sun</last></author>
      <author><first>Jiawen</first><last>Deng</last></author>
      <author><first>Fei</first><last>Mi</last></author>
      <author><first>Yasheng</first><last>Wang</last></author>
      <author><first>Lifeng</first><last>Shang</last></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <pages>3684-3697</pages>
      <abstract>Large pretrained language models can easily produce toxic or biased content, which is prohibitive for practical use. In order to detect such toxic generations, existing methods rely on templates, real-world data extraction, crowdsourcing workers or automatic generation to construct adversarial contexts that are likely to induce toxic generations. However, what type of context is more likely to induce unsafe responses is still under-explored. In this paper, we identify that context toxicity and context category (e.g., profanity, insult, drugs, etc.) are two important factors to cause safety issues in response generation. Hence, we propose a method called reverse generation to construct adversarial contexts conditioned on a given response, with the flexibility to control category, toxicity level and inductivity of the generated contexts. Via reverse generation, we augment the existing BAD dataset and construct a new dataset BAD+ which contains more than 120K diverse and highly inductive contexts in 12 categories. We test three popular pretrained dialogue models (Blender, DialoGPT and Plato2) and find that BAD+ can largely expose their safety problems. Furthermore, we show that BAD+ can greatly enhance the safety of generation, and we reveal the key factors of safety improvement. Our code and dataset is available at <url>https://github.com/thu-coai/Reverse_Generation</url>.</abstract>
      <url hash="753d44bb">2022.findings-emnlp.270</url>
      <bibkey>zhang-etal-2022-constructing</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.270</doi>
    </paper>
    <paper id="271">
      <title>Language Prior Is Not the Only Shortcut: A Benchmark for Shortcut Learning in <fixed-case>VQA</fixed-case></title>
      <author><first>Qingyi</first><last>Si</last></author>
      <author><first>Fandong</first><last>Meng</last></author>
      <author><first>Mingyu</first><last>Zheng</last></author>
      <author><first>Zheng</first><last>Lin</last></author>
      <author><first>Yuanxin</first><last>Liu</last></author>
      <author><first>Peng</first><last>Fu</last></author>
      <author><first>Yanan</first><last>Cao</last></author>
      <author><first>Weiping</first><last>Wang</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>3698-3712</pages>
      <abstract>Visual Question Answering (VQA) models are prone to learn the shortcut solution formed by dataset biases rather than the intended solution. To evaluate the VQA models’ reasoning ability beyond shortcut learning, the VQA-CP v2 dataset introduces a distribution shift between the training and test set given a question type. In this way, the model cannot use the training set shortcut (from question type to answer) to perform well on the test set. However, VQA-CP v2 only considers one type of shortcut and thus still cannot guarantee that the model relies on the intended solution rather than a solution specific to this shortcut. To overcome this limitation, we propose a new dataset that considers varying types of shortcuts by constructing different distribution shifts in multiple OOD test sets. In addition, we overcome the three troubling practices in the use of VQA-CP v2, e.g., selecting models using OOD test sets, and further standardize OOD evaluation procedure. Our benchmark provides a more rigorous and comprehensive testbed for shortcut learning in VQA. We benchmark recent methods and find that methods specifically designed for particular shortcuts fail to simultaneously generalize to our varying OOD test sets. We also systematically study the varying shortcuts and provide several valuable findings, which may promote the exploration of shortcut learning in VQA.</abstract>
      <url hash="26f5447a">2022.findings-emnlp.271</url>
      <bibkey>si-etal-2022-language</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.271</doi>
    </paper>
    <paper id="272">
      <title>Bridging the Training-Inference Gap for Dense Phrase Retrieval</title>
      <author><first>Gyuwan</first><last>Kim</last></author>
      <author><first>Jinhyuk</first><last>Lee</last></author>
      <author><first>Barlas</first><last>Oguz</last></author>
      <author><first>Wenhan</first><last>Xiong</last></author>
      <author><first>Yizhe</first><last>Zhang</last></author>
      <author><first>Yashar</first><last>Mehdad</last></author>
      <author><first>William Yang</first><last>Wang</last></author>
      <pages>3713-3724</pages>
      <abstract>Building dense retrievers requires a series of standard procedures, including training and validating neural models and creating indexes for efficient search. However, these procedures are often misaligned in that training objectives do not exactly reflect the retrieval scenario at inference time. In this paper, we explore how the gap between training and inference in dense retrieval can be reduced, focusing on dense phrase retrieval (Lee et al., 2021) where billions of representations are indexed at inference. Since validating every dense retriever with a large-scale index is practically infeasible, we propose an efficient way of validating dense retrievers using a small subset of the entire corpus. This allows us to validate various training strategies including unifying contrastive loss terms and using hard negatives for phrase retrieval, which largely reduces the training-inference discrepancy. As a result, we improve top-1 phrase retrieval accuracy by 2 3 points and top-20 passage retrieval accuracy by 2 4 points for open-domain question answering. Our work urges modeling dense retrievers with careful consideration of training and inference via efficient validation while advancing phrase retrieval as a general solution for dense retrieval.</abstract>
      <url hash="3f2fb0bd">2022.findings-emnlp.272</url>
      <bibkey>kim-etal-2022-bridging</bibkey>
      <video href="2022.findings-emnlp.272.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.272</doi>
    </paper>
    <paper id="273">
      <title>Beyond Counting Datasets: A Survey of Multilingual Dataset Construction and Necessary Resources</title>
      <author><first>Xinyan</first><last>Yu</last></author>
      <author><first>Trina</first><last>Chatterjee</last></author>
      <author><first>Akari</first><last>Asai</last></author>
      <author><first>Junjie</first><last>Hu</last></author>
      <author><first>Eunsol</first><last>Choi</last></author>
      <pages>3725-3743</pages>
      <abstract>While the NLP community is generally aware of resource disparities among languages, we lack research that quantifies the extent and types of such disparity. Prior surveys estimating the availability of resources based on the number of datasets can be misleading as dataset quality varies: many datasets are automatically induced or translated from English data. To provide a more comprehensive picture of language resources, we examine the characteristics of 156 publicly available NLP datasets. We manually annotate how they are created, including input text and label sources and tools used to build them, and what they study, tasks they address and motivations for their creation. After quantifying the qualitative NLP resource gap across languages, we discuss how to improve data collection in low-resource languages. We survey language-proficient NLP researchers and crowd workers per language, finding that their estimated availability correlates with dataset availability. Through crowdsourcing experiments, we identify strategies for collecting high-quality multilingual data on the Mechanical Turk platform. We conclude by making macro and micro-level suggestions to the NLP community and individual researchers for future multilingual data development.</abstract>
      <url hash="29fd6da0">2022.findings-emnlp.273</url>
      <bibkey>yu-etal-2022-beyond</bibkey>
      <video href="2022.findings-emnlp.273.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.273</doi>
    </paper>
    <paper id="274">
      <title><fixed-case>ERNIE</fixed-case>-Layout: Layout Knowledge Enhanced Pre-training for Visually-rich Document Understanding</title>
      <author><first>Qiming</first><last>Peng</last></author>
      <author><first>Yinxu</first><last>Pan</last></author>
      <author><first>Wenjin</first><last>Wang</last></author>
      <author><first>Bin</first><last>Luo</last></author>
      <author><first>Zhenyu</first><last>Zhang</last></author>
      <author><first>Zhengjie</first><last>Huang</last></author>
      <author><first>Yuhui</first><last>Cao</last></author>
      <author><first>Weichong</first><last>Yin</last></author>
      <author><first>Yongfeng</first><last>Chen</last></author>
      <author><first>Yin</first><last>Zhang</last></author>
      <author><first>Shikun</first><last>Feng</last></author>
      <author><first>Yu</first><last>Sun</last></author>
      <author><first>Hao</first><last>Tian</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <author><first>Haifeng</first><last>Wang</last></author>
      <pages>3744-3756</pages>
      <abstract>Recent years have witnessed the rise and success of pre-training techniques in visually-rich document understanding. However, most existing methods lack the systematic mining and utilization of layout-centered knowledge, leading to sub-optimal performances. In this paper, we propose ERNIE-Layout, a novel document pre-training solution with layout knowledge enhancement in the whole workflow, to learn better representations that combine the features from text, layout, and image. Specifically, we first rearrange input sequences in the serialization stage, and then present a correlative pre-training task, reading order prediction, to learn the proper reading order of documents. To improve the layout awareness of the model, we integrate a spatial-aware disentangled attention into the multi-modal transformer and a replaced regions prediction task into the pre-training phase. Experimental results show that ERNIE-Layout achieves superior performance on various downstream tasks, setting new state-of-the-art on key information extraction, document image classification, and document question answering datasets. The code and models are publicly available at PaddleNLP.</abstract>
      <url hash="9ca4081b">2022.findings-emnlp.274</url>
      <bibkey>peng-etal-2022-ernie</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.274</doi>
    </paper>
    <paper id="275">
      <title>Do Charge Prediction Models Learn Legal Theory?</title>
      <author><first>Zhenwei</first><last>An</last></author>
      <author><first>Quzhe</first><last>Huang</last></author>
      <author><first>Cong</first><last>Jiang</last></author>
      <author><first>Yansong</first><last>Feng</last></author>
      <author><first>Dongyan</first><last>Zhao</last></author>
      <pages>3757-3768</pages>
      <abstract>The charge prediction task aims to predict the charge for a case given its fact description. Recent models have already achieved impressive accuracy in this task, however, little is understood about the mechanisms they use to perform the judgment. For practical applications, a charge prediction model should conform to the certain legal theory in civil law countries, as under the framework of civil law, all cases are judged according to certain local legal theories. In China, for example, nearly all criminal judges make decisions based on the Four Elements Theory (FET).In this paper, we argue that trustworthy charge prediction models should take legal theories into consideration, and standing on prior studies in model interpretation, we propose three principles for trustworthy models should follow in this task, which are sensitive, selective, and presumption of innocence. We further design a new framework to evaluate whether existing charge prediction models learn legal theories. Our findings indicate that, while existing charge prediction models meet the selective principle on a benchmark dataset, most of them are still not sensitive enough and do not satisfy the presumption of innocence. Our code and dataset are released at <url>https://github.com/ZhenweiAn/EXP_LJP</url>.</abstract>
      <url hash="7d5648f4">2022.findings-emnlp.275</url>
      <bibkey>an-etal-2022-charge</bibkey>
      <video href="2022.findings-emnlp.275.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.275</doi>
    </paper>
    <paper id="276">
      <title>Keep Me Updated! Memory Management in Long-term Conversations</title>
      <author><first>Sanghwan</first><last>Bae</last></author>
      <author><first>Donghyun</first><last>Kwak</last></author>
      <author><first>Soyoung</first><last>Kang</last></author>
      <author><first>Min Young</first><last>Lee</last></author>
      <author><first>Sungdong</first><last>Kim</last></author>
      <author><first>Yuin</first><last>Jeong</last></author>
      <author><first>Hyeri</first><last>Kim</last></author>
      <author><first>Sang-Woo</first><last>Lee</last></author>
      <author><first>Woomyoung</first><last>Park</last></author>
      <author><first>Nako</first><last>Sung</last></author>
      <pages>3769-3787</pages>
      <abstract>Remembering important information from the past and continuing to talk about it in the present are crucial in long-term conversations. However, previous literature does not deal with cases where the memorized information is outdated, which may cause confusion in later conversations. To address this issue, we present a novel task and a corresponding dataset of memory management in long-term conversations, in which bots keep track of and bring up the latest information about users while conversing through multiple sessions. In order to support more precise and interpretable memory, we represent memory as unstructured text descriptions of key information and propose a new mechanism of memory management that selectively eliminates invalidated or redundant information. Experimental results show that our approach outperforms the baselines that leave the stored memory unchanged in terms of engagingness and humanness, with larger performance gap especially in the later sessions.</abstract>
      <url hash="ecd69f0f">2022.findings-emnlp.276</url>
      <bibkey>bae-etal-2022-keep</bibkey>
      <video href="2022.findings-emnlp.276.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.276</doi>
    </paper>
    <paper id="277">
      <title>A Unified Dialogue User Simulator for Few-shot Data Augmentation</title>
      <author><first>Dazhen</first><last>Wan</last></author>
      <author><first>Zheng</first><last>Zhang</last></author>
      <author><first>Qi</first><last>Zhu</last></author>
      <author><first>Lizi</first><last>Liao</last></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <pages>3788-3799</pages>
      <abstract>Pre-trained language models have shown superior performance in task-oriented dialogues. However, existing datasets are on limited scales, which cannot support large-scale pre-training. Fortunately, various data augmentation methods have been developed to augment large-scale task-oriented dialogue corpora. However, they heavily rely on annotated data in the target domain, which require a tremendous amount of data collection and human labeling work. In this paper, we build a unified dialogue user simulation model by pre-training on several publicly available datasets. The model can then be tuned on a target domain with few-shot data. The experiments on a target dataset across multiple domains show that our proposed model brings remarkable performance increases through data augmentation.</abstract>
      <url hash="477206fc">2022.findings-emnlp.277</url>
      <bibkey>wan-etal-2022-unified</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.277</doi>
    </paper>
    <paper id="278">
      <title>An Error-Guided Correction Model for <fixed-case>C</fixed-case>hinese Spelling Error Correction</title>
      <author><first>Rui</first><last>Sun</last></author>
      <author><first>Xiuyu</first><last>Wu</last></author>
      <author><first>Yunfang</first><last>Wu</last></author>
      <pages>3800-3810</pages>
      <abstract>Although existing neural network approaches have achieved great progress on Chinese spelling correction, there is still room to improve. The model is required to avoid over-correction and to distinguish a correct token from its phonological and visual similar ones. In this paper, we propose an error-guided correction model to address these issues. By borrowing the powerful ability of the pre-trained BERT model, we propose a novel zero-shot error detection method to do a preliminary detection, which guides our model to attend more on the probably wrong tokens in encoding and to avoid modifying the correct tokens in generating. Furthermore, we introduce a new loss function to integrate the error confusion set, which enables our model to distinguish similar tokens. Moreover, our model supports highly parallel decoding to meet real applications. Experiments are conducted on widely used benchmarks. Our model achieves superior performance against state-of-the-art approaches by a remarkable margin, on both the quality and computation speed.</abstract>
      <url hash="48f2cf88">2022.findings-emnlp.278</url>
      <bibkey>sun-etal-2022-error</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.278</doi>
    </paper>
    <paper id="279">
      <title>Describing Sets of Images with Textual-<fixed-case>PCA</fixed-case></title>
      <author><first>Oded</first><last>Hupert</last></author>
      <author><first>Idan</first><last>Schwartz</last></author>
      <author><first>Lior</first><last>Wolf</last></author>
      <pages>3811-3821</pages>
      <abstract>We seek to semantically describe a set of images, capturing both the attributes of single images and the variations within the set. Our procedure is analogous to Principle Component Analysis, in which the role of projection vectors is replaced with generated phrases. First, a centroid phrase that has the largest average semantic similarity to the images in the set is generated, where both the computation of the similarity and the generation are based on pretrained vision-language models. Then, the phrase that generates the highest variation among the similarity scores is generated, using the same models. The next phrase maximizes the variance subject to being orthogonal, in the latent space, to the highest-variance phrase, and the process continues. Our experiments show that our method is able to convincingly capture the essence of image sets and describe the individual elements in a semantically meaningful way within the context of the entire set. Our code is available at: <url>https://github.com/OdedH/textual-pca</url>.</abstract>
      <url hash="a946b5cf">2022.findings-emnlp.279</url>
      <bibkey>hupert-etal-2022-describing</bibkey>
      <video href="2022.findings-emnlp.279.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.279</doi>
    </paper>
    <paper id="280">
      <title>Learning to Model Editing Processes</title>
      <author><first>Machel</first><last>Reid</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <pages>3822-3832</pages>
      <abstract>Most existing sequence generation models produce outputs in one pass, usually left-to-right. However, this is in contrast with a more natural approach that humans use in generating content; iterative refinement and editing. Recent work has introduced edit-based models for various tasks (such as neural machine translation and text style transfer), but these generally model a single edit step. In this work, we propose modeling editing processes, modeling the whole process of iteratively generating sequences. We form a conceptual framework to describe the likelihood of multi-step edits, and describe neural models that can learn a generative model of sequences based on these multistep edits. We introduce baseline results and metrics on this task, finding that modeling editing processes improves performance on a variety of axes on both our proposed task and related downstream tasks compared to previous single-step models of edits.</abstract>
      <url hash="5ab4e221">2022.findings-emnlp.280</url>
      <bibkey>reid-neubig-2022-learning</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.280</doi>
    </paper>
    <paper id="281">
      <title><fixed-case>PALT</fixed-case>: Parameter-Lite Transfer of Language Models for Knowledge Graph Completion</title>
      <author><first>Jianhao</first><last>Shen</last></author>
      <author><first>Chenguang</first><last>Wang</last></author>
      <author><first>Ye</first><last>Yuan</last></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <author><first>Koushik</first><last>Sen</last></author>
      <author><first>Ming</first><last>Zhang</last></author>
      <author><first>Dawn</first><last>Song</last></author>
      <pages>3833-3847</pages>
      <abstract>This paper presents a parameter-lite transfer learning approach of pretrained language models (LM) for knowledge graph (KG) completion. Instead of finetuning, which modifies all LM parameters, we only tune a few new parameters while keeping the original LM parameters fixed. We establish this via reformulating KG completion as a “fill-in-the-blank” task, and introducing a parameter-lite encoder on top of the original LMs. We show that, by tuning far fewer parameters than finetuning, LMs transfer non-trivially to most tasks and reach competitiveness with prior state-of-the-art approaches. For instance, we outperform the fully finetuning approaches on a KG completion benchmark by tuning only 1% of the parameters.</abstract>
      <url hash="b2194cad">2022.findings-emnlp.281</url>
      <bibkey>shen-etal-2022-palt</bibkey>
      <video href="2022.findings-emnlp.281.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.281</doi>
    </paper>
    <paper id="282">
      <title>Prompt-based Connective Prediction Method for Fine-grained Implicit Discourse Relation Recognition</title>
      <author><first>Hao</first><last>Zhou</last></author>
      <author><first>Man</first><last>Lan</last></author>
      <author><first>Yuanbin</first><last>Wu</last></author>
      <author><first>Yuefeng</first><last>Chen</last></author>
      <author><first>Meirong</first><last>Ma</last></author>
      <pages>3848-3858</pages>
      <abstract>Due to the absence of connectives, implicit discourse relation recognition (IDRR) is still a challenging and crucial task in discourse analysis. Most of the current work adopted multitask learning to aid IDRR through explicit discourse relation recognition (EDRR) or utilized dependencies between discourse relation labels to constrain model predictions. But these methods still performed poorly on fine-grained IDRR and even utterly misidentified on most of the few-shot discourse relation classes. To address these problems, we propose a novel Prompt-based Connective Prediction (PCP) method for IDRR. Our method instructs large-scale pre-trained models to use knowledge relevant to discourse relation and utilizes the strong correlation between connectives and discourse relation to help the model recognize implicit discourse relations. Experimental results show that our method surpasses the current state-of-the-art model and achieves significant improvements on those fine-grained few-shot discourse relation. Moreover, our approach is able to be transferred to EDRR and obtain acceptable results. Our code is released in <url>https://github.com/zh-i9/PCP-for-IDRR</url>.</abstract>
      <url hash="76a6e97f">2022.findings-emnlp.282</url>
      <bibkey>zhou-etal-2022-prompt-based</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.282</doi>
    </paper>
    <paper id="283">
      <title>On Utilizing Constituent Language Resources to Improve Downstream Tasks in <fixed-case>H</fixed-case>inglish</title>
      <author><first>Vishwajeet</first><last>Kumar</last></author>
      <author><first>Rudra</first><last>Murthy</last></author>
      <author><first>Tejas</first><last>Dhamecha</last></author>
      <pages>3859-3865</pages>
      <abstract>Performance of downstream NLP tasks on code-switched Hindi-English (aka ) continues to remain a significant challenge. Intuitively, Hindi and English corpora should aid improve task performance on Hinglish. We show that meta-learning framework can effectively utilize the the labelled resources of the downstream tasks in the constituent languages. The proposed approach improves the performance on downstream tasks on code-switched language. We experiment with code-switching benchmark GLUECoS and report significant improvements.</abstract>
      <url hash="f403c73c">2022.findings-emnlp.283</url>
      <bibkey>kumar-etal-2022-utilizing</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.283</doi>
    </paper>
    <paper id="284">
      <title><fixed-case>SYGMA</fixed-case>: A System for Generalizable and Modular Question Answering Over Knowledge Bases</title>
      <author><first>Sumit</first><last>Neelam</last></author>
      <author><first>Udit</first><last>Sharma</last></author>
      <author><first>Hima</first><last>Karanam</last></author>
      <author><first>Shajith</first><last>Ikbal</last></author>
      <author><first>Pavan</first><last>Kapanipathi</last></author>
      <author><first>Ibrahim</first><last>Abdelaziz</last></author>
      <author><first>Nandana</first><last>Mihindukulasooriya</last></author>
      <author><first>Young-Suk</first><last>Lee</last></author>
      <author><first>Santosh</first><last>Srivastava</last></author>
      <author><first>Cezar</first><last>Pendus</last></author>
      <author><first>Saswati</first><last>Dana</last></author>
      <author><first>Dinesh</first><last>Garg</last></author>
      <author><first>Achille</first><last>Fokoue</last></author>
      <author><first>G P Shrivatsa</first><last>Bhargav</last></author>
      <author><first>Dinesh</first><last>Khandelwal</last></author>
      <author><first>Srinivas</first><last>Ravishankar</last></author>
      <author><first>Sairam</first><last>Gurajada</last></author>
      <author><first>Maria</first><last>Chang</last></author>
      <author><first>Rosario</first><last>Uceda-Sosa</last></author>
      <author><first>Salim</first><last>Roukos</last></author>
      <author><first>Alexander</first><last>Gray</last></author>
      <author><first>Guilherme</first><last>Lima</last></author>
      <author><first>Ryan</first><last>Riegel</last></author>
      <author><first>Francois</first><last>Luus</last></author>
      <author><first>L V</first><last>Subramaniam</last></author>
      <pages>3866-3879</pages>
      <abstract>Knowledge Base Question Answering (KBQA) involving complex reasoning is emerging as an important research direction. However, most KBQA systems struggle with generalizability, particularly on two dimensions: (a) across multiple knowledge bases, where existing KBQA approaches are typically tuned to a single knowledge base, and (b) across multiple reasoning types, where majority of datasets and systems have primarily focused on multi-hop reasoning. In this paper, we present SYGMA, a modular KBQA approach developed with goal of generalization across multiple knowledge bases and multiple reasoning types. To facilitate this, SYGMA is designed as two high level modules: 1) KB-agnostic question understanding module that remain common across KBs, and generates logic representation of the question with high level reasoning constructs that are extensible, and 2) KB-specific question mapping and answering module to address the KB-specific aspects of the answer extraction. We evaluated SYGMA on multiple datasets belonging to distinct knowledge bases (DBpedia and Wikidata) and distinct reasoning types (multi-hop and temporal). State-of-the-art or competitive performances achieved on those datasets demonstrate its generalization capability.</abstract>
      <url hash="7541dd56">2022.findings-emnlp.284</url>
      <bibkey>neelam-etal-2022-sygma</bibkey>
      <video href="2022.findings-emnlp.284.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.284</doi>
    </paper>
    <paper id="285">
      <title>Instance-Guided Prompt Learning for Few-Shot Text Matching</title>
      <author><first>Jia</first><last>Du</last></author>
      <author><first>Xuanyu</first><last>Zhang</last></author>
      <author><first>Siyi</first><last>Wang</last></author>
      <author><first>Kai</first><last>Wang</last></author>
      <author><first>Yanquan</first><last>Zhou</last></author>
      <author><first>Lei</first><last>Li</last></author>
      <author><first>Qing</first><last>Yang</last></author>
      <author><first>Dongliang</first><last>Xu</last></author>
      <pages>3880-3886</pages>
      <abstract>Few-shot text matching is a more practical technique in natural language processing (NLP) to determine whether two texts are semantically identical. They primarily design patterns to reformulate text matching into a pre-trained task with uniform prompts across all instances. But they fail to take into account the connection between prompts and instances. This paper argues that dynamically strengthening the correlation between particular instances and the prompts is necessary because fixed prompts cannot adequately fit all diverse instances in inference. We suggest IGATE: Instance-Guided prompt leArning for few-shoT tExt matching, a novel pluggable prompt learning method. The gate mechanism used by IGATE, which is between the embedding and the PLM encoders, makes use of the semantics of instances to regulate the effects of the gate on the prompt tokens. The experimental findings show that IGATE achieves SOTA performance on MRPC and QQP, outperforming strong baselines. GitHub will host the release of codes.</abstract>
      <url hash="5b8e1e07">2022.findings-emnlp.285</url>
      <bibkey>du-etal-2022-instance</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.285</doi>
    </paper>
    <paper id="286">
      <title><fixed-case>M</fixed-case>3: Multi-level dataset for Multi-document summarisation of Medical studies</title>
      <author><first>Yulia</first><last>Otmakhova</last></author>
      <author><first>Karin</first><last>Verspoor</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <author><first>Antonio</first><last>Jimeno Yepes</last></author>
      <author><first>Jey Han</first><last>Lau</last></author>
      <pages>3887-3901</pages>
      <abstract>We present M3 (Multi-level dataset for Multi-document summarisation of Medical studies), a benchmark dataset for evaluating the quality of summarisation systems in the biomedical domain. The dataset contains sets of multiple input documents and target summaries of three levels of complexity: documents, sentences, and propositions. The dataset also includes several levels of annotation, including biomedical entities, direction, and strength of relations between them, and the discourse relationships between the input documents (“contradiction” or “agreement”). We showcase usage scenarios of the dataset by testing 10 generic and domain-specific summarisation models in a zero-shot setting, and introduce a probing task based on counterfactuals to test if models are aware of the direction and strength of the conclusions generated from input studies.</abstract>
      <url hash="2bf7b9b0">2022.findings-emnlp.286</url>
      <bibkey>otmakhova-etal-2022-m3</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.286</doi>
    </paper>
    <paper id="287">
      <title>Adapters for Enhanced Modeling of Multilingual Knowledge and Text</title>
      <author><first>Yifan</first><last>Hou</last></author>
      <author><first>Wenxiang</first><last>Jiao</last></author>
      <author><first>Meizhen</first><last>Liu</last></author>
      <author><first>Carl</first><last>Allen</last></author>
      <author><first>Zhaopeng</first><last>Tu</last></author>
      <author><first>Mrinmaya</first><last>Sachan</last></author>
      <pages>3902-3917</pages>
      <abstract>Large language models appear to learn facts from the large text corpora they are trained on. Such facts are encoded implicitly within their many parameters, making it difficult to verify or manipulate what knowledge has been learned. Language models have recently been extended to multilingual language models (MLLMs), enabling knowledge to be learned across hundreds of languages. Meanwhile, knowledge graphs contain facts in an explicit triple format, which require careful and costly curation and are only available in a few high-resource languages, restricting their research and application. To address these issues, we propose to enhance MLLMs with knowledge from multilingual knowledge graphs (MLKGs) so as to tackle language and knowledge graph tasks across many languages, including low-resource ones. Specifically, we introducea lightweight adapter set to enhance MLLMs with cross-lingual entity alignment and facts from MLKGs for many languages. Experiments on common benchmarks show that such enhancement benefits both MLLMs and MLKGs, achieving: (1) comparable or improved performance for knowledge graph completion and entity alignment relative to baselines, especially for low-resource languages (for which knowledge graphs are unavailable); and (2) improved MLLM performance on language understanding tasks that require multilingual factual knowledge; all while maintaining performance on other general language tasks.</abstract>
      <url hash="f53ff415">2022.findings-emnlp.287</url>
      <bibkey>hou-etal-2022-adapters</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.287</doi>
    </paper>
    <paper id="288">
      <title><fixed-case>S</fixed-case>ep<fixed-case>LL</fixed-case>: Separating Latent Class Labels from Weak Supervision Noise</title>
      <author><first>Andreas</first><last>Stephan</last></author>
      <author><first>Vasiliki</first><last>Kougia</last></author>
      <author><first>Benjamin</first><last>Roth</last></author>
      <pages>3918-3929</pages>
      <abstract>In the weakly supervised learning paradigm, labeling functions automatically assign heuristic, often noisy, labels to data samples. In this work, we provide a method for learning from weak labels by separating two types of complementary information associated with the labeling functions: information related to the target label and information specific to one labeling function only. Both types of information are reflected to different degrees by all labeled instances. In contrast to previous works that aimed at correcting or removing wrongly labeled instances, we learn a branched deep model that uses all data as-is, but splits the labeling function information in the latent space. Specifically, we propose the end-to-end model SepLL which extends a transformer classifier by introducing a latent space for labeling function specific and task-specific information. The learning signal is only given by the labeling functions matches, no pre-processing or label model is required for our method. Notably, the task prediction is made from the latent layer without any direct task signal. Experiments on Wrench text classification tasks show that our model is competitive with the state-of-the-art, and yields a new best average performance.</abstract>
      <url hash="eaa4cbc3">2022.findings-emnlp.288</url>
      <bibkey>stephan-etal-2022-sepll</bibkey>
      <video href="2022.findings-emnlp.288.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.288</doi>
    </paper>
    <paper id="289">
      <title>Probing Relational Knowledge in Language Models via Word Analogies</title>
      <author><first>Kiamehr</first><last>Rezaee</last></author>
      <author><first>Jose</first><last>Camacho-Collados</last></author>
      <pages>3930-3936</pages>
      <abstract>Understanding relational knowledge plays an integral part in natural language comprehension. When it comes to pre-trained language models (PLM), prior work has been focusing on probing relational knowledge this by filling the blanks in pre-defined prompts such as “The capital of France is —". However, these probes may be affected by the co-occurrence of target relation words and entities (e.g. “capital”, “France” and “Paris”) in the pre-training corpus. In this work, we extend these probing methodologies leveraging analogical proportions as a proxy to probe relational knowledge in transformer-based PLMs without directly presenting the desired relation. In particular, we analysed the ability of PLMs to understand (1) the directionality of a given relation (e.g. Paris-France is not the same as France-Paris); (2) the ability to distinguish types on a given relation (both France and Japan are countries); and (3) the relation itself (Paris is the capital of France, but not Rome). Our results show how PLMs are extremely accurate at (1) and (2), but have clear room for improvement for (3). To better understand the reasons behind this behaviour and mistakes made by PLMs, we provide an extended quantitative analysis based on relevant factors such as frequency.</abstract>
      <url hash="67f576b7">2022.findings-emnlp.289</url>
      <bibkey>rezaee-camacho-collados-2022-probing</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.289</doi>
    </paper>
    <paper id="290">
      <title>Semi-Supervised Lifelong Language Learning</title>
      <author><first>Yingxiu</first><last>Zhao</last></author>
      <author><first>Yinhe</first><last>Zheng</last></author>
      <author><first>Bowen</first><last>Yu</last></author>
      <author><first>Zhiliang</first><last>Tian</last></author>
      <author><first>Dongkyu</first><last>Lee</last></author>
      <author><first>Jian</first><last>Sun</last></author>
      <author><first>Yongbin</first><last>Li</last></author>
      <author><first>Nevin L.</first><last>Zhang</last></author>
      <pages>3937-3951</pages>
      <abstract>Lifelong learning aims to accumulate knowledge and alleviate catastrophic forgetting when learning tasks sequentially. However, existing lifelong language learning methods only focus on the supervised learning setting. Unlabeled data, which can be easily accessed in real-world scenarios, are underexplored. In this paper, we explore a novel setting, semi-supervised lifelong language learning (SSLL), where a model learns sequentially arriving language tasks with both labeled and unlabeled data. We propose an unlabeled data enhanced lifelong learner to explore SSLL. Specially, we dedicate task-specific modules to alleviate catastrophic forgetting and design two modules to exploit unlabeled data: (1) a virtual supervision enhanced task solver is constructed on a teacher-student framework to mine the underlying knowledge from unlabeled data; and (2) a backward augmented learner is built to encourage knowledge transfer from newly arrived unlabeled data to previous tasks. Experimental results on various language tasks demonstrate our model’s effectiveness and superiority over competitive baselines under the new setting SSLL.</abstract>
      <url hash="1eb6bc9c">2022.findings-emnlp.290</url>
      <bibkey>zhao-etal-2022-semi</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.290</doi>
    </paper>
    <paper id="291">
      <title>Parameter-free Automatically Prompting: A Latent Pseudo Label Mapping Model for Prompt-based Learning</title>
      <author><first>Jirui</first><last>Qi</last></author>
      <author><first>Richong</first><last>Zhang</last></author>
      <author><first>Junfan</first><last>Chen</last></author>
      <author><first>Jaein</first><last>Kim</last></author>
      <author><first>Yongyi</first><last>Mao</last></author>
      <pages>3952-3962</pages>
      <abstract>Prompt-based learning has achieved excellent performance in few-shot learning by mapping the outputs of the pre-trained language model to the labels with the help of a label mapping component. Existing manual label mapping (MLM) methods achieve good results but heavily rely on expensive human knowledge. Automatic label mapping (ALM) methods that learn the mapping functions with extra parameters have shown their potentiality. However, no effective ALM model comparable to MLM methods is developed yet due to the limited data. In this paper, we propose a Latent Pseudo Label Mapping (LPLM) method that optimizes the label mapping without human knowledge and extra parameters. LPLM is built upon a probabilistic latent model and is iteratively self-improved with the EM-style algorithm. The empirical results demonstrate that our LPLM method is superior to the mainstream ALM methods and significantly outperforms the SOTA method in few-shot classification tasks. Moreover, LPLM also shows impressively better performance than the vanilla MLM method which requires extra task-specific prior knowledge.</abstract>
      <url hash="e47d2900">2022.findings-emnlp.291</url>
      <bibkey>qi-etal-2022-parameter</bibkey>
      <video href="2022.findings-emnlp.291.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.291</doi>
    </paper>
    <paper id="292">
      <title>Exploring Logographic Image for <fixed-case>C</fixed-case>hinese Aspect-based Sentiment Classification</title>
      <author><first>Xiabing</first><last>Zhou</last></author>
      <author><first>Renjie</first><last>Feng</last></author>
      <author><first>Xiaotong</first><last>Jiang</last></author>
      <author><first>Zhongqing</first><last>Wang</last></author>
      <pages>3963-3972</pages>
      <abstract>In logographic languages like Chinese, word meanings are constructed using specific character formations, which can help to disambiguate word senses and are beneficial for sentiment classification. However, such knowledge is rarely explored in previous sentiment analysis methods. In this paper, we focus on exploring the logographic information for aspect-based sentiment classification in Chinese text. Specifically, we employ a logographic image to capture an internal morphological structure from the character sequence. The logographic image is also used to learn the external relations among context and aspect words. Furthermore, we propose a multimodal language model to explicitly incorporate a logographic image with review text for aspect-based sentiment classification in Chinese. Experimental results show that our method brings substantial performance improvement over strong baselines. The results also indicate that the logographic image is very important for exploring the internal structure and external relations from the character sequence.</abstract>
      <url hash="df2d4d22">2022.findings-emnlp.292</url>
      <bibkey>zhou-etal-2022-exploring</bibkey>
      <video href="2022.findings-emnlp.292.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.292</doi>
    </paper>
    <paper id="293">
      <title>On the Role of Bidirectionality in Language Model Pre-Training</title>
      <author><first>Mikel</first><last>Artetxe</last></author>
      <author><first>Jingfei</first><last>Du</last></author>
      <author><first>Naman</first><last>Goyal</last></author>
      <author><first>Luke</first><last>Zettlemoyer</last></author>
      <author><first>Veselin</first><last>Stoyanov</last></author>
      <pages>3973-3985</pages>
      <abstract>Prior work on language model pre-training has explored different architectures and learning objectives, but differences in data, hyperparameters and evaluation make a principled comparison difficult. In this work, we focus on bidirectionality as a key factor that differentiates existing approaches, and present a comprehensive study of its role in next token prediction, text infilling, zero-shot priming and fine-tuning. We propose a new framework that generalizes prior approaches, including fully unidirectional models like GPT, fully bidirectional models like BERT, and hybrid models like CM3 and prefix LM. Our framework distinguishes between two notions of bidirectionality (bidirectional context and bidirectional attention) and allows us to control each of them separately. We find that the optimal configuration is largely application-dependent (e.g., bidirectional attention is beneficial for fine-tuning and infilling, but harmful for next token prediction and zero-shot priming). We train models with up to 6.7B parameters, and find differences to remain consistent at scale. While prior work on scaling has focused on left-to-right autoregressive models, our results suggest that this approach comes with some trade-offs, and it might be worthwhile to develop very large bidirectional models.</abstract>
      <url hash="1902fbfa">2022.findings-emnlp.293</url>
      <bibkey>artetxe-etal-2022-role</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.293</doi>
    </paper>
    <paper id="294">
      <title>You Are What You Talk About: Inducing Evaluative Topics for Personality Analysis</title>
      <author><first>Josip</first><last>Jukić</last></author>
      <author><first>Iva</first><last>Vukojević</last></author>
      <author><first>Jan</first><last>Snajder</last></author>
      <pages>3986-3999</pages>
      <abstract>Expressing attitude or stance toward entities and concepts is an integral part of human behavior and personality. Recently, evaluative language data has become more accessible with social media’s rapid growth, enabling large-scale opinion analysis. However, surprisingly little research examines the relationship between personality and evaluative language. To bridge this gap, we introduce the notion of evaluative topics, obtained by applying topic models to pre-filtered evaluative text from social media. We then link evaluative topics to individual text authors to build their evaluative profiles. We apply evaluative profiling to Reddit comments labeled with personality scores and conduct an exploratory study on the relationship between evaluative topics and Big Five personality facets, aiming for a more interpretable, facet-level analysis. Finally, we validate our approach by observing correlations consistent with prior research in personality psychology.</abstract>
      <url hash="c2256561">2022.findings-emnlp.294</url>
      <bibkey>jukic-etal-2022-talk</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.294</doi>
    </paper>
    <paper id="295">
      <title><fixed-case>CAT</fixed-case>-probing: A Metric-based Approach to Interpret How Pre-trained Models for Programming Language Attend Code Structure</title>
      <author><first>Nuo</first><last>Chen</last></author>
      <author><first>Qiushi</first><last>Sun</last></author>
      <author><first>Renyu</first><last>Zhu</last></author>
      <author><first>Xiang</first><last>Li</last></author>
      <author><first>Xuesong</first><last>Lu</last></author>
      <author><first>Ming</first><last>Gao</last></author>
      <pages>4000-4008</pages>
      <abstract>Code pre-trained models (CodePTMs) have recently demonstrated significant success in code intelligence. To interpret these models, some probing methods have been applied. However, these methods fail to consider the inherent characteristics of codes. In this paper, to address the problem, we propose a novel probing method CAT-probing to quantitatively interpret how CodePTMs attend code structure. We first denoise the input code sequences based on the token types pre-defined by the compilers to filter those tokens whose attention scores are too small. After that, we define a new metric CAT-score to measure the commonality between the token-level attention scores generated in CodePTMs and the pair-wise distances between corresponding AST nodes. The higher the CAT-score, the stronger the ability of CodePTMs to capture code structure. We conduct extensive experiments to integrate CAT-probing with representative CodePTMs for different programming languages. Experimental results show the effectiveness of CAT-probing in CodePTM interpretation. Our codes and data are publicly available at <url>https://github.com/nchen909/CodeAttention</url>.</abstract>
      <url hash="f72923bc">2022.findings-emnlp.295</url>
      <bibkey>chen-etal-2022-cat</bibkey>
      <video href="2022.findings-emnlp.295.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.295</doi>
    </paper>
    <paper id="296">
      <title>Learning to Revise References for Faithful Summarization</title>
      <author><first>Griffin</first><last>Adams</last></author>
      <author><first>Han-Chin</first><last>Shing</last></author>
      <author><first>Qing</first><last>Sun</last></author>
      <author><first>Christopher</first><last>Winestock</last></author>
      <author><first>Kathleen</first><last>McKeown</last></author>
      <author><first>Noémie</first><last>Elhadad</last></author>
      <pages>4009-4027</pages>
      <abstract>In real-world scenarios with naturally occurring datasets, reference summaries are noisy and may contain information that cannot be inferred from the source text. On large news corpora, removing low quality samples has been shown to reduce model hallucinations. Yet, for smaller, and/or noisier corpora, filtering is detrimental to performance. To improve reference quality while retaining all data, we propose a new approach: to selectively re-write unsupported reference sentences to better reflect source data. We automatically generate a synthetic dataset of positive and negative revisions by corrupting supported sentences and learn to revise reference sentences with contrastive learning. The intensity of revisions is treated as a controllable attribute so that, at inference, diverse candidates can be over-generated-then-rescored to balance faithfulness and abstraction. To test our methods, we extract noisy references from publicly available MIMIC-III discharge summaries for the task of hospital-course summarization, and vary the data on which models are trained. According to metrics and human evaluation, models trained on revised clinical references are much more faithful, informative, and fluent than models trained on original or filtered data.</abstract>
      <url hash="b455cad4">2022.findings-emnlp.296</url>
      <bibkey>adams-etal-2022-learning</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.296</doi>
    </paper>
    <paper id="297">
      <title>Towards Intention Understanding in Suicidal Risk Assessment with Natural Language Processing</title>
      <author><first>Shaoxiong</first><last>Ji</last></author>
      <pages>4028-4038</pages>
      <abstract>Recent applications of natural language processing techniques to suicidal ideation detection and risk assessment frame the detection or assessment task as a text classification problem. Recent advances have developed many models, especially deep learning models, to boost predictive performance. Though the performance (in terms of aggregated evaluation scores) is improving, this position paper urges that better intention understanding is required for reliable suicidal risk assessment with computational methods. This paper reflects the state of natural language processing applied to suicide-associated text classification tasks, differentiates suicidal risk assessment and intention understanding, and points out potential limitations of sentiment features and pretrained language models in suicidal intention understanding. Besides, it urges the necessity for sequential intention understanding and risk assessment, discusses some critical issues in evaluation such as uncertainty, and studies the lack of benchmarks.</abstract>
      <url hash="53217075">2022.findings-emnlp.297</url>
      <bibkey>ji-2022-towards</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.297</doi>
    </paper>
    <paper id="298">
      <title>On the Impact of Temporal Concept Drift on Model Explanations</title>
      <author><first>Zhixue</first><last>Zhao</last></author>
      <author><first>George</first><last>Chrysostomou</last></author>
      <author><first>Kalina</first><last>Bontcheva</last></author>
      <author><first>Nikolaos</first><last>Aletras</last></author>
      <pages>4039-4054</pages>
      <abstract>Explanation faithfulness of model predictions in natural language processing is typically evaluated on held-out data from the same temporal distribution as the training data (i.e. synchronous settings). While model performance often deteriorates due to temporal variation (i.e. temporal concept drift), it is currently unknown how explanation faithfulness is impacted when the time span of the target data is different from the data used to train the model (i.e. asynchronous settings). For this purpose, we examine the impact of temporal variation on model explanations extracted by eight feature attribution methods and three select-then-predict models across six text classification tasks. Our experiments show that (i) faithfulness is not consistent under temporal variations across feature attribution methods (e.g. it decreases or increases depending on the method), with an attention-based method demonstrating the most robust faithfulness scores across datasets; and (ii) select-then-predict models are mostly robust in asynchronous settings with only small degradation in predictive performance. Finally, feature attribution methods show conflicting behavior when used in FRESH (i.e. a select-and-predict model) and for measuring sufficiency/comprehensiveness (i.e. as post-hoc methods), suggesting that we need more robust metrics to evaluate post-hoc explanation faithfulness. Code will be made publicly available.</abstract>
      <url hash="87f26758">2022.findings-emnlp.298</url>
      <bibkey>zhao-etal-2022-impact</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.298</doi>
    </paper>
    <paper id="299">
      <title>Text-Only Training for Image Captioning using Noise-Injected <fixed-case>CLIP</fixed-case></title>
      <author><first>David</first><last>Nukrai</last></author>
      <author><first>Ron</first><last>Mokady</last></author>
      <author><first>Amir</first><last>Globerson</last></author>
      <pages>4055-4063</pages>
      <abstract>We consider the task of image-captioning using only the CLIP model and additional text data at training time and no additional captioned images. Our approach relies on the fact that CLIP is trained to make visual and textual embeddings similar. Therefore, we only need to learn how to translate CLIP textual embeddings back into text, and we can learn how to do this by learning a decoder for the frozen CLIP text encoder using only text. We argue that this intuition is “almost correct” because of a gap between the embedding spaces, and propose to rectify this via noise injection during training. We demonstrate the effectiveness of our approach by showing SOTA zero-shot image captioning across four benchmarks, including style transfer. Code, data, and models are available at <url>https://github.com/DavidHuji/CapDec</url>.</abstract>
      <url hash="10a432b2">2022.findings-emnlp.299</url>
      <bibkey>nukrai-etal-2022-text</bibkey>
      <video href="2022.findings-emnlp.299.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.299</doi>
    </paper>
    <paper id="300">
      <title>Improving Sharpness-Aware Minimization with Fisher Mask for Better Generalization on Language Models</title>
      <author><first>Qihuang</first><last>Zhong</last></author>
      <author><first>Liang</first><last>Ding</last></author>
      <author><first>Li</first><last>Shen</last></author>
      <author><first>Peng</first><last>Mi</last></author>
      <author><first>Juhua</first><last>Liu</last></author>
      <author><first>Bo</first><last>Du</last></author>
      <author><first>Dacheng</first><last>Tao</last></author>
      <pages>4064-4085</pages>
      <abstract>Fine-tuning large pretrained language models on a limited training corpus usually suffers from poor generalization. Prior works show that the recently-proposed sharpness-aware minimization (SAM) optimization method can improve the model generalization. However, SAM adds a perturbation to each model parameter equally (but not all parameters contribute equally to the optimization of training), which we argue is sub-optimal and will lead to excessive computation. In this paper, we propose a novel optimization procedure, namely FSAM, which introduces a Fisher mask to improve the efficiency and performance of SAM. In short, instead of adding perturbation to all parameters, FSAM uses the Fisher information to identity the important parameters and formulates a Fisher mask to obtain the sparse perturbation, i.e., making the optimizer focus on these important parameters. Experiments on various tasks in GLUE and SuperGLUE benchmarks show that FSAM consistently outperforms the vanilla SAM by 0.67 1.98 average score among four different pretrained models. We also empirically show that FSAM works well in other complex scenarios, e.g., fine-tuning on generation tasks or limited training data. Encouragingly, when training data is limited, FSAM improves the SAM by a large margin, i.e., up to 15.1.</abstract>
      <url hash="37c3074a">2022.findings-emnlp.300</url>
      <bibkey>zhong-etal-2022-improving</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.300</doi>
    </paper>
    <paper id="301">
      <title><fixed-case>TINA</fixed-case>: Textual Inference with Negation Augmentation</title>
      <author><first>Chadi</first><last>Helwe</last></author>
      <author><first>Simon</first><last>Coumes</last></author>
      <author><first>Chloé</first><last>Clavel</last></author>
      <author><first>Fabian</first><last>Suchanek</last></author>
      <pages>4086-4099</pages>
      <abstract>Transformer-based language models achieve state-of-the-art results on several natural language processing tasks. One of these is textual entailment, i.e., the task of determining whether a premise logically entails a hypothesis. However, the models perform poorly on this task when the examples contain negations. In this paper, we propose a new definition of textual entailment that captures also negation. This allows us to develop TINA (Textual Inference with Negation Augmentation), a principled technique for negated data augmentation that can be combined with the unlikelihood loss function. Our experiments with different transformer-based models show that our method can significantly improve the performance of the models on textual entailment datasets with negation – without sacrificing performance on datasets without negation.</abstract>
      <url hash="46da044e">2022.findings-emnlp.301</url>
      <bibkey>helwe-etal-2022-tina</bibkey>
      <video href="2022.findings-emnlp.301.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.301</doi>
    </paper>
    <paper id="302">
      <title>Improving Bilingual Lexicon Induction with Cross-Encoder Reranking</title>
      <author><first>Yaoyiran</first><last>Li</last></author>
      <author><first>Fangyu</first><last>Liu</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <author><first>Anna</first><last>Korhonen</last></author>
      <pages>4100-4116</pages>
      <abstract>Bilingual lexicon induction (BLI) with limited bilingual supervision is a crucial yet challenging task in multilingual NLP. Current state-of-the-art BLI methods rely on the induction of cross-lingual word embeddings (CLWEs) to capture cross-lingual word similarities; such CLWEs are obtained <b>1)</b> via traditional static models (e.g., VecMap), or <b>2)</b> by extracting type-level CLWEs from multilingual pretrained language models (mPLMs), or <b>3)</b> through combining the former two options. In this work, we propose a novel semi-supervised <i>post-hoc</i> reranking method termed <b>BLICEr</b> (<b>BLI</b> with <b>C</b>ross-<b>E</b>ncoder <b>R</b>eranking), applicable to any precalculated CLWE space, which improves their BLI capability. The key idea is to ‘extract’ cross-lingual lexical knowledge from mPLMs, and then combine it with the original CLWEs. This crucial step is done via <b>1)</b> creating a word similarity dataset, comprising positive word pairs (i.e., true translations) and hard negative pairs induced from the original CLWE space, and then <b>2)</b> fine-tuning an mPLM (e.g., mBERT or XLM-R) in a cross-encoder manner to predict the similarity scores. At inference, we <b>3)</b> combine the similarity score from the original CLWE space with the score from the BLI-tuned cross-encoder. BLICEr establishes new state-of-the-art results on two standard BLI benchmarks spanning a wide spectrum of diverse languages: it substantially outperforms a series of strong baselines across the board. We also validate the robustness of BLICEr with different CLWEs.</abstract>
      <url hash="599d070b">2022.findings-emnlp.302</url>
      <bibkey>li-etal-2022-improving-bilingual</bibkey>
      <video href="2022.findings-emnlp.302.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.302</doi>
    </paper>
    <paper id="303">
      <title>Mixed-modality Representation Learning and Pre-training for Joint Table-and-Text Retrieval in <fixed-case>O</fixed-case>pen<fixed-case>QA</fixed-case></title>
      <author><first>Junjie</first><last>Huang</last></author>
      <author><first>Wanjun</first><last>Zhong</last></author>
      <author><first>Qian</first><last>Liu</last></author>
      <author><first>Ming</first><last>Gong</last></author>
      <author><first>Daxin</first><last>Jiang</last></author>
      <author><first>Nan</first><last>Duan</last></author>
      <pages>4117-4129</pages>
      <abstract>Retrieving evidences from tabular and textual resources is essential for open-domain question answering (OpenQA), which provides more comprehensive information. However, training an effective dense table-text retriever is difficult due to the challenges of table-text discrepancy and data sparsity problem. To address the above challenges, we introduce an optimized OpenQA Table-Text Retriever (OTTeR) to jointly retrieve tabular and textual evidences. Firstly, we propose to enhance mixed-modality representation learning via two mechanisms: modality-enhanced representation and mixed-modality negative sampling strategy. Secondly, to alleviate data sparsity problem and enhance the general retrieval ability, we conduct retrieval-centric mixed-modality synthetic pre-training. Experimental results demonstrate that OTTeR substantially improves the performance of table-and-text retrieval on the OTT-QA dataset. Comprehensive analyses examine the effectiveness of all the proposed mechanisms. Besides, equipped with OTTeR, our OpenQA system achieves the state-of-the-art result on the downstream QA task, with 10.1% absolute improvement in terms of the exact match over the previous best system.</abstract>
      <url hash="b9d072c2">2022.findings-emnlp.303</url>
      <bibkey>huang-etal-2022-mixed</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.303</doi>
    </paper>
    <paper id="304">
      <title>The Effects of Corpus Choice and Morphosyntax on Multilingual Space Induction</title>
      <author><first>Vinit</first><last>Ravishankar</last></author>
      <author><first>Joakim</first><last>Nivre</last></author>
      <pages>4130-4139</pages>
      <abstract>In an effort to study the inductive biases of language models, numerous studies have attempted to use linguistically motivated tasks as a proxy of sorts, wherein performance on these tasks would imply an inductive bias towards a specific linguistic phenomenon. In this study, we attempt to analyse the inductive biases of language models with respect to natural language phenomena, in the context of building multilingual embedding spaces. We sample corpora from 2 sources in 15 languages and train language models on pseudo-bilingual variants of each corpus, created by duplicating each corpus and shifting token indices for half the resulting corpus. We evaluate the cross-lingual capabilities of these LMs, and show that while correlations with language families tend to be weak, other corpus-level characteristics, such as type-token ratio, tend to be more strongly correlated. Finally, we show that multilingual spaces can be built, albeit less effectively, even when additional destructive perturbations are applied to the training corpora, implying that (effectively) bag-of-words models also have an inductive bias that is sufficient for inducing multilingual spaces.</abstract>
      <url hash="b39ab629">2022.findings-emnlp.304</url>
      <bibkey>ravishankar-nivre-2022-effects</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.304</doi>
    </paper>
    <paper id="305">
      <title>Modeling Complex Dialogue Mappings via Sentence Semantic Segmentation Guided Conditional Variational Auto-Encoder</title>
      <author><first>Bin</first><last>Sun</last></author>
      <author><first>Shaoxiong</first><last>Feng</last></author>
      <author><first>Yiwei</first><last>Li</last></author>
      <author><first>Weichao</first><last>Wang</last></author>
      <author><first>Fei</first><last>Mi</last></author>
      <author><first>Yitong</first><last>Li</last></author>
      <author><first>Kan</first><last>Li</last></author>
      <pages>4140-4153</pages>
      <abstract>Complex dialogue mappings (CDM), including one-to-many and many-to-one mappings, tend to make dialogue models generate incoherent or dull responses, and modeling these mappings remains a huge challenge for neural dialogue systems. To alleviate these problems, methods like introducing external information, reconstructing the optimization function, and manipulating data samples are proposed, while they primarily focus on avoiding training with CDM, inevitably weakening the model’s ability of understanding CDM in human conversations and limiting further improvements in model performance. This paper proposes a Sentence Semantic Segmentation guided Conditional Variational Auto-Encoder (SegCVAE) method which can model and take advantages of the CDM data. Specifically, to tackle the incoherent problem caused by one-to-many, SegCVAE uses response-related prominent semantics to constrained the latent variable. To mitigate the non-diverse problem brought by many-to-one, SegCVAE segments multiple prominent semantics to enrich the latent variables. Three novel components, Internal Separation, External Guidance, and Semantic Norms, are proposed to achieve SegCVAE. On dialogue generation tasks, both the automatic and human evaluation results show that SegCVAE achieves new state-of-the-art performance.</abstract>
      <url hash="bd79ac83">2022.findings-emnlp.305</url>
      <bibkey>sun-etal-2022-modeling</bibkey>
      <video href="2022.findings-emnlp.305.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.305</doi>
    </paper>
    <paper id="306">
      <title>Graph Embeddings for Argumentation Quality Assessment</title>
      <author><first>Santiago</first><last>Marro</last></author>
      <author><first>Elena</first><last>Cabrio</last></author>
      <author><first>Serena</first><last>Villata</last></author>
      <pages>4154-4164</pages>
      <abstract>Argumentation is used by people both internally, by evaluating arguments and counterarguments to make sense of a situation and take a decision, and externally, e.g., in a debate, by exchanging arguments to reach an agreement or to promote an individual position. In this context, the assessment of the quality of the arguments is of extreme importance, as it strongly influences the evaluation of the overall argumentation, impacting on the decision making process. The automatic assessment of the quality of natural language arguments is recently attracting interest in the Argument Mining field. However, the issue of automatically assessing the quality of an argumentation largely remains a challenging unsolved task. Our contribution is twofold: first, we present a novel resource of 402 student persuasive essays, where three main quality dimensions (i.e., cogency, rhetoric, and reasonableness) have been annotated, leading to 1908 arguments tagged with quality facets; second, we address this novel task of argumentation quality assessment proposing a novel neural architecture based on graph embeddings, that combines both the textual features of the natural language arguments and the overall argument graph, i.e., considering also the support and attack relations holding among the arguments. Results on the persuasive essays dataset outperform state-of-the-art and standard baselines’ performance.</abstract>
      <url hash="0933a9b4">2022.findings-emnlp.306</url>
      <bibkey>marro-etal-2022-graph</bibkey>
      <video href="2022.findings-emnlp.306.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.306</doi>
    </paper>
    <paper id="307">
      <title><fixed-case>SM</fixed-case>i<fixed-case>LE</fixed-case>: Schema-augmented Multi-level Contrastive Learning for Knowledge Graph Link Prediction</title>
      <author><first>Miao</first><last>Peng</last></author>
      <author><first>Ben</first><last>Liu</last></author>
      <author><first>Qianqian</first><last>Xie</last></author>
      <author><first>Wenjie</first><last>Xu</last></author>
      <author><first>Hua</first><last>Wang</last></author>
      <author><first>Min</first><last>Peng</last></author>
      <pages>4165-4177</pages>
      <abstract>Link prediction is the task of inferring missing links between entities in knowledge graphs. Embedding-based methods have shown effectiveness in addressing this problem by modeling relational patterns in triples. However, the link prediction task often requires contextual information in entity neighborhoods, while most existing embedding-based methods fail to capture it. Additionally, little attention is paid to the diversity of entity representations in different contexts, which often leads to false prediction results. In this situation, we consider that the schema of knowledge graph contains the specific contextual information, and it is beneficial for preserving the consistency of entities across contexts. In this paper, we propose a novel Schema-augmented Multi-level contrastive LEarning framework (SMiLE) to conduct knowledge graph link prediction. Specifically, we first exploit network schema as the prior constraint to sample negatives and pre-train our model by employing a multi-level contrastive learning method to yield both prior schema and contextual information. Then we fine-tune our model under the supervision of individual triples to learn subtler representations for link prediction. Extensive experimental results on four knowledge graph datasets with thorough analysis of each component demonstrate the effectiveness of our proposed framework against state-of-the-art baselines. The implementation of SMiLE is available at <url>https://github.com/GKNL/SMiLE</url>.</abstract>
      <url hash="17559811">2022.findings-emnlp.307</url>
      <bibkey>peng-etal-2022-smile</bibkey>
      <video href="2022.findings-emnlp.307.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.307</doi>
    </paper>
    <paper id="308">
      <title>Multilingual Multimodal Learning with Machine Translated Text</title>
      <author><first>Chen</first><last>Qiu</last></author>
      <author><first>Dan</first><last>Oneață</last></author>
      <author><first>Emanuele</first><last>Bugliarello</last></author>
      <author><first>Stella</first><last>Frank</last></author>
      <author><first>Desmond</first><last>Elliott</last></author>
      <pages>4178-4193</pages>
      <abstract>Most vision-and-language pretraining research focuses on English tasks. However, the creation of multilingual multimodal evaluation datasets (e.g. Multi30K, xGQA, XVNLI, and MaRVL) poses a new challenge in finding high-quality training data that is both multilingual and multimodal. In this paper, we investigate whether machine translating English multimodal data can be an effective proxy for the lack of readily available multilingual data. We call this framework TD-MML: Translated Data for Multilingual Multimodal Learning, and it can be applied to any multimodal dataset and model. We apply it to both pretraining and fine-tuning data with a state-of-the-art model. In order to prevent models from learning from low-quality translated text, we propose two metrics for automatically removing such translations from the resulting datasets. In experiments on five tasks across 20 languages in the IGLUE benchmark, we show that translated data can provide a useful signal for multilingual multimodal learning, both at pretraining and fine-tuning.</abstract>
      <url hash="84a323e9">2022.findings-emnlp.308</url>
      <bibkey>qiu-etal-2022-multilingual</bibkey>
      <video href="2022.findings-emnlp.308.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.308</doi>
    </paper>
    <paper id="309">
      <title>Learning From the Source Document: Unsupervised Abstractive Summarization</title>
      <author><first>Haojie</first><last>Zhuang</last></author>
      <author><first>Wei Emma</first><last>Zhang</last></author>
      <author><first>Jian</first><last>Yang</last></author>
      <author><first>Congbo</first><last>Ma</last></author>
      <author><first>Yutong</first><last>Qu</last></author>
      <author><first>Quan Z.</first><last>Sheng</last></author>
      <pages>4194-4205</pages>
      <abstract>Most of the state-of-the-art methods for abstractive text summarization are under supervised learning settings, while heavily relying on high-quality and large-scale parallel corpora. In this paper, we remove the need for reference summaries and present an unsupervised learning method SCR (Summarize, Contrast and Review) for abstractive summarization, which leverages contrastive learning and is the first work to apply contrastive learning for unsupervised abstractive summarization. Particularly, we use the true source documents as positive source document examples, and strategically generated fake source documents as negative source document examples to train the model to generate good summaries. Furthermore, we consider and improve the writing quality of the generated summaries by guiding them to be similar to human-written texts. The promising results on extensive experiments show that SCR outperforms other unsupervised abstractive summarization baselines, which demonstrates its effectiveness.</abstract>
      <url hash="df108277">2022.findings-emnlp.309</url>
      <bibkey>zhuang-etal-2022-learning</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.309</doi>
    </paper>
    <paper id="310">
      <title>How to Do Things without Words: Modeling Semantic Drift of Emoji</title>
      <author><first>Eyal</first><last>Arviv</last></author>
      <author><first>Oren</first><last>Tsur</last></author>
      <pages>4206-4211</pages>
      <abstract>Emoji have become a significant part of our informal textual communication. Previous work, addressing the societal and linguistic functions of emoji, overlooked the relation between the semantics and the visual variations of the symbols. In this paper we model and analyze the semantic drift of emoji and discuss the features that may be contributing to the drift, some are unique to emoji and some are more general. Specifically, we explore the relations between graphical changes and semantic changes.</abstract>
      <url hash="8cdb9357">2022.findings-emnlp.310</url>
      <bibkey>arviv-tsur-2022-things</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.310</doi>
    </paper>
    <paper id="311">
      <title>Mind Your Bias: A Critical Review of Bias Detection Methods for Contextual Language Models</title>
      <author><first>Silke</first><last>Husse</last></author>
      <author><first>Andreas</first><last>Spitz</last></author>
      <pages>4212-4234</pages>
      <abstract>The awareness and mitigation of biases are of fundamental importance for the fair and transparent use of contextual language models, yet they crucially depend on the accurate detection of biases as a precursor. Consequently, numerous bias detection methods have been proposed, which vary in their approach, the considered type of bias, and the data used for evaluation. However, while most detection methods are derived from the word embedding association test for static word embeddings, the reported results are heterogeneous, inconsistent, and ultimately inconclusive. To address this issue, we conduct a rigorous analysis and comparison of bias detection methods for contextual language models. Our results show that minor design and implementation decisions (or errors) have a substantial and often significant impact on the derived bias scores. Overall, we find the state of the field to be both worse than previously acknowledged due to systematic and propagated errors in implementations, yet better than anticipated since divergent results in the literature homogenize after accounting for implementation errors. Based on our findings, we conclude with a discussion of paths towards more robust and consistent bias detection methods.</abstract>
      <url hash="9dc378b3">2022.findings-emnlp.311</url>
      <bibkey>husse-spitz-2022-mind</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.311</doi>
    </paper>
    <paper id="312">
      <title><fixed-case>Z</fixed-case>ero<fixed-case>P</fixed-case>rompt: Scaling Prompt-Based Pretraining to 1,000 Tasks Improves Zero-Shot Generalization</title>
      <author><first>Hanwei</first><last>Xu</last></author>
      <author><first>Yujun</first><last>Chen</last></author>
      <author><first>Yulun</first><last>Du</last></author>
      <author><first>Nan</first><last>Shao</last></author>
      <author><first>Wang</first><last>Yanggang</last></author>
      <author><first>Haiyu</first><last>Li</last></author>
      <author><first>Zhilin</first><last>Yang</last></author>
      <pages>4235-4252</pages>
      <abstract>We propose a multitask pretraining approach ZeroPrompt for zero-shot generalization, focusing on task scaling and zero-shot prompting. While previous models are trained on only a few dozen tasks, we scale to 1,000 tasks for the first time using real-world data. This leads to a crucial discovery that task scaling can be an efficient alternative to model scaling; i.e., the model size has less impact on performance with an extremely large number of tasks. Our results show that task scaling can improve training efficiency by 30 times in FLOPs.Empirically, ZeroPrompt substantially improves both the efficiency and the performance of zero-shot learning across a variety of academic and production datasets.</abstract>
      <url hash="25268995">2022.findings-emnlp.312</url>
      <bibkey>xu-etal-2022-zeroprompt</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.312</doi>
    </paper>
    <paper id="313">
      <title>Semantic Role Labeling Meets Definition Modeling: Using Natural Language to Describe Predicate-Argument Structures</title>
      <author><first>Simone</first><last>Conia</last></author>
      <author><first>Edoardo</first><last>Barba</last></author>
      <author><first>Alessandro</first><last>Scirè</last></author>
      <author><first>Roberto</first><last>Navigli</last></author>
      <pages>4253-4270</pages>
      <abstract>One of the common traits of past and present approaches for Semantic Role Labeling (SRL) is that they rely upon discrete labels drawn from a predefined linguistic inventory to classify predicate senses and their arguments. However, we argue this need not be the case. In this paper, we present an approach that leverages Definition Modeling to introduce a generalized formulation of SRL as the task of describing predicate-argument structures using natural language definitions instead of discrete labels. Our novel formulation takes a first step towards placing interpretability and flexibility foremost, and yet our experiments and analyses on PropBank-style and FrameNet-style, dependency-based and span-based SRL also demonstrate that a flexible model with an interpretable output does not necessarily come at the expense of performance. We release our software for research purposes at <url>https://github.com/SapienzaNLP/dsrl</url>.</abstract>
      <url hash="91da7c36">2022.findings-emnlp.313</url>
      <bibkey>conia-etal-2022-semantic</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.313</doi>
    </paper>
    <paper id="314">
      <title>Is anisotropy really the cause of <fixed-case>BERT</fixed-case> embeddings not being semantic?</title>
      <author><first>Alejandro</first><last>Fuster Baggetto</last></author>
      <author><first>Victor</first><last>Fresno</last></author>
      <pages>4271-4281</pages>
      <abstract>In this paper we conduct a set of experiments aimed to improve our understanding of the lack of semantic isometry in BERT, i.e. the lack of correspondence between the embedding and meaning spaces of its contextualized word representations. Our empirical results show that, contrary to popular belief, the anisotropy is not the root cause of the poor performance of these contextual models’ embeddings in semantic tasks. What does affect both the anisotropy and semantic isometry is a set of known biases: frequency, subword, punctuation, and case. For each one of them, we measure its magnitude and the effect of its removal, showing that these biases contribute but do not completely explain the phenomenon of anisotropy and lack of semantic isometry of these contextual language models.</abstract>
      <url hash="3d53286f">2022.findings-emnlp.314</url>
      <bibkey>fuster-baggetto-fresno-2022-anisotropy</bibkey>
      <video href="2022.findings-emnlp.314.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.314</doi>
    </paper>
    <paper id="315">
      <title>m^4 Adapter: Multilingual Multi-Domain Adaptation for Machine Translation with a Meta-Adapter</title>
      <author><first>Wen</first><last>Lai</last></author>
      <author><first>Alexandra</first><last>Chronopoulou</last></author>
      <author><first>Alexander</first><last>Fraser</last></author>
      <pages>4282-4296</pages>
      <abstract>Multilingual neural machine translation models (MNMT) yield state-of-the-art performance when evaluated on data from a domain and language pair seen at training time. However, when a MNMT model is used to translate under domain shift or to a new language pair, performance drops dramatically. We consider a very challenging scenario: adapting the MNMT model both to a new domain and to a new language pair at the same time. In this paper, we propose m^4Adapter (Multilingual Multi-Domain Adaptation for Machine Translation with a Meta-Adapter), which combines domain and language knowledge using meta-learning with adapters. We present results showing that our approach is a parameter-efficient solution which effectively adapts a model to both a new language pair and a new domain, while outperforming other adapter methods. An ablation study also shows that our approach more effectively transfers domain knowledge across different languages and language information across different domains.</abstract>
      <url hash="9c4f5358">2022.findings-emnlp.315</url>
      <bibkey>lai-etal-2022-4</bibkey>
      <video href="2022.findings-emnlp.315.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.315</doi>
    </paper>
    <paper id="316">
      <title>Textual Enhanced Contrastive Learning for Solving Math Word Problems</title>
      <author><first>Yibin</first><last>Shen</last></author>
      <author><first>Qianying</first><last>Liu</last></author>
      <author><first>Zhuoyuan</first><last>Mao</last></author>
      <author><first>Fei</first><last>Cheng</last></author>
      <author><first>Sadao</first><last>Kurohashi</last></author>
      <pages>4297-4307</pages>
      <abstract>Solving math word problems is the task that analyses the relation of quantities and requires an accurate understanding of contextual natural language information. Recent studies show that current models rely on shallow heuristics to predict solutions and could be easily misled by small textual perturbations. To address this problem, we propose a Textual Enhanced Contrastive Learning framework, which enforces the models to distinguish semantically similar examples while holding different mathematical logic. We adopt a self-supervised manner strategy to enrich examples with subtle textual variance by textual reordering or problem re-construction. We then retrieve the hardest to differentiate samples from both equation and textual perspectives and guide the model to learn their representations. Experimental results show that our method achieves state-of-the-art on both widely used benchmark datasets and also exquisitely designed challenge datasets in English and Chinese.</abstract>
      <url hash="4aa89d59">2022.findings-emnlp.316</url>
      <bibkey>shen-etal-2022-textual</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.316</doi>
    </paper>
    <paper id="317">
      <title>What Do Compressed Multilingual Machine Translation Models Forget?</title>
      <author><first>Alireza</first><last>Mohammadshahi</last></author>
      <author><first>Vassilina</first><last>Nikoulina</last></author>
      <author><first>Alexandre</first><last>Berard</last></author>
      <author><first>Caroline</first><last>Brun</last></author>
      <author><first>James</first><last>Henderson</last></author>
      <author><first>Laurent</first><last>Besacier</last></author>
      <pages>4308-4329</pages>
      <abstract>Recently, very large pre-trained models achieve state-of-the-art results in various natural language processing (NLP) tasks, but their size makes it more challenging to apply them in resource-constrained environments. Compression techniques allow to drastically reduce the size of the models and therefore their inference time with negligible impact on top-tier metrics. However, the general performance averaged across multiple tasks and/or languages may hide a drastic performance drop on under-represented features, which could result in the amplification of biases encoded by the models. In this work, we assess the impact of compression methods on Multilingual Neural Machine Translation models (MNMT) for various language groups, gender, and semantic biases by extensive analysis of compressed models on different machine translation benchmarks, i.e. FLORES-101, MT-Gender, and DiBiMT. We show that the performance of under-represented languages drops significantly, while the average BLEU metric only slightly decreases. Interestingly, the removal of noisy memorization with compression leads to a significant improvement for some medium-resource languages. Finally, we demonstrate that compression amplifies intrinsic gender and semantic biases, even in high-resource languages.</abstract>
      <url hash="19f557a4">2022.findings-emnlp.317</url>
      <bibkey>mohammadshahi-etal-2022-compressed</bibkey>
      <video href="2022.findings-emnlp.317.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.317</doi>
    </paper>
    <paper id="318">
      <title>Controllable Dialogue Simulation with In-context Learning</title>
      <author><first>Zekun</first><last>Li</last></author>
      <author><first>Wenhu</first><last>Chen</last></author>
      <author><first>Shiyang</first><last>Li</last></author>
      <author><first>Hong</first><last>Wang</last></author>
      <author><first>Jing</first><last>Qian</last></author>
      <author><first>Xifeng</first><last>Yan</last></author>
      <pages>4330-4347</pages>
      <abstract>Building dialogue systems requires a large corpus of annotated dialogues. Such datasets are usually created via crowdsourcing, which is expensive and time-consuming. In this paper, we propose Dialogic, a novel dialogue simulation method based on large language model in-context learning to automate dataset creation. Seeded with a few annotated dialogues, Dialogic automatically selects in-context examples for demonstration and prompts GPT-3 to generate new dialogues and annotations in a controllable way. Our method can rapidly expand a small set of dialogue data with minimum or zero <i>human involvement</i> and <i>parameter update</i> and is thus much more cost-efficient and time-saving than crowdsourcing. Experimental results on the MultiWOZ dataset demonstrate that training a model on the simulated dialogues leads to even better performance than using the same amount of human-generated dialogues under the challenging low-resource settings, with as few as 85 dialogues as a seed. When the full training set is given, our method can still serve as an effective data augmentation method to further improve performance. Human evaluation results also show that our simulated dialogues have near-human fluency and annotation accuracy. The code and data are available at <b>
          <url>https://github.com/Leezekun/dialogic</url>
 </b>.</abstract>
      <url hash="2ee83aa7">2022.findings-emnlp.318</url>
      <bibkey>li-etal-2022-controllable</bibkey>
      <video href="2022.findings-emnlp.318.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.318</doi>
    </paper>
    <paper id="319">
      <title>Improving the Factual Correctness of Radiology Report Generation with Semantic Rewards</title>
      <author><first>Jean-Benoit</first><last>Delbrouck</last></author>
      <author><first>Pierre</first><last>Chambon</last></author>
      <author><first>Christian</first><last>Bluethgen</last></author>
      <author><first>Emily</first><last>Tsai</last></author>
      <author><first>Omar</first><last>Almusa</last></author>
      <author><first>Curtis</first><last>Langlotz</last></author>
      <pages>4348-4360</pages>
      <abstract>Neural image-to-text radiology report generation systems offer the potential to improve radiology reporting by reducing the repetitive process of report drafting and identifying possible medical errors. These systems have achieved promising performance as measured by widely used NLG metrics such as BLEU and CIDEr. However, the current systems face important limitations. First, they present an increased complexity in architecture that offers only marginal improvements on NLG metrics. Secondly, these systems that achieve high performance on these metrics are not always factually complete or consistent due to both inadequate training and evaluation. Recent studies have shown the systems can be substantially improved by using new methods encouraging 1) the generation of domain entities consistent with the reference and 2) describing these entities in inferentially consistent ways. So far, these methods rely on weakly-supervised approaches (rule-based) and named entity recognition systems that are not specific to the chest X-ray domain. To overcome this limitation, we propose a new method, the RadGraph reward, to further improve the factual completeness and correctness of generated radiology reports. More precisely, we leverage the RadGraph dataset containing annotated chest X-ray reports with entities and relations between entities. On two open radiology report datasets, our system substantially improves the scores up to 14.2% and 25.3% on metrics evaluating the factual correctness and completeness of reports.</abstract>
      <url hash="1bc8cf5f">2022.findings-emnlp.319</url>
      <bibkey>delbrouck-etal-2022-improving</bibkey>
      <video href="2022.findings-emnlp.319.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.319</doi>
    </paper>
    <paper id="320">
      <title>Recursive Neural Networks with Bottlenecks Diagnose (Non-)Compositionality</title>
      <author><first>Verna</first><last>Dankers</last></author>
      <author><first>Ivan</first><last>Titov</last></author>
      <pages>4361-4378</pages>
      <abstract>A recent line of work in NLP focuses on the (dis)ability of models to generalise compositionally for artificial languages. However, when considering natural language tasks, the data involved is not strictly, or locally, compositional. Quantifying the compositionality of data is a challenging task, which has been investigated primarily for short utterances. We use recursive neural models (Tree-LSTMs) with bottlenecks that limit the transfer of information between nodes. We illustrate that comparing data’s representations in models with and without the bottleneck can be used to produce a compositionality metric. The procedure is applied to the evaluation of arithmetic expressions using synthetic data, and sentiment classification using natural language data. We demonstrate that compression through a bottleneck impacts non-compositional examples disproportionatelyand then use the bottleneck compositionality metric (BCM) to distinguish compositional from non-compositional samples, yielding a compositionality ranking over a dataset.</abstract>
      <url hash="55398842">2022.findings-emnlp.320</url>
      <bibkey>dankers-titov-2022-recursive</bibkey>
      <video href="2022.findings-emnlp.320.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.320</doi>
    </paper>
    <paper id="321">
      <title><fixed-case>H</fixed-case>um<fixed-case>S</fixed-case>et: Dataset of Multilingual Information Extraction and Classification for Humanitarian Crises Response</title>
      <author><first>Selim</first><last>Fekih</last></author>
      <author><first>Nicolo’</first><last>Tamagnone</last></author>
      <author><first>Benjamin</first><last>Minixhofer</last></author>
      <author><first>Ranjan</first><last>Shrestha</last></author>
      <author><first>Ximena</first><last>Contla</last></author>
      <author><first>Ewan</first><last>Oglethorpe</last></author>
      <author><first>Navid</first><last>Rekabsaz</last></author>
      <pages>4379-4389</pages>
      <abstract>Timely and effective response to humanitarian crises requires quick and accurate analysis of large amounts of text data – a process that can highly benefit from expert-assisted NLP systems trained on validated and annotated data in the humanitarian response domain. To enable creation of such NLP systems, we introduce and release HumSet, a novel and rich multilingual dataset of humanitarian response documents annotated by experts in the humanitarian response community. The dataset provides documents in three languages (English, French, Spanish) and covers a variety of humanitarian crises from 2018 to 2021 across the globe. For each document, HUMSET provides selected snippets (entries) as well as assigned classes to each entry annotated using common humanitarian information analysis frameworks. HUMSET also provides novel and challenging entry extraction and multi-label entry classification tasks. In this paper, we take a first step towards approaching these tasks and conduct a set of experiments on Pre-trained Language Models (PLM) to establish strong baselines for future research in this domain. The dataset is available at <url>https://blog.thedeep.io/humset/</url>.</abstract>
      <url hash="90d8de05">2022.findings-emnlp.321</url>
      <bibkey>fekih-etal-2022-humset</bibkey>
      <video href="2022.findings-emnlp.321.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.321</doi>
    </paper>
    <paper id="322">
      <title><fixed-case>V</fixed-case>iterbi Decoding of Directed Acyclic Transformer for Non-Autoregressive Machine Translation</title>
      <author><first>Chenze</first><last>Shao</last></author>
      <author><first>Zhengrui</first><last>Ma</last></author>
      <author><first>Yang</first><last>Feng</last></author>
      <pages>4390-4397</pages>
      <abstract>Non-autoregressive models achieve significant decoding speedup in neural machine translation but lack the ability to capture sequential dependency. Directed Acyclic Transformer (DA-Transformer) was recently proposed to model sequential dependency with a directed acyclic graph. Consequently, it has to apply a sequential decision process at inference time, which harms the global translation accuracy. In this paper, we present a Viterbi decoding framework for DA-Transformer, which guarantees to find the joint optimal solution for the translation and decoding path under any length constraint. Experimental results demonstrate that our approach consistently improves the performance of DA-Transformer while maintaining a similar decoding speedup.</abstract>
      <url hash="9220c4e1">2022.findings-emnlp.322</url>
      <bibkey>shao-etal-2022-viterbi</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.322</doi>
    </paper>
    <paper id="323">
      <title>Lexical Generalization Improves with Larger Models and Longer Training</title>
      <author><first>Elron</first><last>Bandel</last></author>
      <author><first>Yoav</first><last>Goldberg</last></author>
      <author><first>Yanai</first><last>Elazar</last></author>
      <pages>4398-4410</pages>
      <abstract>While fine-tuned language models perform well on many language tasks, they were also shown to rely on superficial surface features such as lexical overlap. Excessive utilization of such heuristics can lead to failure on challenging inputs. We analyze the use of lexical overlap heuristics in natural language inference, paraphrase detection, and reading comprehension (using a novel contrastive dataset),and find that larger models are much less susceptible to adopting lexical overlap heuristics. We also find that longer training leads models to abandon lexical overlap heuristics. Finally, We provide evidence that the disparity between models size has its source in the pre-trained model.</abstract>
      <url hash="e31aec2b">2022.findings-emnlp.323</url>
      <bibkey>bandel-etal-2022-lexical</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.323</doi>
    </paper>
    <paper id="324">
      <title>Realistic Data Augmentation Framework for Enhancing Tabular Reasoning</title>
      <author><first>Dibyakanti</first><last>Kumar</last></author>
      <author><first>Vivek</first><last>Gupta</last></author>
      <author><first>Soumya</first><last>Sharma</last></author>
      <author><first>Shuo</first><last>Zhang</last></author>
      <pages>4411-4429</pages>
      <abstract>Existing approaches to constructing training data for Natural Language Inference (NLI) tasks, such as for semi-structured table reasoning, are either via crowdsourcing or fully automatic methods. However, the former is expensive and time consuming and thus limits scale, and the latter often produces naive examples that may lack complex reasoning. This paper develops a realistic semi-automated framework for data augmentation for tabular inference. Instead of manually generating a hypothesis for each table, our methodology generates hypothesis templates transferable to similar tables. In addition, our framework entails the creation of rational counterfactual tables based on human written logical constraints and premise paraphrasing. For our case study, we use the INFOTABS (Gupta et al., 2020), which is an entity centric tabular inference dataset. We observed that our framework could generate human-like tabular inference examples, which could benefit training data augmentation, especially in the scenario with limited supervision.</abstract>
      <url hash="e55bc64b">2022.findings-emnlp.324</url>
      <bibkey>kumar-etal-2022-realistic</bibkey>
      <video href="2022.findings-emnlp.324.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.324</doi>
    </paper>
    <paper id="325">
      <title>Inducing Generalizable and Interpretable Lexica</title>
      <author><first>Yilin</first><last>Geng</last></author>
      <author><first>Zetian</first><last>Wu</last></author>
      <author><first>Roshan</first><last>Santhosh</last></author>
      <author><first>Tejas</first><last>Srivastava</last></author>
      <author><first>Lyle</first><last>Ungar</last></author>
      <author><first>João</first><last>Sedoc</last></author>
      <pages>4430-4448</pages>
      <abstract>Lexica – words and associated scores – are widely used as simple, interpretable, generalizable language features to predict sentiment, emotions, mental health, and personality. They also provide insight into the psychological features behind those moods and traits. Such lexica, historically created by human experts, are valuable to linguists, psychologists, and social scientists, but they take years of refinement and have limited coverage. In this paper, we investigate how the lexica that provide psycholinguistic insights could be computationally induced and how they should be assessed. We identify generalizability and interpretability as two essential properties of such lexica. We induce lexica using both context-oblivious and context-aware approaches, compare their predictive performance both within the training corpus and across various corpora, and evaluate their quality using crowd-worker assessment. We find that lexica induced from context-oblivious models are more generalizable and interpretable than those from more accurate context-aware transformer models. In addition, lexicon scores can identify explanatory words more reliably than a high performing transformer with feature-importance measures like SHAP.</abstract>
      <url hash="3fb619e8">2022.findings-emnlp.325</url>
      <bibkey>geng-etal-2022-inducing</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.325</doi>
    </paper>
    <paper id="326">
      <title>The Curious Case of Absolute Position Embeddings</title>
      <author><first>Koustuv</first><last>Sinha</last></author>
      <author><first>Amirhossein</first><last>Kazemnejad</last></author>
      <author><first>Siva</first><last>Reddy</last></author>
      <author><first>Joelle</first><last>Pineau</last></author>
      <author><first>Dieuwke</first><last>Hupkes</last></author>
      <author><first>Adina</first><last>Williams</last></author>
      <pages>4449-4472</pages>
      <abstract>Transformer language models encode the notion of word order using positional information. Most commonly, this positional information is represented by absolute position embeddings (APEs), that are learned from the pretraining data. However, in natural language, it is not absolute position that matters, but relative position, and the extent to which APEs can capture this type of information has not been studied. In this work, we observe that models trained with APE over-rely on positional information to the point that they break-down when subjected to sentences with shifted position information. Specifically, when models are subjected to sentences starting from a non-zero position (excluding the effect of priming), they exhibit noticeably degraded performance on zero- to full-shot tasks, across a range of model families and model sizes. Our findings raise questions about the efficacy of APEs to model the relativity of position information, and invite further introspection on the sentence and word order processing strategies employed by these models.</abstract>
      <url hash="3c86b738">2022.findings-emnlp.326</url>
      <bibkey>sinha-etal-2022-curious</bibkey>
      <video href="2022.findings-emnlp.326.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.326</doi>
    </paper>
    <paper id="327">
      <title>Goal-oriented Vision-and-Dialog Navigation via Reinforcement Learning</title>
      <author><first>Yan</first><last>Cao</last></author>
      <author><first>Keting</first><last>Lu</last></author>
      <author><first>David</first><last>DeFazio</last></author>
      <author><first>Shiqi</first><last>Zhang</last></author>
      <pages>4473-4482</pages>
      <abstract>Vision-and-dialog navigation is a recent benchmark for evaluating the AI capabilities of perception, interaction, and decision making. While existing methods developed for this benchmark have demonstrated great successes, they mostly rely on large datasets, where data collection can be a challenge, and the learned policies are not adaptive to domain changes. In this paper, we focus on a new problem, referred to as goal-oriented vision-and-dialog navigation (GVDN), where an agent uses reinforcement learning techniques to compute dialog-navigation policies from trial and error. A robot conducts visual navigation to locate target objects, and can talk to a remote human operator as needed. Our remote human is able to provide guidance on navigation only if the robot correctly conveys its location through dialog. Experiments have been conducted using photo-realistic simulation environments. Results suggest that, our agent outperforms competitive baselines in success rate.</abstract>
      <url hash="2f9659af">2022.findings-emnlp.327</url>
      <bibkey>cao-etal-2022-goal</bibkey>
      <video href="2022.findings-emnlp.327.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.327</doi>
    </paper>
    <paper id="328">
      <title>Leveraging Data Recasting to Enhance Tabular Reasoning</title>
      <author><first>Aashna</first><last>Jena</last></author>
      <author><first>Vivek</first><last>Gupta</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <author><first>Julian</first><last>Eisenschlos</last></author>
      <pages>4483-4496</pages>
      <abstract>Creating challenging tabular inference data is essential for learning complex reasoning. Prior work has mostly relied on two data generation strategies. The first is human annotation, which yields linguistically diverse data but is difficult to scale. The second category for creation is synthetic generation, which is scalable and cost effective but lacks inventiveness. In this research, we present a framework for semi-automatically recasting existing tabular data to make use of the benefits of both approaches. We utilize our framework to build tabular NLI instances from five datasets that were initially intended for tasks like table2text creation, tabular Q/A, and semantic parsing. We demonstrate that recasted data could be used as evaluation benchmarks as well as augmentation data to enhance performance on tabular NLI tasks. Furthermore, we investigate the effectiveness of models trained on recasted data in the zero-shot scenario, and analyse trends in performance across different recasted datasets types.</abstract>
      <url hash="0b431084">2022.findings-emnlp.328</url>
      <bibkey>jena-etal-2022-leveraging</bibkey>
      <video href="2022.findings-emnlp.328.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.328</doi>
    </paper>
    <paper id="329">
      <title>Thinking about <fixed-case>GPT</fixed-case>-3 In-Context Learning for Biomedical <fixed-case>IE</fixed-case>? Think Again</title>
      <author><first>Bernal</first><last>Jimenez Gutierrez</last></author>
      <author><first>Nikolas</first><last>McNeal</last></author>
      <author><first>Clayton</first><last>Washington</last></author>
      <author><first>You</first><last>Chen</last></author>
      <author><first>Lang</first><last>Li</last></author>
      <author><first>Huan</first><last>Sun</last></author>
      <author><first>Yu</first><last>Su</last></author>
      <pages>4497-4512</pages>
      <abstract>Large pre-trained language models (PLMs) such as GPT-3 have shown strong in-context learning capabilities, which are highly appealing for domains such as biomedicine that feature high and diverse demands of language technologies but also high data annotation costs. In this paper, we present the first systematic and comprehensive study to compare the few-shot performance of GPT-3 in-context learning with fine-tuning smaller (i.e., BERT-sized) PLMs on two representative biomedical information extraction (IE) tasks: named entity recognition and relation extraction. We follow the true few-shot setting to avoid overestimating models’ few-shot performance by model selection over a large validation set. We also optimize GPT-3’s performance with known techniques such as contextual calibration and dynamic in-context example retrieval. However, our results show that GPT-3 still significantly underperforms compared to simply fine-tuning a smaller PLM. In addition, GPT-3 in-context learning also yields smaller gains in accuracy when more training data becomes available. More in-depth analyses further reveal issues of in-context learning that may be detrimental to IE tasks in general. Given the high cost of experimenting with GPT-3, we hope our study provides helpful guidance for biomedical researchers and practitioners towards more practical solutions such as fine-tuning small PLMs before better in-context learning is available for biomedical IE.</abstract>
      <url hash="a93fb836">2022.findings-emnlp.329</url>
      <bibkey>jimenez-gutierrez-etal-2022-thinking</bibkey>
      <video href="2022.findings-emnlp.329.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.329</doi>
    </paper>
    <paper id="330">
      <title>Attention weights accurately predict language representations in the brain</title>
      <author><first>Mathis</first><last>Lamarre</last></author>
      <author><first>Catherine</first><last>Chen</last></author>
      <author><first>Fatma</first><last>Deniz</last></author>
      <pages>4513-4529</pages>
      <abstract>In Transformer-based language models (LMs) the attention mechanism converts token embeddings into contextual embeddings that incorporate information from neighboring words. The resulting contextual hidden state embeddings have enabled highly accurate models of brain responses, suggesting that the attention mechanism constructs contextual embeddings that carry information reflected in language-related brain representations. However, it is unclear whether the attention weights that are used to integrate information across words are themselves related to language representations in the brain. To address this question we analyzed functional magnetic resonance imaging (fMRI) recordings of participants reading English language narratives. We provided the narrative text as input to two LMs (BERT and GPT-2) and extracted their corresponding attention weights. We then used encoding models to determine how well attention weights can predict recorded brain responses. We find that attention weights accurately predict brain responses in much of the frontal and temporal cortices. Our results suggest that the attention mechanism itself carries information that is reflected in brain representations. Moreover, these results indicate cortical areas in which context integration may occur.</abstract>
      <url hash="b0498748">2022.findings-emnlp.330</url>
      <bibkey>lamarre-etal-2022-attention</bibkey>
      <video href="2022.findings-emnlp.330.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.330</doi>
    </paper>
    <paper id="331">
      <title>Improving <fixed-case>H</fixed-case>ow<fixed-case>N</fixed-case>et-Based <fixed-case>C</fixed-case>hinese Word Sense Disambiguation with Translations</title>
      <author><first>Xiang</first><last>Zhang</last></author>
      <author><first>Bradley</first><last>Hauer</last></author>
      <author><first>Grzegorz</first><last>Kondrak</last></author>
      <pages>4530-4536</pages>
      <abstract>Word sense disambiguation (WSD) is the task of identifying the intended sense of a word in context. While prior work on unsupervised WSD has leveraged lexical knowledge bases, such as WordNet and BabelNet, these resources have proven to be less effective for Chinese. Instead, the most widely used lexical knowledge base for Chinese is HowNet. Previous HowNet-based WSD methods have not exploited contextual translation information. In this paper, we present the first HowNet-based WSD system which combines monolingual contextual information from a pretrained neural language model with bilingual information obtained via machine translation and sense translation information from HowNet. The results of our evaluation experiment on a test set from prior work demonstrate that our new method achieves a new state of the art for unsupervised Chinese WSD.</abstract>
      <url hash="4ab9d0cd">2022.findings-emnlp.331</url>
      <bibkey>zhang-etal-2022-improving-hownet</bibkey>
      <video href="2022.findings-emnlp.331.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.331</doi>
    </paper>
    <paper id="332">
      <title>Mask-then-Fill: A Flexible and Effective Data Augmentation Framework for Event Extraction</title>
      <author><first>Jun</first><last>Gao</last></author>
      <author><first>Changlong</first><last>Yu</last></author>
      <author><first>Wei</first><last>Wang</last></author>
      <author><first>Huan</first><last>Zhao</last></author>
      <author><first>Ruifeng</first><last>Xu</last></author>
      <pages>4537-4544</pages>
      <abstract>We present Mask-then-Fill, a flexible and effective data augmentation framework for event extraction. Our approach allows for more flexible manipulation of text and thus can generate more diverse data while keeping the original event structure unchanged as much as possible. Specifically, it first randomly masks out an adjunct sentence fragment and then infills a variable-length text span with a fine-tuned infilling model. The main advantage lies in that it can replace a fragment of arbitrary length in the text with another fragment of variable length, compared to the existing methods which can only replace a single word or a fixed-length fragment. On trigger and argument extraction tasks, the proposed framework is more effective than baseline methods and it demonstrates particularly strong results in the low-resource setting. Our further analysis shows that it achieves a good balance between diversity and distributional similarity.</abstract>
      <url hash="23c5193d">2022.findings-emnlp.332</url>
      <bibkey>gao-etal-2022-mask</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.332</doi>
    </paper>
    <paper id="333">
      <title><fixed-case>MOBA</fixed-case>-<fixed-case>E</fixed-case>2<fixed-case>C</fixed-case>: Generating <fixed-case>MOBA</fixed-case> Game Commentaries via Capturing Highlight Events from the Meta-Data</title>
      <author><first>Dawei</first><last>Zhang</last></author>
      <author><first>Sixing</first><last>Wu</last></author>
      <author><first>Yao</first><last>Guo</last></author>
      <author><first>Xiangqun</first><last>Chen</last></author>
      <pages>4545-4556</pages>
      <abstract>MOBA (Multiplayer Online Battle Arena) games such as Dota2 are currently one of the most popular e-sports gaming genres. Following professional commentaries is a great way to understand and enjoy a MOBA game. However, massive game competitions lack commentaries because of the shortage of professional human commentators. As an alternative, employing machine commentators that can work at any time and place is a feasible solution. Considering the challenges in modeling MOBA games, we propose a data-driven MOBA commentary generation framework, MOBA-E2C, allowing a model to generate commentaries based on the game meta-data. Subsequently, to alleviate the burden of collecting supervised data, we propose a MOBA-FuseGPT generator to generate MOBA game commentaries by fusing the power of a rule-based generator and a generative GPT generator. Finally, in the experiments, we take a popular MOBA game Dota2 as our case and construct a Chinese Dota2 commentary generation dataset Dota2-Commentary. Experimental results demonstrate the superior performance of our approach. To the best of our knowledge, this work is the first Dota2 machine commentator and Dota2-Commentary is the first dataset.</abstract>
      <url hash="73d56ebe">2022.findings-emnlp.333</url>
      <bibkey>zhang-etal-2022-moba</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.333</doi>
    </paper>
    <paper id="334">
      <title>Enhancing Automatic Readability Assessment with Pre-training and Soft Labels for Ordinal Regression</title>
      <author><first>Jinshan</first><last>Zeng</last></author>
      <author><first>Yudong</first><last>Xie</last></author>
      <author><first>Xianglong</first><last>Yu</last></author>
      <author><first>John</first><last>Lee</last></author>
      <author><first>Ding-Xuan</first><last>Zhou</last></author>
      <pages>4557-4568</pages>
      <abstract>The readability assessment task aims to assign a difficulty grade to a text. While neural models have recently demonstrated impressive performance, most do not exploit the ordinal nature of the difficulty grades, and make little effort for model initialization to facilitate fine-tuning. We address these limitations with soft labels for ordinal regression, and with model pre-training through prediction of pairwise relative text difficulty. We incorporate these two components into a model based on hierarchical attention networks, and evaluate its performance on both English and Chinese datasets. Experimental results show that our proposed model outperforms competitive neural models and statistical classifiers on most datasets.</abstract>
      <url hash="94e65af1">2022.findings-emnlp.334</url>
      <bibkey>zeng-etal-2022-enhancing</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.334</doi>
    </paper>
    <paper id="335">
      <title>Opening up Minds with Argumentative Dialogues</title>
      <author><first>Youmna</first><last>Farag</last></author>
      <author><first>Charlotte</first><last>Brand</last></author>
      <author><first>Jacopo</first><last>Amidei</last></author>
      <author><first>Paul</first><last>Piwek</last></author>
      <author><first>Tom</first><last>Stafford</last></author>
      <author><first>Svetlana</first><last>Stoyanchev</last></author>
      <author><first>Andreas</first><last>Vlachos</last></author>
      <pages>4569-4582</pages>
      <abstract>Recent research on argumentative dialogues has focused on persuading people to take some action, changing their stance on the topic of discussion, or winning debates. In this work, we focus on argumentative dialogues that aim to open up (rather than change) people’s minds to help them become more understanding to views that are unfamiliar or in opposition to their own convictions. To this end, we present a dataset of 183 argumentative dialogues about 3 controversial topics: veganism, Brexit and COVID-19 vaccination. The dialogues were collected using the Wizard of Oz approach, where wizards leverage a knowledge-base of arguments to converse with participants. Open-mindedness is measured before and after engaging in the dialogue using a questionnaire from the psychology literature, and success of the dialogue is measured as the change in the participant’s stance towards those who hold opinions different to theirs. We evaluate two dialogue models: a Wikipedia-based and an argument-based model. We show that while both models perform closely in terms of opening up minds, the argument-based model is significantly better on other dialogue properties such as engagement and clarity.</abstract>
      <url hash="a32f2550">2022.findings-emnlp.335</url>
      <bibkey>farag-etal-2022-opening</bibkey>
      <video href="2022.findings-emnlp.335.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.335</doi>
    </paper>
    <paper id="336">
      <title>You Are My Type! Type Embeddings for Pre-trained Language Models</title>
      <author><first>Mohammed</first><last>Saeed</last></author>
      <author><first>Paolo</first><last>Papotti</last></author>
      <pages>4583-4598</pages>
      <abstract>One reason for the positive impact of Pre-trained Language Models (PLMs) in NLP tasks is their ability to encode semantic types, such as ‘European City’ or ‘Woman’. While previous work has analyzed such information in the context of interpretability, it is not clear how to use types to steer the PLM output. For example, in a cloze statement, it is desirable to steer the model to generate a token that satisfies a user-specified type, e.g., predict a date rather than a location. In this work, we introduce Type Embeddings (TEs), an input embedding that promotes desired types in a PLM. Our proposal is to define a type by a small set of word examples. We empirically study the ability of TEs both in representing types and in steering masking predictions without changes to the prompt text in BERT. Finally, using the LAMA datasets, we show how TEs highly improve the precision in extracting facts from PLMs.</abstract>
      <url hash="0197919f">2022.findings-emnlp.336</url>
      <bibkey>saeed-papotti-2022-type</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.336</doi>
    </paper>
    <paper id="337">
      <title>Generating Textual Adversaries with Minimal Perturbation</title>
      <author><first>Xingyi</first><last>Zhao</last></author>
      <author><first>Lu</first><last>Zhang</last></author>
      <author><first>Depeng</first><last>Xu</last></author>
      <author><first>Shuhan</first><last>Yuan</last></author>
      <pages>4599-4606</pages>
      <abstract>Many word-level adversarial attack approaches for textual data have been proposed in recent studies. However, due to the massive search space consisting of combinations of candidate words, the existing approaches face the problem of preserving the semantics of texts when crafting adversarial counterparts. In this paper, we develop a novel attack strategy to find adversarial texts with high similarity to the original texts while introducing minimal perturbation. The rationale is that we expect the adversarial texts with small perturbation can better preserve the semantic meaning of original texts. Experiments show that, compared with state-of-the-art attack approaches, our approach achieves higher success rates and lower perturbation rates in four benchmark datasets.</abstract>
      <url hash="53d3a88b">2022.findings-emnlp.337</url>
      <bibkey>zhao-etal-2022-generating</bibkey>
      <video href="2022.findings-emnlp.337.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.337</doi>
    </paper>
    <paper id="338">
      <title><fixed-case>S</fixed-case>ense<fixed-case>POLAR</fixed-case>: Word sense aware interpretability for pre-trained contextual word embeddings</title>
      <author><first>Jan</first><last>Engler</last></author>
      <author><first>Sandipan</first><last>Sikdar</last></author>
      <author><first>Marlene</first><last>Lutz</last></author>
      <author><first>Markus</first><last>Strohmaier</last></author>
      <pages>4607-4619</pages>
      <abstract>Adding interpretability to word embeddings represents an area of active research in textrepresentation. Recent work has explored the potential of embedding words via so-called polardimensions (e.g. good vs. bad, correct vs. wrong). Examples of such recent approachesinclude SemAxis, POLAR, FrameAxis, and BiImp. Although these approaches provide interpretabledimensions for words, they have not been designed to deal with polysemy, i.e. they can not easily distinguish between different senses of words. To address this limitation, we present SensePOLAR, an extension of the original POLAR framework that enables wordsense aware interpretability for pre-trained contextual word embeddings. The resulting interpretable word embeddings achieve a level ofperformance that is comparable to original contextual word embeddings across a variety ofnatural language processing tasks including the GLUE and SQuAD benchmarks. Our workremoves a fundamental limitation of existing approaches by offering users sense aware interpretationsfor contextual word embeddings.</abstract>
      <url hash="74d40abb">2022.findings-emnlp.338</url>
      <bibkey>engler-etal-2022-sensepolar</bibkey>
      <video href="2022.findings-emnlp.338.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.338</doi>
    </paper>
    <paper id="339">
      <title>Contextualizing Language Models for Norms Diverging from Social Majority</title>
      <author><first>Niklas</first><last>Kiehne</last></author>
      <author><first>Hermann</first><last>Kroll</last></author>
      <author><first>Wolf-Tilo</first><last>Balke</last></author>
      <pages>4620-4633</pages>
      <abstract>To comprehensibly contextualize decisions, artificial systems in social situations need a high degree of awareness of the rules of conduct of human behavior. Especially transformer-based language models have recently been shown to exhibit some such awareness. But what if norms in some social setting do not adhere to or even blatantly deviate from the mainstream? In this paper, we introduce a novel mechanism based on deontic logic to allow for a flexible adaptation of individual norms by de-biasing training data sets and a task-reduction to textual entailment. Building on the popular ‘Moral Stories’ dataset we on the one hand highlight the intrinsic bias of current language models, on the other hand characterize the adaptability of pre-trained models to deviating norms in fine-tuning settings.</abstract>
      <url hash="a7954371">2022.findings-emnlp.339</url>
      <bibkey>kiehne-etal-2022-contextualizing</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.339</doi>
    </paper>
    <paper id="340">
      <title>Empathetic Dialogue Generation via Sensitive Emotion Recognition and Sensible Knowledge Selection</title>
      <author><first>Lanrui</first><last>Wang</last></author>
      <author><first>Jiangnan</first><last>Li</last></author>
      <author><first>Zheng</first><last>Lin</last></author>
      <author><first>Fandong</first><last>Meng</last></author>
      <author><first>Chenxu</first><last>Yang</last></author>
      <author><first>Weiping</first><last>Wang</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>4634-4645</pages>
      <abstract>Empathy, which is widely used in psychological counseling, is a key trait of everyday human conversations. Equipped with commonsense knowledge, current approaches to empathetic response generation focus on capturing implicit emotion within dialogue context, where the emotions are treated as a static variable throughout the conversations. However, emotions change dynamically between utterances, which makes previous works difficult to perceive the emotion flow and predict the correct emotion of the target response, leading to inappropriate response. Furthermore, simply importing commonsense knowledge without harmonization may trigger the conflicts between knowledge and emotion, which confuse the model to choose the correct information to guide the generation process. To address the above problems, we propose a Serial Encoding and Emotion-Knowledge interaction (SEEK) method for empathetic dialogue generation. We use a fine-grained encoding strategy which is more sensitive to the emotion dynamics (emotion flow) in the conversations to predict the emotion-intent characteristic of response. Besides, we design a novel framework to model the interaction between knowledge and emotion to solve the conflicts generate more sensible response. Extensive experiments on the utterance-level annotated EMPATHETICDIALOGUES demonstrate that SEEK outperforms the strong baseline in both automatic and manual evaluations.</abstract>
      <url hash="89af8b6a">2022.findings-emnlp.340</url>
      <bibkey>wang-etal-2022-empathetic</bibkey>
      <video href="2022.findings-emnlp.340.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.340</doi>
    </paper>
    <paper id="341">
      <title>Joint Multilingual Knowledge Graph Completion and Alignment</title>
      <author><first>Vinh</first><last>Tong</last></author>
      <author><first>Dat Quoc</first><last>Nguyen</last></author>
      <author><first>Trung Thanh</first><last>Huynh</last></author>
      <author><first>Tam Thanh</first><last>Nguyen</last></author>
      <author><first>Quoc Viet Hung</first><last>Nguyen</last></author>
      <author><first>Mathias</first><last>Niepert</last></author>
      <pages>4646-4658</pages>
      <abstract>Knowledge graph (KG) alignment and completion are usually treated as two independent tasks. While recent work has leveraged entity and relation alignments from multiple KGs, such as alignments between multilingual KGs with common entities and relations, a deeper understanding of the ways in which multilingual KG completion (MKGC) can aid the creation of multilingual KG alignments (MKGA) is still limited. Motivated by the observation that structural inconsistencies – the main challenge for MKGA models – can be mitigated through KG completion methods, we propose a novel model for jointly completing and aligning knowledge graphs. The proposed model combines two components that jointly accomplish KG completion and alignment. These two components employ relation-aware graph neural networks that we propose to encode multi-hop neighborhood structures into entity and relation representations. Moreover, we also propose (i) a structural inconsistency reduction mechanism to incorporate information from the completion into the alignment component, and (ii) an alignment seed enlargement and triple transferring mechanism to enlarge alignment seeds and transfer triples during KGs alignment. Extensive experiments on a public multilingual benchmark show that our proposed model outperforms existing competitive baselines, obtaining new state-of-the-art results on both MKGC and MKGA tasks.</abstract>
      <url hash="a9b0dd67">2022.findings-emnlp.341</url>
      <bibkey>tong-etal-2022-joint</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.341</doi>
    </paper>
    <paper id="342">
      <title>A Framework for Automatic Generation of Spoken Question-Answering Data</title>
      <author><first>Merve</first><last>Ünlü Menevşe</last></author>
      <author><first>Yusufcan</first><last>Manav</last></author>
      <author><first>Ebru</first><last>Arisoy</last></author>
      <author><first>Arzucan</first><last>Özgür</last></author>
      <pages>4659-4666</pages>
      <abstract>This paper describes a framework to automatically generate a spoken question answering (QA) dataset. The framework consists of a question generation (QG) module to generate questions automatically from given text documents, a text-to-speech (TTS) module to convert the text documents into spoken form and an automatic speech recognition (ASR) module to transcribe the spoken content. The final dataset contains question-answer pairs for both the reference text and ASR transcriptions as well as the audio files corresponding to each reference text. For QG and ASR systems we used pre-trained multilingual encoder-decoder transformer models and fine-tuned these models using a limited amount of manually generated QA data and TTS-based speech data, respectively. As a proof of concept, we investigated the proposed framework for Turkish and generated the Turkish Question Answering (TurQuAse) dataset using Wikipedia articles. Manual evaluation of the automatically generated question- answer pairs and QA performance evaluation with state of-the-art models on TurQuAse show that the proposed framework is efficient for automatically generating spoken QA datasets. To the best of our knowledge, TurQuAse is the first publicly available spoken question answering dataset for Turkish. The proposed framework can be easily extended to other languages where a limited amount of QA data is available.</abstract>
      <url hash="e9dfb3e5">2022.findings-emnlp.342</url>
      <bibkey>unlu-menevse-etal-2022-framework</bibkey>
      <video href="2022.findings-emnlp.342.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.342</doi>
    </paper>
    <paper id="343">
      <title>Readability Controllable Biomedical Document Summarization</title>
      <author><first>Zheheng</first><last>Luo</last></author>
      <author><first>Qianqian</first><last>Xie</last></author>
      <author><first>Sophia</first><last>Ananiadou</last></author>
      <pages>4667-4680</pages>
      <abstract>Different from general documents, it is recognised that the ease with which people can understand a biomedical text is eminently varied, owing to the highly technical nature of biomedical documents and the variance of readers’ domain knowledge. However, existing biomedical document summarization systems have paid little attention to readability control, leaving users with summaries that are incompatible with their levels of expertise. In recognition of this urgent demand, we introduce a new task of readability controllable summarization for biomedical documents, which aims to recognise users’ readability demands and generate summaries that better suit their needs: technical summaries for experts and plain language summaries (PLS) for laymen. To establish this task, we construct a corpus consisting of biomedical papers with technical summaries and PLSs written by the authors, and benchmark multiple advanced controllable abstractive and extractive summarization models based on pre-trained language models (PLMs) with prevalent controlling and generation techniques. Moreover, we propose a novel masked language model (MLM) based metric and its variant to effectively evaluate the readability discrepancy between lay and technical summaries. Experimental results from automated and human evaluations show that though current control techniques allow for a certain degree of readability adjustment during generation, the performance of existing controllable summarization methods is far from desirable in this task.</abstract>
      <url hash="2c78286d">2022.findings-emnlp.343</url>
      <bibkey>luo-etal-2022-readability</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.343</doi>
    </paper>
    <paper id="344">
      <title>Beyond Additive Fusion: Learning Non-Additive Multimodal Interactions</title>
      <author><first>Torsten</first><last>Wörtwein</last></author>
      <author><first>Lisa</first><last>Sheeber</last></author>
      <author><first>Nicholas</first><last>Allen</last></author>
      <author><first>Jeffrey</first><last>Cohn</last></author>
      <author><first>Louis-Philippe</first><last>Morency</last></author>
      <pages>4681-4696</pages>
      <abstract>Multimodal fusion addresses the problem of analyzing spoken words in the multimodal context, including visual expressions and prosodic cues. Even when multimodal models lead to performance improvements, it is often unclear whether bimodal and trimodal interactions are learned or whether modalities are processed independently of each other. We propose Multimodal Residual Optimization (MRO) to separate unimodal, bimodal, and trimodal interactions in a multimodal model. This improves interpretability as the multimodal interaction can be quantified. Inspired by Occam’s razor, the main intuition of MRO is that (simpler) unimodal contributions should be learned before learning (more complex) bimodal and trimodal interactions. For example, bimodal predictions should learn to correct the mistakes (residuals) of unimodal predictions, thereby letting the bimodal predictions focus on the remaining bimodal interactions. Empirically, we observe that MRO successfully separates unimodal, bimodal, and trimodal interactions while not degrading predictive performance. We complement our empirical results with a human perception study and observe that MRO learns multimodal interactions that align with human judgments.</abstract>
      <url hash="3b0ee42e">2022.findings-emnlp.344</url>
      <bibkey>wortwein-etal-2022-beyond</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.344</doi>
    </paper>
    <paper id="345">
      <title>Generalization Differences between End-to-End and Neuro-Symbolic Vision-Language Reasoning Systems</title>
      <author><first>Wang</first><last>Zhu</last></author>
      <author><first>Jesse</first><last>Thomason</last></author>
      <author><first>Robin</first><last>Jia</last></author>
      <pages>4697-4711</pages>
      <abstract>For vision-and-language reasoning tasks, both fully connectionist, end-to-end methods and hybrid, neuro-symbolic methods have achieved high in-distribution performance. In which out-of-distribution settings does each paradigm excel? We investigate this question on both single-image and multi-image visual question-answering through four types of generalization tests: a novel segment-combine test for multi-image queries, contrast set, compositional generalization, and cross-benchmark transfer. Vision-and-language end-to-end trained systems exhibit sizeable performance drops across all these tests. Neuro-symbolic methods suffer even more on cross-benchmark transfer from GQA to VQA, but they show smaller accuracy drops on the other generalization tests and their performance quickly improves by few-shot training. Overall, our results demonstrate the complementary benefits of these two paradigms, and emphasize the importance of using a diverse suite of generalization tests to fully characterize model robustness to distribution shift.</abstract>
      <url hash="21806e37">2022.findings-emnlp.345</url>
      <bibkey>zhu-etal-2022-generalization</bibkey>
      <video href="2022.findings-emnlp.345.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.345</doi>
    </paper>
    <paper id="346">
      <title>Learning to Model Multimodal Semantic Alignment for Story Visualization</title>
      <author><first>Bowen</first><last>Li</last></author>
      <author><first>Thomas</first><last>Lukasiewicz</last></author>
      <pages>4712-4718</pages>
      <abstract>Story visualization aims to generate a sequence of images to narrate each sentence in a multi-sentence story, where the images should be realistic and keep global consistency across dynamic scenes and characters. Current works face the problem of semantic misalignment because of their fixed architecture and diversity of input modalities. To address this problem, we explore the semantic alignment between text and image representations by learning to match their semantic levels in the GAN-based generative model. More specifically, we introduce dynamic interactions according to learning to dynamically explore various semantic depths and fuse the different-modal information at a matched semantic level, which thus relieves the text-image semantic misalignment problem. Extensive experiments on different datasets demonstrate the improvements of our approach, neither using segmentation masks nor auxiliary captioning networks, on image quality and story consistency, compared with state-of-the-art methods.</abstract>
      <url hash="35e8a35c">2022.findings-emnlp.346</url>
      <bibkey>li-lukasiewicz-2022-learning</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.346</doi>
    </paper>
    <paper id="347">
      <title><fixed-case>S</fixed-case>ci<fixed-case>F</fixed-case>act-Open: Towards open-domain scientific claim verification</title>
      <author><first>David</first><last>Wadden</last></author>
      <author><first>Kyle</first><last>Lo</last></author>
      <author><first>Bailey</first><last>Kuehl</last></author>
      <author><first>Arman</first><last>Cohan</last></author>
      <author><first>Iz</first><last>Beltagy</last></author>
      <author><first>Lucy Lu</first><last>Wang</last></author>
      <author><first>Hannaneh</first><last>Hajishirzi</last></author>
      <pages>4719-4734</pages>
      <abstract>While research on scientific claim verification has led to the development of powerful systems that appear to approach human performance, these approaches have yet to be tested in a realistic setting against large corpora of scientific literature. Moving to this open-domain evaluation setting, however, poses unique challenges; in particular, it is infeasible to exhaustively annotate all evidence documents. In this work, we present SciFact-Open, a new test collection designed to evaluate the performance of scientific claim verification systems on a corpus of 500K research abstracts. Drawing upon pooling techniques from information retrieval, we collect evidence for scientific claims by pooling and annotating the top predictions of four state-of-the-art scientific claim verification models. We find that systems developed on smaller corpora struggle to generalize to SciFact-Open, exhibiting performance drops of at least 15 F1. In addition, analysis of the evidence in SciFact-Open reveals interesting phenomena likely to appear when claim verification systems are deployed in practice, e.g., cases where the evidence supports only a special case of the claim. Our dataset is available at <url>https://github.com/dwadden/scifact-open</url>.</abstract>
      <url hash="5a2145bf">2022.findings-emnlp.347</url>
      <bibkey>wadden-etal-2022-scifact</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.347</doi>
    </paper>
    <paper id="348">
      <title><fixed-case>COMET</fixed-case>-<fixed-case>QE</fixed-case> and Active Learning for Low-Resource Machine Translation</title>
      <author><first>Everlyn</first><last>Chimoto</last></author>
      <author><first>Bruce</first><last>Bassett</last></author>
      <pages>4735-4740</pages>
      <abstract>Active learning aims to deliver maximum benefit when resources are scarce. We use COMET-QE, a reference-free evaluation metric, to select sentences for low-resource neural machine translation. Using Swahili, Kinyarwanda and Spanish for our experiments, we show that COMET-QE significantly outperforms two variants of Round Trip Translation Likelihood (RTTL) and random sentence selection by up to 5 BLEU points for 20k sentences selected by Active Learning on a 30k baseline. This suggests that COMET-QE is a powerful tool for sentence selection in the very low-resource limit.</abstract>
      <url hash="face96c1">2022.findings-emnlp.348</url>
      <bibkey>chimoto-bassett-2022-comet</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.348</doi>
    </paper>
    <paper id="349">
      <title><fixed-case>M</fixed-case>edical<fixed-case>S</fixed-case>um: A Guided Clinical Abstractive Summarization Model for Generating Medical Reports from Patient-Doctor Conversations</title>
      <author><first>George</first><last>Michalopoulos</last></author>
      <author><first>Kyle</first><last>Williams</last></author>
      <author><first>Gagandeep</first><last>Singh</last></author>
      <author><first>Thomas</first><last>Lin</last></author>
      <pages>4741-4749</pages>
      <abstract>We introduce MedicalSum, a transformer-based sequence-to-sequence architecture for summarizing medical conversations by integrating medical domain knowledge from the Unified Medical Language System (UMLS). The novel knowledge augmentation is performed in three ways: (i) introducing a guidance signal that consists of the medical words in the input sequence, (ii) leveraging semantic type knowledge in UMLS to create clinically meaningful input embeddings, and (iii) making use of a novel weighted loss function that provides a stronger incentive for the model to correctly predict words with a medical meaning. By applying these three strategies, MedicalSum takes clinical knowledge into consideration during the summarization process and achieves state-of-the-art ROUGE score improvements of 0.8-2.1 points (including 6.2% ROUGE-1 error reduction in the PE section) when producing medical summaries of patient-doctor conversations.</abstract>
      <url hash="ffc1fe89">2022.findings-emnlp.349</url>
      <bibkey>michalopoulos-etal-2022-medicalsum</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.349</doi>
    </paper>
    <paper id="350">
      <title>Leveraging Training Dynamics and Self-Training for Text Classification</title>
      <author><first>Tiberiu</first><last>Sosea</last></author>
      <author><first>Cornelia</first><last>Caragea</last></author>
      <pages>4750-4762</pages>
      <abstract>The effectiveness of pre-trained language models in downstream tasks is highly dependent on the amount of labeled data available for training. Semi-supervised learning (SSL) is a promising technique that has seen wide attention recently due to its effectiveness in improving deep learning models when training data is scarce. Common approaches employ a teacher-student self-training framework, where a teacher network generates pseudo-labels for unlabeled data, which are then used to iteratively train a student network. In this paper, we propose a new self-training approach for text classification that leverages training dynamics of unlabeled data. We evaluate our approach on a wide range of text classification tasks, including emotion detection, sentiment analysis, question classification and gramaticality, which span a variety of domains, e.g, Reddit, Twitter, and online forums. Notably, our method is successful on all benchmarks, obtaining an average increase in F1 score of 3.5% over strong baselines in low resource settings.</abstract>
      <url hash="212250d8">2022.findings-emnlp.350</url>
      <bibkey>sosea-caragea-2022-leveraging</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.350</doi>
    </paper>
    <paper id="351">
      <title>Learning to Infer from Unlabeled Data: A Semi-supervised Learning Approach for Robust Natural Language Inference</title>
      <author><first>Mobashir</first><last>Sadat</last></author>
      <author><first>Cornelia</first><last>Caragea</last></author>
      <pages>4763-4776</pages>
      <abstract>Natural Language Inference (NLI) or Recognizing Textual Entailment (RTE) aims at predicting the relation between a pair of sentences (premise and hypothesis) as entailment, contradiction or semantic independence. Although deep learning models have shown promising performance for NLI in recent years, they rely on large scale expensive human-annotated datasets. Semi-supervised learning (SSL) is a popular technique for reducing the reliance on human annotation by leveraging unlabeled data for training. However, despite its substantial success on single sentence classification tasks where the challenge in making use of unlabeled data is to assign “good enough” pseudo-labels, for NLI tasks, the nature of unlabeled data is more complex: one of the sentences in the pair (usually the hypothesis) along with the class label are missing from the data and require human annotations, which makes SSL for NLI more challenging. In this paper, we propose a novel way to incorporate unlabeled data in SSL for NLI where we use a conditional language model, BART to generate the hypotheses for the unlabeled sentences (used as premises). Our experiments show that our SSL framework successfully exploits unlabeled data and substantially improves the performance of four NLI datasets in low-resource settings. We release our code here: <url>https://github.com/msadat3/SSL_for_NLI</url></abstract>
      <url hash="6626cfc7">2022.findings-emnlp.351</url>
      <bibkey>sadat-caragea-2022-learning</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.351</doi>
    </paper>
    <paper id="352">
      <title>Unsupervised Text Deidentification</title>
      <author><first>John</first><last>Morris</last></author>
      <author><first>Justin</first><last>Chiu</last></author>
      <author><first>Ramin</first><last>Zabih</last></author>
      <author><first>Alexander</first><last>Rush</last></author>
      <pages>4777-4788</pages>
      <abstract>Deidentification seeks to anonymize textual data prior to distribution. Automatic deidentification primarily uses supervised named entity recognition from human-labeled data points. We propose an unsupervised deidentification method that masks words that leak personally-identifying information. The approach utilizes a specially trained reidentification model to identify individuals from redacted personal documents. Motivated by K-anonymity based privacy, we generate redactions that ensure a minimum reidentification rank for the correct profile of the document. To evaluate this approach, we consider the task of deidentifying Wikipedia Biographies, and evaluate using an adversarial reidentification metric. Compared to a set of unsupervised baselines, our approach deidentifies documents more completely while removing fewer words. Qualitatively, we see that the approach eliminates many identifying aspects that would fall outside of the common named entity based approach.</abstract>
      <url hash="a12a3473">2022.findings-emnlp.352</url>
      <bibkey>morris-etal-2022-unsupervised</bibkey>
      <video href="2022.findings-emnlp.352.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.352</doi>
    </paper>
    <paper id="353">
      <title>Federated Continual Learning for Text Classification via Selective Inter-client Transfer</title>
      <author><first>Yatin</first><last>Chaudhary</last></author>
      <author><first>Pranav</first><last>Rai</last></author>
      <author><first>Matthias</first><last>Schubert</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <author><first>Pankaj</first><last>Gupta</last></author>
      <pages>4789-4799</pages>
      <abstract>In this work, we combine the two paradigms: Federated Learning (FL) and Continual Learning (CL) for text classification task in cloud-edge continuum. The objective of Federated Continual Learning (FCL) is to improve deep learning models over life time at each client by (relevant and efficient) knowledge transfer without sharing data. Here, we address challenges in minimizing inter-client interference while knowledge sharing due to heterogeneous tasks across clients in FCL setup. In doing so, we propose a novel framework, Federated Selective Inter-client Transfer (FedSeIT) which selectively combines model parameters of foreign clients. To further maximize knowledge transfer, we assess domain overlap and select informative tasks from the sequence of historical tasks at each foreign client while preserving privacy. Evaluating against the baselines, we show improved performance, a gain of (average) 12.4% in text classification over a sequence of tasks using five datasets from diverse domains. To the best of our knowledge, this is the first work that applies FCL to NLP.</abstract>
      <url hash="5fac7ca3">2022.findings-emnlp.353</url>
      <bibkey>chaudhary-etal-2022-federated</bibkey>
      <video href="2022.findings-emnlp.353.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.353</doi>
    </paper>
    <paper id="354">
      <title><fixed-case>DOROTHIE</fixed-case>: Spoken Dialogue for Handling Unexpected Situations in Interactive Autonomous Driving Agents</title>
      <author><first>Ziqiao</first><last>Ma</last></author>
      <author><first>Benjamin</first><last>VanDerPloeg</last></author>
      <author><first>Cristian-Paul</first><last>Bara</last></author>
      <author><first>Yidong</first><last>Huang</last></author>
      <author><first>Eui-In</first><last>Kim</last></author>
      <author><first>Felix</first><last>Gervits</last></author>
      <author><first>Matthew</first><last>Marge</last></author>
      <author><first>Joyce</first><last>Chai</last></author>
      <pages>4800-4822</pages>
      <abstract>In the real world, autonomous driving agents navigate in highly dynamic environments full of unexpected situations where pre-trained models are unreliable. In these situations, what is immediately available to vehicles is often only human operators. Empowering autonomous driving agents with the ability to navigate in a continuous and dynamic environment and to communicate with humans through sensorimotor-grounded dialogue becomes critical. To this end, we introduce Dialogue On the ROad To Handle Irregular Events (DOROTHIE), a novel interactive simulation platform that enables the creation of unexpected situations on the fly to support empirical studies on situated communication with autonomous driving agents. Based on this platform, we created the Situated Dialogue Navigation (SDN), a navigation benchmark of 183 trials with a total of 8415 utterances, around 18.7 hours of control streams, and 2.9 hours of trimmed audio. SDN is developed to evaluate the agent’s ability to predict dialogue moves from humans as well as generate its own dialogue moves and physical navigation actions. We further developed a transformer-based baseline model for these SDN tasks. Our empirical results indicate that language guided-navigation in a highly dynamic environment is an extremely difficult task for end-to-end models. These results will provide insight towards future work on robust autonomous driving agents</abstract>
      <url hash="08066aa7">2022.findings-emnlp.354</url>
      <bibkey>ma-etal-2022-dorothie</bibkey>
      <video href="2022.findings-emnlp.354.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.354</doi>
    </paper>
    <paper id="355">
      <title>He Said, She Said: Style Transfer for Shifting the Perspective of Dialogues</title>
      <author><first>Amanda</first><last>Bertsch</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <author><first>Matthew R.</first><last>Gormley</last></author>
      <pages>4823-4840</pages>
      <abstract>In this work, we define a new style transfer task: perspective shift, which reframes a dialouge from informal first person to a formal third person rephrasing of the text. This task requires challenging coreference resolution, emotion attribution, and interpretation of informal text. We explore several baseline approaches and discuss further directions on this task when applied to short dialogues. As a sample application, we demonstrate that applying perspective shifting to a dialogue summarization dataset (SAMSum) substantially improves the zero-shot performance of extractive news summarization models on this data. Additionally, supervised extractive models perform better when trained on perspective shifted data than on the original dialogues. We release our code publicly.</abstract>
      <url hash="25285b00">2022.findings-emnlp.355</url>
      <bibkey>bertsch-etal-2022-said</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.355</doi>
    </paper>
    <paper id="356">
      <title>Dynamic Augmentation Data Selection for Few-shot Text Classification</title>
      <author><first>Guangliang</first><last>Liu</last></author>
      <author><first>Lifeng</first><last>Jin</last></author>
      <author><first>Owen</first><last>Yuan</last></author>
      <author><first>Jiayu</first><last>Zhou</last></author>
      <pages>4841-4852</pages>
      <abstract>Data augmentation has been a popular method for fine-tuning pre-trained language models to increase model robustness and performance. With augmentation data coming from modifying gold train data (in-sample augmentation) or being harvested from general domain unlabeled data (out-of-sample augmentation), the quality of such data is the key to successful fine-tuning. In this paper, we propose a dynamic data selection method to select effective augmentation data from different augmentation sources according to the model’s learning stage, by identifying a set of augmentation samples that optimally facilitates the learning process of the most current model. The method firstly filters out augmentation samples with noisy pseudo labels through a curriculum learning strategy, then estimates the effectiveness of reserved augmentation data by its influence scores on the current model at every update, allowing the data selection process tightly tailored to model parameters. And the two-stage augmentation strategy considers in-sample augmentation and out-of-sample augmentation in different learning stages. Experiments with both kinds of augmentation data on a variety of sentence classification tasks show that our method outperforms strong baselines, proving the effectiveness of our method. Analysis confirms the dynamic nature of the data effectiveness and the importance of model learning stages in utilization of augmentation data.</abstract>
      <url hash="2286672d">2022.findings-emnlp.356</url>
      <bibkey>liu-etal-2022-dynamic-augmentation</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.356</doi>
    </paper>
    <paper id="357">
      <title><fixed-case>KPDROP</fixed-case>: Improving Absent Keyphrase Generation</title>
      <author><first>Jishnu</first><last>Ray Chowdhury</last></author>
      <author><first>Seo Yeon</first><last>Park</last></author>
      <author><first>Tuhin</first><last>Kundu</last></author>
      <author><first>Cornelia</first><last>Caragea</last></author>
      <pages>4853-4870</pages>
      <abstract>Keyphrase generation is the task of generating phrases (keyphrases) that summarize the main topics of a given document. Keyphrases can be either present or absent from the given document. While the extraction of present keyphrases has received much attention in the past, only recently a stronger focus has been placed on the generation of absent keyphrases. However, generating absent keyphrases is challenging; even the best methods show only a modest degree of success. In this paper, we propose a model-agnostic approach called keyphrase dropout (or KPDrop) to improve absent keyphrase generation. In this approach, we randomly drop present keyphrases from the document and turn them into artificial absent keyphrases during training. We test our approach extensively and show that it consistently improves the absent performance of strong baselines in both supervised and resource-constrained semi-supervised settings.</abstract>
      <url hash="a2b7c986">2022.findings-emnlp.357</url>
      <bibkey>ray-chowdhury-etal-2022-kpdrop</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.357</doi>
    </paper>
    <paper id="358">
      <title>Natural Language Deduction through Search over Statement Compositions</title>
      <author><first>Kaj</first><last>Bostrom</last></author>
      <author><first>Zayne</first><last>Sprague</last></author>
      <author><first>Swarat</first><last>Chaudhuri</last></author>
      <author><first>Greg</first><last>Durrett</last></author>
      <pages>4871-4883</pages>
      <abstract>In settings from fact-checking to question answering, we frequently want to know whether a collection of evidence (premises) entails a hypothesis. Existing methods primarily focus on the end-to-end discriminative version of this task, but less work has treated the generative version in which a model searches over the space of statements entailed by the premises to constructively derive the hypothesis. We propose a system for doing this kind of deductive reasoning in natural language by decomposing the task into separate steps coordinated by a search procedure, producing a tree of intermediate conclusions that faithfully reflects the system’s reasoning process. Our experiments on the EntailmentBank dataset (Dalvi et al., 2021) demonstrate that the proposed system can successfully prove true statements while rejecting false ones. Moreover, it produces natural language explanations with a 17% absolute higher step validity than those produced by an end-to-end T5 model.</abstract>
      <url hash="1685cc29">2022.findings-emnlp.358</url>
      <bibkey>bostrom-etal-2022-natural</bibkey>
      <video href="2022.findings-emnlp.358.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.358</doi>
    </paper>
    <paper id="359">
      <title><fixed-case>E</fixed-case>n<fixed-case>D</fixed-case>ex: Evaluation of Dialogue Engagingness at Scale</title>
      <author><first>Guangxuan</first><last>Xu</last></author>
      <author><first>Ruibo</first><last>Liu</last></author>
      <author><first>Fabrice</first><last>Harel-Canada</last></author>
      <author><first>Nischal Reddy</first><last>Chandra</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <pages>4884-4893</pages>
      <abstract>We propose EnDex, the first human-reaction based model to evaluate dialogue engagingness. EnDex is trained on 80k Reddit-based Engagement Dataset (RED) curated using a novel distant-supervision framework. Engagingness is a key measure that captures high-level quality of AI dialogue systems and closely reflects actual user experience. However, data shortage, plus the abstract and extensive definition of engagingness makes it challenging to develop an automatic metric. Our work departs from mainstream approaches that use synthetic negative examples to train binary classifiers, and instead, proposes a solution using distant-supervision from human-reaction feedback. To support the soundness of our EnDex metric, we offer a theoretical foundation for engagement, an extensive ablation study, and empirical evidence of high correlation on five engagingness related datasets. We will release code, off-the-shelf EnDex model, and a large-scale dataset upon paper publication to facilitate future research.</abstract>
      <url hash="35b8841b">2022.findings-emnlp.359</url>
      <bibkey>xu-etal-2022-endex</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.359</doi>
    </paper>
    <paper id="360">
      <title><fixed-case>LOPS</fixed-case>: Learning Order Inspired Pseudo-Label Selection for Weakly Supervised Text Classification</title>
      <author><first>Dheeraj</first><last>Mekala</last></author>
      <author><first>Chengyu</first><last>Dong</last></author>
      <author><first>Jingbo</first><last>Shang</last></author>
      <pages>4894-4908</pages>
      <abstract>Weakly supervised text classification methods typically train a deep neural classifier based on pseudo-labels. The quality of pseudo-labels is crucial to final performance but they are inevitably noisy due to their heuristic nature, so selecting the correct ones has a huge potential for performance boost. One straightforward solution is to select samples based on the softmax probability scores in the neural classifier corresponding to their pseudo-labels. However, we show through our experiments that such solutions are ineffective and unstable due to the erroneously high-confidence predictions from poorly calibrated models. Recent studies on the memorization effects of deep neural models suggest that these models first memorize training samples with clean labels and then those with noisy labels. Inspired by this observation, we propose a novel pseudo-label selection method LOPS that takes learning order of samples into consideration. We hypothesize that the learning order reflects the probability of wrong annotation in terms of ranking, and therefore, propose to select the samples that are learnt earlier. LOPS can be viewed as a strong performance-boost plug-in to most existing weakly-supervised text classification methods, as confirmed in extensive experiments on four real-world datasets.</abstract>
      <url hash="c57a2d57">2022.findings-emnlp.360</url>
      <bibkey>mekala-etal-2022-lops</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.360</doi>
    </paper>
    <paper id="361">
      <title>Train Flat, Then Compress: Sharpness-Aware Minimization Learns More Compressible Models</title>
      <author><first>Clara</first><last>Na</last></author>
      <author><first>Sanket Vaibhav</first><last>Mehta</last></author>
      <author><first>Emma</first><last>Strubell</last></author>
      <pages>4909-4936</pages>
      <abstract>Model compression by way of parameter pruning, quantization, or distillation has recently gained popularity as an approach for reducing the computational requirements of modern deep neural network models for NLP. Inspired by prior works suggesting a connection between simpler, more generalizable models and those that lie within wider loss basins, we hypothesize that optimizing for flat minima should lead to simpler parameterizations and thus more compressible models. We propose to combine sharpness-aware minimization (SAM) with various task-specific model compression methods, including iterative magnitude pruning (IMP), structured pruning with a distillation objective, and post-training dynamic quantization. Empirically, we show that optimizing for flatter minima consistently leads to greater compressibility of parameters compared to vanilla Adam when fine-tuning BERT models, with little to no loss in accuracy on the GLUE text classification and SQuAD question answering benchmarks. Moreover, SAM finds superior winning tickets during IMP that 1) are amenable to vanilla Adam optimization, and 2) transfer more effectively across tasks.</abstract>
      <url hash="7f9674b3">2022.findings-emnlp.361</url>
      <bibkey>na-etal-2022-train</bibkey>
      <video href="2022.findings-emnlp.361.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.361</doi>
    </paper>
    <paper id="362">
      <title>Structural Contrastive Representation Learning for Zero-shot Multi-label Text Classification</title>
      <author><first>Tianyi</first><last>Zhang</last></author>
      <author><first>Zhaozhuo</first><last>Xu</last></author>
      <author><first>Tharun</first><last>Medini</last></author>
      <author><first>Anshumali</first><last>Shrivastava</last></author>
      <pages>4937-4947</pages>
      <abstract>Zero-shot multi-label text classification (ZMTC) is a fundamental task in natural language processing with applications in the cold start problem of recommendation systems. Ideally, one would learn an expressive representation of both input text and label features so that ZMTC is transformed into a nearest neighbor search problem. However, the existing representation learning approaches for ZMTC struggle with accuracy as well as poor training efficiency. Firstly, the input text is structural, consisting of both short title sentences and long content paragraphs. It is challenging to model the correlation between short label descriptions and long structural input documents. Secondly, the enormous label space in ZMTC forces the existing approaches to perform multi-stage learning with label engineering. As a result, the training overhead is significant. In this paper, we address both problems by introducing an end-to-end structural contrastive representation learning approach. We propose a randomized text segmentation (RTS) technique to generate high-quality contrastive pairs. This RTS technique allows us to model title-content correlation. Additionally, we simplify the multi-stage ZMTC learning strategy by avoiding label engineering. Extensive experiments demonstrate that our approach leads to up to 2.33% improvement in precision@1 and 5.94x speedup in training time on publicly available datasets. Our code is available publicly.</abstract>
      <url hash="264c2dc2">2022.findings-emnlp.362</url>
      <bibkey>zhang-etal-2022-structural-contrastive</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.362</doi>
    </paper>
    <paper id="363">
      <title>Improving Generalization of Pre-trained Language Models via Stochastic Weight Averaging</title>
      <author><first>Peng</first><last>Lu</last></author>
      <author><first>Ivan</first><last>Kobyzev</last></author>
      <author><first>Mehdi</first><last>Rezagholizadeh</last></author>
      <author><first>Ahmad</first><last>Rashid</last></author>
      <author><first>Ali</first><last>Ghodsi</last></author>
      <author><first>Phillippe</first><last>Langlais</last></author>
      <pages>4948-4954</pages>
      <abstract>Knowledge Distillation (KD) is a commonly used technique for improving the generalization of compact Pre-trained Language Models (PLMs) on downstream tasks. However, such methods impose the additional burden of training a separate teacher model for every new dataset. Alternatively, one may directly work on the improvement of the optimization procedure of the compact model towards better generalization. Recent works observe that the flatness of the local minimum correlates well with better generalization. In this work, we adapt Stochastic Weight Averaging (SWA), a method encouraging convergence to a flatter minimum, to fine-tuning PLMs. We conduct extensive experiments on various NLP tasks (text classification, question answering, and generation) and different model architectures and demonstrate that our adaptation improves the generalization without extra computation cost. Moreover, we observe that this simple optimization technique is able to outperform the state-of-the-art KD methods for compact models.</abstract>
      <url hash="654e37cd">2022.findings-emnlp.363</url>
      <bibkey>lu-etal-2022-improving</bibkey>
      <video href="2022.findings-emnlp.363.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.363</doi>
    </paper>
    <paper id="364">
      <title>Learn What Is Possible, Then Choose What Is Best: Disentangling One-To-Many Relations in Language Through Text-based Games</title>
      <author><first>Benjamin</first><last>Towle</last></author>
      <author><first>Ke</first><last>Zhou</last></author>
      <pages>4955-4965</pages>
      <abstract>Language models pre-trained on large self-supervised corpora, followed by task-specific fine-tuning has become the dominant paradigm in NLP. These pre-training datasets often have a one-to-many structure—e.g. in dialogue there are many valid responses for a given context. However, only some of these responses will be desirable in our downstream task. This raises the question of how we should train the model such that it can emulate the desirable behaviours, but not the undesirable ones. Current approaches train in a one-to-one setup—only a single target response is given for a single dialogue context—leading to models only learning to predict the average response, while ignoring the full range of possible responses. Using text-based games as a testbed, our approach, PASA, uses discrete latent variables to capture the range of different behaviours represented in our larger pre-training dataset. We then use knowledge distillation to distil the posterior probability distribution into a student model. This probability distribution is far richer than learning from only the hard targets of the dataset, and thus allows the student model to benefit from the richer range of actions the teacher model has learned. Results show up to 49% empirical improvement over the previous state-of-the-art model on the Jericho Walkthroughs dataset.</abstract>
      <url hash="cc5fe77b">2022.findings-emnlp.364</url>
      <bibkey>towle-zhou-2022-learn</bibkey>
      <video href="2022.findings-emnlp.364.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.364</doi>
    </paper>
    <paper id="365">
      <title>Structurally Diverse Sampling for Sample-Efficient Training and Comprehensive Evaluation</title>
      <author><first>Shivanshu</first><last>Gupta</last></author>
      <author><first>Sameer</first><last>Singh</last></author>
      <author><first>Matt</first><last>Gardner</last></author>
      <pages>4966-4979</pages>
      <abstract>A growing body of research has demonstrated the inability of NLP models to generalize compositionally and has tried to alleviate it through specialized architectures, training schemes, and data augmentation, among other approaches. In this work, we study a different approach: training on instances with diverse structures. We propose a model-agnostic algorithm for subsampling such sets of instances from a labeled instance pool with structured outputs. Evaluating on both compositional template splits and traditional IID splits of 5 semantic parsing datasets of varying complexity, we show that structurally diverse training using our algorithm leads to comparable or better generalization than prior algorithms in 9 out of 10 dataset-split type pairs. In general, we find structural diversity to consistently improve sample efficiency compared to random train sets. Moreover, we show that structurally diverse sampling yields comprehensive test sets that are a lot more challenging than IID test sets. Finally, we provide two explanations for improved generalization from diverse train sets: 1) improved coverage of output substructures, and 2) a reduction in spurious correlations between these substructures.</abstract>
      <url hash="11ccece0">2022.findings-emnlp.365</url>
      <bibkey>gupta-etal-2022-structurally</bibkey>
      <video href="2022.findings-emnlp.365.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.365</doi>
    </paper>
    <paper id="366">
      <title>Unsupervised Multi-Granularity Summarization</title>
      <author><first>Ming</first><last>Zhong</last></author>
      <author id="yang-liu-microsoft"><first>Yang</first><last>Liu</last></author>
      <author><first>Suyu</first><last>Ge</last></author>
      <author><first>Yuning</first><last>Mao</last></author>
      <author><first>Yizhu</first><last>Jiao</last></author>
      <author><first>Xingxing</first><last>Zhang</last></author>
      <author><first>Yichong</first><last>Xu</last></author>
      <author><first>Chenguang</first><last>Zhu</last></author>
      <author><first>Michael</first><last>Zeng</last></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <pages>4980-4995</pages>
      <abstract>Text summarization is a user-preference based task, i.e., for one document, users often have different priorities for the summary. As a key aspect of customization in summarization, granularity is used to measure the semantic coverage between the summary and source document. However, developing systems that can generate summaries with customizable semantic coverage is still an under-explored topic. In this paper, we propose the first unsupervised multi-granularity summarization framework, GranuSum. We take events as the basic semantic units of the source documents and propose to rank these events by their salience. We also develop a model to summarize input documents with given events as anchors and hints. By inputting different numbers of events, GranuSum is capable of producing multi-granular summaries in an unsupervised manner. Meanwhile, we annotate a new benchmark GranuDUC that contains multiple summaries at different granularities for each document cluster. Experimental results confirm the substantial superiority of GranuSum on multi-granularity summarization over strong baselines. Furthermore, by exploiting the event information, GranuSum also exhibits state-of-the-art performance under the conventional unsupervised abstractive setting.</abstract>
      <url hash="da7182a7">2022.findings-emnlp.366</url>
      <bibkey>zhong-etal-2022-unsupervised</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.366</doi>
    </paper>
    <paper id="367">
      <title><fixed-case>H</fixed-case>e<fixed-case>L</fixed-case>o: Learning-Free Lookahead Decoding for Conversation Infilling</title>
      <author><first>Ivan</first><last>Lee</last></author>
      <author><first>Taylor</first><last>Berg-Kirkpatrick</last></author>
      <pages>4996-5008</pages>
      <abstract>We propose Heuristic Guided Lookahead Decoding (HeLo), a novel decoding strategy for conversation infilling. Conversation infilling aims to generate a seamless bridge of utterances connecting a given pair of source and target utterances. HeLo does not require fine-tuning or extra models – only the generating model itself. Instead, HeLo leverages a greedy lookahead phase before committing to any token. The HeLo framework is simple and can augment conventional decoding strategies paired with any autoregressive language model. Smooth transitions between utterances are encouraged with an annealing schedule. Our experiments show HeLo outperforms several baselines when evaluated with both automatic and human evaluation metrics, which, we argue, are appropriate for the task.</abstract>
      <url hash="fceb6725">2022.findings-emnlp.367</url>
      <bibkey>lee-berg-kirkpatrick-2022-helo</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.367</doi>
    </paper>
    <paper id="368">
      <title>Invernet: An Inversion Attack Framework to Infer Fine-Tuning Datasets through Word Embeddings</title>
      <author><first>Ishrak</first><last>Hayet</last></author>
      <author><first>Zijun</first><last>Yao</last></author>
      <author><first>Bo</first><last>Luo</last></author>
      <pages>5009-5018</pages>
      <abstract>Word embedding aims to learn the dense representation of words and has become a regular input preparation in many NLP tasks. Due to the data and computation intensive nature of learning embeddings from scratch, a more affordable way is to borrow the pretrained embedding available in public and fine-tune the embedding through a domain specific downstream dataset. A privacy concern can arise if a malicious owner of the pretrained embedding gets access to the fine-tuned embedding and tries to infer the critical information from the downstream datasets. In this study, we propose a novel embedding inversion framework called Invernet that materializes the privacy concern by inferring the context distribution in the downstream dataset, which can lead to key information breach. With extensive experimental studies on two real-world news datasets: Antonio Gulli’s News and New York Times, we validate the feasibility of proposed privacy attack and demonstrate the effectiveness of Invernet on inferring downstream datasets based on multiple word embedding methods.</abstract>
      <url hash="8736482a">2022.findings-emnlp.368</url>
      <bibkey>hayet-etal-2022-invernet</bibkey>
      <video href="2022.findings-emnlp.368.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.368</doi>
    </paper>
    <paper id="369">
      <title><fixed-case>L</fixed-case>awng<fixed-case>NLI</fixed-case>: A Long-Premise Benchmark for In-Domain Generalization from Short to Long Contexts and for Implication-Based Retrieval</title>
      <author><first>William</first><last>Bruno</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>5019-5043</pages>
      <abstract>Natural language inference has trended toward studying contexts beyond the sentence level. An important application area is law: past cases often do not foretell how they apply to new situations and implications must be inferred. This paper introduces LawngNLI, constructed from U.S. legal opinions with automatic labels with high human-validated accuracy. Premises are long and multigranular. Experiments show two use cases. First, LawngNLI can benchmark for in-domain generalization from short to long contexts. It has remained unclear if large-scale long-premise NLI datasets actually need to be constructed: near-top performance on long premises could be achievable by fine-tuning using short premises. Without multigranularity, benchmarks cannot distinguish lack of fine-tuning on long premises versus domain shift between short and long datasets. In contrast, our long and short premises share the same examples and domain. Models fine-tuned using several past NLI datasets and/or our short premises fall short of top performance on our long premises. So for at least certain domains (such as ours), large-scale long-premise datasets are needed. Second, LawngNLI can benchmark for implication-based retrieval. Queries are entailed or contradicted by target documents, allowing users to move between arguments and evidence. Leading retrieval models perform reasonably zero shot on a LawngNLI-derived retrieval task. We compare different systems for re-ranking, including lexical overlap and cross-encoders fine-tuned using a modified LawngNLI or past NLI datasets. LawngNLI can train and test systems for implication-based case retrieval and argumentation.</abstract>
      <url hash="babf4ee4">2022.findings-emnlp.369</url>
      <bibkey>bruno-roth-2022-lawngnli</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.369</doi>
    </paper>
    <paper id="370">
      <title>Distillation-Resistant Watermarking for Model Protection in <fixed-case>NLP</fixed-case></title>
      <author><first>Xuandong</first><last>Zhao</last></author>
      <author><first>Lei</first><last>Li</last></author>
      <author><first>Yu-Xiang</first><last>Wang</last></author>
      <pages>5044-5055</pages>
      <abstract>How can we protect the intellectual property of trained NLP models? Modern NLP models are prone to stealing by querying and distilling from their publicly exposed APIs. However, existing protection methods such as watermarking only work for images but are not applicable to text. We propose Distillation-Resistant Watermarking (DRW), a novel technique to protect NLP models from being stolen via distillation. DRW protects a model by injecting watermarks into the victim’s prediction probability corresponding to a secret key and is able to detect such a key by probing a suspect model. We prove that a protected model still retains the original accuracy within a certain bound. We evaluate DRW on a diverse set of NLP tasks including text classification, part-of-speech tagging, and named entity recognition. Experiments show that DRW protects the original model and detects stealing suspects at 100% mean average precision for all four tasks while the prior method fails on two.</abstract>
      <url hash="114af6f1">2022.findings-emnlp.370</url>
      <bibkey>zhao-etal-2022-distillation</bibkey>
      <video href="2022.findings-emnlp.370.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.370</doi>
    </paper>
    <paper id="371">
      <title><fixed-case>N</fixed-case>euro<fixed-case>C</fixed-case>ounterfactuals: Beyond Minimal-Edit Counterfactuals for Richer Data Augmentation</title>
      <author><first>Phillip</first><last>Howard</last></author>
      <author><first>Gadi</first><last>Singer</last></author>
      <author><first>Vasudev</first><last>Lal</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <author><first>Swabha</first><last>Swayamdipta</last></author>
      <pages>5056-5072</pages>
      <abstract>While counterfactual data augmentation offers a promising step towards robust generalization in natural language processing, producing a set of counterfactuals that offer valuable inductive bias for models remains a challenge. Most existing approaches for producing counterfactuals, manual or automated, rely on small perturbations via minimal edits, resulting in simplistic changes. We introduce NeuroCounterfactuals, designed as loose counterfactuals, allowing for larger edits which result in naturalistic generations containing linguistic diversity, while still bearing similarity to the original document. Our novel generative approach bridges the benefits of constrained decoding, with those of language model adaptation for sentiment steering. Training data augmentation with our generations results in both in-domain and out-of-domain improvements for sentiment classification, outperforming even manually curated counterfactuals, under select settings. We further present detailed analyses to show the advantages of NeuroCounterfactuals over approaches involving simple, minimal edits.</abstract>
      <url hash="ddeaf9b7">2022.findings-emnlp.371</url>
      <bibkey>howard-etal-2022-neurocounterfactuals</bibkey>
      <video href="2022.findings-emnlp.371.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.371</doi>
    </paper>
    <paper id="372">
      <title>Don’t Just Clean It, Proxy Clean It: Mitigating Bias by Proxy in Pre-Trained Models</title>
      <author><first>Swetasudha</first><last>Panda</last></author>
      <author><first>Ari</first><last>Kobren</last></author>
      <author><first>Michael</first><last>Wick</last></author>
      <author><first>Qinlan</first><last>Shen</last></author>
      <pages>5073-5085</pages>
      <abstract>Transformer-based pre-trained models are known to encode societal biases not only in their contextual representations, but also in downstream predictions when fine-tuned on task-specific data. We present D-Bias, an approach that selectively eliminates stereotypical associations (e.g, co-occurrence statistics) at fine-tuning, such that the model doesn’t learn to excessively rely on those signals.D-Bias attenuates biases from both identity words and frequently co-occurring proxies, which we select using pointwise mutual information. We apply D-Bias to a) occupation classification, and b) toxicity classification and find that our approach substantially reduces downstream biases (e.g. by &gt; 60% in toxicity classification, for identities that are most frequently flagged as toxic on online platforms).In addition, we show that D-Bias dramatically improves upon scrubbing, i.e., removing only the identity words in question. We also demonstrate that D-Bias easily extends to multiple identities, and achieves competitive performance with two recently proposed debiasing approaches: R-LACE and INLP.</abstract>
      <url hash="390c2d62">2022.findings-emnlp.372</url>
      <bibkey>panda-etal-2022-dont</bibkey>
      <video href="2022.findings-emnlp.372.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.372</doi>
    </paper>
    <paper id="373">
      <title>The Undesirable Dependence on Frequency of Gender Bias Metrics Based on Word Embeddings</title>
      <author><first>Francisco</first><last>Valentini</last></author>
      <author><first>Germán</first><last>Rosati</last></author>
      <author><first>Diego</first><last>Fernandez Slezak</last></author>
      <author><first>Edgar</first><last>Altszyler</last></author>
      <pages>5086-5092</pages>
      <abstract>Numerous works use word embedding-based metrics to quantify societal biases and stereotypes in texts. Recent studies have found that word embeddings can capture semantic similarity but may be affected by word frequency. In this work we study the effect of frequency when measuring female vs. male gender bias with word embedding-based bias quantification methods. We find that Skip-gram with negative sampling and GloVe tend to detect male bias in high frequency words, while GloVe tends to return female bias in low frequency words. We show these behaviors still exist when words are randomly shuffled. This proves that the frequency-based effect observed in unshuffled corpora stems from properties of the metric rather than from word associations. The effect is spurious and problematic since bias metrics should depend exclusively on word co-occurrences and not individual word frequencies. Finally, we compare these results with the ones obtained with an alternative metric based on Pointwise Mutual Information. We find that this metric does not show a clear dependence on frequency, even though it is slightly skewed towards male bias across all frequencies.</abstract>
      <url hash="2a54c702">2022.findings-emnlp.373</url>
      <bibkey>valentini-etal-2022-undesirable</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.373</doi>
    </paper>
    <paper id="374">
      <title><fixed-case>B</fixed-case>io<fixed-case>NLI</fixed-case>: Generating a Biomedical <fixed-case>NLI</fixed-case> Dataset Using Lexico-semantic Constraints for Adversarial Examples</title>
      <author><first>Mohaddeseh</first><last>Bastan</last></author>
      <author><first>Mihai</first><last>Surdeanu</last></author>
      <author><first>Niranjan</first><last>Balasubramanian</last></author>
      <pages>5093-5104</pages>
      <abstract>Natural language inference (NLI) is critical in many domains requiring complex decision-making, such as the biomedical domain. We introduce a novel semi-supervised procedure that bootstraps biomedical NLI datasets from positive entailment examples present in abstracts of biomedical publications. We focus on challenging texts where the hypothesis includes mechanistic information such as biochemical interactions between two entities. A key contribution of this work is automating the creation of negative examples that are informative without being simplistic. We generate a range of negative examples using nine strategies that manipulate the structure of the underlying mechanisms both with rules, e.g., flip the roles of the entities in the interaction, and, more importantly, by imposing the perturbed conditions as logical constraints in a neuro-logical decoding system (CITATION).We use this procedure to create a novel dataset for NLI in the biomedical domain, called . The accuracy of neural classifiers on this dataset is in the mid 70s F1, which indicates that this NLI task remains to be solved. Critically, we observe that the performance on the different classes of negative examples varies widely, from 97% F1 on the simple negative examples that change the role of the entities in the hypothesis, to barely better than chance on the negative examples generated using neuro-logic decoding.</abstract>
      <url hash="a9c79919">2022.findings-emnlp.374</url>
      <bibkey>bastan-etal-2022-bionli</bibkey>
      <video href="2022.findings-emnlp.374.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.374</doi>
    </paper>
    <paper id="375">
      <title>Self-supervised Cross-modal Pretraining for Speech Emotion Recognition and Sentiment Analysis</title>
      <author><first>Iek-Heng</first><last>Chu</last></author>
      <author><first>Ziyi</first><last>Chen</last></author>
      <author><first>Xinlu</first><last>Yu</last></author>
      <author><first>Mei</first><last>Han</last></author>
      <author><first>Jing</first><last>Xiao</last></author>
      <author><first>Peng</first><last>Chang</last></author>
      <pages>5105-5114</pages>
      <abstract>Multimodal speech emotion recognition (SER) and sentiment analysis (SA) are important techniques for human-computer interaction. Most existing multimodal approaches utilize either shallow cross-modal fusion of pretrained features, or deep cross-modal fusion with raw features. Recently, attempts have been made to fuse pretrained feature representations in a deep fusion manner during fine-tuning stage. However those approaches have not led to improved results, partially due to their relatively simple fusion mechanisms and lack of proper cross-modal pretraining. In this work, leveraging single-modal pretrained models (RoBERTa and HuBERT), we propose a novel deeply-fused audio-text bi-modal transformer with carefully designed cross-modal fusion mechanism and a stage-wise cross-modal pretraining scheme to fully facilitate the cross-modal learning. Our experiment results show that the proposed method achieves state-of-the-art results on the public IEMOCAP emotion and CMU-MOSEI sentiment datasets, exceeding the previous benchmarks by a large margin.</abstract>
      <url hash="975030e6">2022.findings-emnlp.375</url>
      <bibkey>chu-etal-2022-self</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.375</doi>
    </paper>
    <paper id="376">
      <title>Multimodal Conversation Modelling for Topic Derailment Detection</title>
      <author><first>Zhenhao</first><last>Li</last></author>
      <author><first>Marek</first><last>Rei</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <pages>5115-5127</pages>
      <abstract>Conversations on social media tend to go off-topic and turn into different and sometimes toxic exchanges. Previous work focuses on analysing textual dialogues that have derailed into toxic content, but the range of derailment types is much broader, including spam or bot content, tangential comments, etc. In addition, existing work disregards conversations that involve visual information (i.e. images or videos), which are prevalent on most platforms. In this paper, we take a broader view of conversation derailment and propose a new challenge: detecting derailment based on the “change of conversation topic”, where the topic is defined by an initial post containing both a text and an image. For that, we (i) create the first Multimodal Conversation Derailment (MCD) dataset, and (ii) introduce a new multimodal conversational architecture (MMConv) that utilises visual and conversational contexts to classify comments for derailment. Experiments show that MMConv substantially outperforms previous text-based approaches to detect conversation derailment, as well as general multimodal classifiers. MMConv is also more robust to textual noise, since it relies on richer contextual information.</abstract>
      <url hash="0e849d93">2022.findings-emnlp.376</url>
      <bibkey>li-etal-2022-multimodal</bibkey>
      <video href="2022.findings-emnlp.376.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.376</doi>
    </paper>
    <paper id="377">
      <title>Active Learning for Abstractive Text Summarization</title>
      <author><first>Akim</first><last>Tsvigun</last></author>
      <author><first>Ivan</first><last>Lysenko</last></author>
      <author><first>Danila</first><last>Sedashov</last></author>
      <author><first>Ivan</first><last>Lazichny</last></author>
      <author><first>Eldar</first><last>Damirov</last></author>
      <author><first>Vladimir</first><last>Karlov</last></author>
      <author><first>Artemy</first><last>Belousov</last></author>
      <author><first>Leonid</first><last>Sanochkin</last></author>
      <author><first>Maxim</first><last>Panov</last></author>
      <author><first>Alexander</first><last>Panchenko</last></author>
      <author><first>Mikhail</first><last>Burtsev</last></author>
      <author><first>Artem</first><last>Shelmanov</last></author>
      <pages>5128-5152</pages>
      <abstract>Construction of human-curated annotated datasets for abstractive text summarization (ATS) is very time-consuming and expensive because creating each instance requires a human annotator to read a long document and compose a shorter summary that would preserve the key information relayed by the original document. Active Learning (AL) is a technique developed to reduce the amount of annotation required to achieve a certain level of machine learning model performance. In information extraction and text classification, AL can reduce the amount of labor up to multiple times. Despite its potential for aiding expensive annotation, as far as we know, there were no effective AL query strategies for ATS. This stems from the fact that many AL strategies rely on uncertainty estimation, while as we show in our work, uncertain instances are usually noisy, and selecting them can degrade the model performance compared to passive annotation. We address this problem by proposing the first effective query strategy for AL in ATS based on diversity principles. We show that given a certain annotation budget, using our strategy in AL annotation helps to improve the model performance in terms of ROUGE and consistency scores. Additionally, we analyze the effect of self-learning and show that it can additionally increase the performance of the model.</abstract>
      <url hash="149c6fa4">2022.findings-emnlp.377</url>
      <bibkey>tsvigun-etal-2022-active</bibkey>
      <video href="2022.findings-emnlp.377.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.377</doi>
    </paper>
    <paper id="378">
      <title>Finding Memo: Extractive Memorization in Constrained Sequence Generation Tasks</title>
      <author><first>Vikas</first><last>Raunak</last></author>
      <author><first>Arul</first><last>Menezes</last></author>
      <pages>5153-5162</pages>
      <abstract>Memorization presents a challenge for several constrained Natural Language Generation (NLG) tasks such as Neural Machine Translation (NMT), wherein the proclivity of neural models to memorize noisy and atypical samples reacts adversely with the noisy (web crawled) datasets. However, previous studies of memorization in constrained NLG tasks have only focused on counterfactual memorization, linking it to the problem of hallucinations. In this work, we propose a new, inexpensive algorithm for extractive memorization (exact training data generation under insufficient context) in constrained sequence generation tasks and use it to study extractive memorization and its effects in NMT. We demonstrate that extractive memorization poses a serious threat to NMT reliability by qualitatively and quantitatively characterizing the memorized samples as well as the model behavior in their vicinity. Based on empirical observations, we develop a simple algorithm which elicits non-memorized translations of memorized samples from the same model, for a large fraction of such samples. Finally, we show that the proposed algorithm could also be leveraged to mitigate memorization in the model through finetuning. We have released the code to reproduce our results at <url>https://github.com/vyraun/Finding-Memo</url>.</abstract>
      <url hash="a8750040">2022.findings-emnlp.378</url>
      <bibkey>raunak-menezes-2022-finding</bibkey>
      <video href="2022.findings-emnlp.378.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.378</doi>
    </paper>
    <paper id="379">
      <title><fixed-case>SALTED</fixed-case>: A Framework for <fixed-case>SA</fixed-case>lient Long-tail Translation Error Detection</title>
      <author><first>Vikas</first><last>Raunak</last></author>
      <author><first>Matt</first><last>Post</last></author>
      <author><first>Arul</first><last>Menezes</last></author>
      <pages>5163-5179</pages>
      <abstract>Traditional machine translation (MT) metrics provide an average measure of translation quality that is insensitive to the long tail of behavioral problems. Examples include translation of numbers, physical units, dropped content and hallucinations. These errors, which occur rarely and unpredictably in Neural Machine Translation (NMT), greatly undermine the reliability of state-of-the-art MT systems. Consequently, it is important to have visibility into these problems during model development. Towards this end, we introduce SALTED, a specifications-based framework for behavioral testing of NMT models. At the core of our approach is the use of high-precision detectors that flag errors (or alternatively, verify output correctness) between a source sentence and a system output. These detectors provide fine-grained measurements of long-tail errors, providing a trustworthy view of problems that were previously invisible. We demonstrate that such detectors could be used not just to identify salient long-tail errors in MT systems, but also for higher-recall filtering of the training data, fixing targeted errors with model fine-tuning in NMT and generating novel data for metamorphic testing to elicit further bugs in models.</abstract>
      <url hash="338930f5">2022.findings-emnlp.379</url>
      <bibkey>raunak-etal-2022-salted</bibkey>
      <video href="2022.findings-emnlp.379.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.379</doi>
    </paper>
    <paper id="380">
      <title>Discord Questions: A Computational Approach To Diversity Analysis in News Coverage</title>
      <author><first>Philippe</first><last>Laban</last></author>
      <author><first>Chien-Sheng</first><last>Wu</last></author>
      <author><first>Lidiya</first><last>Murakhovs’ka</last></author>
      <author><first>Xiang</first><last>Chen</last></author>
      <author><first>Caiming</first><last>Xiong</last></author>
      <pages>5180-5194</pages>
      <abstract>There are many potential benefits to news readers accessing diverse sources. Modern news aggregators do the hard work of organizing the news, offering readers a plethora of source options, but choosing which source to read remains challenging. We propose a new framework to assist readers in identifying source differences and gaining an understanding of news coverage diversity. The framework is based on the generation of Discord Questions: questions with a diverse answer pool, explicitly illustrating source differences. To assemble a prototype of the framework, we focus on two components: (1) discord question generation, the task of generating questions answered differently by sources, for which we propose an automatic scoring method, and create a model that improves performance from current question generation (QG) methods by 5%, (2) answer consolidation, the task of grouping answers to a question that are semantically similar, for which we collect data and repurpose a method that achieves 81% balanced accuracy on our realistic test set. We illustrate the framework’s feasibility through a prototype interface. Even though model performance at discord QG still lags human performance by more than 15%, generated questions are judged to be more interesting than factoid questions and can reveal differences in the level of detail, sentiment, and reasoning of sources in news coverage. Code is available at <url>https://github.com/Salesforce/discord_questions</url>.</abstract>
      <url hash="31e321af">2022.findings-emnlp.380</url>
      <bibkey>laban-etal-2022-discord</bibkey>
      <video href="2022.findings-emnlp.380.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.380</doi>
    </paper>
    <paper id="381">
      <title><fixed-case>F</fixed-case>ocus<fixed-case>QA</fixed-case>: Open-Domain Question Answering with a Context in Focus</title>
      <author><first>Gianni</first><last>Barlacchi</last></author>
      <author><first>Ivano</first><last>Lauriola</last></author>
      <author><first>Alessandro</first><last>Moschitti</last></author>
      <author><first>Marco</first><last>Del Tredici</last></author>
      <author><first>Xiaoyu</first><last>Shen</last></author>
      <author><first>Thuy</first><last>Vu</last></author>
      <author id="bill-byrne-ucsd"><first>Bill</first><last>Byrne</last></author>
      <author><first>Adrià</first><last>de Gispert</last></author>
      <pages>5195-5208</pages>
      <abstract>We introduce question answering with a cotext in focus, a task that simulates a free interaction with a QA system. The user reads on a screen some information about a topic, and they can follow-up with questions that can be either related or not to the topic; and the answer can be found in the document containing the screen content or from other pages. We call such information context. To study the task, we construct FocusQA, a dataset for answer sentence selection (AS2) with 12,165011unique question/context pairs, and a total of 109,940 answers. To build the dataset, we developed a novel methodology that takes existing questions and pairs them with relevant contexts. To show the benefits of this approach, we present a comparative analysis with a set of questions written by humans after reading the context, showing that our approach greatly helps in eliciting more realistic question/context pairs. Finally, we show that the task poses several challenges for incorporating contextual information. In this respect, we introduce strong baselines for answer sentence selection that outperform the precision of state-of-the-art models for AS2 up to 21.3% absolute points.</abstract>
      <url hash="84864dce">2022.findings-emnlp.381</url>
      <bibkey>barlacchi-etal-2022-focusqa</bibkey>
      <video href="2022.findings-emnlp.381.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.381</doi>
    </paper>
    <paper id="382">
      <title>Challenges and Opportunities in Information Manipulation Detection: An Examination of Wartime <fixed-case>R</fixed-case>ussian Media</title>
      <author><first>Chan Young</first><last>Park</last></author>
      <author><first>Julia</first><last>Mendelsohn</last></author>
      <author><first>Anjalie</first><last>Field</last></author>
      <author><first>Yulia</first><last>Tsvetkov</last></author>
      <pages>5209-5235</pages>
      <abstract>NLP research on public opinion manipulation campaigns has primarily focused on detecting overt strategies such as fake news and disinformation. However, information manipulation in the ongoing Russia-Ukraine war exemplifies how governments and media also employ more nuanced strategies. We release a new dataset, VoynaSlov, containing 38M+ posts from Russian media outlets on Twitter and VKontakte, as well as public activity and responses, immediately preceding and during the 2022 Russia-Ukraine war. We apply standard and recently-developed NLP models on VoynaSlov to examine agenda setting, framing, and priming, several strategies underlying information manipulation, and reveal variation across media outlet control, social media platform, and time. Our examination of these media effects and extensive discussion of current approaches’ limitations encourage further development of NLP models for understanding information manipulation in emerging crises, as well as other real-world and interdisciplinary tasks.</abstract>
      <url hash="dca998c9">2022.findings-emnlp.382</url>
      <bibkey>park-etal-2022-challenges</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.382</doi>
    </paper>
    <paper id="383">
      <title>Disentangling Task Relations for Few-shot Text Classification via Self-Supervised Hierarchical Task Clustering</title>
      <author><first>Juan</first><last>Zha</last></author>
      <author><first>Zheng</first><last>Li</last></author>
      <author><first>Ying</first><last>Wei</last></author>
      <author><first>Yu</first><last>Zhang</last></author>
      <pages>5236-5247</pages>
      <abstract>Few-Shot Text Classification (FSTC) imitates humans to learn a new text classifier efficiently with only few examples, by leveraging prior knowledge from historical tasks. However, most prior works assume that all the tasks are sampled from a single data source, which cannot adapt to real-world scenarios where tasks are heterogeneous and lie in different distributions. As such, existing methods may suffer from their globally knowledge-shared mechanisms to handle the task heterogeneity. On the other hand, inherent task relationships are not explicitly captured, making task knowledge unorganized and hard to transfer to new tasks. Thus, we explore a new FSTC setting where tasks can come from a diverse range of data sources. To address the task heterogeneity, we propose a self-supervised hierarchical task clustering (SS-HTC) method. SS-HTC not only customizes the cluster-specific knowledge by dynamically organizing heterogeneous tasks into different clusters in hierarchical levels but also disentangles the underlying relations between tasks to improve the interpretability. Empirically, extensive experiments on five public FSTC benchmark datasets demonstrate the effectiveness of SS-HTC.</abstract>
      <url hash="9d97b9e6">2022.findings-emnlp.383</url>
      <bibkey>zha-etal-2022-disentangling</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.383</doi>
    </paper>
    <paper id="384">
      <title><fixed-case>XRICL</fixed-case>: Cross-lingual Retrieval-Augmented In-Context Learning for Cross-lingual Text-to-<fixed-case>SQL</fixed-case> Semantic Parsing</title>
      <author><first>Peng</first><last>Shi</last></author>
      <author><first>Rui</first><last>Zhang</last></author>
      <author><first>He</first><last>Bai</last></author>
      <author><first>Jimmy</first><last>Lin</last></author>
      <pages>5248-5259</pages>
      <abstract>In-context learning using large language models has recently shown surprising results for semantic parsing tasks such as Text-to-SQL translation. Prompting GPT-3 or Codex using several examples of question-SQL pairs can produce excellent results, comparable to state-of-the-art finetuning-based models. However, existing work primarily focuses on English datasets, and it is unknown whether large language models can serve as competitive semantic parsers for other languages. To bridge this gap, our work focuses on cross-lingual Text-to-SQL semantic parsing for translating non-English utterances into SQL queries based on an English schema. We consider a zero-shot transfer learning setting with the assumption that we do not have any labeled examples in the target language (but have annotated examples in English).This work introduces the XRICL framework, which learns to retrieve relevant English exemplars for a given query to construct prompts. We also include global translation exemplars for a target language to facilitate the translation process for large language models. To systematically evaluate our model, we construct two new benchmark datasets, XSpider and XKaggle-dbqa, which include questions in Chinese, Vietnamese, Farsi, and Hindi.Our experiments show that XRICL effectively leverages large pre-trained language models to outperform existing baselines. Data and code are publicly available at <url>https://github.com/Impavidity/XRICL</url>.</abstract>
      <url hash="63473418">2022.findings-emnlp.384</url>
      <bibkey>shi-etal-2022-xricl</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.384</doi>
    </paper>
    <paper id="385">
      <title>Continuation <fixed-case>KD</fixed-case>: Improved Knowledge Distillation through the Lens of Continuation Optimization</title>
      <author><first>Aref</first><last>Jafari</last></author>
      <author><first>Ivan</first><last>Kobyzev</last></author>
      <author><first>Mehdi</first><last>Rezagholizadeh</last></author>
      <author><first>Pascal</first><last>Poupart</last></author>
      <author><first>Ali</first><last>Ghodsi</last></author>
      <pages>5260-5269</pages>
      <abstract>Knowledge Distillation (KD) has been extensively used for natural language understanding (NLU) tasks to improve a small model’s (a student) generalization by transferring the knowledge from a larger model (a teacher). Although KD methods achieve state-of-the-art performance in numerous settings, they suffer from several problems limiting their performance. It is shown in the literature that the capacity gap between the teacher and the student networks can make KD ineffective. Additionally, existing KD techniques do not mitigate the noise in the teacher’s output: modeling the noisy behaviour of the teacher can distract the student from learning more useful features. We propose a new KD method that addresses these problems and facilitates the training compared to previous techniques. Inspired by continuation optimization, we design a training procedure that optimizes the highly non-convex KD objective by starting with the smoothed version of this objective and making it more complex as the training proceeds. Our method (Continuation-KD) achieves state-of-the-art performance across various compact architectures on NLU (GLUE benchmark) and computer vision tasks (CIFAR-10 and CIFAR-100).</abstract>
      <url hash="c6d05109">2022.findings-emnlp.385</url>
      <bibkey>jafari-etal-2022-continuation</bibkey>
      <video href="2022.findings-emnlp.385.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.385</doi>
    </paper>
    <paper id="386">
      <title>Detecting Dementia from Long Neuropsychological Interviews</title>
      <author><first>Nauman</first><last>Dawalatabad</last></author>
      <author><first>Yuan</first><last>Gong</last></author>
      <author><first>Sameer</first><last>Khurana</last></author>
      <author><first>Rhoda</first><last>Au</last></author>
      <author><first>James</first><last>Glass</last></author>
      <pages>5270-5283</pages>
      <abstract>Neuropsychological exams are commonly used to diagnose various kinds of cognitive impairment. They typically involve a trained examiner who conducts a series of cognitive tests with a subject. In recent years, there has been growing interest in developing machine learning methods to extract speech and language biomarkers from exam recordings to provide automated input for cognitive assessment. Inspired by recent findings suggesting that the examiner’s language can influence cognitive impairment classifications, in this paper, we study the influence of the examiner on automatic dementia identification decisions in real-world neuropsychological exams. To mitigate the influence of the examiner, we propose a systematic three-stage pipeline for detecting dementia from exam recordings. In the first stage, we perform audio-based speaker diarization (i.e., estimating who spoke when?) by incorporating speaker discriminative features. In the second stage, we employ text-based language models to identify the role of the speaker (i.e., examiner or subject). Finally, in the third stage, we employ text- and audio-based models to detect cognitive impairment from hypothesized subject segments. Our studies suggest that incorporating audio-based diarization followed by text-based role identification helps mitigate the influences from the examiner’s segments. Further, we found that the text and audio modalities complement each other, and the performance improves when we use both modalities. We also perform several carefully designed experimental studies to assess the performance of each stage.</abstract>
      <url hash="ab8bd1d6">2022.findings-emnlp.386</url>
      <bibkey>dawalatabad-etal-2022-detecting</bibkey>
      <video href="2022.findings-emnlp.386.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.386</doi>
    </paper>
    <paper id="387">
      <title>Sarcasm Detection is Way Too Easy! An Empirical Comparison of Human and Machine Sarcasm Detection</title>
      <author><first>Ibrahim</first><last>Abu Farha</last></author>
      <author><first>Steven</first><last>Wilson</last></author>
      <author><first>Silviu</first><last>Oprea</last></author>
      <author><first>Walid</first><last>Magdy</last></author>
      <pages>5284-5295</pages>
      <abstract>Recently, author-annotated sarcasm datasets, which focus on intended, rather than perceived sarcasm, have been introduced. Although datasets collected using first-party annotation have important benefits, there is no comparison of human and machine performance on these new datasets. In this paper, we collect new annotations to provide human-level benchmarks for these first-party annotated sarcasm tasks in both English and Arabic, and compare the performance of human annotators to that of state-of-the-art sarcasm detection systems. Our analysis confirms that sarcasm detection is extremely challenging, with individual humans performing close to or slightly worse than the best trained models. With majority voting, however, humans are able to achieve the best results on all tasks. We also perform error analysis, finding that some of the most challenging examples are those that require additional context. We also highlight common features and patterns used to express sarcasm in English and Arabic such as idioms and proverbs. We suggest that to better capture sarcasm, future sarcasm detection datasets and models should focus on representing conversational and cultural context while leveraging world knowledge and common sense.</abstract>
      <url hash="e88b0f7b">2022.findings-emnlp.387</url>
      <bibkey>abu-farha-etal-2022-sarcasm</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.387</doi>
    </paper>
    <paper id="388">
      <title>Cross-lingual Text-to-<fixed-case>SQL</fixed-case> Semantic Parsing with Representation Mixup</title>
      <author><first>Peng</first><last>Shi</last></author>
      <author><first>Linfeng</first><last>Song</last></author>
      <author><first>Lifeng</first><last>Jin</last></author>
      <author><first>Haitao</first><last>Mi</last></author>
      <author><first>He</first><last>Bai</last></author>
      <author><first>Jimmy</first><last>Lin</last></author>
      <author><first>Dong</first><last>Yu</last></author>
      <pages>5296-5306</pages>
      <abstract>We focus on the cross-lingual Text-to-SQL semantic parsing task,where the parsers are expected to generate SQL for non-English utterances based on English database schemas. Intuitively, English translation as side information is an effective way to bridge the language gap,but noise introduced by the translation system may affect parser effectiveness. In this work, we propose a Representation Mixup Framework (Rex) for effectively exploiting translations in the cross-lingual Text-to-SQL task. Particularly, it uses a general encoding layer, a transition layer, and a target-centric layer to properly guide the information flow of the English translation. Experimental results on CSpider and VSpider show that our framework can benefit from cross-lingual training and improve the effectiveness of semantic parsers, achieving state-of-the-art performance.</abstract>
      <url hash="fbed619f">2022.findings-emnlp.388</url>
      <bibkey>shi-etal-2022-cross</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.388</doi>
    </paper>
    <paper id="389">
      <title><fixed-case>J</fixed-case>am<fixed-case>P</fixed-case>atois<fixed-case>NLI</fixed-case>: A Jamaican Patois Natural Language Inference Dataset</title>
      <author><first>Ruth-Ann</first><last>Armstrong</last></author>
      <author><first>John</first><last>Hewitt</last></author>
      <author><first>Christopher</first><last>Manning</last></author>
      <pages>5307-5320</pages>
      <abstract>JamPatoisNLI provides the first dataset for natural language inference in a creole language, Jamaican Patois.Many of the most-spoken low-resource languages are creoles. These languages commonly have a lexicon derived from a major world language and a distinctive grammar reflecting the languages of the original speakers and the process of language birth by creolization. This gives them a distinctive place in exploring the effectiveness of transfer from large monolingual or multilingual pretrained models. While our work, along with previous work, shows that transfer from these models to low-resource languages that are unrelated to languages in their training set is not very effective, we would expect stronger results from transfer to creoles. Indeed, our experiments show considerably better results from few-shot learning of JamPatoisNLI than for such unrelated languages, and help us begin to understand how the unique relationship between creoles and their high-resource base languages affect cross-lingual transfer. JamPatoisNLI, which consists of naturally-occurring premises and expert-written hypotheses, is a step towards steering research into a traditionally underserved language and a useful benchmark for understanding cross-lingual NLP.</abstract>
      <url hash="eddee9b1">2022.findings-emnlp.389</url>
      <bibkey>armstrong-etal-2022-jampatoisnli</bibkey>
      <video href="2022.findings-emnlp.389.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.389</doi>
    </paper>
    <paper id="390">
      <title>Are Neural Topic Models Broken?</title>
      <author><first>Alexander Miserlis</first><last>Hoyle</last></author>
      <author><first>Pranav</first><last>Goel</last></author>
      <author><first>Rupak</first><last>Sarkar</last></author>
      <author><first>Philip</first><last>Resnik</last></author>
      <pages>5321-5344</pages>
      <abstract>Recently, the relationship between automated and human evaluation of topic models has been called into question. Method developers have staked the efficacy of new topic model variants on automated measures, and their failure to approximate human preferences places these models on uncertain ground. Moreover, existing evaluation paradigms are often divorced from real-world use. Motivated by content analysis as a dominant real-world use case for topic modeling, we analyze two related aspects of topic models that affect their effectiveness and trustworthiness in practice for that purpose: the stability of their estimates and the extent to which the model’s discovered categories align with human-determined categories in the data. We find that neural topic models fare worse in both respects compared to an established classical method. We take a step toward addressing both issues in tandem by demonstrating that a straightforward ensembling method can reliably outperform the members of the ensemble.</abstract>
      <url hash="ecd5e418">2022.findings-emnlp.390</url>
      <bibkey>hoyle-etal-2022-neural</bibkey>
      <video href="2022.findings-emnlp.390.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.390</doi>
    </paper>
    <paper id="391">
      <title>Know Thy Strengths: Comprehensive Dialogue State Tracking Diagnostics</title>
      <author><first>Hyundong</first><last>Cho</last></author>
      <author><first>Chinnadhurai</first><last>Sankar</last></author>
      <author><first>Christopher</first><last>Lin</last></author>
      <author><first>Kaushik</first><last>Sadagopan</last></author>
      <author><first>Shahin</first><last>Shayandeh</last></author>
      <author><first>Asli</first><last>Celikyilmaz</last></author>
      <author><first>Jonathan</first><last>May</last></author>
      <author><first>Ahmad</first><last>Beirami</last></author>
      <pages>5345-5359</pages>
      <abstract>Recent works that revealed the vulnerability of dialogue state tracking (DST) models to distributional shifts have made holistic comparisons on robustness and qualitative analyses increasingly important for understanding their relative performance. We present our findings from standardized and comprehensive DST diagnoses, which have previously been sparse and uncoordinated, using our toolkit, CheckDST, a collection of robustness tests and failure mode analytics. We discover that different classes of DST models have clear strengths and weaknesses, where generation models are more promising for handling language variety while span-based classification models are more robust to unseen entities. Prompted by this discovery, we also compare checkpoints from the same model and find that the standard practice of selecting checkpoints using validation loss/accuracy is prone to overfitting and each model class has distinct patterns of failure. Lastly, we demonstrate how our diagnoses motivate a pre-finetuning procedure with non-dialogue data that offers comprehensive improvements to generation models by alleviating the impact of distributional shifts through transfer learning.</abstract>
      <url hash="796f2fa8">2022.findings-emnlp.391</url>
      <bibkey>cho-etal-2022-know</bibkey>
      <video href="2022.findings-emnlp.391.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.391</doi>
    </paper>
    <paper id="392">
      <title>Open-domain Question Answering via Chain of Reasoning over Heterogeneous Knowledge</title>
      <author><first>Kaixin</first><last>Ma</last></author>
      <author><first>Hao</first><last>Cheng</last></author>
      <author><first>Xiaodong</first><last>Liu</last></author>
      <author><first>Eric</first><last>Nyberg</last></author>
      <author><first>Jianfeng</first><last>Gao</last></author>
      <pages>5360-5374</pages>
      <abstract>We propose a novel open-domain question answering (ODQA) framework for answering single/multi-hop questions across heterogeneous knowledge sources. The key novelty of our method is the introduction of the intermediary modules into the current retriever-reader pipeline. Unlike previous methods that solely rely on the retriever for gathering all evidence in isolation,our intermediary performs a chain of reasoning over the retrieved set. Specifically, our method links the retrieved evidence with its related global context into graphs and organizes them into a candidate list of evidence chains. Built upon pretrained language models, our system achieves competitive performance on two ODQA datasets, OTT-QA and NQ, against tables and passages from Wikipedia.In particular, our model substantially outperforms the previous state-of-the-art on OTT-QA with an exact match score of 47.3 (45% relative gain).</abstract>
      <url hash="9a94baa6">2022.findings-emnlp.392</url>
      <bibkey>ma-etal-2022-open-domain</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.392</doi>
    </paper>
    <paper id="393">
      <title>Detecting Languages Unintelligible to Multilingual Models through Local Structure Probes</title>
      <author><first>Louis</first><last>Clouatre</last></author>
      <author><first>Prasanna</first><last>Parthasarathi</last></author>
      <author><first>Amal</first><last>Zouaq</last></author>
      <author><first>Sarath</first><last>Chandar</last></author>
      <pages>5375-5396</pages>
      <abstract>Providing better language tools for low-resource and endangered languages is imperative for equitable growth. Recent progress with massively multilingual pretrained models has proven surprisingly effective at performing zero-shot transfer to a wide variety of languages. However, this transfer is not universal, with many languages not currently understood by multilingual approaches. It is estimated that only 72 languages possess a “small set of labeled datasets” on which we could test a model’s performance, the vast majority of languages not having the resources available to simply evaluate performances on. In this work, we attempt to clarify which languages do and do not currently benefit from such transfer. To that end, we develop a general approach that requires only unlabelled text to detect which languages are not well understood by a cross-lingual model. Our approach is derived from the hypothesis that if a model’s understanding is insensitive to perturbations to text in a language, it is likely to have a limited understanding of that language. We construct a cross-lingual sentence similarity task to evaluate our approach empirically on 350, primarily low-resource, languages.</abstract>
      <url hash="6d5c1dfb">2022.findings-emnlp.393</url>
      <bibkey>clouatre-etal-2022-detecting</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.393</doi>
    </paper>
    <paper id="394">
      <title>Cards Against <fixed-case>AI</fixed-case>: Predicting Humor in a Fill-in-the-blank Party Game</title>
      <author><first>Dan</first><last>Ofer</last></author>
      <author><first>Dafna</first><last>Shahaf</last></author>
      <pages>5397-5403</pages>
      <abstract>Humor is an inherently social phenomenon, with humorous utterances shaped by what is socially and culturally accepted. Understanding humor is an important NLP challenge, with many applications to human-computer interactions. In this work we explore humor in the context of Cards Against Humanity – a party game where players complete fill-in-the-blank statements using cards that can be offensive or politically incorrect. We introduce a novel dataset of 300,000 online games of Cards Against Humanity, including 785K unique jokes, analyze it and provide insights. We trained machine learning models to predict the winning joke per game, achieving performance twice as good (20%) as random, even without any user information. On the more difficult task of judging novel cards, we see the models’ ability to generalize is moderate. Interestingly, we find that our models are primarily focused on punchline card, with the context having little impact. Analyzing feature importance, we observe that short, crude, juvenile punchlines tend to win.</abstract>
      <url hash="bd3b1a91">2022.findings-emnlp.394</url>
      <bibkey>ofer-shahaf-2022-cards</bibkey>
      <video href="2022.findings-emnlp.394.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.394</doi>
    </paper>
    <paper id="395">
      <title>Open-Vocabulary Argument Role Prediction For Event Extraction</title>
      <author><first>Yizhu</first><last>Jiao</last></author>
      <author><first>Sha</first><last>Li</last></author>
      <author><first>Yiqing</first><last>Xie</last></author>
      <author><first>Ming</first><last>Zhong</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <pages>5404-5418</pages>
      <abstract>The argument role in event extraction refers to the relation between an event and an argument participating in it. Despite the great progress in event extraction, existing studies still depend on roles pre-defined by domain experts. These studies expose obvious weakness when extending to emerging event types or new domains without available roles. Therefore, more attention and effort needs to be devoted to automatically customizing argument roles. In this paper, we define this essential but under-explored task: <b>open-vocabulary argument role prediction</b>. The goal of this task is to infer a set of argument roles for a given event type. We propose a novel unsupervised framework, RolePred for this task. Specifically, we formulate the role prediction problem as an in-filling task and construct prompts for a pre-trained language model to generate candidate roles. By extracting and analyzing the candidate arguments, the event-specific roles are further merged and selected. To standardize the research of this task, we collect a new human-annotated event extraction dataset including 143 customized argument roles with rich semantics. On this dataset, RolePred outperforms the existing methods by a large margin.</abstract>
      <url hash="27a4631b">2022.findings-emnlp.395</url>
      <bibkey>jiao-etal-2022-open</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.395</doi>
    </paper>
    <paper id="396">
      <title>Token-level Sequence Labeling for Spoken Language Understanding using Compositional End-to-End Models</title>
      <author><first>Siddhant</first><last>Arora</last></author>
      <author><first>Siddharth</first><last>Dalmia</last></author>
      <author><first>Brian</first><last>Yan</last></author>
      <author><first>Florian</first><last>Metze</last></author>
      <author><first>Alan W</first><last>Black</last></author>
      <author><first>Shinji</first><last>Watanabe</last></author>
      <pages>5419-5429</pages>
      <abstract>End-to-end spoken language understanding (SLU) systems are gaining popularity over cascaded approaches due to their simplicity and ability to avoid error propagation. However, these systems model sequence labeling as a sequence prediction task causing a divergence from its well-established token-level tagging formulation. We build compositional end-to-end SLU systems that explicitly separate the added complexity of recognizing spoken mentions in SLU from the NLU task of sequence labeling. By relying on intermediate decoders trained for ASR, our end-to-end systems transform the input modality from speech to token-level representations that can be used in the traditional sequence labeling framework. This composition of ASR and NLU formulations in our end-to-end SLU system offers direct compatibility with pre-trained ASR and NLU systems, allows performance monitoring of individual components and enables the use of globally normalized losses like CRF, making them attractive in practical scenarios. Our models outperform both cascaded and direct end-to-end models on a labeling task of named entity recognition across SLU benchmarks.</abstract>
      <url hash="7874a328">2022.findings-emnlp.396</url>
      <bibkey>arora-etal-2022-token</bibkey>
      <video href="2022.findings-emnlp.396.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.396</doi>
    </paper>
    <paper id="397">
      <title>Baked-in State Probing</title>
      <author><first>Shubham</first><last>Toshniwal</last></author>
      <author><first>Sam</first><last>Wiseman</last></author>
      <author><first>Karen</first><last>Livescu</last></author>
      <author><first>Kevin</first><last>Gimpel</last></author>
      <pages>5430-5435</pages>
      <abstract>Neural language models have been analyzed for their linguistic and extra-linguistic knowledge via probing. Of particular interest has been the following question: how much can a language model trained only on form learn about meaning? Recent work has demonstrated via probing classifiers that in the setting of simple procedural text, where by “meaning” we mean the underlying world state, language models have a non-trivial performance on world state tracking. However, our proposed evaluation based on model predictions shows differing results, suggesting that these models are either not capturing the world state or not using it. How do these results change if the model has access to the world state? We explore this alternate setting with access to the underlying world state only during training and investigate ways of “baking in” the state knowledge along with the primary task of language modeling. Our proposed approaches allow for state probing during inference simply via text prompts, avoiding any probing classifier machinery. In terms of performance, we show that baking in the state knowledge during training leads to significant improvements in state tracking performance and text generation quality,</abstract>
      <url hash="156bd320">2022.findings-emnlp.397</url>
      <bibkey>toshniwal-etal-2022-baked</bibkey>
      <video href="2022.findings-emnlp.397.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.397</doi>
    </paper>
    <paper id="398">
      <title><fixed-case>C</fixed-case>linical<fixed-case>T</fixed-case>5: A Generative Language Model for Clinical Text</title>
      <author><first>Qiuhao</first><last>Lu</last></author>
      <author><first>Dejing</first><last>Dou</last></author>
      <author><first>Thien</first><last>Nguyen</last></author>
      <pages>5436-5443</pages>
      <abstract>In the past few years, large pre-trained language models (PLMs) have been widely adopted in different areas and have made fundamental improvements over a variety of downstream tasks in natural language processing (NLP). Meanwhile, domain-specific variants of PLMs are being proposed to address the needs of domains that demonstrate a specific pattern of writing and vocabulary, e.g., BioBERT for the biomedical domain and ClinicalBERT for the clinical domain. Recently, generative language models like BART and T5 are gaining popularity with their competitive performance on text generation as well as on tasks cast as generative problems. However, in the clinical domain, such domain-specific generative variants are still underexplored. To address this need, our work introduces a T5-based text-to-text transformer model pre-trained on clinical text, i.e., ClinicalT5. We evaluate the proposed model both intrinsically and extrinsically over a diverse set of tasks across multiple datasets, and show that ClinicalT5 dramatically outperforms T5 in the domain-specific tasks and compares favorably with its close baselines.</abstract>
      <url hash="06a0b222">2022.findings-emnlp.398</url>
      <bibkey>lu-etal-2022-clinicalt5</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.398</doi>
    </paper>
    <paper id="399">
      <title>Find Someone Who: Visual Commonsense Understanding in Human-Centric Grounding</title>
      <author><first>Haoxuan</first><last>You</last></author>
      <author><first>Rui</first><last>Sun</last></author>
      <author><first>Zhecan</first><last>Wang</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <author><first>Shih-Fu</first><last>Chang</last></author>
      <pages>5444-5454</pages>
      <abstract>From a visual scene containing multiple people, human is able to distinguish each individual given the context descriptions about what happened before, their mental/physical states or intentions, etc. Above ability heavily relies on human-centric commonsense knowledge and reasoning. For example, if asked to identify the “person who needs healing” in an image, we need to first know that they usually have injuries or suffering expressions, then find the corresponding visual clues before finally grounding the person. We present a new commonsense task, Human-centric Commonsense Grounding, that tests the models’ ability to ground individuals given the context descriptions about what happened before, and their mental/physical states or intentions. We further create a benchmark, HumanCog, a dataset with 130k grounded commonsensical descriptions annotated on 67k images, covering diverse types of commonsense and visual scenes. We set up a context-object-aware method as a strong baseline that outperforms previous pre-trained and non-pretrained models. Further analysis demonstrates that rich visual commonsense and powerful integration of multi-modal commonsense are essential, which sheds light on future works. Data and code will be available at <url>https://github.com/Hxyou/HumanCog</url>.</abstract>
      <url hash="2d82c6fb">2022.findings-emnlp.399</url>
      <bibkey>you-etal-2022-find</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.399</doi>
    </paper>
    <paper id="400">
      <title><fixed-case>C</fixed-case>risis<fixed-case>LTLS</fixed-case>um: A Benchmark for Local Crisis Event Timeline Extraction and Summarization</title>
      <author><first>Hossein</first><last>Rajaby Faghihi</last></author>
      <author><first>Bashar</first><last>Alhafni</last></author>
      <author><first>Ke</first><last>Zhang</last></author>
      <author><first>Shihao</first><last>Ran</last></author>
      <author><first>Joel</first><last>Tetreault</last></author>
      <author><first>Alejandro</first><last>Jaimes</last></author>
      <pages>5455-5477</pages>
      <abstract>Social media has increasingly played a key role in emergency response: first responders can use public posts to better react to ongoing crisis events and deploy the necessary resources where they are most needed. Timeline extraction and abstractive summarization are critical technical tasks to leverage large numbers of social media posts about events. Unfortunately, there are few datasets for benchmarking technical approaches for those tasks. This paper presents , the largest dataset of local crisis event timelines available to date. contains 1,000 crisis event timelines across four domains: wildfires, local fires, traffic, and storms. We built using a semi-automated cluster-then-refine approach to collect data from the public Twitter stream. Our initial experiments indicate a significant gap between the performance of strong baselines compared to the human performance on both tasks. Our dataset, code, and models are publicly available (<url>https://github.com/CrisisLTLSum/CrisisTimelines</url>).</abstract>
      <url hash="d2b7ac76">2022.findings-emnlp.400</url>
      <bibkey>rajaby-faghihi-etal-2022-crisisltlsum</bibkey>
      <video href="2022.findings-emnlp.400.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.400</doi>
    </paper>
    <paper id="401">
      <title>Prompt-Tuning Can Be Much Better Than Fine-Tuning on Cross-lingual Understanding With Multilingual Language Models</title>
      <author><first>Lifu</first><last>Tu</last></author>
      <author><first>Caiming</first><last>Xiong</last></author>
      <author><first>Yingbo</first><last>Zhou</last></author>
      <pages>5478-5485</pages>
      <abstract>Pre-trained multilingual language models show significant performance gains for zero-shot cross-lingual model transfer on a wide range of natural language understanding (NLU) tasks. Previously, for zero-shot cross-lingual evaluation, pre-trained models are only fine-tuned on English data and tested on a variety of target languages. In this paper, we do cross-lingualevaluation on various NLU tasks (sentence classification, sequence labeling, question answering) using prompt-tuning and compare it with fine-tuning. The results show that prompt tuning achieves much better cross-lingual transfer than fine-tuning across datasets, with only 0.1% to 0.3% tuned parameters. Additionally, we demonstrate through the analysis that prompt tuning can have better cross-lingual transfer-ability of representations on downstream tasks with better aligned decision boundaries.</abstract>
      <url hash="b8d06953">2022.findings-emnlp.401</url>
      <bibkey>tu-etal-2022-prompt</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.401</doi>
    </paper>
    <paper id="402">
      <title><fixed-case>BERT</fixed-case> Meets <fixed-case>CTC</fixed-case>: New Formulation of End-to-End Speech Recognition with Pre-trained Masked Language Model</title>
      <author><first>Yosuke</first><last>Higuchi</last></author>
      <author><first>Brian</first><last>Yan</last></author>
      <author><first>Siddhant</first><last>Arora</last></author>
      <author><first>Tetsuji</first><last>Ogawa</last></author>
      <author><first>Tetsunori</first><last>Kobayashi</last></author>
      <author><first>Shinji</first><last>Watanabe</last></author>
      <pages>5486-5503</pages>
      <abstract>This paper presents BERT-CTC, a novel formulation of end-to-end speech recognition that adapts BERT for connectionist temporal classification (CTC). Our formulation relaxes the conditional independence assumptions used in conventional CTC and incorporates linguistic knowledge through the explicit output dependency obtained by BERT contextual embedding. BERT-CTC attends to the full contexts of the input and hypothesized output sequences via the self-attention mechanism. This mechanism encourages a model to learn inner/inter-dependencies between the audio and token representations while maintaining CTC’s training efficiency. During inference, BERT-CTC combines a mask-predict algorithm with CTC decoding, which iteratively refines an output sequence. The experimental results reveal that BERT-CTC improves over conventional approaches across variations in speaking styles and languages. Finally, we show that the semantic representations in BERT-CTC are beneficial towards downstream spoken language understanding tasks.</abstract>
      <url hash="0f91b845">2022.findings-emnlp.402</url>
      <bibkey>higuchi-etal-2022-bert</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.402</doi>
    </paper>
    <paper id="403">
      <title><fixed-case>E</fixed-case>tri<fixed-case>CA</fixed-case>: Event-Triggered Context-Aware Story Generation Augmented by Cross Attention</title>
      <author><first>Chen</first><last>Tang</last></author>
      <author><first>Chenghua</first><last>Lin</last></author>
      <author><first>Henglin</first><last>Huang</last></author>
      <author><first>Frank</first><last>Guerin</last></author>
      <author><first>Zhihao</first><last>Zhang</last></author>
      <pages>5504-5518</pages>
      <abstract>One of the key challenges of automatic story generation is how to generate a long narrative that can maintain fluency, relevance, and coherence. Despite recent progress, current story generation systems still face the challenge of how to effectively capture contextual and event features, which has a profound impact on a model’s generation performance. To address these challenges, we present EtriCA, a novel neural generation model, which improves the relevance and coherence of the generated stories through residually mapping context features to event sequences with a cross-attention mechanism. Such a feature capturing mechanism allows our model to better exploit the logical relatedness between events when generating stories. Extensive experiments based on both automatic and human evaluations show that our model significantly outperforms state-of-the-art baselines, demonstrating the effectiveness of our model in leveraging context and event features.</abstract>
      <url hash="7dc7b2c1">2022.findings-emnlp.403</url>
      <bibkey>tang-etal-2022-etrica</bibkey>
      <video href="2022.findings-emnlp.403.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.403</doi>
    </paper>
    <paper id="404">
      <title><fixed-case>LADIS</fixed-case>: Language Disentanglement for 3<fixed-case>D</fixed-case> Shape Editing</title>
      <author><first>Ian</first><last>Huang</last></author>
      <author><first>Panos</first><last>Achlioptas</last></author>
      <author><first>Tianyi</first><last>Zhang</last></author>
      <author><first>Sergei</first><last>Tulyakov</last></author>
      <author><first>Minhyuk</first><last>Sung</last></author>
      <author><first>Leonidas</first><last>Guibas</last></author>
      <pages>5519-5532</pages>
      <abstract>Natural language interaction is a promising direction for democratizing 3D shape design. However, existing methods for text-driven 3D shape editing face challenges in producing decoupled, local edits to 3D shapes. We address this problem by learning disentangled latent representations that ground language in 3D geometry. To this end, we propose a complementary tool set including a novel network architecture, a disentanglement loss, and a new editing procedure. Additionally, to measure edit locality, we define a new metric that we call part-wise edit precision. We show that our method outperforms existing SOTA methods by 20% in terms of edit locality, and up to 6.6% in terms of language reference resolution accuracy. Human evaluations additionally show that compared to the existing SOTA, our method produces shape edits that are more local, more semantically accurate, and more visually obvious. Our work suggests that by solely disentangling language representations, downstream 3D shape editing can become more local to relevant parts, even if the model was never given explicit part-based supervision.</abstract>
      <url hash="1e5d0bd7">2022.findings-emnlp.404</url>
      <bibkey>huang-etal-2022-ladis</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.404</doi>
    </paper>
    <paper id="405">
      <title>Effective Pretraining Objectives for Transformer-based Autoencoders</title>
      <author><first>Luca</first><last>Di Liello</last></author>
      <author><first>Matteo</first><last>Gabburo</last></author>
      <author><first>Alessandro</first><last>Moschitti</last></author>
      <pages>5533-5547</pages>
      <abstract>In this paper, we study trade-offs between efficiency, cost and accuracy when pre-training Transformer encoders with different pre-training objectives. For this purpose, we analyze features of common objectives and combine them to create new effective pre-training approaches. Specifically, we designed light token generators based on a straightforward statistical approach, which can replace ELECTRA computationally heavy generators, thus highly reducing cost. Our experiments also show that (i) there are more efficient alternatives to BERT’s MLM, and (ii) it is possible to efficiently pre-train Transformer-based models using lighter generators without a significant drop in performance.</abstract>
      <url hash="324d730c">2022.findings-emnlp.405</url>
      <bibkey>di-liello-etal-2022-effective</bibkey>
      <video href="2022.findings-emnlp.405.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.405</doi>
    </paper>
    <paper id="406">
      <title>Language Model Detoxification in Dialogue with Contextualized Stance Control</title>
      <author><first>Jing</first><last>Qian</last></author>
      <author><first>Xifeng</first><last>Yan</last></author>
      <pages>5548-5558</pages>
      <abstract>To reduce the toxic degeneration in a pretrained Language Model (LM), previous work on Language Model detoxification has focused on reducing the toxicity of the generation itself (self-toxicity) without consideration of the context. As a result, a type of implicit offensive language where the generations support the offensive language in the context is ignored. Different from the LM controlling tasks in previous work, where the desired attributes are fixed for generation, the desired stance of the generation depends on the offensiveness of the context. Therefore, we propose a novel control method to do context-dependent detoxification with the stance taken into consideration. We introduce meta prefixes to learn the contextualized stance control strategy and to generate the stance control prefix according to the input context. The generated stance prefix is then combined with the toxicity control prefix to guide the response generation. Experimental results show that our proposed method can effectively learn the context-dependent stance control strategies while keeping a low self-toxicity of the underlying LM.</abstract>
      <url hash="75b2ff4a">2022.findings-emnlp.406</url>
      <bibkey>qian-yan-2022-language</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.406</doi>
    </paper>
    <paper id="407">
      <title>Multilingual <fixed-case>S</fixed-case>ub<fixed-case>E</fixed-case>vent Relation Extraction: A Novel Dataset and Structure Induction Method</title>
      <author><first>Viet</first><last>Lai</last></author>
      <author><first>Hieu</first><last>Man</last></author>
      <author><first>Linh</first><last>Ngo</last></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Thien</first><last>Nguyen</last></author>
      <pages>5559-5570</pages>
      <abstract>Subevent Relation Extraction (SRE) is a task in Information Extraction that aims to recognize spatial and temporal containment relations between event mentions in text. Recent methods have utilized pre-trained language models to represent input texts for SRE. However, a key issue in existing SRE methods is the employment of sequential order of words in texts to feed into representation learning methods, thus unable to explicitly focus on important context words and their interactions to enhance representations. In this work, we introduce a new method for SRE that learns to induce effective graph structures for input texts to boost representation learning. Our method features a word alignment framework with dependency paths and optimal transport to identify important context words to form effective graph structures for SRE. In addition, to enable SRE research on non-English languages, we present a new multilingual SRE dataset for five typologically different languages. Extensive experiments reveal the state-of-the-art performance for our method on different datasets and languages.</abstract>
      <url hash="deb68872">2022.findings-emnlp.407</url>
      <bibkey>lai-etal-2022-multilingual-subevent</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.407</doi>
    </paper>
    <paper id="408">
      <title>A Two-Stage Approach towards Generalization in Knowledge Base Question Answering</title>
      <author><first>Srinivas</first><last>Ravishankar</last></author>
      <author><first>Dung</first><last>Thai</last></author>
      <author><first>Ibrahim</first><last>Abdelaziz</last></author>
      <author><first>Nandana</first><last>Mihindukulasooriya</last></author>
      <author><first>Tahira</first><last>Naseem</last></author>
      <author><first>Pavan</first><last>Kapanipathi</last></author>
      <author><first>Gaetano</first><last>Rossiello</last></author>
      <author><first>Achille</first><last>Fokoue</last></author>
      <pages>5571-5580</pages>
      <abstract>Most existing approaches for Knowledge Base Question Answering (KBQA) focus on a specific underlying knowledge base either because of inherent assumptions in the approach, or because evaluating it on a different knowledge base requires non-trivial changes. However, many popular knowledge bases share similarities in their underlying schemas that can be leveraged to facilitate generalization across knowledge bases. To achieve this generalization, we introduce a KBQA framework based on a 2-stage architecture that explicitly separates semantic parsing from the knowledge base interaction, facilitating transfer learning across datasets and knowledge graphs. We show that pretraining on datasets with a different underlying knowledge base can nevertheless provide significant performance gains and reduce sample complexity. Our approach achieves comparable or state-of-the-art performance for LC-QuAD (DBpedia), WebQSP (Freebase), SimpleQuestions (Wikidata) and MetaQA (Wikimovies-KG).</abstract>
      <url hash="3107a4c4">2022.findings-emnlp.408</url>
      <bibkey>ravishankar-etal-2022-two</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.408</doi>
    </paper>
    <paper id="409">
      <title>Few-Shot (Dis)Agreement Identification in Online Discussions with Regularized and Augmented Meta-Learning</title>
      <author><first>Yuanyuan</first><last>Lei</last></author>
      <author><first>Ruihong</first><last>Huang</last></author>
      <pages>5581-5593</pages>
      <abstract>Online discussions are abundant with opinions towards a common topic, and identifying (dis)agreement between a pair of comments enables many opinion mining applications. Realizing the increasing needs to analyze opinions for emergent new topics that however tend to lack annotations, we present the first meta-learning approach for few-shot (dis)agreement identification that can be quickly applied to analyze opinions for new topics with few labeled instances. Furthermore, we enhance the meta-learner’s domain generalization ability from two perspectives. The first is domain-invariant regularization, where we design a lexicon-based regularization loss to enable the meta-learner to learn domain-invariant cues. The second is domain-aware augmentation, where we propose domain-aware task augmentation for meta-training to learn domain-specific expressions. In addition to using an existing dataset, we also evaluate our approach on two very recent new topics, mask mandate and COVID vaccine, using our newly annotated datasets containing 1.5k and 1.4k SubReddits comment pairs respectively. Extensive experiments on three domains/topics demonstrate the effectiveness of our meta-learning approach.</abstract>
      <url hash="10541206">2022.findings-emnlp.409</url>
      <bibkey>lei-huang-2022-shot</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.409</doi>
    </paper>
    <paper id="410">
      <title>Data Cartography for Low-Resource Neural Machine Translation</title>
      <author><first>Aquia</first><last>Richburg</last></author>
      <author><first>Marine</first><last>Carpuat</last></author>
      <pages>5594-5607</pages>
      <abstract>While collecting or generating more parallel data is necessary to improve machine translation (MT) in low-resource settings, we lack an understanding of how the limited amounts of existing data are actually used to help guide the collection of further resources. In this paper, we apply data cartography techniques (Swayamdipta et al., 2020) to characterize the contribution of training samples in two low-resource MT tasks (Swahili-English and Turkish-English) throughout the training of standard neural MT models. Our empirical study shows that, unlike in prior work for classification tasks, most samples contribute to model training in low-resource MT, albeit not uniformly throughout the training process. Furthermore, uni-dimensional characterizations of samples – e.g., based on dual cross-entropy or word frequency – do not suffice to characterize to what degree they are hard or easy to learn. Taken together, our results suggest that data augmentation strategies for low-resource MT would benefit from model-in-the-loop strategies to maximize improvements.</abstract>
      <url hash="3ec06a5e">2022.findings-emnlp.410</url>
      <bibkey>richburg-carpuat-2022-data</bibkey>
      <video href="2022.findings-emnlp.410.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.410</doi>
    </paper>
    <paper id="411">
      <title>Augmenting Multi-Turn Text-to-<fixed-case>SQL</fixed-case> Datasets with Self-Play</title>
      <author><first>Qi</first><last>Liu</last></author>
      <author><first>Zihuiwen</first><last>Ye</last></author>
      <author><first>Tao</first><last>Yu</last></author>
      <author><first>Linfeng</first><last>Song</last></author>
      <author><first>Phil</first><last>Blunsom</last></author>
      <pages>5608-5620</pages>
      <abstract>The task of context-dependent text-to-SQL aims to convert multi-turn user utterances to formal SQL queries. This is a challenging task due to both the scarcity of training data from which to learn complex contextual dependencies and to generalize to unseen databases. In this paper we explore augmenting the training datasets using self-play, which leverages contextual information to synthesize new interactions to adapt the model to new databases. We first design a SQL-to-text model conditioned on a sampled goal query, which represents a user’s intent, that then converses with a text-to-SQL semantic parser to generate new interactions. We then filter the synthesized interactions and retrain the models with the augmented data. We find that self-play improves the accuracy of a strong baseline on SParC and CoSQL, two widely used cross-domain text-to-SQL datasets. Our analysis shows that self-play simulates various conversational thematic relations, enhances cross-domain generalization and improves beam-search.</abstract>
      <url hash="c66645de">2022.findings-emnlp.411</url>
      <bibkey>liu-etal-2022-augmenting-multi</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.411</doi>
    </paper>
    <paper id="412">
      <title>Prompt Compression and Contrastive Conditioning for Controllability and Toxicity Reduction in Language Models</title>
      <author><first>David</first><last>Wingate</last></author>
      <author><first>Mohammad</first><last>Shoeybi</last></author>
      <author><first>Taylor</first><last>Sorensen</last></author>
      <pages>5621-5634</pages>
      <abstract>We explore the idea of compressing the prompts used to condition language models, and show that compressed prompts can retain a substantive amount of information about the original prompt. For severely compressed prompts, while fine-grained information is lost, abstract information and general sentiments can be retained with surprisingly few parameters, which can be useful in the context of decode-time algorithms for controllability and toxicity reduction. We find that some complex prompts can be effectively compressed into a single token to guide generation. We also show that compressed prompts are largely compositional, and can be constructed such that they can be used to control independent aspects of generated text.</abstract>
      <url hash="3b6da520">2022.findings-emnlp.412</url>
      <bibkey>wingate-etal-2022-prompt</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.412</doi>
    </paper>
    <paper id="413">
      <title><fixed-case>N</fixed-case>atural<fixed-case>A</fixed-case>dversaries: Can Naturalistic Adversaries Be as Effective as Artificial Adversaries?</title>
      <author><first>Saadia</first><last>Gabriel</last></author>
      <author><first>Hamid</first><last>Palangi</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <pages>5635-5645</pages>
      <abstract>While a substantial body of prior work has explored adversarial example generation for natural language understanding tasks, these examples are often unrealistic and diverge from the real-world data distributions. In this work, we introduce a two-stage adversarial example generation framework (NaturalAdversaries), for designing adversaries that are effective at fooling a given classifier and demonstrate natural-looking failure cases that could plausibly occur during in-the-wild deployment of the models. At the first stage a token attribution method is used to summarize a given classifier’s behavior as a function of the key tokens in the input. In the second stage a generative model is conditioned on the key tokens from the first stage. NaturalAdversaries is adaptable to both black-box and white-box adversarial attacks based on the level of access to the model parameters. Our results indicate these adversaries generalize across domains, and offer insights for future research on improving robustness of neural text classification models.</abstract>
      <url hash="5155972d">2022.findings-emnlp.413</url>
      <bibkey>gabriel-etal-2022-naturaladversaries</bibkey>
      <video href="2022.findings-emnlp.413.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.413</doi>
    </paper>
    <paper id="414">
      <title>Multi-Path Transformer is Better: A Case Study on Neural Machine Translation</title>
      <author><first>Ye</first><last>Lin</last></author>
      <author><first>Shuhan</first><last>Zhou</last></author>
      <author><first>Yanyang</first><last>Li</last></author>
      <author><first>Anxiang</first><last>Ma</last></author>
      <author><first>Tong</first><last>Xiao</last></author>
      <author><first>Jingbo</first><last>Zhu</last></author>
      <pages>5646-5656</pages>
      <abstract>For years the model performance in machine learning obeyed a power-law relationship with the model size. For the consideration of parameter efficiency, recent studies focus on increasing model depth rather than width to achieve better performance. In this paper, we study how model width affects the Transformer model through a parameter-efficient multi-path structure. To better fuse features extracted from different paths, we add three additional operations to each sublayer: a normalization at the end of each path, a cheap operation to produce more features, and a learnable weighted mechanism to fuse all features flexibly. Extensive experiments on 12 WMT machine translation tasks show that, with the same number of parameters, the shallower multi-path model can achieve similar or even better performance than the deeper model. It reveals that we should pay more attention to the multi-path structure, and there should be a balance between the model depth and width to train a better large-scale Transformer.</abstract>
      <url hash="93db1011">2022.findings-emnlp.414</url>
      <bibkey>lin-etal-2022-multi-path</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.414</doi>
    </paper>
    <paper id="415">
      <title>Unsupervised Learning of Hierarchical Conversation Structure</title>
      <author><first>Bo-Ru</first><last>Lu</last></author>
      <author><first>Yushi</first><last>Hu</last></author>
      <author><first>Hao</first><last>Cheng</last></author>
      <author><first>Noah A.</first><last>Smith</last></author>
      <author><first>Mari</first><last>Ostendorf</last></author>
      <pages>5657-5670</pages>
      <abstract>Human conversations can evolve in many different ways, creating challenges for automatic understanding and summarization. Goal-oriented conversations often have meaningful sub-dialogue structure, but it can be highly domain-dependent. This work introduces an unsupervised approach to learning hierarchical conversation structure, including turn and sub-dialogue segment labels, corresponding roughly to dialogue acts and sub-tasks, respectively. The decoded structure is shown to be useful in enhancing neural models of language for three conversation-level understanding tasks. Further, the learned finite-state sub-dialogue network is made interpretable through automatic summarization.</abstract>
      <url hash="00f12f6a">2022.findings-emnlp.415</url>
      <bibkey>lu-etal-2022-unsupervised</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.415</doi>
    </paper>
    <paper id="416">
      <title>Task Compass: Scaling Multi-task Pre-training with Task Prefix</title>
      <author><first>Zhuosheng</first><last>Zhang</last></author>
      <author><first>Shuohang</first><last>Wang</last></author>
      <author><first>Yichong</first><last>Xu</last></author>
      <author><first>Yuwei</first><last>Fang</last></author>
      <author><first>Wenhao</first><last>Yu</last></author>
      <author id="yang-liu-microsoft"><first>Yang</first><last>Liu</last></author>
      <author><first>Hai</first><last>Zhao</last></author>
      <author><first>Chenguang</first><last>Zhu</last></author>
      <author><first>Michael</first><last>Zeng</last></author>
      <pages>5671-5685</pages>
      <abstract>Leveraging task-aware annotated data as supervised signals to assist with self-supervised learning on large-scale unlabeled data has become a new trend in pre-training language models. Existing studies show that multi-task learning with large-scale supervised tasks suffers from negative effects across tasks. To tackle the challenge, we propose a task prefix guided multi-task pre-training framework to explore the relationships among tasks. We conduct extensive experiments on 40 datasets, which show that our model can not only serve as the strong foundation backbone for a wide range of tasks but also be feasible as a probing tool for analyzing task relationships. The task relationships reflected by the prefixes align transfer learning performance between tasks. They also suggest directions for data augmentation with complementary tasks, which help our model achieve human-parity results on commonsense reasoning leaderboards. Code is available at <url>https://github.com/cooelf/CompassMTL</url>.</abstract>
      <url hash="e0dbba9e">2022.findings-emnlp.416</url>
      <bibkey>zhang-etal-2022-task</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.416</doi>
    </paper>
    <paper id="417">
      <title>Sharpness-Aware Minimization with Dynamic Reweighting</title>
      <author><first>Wenxuan</first><last>Zhou</last></author>
      <author><first>Fangyu</first><last>Liu</last></author>
      <author><first>Huan</first><last>Zhang</last></author>
      <author><first>Muhao</first><last>Chen</last></author>
      <pages>5686-5699</pages>
      <abstract>Deep neural networks are often overparameterized and may not easily achieve model generalization. Adversarial training has shown effectiveness in improving generalization by regularizing the change of loss on top of adversarially chosen perturbations. The recently proposed sharpness-aware minimization (SAM) algorithm conducts adversarial weight perturbation, encouraging the model to converge to a flat minima. SAM finds a common adversarial weight perturbation per-batch. Although per-instance adversarial weight perturbations are stronger adversaries and can potentially lead to better generalization performance, their computational cost is very high and thus it is impossible to use per-instance perturbations efficiently in SAM. In this paper, we tackle this efficiency bottleneck and propose sharpness-aware minimization with dynamic reweighting (delta-SAM). Our theoretical analysis motivates that it is possible to approach the stronger, per-instance adversarial weight perturbations using reweighted per-batch weight perturbations. delta-SAM dynamically reweights perturbation within each batch according to the theoretically principled weighting factors, serving as a good approximation to per-instance perturbation. Experiments on various natural language understanding tasks demonstrate the effectiveness of delta-SAM.</abstract>
      <url hash="c81c5514">2022.findings-emnlp.417</url>
      <bibkey>zhou-etal-2022-sharpness</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.417</doi>
    </paper>
    <paper id="418">
      <title>Predicting Long-Term Citations from Short-Term Linguistic Influence</title>
      <author><first>Sandeep</first><last>Soni</last></author>
      <author><first>David</first><last>Bamman</last></author>
      <author><first>Jacob</first><last>Eisenstein</last></author>
      <pages>5700-5716</pages>
      <abstract>A standard measure of the influence of a research paper is the number of times it is cited. However, papers may be cited for many reasons, and citation count is not informative about the extent to which a paper affected the content of subsequent publications. We therefore propose a novel method to quantify linguistic influence in timestamped document collections. There are two main steps: first, identify lexical and semantic changes using contextual embeddings and word frequencies; second, aggregate information about these changes into per-document influence parameters by estimating a high-dimensional Hawkes process with a low-rank parameter matrix. The resulting measures of linguistic influence are predictive of <i>future</i> citations. Specifically, the estimate of linguistic influence from the two years after a paper’s publication is correlated with and predictive of its citation count in the following three years. This is demonstrated using an online evaluation with incremental temporal training/test splits, in comparison with a strong baseline that includes predictors for initial citation counts, topics, and lexical features.</abstract>
      <url hash="6f7af1ba">2022.findings-emnlp.418</url>
      <bibkey>soni-etal-2022-predicting</bibkey>
      <video href="2022.findings-emnlp.418.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.418</doi>
    </paper>
    <paper id="419">
      <title>Joint Audio/Text Training for Transformer Rescorer of Streaming Speech Recognition</title>
      <author><first>Suyoun</first><last>Kim</last></author>
      <author><first>Ke</first><last>Li</last></author>
      <author><first>Lucas</first><last>Kabela</last></author>
      <author><first>Ron</first><last>Huang</last></author>
      <author><first>Jiedan</first><last>Zhu</last></author>
      <author><first>Ozlem</first><last>Kalinli</last></author>
      <author><first>Duc</first><last>Le</last></author>
      <pages>5717-5722</pages>
      <abstract>Recently, there has been an increasing interest in two-pass streaming end-to-end speech recognition (ASR) that incorporates a 2nd-pass rescoring model on top of the conventional 1st-pass streaming ASR model to improve recognition accuracy while keeping latency low. One of the latest 2nd-pass rescoring model, Transformer Rescorer, takes the n-best initial outputs and audio embeddings from the 1st-pass model, and then choose the best output by re-scoring the n-best initial outputs. However, training this Transformer Rescorer requires expensive paired audio-text training data because the model uses audio embeddings as input. In this work, we present our Joint Audio/Text training method for Transformer Rescorer, to leverage unpaired text-only data which is relatively cheaper than paired audio-text data. We evaluate Transformer Rescorer with our Joint Audio/Text training on Librispeech dataset as well as our large-scale in-house dataset and show that our training method can improve word error rate (WER) significantly compared to standard Transformer Rescorer without requiring any extra model parameters or latency.</abstract>
      <url hash="ae253e1d">2022.findings-emnlp.419</url>
      <bibkey>kim-etal-2022-joint</bibkey>
      <video href="2022.findings-emnlp.419.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.419</doi>
    </paper>
    <paper id="420">
      <title><fixed-case>T</fixed-case>y<fixed-case>D</fixed-case>i<fixed-case>P</fixed-case>: A Dataset for Politeness Classification in Nine Typologically Diverse Languages</title>
      <author><first>Anirudh</first><last>Srinivasan</last></author>
      <author><first>Eunsol</first><last>Choi</last></author>
      <pages>5723-5738</pages>
      <abstract>We study politeness phenomena in nine typologically diverse languages. Politeness is an important facet of communication and is sometimes argued to be cultural-specific, yet existing computational linguistic study is limited to English. We create TyDiP, a dataset containing three-way politeness annotations for 500 examples in each language, totaling 4.5K examples. We evaluate how well multilingual models can identify politeness levels – they show a fairly robust zero-shot transfer ability, yet fall short of estimated human accuracy significantly. We further study mapping the English politeness strategy lexicon into nine languages via automatic translation and lexicon induction, analyzing whether each strategy’s impact stays consistent across languages. Lastly, we empirically study the complicated relationship between formality and politeness through transfer experiments. We hope our dataset will support various research questions and applications, from evaluating multilingual models to constructing polite multilingual agents.</abstract>
      <url hash="821fb452">2022.findings-emnlp.420</url>
      <bibkey>srinivasan-choi-2022-tydip</bibkey>
      <video href="2022.findings-emnlp.420.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.420</doi>
    </paper>
    <paper id="421">
      <title>Probing Cross-modal Semantics Alignment Capability from the Textual Perspective</title>
      <author><first>Zheng</first><last>Ma</last></author>
      <author><first>Shi</first><last>Zong</last></author>
      <author><first>Mianzhi</first><last>Pan</last></author>
      <author><first>Jianbing</first><last>Zhang</last></author>
      <author><first>Shujian</first><last>Huang</last></author>
      <author><first>Xinyu</first><last>Dai</last></author>
      <author><first>Jiajun</first><last>Chen</last></author>
      <pages>5739-5749</pages>
      <abstract>In recent years, vision and language pre-training (VLP) models have advanced the state-of-the-art results in a variety of cross-modal downstream tasks. Aligning cross-modal semantics is claimed to be one of the essential capabilities of VLP models. However, it still remains unclear about the inner working mechanism of alignment in VLP models. In this paper, we propose a new probing method that is based on image captioning to first empirically study the cross-modal semantics alignment of VLP models. Our probing method is built upon the fact that given an image-caption pair, the VLP models will give a score, indicating how well two modalities are aligned; maximizing such scores will generate sentences that VLP models believe are of good alignment. Analyzing these sentences thus will reveal in what way different modalities are aligned and how well these alignments are in VLP models. We apply our probing method to five popular VLP models, including UNITER, ROSITA, ViLBERT, CLIP, and LXMERT, and provide a comprehensive analysis of the generated captions guided by these models. Our results show that VLP models (1) focus more on just aligning objects with visual words, while neglecting global semantics; (2) prefer fixed sentence patterns, thus ignoring more important textual information including fluency and grammar; and (3) deem the captions with more visual words are better aligned with images. These findings indicate that VLP models still have weaknesses in cross-modal semantics alignment and we hope this work will draw researchers’ attention to such problems when designing a new VLP model.</abstract>
      <url hash="724b1c3a">2022.findings-emnlp.421</url>
      <bibkey>ma-etal-2022-probing</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.421</doi>
    </paper>
    <paper id="422">
      <title>Hidden State Variability of Pretrained Language Models Can Guide Computation Reduction for Transfer Learning</title>
      <author><first>Shuo</first><last>Xie</last></author>
      <author><first>Jiahao</first><last>Qiu</last></author>
      <author><first>Ankita</first><last>Pasad</last></author>
      <author><first>Li</first><last>Du</last></author>
      <author><first>Qing</first><last>Qu</last></author>
      <author><first>Hongyuan</first><last>Mei</last></author>
      <pages>5750-5768</pages>
      <abstract>While transferring a pretrained language model, common approaches conventionally attach their task-specific classifiers to the top layer and adapt all the pretrained layers. We investigate whether one could make a task-specific selection on which subset of the layers to adapt and where to place the classifier. The goal is to reduce the computation cost of transfer learning methods (e.g. fine-tuning or adapter-tuning) without sacrificing its performance. We propose to select layers based on the variability of their hidden states given a task-specific corpus. We say a layer is already “well-specialized” in a task if the within-class variability of its hidden states is low relative to the between-class variability. Our variability metric is cheap to compute and doesn’t need any training or hyperparameter tuning. It is robust to data imbalance and data scarcity. Extensive experiments on the GLUE benchmark demonstrate that selecting layers based on our metric can yield significantly stronger performance than using the same number of top layers and often match the performance of fine-tuning or adapter-tuning the entire language model.</abstract>
      <url hash="e750f5de">2022.findings-emnlp.422</url>
      <bibkey>xie-etal-2022-hidden</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.422</doi>
    </paper>
    <paper id="423">
      <title>Language Models as Agent Models</title>
      <author><first>Jacob</first><last>Andreas</last></author>
      <pages>5769-5779</pages>
      <abstract>Language models (LMs) are trained on collections of documents, written by individual human agents to achieve specific goals in the outside world. During training, LMs have access only to text of these documents, with no direct evidence of the internal states of the agents that produced them—a fact often used to argue that LMs are incapable of modeling goal-directed aspects of human language production and comprehension. Can LMs trained on text learn anything at all about the relationship between language and use? I argue that LMs are models of communicative intentions in a specific, narrow sense. When performing next word prediction given a textual context, an LM can infer and represent properties of an agent likely to have produced that context. These representations can in turn influence subsequent LM generation in the same way that agents’ communicative intentions influence their language. I survey findings from the recent literature showing that—even in today’s non-robust and error-prone models—LMs infer and use representations of fine-grained communicative intentions and high-level beliefs and goals. Despite the limited nature of their training data, they can thus serve as building blocks for systems that communicate and act intentionally.</abstract>
      <url hash="7c79aaef">2022.findings-emnlp.423</url>
      <bibkey>andreas-2022-language</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.423</doi>
    </paper>
    <paper id="424">
      <title>Combinatory Grammar Tells Underlying Relevance among Entities</title>
      <author><first>Yuanhe</first><last>Tian</last></author>
      <author><first>Yan</first><last>Song</last></author>
      <pages>5780-5786</pages>
      <abstract>Relation extraction (RE) is an important task in natural language processing which aims to annotate the relation between two given entities, which requires a deep understanding of the running text. To import model performance, existing approaches leverage syntactic information to facilitate the relation extraction process, where they mainly focus on dependencies among words while paying limited attention to other types of syntactic structure. Considering that combinatory categorial grammar (CCG) is a lexicalized grammatical formalism that carries the syntactic and semantic knowledge for text understanding, we propose an alternative solution for RE that takes advantage of CCG to detect the relation between entities. In doing so, we perform a multi-task learning process to learn from RE and auto-annotated CCG supertags, where an attention mechanism is performed over all input words to distinguish the important ones for RE with the attention weights guided by the supertag decoding process. We evaluate our model on two widely used English benchmark datasets (i.e., ACE2005EN and SemEval 2010 Task 8 datasets) for RE, where the effectiveness of our approach is demonstrated by the experimental results with our approach achieving state-of-the-art performance on both datasets.</abstract>
      <url hash="8b98b0ec">2022.findings-emnlp.424</url>
      <bibkey>tian-song-2022-combinatory</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.424</doi>
    </paper>
    <paper id="425">
      <title>Leveraging Open Data and Task Augmentation to Automated Behavioral Coding of Psychotherapy Conversations in Low-Resource Scenarios</title>
      <author><first>Zhuohao</first><last>Chen</last></author>
      <author><first>Nikolaos</first><last>Flemotomos</last></author>
      <author><first>Zac</first><last>Imel</last></author>
      <author><first>David</first><last>Atkins</last></author>
      <author><first>Shrikanth</first><last>Narayanan</last></author>
      <pages>5787-5795</pages>
      <abstract>In psychotherapy interactions, the quality of a session is assessed by codifying the communicative behaviors of participants during the conversation through manual observation and annotation. Developing computational approaches for automated behavioral coding can reduce the burden on human coders and facilitate the objective evaluation of the intervention. In the real world, however, implementing such algorithms is associated with data sparsity challenges since privacy concerns lead to limited available in-domain data. In this paper, we leverage a publicly available conversation-based dataset and transfer knowledge to the low-resource behavioral coding task by performing an intermediate language model training via meta-learning. We introduce a task augmentation method to produce a large number of “analogy tasks” — tasks similar to the target one — and demonstrate that the proposed framework predicts target behaviors more accurately than all the other baseline models.</abstract>
      <url hash="00acf5d9">2022.findings-emnlp.425</url>
      <bibkey>chen-etal-2022-leveraging-open</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.425</doi>
    </paper>
    <paper id="426">
      <title>Learning to Detect Noisy Labels Using Model-Based Features</title>
      <author><first>Zhihao</first><last>Wang</last></author>
      <author><first>Zongyu</first><last>Lin</last></author>
      <author><first>Junjie</first><last>Wen</last></author>
      <author><first>Xianxin</first><last>Chen</last></author>
      <author><first>Peiqi</first><last>Liu</last></author>
      <author><first>Guidong</first><last>Zheng</last></author>
      <author><first>Yujun</first><last>Chen</last></author>
      <author><first>Zhilin</first><last>Yang</last></author>
      <pages>5796-5808</pages>
      <abstract>Label noise is ubiquitous in various machine learning scenarios such as self-labeling with model predictions and erroneous data annotation. Many existing approaches are based on heuristics such as sample losses, which might not be flexible enough to achieve optimal solutions. Meta learning based methods address this issue by learning a data selection function, but can be hard to optimize. In light of these pros and cons, we propose SENT (Selection-Enhanced Noisy label Training) that does not rely on meta learning while having the flexibility of being data-driven. SENT transfers the noise distribution to a clean set and trains a model to distinguish noisy labels from clean ones using model-based features. Empirically, on a wide range of tasks including text classification and speech recognition, SENT improves performance over strong baselines under the settings of self-training and label corruption.</abstract>
      <url hash="17c2f639">2022.findings-emnlp.426</url>
      <bibkey>wang-etal-2022-learning-detect</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.426</doi>
    </paper>
    <paper id="427">
      <title>Keyphrase Generation Beyond the Boundaries of Title and Abstract</title>
      <author><first>Krishna</first><last>Garg</last></author>
      <author><first>Jishnu</first><last>Ray Chowdhury</last></author>
      <author><first>Cornelia</first><last>Caragea</last></author>
      <pages>5809-5821</pages>
      <abstract>Keyphrase generation aims at generating important phrases (keyphrases) that best describe a given document. In scholarly domains, current approaches have largely used only the title and abstract of the articles to generate keyphrases. In this paper, we comprehensively explore whether the integration of additional information from the full text of a given article or from semantically similar articles can be helpful for a neural keyphrase generation model or not. We discover that adding sentences from the full text, particularly in the form of the extractive summary of the article can significantly improve the generation of both types of keyphrases that are either present or absent from the text. Experimental results with three widely used models for keyphrase generation along with one of the latest transformer models suitable for longer documents, Longformer Encoder-Decoder (LED) validate the observation. We also present a new large-scale scholarly dataset FullTextKP for keyphrase generation. Unlike prior large-scale datasets, FullTextKP includes the full text of the articles along with the title and abstract. We release the source code at <url>https://github.com/kgarg8/FullTextKP</url>.</abstract>
      <url hash="fc090def">2022.findings-emnlp.427</url>
      <bibkey>garg-etal-2022-keyphrase</bibkey>
      <video href="2022.findings-emnlp.427.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.427</doi>
    </paper>
    <paper id="428">
      <title>Composition, Attention, or Both?</title>
      <author><first>Ryo</first><last>Yoshida</last></author>
      <author><first>Yohei</first><last>Oseki</last></author>
      <pages>5822-5834</pages>
      <abstract>In this paper, we propose a novel architecture called Composition Attention Grammars (CAGs) that recursively compose subtrees into a single vector representation with a composition function, and selectively attend to previous structural information with a self-attention mechanism. We investigate whether these components—the composition function and the self-attention mechanism—can both induce human-like syntactic generalization. Specifically, we train language models (LMs) with and without these two components with the model sizes carefully controlled, and evaluate their syntactic generalization performance against six test circuits on the SyntaxGym benchmark. The results demonstrated that the composition function and the self-attention mechanism both play an important role to make LMs more human-like, and closer inspection of linguistic phenomenon implied that the composition function allowed syntactic features, but not semantic features, to percolate into subtree representations.</abstract>
      <url hash="1c472a50">2022.findings-emnlp.428</url>
      <bibkey>yoshida-oseki-2022-composition</bibkey>
      <revision id="1" href="2022.findings-emnlp.428v1" hash="59c77ee2"/>
      <revision id="2" href="2022.findings-emnlp.428v2" hash="1c472a50" date="2023-05-14">Updated a footnote.</revision>
      <video href="2022.findings-emnlp.428.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.428</doi>
    </paper>
    <paper id="429">
      <title><fixed-case>CDGP</fixed-case>: Automatic Cloze Distractor Generation based on Pre-trained Language Model</title>
      <author><first>Shang-Hsuan</first><last>Chiang</last></author>
      <author><first>Ssu-Cheng</first><last>Wang</last></author>
      <author><first>Yao-Chung</first><last>Fan</last></author>
      <pages>5835-5840</pages>
      <abstract>Manually designing cloze test consumes enormous time and efforts. The major challenge lies in wrong option (distractor) selection. Having carefully-design distractors improves the effectiveness of learner ability assessment. As a result, the idea of automatically generating cloze distractor is motivated. In this paper, we investigate cloze distractor generation by exploring the employment of pre-trained language models (PLMs) as an alternative for candidate distractor generation. Experiments show that the PLM-enhanced model brings a substantial performance improvement. Our best performing model advances the state-of-the-art result from 14.94 to 34.17 (NDCG@10 score). Our code and dataset is available at <url>https://github.com/AndyChiangSH/CDGP</url>.</abstract>
      <url hash="c482771f">2022.findings-emnlp.429</url>
      <bibkey>chiang-etal-2022-cdgp</bibkey>
      <video href="2022.findings-emnlp.429.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.429</doi>
    </paper>
    <paper id="430">
      <title>G3: Geolocation via Guidebook Grounding</title>
      <author><first>Grace</first><last>Luo</last></author>
      <author><first>Giscard</first><last>Biamby</last></author>
      <author><first>Trevor</first><last>Darrell</last></author>
      <author><first>Daniel</first><last>Fried</last></author>
      <author><first>Anna</first><last>Rohrbach</last></author>
      <pages>5841-5853</pages>
      <abstract>We demonstrate how language can improve geolocation: the task of predicting the location where an image was taken. Here we study explicit knowledge from human-written guidebooks that describe the salient and class-discriminative visual features humans use for geolocation. We propose the task of Geolocation via Guidebook Grounding that uses a dataset of StreetView images from a diverse set of locations and an associated textual guidebook for GeoGuessr, a popular interactive geolocation game. Our approach predicts a country for each image by attending over the clues automatically extracted from the guidebook. Supervising attention with country-level pseudo labels achieves the best performance. Our approach substantially outperforms a state-of-the-art image-only geolocation method, with an improvement of over 5% in Top-1 accuracy. Our dataset and code can be found at <url>https://github.com/g-luo/geolocation_via_guidebook_grounding</url>.</abstract>
      <url hash="75952d9d">2022.findings-emnlp.430</url>
      <bibkey>luo-etal-2022-g3</bibkey>
      <video href="2022.findings-emnlp.430.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.430</doi>
    </paper>
    <paper id="431">
      <title>Controlling Bias Exposure for Fair Interpretable Predictions</title>
      <author><first>Zexue</first><last>He</last></author>
      <author><first>Yu</first><last>Wang</last></author>
      <author><first>Julian</first><last>McAuley</last></author>
      <author><first>Bodhisattwa Prasad</first><last>Majumder</last></author>
      <pages>5854-5866</pages>
      <abstract>Recent work on reducing bias in NLP models usually focuses on protecting or isolating information related to a sensitive attribute (like gender or race). However, when sensitive information is semantically entangled with the task information of the input, e.g., gender information is predictive for a profession, a fair trade-off between task performance and bias mitigation is difficult to achieve. Existing approaches perform this trade-off by eliminating bias information from the latent space, lacking control over how much bias is necessarily required to be removed. We argue that a favorable debiasing method should use sensitive information ‘fairly’, rather than blindly eliminating it (Caliskan et al., 2017; Sun et al., 2019; Bogen et al., 2020). In this work, we provide a novel debiasing algorithm by adjustingthe predictive model’s belief to (1) ignore the sensitive information if it is not useful for the task; (2) use sensitive information minimally as necessary for the prediction (while also incurring a penalty). Experimental results on two text classification tasks (influenced by gender) and an open-ended generation task (influenced by race) indicate that our model achieves a desirable trade-off between debiasing and task performance along with producing debiased rationales as evidence.</abstract>
      <url hash="ef5f785e">2022.findings-emnlp.431</url>
      <bibkey>he-etal-2022-controlling</bibkey>
      <video href="2022.findings-emnlp.431.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.431</doi>
    </paper>
    <paper id="432">
      <title>Investigating the Benefits of Free-Form Rationales</title>
      <author><first>Jiao</first><last>Sun</last></author>
      <author><first>Swabha</first><last>Swayamdipta</last></author>
      <author><first>Jonathan</first><last>May</last></author>
      <author><first>Xuezhe</first><last>Ma</last></author>
      <pages>5867-5882</pages>
      <abstract>Free-form rationales aim to aid model interpretability by supplying the background knowledge that can help understand model decisions. Crowdsourced rationales are provided for commonsense QA instances in popular datasets such as CoS-E and ECQA, but their utility remains under-investigated. We present human studies which show that ECQA rationales indeed provide additional background information to understand a decision, while over 88% of CoS-E rationales do not. Inspired by this finding, we ask: can the additional context provided by free-form rationales benefit models, similar to human users? We investigate the utility of rationales as an additional source of supervision, by varying the quantity and quality of rationales during training. After controlling for instances where rationales leak the correct answer while not providing additional background knowledge, we find that incorporating only 5% of rationales during training can boost model performance by 47.22% for CoS-E and 57.14% for ECQA during inference. Moreover, we also show that rationale quality matters: compared to crowdsourced rationales, T5-generated rationales provide not only weaker supervision to models, but are also not helpful for humans in aiding model interpretability.</abstract>
      <url hash="0776156f">2022.findings-emnlp.432</url>
      <bibkey>sun-etal-2022-investigating</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.432</doi>
    </paper>
    <paper id="433">
      <title>Data-Efficient Concept Extraction from Pre-trained Language Models for Commonsense Explanation Generation</title>
      <author><first>Yanbo</first><last>Fang</last></author>
      <author><first>Yongfeng</first><last>Zhang</last></author>
      <pages>5883-5893</pages>
      <abstract>Predicting the key explanation concept is essential for generating commonsense explanations. This paper introduces a method to predict the concept from pre-trained language models for commonsense explanation generation. Our experiment found that adopting a language model as the concept extractor and fine-tuning it with 20% training data can improve the quality and accuracy of the generated explanations over multiple evaluation metrics. Compared with conventional methods that search concepts over knowledge graphs, our method does not require the preparation and training models to search through knowledge graphs. To better understand the results from pre-trained language models, we also designed a metric to evaluate the retrieved concepts. Through analysis and experiments, we show the correlation between this metric and the performance of the generators, and we also show the importance of attaching concepts for generating high-quality sentences.</abstract>
      <url hash="95c91bab">2022.findings-emnlp.433</url>
      <bibkey>fang-zhang-2022-data</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.433</doi>
    </paper>
    <paper id="434">
      <title>Unsupervised Domain Adaptation for Joint Information Extraction</title>
      <author><first>Nghia</first><last>Ngo</last></author>
      <author><first>Bonan</first><last>Min</last></author>
      <author><first>Thien</first><last>Nguyen</last></author>
      <pages>5894-5905</pages>
      <abstract>Joint Information Extraction (JIE) aims to jointly solve multiple tasks in the Information Extraction pipeline (e.g., entity mention, event trigger, relation, and event argument extraction). Due to their ability to leverage task dependencies and avoid error propagation, JIE models have presented state-of-the-art performance for different IE tasks. However, an issue with current JIE methods is that they only focus on standard supervised learning setting where training and test data comes from the same domain. Cross-domain/domain adaptation learning with training and test data in different domains have not been explored for JIE, thus hindering the application of this technology to different domains in practice. To address this issue, our work introduces the first study to evaluate performance of JIE models in unsupervised domain adaptation setting. In addition, we present a novel method to induce domain-invariant representations for the tasks in JIE, called Domain Adaptation for Joint Information Extraction (DA4JIE). In DA4JIE, we propose an Instance-relational Domain Adaptation mechanism that seeks to align representations of task instances in JIE across domains through a generalized version of domain-adversarial learning approach. We further devise a Context-invariant Structure Learning technique to filter domain-specialized contextual information from induced representations to boost performance of JIE models in new domains. Extensive experiments and analyses demonstrate that DA4JIE can significantly improve out-of-domain performance for current state-of-the-art JIE systems for all IE tasks.</abstract>
      <url hash="8706d806">2022.findings-emnlp.434</url>
      <bibkey>ngo-etal-2022-unsupervised</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.434</doi>
    </paper>
    <paper id="435">
      <title>Foiling Training-Time Attacks on Neural Machine Translation Systems</title>
      <author><first>Jun</first><last>Wang</last></author>
      <author><first>Xuanli</first><last>He</last></author>
      <author><first>Benjamin</first><last>Rubinstein</last></author>
      <author><first>Trevor</first><last>Cohn</last></author>
      <pages>5906-5913</pages>
      <abstract>Neural machine translation (NMT) systems are vulnerable to backdoor attacks, whereby an attacker injects poisoned samples into training such that a trained model produces malicious translations. Nevertheless, there is little research on defending against such backdoor attacks in NMT. In this paper, we first show that backdoor attacks that have been successful in text classification are also effective against machine translation tasks. We then present a novel defence method that exploits a key property of most backdoor attacks: namely the asymmetry between the source and target language sentences, which is used to facilitate malicious text insertions, substitutions and suchlike. Our technique uses word alignment coupled with language model scoring to detect outlier tokens, and thus can find and filter out training instances which may contain backdoors. Experimental results demonstrate that our technique can significantly reduce the success of various attacks by up to 89.0%, while not affecting predictive accuracy.</abstract>
      <url hash="99d7f3dc">2022.findings-emnlp.435</url>
      <bibkey>wang-etal-2022-foiling</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.435</doi>
    </paper>
    <paper id="436">
      <title>Learning Action-Effect Dynamics for Hypothetical Vision-Language Reasoning Task</title>
      <author><first>Shailaja Keyur</first><last>Sampat</last></author>
      <author><first>Pratyay</first><last>Banerjee</last></author>
      <author><first>Yezhou</first><last>Yang</last></author>
      <author><first>Chitta</first><last>Baral</last></author>
      <pages>5914-5924</pages>
      <abstract>‘Actions’ play a vital role in how humans interact with the world. Thus, autonomous agents that would assist us in everyday tasks also require the capability to perform ‘Reasoning about Actions &amp; Change’ (RAC). This has been an important research direction in Artificial Intelligence (AI) in general, but the study of RAC with visual and linguistic inputs is relatively recent. The CLEVR_HYP (Sampat et. al., 2021) is one such testbed for hypothetical vision-language reasoning with actions as the key focus. In this work, we propose a novel learning strategy that can improve reasoning about the effects of actions. We implement an encoder-decoder architecture to learn the representation of actions as vectors. We combine the aforementioned encoder-decoder architecture with existing modality parsers and a scene graph question answering model to evaluate our proposed system on the CLEVR_HYP dataset. We conduct thorough experiments to demonstrate the effectiveness of our proposed approach and discuss its advantages over previous baselines in terms of performance, data efficiency, and generalization capability.</abstract>
      <url hash="dfe9706c">2022.findings-emnlp.436</url>
      <bibkey>sampat-etal-2022-learning</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.436</doi>
    </paper>
    <paper id="437">
      <title>Named Entity and Relation Extraction with Multi-Modal Retrieval</title>
      <author><first>Xinyu</first><last>Wang</last></author>
      <author><first>Jiong</first><last>Cai</last></author>
      <author><first>Yong</first><last>Jiang</last></author>
      <author><first>Pengjun</first><last>Xie</last></author>
      <author><first>Kewei</first><last>Tu</last></author>
      <author><first>Wei</first><last>Lu</last></author>
      <pages>5925-5936</pages>
      <abstract>Multi-modal named entity recognition (NER) and relation extraction (RE) aim to leverage relevant image information to improve the performance of NER and RE. Most existing efforts largely focused on directly extracting potentially useful information from images (such as pixel-level features, identified objects, and associated captions).However, such extraction processes may not be knowledge aware, resulting in information that may not be highly relevant. In this paper, we propose a novel Multi-modal Retrieval based framework (MoRe).MoRe contains a text retrieval module and an image-based retrieval module, which retrieve related knowledge of the input text and image in the knowledge corpus respectively. Next, the retrieval results are sent to the textual and visual models respectively for predictions. Finally, a Mixture of Experts (MoE) module combines the predictions from the two models to make the final decision. Our experiments show that both our textual model and visual model can achieve state-of-the-art performance on four multi-modal NER datasets and one multi-modal RE dataset. With MoE, the model performance can be further improved and our analysis demonstrates the benefits of integrating both textual and visual cues for such tasks.</abstract>
      <url hash="a6c09ed8">2022.findings-emnlp.437</url>
      <bibkey>wang-etal-2022-named</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.437</doi>
    </paper>
    <paper id="438">
      <title>Calibrating Factual Knowledge in Pretrained Language Models</title>
      <author><first>Qingxiu</first><last>Dong</last></author>
      <author><first>Damai</first><last>Dai</last></author>
      <author><first>Yifan</first><last>Song</last></author>
      <author><first>Jingjing</first><last>Xu</last></author>
      <author><first>Zhifang</first><last>Sui</last></author>
      <author><first>Lei</first><last>Li</last></author>
      <pages>5937-5947</pages>
      <abstract>Previous literature has proved that Pretrained Language Models (PLMs) can store factual knowledge. However, we find that facts stored in the PLMs are not always correct. It motivates us to explore a fundamental question: How do we calibrate factual knowledge in PLMs without re-training from scratch? In this work, we propose a simple and lightweight method CaliNet to achieve this goal. To be specific, we first detect whether PLMs can learn the right facts via a contrastive score between right and fake facts. If not, we then use a lightweight method to add and adapt new parameters to specific factual texts. Experiments on the knowledge probing task show the calibration effectiveness and efficiency. In addition, through closed-book question answering, we find that the calibrated PLM possesses knowledge generalization ability after finetuning.Beyond the calibration performance, we further investigate and visualize the knowledge calibration mechanism.</abstract>
      <url hash="e7ebf751">2022.findings-emnlp.438</url>
      <bibkey>dong-etal-2022-calibrating</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.438</doi>
    </paper>
    <paper id="439">
      <title><fixed-case>MCPG</fixed-case>: A Flexible Multi-Level Controllable Framework for Unsupervised Paraphrase Generation</title>
      <author><first>Yi</first><last>Chen</last></author>
      <author><first>Haiyun</first><last>Jiang</last></author>
      <author><first>Lemao</first><last>Liu</last></author>
      <author><first>Rui</first><last>Wang</last></author>
      <author><first>Shuming</first><last>Shi</last></author>
      <author><first>Ruifeng</first><last>Xu</last></author>
      <pages>5948-5958</pages>
      <abstract>We present MCPG: a simple and effectiveapproach for controllable unsupervised paraphrase generation, which is also flexible toadapt to specific domains without extra training. MCPG is controllable in different levels: local lexicons, global semantics, and universal styles. The unsupervised paradigm ofMCPG combines factual keywords and diversified semantic embeddings as local lexical andglobal semantic constraints. The semantic embeddings are diversified by standard dropout,which is exploited for the first time to increaseinference diversity by us. Moreover, MCPGis qualified with good domain adaptability byadding a transfer vector as a universal style constraint, which is refined from the exemplars retrieved from the corpus of the target domain in atraining-free way. Extensive experiments showthat MCPG outperforms state-of-the-art unsupervised baselines by a margin. Meanwhile,our domain-adapted MCPG also achieves competitive performance with strong supervisedbaselines even without training.</abstract>
      <url hash="f7e07088">2022.findings-emnlp.439</url>
      <bibkey>chen-etal-2022-mcpg</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.439</doi>
      <revision id="1" href="2022.findings-emnlp.439v1" hash="d51cb912"/>
      <revision id="2" href="2022.findings-emnlp.439v2" hash="f7e07088" date="2023-10-21">Minor typo fix.</revision>
    </paper>
    <paper id="440">
      <title><fixed-case>W</fixed-case>ord<fixed-case>T</fixed-case>ies: Measuring Word Associations in Language Models via Constrained Sampling</title>
      <author><first>Peiran</first><last>Yao</last></author>
      <author><first>Tobias</first><last>Renwick</last></author>
      <author><first>Denilson</first><last>Barbosa</last></author>
      <pages>5959-5970</pages>
      <abstract>Word associations are widely used in psychology to provide insights on how humans perceive and understand concepts. Comparing word associations in language models (LMs) to those generated by human subjects can serve as a proxy to uncover embedded lexical and commonsense knowledge in language models. While much helpful work has been done applying direct metrics, such as cosine similarity, to help understand latent spaces, these metrics are symmetric, while human word associativity is asymmetric. We propose WordTies, an algorithm based on constrained sampling from LMs, which allows an asymmetric measurement of associated words, given a cue word as the input. Comparing to existing methods, word associations found by this method share more overlap with associations provided by humans, and observe the asymmetric property of human associations. To examine possible reasons behind associations, we analyze the knowledge and reasoning behind the word pairings as they are linked to lexical and commonsense knowledge graphs. When the knowledge about the nature of the word pairings is combined with a probability that the LM has learned that information, we have a new way to examine what information is captured in LMs.</abstract>
      <url hash="ea638fae">2022.findings-emnlp.440</url>
      <bibkey>yao-etal-2022-wordties</bibkey>
      <video href="2022.findings-emnlp.440.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.440</doi>
    </paper>
    <paper id="441">
      <title>Exploring The Landscape of Distributional Robustness for Question Answering Models</title>
      <author><first>Anas</first><last>Awadalla</last></author>
      <author><first>Mitchell</first><last>Wortsman</last></author>
      <author><first>Gabriel</first><last>Ilharco</last></author>
      <author><first>Sewon</first><last>Min</last></author>
      <author><first>Ian</first><last>Magnusson</last></author>
      <author><first>Hannaneh</first><last>Hajishirzi</last></author>
      <author><first>Ludwig</first><last>Schmidt</last></author>
      <pages>5971-5987</pages>
      <abstract>We conduct a large empirical evaluation to investigate the landscape of distributional robustness in question answering. Our investigation spans over 350 models and 16 question answering datasets, including a diverse set of architectures, model sizes, and adaptation methods (e.g., fine-tuning, adapter tuning, in-context learning, etc.). We find that, in many cases, model variations do not affect robustness and in-distribution performance alone determines out-of-distribution performance. Moreover, our findings indicate thati) zero-shot and in-context learning methods are more robust to distribution shifts than fully fine-tuned models;ii) few-shot prompt fine-tuned models exhibit better robustness than few-shot fine-tuned span prediction models;iii) parameter-efficient and robustness enhancing training methods provide no significant robustness improvements. In addition, we publicly release all evaluations to encourage researchers to further analyze robustness trends for question answering models.</abstract>
      <url hash="562398ed">2022.findings-emnlp.441</url>
      <bibkey>awadalla-etal-2022-exploring</bibkey>
      <video href="2022.findings-emnlp.441.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.441</doi>
    </paper>
    <paper id="442">
      <title>Collaborative Reasoning on Multi-Modal Semantic Graphs for Video-Grounded Dialogue Generation</title>
      <author><first>Xueliang</first><last>Zhao</last></author>
      <author><first>Yuxuan</first><last>Wang</last></author>
      <author><first>Chongyang</first><last>Tao</last></author>
      <author><first>Chenshuo</first><last>Wang</last></author>
      <author><first>Dongyan</first><last>Zhao</last></author>
      <pages>5988-5998</pages>
      <abstract>We study video-grounded dialogue generation, where a response is generated based on the dialogue context and the associated video. The primary challenges of this task lie in (1) the difficulty of integrating video data into pre-trained language models (PLMs) which presents obstacles to exploiting the power of large-scale pre-training; and (2) the necessity of taking into account the complementarity of various modalities throughout the reasoning process. Although having made remarkable progress in video-grounded dialogue generation, existing methods still fall short when it comes to integrating with PLMs in a way that allows information from different modalities to complement each other. To alleviate these issues, we first propose extracting pertinent information from videos and turning it into reasoning paths that are acceptable to PLMs. Additionally, we propose a multi-agent reinforcement learning method to collaboratively perform reasoning on different modalities (i.e., video and dialogue context). Empirical experiment results on two public datasets indicate that the proposed model can significantly outperform state-of-the-art models by large margins on both automatic and human evaluations.</abstract>
      <url hash="cf3bf11b">2022.findings-emnlp.442</url>
      <bibkey>zhao-etal-2022-collaborative</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.442</doi>
    </paper>
    <paper id="443">
      <title>Partitioned Gradient Matching-based Data Subset Selection for Compute-Efficient Robust <fixed-case>ASR</fixed-case> Training</title>
      <author><first>Ashish</first><last>Mittal</last></author>
      <author><first>Durga</first><last>Sivasubramanian</last></author>
      <author><first>Rishabh</first><last>Iyer</last></author>
      <author><first>Preethi</first><last>Jyothi</last></author>
      <author><first>Ganesh</first><last>Ramakrishnan</last></author>
      <pages>5999-6010</pages>
      <abstract>Training state-of-the-art ASR systems such as RNN-T often has a high associated financial and environmental cost. Training with a subset of training data could mitigate this problem if the subset selected could achieve on-par performance with training with the entire dataset. Although there are many data subset selection(DSS) algorithms, direct application to the RNN-T is difficult, especially the DSS algorithms that are adaptive and use learning dynamics such as gradients, as RNN-T tend to have gradients with a significantly larger memory footprint. In this paper, we propose Partitioned Gradient Matching (PGM) a novel distributable DSS algorithm, suitable for massive datasets like those used to train RNN-T. Through extensive experiments on Librispeech 100H and Librispeech 960H, we show that PGM achieves between 3x to 6x speedup with only a very small accuracy degradation (under 1% absolute WER difference). In addition, we demonstrate similar results for PGM even in settings where the training data is corrupted with noise.</abstract>
      <url hash="0f6c4f5c">2022.findings-emnlp.443</url>
      <bibkey>mittal-etal-2022-partitioned</bibkey>
      <video href="2022.findings-emnlp.443.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.443</doi>
    </paper>
    <paper id="444">
      <title>Adaptive Graph Convolutional Network for Knowledge Graph Entity Alignment</title>
      <author><first>Renbo</first><last>Zhu</last></author>
      <author><first>Xukun</first><last>Luo</last></author>
      <author><first>Meng</first><last>Ma</last></author>
      <author><first>Ping</first><last>Wang</last></author>
      <pages>6011-6021</pages>
      <abstract>Entity alignment (EA) aims to identify equivalent entities from different Knowledge Graphs (KGs), which is a fundamental task for integrating KGs. Throughout its development, Graph Convolutional Network (GCN) has become one of the mainstream methods for EA. These GCN-based methods learn the representations of entities from two KGs by message passing mechanism and then make alignments via measuring the similarity between entity embeddings. The key idea that GCN works in EA is that entities with similar neighbor structures are highly likely to be aligned. However, the noisy neighbors of entities transfer invalid information, drown out equivalent information, lead to inaccurate entity embeddings, and finally reduce the performance of EA. Based on the Sinkhorn algorithm, we design a reliability measure for potential equivalent entities and propose Adaptive Graph Convolutional Network to deal with neighbor noises in GCN. During the training, the network dynamically updates the adaptive weights of relation triples to weaken the propagation of noises. While calculating entity similarity, it comprehensively considers the self-similarity and neighborhood similarity of the entity pair to alleviate the influence of noises. Furthermore, we design a straightforward but efficient strategy to construct pseudo alignments for unsupervised EA. Extensive experiments on benchmark datasets demonstrate that our framework outperforms the state-of-the-art methods in both supervised and unsupervised settings.</abstract>
      <url hash="b81f4b5a">2022.findings-emnlp.444</url>
      <bibkey>zhu-etal-2022-adaptive</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.444</doi>
    </paper>
    <paper id="445">
      <title>Towards Robust <fixed-case>NLG</fixed-case> Bias Evaluation with Syntactically-diverse Prompts</title>
      <author><first>Arshiya</first><last>Aggarwal</last></author>
      <author><first>Jiao</first><last>Sun</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <pages>6022-6032</pages>
      <abstract>We present a robust methodology for evaluating biases in natural language generation(NLG) systems. Previous works use fixed hand-crafted prefix templates with mentions of various demographic groups to prompt models to generate continuations for bias analysis. These fixed prefix templates could themselves be specific in terms of styles or linguistic structures, which may lead to unreliable fairness conclusions that are not representative of the general trends from tone varying prompts. To study this problem, we paraphrase the prompts with different syntactic structures and use these to evaluate demographic bias in NLG systems. Our results suggest similar overall bias trends but some syntactic structures lead to contradictory conclusions compared to past works. We show that our methodology is more robust and that some syntactic structures prompt more toxic content while others could prompt less biased generation. This suggests the importance of not relying on a fixed syntactic structure and using tone-invariant prompts. Introducing syntactically-diverse prompts can achieve more robust NLG (bias) evaluation.</abstract>
      <url hash="c92da588">2022.findings-emnlp.445</url>
      <bibkey>aggarwal-etal-2022-towards</bibkey>
      <video href="2022.findings-emnlp.445.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.445</doi>
    </paper>
    <paper id="446">
      <title><fixed-case>P</fixed-case>c<fixed-case>MSP</fixed-case>: A Dataset for Scientific Action Graphs Extraction from Polycrystalline Materials Synthesis Procedure Text</title>
      <author><first>Xianjun</first><last>Yang</last></author>
      <author><first>Ya</first><last>Zhuo</last></author>
      <author><first>Julia</first><last>Zuo</last></author>
      <author><first>Xinlu</first><last>Zhang</last></author>
      <author><first>Stephen</first><last>Wilson</last></author>
      <author><first>Linda</first><last>Petzold</last></author>
      <pages>6033-6046</pages>
      <abstract>Scientific action graphs extraction from materials synthesis procedures is important for reproducible research, machine automation, and material prediction. But the lack of annotated data has hindered progress in this field. We demonstrate an effort to annotate Polycrystalline Materials Synthesis Procedures PcMSP from 305 open access scientific articles for the construction of synthesis action graphs. This is a new dataset for material science information extraction that simultaneously contains the synthesis sentences extracted from the experimental paragraphs, as well as the entity mentions and intra-sentence relations. A two-step human annotation and inter-annotator agreement study guarantee the high quality of the PcMSP corpus. We introduce four natural language processing tasks: sentence classification, named entity recognition, relation classification, and joint extraction of entities and relations. Comprehensive experiments validate the effectiveness of several state-of-the-art models for these challenges while leaving large space for improvement. We also perform the error analysis and point out some unique challenges that require further investigation. We will release our annotation scheme, the corpus, and codes to the research community to alleviate the scarcity of labeled data in this domain.</abstract>
      <url hash="972c5cb2">2022.findings-emnlp.446</url>
      <bibkey>yang-etal-2022-pcmsp</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.446</doi>
    </paper>
    <paper id="447">
      <title>Validity Assessment of Legal Will Statements as Natural Language Inference</title>
      <author><first>Alice</first><last>Kwak</last></author>
      <author><first>Jacob</first><last>Israelsen</last></author>
      <author><first>Clayton</first><last>Morrison</last></author>
      <author><first>Derek</first><last>Bambauer</last></author>
      <author><first>Mihai</first><last>Surdeanu</last></author>
      <pages>6047-6056</pages>
      <abstract>This work introduces a natural language inference (NLI) dataset that focuses on the validity of statements in legal wills. This dataset is unique because: (a) each entailment decision requires three inputs: the statement from the will, the law, and the conditions that hold at the time of the testator’s death; and (b) the included texts are longer than the ones in current NLI datasets. We trained eight neural NLI models in this dataset. All the models achieve more than 80% macro F1 and accuracy, which indicates that neural approaches can handle this task reasonably well. However, group accuracy, a stricter evaluation measure that is calculated with a group of positive and negative examples generated from the same statement as a unit, is in mid 80s at best, which suggests that the models’ understanding of the task remains superficial. Further ablative analyses and explanation experiments indicate that all three text segments are used for prediction, but some decisions rely on semantically irrelevant tokens. This indicates that overfitting on these longer texts likely happens, and that additional research is required for this task to be solved.</abstract>
      <url hash="26248710">2022.findings-emnlp.447</url>
      <bibkey>kwak-etal-2022-validity</bibkey>
      <video href="2022.findings-emnlp.447.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.447</doi>
    </paper>
    <paper id="448">
      <title><fixed-case>A</fixed-case>da<fixed-case>P</fixed-case>rompt: Adaptive Model Training for Prompt-based <fixed-case>NLP</fixed-case></title>
      <author><first>Yulong</first><last>Chen</last></author>
      <author id="yang-liu-microsoft"><first>Yang</first><last>Liu</last></author>
      <author><first>Li</first><last>Dong</last></author>
      <author><first>Shuohang</first><last>Wang</last></author>
      <author><first>Chenguang</first><last>Zhu</last></author>
      <author><first>Michael</first><last>Zeng</last></author>
      <author><first>Yue</first><last>Zhang</last></author>
      <pages>6057-6068</pages>
      <abstract>Prompt-based learning, with its capability to tackle zero-shot and few-shot NLP tasks, has gained much attention in the community. The main idea is to bridge the gap between NLP downstream tasks and language modeling (LM), by mapping these tasks into natural language prompts, which are then filled by pre-trained language models (PLMs).However, for prompt learning, there are still two salient gaps between NLP tasks and pretraining. First, prompt information is not necessarily sufficiently present during LM pre-training. Second, task-specific data are not necessarily well represented during pre-training. We address these two issues by proposing AdaPrompt, adaptively retrieving external data for continual pretraining of PLMs by making use of both task and prompt characteristics. In addition, we make use of knowledge in Natural Language Inference models for deriving adaptive verbalizers.Experimental results on five NLP benchmarks show that AdaPrompt can improve over standard PLMs in few-shot settings. In addition, in zero-shot settings, our method outperforms standard prompt-based methods by up to 26.35% relative error reduction.</abstract>
      <url hash="ae93fb10">2022.findings-emnlp.448</url>
      <bibkey>chen-etal-2022-adaprompt</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.448</doi>
    </paper>
    <paper id="449">
      <title>Code Generation From Flowcharts with Texts: A Benchmark Dataset and An Approach</title>
      <author><first>Zejie</first><last>Liu</last></author>
      <author><first>Xiaoyu</first><last>Hu</last></author>
      <author><first>Deyu</first><last>Zhou</last></author>
      <author><first>Lin</first><last>Li</last></author>
      <author><first>Xu</first><last>Zhang</last></author>
      <author><first>Yanzheng</first><last>Xiang</last></author>
      <pages>6069-6077</pages>
      <abstract>Currently, researchers focus on generating codes from the requirement documents. However, current approaches still perform poorly on some requirements needing complex problem-solving skills. In reality, to tackle such complex requirements, instead of directly translating requirement documents into codes, software engineers write codes via unified modeling language diagrams, such as flowcharts, an intermediate tool to analyze and visualize the system. Therefore, we propose a new source code generation task, that is, to generate source code from flowcharts with texts. We manually construct a benchmark dataset containing 320 flowcharts with their corresponding source codes. Obviously, it is not straightforward to employ the current approaches for the new source code generation task since (1) the flowchart is a graph that contains various structures, including loop, selection, and others which is different from texts; (2) the connections between nodes in the flowchart are abundant and diverse which need to be carefully handled. To solve the above problems, we propose a two-stage code generation model. In the first stage, a structure recognition algorithm is employed to transform the flowchart into pseudo-code containing the structural conventions of a typical programming language such as while, if. In the second stage, a code generation model is employed to convert the pseudo-code into code. Experimental results show that the proposed approach can achieve some improvement over the baselines.</abstract>
      <url hash="23c000da">2022.findings-emnlp.449</url>
      <bibkey>liu-etal-2022-code</bibkey>
      <video href="2022.findings-emnlp.449.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.449</doi>
    </paper>
    <paper id="450">
      <title>Focus! Relevant and Sufficient Context Selection for News Image Captioning</title>
      <author><first>Mingyang</first><last>Zhou</last></author>
      <author><first>Grace</first><last>Luo</last></author>
      <author><first>Anna</first><last>Rohrbach</last></author>
      <author><first>Zhou</first><last>Yu</last></author>
      <pages>6078-6088</pages>
      <abstract>News Image Captioning requires describing an image by leveraging additional context derived from a news article. Previous works only coarsely leverage the article to extract the necessary context, which makes it challenging for models to identify relevant events and named entities. In our paper, we first demonstrate that by combining more fine-grained context that captures the key named entities (obtained via an oracle) and the global context that summarizes the news, we can dramatically improve the model’s ability to generate accurate news captions. This begs the question, how to automatically extract such key entities from an image? We propose to use pre-trained vision and language retrieval model CLIP to localize the visually grounded entities in the news article, and then capture the non-visual entities via a open relation extraction model. Our experiments demonstrate that by simply selecting better context from the article, we can significantly improve the performance of existing models and achieve the new state-of-the-art performance on multiple benchmarks.</abstract>
      <url hash="40a9a977">2022.findings-emnlp.450</url>
      <bibkey>zhou-etal-2022-focus</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.450</doi>
    </paper>
    <paper id="451">
      <title>Generative Aspect-Based Sentiment Analysis with Contrastive Learning and Expressive Structure</title>
      <author><first>Joseph</first><last>Peper</last></author>
      <author><first>Lu</first><last>Wang</last></author>
      <pages>6089-6095</pages>
      <abstract>Generative models have demonstrated impressive results on Aspect-based Sentiment Analysis (ABSA) tasks, particularly for the emerging task of extracting Aspect-Category-Opinion-Sentiment (ACOS) quadruples. However, these models struggle with implicit sentiment expressions, which are commonly observed in opinionated content such as online reviews. In this work, we introduce GEN-SCL-NAT, which consists of two techniques for improved structured generation for ACOS quadruple extraction. First, we propose GEN-SCL, a supervised contrastive learning objective that aids quadruple prediction by encouraging the model to produce input representations that are discriminable across key input attributes, such as sentiment polarity and the existence of implicit opinions and aspects. Second, we introduce GEN-NAT, a new structured generation format that better adapts pre-trained autoregressive encoder-decoder models to extract quadruples in a generative fashion. Experimental results show that GEN-SCL-NAT achieves top performance across three ACOS datasets, averaging 1.48% F1 improvement, with a maximum 1.73% increase on the LAPTOP-L1 dataset. Additionally, we see significant gains on implicit aspect and opinion splits that have been shown as challenging for existing ACOS approaches.</abstract>
      <url hash="160aba0f">2022.findings-emnlp.451</url>
      <bibkey>peper-wang-2022-generative</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.451</doi>
    </paper>
    <paper id="452">
      <title>Semantic Dependency Parsing with Edge <fixed-case>GNN</fixed-case>s</title>
      <author><first>Songlin</first><last>Yang</last></author>
      <author><first>Kewei</first><last>Tu</last></author>
      <pages>6096-6102</pages>
      <abstract>Second-order neural parsers have obtained high accuracy in semantic dependency parsing. Inspired by the factor graph representation of second-order parsing, we propose edge graph neural networks (E-GNNs). In an E-GNN, each node corresponds to a dependency edge, and the neighbors are defined in terms of sibling, co-parent, and grandparent relationships. We conduct experiments on SemEval 2015 Task 18 English datasets, showing the superior performance of E-GNNs.</abstract>
      <url hash="091b4871">2022.findings-emnlp.452</url>
      <bibkey>yang-tu-2022-semantic</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.452</doi>
    </paper>
    <paper id="453">
      <title>Explore Unsupervised Structures in Pretrained Models for Relation Extraction</title>
      <author><first>Xi</first><last>Yang</last></author>
      <author><first>Tao</first><last>Ji</last></author>
      <author><first>Yuanbin</first><last>Wu</last></author>
      <pages>6103-6117</pages>
      <abstract>Syntactic trees have been widely applied in relation extraction (RE). However, since parsing qualities are not stable on different text domains and a pre-defined grammar may not well fit the target relation schema, the introduction of syntactic structures sometimes fails to improve RE performances consistently. In this work, we study RE models with various unsupervised structures mined from pre-trained language models (e.g., BERT). We show that, similar to syntactic trees, unsupervised structures are quite informative for RE task: they are able to obtain competitive (even the best) performance scores on benchmark RE datasets (ACE05, WebNLG, SciERC). We also conduct detailed analyses on their abilities of adapting new RE domains and influence of noise links in those structures. The results suggest that unsupervised structures are reasonable alternatives of commonly used syntactic structures in relation extraction models.</abstract>
      <url hash="6786ea0f">2022.findings-emnlp.453</url>
      <bibkey>yang-etal-2022-explore</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.453</doi>
    </paper>
    <paper id="454">
      <title>Identifying Human Strategies for Generating Word-Level Adversarial Examples</title>
      <author><first>Maximilian</first><last>Mozes</last></author>
      <author><first>Bennett</first><last>Kleinberg</last></author>
      <author><first>Lewis</first><last>Griffin</last></author>
      <pages>6118-6126</pages>
      <abstract>Adversarial examples in NLP are receiving increasing research attention. One line of investigation is the generation of word-level adversarial examples against fine-tuned Transformer models that preserve naturalness and grammaticality. Previous work found that human- and machine-generated adversarial examples are comparable in their naturalness and grammatical correctness. Most notably, humans were able to generate adversarial examples much more effortlessly than automated attacks. In this paper, we provide a detailed analysis of exactly how humans create these adversarial examples. By exploring the behavioural patterns of human workers during the generation process, we identify statistically significant tendencies based on which words humans prefer to select for adversarial replacement (e.g., word frequencies, word saliencies, sentiment) as well as where and when words are replaced in an input sequence. With our findings, we seek to inspire efforts that harness human strategies for more robust NLP models.</abstract>
      <url hash="401516cc">2022.findings-emnlp.454</url>
      <bibkey>mozes-etal-2022-identifying</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.454</doi>
    </paper>
    <paper id="455">
      <title>Refinement Matters: Textual Description Needs to be Refined for Zero-shot Learning</title>
      <author><first>Chandan</first><last>Gautam</last></author>
      <author><first>Sethupathy</first><last>Parameswaran</last></author>
      <author><first>Vinay</first><last>Verma</last></author>
      <author><first>Suresh</first><last>Sundaram</last></author>
      <author><first>Savitha</first><last>Ramasamy</last></author>
      <pages>6127-6140</pages>
      <abstract>Zero-Shot Learning (ZSL) has shown great promise at the intersection of vision and language, and generative methods for ZSL are predominant owing to their efficiency. Moreover, textual description or attribute plays a critical role in transferring knowledge from the seen to unseen classes in ZSL. Such generative approaches for ZSL are very costly to train and require the class description of the unseen classes during training. In this work, we propose a non-generative gating-based attribute refinement network for ZSL, which achieves similar accuracies to generative methods of ZSL, at a much lower computational cost. The refined attributes are mapped into the visual domain through an attribute embedder, and the whole network is guided by the circle loss and the well-known softmax cross-entropy loss to obtain a robust class embedding. We refer to our approach as Circle loss guided gating-based Attribute-Refinement Network (CARNet). We perform extensive experiments on the five benchmark datasets over the various challenging scenarios viz., Generalized ZSL (GZSL), Continual GZSL (CGZSL), and conventional ZSL. We observe that the CARNet significantly outperforms recent non-generative ZSL methods and most generative ZSL methods in all three settings by a significant margin. Our extensive ablation study disentangles the performance of various components and justifies their importance. The source code is available at <url>https://github.com/Sethup123/CARNet</url>.</abstract>
      <url hash="8072390c">2022.findings-emnlp.455</url>
      <bibkey>gautam-etal-2022-refinement</bibkey>
      <video href="2022.findings-emnlp.455.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.455</doi>
    </paper>
    <paper id="456">
      <title><fixed-case>SAT</fixed-case>: Improving Semi-Supervised Text Classification with Simple Instance-Adaptive Self-Training</title>
      <author><first>Hui</first><last>Chen</last></author>
      <author><first>Wei</first><last>Han</last></author>
      <author><first>Soujanya</first><last>Poria</last></author>
      <pages>6141-6146</pages>
      <abstract>Self-training methods have been explored in recent years and have exhibited great performance in improving semi-supervised learning. This work presents a simple instance-adaptive self-training method (SAT) for semi-supervised text classification. SAT first generates two augmented views for each unlabeled data, and then trains a meta learner to automatically identify the relative strength of augmentations based on the similarity between the original view and the augmented views. The weakly-augmented view is fed to the model to produce a pseudo-label and the strongly-augmented view is used to train the model to predict the same pseudo-label. We conducted extensive experiments and analyses on three text classification datasets and found that with varying sizes of labeled training data, SAT consistently shows competitive performance compared to existing semi-supervised learning methods.</abstract>
      <url hash="61f73d94">2022.findings-emnlp.456</url>
      <bibkey>chen-etal-2022-sat</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.456</doi>
    </paper>
    <paper id="457">
      <title>Answer Quality Aware Aggregation for Extractive <fixed-case>QA</fixed-case> Crowdsourcing</title>
      <author><first>Peide</first><last>Zhu</last></author>
      <author><first>Zhen</first><last>Wang</last></author>
      <author><first>Claudia</first><last>Hauff</last></author>
      <author><first>Jie</first><last>Yang</last></author>
      <author><first>Avishek</first><last>Anand</last></author>
      <pages>6147-6159</pages>
      <abstract>Quality control is essential for creating extractive question answering (EQA) datasets via crowdsourcing. Aggregation across answers, i.e. word spans within passages annotated, by different crowd workers is one major focus for ensuring its quality. However, crowd workers cannot reach a consensus on a considerable portion of questions. We introduce a simple yet effective answer aggregation method that takes into account the relations among the answer, question, and context passage. We evaluate answer quality from both the view of question answering model to determine how confident the QA model is about each answer and the view of the answer verification model to determine whether the answer is correct. Then we compute aggregation scores with each answer’s quality and its contextual embedding produced by pre-trained language models. The experiments on a large real crowdsourced EQA dataset show that our framework outperforms baselines by around 16% on precision and effectively conduct answer aggregation for extractive QA task.</abstract>
      <url hash="0959509a">2022.findings-emnlp.457</url>
      <bibkey>zhu-etal-2022-answer</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.457</doi>
    </paper>
    <paper id="458">
      <title>Search to Pass Messages for Temporal Knowledge Graph Completion</title>
      <author><first>Zhen</first><last>Wang</last></author>
      <author><first>Haotong</first><last>Du</last></author>
      <author><first>Quanming</first><last>Yao</last></author>
      <author><first>Xuelong</first><last>Li</last></author>
      <pages>6160-6172</pages>
      <abstract>Completing missing facts is a fundamental task for temporal knowledge graphs (TKGs).Recently, graph neural network (GNN) based methods, which can simultaneously explore topological and temporal information, have become the state-of-the-art (SOTA) to complete TKGs. However, these studies are based on hand-designed architectures and fail to explore the diverse topological and temporal properties of TKG.To address this issue, we propose to use neural architecture search (NAS) to design data-specific message passing architecture for TKG completion. In particular, we develop a generalized framework to explore topological and temporal information in TKGs.Based on this framework, we design an expressive search space to fully capture various properties of different TKGs. Meanwhile, we adopt a search algorithm, which trains a supernet structure by sampling single path for efficient search with less cost. We further conduct extensive experiments on three benchmark datasets. The results show that the searched architectures by our method achieve the SOTA performances. Besides, the searched models can also implicitly reveal diverse properties in different TKGs.Our code is released in <url>https://github.com/striderdu/SPA</url>.</abstract>
      <url hash="8e56620e">2022.findings-emnlp.458</url>
      <bibkey>wang-etal-2022-search</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.458</doi>
    </paper>
    <paper id="459">
      <title>Code Vulnerability Detection via Nearest Neighbor Mechanism</title>
      <author><first>Qianjin</first><last>Du</last></author>
      <author><first>Xiaohui</first><last>Kuang</last></author>
      <author><first>Gang</first><last>Zhao</last></author>
      <pages>6173-6178</pages>
      <abstract>Code vulnerability detection is a fundamental and challenging task in the software security field. Existing research works aim to learn semantic information from the source code by utilizing NLP technologies. However, in vulnerability detection tasks, some vulnerable samples are very similar to non-vulnerable samples, which are difficult to identify. To address this issue and improve detection performance, we introduce the <tex-math>k</tex-math>-nearest neighbor mechanism which retrieves multiple neighbor samples and utilizes label information of retrieved neighbor samples to provide help for model predictions. Besides, we use supervised contrastive learning to make the model learn the discriminative representation and ensure that label information of retrieved neighbor samples is as consistent as possible with the label information of testing samples. Extensive experiments show that our method can achieve obvious performance improvements compared to baseline models.</abstract>
      <url hash="9ab1952f">2022.findings-emnlp.459</url>
      <bibkey>du-etal-2022-code</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.459</doi>
    </paper>
    <paper id="460">
      <title>Robust Question Answering against Distribution Shifts with Test-Time Adaption: An Empirical Study</title>
      <author><first>Hai</first><last>Ye</last></author>
      <author><first>Yuyang</first><last>Ding</last></author>
      <author><first>Juntao</first><last>Li</last></author>
      <author><first>Hwee Tou</first><last>Ng</last></author>
      <pages>6179-6192</pages>
      <abstract>A deployed question answering (QA) model can easily fail when the test data has a distribution shift compared to the training data. Robustness tuning (RT) methods have been widely studied to enhance model robustness against distribution shifts before model deployment. However, can we improve a model after deployment? To answer this question, we evaluate test-time adaptation (TTA) to improve a model after deployment. We first introduce ColdQA, a unified evaluation benchmark for robust QA against text corruption and changes in language and domain. We then evaluate previous TTA methods on ColdQA and compare them to RT methods. We also propose a novel TTA method called online imitation learning (OIL). Through extensive experiments, we find that TTA is comparable to RT methods, and applying TTA after RT can significantly boost the performance on ColdQA. Our proposed OIL improves TTA to be more robust to variation in hyper-parameters and test distributions over time.</abstract>
      <url hash="d68ddbf8">2022.findings-emnlp.460</url>
      <bibkey>ye-etal-2022-robust</bibkey>
      <video href="2022.findings-emnlp.460.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.460</doi>
    </paper>
    <paper id="461">
      <title><fixed-case>P</fixed-case>ara<fixed-case>M</fixed-case>ac: A General Unsupervised Paraphrase Generation Framework Leveraging Semantic Constraints and Diversifying Mechanisms</title>
      <author><first>Jinxin</first><last>Liu</last></author>
      <author><first>Jiaxin</first><last>Shi</last></author>
      <author><first>Ji</first><last>Qi</last></author>
      <author><first>Lei</first><last>Hou</last></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <author><first>Qi</first><last>Tian</last></author>
      <pages>6193-6206</pages>
      <abstract>Paraphrase generation reflects the ability to understand the meaning from the language surface form and rephrase it to other expressions. Recent paraphrase generation works have paid attention to unsupervised approaches based on Pre-trained Language Models (PLMs) to avoid heavy reliance on parallel data by utilizing PLMs’ generation ability. However, the generated pairs of existing unsupervised methods are usually weak either in semantic equivalence or expression diversity. In this paper, we present a novel unsupervised paraphrase generation framework called Paraphrase Machine. By employing multi-aspect equivalence constraints and multi-granularity diversifying mechanisms, Paraphrase Machine is able to achieve good semantic equivalence and expressive diversity, producing a high-quality unsupervised paraphrase dataset. Based on this dataset, we train a general paraphrase model, which can be directly applied to rewrite the input sentence of various domains without any fine-tuning, and achieves substantial gains of 9.1% and 3.3% absolutely in BLEU score over previous SOTA on Quora and MSCOCO. By further fine-tuning our model with domain-specific training sets, the improvement can be increased to even 18.0% and 4.6%. Most importantly, by applying it to language understanding and generation tasks under the low-resource setting, we demonstrate that our model can serve as a universal data augmentor to boost the few-shot performance (e.g., average 2.0% gain on GLUE).</abstract>
      <url hash="f8c8ffbe">2022.findings-emnlp.461</url>
      <bibkey>liu-etal-2022-paramac</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.461</doi>
    </paper>
    <paper id="462">
      <title>Semi-supervised New Slot Discovery with Incremental Clustering</title>
      <author><first>Yuxia</first><last>Wu</last></author>
      <author><first>Lizi</first><last>Liao</last></author>
      <author><first>Xueming</first><last>Qian</last></author>
      <author><first>Tat-Seng</first><last>Chua</last></author>
      <pages>6207-6218</pages>
      <abstract>Discovering new slots is critical to the success of dialogue systems. Most existing methods rely on automatic slot induction in unsupervised fashion or perform domain adaptation across zero or few-shot scenarios. They have difficulties in providing high-quality supervised signals to learn clustering-friendly features, and are limited in effectively transferring the prior knowledge from known slots to new slots. In this work, we propose a Semi-supervised Incremental Clustering method (SIC), to discover new slots with the aid of existing linguistic annotation models and limited known slot data. Specifically, we harvest slot value candidates with NLP model cues and innovatively formulate the slot discovery task under an incremental clustering framework. The model gradually calibrate slot representations under the supervision of generated pseudo-labels, and automatically learns to terminate when no more salient slot remains. Our thorough evaluation on five public datasets demonstrates that it significantly outperforms state-of-the-art models.</abstract>
      <url hash="be7bb171">2022.findings-emnlp.462</url>
      <bibkey>wu-etal-2022-semi</bibkey>
      <video href="2022.findings-emnlp.462.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.462</doi>
    </paper>
    <paper id="463">
      <title>Con-<fixed-case>NAT</fixed-case>: Contrastive Non-autoregressive Neural Machine Translation</title>
      <author><first>Hao</first><last>Cheng</last></author>
      <author><first>Zhihua</first><last>Zhang</last></author>
      <pages>6219-6231</pages>
      <abstract>Inspired by the success of contrastive learning in natural language processing, we incorporate contrastive learning into the conditional masked language model which is extensively used in non-autoregressive neural machine translation (NAT). Accordingly, we propose a Contrastive Non-autoregressive Neural Machine Translation (Con-NAT) model. Con-NAT optimizes the similarity of several different representations of the same token in the same sentence. We propose two methods to obtain various representations: Contrastive Common Mask and Contrastive Dropout. Positive pairs are various different representations of the same token, while negative pairs are representations of different tokens. In the feature space, the model with contrastive loss pulls positive pairs together and pushes negative pairs away. We conduct extensive experiments on six translation directions with different data sizes. The results demonstrate that Con-NAT showed a consistent and significant improvement in fully and iterative NAT. Con-NAT is state-of-the-art on WMT’16 Ro-En (34.18 BLEU).</abstract>
      <url hash="7ea5d057">2022.findings-emnlp.463</url>
      <bibkey>cheng-zhang-2022-con</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.463</doi>
    </paper>
    <paper id="464">
      <title>Improved Knowledge Distillation for Pre-trained Language Models via Knowledge Selection</title>
      <author><first>Chenglong</first><last>Wang</last></author>
      <author><first>Yi</first><last>Lu</last></author>
      <author><first>Yongyu</first><last>Mu</last></author>
      <author><first>Yimin</first><last>Hu</last></author>
      <author><first>Tong</first><last>Xiao</last></author>
      <author><first>Jingbo</first><last>Zhu</last></author>
      <pages>6232-6244</pages>
      <abstract>Knowledge distillation addresses the problem of transferring knowledge from a teacher model to a student model. In this process, we typically have multiple types of knowledge extracted from the teacher model. The problem is to make full use of them to train the student model. Our preliminary study shows that: (1) not all of the knowledge is necessary for learning a good student model, and (2) knowledge distillation can benefit from certain knowledge at different training steps. In response to these, we propose an actor-critic approach to selecting appropriate knowledge to transfer during the process of knowledge distillation. In addition, we offer a refinement of the training algorithm to ease the computational burden. Experimental results on the GLUE datasets show that our method outperforms several strong knowledge distillation baselines significantly.</abstract>
      <url hash="c093b6bf">2022.findings-emnlp.464</url>
      <bibkey>wang-etal-2022-improved</bibkey>
      <video href="2022.findings-emnlp.464.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.464</doi>
    </paper>
    <paper id="465">
      <title>Syntactically Robust Training on Partially-Observed Data for Open Information Extraction</title>
      <author><first>Ji</first><last>Qi</last></author>
      <author><first>Yuxiang</first><last>Chen</last></author>
      <author><first>Lei</first><last>Hou</last></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <author><first>Bin</first><last>Xu</last></author>
      <pages>6245-6257</pages>
      <abstract>Open Information Extraction models have shown promising results with sufficient supervision. However, these models face a fundamental challenge that the syntactic distribution of training data is partially observable in comparison to the real world. In this paper, we propose a syntactically robust training framework that enables models to be trained on a syntactic-abundant distribution based on diverse paraphrase generation. To tackle the intrinsic problem of knowledge deformation of paraphrasing, two algorithms based on semantic similarity matching and syntactic tree walking are used to restore the expressionally transformed knowledge. The training framework can be generally applied to other syntactic partial observable domains. Based on the proposed framework, we build a new evaluation set called CaRB-AutoPara, a syntactically diverse dataset consistent with the real-world setting for validating the robustness of the models. Experiments including a thorough analysis show that the performance of the model degrades with the increase of the difference in syntactic distribution, while our framework gives a robust boundary.</abstract>
      <url hash="85c4b1fb">2022.findings-emnlp.465</url>
      <bibkey>qi-etal-2022-syntactically</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.465</doi>
    </paper>
    <paper id="466">
      <title>A Benchmark and Dataset for Post-<fixed-case>OCR</fixed-case> text correction in <fixed-case>S</fixed-case>anskrit</title>
      <author><first>Ayush</first><last>Maheshwari</last></author>
      <author><first>Nikhil</first><last>Singh</last></author>
      <author><first>Amrith</first><last>Krishna</last></author>
      <author><first>Ganesh</first><last>Ramakrishnan</last></author>
      <pages>6258-6265</pages>
      <abstract>Sanskrit is a classical language with about 30 million extant manuscripts fit for digitisation, available in written, printed or scanned-image forms. However, it is still considered to be a low-resource language when it comes to available digital resources. In this work, we release a post-OCR text correction dataset containing around 218,000 sentences, with 1.5 million words, from 30 different books. Texts in Sanskrit are known to be diverse in terms of their linguistic and stylistic usage since Sanskrit was the ‘lingua francua’ for discourse in the Indian subcontinent for about 3 millennia. Keeping this in mind, we release a multi-domain dataset, from areas as diverse as astronomy, medicine and mathematics, with some of them as old as 18 centuries. Further, we release multiple strong baselines as benchmarks for the task, based on pre-trained Seq2Seq language models. We find that our best-performing model, consisting of byte level tokenization in conjunction with phonetic encoding (Byt5+SLP1), yields a 23% point increase over the OCR output in terms of word and character error rates. Moreover, we perform extensive experiments in evaluating these models on their performance and analyse common causes of mispredictions both at the graphemic and lexical levels. Our code and dataset is publicly available at <url>https://github.com/ayushbits/pe-ocr-sanskrit</url>.</abstract>
      <url hash="083a8058">2022.findings-emnlp.466</url>
      <bibkey>maheshwari-etal-2022-benchmark</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.466</doi>
    </paper>
    <paper id="467">
      <title>Knowledge-Enhanced Self-Supervised Prototypical Network for Few-Shot Event Detection</title>
      <author><first>Kailin</first><last>Zhao</last></author>
      <author><first>Xiaolong</first><last>Jin</last></author>
      <author><first>Long</first><last>Bai</last></author>
      <author><first>Jiafeng</first><last>Guo</last></author>
      <author><first>Xueqi</first><last>Cheng</last></author>
      <pages>6266-6275</pages>
      <abstract>Prototypical network based joint methods have attracted much attention in few-shot event detection, which carry out event detection in a unified sequence tagging framework. However, these methods suffer from the inaccurate prototype representation problem, due to two main reasons: the number of instances for calculating prototypes is limited; And, they do not well capture the relationships among event prototypes. To deal with this problem, we propose a Knowledge-Enhanced self-supervised Prototypical Network, called KE-PN, for few-shot event detection. KE-PN adopts hybrid rules, which can automatically align event types to an external knowledge base, i.e., FrameNet, to obtain more instances. It proposes a self-supervised learning method to filter out noisy data from enhanced instances. KE-PN is further equipped with an auxiliary event type relationship classification module, which injects the relationship information into representations of event prototypes. Extensive experiments on three benchmark datasets, i.e., FewEvent, MAVEN, and ACE2005 demonstrate the state-of-the-art performance of KE-PN.</abstract>
      <url hash="816b314b">2022.findings-emnlp.467</url>
      <bibkey>zhao-etal-2022-knowledge</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.467</doi>
    </paper>
    <paper id="468">
      <title><fixed-case>V</fixed-case>ar<fixed-case>MAE</fixed-case>: Pre-training of Variational Masked Autoencoder for Domain-adaptive Language Understanding</title>
      <author><first>Dou</first><last>Hu</last></author>
      <author><first>Xiaolong</first><last>Hou</last></author>
      <author><first>Xiyang</first><last>Du</last></author>
      <author><first>Mengyuan</first><last>Zhou</last></author>
      <author><first>Lianxin</first><last>Jiang</last></author>
      <author><first>Yang</first><last>Mo</last></author>
      <author><first>Xiaofeng</first><last>Shi</last></author>
      <pages>6276-6286</pages>
      <abstract>Pre-trained language models have been widely applied to standard benchmarks. Due to the flexibility of natural language, the available resources in a certain domain can be restricted to support obtaining precise representation. To address this issue, we propose a novel Transformer-based language model named VarMAE for domain-adaptive language understanding. Under the masked autoencoding objective, we design a context uncertainty learning module to encode the token’s context into a smooth latent distribution. The module can produce diverse and well-formed contextual representations. Experiments on science- and finance-domain NLU tasks demonstrate that VarMAE can be efficiently adapted to new domains with limited resources.</abstract>
      <url hash="bd214321">2022.findings-emnlp.468</url>
      <bibkey>hu-etal-2022-varmae</bibkey>
      <video href="2022.findings-emnlp.468.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.468</doi>
    </paper>
    <paper id="469">
      <title>Exploring Methods for Building Dialects-<fixed-case>M</fixed-case>andarin Code-Mixing Corpora: A Case Study in <fixed-case>T</fixed-case>aiwanese Hokkien</title>
      <author><first>Sin-En</first><last>Lu</last></author>
      <author><first>Bo-Han</first><last>Lu</last></author>
      <author><first>Chao-Yi</first><last>Lu</last></author>
      <author><first>Richard Tzong-Han</first><last>Tsai</last></author>
      <pages>6287-6305</pages>
      <abstract>In natural language processing (NLP), code-mixing (CM) is a challenging task, especially when the mixed languages include dialects. In Southeast Asian countries such as Singapore, Indonesia, and Malaysia, Hokkien-Mandarin is the most widespread code-mixed language pair among Chinese immigrants, and it is also common in Taiwan. However, dialects such as Hokkien often have a scarcity of resources and the lack of an official writing system, limiting the development of dialect CM research. In this paper, we propose a method to construct a Hokkien-Mandarin CM dataset to mitigate the limitation, overcome the morphological issue under the Sino-Tibetan language family, and offer an efficient Hokkien word segmentation method through a linguistics-based toolkit. Furthermore, we use our proposed dataset and employ transfer learning to train the XLM (cross-lingual language model) for translation tasks. To fit the code-mixing scenario, we adapt XLM slightly. We found that by using linguistic knowledge, rules, and language tags, the model produces good results on CM data translation while maintaining monolingual translation quality.</abstract>
      <url hash="575827ea">2022.findings-emnlp.469</url>
      <bibkey>lu-etal-2022-exploring</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.469</doi>
    </paper>
    <paper id="470">
      <title>Recurrence Boosts Diversity! Revisiting Recurrent Latent Variable in Transformer-Based Variational <fixed-case>A</fixed-case>uto<fixed-case>E</fixed-case>ncoder for Diverse Text Generation</title>
      <author><first>Jinyi</first><last>Hu</last></author>
      <author><first>Xiaoyuan</first><last>Yi</last></author>
      <author><first>Wenhao</first><last>Li</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <author><first>Xing</first><last>Xie</last></author>
      <pages>6306-6320</pages>
      <abstract>Variational Auto-Encoder (VAE) has been widely adopted in text generation. Among many variants, recurrent VAE learns token-wise latent variables with each conditioned on the preceding ones, which captures sequential variability better in the era of RNN. However, it is unclear how to incorporate such recurrent dynamics into the recently dominant Transformer due to its parallelism. In this work, we propose TRACE, a Transformer-based recurrent VAE structure. TRACE imposes recurrence on segment-wise latent variables with arbitrarily separated text segments and constructs the posterior distribution with residual parameterization. Besides, we design an acceleration method by approximating idempotent matrices, which allows parallelism while maintaining the conditional dependence of latent variables. We demonstrate that TRACE could deduce a non-zero lower bound of the KL term and enhance the entanglement of each segment and preceding latent variables, providing a theoretical guarantee of generation diversity. Experiments on two unconditional and one conditional generation task show that TRACE achieves significantly improved diversity while maintaining satisfactory generation quality.</abstract>
      <url hash="61a20bec">2022.findings-emnlp.470</url>
      <bibkey>hu-etal-2022-recurrence</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.470</doi>
    </paper>
    <paper id="471">
      <title>Tweet Based Reach Aware Temporal Attention Network for <fixed-case>NFT</fixed-case> Valuation</title>
      <author><first>Ramit</first><last>Sawhney</last></author>
      <author><first>Megh</first><last>Thakkar</last></author>
      <author><first>Ritesh</first><last>Soun</last></author>
      <author><first>Atula</first><last>Neerkaje</last></author>
      <author><first>Vasu</first><last>Sharma</last></author>
      <author><first>Dipanwita</first><last>Guhathakurta</last></author>
      <author><first>Sudheer</first><last>Chava</last></author>
      <pages>6321-6332</pages>
      <abstract>Non-Fungible Tokens (NFTs) are a relatively unexplored class of assets. Designing strategies to forecast NFT trends is an intricate task due to its extremely volatile nature. The market is largely driven by public sentiment and “hype”, which in turn has a high correlation with conversations taking place on social media platforms like Twitter. Prior work done for modelling stock market data does not take into account the extent of impact certain highly influential tweets and their authors can have on the market. Building on these limitations and the nature of the NFT market, we propose a novel reach-aware temporal learning approach to make predictions for forecasting future trends in the NFT market. We perform experiments on a new dataset consisting of over 1.3 million tweets and 180 thousand NFT transactions spanning over 15 NFT collections curated by us. Our model (TA-NFT) outperforms other state-of-the-art methods by an average of 36%. Through extensive quantitative and ablative analysis, we demonstrate the ability of our approach as a practical method for predicting NFT trends.</abstract>
      <url hash="7a8f9d59">2022.findings-emnlp.471</url>
      <bibkey>sawhney-etal-2022-tweet</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.471</doi>
    </paper>
    <paper id="472">
      <title>Entity Embedding Completion for Wide-Coverage Entity Disambiguation</title>
      <author><first>Daisuke</first><last>Oba</last></author>
      <author><first>Ikuya</first><last>Yamada</last></author>
      <author><first>Naoki</first><last>Yoshinaga</last></author>
      <author><first>Masashi</first><last>Toyoda</last></author>
      <pages>6333-6344</pages>
      <abstract>Entity disambiguation (ED) is typically solved by learning to classify a given mention into one of the entities in the model’s entity vocabulary by referring to their embeddings. However, this approach cannot address mentions of entities that are not covered by the entity vocabulary. Aiming to enhance the applicability of ED models, we propose a method of extending a state-of-the-art ED model by dynamically computing embeddings of out-of-vocabulary entities. Specifically, our method computes embeddings from entity descriptions and mention contexts. Experiments with standard benchmark datasets show that the extended model performs comparable to or better than existing models whose entity embeddings are trained for all candidate entities as well as embedding-free models. We release our source code and model checkpoints at <url>https://github.com/studio-ousia/steel</url>.</abstract>
      <url hash="225b1946">2022.findings-emnlp.472</url>
      <bibkey>oba-etal-2022-entity</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.472</doi>
    </paper>
    <paper id="473">
      <title>Entity-level Interaction via Heterogeneous Graph for Multimodal Named Entity Recognition</title>
      <author><first>Gang</first><last>Zhao</last></author>
      <author><first>Guanting</first><last>Dong</last></author>
      <author><first>Yidong</first><last>Shi</last></author>
      <author><first>Haolong</first><last>Yan</last></author>
      <author><first>Weiran</first><last>Xu</last></author>
      <author><first>Si</first><last>Li</last></author>
      <pages>6345-6350</pages>
      <abstract>Multimodal Named Entity Recognition (MNER) faces two specific challenges: 1) How to capture useful entity-related visual information. 2) How to alleviate the interference of visual noise. Previous works have gained progress by improving interacting mechanisms or seeking for better visual features. However, existing methods neglect the integrity of entity semantics and conduct cross-modal interaction at token-level, which cuts apart the semantics of entities and makes non-entity tokens easily interfered with by irrelevant visual noise. Thus in this paper, we propose an end-to-end heterogeneous Graph-based Entity-level Interacting model (GEI) for MNER. GEI first utilizes a span detection subtask to obtain entity representations, which serve as the bridge between two modalities. Then, the heterogeneous graph interacting network interacts entity with object nodes to capture entity-related visual information, and fuses it into only entity-associated tokens to rid non-entity tokens of the visual noise. Experiments on two widely used datasets demonstrate the effectiveness of our method. Our code will be available at <url>https://github.com/GangZhao98/GEI</url>.</abstract>
      <url hash="a929a804">2022.findings-emnlp.473</url>
      <bibkey>zhao-etal-2022-entity</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.473</doi>
    </paper>
    <paper id="474">
      <title>Status Biases in Deliberation Online: Evidence from a Randomized Experiment on <fixed-case>C</fixed-case>hange<fixed-case>M</fixed-case>y<fixed-case>V</fixed-case>iew</title>
      <author><first>Emaad</first><last>Manzoor</last></author>
      <author><first>Yohan</first><last>Jo</last></author>
      <author><first>Alan</first><last>Montgomery</last></author>
      <pages>6351-6363</pages>
      <abstract>Status is widely used to incentivize user engagement online. However, visible status indicators could inadvertently bias online deliberation to favor high-status users. In this work, we design and deploy a randomized experiment on the ChangeMyView platform to quantify status biases in deliberation online. We find strong evidence of status bias: hiding status on ChangeMyView increases the persuasion rate of moderate-status users by 84% and decreases the persuasion rate of high-status users by 41% relative to the control group. We also find that the persuasive power of status is moderated by verbosity, suggesting that status is used as an information-processing heuristic under cognitive load. Finally, we find that a user’s status influences the argumentation behavior of other users they interact with in a manner that disadvantages low and moderate-status users.</abstract>
      <url hash="fb118468">2022.findings-emnlp.474</url>
      <bibkey>manzoor-etal-2022-status</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.474</doi>
    </paper>
    <paper id="475">
      <title>Empathetic and Emotionally Positive Conversation Systems with an Emotion-specific Query-Response Memory</title>
      <author><first>Zhiliang</first><last>Tian</last></author>
      <author><first>Yinliang</first><last>Wang</last></author>
      <author><first>Yiping</first><last>Song</last></author>
      <author><first>Chi</first><last>Zhang</last></author>
      <author><first>Dongkyu</first><last>Lee</last></author>
      <author><first>Yingxiu</first><last>Zhao</last></author>
      <author><first>Dongsheng</first><last>Li</last></author>
      <author><first>Nevin L.</first><last>Zhang</last></author>
      <pages>6364-6376</pages>
      <abstract>Emotional conversation systems generate responses for the input queries considering the speaker’s emotions in a conversation. Existing emotional conversation systems output emotional responses according to either a given emotion or the user’s emotion reflected in the input queries. Following a given emotion may lead to an emotional drift between the given emotion and the conversation state, and following only the user’s emotion may aggravate the user’s negative feelings if users suffer from a negative mood. In this paper, we propose to generate empathetic responses catering to the user’s emotions while leading the conversation to be emotionally positive. Particularly, by abstracting the conversation corpus, we extract and store the different responding strategies for different users’ emotions and conversational topics into a memory. We encourage positive emotions in conversation via a sentiment evaluator. We model the memory outputs with a Gaussian mixture distribution and sample a final responding strategy from the distribution. The strategy acts as a condition to a transformer model to generate responses. The experiments verify our model surpasses the baseline methods in appropriateness, diversity, and generating emotionally positive responses.</abstract>
      <url hash="9fcb7cc8">2022.findings-emnlp.475</url>
      <bibkey>tian-etal-2022-empathetic</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.475</doi>
    </paper>
    <paper id="476">
      <title><fixed-case>T</fixed-case>rial2<fixed-case>V</fixed-case>ec: Zero-Shot Clinical Trial Document Similarity Search using Self-Supervision</title>
      <author><first>Zifeng</first><last>Wang</last></author>
      <author><first>Jimeng</first><last>Sun</last></author>
      <pages>6377-6390</pages>
      <abstract>Clinical trials are essential for drug development but are extremely expensive and time-consuming to conduct. It is beneficial to study similar historical trials when designing a clinical trial. However, lengthy trial documents and lack of labeled data make trial similarity search difficult. We propose a zero-shotclinical trial retrieval method, called Trial2Vec, which learns through self-supervision without the need for annotating similar clinical trials. Specifically, the meta-structure of trial documents (e.g., title, eligibility criteria, target disease) along with clinical knowledge (e.g., UMLS knowledge base) are leveraged to automatically generate contrastive samples. Besides, encodes trial documents considering meta-structure thus producing compact embeddings aggregating multi-aspect information from the whole document. We show that our method yields medically interpretable embeddings by visualization and it gets 15% average improvement over the best baselines on precision/recall for trial retrieval, which is evaluated on our labeled 1600 trial pairs. In addition, we prove the pretrained embeddings benefit the downstream trial outcome prediction task over 240k trials. Software is available at <url>https://github.com/RyanWangZf/Trial2Vec</url>.</abstract>
      <url hash="1fd7b284">2022.findings-emnlp.476</url>
      <bibkey>wang-sun-2022-trial2vec</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.476</doi>
    </paper>
    <paper id="477">
      <title>From Mimicking to Integrating: Knowledge Integration for Pre-Trained Language Models</title>
      <author><first>Lei</first><last>Li</last></author>
      <author><first>Yankai</first><last>Lin</last></author>
      <author><first>Xuancheng</first><last>Ren</last></author>
      <author><first>Guangxiang</first><last>Zhao</last></author>
      <author><first>Peng</first><last>Li</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <author><first>Xu</first><last>Sun</last></author>
      <pages>6391-6402</pages>
      <abstract>Investigating better ways to reuse the released pre-trained language models (PLMs) can significantly reduce the computational cost and the potential environmental side-effects. This paper explores a novel PLM reuse paradigm, Knowledge Integration (KI). Without human annotations available, KI aims to merge the knowledge from different teacher-PLMs, each of which specializes in a different classification problem, into a versatile student model. To achieve this, we first derive the correlation between virtual golden supervision and teacher predictions. We then design a Model Uncertainty–aware Knowledge Integration (MUKI) framework to recover the golden supervision for the student. Specifically, MUKI adopts Monte-Carlo Dropout to estimate model uncertainty for the supervision integration. An instance-wise re-weighting mechanism based on the margin of uncertainty scores is further incorporated, to deal with the potential conflicting supervision from teachers. Experimental results demonstrate that MUKI achieves substantial improvements over baselines on benchmark datasets. Further analysis shows that MUKI can generalize well for merging teacher models with heterogeneous architectures, and even teachers major in cross-lingual datasets.</abstract>
      <url hash="266d760a">2022.findings-emnlp.477</url>
      <bibkey>li-etal-2022-mimicking</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.477</doi>
    </paper>
    <paper id="478">
      <title>Model and Data Transfer for Cross-Lingual Sequence Labelling in Zero-Resource Settings</title>
      <author><first>Iker</first><last>García-Ferrero</last></author>
      <author><first>Rodrigo</first><last>Agerri</last></author>
      <author><first>German</first><last>Rigau</last></author>
      <pages>6403-6416</pages>
      <abstract>Zero-resource cross-lingual transfer approaches aim to apply supervised modelsfrom a source language to unlabelled target languages. In this paper we performan in-depth study of the two main techniques employed so far for cross-lingualzero-resource sequence labelling, based either on data or model transfer. Although previous research has proposed translation and annotation projection(data-based cross-lingual transfer) as an effective technique for cross-lingualsequence labelling, in this paper we experimentally demonstrate that highcapacity multilingual language models applied in a zero-shot (model-basedcross-lingual transfer) setting consistently outperform data-basedcross-lingual transfer approaches. A detailed analysis of our results suggeststhat this might be due to important differences in language use. Morespecifically, machine translation often generates a textual signal which isdifferent to what the models are exposed to when using gold standard data,which affects both the fine-tuning and evaluation processes. Our results alsoindicate that data-based cross-lingual transfer approaches remain a competitiveoption when high-capacity multilingual language models are not available.</abstract>
      <url hash="db9a9cb5">2022.findings-emnlp.478</url>
      <bibkey>garcia-ferrero-etal-2022-model</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.478</doi>
    </paper>
    <paper id="479">
      <title>Early Guessing for Dialect Identification</title>
      <author><first>Vani</first><last>Kanjirangat</last></author>
      <author><first>Tanja</first><last>Samardzic</last></author>
      <author><first>Fabio</first><last>Rinaldi</last></author>
      <author><first>Ljiljana</first><last>Dolamic</last></author>
      <pages>6417-6426</pages>
      <abstract>This paper deals with the problem of incre-mental dialect identification. Our goal is toreliably determine the dialect before the fullutterance is given as input. The major partof the previous research on dialect identification has been model-centric, focusing on performance. We address a new question: How much input is needed to identify a dialect? Ourapproach is a data-centric analysis that resultsin general criteria for finding the shortest inputneeded to make a plausible guess. Workingwith three sets of language dialects (Swiss German, Indo-Aryan and Arabic languages), weshow that it is possible to generalize across dialects and datasets with two input shorteningcriteria: model confidence and minimal inputlength (adjusted for the input type). The sourcecode for experimental analysis can be found atGithub.</abstract>
      <url hash="9b19c142">2022.findings-emnlp.479</url>
      <bibkey>kanjirangat-etal-2022-early</bibkey>
      <video href="2022.findings-emnlp.479.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.479</doi>
    </paper>
    <paper id="480">
      <title><fixed-case>R</fixed-case>-<fixed-case>AT</fixed-case>: Regularized Adversarial Training for Natural Language Understanding</title>
      <author><first>Shiwen</first><last>Ni</last></author>
      <author><first>Jiawen</first><last>Li</last></author>
      <author><first>Hung-Yu</first><last>Kao</last></author>
      <pages>6427-6440</pages>
      <abstract>Currently, adversarial training has become a popular and powerful regularization method in the natural language domain. In this paper, we Regularized Adversarial Training (R-AT) via dropout, which forces the output probability distributions of different sub-models generated by dropout to be consistent under the same adversarial samples. Specifically, we generate adversarial samples by perturbing the word embeddings. For each adversarial sample fed to the model, R-AT minimizes both the adversarial risk and the bidirectional KL-divergence between the adversarial output distributions of two sub-models sampled by dropout. Through extensive experiments on 13 public natural language understanding datasets, we found that R-AT has improvements for many models (e.g., rnn-based, cnn-based, and transformer-based models). For the GLUE benchmark, when R-AT is only applied to the fine-tuning stage, it is able to improve the overall test score of the BERT-base model from 78.3 to 79.6 and the RoBERTa-large model from 88.1 to 88.6. Theoretical analysis reveals that R-AT has potential gradient regularization during the training process. Furthermore, R-AT can reduce the inconsistency between training and testing of models with dropout.</abstract>
      <url hash="22a98955">2022.findings-emnlp.480</url>
      <bibkey>ni-etal-2022-r</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.480</doi>
    </paper>
    <paper id="481">
      <title>Multi-View Active Learning for Short Text Classification in User-Generated Data</title>
      <author><first>Payam</first><last>Karisani</last></author>
      <author><first>Negin</first><last>Karisani</last></author>
      <author><first>Li</first><last>Xiong</last></author>
      <pages>6441-6453</pages>
      <abstract>Mining user-generated data often suffers from the lack of enough labeled data, short document lengths, and the informal user language. In this paper, we propose a novel active learning model to overcome these obstacles in the tasks tailored for query phrases–e.g., detecting positive reports of natural disasters. Our model has three novelties: 1) It is the first approach to employ multi-view active learning in this domain. 2) It uses the Parzen-Rosenblatt window method to integrate the representativeness measure into multi-view active learning. 3) It employs a query-by-committee strategy, based on the agreement between predictors, to address the usually noisy language of the documents in this domain. We evaluate our model in four publicly available Twitter datasets with distinctly different applications. We also compare our model with a wide range of baselines including those with multiple classifiers. The experiments testify that our model is highly consistent and outperforms existing models.</abstract>
      <url hash="bf509024">2022.findings-emnlp.481</url>
      <bibkey>karisani-etal-2022-multi</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.481</doi>
    </paper>
    <paper id="482">
      <title>Forging Multiple Training Objectives for Pre-trained Language Models via Meta-Learning</title>
      <author><first>Hongqiu</first><last>Wu</last></author>
      <author><first>Ruixue</first><last>Ding</last></author>
      <author><first>Hai</first><last>Zhao</last></author>
      <author><first>Boli</first><last>Chen</last></author>
      <author><first>Pengjun</first><last>Xie</last></author>
      <author><first>Fei</first><last>Huang</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <pages>6454-6466</pages>
      <abstract>Multiple pre-training objectives fill the vacancy of the understanding capability of single-objective language modeling, which serves the ultimate purpose of pre-trained language models (PrLMs), generalizing well on a mass of scenarios. However, learning multiple training objectives in a single model is challenging due to the unknown relative significance as well as the potential contrariety between them. Empirical studies have shown that the current objective sampling in an ad-hoc manual setting makes the learned language representation barely converge to the desired optimum. Thus, we propose <i>MOMETAS</i>, a novel adaptive sampler based on meta-learning, which learns the latent sampling pattern on arbitrary pre-training objectives. Such a design is lightweight with negligible additional training overhead. To validate our approach, we adopt five objectives and conduct continual pre-training with BERT-base and BERT-large models, where MOMETAS demonstrates universal performance gain over other rule-based sampling strategies on 14 natural language processing tasks.</abstract>
      <url hash="a37641d3">2022.findings-emnlp.482</url>
      <bibkey>wu-etal-2022-forging</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.482</doi>
    </paper>
    <paper id="483">
      <title><fixed-case>C</fixed-case>on<fixed-case>G</fixed-case>en: Unsupervised Control and Generalization Distillation For Sentence Representation</title>
      <author><first>Peerat</first><last>Limkonchotiwat</last></author>
      <author><first>Wuttikorn</first><last>Ponwitayarat</last></author>
      <author><first>Lalita</first><last>Lowphansirikul</last></author>
      <author><first>Can</first><last>Udomcharoenchaikit</last></author>
      <author><first>Ekapol</first><last>Chuangsuwanich</last></author>
      <author><first>Sarana</first><last>Nutanong</last></author>
      <pages>6467-6480</pages>
      <abstract>Sentence representations are essential in many NLP tasks operating at the sentence level. Recently, research attention has shifted towards learning how to represent sentences without any annotations, i.e., unsupervised representation learning. Despite the benefit of training without supervised data, there is still a performance penalty compared to supervised methods. Furthermore, the supervised-unsupervised performance gap widens as we reduce the model size. In this paper, we propose an unsupervised sentence representation method to reduce the supervised-unsupervised performance gap, especially for smaller models. Utilizing the concept for knowledge distillation, we derive a distillation framework comprising two training objectives, control and generalize, called ConGen. Experiments on semantic textual similarity (STS), text classification (transfer), and natural language inference (NLI) tasks show that ConGen is on par with supervised training even on smaller models. Furthermore, our method consistently outperformed competitors on multilingual STS.The code and models are available at <url>https://github.com/KornWtp/ConGen</url>.</abstract>
      <url hash="79a9854e">2022.findings-emnlp.483</url>
      <bibkey>limkonchotiwat-etal-2022-congen</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.483</doi>
    </paper>
    <paper id="484">
      <title>Large-Scale Differentially Private <fixed-case>BERT</fixed-case></title>
      <author><first>Rohan</first><last>Anil</last></author>
      <author><first>Badih</first><last>Ghazi</last></author>
      <author><first>Vineet</first><last>Gupta</last></author>
      <author><first>Ravi</first><last>Kumar</last></author>
      <author><first>Pasin</first><last>Manurangsi</last></author>
      <pages>6481-6491</pages>
      <abstract>In this work, we study the large-scale pretraining of BERT-Large (Devlin et al., 2019) with differentially private SGD (DP-SGD). We show that combined with a careful implementation, scaling up the batch size to millions (i.e., mega-batches) improves the utility of the DP-SGD step for BERT; we also enhance the training efficiency by using an increasing batch size schedule. Our implementation builds on the recent work of Subramani et al (2020), who demonstrated that the overhead of a DP-SGD step is minimized with effective use of JAX (Bradbury et al., 2018; Frostig et al., 2018) primitives in conjunction with the XLA compiler (XLA team and collaborators, 2017). Our implementation achieves a masked language model accuracy of 60.5% at a batch size of 2M, for epsilon=5, which is a reasonable privacy setting. To put this number in perspective, non-private BERT models achieve an accuracy of ∼70%.</abstract>
      <url hash="7042d512">2022.findings-emnlp.484</url>
      <bibkey>anil-etal-2022-large</bibkey>
      <video href="2022.findings-emnlp.484.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.484</doi>
    </paper>
    <paper id="485">
      <title>Improving Zero-Shot Multilingual Translation with Universal Representations and Cross-Mapping</title>
      <author><first>Shuhao</first><last>Gu</last></author>
      <author><first>Yang</first><last>Feng</last></author>
      <pages>6492-6504</pages>
      <abstract>The many-to-many multilingual neural machine translation can translate between language pairs unseen during training, i.e., zero-shot translation. Improving zero-shot translation requires the model to learn universal representations and cross-mapping relationships to transfer the knowledge learned on the supervised directions to the zero-shot directions. In this work, we propose the state mover’s distance based on the optimal theory to model the difference of the representations output by the encoder. Then, we bridge the gap between the semantic-equivalent representations of different languages at the token level by minimizing the proposed distance to learn universal representations. Besides, we propose an agreement-based training scheme, which can help the model make consistent predictions based on the semantic-equivalent sentences to learn universal cross-mapping relationships for all translation directions. The experimental results on diverse multilingual datasets show that our method can improve consistently compared with the baseline system and other contrast methods. The analysis proves that our method can better align the semantic space and improve the prediction consistency.</abstract>
      <url hash="632c74e4">2022.findings-emnlp.485</url>
      <bibkey>gu-feng-2022-improving</bibkey>
      <video href="2022.findings-emnlp.485.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.485</doi>
    </paper>
    <paper id="486">
      <title>Controllable Fake Document Infilling for Cyber Deception</title>
      <author><first>Yibo</first><last>Hu</last></author>
      <author><first>Yu</first><last>Lin</last></author>
      <author><first>Erick</first><last>Skorupa Parolin</last></author>
      <author><first>Latifur</first><last>Khan</last></author>
      <author><first>Kevin</first><last>Hamlen</last></author>
      <pages>6505-6519</pages>
      <abstract>Recent works in cyber deception study how to deter malicious intrusion by generating multiple fake versions of a critical document to impose costs on adversaries who need to identify the correct information. However, existing approaches are context-agnostic, resulting in sub-optimal and unvaried outputs. We propose a novel context-aware model, Fake Document Infilling (FDI), by converting the problem to a controllable mask-then-infill procedure. FDI masks important concepts of varied lengths in the document, then infills a realistic but fake alternative considering both the previous and future contexts. We conduct comprehensive evaluations on technical documents and news stories. Results show that FDI outperforms the baselines in generating highly believable fakes with moderate modification to protect critical information and deceive adversaries.</abstract>
      <url hash="c739fbf7">2022.findings-emnlp.486</url>
      <bibkey>hu-etal-2022-controllable</bibkey>
      <video href="2022.findings-emnlp.486.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.486</doi>
    </paper>
    <paper id="487">
      <title>Weakly Supervised Headline Dependency Parsing</title>
      <author><first>Adrian</first><last>Benton</last></author>
      <author><first>Tianze</first><last>Shi</last></author>
      <author><first>Ozan</first><last>İrsoy</last></author>
      <author><first>Igor</first><last>Malioutov</last></author>
      <pages>6520-6535</pages>
      <abstract>English news headlines form a register with unique syntactic properties that have been documented in linguistics literature since the 1930s. However, headlines have received surprisingly little attention from the NLP syntactic parsing community. We aim to bridge this gap by providing the first news headline corpus of Universal Dependencies annotated syntactic dependency trees, which enables us to evaluate existing state-of-the-art dependency parsers on news headlines. To improve English news headline parsing accuracies, we develop a projection method to bootstrap silver training data from unlabeled news headline-article lead sentence pairs. Models trained on silver headline parses demonstrate significant improvements in performance over models trained solely on gold-annotated long-form texts. Ultimately, we find that, although projected silver training data improves parser performance across different news outlets, the improvement is moderated by constructions idiosyncratic to outlet.</abstract>
      <url hash="fb391d72">2022.findings-emnlp.487</url>
      <bibkey>benton-etal-2022-weakly</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.487</doi>
    </paper>
    <paper id="488">
      <title><fixed-case>BOOKSUM</fixed-case>: A Collection of Datasets for Long-form Narrative Summarization</title>
      <author><first>Wojciech</first><last>Kryscinski</last></author>
      <author><first>Nazneen</first><last>Rajani</last></author>
      <author><first>Divyansh</first><last>Agarwal</last></author>
      <author><first>Caiming</first><last>Xiong</last></author>
      <author><first>Dragomir</first><last>Radev</last></author>
      <pages>6536-6558</pages>
      <abstract>The majority of existing text summarization datasets include short-form source documents that lack long-range causal and temporal dependencies, and often contain strong layout and stylistic biases. While relevant, such datasets will offer limited challenges for future text summarization systems. We address these issues by introducing BOOKSUM, a collection of datasets for long-form narrative summarization. Our dataset covers documents from the literature domain, such as novels, plays and stories, and includes highly abstractive, human written summaries on three levels of granularity of increasing difficulty: paragraph-, chapter-, and book-level. The domain and structure of our dataset poses a unique set of challenges for summarization systems, which include: processing very long documents, non-trivial causal and temporal dependencies, and rich discourse structures. To facilitate future work, we trained and evaluated multiple extractive and abstractive summarization models as baselines for our dataset.</abstract>
      <url hash="31389ef6">2022.findings-emnlp.488</url>
      <bibkey>kryscinski-etal-2022-booksum</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.488</doi>
    </paper>
    <paper id="489">
      <title>Not All Errors are Equal: Learning Text Generation Metrics using Stratified Error Synthesis</title>
      <author><first>Wenda</first><last>Xu</last></author>
      <author><first>Yi-Lin</first><last>Tuan</last></author>
      <author><first>Yujie</first><last>Lu</last></author>
      <author><first>Michael</first><last>Saxon</last></author>
      <author><first>Lei</first><last>Li</last></author>
      <author><first>William Yang</first><last>Wang</last></author>
      <pages>6559-6574</pages>
      <abstract>Is it possible to build a general and automatic natural language generation (NLG) evaluation metric? Existing learned metrics either perform unsatisfactorily or are restricted to tasks where large human rating data is already available. We introduce SESCORE, a model-based metric that is highly correlated with human judgements without requiring human annotation, by utilizing a novel, iterative error synthesis and severity scoring pipeline. This pipeline applies a series of plausible errors to raw text and assigns severity labels by simulating human judgements with entailment. We evaluate SESCORE against existing metrics by comparing how their scores correlate with human ratings. SESCORE outperforms all prior unsupervised metrics on multiple diverse NLG tasks including machine translation, image captioning, and WebNLG text generation. For WMT 20/21En-De and Zh-En, SESCORE improve the average Kendall correlation with human judgement from 0.154 to 0.195. SESCORE even achieves comparable performance to the best supervised metric COMET, despite receiving no human annotated training data.</abstract>
      <url hash="4b461a23">2022.findings-emnlp.489</url>
      <bibkey>xu-etal-2022-errors</bibkey>
      <video href="2022.findings-emnlp.489.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.489</doi>
    </paper>
    <paper id="490">
      <title>Summarization as Indirect Supervision for Relation Extraction</title>
      <author><first>Keming</first><last>Lu</last></author>
      <author><first>I-Hung</first><last>Hsu</last></author>
      <author><first>Wenxuan</first><last>Zhou</last></author>
      <author><first>Mingyu Derek</first><last>Ma</last></author>
      <author><first>Muhao</first><last>Chen</last></author>
      <pages>6575-6594</pages>
      <abstract>Relation extraction (RE) models have been challenged by their reliance on training data with expensive annotations. Considering that summarization tasks aim at acquiring concise expressions of synoptical information from the longer context, these tasks naturally align with the objective of RE, i.e., extracting a kind of synoptical information that describes the relation of entity mentions. We present SuRE, which converts RE into a summarization formulation. SuRE leads to more precise and resource-efficient RE based on indirect supervision from summarization tasks. To achieve this goal, we develop sentence and relation conversion techniques that essentially bridge the formulation of summarization and RE tasks. We also incorporate constraint decoding techniques with Trie scoring to further enhance summarization-based RE with robust inference. Experiments on three RE datasets demonstrate the effectiveness of SuRE in both full-dataset and low-resource settings, showing that summarization is a promising source of indirect supervision signals to improve RE models.</abstract>
      <url hash="6f459aec">2022.findings-emnlp.490</url>
      <bibkey>lu-etal-2022-summarization</bibkey>
      <video href="2022.findings-emnlp.490.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.490</doi>
    </paper>
    <paper id="491">
      <title><fixed-case>DIGAT</fixed-case>: Modeling News Recommendation with Dual-Graph Interaction</title>
      <author><first>Zhiming</first><last>Mao</last></author>
      <author><first>Jian</first><last>Li</last></author>
      <author><first>Hongru</first><last>Wang</last></author>
      <author><first>Xingshan</first><last>Zeng</last></author>
      <author><first>Kam-Fai</first><last>Wong</last></author>
      <pages>6595-6607</pages>
      <abstract>News recommendation (NR) is essential for online news services. Existing NR methods typically adopt a news-user representation learning framework, facing two potential limitations. First, in news encoder, single candidate news encoding suffers from an insufficient semantic information problem. Second, existing graph-based NR methods are promising but lack effective news-user feature interaction, rendering the graph-based recommendation suboptimal. To overcome these limitations, we propose dual-interactive graph attention networks (DIGAT) consisting of news- and user-graph channels. In the news-graph channel, we enrich the semantics of single candidate news by incorporating the semantically relevant news information with a semantic-augmented graph (SAG). In the user-graph channel, multi-level user interests are represented with a news-topic graph. Most notably, we design a dual-graph interaction process to perform effective feature interaction between the news and user graphs, which facilitates accurate news-user representation matching. Experiment results on the benchmark dataset MIND show that DIGAT outperforms existing news recommendation methods. Further ablation studies and analyses validate the effectiveness of (1) semantic-augmented news graph modeling and (2) dual-graph interaction.</abstract>
      <url hash="91b121ab">2022.findings-emnlp.491</url>
      <bibkey>mao-etal-2022-digat</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.491</doi>
    </paper>
    <paper id="492">
      <title><fixed-case>SMASH</fixed-case>: Improving <fixed-case>SMA</fixed-case>ll Language Models’ Few-<fixed-case>SH</fixed-case>ot Ability with Prompt-Based Distillation</title>
      <author><first>Yueqian</first><last>Wang</last></author>
      <author><first>Chang</first><last>Liu</last></author>
      <author><first>Kai</first><last>Chen</last></author>
      <author><first>Xi</first><last>Wang</last></author>
      <author><first>Dongyan</first><last>Zhao</last></author>
      <pages>6608-6619</pages>
      <abstract>Large-scale language models coupled with prompts have shown remarkable performance on few-shot learning. However, through systematic experiments, we find that the few-shot performance of small language models is poor, and using prompts on them brings fewer improvements than on larger ones. In this paper, we propose SMASH, an approach to improve SMAll language models’ few-SHot ability by training on intermediate tasks before prompt-based fine-tuning on downstream tasks. We design intermediate tasks for sentence-pair tasks and sentiment classification tasks by creating training examples with prompt templates similar to downstream tasks using sentences sampled from a large-scale unsupervised corpus, and apply knowledge distillation to distill from outputs of larger pre-trained models as the training objective. We conduct extensive experiments and show that SMASH can make a 6-layer DistilRoBRETa-base achieve comparable performance on few-shot datasets with a 12-layer RoBERTa-base at a low cost.</abstract>
      <url hash="6012c340">2022.findings-emnlp.492</url>
      <bibkey>wang-etal-2022-smash</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.492</doi>
    </paper>
    <paper id="493">
      <title>Consecutive Question Generation via Dynamic Multitask Learning</title>
      <author><first>Yunji</first><last>Li</last></author>
      <author><first>Sujian</first><last>Li</last></author>
      <author><first>Xing</first><last>Shi</last></author>
      <pages>6620-6635</pages>
      <abstract>In this paper, we propose the task of consecutive question generation (CQG), which generates a set of logically related question-answer pairs to understand a whole passage, with a comprehensive consideration of the aspects including accuracy, coverage, and informativeness. To achieve this, we first examine the four key elements of CQG, i.e., question, answer, rationale, and context history, and propose a novel dynamic multitask framework with one main task generating a question-answer pair, and four auxiliary tasks generating other elements. It directly helps the model generate good questions through both joint training and self-reranking. At the same time, to fully explore the worth-asking information in a given passage, we make use of the reranking losses to sample the rationales and search for the best question series globally. Finally, we measure our strategy by QA data augmentation and manual evaluation, as well as a novel application of generated question-answer pairs on DocNLI. We prove that our strategy can improve question generation significantly and benefit multiple related NLP tasks.</abstract>
      <url hash="ecdf373f">2022.findings-emnlp.493</url>
      <bibkey>li-etal-2022-consecutive</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.493</doi>
    </paper>
    <paper id="494">
      <title>Subword Segmental Language Modelling for Nguni Languages</title>
      <author><first>Francois</first><last>Meyer</last></author>
      <author><first>Jan</first><last>Buys</last></author>
      <pages>6636-6649</pages>
      <abstract>Subwords have become the standard units of text in NLP, enabling efficient open-vocabulary models. With algorithms like byte-pair encoding (BPE), subword segmentation is viewed as a preprocessing step applied to the corpus before training. This can lead to sub-optimal segmentations for low-resource languages with complex morphologies. We propose a subword segmental language model (SSLM) that learns how to segment words while being trained for autoregressive language modelling. By unifying subword segmentation and language modelling, our model learns subwords that optimise LM performance. We train our model on the 4 Nguni languages of South Africa. These are low-resource agglutinative languages, so subword information is critical. As an LM, SSLM outperforms existing approaches such as BPE-based models on average across the 4 languages. Furthermore, it outperforms standard subword segmenters on unsupervised morphological segmentation. We also train our model as a word-level sequence model, resulting in an unsupervised morphological segmenter that outperforms existing methods by a large margin for all 4 languages. Our results show that learning subword segmentation is an effective alternative to existing subword segmenters, enabling the model to discover morpheme-like subwords that improve its LM capabilities.</abstract>
      <url hash="0c2a6d6f">2022.findings-emnlp.494</url>
      <bibkey>meyer-buys-2022-subword</bibkey>
      <video href="2022.findings-emnlp.494.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.494</doi>
    </paper>
    <paper id="495">
      <title>Towards Robust Visual Question Answering: Making the Most of Biased Samples via Contrastive Learning</title>
      <author><first>Qingyi</first><last>Si</last></author>
      <author><first>Yuanxin</first><last>Liu</last></author>
      <author><first>Fandong</first><last>Meng</last></author>
      <author><first>Zheng</first><last>Lin</last></author>
      <author><first>Peng</first><last>Fu</last></author>
      <author><first>Yanan</first><last>Cao</last></author>
      <author><first>Weiping</first><last>Wang</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>6650-6662</pages>
      <abstract>Models for Visual Question Answering (VQA) often rely on the spurious correlations, i.e., the language priors, that appear in the biased samples of training set, which make them brittle against the out-of-distribution (OOD) test data. Recent methods have achieved promising progress in overcoming this problem by reducing the impact of biased samples on model training. However, these models reveal a trade-off that the improvements on OOD data severely sacrifice the performance on the in-distribution (ID) data (which is dominated by the biased samples). Therefore, we propose a novel contrastive learning approach, MMBS, for building robust VQA models by Making the Most of Biased Samples. Specifically, we construct positive samples for contrastive learning by eliminating the information related to spurious correlation from the original training samples and explore several strategies to use the constructed positive samples for training. Instead of undermining the importance of biased samples in model training, our approach precisely exploits the biased samples for unbiased information that contributes to reasoning. The proposed method is compatible with various VQA backbones. We validate our contributions by achieving competitive performance on the OOD dataset VQA-CP v2 while preserving robust performance on the ID dataset VQA v2.</abstract>
      <url hash="8af296cf">2022.findings-emnlp.495</url>
      <bibkey>si-etal-2022-towards</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.495</doi>
    </paper>
    <paper id="496">
      <title><fixed-case>P</fixed-case>3<fixed-case>LM</fixed-case>: Probabilistically Permuted Prophet Language Modeling for Generative Pre-Training</title>
      <author><first>Junwei</first><last>Bao</last></author>
      <author><first>Yifan</first><last>Wang</last></author>
      <author><first>Ying</first><last>Jiangyong</last></author>
      <author><first>Yeyun</first><last>Gong</last></author>
      <author><first>Jing</first><last>Zhao</last></author>
      <author><first>Youzheng</first><last>Wu</last></author>
      <author><first>Xiaodong</first><last>He</last></author>
      <pages>6663-6675</pages>
      <abstract>Conventional autoregressive left-to-right (L2R) sequence generation faces two issues during decoding: limited to unidirectional target sequence modeling, and constrained on strong local dependencies. To address the aforementioned problem, we propose P3LM, a probabilistically permuted prophet language model, which strengthens the modeling of bidirectional information and long token dependencies for sequence generation. Specifically, P3LM learns to generate tokens in permuted order upon an order-aware transformer decoder, as well as to generate the corresponding future N tokens with a multi-stream attention mechanism. Extensive experiments are conducted on the GLGE benchmark, which includes four datasets for summarization, two for question generation, one for conversational question answering, and one for dialog response generation, where P3LM achieves state-of-the-art results compared with strong publicly available generative pre-training methods.</abstract>
      <url hash="e7ea22b4">2022.findings-emnlp.496</url>
      <bibkey>bao-etal-2022-p3lm</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.496</doi>
    </paper>
    <paper id="497">
      <title>Holistic Sentence Embeddings for Better Out-of-Distribution Detection</title>
      <author><first>Sishuo</first><last>Chen</last></author>
      <author><first>Xiaohan</first><last>Bi</last></author>
      <author><first>Rundong</first><last>Gao</last></author>
      <author><first>Xu</first><last>Sun</last></author>
      <pages>6676-6686</pages>
      <abstract>Detecting out-of-distribution (OOD) instances is significant for the safe deployment of NLP models. Among recent textual OOD detection works based on pretrained language models (PLMs), distance-based methods have shown superior performance. However, they estimate sample distance scores in the last-layer CLS embedding space and thus do not make full use of linguistic information underlying in PLMs. To address the issue, we propose to boost OOD detection by deriving more holistic sentence embeddings. On the basis of the observations that token averaging and layer combination contribute to improving OOD detection, we propose a simple embedding approach named Avg-Avg, which averages all token representations from each intermediate layer as the sentence embedding and significantly surpasses the state-of-the-art on a comprehensive suite of benchmarks by a 9.33% FAR95 margin. Furthermore, our analysis demonstrates that it indeed helps preserve general linguistic knowledge in fine-tuned PLMs and substantially benefits detecting background shifts. The simple yet effective embedding method can be applied to fine-tuned PLMs with negligible extra costs, providing a free gain in OOD detection. Our code is available at <url>https://github.com/lancopku/Avg-Avg</url>.</abstract>
      <url hash="fc5160bb">2022.findings-emnlp.497</url>
      <bibkey>chen-etal-2022-holistic</bibkey>
      <video href="2022.findings-emnlp.497.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.497</doi>
    </paper>
    <paper id="498">
      <title><fixed-case>M</fixed-case>u<fixed-case>GER</fixed-case>2: Multi-Granularity Evidence Retrieval and Reasoning for Hybrid Question Answering</title>
      <author><first>Yingyao</first><last>Wang</last></author>
      <author><first>Junwei</first><last>Bao</last></author>
      <author><first>Chaoqun</first><last>Duan</last></author>
      <author><first>Youzheng</first><last>Wu</last></author>
      <author><first>Xiaodong</first><last>He</last></author>
      <author><first>Tiejun</first><last>Zhao</last></author>
      <pages>6687-6697</pages>
      <abstract>Hybrid question answering (HQA) aims to answer questions over heterogeneous data, including tables and passages linked to table cells. The heterogeneous data can provide different granularity evidence to HQA models, e.t., column, row, cell, and link. Conventional HQA models usually retrieve coarse- or fine-grained evidence to reason the answer. Through comparison, we find that coarse-grained evidence is easier to retrieve but contributes less to the reasoner, while fine-grained evidence is the opposite. To preserve the advantage and eliminate the disadvantage of different granularity evidence, we propose MuGER2, a Multi-Granularity Evidence Retrieval and Reasoning approach. In evidence retrieval, a unified retriever is designed to learn the multi-granularity evidence from the heterogeneous data. In answer reasoning, an evidence selector is proposed to navigate the fine-grained evidence for the answer reader based on the learned multi-granularity evidence. Experiment results on the HybridQA dataset show that MuGER2 significantly boosts the HQA performance. Further ablation analysis verifies the effectiveness of both the retrieval and reasoning designs.</abstract>
      <url hash="15cef612">2022.findings-emnlp.498</url>
      <bibkey>wang-etal-2022-muger2</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.498</doi>
    </paper>
    <paper id="499">
      <title><fixed-case>E</fixed-case>ntity<fixed-case>CS</fixed-case>: Improving Zero-Shot Cross-lingual Transfer with Entity-Centric Code Switching</title>
      <author><first>Chenxi</first><last>Whitehouse</last></author>
      <author><first>Fenia</first><last>Christopoulou</last></author>
      <author><first>Ignacio</first><last>Iacobacci</last></author>
      <pages>6698-6714</pages>
      <abstract>Accurate alignment between languages is fundamental for improving cross-lingual pre-trained language models (XLMs). Motivated by the natural phenomenon of code-switching (CS) in multilingual speakers, CS has been used as an effective data augmentation method that offers language alignment at word- or phrase-level, in contrast to sentence-level via parallel instances. Existing approaches either use dictionaries or parallel sentences with word-alignment to generate CS data by randomly switching words in a sentence. However, such methods can be suboptimal as dictionaries disregard semantics, and syntax might become invalid after random word switching. In this work, we propose EntityCS, a method that focuses on Entity-level Code-Switching to capture fine-grained cross-lingual semantics without corrupting syntax. We use Wikidata and the English Wikipedia to construct an entity-centric CS corpus by switching entities to their counterparts in other languages. We further propose entity-oriented masking strategies during intermediate model training on the EntityCS corpus for improving entity prediction. Evaluation of the trained models on four entity-centric downstream tasks shows consistent improvements over the baseline with a notable increase of 10% in Fact Retrieval. We release the corpus and models to assist research on code-switching and enriching XLMs with external knowledge.</abstract>
      <url hash="19f17a42">2022.findings-emnlp.499</url>
      <bibkey>whitehouse-etal-2022-entitycs</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.499</doi>
    </paper>
    <paper id="500">
      <title><fixed-case>MBTI</fixed-case> Personality Prediction for Fictional Characters Using Movie Scripts</title>
      <author><first>Yisi</first><last>Sang</last></author>
      <author><first>Xiangyang</first><last>Mou</last></author>
      <author><first>Mo</first><last>Yu</last></author>
      <author><first>Dakuo</first><last>Wang</last></author>
      <author><first>Jing</first><last>Li</last></author>
      <author><first>Jeffrey</first><last>Stanton</last></author>
      <pages>6715-6724</pages>
      <abstract>An NLP model that understands stories should be able to understand the characters in them. To support the development of neural models for this purpose, we construct a benchmark, Story2Personality. The task is to predict a movie character’s MBTI or Big 5 personality types based on the narratives of the character. Experiments show that our task is challenging for the existing text classification models, as none is able to largely outperform random guesses. We further proposed a multi-view model for personality prediction using both verbal and non-verbal descriptions, which gives improvement compared to using only verbal descriptions. The uniqueness and challenges in our dataset call for the development of narrative comprehension techniques from the perspective of understanding characters.</abstract>
      <url hash="d0b38030">2022.findings-emnlp.500</url>
      <bibkey>sang-etal-2022-mbti</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.500</doi>
    </paper>
    <paper id="501">
      <title>A Simple and Strong Baseline for End-to-End Neural <fixed-case>RST</fixed-case>-style Discourse Parsing</title>
      <author><first>Naoki</first><last>Kobayashi</last></author>
      <author><first>Tsutomu</first><last>Hirao</last></author>
      <author><first>Hidetaka</first><last>Kamigaito</last></author>
      <author><first>Manabu</first><last>Okumura</last></author>
      <author><first>Masaaki</first><last>Nagata</last></author>
      <pages>6725-6737</pages>
      <abstract>To promote and further develop RST-style discourse parsing models, we need a strong baseline that can be regarded as a reference for reporting reliable experimental results. This paper explores a strong baseline by integrating existing simple parsing strategies, top-down and bottom-up, with various transformer-based pre-trained language models. The experimental results obtained from two benchmark datasets demonstrate that the parsing performance strongly relies on the pre-trained language models rather than the parsing strategies. In particular, the bottom-up parser achieves large performance gains compared to the current best parser when employing DeBERTa.We further reveal that language models with a span-masking scheme especially boost the parsing performance through our analysis within intra- and multi-sentential parsing, and nuclearity prediction.</abstract>
      <url hash="730172ff">2022.findings-emnlp.501</url>
      <bibkey>kobayashi-etal-2022-simple</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.501</doi>
    </paper>
    <paper id="502">
      <title>Probing for Constituency Structure in Neural Language Models</title>
      <author><first>David</first><last>Arps</last></author>
      <author><first>Younes</first><last>Samih</last></author>
      <author><first>Laura</first><last>Kallmeyer</last></author>
      <author><first>Hassan</first><last>Sajjad</last></author>
      <pages>6738-6757</pages>
      <abstract>In this paper, we investigate to which extent contextual neural language models (LMs) implicitly learn syntactic structure. More concretely, we focus on constituent structure as represented in the Penn Treebank (PTB). Using standard probing techniques based on diagnostic classifiers, we assess the accuracy of representing constituents of different categories within the neuron activations of a LM such as RoBERTa. In order to make sure that our probe focuses on syntactic knowledge and not on implicit semantic generalizations, we also experiment on a PTB version that is obtained by randomly replacing constituents with each other while keeping syntactic structure, i.e., a semantically ill-formed but syntactically well-formed version of the PTB. We find that 4 pretrained transfomer LMs obtain high performance on our probing tasks even on manipulated data, suggesting that semantic and syntactic knowledge in their representations can be separated and that constituency information is in fact learned by the LM. Moreover, we show that a complete constituency tree can be linearly separated from LM representations.</abstract>
      <url hash="18775c1a">2022.findings-emnlp.502</url>
      <bibkey>arps-etal-2022-probing</bibkey>
      <video href="2022.findings-emnlp.502.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.502</doi>
    </paper>
    <paper id="503">
      <title>Table-To-Text generation and pre-training with <fixed-case>T</fixed-case>ab<fixed-case>T</fixed-case>5</title>
      <author><first>Ewa</first><last>Andrejczuk</last></author>
      <author><first>Julian</first><last>Eisenschlos</last></author>
      <author><first>Francesco</first><last>Piccinno</last></author>
      <author><first>Syrine</first><last>Krichene</last></author>
      <author><first>Yasemin</first><last>Altun</last></author>
      <pages>6758-6766</pages>
      <abstract>Encoder-only transformer models have been successfully applied to different table understanding tasks, as in TAPAS. A major limitation of these architectures is that they are constrained to classification-like tasks such as cell selection or entailment detection. We present TabT5, an encoder-decoder model that generates natural language text based on tables and textual inputs. TabT5 overcomes the encoder-only limitation by incorporating a decoder component and leverages the input structure with table specific embeddings and pre-training. TabT5 achieves new state-of-the-art results on several domains, including spreadsheet formula prediction with a 15% increase in sequence accuracy, QA with a 2.5% increase in sequence accuracy and data-to-text generation with a 2.5% increase in BLEU.</abstract>
      <url hash="b098c8b6">2022.findings-emnlp.503</url>
      <bibkey>andrejczuk-etal-2022-table</bibkey>
      <video href="2022.findings-emnlp.503.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.503</doi>
    </paper>
    <paper id="504">
      <title>A <fixed-case>POMDP</fixed-case> Dialogue Policy with 3-way Grounding and Adaptive <fixed-case>S</fixed-case>ensing for Learning through Communication</title>
      <author><first>Maryam</first><last>Zare</last></author>
      <author><first>Alan</first><last>Wagner</last></author>
      <author><first>Rebecca</first><last>Passonneau</last></author>
      <pages>6767-6780</pages>
      <abstract>Agents to assist with rescue, surgery, and similar activities could collaborate better with humans if they could learn new strategic behaviors through communication. We introduce a novel POMDP dialogue policy for learning from people. The policy has 3-way grounding of language in the shared physical context, the dialogue context, and persistent knowledge. It can learn distinct but related games, and can continue learning across dialogues for complex games. A novel sensing component supports adaptation to information-sharing differences across people. The single policy performs better than oracle policies customized to specific games and information behavior.</abstract>
      <url hash="f4e96928">2022.findings-emnlp.504</url>
      <bibkey>zare-etal-2022-pomdp</bibkey>
      <video href="2022.findings-emnlp.504.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.504</doi>
    </paper>
    <paper id="505">
      <title><fixed-case>P</fixed-case>a<fixed-case>C</fixed-case>o: Preconditions Attributed to Commonsense Knowledge</title>
      <author><first>Ehsan</first><last>Qasemi</last></author>
      <author><first>Filip</first><last>Ilievski</last></author>
      <author><first>Muhao</first><last>Chen</last></author>
      <author><first>Pedro</first><last>Szekely</last></author>
      <pages>6781-6796</pages>
      <abstract>Humans can seamlessly reason with circumstantial preconditions of commonsense knowledge. We understand that a glass is used for drinking water, unless the glass is broken or the water is toxic. Despite state-of-the-art (SOTA) language models’ (LMs) impressive performance on inferring commonsense knowledge, it is unclear whether they understand the circumstantial preconditions. To address this gap, we propose a novel challenge of reasoning with circumstantial preconditions. We collect a dataset, called PaCo, consisting of 12.4 thousand preconditions of commonsense statements expressed in natural language. Based on this dataset, we create three canonical evaluation tasks and use them to examine the capability of existing LMs to understand situational preconditions. Our results reveal a 10-30% gap between machine and human performance on our tasks, which shows that reasoning with preconditions is an open challenge.</abstract>
      <url hash="52207a70">2022.findings-emnlp.505</url>
      <bibkey>qasemi-etal-2022-paco</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.505</doi>
    </paper>
    <paper id="506">
      <title>Improving Few-Shot Domain Transfer for Named Entity Disambiguation with Pattern Exploitation</title>
      <author><first>Philip</first><last>Blair</last></author>
      <author><first>Kfir</first><last>Bar</last></author>
      <pages>6797-6810</pages>
      <abstract>Named entity disambiguation (NED) is a critical subtask of entity linking, which seeks to connect knowledge base entities with textual mentions of those entities. Naturally, the performance of a model depends on the domain it was trained on; thus, reducing the amount of data required to train models is advantageous. In this work, we leverage recent research on pattern exploitation for NED and explore whether it can reduce the amount of data required for domain adaptation by reformulating the disambiguation task as a masked language modeling problem. Using ADAPET (Tam et al., 2021), which implements a new approach for few-shot learning using fine-tuned transformer-based language models, we produce an NED model which yields, without any sacrifice of in-domain accuracy, a 7% improvement in zero-shot cross-domain performance as evaluated on NEDMed, a new NED dataset of mental health news which we release with this work.</abstract>
      <url hash="f1caf3ad">2022.findings-emnlp.506</url>
      <bibkey>blair-bar-2022-improving</bibkey>
      <video href="2022.findings-emnlp.506.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.506</doi>
    </paper>
    <paper id="507">
      <title>Capturing Topic Framing via Masked Language Modeling</title>
      <author><first>Xiaobo</first><last>Guo</last></author>
      <author><first>Weicheng</first><last>Ma</last></author>
      <author><first>Soroush</first><last>Vosoughi</last></author>
      <pages>6811-6825</pages>
      <abstract>Differential framing of issues can lead to divergent world views on important issues. This is especially true in domains where the information presented can reach a large audience, such as traditional and social media. Scalable and reliable measurement of such differential framing is an important first step in addressing them. In this work, based on the intuition that framing affects the tone and word choices in written language, we propose a framework for modeling the differential framing of issues through masked token prediction via large-scale fine-tuned language models (LMs). Specifically, we explore three key factors for our framework: 1) prompt generation methods for the masked token prediction; 2) methods for normalizing the output of fine-tuned LMs; 3) robustness to the choice of pre-trained LMs used for fine-tuning. Through experiments on a dataset of articles from traditional media outlets covering five diverse and politically polarized topics, we show that our framework can capture differential framing of these topics with high reliability.</abstract>
      <url hash="e04a8b00">2022.findings-emnlp.507</url>
      <bibkey>guo-etal-2022-capturing</bibkey>
      <video href="2022.findings-emnlp.507.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.507</doi>
    </paper>
    <paper id="508">
      <title><fixed-case>WANLI</fixed-case>: Worker and <fixed-case>AI</fixed-case> Collaboration for Natural Language Inference Dataset Creation</title>
      <author><first>Alisa</first><last>Liu</last></author>
      <author><first>Swabha</first><last>Swayamdipta</last></author>
      <author><first>Noah A.</first><last>Smith</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <pages>6826-6847</pages>
      <abstract>A recurring challenge of crowdsourcing NLP datasets at scale is that human writers often rely on repetitive patterns when crafting examples, leading to a lack of linguistic diversity. We introduce a novel approach for dataset creation based on worker and AI collaboration, which brings together the generative strength of language models and the evaluative strength of humans. Starting with an existing dataset, MultiNLI for natural language inference (NLI), our approach uses dataset cartography to automatically identify examples that demonstrate challenging reasoning patterns, and instructs GPT-3 to compose new examples with similar patterns. Machine generated examples are then automatically filtered, and finally revised and labeled by human crowdworkers. The resulting dataset, WANLI, consists of 107,885 NLI examples and presents unique empirical strengths over existing NLI datasets. Remarkably, training a model on WANLI improves performance on eight out-of-domain test sets we consider, including by 11% on HANS and 9% on Adversarial NLI, compared to training on the 4x larger MultiNLI. Moreover, it continues to be more effective than MultiNLI augmented with other NLI datasets. Our results demonstrate the promise of leveraging natural language generation techniques and re-imagining the role of humans in the dataset creation process.</abstract>
      <url hash="adc07b7b">2022.findings-emnlp.508</url>
      <bibkey>liu-etal-2022-wanli</bibkey>
      <video href="2022.findings-emnlp.508.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.508</doi>
    </paper>
    <paper id="509">
      <title>Sequentially Controlled Text Generation</title>
      <author><first>Alexander</first><last>Spangher</last></author>
      <author><first>Yao</first><last>Ming</last></author>
      <author><first>Xinyu</first><last>Hua</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <pages>6848-6866</pages>
      <abstract>While GPT-2 generates sentences that are remarkably human-like, longer documents can ramble and do not follow human-like writing structure. We study the problem of imposing structure on long-range text. We propose a novel controlled text generation task, sequentially controlled text generation, and identify a dataset, NewsDiscourse as a starting point for this task. We develop a sequential controlled text generation pipeline with generation and editing. We test different degrees of structural awareness and show that, in general, more structural awareness results in higher control- accuracy, grammaticality, coherency and topicality, approaching human-level writing performance.</abstract>
      <url hash="ce9ef763">2022.findings-emnlp.509</url>
      <bibkey>spangher-etal-2022-sequentially</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.509</doi>
    </paper>
    <paper id="510">
      <title>Revisiting the Roles of “Text” in Text Games</title>
      <author><first>Yi</first><last>Gu</last></author>
      <author><first>Shunyu</first><last>Yao</last></author>
      <author><first>Chuang</first><last>Gan</last></author>
      <author><first>Josh</first><last>Tenenbaum</last></author>
      <author><first>Mo</first><last>Yu</last></author>
      <pages>6867-6876</pages>
      <abstract>Text games present opportunities for natural language understanding (NLU) methods to tackle reinforcement learning (RL) challenges. However, recent work has questioned the necessity of NLU by showing random text hashes could perform decently. In this paper, we pursue a fine-grained investigation into the roles of text in the face of different RL challenges, and reconcile that semantic and non-semantic language representations could be complementary rather than contrasting. Concretely, we propose a simple scheme to extract relevant contextual information into an approximate state hash as extra input for an RNN-based text agent. Such a lightweight plug-in achieves competitive performance with state-of-the-art text agents using advanced NLU techniques such as knowledge graph and passage retrieval, suggesting non-NLU methods might suffice to tackle the challenge of partial observability. However, if we remove RNN encoders and use approximate or even ground-truth state hash alone, the model performs miserably, which confirms the importance of semantic function approximation to tackle the challenge of combinatorially large observation and action spaces. Our findings and analysis provide new insights for designing better text game task setups and agents.</abstract>
      <url hash="5a37183a">2022.findings-emnlp.510</url>
      <bibkey>gu-etal-2022-revisiting</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.510</doi>
    </paper>
    <paper id="511">
      <title><fixed-case>FPT</fixed-case>: Improving Prompt Tuning Efficiency via Progressive Training</title>
      <author><first>Yufei</first><last>Huang</last></author>
      <author><first>Yujia</first><last>Qin</last></author>
      <author><first>Huadong</first><last>Wang</last></author>
      <author><first>Yichun</first><last>Yin</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <pages>6877-6887</pages>
      <abstract>Recently, prompt tuning (PT) has gained increasing attention as a parameter-efficient way of tuning pre-trained language models (PLMs). Despite extensively reducing the number of tunable parameters and achieving satisfying performance, PT is training-inefficient due to its slow convergence. To improve PT’s training efficiency, we first make some novel observations about the prompt transferability of “partial PLMs”, which are defined by compressing a PLM in depth or width. We observe that the soft prompts learned by different partial PLMs of various sizes are similar in the parameter space, implying that these soft prompts could potentially be transferred among partial PLMs. Inspired by these observations, we propose Fast Prompt Tuning (FPT), which starts by conducting PT using a small-scale partial PLM, and then progressively expands its depth and width until the full-model size. After each expansion, we recycle the previously learned soft prompts as initialization for the enlarged partial PLM and then proceed PT. We demonstrate the feasibility of FPT on 5 tasks and show that FPT could save over 30% training computations while achieving comparable performance. The codes are publicly available at <url>https://github.com/thunlp/FastPromptTuning</url>.</abstract>
      <url hash="5db485af">2022.findings-emnlp.511</url>
      <bibkey>huang-etal-2022-fpt</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.511</doi>
    </paper>
    <paper id="512">
      <title>Prompt-learning for Fine-grained Entity Typing</title>
      <author><first>Ning</first><last>Ding</last></author>
      <author><first>Yulin</first><last>Chen</last></author>
      <author><first>Xu</first><last>Han</last></author>
      <author><first>Guangwei</first><last>Xu</last></author>
      <author><first>Xiaobin</first><last>Wang</last></author>
      <author><first>Pengjun</first><last>Xie</last></author>
      <author><first>Haitao</first><last>Zheng</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <author><first>Hong-Gee</first><last>Kim</last></author>
      <pages>6888-6901</pages>
      <abstract>As an effective approach to adapting pre-trained language models (PLMs) for specific tasks, prompt-learning has recently attracted much attention from researchers. By using cloze-style language prompts to stimulate the versatile knowledge of PLMs, prompt-learning can achieve promising results on a series of NLP tasks, such as natural language inference, sentiment classification, and knowledge probing. In this work, we investigate the application of prompt-learning on fine-grained entity typing in fully supervised, few-shot, and zero-shot scenarios. We first develop a simple and effective prompt-learning pipeline by constructing entity-oriented verbalizers and templates and conducting masked language modeling. Further, to tackle the zero-shot regime, we propose a self-supervised strategy that carries out distribution-level optimization in prompt-learning to automatically summarize the information of entity types. Extensive experiments on four fine-grained entity typing benchmarks under fully supervised, few-shot, and zero-shot settings show the effectiveness of the prompt-learning paradigm and further make a powerful alternative to vanilla fine-tuning.</abstract>
      <url hash="0f664f68">2022.findings-emnlp.512</url>
      <bibkey>ding-etal-2022-prompt</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.512</doi>
    </paper>
    <paper id="513">
      <title><fixed-case>T</fixed-case>rans<fixed-case>LIST</fixed-case>: A Transformer-Based Linguistically Informed <fixed-case>S</fixed-case>anskrit Tokenizer</title>
      <author><first>Jivnesh</first><last>Sandhan</last></author>
      <author><first>Rathin</first><last>Singha</last></author>
      <author><first>Narein</first><last>Rao</last></author>
      <author><first>Suvendu</first><last>Samanta</last></author>
      <author><first>Laxmidhar</first><last>Behera</last></author>
      <author><first>Pawan</first><last>Goyal</last></author>
      <pages>6902-6912</pages>
      <abstract>Sanskrit Word Segmentation (SWS) is essential in making digitized texts available and in deploying downstream tasks. It is, however, non-trivial because of the sandhi phenomenon that modifies the characters at the word boundaries, and needs special treatment. Existing lexicon driven approaches for SWS make use of Sanskrit Heritage Reader, a lexicon-driven shallow parser, to generate the complete candidate solution space, over which various methods are applied to produce the most valid solution. However, these approaches fail while encountering out-of-vocabulary tokens. On the other hand, purely engineering methods for SWS have made use of recent advances in deep learning, but cannot make use of the latent word information on availability. To mitigate the shortcomings of both families of approaches, we propose Transformer based Linguistically Informed Sanskrit Tokenizer (TransLIST) consisting of (1) a module that encodes the character input along with latent-word information, which takes into account the sandhi phenomenon specific to SWS and is apt to work with partial or no candidate solutions, (2) a novel soft-masked attention to prioritize potential candidate words and (3) a novel path ranking algorithm to rectify the corrupted predictions. Experiments on the benchmark datasets for SWS show that TransLIST outperforms the current state-of-the-art system by an average 7.2 points absolute gain in terms of perfect match (PM) metric.</abstract>
      <url hash="00ba624b">2022.findings-emnlp.513</url>
      <bibkey>sandhan-etal-2022-translist</bibkey>
      <video href="2022.findings-emnlp.513.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.513</doi>
    </paper>
    <paper id="514">
      <title>Fair <fixed-case>NLP</fixed-case> Models with Differentially Private Text Encoders</title>
      <author><first>Gaurav</first><last>Maheshwari</last></author>
      <author><first>Pascal</first><last>Denis</last></author>
      <author><first>Mikaela</first><last>Keller</last></author>
      <author><first>Aurélien</first><last>Bellet</last></author>
      <pages>6913-6930</pages>
      <abstract>Encoded text representations often capture sensitive attributes about individuals (e.g., race or gender), which raise privacy concerns and can make downstream models unfair to certain groups. In this work, we propose FEDERATE, an approach that combines ideas from differential privacy and adversarial training to learn private text representations which also induces fairer models. We empirically evaluate the trade-off between the privacy of the representations and the fairness and accuracy of the downstream model on four NLP datasets. Our results show that FEDERATE consistently improves upon previous methods, and thus suggest that privacy and fairness can positively reinforce each other.</abstract>
      <url hash="c54c60e3">2022.findings-emnlp.514</url>
      <bibkey>maheshwari-etal-2022-fair</bibkey>
      <video href="2022.findings-emnlp.514.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.514</doi>
    </paper>
    <paper id="515">
      <title>Modeling Context With Linear Attention for Scalable Document-Level Translation</title>
      <author><first>Zhaofeng</first><last>Wu</last></author>
      <author><first>Hao</first><last>Peng</last></author>
      <author><first>Nikolaos</first><last>Pappas</last></author>
      <author><first>Noah A.</first><last>Smith</last></author>
      <pages>6931-6939</pages>
      <abstract>Document-level machine translation leverages inter-sentence dependencies to produce more coherent and consistent translations. However, these models, predominantly based on transformers, are difficult to scale to long documents as their attention layers have quadratic complexity in the sequence length. Recent efforts on efficient attention improve scalability, but their effect on document translation remains unexplored. In this work, we investigate the efficacy of a recent linear attention model by Peng et al. (2021) on document translation and augment it with a sentential gate to promote a recency inductive bias. We evaluate the model on IWSLT 2015 and OpenSubtitles 2018 against the transformer, demonstrating substantially increased decoding speed on long sequences with similar or better BLEU scores. We show that sentential gating further improves translation quality on IWSLT.</abstract>
      <url hash="28b1ee92">2022.findings-emnlp.515</url>
      <bibkey>wu-etal-2022-modeling</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.515</doi>
    </paper>
    <paper id="516">
      <title>What do Large Language Models Learn beyond Language?</title>
      <author><first>Avinash</first><last>Madasu</last></author>
      <author><first>Shashank</first><last>Srivastava</last></author>
      <pages>6940-6953</pages>
      <abstract>Large language models (LMs) have rapidly become a mainstay in Natural Language Processing. These models are known to acquire rich linguistic knowledge from training on large amounts of text. In this paper, we investigate if pre-training on text also confers these models with helpful ‘inductive biases’ for non-linguistic reasoning. On a set of 19 diverse non-linguistic tasks involving quantitative computations, recognizing regular expressions and reasoning over strings. We find that pretrained models significantly outperform comparable non-pretrained neural models. This remains true also in experiments with training non-pretrained models with fewer parameters to account for model regularization effects. We further explore the effect of text domain on LMs by pretraining models from text from different domains and provenances. Our experiments surprisingly reveal that the positive effects of pre-training persist even when pretraining on multi-lingual text or computer code, and even for text generated from synthetic languages. Our findings suggest a hithertho unexplored deep connection between pre-training and inductive learning abilities of language models</abstract>
      <url hash="a7f827aa">2022.findings-emnlp.516</url>
      <bibkey>madasu-srivastava-2022-large</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.516</doi>
    </paper>
    <paper id="517">
      <title><fixed-case>CONSISTENT</fixed-case>: Open-Ended Question Generation From News Articles</title>
      <author><first>Tuhin</first><last>Chakrabarty</last></author>
      <author><first>Justin</first><last>Lewis</last></author>
      <author><first>Smaranda</first><last>Muresan</last></author>
      <pages>6954-6968</pages>
      <abstract>Recent work on question generation has largely focused on factoid questions such as who, what,where, when about basic facts. Generating open-ended why, how, what, etc. questions thatrequire long-form answers have proven more difficult. To facilitate the generation of openended questions, we propose CONSISTENT, a new end-to-end system for generating openended questions that are answerable from and faithful to the input text. Using news articles asa trustworthy foundation for experimentation, we demonstrate our model’s strength over several baselines using both automatic and human based evaluations. We contribute an evaluationdataset of expert-generated open-ended questions. We discuss potential downstream applications for news media organizations.</abstract>
      <url hash="a8cb362f">2022.findings-emnlp.517</url>
      <bibkey>chakrabarty-etal-2022-consistent</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.517</doi>
    </paper>
    <paper id="518">
      <title>Efficient (Soft) <fixed-case>Q</fixed-case>-Learning for Text Generation with Limited Good Data</title>
      <author><first>Han</first><last>Guo</last></author>
      <author><first>Bowen</first><last>Tan</last></author>
      <author><first>Zhengzhong</first><last>Liu</last></author>
      <author><first>Eric</first><last>Xing</last></author>
      <author><first>Zhiting</first><last>Hu</last></author>
      <pages>6969-6991</pages>
      <abstract>Maximum likelihood estimation (MLE) is the predominant algorithm for training text generation models. This paradigm relies on direct supervision examples, which is not applicable to many emerging applications, such as generating adversarial attacks or generating prompts to control language models. Reinforcement learning (RL) on the other hand offers a more flexible solution by allowing users to plug in arbitrary task metrics as reward. Yet previous RL algorithms for text generation, such as policy gradient (on-policy RL) and Q-learning (off-policy RL), are often notoriously inefficient or unstable to train due to the large sequence space and the sparse reward received only at the end of sequences. In this paper, we introduce a new RL formulation for text generation from the soft Q-learning (SQL) perspective. It enables us to draw from the latest RL advances, such as path consistency learning, to combine the best of on-/off-policy updates, and learn effectively from sparse reward. We apply the approach to a wide range of novel text generation tasks, including learning from noisy/negative examples, adversarial attacks, and prompt generation. Experiments show our approach consistently outperforms both task-specialized algorithms and the previous RL methods.</abstract>
      <url hash="dc8ae12c">2022.findings-emnlp.518</url>
      <bibkey>guo-etal-2022-efficient</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.518</doi>
    </paper>
    <paper id="519">
      <title><fixed-case>L</fixed-case>exi: Self-Supervised Learning of the <fixed-case>UI</fixed-case> Language</title>
      <author><first>Pratyay</first><last>Banerjee</last></author>
      <author><first>Shweti</first><last>Mahajan</last></author>
      <author><first>Kushal</first><last>Arora</last></author>
      <author><first>Chitta</first><last>Baral</last></author>
      <author><first>Oriana</first><last>Riva</last></author>
      <pages>6992-7007</pages>
      <abstract>Humans can learn to operate the user interface (UI) of an application by reading an instruction manual or how-to guide. Along with text, these resources include visual content such as UI screenshots and images of application icons referenced in the text. We explore how to leverage this data to learn generic visio-linguistic representations of UI screens and their components. These representations are useful in many real applications, such as accessibility, voice navigation, and task automation. Prior UI representation models rely on UI metadata (UI trees and accessibility labels), which is often missing, incompletely defined, or not accessible. We avoid such a dependency, and propose Lexi, a pre-trained vision and language model designed to handle the unique features of UI screens, including their text richness and context sensitivity. To train Lexi we curate the UICaption dataset consisting of 114k UI images paired with descriptions of their functionality. We evaluate Lexi on four tasks: UI action entailment, instruction-based UI image retrieval, grounding referring expressions, and UI entity recognition.</abstract>
      <url hash="5e155317">2022.findings-emnlp.519</url>
      <bibkey>banerjee-etal-2022-lexi</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.519</doi>
    </paper>
    <paper id="520">
      <title>Inferring the Reader: Guiding Automated Story Generation with Commonsense Reasoning</title>
      <author><first>Xiangyu</first><last>Peng</last></author>
      <author><first>Siyan</first><last>Li</last></author>
      <author><first>Sarah</first><last>Wiegreffe</last></author>
      <author><first>Mark</first><last>Riedl</last></author>
      <pages>7008-7029</pages>
      <abstract>Transformer-based language model approaches to automated story generation currently provide state-of-the-art results. However, they still suffer from plot incoherence when generatingnarratives over time, and critically lack basiccommonsense reasoning. Furthermore, existing methods generally focus only on single-character stories, or fail to track charactersat all. To improve the coherence of generated narratives and to expand the scope ofcharacter-centric narrative generation, we introduce Commonsense-inference Augmentedneural StoryTelling (CAST), a framework forintroducing commonsense reasoning into thegeneration process with the option to model theinteraction between multiple characters. Wefind that our CAST method produces significantly more coherent, on-topic, enjoyable andfluent stories than existing models in both thesingle-character and two-character settings inthree storytelling domains.</abstract>
      <url hash="ee25c747">2022.findings-emnlp.520</url>
      <bibkey>peng-etal-2022-inferring</bibkey>
      <video href="2022.findings-emnlp.520.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.520</doi>
    </paper>
    <paper id="521">
      <title>How to Stop an Avalanche? <fixed-case>J</fixed-case>o<fixed-case>D</fixed-case>e<fixed-case>M</fixed-case>: Joint Decision Making through Compare and Contrast for Dialog State Tracking</title>
      <author><first>Haoming</first><last>Wang</last></author>
      <author><first>Wang</first><last>Xin</last></author>
      <pages>7030-7041</pages>
      <abstract>Dialog state tracking (DST) is a core component in task-oriented dialog systems. Existing state-of-the-art DST model incorporates insight and intuition from the human experience into design of supplementary labels, which greatly assisted the training process of turn-by-turn DST model. Though the turn-by-turn scheme and supplementary labels enabled satisfactory performance on the task, most of the DST models of this fashion label or process the raw dialogue data on the premise that the last turn dialogue state is always correct, which is usually not the case. In this paper, we address the negative impact resulted from the premise above as the avalanche phenomenon. After that, we propose JoDeM, a state-of-the-art DST model which can tackle the Avalanche phenomenon with two mechanisms. First mechanism is a jointly decision making method to extract key information from the dialogue. Second mechanism is a compare and contrast dialogue update technique to prevent error accumulation. Example study and graph analysis are presented to support our claim about the harmfulness of avalanche phenomenon. We also conduct quantitative and qualitative experiments on the high quality MultiWOZ2.3 corpus dataset to demonstrate that the proposed model not only outperforms the existing state-of-the-art methods, but also proves the validity of solving avalanche degradation problem.</abstract>
      <url hash="dced1d1e">2022.findings-emnlp.521</url>
      <bibkey>wang-xin-2022-stop</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.521</doi>
    </paper>
    <paper id="522">
      <title>Contrastive Learning with Prompt-derived Virtual Semantic Prototypes for Unsupervised Sentence Embedding</title>
      <author><first>Jiali</first><last>Zeng</last></author>
      <author><first>Yongjing</first><last>Yin</last></author>
      <author><first>Yufan</first><last>Jiang</last></author>
      <author><first>Shuangzhi</first><last>Wu</last></author>
      <author><first>Yunbo</first><last>Cao</last></author>
      <pages>7042-7053</pages>
      <abstract>Contrastive learning has become a new paradigm for unsupervised sentence embeddings.Previous studies focus on instance-wise contrastive learning, attempting to construct positive pairs with textual data augmentation. In this paper, we propose a novel Contrastive learning method with Prompt-derived Virtual semantic Prototypes (ConPVP). Specifically, with the help of prompts, we construct virtual semantic prototypes to each instance, and derive negative prototypes by using the negative form of the prompts. Using a prototypical contrastive loss, we enforce the anchor sentence embedding to be close to its corresponding semantic prototypes, and far apart from the negative prototypes as well as the prototypes of other sentences. Extensive experimental results on semantic textual similarity, transfer, and clustering tasks demonstrate the effectiveness of our proposed model compared to strong baselines. Code is available at <url>https://github.com/lemon0830/promptCSE</url>.</abstract>
      <url hash="ecfeae48">2022.findings-emnlp.522</url>
      <bibkey>zeng-etal-2022-contrastive</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.522</doi>
    </paper>
    <paper id="523">
      <title>Weight Perturbation as Defense against Adversarial Word Substitutions</title>
      <author><first>Jianhan</first><last>Xu</last></author>
      <author><first>Linyang</first><last>Li</last></author>
      <author><first>Jiping</first><last>Zhang</last></author>
      <author><first>Xiaoqing</first><last>Zheng</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <author><first>Cho-Jui</first><last>Hsieh</last></author>
      <author><first>Xuanjing</first><last>Huang</last></author>
      <pages>7054-7063</pages>
      <abstract>The existence and pervasiveness of textual adversarial examples have raised serious concerns to security-critical applications. Many methods have been developed to defend against adversarial attacks for neural natural language processing (NLP) models. Adversarial training is one of the most successful defense methods by adding some random or intentional perturbations to the original input texts and making the models robust to the perturbed examples. In this study, we explore the feasibility of improving the adversarial robustness of NLP models by performing perturbations in the parameter space rather than the input feature space. The weight perturbation helps to find a better solution (i.e., the values of weights) that minimizes the adversarial loss among other feasible solutions. We found that the weight perturbation can significantly improve the robustness of NLP models when it is combined with the perturbation in the input embedding space, yielding the highest accuracy on both clean and adversarial examples across different datasets.</abstract>
      <url hash="e4eab127">2022.findings-emnlp.523</url>
      <bibkey>xu-etal-2022-weight</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.523</doi>
    </paper>
    <paper id="524">
      <title><fixed-case>CORT</fixed-case>: A New Baseline for Comparative Opinion Classification by Dual Prompts</title>
      <author><first>Yequan</first><last>Wang</last></author>
      <author><first>Hengran</first><last>Zhang</last></author>
      <author><first>Aixin</first><last>Sun</last></author>
      <author><first>Xuying</first><last>Meng</last></author>
      <pages>7064-7075</pages>
      <abstract>Comparative opinion is a common linguistic phenomenon. The opinion is expressed by comparing multiple targets on a shared aspect, e.g., “camera A is better than camera B in picture quality”. Among the various subtasks in opinion mining, comparative opinion classification is relatively less studied. Current solutions use rules or classifiers to identify opinions, i.e., better, worse, or same, through feature engineering. Because the features are directly derived from the input sentence, these solutions are sensitive to the order of the targets mentioned in the sentence. For example, “camera A is better than camera B” means the same as “camera B is worse than camera A”; but the features of these two sentences are completely different. In this paper, we approach comparative opinion classification through prompt learning, taking the advantage of embedded knowledge in pre-trained language model. We design a twin framework with dual prompts, named CORT. This extremely simple model delivers state-of-the-art and robust performance on all benchmark datasets for comparative opinion classification. We believe CORT well serves as a new baseline for comparative opinion classification.</abstract>
      <url hash="65e716bd">2022.findings-emnlp.524</url>
      <bibkey>wang-etal-2022-cort</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.524</doi>
    </paper>
    <paper id="525">
      <title><fixed-case>APEACH</fixed-case>: Attacking Pejorative Expressions with Analysis on Crowd-Generated Hate Speech Evaluation Datasets</title>
      <author><first>Kichang</first><last>Yang</last></author>
      <author><first>Wonjun</first><last>Jang</last></author>
      <author><first>Won Ik</first><last>Cho</last></author>
      <pages>7076-7086</pages>
      <abstract>In hate speech detection, developing training and evaluation datasets across various domains is the critical issue. Whereas, major approaches crawl social media texts and hire crowd-workers to annotate the data. Following this convention often restricts the scope of pejorative expressions to a single domain lacking generalization. Sometimes domain overlap between training corpus and evaluation set overestimate the prediction performance when pretraining language models on low-data language. To alleviate these problems in Korean, we propose APEACH that asks unspecified users to generate hate speech examples followed by minimal post-labeling. We find that APEACH can collect useful datasets that are less sensitive to the lexical overlaps between the pretraining corpus and the evaluation set, thereby properly measuring the model performance.</abstract>
      <url hash="f3e4a8aa">2022.findings-emnlp.525</url>
      <bibkey>yang-etal-2022-apeach</bibkey>
      <video href="2022.findings-emnlp.525.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.525</doi>
    </paper>
    <paper id="526">
      <title>Guiding Neural Story Generation with Reader Models</title>
      <author><first>Xiangyu</first><last>Peng</last></author>
      <author><first>Kaige</first><last>Xie</last></author>
      <author><first>Amal</first><last>Alabdulkarim</last></author>
      <author><first>Harshith</first><last>Kayam</last></author>
      <author><first>Samihan</first><last>Dani</last></author>
      <author><first>Mark</first><last>Riedl</last></author>
      <pages>7087-7111</pages>
      <abstract>Automated storytelling has long captured the attention of researchers for the ubiquity of narratives in everyday life. However, it is challenging to maintain coherence and stay on-topictoward a specific ending when generating narratives with neural language models. In this paper, we introduce Story generation with ReaderModels (StoRM), a framework in which areader model is used to reason about the storyshould progress. A reader model infers whata human reader believes about the concepts,entities, and relations about the fictional storyworld. We show how an explicit reader modelrepresented as a knowledge graph affords the storycoherence and provides controllability in theform of achieving a given story world stategoal. Experiments show that our model produces significantly more coherent and on-topicstories, outperforming baselines in dimensionsincluding plot plausibility and staying on topic</abstract>
      <url hash="21d39da4">2022.findings-emnlp.526</url>
      <bibkey>peng-etal-2022-guiding</bibkey>
      <video href="2022.findings-emnlp.526.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.526</doi>
    </paper>
    <paper id="527">
      <title>Reason first, then respond: Modular Generation for Knowledge-infused Dialogue</title>
      <author><first>Leonard</first><last>Adolphs</last></author>
      <author><first>Kurt</first><last>Shuster</last></author>
      <author><first>Jack</first><last>Urbanek</last></author>
      <author><first>Arthur</first><last>Szlam</last></author>
      <author><first>Jason</first><last>Weston</last></author>
      <pages>7112-7132</pages>
      <abstract>Large language models can produce fluent dialogue but often hallucinate factual inaccuracies. While retrieval-augmented models help alleviate this issue, they still face a difficult challenge of both reasoning to provide correct knowledge and generating conversation simultaneously. In this work, we propose a modular model, Knowledge to Response (K2R), for incorporating knowledge into conversational agents, which breaks down this problem into two easier steps. K2R first generates a knowledge sequence, given a dialogue context, as an intermediate step. After this “reasoning step”, the model then attends to its own generated knowledge sequence, as well as the dialogue context, to produce a final response. In detailed experiments, we find that such a model hallucinates less in knowledge-grounded dialogue tasks, and has advantages in terms of interpretability and modularity. In particular, it can be used to fuse QA and dialogue systems together to enable dialogue agents to give knowledgeable answers, or QA models to give conversational responses in a zero-shot setting.</abstract>
      <url hash="d5f7a122">2022.findings-emnlp.527</url>
      <bibkey>adolphs-etal-2022-reason</bibkey>
      <video href="2022.findings-emnlp.527.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.527</doi>
    </paper>
    <paper id="528">
      <title>Adapting Multilingual Models for Code-Mixed Translation</title>
      <author><first>Aditya</first><last>Vavre</last></author>
      <author><first>Abhirut</first><last>Gupta</last></author>
      <author><first>Sunita</first><last>Sarawagi</last></author>
      <pages>7133-7141</pages>
      <abstract>The scarcity of gold standard code-mixed to pure language parallel data makes it difficult to train translation models reliably. Prior work has addressed the paucity of parallel data with data augmentation techniques. Such methods rely heavily on external resources making systems difficult to train and scale effectively for multiple languages. We present a simple yet highly effective two-stage back-translation based training scheme for adapting multilingual models to the task of code-mixed translation which eliminates dependence on external resources. We show a substantial improvement in translation quality (measured through BLEU), beating existing prior work by up to +3.8 BLEU on code-mixed Hi<tex-math>\rightarrow</tex-math>En, Mr<tex-math>\rightarrow</tex-math>En, and Bn<tex-math>\rightarrow</tex-math>En tasks. On the LinCE Machine Translation leader board, we achieve the highest score for code-mixed Es<tex-math>\rightarrow</tex-math>En, beating existing best baseline by +6.5 BLEU, and our own stronger baseline by +1.1 BLEU.</abstract>
      <url hash="00a69d6e">2022.findings-emnlp.528</url>
      <bibkey>vavre-etal-2022-adapting</bibkey>
      <video href="2022.findings-emnlp.528.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.528</doi>
    </paper>
    <paper id="529">
      <title><fixed-case>LPC</fixed-case>: A Logits and Parameter Calibration Framework for Continual Learning</title>
      <author><first>Xiaodi</first><last>Li</last></author>
      <author><first>Zhuoyi</first><last>Wang</last></author>
      <author><first>Dingcheng</first><last>Li</last></author>
      <author><first>Latifur</first><last>Khan</last></author>
      <author><first>Bhavani</first><last>Thuraisingham</last></author>
      <pages>7142-7155</pages>
      <abstract>When we execute the typical fine-tuning paradigm on continuously sequential tasks, the model will suffer from the catastrophic forgetting problem (i.e., the model tends to adjust old parameters according to the new knowledge, which leads to the loss of previously acquired concepts). People proposed replay-based methods by accessing old data from extra storage and maintaining the parameters of old concepts, which actually raise the privacy issue and larger memory requirements. In this work, we aim to achieve the sequential/continual learning of knowledge without accessing the old data. The core idea is to calibrate the parameters and logits (output) so that preserving old parameters and generalized learning on new concepts can be solved simultaneously. Our proposed framework includes two major components, Logits Calibration (LC) and Parameter Calibration (PC). The LC focuses on calibrating the learning of novel models with old models, and PC aims to preserve the parameters of old models. These two operations can maintain the old knowledge while learning new tasks without storing previous data. We conduct experiments on various scenarios of the GLUE (the General Language Understanding Evaluation) benchmark. The experimental results show that our model achieves state-of-the-art performance in all scenarios.</abstract>
      <url hash="93318209">2022.findings-emnlp.529</url>
      <bibkey>li-etal-2022-lpc</bibkey>
      <video href="2022.findings-emnlp.529.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.529</doi>
    </paper>
    <paper id="530">
      <title><fixed-case>S</fixed-case>lovak<fixed-case>BERT</fixed-case>: <fixed-case>S</fixed-case>lovak Masked Language Model</title>
      <author><first>Matúš</first><last>Pikuliak</last></author>
      <author><first>Štefan</first><last>Grivalský</last></author>
      <author><first>Martin</first><last>Konôpka</last></author>
      <author><first>Miroslav</first><last>Blšták</last></author>
      <author><first>Martin</first><last>Tamajka</last></author>
      <author><first>Viktor</first><last>Bachratý</last></author>
      <author><first>Marian</first><last>Simko</last></author>
      <author><first>Pavol</first><last>Balážik</last></author>
      <author><first>Michal</first><last>Trnka</last></author>
      <author><first>Filip</first><last>Uhlárik</last></author>
      <pages>7156-7168</pages>
      <abstract>We introduce a new Slovak masked language model called <i>SlovakBERT</i>. This is to our best knowledge the first paper discussing Slovak transformers-based language models. We evaluate our model on several NLP tasks and achieve state-of-the-art results. This evaluation is likewise the first attempt to establish a benchmark for Slovak language models. We publish the masked language model, as well as the fine-tuned models for part-of-speech tagging, sentiment analysis and semantic textual similarity.</abstract>
      <url hash="de9caea5">2022.findings-emnlp.530</url>
      <bibkey>pikuliak-etal-2022-slovakbert</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.530</doi>
    </paper>
    <paper id="531">
      <title>Efficient Zero-shot Event Extraction with Context-Definition Alignment</title>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Wenlin</first><last>Yao</last></author>
      <author><first>Dong</first><last>Yu</last></author>
      <pages>7169-7179</pages>
      <abstract>Event extraction (EE) is the task of identifying interested event mentions from text. Conventional efforts mainly focus on the supervised setting. However, these supervised models cannot generalize to event types out of the pre-defined ontology. To fill this gap, many efforts have been devoted to the zero-shot EE problem. This paper follows the trend of modeling event-type semantics but moves one step further. We argue that using the static embedding of the event type name might not be enough because a single word could be ambiguous, and we need a sentence to define the type semantics accurately. To model the definition semantics, we use two separate transformer models to project the contextualized event mentions and corresponding definitions into the same embedding space and then minimize their embedding distance via contrastive learning. On top of that, we also propose a warming phase to help the model learn the minor difference between similar definitions. We name our approach Zero-shot Event extraction with Definition (ZED). Experiments on the MAVEN dataset show that our model significantly outperforms all previous zero-shot EE methods with fast inference speed due to the disjoint design. Further experiments also show that can be easily applied to the few-shot setting when the annotation is available and consistently outperforms baseline supervised methods.</abstract>
      <url hash="20fb9ae2">2022.findings-emnlp.531</url>
      <bibkey>zhang-etal-2022-efficient-zero</bibkey>
      <video href="2022.findings-emnlp.531.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.531</doi>
    </paper>
    <paper id="532">
      <title>Logical Fallacy Detection</title>
      <author><first>Zhijing</first><last>Jin</last></author>
      <author><first>Abhinav</first><last>Lalwani</last></author>
      <author><first>Tejas</first><last>Vaidhya</last></author>
      <author><first>Xiaoyu</first><last>Shen</last></author>
      <author><first>Yiwen</first><last>Ding</last></author>
      <author><first>Zhiheng</first><last>Lyu</last></author>
      <author><first>Mrinmaya</first><last>Sachan</last></author>
      <author><first>Rada</first><last>Mihalcea</last></author>
      <author><first>Bernhard</first><last>Schoelkopf</last></author>
      <pages>7180-7198</pages>
      <abstract>Reasoning is central to human intelligence. However, fallacious arguments are common, and some exacerbate problems such as spreading misinformation about climate change. In this paper, we propose the task of logical fallacy detection, and provide a new dataset (Logic) of logical fallacies generally found in text, together with an additional challenge set for detecting logical fallacies in climate change claims (LogicClimate). Detecting logical fallacies is a hard problem as the model must understand the underlying logical structure of the argument. We find that existing pretrained large language models perform poorly on this task. In contrast, we show that a simple structure-aware classifier outperforms the best language model by 5.46% F1 scores on Logic and 4.51% on LogicClimate. We encourage future work to explore this task since (a) it can serve as a new reasoning challenge for language models, and (b) it can have potential applications in tackling the spread of misinformation. Our dataset and code are available at <url>https://github.com/causalNLP/logical-fallacy</url></abstract>
      <url hash="8e282cee">2022.findings-emnlp.532</url>
      <bibkey>jin-etal-2022-logical</bibkey>
      <video href="2022.findings-emnlp.532.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.532</doi>
    </paper>
    <paper id="533">
      <title>Topic-Aware Response Generation in Task-Oriented Dialogue with Unstructured Knowledge Access</title>
      <author><first>Yue</first><last>Feng</last></author>
      <author><first>Gerasimos</first><last>Lampouras</last></author>
      <author><first>Ignacio</first><last>Iacobacci</last></author>
      <pages>7199-7211</pages>
      <abstract>To alleviate the problem of structured databases’ limited coverage, recent task-oriented dialogue systems incorporate external unstructured knowledge to guide the generation of system responses. However, these usually use word or sentence level similarities to detect the relevant knowledge context, which only partially capture the topical level relevance. In this paper, we examine how to better integrate topical information in knowledge grounded task-oriented dialogue and propose “Topic-Aware Response Generation” (TARG), an end-to-end response generation model. TARG incorporates multiple topic-aware attention mechanisms to derive the importance weighting scheme over dialogue utterances and external knowledge sources towards a better understanding of the dialogue history. Experimental results indicate that TARG achieves state-of-the-art performance in knowledge selection and response generation, outperforming previous state-of-the-art by 3.2, 3.6, and 4.2 points in EM, F1 and BLEU-4 respectively on Doc2Dial, and performing comparably with previous work on DSTC9; both being knowledge-grounded task-oriented dialogue datasets.</abstract>
      <url hash="f0fc92f3">2022.findings-emnlp.533</url>
      <bibkey>feng-etal-2022-topic</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.533</doi>
    </paper>
    <paper id="534">
      <title>Revisiting Transformer-based Models for Long Document Classification</title>
      <author><first>Xiang</first><last>Dai</last></author>
      <author><first>Ilias</first><last>Chalkidis</last></author>
      <author><first>Sune</first><last>Darkner</last></author>
      <author><first>Desmond</first><last>Elliott</last></author>
      <pages>7212-7230</pages>
      <abstract>The recent literature in text classification is biased towards short text sequences (e.g., sentences or paragraphs). In real-world applications, multi-page multi-paragraph documents are common and they cannot be efficiently encoded by vanilla Transformer-based models. We compare different Transformer-based Long Document Classification (TrLDC) approaches that aim to mitigate the computational overhead of vanilla transformers to encode much longer text, namely sparse attention and hierarchical encoding methods. We examine several aspects of sparse attention (e.g., size of local attention window, use of global attention) and hierarchical (e.g., document splitting strategy) transformers on four document classification datasets covering different domains. We observe a clear benefit from being able to process longer text, and, based on our results, we derive practical advice of applying Transformer-based models on long document classification tasks.</abstract>
      <url hash="12684f4a">2022.findings-emnlp.534</url>
      <bibkey>dai-etal-2022-revisiting</bibkey>
      <video href="2022.findings-emnlp.534.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.534</doi>
    </paper>
    <paper id="535">
      <title>Time-aware Prompting for Text Generation</title>
      <author><first>Shuyang</first><last>Cao</last></author>
      <author><first>Lu</first><last>Wang</last></author>
      <pages>7231-7246</pages>
      <abstract>In this paper, we study the effects of incorporating timestamps, such as document creation dates, into generation systems. Two types of time-aware prompts are investigated: (1) textual prompts that encode document timestamps in natural language sentences; and (2) linear prompts that convert timestamps into continuous vectors. To explore extrapolation to future data points, we further introduce a new data-to-text generation dataset, TempWikiBio, containing more than 4 millions of chronologically ordered revisions of biographical articles from English Wikipedia, each paired with structured personal profiles. Through data-to-text generation on TempWikiBio, text-to-text generation on the content transfer dataset, and summarization on XSum,we show that linear prompts on encoder and textual prompts improve the generation quality on all datasets. Despite having less performance drop when testing on data drawn from a later time, linear prompts focus more on non-temporal information and are less sensitive to the given timestamps, according to human evaluations and sensitivity analyses. Meanwhile, textual prompts establish the association between the given timestamps and the output dates, yielding more factual temporal information in the output.</abstract>
      <url hash="96544006">2022.findings-emnlp.535</url>
      <bibkey>cao-wang-2022-time</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.535</doi>
    </paper>
    <paper id="536">
      <title>Improving Scheduled Sampling with Elastic Weight Consolidation for Neural Machine Translation</title>
      <author><first>Michalis</first><last>Korakakis</last></author>
      <author><first>Andreas</first><last>Vlachos</last></author>
      <pages>7247-7258</pages>
      <abstract>Despite strong performance in many sequence-to-sequence tasks, autoregressive models trained with maximum likelihood estimation suffer from exposure bias, i.e. the discrepancy between the ground-truth prefixes used during training and the model-generated prefixes used at inference time. Scheduled sampling is a simple and empirically successful approach which addresses this issue by incorporating model-generated prefixes into training. However, it has been argued that it is an inconsistent training objective leading to models ignoring the prefixes altogether. In this paper, we conduct systematic experiments and find that scheduled sampling, while it ameliorates exposure bias by increasing model reliance on the input sequence, worsens performance when the prefix at inference time is correct, a form of catastrophic forgetting. We propose to use Elastic Weight Consolidation to better balance mitigating exposure bias with retaining performance. Experiments on four IWSLT’14 and WMT’14 translation datasets demonstrate that our approach alleviates catastrophic forgetting and significantly outperforms maximum likelihood estimation and scheduled sampling baselines.</abstract>
      <url hash="04baf951">2022.findings-emnlp.536</url>
      <bibkey>korakakis-vlachos-2022-improving</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.536</doi>
    </paper>
    <paper id="537">
      <title>Ensemble Transformer for Efficient and Accurate Ranking Tasks: an Application to Question Answering Systems</title>
      <author><first>Yoshitomo</first><last>Matsubara</last></author>
      <author><first>Luca</first><last>Soldaini</last></author>
      <author><first>Eric</first><last>Lind</last></author>
      <author><first>Alessandro</first><last>Moschitti</last></author>
      <pages>7259-7272</pages>
      <abstract>Large transformer models can highly improve Answer Sentence Selection (AS2) tasks, but their high computational costs prevent their use in many real-world applications. In this paper, we explore the following research question: How can we make the AS2 models more accurate without significantly increasing their model complexity? To address the question, we propose a Multiple Heads Student architecture (named CERBERUS), an efficient neural network designed to distill an ensemble of large transformers into a single smaller model. CERBERUS consists of two components: a stack of transformer layers that is used to encode inputs, and a set of ranking heads; unlike traditional distillation technique, each of them is trained by distilling a different large transformer architecture in a way that preserves the diversity of the ensemble members. The resulting model captures the knowledge of heterogeneous transformer models by using just a few extra parameters. We show the effectiveness of CERBERUS on three English datasets for AS2; our proposed approach outperforms all single-model distillations we consider, rivaling the state-of-the-art large AS2 models that have 2.7× more parameters and run 2.5× slower. Code for our model is available at <url>https://github.com/amazon-research/wqa-cerberus</url>.</abstract>
      <url hash="76c2d2aa">2022.findings-emnlp.537</url>
      <bibkey>matsubara-etal-2022-ensemble</bibkey>
      <video href="2022.findings-emnlp.537.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.537</doi>
    </paper>
    <paper id="538">
      <title>Uncertainty Quantification with Pre-trained Language Models: A Large-Scale Empirical Analysis</title>
      <author><first>Yuxin</first><last>Xiao</last></author>
      <author><first>Paul Pu</first><last>Liang</last></author>
      <author><first>Umang</first><last>Bhatt</last></author>
      <author><first>Willie</first><last>Neiswanger</last></author>
      <author><first>Ruslan</first><last>Salakhutdinov</last></author>
      <author><first>Louis-Philippe</first><last>Morency</last></author>
      <pages>7273-7284</pages>
      <abstract>Pre-trained language models (PLMs) have gained increasing popularity due to their compelling prediction performance in diverse natural language processing (NLP) tasks. When formulating a PLM-based prediction pipeline for NLP tasks, it is also crucial for the pipeline to minimize the calibration error, especially in safety-critical applications. That is, the pipeline should reliably indicate when we can trust its predictions. In particular, there are various considerations behind the pipeline: (1) the choice and (2) the size of PLM, (3) the choice of uncertainty quantifier, (4) the choice of fine-tuning loss, and many more. Although prior work has looked into some of these considerations, they usually draw conclusions based on a limited scope of empirical studies. There still lacks a holistic analysis on how to compose a well-calibrated PLM-based prediction pipeline. To fill this void, we compare a wide range of popular options for each consideration based on three prevalent NLP classification tasks and the setting of domain shift. In response, we recommend the following: (1) use ELECTRA for PLM encoding, (2) use larger PLMs if possible, (3) use Temp Scaling as the uncertainty quantifier, and (4) use Focal Loss for fine-tuning.</abstract>
      <url hash="b1a01125">2022.findings-emnlp.538</url>
      <bibkey>xiao-etal-2022-uncertainty</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.538</doi>
    </paper>
    <paper id="539">
      <title>How to Represent Context Better? An Empirical Study on Context Modeling for Multi-turn Response Selection</title>
      <author><first>Jiazhan</first><last>Feng</last></author>
      <author><first>Chongyang</first><last>Tao</last></author>
      <author><first>Chang</first><last>Liu</last></author>
      <author><first>Rui</first><last>Yan</last></author>
      <author><first>Dongyan</first><last>Zhao</last></author>
      <pages>7285-7298</pages>
      <abstract>Building retrieval-based dialogue models that can predict appropriate responses based on the understanding of multi-turn context messages is a challenging problem. Early models usually concatenate all utterances or independently encode each dialogue turn, which may lead to an inadequate understanding of dialogue status. Although a few researchers have noticed the importance of context modeling in multi-turn response prediction, there is no systematic comparison to analyze how to model context effectively and no framework to unify those methods. In this paper, instead of configuring new architectures, we investigate how to improve existing models with a better context modeling method. Specifically, we heuristically summarize three categories of turn-aware context modeling strategies which model the context messages from the perspective of sequential relationship, local relationship, and query-aware manner respectively. A Turn-Aware Context Modeling (TACM) layer is explored to flexibly adapt and unify these context modeling strategies to several advanced response selection models. Evaluation results on three public data sets indicate that employing each individual context modeling strategy or multiple strategies can consistently improve the performance of existing models.</abstract>
      <url hash="6f380d78">2022.findings-emnlp.539</url>
      <bibkey>feng-etal-2022-represent</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.539</doi>
    </paper>
    <paper id="540">
      <title><fixed-case>CHIA</fixed-case>: <fixed-case>CH</fixed-case>oosing Instances to Annotate for Machine Translation</title>
      <author><first>Rajat</first><last>Bhatnagar</last></author>
      <author><first>Ananya</first><last>Ganesh</last></author>
      <author><first>Katharina</first><last>Kann</last></author>
      <pages>7299-7315</pages>
      <abstract>Neural machine translation (MT) systems have been shown to perform poorly on low-resource language pairs, for which large-scale parallel data is unavailable. Making the data annotation process faster and cheaper is therefore important to ensure equitable access to MT systems. To make optimal use of a limited annotation budget, we present CHIA (choosing instances to annotate), a method for selecting instances to annotate for machine translation. Using an existing multi-way parallel dataset of high-resource languages, we first identify instances, based on model training dynamics, that are most informative for training MT models for high-resource languages. We find that there are cross-lingual commonalities in instances that are useful for MT model training, which we use to identify instances that will be useful to train models on a new target language. Evaluating on 20 languages from two corpora, we show that training on instances selected using our method provides an average performance improvement of 1.59 BLEU over training on randomly selected instances of the same size.</abstract>
      <url hash="74bdeca0">2022.findings-emnlp.540</url>
      <bibkey>bhatnagar-etal-2022-chia</bibkey>
      <video href="2022.findings-emnlp.540.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.540</doi>
    </paper>
    <paper id="541">
      <title>Guiding Neural Machine Translation with Semantic Kernels</title>
      <author><first>Ping</first><last>Guo</last></author>
      <author><first>Yue</first><last>Hu</last></author>
      <author><first>Xiangpeng</first><last>Wei</last></author>
      <author><first>Yubing</first><last>Ren</last></author>
      <author><first>Yunpeng</first><last>Li</last></author>
      <author><first>Luxi</first><last>Xing</last></author>
      <author><first>Yuqiang</first><last>Xie</last></author>
      <pages>7316-7327</pages>
      <abstract>Machine Translation task has made great progress with the help of auto-regressive decoding paradigm and Transformer architecture. In this paradigm, though the encoder can obtain global source representations, the decoder can only use translation history to determine the current word. Previous promising works attempted to address this issue by applying a draft or a fixed-length semantic embedding as target-side global information. However, these methods either degrade model efficiency or show limitations in expressing semantics. Motivated by Functional Equivalence Theory, we extract several semantic kernels from a source sentence, each of which can express one semantic segment of the original sentence. Together, these semantic kernels can capture global semantic information, and we project them into target embedding space to guide target sentence generation. We further force our model to use semantic kernels at each decoding step through an adaptive mask algorithm. Empirical studies on various machine translation benchmarks show that our approach gains approximately an improvement of 1 BLEU score on most benchmarks over the Transformer baseline and about 1.7 times faster than previous works on average at inference time.</abstract>
      <url hash="c3b297b6">2022.findings-emnlp.541</url>
      <bibkey>guo-etal-2022-guiding</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.541</doi>
    </paper>
    <paper id="542">
      <title><fixed-case>H</fixed-case>i<fixed-case>SM</fixed-case>atch: Historical Structure Matching based Temporal Knowledge Graph Reasoning</title>
      <author><first>Zixuan</first><last>Li</last></author>
      <author><first>Zhongni</first><last>Hou</last></author>
      <author><first>Saiping</first><last>Guan</last></author>
      <author><first>Xiaolong</first><last>Jin</last></author>
      <author><first>Weihua</first><last>Peng</last></author>
      <author><first>Long</first><last>Bai</last></author>
      <author><first>Yajuan</first><last>Lyu</last></author>
      <author><first>Wei</first><last>Li</last></author>
      <author><first>Jiafeng</first><last>Guo</last></author>
      <author><first>Xueqi</first><last>Cheng</last></author>
      <pages>7328-7338</pages>
      <abstract>A Temporal Knowledge Graph (TKG) is a sequence of KGs with respective timestamps, which adopts quadruples in the form of (<i>subject</i>, <i>relation</i>, <i>object</i>, <i>timestamp</i>) to describe dynamic facts. TKG reasoning has facilitated many real-world applications via answering such queries as (<i>query entity</i>, <i>query relation</i>, <i>?</i>, <i>future timestamp</i>) about future. This is actually a matching task between a query and candidate entities based on their historical structures, which reflect behavioral trends of the entities at different timestamps. In addition, recent KGs provide background knowledge of all the entities, which is also helpful for the matching. Thus, in this paper, we propose the <b>Hi</b>storical <b>S</b>tructure <b>Match</b>ing (<b>HiSMatch</b>) model. It applies two structure encoders to capture the semantic information contained in the historical structures of the query and candidate entities. Besides, it adopts another encoder to integrate the background knowledge into the model. TKG reasoning experiments on six benchmark datasets demonstrate the significant improvement of the proposed HiSMatch model, with up to 5.6% performance improvement in MRR, compared to the state-of-the-art baselines.</abstract>
      <url hash="bd56f56c">2022.findings-emnlp.542</url>
      <bibkey>li-etal-2022-hismatch</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.542</doi>
    </paper>
    <paper id="543">
      <title>Dependency Parsing via Sequence Generation</title>
      <author><first>Boda</first><last>Lin</last></author>
      <author><first>Zijun</first><last>Yao</last></author>
      <author><first>Jiaxin</first><last>Shi</last></author>
      <author><first>Shulin</first><last>Cao</last></author>
      <author><first>Binghao</first><last>Tang</last></author>
      <author><first>Si</first><last>Li</last></author>
      <author><first>Yong</first><last>Luo</last></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <author><first>Lei</first><last>Hou</last></author>
      <pages>7339-7353</pages>
      <abstract>Dependency parsing aims to extract syntactic dependency structure or semantic dependency structure for sentences. Existing methods for dependency parsing include transition-based method, graph-based method and sequence-to-sequence method. These methods obtain excellent performance and we notice them belong to labeling method. Therefore, it may be very valuable and interesting to explore the possibility of using generative method to implement dependency parsing. In this paper, we propose to achieve Dependency Parsing (DP) via Sequence Generation (SG) by utilizing only the pre-trained language model without any auxiliary structures. We first explore different serialization designing strategies for converting parsing structures into sequences. Then we design dependency units and concatenate these units into the sequence for DPSG.We verify the DPSG is capable of parsing on widely used DP benchmarks, i.e., PTB, UD2.2, SDP15 and SemEval16.In addition, we also investigate the astonishing low-resource applicability of DPSG, which includes unsupervised cross-domain conducted on CODT and few-shot cross-task conducted on SDP15.Our research demonstrates that sequence generation is one of the effective methods to achieve dependency parsing. Our codes are available now.</abstract>
      <url hash="df0c887a">2022.findings-emnlp.543</url>
      <bibkey>lin-etal-2022-dependency</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.543</doi>
    </paper>
    <paper id="544">
      <title>Scaling Laws Under the Microscope: Predicting Transformer Performance from Small Scale Experiments</title>
      <author><first>Maor</first><last>Ivgi</last></author>
      <author><first>Yair</first><last>Carmon</last></author>
      <author><first>Jonathan</first><last>Berant</last></author>
      <pages>7354-7371</pages>
      <abstract>Neural scaling laws define a predictable relationship between a model’s parameter count and its performance after training in the form of a power law. However, most research to date has not explicitly investigated whether scaling laws can be used to accelerate model development. In this work, we perform such an empirical investigation across a wide range of language understanding tasks, starting from models with as few as 10K parameters, and evaluate downstream performance across 9 language understanding tasks. We find that scaling laws emerge at finetuning time in some NLP tasks, and that they can also be exploited for debugging convergence when training large models. Moreover, for tasks where scaling laws exist, they can be used to predict the performance of larger models, which enables effective model selection. However, revealing scaling lawsrequires careful hyperparameter tuning and multiple runs for the purpose of uncertainty estimation, which incurs additional overhead, partially offsetting the computational benefits.</abstract>
      <url hash="47dd83d0">2022.findings-emnlp.544</url>
      <bibkey>ivgi-etal-2022-scaling</bibkey>
      <video href="2022.findings-emnlp.544.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.544</doi>
    </paper>
    <paper id="545">
      <title>Analyzing the Limits of Self-Supervision in Handling Bias in Language</title>
      <author><first>Lisa</first><last>Bauer</last></author>
      <author><first>Karthik</first><last>Gopalakrishnan</last></author>
      <author><first>Spandana</first><last>Gella</last></author>
      <author id="yang-liu-icsi"><first>Yang</first><last>Liu</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <author><first>Dilek</first><last>Hakkani-Tur</last></author>
      <pages>7372-7386</pages>
      <abstract>Prompting inputs with natural language task descriptions has emerged as a popular mechanism to elicit reasonably accurate outputs from large-scale generative language models with little to no in-context supervision. This also helps gain insight into how well language models capture the semantics of a wide range of downstream tasks purely from self-supervised pre-training on massive corpora of unlabeled text. Such models have naturally also been exposed to a lot of undesirable content like racist and sexist language and there is only some work on awareness of models along these dimensions. In this paper, we define and comprehensively evaluate how well such language models capture the semantics of four tasks for bias: diagnosis, identification, extraction and rephrasing. We define three broad classes of task descriptions for these tasks: statement, question, and completion, with numerous lexical variants within each class. We study the efficacy of prompting for each task using these classes and the null task description across several decoding methods and few-shot examples. Our analyses indicate that language models are capable of performing these tasks to widely varying degrees across different bias dimensions, such as gender and political affiliation. We believe our work is an important step towards unbiased language models by quantifying the limits of current self-supervision objectives at accomplishing such sociologically challenging tasks.</abstract>
      <url hash="ed36bb29">2022.findings-emnlp.545</url>
      <bibkey>bauer-etal-2022-analyzing</bibkey>
      <revision id="1" href="2022.findings-emnlp.545v1" hash="4dc1bfcb"/>
      <revision id="2" href="2022.findings-emnlp.545v2" hash="ed36bb29" date="2023-03-02">Updated camera ready version.</revision>
      <video href="2022.findings-emnlp.545.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.545</doi>
    </paper>
    <paper id="546">
      <title>Multiple Instance Learning for Offensive Language Detection</title>
      <author><first>Jiexi</first><last>Liu</last></author>
      <author><first>Dehan</first><last>Kong</last></author>
      <author><first>Longtao</first><last>Huang</last></author>
      <author><first>Dinghui</first><last>Mao</last></author>
      <author><first>Hui</first><last>Xue</last></author>
      <pages>7387-7396</pages>
      <abstract>Automatic offensive language detection has become a crucial issue in recent years. Existing researches on this topic are usually based on a large amount of data annotated at sentence level to train a robust model. However, sentence-level annotations are expensive in practice as the scenario expands, while there exist a large amount of natural labels from historical information on online platforms such as reports and punishments. Notably, these natural labels are usually in bag-level corresponding to the whole documents (articles, user profiles, conversations, etc.). Therefore, we target at proposing an approach capable of utilizing the bag-level labeled data for offensive language detection in this study. For this purpose, we formalize this task into a multiple instance learning (MIL) problem. We break down the design of existing MIL methods and propose a hybrid fusion MIL model with mutual-attention mechanism. In order to verify the validity of the proposed method, we present two new bag-level labeled datasets for offensive language detection: OLID-bags and MINOR. Experimental results based on the proposed datasets demonstrate the effectiveness of the mutual-attention method at both sentence level and bag level.</abstract>
      <url hash="240c5748">2022.findings-emnlp.546</url>
      <bibkey>liu-etal-2022-multiple</bibkey>
      <doi>10.18653/v1/2022.findings-emnlp.546</doi>
    </paper>
    <paper id="547">
      <title>Grounded Keys-to-Text Generation: Towards Factual Open-Ended Generation</title>
      <author><first>Faeze</first><last>Brahman</last></author>
      <author><first>Baolin</first><last>Peng</last></author>
      <author><first>Michel</first><last>Galley</last></author>
      <author><first>Sudha</first><last>Rao</last></author>
      <author><first>Bill</first><last>Dolan</last></author>
      <author><first>Snigdha</first><last>Chaturvedi</last></author>
      <author><first>Jianfeng</first><last>Gao</last></author>
      <pages>7397-7413</pages>
      <abstract>Large pre-trained language models have recently enabled open-ended generation frameworks (e.g., prompt-to-text NLG) to tackle a variety of tasks going beyond the traditional data-to-text generation. While this framework is more general, it is under-specified and often leads to a lack of controllability restricting their real-world usage. We propose a new grounded keys-to-text generation task: the task is to generate a factual description about an entity given a set of guiding keys, and grounding passages. To address this task, we introduce a new dataset, called EntDeGen. Inspired by recent QA-based evaluation measures, we propose an automatic metric, MAFE, for factual correctness of generated descriptions. Our EntDescriptor model is equipped with strong rankers to fetch helpful passages and generate entity descriptions. Experimental result shows a good correlation (60.14) between our proposed metric and human judgments of factuality. Our rankers significantly improved the factual correctness of generated descriptions (15.95% and 34.51% relative gains in recall and precision). Finally, our ablation study highlights the benefit of combining keys and groundings.</abstract>
      <url hash="b90c9790">2022.findings-emnlp.547</url>
      <bibkey>brahman-etal-2022-grounded</bibkey>
      <video href="2022.findings-emnlp.547.mp4"/>
      <doi>10.18653/v1/2022.findings-emnlp.547</doi>
    </paper>
  </volume>
</collection>
