<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.sciprodllm">
  <volume id="1" ingest-date="2026-01-13" type="proceedings">
    <meta>
      <booktitle>Proceedings of The First Workshop on Human–LLM Collaboration for Ethical and Responsible Science Production (SciProdLLM)</booktitle>
      <editor><first>Wei</first><last>Zhao</last></editor>
      <editor><first>Jennifer</first><last>D’Souza</last></editor>
      <editor><first>Steffen</first><last>Eger</last></editor>
      <editor><first>Anne</first><last>Lauscher</last></editor>
      <editor><first>Yufang</first><last>Hou</last></editor>
      <editor><first>Nafise</first><last>Sadat Moosavi</last></editor>
      <editor><first>Tristan</first><last>Miller</last></editor>
      <editor><first>Chenghua</first><last>Lin</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Mumbai, India (Hybrid)</address>
      <month>December</month>
      <year>2025</year>
      <url hash="2fae610e">2025.sciprodllm-1</url>
      <venue>sciprodllm</venue>
      <venue>ws</venue>
      <isbn>979-8-89176-307-4</isbn>
    </meta>
    <frontmatter>
      <url hash="5f88e404">2025.sciprodllm-1.0</url>
      <bibkey>sciprodllm-ws-2025-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Bridging Health Literacy Gaps in <fixed-case>I</fixed-case>ndian Languages: Multilingual <fixed-case>LLM</fixed-case>s for Clinical Text Simplification</title>
      <author orcid="0009-0000-8252-7399"><first>R S</first><last>Pavithra</last></author>
      <pages>1-5</pages>
      <abstract>We demonstrate how open multilingual LLMs (mT5, IndicTrans2) can simplify complex medical documents into culturally sensitive, patient friendly text in Indian languages, advancing equitable healthcare communication and multilingual scientific accessibility.Clinical documents such as discharge summaries, consent forms, and medication instructions are essential for patient care but are often written in complex, jargon-heavy language. This barrier is intensified in multilingual and low-literacy contexts like India, where linguistic diversity meets limited health literacy. We present a multilingual clinical text simplification pipeline using open large language models (mT5 and IndicTrans2) to automatically rewrite complex medical text into accessible, culturally appropriate, and patient-friendly versions in English, Hindi, Tamil, and Telugu. Using a synthetic dataset of 2,000 discharge summaries, our models achieve up to 42% readability improvement while maintaining factual accuracy. The framework demonstrates how open, reproducible LLMs can bridge linguistic inequities in healthcare communication and support inclusive, patient-centric digital health access in India.</abstract>
      <url hash="9ffc87f4">2025.sciprodllm-1.1</url>
      <bibkey>pavithra-2025-bridging</bibkey>
    </paper>
    <paper id="2">
      <title>Human-Centered Disability Bias Detection in Large Language Models</title>
      <author><first>Habiba</first><last>Chakour</last><affiliation>Université du Québec à Montréal</affiliation></author>
      <author orcid="0000-0002-9292-722X"><first>Fatiha</first><last>Sadat</last><affiliation>université du Quebec à Montréal</affiliation></author>
      <pages>6-18</pages>
      <abstract>To promote a more just and inclusive society, developers and researchers are strongly encouraged to design Language Models (LM) with ethical considerations at the forefront, ensuring that the benefits and opportunities of AI are accessible to all users and communities. Incorporating humans in the loop is one approach recognized for mitigating general AI biases. Consequently, the development of new design guidelines and datasets is essential to help AI systems realize their full potential for the benefit of people with disabilities.This study aims to identify disability-related bias in Large Masked Language Models (MLMs), the Electra. A participatory and collaborative research approach was employed, involving three disability organizations to collect information on deaf and hard-of-hearing individuals. Our initial analysis reveals that the studied MLM is highly sensitive to the various identity references used to describe deaf and hard-of-hearing people.</abstract>
      <url hash="59d7a56c">2025.sciprodllm-1.2</url>
      <bibkey>chakour-sadat-2025-human</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>T</fixed-case>rans<fixed-case>L</fixed-case>a<fixed-case>T</fixed-case>e<fixed-case>X</fixed-case>: Exposing the Last-Mile Execution Gap in <fixed-case>LLM</fixed-case>-Agent for Scientific Formatting</title>
      <author><first>Jiawen</first><last>Lyn</last></author>
      <author orcid="0000-0001-6741-4855"><first>Yvette</first><last>Graham</last></author>
      <pages>19-24</pages>
      <abstract>Large Language Models (LLMs) have achieved remarkable progress in tasks such as survey writing and language polishing, yet the final stage of LaTeX formatting and template adaptation remains a neglected and error-prone bottleneck.We identify an execution illusion, where LLMs produce linguistically fluent but unexecutable LaTeX code.To address this, we introduce TransLaTeX—the first reasoning-and-control framework that converts documents between scholarly templates with compiler-level verifiability.TransLaTeX achieves three key innovations:(1) Structure–content separation via placeholder masking, ensuring privacy and less token consumption;(2) SafeFormatBench, the first benchmark dedicated to executable LaTeX generation and template conversion; and(3) Execution-grounded verification across compilation, policy compliance, and visual consistency.TransLaTeX outperforms Pandoc and full-text LLM baselines on SafeFormatBench in compilation rate, ACL policy compliance, and layout fidelity, effectively mitigating the execution illusion.</abstract>
      <url hash="9d0e13ce">2025.sciprodllm-1.3</url>
      <bibkey>lyn-graham-2025-translatex</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>MEDEQUALQA</fixed-case>: Evaluating Biases in <fixed-case>LLM</fixed-case>s with Counterfactual Reasoning</title>
      <author><first>Rajarshi</first><last>Ghosh</last></author>
      <author><first>Abhay</first><last>Gupta</last></author>
      <author><first>Hudson</first><last>McBride</last></author>
      <author><first>Anurag Jayant</first><last>Vaidya</last></author>
      <author orcid="0000-0001-7587-1562"><first>Faisal</first><last>Mahmood</last><affiliation>Harvard University</affiliation></author>
      <pages>25-37</pages>
      <abstract>Large language models (LLMs) are increasingly deployed in clinical decision support, yet subtle demographic cues can influence their reasoning. Prior work has documented disparities in outputs across patient groups, but little is known about how internal reasoning shifts under controlled demographic changes. We introduce MEDEQUALQA, a counterfactual benchmark that perturbs only patient pronouns (he/him, she/her, they/them) while holding critical symptoms and conditions (CSCs) constant. Each vignette is expanded into single-CSC ablations, producing three parallel datasets of approximately 23k items each (69k total). We evaluate a frontier LLM and compute Semantic Textual Similarity (STS) between reasoning traces to measure stability across pronoun variants. Our results show overall high similarity (mean STS &gt; 0.80) but reveal consistent localized divergences in cited risk factors, guideline anchors, and differential ordering—even when final diagnoses remain unchanged. Error analysis identifies specific cases where reasoning shifts occur, highlighting clinically relevant bias loci that may cascade into inequitable care. MEDEQUALQA provides a controlled diagnostic setting for auditing reasoning stability in medical AI.</abstract>
      <url hash="c580d302">2025.sciprodllm-1.4</url>
      <bibkey>ghosh-etal-2025-medequalqa</bibkey>
    </paper>
    <paper id="5">
      <title>Reasoning-Enhanced Retrieval for Misconception Prediction: A <fixed-case>RAG</fixed-case>-Inspired Approach with <fixed-case>LLM</fixed-case>s</title>
      <author><first>Chaudhary</first><last>Divya</last></author>
      <author><first>Chang</first><last>Xue</last></author>
      <author><first>Shaorui</first><last>Sun</last></author>
      <pages>38-51</pages>
      <abstract>Large language models (LLMs) are increasingly deployed in clinical decision support, yet subtle demographic cues can influence their reasoning. Prior work has documented disparities in outputs across patient groups, but little is known about how internal reasoning shifts under controlled demographic changes. We introduce MEDEQUALQA, a counterfactual benchmark that perturbs only patient pronouns (he/him, she/her, they/them) while holding critical symptoms and conditions (CSCs) constant. Each vignette is expanded into single-CSC ablations, producing three parallel datasets of approximately 23k items each (69k total). We evaluate a frontier LLM and compute Semantic Textual Similarity (STS) between reasoning traces to measure stability across pronoun variants. Our results show overall high similarity (mean STS &gt; 0.80) but reveal consistent localized divergences in cited risk factors, guideline anchors, and differential ordering—even when final diagnoses remain unchanged. Error analysis identifies specific cases where reasoning shifts occur, highlighting clinically relevant bias loci that may cascade into inequitable care. MEDEQUALQA provides a controlled diagnostic setting for auditing reasoning stability in medical AI.</abstract>
      <url hash="0969a400">2025.sciprodllm-1.5</url>
      <bibkey>divya-etal-2025-reasoning</bibkey>
    </paper>
  </volume>
</collection>
