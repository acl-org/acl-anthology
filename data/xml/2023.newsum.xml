<?xml version='1.0' encoding='UTF-8'?>
<collection id="2023.newsum">
  <volume id="1" ingest-date="2023-11-30" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 4th New Frontiers in Summarization Workshop</booktitle>
      <editor><first>Yue</first><last>Dong</last></editor>
      <editor><first>Wen</first><last>Xiao</last></editor>
      <editor><first>Lu</first><last>Wang</last></editor>
      <editor id="fei-liu"><first>Fei</first><last>Liu</last></editor>
      <editor><first>Giuseppe</first><last>Carenini</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Singapore</address>
      <month>December</month>
      <year>2023</year>
      <url hash="c865c2ed">2023.newsum-1</url>
      <venue>newsum</venue>
    </meta>
    <frontmatter>
      <url hash="b8ab3f94">2023.newsum-1.0</url>
      <bibkey>newsum-2023-new</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Is <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case> a Good <fixed-case>NLG</fixed-case> Evaluator? A Preliminary Study</title>
      <author><first>Jiaan</first><last>Wang</last><affiliation>School of Computer Science and Technology, Soochow University, Suzhou, China</affiliation></author>
      <author><first>Yunlong</first><last>Liang</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Fandong</first><last>Meng</last><affiliation>WeChat AI, Tencent</affiliation></author>
      <author><first>Zengkui</first><last>Sun</last><affiliation>Beijing Jiaotong university</affiliation></author>
      <author><first>Haoxiang</first><last>Shi</last><affiliation>Waseda University</affiliation></author>
      <author><first>Zhixu</first><last>Li</last><affiliation>Fudan University</affiliation></author>
      <author><first>Jinan</first><last>Xu</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Jianfeng</first><last>Qu</last><affiliation>Soochow University</affiliation></author>
      <author><first>Jie</first><last>Zhou</last><affiliation>Tencent Inc.</affiliation></author>
      <pages>1-11</pages>
      <abstract>Recently, the emergence of ChatGPT has attracted wide attention from the computational linguistics community. Many prior studies have shown that ChatGPT achieves remarkable performance on various NLP tasks in terms of automatic evaluation metrics. However, the ability of ChatGPT to serve as an evaluation metric is still underexplored. Considering assessing the quality of natural language generation (NLG) models is an arduous task and NLG metrics notoriously show their poor correlation with human judgments, we wonder whether ChatGPT is a good NLG evaluation metric. In this report, we provide a preliminary meta-evaluation on ChatGPT to show its reliability as an NLG metric. In detail, we regard ChatGPT as a human evaluator and give task-specific (e.g., summarization) and aspect-specific (e.g., relevance) instruction to prompt ChatGPT to evaluate the generated results of NLG models. We conduct experiments on five NLG meta-evaluation datasets (including summarization, story generation and data-to-text tasks). Experimental results show that compared with previous automatic metrics, ChatGPT achieves state-of-the-art or competitive correlation with human judgments in most cases. In addition, we find that the effectiveness of the ChatGPT evaluator might be influenced by the creation method of the meta-evaluation datasets. For the meta-evaluation datasets which are created greatly depending on the reference and thus are biased, the ChatGPT evaluator might lose its effectiveness. We hope our preliminary study could prompt the emergence of a general-purposed reliable NLG metric.</abstract>
      <url hash="24c07ac5">2023.newsum-1.1</url>
      <attachment type="SupplementaryMaterial" hash="7c24693c">2023.newsum-1.1.SupplementaryMaterial.txt</attachment>
      <bibkey>wang-etal-2023-chatgpt</bibkey>
      <doi>10.18653/v1/2023.newsum-1.1</doi>
    </paper>
    <paper id="2">
      <title>Zero-Shot Cross-Lingual Summarization via Large Language Models</title>
      <author><first>Jiaan</first><last>Wang</last><affiliation>School of Computer Science and Technology, Soochow University, Suzhou, China</affiliation></author>
      <author><first>Yunlong</first><last>Liang</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Fandong</first><last>Meng</last><affiliation>WeChat AI, Tencent</affiliation></author>
      <author><first>Beiqi</first><last>Zou</last><affiliation>PrincetonUniversity</affiliation></author>
      <author><first>Zhixu</first><last>Li</last><affiliation>Fudan University</affiliation></author>
      <author><first>Jianfeng</first><last>Qu</last><affiliation>Soochow University</affiliation></author>
      <author><first>Jie</first><last>Zhou</last><affiliation>Tencent Inc.</affiliation></author>
      <pages>12-23</pages>
      <abstract>Given a document in a source language, cross-lingual summarization (CLS) aims to generate a summary in a different target language. Recently, the emergence of Large Language Models (LLMs), such as GPT-3.5, ChatGPT and GPT-4, has attracted wide attention from the computational linguistics community. However, it is not yet known the performance of LLMs on CLS. In this report, we empirically use various prompts to guide LLMs to perform zero-shot CLS from different paradigms (i.e., end-to-end and pipeline), and provide a preliminary evaluation on the generated summaries. We find that ChatGPT and GPT-4 originally prefer to produce lengthy summaries with detailed information. These two LLMs can further balance informativeness and conciseness with the help of an interactive prompt, significantly improving their CLS performance. Experimental results on three widely-used CLS datasets show that GPT-4 achieves state-of-the-art zero-shot CLS performance, and performs competitively compared with the fine-tuned mBART-50. Moreover, we also find some multi-lingual and bilingual LLMs (i.e., BLOOMZ, ChatGLM-6B, Vicuna-13B and ChatYuan) have limited zero-shot CLS ability. Due to the composite nature of CLS, which requires models to perform summarization and translation simultaneously, accomplishing this task in a zero-shot manner is even a challenge for LLMs. Therefore, we sincerely hope and recommend future LLM research could use CLS as a testbed.</abstract>
      <url hash="b0dad2bf">2023.newsum-1.2</url>
      <attachment type="SupplementaryMaterial" hash="ea857030">2023.newsum-1.2.SupplementaryMaterial.txt</attachment>
      <bibkey>wang-etal-2023-zero</bibkey>
      <doi>10.18653/v1/2023.newsum-1.2</doi>
    </paper>
    <paper id="3">
      <title><fixed-case>S</fixed-case>im<fixed-case>CS</fixed-case>um: Joint Learning of Simplification and Cross-lingual Summarization for Cross-lingual Science Journalism</title>
      <author><first>Mehwish</first><last>Fatima</last><affiliation>Heidelberg Institute for Theoretical Studies</affiliation></author>
      <author><first>Tim</first><last>Kolber</last><affiliation>Heidelberg Institute for Theoretical Studies</affiliation></author>
      <author><first>Katja</first><last>Markert</last><affiliation>Heidelberg University</affiliation></author>
      <author><first>Michael</first><last>Strube</last><affiliation>Heidelberg Institute for Theoretical Studies</affiliation></author>
      <pages>24-40</pages>
      <abstract>Cross-lingual science journalism is a recently introduced task that generates popular science summaries of scientific articles different from the source language for non-expert readers. A popular science summary must contain salient content of the input document while focusing on coherence and comprehensibility. Meanwhile, generating a cross-lingual summary from the scientific texts in a local language for the targeted audience is challenging. Existing research on cross-lingual science journalism investigates the task with a pipeline model to combine text simplification and cross-lingual summarization. We extend the research in cross-lingual science journalism by introducing a novel, multi-task learning architecture that combines the aforementioned NLP tasks. Our approach is to jointly train the two high-level NLP tasks in SimCSum for generating cross-lingual popular science summaries. We investigate the performance of SimCSum against the pipeline model and several other strong baselines with several evaluation metrics and human evaluation. Overall, SimCSum demonstrates statistically significant improvements over the state-of-the-art on two non-synthetic cross-lingual scientific datasets. Furthermore, we conduct an in-depth investigation into the linguistic properties of generated summaries and an error analysis.</abstract>
      <url hash="7b535a1d">2023.newsum-1.3</url>
      <attachment type="SupplementaryMaterial" hash="a29c8bf7">2023.newsum-1.3.SupplementaryMaterial.txt</attachment>
      <bibkey>fatima-etal-2023-simcsum</bibkey>
      <doi>10.18653/v1/2023.newsum-1.3</doi>
    </paper>
    <paper id="4">
      <title>Extract, Select and Rewrite: A Modular Sentence Summarization Method</title>
      <author><first>Shuo</first><last>Guan</last><affiliation>UBS</affiliation></author>
      <author><first>Vishakh</first><last>Padmakumar</last><affiliation>New York University</affiliation></author>
      <pages>41-48</pages>
      <abstract>A modular approach has the advantage of being compositional and controllable, comparing to most end-to-end models. In this paper we propose Extract-Select-Rewrite (ESR), a three-phase abstractive sentence summarization method. We decompose summarization into three stages: (i) knowledge extraction, where we extract relation triples from the text using off-the-shelf tools; (ii) content selection, where a subset of triples are selected; and (iii) rewriting, where the selected triple are realized into natural language. Our results demonstrates that ESR is competitive with the best end-to-end models while being more faithful. %than these baseline models. Being modular, ESR’s modules can be trained on separate data which is beneficial in low-resource settings and enhancing the style controllability on text generation.</abstract>
      <url hash="ca2fd9c5">2023.newsum-1.4</url>
      <attachment type="SupplementaryMaterial" hash="a928a97f">2023.newsum-1.4.SupplementaryMaterial.txt</attachment>
      <bibkey>guan-padmakumar-2023-extract</bibkey>
      <doi>10.18653/v1/2023.newsum-1.4</doi>
      <video href="2023.newsum-1.4.mp4"/>
    </paper>
    <paper id="5">
      <title>Summarization-based Data Augmentation for Document Classification</title>
      <author><first>Yueguan</first><last>Wang</last><affiliation>the University of Tokyo</affiliation></author>
      <author><first>Naoki</first><last>Yoshinaga</last><affiliation>Institute of Industrial Science, The University of Tokyo</affiliation></author>
      <pages>49-55</pages>
      <abstract>Despite the prevalence of pretrained language models in natural language understanding tasks, understanding lengthy text such as document is still challenging due to the data sparseness problem. Inspired by that humans develop their ability of understanding lengthy text form reading shorter text, we propose a simple yet effective summarization-based data augmentation, SUMMaug, for document classification. We first obtain easy-to-learn examples for the target document classification task by summarizing the input of the original training examples, while optionally merging the original labels to conform to the summarized input. We then use the generated pseudo examples to perform curriculum learning. Experimental results on two datasets confirmed the advantage of our method compared to existing baseline methods in terms of robustness and accuracy. We release our code and data at https://github.com/etsurin/summaug.</abstract>
      <url hash="861b0f3b">2023.newsum-1.5</url>
      <attachment type="SupplementaryMaterial" hash="29c356c2">2023.newsum-1.5.SupplementaryMaterial.txt</attachment>
      <bibkey>wang-yoshinaga-2023-summarization</bibkey>
      <doi>10.18653/v1/2023.newsum-1.5</doi>
    </paper>
    <paper id="6">
      <title>In-context Learning of Large Language Models for Controlled Dialogue Summarization: A Holistic Benchmark and Empirical Analysis</title>
      <author><first>Yuting</first><last>Tang</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Ratish</first><last>Puduppully</last><affiliation>Institute for Infocomm Research (I2R), A*STAR, Singapore</affiliation></author>
      <author><first>Zhengyuan</first><last>Liu</last><affiliation>Institute for Infocomm Research (I2R), A*STAR, Singapore</affiliation></author>
      <author><first>Nancy</first><last>Chen</last><affiliation>Institute for Infocomm Research (I2R), A*STAR, Singapore</affiliation></author>
      <pages>56-67</pages>
      <abstract>Large Language Models (LLMs) have shown significant performance in numerous NLP tasks, including summarization and controlled text generation. A notable capability of LLMs is in-context learning (ICL), where the model learns new tasks using input-output pairs in the prompt without any parameter update. However, the performance of LLMs in the context of few-shot abstractive dialogue summarization remains underexplored. This study evaluates various state-of-the-art LLMs on the SAMSum dataset within a few-shot framework. We assess these models in both controlled (entity control, length control, and person-focused planning) and uncontrolled settings, establishing a comprehensive benchmark in few-shot dialogue summarization. Our findings provide insights into summary quality and model controllability, offering a crucial reference for future research in dialogue summarization.</abstract>
      <url hash="fb0bc2f1">2023.newsum-1.6</url>
      <attachment type="SupplementaryMaterial" hash="e250727c">2023.newsum-1.6.SupplementaryMaterial.zip</attachment>
      <attachment type="SupplementaryMaterial" hash="a83c4ed9">2023.newsum-1.6.SupplementaryMaterial.txt</attachment>
      <bibkey>tang-etal-2023-context</bibkey>
      <doi>10.18653/v1/2023.newsum-1.6</doi>
    </paper>
    <paper id="7">
      <title>From Sparse to Dense: <fixed-case>GPT</fixed-case>-4 Summarization with Chain of Density Prompting</title>
      <author><first>Griffin</first><last>Adams</last><affiliation>Columbia University</affiliation></author>
      <author><first>Alex</first><last>Fabbri</last><affiliation>Salesforce AI Research</affiliation></author>
      <author><first>Faisal</first><last>Ladhak</last><affiliation>Columbia University</affiliation></author>
      <author><first>Eric</first><last>Lehman</last><affiliation>MIT</affiliation></author>
      <author><first>Noémie</first><last>Elhadad</last><affiliation>Columbia University</affiliation></author>
      <pages>68-74</pages>
      <abstract>Selecting the “right” amount of information to include in a summary is a difficult task. A good summary should be detailed and entity-centric without being overly dense and hard to follow. To better understand this tradeoff, we solicit increasingly dense GPT-4 summaries with what we refer to as a “Chain of Density” (CoD) prompt. Specifically, GPT-4 generates an initial entity-sparse summary before iteratively incorporating missing salient entities without increasing the length. Summaries generated by CoD are more abstractive, exhibit more fusion, and have less of a lead bias than GPT-4 summaries generated by a vanilla prompt. We conduct a human preference study on 100 CNN DailyMail articles and find that humans prefer GPT-4 summaries that are more dense than those generated by a vanilla prompt and almost as dense as human written summaries. Qualitative analysis supports the notion that there exists a tradeoff between informativeness and readability. 500 annotated CoD summaries, as well as an extra 5,000 unannotated summaries, are freely available on HuggingFace (https://huggingface.co/datasets/griffin/chain_of_density).</abstract>
      <url hash="cef4297f">2023.newsum-1.7</url>
      <attachment type="SupplementaryMaterial" hash="8a718397">2023.newsum-1.7.SupplementaryMaterial.zip</attachment>
      <attachment type="SupplementaryMaterial" hash="dff8b62d">2023.newsum-1.7.SupplementaryMaterial.txt</attachment>
      <bibkey>adams-etal-2023-sparse</bibkey>
      <doi>10.18653/v1/2023.newsum-1.7</doi>
    </paper>
    <paper id="8">
      <title>Generating Extractive and Abstractive Summaries in Parallel from Scientific Articles Incorporating Citing Statements</title>
      <author><first>Sudipta</first><last>Singha Roy</last><affiliation>University of Western Ontario</affiliation></author>
      <author><first>Robert E.</first><last>Mercer</last><affiliation>The University of Western Ontario</affiliation></author>
      <pages>75-86</pages>
      <abstract>Summarization of scientific articles often overlooks insights from citing papers, focusing solely on the document’s content. To incorporate citation contexts, we develop a model to summarize a scientific document using the information in the source and citing documents. It concurrently generates abstractive and extractive summaries, each enhancing the other. The extractive summarizer utilizes a blend of heterogeneous graph-based neural networks and graph attention networks, while the abstractive summarizer employs an autoregressive decoder. These modules exchange control signals through the loss function, ensuring the creation of high-quality summaries in both styles.</abstract>
      <url hash="ace451ed">2023.newsum-1.8</url>
      <attachment type="SupplementaryMaterial" hash="3f5e2fd9">2023.newsum-1.8.SupplementaryMaterial.zip</attachment>
      <attachment type="SupplementaryMaterial" hash="61c063b1">2023.newsum-1.8.SupplementaryMaterial.txt</attachment>
      <bibkey>singha-roy-mercer-2023-generating</bibkey>
      <doi>10.18653/v1/2023.newsum-1.8</doi>
    </paper>
    <paper id="9">
      <title>Supervising the Centroid Baseline for Extractive Multi-Document Summarization</title>
      <author><first>Simão</first><last>Gonçalves</last><affiliation>Priberam</affiliation></author>
      <author><first>Gonçalo</first><last>Correia</last><affiliation>Priberam</affiliation></author>
      <author><first>Diogo</first><last>Pernes</last><affiliation>Priberam; University of Porto</affiliation></author>
      <author><first>Afonso</first><last>Mendes</last><affiliation>Priberam Informática, SA.</affiliation></author>
      <pages>87-96</pages>
      <abstract>The centroid method is a simple approach for extractive multi-document summarization and many improvements to its pipeline have been proposed. We further refine it by adding a beam search process to the sentence selection and also a centroid estimation attention model that leads to improved results. We demonstrate this in several multi-document summarization datasets, including in a multilingual scenario.</abstract>
      <url hash="bc80754c">2023.newsum-1.9</url>
      <attachment type="SupplementaryMaterial" hash="f2699e44">2023.newsum-1.9.SupplementaryMaterial.txt</attachment>
      <bibkey>goncalves-etal-2023-supervising</bibkey>
      <doi>10.18653/v1/2023.newsum-1.9</doi>
    </paper>
    <paper id="10">
      <title><fixed-case>D</fixed-case>ebate<fixed-case>KG</fixed-case> – Automatic Policy Debate Case Creation with Semantic Knowledge Graphs</title>
      <author><first>Allen</first><last>Roush</last><affiliation>University of Oregon</affiliation></author>
      <author><first>David</first><last>Mezzetti</last><affiliation>NeuML</affiliation></author>
      <pages>97-104</pages>
      <abstract>Recent work within the Argument Mining community has shown the applicability of Natural Language Processing systems for solving problems found within competitive debate. One of the most important tasks within competitive debate is for debaters to create high quality debate cases. We show that effective debate cases can be constructed using constrained shortest path traversals on Argumentative Semantic Knowledge Graphs. We study this potential in the context of a type of American Competitive Debate, called “Policy Debate”, which already has a large scale dataset targeting it called “DebateSum”. We significantly improve upon DebateSum by introducing 53180 new examples, as well as further useful metadata for every example, to the dataset. We leverage the txtai semantic search and knowledge graph toolchain to produce and contribute 9 semantic knowledge graphs built on this dataset. We create a unique method for evaluating which knowledge graphs are better in the context of producing policy debate cases. A demo which automatically generates debate cases, along with all other code and the Knowledge Graphs, are open-sourced and made available to the public here: https://huggingface.co/spaces/Hellisotherpeople/DebateKG</abstract>
      <url hash="6d800ac5">2023.newsum-1.10</url>
      <attachment type="SupplementaryMaterial" hash="37c580ac">2023.newsum-1.10.SupplementaryMaterial.txt</attachment>
      <bibkey>roush-mezzetti-2023-debatekg</bibkey>
      <doi>10.18653/v1/2023.newsum-1.10</doi>
      <video href="2023.newsum-1.10.mp4"/>
    </paper>
    <paper id="11">
      <title>Unsupervised Opinion Summarization Using Approximate Geodesics</title>
      <author><first>Somnath</first><last>Basu Roy Chowdhury</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Nicholas</first><last>Monath</last><affiliation>Google</affiliation></author>
      <author><first>Kumar</first><last>Dubey</last><affiliation>Google Research</affiliation></author>
      <author><first>Amr</first><last>Ahmed</last><affiliation>Research Scientist, Google Research</affiliation></author>
      <author><first>Snigdha</first><last>Chaturvedi</last><affiliation>University of North Carolina, Chapel Hill</affiliation></author>
      <pages>105-120</pages>
      <abstract>Opinion summarization is the task of creating summaries capturing popular opinions from user reviews.In this paper, we introduce Geodesic Summarizer (GeoSumm), a novel system to perform unsupervised extractive opinion summarization. GeoSumm consists of an encoder-decoder based representation learning model that generates topical representations of texts. These representations capture the underlying semantics of the text as a distribution over learnable latent units. GeoSumm generates these topical representations by performing dictionary learning over pre-trained text representations at multiple layers of the decoder. We then use these topical representations to quantify the importance of review sentences using a novel approximate geodesic distance-based scoring mechanism. We use the importance scores to identify popular opinions in order to compose general and aspect-specific summaries. Our proposed model, GeoSumm, achieves strong performance on three opinion summarization datasets. We perform additional experiments to analyze the functioning of our model and showcase the generalization ability of GeoSumm across different domains.</abstract>
      <url hash="dc1f666b">2023.newsum-1.11</url>
      <attachment type="SupplementaryMaterial" hash="c4fbc6cd">2023.newsum-1.11.SupplementaryMaterial.txt</attachment>
      <bibkey>basu-roy-chowdhury-etal-2023-unsupervised</bibkey>
      <doi>10.18653/v1/2023.newsum-1.11</doi>
      <video href="2023.newsum-1.11.mp4"/>
    </paper>
    <paper id="12">
      <title>Analyzing Multi-Sentence Aggregation in Abstractive Summarization via the Shapley Value</title>
      <author><first>Jingyi</first><last>He</last><affiliation>McGill University</affiliation></author>
      <author><first>Meng</first><last>Cao</last><affiliation>McGill University</affiliation></author>
      <author><first>Jackie Chi Kit</first><last>Cheung</last><affiliation>Mila / McGill University</affiliation></author>
      <pages>121-134</pages>
      <abstract>Abstractive summarization systems aim to write concise summaries capturing the most essential information of the input document in their own words. One of the ways to achieve this is to gather and combine multiple pieces of information from the source document, a process we call aggregation. Despite its importance, the extent to which both reference summaries in benchmark datasets and system-generated summaries require aggregation is yet unknown. In this work, we propose AggSHAP, a measure of the degree of aggregation in a summary sentence. We show that AggSHAP distinguishes multi-sentence aggregation from single-sentence extraction or paraphrasing through automatic and human evaluations. We find that few reference or model-generated summary sentences have a high degree of aggregation measured by the proposed metric. We also demonstrate negative correlations between AggSHAP and other quality scores of system summaries. These findings suggest the need to develop new tasks and datasets to encourage multi-sentence aggregation in summarization.</abstract>
      <url hash="e9c0e533">2023.newsum-1.12</url>
      <attachment type="SupplementaryMaterial" hash="8f8be2ab">2023.newsum-1.12.SupplementaryMaterial.txt</attachment>
      <bibkey>he-etal-2023-analyzing</bibkey>
      <doi>10.18653/v1/2023.newsum-1.12</doi>
    </paper>
    <paper id="13">
      <title>Improving Multi-Stage Long Document Summarization with Enhanced Coarse Summarizer</title>
      <author><first>Jinhyeong</first><last>Lim</last><affiliation>Graduate School of Electrical Engineering and Computer Science, Jeonbuk National University</affiliation></author>
      <author><first>Hyun-Je</first><last>Song</last><affiliation>Jeonbuk National University</affiliation></author>
      <pages>135-144</pages>
      <abstract>Multi-stage long document summarization, which splits a long document as multiple segments and each of which is used to generate a coarse summary in multiple stage, and then the final summary is produced using the last coarse summary, is a flexible approach to capture salient information from the long document. Even if the coarse summary affects the final summary, however, the coarse summarizer in the existing multi-stage summarization is coarsely trained using data segments that are not useful to generate the final summary. In this paper, we propose a novel method for multi-stage long document summarization. The proposed method first generates new segment pairs, ensuring that all of them are relevant to generating the final summary. We then incorporate contrastive learning into the training of the coarse summarizer, which tries to maximize the similarities between source segments and the target summary during training. Through extensive experiments on six long document summarization datasets, we demonstrate that our proposed method not only enhances the existing multi-stage long document summarization approach, but also achieves performance comparable to state-of-the-art methods, including those utilizing large language models for long document summarization.</abstract>
      <url hash="874fdbc6">2023.newsum-1.13</url>
      <attachment type="SupplementaryMaterial" hash="1f0de3e4">2023.newsum-1.13.SupplementaryMaterial.zip</attachment>
      <attachment type="SupplementaryMaterial" hash="e2cc0abe">2023.newsum-1.13.SupplementaryMaterial.txt</attachment>
      <bibkey>lim-song-2023-improving</bibkey>
      <doi>10.18653/v1/2023.newsum-1.13</doi>
      <video href="2023.newsum-1.13.mp4"/>
    </paper>
  </volume>
</collection>
