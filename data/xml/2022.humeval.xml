<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.humeval">
  <volume id="1" ingest-date="2022-05-15" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 2nd Workshop on Human Evaluation of NLP Systems (HumEval)</booktitle>
      <editor><first>Anya</first><last>Belz</last></editor>
      <editor><first>Maja</first><last>Popović</last></editor>
      <editor><first>Ehud</first><last>Reiter</last></editor>
      <editor><first>Anastasia</first><last>Shimorina</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Dublin, Ireland</address>
      <month>May</month>
      <year>2022</year>
      <url hash="d965de37">2022.humeval-1</url>
      <venue>humeval</venue>
    </meta>
    <frontmatter>
      <url hash="f70b06e9">2022.humeval-1.0</url>
      <bibkey>humeval-2022-human</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Vacillating Human Correlation of <fixed-case>S</fixed-case>acre<fixed-case>BLEU</fixed-case> in Unprotected Languages</title>
      <author><first>Ahrii</first><last>Kim</last></author>
      <author><first>Jinhyeon</first><last>Kim</last></author>
      <pages>1-15</pages>
      <abstract>SacreBLEU, by incorporating a text normalizing step in the pipeline, has become a rising automatic evaluation metric in recent MT studies. With agglutinative languages such as Korean, however, the lexical-level metric cannot provide a conceivable result without a customized pre-tokenization. This paper endeavors to ex- amine the influence of diversified tokenization schemes –word, morpheme, subword, character, and consonants &amp; vowels (CV)– on the metric after its protective layer is peeled off. By performing meta-evaluation with manually- constructed into-Korean resources, our empirical study demonstrates that the human correlation of the surface-based metric and other homogeneous ones (as an extension) vacillates greatly by the token type. Moreover, the human correlation of the metric often deteriorates due to some tokenization, with CV one of its culprits. Guiding through the proper usage of tokenizers for the given metric, we discover i) the feasibility of the character tokens and ii) the deficit of CV in the Korean MT evaluation.</abstract>
      <url hash="c26fee50">2022.humeval-1.1</url>
      <bibkey>kim-kim-2022-vacillating</bibkey>
      <doi>10.18653/v1/2022.humeval-1.1</doi>
      <video href="2022.humeval-1.1.mp4"/>
    </paper>
    <paper id="2">
      <title>A Methodology for the Comparison of Human Judgments With Metrics for Coreference Resolution</title>
      <author><first>Mariya</first><last>Borovikova</last></author>
      <author><first>Loïc</first><last>Grobol</last></author>
      <author><first>Anaïs</first><last>Halftermeyer</last></author>
      <author><first>Sylvie</first><last>Billot</last></author>
      <pages>16-23</pages>
      <abstract>We propose a method for investigating the interpretability of metrics used for the coreference resolution task through comparisons with human judgments. We provide a corpus with annotations of different error types and human evaluations of their gravity. Our preliminary analysis shows that metrics considerably overlook several error types and overlook errors in general in comparison to humans. This study is conducted on French texts, but the methodology is language-independent.</abstract>
      <url hash="5dfc3af5">2022.humeval-1.2</url>
      <bibkey>borovikova-etal-2022-methodology</bibkey>
      <doi>10.18653/v1/2022.humeval-1.2</doi>
      <video href="2022.humeval-1.2.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2012-1">CoNLL-2012</pwcdataset>
    </paper>
    <paper id="3">
      <title>Perceptual Quality Dimensions of Machine-Generated Text with a Focus on Machine Translation</title>
      <author><first>Vivien</first><last>Macketanz</last></author>
      <author><first>Babak</first><last>Naderi</last></author>
      <author><first>Steven</first><last>Schmidt</last></author>
      <author><first>Sebastian</first><last>Möller</last></author>
      <pages>24-31</pages>
      <abstract>The quality of machine-generated text is a complex construct consisting of various aspects and dimensions. We present a study that aims to uncover relevant perceptual quality dimensions for one type of machine-generated text, that is, Machine Translation. We conducted a crowdsourcing survey in the style of a Semantic Differential to collect attribute ratings for German MT outputs. An Exploratory Factor Analysis revealed the underlying perceptual dimensions. As a result, we extracted four factors that operate as relevant dimensions for the Quality of Experience of MT outputs: precision, complexity, grammaticality, and transparency.</abstract>
      <url hash="4301eae2">2022.humeval-1.3</url>
      <bibkey>macketanz-etal-2022-perceptual</bibkey>
      <doi>10.18653/v1/2022.humeval-1.3</doi>
      <video href="2022.humeval-1.3.mp4"/>
      <pwccode url="https://github.com/dfki-nlp/textq" additional="false">dfki-nlp/textq</pwccode>
    </paper>
    <paper id="4">
      <title>Human evaluation of web-crawled parallel corpora for machine translation</title>
      <author><first>Gema</first><last>Ramírez-Sánchez</last></author>
      <author><first>Marta</first><last>Bañón</last></author>
      <author><first>Jaume</first><last>Zaragoza-Bernabeu</last></author>
      <author><first>Sergio</first><last>Ortiz Rojas</last></author>
      <pages>32-41</pages>
      <abstract>Quality assessment has been an ongoing activity of the series of ParaCrawl efforts to crawl massive amounts of parallel data from multilingual websites for 29 languages. The goal of ParaCrawl is to get parallel data that is good for machine translation. To prove so, both, automatic (extrinsic) and human (intrinsic and extrinsic) evaluation tasks have been included as part of the quality assessment activity of the project. We sum up the various methods followed to address these evaluation tasks for the web-crawled corpora produced and their results. We review their advantages and disadvantages for the final goal of the ParaCrawl project and the related ongoing project MaCoCu.</abstract>
      <url hash="141f5f3f">2022.humeval-1.4</url>
      <bibkey>ramirez-sanchez-etal-2022-human</bibkey>
      <doi>10.18653/v1/2022.humeval-1.4</doi>
      <video href="2022.humeval-1.4.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/paracrawl">ParaCrawl</pwcdataset>
    </paper>
    <paper id="5">
      <title>Beyond calories: evaluating how tailored communication reduces emotional load in diet-coaching</title>
      <author><first>Simone</first><last>Balloccu</last></author>
      <author><first>Ehud</first><last>Reiter</last></author>
      <pages>42-53</pages>
      <abstract>Dieting is a behaviour change task that is difficult for many people to conduct successfully. This is due to many factors, including stress and cost. Mobile applications offer an alternative to traditional coaching. However, previous work on apps evaluation only focused on dietary outcomes, ignoring users’ emotional state despite its influence on eating habits. In this work, we introduce a novel evaluation of the effects that tailored communication can have on the emotional load of dieting. We implement this by augmenting a traditional diet-app with affective NLG, text-tailoring and persuasive communication techniques. We then run a short 2-weeks experiment and check dietary outcomes, user feedback of produced text and, most importantly, its impact on emotional state, through PANAS questionnaire. Results show that tailored communication significantly improved users’ emotional state, compared to an app-only control group.</abstract>
      <url hash="0edef9d6">2022.humeval-1.5</url>
      <bibkey>balloccu-reiter-2022-beyond</bibkey>
      <doi>10.18653/v1/2022.humeval-1.5</doi>
      <video href="2022.humeval-1.5.mp4"/>
    </paper>
    <paper id="6">
      <title>The Human Evaluation Datasheet: A Template for Recording Details of Human Evaluation Experiments in <fixed-case>NLP</fixed-case></title>
      <author><first>Anastasia</first><last>Shimorina</last></author>
      <author><first>Anya</first><last>Belz</last></author>
      <pages>54-75</pages>
      <abstract>This paper presents the Human Evaluation Datasheet (HEDS), a template for recording the details of individual human evaluation experiments in Natural Language Processing (NLP), and reports on first experience of researchers using HEDS sheets in practice. Originally taking inspiration from seminal papers by Bender and Friedman (2018), Mitchell et al. (2019), and Gebru et al. (2020), HEDS facilitates the recording of properties of human evaluations in sufficient detail, and with sufficient standardisation, to support comparability, meta-evaluation,and reproducibility assessments for human evaluations. These are crucial for scientifically principled evaluation, but the overhead of completing a detailed datasheet is substantial, and we discuss possible ways of addressing this and other issues observed in practice.</abstract>
      <url hash="526cc7c0">2022.humeval-1.6</url>
      <bibkey>shimorina-belz-2022-human</bibkey>
      <doi>10.18653/v1/2022.humeval-1.6</doi>
      <video href="2022.humeval-1.6.mp4"/>
      <pwccode url="https://github.com/Shimorina/human-evaluation-datasheet" additional="false">Shimorina/human-evaluation-datasheet</pwccode>
    </paper>
    <paper id="7">
      <title>Toward More Effective Human Evaluation for Machine Translation</title>
      <author><first>Belén</first><last>Saldías Fuentes</last></author>
      <author><first>George</first><last>Foster</last></author>
      <author><first>Markus</first><last>Freitag</last></author>
      <author><first>Qijun</first><last>Tan</last></author>
      <pages>76-89</pages>
      <abstract>Improvements in text generation technologies such as machine translation have necessitated more costly and time-consuming human evaluation procedures to ensure an accurate signal. We investigate a simple way to reduce cost by reducing the number of text segments that must be annotated in order to accurately predict a score for a complete test set. Using a sampling approach, we demonstrate that information from document membership and automatic metrics can help improve estimates compared to a pure random sampling baseline. We achieve gains of up to 20% in average absolute error by leveraging stratified sampling and control variates. Our techniques can improve estimates made from a fixed annotation budget, are easy to implement, and can be applied to any problem with structure similar to the one we study.</abstract>
      <url hash="9fffacea">2022.humeval-1.7</url>
      <bibkey>saldias-fuentes-etal-2022-toward</bibkey>
      <doi>10.18653/v1/2022.humeval-1.7</doi>
      <video href="2022.humeval-1.7.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2020">WMT 2020</pwcdataset>
    </paper>
    <paper id="8">
      <title>A Study on Manual and Automatic Evaluation for Text Style Transfer: The Case of Detoxification</title>
      <author><first>Varvara</first><last>Logacheva</last></author>
      <author><first>Daryna</first><last>Dementieva</last></author>
      <author><first>Irina</first><last>Krotova</last></author>
      <author><first>Alena</first><last>Fenogenova</last></author>
      <author><first>Irina</first><last>Nikishina</last></author>
      <author><first>Tatiana</first><last>Shavrina</last></author>
      <author><first>Alexander</first><last>Panchenko</last></author>
      <pages>90-101</pages>
      <abstract>It is often difficult to reliably evaluate models which generate text. Among them, text style transfer is a particularly difficult to evaluate, because its success depends on a number of parameters. We conduct an evaluation of a large number of models on a detoxification task. We explore the relations between the manual and automatic metrics and find that there is only weak correlation between them, which is dependent on the type of model which generated text. Automatic metrics tend to be less reliable for better-performing models. However, our findings suggest that, ChrF and BertScore metrics can be used as a proxy for human evaluation of text detoxification to some extent.</abstract>
      <url hash="3324ce8f">2022.humeval-1.8</url>
      <bibkey>logacheva-etal-2022-study</bibkey>
      <doi>10.18653/v1/2022.humeval-1.8</doi>
      <video href="2022.humeval-1.8.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
    </paper>
    <paper id="9">
      <title>Human Judgement as a Compass to Navigate Automatic Metrics for Formality Transfer</title>
      <author><first>Huiyuan</first><last>Lai</last></author>
      <author><first>Jiali</first><last>Mao</last></author>
      <author><first>Antonio</first><last>Toral</last></author>
      <author><first>Malvina</first><last>Nissim</last></author>
      <pages>102-115</pages>
      <abstract>Although text style transfer has witnessed rapid development in recent years, there is as yet no established standard for evaluation, which is performed using several automatic metrics, lacking the possibility of always resorting to human judgement. We focus on the task of formality transfer, and on the three aspects that are usually evaluated: style strength, content preservation, and fluency. To cast light on how such aspects are assessed by common and new metrics, we run a human-based evaluation and perform a rich correlation analysis. We are then able to offer some recommendations on the use of such metrics in formality transfer, also with an eye to their generalisability (or not) to related tasks.</abstract>
      <url hash="9d026598">2022.humeval-1.9</url>
      <bibkey>lai-etal-2022-human</bibkey>
      <doi>10.18653/v1/2022.humeval-1.9</doi>
      <video href="2022.humeval-1.9.mp4"/>
      <pwccode url="https://github.com/laihuiyuan/eval-formality-transfer" additional="false">laihuiyuan/eval-formality-transfer</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/gyafc">GYAFC</pwcdataset>
    </paper>
    <paper id="10">
      <title>Towards Human Evaluation of Mutual Understanding in Human-Computer Spontaneous Conversation: An Empirical Study of Word Sense Disambiguation for Naturalistic Social Dialogs in <fixed-case>A</fixed-case>merican <fixed-case>E</fixed-case>nglish</title>
      <author><first>Alex</first><last>Lưu</last></author>
      <pages>116-125</pages>
      <abstract>Current evaluation practices for social dialog systems, dedicated to human-computer spontaneous conversation, exclusively focus on the quality of system-generated surface text, but not human-verifiable aspects of mutual understanding between the systems and their interlocutors. This work proposes Word Sense Disambiguation (WSD) as an essential component of a valid and reliable human evaluation framework, whose long-term goal is to radically improve the usability of dialog systems in real-life human-computer collaboration. The practicality of this proposal is proved via experimentally investigating (1) the WordNet 3.0 sense inventory coverage of lexical meanings in spontaneous conversation between humans in American English, assumed as an upper bound of lexical diversity of human-computer communication, and (2) the effectiveness of state-of-the-art WSD models and pretrained transformer-based contextual embeddings on this type of data.</abstract>
      <url hash="f73bd479">2022.humeval-1.10</url>
      <bibkey>luu-2022-towards</bibkey>
      <doi>10.18653/v1/2022.humeval-1.10</doi>
      <video href="2022.humeval-1.10.mp4"/>
    </paper>
  </volume>
</collection>
