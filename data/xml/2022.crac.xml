<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.crac">
  <volume id="1" ingest-date="2022-10-06">
    <meta>
      <booktitle>Proceedings of the Fifth Workshop on Computational Models of Reference, Anaphora and Coreference</booktitle>
      <editor><first>Maciej</first><last>Ogrodniczuk</last></editor>
      <editor><first>Sameer</first><last>Pradhan</last></editor>
      <editor><first>Anna</first><last>Nedoluzhko</last></editor>
      <editor><first>Vincent</first><last>Ng</last></editor>
      <editor><first>Massimo</first><last>Poesio</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Gyeongju, Republic of Korea</address>
      <month>October</month>
      <year>2022</year>
      <url hash="0bd52677">2022.crac-1</url>
      <venue>crac</venue>
    </meta>
    <frontmatter>
      <url hash="6ae76a0e">2022.crac-1.0</url>
      <bibkey>crac-2022-models</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Quantifying Discourse Support for Omitted Pronouns</title>
      <author><first>Shulin</first><last>Zhang</last></author>
      <author><first>Jixing</first><last>Li</last></author>
      <author><first>John</first><last>Hale</last></author>
      <pages>1–12</pages>
      <abstract>Pro-drop is commonly seen in many languages, but its discourse motivations have not been well characterized. Inspired by the topic chain theory in Chinese, this study shows how character-verb usage continuity distinguishes dropped pronouns from overt references to story characters. We model the choice to drop vs. not drop as a function of character-verb continuity. The results show that omitted subjects have higher character history-current verb continuity salience than non-omitted subjects. This is consistent with the idea that discourse coherence with a particular topic, such as a story character, indeed facilitates the omission of pronouns in languages and contexts where they are optional.</abstract>
      <url hash="0de42ba0">2022.crac-1.1</url>
      <bibkey>zhang-etal-2022-quantifying</bibkey>
    </paper>
    <paper id="2">
      <title>Online Neural Coreference Resolution with Rollback</title>
      <author><first>Patrick</first><last>Xia</last></author>
      <author><first>Benjamin</first><last>Van Durme</last></author>
      <pages>13–21</pages>
      <abstract>Humans process natural language online, whether reading a document or participating in multiparty dialogue. Recent advances in neural coreference resolution have focused on offline approaches that assume the full communication history as input. This is neither realistic nor sufficient if we wish to support dialogue understanding in real-time. We benchmark two existing, offline, models and highlight their shortcomings in the online setting. We then modify these models to perform online inference and introduce rollback: a short-term mechanism to correct mistakes. We demonstrate across five English datasets the effectiveness of this approach against an offline and a naive online model in terms of latency, final document-level coreference F1, and average running F1.</abstract>
      <url hash="780315b1">2022.crac-1.2</url>
      <bibkey>xia-van-durme-2022-online</bibkey>
    </paper>
    <paper id="3">
      <title>Analyzing Coreference and Bridging in Product Reviews</title>
      <author><first>Hideo</first><last>Kobayashi</last></author>
      <author><first>Christopher</first><last>Malon</last></author>
      <pages>22–30</pages>
      <abstract>Product reviews may have complex discourse including coreference and bridging relations to a main product, competing products, and interacting products. Current approaches to aspect-based sentiment analysis (ABSA) and opinion summarization largely ignore this complexity. On the other hand, existing systems for coreference and bridging were trained in a different domain. We collect mention type annotations relevant to coreference and bridging for 498 product reviews. Using these annotations, we show that a state-of-the-art factuality score fails to catch coreference errors in product reviews, and that a state-of-the-art coreference system trained on OntoNotes does not perform nearly as well on product mentions. As our dataset grows, we expect it to help ABSA and opinion summarization systems to avoid entity reference errors.</abstract>
      <url hash="c05b6721">2022.crac-1.3</url>
      <attachment type="dataset" hash="4cc16c29">2022.crac-1.3.Datasets.zip</attachment>
      <bibkey>kobayashi-malon-2022-analyzing</bibkey>
    </paper>
    <paper id="4">
      <title>Anaphoric Phenomena in Situated dialog: A First Round of Annotations</title>
      <author><first>Sharid</first><last>Loáiciga</last></author>
      <author><first>Simon</first><last>Dobnik</last></author>
      <author><first>David</first><last>Schlangen</last></author>
      <pages>31–37</pages>
      <abstract>We present a first release of 500 documents from the multimodal corpus Tell-me-more (Ilinykh et al., 2019) annotated with coreference information according to the ARRAU guidelines (Poesio et al., 2021). The corpus consists of images and short texts of five sentences. We describe the annotation process and present the adaptations to the original guidelines in order to account for the challenges of grounding the annotations to the image. 50 documents from the 500 available are annotated by two people and used to estimate inter-annotator agreement (IAA) relying on Krippendorff’s alpha.</abstract>
      <url hash="ce46d9c2">2022.crac-1.4</url>
      <bibkey>loaiciga-etal-2022-anaphoric</bibkey>
    </paper>
    <paper id="5">
      <title>Building a Manually Annotated <fixed-case>H</fixed-case>ungarian Coreference Corpus: Workflow and Tools</title>
      <author><first>Noémi</first><last>Vadász</last></author>
      <pages>38–47</pages>
      <abstract>This paper presents the complete workflow of building a manually annotated Hungarian corpus, KorKor, with particular reference to anaphora and coreference annotation. All linguistic annotation layers were corrected manually. The corpus is freely available in two formats. The paper gives insight into the process of setting up the workflow and the challenges that have arisen.</abstract>
      <url hash="12065428">2022.crac-1.5</url>
      <bibkey>vadasz-2022-building</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>NARC</fixed-case> – <fixed-case>N</fixed-case>orwegian Anaphora Resolution Corpus</title>
      <author><first>Petter</first><last>Mæhlum</last></author>
      <author><first>Dag</first><last>Haug</last></author>
      <author><first>Tollef</first><last>Jørgensen</last></author>
      <author><first>Andre</first><last>Kåsen</last></author>
      <author><first>Anders</first><last>Nøklestad</last></author>
      <author><first>Egil</first><last>Rønningstad</last></author>
      <author><first>Per Erik</first><last>Solberg</last></author>
      <author><first>Erik</first><last>Velldal</last></author>
      <author><first>Lilja</first><last>Øvrelid</last></author>
      <pages>48–60</pages>
      <abstract>We present the Norwegian Anaphora Resolution Corpus (NARC), the first publicly available corpus annotated with anaphoric relations between noun phrases for Norwegian. The paper describes the annotated data for 326 documents in Norwegian Bokmål, together with inter-annotator agreement and discussions of relevant statistics. We also present preliminary modelling results which are comparable to existing corpora for other languages, and discuss relevant problems in relation to both modelling and the annotations themselves.</abstract>
      <url hash="95f09c2d">2022.crac-1.6</url>
      <bibkey>maehlum-etal-2022-narc</bibkey>
      <pwccode url="https://github.com/ltgoslo/narc" additional="false">ltgoslo/narc</pwccode>
    </paper>
    <paper id="7">
      <title>Evaluating Coreference Resolvers on Community-based Question Answering: From Rule-based to State of the Art</title>
      <author><first>Haixia</first><last>Chai</last></author>
      <author><first>Nafise Sadat</first><last>Moosavi</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <author><first>Michael</first><last>Strube</last></author>
      <pages>61–73</pages>
      <abstract>Coreference resolution is a key step in natural language understanding. Developments in coreference resolution are mainly focused on improving the performance on standard datasets annotated for coreference resolution. However, coreference resolution is an intermediate step for text understanding and it is not clear how these improvements translate into downstream task performance. In this paper, we perform a thorough investigation on the impact of coreference resolvers in multiple settings of community-based question answering task, i.e., answer selection with long answers. Our settings cover multiple text domains and encompass several answer selection methods. We first inspect extrinsic evaluation of coreference resolvers on answer selection by using coreference relations to decontextualize individual sentences of candidate answers, and then annotate a subset of answers with coreference information for intrinsic evaluation. The results of our extrinsic evaluation show that while there is a significant difference between the performance of the rule-based system vs. state-of-the-art neural model on coreference resolution datasets, we do not observe a considerable difference on their impact on downstream models. Our intrinsic evaluation shows that (i) resolving coreference relations on less-formal text genres is more difficult even for trained annotators, and (ii) the values of linguistic-agnostic coreference evaluation metrics do not correlate with the impact on downstream data.</abstract>
      <url hash="0f594a8d">2022.crac-1.7</url>
      <bibkey>chai-etal-2022-evaluating</bibkey>
      <pwccode url="https://github.com/haixiachai/coref_cqa" additional="false">haixiachai/coref_cqa</pwccode>
    </paper>
    <paper id="8">
      <title>Improving Bridging Reference Resolution using Continuous Essentiality from Crowdsourcing</title>
      <author><first>Nobuhiro</first><last>Ueda</last></author>
      <author><first>Sadao</first><last>Kurohashi</last></author>
      <pages>74–87</pages>
      <abstract>Bridging reference resolution is the task of finding nouns that complement essential information of another noun. The essentiality varies depending on noun combination and context and has a continuous distribution. Despite the continuous nature of essentiality, existing datasets of bridging reference have only a few coarse labels to represent the essentiality. In this work, we propose a crowdsourcing-based annotation method that considers continuous essentiality. In the crowdsourcing task, we asked workers to select both all nouns with a bridging reference relation and a noun with the highest essentiality among them. Combining these annotations, we can obtain continuous essentiality. Experimental results demonstrated that the constructed dataset improves bridging reference resolution performance. The code is available at https://github.com/nobu-g/bridging-resolution.</abstract>
      <url hash="73315e9d">2022.crac-1.8</url>
      <bibkey>ueda-kurohashi-2022-improving</bibkey>
      <pwccode url="https://github.com/nobu-g/bridging-resolution" additional="false">nobu-g/bridging-resolution</pwccode>
    </paper>
    <paper id="9">
      <title>Investigating Cross-Document Event Coreference for <fixed-case>D</fixed-case>utch</title>
      <author><first>Loic</first><last>De Langhe</last></author>
      <author><first>Orphee</first><last>De Clercq</last></author>
      <author><first>Veronique</first><last>Hoste</last></author>
      <pages>88–98</pages>
      <abstract>In this paper we present baseline results for Event Coreference Resolution (ECR) in Dutch using gold-standard (i.e non-predicted) event mentions. A newly developed benchmark dataset allows us to properly investigate the possibility of creating ECR systems for both within and cross-document coreference. We give an overview of the state of the art for ECR in other languages, as well as a detailed overview of existing ECR resources. Afterwards, we provide a comparative report on our own dataset. We apply a significant number of approaches that have been shown to attain good results for English ECR including feature-based models, monolingual transformer language models and multilingual language models. The best results were obtained using the monolingual BERTje model. Finally, results for all models are thoroughly analysed and visualised, as to provide insight into the inner workings of ECR and long-distance semantic NLP tasks in general.</abstract>
      <url hash="cc0c215f">2022.crac-1.9</url>
      <bibkey>de-langhe-etal-2022-investigating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ecb">ECB+</pwcdataset>
    </paper>
    <paper id="10">
      <title>The Role of Common Ground for Referential Expressions in Social Dialogues</title>
      <author><first>Jaap</first><last>Kruijt</last></author>
      <author><first>Piek</first><last>Vossen</last></author>
      <pages>99–110</pages>
      <abstract>In this paper, we frame the problem of co-reference resolution in dialogue as a dynamic social process in which mentions to people previously known and newly introduced are mixed when people know each other well. We restructured an existing data set for the Friends sitcom as a coreference task that evolves over time, where close friends make reference to other people either part of their common ground (inner circle) or not (outer circle). We expect that awareness of common ground is key in social dialogue in order to resolve references to the inner social circle, whereas local contextual information plays a more important role for outer circle mentions. Our analysis of these references confirms that there are differences in naming and introducing these people. We also experimented with the SpanBERT coreference system with and without fine-tuning to measure whether preceding discourse contexts matter for resolving inner and outer circle mentions. Our results show that more inner circle mentions lead to a decrease in model performance, and that fine-tuning on preceding contexts reduces false negatives for both inner and outer circle mentions but increases the false positives as well, showing that the models overfit on these contexts.</abstract>
      <url hash="cc8d8192">2022.crac-1.10</url>
      <bibkey>kruijt-vossen-2022-role</bibkey>
      <pwccode url="https://github.com/cltl/inner-outer-coreference" additional="false">cltl/inner-outer-coreference</pwccode>
    </paper>
  </volume>
  <volume id="mcr" ingest-date="2022-10-06">
    <meta>
      <booktitle>Proceedings of the CRAC 2022 Shared Task on Multilingual Coreference Resolution</booktitle>
      <editor><first>Zdeněk</first><last>Žabokrtský</last></editor>
      <editor><first>Maciej</first><last>Ogrodniczuk</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Gyeongju, Republic of Korea</address>
      <month>October</month>
      <year>2022</year>
      <url hash="0594e949">2022.crac-mcr</url>
      <venue>crac</venue>
    </meta>
    <frontmatter>
      <url hash="93c414fb">2022.crac-mcr.0</url>
      <bibkey>crac-2022-crac</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Findings of the Shared Task on Multilingual Coreference Resolution</title>
      <author><first>Zdeněk</first><last>Žabokrtský</last></author>
      <author><first>Miloslav</first><last>Konopík</last></author>
      <author><first>Anna</first><last>Nedoluzhko</last></author>
      <author><first>Michal</first><last>Novák</last></author>
      <author><first>Maciej</first><last>Ogrodniczuk</last></author>
      <author><first>Martin</first><last>Popel</last></author>
      <author><first>Ondřej</first><last>Pražák</last></author>
      <author><first>Jakub</first><last>Sido</last></author>
      <author><first>Daniel</first><last>Zeman</last></author>
      <author><first>Yilun</first><last>Zhu</last></author>
      <pages>1–17</pages>
      <abstract>This paper presents an overview of the shared task on multilingual coreference resolution associated with the CRAC 2022 workshop. Shared task participants were supposed to develop trainable systems capable of identifying mentions and clustering them according to identity coreference. The public edition of CorefUD 1.0, which contains 13 datasets for 10 languages, was used as the source of training and evaluation data. The CoNLL score used in previous coreference-oriented shared tasks was used as the main evaluation metric. There were 8 coreference prediction systems submitted by 5 participating teams; in addition, there was a competitive Transformer-based baseline system provided by the organizers at the beginning of the shared task. The winner system outperformed the baseline by 12 percentage points (in terms of the CoNLL scores averaged across all datasets for individual languages).</abstract>
      <url hash="33817126">2022.crac-mcr.1</url>
      <bibkey>zabokrtsky-etal-2022-findings</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/parcorfull">ParCorFull</pwcdataset>
    </paper>
    <paper id="2">
      <title>Coreference Resolution for <fixed-case>P</fixed-case>olish: Improvements within the <fixed-case>CRAC</fixed-case> 2022 Shared Task</title>
      <author><first>Karol</first><last>Saputa</last></author>
      <pages>18–22</pages>
      <abstract>The paper presents our system for coreference resolution in Polish. We compare the system with previous works for the Polish language as well as with the multilingual approach in the CRAC 2022 Shared Task on Multilingual Coreference Resolution thanks to a universal, multilingual data format and evaluation tool. We discuss the accuracy, computational performance, and evaluation approach of the new System which is a faster, end-to-end solution.</abstract>
      <url hash="9b3ffb4e">2022.crac-mcr.2</url>
      <bibkey>saputa-2022-coreference</bibkey>
    </paper>
    <paper id="3">
      <title>End-to-end Multilingual Coreference Resolution with Mention Head Prediction</title>
      <author><first>Ondřej</first><last>Pražák</last></author>
      <author><first>Miloslav</first><last>Konopik</last></author>
      <pages>23–27</pages>
      <abstract>This paper describes our approach to the CRAC 2022 Shared Task on Multilingual Coreference Resolution. Our model is based on a state-of-the-art end-to-end coreference resolution system. Apart from joined multilingual training, we improved our results with mention head prediction. We also tried to integrate dependency information into our model. Our system ended up in third place. Moreover, we reached the best performance on two datasets out of 13.</abstract>
      <url hash="695ef8e4">2022.crac-mcr.3</url>
      <bibkey>prazak-konopik-2022-end</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>ÚFAL</fixed-case> <fixed-case>C</fixed-case>or<fixed-case>P</fixed-case>ipe at <fixed-case>CRAC</fixed-case> 2022: Effectivity of Multilingual Models for Coreference Resolution</title>
      <author><first>Milan</first><last>Straka</last></author>
      <author><first>Jana</first><last>Straková</last></author>
      <pages>28–37</pages>
      <abstract>We describe the winning submission to the CRAC 2022 Shared Task on Multilingual Coreference Resolution. Our system first solves mention detection and then coreference linking on the retrieved spans with an antecedent-maximization approach, and both tasks are fine-tuned jointly with shared Transformer weights. We report results of finetuning a wide range of pretrained models. The center of this contribution are fine-tuned multilingual models. We found one large multilingual model with sufficiently large encoder to increase performance on all datasets across the board, with the benefit not limited only to the underrepresented languages or groups of typologically relative languages. The source code is available at https://github.com/ufal/crac2022-corpipe.</abstract>
      <url hash="e85a462b">2022.crac-mcr.4</url>
      <bibkey>straka-strakova-2022-ufal</bibkey>
      <pwccode url="https://github.com/ufal/crac2022-corpipe" additional="false">ufal/crac2022-corpipe</pwccode>
    </paper>
  </volume>
  <event id="crac-2022">
    <colocated>
      <volume-id>2022.codi-1</volume-id>
      <volume-id>2022.codi-crac</volume-id>
    </colocated>
  </event>
</collection>
