<?xml version='1.0' encoding='UTF-8'?>
<collection id="2021.calcs">
  <volume id="1" ingest-date="2021-05-24">
    <meta>
      <booktitle>Proceedings of the Fifth Workshop on Computational Approaches to Linguistic Code-Switching</booktitle>
      <editor><first>Thamar</first><last>Solorio</last></editor>
      <editor><first>Shuguang</first><last>Chen</last></editor>
      <editor><first>Alan W.</first><last>Black</last></editor>
      <editor><first>Mona</first><last>Diab</last></editor>
      <editor><first>Sunayana</first><last>Sitaram</last></editor>
      <editor><first>Victor</first><last>Soto</last></editor>
      <editor><first>Emre</first><last>Yilmaz</last></editor>
      <editor><first>Anirudh</first><last>Srinivasan</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>June</month>
      <year>2021</year>
      <url hash="85c83d62">2021.calcs-1</url>
      <venue>calcs</venue>
    </meta>
    <frontmatter>
      <url hash="238b0a33">2021.calcs-1.0</url>
      <bibkey>calcs-2021-approaches</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Political Discourse Analysis: A Case Study of Code Mixing and Code Switching in Political Speeches</title>
      <author><first>Dama</first><last>Sravani</last></author>
      <author><first>Lalitha</first><last>Kameswari</last></author>
      <author><first>Radhika</first><last>Mamidi</last></author>
      <pages>1–5</pages>
      <abstract>Political discourse is one of the most interesting data to study power relations in the framework of Critical Discourse Analysis. With the increase in the modes of textual and spoken forms of communication, politicians use language and linguistic mechanisms that contribute significantly in building their relationship with people, especially in a multilingual country like India with many political parties with different ideologies. This paper analyses code-mixing and code-switching in Telugu political speeches to determine the factors responsible for their usage levels in various social settings and communicative contexts. We also compile a detailed set of rules capturing dialectal variations between Standard and Telangana dialects of Telugu.</abstract>
      <url hash="347ff9a0">2021.calcs-1.1</url>
      <doi>10.18653/v1/2021.calcs-1.1</doi>
      <bibkey>sravani-etal-2021-political</bibkey>
    </paper>
    <paper id="2">
      <title>Challenges and Limitations with the Metrics Measuring the Complexity of Code-Mixed Text</title>
      <author><first>Vivek</first><last>Srivastava</last></author>
      <author><first>Mayank</first><last>Singh</last></author>
      <pages>6–14</pages>
      <abstract>Code-mixing is a frequent communication style among multilingual speakers where they mix words and phrases from two different languages in the same utterance of text or speech. Identifying and filtering code-mixed text is a challenging task due to its co-existence with monolingual and noisy text. Over the years, several code-mixing metrics have been extensively used to identify and validate code-mixed text quality. This paper demonstrates several inherent limitations of code-mixing metrics with examples from the already existing datasets that are popularly used across various experiments.</abstract>
      <url hash="0bede426">2021.calcs-1.2</url>
      <doi>10.18653/v1/2021.calcs-1.2</doi>
      <bibkey>srivastava-singh-2021-challenges</bibkey>
    </paper>
    <paper id="3">
      <title>Translate and Classify: Improving Sequence Level Classification for <fixed-case>E</fixed-case>nglish-<fixed-case>H</fixed-case>indi Code-Mixed Data</title>
      <author><first>Devansh</first><last>Gautam</last></author>
      <author><first>Kshitij</first><last>Gupta</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <pages>15–25</pages>
      <abstract>Code-mixing is a common phenomenon in multilingual societies around the world and is especially common in social media texts. Traditional NLP systems, usually trained on monolingual corpora, do not perform well on code-mixed texts. Training specialized models for code-switched texts is difficult due to the lack of large-scale datasets. Translating code-mixed data into standard languages like English could improve performance on various code-mixed tasks since we can use transfer learning from state-of-the-art English models for processing the translated data. This paper focuses on two sequence-level classification tasks for English-Hindi code mixed texts, which are part of the GLUECoS benchmark - Natural Language Inference and Sentiment Analysis. We propose using various pre-trained models that have been fine-tuned for similar English-only tasks and have shown state-of-the-art performance. We further fine-tune these models on the translated code-mixed datasets and achieve state-of-the-art performance in both tasks. To translate English-Hindi code-mixed data to English, we use mBART, a pre-trained multilingual sequence-to-sequence model that has shown competitive performance on various low-resource machine translation pairs and has also shown performance gains in languages that were not in its pre-training corpus.</abstract>
      <url hash="43f455d9">2021.calcs-1.3</url>
      <doi>10.18653/v1/2021.calcs-1.3</doi>
      <bibkey>gautam-etal-2021-translate</bibkey>
      <pwccode url="https://github.com/devanshg27/cm_translatify" additional="false">devanshg27/cm_translatify</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="4">
      <title>Gated Convolutional Sequence to Sequence Based Learning for <fixed-case>E</fixed-case>nglish-Hingilsh Code-Switched Machine Translation.</title>
      <author><first>Suman</first><last>Dowlagar</last></author>
      <author><first>Radhika</first><last>Mamidi</last></author>
      <pages>26–30</pages>
      <abstract>Code-Switching is the embedding of linguistic units or phrases from two or more languages in a single sentence. This phenomenon is practiced in all multilingual communities and is prominent in social media. Consequently, there is a growing need to understand code-switched translations by translating the code-switched text into one of the standard languages or vice versa. Neural Machine translation is a well-studied research problem in the monolingual text. In this paper, we have used the gated convolutional sequences to sequence networks for English-Hinglish translation. The convolutions in the model help to identify the compositional structure in the sequences more easily. The model relies on gating and performs multiple attention steps at encoder and decoder layers.</abstract>
      <url hash="aead8934">2021.calcs-1.4</url>
      <doi>10.18653/v1/2021.calcs-1.4</doi>
      <bibkey>dowlagar-mamidi-2021-gated</bibkey>
    </paper>
    <paper id="5">
      <title><fixed-case>IITP</fixed-case>-<fixed-case>MT</fixed-case> at <fixed-case>CALCS</fixed-case>2021: <fixed-case>E</fixed-case>nglish to <fixed-case>H</fixed-case>inglish Neural Machine Translation using Unsupervised Synthetic Code-Mixed Parallel Corpus</title>
      <author><first>Ramakrishna</first><last>Appicharla</last></author>
      <author><first>Kamal Kumar</first><last>Gupta</last></author>
      <author><first>Asif</first><last>Ekbal</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>31–35</pages>
      <abstract>This paper describes the system submitted by IITP-MT team to Computational Approaches to Linguistic Code-Switching (CALCS 2021) shared task on MT for English→Hinglish. We submit a neural machine translation (NMT) system which is trained on the synthetic code-mixed (cm) English-Hinglish parallel corpus. We propose an approach to create code-mixed parallel corpus from a clean parallel corpus in an unsupervised manner. It is an alignment based approach and we do not use any linguistic resources for explicitly marking any token for code-switching. We also train NMT model on the gold corpus provided by the workshop organizers augmented with the generated synthetic code-mixed parallel corpus. The model trained over the generated synthetic cm data achieves 10.09 BLEU points over the given test set.</abstract>
      <url hash="2ded8ad2">2021.calcs-1.5</url>
      <doi>10.18653/v1/2021.calcs-1.5</doi>
      <bibkey>appicharla-etal-2021-iitp</bibkey>
    </paper>
    <paper id="6">
      <title>Exploring Text-to-Text Transformers for <fixed-case>E</fixed-case>nglish to <fixed-case>H</fixed-case>inglish Machine Translation with Synthetic Code-Mixing</title>
      <author><first>Ganesh</first><last>Jawahar</last></author>
      <author><first>El Moatez Billah</first><last>Nagoudi</last></author>
      <author><first>Muhammad</first><last>Abdul-Mageed</last></author>
      <author><first>Laks</first><last>Lakshmanan, V.S.</last></author>
      <pages>36–46</pages>
      <abstract>We describe models focused at the understudied problem of translating between monolingual and code-mixed language pairs. More specifically, we offer a wide range of models that convert monolingual English text into Hinglish (code-mixed Hindi and English). Given the recent success of pretrained language models, we also test the utility of two recent Transformer-based encoder-decoder models (i.e., mT5 and mBART) on the task finding both to work well. Given the paucity of training data for code-mixing, we also propose a dependency-free method for generating code-mixed texts from bilingual distributed representations that we exploit for improving language model performance. In particular, armed with this additional data, we adopt a curriculum learning approach where we first finetune the language models on synthetic data then on gold code-mixed data. We find that, although simple, our synthetic code-mixing method is competitive with (and in some cases is even superior to) several standard methods (backtranslation, method based on equivalence constraint theory) under a diverse set of conditions. Our work shows that the mT5 model, finetuned following the curriculum learning procedure, achieves best translation performance (12.67 BLEU). Our models place first in the overall ranking of the English-Hinglish official shared task.</abstract>
      <url hash="b5947756">2021.calcs-1.6</url>
      <doi>10.18653/v1/2021.calcs-1.6</doi>
      <bibkey>jawahar-etal-2021-exploring</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/phinc">PHINC</pwcdataset>
    </paper>
    <paper id="7">
      <title><fixed-case>C</fixed-case>o<fixed-case>M</fixed-case>e<fixed-case>T</fixed-case>: Towards Code-Mixed Translation Using Parallel Monolingual Sentences</title>
      <author><first>Devansh</first><last>Gautam</last></author>
      <author><first>Prashant</first><last>Kodali</last></author>
      <author><first>Kshitij</first><last>Gupta</last></author>
      <author><first>Anmol</first><last>Goel</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <author><first>Ponnurangam</first><last>Kumaraguru</last></author>
      <pages>47–55</pages>
      <abstract>Code-mixed languages are very popular in multilingual societies around the world, yet the resources lag behind to enable robust systems on such languages. A major contributing factor is the informal nature of these languages which makes it difficult to collect code-mixed data. In this paper, we propose our system for Task 1 of CACLS 2021 to generate a machine translation system for English to Hinglish in a supervised setting. Translating in the given direction can help expand the set of resources for several tasks by translating valuable datasets from high resource languages. We propose to use mBART, a pre-trained multilingual sequence-to-sequence model, and fully utilize the pre-training of the model by transliterating the roman Hindi words in the code-mixed sentences to Devanagri script. We evaluate how expanding the input by concatenating Hindi translations of the English sentences improves mBART’s performance. Our system gives a BLEU score of 12.22 on test set. Further, we perform a detailed error analysis of our proposed systems and explore the limitations of the provided dataset and metrics.</abstract>
      <url hash="b23b1521">2021.calcs-1.7</url>
      <doi>10.18653/v1/2021.calcs-1.7</doi>
      <bibkey>gautam-etal-2021-comet</bibkey>
      <pwccode url="https://github.com/devanshg27/cm_translation" additional="false">devanshg27/cm_translation</pwccode>
    </paper>
    <paper id="8">
      <title>Investigating Code-Mixed <fixed-case>M</fixed-case>odern <fixed-case>S</fixed-case>tandard <fixed-case>A</fixed-case>rabic-<fixed-case>E</fixed-case>gyptian to <fixed-case>E</fixed-case>nglish Machine Translation</title>
      <author><first>El Moatez Billah</first><last>Nagoudi</last></author>
      <author><first>AbdelRahim</first><last>Elmadany</last></author>
      <author><first>Muhammad</first><last>Abdul-Mageed</last></author>
      <pages>56–64</pages>
      <abstract>Recent progress in neural machine translation (NMT) has made it possible to translate successfully between monolingual language pairs where large parallel data exist, with pre-trained models improving performance even further. Although there exists work on translating in code-mixed settings (where one of the pairs includes text from two or more languages), it is still unclear what recent success in NMT and language modeling exactly means for translating code-mixed text. We investigate one such context, namely MT from code-mixed Modern Standard Arabic and Egyptian Arabic (MSAEA) into English. We develop models under different conditions, employing both (i) standard end-to-end sequence-to-sequence (S2S) Transformers trained from scratch and (ii) pre-trained S2S language models (LMs). We are able to acquire reasonable performance using only MSA-EN parallel data with S2S models trained from scratch. We also find LMs fine-tuned on data from various Arabic dialects to help the MSAEA-EN task. Our work is in the context of the Shared Task on Machine Translation in Code-Switching. Our best model achieves 25.72 BLEU, placing us first on the official shared task evaluation for MSAEA-EN.</abstract>
      <url hash="642d258a">2021.calcs-1.8</url>
      <doi>10.18653/v1/2021.calcs-1.8</doi>
      <bibkey>nagoudi-etal-2021-investigating</bibkey>
    </paper>
    <paper id="9">
      <title>Much Gracias: Semi-supervised Code-switch Detection for <fixed-case>S</fixed-case>panish-<fixed-case>E</fixed-case>nglish: How far can we get?</title>
      <author><first>Dana-Maria</first><last>Iliescu</last></author>
      <author><first>Rasmus</first><last>Grand</last></author>
      <author><first>Sara</first><last>Qirko</last></author>
      <author><first>Rob</first><last>van der Goot</last></author>
      <pages>65–71</pages>
      <abstract>Because of globalization, it is becoming more and more common to use multiple languages in a single utterance, also called code-switching. This results in special linguistic structures and, therefore, poses many challenges for Natural Language Processing. Existing models for language identification in code-switched data are all supervised, requiring annotated training data which is only available for a limited number of language pairs. In this paper, we explore semi-supervised approaches, that exploit out-of-domain mono-lingual training data. We experiment with word uni-grams, word n-grams, character n-grams, Viterbi Decoding, Latent Dirichlet Allocation, Support Vector Machine and Logistic Regression. The Viterbi model was the best semi-supervised model, scoring a weighted F1 score of 92.23%, whereas a fully supervised state-of-the-art BERT-based model scored 98.43%.</abstract>
      <url hash="d33fab02">2021.calcs-1.9</url>
      <doi>10.18653/v1/2021.calcs-1.9</doi>
      <bibkey>iliescu-etal-2021-much</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/lince">LinCE</pwcdataset>
    </paper>
    <paper id="10">
      <title>A Language-aware Approach to Code-switched Morphological Tagging</title>
      <author><first>Şaziye Betül</first><last>Özateş</last></author>
      <author><first>Özlem</first><last>Çetinoğlu</last></author>
      <pages>72–83</pages>
      <abstract>Morphological tagging of code-switching (CS) data becomes more challenging especially when language pairs composing the CS data have different morphological representations. In this paper, we explore a number of ways of implementing a language-aware morphological tagging method and present our approach for integrating language IDs into a transformer-based framework for CS morphological tagging. We perform our set of experiments on the Turkish-German SAGT Treebank. Experimental results show that including language IDs to the learning model significantly improves accuracy over other approaches.</abstract>
      <url hash="8a6bdcaa">2021.calcs-1.10</url>
      <doi>10.18653/v1/2021.calcs-1.10</doi>
      <bibkey>ozates-cetinoglu-2021-language</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="11">
      <title>Can You Traducir This? Machine Translation for Code-Switched Input</title>
      <author><first>Jitao</first><last>Xu</last></author>
      <author><first>François</first><last>Yvon</last></author>
      <pages>84–94</pages>
      <abstract>Code-Switching (CSW) is a common phenomenon that occurs in multilingual geographic or social contexts, which raises challenging problems for natural language processing tools. We focus here on Machine Translation (MT) of CSW texts, where we aim to simultaneously disentangle and translate the two mixed languages. Due to the lack of actual translated CSW data, we generate artificial training data from regular parallel texts. Experiments show this training strategy yields MT systems that surpass multilingual systems for code-switched texts. These results are confirmed in an alternative task aimed at providing contextual translations for a L2 writing assistant.</abstract>
      <url hash="9dbd2693">2021.calcs-1.11</url>
      <doi>10.18653/v1/2021.calcs-1.11</doi>
      <bibkey>xu-yvon-2021-traducir</bibkey>
    </paper>
    <paper id="12">
      <title>On the logistical difficulties and findings of Jopara Sentiment Analysis</title>
      <author><first>Marvin</first><last>Agüero-Torales</last></author>
      <author><first>David</first><last>Vilares</last></author>
      <author><first>Antonio</first><last>López-Herrera</last></author>
      <pages>95–102</pages>
      <abstract>This paper addresses the problem of sentiment analysis for Jopara, a code-switching language between Guarani and Spanish. We first collect a corpus of Guarani-dominant tweets and discuss on the difficulties of finding quality data for even relatively easy-to-annotate tasks, such as sentiment analysis. Then, we train a set of neural models, including pre-trained language models, and explore whether they perform better than traditional machine learning ones in this low-resource setup. Transformer architectures obtain the best results, despite not considering Guarani during pre-training, but traditional machine learning models perform close due to the low-resource nature of the problem.</abstract>
      <url hash="95ba47a6">2021.calcs-1.12</url>
      <doi>10.18653/v1/2021.calcs-1.12</doi>
      <bibkey>aguero-torales-etal-2021-logistical</bibkey>
      <pwccode url="https://github.com/mmaguero/josa-corpus" additional="false">mmaguero/josa-corpus</pwccode>
    </paper>
    <paper id="13">
      <title>Unsupervised Self-Training for Sentiment Analysis of Code-Switched Data</title>
      <author><first>Akshat</first><last>Gupta</last></author>
      <author><first>Sargam</first><last>Menghani</last></author>
      <author><first>Sai Krishna</first><last>Rallabandi</last></author>
      <author><first>Alan W</first><last>Black</last></author>
      <pages>103–112</pages>
      <abstract>Sentiment analysis is an important task in understanding social media content like customer reviews, Twitter and Facebook feeds etc. In multilingual communities around the world, a large amount of social media text is characterized by the presence of Code-Switching. Thus, it has become important to build models that can handle code-switched data. However, annotated code-switched data is scarce and there is a need for unsupervised models and algorithms. We propose a general framework called Unsupervised Self-Training and show its applications for the specific use case of sentiment analysis of code-switched data. We use the power of pre-trained BERT models for initialization and fine-tune them in an unsupervised manner, only using pseudo labels produced by zero-shot transfer. We test our algorithm on multiple code-switched languages and provide a detailed analysis of the learning dynamics of the algorithm with the aim of answering the question - ‘Does our unsupervised model understand the Code-Switched languages or does it just learn its representations?’. Our unsupervised models compete well with their supervised counterparts, with their performance reaching within 1-7% (weighted F1 scores) when compared to supervised models trained for a two class problem.</abstract>
      <url hash="3508c0a7">2021.calcs-1.13</url>
      <doi>10.18653/v1/2021.calcs-1.13</doi>
      <bibkey>gupta-etal-2021-unsupervised</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/tweeteval">TweetEval</pwcdataset>
    </paper>
    <paper id="14">
      <title><fixed-case>C</fixed-case>odemixed<fixed-case>NLP</fixed-case>: An Extensible and Open <fixed-case>NLP</fixed-case> Toolkit for Code-Mixing</title>
      <author><first>Sai Muralidhar</first><last>Jayanthi</last></author>
      <author><first>Kavya</first><last>Nerella</last></author>
      <author><first>Khyathi Raghavi</first><last>Chandu</last></author>
      <author><first>Alan W</first><last>Black</last></author>
      <pages>113–118</pages>
      <abstract>The NLP community has witnessed steep progress in a variety of tasks across the realms of monolingual and multilingual language processing recently. These successes, in conjunction with the proliferating mixed language interactions on social media, have boosted interest in modeling code-mixed texts. In this work, we present CodemixedNLP, an open-source library with the goals of bringing together the advances in code-mixed NLP and opening it up to a wider machine learning community. The library consists of tools to develop and benchmark versatile model architectures that are tailored for mixed texts, methods to expand training sets, techniques to quantify mixing styles, and fine-tuned state-of-the-art models for 7 tasks in Hinglish. We believe this work has the potential to foster a distributed yet collaborative and sustainable ecosystem in an otherwise dispersed space of code-mixing research. The toolkit is designed to be simple, easily extensible, and resourceful to both researchers as well as practitioners. Demo: &lt;http://k-ikkees.pc.cs.cmu.edu:5000&gt; and Library: &lt;https://github.com/murali1996/CodemixedNLP&gt;</abstract>
      <url hash="8ab04c65">2021.calcs-1.14</url>
      <doi>10.18653/v1/2021.calcs-1.14</doi>
      <bibkey>jayanthi-etal-2021-codemixednlp</bibkey>
      <pwccode url="https://github.com/murali1996/CodemixedNLP" additional="false">murali1996/CodemixedNLP</pwccode>
    </paper>
    <paper id="15">
      <title>Normalization and Back-Transliteration for Code-Switched Data</title>
      <author><first>Dwija</first><last>Parikh</last></author>
      <author><first>Thamar</first><last>Solorio</last></author>
      <pages>119–124</pages>
      <abstract>Code-switching is an omnipresent phenomenon in multilingual communities all around the world but remains a challenge for NLP systems due to the lack of proper data and processing techniques. Hindi-English code-switched text on social media is often transliterated to the Roman script which prevents from utilizing monolingual resources available in the native Devanagari script. In this paper, we propose a method to normalize and back-transliterate code-switched Hindi-English text. In addition, we present a grapheme-to-phoneme (G2P) conversion technique for romanized Hindi data. We also release a dataset of script-corrected Hindi-English code-switched sentences labeled for the named entity recognition and part-of-speech tagging tasks to facilitate further research.</abstract>
      <url hash="c6d8a7d3">2021.calcs-1.15</url>
      <doi>10.18653/v1/2021.calcs-1.15</doi>
      <bibkey>parikh-solorio-2021-normalization</bibkey>
    </paper>
    <paper id="16">
      <title>Abusive content detection in transliterated <fixed-case>B</fixed-case>engali-<fixed-case>E</fixed-case>nglish social media corpus</title>
      <author><first>Salim</first><last>Sazzed</last></author>
      <pages>125–130</pages>
      <abstract>Abusive text detection in low-resource languages such as Bengali is a challenging task due to the inadequacy of resources and tools. The ubiquity of transliterated Bengali comments in social media makes the task even more involved as monolingual approaches cannot capture them. Unfortunately, no transliterated Bengali corpus is publicly available yet for abusive content analysis. Therefore, in this paper, we introduce an annotated Bengali corpus of 3000 transliterated Bengali comments categorized into two classes, abusive and non-abusive, 1500 comments for each. For baseline evaluations, we employ several supervised machine learning (ML) and deep learning-based classifiers. We find support vector machine (SVM) shows the highest efficacy for identifying abusive content. We make the annotated corpus freely available for the researcher to aid abusive content detection in Bengali social media data.</abstract>
      <url hash="b1df1791">2021.calcs-1.16</url>
      <doi>10.18653/v1/2021.calcs-1.16</doi>
      <bibkey>sazzed-2021-abusive</bibkey>
      <pwccode url="https://github.com/sazzadcsedu/abusivecorpus" additional="false">sazzadcsedu/abusivecorpus</pwccode>
    </paper>
    <paper id="17">
      <title>Developing <fixed-case>ASR</fixed-case> for <fixed-case>I</fixed-case>ndonesian-<fixed-case>E</fixed-case>nglish Bilingual Language Teaching</title>
      <author><first>Zara</first><last>Maxwell-Smith</last></author>
      <author><first>Ben</first><last>Foley</last></author>
      <pages>131–132</pages>
      <abstract>Usage-based analyses of teacher corpora and code-switching (Boztepe, 2003) are an important next stage in understanding language acquisition. Multilingual corpora are difficult to compile and a classroom setting adds pedagogy to the mix of factors which make this data so rich and problematic to classify. Using quantitative methods to understand language learning and teaching is difficult work as the ‘transcription bottleneck’ constrains the size of datasets. We found that using an automatic speech recognition (ASR) toolkit with a small set of training data is likely to speed data collection in this context (Maxwelll-Smith et al., 2020).</abstract>
      <url hash="7805f87e">2021.calcs-1.17</url>
      <doi>10.18653/v1/2021.calcs-1.17</doi>
      <bibkey>maxwelll-smith-foley-2021-developing</bibkey>
    </paper>
    <paper id="18">
      <title>Transliteration for Low-Resource Code-Switching Texts: Building an Automatic <fixed-case>C</fixed-case>yrillic-to-<fixed-case>L</fixed-case>atin Converter for <fixed-case>T</fixed-case>atar</title>
      <author><first>Chihiro</first><last>Taguchi</last></author>
      <author><first>Yusuke</first><last>Sakai</last></author>
      <author><first>Taro</first><last>Watanabe</last></author>
      <pages>133–140</pages>
      <abstract>We introduce a Cyrillic-to-Latin transliterator for the Tatar language based on subword-level language identification. The transliteration is a challenging task due to the following two reasons. First, because modern Tatar texts often contain intra-word code-switching to Russian, a different transliteration set of rules needs to be applied to each morpheme depending on the language, which necessitates morpheme-level language identification. Second, the fact that Tatar is a low-resource language, with most of the texts in Cyrillic, makes it difficult to prepare a sufficient dataset. Given this situation, we proposed a transliteration method based on subword-level language identification. We trained a language classifier with monolingual Tatar and Russian texts, and applied different transliteration rules in accord with the identified language. The results demonstrate that our proposed method outscores other Tatar transliteration tools, and imply that it correctly transcribes Russian loanwords to some extent.</abstract>
      <url hash="6f463e0e">2021.calcs-1.18</url>
      <doi>10.18653/v1/2021.calcs-1.18</doi>
      <bibkey>taguchi-etal-2021-transliteration</bibkey>
    </paper>
    <paper id="19">
      <title>Code-Mixing on Sesame Street: Dawn of the Adversarial Polyglots</title>
      <author><first>Samson</first><last>Tan</last></author>
      <author><first>Shafiq</first><last>Joty</last></author>
      <pages>141</pages>
      <abstract>Multilingual models have demonstrated impressive cross-lingual transfer performance. However, test sets like XNLI are monolingual at the example level. In multilingual communities, it is common for polyglots to code-mix when conversing with each other. Inspired by this phenomenon, we present two strong black-box adversarial attacks (one word-level, one phrase-level) for multilingual models that push their ability to handle code-mixed sentences to the limit. The former (PolyGloss) uses bilingual dictionaries to propose perturbations and translations of the clean example for sense disambiguation. The latter (Bumblebee) directly aligns the clean example with its translations before extracting phrases as perturbations. Bumblebee has a success rate of 89.75% against XLM-R-large, bringing its average accuracy of 79.85 down to 8.18 on XNLI. Finally, we propose an efficient adversarial training scheme, Code-mixed Adversarial Training (CAT), that trains in the same number of steps as the original model. Even after controlling for the extra training data introduced, CAT improves model accuracy when the model is prevented from relying on lexical overlaps (+3.45), with a negligible drop (-0.15 points) in performance on the original XNLI test set. t-SNE visualizations reveal that CAT improves a model’s language agnosticity. This paper will be published in the proceedings of NAACL-HLT 2021.</abstract>
      <url hash="85f19b06">2021.calcs-1.19</url>
      <doi>10.18653/v1/2021.calcs-1.19</doi>
      <bibkey>tan-joty-2021-code-mixing</bibkey>
      <video href="2021.calcs-1.19.mp4"/>
      <pwccode url="https://github.com/salesforce/adversarial-polyglots" additional="false">salesforce/adversarial-polyglots</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xquad">XQuAD</pwcdataset>
    </paper>
    <paper id="20">
      <title>Are Multilingual Models Effective in Code-Switching?</title>
      <author><first>Genta Indra</first><last>Winata</last></author>
      <author><first>Samuel</first><last>Cahyawijaya</last></author>
      <author><first>Zihan</first><last>Liu</last></author>
      <author><first>Zhaojiang</first><last>Lin</last></author>
      <author><first>Andrea</first><last>Madotto</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <pages>142–153</pages>
      <abstract>Multilingual language models have shown decent performance in multilingual and cross-lingual natural language understanding tasks. However, the power of these multilingual models in code-switching tasks has not been fully explored. In this paper, we study the effectiveness of multilingual language models to understand their capability and adaptability to the mixed-language setting by considering the inference speed, performance, and number of parameters to measure their practicality. We conduct experiments in three language pairs on named entity recognition and part-of-speech tagging and compare them with existing methods, such as using bilingual embeddings and multilingual meta-embeddings. Our findings suggest that pre-trained multilingual models do not necessarily guarantee high-quality representations on code-switching, while using meta-embeddings achieves similar results with significantly fewer parameters.</abstract>
      <url hash="05776079">2021.calcs-1.20</url>
      <doi>10.18653/v1/2021.calcs-1.20</doi>
      <bibkey>winata-etal-2021-multilingual</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/lince">LinCE</pwcdataset>
    </paper>
  </volume>
</collection>
