<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.analogyangle">
  <volume id="1" ingest-date="2025-07-18" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 2nd Workshop on Analogical Abstraction in Cognition, Perception, and Language (Analogy-Angle II)</booktitle>
      <editor><first>Giulia</first><last>Rambelli</last></editor>
      <editor><first>Filip</first><last>Ilievski</last></editor>
      <editor><first>Marianna</first><last>Bolognesi</last></editor>
      <editor><first>Pia</first><last>Sommerauer</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Vienna, Austria</address>
      <month>August</month>
      <year>2025</year>
      <url hash="3d46d847">2025.analogyangle-1</url>
      <venue>analogyangle</venue>
      <venue>ws</venue>
      <isbn>979-8-89176-274-9</isbn>
      <doi>10.18653/v1/2025.analogyangle-1</doi>
    </meta>
    <frontmatter>
      <url hash="154bbf83">2025.analogyangle-1.0</url>
      <bibkey>analogy-angle-ws-2025-1</bibkey>
      <doi>10.18653/v1/2025.analogyangle-1.0</doi>
    </frontmatter>
    <paper id="1">
      <title>Tore-Klose: Record Scorer, Goal Hunter, Machine? Human Association Norms for <fixed-case>G</fixed-case>erman Personal Name Compounds</title>
      <author><first>Annerose</first><last>Eichel</last><affiliation>University of Stuttgart, Universität Stuttgart</affiliation></author>
      <author><first>Tana</first><last>Deeg</last></author>
      <author><first>Andre</first><last>Blessing</last><affiliation>University of Stuttgart, Universität Stuttgart</affiliation></author>
      <author><first>Milena</first><last>Belosevic</last><affiliation>Universität Bielefeld</affiliation></author>
      <author><first>Sabine</first><last>Arndt-Lappe</last><affiliation>Trier University</affiliation></author>
      <author><first>Sabine</first><last>Schulte Im Walde</last><affiliation>University of Stuttgart</affiliation></author>
      <pages>1-9</pages>
      <abstract>We present a collection of human association norms to German personal name compounds (PNCs) such as “Tore-Klose” (goal-Klose) and corresponding full names (Miroslav Klose), thus providing a novel testbed for PNC evaluation, i.e., analogical vs. contrastive positive vs. negative perception effects. The associations are obtained in an online experiment with German native speakers, analyzed regarding our novel intertwined PNC–person association setup, and accompanied by an LLM synthetic generation approach for augmentation.</abstract>
      <url hash="d3c8b9ec">2025.analogyangle-1.1</url>
      <bibkey>eichel-etal-2025-tore</bibkey>
      <doi>10.18653/v1/2025.analogyangle-1.1</doi>
    </paper>
    <paper id="2">
      <title>Using Large Language Models to Perform <fixed-case>MIPVU</fixed-case>-Inspired Automatic Metaphor Detection</title>
      <author><first>Sebastian</first><last>Reimann</last><affiliation>Ruhr-Universität Bochum</affiliation></author>
      <author><first>Tatjana</first><last>Scheffler</last><affiliation>Ruhr-Universtät Bochum</affiliation></author>
      <pages>10-21</pages>
      <abstract>Automatic metaphor detection has often been inspired by linguistic procedures for manual metaphor identification. In this work, we test how closely the steps required by the Metaphor Identification Procedure VU Amsterdam (MIPVU) can be translated into prompts for generative Large Language Models (LLMs) and how well three commonly used LLMs are able to perform these steps. We find that while the procedure itself can be modeled with only a few compromises, neither language model is able to match the performance of supervised, fine-tuned methods for metaphor detection. All models failed to sufficiently filter out literal examples, where no contrast between the contextual and a more basic or concrete meaning was present. Both versions of LLaMa however signaled interesting potentials in detecting similarities between literal and metaphoric meanings that may be exploited in further work.</abstract>
      <url hash="c3dd6554">2025.analogyangle-1.2</url>
      <bibkey>reimann-scheffler-2025-using</bibkey>
      <doi>10.18653/v1/2025.analogyangle-1.2</doi>
    </paper>
    <paper id="3">
      <title>Modeling Background Knowledge with Frame Semantics for Fine-grained Sentiment Classification</title>
      <author><first>Muhammad Okky</first><last>Ibrohim</last><affiliation>University of Turin</affiliation></author>
      <author><first>Valerio</first><last>Basile</last><affiliation>University of Turin</affiliation></author>
      <author><first>Danilo</first><last>Croce</last></author>
      <author><first>Cristina</first><last>Bosco</last><affiliation>University of Turin</affiliation></author>
      <author><first>Roberto</first><last>Basili</last><affiliation>University of Roma, Tor Vergata</affiliation></author>
      <pages>22-36</pages>
      <abstract>Few-shot learning via in-context learning (ICL) is widely used in NLP, but its effectiveness is highly sensitive to example selection, often leading to unstable performance. To address this, we introduce BacKGen, a framework for generating structured Background Knowledge (BK) as an alternative to instance-based prompting. Our approach leverages Frame Semantics to uncover recurring conceptual patterns across data instances, clustering examples based on shared event structures and semantic roles. These patterns are then synthesized into generalized knowledge statements using a large language model (LLM) and injected into prompts to support contextual reasoning beyond surface-level cues. We apply BacKGen to Sentiment Phrase Classification (SPC), a task where polarity judgments frequently depend on implicit commonsense knowledge. In this setting, BK serves as an abstract representation of prototypical scenarios, enabling schematic generalization to help the model perform analogical reasoning by mapping new inputs onto generalized event structures. Experimental results with Mistral-7B and Llama3-8B demonstrate that BK-based prompting consistently outperforms standard few-shot approaches, achieving up to 29.94% error reduction.</abstract>
      <url hash="0c7841f7">2025.analogyangle-1.3</url>
      <bibkey>ibrohim-etal-2025-modeling</bibkey>
      <doi>10.18653/v1/2025.analogyangle-1.3</doi>
    </paper>
    <paper id="4">
      <title>On choosing the vehicles of metaphors without a body: evidence from Large Language Models</title>
      <author><first>Veronica</first><last>Mangiaterra</last></author>
      <author><first>Chiara</first><last>Barattieri Di San Pietro</last><affiliation>Istituto Universitario di Studi Superiori</affiliation></author>
      <author><first>Federico</first><last>Frau</last><affiliation>Istituto Universitario di Studi Superiori</affiliation></author>
      <author><first>Valentina</first><last>Bambini</last><affiliation>Istituto Universitario di Studi Superiori</affiliation></author>
      <author><first>Hamad</first><last>Al-Azary</last><affiliation>Lawrence Technological University</affiliation></author>
      <pages>37-44</pages>
      <abstract>Since the advent of Large Language Models (LLMs), much work has been devoted to comparing the linguistic abilities of humans and machines. Figurative language, which is known to rely on pragmatic inferential processes as well as lexical-semantic, sensorimotor, and socio-cognitive information, has been often used as a benchmark for this comparison. In the present study, we build on previous behavioral evidence showing that both distributional and sensorimotor variables come into play when people are asked to produce novel and apt metaphors and examine the behavior of LLMs in the same task. We show that, while distributional features still hold a special status, LLMs are insensitive to the sensorimotor aspects of words. This points to the lack of human-like experience-based grounding in LLMs trained on linguistic input only, while offering further support to the multimodality of conceptual knowledge involved in metaphor processes in humans.</abstract>
      <url hash="4ec7f136">2025.analogyangle-1.4</url>
      <bibkey>mangiaterra-etal-2025-choosing</bibkey>
      <doi>10.18653/v1/2025.analogyangle-1.4</doi>
    </paper>
    <paper id="5">
      <title>Prompting Metaphoricity: Soft Labeling with Large Language Models in Popular Communication of Science Tweets in <fixed-case>S</fixed-case>panish</title>
      <author><first>Alec</first><last>Sánchez-Montero</last><affiliation>Universidad Nacional Autónoma de México</affiliation></author>
      <author><first>Gemma</first><last>Bel-Enguix</last><affiliation>Universidad Nacional Autonoma de Mexico</affiliation></author>
      <author><first>Sergio-Luis</first><last>Ojeda-Trueba</last></author>
      <author><first>Gerardo</first><last>Sierra</last><affiliation>Universidad Nacional Autónoma de México</affiliation></author>
      <pages>45-56</pages>
      <abstract>In this paper, we explore how large language models (LLMs) can be used to assign soft labels for metaphoricity in Popular Communication of Science (PCS) tweets written in Spanish. Instead of treating metaphors as a binary yes/no phenomenon, we focus on their graded nature and the variability commonly found in human annotations. Through a combination of prompt design and quantitative evaluation over a stratified sample of our dataset, we show that GPT-4 can assign probabilistic scores not only for general metaphoricity but also for specific metaphor types with consistency (Direct, Indirect, and Personification). The results show that, while LLMs align reasonably well with average human judgments for some categories, capturing the subtle patterns of inter-annotator disagreement remains a challenge. We present a corpus of 3,733 tweets annotated with LLM-generated soft labels, a valuable resource for further metaphor analysis in scientific discourse and figurative language annotation with LLMs.</abstract>
      <url hash="a2ae8aec">2025.analogyangle-1.5</url>
      <bibkey>sanchez-montero-etal-2025-prompting</bibkey>
      <doi>10.18653/v1/2025.analogyangle-1.5</doi>
    </paper>
    <paper id="6">
      <title><fixed-case>HATS</fixed-case> : <fixed-case>H</fixed-case>indi Analogy Test Set for Evaluating Reasoning in Large Language Models</title>
      <author><first>Ashray</first><last>Gupta</last></author>
      <author><first>Rohan</first><last>Joseph</last></author>
      <author><first>Sunny</first><last>Rai</last><affiliation>School of Engineering and Applied Science, University of Pennsylvania</affiliation></author>
      <pages>57-80</pages>
      <abstract>Analogies test a model’s ability to infer implicit relationships between concepts, making them a key benchmark for evaluating reasoning capabilities. While large language models (LLMs) are widely evaluated for reasoning in English, their abilities in Indic languages remain understudied, limiting our understanding of whether these models generalize across languages. To address this gap, we introduce a new Hindi Analogy Test Set (HATS), comprising 405 multiple-choice questions sourced from Indian government exams. We benchmark state-of-the-art multilingual LLMs using various prompting strategies and introduce a grounded Chain of Thought approach that leverages cognitive theories of analogical reasoning. This approach improves model performance on Hindi analogy questions. Our experiments show that models perform best with English prompts, irrespective of the prompting strategy. Our test set addresses the lack of a critical resource to evaluate LLM reasoning capabilities in Hindi. The test set is publicly available for research purposes here https://github.com/Inequilazitive/HATS-Hindi_Analogy_Test_Set</abstract>
      <url hash="c381f7e7">2025.analogyangle-1.6</url>
      <bibkey>gupta-etal-2025-hats</bibkey>
      <doi>10.18653/v1/2025.analogyangle-1.6</doi>
    </paper>
    <paper id="7">
      <title>Simulating Emotional Intelligence in <fixed-case>LLM</fixed-case>s through Behavioral Conditioning and Analogical Retrieval</title>
      <author><first>G.Sai Linisha</first><last>Reddy</last></author>
      <author><first>Mounil Hiren</first><last>Kankhara</last></author>
      <author><first>Mridul</first><last>Maheshwari</last></author>
      <author><first>Swayam</first><last>Bansal</last></author>
      <author><first>Rishit</first><last>Kapoor</last></author>
      <author><first>Himesh Reddy</first><last>M</last></author>
      <author><first>Bagesh</first><last>Kumar</last></author>
      <pages>81-91</pages>
      <abstract>Human emotional expression emerges from a complex interplay of verbal, para-verbal, and non-verbal cues. This paper presents a dual-path framework for emotionally grounded text generation in large language models by integrating behavioral metadata with analogical retrieval. We introduce the MECC (Multimodal Emotionally Conditioned Corpus), a dataset of 1,764 question-answer pairs collected via structured interviews and annotated across 15 emotion categories with tone, response time, and body language. A LLaMA-3.1–8B–Instruct model is fine-tuned on MECC using behavior-encoded prompts, and inference is supported by a metadata-filtered Retrieval-Augmented Generation (RAG) pipeline. Detailed emotion-level analysis reveals trade-offs between emotional fidelity and semantic diversity, emphasizing the need for nuanced evaluation. This study contributes a richly annotated multimodal emotion corpus, a metadata-driven RAG architecture, a well-structured framework for building emotionally aware language models.Our code is available at https://github.com/MetaResearcher/Framework</abstract>
      <url hash="3752d05d">2025.analogyangle-1.7</url>
      <bibkey>reddy-etal-2025-simulating</bibkey>
      <doi>10.18653/v1/2025.analogyangle-1.7</doi>
    </paper>
    <paper id="8">
      <title>Can Stories Help <fixed-case>LLM</fixed-case>s Reason? Curating Information Space Through Narrative</title>
      <author><first>Vahid</first><last>Sadiri Javadi</last></author>
      <author><first>Johanne</first><last>Trippas</last><affiliation>Royal Melbourne Institute of Technology</affiliation></author>
      <author><first>Yash Kumar</first><last>Lal</last><affiliation>State University of New York, Stony Brook</affiliation></author>
      <author><first>Lucie</first><last>Flek</last><affiliation>Rheinische Friedrich-Wilhelms Universität Bonn</affiliation></author>
      <pages>92-107</pages>
      <abstract>Narratives are widely recognized as a powerful tool for structuring information and facilitating comprehension of complex ideas in various domains such as science communication. This paper explores whether generating narratives can serve “as a specialized mode of thinking” that improves the reasoning abilities of Large Language Models (LLMs). We introduce Story of Thought (SoT), a novel prompt-driven reasoning framework that guides LLMs to construct narratives around the problem statement to solve the task more effectively. SoT enables LLMs to integrate narrative techniques such as metaphor and analogy into their reasoning process. Our experiments show that SoT significantly improves the LLMs’ problem-solving abilities on various tasks, including physics, chemistry, and biology in both JEEBench and GPQA (e.g., SoT resulted in 13% improvement compared to CoT when using GPT-4). To validate LLM-based evaluation for generated narratives, we conduct a human annotation of the narrative techniques used by LLMs. Our results show strong inter-annotator agreement between Llama 3 70B and human annotators. This work brings LLM reasoning closer to human cognitive processes by mirroring mechanisms such as analogical problem-solving, which are central to how humans understand and process complex ideas.</abstract>
      <url hash="906c88d2">2025.analogyangle-1.8</url>
      <bibkey>sadiri-javadi-etal-2025-stories</bibkey>
      <doi>10.18653/v1/2025.analogyangle-1.8</doi>
    </paper>
    <paper id="9">
      <title>Testing Spatial Intuitions of Humans and Large Language and Multimodal Models in Analogies</title>
      <author><first>Ivo</first><last>Bueno</last></author>
      <author><first>Anna</first><last>Bavaresco</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>João Miguel</first><last>Cunha</last><affiliation>Universidade de Coimbra</affiliation></author>
      <author><first>Philipp</first><last>Wicke</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <pages>108-132</pages>
      <abstract>Language and Vision-Language Models exhibit impressive language capabilities akin to human reasoning. However, unlike humans who acquire language through embodied, interactive experiences, these models learn from static datasets without real-world interaction. This difference raises questions about how they conceptualize abstract notions and whether their reasoning aligns with human cognition. We investigate spatial conceptualizations of LLMs and VLMs by conducting analogy prompting studies with LLMs, VLMs, and human participants. We assess their ability to generate and interpret analogies for spatial concepts. We quantitatively compare the analogies produced by each group, examining the impact of multimodal inputs and reasoning mechanisms. Our findings indicate that generative models can produce and interpret analogies but differ significantly from human reasoning in their abstraction of spatial concepts - variability influenced by input modality, model size, and prompting methods, with analogy-based prompts not consistently enhancing alignment. Contributions include a methodology for probing generative models through analogies; a comparative analysis of analogical reasoning among models, and humans; and insights into the effect of multimodal inputs on reasoning.</abstract>
      <url hash="4e667f24">2025.analogyangle-1.9</url>
      <bibkey>bueno-etal-2025-testing</bibkey>
      <doi>10.18653/v1/2025.analogyangle-1.9</doi>
    </paper>
  </volume>
</collection>
