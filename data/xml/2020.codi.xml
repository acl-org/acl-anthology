<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.codi">
  <volume id="1" ingest-date="2020-11-06">
    <meta>
      <booktitle>Proceedings of the First Workshop on Computational Approaches to Discourse</booktitle>
      <editor><first>Chloé</first><last>Braud</last></editor>
      <editor><first>Christian</first><last>Hardmeier</last></editor>
      <editor><first>Junyi Jessy</first><last>Li</last></editor>
      <editor><first>Annie</first><last>Louis</last></editor>
      <editor><first>Michael</first><last>Strube</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>November</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="f32c0e1b">2020.codi-1.0</url>
    </frontmatter>
    <paper id="1">
      <title>How does discourse affect <fixed-case>S</fixed-case>panish-<fixed-case>C</fixed-case>hinese Translation? A case study based on a <fixed-case>S</fixed-case>panish-<fixed-case>C</fixed-case>hinese parallel corpus</title>
      <author><first>Shuyuan</first><last>Cao</last></author>
      <pages>1–10</pages>
      <abstract>With their huge speaking populations in the world, Spanish and Chinese occupy important positions in linguistic studies. Since the two languages come from different language systems, the translation between Spanish and Chinese is complicated. A comparative study for the language pair can discover the discourse differences between Spanish and Chinese, and can benefit the Spanish-Chinese translation. In this work, based on a Spanish-Chinese parallel corpus annotated with discourse information, we compare the annotation results between the language pair and analyze how discourse affects Spanish-Chinese translation. The research results in our study can help human translators who work with the language pair.</abstract>
      <url hash="ceb51ab7">2020.codi-1.1</url>
      <doi>10.18653/v1/2020.codi-1.1</doi>
    </paper>
    <paper id="2">
      <title>Beyond Adjacency Pairs: Hierarchical Clustering of Long Sequences for Human-Machine Dialogues</title>
      <author><first>Maitreyee</first><last>Maitreyee</last></author>
      <pages>11–19</pages>
      <abstract>This work proposes a framework to predict sequences in dialogues, using turn based syntactic features and dialogue control functions. Syntactic features were extracted using dependency parsing, while dialogue control functions were manually labelled. These features were transformed using tf-idf and word embedding; feature selection was done using Principal Component Analysis (PCA). We ran experiments on six combinations of features to predict sequences with Hierarchical Agglomerative Clustering. An analysis of the clustering results indicate that using word embeddings and syntactic features, significantly improved the results.</abstract>
      <url hash="4642577e">2020.codi-1.2</url>
      <doi>10.18653/v1/2020.codi-1.2</doi>
    </paper>
    <paper id="3">
      <title>Using Type Information to Improve Entity Coreference Resolution</title>
      <author><first>Sopan</first><last>Khosla</last></author>
      <author><first>Carolyn</first><last>Rose</last></author>
      <pages>20–31</pages>
      <abstract>Coreference resolution (CR) is an essential part of discourse analysis. Most recently, neural approaches have been proposed to improve over SOTA models from earlier paradigms. So far none of the published neural models leverage external semantic knowledge such as type information. This paper offers the first such model and evaluation, demonstrating modest gains in accuracy by introducing either gold standard or predicted types. In the proposed approach, type information serves both to (1) improve mention representation and (2) create a soft type consistency check between coreference candidate mentions. Our evaluation covers two different grain sizes of types over four different benchmark corpora.</abstract>
      <url hash="4688eb6d">2020.codi-1.3</url>
      <doi>10.18653/v1/2020.codi-1.3</doi>
    </paper>
    <paper id="4">
      <title>Exploring Span Representations in Neural Coreference Resolution</title>
      <author><first>Patrick</first><last>Kahardipraja</last></author>
      <author><first>Olena</first><last>Vyshnevska</last></author>
      <author><first>Sharid</first><last>Loáiciga</last></author>
      <pages>32–41</pages>
      <abstract>In coreference resolution, span representations play a key role to predict coreference links accurately. We present a thorough examination of the span representation derived by applying BERT on coreference resolution (Joshi et al., 2019) using a probing model. Our results show that the span representation is able to encode a significant amount of coreference information. In addition, we find that the head-finding attention mechanism involved in creating the spans is crucial in encoding coreference knowledge. Last, our analysis shows that the span representation cannot capture non-local coreference as efficiently as local coreference.</abstract>
      <url hash="e959b0c7">2020.codi-1.4</url>
      <doi>10.18653/v1/2020.codi-1.4</doi>
    </paper>
    <paper id="5">
      <title>Supporting Comedy Writers: Predicting Audience’s Response from Sketch Comedy and Crosstalk Scripts</title>
      <author><first>Maolin</first><last>Li</last></author>
      <pages>42–52</pages>
      <abstract>Sketch comedy and crosstalk are two popular types of comedy. They can relieve people’s stress and thus benefit their mental health, especially when performances and scripts are high-quality. However, writing a script is time-consuming and its quality is difficult to achieve. In order to minimise the time and effort needed for producing an excellent script, we explore ways of predicting the audience’s response from the comedy scripts. For this task, we present a corpus of annotated scripts from popular television entertainment programmes in recent years. Annotations include a) text classification labels, indicating which actor’s lines made the studio audience laugh; b) information extraction labels, i.e. the text spans that made the audience laughed immediately after the performers said them. The corpus will also be useful for dialogue systems and discourse analysis, since our annotations are based on entire scripts. In addition, we evaluate different baseline algorithms. Experimental results demonstrate that BERT models can achieve the best predictions among all the baseline methods. Furthermore, we conduct an error analysis and investigate predictions across scripts with different styles.</abstract>
      <url hash="a0c41024">2020.codi-1.5</url>
      <doi>10.18653/v1/2020.codi-1.5</doi>
    </paper>
    <paper id="6">
      <title>Exploring Coreference Features in Heterogeneous Data</title>
      <author><first>Ekaterina</first><last>Lapshinova-Koltunski</last></author>
      <author><first>Kerstin</first><last>Kunz</last></author>
      <pages>53–64</pages>
      <abstract>The present paper focuses on variation phenomena in coreference chains. We address the hypothesis that the degree of structural variation between chain elements depends on language-specific constraints and preferences and, even more, on the communicative situation of language production. We define coreference features that also include reference to abstract entities and events. These features are inspired through several sources – cognitive parameters, pragmatic factors and typological status. We pay attention to the distributions of these features in a dataset containing English and German texts of spoken and written discourse mode, which can be classified into seven different registers. We apply text classification and feature selection to find out how these variational dimensions (language, mode and register) impact on coreference features. Knowledge on the variation under analysis is valuable for contrastive linguistics, translation studies and multilingual natural language processing (NLP), e.g. machine translation or cross-lingual coreference resolution.</abstract>
      <url hash="8d1943cb">2020.codi-1.6</url>
      <doi>10.18653/v1/2020.codi-1.6</doi>
    </paper>
    <paper id="7">
      <title>Contextualized Embeddings for Connective Disambiguation in Shallow Discourse Parsing</title>
      <author><first>René</first><last>Knaebel</last></author>
      <author><first>Manfred</first><last>Stede</last></author>
      <pages>65–75</pages>
      <abstract>This paper studies a novel model that simplifies the disambiguation of connectives for explicit discourse relations. We use a neural approach that integrates contextualized word embeddings and predicts whether a connective candidate is part of a discourse relation or not. We study the influence of those context-specific embeddings. Further, we show the benefit of training the tasks of connective disambiguation and sense classification together at the same time. The success of our approach is supported by state-of-the-art results.</abstract>
      <url hash="511142b1">2020.codi-1.7</url>
      <doi>10.18653/v1/2020.codi-1.7</doi>
    </paper>
    <paper id="8">
      <title><fixed-case>DSNDM</fixed-case>: Deep <fixed-case>S</fixed-case>iamese Neural Discourse Model with Attention for Text Pairs Categorization and Ranking</title>
      <author><first>Alexander</first><last>Chernyavskiy</last></author>
      <author><first>Dmitry</first><last>Ilvovsky</last></author>
      <pages>76–85</pages>
      <abstract>In this paper, the utility and advantages of the discourse analysis for text pairs categorization and ranking are investigated. We consider two tasks in which discourse structure seems useful and important: automatic verification of political statements, and ranking in question answering systems. We propose a neural network based approach to learn the match between pairs of discourse tree structures. To this end, the neural TreeLSTM model is modified to effectively encode discourse trees and DSNDM model based on it is suggested to analyze pairs of texts. In addition, the integration of the attention mechanism in the model is proposed. Moreover, different ranking approaches are investigated for the second task. In the paper, the comparison with state-of-the-art methods is given. Experiments illustrate that combination of neural networks and discourse structure in DSNDM is effective since it reaches top results in the assigned tasks. The evaluation also demonstrates that discourse analysis improves quality for the processing of longer texts.</abstract>
      <url hash="763c07db">2020.codi-1.8</url>
      <doi>10.18653/v1/2020.codi-1.8</doi>
    </paper>
    <paper id="9">
      <title>Do sentence embeddings capture discourse properties of sentences from Scientific Abstracts ?</title>
      <author><first>Laurine</first><last>Huber</last></author>
      <author><first>Chaker</first><last>Memmadi</last></author>
      <author><first>Mathilde</first><last>Dargnat</last></author>
      <author><first>Yannick</first><last>Toussaint</last></author>
      <pages>86–95</pages>
      <abstract>We introduce four tasks designed to determine which sentence encoders best capture discourse properties of sentences from scientific abstracts, namely coherence and cohesion between clauses of a sentence, and discourse relations within sentences. We show that even if contextual encoders such as BERT or SciBERT encodes the coherence in discourse units, they do not help to predict three discourse relations commonly used in scientific abstracts. We discuss what these results underline, namely that these discourse relations are based on particular phrasing that allow non-contextual encoders to perform well.</abstract>
      <url hash="f88b9d96">2020.codi-1.9</url>
      <doi>10.18653/v1/2020.codi-1.9</doi>
    </paper>
    <paper id="10">
      <title>Joint Modeling of Arguments for Event Understanding</title>
      <author><first>Yunmo</first><last>Chen</last></author>
      <author><first>Tongfei</first><last>Chen</last></author>
      <author><first>Benjamin</first><last>Van Durme</last></author>
      <pages>96–101</pages>
      <abstract>We recognize the task of event argument linking in documents as similar to that of intent slot resolution in dialogue, providing a Transformer-based model that extends from a recently proposed solution to resolve references to slots. The approach allows for joint consideration of argument candidates given a detected event, which we illustrate leads to state-of-the-art performance in multi-sentence argument linking.</abstract>
      <url hash="b66d3c30">2020.codi-1.10</url>
      <doi>10.18653/v1/2020.codi-1.10</doi>
    </paper>
    <paper id="11">
      <title>Analyzing Neural Discourse Coherence Models</title>
      <author><first>Youmna</first><last>Farag</last></author>
      <author><first>Josef</first><last>Valvoda</last></author>
      <author><first>Helen</first><last>Yannakoudakis</last></author>
      <author><first>Ted</first><last>Briscoe</last></author>
      <pages>102–112</pages>
      <abstract>In this work, we systematically investigate how well current models of coherence can capture aspects of text implicated in discourse organisation. We devise two datasets of various linguistic alterations that undermine coherence and test model sensitivity to changes in syntax and semantics. We furthermore probe discourse embedding space and examine the knowledge that is encoded in representations of coherence. We hope this study shall provide further insight into how to frame the task and improve models of coherence assessment further. Finally, we make our datasets publicly available as a resource for researchers to use to test discourse coherence models.</abstract>
      <url hash="475e34d1">2020.codi-1.11</url>
      <doi>10.18653/v1/2020.codi-1.11</doi>
    </paper>
    <paper id="12">
      <title>Computational Interpretations of Recency for the Choice of Referring Expressions in Discourse</title>
      <author><first>Fahime</first><last>Same</last></author>
      <author><first>Kees</first><last>van Deemter</last></author>
      <pages>113–123</pages>
      <abstract>First, we discuss the most common linguistic perspectives on the concept of recency and propose a taxonomy of recency metrics employed in Machine Learning studies for choosing the form of referring expressions in discourse context. We then report on a Multi-Layer Perceptron study and a Sequential Forward Search experiment, followed by Bayes Factor analysis of the outcomes. The results suggest that recency metrics counting paragraphs and sentences contribute to referential choice prediction more than other recency-related metrics. Based on the results of our analysis, we argue that, sensitivity to discourse structure is important for recency metrics used in determining referring expression forms.</abstract>
      <url hash="48d2673d">2020.codi-1.12</url>
      <doi>10.18653/v1/2020.codi-1.12</doi>
    </paper>
    <paper id="13">
      <title>Do We Really Need That Many Parameters In Transformer For Extractive Summarization? Discourse Can Help !</title>
      <author><first>Wen</first><last>Xiao</last></author>
      <author><first>Patrick</first><last>Huber</last></author>
      <author><first>Giuseppe</first><last>Carenini</last></author>
      <pages>124–134</pages>
      <abstract>The multi-head self-attention of popular transformer models is widely used within Natural Language Processing (NLP), including for the task of extractive summarization. With the goal of analyzing and pruning the parameter-heavy self-attention mechanism, there are multiple approaches proposing more parameter-light self-attention alternatives. In this paper, we present a novel parameter-lean self-attention mechanism using discourse priors. Our new tree self-attention is based on document-level discourse information, extending the recently proposed “Synthesizer” framework with another lightweight alternative. We show empirical results that our tree self-attention approach achieves competitive ROUGE-scores on the task of extractive summarization. When compared to the original single-head transformer model, the tree attention approach reaches similar performance on both, EDU and sentence level, despite the significant reduction of parameters in the attention component. We further significantly outperform the 8-head transformer model on sentence level when applying a more balanced hyper-parameter setting, requiring an order of magnitude less parameters.</abstract>
      <url hash="12e8ebbc">2020.codi-1.13</url>
      <doi>10.18653/v1/2020.codi-1.13</doi>
    </paper>
    <paper id="14">
      <title>Extending Implicit Discourse Relation Recognition to the <fixed-case>PDTB</fixed-case>-3</title>
      <author><first>Li</first><last>Liang</last></author>
      <author><first>Zheng</first><last>Zhao</last></author>
      <author><first>Bonnie</first><last>Webber</last></author>
      <pages>135–147</pages>
      <abstract>The PDTB-3 contains many more Implicit discourse relations than the previous PDTB-2. This is in part because implicit relations have now been annotated within sentences as well as between them. In addition, some now co-occur with explicit discourse relations, instead of standing on their own. Here we show that while this can complicate the problem of identifying the location of implicit discourse relations, it can in turn simplify the problem of identifying their senses. We present data to support this claim, as well as methods that can serve as a non-trivial baseline for future state-of-the-art recognizers for implicit discourse relations.</abstract>
      <url hash="d2d7553c">2020.codi-1.14</url>
      <doi>10.18653/v1/2020.codi-1.14</doi>
    </paper>
    <paper id="15">
      <title><fixed-case>TED</fixed-case>-<fixed-case>MDB</fixed-case> Lexicons: Tr-<fixed-case>E</fixed-case>n<fixed-case>C</fixed-case>onn<fixed-case>L</fixed-case>ex, Pt-<fixed-case>E</fixed-case>n<fixed-case>C</fixed-case>onn<fixed-case>L</fixed-case>ex</title>
      <author><first>Murathan</first><last>Kurfalı</last></author>
      <author><first>Sibel</first><last>Ozer</last></author>
      <author><first>Deniz</first><last>Zeyrek</last></author>
      <author><first>Amália</first><last>Mendes</last></author>
      <pages>148–153</pages>
      <abstract>In this work, we present two new bilingual discourse connective lexicons, namely, for Turkish-English and European Portuguese-English created automatically using the existing discourse relation-aligned TED-MDB corpus. In their current form, the Pt-En lexicon includes 95 entries, whereas the Tr-En lexicon contains 133 entries. The lexicons constitute the first step of a larger project of developing a multilingual discourse connective lexicon.</abstract>
      <url hash="3dcfa05e">2020.codi-1.15</url>
      <doi>10.18653/v1/2020.codi-1.15</doi>
    </paper>
    <paper id="16">
      <title>Evaluation of Coreference Resolution Systems Under Adversarial Attacks</title>
      <author><first>Haixia</first><last>Chai</last></author>
      <author><first>Wei</first><last>Zhao</last></author>
      <author><first>Steffen</first><last>Eger</last></author>
      <author><first>Michael</first><last>Strube</last></author>
      <pages>154–159</pages>
      <abstract>A substantial overlap of coreferent mentions in the CoNLL dataset magnifies the recent progress on coreference resolution. This is because the CoNLL benchmark fails to evaluate the ability of coreference resolvers that requires linking novel mentions unseen at train time. In this work, we create a new dataset based on CoNLL, which largely decreases mention overlaps in the entire dataset and exposes the limitations of published resolvers on two aspects—lexical inference ability and understanding of low-level orthographic noise. Our findings show (1) the requirements for embeddings, used in resolvers, and for coreference resolutions are, by design, in conflict and (2) adversarial approaches are sometimes not legitimate to mitigate the obstacles, as they may falsely introduce mention overlaps in adversarial training and test sets, thus giving an inflated impression for the improvements.</abstract>
      <url hash="ddf4ca9f">2020.codi-1.16</url>
      <doi>10.18653/v1/2020.codi-1.16</doi>
    </paper>
    <paper id="17">
      <title>Coreference for Discourse Parsing: A Neural Approach</title>
      <author><first>Grigorii</first><last>Guz</last></author>
      <author><first>Giuseppe</first><last>Carenini</last></author>
      <pages>160–167</pages>
      <abstract>We present preliminary results on investigating the benefits of coreference resolution features for neural RST discourse parsing by considering different levels of coupling of the discourse parser with the coreference resolver. In particular, starting with a strong baseline neural parser unaware of any coreference information, we compare a parser which utilizes only the output of a neural coreference resolver, with a more sophisticated model, where discourse parsing and coreference resolution are jointly learned in a neural multitask fashion. Results indicate that these initial attempts to incorporate coreference information do not boost the performance of discourse parsing in a statistically significant way.</abstract>
      <url hash="cf1e8a33">2020.codi-1.17</url>
      <doi>10.18653/v1/2020.codi-1.17</doi>
    </paper>
  </volume>
</collection>
