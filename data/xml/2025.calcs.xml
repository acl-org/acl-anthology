<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.calcs">
  <volume id="1" ingest-date="2025-04-26" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 7th Workshop on Computational Approaches to Linguistic Code-Switching</booktitle>
      <editor><first>Genta Indra</first><last>Winata</last></editor>
      <editor><first>Sudipta</first><last>Kar</last></editor>
      <editor><first>Marina</first><last>Zhukova</last></editor>
      <editor><first>Thamar</first><last>Solorio</last></editor>
      <editor><first>Xi</first><last>Ai</last></editor>
      <editor><first>Injy</first><last>Hamed</last></editor>
      <editor><first>Mahardika Krisna Krisna</first><last>Ihsani</last></editor>
      <editor><first>Derry Tanti</first><last>Wijaya</last></editor>
      <editor><first>Garry</first><last>Kuwanto</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Albuquerque, New Mexico, USA</address>
      <month>May</month>
      <year>2025</year>
      <url hash="95368786">2025.calcs-1</url>
      <venue>calcs</venue>
      <venue>ws</venue>
      <isbn>979-8-89176-053-0</isbn>
    </meta>
    <frontmatter>
      <url hash="9a12dff2">2025.calcs-1.0</url>
      <bibkey>calcs-ws-2025-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>E</fixed-case>uskañol<fixed-case>DS</fixed-case>: A Naturally Sourced Corpus for <fixed-case>B</fixed-case>asque-<fixed-case>S</fixed-case>panish Code-Switching</title>
      <author><first>Maite</first><last>Heredia</last><affiliation>HiTZ Center - Ixa, University of the Basque Country UPV/EHU</affiliation></author>
      <author><first>Jeremy</first><last>Barnes</last><affiliation>HiTZ Center - Ixa, University of the Basque Country UPV/EHU</affiliation></author>
      <author><first>Aitor</first><last>Soroa</last><affiliation>HiTZ Center - Ixa, University of the Basque Country UPV/EHU</affiliation></author>
      <pages>1-5</pages>
      <abstract>Code-switching (CS) remains a significant challenge in Natural Language Processing (NLP), mainly due a lack of relevant data. In the context of the contact between the Basque and Spanish languages in the north of the Iberian Peninsula, CS frequently occurs in both formal and informal spontaneous interactions. However, resources to analyse this phenomenon and support the development and evaluation of models capable of understanding and generating code-switched language for this language pair are almost non-existent. We introduce a first approach to develop a naturally sourced corpus for Basque-Spanish code-switching. Our methodology consists of identifying CS texts from previously available corpora using language identification models, which are then manually validated to obtain a reliable subset of CS instances. We present the properties of our corpus and make it available under the name EuskañolDS.</abstract>
      <url hash="43d74985">2025.calcs-1.1</url>
      <bibkey>heredia-etal-2025-euskanolds</bibkey>
    </paper>
    <paper id="2">
      <title>The Impact of Code-switched Synthetic Data Quality is Task Dependent: Insights from <fixed-case>MT</fixed-case> and <fixed-case>ASR</fixed-case></title>
      <author><first>Injy</first><last>Hamed</last><affiliation>MBZUAI</affiliation></author>
      <author><first>Thang</first><last>Vu</last><affiliation>University of Stuttgart</affiliation></author>
      <author><first>Nizar</first><last>Habash</last><affiliation>New York University Abu Dhabi</affiliation></author>
      <pages>6-17</pages>
      <abstract>Code-switching, the act of alternating between languages, emerged as a prevalent global phenomenon that needs to be addressed for building user-friendly language technologies. A main bottleneck in this pursuit is data scarcity, motivating research in the direction of code-switched data augmentation. However, current literature lacks comprehensive studies that enable us to understand the relation between the quality of synthetic data and improvements on NLP tasks. We extend previous research conducted in this direction on machine translation (MT) with results on automatic speech recognition (ASR) and cascaded speech translation (ST) to test generalizability of findings. Our experiments involve a wide range of augmentation techniques, covering lexical replacements, linguistic theories, and back-translation. Based on the results of MT, ASR, and ST, we draw conclusions and insights regarding the efficacy of various augmentation techniques and the impact of quality on performance.</abstract>
      <url hash="cb1f31c4">2025.calcs-1.2</url>
      <bibkey>hamed-etal-2025-impact</bibkey>
    </paper>
    <paper id="3">
      <title>Beyond Monolingual Limits: Fine-Tuning Monolingual <fixed-case>ASR</fixed-case> for <fixed-case>Y</fixed-case>oruba-<fixed-case>E</fixed-case>nglish Code-Switching</title>
      <author><first>Oreoluwa Boluwatife</first><last>Babatunde</last><affiliation>LyngualLabs</affiliation></author>
      <author><first>Victor Tolulope</first><last>Olufemi</last><affiliation>LyngualLabs</affiliation></author>
      <author><first>Emmanuel</first><last>Bolarinwa</last><affiliation>Lynguallabs</affiliation></author>
      <author><first>Kausar Yetunde</first><last>Moshood</last><affiliation>LyngualLabs</affiliation></author>
      <author><first>Chris Chinenye</first><last>Emezue</last><affiliation>Lanfrica, Mila-Quebec Institute</affiliation></author>
      <pages>18-25</pages>
      <abstract>Code-switching (CS) presents a significant challenge for Automatic Speech Recognition (ASR) systems, particularly in low-resource settings. While multilingual ASR models like OpenAI Whisper Large v3 are designed to handle multiple languages, their high computational demands make them less practical for real-world deployment in resource-constrained environments. In this study, we investigate the effectiveness of fine-tuning both monolingual and multilingual ASR models for Yoruba-English CS speech. Our results show that unadapted monolingual ASR models outperform Whisper Large v3 in a zero-shot setting on CS speech. Fine-tuning significantly reduces WER for both monolingual and multilingual models, with monolingual models achieving over a 20% WER reduction on CS and Yoruba speech while maintaining lower computational costs. However, we observe a trade-off, as fine-tuning leads to some degradation in English recognition, particularly for multilingual models. Our findings highlight that while multilingual models benefit from fine-tuning, monolingual models provide a computationally efficient and competitive alternative for CS-ASR, making them a viable choice for resource-constrained environments.</abstract>
      <url hash="54d8d045">2025.calcs-1.3</url>
      <bibkey>babatunde-etal-2025-beyond</bibkey>
    </paper>
    <paper id="4">
      <title>Where and How Do Languages Mix? A Study of <fixed-case>S</fixed-case>panish-<fixed-case>G</fixed-case>uaraní Code-Switching in <fixed-case>P</fixed-case>araguay</title>
      <author><first>Olga</first><last>Kellert</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Nemika</first><last>Tyagi</last><affiliation>Arizona State University</affiliation></author>
      <pages>26-31</pages>
      <abstract>Code-switching, the alternating use of multiple languages within a single utterance, is a widespread linguistic phenomenon that poses unique challenges for both sociolinguistic analysis and Natural Language Processing (NLP). While prior research has explored code-switching from either a syntactic or geographic perspective, few studies have integrated both aspects, particularly for underexplored language pairs like Spanish-Guaraní. In this paper, we analyze Spanish-Guaraní code-switching using a dataset of geotagged tweets from Asunción, Paraguay, collected from 2017 to 2021. We employ a differential distribution method to map the geographic distribution of code-switching across urban zones and analyze its syntactic positioning within sentences. Our findings reveal distinct spatial patterns, with Guaraní-dominant tweets concentrated in the western and southwestern areas, while Spanish-only tweets are more prevalent in central and eastern regions. Syntactic analysis shows that code-switching occurs most frequently in the middle of sentences, often involving verbs, pronouns, and adjectives. These results provide new insights into the interaction between linguistic, social, and geographic factors in bilingual communication. Our study contributes to both sociolinguistic research and NLP applications, offering a framework for analyzing mixed-language data in digital communication.</abstract>
      <url hash="4907e2eb">2025.calcs-1.4</url>
      <bibkey>kellert-tyagi-2025-languages</bibkey>
    </paper>
    <paper id="5">
      <title>Tongue-Tied: Breaking <fixed-case>LLM</fixed-case>s Safety Through New Language Learning</title>
      <author><first>Bibek</first><last>Upadhayay</last><affiliation>University of New Haven</affiliation></author>
      <author><first>Vahid</first><last>Behzadan</last><affiliation>University of New Haven</affiliation></author>
      <pages>32-47</pages>
      <abstract>The safety mechanisms of large language models (LLMs) have been shown to be fragile, as attackers can exploit prompts to generate harmful responses. Low-cost jailbreak attacks, such as those utilizing low-resource languages and code-switching, demonstrate that LLM safety mechanisms are vulnerable to low-resource languages. This indicates that safety training is particularly ineffective in low-resource languages. Furthermore, research has shown that fine-tuning LLMs with a small number of adversarial samples can compromise their safety training, implying that safety mechanism objectives can be overridden with the latest fine-tuning objectives. Based on the aforementioned statements, we hypothesize that the safety training of LLMs is language-dependent, and LLMs can potentially be compromised by fine-tuning them with new languages, even when using only harmless data.In this work, we used the low-resource language Newari and created two fake languages to LoRA-finetune LLMs with non-harmful data. Our results show that simply fine-tuning LLMs with new languages, even without the presence of harmful data, will jailbreak LLMs. Furthermore, we demonstrate that as we introduce English-to-and-from new language translation pairs in the training dataset, the attack success rate increases with harmful responses becoming more coherent. Additionally, we show the transferability of the attack by jailbreaking GPT-4 through finetuning with only 4,000 data points, and demonstrate that higher-capability models such as Claude-3.5-Sonnet can be compelled to learn to write in new languages through few-shot examples from in-context learning and can be jailbroken with new languages without fine-tuning. We furthermore investigate the fine-tuned LLMs’ latents with logit lens and find that the new language fine-tuning weakens safety mechanisms by prioritizing new language fidelity over alignment, enabling jailbreaks via late-layer pivots to new language tokens that bypass English-centric safeguards. We have publicly released our trained model weights, dataset, and artifacts at this URL: https://github.com/UNHSAILLab/tongue-tied-breaking-llms-safety-through-new-language-learning</abstract>
      <url hash="4117ca73">2025.calcs-1.5</url>
      <bibkey>upadhayay-behzadan-2025-tongue</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>L</fixed-case>exi<fixed-case>L</fixed-case>ogic@<fixed-case>CALCS</fixed-case> 2025: Predicting Preferences in Generated Code-Switched Text</title>
      <author><first>Pranav</first><last>Gupta</last><affiliation>Lowe’s</affiliation></author>
      <author><first>Souvik</first><last>Bhattacharyya</last><affiliation>Lowe’s</affiliation></author>
      <author><first>Niranjan Kumar</first><last>M</last><affiliation>Lowe’s</affiliation></author>
      <author><first>Billodal</first><last>Roy</last><affiliation>Lowe’s</affiliation></author>
      <pages>48-53</pages>
      <abstract>Code-switched generation is an emerging application in NLP systems, as code-switched text and speech are common and natural forms of conversation in multilingual communities worldwide. While monolingual generation has matured significantly with advances in large language models, code-switched generation still remains challenging, especially for languages and domains with less representation in pre-training datasets. In this paper, we describe our submission to the shared task of predicting human preferences for code-switched text in English-Malayalam, English-Tamil, and English-Hindi. We discuss our various approaches and report on the accuracy scores for each approach.</abstract>
      <url hash="091de5e8">2025.calcs-1.6</url>
      <bibkey>gupta-etal-2025-lexilogic</bibkey>
    </paper>
  </volume>
</collection>
