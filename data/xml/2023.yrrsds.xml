<?xml version='1.0' encoding='UTF-8'?>
<collection id="2023.yrrsds">
  <volume id="1" ingest-date="2023-11-09" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 19th Annual Meeting of the Young Reseachers' Roundtable on Spoken Dialogue Systems</booktitle>
      <editor><first>Vojtech</first><last>Hudecek</last></editor>
      <editor><first>Patricia</first><last>Schmidtova</last></editor>
      <editor><first>Tanvi</first><last>Dinkar</last></editor>
      <editor><first>Javier</first><last>Chiyah-Garcia</last></editor>
      <editor><first>Weronika</first><last>Sieinska</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Prague, Czechia</address>
      <month>September</month>
      <year>2023</year>
      <url hash="e1086b76">2023.yrrsds-1</url>
      <venue>yrrsds</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="97964136">2023.yrrsds-1.0</url>
      <bibkey>yrrsds-2023-young</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Processing Referential Ambiguities in Situated Dialogue Systems</title>
      <author><first>Javier</first><last>Chiyah-Garcia</last></author>
      <pages>1–4</pages>
      <abstract>Position paper for YRRSDS 2023</abstract>
      <url hash="63a7b8e1">2023.yrrsds-1.1</url>
      <bibkey>chiyah-garcia-2023-processing</bibkey>
    </paper>
    <paper id="2">
      <title>Safety and Robustness in Conversational <fixed-case>AI</fixed-case></title>
      <author><first>Tanvi</first><last>Dinkar</last></author>
      <pages>5–8</pages>
      <abstract>In this position paper, I will present the research interests in my PostDoc on safety and robustness specific to conversational AI, including then relevant overlap from my PhD.</abstract>
      <url hash="b3f1c209">2023.yrrsds-1.2</url>
      <bibkey>dinkar-2023-safety</bibkey>
    </paper>
    <paper id="3">
      <title>Incremental Speech Processing for Voice Assistant Accessibility</title>
      <author><first>Angus</first><last>Addlesee</last></author>
      <pages>9–11</pages>
      <abstract>Speech production is nuanced and unique to every individual, but today’s Spoken Dialogue Systems (SDSs) are trained to use general speech patterns to successfully improve performance on various evaluation metrics. However, these patterns do not apply to certain user groups - often the very people that can benefit the most from SDSs. For example, people with dementia produce more disfluent speech than the general population. The healthcare domain is now a popular setting for spoken dialogue and human-robot interaction research. This trend is similar when observing company behaviour. Charities promote industry voice assistants, the creators are getting HIPAA compliance, and their features sometimes target vulnerable user groups. It is therefore critical to adapt SDSs to be more accessible.</abstract>
      <url hash="798a62e3">2023.yrrsds-1.3</url>
      <bibkey>addlesee-2023-incremental</bibkey>
    </paper>
    <paper id="4">
      <title>Advancing Spoken Dialog Systems for Manufacturing: From Conceptual Architecture and Taxonomy to Real Case Applications and Future Directions</title>
      <author><first>Silvia</first><last>Colabianchi</last></author>
      <pages>12–14</pages>
      <abstract>This research encompasses a comprehensive exploration of Spoken Dialogue Systems (SDSs) in the manufacturing sector. It begins by establishing a conceptual architecture and taxonomy to guide the design and selection of SDS elements. Real case applications, including worker safety and cybersecurity support, validate the research findings and highlight areas for improvement. Looking ahead, the study delves into the potential of Large Language Models (LLMs) and multi-modal applications. Emphasizing the importance of extreme personalization, the study highlights the need to cater to the diverse qualifications and preferences of workers. Additionally, it investigates the integration of SDSs with other sensory modalities, such as images, videos, and augmented or virtual reality scenarios, to enhance the user experience and productivity. The research also addresses crucial considerations related to knowledge base optimization. It examines semantic variations of words across different application contexts, the continuous updating of procedures and data, and the adaptability of SDSs to diverse dialects and linguistic abilities, particularly in low-schooling personnel scenarios. Privacy, industrial protection, and ethical concerns in the era of LLMs and external players like OpenAI are given due attention. The study explores the boundaries of knowledge that conversational systems should possess, advocating for transparency, explainability, and responsible data handling practices.</abstract>
      <url hash="5e0208bd">2023.yrrsds-1.4</url>
      <bibkey>colabianchi-2023-advancing</bibkey>
    </paper>
    <paper id="5">
      <title>Conversational Grounding in Multimodal Dialog Systems</title>
      <author><first>Biswesh</first><last>Mohapatra</last></author>
      <pages>15–17</pages>
      <abstract>The process of “conversational grounding” is an interactive process that has been studied extensively in cognitive science, whereby participants in a conversation check to make sure their interlocutors understand what is being referred to. This interactive process uses multiple modes of communication to establish the information between the participants. This could include information provided through eye-gaze, head movements, intonation in speech, along with the content of the speech. While the process is essential to successful communication between humans and between humans and machines, work needs to be done on testing and building the capabilities of the current dialogue system in managing conversational grounding, especially in multimodal medium of communication. Recent work such as Benotti and Blackburn have shown the importance of conversational grounding in dialog systems and how current systems fail in them. This is essential for the advancement of Embodied Conversational Agents and Social Robots. Thus my PhD project aims to test, understand and improve the functioning of current dialog models with respect to Conversational Grounding.</abstract>
      <url hash="74e37877">2023.yrrsds-1.5</url>
      <bibkey>mohapatra-2023-conversational</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>SQL</fixed-case> Comment Generation and Additional Research Interests</title>
      <author><first>Alyssa</first><last>Allen</last></author>
      <pages>18–20</pages>
      <abstract>My research interests focus on natural language generation (NLG) regarding how to make system outputs more intuitive and comprehensible for the human-user and conversational entrainment and alignment from the perspective of how dialogue systems could or should personalize its responses to the human user. As it relates to NLG, my current work focuses on training a system to auto-generate comments for SQL queries produced by a Text-to-SQL parser. The goal is to make the connection between technical SQL language and the user’s question more transparent. My linguistic training lies primarily at the intersection of computational and socio-linguistics. As such, my curiosities in conversational entrainment and alignment focus on the extent to which conversational agents can or should adjust their language based on human characteristics such as age, race, or gender.</abstract>
      <url hash="2f2f9453">2023.yrrsds-1.6</url>
      <bibkey>allen-2023-sql</bibkey>
    </paper>
    <paper id="7">
      <title>On Referring Language Use in Visually Grounded Dialogue</title>
      <author><first>Bram</first><last>Willemsen</last></author>
      <pages>21–23</pages>
      <abstract>Position paper for YRRSDS 2023</abstract>
      <url hash="6d3dfd86">2023.yrrsds-1.7</url>
      <bibkey>willemsen-2023-referring</bibkey>
    </paper>
    <paper id="8">
      <title>Challenges and Approaches in Designing Social <fixed-case>SDS</fixed-case> in the <fixed-case>LLM</fixed-case> Era</title>
      <author><first>Koji</first><last>Inoue</last></author>
      <pages>24–25</pages>
      <abstract>Large language models (LLMs) have brought about a significant transformation in spoken dialogue systems (SDSs). It is anticipated that these systems will be implemented into diverse robotic applications and employed in a variety of social settings. The author presents research interest with the aim of realizing social SDSs from multiple perspectives, including task design, turn-taking mechanisms, and evaluation methodologies. Additionally, future research in social SDSs should delve into a deeper understanding of user mental states and a relationship with society via multi-party conversations. Finally, the author suggests topics for discussion regarding the future directions of SDS researchers in the LLM era.</abstract>
      <url hash="a8fcdb4d">2023.yrrsds-1.8</url>
      <bibkey>inoue-2023-challenges</bibkey>
    </paper>
    <paper id="9">
      <title>Breakdowns and Repairs. Detecting Patterns that Lead to Breakdowns in Customer Service Messages</title>
      <author><first>Anouck</first><last>Braggaar</last></author>
      <pages>26–29</pages>
      <abstract>Many companies use dialogue systems for their customer service, and although there has been a rise in the usage of these systems (Costello and LoDolce, 2022), many of these systems still face challenges in comprehending and properly responding to the customer (Følstadet al., 2021). In our project we aim to figure out how to develop and improve these conversational agents. Part of this project (detailed in this paper) will focus on the detection of breakdown patterns and the possible solutions (repairs) to mitigate negative results of these errors.</abstract>
      <url hash="58e3f5fa">2023.yrrsds-1.9</url>
      <bibkey>braggaar-2023-breakdowns</bibkey>
    </paper>
    <paper id="10">
      <title>Towards More Natural Dialogues: Integrating Open-Domain Dialogue Skills into Task-Oriented Agents</title>
      <author><first>Armand</first><last>Stricker</last></author>
      <pages>30–32</pages>
      <abstract>Position paper on the intersection between chitchat and task-oriented dialogues (TODs), with a focus on integrating capabilities typically associated with chitchat systems into task-oriented agents.</abstract>
      <url hash="ac25af2a">2023.yrrsds-1.10</url>
      <bibkey>stricker-2023-towards</bibkey>
    </paper>
    <paper id="11">
      <title>The Future of Designing Spoken Dialogue Systems and Analyzing Written Conversations</title>
      <author><first>Livia</first><last>Qian</last></author>
      <pages>33–34</pages>
      <abstract>This is my position paper for YRRSDS 2023. In it, I write about the details of my research interests as well as past, current and future projects, talk about the status of spoken dialogue system research, include a short bio, and suggest topics for discussion.</abstract>
      <url hash="53b556fb">2023.yrrsds-1.11</url>
      <bibkey>qian-2023-future</bibkey>
    </paper>
    <paper id="12">
      <title>Exploring the Synergy of Deep Learning and Anthropomorphism in Multimodal Dialogue Systems</title>
      <author><first>Iwona</first><last>Christop</last></author>
      <pages>35–36</pages>
      <abstract>This position paper is an overview of author’s main research interests and work considering deep learning techniques in audio classification, sign languages, and multimodality in dialogue systems. Author also shares her opinion on current and future research considering dialogue agents, and suggests topics for discussion panels.</abstract>
      <url hash="99f8bb72">2023.yrrsds-1.12</url>
      <bibkey>christop-2023-exploring</bibkey>
    </paper>
    <paper id="13">
      <title>A Perspective on Anchoring and Dialogue History Propagation for Smoother Interactions with Spoken Task-Oriented Dialogue Systems</title>
      <author><first>Lucas</first><last>Druart</last></author>
      <pages>37–39</pages>
      <abstract>Task-Oriented Dialogue (TOD) systems provide interactive assistance to a user in order to accomplish a specific task such as making a reservation at a restaurant or booking a room in a hotel. Speech presents itself as a natural interface for TOD systems. A typical approach to implement them is to use a modular architecture (Gao et al., 2018). A core component of such dialogue systems is Spoken Language Understanding (SLU) whose goal is to extract the relevant information from the user’s utterances. While spoken dialogue was the focus of earlier work (Williams et al., 2013; Henderson et al., 2014), recent work has focused on text inputs with no regard for the specificities of spoken language (Wu et al., 2019; Heck et al., 2020; Feng et al., 2021). However, this approach fails to account for the differences between written and spoken language (Faruqui and Hakkani-Tür, 2022) such as disfluencies. My research focuses on Spoken Language Understanding in the context of Task-Oriented Dialogue. More specifically I am interested in the two following research directions: • Annotation schema for spoken TODs, • Integration of dialogue history for contextually coherent predictions.</abstract>
      <url hash="a8ca7c2d">2023.yrrsds-1.13</url>
      <bibkey>druart-2023-perspective</bibkey>
    </paper>
    <paper id="14">
      <title>More Human-Like Interaction in Spoken Dialogue Systems: Global Context for Natural Language Understanding and Multimodal Solutions</title>
      <author><first>Kacper</first><last>Dudzic</last></author>
      <pages>40–41</pages>
      <abstract>My position paper for the YRRSDS 2023 workshop.</abstract>
      <url hash="ce43c3ca">2023.yrrsds-1.14</url>
      <bibkey>dudzic-2023-human</bibkey>
    </paper>
    <paper id="15">
      <title>Designing and Evaluating <fixed-case>LLM</fixed-case>-based Conversational Agents for Behaviour Change</title>
      <author><first>Selina</first><last>Meyer</last></author>
      <pages>42–43</pages>
      <abstract>My PhD focuses on conversational agents for behaviour change, with a focus on the feasibility of applying Large Language Models (LLMs) such as GPT-4 in this context.</abstract>
      <url hash="c7c09c2e">2023.yrrsds-1.15</url>
      <bibkey>meyer-2023-designing</bibkey>
    </paper>
    <paper id="16">
      <title>Stylized Dialog Response Generation</title>
      <author><first>Sourabrata</first><last>Mukherjee</last></author>
      <pages>44–46</pages>
      <abstract>My primary research focus lies in the domain of Text Style Transfer (TST), a fascinating area within Natural Language Processing (NLP). TST involves the transfor- mation of text into a desired style while approximately preserving its underlying content. In my research, I am also driven by the goal of incorporating TST techniques into NLP systems, particularly within the realm of dia- logue systems. I am intrigued by the concept of Stylized Dialog Response Generation, which aims to enhance the versatility and adaptability of dialog systems in generat- ing text responses with specific style attributes. By ad- vancing our understanding of TST and its integration into dialogue systems, my research seeks to contribute to the broader field of human-computer interaction. Through the development of robust and versatile dialogue systems with enhanced style transfer capabilities, we can facili- tate more engaging and personalized conversational experiences.</abstract>
      <url hash="6fbbf2fd">2023.yrrsds-1.16</url>
      <bibkey>mukherjee-2023-stylized</bibkey>
    </paper>
    <paper id="17">
      <title>Take the Most out of Text Data Augmentation Strategies For Intent Clustering And Induction Based on <fixed-case>DSTC</fixed-case> 11 Track 2</title>
      <author><first>Mikołaj</first><last>Krzymiński</last></author>
      <pages>47–48</pages>
      <abstract>A brief introduction to author’s keyinterests and research topics which are: multimodal dialogue systems and impact of data augmentation to NLU performance. In addition to that the author shares his biography and view on the future of dialogue assistants.</abstract>
      <url hash="64a85986">2023.yrrsds-1.17</url>
      <bibkey>krzyminski-2023-take</bibkey>
    </paper>
    <paper id="18">
      <title>Advancing Dialogue Systems: Measuring User Satisfaction and Embracing Multimodality</title>
      <author><first>Adrian</first><last>Charkiewicz</last></author>
      <pages>49–50</pages>
      <abstract>This submission discusses my research interests in two areas: measuring user satisfaction in goal-oriented dialogue systems and exploring the potential of multi-modal interactions. For goal-oriented dialogue systems, I focus on evaluating and enhancing user satisfaction throughout the interaction process, aiming to propose innovative strategies and address the limitations of existing evaluation techniques. Additionally, I explore the benefits of multi-modal dialogue systems, highlighting their ability to provide more natural and immersive conversations by incorporating various communication modes such as speech, text, gestures, and visuals.</abstract>
      <url hash="a35e48c9">2023.yrrsds-1.18</url>
      <bibkey>charkiewicz-2023-advancing</bibkey>
    </paper>
    <paper id="19">
      <title>Information Extraction and Program Synthesis from Goal-Oriented Dialogue</title>
      <author><first>Sopan</first><last>Khosla</last></author>
      <pages>51–53</pages>
      <abstract>My research interests broadly lie in the area of Information Extraction from Spoken Dialogue, with a spacial focus on state modeling, anaphora resolution, program synthesis &amp; planning, and intent classification in goal-oriented conversations. My aim is to create embedded dialogue systems that can interact with humans in a collaborative setup to solve tasks in a digital/non-digital environment. Most of the goal-oriented conversations usually involve experts and a laypersons. The aim for the expert is to consider all the information provided by the layperson, identify the underlying set of issues or intents, and prescribe solutions. While human experts are very good at extracting such information, AI agents (that build up most of the automatic dialog systems today) not so much. Most of the existing assistants (or chatbots) only consider individual utterances and do not ground them in the context of the dialogue. My work in this direction has focused on making these systems more effective at extracting the most relevant information from the dialogue to help the human user reach their end-goal.</abstract>
      <url hash="cdaee196">2023.yrrsds-1.19</url>
      <bibkey>khosla-2023-information</bibkey>
    </paper>
    <paper id="20">
      <title>Modelling Emotions in Task-Oriented Dialogue</title>
      <author><first>Shutong</first><last>Feng</last></author>
      <pages>54–56</pages>
      <abstract>My research interests lie in the area of modelling natural and human-like conversations, with a special focus on emotions in task-oriented dialogue (ToD) systems. ToD systems need to produce semantically and grammatically correct responses to fulfil the user’s goal. Being able to perceive and express emotions pushes them one more step towards achieving human-likeness. To begin with, I constructed a dataset with meaningful emotion labels as well as a wide coverage of emotions and linguistic features in ToDs. Then, I improved emotion recognition in conversations (ERC) in the task-oriented domain by exploiting key characteristics of ToDs. Currently, I am working towards enhancing ToD systems with emotions.</abstract>
      <url hash="5c0cc104">2023.yrrsds-1.20</url>
      <bibkey>feng-2023-modelling</bibkey>
    </paper>
    <paper id="21">
      <title>Incrementally Enriching the Common Ground: A Research Path</title>
      <author><first>Brielen</first><last>Madureira</last></author>
      <pages>57–58</pages>
      <abstract>I am broadly interested in evaluation of dialogue systems, in all its many facets: The data they are trained on, their ability to perform a task successfully, their skills with respect to various dialogue phenomena, their resemblance to human cognitive processes, and their ethical and societal impact. More specifically, my research topics focus on understanding the possibilities and limits of current multimodal neural network-based models to incrementally encode information for natural language understanding in general and also for building common ground and asking for clarification. Besides, I am interested in dialogue games as a means to elicit and collect dialogue data and to evaluate the abilities of dialogue models.</abstract>
      <url hash="8f58ff4b">2023.yrrsds-1.21</url>
      <bibkey>madureira-2023-incrementally</bibkey>
    </paper>
    <paper id="22">
      <title>Commonsense Enabled Conversational Model and System-Initiated transitions in Unified <fixed-case>SDS</fixed-case>s</title>
      <author><first>Ye</first><last>Liu</last></author>
      <pages>59–61</pages>
      <abstract>My research work centers on how to enable a human-like interaction through generating contextual, emotional or proactive responses, both in task-oriented and in chitchat spoken dialogue systems (SDSs), because natural lan- guage generation (NLG) is an indispensable component in SDSs and can directly affect the user interactive expe- rience of the entire dialogue system. In addition to NLG, I am also interested in natural language understanding (NLU), as it plays a crucial role in SDSs and is a prerequisite for dialogue systems to generate replies.</abstract>
      <url hash="dba1ed11">2023.yrrsds-1.22</url>
      <bibkey>liu-2023-commonsense</bibkey>
    </paper>
    <paper id="23">
      <title>Causality Reasoning for Empathy-Enriched and Personality-Conditioned Spoken Dialogue System</title>
      <author><first>Yahui</first><last>Fu</last></author>
      <pages>62–63</pages>
      <abstract>The author’s objective centers around developing a spoken dialogue system (SDS) that can emulate the cognitive and conversational qualities of a human friend. Key attributes such as empathy, knowledge/causality reasoning, and personality are integral components of human interaction. The proposed approach involves the creation of an <b>Empathy-enriched SDS</b>, capable of comprehending human emotions and circumstances, thus providing companionship and assistance akin to a trusted friend. Additionally, the <b>Causality-reasoning for SDS</b> aims to ground the system in commonsense knowledge and equip it with the ability to reason about causalities, such as predicting user desires/reactions and system intentions/reactions, thereby enhancing the system’s intelligence and human-like behavior. Finally, the concept of a <b>Personality-conditioned SDS</b> involves enabling systems to exhibit distinct personalities, further enhancing the naturalness of human-robot interaction.</abstract>
      <url hash="97799e2a">2023.yrrsds-1.23</url>
      <bibkey>fu-2023-causality</bibkey>
    </paper>
    <paper id="24">
      <title>Tutorials and User Adaptation in Task Oriented Dialogue</title>
      <author><first>Ryu</first><last>Hirai</last></author>
      <pages>64–65</pages>
      <abstract>This position paper describes my research interests, spoken dialogue system research, and suggested topics for discussion.</abstract>
      <url hash="703e0bf7">2023.yrrsds-1.24</url>
      <bibkey>hirai-2023-tutorials</bibkey>
    </paper>
  </volume>
</collection>
