<?xml version='1.0' encoding='UTF-8'?>
<collection id="2021.codi">
  <volume id="1" ingest-date="2021-11-01">
    <meta>
      <booktitle>Proceedings of the 2nd Workshop on Computational Approaches to Discourse</booktitle>
      <editor><first>Chloé</first><last>Braud</last></editor>
      <editor><first>Christian</first><last>Hardmeier</last></editor>
      <editor><first>Junyi Jessy</first><last>Li</last></editor>
      <editor><first>Annie</first><last>Louis</last></editor>
      <editor><first>Michael</first><last>Strube</last></editor>
      <editor><first>Amir</first><last>Zeldes</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Punta Cana, Dominican Republic and Online</address>
      <month>November</month>
      <year>2021</year>
    </meta>
    <frontmatter>
      <url hash="5c380ae2">2021.codi-1.0</url>
      <bibkey>codi-2021-approaches</bibkey>
    </frontmatter>
    <paper id="1">
      <title>“<fixed-case>I</fixed-case>’ll be there for you”: The One with Understanding Indirect Answers</title>
      <author><first>Cathrine</first><last>Damgaard</last></author>
      <author><first>Paulina</first><last>Toborek</last></author>
      <author><first>Trine</first><last>Eriksen</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <pages>1–11</pages>
      <abstract>Indirect answers are replies to polar questions without the direct use of word cues such as ‘yes’ and ‘no’. Humans are very good at understanding indirect answers, such as ‘I gotta go home sometime’, when asked ‘You wanna crash on the couch?’. Understanding indirect answers is a challenging problem for dialogue systems. In this paper, we introduce a new English corpus to study the problem of understanding indirect answers. Instead of crowd-sourcing both polar questions and answers, we collect questions and indirect answers from transcripts of a prominent TV series and manually annotate them for answer type. The resulting dataset contains 5,930 question-answer pairs. We release both aggregated and raw human annotations. We present a set of experiments in which we evaluate Convolutional Neural Networks (CNNs) for this task, including a cross-dataset evaluation and experiments with learning from disagreements in annotation. Our results show that the task of interpreting indirect answers remains challenging, yet we obtain encouraging improvements when explicitly modeling human disagreement.</abstract>
      <url hash="dd62e4f1">2021.codi-1.1</url>
      <bibkey>damgaard-etal-2021-ill</bibkey>
    </paper>
    <paper id="2">
      <title>Developing Conversational Data and Detection of Conversational Humor in <fixed-case>T</fixed-case>elugu</title>
      <author><first>Vaishnavi</first><last>Pamulapati</last></author>
      <author><first>Radhika</first><last>Mamidi</last></author>
      <pages>12–19</pages>
      <abstract>In the field of humor research, there has been a recent surge of interest in the sub-domain of Conversational Humor (CH). This study has two main objectives. (a) develop a conversational (humorous and non-humorous) dataset in Telugu. (b) detect CH in the compiled dataset. In this paper, the challenges faced while collecting the data and experiments carried out are elucidated. Transfer learning and non-transfer learning techniques are implemented by utilizing pre-trained models such as FastText word embeddings, BERT language models and Text GCN, which learns the word and document embeddings simultaneously of the corpus given. State-of-the-art results are observed with a 99.3% accuracy and a 98.5% f1 score achieved by BERT.</abstract>
      <url hash="0ae5fcbe">2021.codi-1.2</url>
      <bibkey>pamulapati-mamidi-2021-developing</bibkey>
    </paper>
    <paper id="3">
      <title>Investigating non lexical markers of the language of schizophrenia in spontaneous conversations</title>
      <author><first>Chuyuan</first><last>Li</last></author>
      <author><first>Maxime</first><last>Amblard</last></author>
      <author><first>Chloé</first><last>Braud</last></author>
      <author><first>Caroline</first><last>Demily</last></author>
      <author><first>Nicolas</first><last>Franck</last></author>
      <author><first>Michel</first><last>Musiol</last></author>
      <pages>20–28</pages>
      <abstract>We investigate linguistic markers associated with schizophrenia in clinical conversations by detecting predictive features among French-speaking patients. Dealing with human-human dialogues makes for a realistic situation, but it calls for strategies to represent the context and face data sparsity. We compare different approaches for data representation – from individual speech turns to entire conversations –, and data modeling, using lexical, morphological, syntactic, and discourse features, dimensions presumed to be tightly connected to the language of schizophrenia. Previous English models were mostly lexical and reached high performance, here replicated (93.7% acc.). However, our analysis reveals that these models are heavily biased, which probably concerns most datasets on this task. Our new delexicalized models are more general and robust, with the best accuracy score at 77.9%.</abstract>
      <url hash="79dbbe75">2021.codi-1.3</url>
      <bibkey>li-etal-2021-investigating</bibkey>
    </paper>
    <paper id="4">
      <title>Discourse-Driven Integrated Dialogue Development Environment for Open-Domain Dialogue Systems</title>
      <author><first>Denis</first><last>Kuznetsov</last></author>
      <author><first>Dmitry</first><last>Evseev</last></author>
      <author><first>Lidia</first><last>Ostyakova</last></author>
      <author><first>Oleg</first><last>Serikov</last></author>
      <author><first>Daniel</first><last>Kornev</last></author>
      <author><first>Mikhail</first><last>Burtsev</last></author>
      <pages>29–51</pages>
      <abstract>Development environments for spoken dialogue systems are popular today because they enable rapid creation of the dialogue systems in times when usage of the voice AI Assistants is constantly growing. We describe a graphical Discourse-Driven Integrated Dialogue Development Environment (DD-IDDE) for spoken open-domain dialogue systems. The DD-IDDE allows dialogue architects to interactively define dialogue flows of their skills/chatbots with the aid of the discourse-driven recommendation system, enhance these flows in the Python-based DSL, deploy, and then further improve based on the skills/chatbots usage statistics. We show how these skills/chatbots can be specified through a graphical user interface within the VS Code Extension, and then run on top of the Dialog Flow Framework (DFF). An earlier version of this framework has been adopted in one of the Alexa Prize 4 socialbots while the updated version was specifically designed to power the described DD-IDDE solution.</abstract>
      <url hash="97163d12">2021.codi-1.4</url>
      <bibkey>kuznetsov-etal-2021-discourse</bibkey>
    </paper>
    <paper id="5">
      <title>Coreference Chains Categorization by Sequence Clustering</title>
      <author><first>Silvia</first><last>Federzoni</last></author>
      <author><first>Lydia-Mai</first><last>Ho-Dac</last></author>
      <author><first>Cécile</first><last>Fabre</last></author>
      <pages>52–57</pages>
      <abstract>The diversity of coreference chains is usually tackled by means of global features (length, types and number of referring expressions, distance between them, etc.). In this paper, we propose a novel approach that provides a description of their composition in terms of sequences of expressions. To this end, we apply sequence analysis techniques to bring out the various strategies for introducing a referent and keeping it active throughout discourse. We discuss a first application of this method to a French written corpus annotated with coreference chains. We obtain clusters that are linguistically coherent and interpretable in terms of reference strategies and we demonstrate the influence of text genre and semantic type of the referent on chain composition.</abstract>
      <url hash="1d653dd3">2021.codi-1.5</url>
      <bibkey>federzoni-etal-2021-coreference</bibkey>
    </paper>
    <paper id="6">
      <title>Resolving Implicit References in Instructional Texts</title>
      <author><first>Talita</first><last>Anthonio</last></author>
      <author><first>Michael</first><last>Roth</last></author>
      <pages>58–71</pages>
      <abstract>The usage of (co-)referring expressions in discourse contributes to the coherence of a text. However, text comprehension can be difficult when referring expressions are non-verbalized and have to be resolved in the discourse context. In this paper, we propose a novel dataset of such implicit references, which we automatically derive from insertions of references in collaboratively edited how-to guides. Our dataset consists of 6,014 instances, making it one of the largest datasets of implicit references and a useful starting point to investigate misunderstandings caused by underspecified language. We test different methods for resolving implicit references in our dataset based on the Generative Pre-trained Transformer model (GPT) and compare them to heuristic baselines. Our experiments indicate that GPT can accurately resolve the majority of implicit references in our data. Finally, we investigate remaining errors and examine human preferences regarding different resolutions of an implicit reference given the discourse context.</abstract>
      <url hash="d2c08c27">2021.codi-1.6</url>
      <bibkey>anthonio-roth-2021-resolving</bibkey>
    </paper>
    <paper id="7">
      <title>A practical perspective on connective generation</title>
      <author><first>Frances</first><last>Yung</last></author>
      <author><first>Merel</first><last>Scholman</last></author>
      <author><first>Vera</first><last>Demberg</last></author>
      <pages>72–83</pages>
      <abstract>In data-driven natural language generation, we typically know what relation should be expressed and need to select a connective to lexicalize it. In the current contribution, we analyse whether a sophisticated connective generation module is necessary to select a connective, or whether this can be solved with simple methods (such as random choice between connectives that are known to express a given relation, or usage of a generic language model). Comparing these methods to the distributions of connective choices from a human connective insertion task, we find mixed results: for some relations, it is acceptable to lexicalize them using any of the connectives that mark this relation. However, for other relations (temporals, concessives) either a more detailed relation distinction needs to be introduced, or a more sophisticated connective choice module would be necessary.</abstract>
      <url hash="ec994292">2021.codi-1.7</url>
      <bibkey>yung-etal-2021-practical</bibkey>
    </paper>
    <paper id="8">
      <title>Semi-automatic discourse annotation in a low-resource language: Developing a connective lexicon for <fixed-case>N</fixed-case>igerian <fixed-case>P</fixed-case>idgin</title>
      <author><first>Marian</first><last>Marchal</last></author>
      <author><first>Merel</first><last>Scholman</last></author>
      <author><first>Vera</first><last>Demberg</last></author>
      <pages>84–94</pages>
      <abstract>Cross-linguistic research on discourse structure and coherence marking requires discourse-annotated corpora and connective lexicons in a large number of languages. However, the availability of such resources is limited, especially for languages for which linguistic resources are scarce in general, such as Nigerian Pidgin. In this study, we demonstrate how a semi-automatic approach can be used to source connectives and their relation senses and develop a discourse-annotated corpus in a low-resource language. Connectives and their relation senses were extracted from a parallel corpus combining automatic (PDTB end-to-end parser) and manual annotations. This resulted in Naija-Lex, a lexicon of discourse connectives in Nigerian Pidgin with English translations. The lexicon shows that the majority of Nigerian Pidgin connectives are borrowed from its English lexifier, but that there are also some connectives that are unique to Nigerian Pidgin.</abstract>
      <url hash="6beaa152">2021.codi-1.8</url>
      <bibkey>marchal-etal-2021-semi</bibkey>
    </paper>
    <paper id="9">
      <title>Comparison of methods for explicit discourse connective identification across various domains</title>
      <author><first>Merel</first><last>Scholman</last></author>
      <author><first>Tianai</first><last>Dong</last></author>
      <author><first>Frances</first><last>Yung</last></author>
      <author><first>Vera</first><last>Demberg</last></author>
      <pages>95–106</pages>
      <abstract>Existing parse methods use varying approaches to identify explicit discourse connectives, but their performance has not been consistently evaluated in comparison to each other, nor have they been evaluated consistently on text other than newspaper articles. We here assess the performance on explicit connective identification of three parse methods (PDTB e2e, Lin et al., 2014; the winner of CONLL2015, Wang et al., 2015; and DisSent, Nie et al., 2019), along with a simple heuristic. We also examine how well these systems generalize to different datasets, namely written newspaper text (PDTB), written scientific text (BioDRB), prepared spoken text (TED-MDB) and spontaneous spoken text (Disco-SPICE). The results show that the e2e parser outperforms the other parse methods in all datasets. However, performance drops significantly from the PDTB to all other datasets. We provide a more fine-grained analysis of domain differences and connectives that prove difficult to parse, in order to highlight the areas where gains can be made.</abstract>
      <url hash="0b025329">2021.codi-1.9</url>
      <bibkey>scholman-etal-2021-comparison</bibkey>
    </paper>
    <paper id="10">
      <title>Revisiting Shallow Discourse Parsing in the <fixed-case>PDTB</fixed-case>-3: Handling Intra-sentential Implicits</title>
      <author><first>Zheng</first><last>Zhao</last></author>
      <author><first>Bonnie</first><last>Webber</last></author>
      <pages>107–121</pages>
      <abstract>In the PDTB-3, several thousand implicit discourse relations were newly annotated within individual sentences, adding to the over 15,000 implicit relations annotated across adjacent sentences in the PDTB-2. Given that the position of the arguments to these intra-sentential implicits is no longer as well-defined as with inter-sentential implicits, a discourse parser must identify both their location and their sense. That is the focus of the current work. The paper provides a comprehensive analysis of our results, showcasing model performance under different scenarios, pointing out limitations and noting future directions.</abstract>
      <url hash="5e65bfe2">2021.codi-1.10</url>
      <bibkey>zhao-webber-2021-revisiting</bibkey>
    </paper>
    <paper id="11">
      <title>Improving Multi-Party Dialogue Discourse Parsing via Domain Integration</title>
      <author><first>Zhengyuan</first><last>Liu</last></author>
      <author><first>Nancy</first><last>Chen</last></author>
      <pages>122–127</pages>
      <abstract>While multi-party conversations are often less structured than monologues and documents, they are implicitly organized by semantic level correlations across the interactive turns, and dialogue discourse analysis can be applied to predict the dependency structure and relations between the elementary discourse units, and provide feature-rich structural information for downstream tasks. However, the existing corpora with dialogue discourse annotation are collected from specific domains with limited sample sizes, rendering the performance of data-driven approaches poor on incoming dialogues without any domain adaptation. In this paper, we first introduce a Transformer-based parser, and assess its cross-domain performance. We next adopt three methods to gain domain integration from both data and language modeling perspectives to improve the generalization capability. Empirical results show that the neural parser can benefit from our proposed methods, and performs better on cross-domain dialogue samples.</abstract>
      <url hash="8ec86dbb">2021.codi-1.11</url>
      <bibkey>liu-chen-2021-improving</bibkey>
    </paper>
    <paper id="12">
      <title>discopy: A Neural System for Shallow Discourse Parsing</title>
      <author><first>René</first><last>Knaebel</last></author>
      <pages>128–133</pages>
      <abstract>This paper demonstrates discopy, a novel framework that makes it easy to design components for end-to-end shallow discourse parsing. For the purpose of demonstration, we implement recent neural approaches and integrate contextualized word embeddings to predict explicit and non-explicit discourse relations. Our proposed neural feature-free system performs competitively to systems presented at the latest Shared Task on Shallow Discourse Parsing. Finally, a web front end is shown that simplifies the inspection of annotated documents. The source code, documentation, and pretrained models are publicly accessible.</abstract>
      <url hash="2de58919">2021.codi-1.12</url>
      <bibkey>knaebel-2021-discopy</bibkey>
    </paper>
    <paper id="13">
      <title>Tracing variation in discourse connectives in translation and interpreting through neural semantic spaces</title>
      <author><first>Ekaterina</first><last>Lapshinova-Koltunski</last></author>
      <author><first>Heike</first><last>Przybyl</last></author>
      <author><first>Yuri</first><last>Bizzoni</last></author>
      <pages>134–142</pages>
      <abstract>In the present paper, we explore lexical contexts of discourse markers in translation and interpreting on the basis of word embeddings. Our special interest is on contextual variation of the same discourse markers in (written) translation vs. (simultaneous) interpreting. To explore this variation at the lexical level, we use a data-driven approach: we compare bilingual neural word embeddings trained on source-to-translation and source-to-interpreting aligned corpora. Our results show more variation of semantically related items in translation spaces vs. interpreting ones and a more consistent use of fewer connectives in interpreting. We also observe different trends with regard to the discourse relation types.</abstract>
      <url hash="8e7d5003">2021.codi-1.13</url>
      <bibkey>lapshinova-koltunski-etal-2021-tracing</bibkey>
    </paper>
    <paper id="14">
      <title>Capturing document context inside sentence-level neural machine translation models with self-training</title>
      <author><first>Elman</first><last>Mansimov</last></author>
      <author><first>Gábor</first><last>Melis</last></author>
      <author><first>Lei</first><last>Yu</last></author>
      <pages>143–153</pages>
      <abstract>Neural machine translation (NMT) has arguably achieved human level parity when trained and evaluated at the sentence-level. Document-level neural machine translation has received less attention and lags behind its sentence-level counterpart. The majority of the proposed document-level approaches investigate ways of conditioning the model on several source or target sentences to capture document context. These approaches require training a specialized NMT model from scratch on parallel document-level corpora. We propose an approach that doesn’t require training a specialized model on parallel document-level corpora and is applied to a trained sentence-level NMT model at decoding time. We process the document from left to right multiple times and self-train the sentence-level model on pairs of source sentences and generated translations. Our approach reinforces the choices made by the model, thus making it more likely that the same choices will be made in other sentences in the document. We evaluate our approach on three document-level datasets: NIST Chinese-English, WMT19 Chinese-English and OpenSubtitles English-Russian. We demonstrate that our approach has higher BLEU score and higher human preference than the baseline. Qualitative analysis of our approach shows that choices made by model are consistent across the document.</abstract>
      <url hash="92f91b85">2021.codi-1.14</url>
      <bibkey>mansimov-etal-2021-capturing</bibkey>
    </paper>
    <paper id="15">
      <title><fixed-case>DMRST</fixed-case>: A Joint Framework for Document-Level Multilingual <fixed-case>RST</fixed-case> Discourse Segmentation and Parsing</title>
      <author><first>Zhengyuan</first><last>Liu</last></author>
      <author><first>Ke</first><last>Shi</last></author>
      <author><first>Nancy</first><last>Chen</last></author>
      <pages>154–164</pages>
      <abstract>Text discourse parsing weighs importantly in understanding information flow and argumentative structure in natural language, making it beneficial for downstream tasks. While previous work significantly improves the performance of RST discourse parsing, they are not readily applicable to practical use cases: (1) EDU segmentation is not integrated into most existing tree parsing frameworks, thus it is not straightforward to apply such models on newly-coming data. (2) Most parsers cannot be used in multilingual scenarios, because they are developed only in English. (3) Parsers trained from single-domain treebanks do not generalize well on out-of-domain inputs. In this work, we propose a document-level multilingual RST discourse parsing framework, which conducts EDU segmentation and discourse tree parsing jointly. Moreover, we propose a cross-translation augmentation strategy to enable the framework to support multilingual parsing and improve its domain generality. Experimental results show that our model achieves state-of-the-art performance on document-level multilingual RST parsing in all sub-tasks.</abstract>
      <url hash="1e1fea12">2021.codi-1.15</url>
      <bibkey>liu-etal-2021-dmrst</bibkey>
    </paper>
    <paper id="16">
      <title>Visualizing <fixed-case>C</fixed-case>ross‐<fixed-case>L</fixed-case>ingual Discourse Relations in Multilingual <fixed-case>TED</fixed-case> Corpora</title>
      <author><first>Zae Myung</first><last>Kim</last></author>
      <author><first>Vassilina</first><last>Nikoulina</last></author>
      <author><first>Dongyeop</first><last>Kang</last></author>
      <author><first>Didier</first><last>Schwab</last></author>
      <author><first>Laurent</first><last>Besacier</last></author>
      <pages>165–170</pages>
      <abstract>This paper presents an interactive data dashboard that provides users with an overview of the preservation of discourse relations among 28 language pairs. We display a graph network depicting the cross-lingual discourse relations between a pair of languages for multilingual TED talks and provide a search function to look for sentences with specific keywords or relation types, facilitating ease of analysis on the cross-lingual discourse relations.</abstract>
      <url hash="ef794298">2021.codi-1.16</url>
      <bibkey>kim-etal-2021-visualizing</bibkey>
    </paper>
  </volume>
</collection>
