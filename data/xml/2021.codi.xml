<?xml version='1.0' encoding='UTF-8'?>
<collection id="2021.codi">
  <volume id="main" ingest-date="2021-11-01">
    <meta>
      <booktitle>Proceedings of the 2nd Workshop on Computational Approaches to Discourse</booktitle>
      <editor><first>Chloé</first><last>Braud</last></editor>
      <editor><first>Christian</first><last>Hardmeier</last></editor>
      <editor><first>Junyi Jessy</first><last>Li</last></editor>
      <editor><first>Annie</first><last>Louis</last></editor>
      <editor><first>Michael</first><last>Strube</last></editor>
      <editor><first>Amir</first><last>Zeldes</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Punta Cana, Dominican Republic and Online</address>
      <month>November</month>
      <year>2021</year>
      <venue>codi</venue>
    </meta>
    <frontmatter>
      <url hash="5c380ae2">2021.codi-main.0</url>
      <bibkey>codi-2021-approaches</bibkey>
    </frontmatter>
    <paper id="1">
      <title>“<fixed-case>I</fixed-case>’ll be there for you”: The One with Understanding Indirect Answers</title>
      <author><first>Cathrine</first><last>Damgaard</last></author>
      <author><first>Paulina</first><last>Toborek</last></author>
      <author><first>Trine</first><last>Eriksen</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <pages>1–11</pages>
      <abstract>Indirect answers are replies to polar questions without the direct use of word cues such as ‘yes’ and ‘no’. Humans are very good at understanding indirect answers, such as ‘I gotta go home sometime’, when asked ‘You wanna crash on the couch?’. Understanding indirect answers is a challenging problem for dialogue systems. In this paper, we introduce a new English corpus to study the problem of understanding indirect answers. Instead of crowd-sourcing both polar questions and answers, we collect questions and indirect answers from transcripts of a prominent TV series and manually annotate them for answer type. The resulting dataset contains 5,930 question-answer pairs. We release both aggregated and raw human annotations. We present a set of experiments in which we evaluate Convolutional Neural Networks (CNNs) for this task, including a cross-dataset evaluation and experiments with learning from disagreements in annotation. Our results show that the task of interpreting indirect answers remains challenging, yet we obtain encouraging improvements when explicitly modeling human disagreement.</abstract>
      <url hash="dd62e4f1">2021.codi-main.1</url>
      <bibkey>damgaard-etal-2021-ill</bibkey>
      <doi>10.18653/v1/2021.codi-main.1</doi>
      <video href="2021.codi-main.1.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/boolq">BoolQ</pwcdataset>
    </paper>
    <paper id="2">
      <title>Developing Conversational Data and Detection of Conversational Humor in <fixed-case>T</fixed-case>elugu</title>
      <author><first>Vaishnavi</first><last>Pamulapati</last></author>
      <author><first>Radhika</first><last>Mamidi</last></author>
      <pages>12–19</pages>
      <abstract>In the field of humor research, there has been a recent surge of interest in the sub-domain of Conversational Humor (CH). This study has two main objectives. (a) develop a conversational (humorous and non-humorous) dataset in Telugu. (b) detect CH in the compiled dataset. In this paper, the challenges faced while collecting the data and experiments carried out are elucidated. Transfer learning and non-transfer learning techniques are implemented by utilizing pre-trained models such as FastText word embeddings, BERT language models and Text GCN, which learns the word and document embeddings simultaneously of the corpus given. State-of-the-art results are observed with a 99.3% accuracy and a 98.5% f1 score achieved by BERT.</abstract>
      <url hash="0ae5fcbe">2021.codi-main.2</url>
      <bibkey>pamulapati-mamidi-2021-developing</bibkey>
      <doi>10.18653/v1/2021.codi-main.2</doi>
      <video href="2021.codi-main.2.mp4"/>
    </paper>
    <paper id="3">
      <title>Investigating non lexical markers of the language of schizophrenia in spontaneous conversations</title>
      <author><first>Chuyuan</first><last>Li</last></author>
      <author><first>Maxime</first><last>Amblard</last></author>
      <author><first>Chloé</first><last>Braud</last></author>
      <author><first>Caroline</first><last>Demily</last></author>
      <author><first>Nicolas</first><last>Franck</last></author>
      <author><first>Michel</first><last>Musiol</last></author>
      <pages>20–28</pages>
      <abstract>We investigate linguistic markers associated with schizophrenia in clinical conversations by detecting predictive features among French-speaking patients. Dealing with human-human dialogues makes for a realistic situation, but it calls for strategies to represent the context and face data sparsity. We compare different approaches for data representation – from individual speech turns to entire conversations –, and data modeling, using lexical, morphological, syntactic, and discourse features, dimensions presumed to be tightly connected to the language of schizophrenia. Previous English models were mostly lexical and reached high performance, here replicated (93.7% acc.). However, our analysis reveals that these models are heavily biased, which probably concerns most datasets on this task. Our new delexicalized models are more general and robust, with the best accuracy score at 77.9%.</abstract>
      <url hash="79dbbe75">2021.codi-main.3</url>
      <bibkey>li-etal-2021-investigating</bibkey>
      <doi>10.18653/v1/2021.codi-main.3</doi>
      <video href="2021.codi-main.3.mp4"/>
      <pwccode url="https://github.com/chuyuanli/non-lexical-markers-scz-conv" additional="false">chuyuanli/non-lexical-markers-scz-conv</pwccode>
    </paper>
    <paper id="4">
      <title>Discourse-Driven Integrated Dialogue Development Environment for Open-Domain Dialogue Systems</title>
      <author><first>Denis</first><last>Kuznetsov</last></author>
      <author><first>Dmitry</first><last>Evseev</last></author>
      <author><first>Lidia</first><last>Ostyakova</last></author>
      <author><first>Oleg</first><last>Serikov</last></author>
      <author><first>Daniel</first><last>Kornev</last></author>
      <author><first>Mikhail</first><last>Burtsev</last></author>
      <pages>29–51</pages>
      <abstract>Development environments for spoken dialogue systems are popular today because they enable rapid creation of the dialogue systems in times when usage of the voice AI Assistants is constantly growing. We describe a graphical Discourse-Driven Integrated Dialogue Development Environment (DD-IDDE) for spoken open-domain dialogue systems. The DD-IDDE allows dialogue architects to interactively define dialogue flows of their skills/chatbots with the aid of the discourse-driven recommendation system, enhance these flows in the Python-based DSL, deploy, and then further improve based on the skills/chatbots usage statistics. We show how these skills/chatbots can be specified through a graphical user interface within the VS Code Extension, and then run on top of the Dialog Flow Framework (DFF). An earlier version of this framework has been adopted in one of the Alexa Prize 4 socialbots while the updated version was specifically designed to power the described DD-IDDE solution.</abstract>
      <url hash="97163d12">2021.codi-main.4</url>
      <bibkey>kuznetsov-etal-2021-discourse</bibkey>
      <doi>10.18653/v1/2021.codi-main.4</doi>
      <video href="2021.codi-main.4.mp4"/>
    </paper>
    <paper id="5">
      <title>Coreference Chains Categorization by Sequence Clustering</title>
      <author><first>Silvia</first><last>Federzoni</last></author>
      <author><first>Lydia-Mai</first><last>Ho-Dac</last></author>
      <author><first>Cécile</first><last>Fabre</last></author>
      <pages>52–57</pages>
      <abstract>The diversity of coreference chains is usually tackled by means of global features (length, types and number of referring expressions, distance between them, etc.). In this paper, we propose a novel approach that provides a description of their composition in terms of sequences of expressions. To this end, we apply sequence analysis techniques to bring out the various strategies for introducing a referent and keeping it active throughout discourse. We discuss a first application of this method to a French written corpus annotated with coreference chains. We obtain clusters that are linguistically coherent and interpretable in terms of reference strategies and we demonstrate the influence of text genre and semantic type of the referent on chain composition.</abstract>
      <url hash="1d653dd3">2021.codi-main.5</url>
      <bibkey>federzoni-etal-2021-coreference</bibkey>
      <doi>10.18653/v1/2021.codi-main.5</doi>
    </paper>
    <paper id="6">
      <title>Resolving Implicit References in Instructional Texts</title>
      <author><first>Talita</first><last>Anthonio</last></author>
      <author><first>Michael</first><last>Roth</last></author>
      <pages>58–71</pages>
      <abstract>The usage of (co-)referring expressions in discourse contributes to the coherence of a text. However, text comprehension can be difficult when referring expressions are non-verbalized and have to be resolved in the discourse context. In this paper, we propose a novel dataset of such implicit references, which we automatically derive from insertions of references in collaboratively edited how-to guides. Our dataset consists of 6,014 instances, making it one of the largest datasets of implicit references and a useful starting point to investigate misunderstandings caused by underspecified language. We test different methods for resolving implicit references in our dataset based on the Generative Pre-trained Transformer model (GPT) and compare them to heuristic baselines. Our experiments indicate that GPT can accurately resolve the majority of implicit references in our data. Finally, we investigate remaining errors and examine human preferences regarding different resolutions of an implicit reference given the discourse context.</abstract>
      <url hash="d2c08c27">2021.codi-main.6</url>
      <bibkey>anthonio-roth-2021-resolving</bibkey>
      <doi>10.18653/v1/2021.codi-main.6</doi>
      <video href="2021.codi-main.6.mp4"/>
    </paper>
    <paper id="7">
      <title>A practical perspective on connective generation</title>
      <author><first>Frances</first><last>Yung</last></author>
      <author><first>Merel</first><last>Scholman</last></author>
      <author><first>Vera</first><last>Demberg</last></author>
      <pages>72–83</pages>
      <abstract>In data-driven natural language generation, we typically know what relation should be expressed and need to select a connective to lexicalize it. In the current contribution, we analyse whether a sophisticated connective generation module is necessary to select a connective, or whether this can be solved with simple methods (such as random choice between connectives that are known to express a given relation, or usage of a generic language model). Comparing these methods to the distributions of connective choices from a human connective insertion task, we find mixed results: for some relations, it is acceptable to lexicalize them using any of the connectives that mark this relation. However, for other relations (temporals, concessives) either a more detailed relation distinction needs to be introduced, or a more sophisticated connective choice module would be necessary.</abstract>
      <url hash="ec994292">2021.codi-main.7</url>
      <bibkey>yung-etal-2021-practical</bibkey>
      <doi>10.18653/v1/2021.codi-main.7</doi>
      <video href="2021.codi-main.7.mp4"/>
    </paper>
    <paper id="8">
      <title>Semi-automatic discourse annotation in a low-resource language: Developing a connective lexicon for <fixed-case>N</fixed-case>igerian <fixed-case>P</fixed-case>idgin</title>
      <author><first>Marian</first><last>Marchal</last></author>
      <author><first>Merel</first><last>Scholman</last></author>
      <author><first>Vera</first><last>Demberg</last></author>
      <pages>84–94</pages>
      <abstract>Cross-linguistic research on discourse structure and coherence marking requires discourse-annotated corpora and connective lexicons in a large number of languages. However, the availability of such resources is limited, especially for languages for which linguistic resources are scarce in general, such as Nigerian Pidgin. In this study, we demonstrate how a semi-automatic approach can be used to source connectives and their relation senses and develop a discourse-annotated corpus in a low-resource language. Connectives and their relation senses were extracted from a parallel corpus combining automatic (PDTB end-to-end parser) and manual annotations. This resulted in Naija-Lex, a lexicon of discourse connectives in Nigerian Pidgin with English translations. The lexicon shows that the majority of Nigerian Pidgin connectives are borrowed from its English lexifier, but that there are also some connectives that are unique to Nigerian Pidgin.</abstract>
      <url hash="6beaa152">2021.codi-main.8</url>
      <bibkey>marchal-etal-2021-semi</bibkey>
      <doi>10.18653/v1/2021.codi-main.8</doi>
      <video href="2021.codi-main.8.mp4"/>
    </paper>
    <paper id="9">
      <title>Comparison of methods for explicit discourse connective identification across various domains</title>
      <author><first>Merel</first><last>Scholman</last></author>
      <author><first>Tianai</first><last>Dong</last></author>
      <author><first>Frances</first><last>Yung</last></author>
      <author><first>Vera</first><last>Demberg</last></author>
      <pages>95–106</pages>
      <abstract>Existing parse methods use varying approaches to identify explicit discourse connectives, but their performance has not been consistently evaluated in comparison to each other, nor have they been evaluated consistently on text other than newspaper articles. We here assess the performance on explicit connective identification of three parse methods (PDTB e2e, Lin et al., 2014; the winner of CONLL2015, Wang et al., 2015; and DisSent, Nie et al., 2019), along with a simple heuristic. We also examine how well these systems generalize to different datasets, namely written newspaper text (PDTB), written scientific text (BioDRB), prepared spoken text (TED-MDB) and spontaneous spoken text (Disco-SPICE). The results show that the e2e parser outperforms the other parse methods in all datasets. However, performance drops significantly from the PDTB to all other datasets. We provide a more fine-grained analysis of domain differences and connectives that prove difficult to parse, in order to highlight the areas where gains can be made.</abstract>
      <url hash="0b025329">2021.codi-main.9</url>
      <bibkey>scholman-etal-2021-comparison</bibkey>
      <doi>10.18653/v1/2021.codi-main.9</doi>
      <video href="2021.codi-main.9.mp4"/>
    </paper>
    <paper id="10">
      <title>Revisiting Shallow Discourse Parsing in the <fixed-case>PDTB</fixed-case>-3: Handling Intra-sentential Implicits</title>
      <author><first>Zheng</first><last>Zhao</last></author>
      <author><first>Bonnie</first><last>Webber</last></author>
      <pages>107–121</pages>
      <abstract>In the PDTB-3, several thousand implicit discourse relations were newly annotated within individual sentences, adding to the over 15,000 implicit relations annotated across adjacent sentences in the PDTB-2. Given that the position of the arguments to these intra-sentential implicits is no longer as well-defined as with inter-sentential implicits, a discourse parser must identify both their location and their sense. That is the focus of the current work. The paper provides a comprehensive analysis of our results, showcasing model performance under different scenarios, pointing out limitations and noting future directions.</abstract>
      <url hash="5e65bfe2">2021.codi-main.10</url>
      <bibkey>zhao-webber-2021-revisiting</bibkey>
      <doi>10.18653/v1/2021.codi-main.10</doi>
    </paper>
    <paper id="11">
      <title>Improving Multi-Party Dialogue Discourse Parsing via Domain Integration</title>
      <author><first>Zhengyuan</first><last>Liu</last></author>
      <author><first>Nancy</first><last>Chen</last></author>
      <pages>122–127</pages>
      <abstract>While multi-party conversations are often less structured than monologues and documents, they are implicitly organized by semantic level correlations across the interactive turns, and dialogue discourse analysis can be applied to predict the dependency structure and relations between the elementary discourse units, and provide feature-rich structural information for downstream tasks. However, the existing corpora with dialogue discourse annotation are collected from specific domains with limited sample sizes, rendering the performance of data-driven approaches poor on incoming dialogues without any domain adaptation. In this paper, we first introduce a Transformer-based parser, and assess its cross-domain performance. We next adopt three methods to gain domain integration from both data and language modeling perspectives to improve the generalization capability. Empirical results show that the neural parser can benefit from our proposed methods, and performs better on cross-domain dialogue samples.</abstract>
      <url hash="8ec86dbb">2021.codi-main.11</url>
      <bibkey>liu-chen-2021-improving</bibkey>
      <doi>10.18653/v1/2021.codi-main.11</doi>
      <video href="2021.codi-main.11.mp4"/>
      <pwccode url="https://github.com/seq-to-mind/DDP_parsing" additional="false">seq-to-mind/DDP_parsing</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/molweni">Molweni</pwcdataset>
    </paper>
    <paper id="12">
      <title>discopy: A Neural System for Shallow Discourse Parsing</title>
      <author><first>René</first><last>Knaebel</last></author>
      <pages>128–133</pages>
      <abstract>This paper demonstrates discopy, a novel framework that makes it easy to design components for end-to-end shallow discourse parsing. For the purpose of demonstration, we implement recent neural approaches and integrate contextualized word embeddings to predict explicit and non-explicit discourse relations. Our proposed neural feature-free system performs competitively to systems presented at the latest Shared Task on Shallow Discourse Parsing. Finally, a web front end is shown that simplifies the inspection of annotated documents. The source code, documentation, and pretrained models are publicly accessible.</abstract>
      <url hash="2de58919">2021.codi-main.12</url>
      <bibkey>knaebel-2021-discopy</bibkey>
      <doi>10.18653/v1/2021.codi-main.12</doi>
      <video href="2021.codi-main.12.mp4"/>
      <pwccode url="https://github.com/rknaebel/discopy" additional="false">rknaebel/discopy</pwccode>
    </paper>
    <paper id="13">
      <title>Tracing variation in discourse connectives in translation and interpreting through neural semantic spaces</title>
      <author><first>Ekaterina</first><last>Lapshinova-Koltunski</last></author>
      <author><first>Heike</first><last>Przybyl</last></author>
      <author><first>Yuri</first><last>Bizzoni</last></author>
      <pages>134–142</pages>
      <abstract>In the present paper, we explore lexical contexts of discourse markers in translation and interpreting on the basis of word embeddings. Our special interest is on contextual variation of the same discourse markers in (written) translation vs. (simultaneous) interpreting. To explore this variation at the lexical level, we use a data-driven approach: we compare bilingual neural word embeddings trained on source-to-translation and source-to-interpreting aligned corpora. Our results show more variation of semantically related items in translation spaces vs. interpreting ones and a more consistent use of fewer connectives in interpreting. We also observe different trends with regard to the discourse relation types.</abstract>
      <url hash="8e7d5003">2021.codi-main.13</url>
      <bibkey>lapshinova-koltunski-etal-2021-tracing</bibkey>
      <doi>10.18653/v1/2021.codi-main.13</doi>
      <video href="2021.codi-main.13.mp4"/>
    </paper>
    <paper id="14">
      <title>Capturing document context inside sentence-level neural machine translation models with self-training</title>
      <author><first>Elman</first><last>Mansimov</last></author>
      <author><first>Gábor</first><last>Melis</last></author>
      <author><first>Lei</first><last>Yu</last></author>
      <pages>143–153</pages>
      <abstract>Neural machine translation (NMT) has arguably achieved human level parity when trained and evaluated at the sentence-level. Document-level neural machine translation has received less attention and lags behind its sentence-level counterpart. The majority of the proposed document-level approaches investigate ways of conditioning the model on several source or target sentences to capture document context. These approaches require training a specialized NMT model from scratch on parallel document-level corpora. We propose an approach that doesn’t require training a specialized model on parallel document-level corpora and is applied to a trained sentence-level NMT model at decoding time. We process the document from left to right multiple times and self-train the sentence-level model on pairs of source sentences and generated translations. Our approach reinforces the choices made by the model, thus making it more likely that the same choices will be made in other sentences in the document. We evaluate our approach on three document-level datasets: NIST Chinese-English, WMT19 Chinese-English and OpenSubtitles English-Russian. We demonstrate that our approach has higher BLEU score and higher human preference than the baseline. Qualitative analysis of our approach shows that choices made by model are consistent across the document.</abstract>
      <url hash="92f91b85">2021.codi-main.14</url>
      <bibkey>mansimov-etal-2021-capturing</bibkey>
      <doi>10.18653/v1/2021.codi-main.14</doi>
    </paper>
    <paper id="15">
      <title><fixed-case>DMRST</fixed-case>: A Joint Framework for Document-Level Multilingual <fixed-case>RST</fixed-case> Discourse Segmentation and Parsing</title>
      <author><first>Zhengyuan</first><last>Liu</last></author>
      <author><first>Ke</first><last>Shi</last></author>
      <author><first>Nancy</first><last>Chen</last></author>
      <pages>154–164</pages>
      <abstract>Text discourse parsing weighs importantly in understanding information flow and argumentative structure in natural language, making it beneficial for downstream tasks. While previous work significantly improves the performance of RST discourse parsing, they are not readily applicable to practical use cases: (1) EDU segmentation is not integrated into most existing tree parsing frameworks, thus it is not straightforward to apply such models on newly-coming data. (2) Most parsers cannot be used in multilingual scenarios, because they are developed only in English. (3) Parsers trained from single-domain treebanks do not generalize well on out-of-domain inputs. In this work, we propose a document-level multilingual RST discourse parsing framework, which conducts EDU segmentation and discourse tree parsing jointly. Moreover, we propose a cross-translation augmentation strategy to enable the framework to support multilingual parsing and improve its domain generality. Experimental results show that our model achieves state-of-the-art performance on document-level multilingual RST parsing in all sub-tasks.</abstract>
      <url hash="1e1fea12">2021.codi-main.15</url>
      <bibkey>liu-etal-2021-dmrst</bibkey>
      <doi>10.18653/v1/2021.codi-main.15</doi>
      <video href="2021.codi-main.15.mp4"/>
      <pwccode url="https://github.com/seq-to-mind/DMRST_Parser" additional="false">seq-to-mind/DMRST_Parser</pwccode>
    </paper>
    <paper id="16">
      <title>Visualizing <fixed-case>C</fixed-case>ross‐<fixed-case>L</fixed-case>ingual Discourse Relations in Multilingual <fixed-case>TED</fixed-case> Corpora</title>
      <author><first>Zae Myung</first><last>Kim</last></author>
      <author><first>Vassilina</first><last>Nikoulina</last></author>
      <author><first>Dongyeop</first><last>Kang</last></author>
      <author><first>Didier</first><last>Schwab</last></author>
      <author><first>Laurent</first><last>Besacier</last></author>
      <pages>165–170</pages>
      <abstract>This paper presents an interactive data dashboard that provides users with an overview of the preservation of discourse relations among 28 language pairs. We display a graph network depicting the cross-lingual discourse relations between a pair of languages for multilingual TED talks and provide a search function to look for sentences with specific keywords or relation types, facilitating ease of analysis on the cross-lingual discourse relations.</abstract>
      <url hash="ef794298">2021.codi-main.16</url>
      <bibkey>kim-etal-2021-visualizing</bibkey>
      <doi>10.18653/v1/2021.codi-main.16</doi>
      <video href="2021.codi-main.16.mp4"/>
      <pwccode url="https://github.com/zaemyung/visualizing-cross-lingual-discourse-relations" additional="false">zaemyung/visualizing-cross-lingual-discourse-relations</pwccode>
    </paper>
  </volume>
  <volume id="sharedtask" ingest-date="2021-10-28">
    <meta>
      <booktitle>Proceedings of the CODI-CRAC 2021 Shared Task on Anaphora, Bridging, and Discourse Deixis in Dialogue</booktitle>
      <editor><first>Sopan</first><last>Khosla</last></editor>
      <editor><first>Ramesh</first><last>Manuvinakurike</last></editor>
      <editor><first>Vincent</first><last>Ng</last></editor>
      <editor><first>Massimo</first><last>Poesio</last></editor>
      <editor><first>Michael</first><last>Strube</last></editor>
      <editor><first>Carolyn</first><last>Rosé</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Punta Cana, Dominican Republic</address>
      <month>November</month>
      <year>2021</year>
      <venue>codi</venue>
    </meta>
    <frontmatter>
      <url hash="ac624928">2021.codi-sharedtask.0</url>
      <bibkey>codicrac-2021-codi</bibkey>
    </frontmatter>
    <paper id="1">
      <title>The <fixed-case>CODI</fixed-case>-<fixed-case>CRAC</fixed-case> 2021 Shared Task on Anaphora, Bridging, and Discourse Deixis in Dialogue</title>
      <author><first>Sopan</first><last>Khosla</last></author>
      <author><first>Juntao</first><last>Yu</last></author>
      <author><first>Ramesh</first><last>Manuvinakurike</last></author>
      <author><first>Vincent</first><last>Ng</last></author>
      <author><first>Massimo</first><last>Poesio</last></author>
      <author><first>Michael</first><last>Strube</last></author>
      <author><first>Carolyn</first><last>Rosé</last></author>
      <pages>1–15</pages>
      <abstract>In this paper, we provide an overview of the CODI-CRAC 2021 Shared-Task: Anaphora Resolution in Dialogue. The shared task focuses on detecting anaphoric relations in different genres of conversations. Using five conversational datasets, four of which have been newly annotated with a wide range of anaphoric relations: identity, bridging references and discourse deixis, we defined multiple subtasks focusing individually on these key relations. We discuss the evaluation scripts used to assess the system performance on these subtasks, and provide a brief summary of the participating systems and the results obtained across ?? runs from 5 teams, with most submissions achieving significantly better results than our baseline methods.</abstract>
      <url hash="6274ff09">2021.codi-sharedtask.1</url>
      <bibkey>khosla-etal-2021-codi</bibkey>
      <doi>10.18653/v1/2021.codi-sharedtask.1</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="2">
      <title>Neural Anaphora Resolution in Dialogue</title>
      <author><first>Hideo</first><last>Kobayashi</last></author>
      <author><first>Shengjie</first><last>Li</last></author>
      <author><first>Vincent</first><last>Ng</last></author>
      <pages>16–31</pages>
      <abstract>We describe the systems that we developed for the three tracks of the CODI-CRAC 2021 shared task, namely entity coreference resolution, bridging resolution, and discourse deixis resolution. Our team ranked second for entity coreference resolution, first for bridging resolution, and first for discourse deixis resolution.</abstract>
      <url hash="b486f23f">2021.codi-sharedtask.2</url>
      <bibkey>kobayashi-etal-2021-neural</bibkey>
      <doi>10.18653/v1/2021.codi-sharedtask.2</doi>
    </paper>
    <paper id="3">
      <title>Anaphora Resolution in Dialogue: Description of the <fixed-case>DFKI</fixed-case>-<fixed-case>T</fixed-case>alking<fixed-case>R</fixed-case>obots System for the <fixed-case>CODI</fixed-case>-<fixed-case>CRAC</fixed-case> 2021 Shared-Task</title>
      <author><first>Tatiana</first><last>Anikina</last></author>
      <author><first>Cennet</first><last>Oguz</last></author>
      <author><first>Natalia</first><last>Skachkova</last></author>
      <author><first>Siyu</first><last>Tao</last></author>
      <author><first>Sharmila</first><last>Upadhyaya</last></author>
      <author><first>Ivana</first><last>Kruijff-Korbayova</last></author>
      <pages>32–42</pages>
      <abstract>We describe the system developed by the DFKI-TalkingRobots Team for the CODI-CRAC 2021 Shared-Task on anaphora resolution in dialogue. Our system consists of three subsystems: (1) the Workspace Coreference System (WCS) incrementally clusters mentions using semantic similarity based on embeddings combined with lexical feature heuristics; (2) the Mention-to-Mention (M2M) coreference resolution system pairs same entity mentions; (3) the Discourse Deixis Resolution (DDR) system employs a Siamese Network to detect discourse anaphor-antecedent pairs. WCS achieved F1-score of 55.6% averaged across the evaluation test sets, M2M achieved 57.2% and DDR achieved 21.5%.</abstract>
      <url hash="5a6d3cc9">2021.codi-sharedtask.3</url>
      <bibkey>anikina-etal-2021-anaphora</bibkey>
      <doi>10.18653/v1/2021.codi-sharedtask.3</doi>
    </paper>
    <paper id="4">
      <title>The Pipeline Model for Resolution of Anaphoric Reference and Resolution of Entity Reference</title>
      <author><first>Hongjin</first><last>Kim</last></author>
      <author><first>Damrin</first><last>Kim</last></author>
      <author><first>Harksoo</first><last>Kim</last></author>
      <pages>43–47</pages>
      <abstract>The objective of anaphora resolution in dialogue shared-task is to go above and beyond the simple cases of coreference resolution in written text on which NLP has mostly focused so far, which arguably overestimate the performance of current SOTA models. The anaphora resolution in dialogue shared-task consists of three subtasks; subtask1, resolution of anaphoric identity and non-referring expression identification, subtask2, resolution of bridging references, and subtask3, resolution of discourse deixis/abstract anaphora. In this paper, we propose the pipelined model (i.e., a resolution of anaphoric identity and a resolution of bridging references) for the subtask1 and the subtask2. In the subtask1, our model detects mention via the parentheses prediction. Then, we yield mention representation using the token representation constituting the mention. Mention representation is fed to the coreference resolution model for clustering. In the subtask2, our model resolves bridging references via the MRC framework. We construct query for each entity as “What is related of ENTITY?”. The input of our model is query and documents(i.e., all utterances of dialogue). Then, our model predicts entity span that is answer for query.</abstract>
      <url hash="4f5e645d">2021.codi-sharedtask.4</url>
      <bibkey>kim-etal-2021-pipeline</bibkey>
      <doi>10.18653/v1/2021.codi-sharedtask.4</doi>
    </paper>
    <paper id="5">
      <title>An End-to-End Approach for Full Bridging Resolution</title>
      <author><first>Joseph</first><last>Renner</last></author>
      <author><first>Priyansh</first><last>Trivedi</last></author>
      <author><first>Gaurav</first><last>Maheshwari</last></author>
      <author><first>Rémi</first><last>Gilleron</last></author>
      <author><first>Pascal</first><last>Denis</last></author>
      <pages>48–54</pages>
      <abstract>In this article, we describe our submission to the CODI-CRAC 2021 Shared Task on Anaphora Resolution in Dialogues – Track BR (Gold). We demonstrate the performance of an end-to-end transformer-based higher-order coreference model finetuned for the task of full bridging. We find that while our approach is not effective at modeling the complexities of the task, it performs well on bridging resolution, suggesting a need for investigations into a robust anaphor identification model for future improvements.</abstract>
      <url hash="d5cebe03">2021.codi-sharedtask.5</url>
      <bibkey>renner-etal-2021-end</bibkey>
      <doi>10.18653/v1/2021.codi-sharedtask.5</doi>
    </paper>
    <paper id="6">
      <title>Adapted End-to-End Coreference Resolution System for Anaphoric Identities in Dialogues</title>
      <author><first>Liyan</first><last>Xu</last></author>
      <author><first>Jinho D.</first><last>Choi</last></author>
      <pages>55–62</pages>
      <abstract>We present an effective system adapted from the end-to-end neural coreference resolution model, targeting on the task of anaphora resolution in dialogues. Three aspects are specifically addressed in our approach, including the support of singletons, encoding speakers and turns throughout dialogue interactions, and knowledge transfer utilizing existing resources. Despite the simplicity of our adaptation strategies, they are shown to bring significant impact to the final performance, with up to 27 F1 improvement over the baseline. Our final system ranks the 1st place on the leaderboard of the anaphora resolution track in the CRAC 2021 shared task, and achieves the best evaluation results on all four datasets.</abstract>
      <url hash="c4f0e1b5">2021.codi-sharedtask.6</url>
      <bibkey>xu-choi-2021-adapted</bibkey>
      <doi>10.18653/v1/2021.codi-sharedtask.6</doi>
    </paper>
    <paper id="7">
      <title>Anaphora Resolution in Dialogue: Cross-Team Analysis of the <fixed-case>DFKI</fixed-case>-<fixed-case>T</fixed-case>alking<fixed-case>R</fixed-case>obots Team Submissions for the <fixed-case>CODI</fixed-case>-<fixed-case>CRAC</fixed-case> 2021 Shared-Task</title>
      <author><first>Natalia</first><last>Skachkova</last></author>
      <author><first>Cennet</first><last>Oguz</last></author>
      <author><first>Tatiana</first><last>Anikina</last></author>
      <author><first>Siyu</first><last>Tao</last></author>
      <author><first>Sharmila</first><last>Upadhyaya</last></author>
      <author><first>Ivana</first><last>Kruijff-Korbayova</last></author>
      <pages>63–70</pages>
      <abstract>We compare our team’s systems to others submitted for the CODI-CRAC 2021 Shared-Task on anaphora resolution in dialogue. We analyse the architectures and performance, report some problematic cases in gold annotations, and suggest possible improvements of the systems, their evaluation, data annotation, and the organization of the shared task.</abstract>
      <url hash="14494493">2021.codi-sharedtask.7</url>
      <bibkey>skachkova-etal-2021-anaphora</bibkey>
      <doi>10.18653/v1/2021.codi-sharedtask.7</doi>
    </paper>
    <paper id="8">
      <title>The <fixed-case>CODI</fixed-case>-<fixed-case>CRAC</fixed-case> 2021 Shared Task on Anaphora, Bridging, and Discourse Deixis Resolution in Dialogue: A Cross-Team Analysis</title>
      <author><first>Shengjie</first><last>Li</last></author>
      <author><first>Hideo</first><last>Kobayashi</last></author>
      <author><first>Vincent</first><last>Ng</last></author>
      <pages>71–95</pages>
      <abstract>The CODI-CRAC 2021 shared task is the first shared task that focuses exclusively on anaphora resolution in dialogue and provides three tracks, namely entity coreference resolution, bridging resolution, and discourse deixis resolution. We perform a cross-task analysis of the systems that participated in the shared task in each of these tracks.</abstract>
      <url hash="e549ffdc">2021.codi-sharedtask.8</url>
      <bibkey>li-etal-2021-codi</bibkey>
      <doi>10.18653/v1/2021.codi-sharedtask.8</doi>
    </paper>
  </volume>
</collection>
