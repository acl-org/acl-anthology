<?xml version='1.0' encoding='UTF-8'?>
<collection id="2023.naloma">
  <volume id="1" ingest-date="2023-10-22" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 4th Natural Logic Meets Machine Learning Workshop</booktitle>
      <editor><first>Stergios</first><last>Chatzikyriakidis</last></editor>
      <editor><first>Valeria</first><last>de Paiva</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Nancy, France</address>
      <month>June</month>
      <year>2023</year>
      <url hash="9d168b56">2023.naloma-1</url>
      <venue>naloma</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="2d911c6c">2023.naloma-1.0</url>
      <bibkey>naloma-2023-natural</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Evaluating Large Language Models with <fixed-case>N</fixed-case>eu<fixed-case>BAROCO</fixed-case>: Syllogistic Reasoning Ability and Human-like Biases</title>
      <author><first>Risako</first><last>Ando</last></author>
      <author><first>Takanobu</first><last>Morishita</last></author>
      <author><first>Hirohiko</first><last>Abe</last></author>
      <author><first>Koji</first><last>Mineshima</last></author>
      <author><first>Mitsuhiro</first><last>Okada</last></author>
      <pages>1–11</pages>
      <abstract>This paper investigates whether current large language models exhibit biases in logical reasoning, similar to humans. Specifically, we focus on syllogistic reasoning, a well-studied form of inference in the cognitive science of human deduction. To facilitate our analysis, we introduce a dataset called NeuBAROCO, originally designed for psychological experiments that assess human logical abilities in syllogistic reasoning. The dataset consists of syllogistic inferences in both English and Japanese. We examine three types of biases observed in human syllogistic reasoning: belief biases, conversion errors, and atmosphere effects. Our findings demonstrate that current large language models struggle more with problems involving these three types of biases.</abstract>
      <url hash="d9329780">2023.naloma-1.1</url>
      <bibkey>ando-etal-2023-evaluating</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>S</fixed-case>pace<fixed-case>NLI</fixed-case>: Evaluating the Consistency of Predicting Inferences In Space</title>
      <author><first>Lasha</first><last>Abzianidze</last></author>
      <author><first>Joost</first><last>Zwarts</last></author>
      <author><first>Yoad</first><last>Winter</last></author>
      <pages>12–24</pages>
      <abstract>While many natural language inference (NLI) datasets target certain semantic phenomena, e.g., negation, tense &amp; aspect, monotonicity, and presupposition, to the best of our knowledge, there is no NLI dataset that involves diverse types of spatial expressions and reasoning. We fill this gap by semi-automatically creating an NLI dataset for spatial reasoning, called SpaceNLI. The data samples are automatically generated from a curated set of reasoning patterns (see Figure 1), where the patterns are annotated with inference labels by experts. We test several SOTA NLI systems on SpaceNLI to gauge the complexity of the dataset and the system’s capacity for spatial reasoning. Moreover, we introduce a <i>Pattern Accuracy</i> and argue that it is a more reliable and stricter measure than the accuracy for evaluating a system’s performance on pattern-based generated data samples. Based on the evaluation results we find that the systems obtain moderate results on the spatial NLI problems but lack consistency per inference pattern. The results also reveal that non-projective spatial inferences (especially due to the “between” preposition) are the most challenging ones.</abstract>
      <url hash="325c10a5">2023.naloma-1.2</url>
      <attachment type="OptionalSupplementaryMaterial" hash="f49777fb">2023.naloma-1.2.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>abzianidze-etal-2023-spacenli</bibkey>
    </paper>
    <paper id="3">
      <title>Does <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case> Resemble Humans in Processing Implicatures?</title>
      <author><first>Zhuang</first><last>Qiu</last></author>
      <author><first>Xufeng</first><last>Duan</last></author>
      <author><first>Zhenguang</first><last>Cai</last></author>
      <pages>25–34</pages>
      <abstract>Recent advances in large language models (LLMs) and LLM-driven chatbots, such as ChatGPT, have sparked interest in the extent to which these artificial systems possess human-like linguistic abilities. In this study, we assessed ChatGPT’s pragmatic capabilities by conducting three preregistered experiments focused on its ability to compute pragmatic implicatures. The first experiment tested whether ChatGPT inhibits the computation of generalized conversational implicatures (GCIs) when explicitly required to process the text’s truth-conditional meaning. The second and third experiments examined whether the communicative context affects ChatGPT’s ability to compute scalar implicatures (SIs). Our results showed that ChatGPT did not demonstrate human-like flexibility in switching between pragmatic and semantic processing. Additionally, ChatGPT’s judgments did not exhibit the well-established effect of communicative context on SI rates.</abstract>
      <url hash="930e5a12">2023.naloma-1.3</url>
      <bibkey>qiu-etal-2023-chatgpt</bibkey>
    </paper>
    <paper id="4">
      <title>Recurrent Neural Network <fixed-case>CCG</fixed-case> Parser</title>
      <author><first>Sora</first><last>Tagami</last></author>
      <author><first>Daisuke</first><last>Bekki</last></author>
      <pages>35–40</pages>
      <abstract>The two contrasting approaches are end-to-end neural NLI systems and linguistically-oriented NLI pipelines consisting of modules such as neural CCG parsers and theorem provers. The latter, however, faces the challenge of integrating the neural models used in the syntactic and semantic components. RNNGs are frameworks that can potentially fill this gap, but conventional RNNGs adopt CFG as the syntactic theory. To address this issue, we implemented RNN-CCG, a syntactic parser that replaces CFG with CCG. We then conducted experiments comparing RNN-CCG to RNNGs with/without POS tags and evaluated their behavior as a first step towards building an NLI system based on RNN-CCG.</abstract>
      <url hash="ccf7aadb">2023.naloma-1.4</url>
      <bibkey>tagami-bekki-2023-recurrent</bibkey>
    </paper>
    <paper id="5">
      <title><fixed-case>TTR</fixed-case> at the <fixed-case>SPA</fixed-case>: Relating type-theoretical semantics to neural semantic pointers</title>
      <author><first>Staffan</first><last>Larsson</last></author>
      <author><first>Robin</first><last>Cooper</last></author>
      <author><first>Jonathan</first><last>Ginzburg</last></author>
      <author><first>Andy</first><last>Luecking</last></author>
      <pages>41–50</pages>
      <abstract>This paper considers how the kind of formal semantic objects used in TTR (a theory of types with records, Cooper 2013) might be related to the vector representations used in Eliasmith (2013). An advantage of doing this is that it would immediately give us a neural representation for TTR objects as Eliasmith relates vectors to neural activity in his semantic pointer architecture (SPA). This would be an alternative using convolution to the suggestions made by Cooper (2019) based on the phasing of neural activity. The project seems potentially hopeful since all complex TTR objects are constructed from labelled sets (essentially sets of ordered pairs consisting of labels and values) which might be seen as corresponding to the representation of structured objects which Eliasmith achieves using superposition and circular convolution.</abstract>
      <url hash="8f1b855b">2023.naloma-1.5</url>
      <bibkey>larsson-etal-2023-ttr</bibkey>
    </paper>
    <paper id="6">
      <title>Triadic temporal representations and deformations</title>
      <author><first>Tim</first><last>Fernando</last></author>
      <pages>51–61</pages>
      <abstract>Triadic representations that temporally order events and states are described, consisting of strings and sets of strings of bounded but refinable granularities. The strings are compressed according to J.A. Wheeler’s dictum it-from-bit, with bits given by statives and non-statives alike. A choice of vocabulary and constraints expressed in that vocabulary shape representations of cause-and-effect with deformations characteristic, Mumford posits, of patterns at various levels of cognitive processing. These deformations point to an ongoing process of learning, formulated as grammatical inference of finite automata, structured around Goguen and Burstall’s institutions.</abstract>
      <url hash="9748ad8a">2023.naloma-1.6</url>
      <bibkey>fernando-2023-triadic</bibkey>
    </paper>
    <paper id="7">
      <title>Discourse Representation Structure Parsing for <fixed-case>C</fixed-case>hinese</title>
      <author><first>Chunliu</first><last>Wang</last></author>
      <author><first>Xiao</first><last>Zhang</last></author>
      <author><first>Johan</first><last>Bos</last></author>
      <pages>62–74</pages>
      <abstract>Previous work has predominantly focused on monolingual English semantic parsing. We, instead, explore the feasibility of Chinese semantic parsing in the absence of labeled data for Chinese meaning representations. We describe the pipeline of automatically collecting the linearized Chinese meaning representation data for sequential-to-sequential neural networks. We further propose a test suite designed explicitly for Chinese semantic parsing, which provides fine-grained evaluation for parsing performance, where we aim to study Chinese parsing difficulties. Our experimental results show that the difficulty of Chinese semantic parsing is mainly caused by adverbs. Realizing Chinese parsing through machine translation and an English parser yields slightly lower performance than training a model directly on Chinese data.</abstract>
      <url hash="b499edae">2023.naloma-1.7</url>
      <bibkey>wang-etal-2023-discourse</bibkey>
    </paper>
  </volume>
</collection>
