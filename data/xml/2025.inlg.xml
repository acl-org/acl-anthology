<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.inlg">
  <volume id="main" ingest-date="2025-11-11" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 18th International Natural Language Generation Conference</booktitle>
      <editor><first>Lucie</first><last>Flek</last></editor>
      <editor><first>Shashi</first><last>Narayan</last></editor>
      <editor><first>Lê Hồng</first><last>Phương</last></editor>
      <editor><first>Jiahuan</first><last>Pei</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Hanoi, Vietnam</address>
      <month>October</month>
      <year>2025</year>
      <url hash="2aaaee7d">2025.inlg-main</url>
      <venue>inlg</venue>
    </meta>
    <frontmatter>
      <url hash="7d4079e8">2025.inlg-main.0</url>
      <bibkey>inlg-2025-main</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Enhancing Coherence and Interestingness in Knowledge-Grounded Dialogue Generation</title>
      <author><first>Hiroki</first><last>Onozeki</last></author>
      <author><first>Michimasa</first><last>Inaba</last></author>
      <pages>1–19</pages>
      <abstract>Open-domain dialogue systems have been increasingly applied in various situations, with a growing need to improve user engagement. One effective approach is to generate responses based on interesting external knowledge using knowledge-grounded response generation models. However, relying solely on interestingness can lead to incoherent responses, potentially diminishing user engagement. This paper proposes a novel method for generating engaging responses while maintaining contextual coherence. Our approach leverages a pre-trained knowledge-grounded response generation model and modifies the knowledge selection process to enhance response coherence and interestingness without requiring additional training. First, knowledge candidates with high contextual relevance are retrieved. These candidates are then reranked based on their interestingness and used to generate the responses. Finally, the method detects dialogue breakdowns and regenerates responses as necessary to ensure coherence. We conducted experiments using the Wizard of Wikipedia dataset and two state-of-the-art response generation models. The results indicate that the proposed method improves both response coherence and interestingness.</abstract>
      <url hash="cc2d2b77">2025.inlg-main.1</url>
      <bibkey>onozeki-inaba-2025-enhancing</bibkey>
    </paper>
    <paper id="2">
      <title>Evaluating <fixed-case>LLM</fixed-case>-Generated Versus Human-Authored Responses in Role-Play Dialogues</title>
      <author><first>Dongxu</first><last>Lu</last></author>
      <author><first>Johan</first><last>Jeuring</last></author>
      <author><first>Albert</first><last>Gatt</last></author>
      <pages>20–40</pages>
      <abstract>Evaluating large language models (LLMs) in long-form, knowledge-grounded role-play dialogues remains challenging. This study compares LLM-generated and human-authored responses in multi-turn professional training simulations through human evaluation (N = 38) and automated LLM-as-a-judge assessment. Human evaluation revealed significant degradation in LLM-generated response quality across turns, particularly in naturalness, context maintenance and overall quality, while human-authored responses progressively improved. In line with this finding, participants also indicated a consistent preference for human-authored dialogue. These human judgements were validated by our automated LLM-as-a-judge evaluation, where GEMINI 2.0 FLASH achieved strong alignment with human evaluators on both zero-shot pairwise preference and stochastic 6-shot construct ratings, confirming the widening quality gap between LLM and human responses over time. Our work contributes a multi-turn benchmark exposing LLM degradation in knowledge-grounded role-play dialogues and provides a validated hybrid evaluation framework to guide the reliable integration of LLMs in training simulations.</abstract>
      <url hash="d1ad7ecc">2025.inlg-main.2</url>
      <bibkey>lu-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="3">
      <title>Human ratings of <fixed-case>LLM</fixed-case> response generation in pair-programming dialogue</title>
      <author><first>Cecilia</first><last>Domingo</last></author>
      <author><first>Paul</first><last>Piwek</last></author>
      <author><first>Svetlana</first><last>Stoyanchev</last></author>
      <author><first>Michel</first><last>Wermelinger</last></author>
      <author><first>Kaustubh</first><last>Adhikari</last></author>
      <author><first>Rama Sanand</first><last>Doddipatla</last></author>
      <pages>41–59</pages>
      <abstract>We take first steps in exploring whether Large Language Models (LLMs) can be adapted to dialogic learning practices, specifically pair programming — LLMs have primarily been implemented as programming assistants, not fully exploiting their dialogic potential. We used new dialogue data from real pair-programming interactions between students, prompting state-of-the-art LLMs to assume the role of a student, when generating a response that continues the real dialogue. We asked human annotators to rate human and AI responses on the criteria through which we operationalise the LLMs’ suitability for educational dialogue: Coherence, Collaborativeness, and whether they appeared human. Results show model differences, with Llama-generated responses being rated similarly to human answers on all three criteria. Thus, for at least one of the models we investigated, the LLM utterance-level response generation appears to be suitable for pair-programming dialogue.</abstract>
      <url hash="a93e922e">2025.inlg-main.3</url>
      <bibkey>domingo-etal-2025-human</bibkey>
    </paper>
    <paper id="4">
      <title>Do My Eyes Deceive Me? A Survey of Human Evaluations of Hallucinations in <fixed-case>NLG</fixed-case></title>
      <author><first>Patricia</first><last>Schmidtova</last></author>
      <author><first>Eduardo</first><last>Calò</last></author>
      <author><first>Simone</first><last>Balloccu</last></author>
      <author><first>Dimitra</first><last>Gkatzia</last></author>
      <author><first>Rudali</first><last>Huidrom</last></author>
      <author><first>Mateusz</first><last>Lango</last></author>
      <author><first>Fahime</first><last>Same</last></author>
      <author><first>Vilém</first><last>Zouhar</last></author>
      <author><first>Saad</first><last>Mahamood</last></author>
      <author><first>Ondrej</first><last>Dusek</last></author>
      <pages>60–79</pages>
      <abstract>Hallucinations are one of the most pressing challenges for large language models (LLMs). While numerous methods have been proposed to detect and mitigate them automatically, human evaluation continues to serve as the gold standard. However, these human evaluations of hallucinations show substantial variation in definitions, terminology, and evaluation practices. In this paper, we survey 64 studies involving human evaluation of hallucination published between 2019 and 2024, to investigate how hallucinations are currently defined and assessed. Our analysis reveals a lack of consistency in definitions and exposes several concerning methodological shortcomings. Crucial details, such as evaluation guidelines, user interface design, inter-annotator agreement metrics, and annotator demographics, are frequently under-reported or omitted altogether.</abstract>
      <url hash="c21cea76">2025.inlg-main.4</url>
      <bibkey>schmidtova-etal-2025-eyes</bibkey>
    </paper>
    <paper id="5">
      <title>Truth or Twist? Optimal Model Selection for Reliable Label Flipping Evaluation in <fixed-case>LLM</fixed-case>-based Counterfactuals</title>
      <author><first>Qianli</first><last>Wang</last></author>
      <author><first>Van Bach</first><last>Nguyen</last></author>
      <author><first>Nils</first><last>Feldhus</last></author>
      <author><first>Luis Felipe</first><last>Villa-Arenas</last></author>
      <author><first>Christin</first><last>Seifert</last></author>
      <author><first>Sebastian</first><last>Möller</last></author>
      <author><first>Vera</first><last>Schmitt</last></author>
      <pages>80–97</pages>
      <abstract>Counterfactual examples are widely employed to enhance the performance and robustness of large language models (LLMs) through counterfactual data augmentation (CDA). However, the selection of the judge model used to evaluate label flipping, the primary metric for assessing the validity of generated counterfactuals for CDA, yields inconsistent results. To decipher this, we define four types of relationships between the counterfactual generator and judge models: being the same model, belonging to the same model family, being independent models, and having an distillation relationship. Through extensive experiments involving two state-of-the-art LLM-based methods, three datasets, four generator models, and 15 judge models, complemented by a user study (n = 90), we demonstrate that judge models with an independent, non-fine-tuned relationship to the generator model provide the most reliable label flipping evaluations. Relationships between the generator and judge models, which are closely aligned with the user study for CDA, result in better model performance and robustness. Nevertheless, we find that the gap between the most effective judge models and the results obtained from the user study remains considerably large. This suggests that a fully automated pipeline for CDA may be inadequate and requires human intervention.</abstract>
      <url hash="bc7ae6f8">2025.inlg-main.5</url>
      <bibkey>wang-etal-2025-truth</bibkey>
    </paper>
    <paper id="6">
      <title>Assessing Semantic Consistency in <fixed-case>D</fixed-case>ata‐to‐<fixed-case>T</fixed-case>ext Generation: A Meta-Evaluation of Textual, Semantic and Model-Based Metrics</title>
      <author><first>Rudali</first><last>Huidrom</last></author>
      <author><first>Michela</first><last>Lorandi</last></author>
      <author><first>Simon</first><last>Mille</last></author>
      <author><first>Craig</first><last>Thomson</last></author>
      <author id="anja-belz"><first>Anya</first><last>Belz</last></author>
      <pages>98–107</pages>
      <abstract>Ensuring semantic consistency between semantic-triple inputs and generated text is crucial in data‐to‐text generation, but continues to pose challenges both during generation and in evaluation. In order to assess how accurately semantic consistency can currently be assessed, we meta-evaluate 29 different evaluation methods in terms of their ability to predict human semantic-consistency ratings. The evaluation methods include embeddings‐based, overlap‐based, and edit‐distance metrics, as well as learned regressors and a prompted ‘LLM‐as‐judge’ protocol. We meta-evaluate on two datasets: the WebNLG 2017 human evaluation dataset, and a newly created WebNLG-style dataset that none of the methods can have seen during training. We find that none of the traditional textual similarity metrics or the pre-Transformer model-based metrics are suitable for the task of semantic consistency assessment. LLM-based methods perform well on the whole, but best correlations with human judgments still lag behind those seen in other text generation tasks.</abstract>
      <url hash="2916fc9b">2025.inlg-main.6</url>
      <bibkey>huidrom-etal-2025-assessing</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>F</fixed-case>resh<fixed-case>T</fixed-case>ab: Sourcing Fresh Data for Table-to-Text Generation Evaluation</title>
      <author><first>Kristýna</first><last>Onderková</last></author>
      <author><first>Ondrej</first><last>Platek</last></author>
      <author><first>Zdeněk</first><last>Kasner</last></author>
      <author><first>Ondrej</first><last>Dusek</last></author>
      <pages>108–121</pages>
      <abstract>Table-to-text generation (insight generation from tables) is a challenging task that requires precision in analyzing the data. In addition, the evaluation of existing benchmarks is affected by contamination of Large Language Model (LLM) training data as well as domain imbalance. We introduce FreshTab, an on-the-fly table-to-text benchmark generation from Wikipedia, to combat the LLM data contamination problem and enable domain-sensitive evaluation. While non-English table-to-text datasets are limited, FreshTab collects datasets in different languages on demand (we experiment with German, Russian and French in addition to English). We find that insights generated by LLMs from recent tables collected by our method appear clearly worse by automatic metrics, but this does not translate into LLM and human evaluations. Domain effects are visible in all evaluations, showing that a domain-balanced benchmark is more challenging.</abstract>
      <url hash="2b8e3ff0">2025.inlg-main.7</url>
      <bibkey>onderkova-etal-2025-freshtab</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>KDA</fixed-case>: Knowledge Distillation Adapter for Cross-Lingual Transfer</title>
      <author><first>Ta-Bao</first><last>Nguyen</last></author>
      <author><first>Nguyen-Phuong</first><last>Phan</last></author>
      <author><first>Tung</first><last>Le</last></author>
      <author id="huy-tien-nguyen"><first>Huy Tien</first><last>Nguyen</last></author>
      <pages>122–133</pages>
      <abstract>State-of-the-art cross-lingual transfer often relies on massive multilingual models, but their prohibitive size and computational cost limit their practicality for low-resource languages. An alternative is to adapt powerful, task-specialized monolingual models, but this presents challenges in bridging the vocabulary and structural gaps between languages. To address this, we propose KDA, a Knowledge Distillation Adapter framework that efficiently adapts a fine-tuned, high-resource monolingual model to a low-resource target language. KDA utilizes knowledge distillation to transfer the source model’s task-solving capabilities to the target language in a parameter-efficient manner. In addition, we introduce a novel adapter architecture that integrates source-language token embeddings while learning new positional embeddings, directly mitigating cross-lingual representational mismatches. Our empirical results on zero-shot transfer for Vietnamese Sentiment Analysis demonstrate that KDA significantly outperforms existing methods, offering a new, effective, and computationally efficient pathway for cross-lingual transfer.</abstract>
      <url hash="fa756a85">2025.inlg-main.8</url>
      <bibkey>nguyen-etal-2025-kda</bibkey>
    </paper>
    <paper id="9">
      <title><fixed-case>V</fixed-case>i<fixed-case>N</fixed-case>um<fixed-case>FCR</fixed-case>: A Novel <fixed-case>V</fixed-case>ietnamese Benchmark for Numerical Reasoning Fact Checking on Social Media News</title>
      <author><first>Nhi Ngoc Phuong</first><last>Luong</last></author>
      <author><first>Anh Thi Lan</first><last>Le</last></author>
      <author><first>Tin Van</first><last>Huynh</last></author>
      <author><first>Kiet Van</first><last>Nguyen</last></author>
      <author><first>Ngan</first><last>Nguyen</last></author>
      <pages>134–147</pages>
      <abstract>In the digital era, the internet provides rapid and convenient access to vast amounts of information. However, much of this information remains unverified, particularly with the increasing prevalence of falsified numerical data, leading to public confusion and negative societal impacts. To address this issue, we developed ViNumFCR, a first dataset dedicated to fact-checking numerical information in Vietnamese. Comprising over 10,000 samples collected and constructed from online newspaper across 12 different topics. We assessed the performance of various fact-checking models, including Pretrained Language Models and Large Language Models, alongside retrieval techniques for gathering supporting evidence. Experimental results demonstrate that the XLM-R_Large model achieved the highest accuracy of 90.05% on the fact-checking task, while the combined SBERT + BM25 model attained a precision of over 97% on the evidence retrieval task. Additionally, we conducted an in-depth analysis of the linguistic features of the dataset to understand the factors influencing the performance models. The ViNumFCR dataset is publicly available to support further research.</abstract>
      <url hash="9a0bdf6a">2025.inlg-main.9</url>
      <bibkey>luong-etal-2025-vinumfcr</bibkey>
    </paper>
    <paper id="10">
      <title>Dual Debiasing: Remove Stereotypes and Keep Factual Gender for Fair Language Modeling and Translation</title>
      <author><first>Tomasz</first><last>Limisiewicz</last></author>
      <author><first>David</first><last>Mareček</last></author>
      <author><first>Tomáš</first><last>Musil</last></author>
      <pages>148–164</pages>
      <abstract>Mitigation of biases, such as language models’ reliance on gender stereotypes, is a crucial endeavor required for the creation of reliable and useful language technology. The crucial aspect of debiasing is to ensure that the models preserve their versatile capabilities, including their ability to solve language tasks and equitably represent various genders. To address these issues, we introduce Dual Dabiasing Algorithm through Model Adaptation (2DAMA). Novel Dual Debiasing enables robust reduction of stereotypical bias while preserving desired factual gender information encoded by language models. We show that 2DAMA effectively reduces gender bias in language models for English and is one of the first approaches facilitating the mitigation of their stereotypical tendencies in translation. The proposed method’s key advantage is the preservation of factual gender cues, which are useful in a wide range of natural language processing tasks.</abstract>
      <url hash="fbabdcad">2025.inlg-main.10</url>
      <bibkey>limisiewicz-etal-2025-dual</bibkey>
    </paper>
    <paper id="11">
      <title>Mining Contextualized Visual Associations from Images for Creativity Understanding</title>
      <author><first>Ananya</first><last>Sahu</last></author>
      <author><first>Amith</first><last>Ananthram</last></author>
      <author id="kathleen-mckeown"><first>Kathleen</first><last>McKeown</last></author>
      <pages>165–181</pages>
      <abstract>Understanding another person’s creative output requires a shared language of association. However, when training vision-language models such as CLIP, we rely on web-scraped datasets containing short, predominantly literal, alt-text. In this work, we introduce a method for mining contextualized associations for salient visual elements in an image that can scale to any unlabeled dataset. Given an image, we can use these mined associations to generate high quality creative captions at increasing degrees of abstraction. With our method, we produce a new dataset of visual associations and 1.7m creative captions for the images in MSCOCO. Human evaluation confirms that these captions remain visually grounded while exhibiting recognizably increasing abstraction. Moreover, fine-tuning a visual encoder on this dataset yields meaningful improvements in zero-shot image-text retrieval in two creative domains: poetry and metaphor visualization. We release our dataset, our generation code and our models for use by the broader community.</abstract>
      <url hash="ec4e5771">2025.inlg-main.11</url>
      <bibkey>sahu-etal-2025-mining</bibkey>
    </paper>
    <paper id="12">
      <title>Analysing Reference Production of Large Language Models</title>
      <author><first>Chengzhao</first><last>Wu</last></author>
      <author><first>Guanyi</first><last>Chen</last></author>
      <author><first>Fahime</first><last>Same</last></author>
      <author><first>Tingting</first><last>He</last></author>
      <pages>182–194</pages>
      <abstract>This study investigates how large language models (LLMs) produce referring expressions (REs) and to what extent their behaviour aligns with human patterns. We evaluate LLM performance in two settings: slot filling, %KvD the conventional task of referring expression generation, where REs are generated within a fixed context, and language generation, where REs are analysed within fully generated texts. Using the WebNLG corpus, we assess how well LLMs capture human variation in reference production and analyse their behaviour by examining the influence of several factors known to affect human reference production, including referential form, syntactic position, recency, and discourse status. Our findings show that (1) task framing significantly affects LLMs’ reference production; (2) while LLMs are sensitive to some of these factors, their referential behaviour consistently diverges from human use; and (3) larger model size does not necessarily yield more human-like variation. These results underscore key limitations in current LLMs’ ability to replicate human referential choices.</abstract>
      <url hash="298a1e1b">2025.inlg-main.12</url>
      <bibkey>wu-etal-2025-analysing</bibkey>
    </paper>
    <paper id="13">
      <title>Live Football Commentary (<fixed-case>LFC</fixed-case>): A <fixed-case>L</fixed-case>arge‐<fixed-case>S</fixed-case>cale Dataset for Building Football Commentary Generation Models</title>
      <author><first>Taiga</first><last>Someya</last></author>
      <author><first>Tatsuya</first><last>Ishigaki</last></author>
      <author><first>Hiroya</first><last>Takamura</last></author>
      <pages>195–201</pages>
      <abstract>Live football commentary brings the atmosphere and excitement of matches to fans in real time, but producing it requires costly professional announcers. We address this challenge by formulating commentary generation from player- and ball-tracking coordinates as a new language–generation task. To facilitate research on this problem we compile the <i>Live Football Commentary (LFC)</i> dataset, 12,440 time-stamped Japanese utterances aligned with tracking data for 40 J1 League matches ( 60 h). We benchmark three LLM-based baselines that receive the tracking data (i) as plain text, (ii) as pitch-map images, or (iii) in both modalities. Human evaluation shows that the text encoding already outperforms image and multimodal variants in both accuracy and relevance, indicating that current LLMs exploit structured coordinates more effectively than raw visuals. We release the LFC transcripts and evaluation code to establish a public test bed and spur future work on tracking-based commentary generation, saliency detection, and cross-modal integration.</abstract>
      <url hash="661b1453">2025.inlg-main.13</url>
      <bibkey>someya-etal-2025-live</bibkey>
    </paper>
    <paper id="14">
      <title>Exploring the Power of Large Language Models for <fixed-case>V</fixed-case>ietnamese Implitcit Sentiment Analysis</title>
      <author><first>Huy Gia</first><last>Luu</last></author>
      <author><first>Dang Van</first><last>Thin</last></author>
      <pages>202–214</pages>
      <abstract>We present the first benchmark for implicit sentiment analysis (ISA) in Vietnamese, aimed at evaluating large language models (LLMs) on their ability to interpret implicit sentiment accompanied by ViISA, a dataset specifically constructed for this task. We assess a variety of open-source and close-source LLMs using state-of-the-art (SOTA) prompting techniques. While LLMs achieve strong recall, they often misclassify implicit cues such as sarcasm and exaggeration, resulting in low precision. Through detailed error analysis, we highlight key challenges and suggest improvements to Chain-of-Thought prompting via more contextually aligned demonstrations.</abstract>
      <url hash="b32f7dfb">2025.inlg-main.14</url>
      <bibkey>luu-thin-2025-exploring</bibkey>
    </paper>
    <paper id="15">
      <title>Towards Trustworthy Lexical Simplification: Exploring Safety and Efficiency with Small <fixed-case>LLM</fixed-case>s</title>
      <author><first>Akio</first><last>Hayakawa</last></author>
      <author><first>Stefan</first><last>Bott</last></author>
      <author><first>Horacio</first><last>Saggion</last></author>
      <pages>215–231</pages>
      <abstract>Despite their strong performance, large language models (LLMs) face challenges in real-world application of lexical simplification (LS), particularly in privacy-sensitive and resource-constrained environments. Moreover, since vulnerable user groups (e.g., people with disabilities) are one of the key target groups of this technology, it is crucial to ensure the safety and correctness of the output of LS systems. To address these issues, we propose an efficient framework for LS systems that utilizes small LLMs deployable in local environments. Within this framework, we explore knowledge distillation with synthesized data and in-context learning as baselines. Our experiments in five languages evaluate model outputs both automatically and manually. Our manual analysis reveals that while knowledge distillation boosts automatic metric scores, it also introduces a safety trade-off by increasing harmful simplifications. Importantly, we find that the model’s output probability is a useful signal for detecting harmful simplifications. Leveraging this, we propose a filtering strategy that suppresses harmful simplifications while largely preserving beneficial ones. This work establishes a benchmark for efficient and safe LS with small LLMs. It highlights the key trade-offs between performance, efficiency, and safety, and demonstrates a promising approach for safe real-world deployment.</abstract>
      <url hash="42c94851">2025.inlg-main.15</url>
      <bibkey>hayakawa-etal-2025-towards</bibkey>
    </paper>
    <paper id="16">
      <title>Evaluating <fixed-case>LLM</fixed-case>s’ Ability to Understand Numerical Time Series for Text Generation</title>
      <author><first>Mizuki</first><last>Arai</last></author>
      <author><first>Tatsuya</first><last>Ishigaki</last></author>
      <author><first>Masayuki</first><last>Kawarada</last></author>
      <author><first>Yusuke</first><last>Miyao</last></author>
      <author><first>Hiroya</first><last>Takamura</last></author>
      <author><first>Ichiro</first><last>Kobayashi</last></author>
      <pages>232–248</pages>
      <abstract>Data-to-text generation tasks often involve processing numerical time-series as input such as financial statistics or meteorological data. Although large language models (LLMs) are a powerful approach to data-to-text, we still lack a comprehensive understanding of how well they actually understand time-series data. We therefore introduce a benchmark with 18 evaluation tasks to assess LLMs’ abilities of interpreting numerical time-series, which are categorized into: 1) event detection—identifying maxima and minima; 2) computation—averaging and summation; 3) pairwise comparison—comparing values over time; and 4) inference—imputation and forecasting. Our experiments reveal five key findings: 1) even state-of-the-art LLMs struggle with complex multi-step reasoning; 2) tasks that require extracting values or performing computations within a specified range of the time-series significantly reduce accuracy; 3) instruction tuning offers inconsistent improvements for numerical interpretation; 4) reasoning-based models outperform standard LLMs in complex numerical tasks; and 5) LLMs perform interpolation better than forecasting. These results establish a clear baseline and serve as a wake-up call for anyone aiming to blend fluent language with trustworthy numeric precision in time-series scenarios.</abstract>
      <url hash="4290e208">2025.inlg-main.16</url>
      <bibkey>arai-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="17">
      <title>Can <fixed-case>GPT</fixed-case> models Follow Human Summarization Guidelines? A Study for Targeted Communication Goals</title>
      <author><first>Yongxin</first><last>Zhou</last></author>
      <author><first>Fabien</first><last>Ringeval</last></author>
      <author><first>François</first><last>Portet</last></author>
      <pages>249–273</pages>
      <abstract>This study investigates the ability of GPT models (ChatGPT, GPT-4 and GPT-4o) to generate dialogue summaries that adhere to human guidelines. Our evaluation involved experimenting with various prompts to guide the models in complying with guidelines on two datasets: DialogSum (English social conversations) and DECODA (French call center interactions). Human evaluation, based on summarization guidelines, served as the primary assessment method, complemented by extensive quantitative and qualitative analyses. Our findings reveal a preference for GPT-generated summaries over those from task-specific pre-trained models and reference summaries, highlighting GPT models’ ability to follow human guidelines despite occasionally producing longer outputs and exhibiting divergent lexical and structural alignment with references. The discrepancy between ROUGE, BERTScore, and human evaluation underscores the need for more reliable automatic evaluation metrics.</abstract>
      <url hash="7be75a3f">2025.inlg-main.17</url>
      <bibkey>zhou-etal-2025-gpt</bibkey>
    </paper>
    <paper id="18">
      <title>References Matter: Investigating the Impact of Reference Set Variation on Summarization Evaluation</title>
      <author><first>Silvia</first><last>Casola</last></author>
      <author id="yang-janet-liu"><first>Yang Janet</first><last>Liu</last></author>
      <author><first>Siyao</first><last>Peng</last></author>
      <author><first>Oliver</first><last>Kraus</last></author>
      <author><first>Albert</first><last>Gatt</last></author>
      <author id="barbara-plank"><first>Barbara</first><last>Plank</last></author>
      <pages>274–291</pages>
      <abstract>Human language production exhibits remarkable richness and variation, reflecting diverse communication styles and intents. However, this variation is often overlooked in summarization evaluation. While having multiple reference summaries is known to improve correlation with human judgments, the impact of the reference set on reference-based metrics has not been systematically investigated. This work examines the sensitivity of widely used reference-based metrics in relation to the choice of reference sets, analyzing three diverse multi-reference summarization datasets: SummEval, GUMSum, and DUC2004. We demonstrate that many popular metrics exhibit significant instability. This instability is particularly concerning for n-gram-based metrics like ROUGE, where model rankings vary depending on the reference sets, undermining the reliability of model comparisons. We also collect human judgments on LLM outputs for genre-diverse data and examine their correlation with metrics to supplement existing findings beyond newswire summaries, finding weak-to-no correlation. Taken together, we recommend incorporating reference set variation into summarization evaluation to enhance consistency alongside correlation with human judgments, especially when evaluating LLMs.</abstract>
      <url hash="a1368352">2025.inlg-main.18</url>
      <bibkey>casola-etal-2025-references</bibkey>
    </paper>
    <paper id="19">
      <title><fixed-case>O</fixed-case>pe<fixed-case>NLG</fixed-case>auge: An Explainable Metric for <fixed-case>NLG</fixed-case> Evaluation with Open-Weights <fixed-case>LLM</fixed-case>s</title>
      <author><first>Ivan</first><last>Kartac</last></author>
      <author><first>Mateusz</first><last>Lango</last></author>
      <author><first>Ondrej</first><last>Dusek</last></author>
      <pages>292–337</pages>
      <abstract>Large Language Models (LLMs) have demonstrated great potential as evaluators of NLG systems, allowing for high-quality, reference-free, and multi-aspect assessments. However, existing LLM-based metrics suffer from two major drawbacks: reliance on proprietary models to generate training data or perform evaluations, and a lack of fine-grained, explanatory feedback. We introduce OpeNLGauge, a fully open-source, reference-free NLG evaluation metric that provides accurate explanations based on individual error spans. OpeNLGauge is available as a two-stage ensemble of larger open-weight LLMs, or as a small fine-tuned evaluation model, with confirmed generalizability to unseen tasks, domains and aspects. Our extensive meta-evaluation shows that OpeNLGauge achieves competitive correlation with human judgments, outperforming state-of-the-art models on certain tasks while maintaining full reproducibility and providing explanations more than twice as accurate.</abstract>
      <url hash="a465254a">2025.inlg-main.19</url>
      <bibkey>kartac-etal-2025-openlgauge</bibkey>
    </paper>
    <paper id="20">
      <title>Statistical Multicriteria Evaluation of <fixed-case>LLM</fixed-case>-Generated Text</title>
      <author><first>Esteban</first><last>Garces Arias</last></author>
      <author><first>Hannah</first><last>Blocher</last></author>
      <author><first>Julian</first><last>Rodemann</last></author>
      <author><first>Matthias</first><last>Assenmacher</last></author>
      <author><first>Christoph</first><last>Jansen</last></author>
      <pages>338–351</pages>
      <abstract>Assessing the quality of LLM-generated text remains a fundamental challenge in natural language processing. Current evaluation approaches often rely on isolated metrics or simplistic aggregations that fail to capture the nuanced trade-offs between coherence, diversity, fluency, and other relevant indicators of text quality. In this work, we adapt a recently proposed framework for statistical inference based on Generalized Stochastic Dominance (GSD) that addresses three critical limitations in existing benchmarking methodologies: the inadequacy of single-metric evaluation, the incompatibility between cardinal automatic metrics and ordinal human judgments, and the lack of inferential statistical guarantees. The GSD-front approach enables simultaneous evaluation across multiple quality dimensions while respecting their different measurement scales, building upon partial orders of decoding strategies, thus avoiding arbitrary weighting of the involved metrics. By applying this framework to evaluate common decoding strategies against human-generated text, we demonstrate its ability to identify statistically significant performance differences while accounting for potential deviations from the i.i.d. assumption of the sampling design.</abstract>
      <url hash="cd927366">2025.inlg-main.20</url>
      <bibkey>garces-arias-etal-2025-statistical</bibkey>
    </paper>
    <paper id="21">
      <title>Incorporating Formulaicness in the Automatic Evaluation of Naturalness: A Case Study in Logic-to-Text Generation</title>
      <author><first>Eduardo</first><last>Calò</last></author>
      <author><first>Guanyi</first><last>Chen</last></author>
      <author><first>Elias</first><last>Stengel-Eskin</last></author>
      <author><first>Albert</first><last>Gatt</last></author>
      <author id="kees-van-deemter"><first>Kees</first><last>van Deemter</last></author>
      <pages>352–365</pages>
      <abstract>Data-to-text natural language generation (NLG) models may produce outputs that closely mirror the structure of their input. We introduce formulaicness as a measure of the output-to-input structural resemblance, proposing it as an enhancement for reference-less naturalness evaluation. Focusing on logic-to-text generation, we construct a dataset and train a regressor to predict formulaicness scores. We collect human judgments on naturalness and examine how incorporating formulaicness into existing metrics affects alignment with these judgments.</abstract>
      <url hash="fdd0308b">2025.inlg-main.21</url>
      <bibkey>calo-etal-2025-incorporating</bibkey>
    </paper>
    <paper id="22">
      <title>Surprisal reveals diversity gaps in image captioning and different scorers change the story</title>
      <author><first>Nikolai</first><last>Ilinykh</last></author>
      <author><first>Simon</first><last>Dobnik</last></author>
      <pages>366–375</pages>
      <abstract>We quantify linguistic diversity in image captioning with surprisal variance – the spread of token-level negative log-probabilities within a caption set. On the MSCOCO test set, we compare five state-of-the-art vision-and-language LLMs, decoded with greedy and nucleus sampling, to human captions. Measured with a caption-trained n-gram LM, humans display roughly twice the surprisal variance of models, but rescoring the same captions with a general-language model reverses the pattern. Our analysis introduces the surprisal-based diversity metric for image captioning. We show that relying on a single scorer can completely invert conclusions, thus, robust diversity evaluation must report surprisal under several scorers.</abstract>
      <url hash="bffbe801">2025.inlg-main.22</url>
      <bibkey>ilinykh-dobnik-2025-surprisal</bibkey>
    </paper>
    <paper id="23">
      <title>Natural Language Translation of Formal Proofs through Informalization of Proof Steps and Recursive Summarization along Proof Structure</title>
      <author><first>Seiji</first><last>Hattori</last></author>
      <author><first>Takuya</first><last>Matsuzaki</last></author>
      <author><first>Makoto</first><last>Fujiwara</last></author>
      <pages>376–389</pages>
      <abstract>This paper proposes a natural language translation method for machine-verifiable formal proofs that leverages the informalization (verbalization of formal language proof steps) and summarization capabilities of LLMs. For evaluation, it was applied to formal proof data created in accordance with natural language proofs taken from an undergraduate-level textbook, and the quality of the generated natural language proofs was analyzed in comparison with the original natural language proofs. Furthermore, we will demonstrate that this method can output highly readable and accurate natural language proofs by applying it to existing formal proof library of the Lean proof assistant.</abstract>
      <url hash="78928bb6">2025.inlg-main.23</url>
      <bibkey>hattori-etal-2025-natural</bibkey>
    </paper>
    <paper id="24">
      <title><fixed-case>G</fixed-case>erman4<fixed-case>A</fixed-case>ll – A Dataset and Model for Readability-Controlled Paraphrasing in <fixed-case>G</fixed-case>erman</title>
      <author><first>Miriam</first><last>Anschütz</last></author>
      <author><first>Thanh Mai</first><last>Pham</last></author>
      <author><first>Eslam</first><last>Nasrallah</last></author>
      <author><first>Maximilian</first><last>Müller</last></author>
      <author><first>Cristian-George</first><last>Craciun</last></author>
      <author><first>Georg</first><last>Groh</last></author>
      <pages>390–407</pages>
      <abstract>The ability to paraphrase texts across different complexity levels is essential for creating accessible texts that can be tailored toward diverse reader groups. Thus, we introduce <b>German4All</b>, the first large-scale German dataset of aligned readability-controlled, paragraph-level paraphrases. It spans five readability levels and comprises over 25,000 samples. The dataset is automatically synthesized using GPT-4 and rigorously evaluated through both human and LLM-based judgments. Using German4All, we train an open-source, readability-controlled paraphrasing model that achieves state-of-the-art performance in German text simplification, enabling more nuanced and reader-specific adaptations. We open-source both the dataset and the model to encourage further research on multi-level paraphrasing.</abstract>
      <url hash="b17c2204">2025.inlg-main.24</url>
      <bibkey>anschutz-etal-2025-german4all</bibkey>
    </paper>
    <paper id="25">
      <title>Enhancing Named Entity Translation from Classical <fixed-case>C</fixed-case>hinese to <fixed-case>V</fixed-case>ietnamese in Traditional <fixed-case>V</fixed-case>ietnamese Medicine Domain: A Hybrid Masking and Dictionary-Augmented Approach</title>
      <author><first>Nhu</first><last>Pham</last></author>
      <author><first>Uyen</first><last>Nguyen</last></author>
      <author id="long-nguyen"><first>Long</first><last>Nguyen</last></author>
      <author id="dinh-dien"><first>Dien</first><last>Dinh</last></author>
      <pages>408–418</pages>
      <abstract>Vietnam’s traditional medical texts were historically written in Classical Chinese using Sino-Vietnamese pronunciations. As the Vietnamese language transitioned to a Latin-based national script and interest in integrating traditional medicine with modern healthcare grows, accurate translation of these texts has become increasingly important. However, the diversity of terminology and the complexity of translating medical entities into modern contexts pose significant challenges. To address this, we propose a method that fine-tunes large language models (LLMs) using augmented data and a Hybrid Entity Masking and Replacement (HEMR) strategy to improve named entity translation. We also introduce a parallel named entity translation dataset specifically curated for traditional Vietnamese medicine. Our evaluation across multiple LLMs shows that the proposed approach achieves a translation accuracy of 71.91%, demonstrating its effectiveness. These results underscore the importance of incorporating named entity awareness into translation systems, particularly in low-resource and domain-specific settings like traditional Vietnamese medicine.</abstract>
      <url hash="d029fe8d">2025.inlg-main.25</url>
      <bibkey>pham-etal-2025-enhancing</bibkey>
    </paper>
    <paper id="26">
      <title>Fine-Tuning, Prompting and <fixed-case>RAG</fixed-case> for Knowledge Graph-to-<fixed-case>R</fixed-case>ussian Text Generation. How do these Methods generalise to Out-of-Distribution Data?</title>
      <author><first>Anna</first><last>Nikiforovskaya</last></author>
      <author><first>William Eduardo</first><last>Soto Martinez</last></author>
      <author><first>Evan Parker Kelly</first><last>Chapple</last></author>
      <author><first>Claire</first><last>Gardent</last></author>
      <pages>419–448</pages>
      <abstract>Prior work on Knowledge Graph-to-Text generation has mostly evaluated models on in-domain test sets and/or with English as the target language. In contrast, we focus on Russian and we assess how various generation methods perform on out-of-domain, unseen data. Previous studies have shown that enriching the input with target-language verbalisations of entities and properties substantially improves the performance of fine-tuned models for Russian. We compare multiple variants of two contemporary paradigms — LLM prompting and Retrieval-Augmented Generation (RAG) — and investigate alternative ways to integrate such external knowledge into the generation process. Using automatic metrics and human evaluation, we find that on unseen data the fine-tuned model consistently underperforms, revealing limited generalisation capacity; that while it outperforms RAG by a small margin on most datasets, prompting generates less fluent text; and conversely, that RAG generates text that is less faithful to the input. Overall, both LLM prompting and RAG outperform Fine-Tuning across all unseen testsets. The code for this paper is available at https://github.com/Javanochka/KG-to-text-fine-tuning-prompting-rag</abstract>
      <url hash="777c10fb">2025.inlg-main.26</url>
      <bibkey>nikiforovskaya-etal-2025-fine</bibkey>
    </paper>
    <paper id="27">
      <title><fixed-case>F</fixed-case>in<fixed-case>S</fixed-case>tat2<fixed-case>SQL</fixed-case>: A <fixed-case>T</fixed-case>ext2<fixed-case>SQL</fixed-case> Pipeline for Financial Statement Analysis</title>
      <author><first>Hung Quang</first><last>Nguyen</last></author>
      <author><first>Anh Phuong</first><last>Trinh</last></author>
      <author><first>Hung Phan Quoc</first><last>Mai</last></author>
      <author><first>Phong Tuan</first><last>Trinh</last></author>
      <pages>449–464</pages>
      <abstract>Despite recent advances in LLMs, Text2SQL remains challenging for complex, domain-specific queries such as finance, where database designs and reporting standards vary widely. We introduce FinStat2SQL, a lightweight pipeline enabling natural language queries over financial statements, tailored to local standards like VAS. Our multi-agent setup combines large and small language models for entity extraction, SQL generation, and self-correction, and includes a fully automatic pipeline for synthetic data generation. Leveraging this synthetic data, we fine-tuned a 7B model that achieves 61.33% accuracy with sub-4s latency on consumer hardware, outperforming GPT-4o-mini on SQL generation. FinStat2SQL provides a scalable, cost-efficient solution for financial analysis. We made our source code publicly available at: https://github.com/hung20gg/chatbot_financial_statement.</abstract>
      <url hash="bd62fa1d">2025.inlg-main.27</url>
      <bibkey>nguyen-etal-2025-finstat2sql</bibkey>
    </paper>
    <paper id="28">
      <title>From Prototypical to Relational: How <fixed-case>LLM</fixed-case>s Navigate Complex Analogies</title>
      <author><first>Mayukh</first><last>Das</last></author>
      <author><first>Wolf-Tilo</first><last>Balke</last></author>
      <pages>465–485</pages>
      <abstract>We introduce a comprehensive benchmark to assess the analogical reasoning capabilities of large language models (LLMs) on complex analogy tasks that go beyond conventional formats with single correct answers. Unlike standard benchmarks that assume a singular ground truth, our framework presents a four-way multiple-choice analogy task in which all target options are semantically plausible. Leveraging concept pairs from Wikidata and AnalogyKB, we construct analogy instances enriched with multiple overlapping relational structures, where the relations are mined with RAG and ranked in salience through a GPT-4-assisted Max-Diff survey. To enable systematic evaluation, we propose three complementary semantic measures i.e. ranked relational overlap, context embedding similarity, and prototypicality; each grounded in established literature on analogical reasoning. Our experiments span a range of LLMs, evaluated under zero-shot, few-shot, and knowledge-enhanced prompting conditions. While models such as GPT-4 perform well on embedding-based and prototypicality-based measures, they consistently underperform when tasked with capturing fine-grained relational mappings. These results reveal that, despite their impressive surface-level semantic fluency, current LLMs exhibit notable limitations in structured relational reasoning.</abstract>
      <url hash="a2f9b068">2025.inlg-main.28</url>
      <attachment type="Supplementary_Attachment" hash="85dc1dd4">2025.inlg-main.28.Supplementary_Attachment.zip</attachment>
      <bibkey>das-balke-2025-prototypical</bibkey>
    </paper>
    <paper id="29">
      <title>Automated and Context-Aware Code Documentation Leveraging Advanced <fixed-case>LLM</fixed-case>s</title>
      <author><first>Swapnil Sharma</first><last>Sarker</last></author>
      <author><first>Tanzina Taher</first><last>Ifty</last></author>
      <pages>486–498</pages>
      <abstract>Code documentation is essential to improve software maintainability and comprehension. The tedious nature of manual code documentation has led to much research on automated documentation generation. Existing automated approaches primarily focused on code summarization, leaving a gap in template-based documentation generation (e.g., Javadoc), particularly with publicly available Large Language Models (LLMs). Furthermore, progress in this area has been hindered by the lack of a Javadoc-specific dataset that incorporates modern language features, provides broad framework/library coverage, and includes necessary contextual information. This study aims to address these gaps by developing a tailored dataset and assessing the capabilities of publicly available LLMs for context-aware, template-based Javadoc generation. In this work, we present a novel, context-aware dataset for Javadoc generation that includes critical structural and semantic information from modern Java codebases. We evaluate five open-source LLMs (including LLaMA-3.1, Gemma-2, Phi-3, Mistral, Qwen-2.5) using zero-shot, few-shot, and fine-tuned setups and provide a comparative analysis of their performance. Our results demonstrate that LLaMA 3.1 performs consistently well and is a reliable candidate for practical, automated Javadoc generation, offering a viable alternative to proprietary systems.</abstract>
      <url hash="ad957b7a">2025.inlg-main.29</url>
      <bibkey>sarker-ifty-2025-automated</bibkey>
    </paper>
    <paper id="30">
      <title><fixed-case>L</fixed-case>ogit<fixed-case>R</fixed-case>outer: a novel Attention variant for reducing Myopic Routing in Mixture of Experts</title>
      <author><first>Felipe</first><last>Rodriguez</last></author>
      <author><first>Marcelo</first><last>Mendoza</last></author>
      <pages>499–510</pages>
      <abstract>Mixture of Experts (MoEs) have emerged as strong alternatives to traditional transformers, offering significant advantages in terms of training and inference efficiency. At the core of this architecture lies the router, responsible for selecting which experts are activated for each token. However, despite these advances, routing mechanisms continue to face stability challenges that the basic architecture fails to fully address. One such issue is Myopic Routing, where each token determines its route independently, without considering the routing decisions made for other tokens. To address this limitation, the LogitAttention mechanism is introduced—a variant of traditional attention—and, building upon it, the LogitRouter, a novel routing architecture that incorporates contextual information about the routing of other tokens. Due to budget constraints, a set of simple experiments is designed to obtain preliminary evidence of performance trends. These experiments are empirically validated on established benchmarks such as BoolQ, MMLU, and ARC. Finally, the work concludes with an in-depth discussion of architectural variants, applicability, limitations, and future directions, which aims to support continued research in this area.</abstract>
      <url hash="f3aa8805">2025.inlg-main.30</url>
      <bibkey>rodriguez-mendoza-2025-logitrouter</bibkey>
    </paper>
    <paper id="31">
      <title>Restaurant Menu Categorization at Scale: <fixed-case>LLM</fixed-case>-Guided Hybrid Clustering</title>
      <author><first>Seemab</first><last>Latif</last></author>
      <author><first>Ashar</first><last>Mehmood</last></author>
      <author><first>Selim</first><last>Turki</last></author>
      <author><first>Huma</first><last>Ameer</last></author>
      <author><first>Ivan</first><last>Gorban</last></author>
      <author><first>Faysal</first><last>Fateh</last></author>
      <pages>511–521</pages>
      <abstract>Inconsistent naming of menu items across merchants presents a major challenge for businesses that rely on large-scale menu item catalogs. It hinders downstream tasks like pricing analysis, menu item deduplication, and recommendations. To address this, we propose the Cross-Platform Semantic Alignment Framework (CPSAF), a hybrid approach that integrates DBSCAN-based clustering with SIGMA (Semantic Item Grouping and Menu Abstraction), a Large Language Model based refinement module. SIGMA employs in-context learning with a large language model to generate generic menu item names and categories. We evaluate our framework on a proprietary dataset comprising over 700,000 unique menu items. Experiments involve tuning DBSCAN parameters and applying SIGMA to refine clusters. The performance is assessed using both structural metrics i.e. cluster count, coverage and semantic metrics i.e. intra and inter-cluster similarity along with manual qualitative inspection. CPSAF improves intra-cluster similarity from 0.88 to 0.98 and reduces singleton clusters by 33%, demonstrating its effectiveness in recovering soft semantic drift.</abstract>
      <url hash="2542152a">2025.inlg-main.31</url>
      <bibkey>latif-etal-2025-restaurant</bibkey>
    </paper>
    <paper id="32">
      <title>Taming the Titans: A Survey of Efficient <fixed-case>LLM</fixed-case> Inference Serving</title>
      <author><first>Ranran</first><last>Zhen</last></author>
      <author><first>Juntao</first><last>Li</last></author>
      <author><first>Yixin</first><last>Ji</last></author>
      <author><first>Zhenlin</first><last>Yang</last></author>
      <author><first>Tong</first><last>Liu</last></author>
      <author><first>Qingrong</first><last>Xia</last></author>
      <author><first>Xinyu</first><last>Duan</last></author>
      <author><first>Zhefeng</first><last>Wang</last></author>
      <author><first>Baoxing</first><last>Huai</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <pages>522–541</pages>
      <abstract>Large Language Models (LLMs) for Generative AI have achieved remarkable progress, evolving into sophisticated and versatile tools widely adopted across various domains and applications. However, the substantial memory overhead caused by their vast number of parameters, combined with the high computational demands of the attention mechanism, poses significant challenges in achieving low latency and high throughput for LLM inference services. Recent advancements, driven by groundbreaking research, have significantly accelerated progress in this field. This paper provides a comprehensive survey of these methods, covering fundamental instance-level approaches, in-depth cluster-level strategies, and emerging scenarios. At the instance level, we review model placement, request scheduling, decoding length prediction, storage management, and the disaggregation paradigm. At the cluster level, we explore GPU cluster deployment, multi-instance load balancing, and cloud service solutions. Additionally, we discuss specific tasks, modules, and auxiliary methods in emerging scenarios. Finally, we outline potential research directions to further advance the field of LLM inference serving.</abstract>
      <url hash="a99844d9">2025.inlg-main.32</url>
      <bibkey>zhen-etal-2025-taming</bibkey>
    </paper>
    <paper id="33">
      <title>Are Multi-Agents the new Pipeline Architecture for Data-to-Text Systems?</title>
      <author><first>Chinonso Cynthia</first><last>Osuji</last></author>
      <author><first>Brian</first><last>Timoney</last></author>
      <author><first>Mark</first><last>Andrade</last></author>
      <author id="thiago-castro-ferreira"><first>Thiago</first><last>Castro Ferreira</last></author>
      <author><first>Brian</first><last>Davis</last></author>
      <pages>542–553</pages>
      <abstract>Large Language Models (LLMs) have achieved remarkable results in natural language generation, yet challenges remain in data-to-text (D2T) tasks, particularly in controlling output, ensuring transparency, and maintaining factual consistency with the input. We introduce the first LLM-based multi-agent framework for D2T generation, coordinating specialized agents to produce high-quality, interpretable outputs. Our system combines the reasoning and acting abilities of ReAct agents, the self-correction of Reflexion agents, and the quality assurance of Guardrail agents, all directed by an Orchestrator agent that assigns tasks to three specialists—content ordering, text structuring, and surface realization—and iteratively refines outputs based on Guardrail feedback. This closed-loop design enables precise control and dynamic optimization, yielding text that is coherent, accurate, and grounded in the input data. On a relatively simple dataset like WebNLG, our framework performs competitively with end-to-end systems, highlighting its promise for more complex D2T scenarios.</abstract>
      <url hash="57887c37">2025.inlg-main.33</url>
      <bibkey>osuji-etal-2025-multi</bibkey>
    </paper>
    <paper id="34">
      <title>Generating Impact and Critique Explanations of Predictions made by a Goal Recognizer</title>
      <author><first>Jair</first><last>da Silva Ferreira Junior</last></author>
      <author><first>Ingrid</first><last>Zukerman</last></author>
      <author><first>Enes</first><last>Makalic</last></author>
      <author id="cecile-paris"><first>Cecile L.</first><last>Paris</last></author>
      <author><first>Mor</first><last>Vered</last></author>
      <pages>554–575</pages>
      <abstract>In this paper, we generate two types of explanations, Impact and Critique, of predictions made by a Goal Recognizer (GR) – a system that infers agents’ goals from observations. Impact explanations describe the top-predicted goal(s) and the main observations that led to these predictions. Critique explanations augment these explanations with evidence that challenges the GR’s predictions if so warranted. Our user study compares users’ goal-recognition accuracy for Impact and Critique explanations, and users’ views about these explanations, under three prediction-correctness conditions: correct, partially correct and incorrect. Our results show that (1) users stick with a GR’s predictions, even when a Critique explanation highlights its flaws; yet (2) Critique explanations are deemed better than Impact explanations in most respects.</abstract>
      <url hash="dbf7b276">2025.inlg-main.34</url>
      <bibkey>da-silva-ferreira-junior-etal-2025-generating</bibkey>
    </paper>
    <paper id="35">
      <title><fixed-case>PRIC</fixed-case>o<fixed-case>T</fixed-case>: Principle Retrieval and Injection from Inference Successes and Failures for <fixed-case>C</fixed-case>o<fixed-case>T</fixed-case> Improvement</title>
      <author><first>Yudai</first><last>Yamazaki</last></author>
      <author><first>Naoto</first><last>Takeda</last></author>
      <author><first>Yasutaka</first><last>Nishimura</last></author>
      <author><first>Kazushi</first><last>Ikeda</last></author>
      <pages>576–595</pages>
      <abstract>In-Context Learning (ICL) approaches, such as Zero-Shot and Few-Shot prompting, allow Large Language Models (LLMs) to tackle reasoning tasks without additional fine-tuning. However, Zero-Shot prompting often struggles with more complex tasks, whereas Few-Shot prompting demands considerable manual effort and domain expertise to design effective prompts. Although existing work has attempted to alleviate these issues by extracting reasoning rules from carefully crafted, task-specific representative examples, creating or obtaining such examples can be impractical in real-world scenarios. In this paper, we propose a novel approach that enhances the inference accuracy by injecting reasoning principles extracted from QA data, without relying on representative Few-Shot exemplars. This offers a lightweight yet adaptive way to boost accuracy on complex reasoning tasks, while avoiding manual effort and the high exploration costs typical of prior methods. Experiments on benchmarks show that, using GPT-4o, our method outperforms similarity-based Few-Shot and Zero-Shot prompting methods on challenging benchmarks such as GPQA-diamond, achieving an absolute accuracy improvement of up to 2% in scenarios where carefully crafted Few-Shot examples are unavailable.</abstract>
      <url hash="a0b2ace0">2025.inlg-main.35</url>
      <bibkey>yamazaki-etal-2025-pricot</bibkey>
    </paper>
    <paper id="36">
      <title>Cognitive Flow: An <fixed-case>LLM</fixed-case>-Automated Framework for Quantifying Reasoning Distillation</title>
      <author><first>José</first><last>Matos</last></author>
      <author><first>Catarina</first><last>Silva</last></author>
      <author><first>Hugo</first><last>Goncalo Oliveira</last></author>
      <pages>596–616</pages>
      <abstract>The ability of large language models (LLMs) to reason effectively is crucial for a wide range of applications, from complex decision-making to scientific research. However, it remains unclear how well reasoning capabilities are transferred or preserved when LLMs undergo Knowledge Distillation (KD), a process that typically reduces model size while attempting to retain performance. In this study, we explore the effects of model distillation on the reasoning abilities of various reasoning language models (RLMs). We introduce Cognitive Flow, a novel framework that systematically extracts meaning and map states in Chain-of-Thought (CoT) processes, offering new insights on model reasoning and enabling quantitative comparisons across RLMs. Using this framework, we investigate the impact of KD on CoTs produced by RLMs. We target DeepSeek-R1-671B and its distilled 70B, 32B and 14B versions, as well as QwenQwQ-32B from the Qwen series. We evaluate the models on three subsets of mathematical reasoning tasks with varying complexity from the MMLU benchmark. Our findings demonstrate that while distillation can effectively replicate a similar reasoning style under specific conditions, it struggles with simpler problems, revealing a significant divergence in the observable thought process and a potential limitation in the transfer of a robust and adaptable problem-solving capability.</abstract>
      <url hash="43f0a8bf">2025.inlg-main.36</url>
      <bibkey>matos-etal-2025-cognitive</bibkey>
    </paper>
    <paper id="37">
      <title>How (un)faithful are explainable <fixed-case>LLM</fixed-case>-based <fixed-case>NLG</fixed-case> metrics?</title>
      <author><first>Alex</first><last>Terentowicz</last></author>
      <author><first>Mateusz</first><last>Lango</last></author>
      <author><first>Ondrej</first><last>Dusek</last></author>
      <pages>617–658</pages>
      <abstract>Explainable NLG metrics are becoming a popular research topic; however, the faithfulness of the explanations they provide is typically not evaluated. In this work, we propose a testbed for assessing the faithfulness of span-based metrics by performing controlled perturbations of their explanations and observing changes in the final score. We show that several popular LLM evaluators do not consistently produce faithful explanations.</abstract>
      <url hash="448bf98b">2025.inlg-main.37</url>
      <bibkey>terentowicz-etal-2025-un</bibkey>
    </paper>
    <paper id="38">
      <title>Counterfactual Simulatability of <fixed-case>LLM</fixed-case> Explanations for Generation Tasks</title>
      <author><first>Marvin</first><last>Limpijankit</last></author>
      <author><first>Yanda</first><last>Chen</last></author>
      <author><first>Melanie</first><last>Subbiah</last></author>
      <author><first>Nicholas</first><last>Deas</last></author>
      <author id="kathleen-mckeown"><first>Kathleen</first><last>McKeown</last></author>
      <pages>659–683</pages>
      <abstract>LLMs can be unpredictable, as even slight alterations to the prompt can cause the output to change in unexpected ways. Thus, the ability of models to accurately explain their behavior is critical, especially in high-stakes settings. Counterfactual simulatability measures how well an explanation allows users to infer the model’s output on related counterfactuals and has been previously studied for yes/no question answering. We provide a general framework for extending this method to generation tasks, using news summarization and medical suggestion as example use cases. We find that while LLM explanations do enable users to better predict their outputs on counterfactuals in the summarization setting, there is significant room for improvement for medical suggestion. Furthermore, our results suggest that evaluating counterfactual simulatability may be more appropriate for skill-based tasks as opposed to knowledge-based tasks.</abstract>
      <url hash="762509ae">2025.inlg-main.38</url>
      <bibkey>limpijankit-etal-2025-counterfactual</bibkey>
    </paper>
    <paper id="39">
      <title><fixed-case>SWI</fixed-case>: Speaking with Intent in Large Language Models</title>
      <author><first>Yuwei</first><last>Yin</last></author>
      <author><first>Eunjeong</first><last>Hwang</last></author>
      <author><first>Giuseppe</first><last>Carenini</last></author>
      <pages>684–698</pages>
      <abstract>Intent, typically clearly formulated and planned, functions as a cognitive framework for communication and problem-solving. This paper introduces the concept of Speaking with Intent (SWI) in large language models (LLMs), where the explicitly generated intent encapsulates the model’s underlying intention and provides high-level planning to guide subsequent analysis and action. By emulating deliberate and purposeful thoughts in the human mind, SWI is hypothesized to enhance the reasoning capabilities and generation quality of LLMs. Extensive experiments on text summarization, multi-task question answering, and mathematical reasoning benchmarks consistently demonstrate the effectiveness and generalizability of Speaking with Intent over direct generation without explicit intent. Further analysis corroborates the generalizability of SWI under different experimental settings. Moreover, human evaluations verify the coherence, effectiveness, and interpretability of the intent produced by SWI. The promising results in enhancing LLMs with explicit intents pave a new avenue for boosting LLMs’ generation and reasoning abilities with cognitive notions.</abstract>
      <url hash="88a83c08">2025.inlg-main.39</url>
      <bibkey>yin-etal-2025-swi</bibkey>
    </paper>
    <paper id="40">
      <title>Forecasting Conversation Derailments Through Generation</title>
      <author><first>Yunfan</first><last>Zhang</last></author>
      <author id="kathleen-mckeown"><first>Kathleen</first><last>McKeown</last></author>
      <author><first>Smaranda</first><last>Muresan</last></author>
      <pages>699–715</pages>
      <abstract>Forecasting conversation derailment can be useful in real-world settings such as online content moderation, conflict resolution, and business negotiations. However, despite language models’ success at identifying offensive speech present in conversations, they struggle to forecast future conversation derailments. In contrast to prior work that predicts conversation outcomes solely based on the past conversation history, our approach samples multiple future conversation trajectories conditioned on existing conversation history using a fine-tuned LLM. It predicts the conversation outcome based on the consensus of these trajectories. We also experimented with leveraging socio-linguistic attributes, which reflect turn-level conversation dynamics, as guidance when generating future conversations. Our method of future conversation trajectories surpasses state-of-the-art results on English conversation derailment prediction benchmarks and demonstrates significant accuracy gains in ablation studies.</abstract>
      <url hash="b17a81cd">2025.inlg-main.40</url>
      <bibkey>zhang-etal-2025-forecasting</bibkey>
    </paper>
    <paper id="41">
      <title>Can <fixed-case>LLM</fixed-case>s Help Encoder Models Maintain Both High Accuracy and Consistency in Temporal Relation Classification?</title>
      <author><first>Adiel</first><last>Meir</last></author>
      <author><first>Kfir</first><last>Bar</last></author>
      <pages>716–733</pages>
      <abstract>Temporal relation classification (TRC) demands both accuracy and temporal consistency in event timeline extraction. Encoder-based models achieve high accuracy but introduce inconsistencies because they rely on pairwise classification, while LLMs leverage global context to generate temporal graphs, improving consistency at the cost of accuracy. We assess LLM prompting strategies for TRC and their effectiveness in assisting encoder models with cycle resolution. Results show that while LLMs improve consistency, they struggle with accuracy and do not outperform a simple confidence-based cycle resolution approach. Our code is publicly available at: <url>https://github.com/MatufA/timeline-extraction</url>.</abstract>
      <url hash="d6354b38">2025.inlg-main.41</url>
      <bibkey>meir-bar-2025-llms</bibkey>
    </paper>
    <paper id="42">
      <title>Benchmarking and Improving <fixed-case>LVLM</fixed-case>s on Event Extraction from Multimedia Documents</title>
      <author><first>Fuyu</first><last>Xing</last></author>
      <author><first>Zimu</first><last>Wang</last></author>
      <author><first>Wei</first><last>Wang</last></author>
      <author><first>Haiyang</first><last>Zhang</last></author>
      <pages>734–742</pages>
      <abstract>The proliferation of multimedia content necessitates the development of effective Multimedia Event Extraction (M²E²) systems. Though Large Vision-Language Models (LVLMs) have shown strong cross-modal capabilities, their utility in the M²E² task remains underexplored. In this paper, we present the first systematic evaluation of representative LVLMs, including DeepSeek-VL2 and the Qwen-VL series, on the M²E² dataset. Our evaluations cover text-only, image-only, and cross-media subtasks, assessed under both few-shot prompting and fine-tuning settings. Our key findings highlight the following valuable insights: (1) Few-shot LVLMs perform notably better on visual tasks but struggle significantly with textual tasks; (2) Fine-tuning LVLMs with LoRA substantially enhances model performance; and (3) LVLMs exhibit strong synergy when combining modalities, achieving superior performance in cross-modal settings. We further provide a detailed error analysis to reveal persistent challenges in areas such as semantic precision, localization, and cross-modal grounding, which remain critical obstacles for advancing M²E² capabilities.</abstract>
      <url hash="e1df3064">2025.inlg-main.42</url>
      <bibkey>xing-etal-2025-benchmarking</bibkey>
    </paper>
    <paper id="43">
      <title><fixed-case>QC</fixed-case>oder Benchmark: Bridging Language Generation and Quantum Hardware through Simulator-Based Feedback</title>
      <author><first>Taku</first><last>Mikuriya</last></author>
      <author><first>Tatsuya</first><last>Ishigaki</last></author>
      <author><first>Masayuki</first><last>Kawarada</last></author>
      <author><first>Shunya</first><last>Minami</last></author>
      <author><first>Tadashi</first><last>Kadowaki</last></author>
      <author><first>Yohichi</first><last>Suzuki</last></author>
      <author><first>Soshun</first><last>Naito</last></author>
      <author><first>Shunya</first><last>Takada</last></author>
      <author><first>Takumi</first><last>Kato</last></author>
      <author><first>Tamotsu</first><last>Basseda</last></author>
      <author><first>Reo</first><last>Yamada</last></author>
      <author><first>Hiroya</first><last>Takamura</last></author>
      <pages>743–752</pages>
      <abstract>Large language models (LLMs) have increasingly been applied to automatic programming code generation. This task can be viewed as a language generation task that bridges natural language, human knowledge, and programming logic. However, it remains underexplored in domains that require interaction with hardware devices, such as quantum programming, where human coders write Python code that is executed on a quantum computer. To address this gap, we introduce QCoder Benchmark, an evaluation framework that assesses LLMs on quantum programming with feedback from simulated hardware devices. Our benchmark offers two key features. First, it supports evaluation using a quantum simulator environment beyond conventional Python execution, allowing feedback of domain-specific metrics such as circuit depth, execution time, and error classification, which can be used to guide better generation. Second, it incorporates human-written code submissions collected from real programming contests, enabling both quantitative comparisons and qualitative analyses of LLM outputs against human-written codes. Our experiments reveal that even advanced models like GPT-4o achieve only around 18.97% accuracy, highlighting the difficulty of the benchmark. In contrast, reasoning-based models such as o3 reach up to 78% accuracy, outperforming averaged success rates of human-written codes (39.98%). We release the QCoder Benchmark dataset and public evaluation API to support further research.</abstract>
      <url hash="f9678b15">2025.inlg-main.43</url>
      <bibkey>mikuriya-etal-2025-qcoder</bibkey>
    </paper>
    <paper id="44">
      <title>When <fixed-case>LLM</fixed-case>s Can’t Help: Real-World Evaluation of <fixed-case>LLM</fixed-case>s in Nutrition</title>
      <author><first>Karen Jia-Hui</first><last>Li</last></author>
      <author><first>Simone</first><last>Balloccu</last></author>
      <author><first>Ondrej</first><last>Dusek</last></author>
      <author><first>Ehud</first><last>Reiter</last></author>
      <pages>753–779</pages>
      <abstract>The increasing trust in large language models (LLMs), especially in the form of chatbots, is often undermined by the lack of their extrinsic evaluation. This holds particularly true in nutrition, where randomised controlled trials (RCTs) are the gold standard, and experts demand them for evidence-based deployment. LLMs have shown promising results in this field, but these are limited to intrinsic setups. We address this gap by running the first RCT involving LLMs for nutrition. We augment a rule-based chatbot with two LLM-based features: (1) message rephrasing for conversational variety and engagement, and (2) nutritional counselling through a fine-tuned model. In our seven-week RCT (n=81), we compare chatbot variants with and without LLM integration. We measure effects on dietary outcome, emotional well-being, and engagement. Despite our LLM-based features performing well in intrinsic evaluation, we find that they did not yield consistent benefits in real-world deployment. These results highlight critical gaps between intrinsic evaluations and real-world impact, emphasising the need for interdisciplinary, human-centred approaches.</abstract>
      <url hash="ca67e0ea">2025.inlg-main.44</url>
      <bibkey>li-etal-2025-llms-cant</bibkey>
    </paper>
    <paper id="45">
      <title>Who’s Laughing Now? An Overview of Computational Humour Generation and Explanation</title>
      <author><first>Tyler</first><last>Loakman</last></author>
      <author><first>William</first><last>Thorne</last></author>
      <author><first>Chenghua</first><last>Lin</last></author>
      <pages>780–794</pages>
      <abstract>The creation and perception of humour is a fundamental human trait, positioning its computational understanding as one of the most challenging tasks in natural language processing (NLP). As an abstract, creative, and frequently context-dependent construct, humour requires extensive reasoning to understand and create, making it a pertinent task for assessing the common-sense knowledge and reasoning abilities of modern large language models (LLMs). In this work, we survey the landscape of computational humour as it pertains to the generative tasks of creation and explanation. We observe that, despite the task of understanding humour bearing all the hallmarks of a foundational NLP task, work on generating and explaining humour beyond puns remains sparse, while state-of-the-art models continue to fall short of human capabilities. We bookend our literature survey by motivating the importance of computational humour processing as a subdiscipline of NLP and presenting an extensive discussion of future directions for research in the area that takes into account the subjective and ethically ambiguous nature of humour.</abstract>
      <url hash="f35ee9f3">2025.inlg-main.45</url>
      <bibkey>loakman-etal-2025-whos</bibkey>
    </paper>
    <paper id="46">
      <title>Input Matters: Evaluating Input Structure’s Impact on <fixed-case>LLM</fixed-case> Summaries of Sports Play-by-Play</title>
      <author><first>Barkavi</first><last>Sundararajan</last></author>
      <author id="somayajulu-sripada"><first>Somayajulu</first><last>Sripada</last></author>
      <author><first>Ehud</first><last>Reiter</last></author>
      <pages>795–809</pages>
      <abstract>A major concern when deploying LLMs in accuracy-critical domains such as sports reporting is that the generated text may not faithfully reflect the input data. We quantify how input structure affects hallucinations and other factual errors in LLM-generated summaries of NBA play-by-play data, across three formats: row-structured, JSON and unstructured. We manually annotated 3,312 factual errors across 180 game summaries produced by two models, Llama-3.1-70B and Qwen2.5-72B. Input structure has a strong effect: JSON input reduces error rates by 69% for Llama and 65% for Qwen compared to unstructured input, while row-structured input reduces errors by 54% for Llama and 51% for Qwen. A two-way repeated-measures ANOVA shows that input structure accounts for over 80% of the variance in error rates, with Tukey HSD post hoc tests confirming statistically significant differences between all input formats.</abstract>
      <url hash="c4280413">2025.inlg-main.46</url>
      <bibkey>sundararajan-etal-2025-input</bibkey>
    </paper>
    <paper id="47">
      <title>Scaling Up Data-to-Text Generation to Longer Sequences: A New Dataset and Benchmark Results for Generation from Large Triple Sets</title>
      <author><first>Chinonso Cynthia</first><last>Osuji</last></author>
      <author><first>Simon</first><last>Mille</last></author>
      <author><first>Ornait</first><last>O’Connell</last></author>
      <author id="thiago-castro-ferreira"><first>Thiago</first><last>Castro Ferreira</last></author>
      <author id="anja-belz"><first>Anya</first><last>Belz</last></author>
      <author><first>Brian</first><last>Davis</last></author>
      <pages>810–822</pages>
      <abstract>The ability of LLMs to write coherent, faithful long texts from structured data inputs remains relatively uncharted, in part because nearly all public data-to-text datasets contain only short input-output pairs. To address these gaps, we benchmark six LLMs, a rule‐based system and human-written texts on a new long-input dataset in English and Irish via LLM-based evaluation. We find substantial differences between models and languages.</abstract>
      <url hash="0d9f1bcd">2025.inlg-main.47</url>
      <bibkey>osuji-etal-2025-scaling</bibkey>
    </paper>
    <paper id="48">
      <title>Annotating Hallucinations in Question-Answering using Rewriting</title>
      <author><first>Xu</first><last>Liu</last></author>
      <author><first>Guanyi</first><last>Chen</last></author>
      <author id="kees-van-deemter"><first>Kees</first><last>van Deemter</last></author>
      <author><first>Tingting</first><last>He</last></author>
      <pages>823–832</pages>
      <abstract>Hallucinations pose a persistent challenge in open-ended question answering (QA). Traditional annotation methods, such as span-labelling, suffer from inconsistency and limited coverage. In this paper, we propose a rewriting-based framework as a new perspective on hallucinations in open-ended QA. We report on an experiment in which annotators are instructed to rewrite LLM-generated answers directly to ensure factual accuracy, with edits automatically recorded. Using the Chinese portion of the Mu-SHROOM dataset, we conduct a controlled rewriting experiment, comparing fact-checking tools (Google vs. GPT-4o), and analysing how tool choice, annotator background, and question openness influence rewriting behaviour. We find that rewriting leads to more hallucinations being identified, with higher inter-annotator agreement, than span-labelling.</abstract>
      <url hash="7514ac2c">2025.inlg-main.48</url>
      <attachment type="Supplementary_Attachment" hash="83b47947">2025.inlg-main.48.Supplementary_Attachment.zip</attachment>
      <bibkey>liu-etal-2025-annotating</bibkey>
    </paper>
    <paper id="49">
      <title>Effectiveness of Chain-of-Thought in Distilling Reasoning Capability from Large Language Models</title>
      <author><first>Cong Thanh</first><last>Do</last></author>
      <author><first>Rama Sanand</first><last>Doddipatla</last></author>
      <author><first>Kate</first><last>Knill</last></author>
      <pages>833–845</pages>
      <abstract>Chain-of-Thought (CoT) prompting is a widely used method to improve the reasoning capability of Large Language Models (LLMs). More recently, CoT has been leveraged in Knowledge Distillation (KD) to transfer reasoning capability from a larger LLM to a smaller one. This paper examines the role of CoT in distilling the reasoning capability from larger LLMs to smaller LLMs using white-box KD, analyzing its effectiveness in improving the performance of the distilled models for various natural language reasoning and understanding tasks. We conduct white-box KD experiments using LLMs from the Qwen and Llama2 families, employing CoT data from the CoT-Collection dataset. The distilled models are then evaluated on natural language reasoning and understanding tasks from the BIG-Bench-Hard (BBH) benchmark, which presents complex challenges for smaller LLMs. Experimental results demonstrate the role of CoT in improving white-box KD effectiveness, enabling the distilled models to achieve better average performance in natural language reasoning and understanding tasks from BBH.</abstract>
      <url hash="4755ca65">2025.inlg-main.49</url>
      <bibkey>do-etal-2025-effectiveness</bibkey>
    </paper>
    <paper id="50">
      <title>Face the Facts! Evaluating <fixed-case>RAG</fixed-case>-based Pipelines for Professional Fact-Checking</title>
      <author><first>Daniel</first><last>Russo</last></author>
      <author><first>Stefano</first><last>Menini</last></author>
      <author><first>Jacopo</first><last>Staiano</last></author>
      <author><first>Marco</first><last>Guerini</last></author>
      <pages>846–865</pages>
      <abstract>Natural Language Processing and Generation systems have recently shown the potential to complement and streamline the costly and time-consuming job of professional fact-checkers. In this work, we lift several constraints of current state-of-the-art pipelines for automated fact-checking based on the Retrieval-Augmented Generation (RAG) paradigm. Our goal is to benchmark, following professional fact-checking practices, RAG-based methods for the generation of verdicts - i.e., short texts discussing the veracity of a claim - evaluating them on stylistically complex claims and heterogeneous, yet reliable, knowledge bases. Our findings show a complex landscape, where, for example, LLM-based retrievers outperform other retrieval techniques, though they still struggle with heterogeneous knowledge bases; larger models excel in verdict faithfulness, while smaller models provide better context adherence, with human evaluations favouring zero-shot and one-shot approaches for informativeness, and fine-tuned models for emotional alignment.</abstract>
      <url hash="76ea7cb7">2025.inlg-main.50</url>
      <bibkey>russo-etal-2025-face</bibkey>
    </paper>
  </volume>
  <volume id="genchal" ingest-date="2025-11-11" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 18th International Natural Language Generation Conference: Generation Challenges</booktitle>
      <editor><first>Simon</first><last>Mille</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Hanoi, Vietnam</address>
      <month>October</month>
      <year>2025</year>
      <url hash="6cb7e18b">2025.inlg-genchal</url>
      <venue>inlg</venue>
    </meta>
    <frontmatter>
      <url hash="85cb629d">2025.inlg-genchal.0</url>
      <bibkey>inlg-2025-genchal</bibkey>
    </frontmatter>
    <paper id="1">
      <title>The 2024 <fixed-case>GEM</fixed-case> Shared Task on Multilingual Data-to-Text Generation: <fixed-case>E</fixed-case>nglish and <fixed-case>S</fixed-case>panish Qualitative Evaluation Results</title>
      <author><first>João</first><last>Sedoc</last></author>
      <author><first>Simon</first><last>Mille</last></author>
      <author><first>Miruna Adriana</first><last>Clinciu</last></author>
      <author><first>Yixin</first><last>Liu</last></author>
      <author><first>Kaustubh</first><last>Dhole</last></author>
      <author><first>Saad</first><last>Mahamood</last></author>
      <pages>1–36</pages>
      <url hash="6be32143">2025.inlg-genchal.1</url>
      <bibkey>sedoc-etal-2025-2024</bibkey>
    </paper>
    <paper id="2">
      <title>Live Commentary Planning and Generation</title>
      <author><first>Chung-Chi</first><last>Chen</last></author>
      <author><first>Huan-Wen</first><last>Ho</last></author>
      <author><first>Yu-Yu</first><last>Chang</last></author>
      <author><first>Ming-Hung</first><last>Wang</last></author>
      <author id="ramon-ruiz-dolz"><first>Ramon</first><last>Ruiz-Dolz</last></author>
      <author id="chris-reed"><first>Chris</first><last>Reed</last></author>
      <author><first>Ichiro</first><last>Kobayashi</last></author>
      <author><first>Yusuke</first><last>Miyao</last></author>
      <author><first>Hiroya</first><last>Takamura</last></author>
      <pages>37–43</pages>
      <url hash="b8557be8">2025.inlg-genchal.2</url>
      <bibkey>chen-etal-2025-live</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>DCU</fixed-case>-<fixed-case>ADAPT</fixed-case>-mod<fixed-case>PB</fixed-case> at the <fixed-case>GEM</fixed-case>’24 Data-to-Text Task: Analysis of Human Evaluation Results</title>
      <author><first>Rudali</first><last>Huidrom</last></author>
      <author><first>Chinonso Cynthia</first><last>Osuji</last></author>
      <author><first>Kolawole John</first><last>Adebayo</last></author>
      <author id="thiago-castro-ferreira"><first>Thiago</first><last>Castro Ferreira</last></author>
      <author><first>Brian</first><last>Davis</last></author>
      <pages>44–47</pages>
      <url hash="f456e4a0">2025.inlg-genchal.3</url>
      <bibkey>huidrom-etal-2025-dcu</bibkey>
    </paper>
    <paper id="4">
      <title>Team <fixed-case>S</fixed-case>aar<fixed-case>LST</fixed-case> at the <fixed-case>GEM</fixed-case>’24 <fixed-case>D</fixed-case>2<fixed-case>T</fixed-case> Task: Symbolic retrieval substantially reduces hallucination in data-to-text generation</title>
      <author><first>Mayank</first><last>Jobanputra</last></author>
      <author><first>Vera</first><last>Demberg</last></author>
      <pages>48–50</pages>
      <url hash="87c1edeb">2025.inlg-genchal.4</url>
      <bibkey>jobanputra-demberg-2025-team</bibkey>
    </paper>
  </volume>
  <volume id="demos" ingest-date="2025-11-11" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 18th International Natural Language Generation Conference: System Demonstrations</booktitle>
      <editor><first>Lucie</first><last>Flek</last></editor>
      <editor><first>Shashi</first><last>Narayan</last></editor>
      <editor><first>Lê Hồng</first><last>Phương</last></editor>
      <editor><first>Jiahuan</first><last>Pei</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Hanoi, Vietnam</address>
      <month>October</month>
      <year>2025</year>
      <url hash="aae9c9ad">2025.inlg-demos</url>
      <venue>inlg</venue>
    </meta>
    <frontmatter>
      <url hash="c0838755">2025.inlg-demos.0</url>
      <bibkey>inlg-2025-demos</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Echoes of Others: Real-Time <fixed-case>LLM</fixed-case> Dialogue Generation for Immersive <fixed-case>NPC</fixed-case> Interaction</title>
      <author><first>James</first><last>McGrath</last></author>
      <author><first>Michela</first><last>Lorandi</last></author>
      <author id="anja-belz"><first>Anya</first><last>Belz</last></author>
      <pages>1–2</pages>
      <url hash="7d579ea5">2025.inlg-demos.1</url>
      <bibkey>mcgrath-etal-2025-echoes</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>CSP</fixed-case>aper Review: Fast, Rubric-Faithful Conference Feedback</title>
      <author><first>Lele</first><last>Cao</last></author>
      <author><first>Lei</first><last>You</last></author>
      <author><first>R&amp;d</first><last>Team</last></author>
      <pages>3–7</pages>
      <url hash="abb99592">2025.inlg-demos.2</url>
      <bibkey>cao-etal-2025-cspaper</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>V</fixed-case>ita<fixed-case>E</fixed-case>val: Open-source Human Evaluation Tool for Video-to-Text and Video-to-Audio Systems</title>
      <author><first>Goran</first><last>Topic</last></author>
      <author><first>Yuki</first><last>Saito</last></author>
      <author><first>Katsuhito</first><last>Sudoh</last></author>
      <author><first>Shinnosuke</first><last>Takamichi</last></author>
      <author><first>Hiroya</first><last>Takamura</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <author><first>Tatsuya</first><last>Ishigaki</last></author>
      <pages>8–9</pages>
      <url hash="0d81d9f4">2025.inlg-demos.3</url>
      <bibkey>topic-etal-2025-vitaeval</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>ARTIST</fixed-case>: A Learning Support System for Fostering Students’ Argumentative Writing Skills</title>
      <author><first>Thomas</first><last>Huber</last></author>
      <author><first>Christina</first><last>Niklaus</last></author>
      <pages>10–13</pages>
      <url hash="23178e72">2025.inlg-demos.4</url>
      <bibkey>huber-niklaus-2025-artist</bibkey>
    </paper>
  </volume>
  <event id="inlg-2025">
    <colocated>
      <volume-id>2025.llm4medr-1</volume-id>
      <volume-id>2025.aiwolfdial-1</volume-id>
      <volume-id>2025.vlsp-1</volume-id>
    </colocated>
  </event>
</collection>
