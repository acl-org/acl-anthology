<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.wmt">
  <volume id="1" ingest-date="2020-12-22" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Fifth Conference on Machine Translation</booktitle>
      <editor><first>Loïc</first><last>Barrault</last></editor>
      <editor><first>Ondřej</first><last>Bojar</last></editor>
      <editor><first>Fethi</first><last>Bougares</last></editor>
      <editor><first>Rajen</first><last>Chatterjee</last></editor>
      <editor><first>Marta R.</first><last>Costa-jussà</last></editor>
      <editor><first>Christian</first><last>Federmann</last></editor>
      <editor><first>Mark</first><last>Fishel</last></editor>
      <editor><first>Alexander</first><last>Fraser</last></editor>
      <editor><first>Yvette</first><last>Graham</last></editor>
      <editor><first>Paco</first><last>Guzman</last></editor>
      <editor><first>Barry</first><last>Haddow</last></editor>
      <editor><first>Matthias</first><last>Huck</last></editor>
      <editor><first>Antonio Jimeno</first><last>Yepes</last></editor>
      <editor><first>Philipp</first><last>Koehn</last></editor>
      <editor><first>André</first><last>Martins</last></editor>
      <editor><first>Makoto</first><last>Morishita</last></editor>
      <editor><first>Christof</first><last>Monz</last></editor>
      <editor><first>Masaaki</first><last>Nagata</last></editor>
      <editor><first>Toshiaki</first><last>Nakazawa</last></editor>
      <editor><first>Matteo</first><last>Negri</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>November</month>
      <year>2020</year>
      <venue>wmt</venue>
    </meta>
    <frontmatter>
      <url hash="42919a30">2020.wmt-1.0</url>
      <bibkey>wmt-2020-machine</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Findings of the 2020 Conference on Machine Translation (<fixed-case>WMT</fixed-case>20)</title>
      <author><first>Loïc</first><last>Barrault</last></author>
      <author><first>Magdalena</first><last>Biesialska</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <author><first>Marta R.</first><last>Costa-jussà</last></author>
      <author><first>Christian</first><last>Federmann</last></author>
      <author><first>Yvette</first><last>Graham</last></author>
      <author><first>Roman</first><last>Grundkiewicz</last></author>
      <author><first>Barry</first><last>Haddow</last></author>
      <author><first>Matthias</first><last>Huck</last></author>
      <author><first>Eric</first><last>Joanis</last></author>
      <author><first>Tom</first><last>Kocmi</last></author>
      <author><first>Philipp</first><last>Koehn</last></author>
      <author><first>Chi-kiu</first><last>Lo</last></author>
      <author><first>Nikola</first><last>Ljubešić</last></author>
      <author><first>Christof</first><last>Monz</last></author>
      <author><first>Makoto</first><last>Morishita</last></author>
      <author><first>Masaaki</first><last>Nagata</last></author>
      <author><first>Toshiaki</first><last>Nakazawa</last></author>
      <author><first>Santanu</first><last>Pal</last></author>
      <author><first>Matt</first><last>Post</last></author>
      <author><first>Marcos</first><last>Zampieri</last></author>
      <pages>1–55</pages>
      <abstract>This paper presents the results of the news translation task and the similar language translation task, both organised alongside the Conference on Machine Translation (WMT) 2020. In the news task, participants were asked to build machine translation systems for any of 11 language pairs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages.</abstract>
      <url hash="fe60b8c7">2020.wmt-1.1</url>
      <bibkey>barrault-etal-2020-findings</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2020">WMT 2020</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/pmindia">PMIndia</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2018">WMT 2018</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikimatrix">WikiMatrix</pwcdataset>
    </paper>
    <paper id="2">
      <title>Findings of the First Shared Task on Lifelong Learning Machine Translation</title>
      <author><first>Loïc</first><last>Barrault</last></author>
      <author><first>Magdalena</first><last>Biesialska</last></author>
      <author><first>Marta R.</first><last>Costa-jussà</last></author>
      <author><first>Fethi</first><last>Bougares</last></author>
      <author><first>Olivier</first><last>Galibert</last></author>
      <pages>56–64</pages>
      <abstract>A lifelong learning system can adapt to new data without forgetting previously acquired knowledge. In this paper, we introduce the first benchmark for lifelong learning machine translation. For this purpose, we provide training, lifelong and test data sets for two language pairs: English-German and English-French. Additionally, we report the results of our baseline systems, which we make available to the public. The goal of this shared task is to encourage research on the emerging topic of lifelong learning machine translation.</abstract>
      <url hash="b24dc9ce">2020.wmt-1.2</url>
      <video href="https://slideslive.com/38939679"/>
      <bibkey>barrault-etal-2020-findings-first</bibkey>
    </paper>
    <paper id="3">
      <title>Findings of the <fixed-case>WMT</fixed-case> 2020 Shared Task on Chat Translation</title>
      <author><first>M. Amin</first><last>Farajian</last></author>
      <author><first>António V.</first><last>Lopes</last></author>
      <author><first>André F. T.</first><last>Martins</last></author>
      <author><first>Sameen</first><last>Maruf</last></author>
      <author><first>Gholamreza</first><last>Haffari</last></author>
      <pages>65–75</pages>
      <abstract>We report the results of the first edition of the WMT shared task on chat translation. The task consisted of translating bilingual conversational text, in particular customer support chats for the English-German language pair (English agent, German customer). This task varies from the other translation shared tasks, i.e. news and biomedical, mainly due to the fact that the conversations are bilingual, less planned, more informal, and often ungrammatical. Furthermore, such conversations are usually characterized by shorter and simpler sentences and contain more pronouns. We received 14 submissions from 6 participating teams, all of them covering both directions, i.e. En-&gt;De for agent utterances and De-&gt;En for customer messages. We used automatic metrics (BLEU and TER) for evaluating the translations of both agent and customer messages and human document-level direct assessments (DDA) to evaluate the agent translations.</abstract>
      <url hash="26f9356c">2020.wmt-1.3</url>
      <video href="https://slideslive.com/38939674"/>
      <bibkey>farajian-etal-2020-findings</bibkey>
      <pwccode url="https://github.com/Unbabel/BConTrasT" additional="false">Unbabel/BConTrasT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/taskmaster-1">Taskmaster-1</pwcdataset>
    </paper>
    <paper id="4">
      <title>Findings of the <fixed-case>WMT</fixed-case> 2020 Shared Task on Machine Translation Robustness</title>
      <author><first>Lucia</first><last>Specia</last></author>
      <author><first>Zhenhao</first><last>Li</last></author>
      <author><first>Juan</first><last>Pino</last></author>
      <author><first>Vishrav</first><last>Chaudhary</last></author>
      <author><first>Francisco</first><last>Guzmán</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <author><first>Nadir</first><last>Durrani</last></author>
      <author><first>Yonatan</first><last>Belinkov</last></author>
      <author><first>Philipp</first><last>Koehn</last></author>
      <author><first>Hassan</first><last>Sajjad</last></author>
      <author><first>Paul</first><last>Michel</last></author>
      <author><first>Xian</first><last>Li</last></author>
      <pages>76–91</pages>
      <abstract>We report the findings of the second edition of the shared task on improving robustness in Machine Translation (MT). The task aims to test current machine translation systems in their ability to handle challenges facing MT models to be deployed in the real world, including domain diversity and non-standard texts common in user generated content, especially in social media. We cover two language pairs – English-German and English-Japanese and provide test sets in zero-shot and few-shot variants. Participating systems are evaluated both automatically and manually, with an additional human evaluation for ”catastrophic errors”. We received 59 submissions by 11 participating teams from a variety of types of institutions.</abstract>
      <url hash="66a5424e">2020.wmt-1.4</url>
      <video href="https://slideslive.com/38939676"/>
      <bibkey>specia-etal-2020-findings</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/covost">CoVoST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mtnt">MTNT</pwcdataset>
    </paper>
    <paper id="5">
      <title>The <fixed-case>U</fixed-case>niversity of <fixed-case>E</fixed-case>dinburgh’s <fixed-case>E</fixed-case>nglish-<fixed-case>T</fixed-case>amil and <fixed-case>E</fixed-case>nglish-<fixed-case>I</fixed-case>nuktitut Submissions to the <fixed-case>WMT</fixed-case>20 News Translation Task</title>
      <author><first>Rachel</first><last>Bawden</last></author>
      <author><first>Alexandra</first><last>Birch</last></author>
      <author><first>Radina</first><last>Dobreva</last></author>
      <author><first>Arturo</first><last>Oncevay</last></author>
      <author><first>Antonio Valerio</first><last>Miceli Barone</last></author>
      <author><first>Philip</first><last>Williams</last></author>
      <pages>92–99</pages>
      <abstract>We describe the University of Edinburgh’s submissions to the WMT20 news translation shared task for the low resource language pair English-Tamil and the mid-resource language pair English-Inuktitut. We use the neural machine translation transformer architecture for all submissions and explore a variety of techniques to improve translation quality to compensate for the lack of parallel training data. For the very low-resource English-Tamil, this involves exploring pretraining, using both language model objectives and translation using an unrelated high-resource language pair (German-English), and iterative backtranslation. For English-Inuktitut, we explore the use of multilingual systems, which, despite not being part of the primary submission, would have achieved the best results on the test set.</abstract>
      <url hash="92be8f2c">2020.wmt-1.5</url>
      <video href="https://slideslive.com/38939642"/>
      <bibkey>bawden-etal-2020-university</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>GTCOM</fixed-case> Neural Machine Translation Systems for <fixed-case>WMT</fixed-case>20</title>
      <author><first>Chao</first><last>Bei</last></author>
      <author><first>Hao</first><last>Zong</last></author>
      <author><first>Qingmin</first><last>Liu</last></author>
      <author><first>Conghu</first><last>Yuan</last></author>
      <pages>100–104</pages>
      <abstract>This paper describes the Global Tone Communication Co., Ltd.’s submission of the WMT20 shared news translation task. We participate in four directions: English to (Khmer and Pashto) and (Khmer and Pashto) to English. Further, we get the best BLEU scores in the directions of English to Pashto, Pashto to English and Khmer to English (13.1, 23.1 and 25.5 respectively) among all the participants. Our submitted systems are unconstrained and focus on mBART (Multilingual Bidirectional and Auto-Regressive Transformers), back-translation and forward-translation. Also, we apply rules, language model and RoBERTa model to filter monolingual, parallel sentences and synthetic sentences. Besides, we validate the difference of the vocabulary built from monolingual data and parallel data.</abstract>
      <url hash="b0bb728f">2020.wmt-1.6</url>
      <video href="https://slideslive.com/38939603"/>
      <bibkey>bei-etal-2020-gtcom</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/flores">FLoRes</pwcdataset>
    </paper>
    <paper id="7">
      <title><fixed-case>D</fixed-case>i<fixed-case>D</fixed-case>i’s Machine Translation System for <fixed-case>WMT</fixed-case>2020</title>
      <author><first>Tanfang</first><last>Chen</last></author>
      <author><first>Weiwei</first><last>Wang</last></author>
      <author><first>Wenyang</first><last>Wei</last></author>
      <author><first>Xing</first><last>Shi</last></author>
      <author><first>Xiangang</first><last>Li</last></author>
      <author><first>Jieping</first><last>Ye</last></author>
      <author><first>Kevin</first><last>Knight</last></author>
      <pages>105–112</pages>
      <abstract>This paper describes the DiDi AI Labs’ submission to the WMT2020 news translation shared task. We participate in the translation direction of Chinese-&gt;English. In this direction, we use the Transformer as our baseline model and integrate several techniques for model enhancement, including data filtering, data selection, back-translation, fine-tuning, model ensembling, and re-ranking. As a result, our submission achieves a BLEU score of 36.6 in Chinese-&gt;English.</abstract>
      <url hash="994815ba">2020.wmt-1.7</url>
      <video href="https://slideslive.com/38939543"/>
      <bibkey>chen-etal-2020-didis</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>F</fixed-case>acebook <fixed-case>AI</fixed-case>’s <fixed-case>WMT</fixed-case>20 News Translation Task Submission</title>
      <author><first>Peng-Jen</first><last>Chen</last></author>
      <author><first>Ann</first><last>Lee</last></author>
      <author><first>Changhan</first><last>Wang</last></author>
      <author><first>Naman</first><last>Goyal</last></author>
      <author><first>Angela</first><last>Fan</last></author>
      <author><first>Mary</first><last>Williamson</last></author>
      <author><first>Jiatao</first><last>Gu</last></author>
      <pages>113–125</pages>
      <abstract>This paper describes Facebook AI’s submission to WMT20 shared news translation task. We focus on the low resource setting and participate in two language pairs, Tamil &lt;-&gt; English and Inuktitut &lt;-&gt; English, where there are limited out-of-domain bitext and monolingual data. We approach the low resource problem using two main strategies, leveraging all available data and adapting the system to the target news domain. We explore techniques that leverage bitext and monolingual data from all languages, such as self-supervised model pretraining, multilingual models, data augmentation, and reranking. To better adapt the translation system to the test domain, we explore dataset tagging and fine-tuning on in-domain data. We observe that different techniques provide varied improvements based on the available data of the language pair. Based on the finding, we integrate these techniques into one training pipeline. For En-&gt;Ta, we explore an unconstrained setup with additional Tamil bitext and monolingual data and show that further improvement can be obtained. On the test set, our best submitted systems achieve 21.5 and 13.7 BLEU for Ta-&gt;En and En-&gt;Ta respectively, and 27.9 and 13.0 for Iu-&gt;En and En-&gt;Iu respectively.</abstract>
      <url hash="cdfa810a">2020.wmt-1.8</url>
      <video href="https://slideslive.com/38939624"/>
      <bibkey>chen-etal-2020-facebook</bibkey>
    </paper>
    <paper id="9">
      <title>Linguistically Motivated Subwords for <fixed-case>E</fixed-case>nglish-<fixed-case>T</fixed-case>amil Translation: <fixed-case>U</fixed-case>niversity of <fixed-case>G</fixed-case>roningen’s Submission to <fixed-case>WMT</fixed-case>-2020</title>
      <author><first>Prajit</first><last>Dhar</last></author>
      <author><first>Arianna</first><last>Bisazza</last></author>
      <author><first>Gertjan</first><last>van Noord</last></author>
      <pages>126–133</pages>
      <abstract>This paper describes our submission for the English-Tamil news translation task of WMT-2020. The various techniques and Neural Machine Translation (NMT) models used by our team are presented and discussed, including back-translation, fine-tuning and word dropout. Additionally, our experiments show that using a linguistically motivated subword segmentation technique (Ataman et al., 2017) does not consistently outperform the more widely used, non-linguistically motivated SentencePiece algorithm (Kudo and Richardson, 2018), despite the agglutinative nature of Tamil morphology.</abstract>
      <url hash="ea075060">2020.wmt-1.9</url>
      <video href="https://slideslive.com/38939635"/>
      <bibkey>dhar-etal-2020-linguistically</bibkey>
    </paper>
    <paper id="10">
      <title>The <fixed-case>TALP</fixed-case>-<fixed-case>UPC</fixed-case> System Description for <fixed-case>WMT</fixed-case>20 News Translation Task: Multilingual Adaptation for Low Resource <fixed-case>MT</fixed-case></title>
      <author><first>Carlos</first><last>Escolano</last></author>
      <author><first>Marta R.</first><last>Costa-jussà</last></author>
      <author><first>José A. R.</first><last>Fonollosa</last></author>
      <pages>134–138</pages>
      <abstract>In this article, we describe the TALP-UPC participation in the WMT20 news translation shared task for Tamil-English. Given the low amount of parallel training data, we resort to adapt the task to a multilingual system to benefit from the positive transfer from high resource languages. We use iterative backtranslation to fine-tune the system and benefit from the monolingual data available. In order to measure the effectivity of such methods, we compare our results to a bilingual baseline system.</abstract>
      <url hash="fc8623ea">2020.wmt-1.10</url>
      <video href="https://slideslive.com/38939594"/>
      <bibkey>escolano-etal-2020-talp</bibkey>
    </paper>
    <paper id="11">
      <title>An Iterative Knowledge Transfer <fixed-case>NMT</fixed-case> System for <fixed-case>WMT</fixed-case>20 News Translation Task</title>
      <author><first>Jiwan</first><last>Kim</last></author>
      <author><first>Soyoon</first><last>Park</last></author>
      <author><first>Sangha</first><last>Kim</last></author>
      <author><first>Yoonjung</first><last>Choi</last></author>
      <pages>139–144</pages>
      <abstract>This paper describes our submission to the WMT20 news translation shared task in English to Japanese direction. Our main approach is based on transferring knowledge of domain and linguistic characteristics by pre-training the encoder-decoder model with large amount of in-domain monolingual data through unsupervised and supervised prediction task. We then fine-tune the model with parallel data and in-domain synthetic data, generated with iterative back-translation. For additional gain, we generate final results with an ensemble model and re-rank them with averaged models and language models. Through these methods, we achieve +5.42 BLEU score compare to the baseline model.</abstract>
      <url hash="74b22739">2020.wmt-1.11</url>
      <video href="https://slideslive.com/38939599"/>
      <bibkey>kim-etal-2020-iterative</bibkey>
    </paper>
    <paper id="12">
      <title>Tohoku-<fixed-case>AIP</fixed-case>-<fixed-case>NTT</fixed-case> at <fixed-case>WMT</fixed-case> 2020 News Translation Task</title>
      <author><first>Shun</first><last>Kiyono</last></author>
      <author><first>Takumi</first><last>Ito</last></author>
      <author><first>Ryuto</first><last>Konno</last></author>
      <author><first>Makoto</first><last>Morishita</last></author>
      <author><first>Jun</first><last>Suzuki</last></author>
      <pages>145–155</pages>
      <abstract>In this paper, we describe the submission of Tohoku-AIP-NTT to the WMT’20 news translation task. We participated in this task in two language pairs and four language directions: English &lt;–&gt; German and English &lt;–&gt; Japanese. Our system consists of techniques such as back-translation and fine-tuning, which are already widely adopted in translation tasks. We attempted to develop new methods for both synthetic data filtering and reranking. However, the methods turned out to be ineffective, and they provided us with no significant improvement over the baseline. We analyze these negative results to provide insights for future studies.</abstract>
      <url hash="e5b6c4c9">2020.wmt-1.12</url>
      <video href="https://slideslive.com/38939545"/>
      <bibkey>kiyono-etal-2020-tohoku</bibkey>
    </paper>
    <paper id="13">
      <title><fixed-case>NRC</fixed-case> Systems for the 2020 <fixed-case>I</fixed-case>nuktitut-<fixed-case>E</fixed-case>nglish News Translation Task</title>
      <author><first>Rebecca</first><last>Knowles</last></author>
      <author><first>Darlene</first><last>Stewart</last></author>
      <author><first>Samuel</first><last>Larkin</last></author>
      <author><first>Patrick</first><last>Littell</last></author>
      <pages>156–170</pages>
      <abstract>We describe the National Research Council of Canada (NRC) submissions for the 2020 Inuktitut-English shared task on news translation at the Fifth Conference on Machine Translation (WMT20). Our submissions consist of ensembled domain-specific finetuned transformer models, trained using the Nunavut Hansard and news data and, in the case of Inuktitut-English, backtranslated news and parliamentary data. In this work we explore challenges related to the relatively small amount of parallel data, morphological complexity, and domain shifts.</abstract>
      <url hash="a834ee2f">2020.wmt-1.13</url>
      <video href="https://slideslive.com/38939639"/>
      <bibkey>knowles-etal-2020-nrc</bibkey>
    </paper>
    <paper id="14">
      <title><fixed-case>CUNI</fixed-case> Submission for the <fixed-case>I</fixed-case>nuktitut Language in <fixed-case>WMT</fixed-case> News 2020</title>
      <author><first>Tom</first><last>Kocmi</last></author>
      <pages>171–174</pages>
      <abstract>This paper describes CUNI submission to the WMT 2020 News Translation Shared Task for the low-resource scenario Inuktitut–English in both translation directions. Our system combines transfer learning from a Czech–English high-resource language pair and backtranslation. We notice surprising behaviour when using synthetic data, which can be possibly attributed to a narrow domain of training and test data. We are using the Transformer model in a constrained submission.</abstract>
      <url hash="bb9e6059">2020.wmt-1.14</url>
      <video href="https://slideslive.com/38939666"/>
      <bibkey>kocmi-2020-cuni</bibkey>
    </paper>
    <paper id="15">
      <title>Tilde at <fixed-case>WMT</fixed-case> 2020: News Task Systems</title>
      <author><first>Rihards</first><last>Krišlauks</last></author>
      <author><first>Mārcis</first><last>Pinnis</last></author>
      <pages>175–180</pages>
      <abstract>This paper describes Tilde’s submission to the WMT2020 shared task on news translation for both directions of the English-Polish language pair in both the constrained and the unconstrained tracks. We follow our submissions form the previous years and build our baseline systems to be morphologically motivated sub-word unit-based Transformer base models that we train using the Marian machine translation toolkit. Additionally, we experiment with different parallel and monolingual data selection schemes, as well as sampled back-translation. Our final models are ensembles of Transformer base and Transformer big models which feature right-to-left re-ranking.</abstract>
      <url hash="f3fb101e">2020.wmt-1.15</url>
      <video href="https://slideslive.com/38939633"/>
      <bibkey>krislauks-pinnis-2020-tilde</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/paracrawl">ParaCrawl</pwcdataset>
    </paper>
    <paper id="16">
      <title><fixed-case>S</fixed-case>amsung <fixed-case>R</fixed-case>&amp;<fixed-case>D</fixed-case> Institute <fixed-case>P</fixed-case>oland submission to <fixed-case>WMT</fixed-case>20 News Translation Task</title>
      <author><first>Mateusz</first><last>Krubiński</last></author>
      <author><first>Marcin</first><last>Chochowski</last></author>
      <author><first>Bartłomiej</first><last>Boczek</last></author>
      <author><first>Mikołaj</first><last>Koszowski</last></author>
      <author><first>Adam</first><last>Dobrowolski</last></author>
      <author><first>Marcin</first><last>Szymański</last></author>
      <author><first>Paweł</first><last>Przybysz</last></author>
      <pages>181–190</pages>
      <abstract>This paper describes the submission to the WMT20 shared news translation task by Samsung R&amp;D Institute Poland. We submitted systems for six language directions: English to Czech, Czech to English, English to Polish, Polish to English, English to Inuktitut and Inuktitut to English. For each, we trained a single-direction model. However, directions including English, Polish and Czech were derived from a common multilingual base, which was later fine-tuned on each particular direction. For all the translation directions, we used a similar training regime, with iterative training corpora improvement through back-translation and model ensembling. For the En → Cs direction, we additionally leveraged document-level information by re-ranking the beam output with a separate model.</abstract>
      <url hash="5ef353b3">2020.wmt-1.16</url>
      <video href="https://slideslive.com/38939566"/>
      <bibkey>krubinski-etal-2020-samsung</bibkey>
    </paper>
    <paper id="17">
      <title>Speed-optimized, Compact Student Models that Distill Knowledge from a Larger Teacher Model: the <fixed-case>UEDIN</fixed-case>-<fixed-case>CUNI</fixed-case> Submission to the <fixed-case>WMT</fixed-case> 2020 News Translation Task</title>
      <author><first>Ulrich</first><last>Germann</last></author>
      <author><first>Roman</first><last>Grundkiewicz</last></author>
      <author><first>Martin</first><last>Popel</last></author>
      <author><first>Radina</first><last>Dobreva</last></author>
      <author><first>Nikolay</first><last>Bogoychev</last></author>
      <author><first>Kenneth</first><last>Heafield</last></author>
      <pages>191–196</pages>
      <abstract>We describe the joint submission of the University of Edinburgh and Charles University, Prague, to the Czech/English track in the WMT 2020 Shared Task on News Translation. Our fast and compact student models distill knowledge from a larger, slower teacher. They are designed to offer a good trade-off between translation quality and inference efficiency. On the WMT 2020 Czech ↔ English test sets, they achieve translation speeds of over 700 whitespace-delimited source words per second on a single CPU thread, thus making neural translation feasible on consumer hardware without a GPU.</abstract>
      <url hash="511f1989">2020.wmt-1.17</url>
      <video href="https://slideslive.com/38939661"/>
      <bibkey>germann-etal-2020-speed</bibkey>
    </paper>
    <paper id="18">
      <title>The <fixed-case>U</fixed-case>niversity of <fixed-case>E</fixed-case>dinburgh’s submission to the <fixed-case>G</fixed-case>erman-to-<fixed-case>E</fixed-case>nglish and <fixed-case>E</fixed-case>nglish-to-<fixed-case>G</fixed-case>erman Tracks in the <fixed-case>WMT</fixed-case> 2020 News Translation and Zero-shot Translation Robustness Tasks</title>
      <author><first>Ulrich</first><last>Germann</last></author>
      <pages>197–201</pages>
      <abstract>This paper describes the University of Edinburgh’s submission of German &lt;-&gt; English systems to the WMT2020 Shared Tasks on News Translation and Zero-shot Robustness.</abstract>
      <url hash="9fa30b19">2020.wmt-1.18</url>
      <video href="https://slideslive.com/38939663"/>
      <bibkey>germann-2020-university</bibkey>
    </paper>
    <paper id="19">
      <title>Contact Relatedness can help improve multilingual <fixed-case>NMT</fixed-case>: <fixed-case>M</fixed-case>icrosoft <fixed-case>STCI</fixed-case>-<fixed-case>MT</fixed-case> @ <fixed-case>WMT</fixed-case>20</title>
      <author><first>Vikrant</first><last>Goyal</last></author>
      <author><first>Anoop</first><last>Kunchukuttan</last></author>
      <author><first>Rahul</first><last>Kejriwal</last></author>
      <author><first>Siddharth</first><last>Jain</last></author>
      <author><first>Amit</first><last>Bhagwat</last></author>
      <pages>202–206</pages>
      <abstract>We describe our submission for the English→Tamil and Tamil→English news translation shared task. In this submission, we focus on exploring if a low-resource language (Tamil) can benefit from a high-resource language (Hindi) with which it shares contact relatedness. We show utilizing contact relatedness via multilingual NMT can significantly improve translation quality for English-Tamil translation.</abstract>
      <url hash="a290ccf3">2020.wmt-1.19</url>
      <video href="https://slideslive.com/38939654"/>
      <bibkey>goyal-etal-2020-contact</bibkey>
    </paper>
    <paper id="20">
      <title>The <fixed-case>AFRL</fixed-case> <fixed-case>WMT</fixed-case>20 News Translation Systems</title>
      <author><first>Jeremy</first><last>Gwinnup</last></author>
      <author><first>Tim</first><last>Anderson</last></author>
      <pages>207–212</pages>
      <abstract>This report summarizes the Air Force Research Laboratory (AFRL) machine translation (MT) systems submitted to the news-translation task as part of the 2020 Conference on Machine Translation (WMT20) evaluation campaign. This year we largely repurpose strategies from previous years’ efforts with larger datasets and also train models with precomputed word alignments under various settings in an effort to improve translation quality.</abstract>
      <url hash="f1fc779f">2020.wmt-1.20</url>
      <video href="https://slideslive.com/38939648"/>
      <bibkey>gwinnup-anderson-2020-afrl</bibkey>
    </paper>
    <paper id="21">
      <title>The Ubiqus <fixed-case>E</fixed-case>nglish-<fixed-case>I</fixed-case>nuktitut System for <fixed-case>WMT</fixed-case>20</title>
      <author><first>François</first><last>Hernandez</last></author>
      <author><first>Vincent</first><last>Nguyen</last></author>
      <pages>213–217</pages>
      <abstract>This paper describes Ubiqus’ submission to the WMT20 English-Inuktitut shared news translation task. Our main system, and only submission, is based on a multilingual approach, jointly training a Transformer model on several agglutinative languages. The English-Inuktitut translation task is challenging at every step, from data selection, preparation and tokenization to quality evaluation down the line. Difficulties emerge both because of the peculiarities of the Inuktitut language as well as the low-resource context.</abstract>
      <url hash="f293cd16">2020.wmt-1.21</url>
      <bibkey>hernandez-nguyen-2020-ubiqus</bibkey>
    </paper>
    <paper id="22">
      <title><fixed-case>SJTU</fixed-case>-<fixed-case>NICT</fixed-case>’s Supervised and Unsupervised Neural Machine Translation Systems for the <fixed-case>WMT</fixed-case>20 News Translation Task</title>
      <author><first>Zuchao</first><last>Li</last></author>
      <author><first>Hai</first><last>Zhao</last></author>
      <author><first>Rui</first><last>Wang</last></author>
      <author><first>Kehai</first><last>Chen</last></author>
      <author><first>Masao</first><last>Utiyama</last></author>
      <author><first>Eiichiro</first><last>Sumita</last></author>
      <pages>218–229</pages>
      <abstract>In this paper, we introduced our joint team SJTU-NICT ‘s participation in the WMT 2020 machine translation shared task. In this shared task, we participated in four translation directions of three language pairs: English-Chinese, English-Polish on supervised machine translation track, German-Upper Sorbian on low-resource and unsupervised machine translation tracks. Based on different conditions of language pairs, we have experimented with diverse neural machine translation (NMT) techniques: document-enhanced NMT, XLM pre-trained language model enhanced NMT, bidirectional translation as a pre-training, reference language based UNMT, data-dependent gaussian prior objective, and BT-BLEU collaborative filtering self-training. We also used the TF-IDF algorithm to filter the training set to obtain a domain more similar set with the test set for finetuning. In our submissions, the primary systems won the first place on English to Chinese, Polish to English, and German to Upper Sorbian translation directions.</abstract>
      <url hash="65d90d87">2020.wmt-1.22</url>
      <video href="https://slideslive.com/38939657"/>
      <bibkey>li-etal-2020-sjtu</bibkey>
    </paper>
    <paper id="23">
      <title>Combination of Neural Machine Translation Systems at <fixed-case>WMT</fixed-case>20</title>
      <author><first>Benjamin</first><last>Marie</last></author>
      <author><first>Raphael</first><last>Rubino</last></author>
      <author><first>Atsushi</first><last>Fujita</last></author>
      <pages>230–238</pages>
      <abstract>This paper presents neural machine translation systems and their combination built for the WMT20 English-Polish and Japanese-&gt;English translation tasks. We show that using a Transformer Big architecture, additional training data synthesized from monolingual data, and combining many NMT systems through n-best list reranking improve translation quality. However, while we observed such improvements on the validation data, we did not observed similar improvements on the test data. Our analysis reveals that the presence of translationese texts in the validation data led us to take decisions in building NMT systems that were not optimal to obtain the best results on the test data.</abstract>
      <url hash="35423549">2020.wmt-1.23</url>
      <video href="https://slideslive.com/38939600"/>
      <bibkey>marie-etal-2020-combination</bibkey>
    </paper>
    <paper id="24">
      <title><fixed-case>W</fixed-case>e<fixed-case>C</fixed-case>hat Neural Machine Translation Systems for <fixed-case>WMT</fixed-case>20</title>
      <author><first>Fandong</first><last>Meng</last></author>
      <author><first>Jianhao</first><last>Yan</last></author>
      <author><first>Yijin</first><last>Liu</last></author>
      <author><first>Yuan</first><last>Gao</last></author>
      <author><first>Xianfeng</first><last>Zeng</last></author>
      <author><first>Qinsong</first><last>Zeng</last></author>
      <author><first>Peng</first><last>Li</last></author>
      <author><first>Ming</first><last>Chen</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <author><first>Sifan</first><last>Liu</last></author>
      <author><first>Hao</first><last>Zhou</last></author>
      <pages>239–247</pages>
      <abstract>We participate in the WMT 2020 shared newstranslation task on Chinese→English. Our system is based on the Transformer (Vaswaniet al., 2017a) with effective variants and the DTMT (Meng and Zhang, 2019) architecture. In our experiments, we employ data selection, several synthetic data generation approaches (i.e., back-translation, knowledge distillation, and iterative in-domain knowledge transfer), advanced finetuning approaches and self-bleu based model ensemble. Our constrained Chinese→English system achieves 36.9 case-sensitive BLEU score, which is thehighest among all submissions.</abstract>
      <url hash="b8c724d3">2020.wmt-1.24</url>
      <video href="https://slideslive.com/38939596"/>
      <bibkey>meng-etal-2020-wechat</bibkey>
    </paper>
    <paper id="25">
      <title><fixed-case>PROMT</fixed-case> Systems for <fixed-case>WMT</fixed-case> 2020 Shared News Translation Task</title>
      <author><first>Alexander</first><last>Molchanov</last></author>
      <pages>248–253</pages>
      <abstract>This paper describes the PROMT submissions for the WMT 2020 Shared News Translation Task. This year we participated in four language pairs and six directions: English-Russian, Russian-English, English-German, German-English, Polish-English and Czech-English. All our submissions are MarianNMT-based neural systems. We use more data compared to last year and update our back-translations with better models from the previous year. We show competitive results in terms of BLEU in most directions.</abstract>
      <url hash="c90da55f">2020.wmt-1.25</url>
      <bibkey>molchanov-2020-promt</bibkey>
    </paper>
    <paper id="26">
      <title>e<fixed-case>T</fixed-case>ranslation’s Submissions to the <fixed-case>WMT</fixed-case> 2020 News Translation Task</title>
      <author><first>Csaba</first><last>Oravecz</last></author>
      <author><first>Katina</first><last>Bontcheva</last></author>
      <author><first>László</first><last>Tihanyi</last></author>
      <author><first>David</first><last>Kolovratnik</last></author>
      <author><first>Bhavani</first><last>Bhaskar</last></author>
      <author><first>Adrien</first><last>Lardilleux</last></author>
      <author><first>Szymon</first><last>Klocek</last></author>
      <author><first>Andreas</first><last>Eisele</last></author>
      <pages>254–261</pages>
      <abstract>The paper describes the submissions of the eTranslation team to the WMT 2020 news translation shared task. Leveraging the experience from the team’s participation last year we developed systems for 5 language pairs with various strategies. Compared to last year, for some language pairs we dedicated a lot more resources to training, and tried to follow standard best practices to build competitive systems which can achieve good results in the rankings. By using deep and complex architectures we sacrificed direct re-usability of our systems in production environments but evaluation showed that this approach could result in better models that significantly outperform baseline architectures. We submitted two systems to the zero shot robustness task. These submissions are described briefly in this paper as well.</abstract>
      <url hash="9939e8bd">2020.wmt-1.26</url>
      <video href="https://slideslive.com/38939574"/>
      <bibkey>oravecz-etal-2020-etranslations</bibkey>
    </paper>
    <paper id="27">
      <title>The <fixed-case>ADAPT</fixed-case> System Description for the <fixed-case>WMT</fixed-case>20 News Translation Task</title>
      <author><first>Venkatesh</first><last>Parthasarathy</last></author>
      <author><first>Akshai</first><last>Ramesh</last></author>
      <author><first>Rejwanul</first><last>Haque</last></author>
      <author><first>Andy</first><last>Way</last></author>
      <pages>262–268</pages>
      <abstract>This paper describes the ADAPT Centre’s submissions to the WMT20 News translation shared task for English-to-Tamil and Tamil-to-English. We present our machine translation (MT) systems that were built using the state-of-the-art neural MT (NMT) model, Transformer. We applied various strategies in order to improve our baseline MT systems, e.g. onolin- gual sentence selection for creating synthetic training data, mining monolingual sentences for adapting our MT systems to the task, hyperparameters search for Transformer in lowresource scenarios. Our experiments show that adding the aforementioned techniques to the baseline yields an excellent performance in the English-to-Tamil and Tamil-to-English translation tasks.</abstract>
      <url hash="789681c7">2020.wmt-1.27</url>
      <video href="https://slideslive.com/38939616"/>
      <bibkey>parthasarathy-etal-2020-adapt</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wikimatrix">WikiMatrix</pwcdataset>
    </paper>
    <paper id="28">
      <title><fixed-case>CUNI</fixed-case> <fixed-case>E</fixed-case>nglish-<fixed-case>C</fixed-case>zech and <fixed-case>E</fixed-case>nglish-<fixed-case>P</fixed-case>olish Systems in <fixed-case>WMT</fixed-case>20: Robust Document-Level Training</title>
      <author><first>Martin</first><last>Popel</last></author>
      <pages>269–273</pages>
      <abstract>We describe our two NMT systems submitted to the WMT 2020 shared task in English&lt;-&gt;Czech and English&lt;-&gt;Polish news translation. One system is sentence level, translating each sentence independently. The second system is document level, translating multiple sentences, trained on multi-sentence sequences up to 3000 characters long.</abstract>
      <url hash="fdb9dba0">2020.wmt-1.28</url>
      <video href="https://slideslive.com/38939668"/>
      <bibkey>popel-2020-cuni</bibkey>
    </paper>
    <paper id="29">
      <title>Machine Translation for <fixed-case>E</fixed-case>nglish–<fixed-case>I</fixed-case>nuktitut with Segmentation, Data Acquisition and Pre-Training</title>
      <author><first>Christian</first><last>Roest</last></author>
      <author><first>Lukas</first><last>Edman</last></author>
      <author><first>Gosse</first><last>Minnema</last></author>
      <author><first>Kevin</first><last>Kelly</last></author>
      <author><first>Jennifer</first><last>Spenader</last></author>
      <author><first>Antonio</first><last>Toral</last></author>
      <pages>274–281</pages>
      <abstract>Translating to and from low-resource polysynthetic languages present numerous challenges for NMT. We present the results of our systems for the English–Inuktitut language pair for the WMT 2020 translation tasks. We investigated the importance of correct morphological segmentation, whether or not adding data from a related language (Greenlandic) helps, and whether using contextual word embeddings improves translation. While each method showed some promise, the results are mixed.</abstract>
      <url hash="9c64fd38">2020.wmt-1.29</url>
      <video href="https://slideslive.com/38939665"/>
      <bibkey>roest-etal-2020-machine</bibkey>
    </paper>
    <paper id="30">
      <title><fixed-case>OPPO</fixed-case>’s Machine Translation Systems for <fixed-case>WMT</fixed-case>20</title>
      <author><first>Tingxun</first><last>Shi</last></author>
      <author><first>Shiyu</first><last>Zhao</last></author>
      <author><first>Xiaopu</first><last>Li</last></author>
      <author><first>Xiaoxue</first><last>Wang</last></author>
      <author><first>Qian</first><last>Zhang</last></author>
      <author><first>Di</first><last>Ai</last></author>
      <author><first>Dawei</first><last>Dang</last></author>
      <author><first>Xue</first><last>Zhengshan</last></author>
      <author><first>Jie</first><last>Hao</last></author>
      <pages>282–292</pages>
      <abstract>In this paper we demonstrate our (OPPO’s) machine translation systems for the WMT20 Shared Task on News Translation for all the 22 language pairs. We will give an overview of the common aspects across all the systems firstly, including two parts: the data preprocessing part will show how the data are preprocessed and filtered, and the system part will show our models architecture and the techniques we followed. Detailed information, such as training hyperparameters and the results generated by each technique will be depicted in the corresponding subsections. Our final submissions ranked top in 6 directions (English <tex-math>\leftrightarrow</tex-math> Czech, English <tex-math>\leftrightarrow</tex-math> Russian, French <tex-math>\rightarrow</tex-math> German and Tamil <tex-math>\rightarrow</tex-math> English), third in 2 directions (English <tex-math>\rightarrow</tex-math> German, English <tex-math>\rightarrow</tex-math> Japanese), and fourth in 2 directions (English <tex-math>\rightarrow</tex-math> Pashto and and English <tex-math>\rightarrow</tex-math> Tamil).</abstract>
      <url hash="b7ee25aa">2020.wmt-1.30</url>
      <video href="https://slideslive.com/38939558"/>
      <bibkey>shi-etal-2020-oppos</bibkey>
    </paper>
    <paper id="31">
      <title><fixed-case>HW</fixed-case>-<fixed-case>TSC</fixed-case>’s Participation in the <fixed-case>WMT</fixed-case> 2020 News Translation Shared Task</title>
      <author><first>Daimeng</first><last>Wei</last></author>
      <author><first>Hengchao</first><last>Shang</last></author>
      <author><first>Zhanglin</first><last>Wu</last></author>
      <author><first>Zhengzhe</first><last>Yu</last></author>
      <author><first>Liangyou</first><last>Li</last></author>
      <author><first>Jiaxin</first><last>Guo</last></author>
      <author><first>Minghan</first><last>Wang</last></author>
      <author><first>Hao</first><last>Yang</last></author>
      <author><first>Lizhi</first><last>Lei</last></author>
      <author><first>Ying</first><last>Qin</last></author>
      <author><first>Shiliang</first><last>Sun</last></author>
      <pages>293–299</pages>
      <abstract>This paper presents our work in the WMT 2020 News Translation Shared Task. We participate in 3 language pairs including Zh/En, Km/En, and Ps/En and in both directions under the constrained condition. We use the standard Transformer-Big model as the baseline and obtain the best performance via two variants with larger parameter sizes. We perform detailed pre-processing and filtering on the provided large-scale bilingual and monolingual dataset. Several commonly used strategies are used to train our models such as Back Translation, Ensemble Knowledge Distillation, etc. We also conduct experiment with similar language augmentation, which lead to positive results, although not used in our submission. Our submission obtains remarkable results in the final evaluation.</abstract>
      <url hash="db78d297">2020.wmt-1.31</url>
      <video href="https://slideslive.com/38939573"/>
      <bibkey>wei-etal-2020-hw</bibkey>
    </paper>
    <paper id="32">
      <title><fixed-case>IIE</fixed-case>’s Neural Machine Translation Systems for <fixed-case>WMT</fixed-case>20</title>
      <author><first>Xiangpeng</first><last>Wei</last></author>
      <author><first>Ping</first><last>Guo</last></author>
      <author><first>Yunpeng</first><last>Li</last></author>
      <author><first>Xingsheng</first><last>Zhang</last></author>
      <author><first>Luxi</first><last>Xing</last></author>
      <author><first>Yue</first><last>Hu</last></author>
      <pages>300–304</pages>
      <abstract>In this paper we introduce the systems IIE submitted for the WMT20 shared task on German-French news translation. Our systems are based on the Transformer architecture with some effective improvements. Multiscale collaborative deep architecture, data selection, back translation, knowledge distillation, domain adaptation, model ensemble and re-ranking are employed and proven effective in our experiments. Our German-to-French system achieved 35.0 BLEU and ranked the second among all anonymous submissions, and our French-to-German system achieved 36.6 BLEU and ranked the fourth in all anonymous submissions.</abstract>
      <url hash="4ce272b3">2020.wmt-1.32</url>
      <video href="https://slideslive.com/38939579"/>
      <bibkey>wei-etal-2020-iies</bibkey>
    </paper>
    <paper id="33">
      <title>The Volctrans Machine Translation System for <fixed-case>WMT</fixed-case>20</title>
      <author><first>Liwei</first><last>Wu</last></author>
      <author><first>Xiao</first><last>Pan</last></author>
      <author><first>Zehui</first><last>Lin</last></author>
      <author><first>Yaoming</first><last>Zhu</last></author>
      <author><first>Mingxuan</first><last>Wang</last></author>
      <author><first>Lei</first><last>Li</last></author>
      <pages>305–312</pages>
      <abstract>This paper describes our submission systems for VolcTrans for WMT20 shared news translation task. We participated in 8 translation directions. Our basic systems are based on Transformer (CITATION), into which we also employed new architectures (bigger or deeper Transformers, dynamic convolution). The final systems include text pre-process, subword(a.k.a. BPE(CITATION)), baseline model training, iterative back-translation, model ensemble, knowledge distillation and multilingual pre-training.</abstract>
      <url hash="58264a1d">2020.wmt-1.33</url>
      <video href="https://slideslive.com/38939581"/>
      <bibkey>wu-etal-2020-volctrans</bibkey>
    </paper>
    <paper id="34">
      <title>Tencent Neural Machine Translation Systems for the <fixed-case>WMT</fixed-case>20 News Translation Task</title>
      <author><first>Shuangzhi</first><last>Wu</last></author>
      <author><first>Xing</first><last>Wang</last></author>
      <author><first>Longyue</first><last>Wang</last></author>
      <author><first>Fangxu</first><last>Liu</last></author>
      <author><first>Jun</first><last>Xie</last></author>
      <author><first>Zhaopeng</first><last>Tu</last></author>
      <author><first>Shuming</first><last>Shi</last></author>
      <author><first>Mu</first><last>Li</last></author>
      <pages>313–319</pages>
      <abstract>This paper describes Tencent Neural Machine Translation systems for the WMT 2020 news translation tasks. We participate in the shared news translation task on English <tex-math>\leftrightarrow</tex-math> Chinese and English <tex-math>\rightarrow</tex-math> German language pairs. Our systems are built on deep Transformer and several data augmentation methods. We propose a boosted in-domain finetuning method to improve single models. Ensemble is used to combine single models and we propose an iterative transductive ensemble method which can further improve the translation performance based on the ensemble results. We achieve a BLEU score of 36.8 and the highest chrF score of 0.648 on Chinese <tex-math>\rightarrow</tex-math> English task.</abstract>
      <url hash="7dc4a0d9">2020.wmt-1.34</url>
      <bibkey>wu-etal-2020-tencent</bibkey>
    </paper>
    <paper id="35">
      <title><fixed-case>R</fixed-case>ussian-<fixed-case>E</fixed-case>nglish Bidirectional Machine Translation System</title>
      <author><first>Ariel</first><last>Xv</last></author>
      <pages>320–325</pages>
      <abstract>This review depicts our submission to the WMT20 shared news translation task. WMT is the conference to assess the level of machine translation capabilities of organizations in the word. We participated in one language pair and two language directions, from Russian to English and from English to Russian. We used official training data, 102 million parallel corpora and 10 million monolingual corpora. Our baseline systems are Transformer models trained with the Sockeye sequence modeling toolkit, supplemented by bi-text data filtering schemes, back-translations, reordering and other related processing methods. The BLEU value of our translation result from Russian to English is 35.7, ranking 5th, while from English to Russian is 39.8, ranking 2th.</abstract>
      <url hash="415e8482">2020.wmt-1.35</url>
      <video href="https://slideslive.com/38939619"/>
      <bibkey>xv-2020-russian</bibkey>
    </paper>
    <paper id="36">
      <title>The <fixed-case>D</fixed-case>eep<fixed-case>M</fixed-case>ind <fixed-case>C</fixed-case>hinese–<fixed-case>E</fixed-case>nglish Document Translation System at <fixed-case>WMT</fixed-case>2020</title>
      <author><first>Lei</first><last>Yu</last></author>
      <author><first>Laurent</first><last>Sartran</last></author>
      <author><first>Po-Sen</first><last>Huang</last></author>
      <author><first>Wojciech</first><last>Stokowiec</last></author>
      <author><first>Domenic</first><last>Donato</last></author>
      <author><first>Srivatsan</first><last>Srinivasan</last></author>
      <author><first>Alek</first><last>Andreev</last></author>
      <author><first>Wang</first><last>Ling</last></author>
      <author><first>Sona</first><last>Mokra</last></author>
      <author><first>Agustin</first><last>Dal Lago</last></author>
      <author><first>Yotam</first><last>Doron</last></author>
      <author><first>Susannah</first><last>Young</last></author>
      <author><first>Phil</first><last>Blunsom</last></author>
      <author><first>Chris</first><last>Dyer</last></author>
      <pages>326–337</pages>
      <abstract>This paper describes the DeepMind submission to the Chinese<tex-math>\rightarrow</tex-math>English constrained data track of the WMT2020 Shared Task on News Translation. The submission employs a noisy channel factorization as the backbone of a document translation system. This approach allows the flexible combination of a number of independent component models which are further augmented with back-translation, distillation, fine-tuning with in-domain data, Monte-Carlo Tree Search decoding, and improved uncertainty estimation. In order to address persistent issues with the premature truncation of long sequences we included specialized length models and sentence segmentation techniques. Our final system provides a 9.9 BLEU points improvement over a baseline Transformer on our test set (newstest 2019).</abstract>
      <url hash="e3567ca1">2020.wmt-1.36</url>
      <video href="https://slideslive.com/38939586"/>
      <bibkey>yu-etal-2020-deepmind</bibkey>
    </paper>
    <paper id="37">
      <title>The <fixed-case>N</fixed-case>iu<fixed-case>T</fixed-case>rans Machine Translation Systems for <fixed-case>WMT</fixed-case>20</title>
      <author><first>Yuhao</first><last>Zhang</last></author>
      <author><first>Ziyang</first><last>Wang</last></author>
      <author><first>Runzhe</first><last>Cao</last></author>
      <author><first>Binghao</first><last>Wei</last></author>
      <author><first>Weiqiao</first><last>Shan</last></author>
      <author><first>Shuhan</first><last>Zhou</last></author>
      <author><first>Abudurexiti</first><last>Reheman</last></author>
      <author><first>Tao</first><last>Zhou</last></author>
      <author><first>Xin</first><last>Zeng</last></author>
      <author><first>Laohu</first><last>Wang</last></author>
      <author><first>Yongyu</first><last>Mu</last></author>
      <author><first>Jingnan</first><last>Zhang</last></author>
      <author><first>Xiaoqian</first><last>Liu</last></author>
      <author><first>Xuanjun</first><last>Zhou</last></author>
      <author><first>Yinqiao</first><last>Li</last></author>
      <author><first>Bei</first><last>Li</last></author>
      <author><first>Tong</first><last>Xiao</last></author>
      <author><first>Jingbo</first><last>Zhu</last></author>
      <pages>338–345</pages>
      <abstract>This paper describes NiuTrans neural machine translation systems of the WMT20 news translation tasks. We participated in Japanese&lt;-&gt;English, English-&gt;Chinese, Inuktitut-&gt;English and Tamil-&gt;English total five tasks and rank first in Japanese&lt;-&gt;English both sides. We mainly utilized iterative back-translation, different depth and widen model architectures, iterative knowledge distillation and iterative fine-tuning. And we find that adequately widened and deepened the model simultaneously, the performance will significantly improve. Also, iterative fine-tuning strategy we implemented is effective during adapting domain. For Inuktitut-&gt;English and Tamil-&gt;English tasks, we built multilingual models separately and employed pretraining word embedding to obtain better performance.</abstract>
      <url hash="d2a127ab">2020.wmt-1.37</url>
      <video href="https://slideslive.com/38939572"/>
      <bibkey>zhang-etal-2020-niutrans</bibkey>
    </paper>
    <paper id="38">
      <title>Fine-grained linguistic evaluation for state-of-the-art Machine Translation</title>
      <author><first>Eleftherios</first><last>Avramidis</last></author>
      <author><first>Vivien</first><last>Macketanz</last></author>
      <author><first>Ursula</first><last>Strohriegel</last></author>
      <author><first>Aljoscha</first><last>Burchardt</last></author>
      <author><first>Sebastian</first><last>Möller</last></author>
      <pages>346–356</pages>
      <abstract>This paper describes a test suite submission providing detailed statistics of linguistic performance for the state-of-the-art German-English systems of the Fifth Conference of Machine Translation (WMT20). The analysis covers 107 phenomena organized in 14 categories based on about 5,500 test items, including a manual annotation effort of 45 person hours. Two systems (Tohoku and Huoshan) appear to have significantly better test suite accuracy than the others, although the best system of WMT20 is not significantly better than the one from WMT19 in a macro-average. Additionally, we identify some linguistic phenomena where all systems suffer (such as idioms, resultative predicates and pluperfect), but we are also able to identify particular weaknesses for individual systems (such as quotation marks, lexical ambiguity and sluicing). Most of the systems of WMT19 which submitted new versions this year show improvements.</abstract>
      <url hash="afb54f9b">2020.wmt-1.38</url>
      <video href="https://slideslive.com/38939553"/>
      <bibkey>avramidis-etal-2020-fine</bibkey>
    </paper>
    <paper id="39">
      <title>Gender Coreference and Bias Evaluation at <fixed-case>WMT</fixed-case> 2020</title>
      <author><first>Tom</first><last>Kocmi</last></author>
      <author><first>Tomasz</first><last>Limisiewicz</last></author>
      <author><first>Gabriel</first><last>Stanovsky</last></author>
      <pages>357–364</pages>
      <abstract>Gender bias in machine translation can manifest when choosing gender inflections based on spurious gender correlations. For example, always translating doctors as men and nurses as women. This can be particularly harmful as models become more popular and deployed within commercial systems. Our work presents the largest evidence for the phenomenon in more than 19 systems submitted to the WMT over four diverse target languages: Czech, German, Polish, and Russian. To achieve this, we use WinoMT, a recent automatic test suite which examines gender coreference and bias when translating from English to languages with grammatical gender. We extend WinoMT to handle two new languages tested in WMT: Polish and Czech. We find that all systems consistently use spurious correlations in the data rather than meaningful contextual information.</abstract>
      <url hash="7a9c379f">2020.wmt-1.39</url>
      <video href="https://slideslive.com/38939659"/>
      <bibkey>kocmi-etal-2020-gender</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2020">WMT 2020</pwcdataset>
    </paper>
    <paper id="40">
      <title>The <fixed-case>MUCOW</fixed-case> word sense disambiguation test suite at <fixed-case>WMT</fixed-case> 2020</title>
      <author><first>Yves</first><last>Scherrer</last></author>
      <author><first>Alessandro</first><last>Raganato</last></author>
      <author><first>Jörg</first><last>Tiedemann</last></author>
      <pages>365–370</pages>
      <abstract>This paper reports on our participation with the MUCOW test suite at the WMT 2020 news translation task. We introduced MUCOW at WMT 2019 to measure the ability of MT systems to perform word sense disambiguation (WSD), i.e., to translate an ambiguous word with its correct sense. MUCOW is created automatically using existing resources, and the evaluation process is also entirely automated. We evaluate all participating systems of the language pairs English -&gt; Czech, English -&gt; German, and English -&gt; Russian and compare the results with those obtained at WMT 2019. While current NMT systems are fairly good at handling ambiguous source words, we could not identify any substantial progress - at least to the extent that it is measurable by the MUCOW method - in that area over the last year.</abstract>
      <url hash="4ca2c794">2020.wmt-1.40</url>
      <video href="https://slideslive.com/38939567"/>
      <bibkey>scherrer-etal-2020-mucow</bibkey>
      <pwccode url="https://github.com/Helsinki-NLP/MuCoW" additional="false">Helsinki-NLP/MuCoW</pwccode>
    </paper>
    <paper id="41">
      <title><fixed-case>WMT</fixed-case>20 Document-Level Markable Error Exploration</title>
      <author><first>Vilém</first><last>Zouhar</last></author>
      <author><first>Tereza</first><last>Vojtěchová</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <pages>371–380</pages>
      <abstract>Even though sentence-centric metrics are used widely in machine translation evaluation, document-level performance is at least equally important for professional usage. In this paper, we bring attention to detailed document-level evaluation focused on markables (expressions bearing most of the document meaning) and the negative impact of various markable error phenomena on the translation. For an annotation experiment of two phases, we chose Czech and English documents translated by systems submitted to WMT20 News Translation Task. These documents are from the News, Audit and Lease domains. We show that the quality and also the kind of errors varies significantly among the domains. This systematic variance is in contrast to the automatic evaluation results. We inspect which specific markables are problematic for MT systems and conclude with an analysis of the effect of markable error types on the MT performance measured by humans and automatic evaluation tools.</abstract>
      <url hash="b55c42dd">2020.wmt-1.41</url>
      <attachment type="OptionalSupplementaryMaterial" hash="c2cb5692">2020.wmt-1.41.OptionalSupplementaryMaterial.zip</attachment>
      <video href="https://slideslive.com/38939563"/>
      <bibkey>zouhar-etal-2020-wmt20</bibkey>
      <pwccode url="https://github.com/ELITR/wmt20-elitr-testsuite" additional="false">ELITR/wmt20-elitr-testsuite</pwccode>
    </paper>
    <paper id="42">
      <title>Translating Similar Languages: Role of Mutual Intelligibility in Multilingual Transformers</title>
      <author><first>Ife</first><last>Adebara</last></author>
      <author><first>El Moatez Billah</first><last>Nagoudi</last></author>
      <author><first>Muhammad</first><last>Abdul Mageed</last></author>
      <pages>381–386</pages>
      <abstract>In this work we investigate different approaches to translate between similar languages despite low resource limitations. This work is done as the participation of the UBC NLP research group in the WMT 2019 Similar Languages Translation Shared Task. We participated in all language pairs and performed various experiments. We used a transformer architecture for all the models and used back-translation for one of the language pairs. We explore both bilingual and multi-lingual approaches. We describe the pre-processing, training, translation and results for each model. We also investigate the role of mutual intelligibility in model performance.</abstract>
      <url hash="56245539">2020.wmt-1.42</url>
      <video href="https://slideslive.com/38939640"/>
      <bibkey>adebara-etal-2020-translating</bibkey>
    </paper>
    <paper id="43">
      <title>Attention Transformer Model for Translation of Similar Languages</title>
      <author><first>Farhan</first><last>Dhanani</last></author>
      <author><first>Muhammad</first><last>Rafi</last></author>
      <pages>387–392</pages>
      <abstract>This paper illustrates our approach to the shared task on similar language translation in the fifth conference on machine translation (WMT-20). Our motivation comes from the latest state of the art neural machine translation in which Transformers and Recurrent Attention models are effectively used. A typical sequence-sequence architecture consists of an encoder and a decoder Recurrent Neural Network (RNN). The encoder recursively processes a source sequence and reduces it into a fixed-length vector (context), and the decoder generates a target sequence, token by token, conditioned on the same context. In contrast, the advantage of transformers is to reduce the training time by offering a higher degree of parallelism at the cost of freedom for sequential order. With the introduction of Recurrent Attention, it allows the decoder to focus effectively on order of the source sequence at different decoding steps. In our approach, we have combined the recurrence based layered encoder-decoder model with the Transformer model. Our Attention Transformer model enjoys the benefits of both Recurrent Attention and Transformer to quickly learn the most probable sequence for decoding in the target language. The architecture is especially suited for similar languages (languages coming from the same family). We have submitted our system for both Indo-Aryan Language forward (Hindi to Marathi) and reverse (Marathi to Hindi) pair. Our system trains on the parallel corpus of the training dataset provided by the organizers and achieved an average BLEU point of 3.68 with 97.64 TER score for the Hindi-Marathi, along with 9.02 BLEU point and 88.6 TER score for Marathi-Hindi testing set.</abstract>
      <url hash="7395f105">2020.wmt-1.43</url>
      <video href="https://slideslive.com/38939592"/>
      <bibkey>dhanani-rafi-2020-attention</bibkey>
      <pwccode url="https://github.com/farhandhanani/wmt-20-submission-shared-task-similar-language-translation" additional="false">farhandhanani/wmt-20-submission-shared-task-similar-language-translation</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2020">WMT 2020</pwcdataset>
    </paper>
    <paper id="44">
      <title>Transformer-based Neural Machine Translation System for <fixed-case>H</fixed-case>indi – <fixed-case>M</fixed-case>arathi: <fixed-case>WMT</fixed-case>20 Shared Task</title>
      <author><first>Amit</first><last>Kumar</last></author>
      <author><first>Rupjyoti</first><last>Baruah</last></author>
      <author><first>Rajesh Kumar</first><last>Mundotiya</last></author>
      <author><first>Anil Kumar</first><last>Singh</last></author>
      <pages>393–395</pages>
      <abstract>This paper reports the results for the Machine Translation (MT) system submitted by the NLPRL team for the Hindi – Marathi Similar Translation Task at WMT 2020. We apply the Transformer-based Neural Machine Translation (NMT) approach on both translation directions for this language pair. The trained model is evaluated on the corpus provided by shared task organizers, using BLEU, RIBES, and TER scores. There were a total of 23 systems submitted for Marathi to Hindi and 21 systems submitted for Hindi to Marathi in the shared task. Out of these, our submission ranked 6th and 9th, respectively.</abstract>
      <url hash="e040232a">2020.wmt-1.44</url>
      <video href="https://slideslive.com/38939605"/>
      <bibkey>kumar-etal-2020-transformer</bibkey>
    </paper>
    <paper id="45">
      <title><fixed-case>H</fixed-case>indi-<fixed-case>M</fixed-case>arathi Cross Lingual Model</title>
      <author><first>Sahinur Rahman</first><last>Laskar</last></author>
      <author><first>Abdullah Faiz Ur Rahman</first><last>Khilji</last></author>
      <author><first>Partha</first><last>Pakray</last></author>
      <author><first>Sivaji</first><last>Bandyopadhyay</last></author>
      <pages>396–401</pages>
      <abstract>Machine Translation (MT) is a vital tool for aiding communication between linguistically separate groups of people. The neural machine translation (NMT) based approaches have gained widespread acceptance because of its outstanding performance. We have participated in WMT20 shared task of similar language translation on Hindi-Marathi pair. The main challenge of this task is by utilization of monolingual data and similarity features of similar language pair to overcome the limitation of available parallel data. In this work, we have implemented NMT based model that simultaneously learns bilingual embedding from both the source and target language pairs. Our model has achieved Hindi to Marathi bilingual evaluation understudy (BLEU) score of 11.59, rank-based intuitive bilingual evaluation score (RIBES) score of 57.76 and translation edit rate (TER) score of 79.07 and Marathi to Hindi BLEU score of 15.44, RIBES score of 61.13 and TER score of 75.96.</abstract>
      <url hash="cc08664f">2020.wmt-1.45</url>
      <video href="https://slideslive.com/38939611"/>
      <bibkey>laskar-etal-2020-hindi</bibkey>
    </paper>
    <paper id="46">
      <title>Transfer Learning for Related Languages: Submissions to the <fixed-case>WMT</fixed-case>20 Similar Language Translation Task</title>
      <author><first>Lovish</first><last>Madaan</last></author>
      <author><first>Soumya</first><last>Sharma</last></author>
      <author><first>Parag</first><last>Singla</last></author>
      <pages>402–408</pages>
      <abstract>In this paper, we describe IIT Delhi’s submissions to the WMT 2020 task on Similar Language Translation for four language directions: Hindi &lt;-&gt; Marathi and Spanish &lt;-&gt; Portuguese. We try out three different model settings for the translation task and select our primary and contrastive submissions on the basis of performance of these three models. For our best submissions, we fine-tune the mBART model on the parallel data provided for the task. The pre-training is done using self-supervised objectives on a large amount of monolingual data for many languages. Overall, our models are ranked in the top four of all systems for the submitted language pairs, with first rank in Spanish -&gt; Portuguese.</abstract>
      <url hash="b68f3fb8">2020.wmt-1.46</url>
      <video href="https://slideslive.com/38939620"/>
      <bibkey>madaan-etal-2020-transfer</bibkey>
    </paper>
    <paper id="47">
      <title>The <fixed-case>IPN</fixed-case>-<fixed-case>CIC</fixed-case> team system submission for the <fixed-case>WMT</fixed-case> 2020 similar language task</title>
      <author><first>Luis A.</first><last>Menéndez-Salazar</last></author>
      <author><first>Grigori</first><last>Sidorov</last></author>
      <author><first>Marta R.</first><last>Costa-Jussà</last></author>
      <pages>409–413</pages>
      <abstract>This paper describes the participation of the NLP research team of the IPN Computer Research center in the WMT 2020 Similar Language Translation Task. We have submitted systems for the Spanish-Portuguese language pair (in both directions). The three submitted systems are based on the Transformer architecture and used fine tuning for domain Adaptation.</abstract>
      <url hash="553ba031">2020.wmt-1.47</url>
      <video href="https://slideslive.com/38939595"/>
      <bibkey>menendez-salazar-etal-2020-ipn</bibkey>
    </paper>
    <paper id="48">
      <title><fixed-case>NMT</fixed-case> based Similar Language Translation for <fixed-case>H</fixed-case>indi - <fixed-case>M</fixed-case>arathi</title>
      <author><first>Vandan</first><last>Mujadia</last></author>
      <author><first>Dipti</first><last>Sharma</last></author>
      <pages>414–417</pages>
      <abstract>This paper describes the participation of team F1toF6 (LTRC, IIIT-Hyderabad) for the WMT 2020 task, similar language translation. We experimented with attention based recurrent neural network architecture (seq2seq) for this task. We explored the use of different linguistic features like POS and Morph along with back translation for Hindi-Marathi and Marathi-Hindi machine translation.</abstract>
      <url hash="83fd0ffd">2020.wmt-1.48</url>
      <bibkey>mujadia-sharma-2020-nmt</bibkey>
    </paper>
    <paper id="49">
      <title><fixed-case>NUIG</fixed-case>-Panlingua-<fixed-case>KMI</fixed-case> <fixed-case>H</fixed-case>indi-<fixed-case>M</fixed-case>arathi <fixed-case>MT</fixed-case> Systems for Similar Language Translation Task @ <fixed-case>WMT</fixed-case> 2020</title>
      <author><first>Atul Kr.</first><last>Ojha</last></author>
      <author><first>Priya</first><last>Rani</last></author>
      <author><first>Akanksha</first><last>Bansal</last></author>
      <author><first>Bharathi Raja</first><last>Chakravarthi</last></author>
      <author><first>Ritesh</first><last>Kumar</last></author>
      <author><first>John P.</first><last>McCrae</last></author>
      <pages>418–423</pages>
      <abstract>NUIG-Panlingua-KMI submission to WMT 2020 seeks to push the state-of-the-art in Similar Language Translation Task for Hindi↔Marathi language pair. As part of these efforts, we conducteda series of experiments to address the challenges for translation between similar languages. Among the 4 MT systems prepared under this task, 1 PBSMT systems were prepared for Hindi↔Marathi each and 1 NMT systems were developed for Hindi↔Marathi using Byte PairEn-coding (BPE) into subwords. The results show that different architectures NMT could be an effective method for developing MT systems for closely related languages. Our Hindi-Marathi NMT system was ranked 8th among the 14 teams that participated and our Marathi-Hindi NMT system was ranked 8th among the 11 teams participated for the task.</abstract>
      <url hash="4d53a2ad">2020.wmt-1.49</url>
      <video href="https://slideslive.com/38939638"/>
      <bibkey>ojha-etal-2020-nuig</bibkey>
    </paper>
    <paper id="50">
      <title>Neural Machine Translation for Similar Languages: The Case of <fixed-case>I</fixed-case>ndo-<fixed-case>A</fixed-case>ryan Languages</title>
      <author><first>Santanu</first><last>Pal</last></author>
      <author><first>Marcos</first><last>Zampieri</last></author>
      <pages>424–429</pages>
      <abstract>In this paper we present the WIPRO-RIT systems submitted to the Similar Language Translation shared task at WMT 2020. The second edition of this shared task featured parallel data from pairs/groups of similar languages from three different language families: Indo-Aryan languages (Hindi and Marathi), Romance languages (Catalan, Portuguese, and Spanish), and South Slavic Languages (Croatian, Serbian, and Slovene). We report the results obtained by our systems in translating from Hindi to Marathi and from Marathi to Hindi. WIPRO-RIT achieved competitive performance ranking 1st in Marathi to Hindi and 2nd in Hindi to Marathi translation among 22 systems.</abstract>
      <url hash="d48605c2">2020.wmt-1.50</url>
      <bibkey>pal-zampieri-2020-neural</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/pmindia">PMIndia</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2014">WMT 2014</pwcdataset>
    </paper>
    <paper id="51">
      <title>Neural Machine Translation between similar <fixed-case>S</fixed-case>outh-<fixed-case>S</fixed-case>lavic languages</title>
      <author><first>Maja</first><last>Popović</last></author>
      <author><first>Alberto</first><last>Poncelas</last></author>
      <pages>430–436</pages>
      <abstract>This paper describes the ADAPT-DCU machine translation systems built for the WMT 2020 shared task on Similar Language Translation. We explored several set-ups for NMT for Croatian–Slovenian and Serbian–Slovenian language pairs in both translation directions. Our experiments focus on different amounts and types of training data: we first apply basic filtering on the OpenSubtitles training corpora, then we perform additional cleaning of remaining misaligned segments based on character n-gram matching. Finally, we make use of additional monolingual data by creating synthetic parallel data through back-translation. Automatic evaluation shows that multilingual systems with joint Serbian and Croatian data are better than bilingual, as well as that character-based cleaning leads to improved scores while using less data. The results also confirm once more that adding back-translated data further improves the performance, especially when the synthetic data is similar to the desired domain of the development and test set. This, however, might come at a price of prolonged training time, especially for multitarget systems.</abstract>
      <url hash="d6c92eb3">2020.wmt-1.51</url>
      <bibkey>popovic-poncelas-2020-neural</bibkey>
    </paper>
    <paper id="52">
      <title>Infosys Machine Translation System for <fixed-case>WMT</fixed-case>20 Similar Language Translation Task</title>
      <author><first>Kamalkumar</first><last>Rathinasamy</last></author>
      <author><first>Amanpreet</first><last>Singh</last></author>
      <author><first>Balaguru</first><last>Sivasambagupta</last></author>
      <author><first>Prajna</first><last>Prasad Neerchal</last></author>
      <author><first>Vani</first><last>Sivasankaran</last></author>
      <pages>437–441</pages>
      <abstract>This paper describes Infosys’s submission to the WMT20 Similar Language Translation shared task. We participated in Indo-Aryan language pair in the language direction Hindi to Marathi. Our baseline system is byte-pair encoding based transformer model trained with the Fairseq sequence modeling toolkit. Our final system is an ensemble of two transformer models, which ranked first in WMT20 evaluation. One model is designed to learn the nuances of translation of this low resource language pair by taking advantage of the fact that the source and target languages are same alphabet languages. The other model is the result of experimentation with the proportion of back-translated data to the parallel data to improve translation fluency.</abstract>
      <url hash="a916cc0e">2020.wmt-1.52</url>
      <video href="https://slideslive.com/38939615"/>
      <bibkey>rathinasamy-etal-2020-infosys</bibkey>
    </paper>
    <paper id="53">
      <title>Document Level <fixed-case>NMT</fixed-case> of Low-Resource Languages with Backtranslation</title>
      <author><first>Sami</first><last>Ul Haq</last></author>
      <author><first>Sadaf</first><last>Abdul Rauf</last></author>
      <author><first>Arsalan</first><last>Shaukat</last></author>
      <author><first>Abdullah</first><last>Saeed</last></author>
      <pages>442–446</pages>
      <abstract>This paper describes our system submission to WMT20 shared task on similar language translation. We examined the use of documentlevel neural machine translation (NMT) systems for low-resource, similar language pair Marathi−Hindi. Our system is an extension of state-of-the-art Transformer architecture with hierarchical attention networks to incorporate contextual information. Since, NMT requires large amount of parallel data which is not available for this task, our approach is focused on utilizing monolingual data with back translation to train our models. Our experiments reveal that document-level NMT can be a reasonable alternative to sentence-level NMT for improving translation quality of low resourced languages even when used with synthetic data.</abstract>
      <url hash="a32a4c70">2020.wmt-1.53</url>
      <video href="https://slideslive.com/38939608"/>
      <bibkey>ul-haq-etal-2020-document</bibkey>
    </paper>
    <paper id="54">
      <title>Multilingual Neural Machine Translation: Case-study for <fixed-case>C</fixed-case>atalan, <fixed-case>S</fixed-case>panish and <fixed-case>P</fixed-case>ortuguese <fixed-case>R</fixed-case>omance Languages</title>
      <author><first>Pere</first><last>Vergés Boncompte</last></author>
      <author><first>Marta</first><last>R. Costa-jussà</last></author>
      <pages>447–450</pages>
      <abstract>In this paper, we describe the TALP-UPC participation in the WMT Similar Language Translation task between Catalan, Spanish, and Portuguese, all of them, Romance languages. We made use of different techniques to improve the translation between these languages. The multilingual shared encoder/decoder has been used for all of them. Additionally, we applied back-translation to take advantage of the monolingual data. Finally, we have applied fine-tuning to improve the in-domain data. Each of these techniques brings improvements over the previous one. In the official evaluation, our system was ranked 1st in the Portuguese-to-Spanish direction, 2nd in the opposite direction, and 3rd in the Catalan-Spanish pair.</abstract>
      <url hash="de44a7e5">2020.wmt-1.54</url>
      <video href="https://slideslive.com/38939555"/>
      <bibkey>verges-boncompte-r-costa-jussa-2020-multilingual</bibkey>
    </paper>
    <paper id="55">
      <title>A3-108 Machine Translation System for Similar Language Translation Shared Task 2020</title>
      <author><first>Saumitra</first><last>Yadav</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <pages>451–455</pages>
      <abstract>In this paper, we describe our submissions for Similar Language Translation Shared Task 2020. We built 12 systems in each direction for Hindi⇐⇒Marathi language pair. This paper outlines initial baseline experiments with various tokenization schemes to train statistical models. Using optimal tokenization scheme among these we created synthetic source side text with back translation. And prune synthetic text with language model scores. This synthetic data was then used along with training data in various settings to build translation models. We also report configuration of the submitted systems and results produced by them.</abstract>
      <url hash="f774e4f8">2020.wmt-1.55</url>
      <video href="https://slideslive.com/38939590"/>
      <bibkey>yadav-shrivastava-2020-a3</bibkey>
    </paper>
    <paper id="56">
      <title>The <fixed-case>U</fixed-case>niversity of <fixed-case>M</fixed-case>aryland’s Submissions to the <fixed-case>WMT</fixed-case>20 Chat Translation Task: Searching for More Data to Adapt Discourse-Aware Neural Machine Translation</title>
      <author><first>Calvin</first><last>Bao</last></author>
      <author><first>Yow-Ting</first><last>Shiue</last></author>
      <author><first>Chujun</first><last>Song</last></author>
      <author><first>Jie</first><last>Li</last></author>
      <author><first>Marine</first><last>Carpuat</last></author>
      <pages>456–461</pages>
      <abstract>This paper describes the University of Maryland’s submissions to the WMT20 Shared Task on Chat Translation. We focus on translating agent-side utterances from English to German. We started from an off-the-shelf BPE-based standard transformer model trained with WMT17 news and fine-tuned it with the provided in-domain training data. In addition, we augment the training set with its best matches in the WMT19 news dataset. Our primary submission uses a standard Transformer, while our contrastive submissions use multi-encoder Transformers to attend to previous utterances. Our primary submission achieves 56.7 BLEU on the agent side (en→de), outperforming a baseline system provided by the task organizers by more than 13 BLEU points. Moreover, according to an evaluation on a set of carefully-designed examples, the multi-encoder architecture is able to generate more coherent translations.</abstract>
      <url hash="aab797e5">2020.wmt-1.56</url>
      <video href="https://slideslive.com/38939647"/>
      <bibkey>bao-etal-2020-university</bibkey>
    </paper>
    <paper id="57">
      <title>Naver Labs <fixed-case>E</fixed-case>urope’s Participation in the Robustness, Chat, and Biomedical Tasks at <fixed-case>WMT</fixed-case> 2020</title>
      <author><first>Alexandre</first><last>Berard</last></author>
      <author><first>Ioan</first><last>Calapodescu</last></author>
      <author><first>Vassilina</first><last>Nikoulina</last></author>
      <author><first>Jerin</first><last>Philip</last></author>
      <pages>462–472</pages>
      <abstract>This paper describes Naver Labs Europe’s participation in the Robustness, Chat, and Biomedical Translation tasks at WMT 2020. We propose a bidirectional German-English model that is multi-domain, robust to noise, and which can translate entire documents (or bilingual dialogues) at once. We use the same ensemble of such models as our primary submission to all three tasks and achieve competitive results. We also experiment with language model pre-training techniques and evaluate their impact on robustness to noise and out-of-domain translation. For German, Spanish, Italian, and French to English translation in the Biomedical Task, we also submit our recently released multilingual Covid19NMT model.</abstract>
      <url hash="46e020b8">2020.wmt-1.57</url>
      <video href="https://slideslive.com/38939577"/>
      <bibkey>berard-etal-2020-naver</bibkey>
    </paper>
    <paper id="58">
      <title>The <fixed-case>U</fixed-case>niversity of <fixed-case>E</fixed-case>dinburgh-<fixed-case>U</fixed-case>ppsala <fixed-case>U</fixed-case>niversity’s Submission to the <fixed-case>WMT</fixed-case> 2020 Chat Translation Task</title>
      <author><first>Nikita</first><last>Moghe</last></author>
      <author><first>Christian</first><last>Hardmeier</last></author>
      <author><first>Rachel</first><last>Bawden</last></author>
      <pages>473–478</pages>
      <abstract>This paper describes the joint submission of the University of Edinburgh and Uppsala University to the WMT’20 chat translation task for both language directions (English-German). We use existing state-of-the-art machine translation models trained on news data and fine-tune them on in-domain and pseudo-in-domain web crawled data. Our baseline systems are transformer-big models that are pre-trained on the WMT’19 News Translation task and fine-tuned on pseudo-in-domain web crawled data and in-domain task data. We also experiment with (i) adaptation using speaker and domain tags and (ii) using different types and amounts of preceding context. We observe that contrarily to expectations, exploiting context degrades the results (and on analysis the data is not highly contextual). However using domain tags does improve scores according to the automatic evaluation. Our final primary systems use domain tags and are ensembles of 4 models, with noisy channel reranking of outputs. Our en-de system was ranked second in the shared task while our de-en system outperformed all the other systems.</abstract>
      <url hash="e115ed18">2020.wmt-1.58</url>
      <attachment type="OptionalSupplementaryMaterial" hash="e307200e">2020.wmt-1.58.OptionalSupplementaryMaterial.pdf</attachment>
      <video href="https://slideslive.com/38939627"/>
      <bibkey>moghe-etal-2020-university</bibkey>
    </paper>
    <paper id="59">
      <title><fixed-case>JUST</fixed-case> System for <fixed-case>WMT</fixed-case>20 Chat Translation Task</title>
      <author><first>Roweida</first><last>Mohammed</last></author>
      <author><first>Mahmoud</first><last>Al-Ayyoub</last></author>
      <author><first>Malak</first><last>Abdullah</last></author>
      <pages>479–482</pages>
      <abstract>Machine Translation (MT) is a sub-field of Artificial Intelligence and Natural Language Processing that investigates and studies the ways of automatically translating a text from one language to another. In this paper, we present the details of our submission to the WMT20 Chat Translation Task, which consists of two language directions, English –&gt; German and German –&gt; English. The major feature of our system is applying a pre-trained BERT embedding with a bidirectional recurrent neural network. Our system ensembles three models, each with different hyperparameters. Despite being trained on a very small corpus, our model produces surprisingly good results.</abstract>
      <url hash="31e60802">2020.wmt-1.59</url>
      <video href="https://slideslive.com/38939629"/>
      <bibkey>mohammed-etal-2020-just</bibkey>
    </paper>
    <paper id="60">
      <title>Tencent <fixed-case>AI</fixed-case> Lab Machine Translation Systems for <fixed-case>WMT</fixed-case>20 Chat Translation Task</title>
      <author><first>Longyue</first><last>Wang</last></author>
      <author><first>Zhaopeng</first><last>Tu</last></author>
      <author><first>Xing</first><last>Wang</last></author>
      <author><first>Li</first><last>Ding</last></author>
      <author><first>Liang</first><last>Ding</last></author>
      <author><first>Shuming</first><last>Shi</last></author>
      <pages>483–491</pages>
      <abstract>This paper describes the Tencent AI Lab’s submission of the WMT 2020 shared task on chat translation in English-German. Our neural machine translation (NMT) systems are built on sentence-level, document-level, non-autoregressive (NAT) and pretrained models. We integrate a number of advanced techniques into our systems, including data selection, back/forward translation, larger batch learning, model ensemble, finetuning as well as system combination. Specifically, we proposed a hybrid data selection method to select high-quality and in-domain sentences from out-of-domain data. To better capture the source contexts, we exploit to augment NAT models with evolved cross-attention. Furthermore, we explore to transfer general knowledge from four different pre-training language models to the downstream translation task. In general, we present extensive experimental results for this new translation task. Among all the participants, our German-to-English primary system is ranked the second in terms of BLEU scores.</abstract>
      <url hash="100718c4">2020.wmt-1.60</url>
      <video href="https://slideslive.com/38939671"/>
      <bibkey>wang-etal-2020-tencent</bibkey>
    </paper>
    <paper id="61">
      <title>Combining Sequence Distillation and Transfer Learning for Efficient Low-Resource Neural Machine Translation Models</title>
      <author><first>Raj</first><last>Dabre</last></author>
      <author><first>Atsushi</first><last>Fujita</last></author>
      <pages>492–502</pages>
      <abstract>In neural machine translation (NMT), sequence distillation (SD) through creation of distilled corpora leads to efficient (compact and fast) models. However, its effectiveness in extremely low-resource (ELR) settings has not been well-studied. On the other hand, transfer learning (TL) by leveraging larger helping corpora greatly improves translation quality in general. This paper investigates a combination of SD and TL for training efficient NMT models for ELR settings, where we utilize TL with helping corpora twice: once for distilling the ELR corpora and then during compact model training. We experimented with two ELR settings: Vietnamese–English and Hindi–English from the Asian Language Treebank dataset with 18k training sentence pairs. Using the compact models with 40% smaller parameters trained on the distilled ELR corpora, greedy search achieved 3.6 BLEU points improvement in average while reducing 40% of decoding time. We also confirmed that using both the distilled ELR and helping corpora in the second round of TL further improves translation quality. Our work highlights the importance of stage-wise application of SD and TL for efficient NMT modeling for ELR settings.</abstract>
      <url hash="9ecbf93e">2020.wmt-1.61</url>
      <video href="https://slideslive.com/38939551"/>
      <bibkey>dabre-fujita-2020-combining</bibkey>
    </paper>
    <paper id="62">
      <title>Fast Interleaved Bidirectional Sequence Generation</title>
      <author><first>Biao</first><last>Zhang</last></author>
      <author><first>Ivan</first><last>Titov</last></author>
      <author><first>Rico</first><last>Sennrich</last></author>
      <pages>503–515</pages>
      <abstract>Independence assumptions during sequence generation can speed up inference, but parallel generation of highly inter-dependent tokens comes at a cost in quality. Instead of assuming independence between neighbouring tokens (semi-autoregressive decoding, SA), we take inspiration from bidirectional sequence generation and introduce a decoder that generates target words from the left-to-right and right-to-left directions simultaneously. We show that we can easily convert a standard architecture for unidirectional decoding into a bidirectional decoder by simply interleaving the two directions and adapting the word positions and selfattention masks. Our interleaved bidirectional decoder (IBDecoder) retains the model simplicity and training efficiency of the standard Transformer, and on five machine translation tasks and two document summarization tasks, achieves a decoding speedup of ~2x compared to autoregressive decoding with comparable quality. Notably, it outperforms left-to-right SA because the independence assumptions in IBDecoder are more felicitous. To achieve even higher speedups, we explore hybrid models where we either simultaneously predict multiple neighbouring tokens per direction, or perform multi-directional decoding by partitioning the target sequence. These methods achieve speedups to 4x–11x across different tasks at the cost of &lt;1 BLEU or &lt;0.5 ROUGE (on average)</abstract>
      <url hash="1b00560c">2020.wmt-1.62</url>
      <video href="https://slideslive.com/38939588"/>
      <bibkey>zhang-etal-2020-fast</bibkey>
      <pwccode url="https://github.com/bzhangGo/zero" additional="false">bzhangGo/zero</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/c4">C4</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2014">WMT 2014</pwcdataset>
    </paper>
    <paper id="63">
      <title>Priming Neural Machine Translation</title>
      <author><first>Minh Quang</first><last>Pham</last></author>
      <author><first>Jitao</first><last>Xu</last></author>
      <author><first>Josep</first><last>Crego</last></author>
      <author><first>François</first><last>Yvon</last></author>
      <author><first>Jean</first><last>Senellart</last></author>
      <pages>516–527</pages>
      <abstract>Priming is a well known and studied psychology phenomenon based on the prior presentation of one stimulus (cue) to influence the processing of a response. In this paper, we propose a framework to mimic the process of priming in the context of neural machine translation (NMT). We evaluate the effect of using similar translations as priming cues on the NMT network. We propose a method to inject priming cues into the NMT network and compare our framework to other mechanisms that perform micro-adaptation during inference. Overall, experiments conducted in a multi-domain setting confirm that adding priming cues in the NMT decoder can go a long way towards improving the translation accuracy. Besides, we show the suitability of our framework to gather valuable information for an NMT network from monolingual resources.</abstract>
      <url hash="ee009843">2020.wmt-1.63</url>
      <video href="https://slideslive.com/38939637"/>
      <bibkey>pham-etal-2020-priming</bibkey>
    </paper>
    <paper id="64">
      <title>Subword Segmentation and a Single Bridge Language Affect Zero-Shot Neural Machine Translation</title>
      <author><first>Annette</first><last>Rios</last></author>
      <author><first>Mathias</first><last>Müller</last></author>
      <author><first>Rico</first><last>Sennrich</last></author>
      <pages>528–537</pages>
      <abstract>Zero-shot neural machine translation is an attractive goal because of the high cost of obtaining data and building translation systems for new translation directions. However, previous papers have reported mixed success in zero-shot translation. It is hard to predict in which settings it will be effective, and what limits performance compared to a fully supervised system. In this paper, we investigate zero-shot performance of a multilingual EN&lt;-&gt;FR,CS,DE,FI system trained on WMT data. We find that zero-shot performance is highly unstable and can vary by more than 6 BLEU between training runs, making it difficult to reliably track improvements. We observe a bias towards copying the source in zero-shot translation, and investigate how the choice of subword segmentation affects this bias. We find that language-specific subword segmentation results in less subword copying at training time, and leads to better zero-shot performance compared to jointly trained segmentation. A recent trend in multilingual models is to not train on parallel data between all language pairs, but have a single bridge language, e.g. English. We find that this negatively affects zero-shot translation and leads to a failure mode where the model ignores the language tag and instead produces English output in zero-shot directions. We show that this bias towards English can be effectively reduced with even a small amount of parallel data in some of the non-English pairs.</abstract>
      <url hash="e9a956ef">2020.wmt-1.64</url>
      <video href="https://slideslive.com/38939660"/>
      <bibkey>rios-etal-2020-subword</bibkey>
      <pwccode url="https://github.com/awslabs/sockeye" additional="false">awslabs/sockeye</pwccode>
    </paper>
    <paper id="65">
      <title>Look It Up: Bilingual and Monolingual Dictionaries Improve Neural Machine Translation</title>
      <author><first>Xing Jie</first><last>Zhong</last></author>
      <author><first>David</first><last>Chiang</last></author>
      <pages>538–549</pages>
      <abstract>Despite advances in neural machine translation (NMT) quality, rare words continue to be problematic. For humans, the solution to the rare-word problem has long been dictionaries, but dictionaries cannot be straightforwardly incorporated into NMT. In this paper, we describe a new method for “attaching” dictionary definitions to rare words so that the network can learn the best way to use them. We demonstrate improvements of up to 3.1 BLEU using bilingual dictionaries and up to 0.7 BLEU using monolingual source-language dictionaries.</abstract>
      <url hash="a02954a1">2020.wmt-1.65</url>
      <video href="https://slideslive.com/38939597"/>
      <bibkey>zhong-chiang-2020-look</bibkey>
      <revision id="1" href="2020.wmt-1.65v1" hash="72c9dc0b"/>
      <revision id="2" href="2020.wmt-1.65v2" hash="a02954a1" date="2021-08-08">Retracted.</revision>
      <retracted date="2021-07-08">The authors discovered a problem with the experiments, whose correction unfortunately changes the findings of the paper.</retracted>
    </paper>
    <paper id="66">
      <title>Complete Multilingual Neural Machine Translation</title>
      <author><first>Markus</first><last>Freitag</last></author>
      <author><first>Orhan</first><last>Firat</last></author>
      <pages>550–560</pages>
      <abstract>Multilingual Neural Machine Translation (MNMT) models are commonly trained on a joint set of bilingual corpora which is acutely English-centric (i.e. English either as source or target language). While direct data between two languages that are non-English is explicitly available at times, its use is not common. In this paper, we first take a step back and look at the commonly used bilingual corpora (WMT), and resurface the existence and importance of implicit structure that existed in it: multi-way alignment across examples (the same sentence in more than two languages). We set out to study the use of multi-way aligned examples in order to enrich the original English-centric parallel corpora. We reintroduce this direct parallel data from multi-way aligned corpora between all source and target languages. By doing so, the English-centric graph expands into a complete graph, every language pair being connected. We call MNMT with such connectivity pattern complete Multilingual Neural Machine Translation (cMNMT) and demonstrate its utility and efficacy with a series of experiments and analysis. In combination with a novel training data sampling strategy that is conditioned on the target language only, cMNMT yields competitive translation quality for all language pairs. We further study the size effect of multi-way aligned data, its transfer learning capabilities and how it eases adding a new language in MNMT. Finally, we stress test cMNMT at scale and demonstrate that we can train a cMNMT model with up to 12,432 language pairs that provides competitive translation quality for all language pairs.</abstract>
      <url hash="eedd568a">2020.wmt-1.66</url>
      <video href="https://slideslive.com/38939550"/>
      <bibkey>freitag-firat-2020-complete</bibkey>
    </paper>
    <paper id="67">
      <title>Paraphrase Generation as Zero-Shot Multilingual Translation: Disentangling Semantic Similarity from Lexical and Syntactic Diversity</title>
      <author><first>Brian</first><last>Thompson</last></author>
      <author><first>Matt</first><last>Post</last></author>
      <pages>561–570</pages>
      <abstract>Recent work has shown that a multilingual neural machine translation (NMT) model can be used to judge how well a sentence paraphrases another sentence in the same language (Thompson and Post, 2020); however, attempting to generate paraphrases from such a model using standard beam search produces trivial copies or near copies. We introduce a simple paraphrase generation algorithm which discourages the production of n-grams that are present in the input. Our approach enables paraphrase generation in many languages from a single multilingual NMT model. Furthermore, the amount of lexical diversity between the input and output can be controlled at generation time. We conduct a human evaluation to compare our method to a paraphraser trained on the large English synthetic paraphrase database ParaBank 2 (Hu et al., 2019c) and find that our method produces paraphrases that better preserve meaning and are more gramatical, for the same level of lexical diversity. Additional smaller human assessments demonstrate our approach also works in two non-English languages.</abstract>
      <url hash="b5a8bec9">2020.wmt-1.67</url>
      <video href="https://slideslive.com/38939552"/>
      <bibkey>thompson-post-2020-paraphrase</bibkey>
      <pwccode url="https://github.com/thompsonb/prism" additional="false">thompsonb/prism</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/parabank">ParaBank</pwcdataset>
    </paper>
    <paper id="68">
      <title>When Does Unsupervised Machine Translation Work?</title>
      <author><first>Kelly</first><last>Marchisio</last></author>
      <author><first>Kevin</first><last>Duh</last></author>
      <author><first>Philipp</first><last>Koehn</last></author>
      <pages>571–583</pages>
      <abstract>Despite the reported success of unsupervised machine translation (MT), the field has yet to examine the conditions under which the methods succeed and fail. We conduct an extensive empirical evaluation using dissimilar language pairs, dissimilar domains, and diverse datasets. We find that performance rapidly deteriorates when source and target corpora are from different domains, and that stochasticity during embedding training can dramatically affect downstream results. We additionally find that unsupervised MT performance declines when source and target languages use different scripts, and observe very poor performance on authentic low-resource language pairs. We advocate for extensive empirical evaluation of unsupervised MT systems to highlight failure points and encourage continued research on the most promising paradigms. We release our preprocessed dataset to encourage evaluations that stress-test systems under multiple data conditions.</abstract>
      <url hash="f9dc4fa2">2020.wmt-1.68</url>
      <video href="https://slideslive.com/38939554"/>
      <bibkey>marchisio-etal-2020-unsupervised</bibkey>
    </paper>
    <paper id="69">
      <title>Language Models not just for Pre-training: Fast Online Neural Noisy Channel Modeling</title>
      <author><first>Shruti</first><last>Bhosale</last></author>
      <author><first>Kyra</first><last>Yee</last></author>
      <author><first>Sergey</first><last>Edunov</last></author>
      <author><first>Michael</first><last>Auli</last></author>
      <pages>584–593</pages>
      <abstract>Pre-training models on vast quantities of unlabeled data has emerged as an effective approach to improving accuracy on many NLP tasks. On the other hand, traditional machine translation has a long history of leveraging unlabeled data through noisy channel modeling. The same idea has recently been shown to achieve strong improvements for neural machine translation. Unfortunately, na ̈ıve noisy channel modeling with modern sequence to sequence models is up to an order of magnitude slower than alternatives. We address this issue by introducing efficient approximations to make inference with the noisy channel approach as fast as strong ensembles while increasing accuracy. We also show that the noisy channel approach can outperform strong pre-training results by achieving a new state of the art on WMT Romanian-English translation.</abstract>
      <url hash="4dc3e269">2020.wmt-1.69</url>
      <video href="https://slideslive.com/38939568"/>
      <bibkey>bhosale-etal-2020-language</bibkey>
      <pwccode url="https://github.com/pytorch/fairseq/tree/master/examples/fast_noisy_channel" additional="false">pytorch/fairseq</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2016">WMT 2016</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2016-news">WMT 2016 News</pwcdataset>
    </paper>
    <paper id="70">
      <title>Towards Multimodal Simultaneous Neural Machine Translation</title>
      <author><first>Aizhan</first><last>Imankulova</last></author>
      <author><first>Masahiro</first><last>Kaneko</last></author>
      <author><first>Tosho</first><last>Hirasawa</last></author>
      <author><first>Mamoru</first><last>Komachi</last></author>
      <pages>594–603</pages>
      <abstract>Simultaneous translation involves translating a sentence before the speaker’s utterance is completed in order to realize real-time understanding in multiple languages. This task is significantly more challenging than the general full sentence translation because of the shortage of input information during decoding. To alleviate this shortage, we propose multimodal simultaneous neural machine translation (MSNMT), which leverages visual information as an additional modality. Our experiments with the Multi30k dataset showed that MSNMT significantly outperforms its text-only counterpart in more timely translation situations with low latency. Furthermore, we verified the importance of visual information during decoding by performing an adversarial evaluation of MSNMT, where we studied how models behaved with incongruent input modality and analyzed the effect of different word order between source and target languages.</abstract>
      <url hash="6f2582e1">2020.wmt-1.70</url>
      <video href="https://slideslive.com/38939559"/>
      <bibkey>imankulova-etal-2020-towards</bibkey>
      <pwccode url="https://github.com/toshohirasawa/mst" additional="false">toshohirasawa/mst</pwccode>
    </paper>
    <paper id="71">
      <title>Diving Deep into Context-Aware Neural Machine Translation</title>
      <author><first>Jingjing</first><last>Huo</last></author>
      <author><first>Christian</first><last>Herold</last></author>
      <author><first>Yingbo</first><last>Gao</last></author>
      <author><first>Leonard</first><last>Dahlmann</last></author>
      <author><first>Shahram</first><last>Khadivi</last></author>
      <author><first>Hermann</first><last>Ney</last></author>
      <pages>604–616</pages>
      <abstract>Context-aware neural machine translation (NMT) is a promising direction to improve the translation quality by making use of the additional context, e.g., document-level translation, or having meta-information. Although there exist various architectures and analyses, the effectiveness of different context-aware NMT models is not well explored yet. This paper analyzes the performance of document-level NMT models on four diverse domains with a varied amount of parallel document-level bilingual data. We conduct a comprehensive set of experiments to investigate the impact of document-level NMT. We find that there is no single best approach to document-level NMT, but rather that different architectures come out on top on different tasks. Looking at task-specific problems, such as pronoun resolution or headline translation, we find improvements in the context-aware systems, even in cases where the corpus-level metrics like BLEU show no significant improvement. We also show that document-level back-translation significantly helps to compensate for the lack of document-level bi-texts.</abstract>
      <url hash="179ef0ee">2020.wmt-1.71</url>
      <attachment type="OptionalSupplementaryMaterial" hash="ec7c670b">2020.wmt-1.71.OptionalSupplementaryMaterial.zip</attachment>
      <video href="https://slideslive.com/38939585"/>
      <bibkey>huo-etal-2020-diving</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
    </paper>
    <paper id="72">
      <title>A Study of Residual Adapters for Multi-Domain Neural Machine Translation</title>
      <author><first>Minh Quang</first><last>Pham</last></author>
      <author><first>Josep Maria</first><last>Crego</last></author>
      <author><first>François</first><last>Yvon</last></author>
      <author><first>Jean</first><last>Senellart</last></author>
      <pages>617–628</pages>
      <abstract>Domain adaptation is an old and vexing problem for machine translation systems. The most common approach and successful to supervised adaptation is to fine-tune a baseline system with in-domain parallel data. Standard fine-tuning however modifies all the network parameters, which makes this approach computationally costly and prone to overfitting. A recent, lightweight approach, instead augments a baseline model with supplementary (small) adapter layers, keeping the rest of the mode unchanged. This has the additional merit to leave the baseline model intact, and adaptable to multiple domains. In this paper, we conduct a thorough analysis of the adapter model in the context of a multidomain machine translation task. We contrast multiple implementations of this idea on two language pairs. Our main conclusions are that residual adapters provide a fast and cheap method for supervised multi-domain adaptation; our two variants prove as effective as the original adapter model, and open perspective to also make adapted models more robust to label domain errors.</abstract>
      <url hash="82440d29">2020.wmt-1.72</url>
      <video href="https://slideslive.com/38939655"/>
      <bibkey>pham-etal-2020-study</bibkey>
      <pwccode url="https://github.com/qmpham/experiments" additional="false">qmpham/experiments</pwccode>
    </paper>
    <paper id="73">
      <title>Mitigating Gender Bias in Machine Translation with Target Gender Annotations</title>
      <author><first>Artūrs</first><last>Stafanovičs</last></author>
      <author><first>Toms</first><last>Bergmanis</last></author>
      <author><first>Mārcis</first><last>Pinnis</last></author>
      <pages>629–638</pages>
      <abstract>When translating “The secretary asked for details.” to a language with grammatical gender, it might be necessary to determine the gender of the subject “secretary”. If the sentence does not contain the necessary information, it is not always possible to disambiguate. In such cases, machine translation systems select the most common translation option, which often corresponds to the stereotypical translations, thus potentially exacerbating prejudice and marginalisation of certain groups and people. We argue that the information necessary for an adequate translation can not always be deduced from the sentence being translated or even might depend on external knowledge. Therefore, in this work, we propose to decouple the task of acquiring the necessary information from the task of learning to translate correctly when such information is available. To that end, we present a method for training machine translation systems to use word-level annotations containing information about subject’s gender. To prepare training data, we annotate regular source language words with grammatical gender information of the corresponding target language words. Using such data to train machine translation systems reduces their reliance on gender stereotypes when information about the subject’s gender is available. Our experiments on five language pairs show that this allows improving accuracy on the WinoMT test set by up to 25.8 percentage points.</abstract>
      <url hash="d841f0d6">2020.wmt-1.73</url>
      <video href="https://slideslive.com/38939614"/>
      <bibkey>stafanovics-etal-2020-mitigating</bibkey>
      <pwccode url="https://github.com/artursstaf/mitigating-gender-bias-wmt-2020" additional="false">artursstaf/mitigating-gender-bias-wmt-2020</pwccode>
    </paper>
    <paper id="74">
      <title>Document-aligned <fixed-case>J</fixed-case>apanese-<fixed-case>E</fixed-case>nglish Conversation Parallel Corpus</title>
      <author><first>Matīss</first><last>Rikters</last></author>
      <author><first>Ryokan</first><last>Ri</last></author>
      <author><first>Tong</first><last>Li</last></author>
      <author><first>Toshiaki</first><last>Nakazawa</last></author>
      <pages>639–645</pages>
      <abstract>Sentence-level (SL) machine translation (MT) has reached acceptable quality for many high-resourced languages, but not document-level (DL) MT, which is difficult to 1) train with little amount of DL data; and 2) evaluate, as the main methods and data sets focus on SL evaluation. To address the first issue, we present a document-aligned Japanese-English conversation corpus, including balanced, high-quality business conversation data for tuning and testing. As for the second issue, we manually identify the main areas where SL MT fails to produce adequate translations in lack of context. We then create an evaluation set where these phenomena are annotated to alleviate automatic evaluation of DL systems. We train MT models using our corpus to demonstrate how using context leads to improvements.</abstract>
      <url hash="1ab7229f">2020.wmt-1.74</url>
      <attachment type="OptionalSupplementaryMaterial" hash="b789c2b8">2020.wmt-1.74.OptionalSupplementaryMaterial.pdf</attachment>
      <video href="https://slideslive.com/38939560"/>
      <bibkey>rikters-etal-2020-document</bibkey>
      <pwccode url="https://github.com/tsuruoka-lab/AMI-Meeting-Parallel-Corpus" additional="false">tsuruoka-lab/AMI-Meeting-Parallel-Corpus</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/business-scene-dialogue">Business Scene Dialogue</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/jparacrawl">JParaCrawl</pwcdataset>
    </paper>
    <paper id="75">
      <title>Findings of the <fixed-case>WMT</fixed-case> 2020 Shared Task on Automatic Post-Editing</title>
      <author><first>Rajen</first><last>Chatterjee</last></author>
      <author><first>Markus</first><last>Freitag</last></author>
      <author><first>Matteo</first><last>Negri</last></author>
      <author><first>Marco</first><last>Turchi</last></author>
      <pages>646–659</pages>
      <abstract>We present the results of the 6th round of the WMT task on MT Automatic Post-Editing. The task consists in automatically correcting the output of a “black-box” machine translation system by learning from existing human corrections of different sentences. This year, the challenge consisted of fixing the errors present in English Wikipedia pages translated into German and Chinese by state-ofthe-art, not domain-adapted neural MT (NMT) systems unknown to participants. Six teams participated in the English-German task, submitting a total of 11 runs. Two teams participated in the English-Chinese task submitting 2 runs each. Due to i) the different source/domain of data compared to the past (Wikipedia vs Information Technology), ii) the different quality of the initial translations to be corrected and iii) the introduction of a new language pair (English-Chinese), this year’s results are not directly comparable with last year’s round. However, on both language directions, participants’ submissions show considerable improvements over the baseline results. On English-German, the top ranked system improves over the baseline by -11.35 TER and +16.68 BLEU points, while on EnglishChinese the improvements are respectively up to -12.13 TER and +14.57 BLEU points. Overall, coherent gains are also highlighted by the outcomes of human evaluation, which confirms the effectiveness of APE to improve MT quality, especially in the new generic domain selected for this year’s round.</abstract>
      <url hash="a77d9210">2020.wmt-1.75</url>
      <video href="https://slideslive.com/38939672"/>
      <bibkey>chatterjee-etal-2020-findings</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/escape">eSCAPE</pwcdataset>
    </paper>
    <paper id="76">
      <title>Findings of the <fixed-case>WMT</fixed-case> 2020 Biomedical Translation Shared Task: <fixed-case>B</fixed-case>asque, <fixed-case>I</fixed-case>talian and <fixed-case>R</fixed-case>ussian as New Additional Languages</title>
      <author><first>Rachel</first><last>Bawden</last></author>
      <author><first>Giorgio Maria</first><last>Di Nunzio</last></author>
      <author><first>Cristian</first><last>Grozea</last></author>
      <author><first>Inigo</first><last>Jauregi Unanue</last></author>
      <author><first>Antonio</first><last>Jimeno Yepes</last></author>
      <author><first>Nancy</first><last>Mah</last></author>
      <author><first>David</first><last>Martinez</last></author>
      <author><first>Aurélie</first><last>Névéol</last></author>
      <author><first>Mariana</first><last>Neves</last></author>
      <author><first>Maite</first><last>Oronoz</last></author>
      <author><first>Olatz</first><last>Perez-de-Viñaspre</last></author>
      <author><first>Massimo</first><last>Piccardi</last></author>
      <author><first>Roland</first><last>Roller</last></author>
      <author><first>Amy</first><last>Siu</last></author>
      <author><first>Philippe</first><last>Thomas</last></author>
      <author><first>Federica</first><last>Vezzani</last></author>
      <author><first>Maika</first><last>Vicente Navarro</last></author>
      <author><first>Dina</first><last>Wiemann</last></author>
      <author><first>Lana</first><last>Yeganova</last></author>
      <pages>660–687</pages>
      <abstract>Machine translation of scientific abstracts and terminologies has the potential to support health professionals and biomedical researchers in some of their activities. In the fifth edition of the WMT Biomedical Task, we addressed a total of eight language pairs. Five language pairs were previously addressed in past editions of the shared task, namely, English/German, English/French, English/Spanish, English/Portuguese, and English/Chinese. Three additional languages pairs were also introduced this year: English/Russian, English/Italian, and English/Basque. The task addressed the evaluation of both scientific abstracts (all language pairs) and terminologies (English/Basque only). We received submissions from a total of 20 teams. For recurring language pairs, we observed an improvement in the translations in terms of automatic scores and qualitative evaluations, compared to previous years.</abstract>
      <url hash="5073fc19">2020.wmt-1.76</url>
      <video href="https://slideslive.com/38939675"/>
      <bibkey>bawden-etal-2020-findings</bibkey>
      <pwccode url="https://github.com/biomedical-translation-corpora/corpora" additional="false">biomedical-translation-corpora/corpora</pwccode>
    </paper>
    <paper id="77">
      <title>Results of the <fixed-case>WMT</fixed-case>20 Metrics Shared Task</title>
      <author><first>Nitika</first><last>Mathur</last></author>
      <author><first>Johnny</first><last>Wei</last></author>
      <author><first>Markus</first><last>Freitag</last></author>
      <author><first>Qingsong</first><last>Ma</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <pages>688–725</pages>
      <abstract>This paper presents the results of the WMT20 Metrics Shared Task. Participants were asked to score the outputs of the translation systems competing in the WMT20 News Translation Task with automatic metrics. Ten research groups submitted 27 metrics, four of which are reference-less “metrics”. In addition, we computed five baseline metrics, including sentBLEU, BLEU, TER and using the SacreBLEU scorer. All metrics were evaluated on how well they correlate at the system-, document- and segment-level with the WMT20 official human scores. We present an extensive analysis on influence of different reference translations on metric reliability, how well automatic metrics score human translations, and we also flag major discrepancies between metric and human scores when evaluating MT systems. Finally, we investigate whether we can use automatic metrics to flag incorrect human ratings.</abstract>
      <url hash="8830e5cf">2020.wmt-1.77</url>
      <bibkey>mathur-etal-2020-results</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2020">WMT 2020</pwcdataset>
    </paper>
    <paper id="78">
      <title>Findings of the <fixed-case>WMT</fixed-case> 2020 Shared Task on Parallel Corpus Filtering and Alignment</title>
      <author><first>Philipp</first><last>Koehn</last></author>
      <author><first>Vishrav</first><last>Chaudhary</last></author>
      <author><first>Ahmed</first><last>El-Kishky</last></author>
      <author><first>Naman</first><last>Goyal</last></author>
      <author><first>Peng-Jen</first><last>Chen</last></author>
      <author><first>Francisco</first><last>Guzmán</last></author>
      <pages>726–742</pages>
      <abstract>Following two preceding WMT Shared Task on Parallel Corpus Filtering (Koehn et al., 2018, 2019), we posed again the challenge of assigning sentence-level quality scores for very noisy corpora of sentence pairs crawled from the web, with the goal of sub-selecting the highest-quality data to be used to train ma-chine translation systems. This year, the task tackled the low resource condition of Pashto–English and Khmer–English and also included the challenge of sentence alignment from document pairs.</abstract>
      <url hash="05380a7e">2020.wmt-1.78</url>
      <video href="https://slideslive.com/38939678"/>
      <bibkey>koehn-etal-2020-findings</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ccaligned">CCAligned</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paracrawl">ParaCrawl</pwcdataset>
    </paper>
    <paper id="79">
      <title>Findings of the <fixed-case>WMT</fixed-case> 2020 Shared Task on Quality Estimation</title>
      <author><first>Lucia</first><last>Specia</last></author>
      <author><first>Frédéric</first><last>Blain</last></author>
      <author><first>Marina</first><last>Fomicheva</last></author>
      <author><first>Erick</first><last>Fonseca</last></author>
      <author><first>Vishrav</first><last>Chaudhary</last></author>
      <author><first>Francisco</first><last>Guzmán</last></author>
      <author><first>André F. T.</first><last>Martins</last></author>
      <pages>743–764</pages>
      <abstract>We report the results of the WMT20 shared task on Quality Estimation, where the challenge is to predict the quality of the output of neural machine translation systems at the word, sentence and document levels. This edition included new data with open domain texts, direct assessment annotations, and multiple language pairs: English-German, English-Chinese, Russian-English, Romanian-English, Estonian-English, Sinhala-English and Nepali-English data for the sentence-level subtasks, English-German and English-Chinese for the word-level subtask, and English-French data for the document-level subtask. In addition, we made neural machine translation models available to participants. 19 participating teams from 27 institutions submitted altogether 1374 systems to different task variants and language pairs.</abstract>
      <url hash="0fe70db5">2020.wmt-1.79</url>
      <video href="https://slideslive.com/38939677"/>
      <bibkey>specia-etal-2020-findings-wmt</bibkey>
    </paper>
    <paper id="80">
      <title>Findings of the <fixed-case>WMT</fixed-case> 2020 Shared Tasks in Unsupervised <fixed-case>MT</fixed-case> and Very Low Resource Supervised <fixed-case>MT</fixed-case></title>
      <author><first>Alexander</first><last>Fraser</last></author>
      <pages>765–771</pages>
      <abstract>We describe the WMT 2020 Shared Tasks in Unsupervised MT and Very Low Resource Supervised MT. In both tasks, the community studied German to Upper Sorbian and Upper Sorbian to German MT, which is a very realistic machine translation scenario (unlike the simulated scenarios used in particular in much of the unsupervised MT work in the past). We were able to obtain most of the digital data available for Upper Sorbian, a minority language of Germany, which was the original motivation for the Unsupervised MT shared task. As we were defining the task, we also obtained a small amount of parallel data (about 60000 parallel sentences), allowing us to offer a Very Low Resource Supervised MT task as well. Six primary systems participated in the unsupervised shared task, two of these systems used additional data beyond the data released by the organizers. Ten primary systems participated in the very low resource supervised task. The paper discusses the background, presents the tasks and results, and discusses best practices for the future.</abstract>
      <url hash="97267138">2020.wmt-1.80</url>
      <video href="https://slideslive.com/38939667"/>
      <bibkey>fraser-2020-findings</bibkey>
    </paper>
    <paper id="81">
      <title>Cross-Lingual Transformers for Neural Automatic Post-Editing</title>
      <author><first>Dongjun</first><last>Lee</last></author>
      <pages>772–776</pages>
      <abstract>In this paper, we describe the Bering Lab’s submission to the WMT 2020 Shared Task on Automatic Post-Editing (APE). First, we propose a cross-lingual Transformer architecture that takes a concatenation of a source sentence and a machine-translated (MT) sentence as an input to generate the post-edited (PE) output. For further improvement, we mask incorrect or missing words in the PE output based on word-level quality estimation and then predict the actual word for each mask based on the fine-tuned cross-lingual language model (XLM-RoBERTa). Finally, to address the over-correction problem, we select the final output among the PE outputs and the original MT sentence based on a sentence-level quality estimation. When evaluated on the WMT 2020 English-German APE test dataset, our system improves the NMT output by -3.95 and +4.50 in terms of TER and BLEU, respectively.</abstract>
      <url hash="08284caf">2020.wmt-1.81</url>
      <video href="https://slideslive.com/38939547"/>
      <bibkey>lee-2020-cross</bibkey>
    </paper>
    <paper id="82">
      <title><fixed-case>POSTECH</fixed-case>-<fixed-case>ETRI</fixed-case>’s Submission to the <fixed-case>WMT</fixed-case>2020 <fixed-case>APE</fixed-case> Shared Task: Automatic Post-Editing with Cross-lingual Language Model</title>
      <author><first>Jihyung</first><last>Lee</last></author>
      <author><first>WonKee</first><last>Lee</last></author>
      <author><first>Jaehun</first><last>Shin</last></author>
      <author><first>Baikjin</first><last>Jung</last></author>
      <author><first>Young-Kil</first><last>Kim</last></author>
      <author><first>Jong-Hyeok</first><last>Lee</last></author>
      <pages>777–782</pages>
      <abstract>This paper describes POSTECH-ETRI’s submission to WMT2020 for the shared task on automatic post-editing (APE) for 2 language pairs: English-German (En-De) and English-Chinese (En-Zh). We propose APE systems based on a cross-lingual language model, which jointly adopts translation language modeling (TLM) and masked language modeling (MLM) training objectives in the pre-training stage; the APE models then utilize jointly learned language representations between the source language and the target language. In addition, we created 19 million new sythetic triplets as additional training data for our final ensemble model. According to experimental results on the WMT2020 APE development data set, our models showed an improvement over the baseline by TER of -3.58 and a BLEU score of +5.3 for the En-De subtask; and TER of -5.29 and a BLEU score of +7.32 for the En-Zh subtask.</abstract>
      <url hash="f1249c8d">2020.wmt-1.82</url>
      <video href="https://slideslive.com/38939561"/>
      <bibkey>lee-etal-2020-postech</bibkey>
    </paper>
    <paper id="83">
      <title>Noising Scheme for Data Augmentation in Automatic Post-Editing</title>
      <author><first>WonKee</first><last>Lee</last></author>
      <author><first>Jaehun</first><last>Shin</last></author>
      <author><first>Baikjin</first><last>Jung</last></author>
      <author><first>Jihyung</first><last>Lee</last></author>
      <author><first>Jong-Hyeok</first><last>Lee</last></author>
      <pages>783–788</pages>
      <abstract>This paper describes POSTECH’s submission to WMT20 for the shared task on Automatic Post-Editing (APE). Our focus is on increasing the quantity of available APE data to overcome the shortage of human-crafted training data. In our experiment, we implemented a noising module that simulates four types of post-editing errors, and we introduced this module into a Transformer-based multi-source APE model. Our noising module implants errors into texts on the target side of parallel corpora during the training phase to make synthetic MT outputs, increasing the entire number of training samples. We also generated additional training data using the parallel corpora and NMT model that were released for the Quality Estimation task, and we used these data to train our APE model. Experimental results on the WMT20 English-German APE data set show improvements over the baseline in terms of both the TER and BLEU scores: our primary submission achieved an improvement of -3.15 TER and +4.01 BLEU, and our contrastive submission achieved an improvement of -3.34 TER and +4.30 BLEU.</abstract>
      <url hash="e8b56ab5">2020.wmt-1.83</url>
      <video href="https://slideslive.com/38939646"/>
      <bibkey>lee-etal-2020-noising</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/escape">eSCAPE</pwcdataset>
    </paper>
    <paper id="84">
      <title><fixed-case>A</fixed-case>libaba’s Submission for the <fixed-case>WMT</fixed-case> 2020 <fixed-case>APE</fixed-case> Shared Task: Improving Automatic Post-Editing with Pre-trained Conditional Cross-Lingual <fixed-case>BERT</fixed-case></title>
      <author><first>Jiayi</first><last>Wang</last></author>
      <author><first>Ke</first><last>Wang</last></author>
      <author><first>Kai</first><last>Fan</last></author>
      <author><first>Yuqi</first><last>Zhang</last></author>
      <author><first>Jun</first><last>Lu</last></author>
      <author><first>Xin</first><last>Ge</last></author>
      <author><first>Yangbin</first><last>Shi</last></author>
      <author><first>Yu</first><last>Zhao</last></author>
      <pages>789–796</pages>
      <abstract>The goal of Automatic Post-Editing (APE) is basically to examine the automatic methods for correcting translation errors generated by an unknown machine translation (MT) system. This paper describes Alibaba’s submissions to the WMT 2020 APE Shared Task for the English-German language pair. We design a two-stage training pipeline. First, a BERT-like cross-lingual language model is pre-trained by randomly masking target sentences alone. Then, an additional neural decoder on the top of the pre-trained model is jointly fine-tuned for the APE task. We also apply an imitation learning strategy to augment a reasonable amount of pseudo APE training data, potentially preventing the model to overfit on the limited real training data and boosting the performance on held-out data. To verify our proposed model and data augmentation, we examine our approach with the well-known benchmarking English-German dataset from the WMT 2017 APE task. The experiment results demonstrate that our system significantly outperforms all other baselines and achieves the state-of-the-art performance. The final results on the WMT 2020 test dataset show that our submission can achieve +5.56 BLEU and -4.57 TER with respect to the official MT baseline.</abstract>
      <url hash="42810c42">2020.wmt-1.84</url>
      <video href="https://slideslive.com/38939622"/>
      <bibkey>wang-etal-2020-alibabas</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/escape">eSCAPE</pwcdataset>
    </paper>
    <paper id="85">
      <title><fixed-case>HW</fixed-case>-<fixed-case>TSC</fixed-case>’s Participation at <fixed-case>WMT</fixed-case> 2020 Automatic Post Editing Shared Task</title>
      <author><first>Hao</first><last>Yang</last></author>
      <author><first>Minghan</first><last>Wang</last></author>
      <author><first>Daimeng</first><last>Wei</last></author>
      <author><first>Hengchao</first><last>Shang</last></author>
      <author><first>Jiaxin</first><last>Guo</last></author>
      <author><first>Zongyao</first><last>Li</last></author>
      <author><first>Lizhi</first><last>Lei</last></author>
      <author><first>Ying</first><last>Qin</last></author>
      <author><first>Shimin</first><last>Tao</last></author>
      <author><first>Shiliang</first><last>Sun</last></author>
      <author><first>Yimeng</first><last>Chen</last></author>
      <pages>797–802</pages>
      <abstract>The paper presents the submission by HW-TSC in the WMT 2020 Automatic Post Editing Shared Task. We participate in the English-German and English-Chinese language pairs. Our system is built based on the Transformer pre-trained on WMT 2019 and WMT 2020 News Translation corpora, and fine-tuned on the APE corpus. Bottleneck Adapter Layers are integrated into the model to prevent over-fitting. We further collect external translations as the augmented MT candidates to improve the performance. The experiment demonstrates that pre-trained NMT models are effective when fine-tuning with the APE corpus of a limited size, and the performance can be further improved with external MT augmentation. Our system achieves competitive results on both directions in the final evaluation.</abstract>
      <url hash="c203155a">2020.wmt-1.85</url>
      <video href="https://slideslive.com/38939570"/>
      <bibkey>yang-etal-2020-hw</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/escape">eSCAPE</pwcdataset>
    </paper>
    <paper id="86">
      <title><fixed-case>LIMSI</fixed-case> @ <fixed-case>WMT</fixed-case> 2020</title>
      <author><first>Sadaf</first><last>Abdul Rauf</last></author>
      <author><first>José Carlos</first><last>Rosales Núñez</last></author>
      <author><first>Minh Quang</first><last>Pham</last></author>
      <author><first>François</first><last>Yvon</last></author>
      <pages>803–812</pages>
      <abstract>This paper describes LIMSI’s submissions to the translation shared tasks at WMT’20. This year we have focused our efforts on the biomedical translation task, developing a resource-heavy system for the translation of medical abstracts from English into French, using back-translated texts, terminological resources as well as multiple pre-processing pipelines, including pre-trained representations. Systems were also prepared for the robustness task for translating from English into German; for this large-scale task we developed multi-domain, noise-robust, translation systems aim to handle the two test conditions: zero-shot and few-shot domain adaptation.</abstract>
      <url hash="02a6bd30">2020.wmt-1.86</url>
      <video href="https://slideslive.com/38939618"/>
      <bibkey>abdul-rauf-etal-2020-limsi</bibkey>
    </paper>
    <paper id="87">
      <title>Elhuyar submission to the Biomedical Translation Task 2020 on terminology and abstracts translation</title>
      <author><first>Ander</first><last>Corral</last></author>
      <author><first>Xabier</first><last>Saralegi</last></author>
      <pages>813–819</pages>
      <abstract>This article describes the systems submitted by Elhuyar to the 2020 Biomedical Translation Shared Task, specifically the systems presented in the subtasks of terminology translation for English-Basque and abstract translation for English-Basque and English-Spanish. In all cases a Transformer architecture was chosen and we studied different strategies to combine open domain data with biomedical domain data for building the training corpora. For the English-Basque pair, given the scarcity of parallel corpora in the biomedical domain, we set out to create domain training data in a synthetic way. The systems presented in the terminology and abstract translation subtasks for the English-Basque language pair ranked first in their respective tasks among four participants, achieving 0.78 accuracy for terminology translation and a BLEU of 0.1279 for the translation of abstracts. In the abstract translation task for the English-Spanish pair our team ranked second (BLEU=0.4498) in the case of OK sentences.</abstract>
      <url hash="1120ae61">2020.wmt-1.87</url>
      <video href="https://slideslive.com/38939591"/>
      <bibkey>corral-saralegi-2020-elhuyar</bibkey>
    </paper>
    <paper id="88">
      <title><fixed-case>Y</fixed-case>ereva<fixed-case>NN</fixed-case>’s Systems for <fixed-case>WMT</fixed-case>20 Biomedical Translation Task: The Effect of Fixing Misaligned Sentence Pairs</title>
      <author><first>Karen</first><last>Hambardzumyan</last></author>
      <author><first>Hovhannes</first><last>Tamoyan</last></author>
      <author><first>Hrant</first><last>Khachatrian</last></author>
      <pages>820–825</pages>
      <abstract>This report describes YerevaNN’s neural machine translation systems and data processing pipelines developed for WMT20 biomedical translation task. We provide systems for English-Russian and English-German language pairs. For the English-Russian pair, our submissions achieve the best BLEU scores, with en<tex-math>\rightarrow</tex-math>ru direction outperforming the other systems by a significant margin. We explain most of the improvements by our heavy data preprocessing pipeline which attempts to fix poorly aligned sentences in the parallel data.</abstract>
      <url hash="76c0c7c9">2020.wmt-1.88</url>
      <attachment type="OptionalSupplementaryMaterial" hash="c2ea9401">2020.wmt-1.88.OptionalSupplementaryMaterial.tgz</attachment>
      <video href="https://slideslive.com/38939644"/>
      <bibkey>hambardzumyan-etal-2020-yerevanns</bibkey>
    </paper>
    <paper id="89">
      <title>Pretrained Language Models and Backtranslation for <fixed-case>E</fixed-case>nglish-<fixed-case>B</fixed-case>asque Biomedical Neural Machine Translation</title>
      <author><first>Inigo</first><last>Jauregi Unanue</last></author>
      <author><first>Massimo</first><last>Piccardi</last></author>
      <pages>826–832</pages>
      <abstract>This paper describes the machine translation systems proposed by the University of Technology Sydney Natural Language Processing (UTS_NLP) team for the WMT20 English-Basque biomedical translation tasks. Due to the limited parallel corpora available, we have proposed to train a BERT-fused NMT model that leverages the use of pretrained language models. Furthermore, we have augmented the training corpus by backtranslating monolingual data. Our experiments show that NMT models in low-resource scenarios can benefit from combining these two training techniques, with improvements of up to 6.16 BLEU percentual points in the case of biomedical abstract translations.</abstract>
      <url hash="85596cf6">2020.wmt-1.89</url>
      <video href="https://slideslive.com/38939562"/>
      <bibkey>jauregi-unanue-piccardi-2020-pretrained</bibkey>
    </paper>
    <paper id="90">
      <title>Lite Training Strategies for <fixed-case>P</fixed-case>ortuguese-<fixed-case>E</fixed-case>nglish and <fixed-case>E</fixed-case>nglish-<fixed-case>P</fixed-case>ortuguese Translation</title>
      <author><first>Alexandre</first><last>Lopes</last></author>
      <author><first>Rodrigo</first><last>Nogueira</last></author>
      <author><first>Roberto</first><last>Lotufo</last></author>
      <author><first>Helio</first><last>Pedrini</last></author>
      <pages>833–840</pages>
      <abstract>Despite the widespread adoption of deep learning for machine translation, it is still expensive to develop high-quality translation models. In this work, we investigate the use of pre-trained models, such as T5 for Portuguese-English and English-Portuguese translation tasks using low-cost hardware. We explore the use of Portuguese and English pre-trained language models and propose an adaptation of the English tokenizer to represent Portuguese characters, such as diaeresis, acute and grave accents. We compare our models to the Google Translate API and MarianMT on a subset of the ParaCrawl dataset, as well as to the winning submission to the WMT19 Biomedical Translation Shared Task. We also describe our submission to the WMT20 Biomedical Translation Shared Task. Our results show that our models have a competitive performance to state-of-the-art models while being trained on modest hardware (a single 8GB gaming GPU for nine days). Our data, models and code are available in our GitHub repository.</abstract>
      <url hash="6264f17a">2020.wmt-1.90</url>
      <video href="https://slideslive.com/38939645"/>
      <bibkey>lopes-etal-2020-lite</bibkey>
      <pwccode url="https://github.com/unicamp-dl/Lite-T5-Translation" additional="false">unicamp-dl/Lite-T5-Translation</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/capes">capes</pwcdataset>
    </paper>
    <paper id="91">
      <title>The <fixed-case>ADAPT</fixed-case>’s Submissions to the <fixed-case>WMT</fixed-case>20 Biomedical Translation Task</title>
      <author><first>Prashant</first><last>Nayak</last></author>
      <author><first>Rejwanul</first><last>Haque</last></author>
      <author><first>Andy</first><last>Way</last></author>
      <pages>841–848</pages>
      <abstract>This paper describes the ADAPT Centre’s submissions to the WMT20 Biomedical Translation Shared Task for English-to-Basque. We present the machine translation (MT) systems that were built to translate scientific abstracts and terms from biomedical terminologies, and using the state-of-the-art neural MT (NMT) model: Transformer. In order to improve our baseline NMT system, we employ a number of methods, e.g. “pseudo” parallel data selection, monolingual data selection for synthetic corpus creation, mining monolingual sentences for adapting our NMT systems to this task, hyperparameters search for Transformer in lowresource scenarios. Our experiments show that systematic addition of the aforementioned techniques to the baseline yields an excellent performance in the English-to-Basque translation task.</abstract>
      <url hash="e9f0540b">2020.wmt-1.91</url>
      <video href="https://slideslive.com/38939617"/>
      <bibkey>nayak-etal-2020-adapts</bibkey>
    </paper>
    <paper id="92">
      <title><fixed-case>FJWU</fixed-case> participation for the <fixed-case>WMT</fixed-case>20 Biomedical Translation Task</title>
      <author><first>Sumbal</first><last>Naz</last></author>
      <author><first>Sadaf</first><last>Abdul Rauf</last></author>
      <author><first>Noor-e-</first><last>Hira</last></author>
      <author><first>Sami</first><last>Ul Haq</last></author>
      <pages>849–856</pages>
      <abstract>This paper reports system descriptions for FJWU-NRPU team for participation in the WMT20 Biomedical shared translation task. We focused our submission on exploring the effects of adding in-domain corpora extracted from various out-of-domain sources. Systems were built for French to English using in-domain corpora through fine tuning and selective data training. We further explored BERT based models specifically with focus on effect of domain adaptive subword units.</abstract>
      <url hash="23b56092">2020.wmt-1.92</url>
      <video href="https://slideslive.com/38939656"/>
      <bibkey>naz-etal-2020-fjwu</bibkey>
    </paper>
    <paper id="93">
      <title>Huawei’s Submissions to the <fixed-case>WMT</fixed-case>20 Biomedical Translation Task</title>
      <author><first>Wei</first><last>Peng</last></author>
      <author><first>Jianfeng</first><last>Liu</last></author>
      <author><first>Minghan</first><last>Wang</last></author>
      <author><first>Liangyou</first><last>Li</last></author>
      <author><first>Xupeng</first><last>Meng</last></author>
      <author><first>Hao</first><last>Yang</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <pages>857–861</pages>
      <abstract>This paper describes Huawei’s submissions to the WMT20 biomedical translation shared task. Apart from experimenting with finetuning on domain-specific bitexts, we explore effects of in-domain dictionaries on enhancing cross-domain neural machine translation performance. We utilize a transfer learning strategy through pre-trained machine translation models and extensive scope of engineering endeavors. Four of our ten submissions achieve state-of-the-art performance according to the official automatic evaluation results, namely translation directions on English&lt;-&gt;French, English-&gt;German and English-&gt;Italian.</abstract>
      <url hash="0f9dd278">2020.wmt-1.93</url>
      <video href="https://slideslive.com/38939576"/>
      <bibkey>peng-etal-2020-huaweis</bibkey>
    </paper>
    <paper id="94">
      <title>Addressing Exposure Bias With Document Minimum Risk Training: <fixed-case>C</fixed-case>ambridge at the <fixed-case>WMT</fixed-case>20 Biomedical Translation Task</title>
      <author><first>Danielle</first><last>Saunders</last></author>
      <author id="bill-byrne"><first>Bill</first><last>Byrne</last></author>
      <pages>862–869</pages>
      <abstract>The 2020 WMT Biomedical translation task evaluated Medline abstract translations. This is a small-domain translation task, meaning limited relevant training data with very distinct style and vocabulary. Models trained on such data are susceptible to exposure bias effects, particularly when training sentence pairs are imperfect translations of each other. This can result in poor behaviour during inference if the model learns to neglect the source sentence. The UNICAM entry addresses this problem during fine-tuning using a robust variant on Minimum Risk Training. We contrast this approach with data-filtering to remove ‘problem’ training examples. Under MRT fine-tuning we obtain good results for both directions of English-German and English-Spanish biomedical translation. In particular we achieve the best English-to-Spanish translation result and second-best Spanish-to-English result, despite using only single models with no ensembling.</abstract>
      <url hash="dcb9d8e6">2020.wmt-1.94</url>
      <video href="https://slideslive.com/38939583"/>
      <bibkey>saunders-byrne-2020-addressing</bibkey>
    </paper>
    <paper id="95">
      <title><fixed-case>U</fixed-case>o<fixed-case>S</fixed-case> Participation in the <fixed-case>WMT</fixed-case>20 Translation of Biomedical Abstracts</title>
      <author><first>Felipe</first><last>Soares</last></author>
      <author><first>Delton</first><last>Vaz</last></author>
      <pages>870–874</pages>
      <abstract>This paper describes the machine translation systems developed by the University of Sheffield (UoS) team for the biomedical translation shared task of WMT20. Our system is based on a Transformer model with TensorFlow Model Garden toolkit. We participated in ten translation directions for the English/Spanish, English/Portuguese, English/Russian, English/Italian, and English/French language pairs. To create our training data, we concatenated several parallel corpora, both from in-domain and out-of-domain sources.</abstract>
      <url hash="454b70e5">2020.wmt-1.95</url>
      <video href="https://slideslive.com/38939632"/>
      <bibkey>soares-vaz-2020-uos</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/parapat">ParaPat</pwcdataset>
    </paper>
    <paper id="96">
      <title>Ixamed’s submission description for <fixed-case>WMT</fixed-case>20 Biomedical shared task: benefits and limitations of using terminologies for domain adaptation</title>
      <author><first>Xabier</first><last>Soto</last></author>
      <author><first>Olatz</first><last>Perez-de-Viñaspre</last></author>
      <author><first>Gorka</first><last>Labaka</last></author>
      <author><first>Maite</first><last>Oronoz</last></author>
      <pages>875–880</pages>
      <abstract>In this paper we describe the systems developed at Ixa for our participation in WMT20 Biomedical shared task in three language pairs, en-eu, en-es and es-en. When defining our approach, we have put the focus on making an efficient use of corpora recently compiled for training Machine Translation (MT) systems to translate Covid-19 related text, as well as reusing previously compiled corpora and developed systems for biomedical or clinical domain. Regarding the techniques used, we base on the findings from our previous works for translating clinical texts into Basque, making use of clinical terminology for adapting the MT systems to the clinical domain. However, after manually inspecting some of the outputs generated by our systems, for most of the submissions we end up using the system trained only with the basic corpus, since the systems including the clinical terminologies generated outputs shorter in length than the corresponding references. Thus, we present simple baselines for translating abstracts between English and Spanish (en/es); while for translating abstracts and terms from English into Basque (en-eu), we concatenate the best en-es system for each kind of text with our es-eu system. We present automatic evaluation results in terms of BLEU scores, and analyse the effect of including clinical terminology on the average sentence length of the generated outputs. Following the recent recommendations for a responsible use of GPUs for NLP research, we include an estimation of the generated CO2 emissions, based on the power consumed for training the MT systems.</abstract>
      <url hash="7c2d0baf">2020.wmt-1.96</url>
      <video href="https://slideslive.com/38939549"/>
      <bibkey>soto-etal-2020-ixameds</bibkey>
    </paper>
    <paper id="97">
      <title>Tencent <fixed-case>AI</fixed-case> Lab Machine Translation Systems for the <fixed-case>WMT</fixed-case>20 Biomedical Translation Task</title>
      <author><first>Xing</first><last>Wang</last></author>
      <author><first>Zhaopeng</first><last>Tu</last></author>
      <author><first>Longyue</first><last>Wang</last></author>
      <author><first>Shuming</first><last>Shi</last></author>
      <pages>881–886</pages>
      <abstract>This paper describes the Tencent AI Lab submission of the WMT2020 shared task on biomedical translation in four language directions: German&lt;-&gt;English, English&lt;-&gt;German, Chinese&lt;-&gt;English and English&lt;-&gt;Chinese. We implement our system with model ensemble technique on different transformer architectures (Deep, Hybrid, Big, Large Transformers). To enlarge the in-domain bilingual corpus, we use back-translation of monolingual in-domain data in the target language as additional in-domain training data. Our systems in German-&gt;English and English-&gt;German are ranked 1st and 3rd respectively according to the official evaluation results in terms of BLEU scores.</abstract>
      <url hash="42b4290a">2020.wmt-1.97</url>
      <video href="https://slideslive.com/38939664"/>
      <bibkey>wang-etal-2020-tencent-ai</bibkey>
      <pwccode url="https://github.com/hsing-wang/wmt2020_biomedical" additional="false">hsing-wang/wmt2020_biomedical</pwccode>
    </paper>
    <paper id="98">
      <title><fixed-case>P</fixed-case>ar<fixed-case>BLEU</fixed-case>: Augmenting Metrics with Automatic Paraphrases for the <fixed-case>WMT</fixed-case>’20 Metrics Shared Task</title>
      <author><first>Rachel</first><last>Bawden</last></author>
      <author><first>Biao</first><last>Zhang</last></author>
      <author><first>Andre</first><last>Tättar</last></author>
      <author><first>Matt</first><last>Post</last></author>
      <pages>887–894</pages>
      <abstract>We describe parBLEU, parCHRF++, and parESIM, which augment baseline metrics with automatically generated paraphrases produced by PRISM (Thompson and Post, 2020a), a multilingual neural machine translation system. We build on recent work studying how to improve BLEU by using diverse automatically paraphrased references (Bawden et al., 2020), extending experiments to the multilingual setting for the WMT2020 metrics shared task and for three base metrics. We compare their capacity to exploit up to 100 additional synthetic references. We find that gains are possible when using additional, automatically paraphrased references, although they are not systematic. However, segment-level correlations, particularly into English, are improved for all three metrics and even with higher numbers of paraphrased references.</abstract>
      <url hash="f57a3d89">2020.wmt-1.98</url>
      <video href="https://slideslive.com/38939673"/>
      <bibkey>bawden-etal-2020-parbleu</bibkey>
    </paper>
    <paper id="99">
      <title>Extended Study on Using Pretrained Language Models and <fixed-case>Y</fixed-case>i<fixed-case>S</fixed-case>i-1 for Machine Translation Evaluation</title>
      <author><first>Chi-kiu</first><last>Lo</last></author>
      <pages>895–902</pages>
      <abstract>We present an extended study on using pretrained language models and YiSi-1 for machine translation evaluation. Although the recently proposed contextual embedding based metrics, YiSi-1, significantly outperform BLEU and other metrics in correlating with human judgment on translation quality, we have yet to understand the full strength of using pretrained language models for machine translation evaluation. In this paper, we study YiSi-1’s correlation with human translation quality judgment by varying three major attributes (which architecture; which inter- mediate layer; whether it is monolingual or multilingual) of the pretrained language mod- els. Results of the study show further improvements over YiSi-1 on the WMT 2019 Metrics shared task. We also describe the pretrained language model we trained for evaluating Inuktitut machine translation output.</abstract>
      <url hash="d70d9690">2020.wmt-1.99</url>
      <video href="https://slideslive.com/38939652"/>
      <bibkey>lo-2020-extended</bibkey>
    </paper>
    <paper id="100">
      <title>Machine Translation Reference-less Evaluation using <fixed-case>Y</fixed-case>i<fixed-case>S</fixed-case>i-2 with Bilingual Mappings of Massive Multilingual Language Model</title>
      <author><first>Chi-kiu</first><last>Lo</last></author>
      <author><first>Samuel</first><last>Larkin</last></author>
      <pages>903–910</pages>
      <abstract>We present a study on using YiSi-2 with massive multilingual pretrained language models for machine translation (MT) reference-less evaluation. Aiming at finding better semantic representation for semantic MT evaluation, we first test YiSi-2 with contextual embed- dings extracted from different layers of two different pretrained models, multilingual BERT and XLM-RoBERTa. We also experiment with learning bilingual mappings that trans- form the vector subspace of the source language to be closer to that of the target language in the pretrained model to obtain more accurate cross-lingual semantic similarity representations. Our results show that YiSi-2’s correlation with human direct assessment on translation quality is greatly improved by replacing multilingual BERT with XLM-RoBERTa and projecting the source embeddings into the tar- get embedding space using a cross-lingual lin- ear projection (CLP) matrix learnt from a small development set.</abstract>
      <url hash="6e1566de">2020.wmt-1.100</url>
      <video href="https://slideslive.com/38939653"/>
      <bibkey>lo-larkin-2020-machine</bibkey>
    </paper>
    <paper id="101">
      <title>Unbabel’s Participation in the <fixed-case>WMT</fixed-case>20 Metrics Shared Task</title>
      <author><first>Ricardo</first><last>Rei</last></author>
      <author><first>Craig</first><last>Stewart</last></author>
      <author><first>Ana C</first><last>Farinha</last></author>
      <author><first>Alon</first><last>Lavie</last></author>
      <pages>911–920</pages>
      <abstract>We present the contribution of the Unbabel team to the WMT 2020 Shared Task on Metrics. We intend to participate on the segmentlevel, document-level and system-level tracks on all language pairs, as well as the “QE as a Metric” track. Accordingly, we illustrate results of our models in these tracks with reference to test sets from the previous year. Our submissions build upon the recently proposed COMET framework: we train several estimator models to regress on different humangenerated quality scores and a novel ranking model trained on relative ranks obtained from Direct Assessments. We also propose a simple technique for converting segment-level predictions into a document-level score. Overall, our systems achieve strong results for all language pairs on previous test sets and in many cases set a new state-of-the-art.</abstract>
      <url hash="362e34a3">2020.wmt-1.101</url>
      <attachment type="OptionalSupplementaryMaterial" hash="828c2126">2020.wmt-1.101.OptionalSupplementaryMaterial.pdf</attachment>
      <video href="https://slideslive.com/38939604"/>
      <bibkey>rei-etal-2020-unbabels</bibkey>
    </paper>
    <paper id="102">
      <title>Learning to Evaluate Translation Beyond <fixed-case>E</fixed-case>nglish: <fixed-case>BLEURT</fixed-case> Submissions to the <fixed-case>WMT</fixed-case> Metrics 2020 Shared Task</title>
      <author><first>Thibault</first><last>Sellam</last></author>
      <author><first>Amy</first><last>Pu</last></author>
      <author><first>Hyung Won</first><last>Chung</last></author>
      <author><first>Sebastian</first><last>Gehrmann</last></author>
      <author><first>Qijun</first><last>Tan</last></author>
      <author><first>Markus</first><last>Freitag</last></author>
      <author><first>Dipanjan</first><last>Das</last></author>
      <author><first>Ankur</first><last>Parikh</last></author>
      <pages>921–927</pages>
      <abstract>The quality of machine translation systems has dramatically improved over the last decade, and as a result, evaluation has become an increasingly challenging problem. This paper describes our contribution to the WMT 2020 Metrics Shared Task, the main benchmark for automatic evaluation of translation. We make several submissions based on BLEURT, a previously published which uses transfer learning. We extend the metric beyond English and evaluate it on 14 language pairs for which fine-tuning data is available, as well as 4 “zero-shot” language pairs, for which we have no labelled examples. Additionally, we focus on English to German and demonstrate how to combine BLEURT’s predictions with those of YiSi and use alternative reference translations to enhance the performance. Empirical results show that the models achieve competitive results on the WMT Metrics 2019 Shared Task, indicating their promise for the 2020 edition.</abstract>
      <url hash="71669019">2020.wmt-1.102</url>
      <video href="https://slideslive.com/38939681"/>
      <bibkey>sellam-etal-2020-learning</bibkey>
    </paper>
    <paper id="103">
      <title>Towards a Better Evaluation of Metrics for Machine Translation</title>
      <author><first>Peter</first><last>Stanchev</last></author>
      <author><first>Weiyue</first><last>Wang</last></author>
      <author><first>Hermann</first><last>Ney</last></author>
      <pages>928–933</pages>
      <abstract>An important aspect of machine translation is its evaluation, which can be achieved through the use of a variety of metrics. To compare these metrics, the workshop on statistical machine translation annually evaluates metrics based on their correlation with human judgement. Over the years, methods for measuring correlation with humans have changed, but little research has been performed on what the optimal methods for acquiring human scores are and how human correlation can be measured. In this work, the methods for evaluating metrics at both system- and segment-level are analyzed in detail and their shortcomings are pointed out.</abstract>
      <url hash="b2c88bbf">2020.wmt-1.103</url>
      <video href="https://slideslive.com/38939548"/>
      <bibkey>stanchev-etal-2020-towards</bibkey>
    </paper>
    <paper id="104">
      <title>Incorporate Semantic Structures into Machine Translation Evaluation via <fixed-case>UCCA</fixed-case></title>
      <author><first>Jin</first><last>Xu</last></author>
      <author><first>Yinuo</first><last>Guo</last></author>
      <author><first>Junfeng</first><last>Hu</last></author>
      <pages>934–939</pages>
      <abstract>Copying mechanism has been commonly used in neural paraphrasing networks and other text generation tasks, in which some important words in the input sequence are preserved in the output sequence. Similarly, in machine translation, we notice that there are certain words or phrases appearing in all good translations of one source text, and these words tend to convey important semantic information. Therefore, in this work, we define words carrying important semantic meanings in sentences as semantic core words. Moreover, we propose an MT evaluation approach named Semantically Weighted Sentence Similarity (SWSS). It leverages the power of UCCA to identify semantic core words, and then calculates sentence similarity scores on the overlap of semantic core words. Experimental results show that SWSS can consistently improve the performance of popular MT evaluation metrics which are based on lexical similarity.</abstract>
      <url hash="23fd41d7">2020.wmt-1.104</url>
      <video href="https://slideslive.com/38939565"/>
      <bibkey>xu-etal-2020-incorporate</bibkey>
    </paper>
    <paper id="105">
      <title>Filtering Noisy Parallel Corpus using Transformers with Proxy Task Learning</title>
      <author><first>Haluk</first><last>Açarçiçek</last></author>
      <author><first>Talha</first><last>Çolakoğlu</last></author>
      <author><first>Pınar Ece</first><last>Aktan Hatipoğlu</last></author>
      <author><first>Chong Hsuan</first><last>Huang</last></author>
      <author><first>Wei</first><last>Peng</last></author>
      <pages>940–946</pages>
      <abstract>This paper illustrates Huawei’s submission to the WMT20 low-resource parallel corpus filtering shared task. Our approach focuses on developing a proxy task learner on top of a transformer-based multilingual pre-trained language model to boost the filtering capability for noisy parallel corpora. Such a supervised task also helps us to iterate much more quickly than using an existing neural machine translation system to perform the same task. After performing empirical analyses of the finetuning task, we benchmark our approach by comparing the results with past years’ state-of-theart records. This paper wraps up with a discussion of limitations and future work. The scripts for this study will be made publicly available.</abstract>
      <url hash="5dc0edc3">2020.wmt-1.105</url>
      <video href="https://slideslive.com/38939606"/>
      <bibkey>acarcicek-etal-2020-filtering</bibkey>
      <pwccode url="https://github.com/wpti/proxy-filter" additional="false">wpti/proxy-filter</pwccode>
    </paper>
    <paper id="106">
      <title>Score Combination for Improved Parallel Corpus Filtering for Low Resource Conditions</title>
      <author><first>Muhammad</first><last>ElNokrashy</last></author>
      <author><first>Amr</first><last>Hendy</last></author>
      <author><first>Mohamed</first><last>Abdelghaffar</last></author>
      <author><first>Mohamed</first><last>Afify</last></author>
      <author><first>Ahmed</first><last>Tawfik</last></author>
      <author><first>Hany</first><last>Hassan Awadalla</last></author>
      <pages>947–951</pages>
      <abstract>This paper presents the description of our submission to WMT20 sentence filtering task. We combine scores from custom LASER built for each source language, a classifier built to distinguish positive and negative pairs and the original scores provided with the task. For the mBART setup, provided by the organizers, our method shows 7% and 5% relative improvement, over the baseline, in sacreBLEU score on the test set for Pashto and Khmer respectively.</abstract>
      <url hash="f7f93dee">2020.wmt-1.106</url>
      <video href="https://slideslive.com/38939612"/>
      <bibkey>elnokrashy-etal-2020-score</bibkey>
    </paper>
    <paper id="107">
      <title>Bicleaner at <fixed-case>WMT</fixed-case> 2020: <fixed-case>U</fixed-case>niversitat d’Alacant-Prompsit’s submission to the parallel corpus filtering shared task</title>
      <author><first>Miquel</first><last>Esplà-Gomis</last></author>
      <author><first>Víctor M.</first><last>Sánchez-Cartagena</last></author>
      <author><first>Jaume</first><last>Zaragoza-Bernabeu</last></author>
      <author><first>Felipe</first><last>Sánchez-Martínez</last></author>
      <pages>952–958</pages>
      <abstract>This paper describes the joint submission of Universitat d’Alacant and Prompsit Language Engineering to the WMT 2020 shared task on parallel corpus filtering. Our submission, based on the free/open-source tool Bicleaner, enhances it with Extremely Randomised Trees and lexical similarity features that account for the frequency of the words in the parallel sentences to determine if two sentences are parallel. To train this classifier we used the clean corpora provided for the task and synthetic noisy parallel sentences. In addition we re-score the output of Bicleaner using character-level language models and n-gram saturation.</abstract>
      <url hash="910c28a2">2020.wmt-1.107</url>
      <video href="https://slideslive.com/38939569"/>
      <bibkey>espla-gomis-etal-2020-bicleaner</bibkey>
    </paper>
    <paper id="108">
      <title>An exploratory approach to the Parallel Corpus Filtering shared task <fixed-case>WMT</fixed-case>20</title>
      <author><first>Ankur</first><last>Kejriwal</last></author>
      <author><first>Philipp</first><last>Koehn</last></author>
      <pages>959–965</pages>
      <abstract>In this document we describe our submission to the parallel corpus filtering task using multilingual word embedding, language models and an ensemble of pre and post filtering rules. We use the norms of embedding and the perplexities of language models along with pre/post filtering rules to complement the LASER baseline scores and in the end get an improvement on the dev set in both language pairs.</abstract>
      <url hash="86dd458a">2020.wmt-1.108</url>
      <video href="https://slideslive.com/38939649"/>
      <bibkey>kejriwal-koehn-2020-exploratory</bibkey>
    </paper>
    <paper id="109">
      <title>Dual Conditional Cross Entropy Scores and <fixed-case>LASER</fixed-case> Similarity Scores for the <fixed-case>WMT</fixed-case>20 Parallel Corpus Filtering Shared Task</title>
      <author><first>Felicia</first><last>Koerner</last></author>
      <author><first>Philipp</first><last>Koehn</last></author>
      <pages>966–971</pages>
      <abstract>This paper describes our submission to the WMT20 Parallel Corpus Filtering and Alignment for Low-Resource Conditions Shared Task. This year’s corpora are noisy Khmer-English and Pashto-English, with 58.3 million and 11.6 million words respectively (English token count). Our submission focuses on filtering Pashto-English, building on previously successful methods to produce two sets of scores: LASER_LM, a combination of the LASER similarity scores provided in the shared task and perplexity scores from language models, and DCCEF_DUP, dual conditional cross entropy scores combined with a duplication penalty. We improve slightly on the LASER similarity score and find that the provided clean data can successfully be supplemented with a subsampled set of the noisy data, effectively increasing the training data for the models used for dual conditional cross entropy scoring.</abstract>
      <url hash="c355dbb7">2020.wmt-1.109</url>
      <video href="https://slideslive.com/38939631"/>
      <bibkey>koerner-koehn-2020-dual</bibkey>
    </paper>
    <paper id="110">
      <title>Improving Parallel Data Identification using Iteratively Refined Sentence Alignments and Bilingual Mappings of Pre-trained Language Models</title>
      <author><first>Chi-kiu</first><last>Lo</last></author>
      <author><first>Eric</first><last>Joanis</last></author>
      <pages>972–978</pages>
      <abstract>The National Research Council of Canada’s team submissions to the parallel corpus filtering task at the Fifth Conference on Machine Translation are based on two key components: (1) iteratively refined statistical sentence alignments for extracting sentence pairs from document pairs and (2) a crosslingual semantic textual similarity metric based on a pretrained multilingual language model, XLM-RoBERTa, with bilingual mappings learnt from a minimal amount of clean parallel data for scoring the parallelism of the extracted sentence pairs. The translation quality of the neural machine translation systems trained and fine-tuned on the parallel data extracted by our submissions improved significantly when compared to the organizers’ LASER-based baseline, a sentence-embedding method that worked well last year. For re-aligning the sentences in the document pairs (component 1), our statistical approach has outperformed the current state-of-the-art neural approach in this low-resource context.</abstract>
      <url hash="ac7c434c">2020.wmt-1.110</url>
      <video href="https://slideslive.com/38939658"/>
      <bibkey>lo-joanis-2020-improving</bibkey>
    </paper>
    <paper id="111">
      <title><fixed-case>A</fixed-case>libaba Submission to the <fixed-case>WMT</fixed-case>20 Parallel Corpus Filtering Task</title>
      <author><first>Jun</first><last>Lu</last></author>
      <author><first>Xin</first><last>Ge</last></author>
      <author><first>Yangbin</first><last>Shi</last></author>
      <author><first>Yuqi</first><last>Zhang</last></author>
      <pages>979–984</pages>
      <abstract>This paper describes the Alibaba Machine Translation Group submissions to the WMT 2020 Shared Task on Parallel Corpus Filtering and Alignment. In the filtering task, three main methods are applied to evaluate the quality of the parallel corpus, i.e. a) Dual Bilingual GPT-2 model, b) Dual Conditional Cross-Entropy Model and c) IBM word alignment model. The scores of these models are combined by using a positive-unlabeled (PU) learning model and a brute-force search to obtain additional gains. Besides, a few simple but efficient rules are adopted to evaluate the quality and the diversity of the corpus. In the alignment-filtering task, the extraction pipeline of bilingual sentence pairs includes the following steps: bilingual lexicon mining, language identification, sentence segmentation and sentence alignment. The final result shows that, in both filtering and alignment tasks, our system significantly outperforms the LASER-based system.</abstract>
      <url hash="871c51cf">2020.wmt-1.111</url>
      <video href="https://slideslive.com/38939580"/>
      <bibkey>lu-etal-2020-alibaba</bibkey>
    </paper>
    <paper id="112">
      <title>Volctrans Parallel Corpus Filtering System for <fixed-case>WMT</fixed-case> 2020</title>
      <author><first>Runxin</first><last>Xu</last></author>
      <author><first>Zhuo</first><last>Zhi</last></author>
      <author><first>Jun</first><last>Cao</last></author>
      <author><first>Mingxuan</first><last>Wang</last></author>
      <author><first>Lei</first><last>Li</last></author>
      <pages>985–990</pages>
      <abstract>In this paper, we describe our submissions to the WMT20 shared task on parallel corpus filtering and alignment for low-resource conditions. The task requires the participants to align potential parallel sentence pairs out of the given document pairs, and score them so that low-quality pairs can be filtered. Our system, Volctrans, is made of two modules, i.e., a mining module and a scoring module. Based on the word alignment model, the mining mod- ule adopts an iterative mining strategy to extract latent parallel sentences. In the scoring module, an XLM-based scorer provides scores, followed by reranking mechanisms and ensemble. Our submissions outperform the baseline by 3.x/2.x and 2.x/2.x for km-en and ps-en on From Scratch/Fine-Tune conditions.</abstract>
      <url hash="98a59e41">2020.wmt-1.112</url>
      <video href="https://slideslive.com/38939544"/>
      <bibkey>xu-etal-2020-volctrans</bibkey>
    </paper>
    <paper id="113">
      <title><fixed-case>PATQUEST</fixed-case>: Papago Translation Quality Estimation</title>
      <author><first>Yujin</first><last>Baek</last></author>
      <author><first>Zae Myung</first><last>Kim</last></author>
      <author><first>Jihyung</first><last>Moon</last></author>
      <author><first>Hyunjoong</first><last>Kim</last></author>
      <author><first>Eunjeong</first><last>Park</last></author>
      <pages>991–998</pages>
      <abstract>This paper describes the system submitted by Papago team for the quality estimation task at WMT 2020. It proposes two key strategies for quality estimation: (1) task-specific pretraining scheme, and (2) task-specific data augmentation. The former focuses on devising learning signals for pretraining that are closely related to the downstream task. We also present data augmentation techniques that simulate the varying levels of errors that the downstream dataset may contain. Thus, our PATQUEST models are exposed to erroneous translations in both stages of task-specific pretraining and finetuning, effectively enhancing their generalization capability. Our submitted models achieve significant improvement over the baselines for Task 1 (Sentence-Level Direct Assessment; EN-DE only), and Task 3 (Document-Level Score).</abstract>
      <url hash="ca5662d5">2020.wmt-1.113</url>
      <video href="https://slideslive.com/38939610"/>
      <bibkey>baek-etal-2020-patquest</bibkey>
    </paper>
    <paper id="114">
      <title><fixed-case>RTM</fixed-case> Ensemble Learning Results at Quality Estimation Task</title>
      <author><first>Ergun</first><last>Biçici</last></author>
      <pages>999–1003</pages>
      <abstract>We obtain new results using referential translation machines (RTMs) with predictions mixed and stacked to obtain a better mixture of experts prediction. We are able to achieve better results than the baseline model in Task 1 subtasks. Our stacking results significantly improve the results on the training sets but decrease the test set results. RTMs can achieve to become the 5th among 13 models in ru-en subtask and 5th in the multilingual track of sentence-level Task 1 based on MAE.</abstract>
      <url hash="ef547830">2020.wmt-1.114</url>
      <video href="https://slideslive.com/38939628"/>
      <bibkey>bicici-2020-rtm</bibkey>
    </paper>
    <paper id="115">
      <title><fixed-case>NJU</fixed-case>’s submission to the <fixed-case>WMT</fixed-case>20 <fixed-case>QE</fixed-case> Shared Task</title>
      <author><first>Qu</first><last>Cui</last></author>
      <author><first>Xiang</first><last>Geng</last></author>
      <author><first>Shujian</first><last>Huang</last></author>
      <author><first>Jiajun</first><last>Chen</last></author>
      <pages>1004–1009</pages>
      <abstract>This paper describes our system of the sentence-level and word-level Quality Estimation Shared Task of WMT20. Our system is based on the QE Brain, and we simply enhance it by injecting noise at the target side. And to obtain the deep bi-directional information, we use a masked language model at the target side instead of two single directional decoders. Meanwhile, we try to use the extra QE data from the WMT17 and WMT19 to improve our system’s performance. Finally, we ensemble the features or the results from different models to get our best results. Our system finished fifth in the end at sentence-level on both EN-ZH and EN-DE language pairs.</abstract>
      <url hash="03b92828">2020.wmt-1.115</url>
      <video href="https://slideslive.com/38939651"/>
      <bibkey>cui-etal-2020-njus</bibkey>
    </paper>
    <paper id="116">
      <title><fixed-case>BERGAMOT</fixed-case>-<fixed-case>LATTE</fixed-case> Submissions for the <fixed-case>WMT</fixed-case>20 Quality Estimation Shared Task</title>
      <author><first>Marina</first><last>Fomicheva</last></author>
      <author><first>Shuo</first><last>Sun</last></author>
      <author><first>Lisa</first><last>Yankovskaya</last></author>
      <author><first>Frédéric</first><last>Blain</last></author>
      <author><first>Vishrav</first><last>Chaudhary</last></author>
      <author><first>Mark</first><last>Fishel</last></author>
      <author><first>Francisco</first><last>Guzmán</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <pages>1010–1017</pages>
      <abstract>This paper presents our submission to the WMT2020 Shared Task on Quality Estimation (QE). We participate in Task and Task 2 focusing on sentence-level prediction. We explore (a) a black-box approach to QE based on pre-trained representations; and (b) glass-box approaches that leverage various indicators that can be extracted from the neural MT systems. In addition to training a feature-based regression model using glass-box quality indicators, we also test whether they can be used to predict MT quality directly with no supervision. We assess our systems in a multi-lingual setting and show that both types of approaches generalise well across languages. Our black-box QE models tied for the winning submission in four out of seven language pairs inTask 1, thus demonstrating very strong performance. The glass-box approaches also performed competitively, representing a light-weight alternative to the neural-based models.</abstract>
      <url hash="57ec1319">2020.wmt-1.116</url>
      <video href="https://slideslive.com/38939630"/>
      <bibkey>fomicheva-etal-2020-bergamot</bibkey>
    </paper>
    <paper id="117">
      <title>The <fixed-case>N</fixed-case>iu<fixed-case>T</fixed-case>rans System for the <fixed-case>WMT</fixed-case>20 Quality Estimation Shared Task</title>
      <author><first>Chi</first><last>Hu</last></author>
      <author><first>Hui</first><last>Liu</last></author>
      <author><first>Kai</first><last>Feng</last></author>
      <author><first>Chen</first><last>Xu</last></author>
      <author><first>Nuo</first><last>Xu</last></author>
      <author><first>Zefan</first><last>Zhou</last></author>
      <author><first>Shiqin</first><last>Yan</last></author>
      <author><first>Yingfeng</first><last>Luo</last></author>
      <author><first>Chenglong</first><last>Wang</last></author>
      <author><first>Xia</first><last>Meng</last></author>
      <author><first>Tong</first><last>Xiao</last></author>
      <author><first>Jingbo</first><last>Zhu</last></author>
      <pages>1018–1023</pages>
      <abstract>This paper describes the submissions of the NiuTrans Team to the WMT 2020 Quality Estimation Shared Task. We participated in all tasks and all language pairs. We explored the combination of transfer learning, multi-task learning and model ensemble. Results on multiple tasks show that deep transformer machine translation models and multilingual pretraining methods significantly improve translation quality estimation performance. Our system achieved remarkable results in multiple level tasks, e.g., our submissions obtained the best results on all tracks in the sentence-level Direct Assessment task.</abstract>
      <url hash="6e215809">2020.wmt-1.117</url>
      <video href="https://slideslive.com/38939601"/>
      <bibkey>hu-etal-2020-niutrans-system</bibkey>
    </paper>
    <paper id="118">
      <title>Two-Phase Cross-Lingual Language Model Fine-Tuning for Machine Translation Quality Estimation</title>
      <author><first>Dongjun</first><last>Lee</last></author>
      <pages>1024–1028</pages>
      <abstract>In this paper, we describe the Bering Lab’s submission to the WMT 2020 Shared Task on Quality Estimation (QE). For word-level and sentence-level translation quality estimation, we fine-tune XLM-RoBERTa, the state-of-the-art cross-lingual language model, with a few additional parameters. Model training consists of two phases. We first pre-train our model on a huge artificially generated QE dataset, and then we fine-tune the model with a human-labeled dataset. When evaluated on the WMT 2020 English-German QE test set, our systems achieve the best result on the target-side of word-level QE and the second best results on the source-side of word-level QE and sentence-level QE among all submissions.</abstract>
      <url hash="4beb1299">2020.wmt-1.118</url>
      <video href="https://slideslive.com/38939546"/>
      <bibkey>lee-2020-two</bibkey>
    </paper>
    <paper id="119">
      <title><fixed-case>IST</fixed-case>-Unbabel Participation in the <fixed-case>WMT</fixed-case>20 Quality Estimation Shared Task</title>
      <author><first>João</first><last>Moura</last></author>
      <author><first>Miguel</first><last>Vera</last></author>
      <author><first>Daan</first><last>van Stigt</last></author>
      <author><first>Fabio</first><last>Kepler</last></author>
      <author><first>André F. T.</first><last>Martins</last></author>
      <pages>1029–1036</pages>
      <abstract>We present the joint contribution of IST and Unbabel to the WMT 2020 Shared Task on Quality Estimation. Our team participated on all tracks (Direct Assessment, Post-Editing Effort, Document-Level), encompassing a total of 14 submissions. Our submitted systems were developed by extending the OpenKiwi framework to a transformer-based predictor-estimator architecture, and to cope with glass-box, uncertainty-based features coming from neural machine translation systems.</abstract>
      <url hash="1dfb6e6c">2020.wmt-1.119</url>
      <video href="https://slideslive.com/38939643"/>
      <bibkey>moura-etal-2020-ist</bibkey>
    </paper>
    <paper id="120">
      <title><fixed-case>TMUOU</fixed-case> Submission for <fixed-case>WMT</fixed-case>20 Quality Estimation Shared Task</title>
      <author><first>Akifumi</first><last>Nakamachi</last></author>
      <author><first>Hiroki</first><last>Shimanaka</last></author>
      <author><first>Tomoyuki</first><last>Kajiwara</last></author>
      <author><first>Mamoru</first><last>Komachi</last></author>
      <pages>1037–1041</pages>
      <abstract>We introduce the TMUOU submission for the WMT20 Quality Estimation Shared Task 1: Sentence-Level Direct Assessment. Our system is an ensemble model of four regression models based on XLM-RoBERTa with language tags. We ranked 4th in Pearson and 2nd in MAE and RMSE on a multilingual track.</abstract>
      <url hash="e77f4cf0">2020.wmt-1.120</url>
      <video href="https://slideslive.com/38939602"/>
      <bibkey>nakamachi-etal-2020-tmuou</bibkey>
    </paper>
    <paper id="121">
      <title><fixed-case>NICT</fixed-case> <fixed-case>K</fixed-case>yoto Submission for the <fixed-case>WMT</fixed-case>’20 Quality Estimation Task: Intermediate Training for Domain and Task Adaptation</title>
      <author><first>Raphael</first><last>Rubino</last></author>
      <pages>1042–1048</pages>
      <abstract>This paper describes the NICT Kyoto submission for the WMT’20 Quality Estimation (QE) shared task. We participated in Task 2: Word and Sentence-level Post-editing Effort, which involved Wikipedia data and two translation directions, namely English-to-German and English-to-Chinese. Our approach is based on multi-task fine-tuned cross-lingual language models (XLM), initially pre-trained and further domain-adapted through intermediate training using the translation language model (TLM) approach complemented with a novel self-supervised learning task which aim is to model errors inherent to machine translation outputs. Results obtained on both word and sentence-level QE show that the proposed intermediate training method is complementary to language model domain adaptation and outperforms the fine-tuning only approach.</abstract>
      <url hash="25fbfc81">2020.wmt-1.121</url>
      <video href="https://slideslive.com/38939556"/>
      <bibkey>rubino-2020-nict</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wikimatrix">WikiMatrix</pwcdataset>
    </paper>
    <paper id="122">
      <title><fixed-case>T</fixed-case>rans<fixed-case>Q</fixed-case>uest at <fixed-case>WMT</fixed-case>2020: Sentence-Level Direct Assessment</title>
      <author><first>Tharindu</first><last>Ranasinghe</last></author>
      <author><first>Constantin</first><last>Orasan</last></author>
      <author><first>Ruslan</first><last>Mitkov</last></author>
      <pages>1049–1055</pages>
      <abstract>This paper presents the team TransQuest’s participation in Sentence-Level Direct Assessment shared task in WMT 2020. We introduce a simple QE framework based on cross-lingual transformers, and we use it to implement and evaluate two different neural architectures. The proposed methods achieve state-of-the-art results surpassing the results obtained by OpenKiwi, the baseline used in the shared task. We further fine tune the QE framework by performing ensemble and data augmentation. Our approach is the winning solution in all of the language pairs according to the WMT 2020 official results.</abstract>
      <url hash="32d15a4f">2020.wmt-1.122</url>
      <video href="https://slideslive.com/38939607"/>
      <bibkey>ranasinghe-etal-2020-transquest-wmt2020</bibkey>
      <pwccode url="https://github.com/tharindudr/transQuest" additional="false">tharindudr/transQuest</pwccode>
    </paper>
    <paper id="123">
      <title><fixed-case>HW</fixed-case>-<fixed-case>TSC</fixed-case>’s Participation at <fixed-case>WMT</fixed-case> 2020 Quality Estimation Shared Task</title>
      <author><first>Minghan</first><last>Wang</last></author>
      <author><first>Hao</first><last>Yang</last></author>
      <author><first>Hengchao</first><last>Shang</last></author>
      <author><first>Daimeng</first><last>Wei</last></author>
      <author><first>Jiaxin</first><last>Guo</last></author>
      <author><first>Lizhi</first><last>Lei</last></author>
      <author><first>Ying</first><last>Qin</last></author>
      <author><first>Shimin</first><last>Tao</last></author>
      <author><first>Shiliang</first><last>Sun</last></author>
      <author><first>Yimeng</first><last>Chen</last></author>
      <author><first>Liangyou</first><last>Li</last></author>
      <pages>1056–1061</pages>
      <abstract>This paper presents our work in the WMT 2020 Word and Sentence-Level Post-Editing Quality Estimation (QE) Shared Task. Our system follows standard Predictor-Estimator architecture, with a pre-trained Transformer as the Predictor, and specific classifiers and regressors as Estimators. We integrate Bottleneck Adapter Layers in the Predictor to improve the transfer learning efficiency and prevent from over-fitting. At the same time, we jointly train the word- and sentence-level tasks with a unified model with multitask learning. Pseudo-PE assisted QE (PEAQE) is proposed, resulting in significant improvements on the performance. Our submissions achieve competitive result in word/sentence-level sub-tasks for both of En-De/Zh language pairs.</abstract>
      <url hash="ecf68449">2020.wmt-1.123</url>
      <video href="https://slideslive.com/38939571"/>
      <bibkey>wang-etal-2020-hw-tscs</bibkey>
    </paper>
    <paper id="124">
      <title>Tencent submission for <fixed-case>WMT</fixed-case>20 Quality Estimation Shared Task</title>
      <author><first>Haijiang</first><last>Wu</last></author>
      <author><first>Zixuan</first><last>Wang</last></author>
      <author><first>Qingsong</first><last>Ma</last></author>
      <author><first>Xinjie</first><last>Wen</last></author>
      <author><first>Ruichen</first><last>Wang</last></author>
      <author><first>Xiaoli</first><last>Wang</last></author>
      <author><first>Yulin</first><last>Zhang</last></author>
      <author><first>Zhipeng</first><last>Yao</last></author>
      <author><first>Siyao</first><last>Peng</last></author>
      <pages>1062–1067</pages>
      <abstract>This paper presents Tencent’s submission to the WMT20 Quality Estimation (QE) Shared Task: Sentence-Level Post-editing Effort for English-Chinese in Task 2. Our system ensembles two architectures, XLM-based and Transformer-based Predictor-Estimator models. For the XLM-based Predictor-Estimator architecture, the predictor produces two types of contextualized token representations, i.e., masked XLM and non-masked XLM; the LSTM-estimator and Transformer-estimator employ two effective strategies, top-K and multi-head attention, to enhance the sentence feature representation. For Transformer-based Predictor-Estimator architecture, we improve a top-performing model by conducting three modifications: using multi-decoding in machine translation module, creating a new model by replacing the transformer-based predictor with XLM-based predictor, and finally integrating two models by a weighted average. Our submission achieves a Pearson correlation of 0.664, ranking first (tied) on English-Chinese.</abstract>
      <url hash="193481f0">2020.wmt-1.124</url>
      <video href="https://slideslive.com/38939609"/>
      <bibkey>wu-etal-2020-tencent-submission</bibkey>
    </paper>
    <paper id="125">
      <title>Zero-Shot Translation Quality Estimation with Explicit Cross-Lingual Patterns</title>
      <author><first>Lei</first><last>Zhou</last></author>
      <author><first>Liang</first><last>Ding</last></author>
      <author><first>Koichi</first><last>Takeda</last></author>
      <pages>1068–1074</pages>
      <abstract>This paper describes our submission of the WMT 2020 Shared Task on Sentence Level Direct Assessment, Quality Estimation (QE). In this study, we empirically reveal the mismatching issue when directly adopting BERTScore (Zhang et al., 2020) to QE. Specifically, there exist lots of mismatching errors between source sentence and translated candidate sentence with token pairwise similarity. In response to this issue, we propose to expose explicit cross lingual patterns, e.g. word alignments and generation score, to our proposed zero-shot models. Experiments show that our proposed QE model with explicit cross-lingual patterns could alleviate the mismatching issue, thereby improving the performance. Encouragingly, our zero-shot QE method could achieve comparable performance with supervised QE method, and even outperforms the supervised counterpart on 2 out of 6 directions. We expect our work could shed light on the zero-shot QE model improvement.</abstract>
      <url hash="bcd58064">2020.wmt-1.125</url>
      <video href="https://slideslive.com/38939669"/>
      <bibkey>zhou-etal-2020-zero</bibkey>
    </paper>
    <paper id="126">
      <title><fixed-case>NLPRL</fixed-case> System for Very Low Resource Supervised Machine Translation</title>
      <author><first>Rupjyoti</first><last>Baruah</last></author>
      <author><first>Rajesh Kumar</first><last>Mundotiya</last></author>
      <author><first>Amit</first><last>Kumar</last></author>
      <author><first>Anil kumar</first><last>Singh</last></author>
      <pages>1075–1078</pages>
      <abstract>This paper describes the results of the system that we used for the WMT20 very low resource (VLR) supervised MT shared task. For our experiments, we use a byte-level version of BPE, which requires a base vocabulary of size 256 only. BPE based models are a kind of sub-word models. Such models try to address the Out of Vocabulary (OOV) word problem by performing word segmentation so that segments correspond to morphological units. They are also reported to work across different languages, especially similar languages due to their sub-word nature. Based on BLEU cased score, our NLPRL systems ranked ninth for HSB to GER and tenth in GER to HSB translation scenario.</abstract>
      <url hash="671b4450">2020.wmt-1.126</url>
      <video href="https://slideslive.com/38939625"/>
      <bibkey>baruah-etal-2020-nlprl</bibkey>
    </paper>
    <paper id="127">
      <title>Low-Resource Translation as Language Modeling</title>
      <author><first>Tucker</first><last>Berckmann</last></author>
      <author><first>Berkan</first><last>Hiziroglu</last></author>
      <pages>1079–1083</pages>
      <abstract>We present our submission to the very low resource supervised machine translation task at WMT20. We use a decoder-only transformer architecture and formulate the translation task as language modeling. To address the low-resource aspect of the problem, we pretrain over a similar language parallel corpus. Then, we employ an intermediate back-translation step before fine-tuning. Finally, we present an analysis of the system’s performance.</abstract>
      <url hash="2f4dadc6">2020.wmt-1.127</url>
      <video href="https://slideslive.com/38939598"/>
      <bibkey>berckmann-hiziroglu-2020-low</bibkey>
    </paper>
    <paper id="128">
      <title><fixed-case>T</fixed-case>he <fixed-case>LMU</fixed-case> <fixed-case>M</fixed-case>unich <fixed-case>S</fixed-case>ystem for the <fixed-case>WMT</fixed-case> 2020 <fixed-case>U</fixed-case>nsupervised <fixed-case>M</fixed-case>achine <fixed-case>T</fixed-case>ranslation <fixed-case>S</fixed-case>hared <fixed-case>T</fixed-case>ask</title>
      <author><first>Alexandra</first><last>Chronopoulou</last></author>
      <author><first>Dario</first><last>Stojanovski</last></author>
      <author><first>Viktor</first><last>Hangya</last></author>
      <author><first>Alexander</first><last>Fraser</last></author>
      <pages>1084–1091</pages>
      <abstract>This paper describes the submission of LMU Munich to the WMT 2020 unsupervised shared task, in two language directions, German↔Upper Sorbian. Our core unsupervised neural machine translation (UNMT) system follows the strategy of Chronopoulou et al. (2020), using a monolingual pretrained language generation model (on German) and fine-tuning it on both German and Upper Sorbian, before initializing a UNMT model, which is trained with online backtranslation. Pseudo-parallel data obtained from an unsupervised statistical machine translation (USMT) system is used to fine-tune the UNMT model. We also apply BPE-Dropout to the low resource (Upper Sorbian) data to obtain a more robust system. We additionally experiment with residual adapters and find them useful in the Upper Sorbian→German direction. We explore sampling during backtranslation and curriculum learning to use SMT translations in a more principled way. Finally, we ensemble our best-performing systems and reach a BLEU score of 32.4 on German→Upper Sorbian and 35.2 on Upper Sorbian→German.</abstract>
      <url hash="a1c52b63">2020.wmt-1.128</url>
      <video href="https://slideslive.com/38939582"/>
      <bibkey>chronopoulou-etal-2020-lmu</bibkey>
      <pwccode url="https://github.com/alexandra-chron/umt-lmu-wmt2020" additional="false">alexandra-chron/umt-lmu-wmt2020</pwccode>
    </paper>
    <paper id="129">
      <title><fixed-case>U</fixed-case>d<fixed-case>S</fixed-case>-<fixed-case>DFKI</fixed-case>@<fixed-case>WMT</fixed-case>20: Unsupervised <fixed-case>MT</fixed-case> and Very Low Resource Supervised <fixed-case>MT</fixed-case> for <fixed-case>G</fixed-case>erman-<fixed-case>U</fixed-case>pper <fixed-case>S</fixed-case>orbian</title>
      <author><first>Sourav</first><last>Dutta</last></author>
      <author><first>Jesujoba</first><last>Alabi</last></author>
      <author><first>Saptarashmi</first><last>Bandyopadhyay</last></author>
      <author><first>Dana</first><last>Ruiter</last></author>
      <author><first>Josef</first><last>van Genabith</last></author>
      <pages>1092–1098</pages>
      <abstract>This paper describes the UdS-DFKI submission to the shared task for unsupervised machine translation (MT) and very low-resource supervised MT between German (de) and Upper Sorbian (hsb) at the Fifth Conference of Machine Translation (WMT20). We submit systems for both the supervised and unsupervised tracks. Apart from various experimental approaches like bitext mining, model pre-training, and iterative back-translation, we employ a factored machine translation approach on a small BPE vocabulary.</abstract>
      <url hash="70031bea">2020.wmt-1.129</url>
      <video href="https://slideslive.com/38939584"/>
      <bibkey>dutta-etal-2020-uds</bibkey>
    </paper>
    <paper id="130">
      <title>Data Selection for Unsupervised Translation of <fixed-case>G</fixed-case>erman–<fixed-case>U</fixed-case>pper <fixed-case>S</fixed-case>orbian</title>
      <author><first>Lukas</first><last>Edman</last></author>
      <author><first>Antonio</first><last>Toral</last></author>
      <author><first>Gertjan</first><last>van Noord</last></author>
      <pages>1099–1103</pages>
      <abstract>This paper describes the methods behind the systems submitted by the University of Groningen for the WMT 2020 Unsupervised Machine Translation task for German–Upper Sorbian. We investigate the usefulness of data selection in the unsupervised setting. We find that we can perform data selection using a pretrained model and show that the quality of a set of sentences or documents can have a great impact on the performance of the UNMT system trained on it. Furthermore, we show that document-level data selection should be preferred for training the XLM model when possible. Finally, we show that there is a trade-off between quality and quantity of the data used to train UNMT systems.</abstract>
      <url hash="845ca95e">2020.wmt-1.130</url>
      <video href="https://slideslive.com/38939613"/>
      <bibkey>edman-etal-2020-data</bibkey>
    </paper>
    <paper id="131">
      <title>The <fixed-case>LMU</fixed-case> <fixed-case>M</fixed-case>unich System for the <fixed-case>WMT</fixed-case>20 Very Low Resource Supervised <fixed-case>MT</fixed-case> Task</title>
      <author><first>Jindřich</first><last>Libovický</last></author>
      <author><first>Viktor</first><last>Hangya</last></author>
      <author><first>Helmut</first><last>Schmid</last></author>
      <author><first>Alexander</first><last>Fraser</last></author>
      <pages>1104–1111</pages>
      <abstract>We present our systems for the WMT20 Very Low Resource MT Task for translation between German and Upper Sorbian. For training our systems, we generate synthetic data by both back- and forward-translation. Additionally, we enrich the training data with German-Czech translated from Czech to Upper Sorbian by an unsupervised statistical MT system incorporating orthographically similar word pairs and transliterations of OOV words. Our best translation system between German and Sorbian is based on transfer learning from a Czech-German system and scores 12 to 13 BLEU higher than a baseline system built using the available parallel data only.</abstract>
      <url hash="1fc575e1">2020.wmt-1.131</url>
      <video href="https://slideslive.com/38939564"/>
      <bibkey>libovicky-etal-2020-lmu</bibkey>
    </paper>
    <paper id="132">
      <title><fixed-case>NRC</fixed-case> Systems for Low Resource <fixed-case>G</fixed-case>erman-<fixed-case>U</fixed-case>pper <fixed-case>S</fixed-case>orbian Machine Translation 2020: Transfer Learning with Lexical Modifications</title>
      <author><first>Rebecca</first><last>Knowles</last></author>
      <author><first>Samuel</first><last>Larkin</last></author>
      <author><first>Darlene</first><last>Stewart</last></author>
      <author><first>Patrick</first><last>Littell</last></author>
      <pages>1112–1122</pages>
      <abstract>We describe the National Research Council of Canada (NRC) neural machine translation systems for the German-Upper Sorbian supervised track of the 2020 shared task on Unsupervised MT and Very Low Resource Supervised MT. Our models are ensembles of Transformer models, built using combinations of BPE-dropout, lexical modifications, and backtranslation.</abstract>
      <url hash="218d3ef7">2020.wmt-1.132</url>
      <video href="https://slideslive.com/38939634"/>
      <bibkey>knowles-etal-2020-nrc-systems</bibkey>
    </paper>
    <paper id="133">
      <title><fixed-case>CUNI</fixed-case> Systems for the Unsupervised and Very Low Resource Translation Task in <fixed-case>WMT</fixed-case>20</title>
      <author><first>Ivana</first><last>Kvapilíková</last></author>
      <author><first>Tom</first><last>Kocmi</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <pages>1123–1128</pages>
      <abstract>This paper presents a description of CUNI systems submitted to the WMT20 task on unsupervised and very low-resource supervised machine translation between German and Upper Sorbian. We experimented with training on synthetic data and pre-training on a related language pair. In the fully unsupervised scenario, we achieved 25.5 and 23.7 BLEU translating from and into Upper Sorbian, respectively. Our low-resource systems relied on transfer learning from German-Czech parallel data and achieved 57.4 BLEU and 56.1 BLEU, which is an improvement of 10 BLEU points over the baseline trained only on the available small German-Upper Sorbian parallel corpus.</abstract>
      <url hash="a9c59155">2020.wmt-1.133</url>
      <video href="https://slideslive.com/38939641"/>
      <bibkey>kvapilikova-etal-2020-cuni</bibkey>
    </paper>
    <paper id="134">
      <title>The <fixed-case>U</fixed-case>niversity of <fixed-case>H</fixed-case>elsinki and Aalto University submissions to the <fixed-case>WMT</fixed-case> 2020 news and low-resource translation tasks</title>
      <author><first>Yves</first><last>Scherrer</last></author>
      <author><first>Stig-Arne</first><last>Grönroos</last></author>
      <author><first>Sami</first><last>Virpioja</last></author>
      <pages>1129–1138</pages>
      <abstract>This paper describes the joint participation of University of Helsinki and Aalto University to two shared tasks of WMT 2020: the news translation between Inuktitut and English and the low-resource translation between German and Upper Sorbian. For both tasks, our efforts concentrate on efficient use of monolingual and related bilingual corpora with scheduled multi-task learning as well as an optimized subword segmentation with sampling. Our submission obtained the highest score for Upper Sorbian -&gt; German and was ranked second for German -&gt; Upper Sorbian according to BLEU scores. For English–Inuktitut, we reached ranks 8 and 10 out of 11 according to BLEU scores.</abstract>
      <url hash="eb48442e">2020.wmt-1.134</url>
      <video href="https://slideslive.com/38939589"/>
      <bibkey>scherrer-etal-2020-university</bibkey>
    </paper>
    <paper id="135">
      <title>The <fixed-case>NITS</fixed-case>-<fixed-case>CNLP</fixed-case> System for the Unsupervised <fixed-case>MT</fixed-case> Task at <fixed-case>WMT</fixed-case> 2020</title>
      <author><first>Salam Michael</first><last>Singh</last></author>
      <author><first>Thoudam Doren</first><last>Singh</last></author>
      <author><first>Sivaji</first><last>Bandyopadhyay</last></author>
      <pages>1139–1143</pages>
      <abstract>We describe NITS-CNLP’s submission to WMT 2020 unsupervised machine translation shared task for German language (de) to Upper Sorbian (hsb) in a constrained setting i.e, using only the data provided by the organizers. We train our unsupervised model using monolingual data from both the languages by jointly pre-training the encoder and decoder and fine-tune using backtranslation loss. The final model uses the source side (de) monolingual data and the target side (hsb) synthetic data as a pseudo-parallel data to train a pseudo-supervised system which is tuned using the provided development set(dev set).</abstract>
      <url hash="64ef4f2f">2020.wmt-1.135</url>
      <video href="https://slideslive.com/38939575"/>
      <bibkey>singh-etal-2020-nits</bibkey>
    </paper>
    <paper id="136">
      <title>Adobe <fixed-case>AMPS</fixed-case>’s Submission for Very Low Resource Supervised Translation Task at <fixed-case>WMT</fixed-case>20</title>
      <author><first>Keshaw</first><last>Singh</last></author>
      <pages>1144–1149</pages>
      <abstract>In this paper, we describe our systems submitted to the very low resource supervised translation task at WMT20. We participate in both translation directions for Upper Sorbian-German language pair. Our primary submission is a subword-level Transformer-based neural machine translation model trained on original training bitext. We also conduct several experiments with backtranslation using limited monolingual data in our post-submission work and include our results for the same. In one such experiment, we observe jumps of up to 2.6 BLEU points over the primary system by pretraining on a synthetic, backtranslated corpus followed by fine-tuning on the original parallel training data.</abstract>
      <url hash="7d02bb70">2020.wmt-1.136</url>
      <video href="https://slideslive.com/38939621"/>
      <bibkey>singh-2020-adobe</bibkey>
    </paper>
    <paper id="137">
      <title>On the Same Page? Comparing Inter-Annotator Agreement in Sentence and Document Level Human Machine Translation Evaluation</title>
      <author><first>Sheila</first><last>Castilho</last></author>
      <pages>1150–1159</pages>
      <abstract>Document-level evaluation of machine translation has raised interest in the community especially since responses to the claims of “human parity” (Toral et al., 2018; Läubli et al., 2018) with document-level human evaluations have been published. Yet, little is known about best practices regarding human evaluation of machine translation at the document-level. This paper presents a comparison of the differences in inter-annotator agreement between quality assessments using sentence and document-level set-ups. We report results of the agreement between professional translators for fluency and adequacy scales, error annotation, and pair-wise ranking, along with the effort needed to perform the different tasks. To best of our knowledge, this is the first study of its kind.</abstract>
      <url hash="7067fc23">2020.wmt-1.137</url>
      <video href="https://slideslive.com/38939557"/>
      <bibkey>castilho-2020-page</bibkey>
    </paper>
    <paper id="138">
      <title>How Should Markup Tags Be Translated?</title>
      <author><first>Greg</first><last>Hanneman</last></author>
      <author><first>Georgiana</first><last>Dinu</last></author>
      <pages>1160–1173</pages>
      <abstract>The ability of machine translation (MT) models to correctly place markup is crucial to generating high-quality translations of formatted input. This paper compares two commonly used methods of representing markup tags and tests the ability of MT models to learn tag placement via training data augmentation. We study the interactions of tag representation, data augmentation size, tag complexity, and language pair to show the drawbacks and benefits of each method. We construct and release new test sets containing tagged data for three language pairs of varying difficulty.</abstract>
      <url hash="4780b198">2020.wmt-1.138</url>
      <video href="https://slideslive.com/38939626"/>
      <bibkey>hanneman-dinu-2020-markup</bibkey>
      <pwccode url="https://github.com/amazon-research/mt-markup-tags" additional="false">amazon-research/mt-markup-tags</pwccode>
    </paper>
    <paper id="139">
      <title>The Tatoeba Translation Challenge – Realistic Data Sets for Low Resource and Multilingual <fixed-case>MT</fixed-case></title>
      <author><first>Jörg</first><last>Tiedemann</last></author>
      <pages>1174–1182</pages>
      <abstract>This paper describes the development of a new benchmark for machine translation that provides training and test data for thousands of language pairs covering over 500 languages and tools for creating state-of-the-art translation models from that collection. The main goal is to trigger the development of open translation tools and models with a much broader coverage of the World’s languages. Using the package it is possible to work on realistic low-resource scenarios avoiding artificially reduced setups that are common when demonstrating zero-shot or few-shot learning. For the first time, this package provides a comprehensive collection of diverse data sets in hundreds of languages with systematic language and script annotation and data splits to extend the narrow coverage of existing benchmarks. Together with the data release, we also provide a growing number of pre-trained baseline models for individual language pairs and selected language groups.</abstract>
      <url hash="87ab7dd0">2020.wmt-1.139</url>
      <video href="https://slideslive.com/38939636"/>
      <bibkey>tiedemann-2020-tatoeba</bibkey>
    </paper>
    <paper id="140">
      <title>Human-Paraphrased References Improve Neural Machine Translation</title>
      <author><first>Markus</first><last>Freitag</last></author>
      <author><first>George</first><last>Foster</last></author>
      <author><first>David</first><last>Grangier</last></author>
      <author><first>Colin</first><last>Cherry</last></author>
      <pages>1183–1192</pages>
      <abstract>Automatic evaluation comparing candidate translations to human-generated paraphrases of reference translations has recently been proposed by freitag2020bleu. When used in place of original references, the paraphrased versions produce metric scores that correlate better with human judgment. This effect holds for a variety of different automatic metrics, and tends to favor natural formulations over more literal (translationese) ones. In this paper we compare the results of performing end-to-end system development using standard and paraphrased references. With state-of-the-art English-German NMT components, we show that tuning to paraphrased references produces a system that is ignificantly better according to human judgment, but 5 BLEU points worse when tested on standard references. Our work confirms the finding that paraphrased references yield metric scores that correlate better with human judgment, and demonstrates for the first time that using these scores for system development can lead to significant improvements.</abstract>
      <url hash="3dccb6eb">2020.wmt-1.140</url>
      <video href="https://slideslive.com/38939593"/>
      <bibkey>freitag-etal-2020-human</bibkey>
    </paper>
    <paper id="141">
      <title>Incorporating Terminology Constraints in Automatic Post-Editing</title>
      <author><first>David</first><last>Wan</last></author>
      <author><first>Chris</first><last>Kedzie</last></author>
      <author><first>Faisal</first><last>Ladhak</last></author>
      <author><first>Marine</first><last>Carpuat</last></author>
      <author><first>Kathleen</first><last>McKeown</last></author>
      <pages>1193–1204</pages>
      <abstract>Users of machine translation (MT) may want to ensure the use of specific lexical terminologies. While there exist techniques for incorporating terminology constraints during inference for MT, current APE approaches cannot ensure that they will appear in the final translation. In this paper, we present both autoregressive and non-autoregressive models for lexically constrained APE, demonstrating that our approach enables preservation of 95% of the terminologies and also improves translation quality on English-German benchmarks. Even when applied to lexically constrained MT output, our approach is able to improve preservation of the terminologies. However, we show that our models do not learn to copy constraints systematically and suggest a simple data augmentation technique that leads to improved performance and robustness.</abstract>
      <url hash="e3b0f9c7">2020.wmt-1.141</url>
      <video href="https://slideslive.com/38939650"/>
      <bibkey>wan-etal-2020-incorporating</bibkey>
      <pwccode url="https://github.com/zerocstaker/constrained_ape" additional="false">zerocstaker/constrained_ape</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/escape">eSCAPE</pwcdataset>
    </paper>
  </volume>
</collection>
