<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.knowllm">
  <volume id="1" ingest-date="2024-07-30" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 1st Workshop on Towards Knowledgeable Language Models (KnowLLM 2024)</booktitle>
      <editor><first>Sha</first><last>Li</last></editor>
      <editor><first>Manling</first><last>Li</last></editor>
      <editor><first>Michael JQ</first><last>Zhang</last></editor>
      <editor><first>Eunsol</first><last>Choi</last></editor>
      <editor><first>Mor</first><last>Geva</last></editor>
      <editor><first>Peter</first><last>Hase</last></editor>
      <editor><first>Heng</first><last>Ji</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Bangkok, Thailand</address>
      <month>August</month>
      <year>2024</year>
      <url hash="50b8c2df">2024.knowllm-1</url>
      <venue>knowllm</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="00940f27">2024.knowllm-1.0</url>
      <bibkey>knowllm-1-2024</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>P</fixed-case>honology<fixed-case>B</fixed-case>ench: Evaluating Phonological Skills of Large Language Models</title>
      <author><first>Ashima</first><last>Suvarna</last></author>
      <author><first>Harshita</first><last>Khandelwal</last><affiliation>UCLA Computer Science Department, University of California, Los Angeles</affiliation></author>
      <author><first>Nanyun</first><last>Peng</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>1-14</pages>
      <abstract>Phonology, the study of speech’s structure and pronunciation rules, is a critical yet often overlooked component in Large Language Model (LLM) research. LLMs are widely used in various downstream applications that leverage phonology such as educational tools and poetry generation. Moreover, LLMs can potentially learn imperfect associations between orthographic and phonological forms from the training data. Thus, it is imperative to benchmark the phonological skills of LLMs. To this end, we present PhonologyBench, a novel benchmark consisting of three diagnostic tasks designed to explicitly test the phonological skills of LLMs in English: grapheme-to-phoneme conversion, syllable counting, and rhyme word generation. Despite having no access to speech data, LLMs showcased notable performance on the PhonologyBench tasks. However, we observe a significant gap of 17% and 45% on Rhyme Word Generation and Syllable counting, respectively, when compared to humans. Our findings underscore the importance of studying LLM performance on phonological tasks that inadvertently impact real-world applications. Furthermore, we encourage researchers to choose LLMs that perform well on the phonological task that is closely related to the downstream application since we find that no single model consistently outperforms the others on all the tasks.</abstract>
      <url hash="8d11b1d3">2024.knowllm-1.1</url>
      <bibkey>suvarna-etal-2024-phonologybench</bibkey>
      <doi>10.18653/v1/2024.knowllm-1.1</doi>
    </paper>
    <paper id="2">
      <title>Is Your Large Language Model Knowledgeable or a Choices-Only Cheater?</title>
      <author><first>Nishant</first><last>Balepur</last></author>
      <author><first>Rachel</first><last>Rudinger</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>15-26</pages>
      <abstract>Recent work shows that large language models (LLMs) can answer multiple-choice questions using only the choices, but does this mean that MCQA leaderboard rankings of LLMs are largely influenced by abilities in choices-only settings? To answer this, we use a contrast set that probes if LLMs over-rely on choices-only shortcuts in MCQA. While previous works build contrast sets via expensive human annotations or model-generated data which can be biased, we employ graph mining to extract contrast sets from existing MCQA datasets. We use our method on UnifiedQA, a group of six commonsense reasoning datasets with high choices-only accuracy, to build an 820-question contrast set. After validating our contrast set, we test 12 LLMs, finding that these models do not exhibit reliance on choice-only shortcuts when given both the question and choices. Thus, despite the susceptibility of MCQA to high choices-only accuracy, we argue that LLMs are not obtaining high ranks on MCQA leaderboards solely due to their ability to exploit choices-only shortcuts.</abstract>
      <url hash="6494fc2c">2024.knowllm-1.2</url>
      <bibkey>balepur-rudinger-2024-large</bibkey>
      <doi>10.18653/v1/2024.knowllm-1.2</doi>
    </paper>
    <paper id="3">
      <title>Reassess Summary Factual Inconsistency Detection with Large Language Model</title>
      <author><first>Jiuding</first><last>Yang</last></author>
      <author><first>Hui</first><last>Liu</last><affiliation>QQ Browser Lab, Tecent</affiliation></author>
      <author><first>Weidong</first><last>Guo</last><affiliation>Tencent</affiliation></author>
      <author><first>Zhuwei</first><last>Rao</last></author>
      <author><first>Yu</first><last>Xu</last><affiliation>Tencent</affiliation></author>
      <author><first>Di</first><last>Niu</last><affiliation>University of Alberta and University of Alberta</affiliation></author>
      <pages>27-31</pages>
      <abstract>Ensuring factual consistency between the summary and the original document is paramount in summarization tasks. Consequently, considerable effort has been dedicated to detecting inconsistencies. With the advent of Large Language Models (LLMs), recent studies have begun to leverage their advanced language understanding capabilities for inconsistency detection. However, early attempts have shown that LLMs underperform traditional models due to their limited ability to follow instructions and the absence of an effective detection methodology. In this study, we reassess summary inconsistency detection with LLMs, comparing the performances of GPT-3.5 and GPT-4. To advance research in LLM-based inconsistency detection, we propose SIFiD (Summary Inconsistency Detection with Filtered Document) that identify key sentences within documents by either employing natural language inference or measuring semantic similarity between summaries and documents.</abstract>
      <url hash="f893749c">2024.knowllm-1.3</url>
      <bibkey>yang-etal-2024-reassess</bibkey>
      <doi>10.18653/v1/2024.knowllm-1.3</doi>
    </paper>
    <paper id="4">
      <title>Beyond Text: Unveiling Multimodal Proficiency of Large Language Models with <fixed-case>M</fixed-case>ulti<fixed-case>API</fixed-case> Benchmark</title>
      <author><first>Xiao</first><last>Liu</last><affiliation>University of California, Davis</affiliation></author>
      <author><first>Jianfeng</first><last>Lin</last></author>
      <author><first>Jiawei</first><last>Zhang</last><affiliation>University of California, Davis</affiliation></author>
      <pages>32-44</pages>
      <abstract>The proliferation of Large Language Models like ChatGPT has significantly advanced language understanding and generation, impacting a broad spectrum of applications. However, these models predominantly excel in text-based tasks, overlooking the complexity of real-world multimodal information. This study introduces <b>MultiAPI</b>, a pioneering comprehensive large-scale API benchmark dataset aimed at expanding LLMs’ proficiency in multimodal contexts. Developed collaboratively through ChatGPT, <b>MultiAPI</b> consists of 187 diverse API calls and 1,799 contextual prompts, offering a unique platform evaluation of tool-augmented LLMs handling multimodal tasks. Through comprehensive experiments, our findings reveal that while LLMs demonstrate proficiency in API call decision-making, they face challenges in domain identification, function selection, and argument generation. What’s more, we surprisingly notice that auxiliary context can actually impair the performance. An in-depth error analysis paves the way for a new paradigm to address these challenges, suggesting a potential direction for future LLM research.</abstract>
      <url hash="ff845586">2024.knowllm-1.4</url>
      <bibkey>liu-etal-2024-beyond-text</bibkey>
      <doi>10.18653/v1/2024.knowllm-1.4</doi>
    </paper>
    <paper id="5">
      <title>Retrieval-Augmented Knowledge Integration into Language Models: A Survey</title>
      <author><first>Yuxuan</first><last>Chen</last><affiliation>German Research Center for AI, German Research Center for AI and Freie Universität Berlin</affiliation></author>
      <author><first>Daniel</first><last>Röder</last><affiliation>German Research Center for AI</affiliation></author>
      <author><first>Justus-Jonas</first><last>Erker</last></author>
      <author><first>Leonhard</first><last>Hennig</last><affiliation>German Research Center for AI</affiliation></author>
      <author><first>Philippe</first><last>Thomas</last><affiliation>German Research Center for AI</affiliation></author>
      <author><first>Sebastian</first><last>Möller</last></author>
      <author><first>Roland</first><last>Roller</last><affiliation>German Research Center for AI</affiliation></author>
      <pages>45-63</pages>
      <abstract>This survey analyses how external knowledge can be integrated into language models in the context of retrieval-augmentation.The main goal of this work is to give an overview of: (1) Which external knowledge can be augmented? (2) Given a knowledge source, how to retrieve from it and then integrate the retrieved knowledge? To achieve this, we define and give a mathematical formulation of retrieval-augmented knowledge integration (RAKI). We discuss retrieval and integration techniques separately in detail, for each of the following knowledge formats: knowledge graph, tabular and natural language.</abstract>
      <url hash="43274ba6">2024.knowllm-1.5</url>
      <bibkey>chen-etal-2024-retrieval</bibkey>
      <doi>10.18653/v1/2024.knowllm-1.5</doi>
    </paper>
    <paper id="6">
      <title><fixed-case>C</fixed-case>linical<fixed-case>RAG</fixed-case>: Enhancing Clinical Decision Support through Heterogeneous Knowledge Retrieval</title>
      <author><first>Yuxing</first><last>Lu</last></author>
      <author><first>Xukai</first><last>Zhao</last></author>
      <author><first>Jinzhuo</first><last>Wang</last><affiliation>Peking University</affiliation></author>
      <pages>64-68</pages>
      <abstract>Large Language Models (LLMs) have revolutionized text generation across diverse domains, showcasing an ability to mimic human-like text with remarkable accuracy. Yet, these models frequently encounter a significant hurdle: producing hallucinations, a flaw particularly detrimental in the healthcare domain where precision is crucial. In this paper, we introduce ClinicalRAG, a novel multi-agent pipeline to rectify this issue by incorporating heterogeneous medical knowledge—both structured and unstructured—into LLMs to bolster diagnosis accuracy. ClinicalRAG can extract related medical entities from user inputs and dynamically integrate relevant medical knowledge during the text generation process. Comparative analyses reveal that ClinicalRAG significantly outperforms knowledge-deficient methods, offering enhanced reliability in clinical decision support. This advancement marks a pivotal proof-of-concept step towards mitigating misinformation risks in healthcare applications of LLMs.</abstract>
      <url hash="9a05f4ab">2024.knowllm-1.6</url>
      <bibkey>lu-etal-2024-clinicalrag</bibkey>
      <doi>10.18653/v1/2024.knowllm-1.6</doi>
    </paper>
    <paper id="7">
      <title>Modeling Uncertainty and Using Post-fusion as Fallback Improves Retrieval Augmented Generation with <fixed-case>LLM</fixed-case>s</title>
      <author><first>Ye</first><last>Liu</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Rui</first><last>Meng</last><affiliation>SalesForce Research</affiliation></author>
      <author><first>Meghana Moorthy</first><last>Bhat</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Shafiq</first><last>Joty</last><affiliation>SalesForce.com and Nanyang Technological University</affiliation></author>
      <author><first>Caiming</first><last>Xiong</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Yingbo</first><last>Zhou</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Semih</first><last>Yavuz</last><affiliation>SalesForce.com</affiliation></author>
      <pages>69-82</pages>
      <abstract>The integration of retrieved passages and large language models (LLMs), such as ChatGPTs, has significantly contributed to improving open-domain question answering. However, there is still a lack of exploration regarding the optimal approach for incorporating retrieved passages into the answer generation process. This paper aims to fill this gap by investigating different methods of combining retrieved passages with LLMs to enhance answer generation. We begin by examining the limitations of a commonly-used concatenation approach. Surprisingly, this approach often results in generating “unknown” outputs, even when the correct document is among the top-<tex-math>k</tex-math> retrieved passages. To address this issue, we explore four alternative strategies for integrating the retrieved passages with the LLMs. These strategies include two single-round methods that utilize chain-of-thought reasoning and two multi-round strategies that incorporate feedback loops. Through comprehensive analyses and experiments, we provide insightful observations on how to effectively leverage retrieved passages to enhance the answer generation capability of LLMs. On three open-domain question answering datesets, NQ, TriviaQA and SQuAD, our multi-round approaches outperform traditional concatenation approach, achieving over a 10% improvement in answer EM.</abstract>
      <url hash="ac935912">2024.knowllm-1.7</url>
      <bibkey>liu-etal-2024-modeling</bibkey>
      <doi>10.18653/v1/2024.knowllm-1.7</doi>
    </paper>
    <paper id="8">
      <title><fixed-case>A</fixed-case>c<fixed-case>K</fixed-case>nowledge: Acquired Knowledge Representation by Small Language Model Without Pre-training</title>
      <author><first>Sourav</first><last>Das</last></author>
      <author><first>Sanjay</first><last>Chatterji</last></author>
      <author><first>Imon</first><last>Mukherjee</last></author>
      <pages>83-95</pages>
      <abstract>Large language models (LLMs) are pre-trained on enormous amounts of text data and show acclaimed success in knowledge representation. However, there are two bottlenecks with this approach. (1) Pre-training data cannot be regularly updated once the models are deployed, and it is not very fruitful if the model cannot represent updated knowledge. (2) The consistently increasing size and computational resources make it difficult for non-commercial and individual researchers to fine-tune and scale these language models. Major LLMs with external knowledge are also proprietary. In this paper, we propose AcKnowledge, a framework wrapped around a small, non-pre-trained language model for an open-domain question-answering (QA) experiment. AcKnowledge learns relevant knowledge from the internet via meta-learning based on user questions, and re-learns from user feedback if knowledge is misrepresented. Our efficient knowledge representation framework avoids pre-training overhead while enabling updated information. Benchmarking shows competitive performance against similarly sized state-of-the-art (SoTA) LLMs on gold standard QA datasets, demonstrating the potential of integrating internet search and user feedback for improved performance and generalizability.</abstract>
      <url hash="6381c235">2024.knowllm-1.8</url>
      <bibkey>das-etal-2024-acknowledge</bibkey>
      <doi>10.18653/v1/2024.knowllm-1.8</doi>
    </paper>
    <paper id="9">
      <title>Knowledge Acquisition through Continued Pretraining is Difficult: A Case Study on r/<fixed-case>A</fixed-case>sk<fixed-case>H</fixed-case>istorians</title>
      <author><first>Jan</first><last>Hoffbauer</last></author>
      <author><first>Sylwester</first><last>Sawicki</last><affiliation>Universität Potsdam</affiliation></author>
      <author><first>Marc</first><last>Ulrich</last><affiliation>Universität Potsdam</affiliation></author>
      <author><first>Tolga</first><last>Buz</last><affiliation>Kearney</affiliation></author>
      <author><first>Konstantin</first><last>Dobler</last><affiliation>Hasso Plattner Institute</affiliation></author>
      <author><first>Moritz</first><last>Schneider</last><affiliation>Hasso Plattner Institute</affiliation></author>
      <author><first>Gerard</first><last>De Melo</last><affiliation>Hasso Plattner Institute and University of Potsdam</affiliation></author>
      <pages>96-108</pages>
      <abstract>Powerful LLMs like ChatGPT are adopted rapidly for a wide array of tasks, but their limitations in domain-specific areas become apparent, particularly when prompted to recite facts. This is critical especially for knowledge workers, who are adopting LLM-based tools rapidly.While there are various techniques that can help ingest knowledge into LLMs such as instruction tuning and alignment, most have disadvantages. We examine the impact of prominent training techniques on LLMs’ knowledge accuracy using a knowledge-dense dataset that we curate from r/AskHistorians, a rich source of historical knowledge. We evaluate the impact of different models sizes from 1.3B to 7B parameters and other factors such as LoRA adapters, quantization, overfitting, and the inclusion of Reddit data in pretraining.In addition, we measure linguistic metrics and human and LLM-based preference. Our results suggest that pretraining and model size have a much stronger effect on knowledge accuracy than continued pretraining – unless the model is overfit to the tested knowledge.Fine-tuning on our Reddit dataset introduces less complex, but slightly more toxic language. Our study explores the challenges of injecting domain-specific datasets into LLMs and has implications for practitioners, e.g., when LLMs are to be fine-tuned with a company’s datasets.</abstract>
      <url hash="dfd96cbb">2024.knowllm-1.9</url>
      <bibkey>hoffbauer-etal-2024-knowledge</bibkey>
      <doi>10.18653/v1/2024.knowllm-1.9</doi>
    </paper>
    <paper id="10">
      <title>Beyond Probabilities: Unveiling the Misalignment in Evaluating Large Language Models</title>
      <author><first>Chenyang</first><last>Lyu</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Minghao</first><last>Wu</last></author>
      <author><first>Alham</first><last>Aji</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Amazon</affiliation></author>
      <pages>109-131</pages>
      <abstract>Large Language Models (LLMs) have demonstrated remarkable capabilities across various applications, fundamentally reshaping the landscape of natural language processing (NLP) research. However, recent evaluation frameworks often rely on the output probabilities of LLMs for predictions, primarily due to computational constraints, diverging from real-world LLM usage scenarios. While widely employed, the efficacy of these probability-based evaluation strategies remains an open research question. This study aims to scrutinize the validity of such probability-based evaluation methods within the context of using LLMs for Multiple Choice Questions (MCQs), highlighting their inherent limitations. Our empirical investigation reveals that the prevalent probability-based evaluation method inadequately aligns with generation-based prediction. Furthermore, current evaluation frameworks typically assess LLMs through predictive tasks based on output probabilities rather than directly generating responses, owing to computational limitations. We illustrate that these probability-based approaches do not effectively correspond with generative predictions. The outcomes of our study can enhance the understanding of LLM evaluation methodologies and provide insights for future research in this domain.</abstract>
      <url hash="960ef59b">2024.knowllm-1.10</url>
      <bibkey>lyu-etal-2024-beyond</bibkey>
      <doi>10.18653/v1/2024.knowllm-1.10</doi>
    </paper>
    <paper id="11">
      <title><fixed-case>P</fixed-case>rompt<fixed-case>RE</fixed-case>: Weakly-Supervised Document-Level Relation Extraction via Prompting-Based Data Programming</title>
      <author><first>Chufan</first><last>Gao</last></author>
      <author><first>Xulin</first><last>Fan</last></author>
      <author><first>Jimeng</first><last>Sun</last><affiliation>University of Illinois, Urbana Champaign, College of Computing and Georgia Institute of Technology</affiliation></author>
      <author><first>Xuan</first><last>Wang</last><affiliation>Virginia Polytechnic Institute and State University</affiliation></author>
      <pages>132-145</pages>
      <abstract>Relation extraction aims to classify the relationships between two entities into pre-defined categories. While previous research has mainly focused on sentence-level relation extraction, recent studies have expanded the scope to document-level relation extraction. Traditional relation extraction methods heavily rely on human-annotated training data, which is time-consuming and labor-intensive. To mitigate the need for manual annotation, recent weakly-supervised approaches have been developed for sentence-level relation extraction while limited work has been done on document-level relation extraction. Weakly-supervised document-level relation extraction faces significant challenges due to an imbalanced number “no relation” instances and the failure of directly probing pretrained large language models for document relation extraction. To address these challenges, we propose PromptRE, a novel weakly-supervised document-level relation extraction method that combines prompting-based techniques with data programming. Furthermore, PromptRE incorporates the label distribution and entity types as prior knowledge to improve the performance. By leveraging the strengths of both prompting and data programming, PromptRE achieves improved performance in relation classification and effectively handles the “no relation” problem. Experimental results on ReDocRED, a benchmark dataset for document-level relation extraction, demonstrate the superiority of PromptRE over baseline approaches.</abstract>
      <url hash="9bd9ef9c">2024.knowllm-1.11</url>
      <bibkey>gao-etal-2024-promptre</bibkey>
      <doi>10.18653/v1/2024.knowllm-1.11</doi>
    </paper>
    <paper id="12">
      <title>Patent Response System Optimised for Faithfulness: Procedural Knowledge Embodiment with Knowledge Graph and Retrieval Augmented Generation</title>
      <author><first>Jung-Mei</first><last>Chu</last><affiliation>National Taiwan University</affiliation></author>
      <author><first>Hao-Cheng</first><last>Lo</last><affiliation>JCIPRNET and National Taiwan University</affiliation></author>
      <author><first>Jieh</first><last>Hsiang</last><affiliation>National Taiwan University</affiliation></author>
      <author><first>Chun-Chieh</first><last>Cho</last><affiliation>JCIPRNET</affiliation></author>
      <pages>146-155</pages>
      <abstract>A successful response to Office Action is crucial for an invention to obtain a patent. While previous attempts have applied generalised LLMs, such as GPT-4, in the response process, there remains significant room for improvement in generating faithful, unbiased, and practically valuable responses. To address this issue, we propose the Patent Response System Optimised for Faithfulness (PRO). PRO explicitly incorporates procedural knowledge used by patent agents during drafting arguments in response. This framework comprises several key components: (1) Our proposed PRLLM is a LLM tailored for patent responses, designed to have comprehensive patent domain-specific knowledge. (2) Our proposed PPNet encodes legal interpretations and relationships between technical components from judicial sources through a knowledge graph. (3) The augmented generation processes retrieve relevant information from both the patent text and PPNet to augment the PRLLM’s input and generate faithful responses. Results show that PRO significantly reduces unfaithfulness across six error types compared to several settings. For instance, PRO outperforms GPT-4 by an average of 39% in terms of faithfulness. This demonstrates the effectiveness of our domain-specific approach in improving the quality of automated patent responses.</abstract>
      <url hash="92fde9aa">2024.knowllm-1.12</url>
      <bibkey>chu-etal-2024-patent</bibkey>
      <doi>10.18653/v1/2024.knowllm-1.12</doi>
    </paper>
    <paper id="13">
      <title>Safe-Embed: Unveiling the Safety-Critical Knowledge of Sentence Encoders</title>
      <author><first>Jinseok</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Jaewon</first><last>Jung</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Sangyeop</first><last>Kim</last><affiliation>Coxwave and Seoul National University</affiliation></author>
      <author><first>Sohhyung</first><last>Park</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Sungzoon</first><last>Cho</last><affiliation>Seoul National University</affiliation></author>
      <pages>156-170</pages>
      <abstract>Despite the impressive capabilities of Large Language Models (LLMs) in various tasks, their vulnerability to unsafe prompts remains a critical issue. These prompts can lead LLMs to generate responses on illegal or sensitive topics, posing a significant threat to their safe and ethical use. Existing approaches address this issue using classification models, divided into LLM-based and API-based methods. LLM based models demand substantial resources and large datasets, whereas API-based models are cost-effective but might overlook linguistic nuances. With the increasing complexity of unsafe prompts, similarity search-based techniques that identify specific features of unsafe content provide a more robust and effective solution to this evolving problem. This paper investigates the potential of sentence encoders to distinguish safe from unsafe content. We introduce new pairwise datasets and the Cate021 gorical Purity (CP) metric to measure this capability. Our findings reveal both the effectiveness and limitations of existing sentence encoders, proposing directions to improve sentence encoders to operate as robust safety detectors.</abstract>
      <url hash="88843bcd">2024.knowllm-1.13</url>
      <bibkey>kim-etal-2024-safe</bibkey>
      <doi>10.18653/v1/2024.knowllm-1.13</doi>
    </paper>
    <paper id="14">
      <title>Measuring the Inconsistency of Large Language Models in Preferential Ranking</title>
      <author><first>Xiutian</first><last>Zhao</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Ke</first><last>Wang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Wei</first><last>Peng</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <pages>171-176</pages>
      <abstract>Despite large language models’ (LLMs’) recent advancements, their bias and hallucination issues persist, and their ability to offer consistent and preferential rankings remains underexplored. This study investigates the capacity of LLMs to provide consistent ordinal preferences, a crucial aspect in scenarios lacking absolute answers. We introduce a formalization of consistency based on order theory, outlining criteria such as transitivity, asymmetry, reversibility, and independence from irrelevant alternatives. Our diagnostic experiments on selected state-of-the-art LLMs reveal their inability to meet these criteria, indicating a strong positional bias and poor transitivity, with preferences easily swayed by irrelevant alternatives. These findings highlight a significant inconsistency in LLM-generated preferential rankings, underscoring the need for further research to address these limitations.</abstract>
      <url hash="2982cc5d">2024.knowllm-1.14</url>
      <bibkey>zhao-etal-2024-measuring</bibkey>
      <doi>10.18653/v1/2024.knowllm-1.14</doi>
    </paper>
    <paper id="15">
      <title>Retrieval-augmented generation in multilingual settings</title>
      <author><first>Nadezhda</first><last>Chirkova</last><affiliation>Naver Labs Europe</affiliation></author>
      <author><first>David</first><last>Rau</last></author>
      <author><first>Hervé</first><last>Déjean</last><affiliation>Naver Labs Europe</affiliation></author>
      <author><first>Thibault</first><last>Formal</last><affiliation>Naver Labs Europe</affiliation></author>
      <author><first>Stéphane</first><last>Clinchant</last><affiliation>Naver Labs Europe</affiliation></author>
      <author><first>Vassilina</first><last>Nikoulina</last><affiliation>Naver Labs Europe</affiliation></author>
      <pages>177-188</pages>
      <abstract>Retrieval-augmented generation (RAG) has recently emerged as a promising solution for incorporating up-to-date or domain-specific knowledge into large language models (LLMs) and improving LLM factuality, but is predominantly studied in English-only settings. In this work, we consider RAG in the multilingual setting (mRAG), i.e. with user queries and the datastore in 13 languages, and investigate which components and with which adjustments are needed to build a well-performing mRAG pipeline, that can be used as a strong baseline in future works. Our findings highlight that despite the availability of high-quality off-the-shelf multilingual retrievers and generators, task-specific prompt engineering is needed to enable generation in user languages. Moreover, current evaluation metrics need adjustments for multilingual setting, to account for variations in spelling named entities. The main limitations to be addressed in future works include frequent code-switching in non-Latin alphabet languages, occasional fluency errors, wrong reading of the provided documents, or irrelevant retrieval. We release the code for the resulting mRAG baseline pipeline at https://github.com/naver/bergen, Documentation: https://github.com/naver/bergen/blob/main/documentations/multilingual.md.</abstract>
      <url hash="cdacf29b">2024.knowllm-1.15</url>
      <bibkey>chirkova-etal-2024-retrieval</bibkey>
      <doi>10.18653/v1/2024.knowllm-1.15</doi>
    </paper>
    <paper id="16">
      <title>Retrieve, Generate, Evaluate: A Case Study for Medical Paraphrases Generation with Small Language Models</title>
      <author><first>Ioana</first><last>Buhnila</last></author>
      <author><first>Aman</first><last>Sinha</last></author>
      <author><first>Mathieu</first><last>Constant</last><affiliation>Université de Lorraine, CNRS, ATILF</affiliation></author>
      <pages>189-203</pages>
      <abstract>Recent surge in the accessibility of large language models (LLMs) to the general population can lead to untrackable use of such models for medical-related recommendations. Language generation via LLMs models has two key problems: firstly, they are prone to hallucination and therefore, for any medical purpose they require scientific and factual grounding; secondly, LLMs pose tremendous challenge to computational resources due to their gigantic model size. In this work, we introduce pRAGe, a Pipeline for Retrieval Augmented Generation and Evaluation of medical paraphrases generation using Small Language Models (SLM). We study the effectiveness of SLMs and the impact of external knowledge base for medical paraphrase generation in French.</abstract>
      <url hash="2cd8cfe2">2024.knowllm-1.16</url>
      <bibkey>buhnila-etal-2024-retrieve</bibkey>
      <doi>10.18653/v1/2024.knowllm-1.16</doi>
    </paper>
  </volume>
</collection>
