<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.woah">
  <volume id="1" ingest-date="2024-06-26" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 8th Workshop on Online Abuse and Harms (WOAH 2024)</booktitle>
      <editor><first>Yi-Ling</first><last>Chung</last></editor>
      <editor><first>Zeerak</first><last>Talat</last></editor>
      <editor><first>Debora</first><last>Nozza</last></editor>
      <editor><first>Flor Miriam</first><last>Plaza-del-Arco</last></editor>
      <editor><first>Paul</first><last>Röttger</last></editor>
      <editor><first>Aida</first><last>Mostafazadeh Davani</last></editor>
      <editor><first>Agostina</first><last>Calabrese</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Mexico City, Mexico</address>
      <month>June</month>
      <year>2024</year>
      <url hash="17234fd0">2024.woah-1</url>
      <venue>woah</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="1c02f868">2024.woah-1.0</url>
      <bibkey>woah-2024-online</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Investigating radicalisation indicators in online extremist communities</title>
      <author><first>Christine</first><last>De Kock</last><affiliation>University of Melbourne</affiliation></author>
      <author><first>Eduard</first><last>Hovy</last><affiliation>University of Melbourne</affiliation></author>
      <pages>1-12</pages>
      <abstract>We identify and analyse three sociolinguistic indicators of radicalisation within online extremist forums: hostility, longevity and social connectivity. We develop models to predict the maximum degree of each indicator measured over an individual’s lifetime, based on a minimal number of initial interactions. Drawing on data from two diverse extremist communities, our results demonstrate that NLP methods are effective at prioritising at-risk users. This work offers practical insights for intervention strategies and policy development, and highlights an important but under-studied research direction.</abstract>
      <url hash="1e44891c">2024.woah-1.1</url>
      <bibkey>de-kock-hovy-2024-investigating</bibkey>
    </paper>
    <paper id="2">
      <title>Detection of Conspiracy Theories Beyond Keyword Bias in <fixed-case>G</fixed-case>erman-Language Telegram Using Large Language Models</title>
      <author><first>Milena</first><last>Pustet</last><affiliation>Hochschule für Technik und Wirtschaft Berlin</affiliation></author>
      <author><first>Elisabeth</first><last>Steffen</last><affiliation>Hochschule für Technik und Wirtschaft Berlin</affiliation></author>
      <author><first>Helena</first><last>Mihaljevic</last><affiliation>Hochschule für Technik und Wirtschaft Berlin</affiliation></author>
      <pages>13-27</pages>
      <abstract>To protect users from massive hateful content, existing works studied automated hate speech detection. Despite the existing efforts, one question remains: do automated hate speech detectors conform to social media content policies? A platform’s content policies are a checklist of content moderated by the social media platform. Because content moderation rules are often uniquely defined, existing hate speech datasets cannot directly answer this question. This work seeks to answer this question by creating HateModerate, a dataset for testing the behaviors of automated content moderators against content policies. First, we engage 28 annotators and GPT in a six-step annotation process, resulting in a list of hateful and non-hateful test suites matching each of Facebook’s 41 hate speech policies. Second, we test the performance of state-of-the-art hate speech detectors against HateModerate, revealing substantial failures these models have in their conformity to the policies. Third, using HateModerate, we augment the training data of a top-downloaded hate detector on HuggingFace. We observe significant improvement in the models’ conformity to content policies while having comparable scores on the original test data. Our dataset and code can be found in the attachment.</abstract>
      <url hash="749567fa">2024.woah-1.2</url>
      <bibkey>pustet-etal-2024-detection</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>E</fixed-case>ko<fixed-case>H</fixed-case>ate: Abusive Language and Hate Speech Detection for Code-switched Political Discussions on <fixed-case>N</fixed-case>igerian <fixed-case>T</fixed-case>witter</title>
      <author><first>Comfort</first><last>Ilevbare</last><affiliation>Afe Babalola University</affiliation></author>
      <author><first>Jesujoba</first><last>Alabi</last><affiliation>Saarland University</affiliation></author>
      <author><first>David Ifeoluwa</first><last>Adelani</last><affiliation>University College London</affiliation></author>
      <author><first>Firdous</first><last>Bakare</last><affiliation>Afe Babalola University, Ado-Ekiti, Nigeria</affiliation></author>
      <author><first>Oluwatoyin</first><last>Abiola</last><affiliation>Afe Babalola University, Ado Ekiti, Ekiti State Nigeria</affiliation></author>
      <author><first>Oluwaseyi</first><last>Adeyemo</last><affiliation>Afe Babalola University</affiliation></author>
      <pages>28-37</pages>
      <abstract>Nigerians have a notable online presence and actively discuss political and topical matters. This was particularly evident throughout the 2023 general election, where Twitter was used for campaigning, fact-checking and verification, and even positive and negative discourse. However, little or none has been done in the detection of abusive language and hate speech in Nigeria. In this paper, we curated code-switched Twitter data directed at three musketeers of the governorship election on the most populous and economically vibrant state in Nigeria; Lagos state, with the view to detect offensive speech in political discussions. We developed EkoHate—an abusive language and hate speech dataset for political discussions between the three candidates and their followers using a binary (normal vs offensive) and fine-grained four-label annotation scheme. We analysed our dataset and provided an empirical evaluation of state-of-the-art methods across both supervised and cross-lingual transfer learning settings. In the supervised setting, our evaluation results in both binary and four-label annotation schemes show that we can achieve 95.1 and 70.3 F1 points respectively. Furthermore, we show that our dataset adequately transfers very well to three publicly available offensive datasets (OLID, HateUS2020, and FountaHate), generalizing to political discussions in other regions like the US.</abstract>
      <url hash="6e8f4489">2024.woah-1.3</url>
      <bibkey>ilevbare-etal-2024-ekohate</bibkey>
    </paper>
    <paper id="4">
      <title>A Study of the Class Imbalance Problem in Abusive Language Detection</title>
      <author><first>Yaqi</first><last>Zhang</last><affiliation>Technical University of Munich</affiliation></author>
      <author><first>Viktor</first><last>Hangya</last><affiliation>Ludwig Maximilian University of Munich</affiliation></author>
      <author><first>Alexander</first><last>Fraser</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <pages>38-51</pages>
      <abstract>Abusive language detection has drawn increasing interest in recent years. However, a less systematically explored obstacle is label imbalance, i.e., the amount of abusive data is much lower than non-abusive data, leading to performance issues. The aim of this work is to conduct a comprehensive comparative study of popular methods for addressing the class imbalance issue. We explore 10 well-known approaches on 8 datasets with distinct characteristics: binary or multi-class, moderately or largely imbalanced, focusing on various types of abuse, etc. Additionally, we pro-pose two novel methods specialized for abuse detection: AbusiveLexiconAug and ExternalDataAug, which enrich the training data using abusive lexicons and external abusive datasets, respectively. We conclude that: 1) our AbusiveLexiconAug approach, random oversampling, and focal loss are the most versatile methods on various datasets; 2) focal loss tends to yield peak model performance; 3) oversampling and focal loss provide promising results for binary datasets and small multi-class sets, while undersampling and weighted cross-entropy are more suitable for large multi-class sets; 4) most methods are sensitive to hyperparameters, yet our suggested choice of hyperparameters provides a good starting point.</abstract>
      <url hash="75e609a2">2024.woah-1.4</url>
      <bibkey>zhang-etal-2024-study-class</bibkey>
    </paper>
    <paper id="5">
      <title><fixed-case>H</fixed-case>ausa<fixed-case>H</fixed-case>ate: An Expert Annotated Corpus for <fixed-case>H</fixed-case>ausa Hate Speech Detection</title>
      <author><first>Francielle</first><last>Vargas</last><affiliation>University of São Paulo</affiliation></author>
      <author><first>Samuel</first><last>Guimarães</last><affiliation>UFMG</affiliation></author>
      <author><first>Shamsuddeen Hassan</first><last>Muhammad</last><affiliation>Bayero University, Kano</affiliation></author>
      <author><first>Diego</first><last>Alves</last><affiliation>Saarland University</affiliation></author>
      <author><first>Ibrahim Said</first><last>Ahmad</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Idris</first><last>Abdulmumin</last><affiliation>Ahmadu Bello University, Zaria</affiliation></author>
      <author><first>Diallo</first><last>Mohamed</last><affiliation>Université Saint Thomas D’aquin</affiliation></author>
      <author><first>Thiago</first><last>Pardo</last><affiliation>University of São Paulo</affiliation></author>
      <author><first>Fabrício</first><last>Benevenuto</last><affiliation>Federal University of Minas Gerais (UFMG)</affiliation></author>
      <pages>52-58</pages>
      <abstract>We introduce the first expert annotated corpus of Facebook comments for Hausa hate speech detection. The corpus titled HausaHate comprises 2,000 comments extracted from Western African Facebook pages and manually annotated by three Hausa native speakers, who are also NLP experts. Our corpus was annotated using two different layers. We first labeled each comment according to a binary classification: offensive versus non-offensive. Then, offensive comments were also labeled according to hate speech targets: race, gender and none. Lastly, a baseline model using fine-tuned LLM for Hausa hate speech detection is presented, highlighting the challenges of hate speech detection tasks for indigenous languages in Africa, as well as future advances.</abstract>
      <url hash="0ac434f7">2024.woah-1.5</url>
      <bibkey>vargas-etal-2024-hausahate</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>VIDA</fixed-case>: The Visual Incel Data Archive. A Theory-oriented Annotated Dataset To Enhance Hate Detection Through Visual Culture</title>
      <author><first>Selenia</first><last>Anastasi</last><affiliation>Università degli Studi di Genova</affiliation></author>
      <author><first>Florian</first><last>Schneider</last><affiliation>Hamburg University</affiliation></author>
      <author><first>Chris</first><last>Biemann</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Tim</first><last>Fischer</last><affiliation>Universität Hamburg</affiliation></author>
      <pages>59-67</pages>
      <abstract>Images increasingly constitute a larger portion of internet content, encoding even more complex meanings. Recent studies have highlight the pivotal role of visual communication in the spread of extremist content, particularly that associated with right-wing political ideologies. However, the capability of machine learning systems to recognize such meanings, sometimes implicit, remains limited. To enable future research in this area, we introduce and release VIDA, the Visual Incel Data Archive, a multimodal dataset comprising visual material and internet memes collected from two main Incel communities (Italian and Anglophone) known for their extremist misogynistic content. Following the analytical framework of Shifman (2014), we propose a new taxonomy for annotation across three main levels of analysis: content, form, and stance (hate). This allows for the association of images with fine-grained contextual information that help to identify the presence of offensiveness and a broader set of cultural references, enhancing the understanding of more nuanced aspects in visual communication. In this work we present a statistical analysis of the annotated dataset as well as discuss annotation examples and future line of research.</abstract>
      <url hash="32cf8fa3">2024.woah-1.6</url>
      <bibkey>anastasi-etal-2024-vida</bibkey>
    </paper>
    <paper id="7">
      <title>Towards a Unified Framework for Adaptable Problematic Content Detection via Continual Learning</title>
      <author><first>Ali</first><last>Omrani</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Alireza</first><last>Salkhordeh Ziabari</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Preni</first><last>Golazizian</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Jeffrey</first><last>Sorensen</last><affiliation>Google Jigsaw</affiliation></author>
      <author><first>Morteza</first><last>Dehghani</last><affiliation>University of Southern California</affiliation></author>
      <pages>68-109</pages>
      <abstract>Detecting problematic content, such as hate speech, is a multifaceted and ever-changing task, influenced by social dynamics, user populations, diversity of sources, and evolving language. There has been significant efforts, both in academia and in industry, to develop annotated resources that capture various aspects of problematic content. Due to researchers’ diverse objectives, these annotations are often inconsistent and hence, reports of progress on the detection of problematic content are fragmented. This pattern is expected to persist unless we pool these resources, taking into account the dynamic nature of this issue. In this paper, we propose integrating the available resources, leveraging their dynamic nature to break this pattern, and introduce a continual learning framework and benchmark for problematic content detection. Our benchmark, comprising 84 related tasks, creates a novel measure of progress: prioritizing the adaptability of classifiers to evolving tasks over excelling in specific tasks. To ensure continuous relevance, our benchmark is designed for seamless integration of new tasks. Our results demonstrate that continual learning methods outperform static approaches by up to 17% and 4% AUC in capturing the evolving content and adapting to novel forms of problematic content</abstract>
      <url hash="90d3703f">2024.woah-1.7</url>
      <bibkey>omrani-etal-2024-towards</bibkey>
    </paper>
    <paper id="8">
      <title>From Linguistics to Practice: a Case Study of Offensive Language Taxonomy in <fixed-case>H</fixed-case>ebrew</title>
      <author><first>Chaya</first><last>Liebeskind</last><affiliation>Jerusalem College of Technology , Lev Academic Center</affiliation></author>
      <author><first>Marina</first><last>Litvak</last><affiliation>Shamoon College of Engineering</affiliation></author>
      <author><first>Natalia</first><last>Vanetik</last><affiliation>Shamoon College of Engineering</affiliation></author>
      <pages>110-117</pages>
      <abstract>The perception of offensive language varies based on cultural, social, and individual perspectives. With the spread of social media, there has been an increase in offensive content online, necessitating advanced solutions for its identification and moderation. This paper addresses the practical application of an offensive language taxonomy, specifically targeting Hebrew social media texts. By introducing a newly annotated dataset, modeled after the taxonomy of explicit offensive language of (Lewandowska-Tomaszczyk et al., 2023)„ we provide a comprehensive examination of various degrees and aspects of offensive language. Our findings indicate the complexities involved in the classification of such content. We also outline the implications of relying on fixed taxonomies for Hebrew.</abstract>
      <url hash="79a6ad9f">2024.woah-1.8</url>
      <bibkey>liebeskind-etal-2024-linguistics</bibkey>
    </paper>
    <paper id="9">
      <title>Estimating the Emotion of Disgust in <fixed-case>G</fixed-case>reek Parliament Records</title>
      <author><first>Vanessa</first><last>Lislevand</last><affiliation>Athens University of Economics and Business</affiliation></author>
      <author><first>John</first><last>Pavlopoulos</last><affiliation>Athens University of Economics and Business</affiliation></author>
      <author><first>Panos</first><last>Louridas</last><affiliation>Athens University of Economics and Business</affiliation></author>
      <author><first>Konstantina</first><last>Dritsa</last><affiliation>Athens University of Economics and Business, Greece</affiliation></author>
      <pages>118-135</pages>
      <abstract>We present an analysis of the sentiment in Greek political speech, by focusing on the most frequently occurring emotion in electoral data, the emotion of “disgust”. We show that emotion classification is generally tough, but high accuracy can be achieved for that particular emotion. Using our best-performing model to classify political records of the Greek Parliament Corpus from 1989 to 2020, we studied the points in time when this emotion was frequently occurring and we ranked the Greek political parties based on their estimated score. We then devised an algorithm to investigate the emotional context shift of words that describe specific conditions and that can be used to stigmatise. Given that early detection of such word usage is essential for policy-making, we report two words we found being increasingly used in a negative emotional context, and one that is likely to be carrying stigma, in the studied parliamentary records.</abstract>
      <url hash="b9692913">2024.woah-1.9</url>
      <bibkey>lislevand-etal-2024-estimating</bibkey>
    </paper>
    <paper id="10">
      <title>Simple <fixed-case>LLM</fixed-case> based Approach to Counter Algospeak</title>
      <author><first>Jan</first><last>Fillies</last><affiliation>Freie Universität Berlin</affiliation></author>
      <author><first>Adrian</first><last>Paschke</last><affiliation>Fraunhofer FOKUS</affiliation></author>
      <pages>136-145</pages>
      <abstract>With the use of algorithmic moderation on online communication platforms, an increase in adaptive language aiming to evade the automatic detection of problematic content has been observed. One form of this adapted language is known as “Algospeak” and is most commonly associated with large social media platforms, e.g., TikTok. It builds upon Leetspeak or online slang with its explicit intention to avoid machine readability. The machine-learning algorithms employed to automate the process of content moderation mostly rely on human-annotated datasets and supervised learning, often not adjusted for a wide variety of languages and changes in language. This work uses linguistic examples identified in research literature to introduce a taxonomy for Algospeak and shows that with the use of an LLM (GPT-4), 79.4% of the established terms can be corrected to their true form, or if needed, their underlying associated concepts. With an example sentence, 98.5% of terms are correctly identified. This research demonstrates that LLMs are the future in solving the current problem of moderation avoidance by Algospeak.</abstract>
      <url hash="fe010e66">2024.woah-1.10</url>
      <bibkey>fillies-paschke-2024-simple</bibkey>
    </paper>
    <paper id="11">
      <title>Harnessing Personalization Methods to Identify and Predict Unreliable Information Spreader Behavior</title>
      <author><first>Shaina</first><last>Ashraf</last><affiliation>Philipps-University Marburg</affiliation></author>
      <author><first>Fabio</first><last>Gruschka</last><affiliation>University of Marburg</affiliation></author>
      <author><first>Lucie</first><last>Flek</last><affiliation>CAISA Lab, University of Bonn</affiliation></author>
      <author><first>Charles</first><last>Welch</last><affiliation>University of Bonn</affiliation></author>
      <pages>146-158</pages>
      <abstract>Studies on detecting and understanding the spread of unreliable news on social media have identified key characteristic differences between reliable and unreliable posts. These differences in language use also vary in expression across individuals, making it important to consider personal factors in unreliable news detection. The application of personalization methods for this has been made possible by recent publication of datasets with user histories, though this area is still largely unexplored. In this paper we present approaches to represent social media users in order to improve performance on three tasks: (1) classification of unreliable news posts, (2) classification of unreliable news spreaders, and, (3) prediction of the spread of unreliable news. We compare the User2Vec method from previous work to two other approaches; a learnable user embedding layer trained with the downstream task, and a representation derived from an authorship attribution classifier. We demonstrate that the implemented strategies substantially improve classification performance over state-of-the-art and provide initial results on the task of unreliable news prediction.</abstract>
      <url hash="ea6745f6">2024.woah-1.11</url>
      <bibkey>ashraf-etal-2024-harnessing</bibkey>
    </paper>
    <paper id="12">
      <title>Robust Safety Classifier Against Jailbreaking Attacks: Adversarial Prompt Shield</title>
      <author><first>Jinhwa</first><last>Kim</last><affiliation>University of California, Irvine</affiliation></author>
      <author><first>Ali</first><last>Derakhshan</last><affiliation>University of California, Irvine</affiliation></author>
      <author><first>Ian</first><last>Harris</last><affiliation>University of California, Irvine</affiliation></author>
      <pages>159-170</pages>
      <abstract>Large Language Models’ safety remains a critical concern due to their vulnerability to jailbreaking attacks, which can prompt these systems to produce harmful and malicious responses. Safety classifiers, computational models trained to discern and mitigate potentially harmful, offensive, or unethical outputs, offer a practical solution to address this issue. However, despite their potential, existing safety classifiers often fail when exposed to adversarial attacks such as gradient-optimized suffix attacks. In response, our study introduces Adversarial Prompt Shield (APS), a lightweight safety classifier model that excels in detection accuracy and demonstrates resilience against unseen jailbreaking prompts. We also introduce efficiently generated adversarial training datasets, named Bot Adversarial Noisy Dialogue (BAND), which are designed to fortify the classifier’s robustness. Through extensive testing on various safety tasks and unseen jailbreaking attacks, we demonstrate the effectiveness and resilience of our models. Evaluations show that our classifier has the potential to significantly reduce the Attack Success Rate by up to 44.9%. This advance paves the way for the next generation of more reliable and resilient Large Language Models.</abstract>
      <url hash="875c0f3d">2024.woah-1.12</url>
      <bibkey>kim-etal-2024-robust</bibkey>
    </paper>
    <paper id="13">
      <title>Improving aggressiveness detection using a data augmentation technique based on a Diffusion Language Model</title>
      <author><first>Antonio</first><last>Reyes-Ramírez</last><affiliation>Mathematics Research Center (CIMAT)</affiliation></author>
      <author><first>Mario</first><last>Aragón</last><affiliation>Centro Singular de Investigación en Tecnoloxias Intelixentes (CiTIUS), Universidade de Santiago de Compostela</affiliation></author>
      <author><first>Fernando</first><last>Sánchez-Vega</last><affiliation>Mathematics Research Center (CIMAT)</affiliation></author>
      <author><first>Adrian</first><last>López-Monroy</last><affiliation>Mathematics Research Center (CIMAT)</affiliation></author>
      <pages>171-177</pages>
      <abstract>Cyberbullying has grown in recent years, largely attributed to the proliferation of social media users. This phenomenon manifests in various forms, such as hate speech and offensive language, increasing the necessity of effective detection models to tackle this problem. Most approaches focus on supervised algorithms, which have an important drawback—they heavily depend on the availability of ample training data. This paper attempts to tackle this insufficient data problem using data augmentation (DA) techniques. Concretely, we propose a novel data augmentation technique based on a Diffusion Language Model (DLA). We compare our proposed method against well-known DA techniques, such as contextual augmentation and Easy Data Augmentation (EDA). Our findings reveal a slight but promising improvement, leading to more robust results with very low variance. Additionally, we provide a comprehensive qualitative analysis using classification errors, and complementary analysis, shedding light on the nuances of our approach.</abstract>
      <url hash="aa773895">2024.woah-1.13</url>
      <bibkey>reyes-ramirez-etal-2024-improving</bibkey>
    </paper>
    <paper id="14">
      <title>The <fixed-case>M</fixed-case>exican Gayze: A Computational Analysis of the Attitudes towards the <fixed-case>LGBT</fixed-case>+ Population in <fixed-case>M</fixed-case>exico on Social Media Across a Decade</title>
      <author><first>Scott</first><last>Andersen</last><affiliation>Posgrado en Ciencia e Ingeniería de la Computación, Universidad Nacional Autónoma de México</affiliation></author>
      <author><first>Segio-Luis</first><last>Ojeda-Trueba</last><affiliation>Instituto de Ingeniería, Universidad Nacional Autónoma de México</affiliation></author>
      <author><first>Juan</first><last>Vásquez</last><affiliation>Department of Computer Science, University of Colorado Boulder</affiliation></author>
      <author><first>Gemma</first><last>Bel-Enguix</last><affiliation>Universidad Nacional Autónoma de México</affiliation></author>
      <pages>178-200</pages>
      <abstract>Thanks to the popularity of social media, data generated by online communities provides an abundant source of diverse language information. This abundance of data allows NLP practitioners and computational linguists to analyze sociolinguistic phenomena occurring in digital communication. In this paper, we analyze the Twitter discourse around the Mexican Spanish-speaking LGBT+ community. For this, we evaluate how the polarity of some nouns related to the LGBT+ community has evolved in conversational settings using a corpus of tweets that cover a time span of ten years. We hypothesize that social media’s fast-moving, turbulent linguistic environment encourages language evolution faster than ever before. Our results indicate that most of the inspected terms have undergone some shift in denotation or connotation. No other generalizations can be observed in the data, given the difficulty that current NLP methods have to account for polysemy, and the wide differences between the various subgroups that make up the LGBT+ community. A fine-grained analysis of a series of LGBT+-related lexical terms is also included in this work.</abstract>
      <url hash="fcc9a255">2024.woah-1.14</url>
      <bibkey>andersen-etal-2024-mexican</bibkey>
    </paper>
    <paper id="15">
      <title><fixed-case>X</fixed-case>-posing Free Speech: Examining the Impact of Moderation Relaxation on Online Social Networks</title>
      <author><first>Arvindh</first><last>Arun</last><affiliation>IIIT Hyderabad</affiliation></author>
      <author><first>Saurav</first><last>Chhatani</last><affiliation>IIIT Hyderabad</affiliation></author>
      <author><first>Jisun</first><last>An</last><affiliation>Luddy School of Informatics, Computing, and Engineering, Indiana University Bloomington</affiliation></author>
      <author><first>Ponnurangam</first><last>Kumaraguru</last><affiliation>IIIT Hyderabad</affiliation></author>
      <pages>201-211</pages>
      <abstract>We investigate the impact of free speech and the relaxation of moderation on online social media platforms using Elon Musk’s takeover of Twitter as a case study. By curating a dataset of over 10 million tweets, our study employs a novel framework combining content and network analysis. Our findings reveal a significant increase in the distribution of certain forms of hate content, particularly targeting the LGBTQ+ community and liberals. Network analysis reveals the formation of cohesive hate communities facilitated by influential bridge users, with substantial growth in interactions hinting at increased hate production and diffusion. By tracking the temporal evolution of PageRank, we identify key influencers, primarily self-identified far-right supporters disseminating hate against liberals and woke culture. Ironically, embracing free speech principles appears to have enabled hate speech against the very concept of freedom of expression and free speech itself. Our findings underscore the delicate balance platforms must strike between open expression and robust moderation to curb the proliferation of hate online.</abstract>
      <url hash="37dd8fda">2024.woah-1.15</url>
      <bibkey>arun-etal-2024-x</bibkey>
    </paper>
    <paper id="16">
      <title>The Uli Dataset: An Exercise in Experience Led Annotation of o<fixed-case>GBV</fixed-case></title>
      <author><first>Arnav</first><last>Arora</last><affiliation>University of Copenhagen</affiliation></author>
      <author><first>Maha</first><last>Jinadoss</last><affiliation>Independent</affiliation></author>
      <author><first>Cheshta</first><last>Arora</last><affiliation>Independent</affiliation></author>
      <author><first>Denny</first><last>George</last><affiliation>Tattle Civic Tech</affiliation></author>
      <author><first/><last>Brindaalakshmi</last><affiliation>Independent</affiliation></author>
      <author><first>Haseena</first><last>Khan</last><affiliation>Bebaak Collective</affiliation></author>
      <author><first>Kirti</first><last>Rawat</last><affiliation>Independent</affiliation></author>
      <author><first/><last>Div</last><affiliation>Independent</affiliation></author>
      <author><first/><last>Ritash</last><affiliation>Independent</affiliation></author>
      <author><first>Seema</first><last>Mathur</last><affiliation>National Council of Women Leaders</affiliation></author>
      <pages>212-222</pages>
      <abstract>Online gender-based violence has grown concomitantly with the adoption of the internet and social media. Its effects are worse in the Global majority where many users use social media in languages other than English. The scale and volume of conversations on the internet have necessitated the need for automated detection of hate speech and, more specifically, gendered abuse. There is, however, a lack of language-specific and contextual data to build such automated tools. In this paper, we present a dataset on gendered abuse in three languages- Hindi, Tamil and Indian English. The dataset comprises of tweets annotated along three questions pertaining to the experience of gender abuse, by experts who identify as women or a member of the LGBTQIA+ community in South Asia. Through this dataset, we demonstrate a participatory approach to creating datasets that drive AI systems.</abstract>
      <url hash="66cbf046">2024.woah-1.16</url>
      <bibkey>arora-etal-2024-uli</bibkey>
    </paper>
    <paper id="17">
      <title>Towards Interpretable Hate Speech Detection using Large Language Model-extracted Rationales</title>
      <author><first>Ayushi</first><last>Nirmal</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Amrita</first><last>Bhattacharjee</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Paras</first><last>Sheth</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Huan</first><last>Liu</last><affiliation>Arizona State University</affiliation></author>
      <pages>223-233</pages>
      <abstract>Although social media platforms are a prominent arena for users to engage in interpersonal discussions and express opinions, the facade and anonymity offered by social media may allow users to spew hate speech and offensive content. Given the massive scale of such platforms, there arises a need to automatically identify and flag instances of hate speech. Although several hate speech detection methods exist, most of these black-box methods are not interpretable or explainable by design. To address the lack of interpretability, in this paper, we propose to use state-of-the-art Large Language Models (LLMs) to extract features in the form of rationales from the input text, to train a base hate speech classifier, thereby enabling faithful interpretability by design. Our framework effectively combines the textual understanding capabilities of LLMs and the discriminative power of state-of-the-art hate speech classifiers to make these classifiers faithfully interpretable. Our comprehensive evaluation on a variety of social media hate speech datasets demonstrate: (1) the goodness of the LLM-extracted rationales, and (2) the surprising retention of detector performance even after training to ensure interpretability. All code and data will be made available at https://github.com/AmritaBh/shield.</abstract>
      <url hash="b5167a00">2024.woah-1.17</url>
      <bibkey>nirmal-etal-2024-towards</bibkey>
    </paper>
    <paper id="18">
      <title>A <fixed-case>B</fixed-case>ayesian Quantification of Aporophobia and the Aggravating Effect of Low–Wealth Contexts on Stigmatization</title>
      <author><first>Ryan</first><last>Brate</last><affiliation>KNAW</affiliation></author>
      <author><first>Marieke</first><last>Van Erp</last><affiliation>KNAW Humanities Cluster</affiliation></author>
      <author><first>Antal</first><last>Van Den Bosch</last><affiliation>Utrecht University</affiliation></author>
      <pages>234-243</pages>
      <abstract>Aporophobia, a negative social bias against poverty and the poor, has been highlighted asan overlooked phenomenon in toxicity detec-tion in texts. Aporophobia is potentially im-portant both as a standalone form of toxicity,but also given its potential as an aggravatingfactor in the wider stigmatization of groups. Asyet, there has been limited quantification of thisphenomenon. In this paper, we first quantifythe extent of aporophobia, as observable in Red-dit data: contrasting estimates of stigmatisingtopic propensity between low–wealth contextsand high–wealth contexts via Bayesian estima-tion. Next, we consider aporophobia as a causalfactor in the prejudicial association of groupswith stigmatising topics, by introducing peoplegroup as a variable, specifically Black people.This group is selected given its history of be-ing the subject of toxicity. We evaluate theaggravating effect on the observed n–grams in-dicative of stigmatised topics observed in com-ments which refer to Black people, due to thepresence of low–wealth contexts. We performthis evaluation via a Structural Causal Mod-elling approach, performing interventions onsimulations via Bayesian models, for three hy-pothesised causal mechanisms.</abstract>
      <url hash="73ef8c55">2024.woah-1.18</url>
      <bibkey>brate-etal-2024-bayesian</bibkey>
    </paper>
    <paper id="19">
      <title>Toxicity Classification in <fixed-case>U</fixed-case>krainian</title>
      <author><first>Daryna</first><last>Dementieva</last><affiliation>Technical University of Munich</affiliation></author>
      <author><first>Valeriia</first><last>Khylenko</last><affiliation>TU Munich</affiliation></author>
      <author><first>Nikolay</first><last>Babakov</last><affiliation>Centro Singular de Investigación en Tecnoloxías Intelixentes (CiTIUS), Universidade de Santiago de Compostela</affiliation></author>
      <author><first>Georg</first><last>Groh</last><affiliation>TUM</affiliation></author>
      <pages>244-255</pages>
      <abstract>The task of toxicity detection is still a relevant task, especially in the context of safe and fair LMs development. Nevertheless, labeled binary toxicity classification corpora are not available for all languages, which is understandable given the resource-intensive nature of the annotation process. Ukrainian, in particular, is among the languages lacking such resources. To our knowledge, there has been no existing toxicity classification corpus in Ukrainian. In this study, we aim to fill this gap by investigating cross-lingual knowledge transfer techniques and creating labeled corpora by: (i)~translating from an English corpus, (ii)~filtering toxic samples using keywords, and (iii)~annotating with crowdsourcing. We compare LLMs prompting and other cross-lingual transfer approaches with and without fine-tuning offering insights into the most robust and efficient baselines.</abstract>
      <url hash="20a37f0f">2024.woah-1.19</url>
      <bibkey>dementieva-etal-2024-toxicity</bibkey>
    </paper>
    <paper id="20">
      <title>A Strategy Labelled Dataset of Counterspeech</title>
      <author><first>Aashima</first><last>Poudhar</last><affiliation>Heriot-Watt University</affiliation></author>
      <author><first>Ioannis</first><last>Konstas</last><affiliation>Heriot-Watt University</affiliation></author>
      <author><first>Gavin</first><last>Abercrombie</last><affiliation>Heriot Watt University</affiliation></author>
      <pages>256-265</pages>
      <abstract>Increasing hateful conduct online demands effective counterspeech strategies to mitigate its impact. We introduce a novel dataset annotated with such strategies, aimed at facilitating the generation of targeted responses to hateful language. We labelled 1000 hate speech/counterspeech pairs from an existing dataset with strategies established in the social sciences. We find that a one-shot prompted classification model achieves promising accuracy in classifying the strategies according to the manual labels, demonstrating the potential of generative Large Language Models (LLMs) to distinguish between counterspeech strategies.</abstract>
      <url hash="edeb0e7b">2024.woah-1.20</url>
      <bibkey>poudhar-etal-2024-strategy</bibkey>
    </paper>
    <paper id="21">
      <title>Improving Covert Toxicity Detection by Retrieving and Generating References</title>
      <author><first>Dong-Ho</first><last>Lee</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Hyundong</first><last>Cho</last><affiliation>USC, Information Sciences Institute</affiliation></author>
      <author><first>Woojeong</first><last>Jin</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Jihyung</first><last>Moon</last><affiliation>SoftlyAI</affiliation></author>
      <author><first>Sungjoon</first><last>Park</last><affiliation>SoftlyAI</affiliation></author>
      <author><first>Paul</first><last>Röttger</last><affiliation>University of Oxford</affiliation></author>
      <author><first>Jay</first><last>Pujara</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Roy Ka-wei</first><last>Lee</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <pages>266-274</pages>
      <abstract>Models for detecting toxic content play an important role in keeping people safe online. There has been much progress in detecting overt toxicity. Covert toxicity, however, remains a challenge because its detection requires an understanding of implicit meaning and subtle connotations. In this paper, we explore the potential of leveraging references, such as external knowledge and textual interpretations, to enhance the detection of covert toxicity. We run experiments on two covert toxicity datasets with two types of references: 1) information retrieved from a search API, and 2) interpretations generated by large language models. We find that both types of references improve detection, with the latter being more useful than the former. We also find that generating interpretations grounded on properties of covert toxicity, such as humor and irony, lead to the largest improvements</abstract>
      <url hash="3b9c302d">2024.woah-1.21</url>
      <bibkey>lee-etal-2024-improving</bibkey>
    </paper>
    <paper id="22">
      <title>Subjective Isms? On the Danger of Conflating Hate and Offence in Abusive Language Detection</title>
      <author><first>Amanda</first><last>Cercas Curry</last><affiliation>Bocconi University</affiliation></author>
      <author><first>Gavin</first><last>Abercrombie</last><affiliation>Heriot Watt University</affiliation></author>
      <author><first>Zeerak</first><last>Talat</last><affiliation>Independent Researcher</affiliation></author>
      <pages>275-282</pages>
      <abstract>Natural language processing research has begun to embrace the notion of annotator subjectivity, motivated by variations in labelling. This approach understands each annotator’s view as valid, which can be highly suitable for tasks that embed subjectivity, e.g., sentiment analysis. However, this construction may be inappropriate for tasks such as hate speech detection, as it affords equal validity to all positions on e.g., sexism or racism. We argue that the conflation of hate and offence can invalidate findings on hate speech, and call for future work to be situated in theory, disentangling hate from its orthogonal concept, offence.</abstract>
      <url hash="7fd9dd11">2024.woah-1.22</url>
      <bibkey>cercas-curry-etal-2024-subjective</bibkey>
    </paper>
    <paper id="23">
      <title>From Languages to Geographies: Towards Evaluating Cultural Bias in Hate Speech Datasets</title>
      <author><first>Manuel</first><last>Tonneau</last><affiliation>University of Oxford, World Bank</affiliation></author>
      <author><first>Diyi</first><last>Liu</last><affiliation>University of Oxford</affiliation></author>
      <author><first>Samuel</first><last>Fraiberger</last><affiliation>World Bank, NYU, MIT</affiliation></author>
      <author><first>Ralph</first><last>Schroeder</last><affiliation>University of Oxford</affiliation></author>
      <author><first>Scott</first><last>Hale</last><affiliation>University of Oxford</affiliation></author>
      <author><first>Paul</first><last>Röttger</last><affiliation>University of Oxford</affiliation></author>
      <pages>283-311</pages>
      <abstract>Perceptions of hate can vary greatly across cultural contexts. Hate speech (HS) datasets, however, have traditionally been developed by language. This hides potential cultural biases, as one language may be spoken in different countries home to different cultures. In this work, we evaluate cultural bias in HS datasets by leveraging two interrelated cultural proxies: language and geography. We conduct a systematic survey of HS datasets in eight languages and confirm past findings on their English-language bias, but also show that this bias has been steadily decreasing in the past few years. For three geographically-widespread languages—English, Arabic and Spanish—we then leverage geographical metadata from tweets to approximate geo-cultural contexts by pairing language and country information. We find that HS datasets for these languages exhibit a strong geo-cultural bias, largely overrepresenting a handful of countries (e.g., US and UK for English) relative to their prominence in both the broader social media population and the general population speaking these languages. Based on these findings, we formulate recommendations for the creation of future HS datasets.</abstract>
      <url hash="38be5508">2024.woah-1.23</url>
      <bibkey>tonneau-etal-2024-languages</bibkey>
    </paper>
    <paper id="24">
      <title><fixed-case>SGH</fixed-case>ate<fixed-case>C</fixed-case>heck: Functional Tests for Detecting Hate Speech in Low-Resource Languages of <fixed-case>S</fixed-case>ingapore</title>
      <author><first>Ri Chi</first><last>Ng</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Nirmalendu</first><last>Prakash</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Ming Shan</first><last>Hee</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Kenny Tsu Wei</first><last>Choo</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Roy Ka-wei</first><last>Lee</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <pages>312-327</pages>
      <abstract>To address the limitations of current hate speech detection models, we introduce SGHateCheck, a novel framework designed for the linguistic and cultural context of Singapore and Southeast Asia. It extends the functional testing approach of HateCheck and MHC, employing large language models for translation and paraphrasing into Singapore’s main languages, and refining these with native annotators. SGHateCheck reveals critical flaws in state-of-the-art models, highlighting their inadequacy in sensitive content moderation. This work aims to foster the development of more effective hate speech detection tools for diverse linguistic environments, particularly for Singapore and Southeast Asia contexts.</abstract>
      <url hash="40a89d0e">2024.woah-1.24</url>
      <bibkey>ng-etal-2024-sghatecheck</bibkey>
    </paper>
  </volume>
</collection>
