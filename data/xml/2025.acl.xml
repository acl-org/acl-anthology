<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.acl">
  <volume id="long" ingest-date="2025-07-14" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (ACL 2025)</booktitle>
      <editor><first>Wanxiang</first><last>Che</last></editor>
      <editor><first>Joyce</first><last>Nabende</last></editor>
      <editor><first>Ekaterina</first><last>Shutova</last></editor>
      <editor><first>Mohammad Taher</first><last>Pilehvar</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Vienna, Austria</address>
      <month>July</month>
      <year>2025</year>
      <venue>acl</venue>
      <venue>ws</venue>
      <isbn>979-8-89176-251-0</isbn>
    </meta>
    <frontmatter>
      <url hash="4ca339c1">2025.acl-long.0</url>
      <bibkey>acl-ws-2025-long</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>E</fixed-case>com<fixed-case>S</fixed-case>cript<fixed-case>B</fixed-case>ench: A Multi-task Benchmark for <fixed-case>E</fixed-case>-commerce Script Planning via Step-wise Intention-Driven Product Association</title>
      <author><first>Weiqi</first><last>Wang</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Limeng</first><last>Cui</last><affiliation>Amazon</affiliation></author>
      <author><first>Xin</first><last>Liu</last><affiliation>Amazon</affiliation></author>
      <author><first>Sreyashi</first><last>Nag</last><affiliation>Amazon</affiliation></author>
      <author><first>Wenju</first><last>Xu</last><affiliation>Amazon</affiliation></author>
      <author><first>Chen</first><last>Luo</last><affiliation>Amazon</affiliation></author>
      <author><first>Sheikh Muhammad</first><last>Sarwar</last></author>
      <author><first>Yang</first><last>Li</last></author>
      <author><first>Hansu</first><last>Gu</last><affiliation>Amazon</affiliation></author>
      <author><first>Hui</first><last>Liu</last><affiliation>Amazon</affiliation></author>
      <author><first>Changlong</first><last>Yu</last><affiliation>Department of Computer Science and Engineering, The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Jiaxin</first><last>Bai</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Yifan</first><last>Gao</last><affiliation>Amazon</affiliation></author>
      <author><first>Haiyang</first><last>Zhang</last><affiliation>Amazon</affiliation></author>
      <author><first>Qi</first><last>He</last><affiliation>Amazon</affiliation></author>
      <author><first>Shuiwang</first><last>Ji</last><affiliation>Texas A&amp;M University</affiliation></author>
      <author><first>Yangqiu</first><last>Song</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <pages>1-22</pages>
      <abstract>Goal-oriented script planning, or the ability to devise coherent sequences of actions toward specific goals, is commonly employed by humans to plan for typical activities. In e-commerce, customers increasingly seek LLM-based assistants to generate scripts and recommend products at each step, thereby facilitating convenient and efficient shopping experiences. However, this capability remains underexplored due to several challenges, including the inability of LLMs to simultaneously conduct script planning and product retrieval, difficulties in matching products caused by semantic discrepancies between planned actions and search queries, and a lack of methods and benchmark data for evaluation. In this paper, we step forward by formally defining the task of E-commerce Script Planning (EcomScript) as three sequential subtasks. We propose a novel framework that enables the scalable generation of product-enriched scripts by associating products with each step based on the semantic similarity between the actions and their purchase intentions. By applying our framework to real-world e-commerce data, we construct the very first large-scale EcomScript dataset, EcomScriptBench, which includes 605,229 scripts sourced from 2.4 million products. Human annotations are then conducted to provide gold labels for a sampled subset, forming an evaluation benchmark. Extensive experiments reveal that current (L)LMs face significant challenges with EcomScript tasks, even after fine-tuning, while injecting product purchase intentions improves their performance.</abstract>
      <url hash="a77aa6a5">2025.acl-long.1</url>
      <bibkey>wang-etal-2025-ecomscriptbench</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>G</fixed-case>raph<fixed-case>N</fixed-case>arrator: Generating Textual Explanations for Graph Neural Networks</title>
      <author><first>Bo</first><last>Pan</last></author>
      <author><first>Zhen</first><last>Xiong</last></author>
      <author><first>Guanchen</first><last>Wu</last></author>
      <author><first>Zheng</first><last>Zhang</last></author>
      <author><first>Yifei</first><last>Zhang</last><affiliation>Emory University</affiliation></author>
      <author><first>Yuntong</first><last>Hu</last><affiliation>Emory University</affiliation></author>
      <author><first>Liang</first><last>Zhao</last><affiliation>Emory University</affiliation></author>
      <pages>23-42</pages>
      <abstract>Graph representation learning has garnered significant attention due to its broad applications in various domains, such as recommendation systems and social network analysis. Despite advancements in graph learning methods, challenges still remain in explainability when graphs are associated with semantic features. In this paper, we present GraphNarrator, the first method designed to generate natural language explanations for Graph Neural Networks. GraphNarrator employs a generative language model that maps input-output pairs to explanations reflecting the model’s decision-making process. To address the lack of ground truth explanations to train the model, we propose first generating pseudo-labels that capture the model’s decisions from saliency-based explanations, then using Expert Iteration to iteratively train the pseudo-label generator based on training objectives on explanation quality. The high-quality pseudo-labels are finally utilized to train an end-to-end explanation generator model. Extensive experiments are conducted to demonstrate the effectiveness of GraphNarrator in producing faithful, concise, and human-preferred natural language explanations.</abstract>
      <url hash="cb020e86">2025.acl-long.2</url>
      <bibkey>pan-etal-2025-graphnarrator</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>M</fixed-case>-<fixed-case>R</fixed-case>eward<fixed-case>B</fixed-case>ench: Evaluating Reward Models in Multilingual Settings</title>
      <author><first>Srishti</first><last>Gureja</last><affiliation>Writesonic</affiliation></author>
      <author><first>Lester James Validad</first><last>Miranda</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Shayekh Bin</first><last>Islam</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Rishabh</first><last>Maheshwary</last><affiliation>ServiceNow</affiliation></author>
      <author><first>Drishti</first><last>Sharma</last><affiliation>Cohere</affiliation></author>
      <author><first>Gusti Triandi</first><last>Winata</last></author>
      <author><first>Nathan</first><last>Lambert</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Sebastian</first><last>Ruder</last><affiliation>Facebook</affiliation></author>
      <author><first>Sara</first><last>Hooker</last><affiliation>Cohere For AI</affiliation></author>
      <author><first>Marzieh</first><last>Fadaee</last><affiliation>Cohere Labs</affiliation></author>
      <pages>43-58</pages>
      <abstract>Reward models (RMs) have driven the state-of-the-art performance of LLMs today by enabling the integration of human feedback into the language modeling process. However, RMs are primarily trained and evaluated in English, and their capabilities in multilingual settings remain largely understudied. In this work, we conduct a systematic evaluation of several reward models in multilingual settings. We first construct the first-of-its-kind multilingual RM evaluation benchmark, M-RewardBench, consisting of 2.87k preference instances for 23 typologically diverse languages, that tests the chat, safety, reasoning, and translation capabilities of RMs. We then rigorously evaluate a wide range of reward models on M-RewardBench, offering fresh insights into their performance across diverse languages. We identify a significant gap in RMs’ performances between English and non-English languages and show that RM preferences can change substantially from one language to another. We also present several findings on how different multilingual aspects impact RM performance. Specifically, we show that the performance of RMs is improved with improved translation quality. Similarly, we demonstrate that the models exhibit better performance for high-resource languages. We release M-RewardBench dataset and the codebase in this study to facilitate a better understanding of RM evaluation in multilingual settings.</abstract>
      <url hash="95b89af0">2025.acl-long.3</url>
      <bibkey>gureja-etal-2025-rewardbench</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>ELABORATION</fixed-case>: A Comprehensive Benchmark on Human-<fixed-case>LLM</fixed-case> Competitive Programming</title>
      <author><first>Xinwei</first><last>Yang</last><affiliation>Sichuan University</affiliation></author>
      <author><first>Zhaofeng</first><last>Liu</last></author>
      <author><first>Chen</first><last>Huang</last><affiliation>Sichuan University</affiliation></author>
      <author><first>Jiashuai</first><last>Zhang</last><affiliation>Sichuan University</affiliation></author>
      <author><first>Tong</first><last>Zhang</last></author>
      <author><first>Yifan</first><last>Zhang</last></author>
      <author><first>Wenqiang</first><last>Lei</last><affiliation>Sichuan University</affiliation></author>
      <pages>59-104</pages>
      <abstract>While recent research increasingly emphasizes the value of human-LLM collaboration in competitive programming and proposes numerous empirical methods, a comprehensive understanding remains elusive due to the fragmented nature of existing studies and their use of diverse, application-specific human feedback. Thus, our work serves a three-fold purpose: First, we present the first taxonomy of human feedback consolidating the entire programming process, which promotes fine-grained evaluation. Second, we introduce ELABORATIONSET, a novel programming dataset specifically designed for human-LLM collaboration, meticulously annotated to enable large-scale simulated human feedback and facilitate cost-effective real human interaction studies. Third, we introduce ELABORATION, a novel benchmark to facilitate a thorough assessment of human-LLM competitive programming. With ELABORATION, we pinpoint strengthes and weaknesses of existing methods, thereby setting the foundation for furture improvement. Our dataset and code will be openly released.</abstract>
      <url hash="d57e56b2">2025.acl-long.4</url>
      <bibkey>yang-etal-2025-elaboration</bibkey>
    </paper>
    <paper id="5">
      <title>The Impossibility of Fair <fixed-case>LLM</fixed-case>s</title>
      <author><first>Jacy Reese</first><last>Anthis</last><affiliation>Computer Science Department, Stanford University, University of Chicago and Sentience Institute</affiliation></author>
      <author><first>Kristian</first><last>Lum</last><affiliation>Twitter</affiliation></author>
      <author><first>Michael</first><last>Ekstrand</last><affiliation>Drexel University</affiliation></author>
      <author><first>Avi</first><last>Feller</last><affiliation>University of California, Berkeley</affiliation></author>
      <author><first>Chenhao</first><last>Tan</last><affiliation>University of Chicago</affiliation></author>
      <pages>105-120</pages>
      <abstract>The rise of general-purpose artificial intelligence (AI) systems, particularly large language models (LLMs), has raised pressing moral questions about how to reduce bias and ensure fairness at scale. Researchers have documented a sort of “bias” in the significant correlations between demographics (e.g., race, gender) in LLM prompts and responses, but it remains unclear how LLM fairness could be evaluated with more rigorous definitions, such as group fairness or fair representations. We analyze a variety of technical fairness frameworks and find inherent challenges in each that make the development of a fair LLM intractable. We show that each framework either does not logically extend to the general-purpose AI context or is infeasible in practice, primarily due to the large amounts of unstructured training data and the many potential combinations of human populations, use cases, and sensitive attributes. These inherent challenges would persist for general-purpose AI, including LLMs, even if empirical challenges, such as limited participatory input and limited measurement methods, were overcome. Nonetheless, fairness will remain an important type of model evaluation, and there are still promising research directions, particularly the development of standards for the responsibility of LLM developers, context-specific evaluations, and methods of iterative, participatory, and AI-assisted evaluation that could scale fairness across the diverse contexts of modern human-AI interaction.</abstract>
      <url hash="f9d6beda">2025.acl-long.5</url>
      <bibkey>anthis-etal-2025-impossibility</bibkey>
    </paper>
    <paper id="6">
      <title>Intuitive Fine-Tuning: Towards Simplifying Alignment into a Single Process</title>
      <author><first>Ermo</first><last>Hua</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Biqing</first><last>Qi</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Kaiyan</first><last>Zhang</last></author>
      <author><first>Kai</first><last>Tian</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Xingtai</first><last>Lv</last></author>
      <author><first>Ning</first><last>Ding</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Bowen</first><last>Zhou</last><affiliation>Tsinghua University</affiliation></author>
      <pages>121-136</pages>
      <abstract>Supervised Fine-Tuning (SFT) and Preference Optimization (PO) are key processes for aligning Language Models (LMs) with human preferences post pre-training. While SFT excels in efficiency and PO in effectiveness, they are often combined sequentially without integrating their optimization objectives. This approach ignores the opportunities to bridge their paradigm gap and take the strengths from both. In this paper, we interpret SFT and PO with two sub-processes — *Preference Estimation* and *Transition Optimization* — defined at token level within the Markov Decision Process (MDP). This modeling shows that SFT is only a special case of PO with inferior estimation and optimization. PO estimates the model’s preference by its entire generation, while SFT only scores model’s subsequent predicted tokens based on prior tokens from ground truth answer. These priors deviates from model’s distribution, hindering the preference estimation and transition optimization. Building on this view, we introduce **Intuitive Fine-Tuning (IFT)** to integrate SFT and PO into a single process. Through a temporal residual connection, IFT brings better estimation and optimization by capturing LMs’ intuitive sense of its entire answers. But it solely relies on a single policy and the same volume of non-preference-labeled data as SFT. Our experiments show that IFT performs comparably or even superiorly to SFT and some typical PO methods across several tasks, particularly those requires generation, reasoning, and fact-following abilities. An explainable Frozen Lake game further validates the effectiveness of IFT for getting competitive policy.</abstract>
      <url hash="572e112d">2025.acl-long.6</url>
      <bibkey>hua-etal-2025-intuitive</bibkey>
    </paper>
    <paper id="7">
      <title>Bias in Language Models: Beyond Trick Tests and Towards <fixed-case>RUTE</fixed-case>d Evaluation</title>
      <author><first>Kristian</first><last>Lum</last><affiliation>Twitter</affiliation></author>
      <author><first>Jacy Reese</first><last>Anthis</last><affiliation>Computer Science Department, Stanford University, University of Chicago and Sentience Institute</affiliation></author>
      <author><first>Kevin</first><last>Robinson</last><affiliation>Google</affiliation></author>
      <author><first>Chirag</first><last>Nagpal</last><affiliation>Google</affiliation></author>
      <author><first>Alexander Nicholas</first><last>D’Amour</last><affiliation>Google</affiliation></author>
      <pages>137-161</pages>
      <abstract>Standard bias benchmarks used for large language models (LLMs) measure the association between social attributes in model inputs and single-word model outputs. We test whether these benchmarks are robust to lengthening the model outputs via a more realistic user prompt, in the commonly studied domain of gender-occupation bias, as a step towards measuring Realistic Use and Tangible Effects (i.e., RUTEd evaluations). From the current literature, we adapt three standard metrics of next-word prediction (neutrality, skew, and stereotype), and we develop analogous RUTEd evaluations in three contexts of real-world LLM use: children’s bedtime stories, user personas, and English language learning exercises. We find that standard bias metrics have no significant correlation with long-form output metrics. For example, selecting the least biased model based on the standard “trick tests” coincides with selecting the least biased model based on longer output no more than random chance. There may not yet be evidence to justify standard benchmarks as reliable proxies of real-world biases, and we encourage further development of context-specific RUTEd evaluations.</abstract>
      <url hash="79ed094f">2025.acl-long.7</url>
      <bibkey>lum-etal-2025-bias</bibkey>
    </paper>
    <paper id="8">
      <title>Sliding Windows Are Not the End: Exploring Full Ranking with Long-Context Large Language Models</title>
      <author><first>Wenhan</first><last>Liu</last></author>
      <author><first>Xinyu</first><last>Ma</last><affiliation>Baidu</affiliation></author>
      <author><first>Yutao</first><last>Zhu</last></author>
      <author><first>Ziliang</first><last>Zhao</last></author>
      <author><first>Shuaiqiang</first><last>Wang</last></author>
      <author><first>Dawei</first><last>Yin</last><affiliation>Baidu</affiliation></author>
      <author><first>Zhicheng</first><last>Dou</last><affiliation>Renmin University of China</affiliation></author>
      <pages>162-176</pages>
      <abstract>Large Language Models (LLMs) have shown exciting performance in listwise passage ranking. Due to the limited input length, existing methods often adopt the sliding window strategy. Such a strategy, though effective, is inefficient as it involves repetitive and serialized processing, which usually re-evaluates relevant passages multiple times. As a result, it incurs redundant API costs, which are proportional to the number of inference tokens. The development of long-context LLMs enables the full ranking of all passages within a single inference, avoiding redundant API costs. In this paper, we conduct a comprehensive study of long-context LLMs for ranking tasks in terms of efficiency and effectiveness. Surprisingly, our experiments reveal that full ranking with long-context LLMs can deliver superior performance in the supervised fine-tuning setting with a huge efficiency improvement. Furthermore, we identify two limitations of fine-tuning the full ranking model based on existing methods: (1) sliding window strategy fails to produce a full ranking list as a training label, and (2) the language modeling loss cannot emphasize top-ranked passage IDs in the label. To alleviate these issues, we propose a new complete listwise label construction approach and a novel importance-aware learning objective for full ranking. Experiments show the superior performance of our method over baselines.</abstract>
      <url hash="38f71c05">2025.acl-long.8</url>
      <bibkey>liu-etal-2025-sliding</bibkey>
    </paper>
    <paper id="9">
      <title>The Impact of Auxiliary Patient Data on Automated Chest <fixed-case>X</fixed-case>-Ray Report Generation and How to Incorporate It</title>
      <author><first>Aaron</first><last>Nicolson</last></author>
      <author><first>Shengyao</first><last>Zhuang</last><affiliation>CSIRO</affiliation></author>
      <author><first>Jason</first><last>Dowling</last><affiliation>CSIRO</affiliation></author>
      <author><first>Bevan</first><last>Koopman</last><affiliation>CSIRO and University of Queensland</affiliation></author>
      <pages>177-203</pages>
      <abstract>This study investigates the integration of diverse patient data sources into multimodal language models for automated chest X-ray (CXR) report generation. Traditionally, CXR report generation relies solely on data from a patient’s CXR exam, overlooking valuable information from patient electronic health records. Utilising the MIMIC-CXR and MIMIC-IV-ED datasets, we investigate the use of patient data from emergency department (ED) records — such as vital signs measured and medicines reconciled during an ED stay — for CXR report generation, with the aim of enhancing diagnostic accuracy. We also investigate conditioning CXR report generation on the clinical history section of radiology reports, which has been overlooked in the literature. We introduce a novel approach to transform these heterogeneous data sources into patient data embeddings that prompt a multimodal language model (CXRMate-ED). Our comprehensive evaluation indicates that using a broader set of patient data significantly enhances diagnostic accuracy. The model, training code, and dataset are publicly available.</abstract>
      <url hash="267bb051">2025.acl-long.9</url>
      <bibkey>nicolson-etal-2025-impact</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>CLEME</fixed-case>2.0: Towards Interpretable Evaluation by Disentangling Edits for Grammatical Error Correction</title>
      <author><first>Jingheng</first><last>Ye</last></author>
      <author><first>Zishan</first><last>Xu</last></author>
      <author><first>Yinghui</first><last>Li</last></author>
      <author><first>Linlin</first><last>Song</last></author>
      <author><first>Qingyu</first><last>Zhou</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Hai-Tao</first><last>Zheng</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Ying</first><last>Shen</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Wenhao</first><last>Jiang</last><affiliation>Guangming Laboratory</affiliation></author>
      <author><first>Hong-Gee</first><last>Kim</last></author>
      <author><first>Ruitong</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Xin</first><last>Su</last><affiliation>WeChat, Tencent</affiliation></author>
      <author><first>Zifei</first><last>Shan</last><affiliation>WeChat, Tencent</affiliation></author>
      <pages>204-222</pages>
      <abstract>The paper focuses on the interpretability of Grammatical Error Correction (GEC) evaluation metrics, which received little attention in previous studies. To bridge the gap, we introduce **CLEME2.0**, a reference-based metric describing four fundamental aspects of GEC systems: hit-correction, wrong-correction, under-correction, and over-correction. They collectively contribute to exposing critical qualities and locating drawbacks of GEC systems. Evaluating systems by combining these aspects also leads to superior human consistency over other reference-based and reference-less metrics. Extensive experiments on two human judgment datasets and six reference datasets demonstrate the effectiveness and robustness of our method, achieving a new state-of-the-art result. Our codes are released at https://github.com/THUKElab/CLEME.</abstract>
      <url hash="2ce4ee34">2025.acl-long.10</url>
      <bibkey>ye-etal-2025-cleme2</bibkey>
    </paper>
    <paper id="11">
      <title><fixed-case>S</fixed-case>truc<fixed-case>T</fixed-case>ext-Eval: Evaluating Large Language Model’s Reasoning Ability in Structure-Rich Text</title>
      <author><first>Zhouhong</first><last>Gu</last></author>
      <author><first>Haoning</first><last>Ye</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xingzhou</first><last>Chen</last></author>
      <author><first>Zeyang</first><last>Zhou</last></author>
      <author><first>Hongwei</first><last>Feng</last><affiliation>Fudan University</affiliation></author>
      <author><first>Yanghua</first><last>Xiao</last><affiliation>Fudan University</affiliation></author>
      <pages>223-244</pages>
      <abstract>The effective utilization of structured data, integral to corporate data strategies, has been challenged by the rise of large language models (LLMs) capable of processing unstructured information. This shift prompts the question: can LLMs interpret structured data directly in its unstructured form? We propose an automatic evaluation data generation method for assessing LLMs’ reasoning capabilities on structure-rich text to explore this. Our approach supports 8 structured languages and 29 tasks, generating data with adjustable complexity through controllable nesting and structural width. We introduce StrucText-Eval, a benchmark containing 5,800 pre-generated and annotated samples designed to evaluate how well LLMs understand and reason through structured text. StrucText-Eval is divided into two suites: a regular Test suite (3,712 samples) and a Test-Hard suite (2,088 samples), the latter emphasizing the gap between human and model performance on more complex tasks. Experimental results show that while open-source LLMs achieve a maximum accuracy of 74.9% on the standard dataset, their performance drops significantly to 45.8% on the harder dataset. In contrast, human participants reach an accuracy of 92.6% on StrucText-Eval-Hard, highlighting LLMs’ current limitations in handling intricate structural information.</abstract>
      <url hash="aee53b8e">2025.acl-long.11</url>
      <bibkey>gu-etal-2025-structext</bibkey>
    </paper>
    <paper id="12">
      <title>Literature Meets Data: A Synergistic Approach to Hypothesis Generation</title>
      <author><first>Haokun</first><last>Liu</last><affiliation>University of Chicago</affiliation></author>
      <author><first>Yangqiaoyu</first><last>Zhou</last><affiliation>University of Chicago</affiliation></author>
      <author><first>Mingxuan</first><last>Li</last></author>
      <author><first>Chenfei</first><last>Yuan</last></author>
      <author><first>Chenhao</first><last>Tan</last><affiliation>University of Chicago</affiliation></author>
      <pages>245-281</pages>
      <abstract>AI holds promise for transforming scientific processes, including hypothesis generation. Prior work on hypothesis generation can be broadly categorized into theory-driven and data-driven approaches. While both have proven effective in generating novel and plausible hypotheses, it remains an open question whether they can complement each other. To address this, we develop the first method that combines literature-based insights with data to perform LLM-powered hypothesis generation. We apply our method on five different datasets and demonstrate that integrating literature and data outperforms other baselines (8.97% over few-shot, 15.75% over literature-based alone, and 3.37% over data-driven alone). Additionally, we conduct the first human evaluation to assess the utility of LLM-generated hypotheses in assisting human decision-making on two challenging tasks: deception detection and AI generated content detection. Our results show that human accuracy improves significantly by 7.44% and 14.19% on these tasks, respectively. These findings suggest that integrating literature-based and data-driven approaches provides a comprehensive and nuanced framework for hypothesis generation and could open new avenues for scientific inquiry.</abstract>
      <url hash="fa3eed53">2025.acl-long.12</url>
      <bibkey>liu-etal-2025-literature</bibkey>
    </paper>
    <paper id="13">
      <title><fixed-case>GAPO</fixed-case>: Learning Preferential Prompt through Generative Adversarial Policy Optimization</title>
      <author><first>Zhouhong</first><last>Gu</last></author>
      <author><first>Xingzhou</first><last>Chen</last></author>
      <author><first>Xiaoran</first><last>Shi</last></author>
      <author><first>Tao</first><last>Wang</last></author>
      <author><first>Suhang</first><last>Zheng</last></author>
      <author><first>Tianyu</first><last>Li</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Hongwei</first><last>Feng</last><affiliation>Fudan University</affiliation></author>
      <author><first>Yanghua</first><last>Xiao</last><affiliation>Fudan University</affiliation></author>
      <pages>282-296</pages>
      <abstract>Recent advances in large language models have highlighted the critical need for precise control over model outputs through predefined constraints. While existing methods attempt to achieve this through either direct instruction-response synthesis or preferential response optimization, they often struggle with constraint understanding and adaptation. This limitation becomes particularly evident when handling fine-grained constraints, leading to either hallucination or brittle performance. We introduce Generative Adversarial Policy Optimization (GAPO), a novel framework that combines GAN-based training dynamics with an encoder-only reward model to progressively learn and adapt to increasingly complex constraints. GAPO leverages adversarial training to automatically generate training samples of varying difficulty while utilizing the encoder-only architecture to better capture prompt-response relationships. Extensive experiments demonstrate GAPO’s superior performance across multiple benchmarks, particularly in scenarios requiring fine-grained constraint handling, where it significantly outperforms existing methods like PPO, DPO, and KTO. Our results suggest that GAPO’s unique approach to preferential prompt learning offers a more robust and effective solution for controlling LLM outputs.</abstract>
      <url hash="fac01cc3">2025.acl-long.13</url>
      <bibkey>gu-etal-2025-gapo</bibkey>
    </paper>
    <paper id="14">
      <title>Tree-of-Evolution: Tree-Structured Instruction Evolution for Code Generation in Large Language Models</title>
      <author><first>Ziyang</first><last>Luo</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Kaixin</first><last>Li</last></author>
      <author><first>Hongzhan</first><last>Lin</last><affiliation>Hong Kong Baptist University</affiliation></author>
      <author><first>Yuchen</first><last>Tian</last></author>
      <author><first>Mohan</first><last>Kankanhalli</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Jing</first><last>Ma</last><affiliation>Hong Kong Baptist University</affiliation></author>
      <pages>297-316</pages>
      <abstract>Data synthesis has become a crucial research area in large language models (LLMs), especially for generating high-quality instruction fine-tuning data to enhance downstream performance. In code generation, a key application of LLMs, manual annotation of code instruction data is costly. Recent methods, such as Code Evol-Instruct and OSS-Instruct, leverage LLMs to synthesize large-scale code instruction data, significantly improving LLM coding capabilities. However, these approaches face limitations due to unidirectional synthesis and randomness-driven generation, which restrict data quality and diversity. To overcome these challenges, we introduce Tree-of-Evolution (ToE), a novel framework that models code instruction synthesis process with a tree structure, exploring multiple evolutionary paths to alleviate the constraints of unidirectional generation. Additionally, we propose optimization-driven evolution, which refines each generation step based on the quality of the previous iteration. Experimental results across five widely-used coding benchmarks—HumanEval, MBPP, EvalPlus, LiveCodeBench, and BigCodeBench—demonstrate that base models fine-tuned on just 75k data synthesized by our method achieve comparable or superior performance to the state-of-the-art open-weight Code LLM, Qwen2.5-Coder-Instruct, which was fine-tuned on millions of samples.</abstract>
      <url hash="9f6299af">2025.acl-long.14</url>
      <bibkey>luo-etal-2025-tree</bibkey>
    </paper>
    <paper id="15">
      <title>Delving into Multilingual Ethical Bias: The <fixed-case>MSQAD</fixed-case> with Statistical Hypothesis Tests for Large Language Models</title>
      <author><first>Seunguk</first><last>Yu</last><affiliation>Chung-Ang University</affiliation></author>
      <author><first>Juhwan</first><last>Choi</last><affiliation>AITRICS</affiliation></author>
      <author><first>YoungBin</first><last>Kim</last><affiliation>Chung-Ang University</affiliation></author>
      <pages>317-340</pages>
      <abstract>Despite the recent strides in large language models, studies have underscored the existence of social biases within these systems. In this paper, we delve into the validation and comparison of the ethical biases of LLMs concerning globally discussed and potentially sensitive topics, hypothesizing that these biases may arise from language-specific distinctions. Introducing the Multilingual Sensitive Questions &amp; Answers Dataset (**MSQAD**), we collected news articles from Human Rights Watch covering 17 topics, and generated socially sensitive questions along with corresponding responses in multiple languages. We scrutinized the biases of these responses across languages and topics, employing two statistical hypothesis tests. The results showed that the null hypotheses were rejected in most cases, indicating biases arising from cross-language differences. It demonstrates that ethical biases in responses are widespread across various languages, and notably, these biases were prevalent even among different LLMs. By making the proposed MSQAD openly available, we aim to facilitate future research endeavors focused on examining cross-language biases in LLMs and their variant models.</abstract>
      <url hash="dbe0e0d5">2025.acl-long.15</url>
      <bibkey>yu-etal-2025-delving</bibkey>
    </paper>
    <paper id="16">
      <title><fixed-case>R</fixed-case>e<fixed-case>SCORE</fixed-case>: Label-free Iterative Retriever Training for Multi-hop Question Answering with Relevance-Consistency Supervision</title>
      <author><first>Dosung</first><last>Lee</last><affiliation>Korea University</affiliation></author>
      <author><first>Wonjun</first><last>Oh</last></author>
      <author><first>Boyoung</first><last>Kim</last></author>
      <author><first>Minyoung</first><last>Kim</last></author>
      <author><first>Joonsuk</first><last>Park</last><affiliation>University of Richmond</affiliation></author>
      <author><first>Paul Hongsuck</first><last>Seo</last><affiliation>Korea University</affiliation></author>
      <pages>341-359</pages>
      <abstract>Multi-hop question answering (MHQA) involves reasoning across multiple documents to answer complex questions. Dense retrievers typically outperform sparse methods like BM25 by leveraging semantic embeddings in many tasks; however, they require labeled query-document pairs for fine-tuning, which poses a significant challenge in MHQA due to the complexity of the reasoning steps. To overcome this limitation, we introduce Retriever Supervision with Consistency and Relevance (ReSCORE), a novel method for training dense retrievers for MHQA without the need for labeled documents. ReSCORE leverages large language models to measure document-question relevance with answer consistency and utilizes this information to train a retriever within an iterative question-answering framework. Evaluated on three MHQA benchmarks, our extensive experiments demonstrate the effectiveness of ReSCORE, with significant improvements in retrieval performance that consequently lead to state-of-the-art Exact Match and F1 scores for MHQA.</abstract>
      <url hash="43109489">2025.acl-long.16</url>
      <bibkey>lee-etal-2025-rescore</bibkey>
    </paper>
    <paper id="17">
      <title><fixed-case>FACT</fixed-case>-<fixed-case>AUDIT</fixed-case>: An Adaptive Multi-Agent Framework for Dynamic Fact-Checking Evaluation of Large Language Models</title>
      <author><first>Hongzhan</first><last>Lin</last><affiliation>Hong Kong Baptist University</affiliation></author>
      <author><first>Yang</first><last>Deng</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Yuxuan</first><last>Gu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Wenxuan</first><last>Zhang</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Jing</first><last>Ma</last><affiliation>Hong Kong Baptist University</affiliation></author>
      <author><first>See-Kiong</first><last>Ng</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Tat-Seng</first><last>Chua</last><affiliation>National University of Singapore</affiliation></author>
      <pages>360-381</pages>
      <abstract>Large Language Models (LLMs) have significantly advanced the fact-checking studies. However, existing automated fact-checking evaluation methods rely on static datasets and classification metrics, which fail to automatically evaluate the justification production and uncover the nuanced limitations of LLMs in fact-checking. In this work, we introduce FACT-AUDIT, an agent-driven framework that adaptively and dynamically assesses LLMs’ fact-checking capabilities. Leveraging importance sampling principles and multi-agent collaboration, FACT-AUDIT generates adaptive and scalable datasets, performs iterative model-centric evaluations, and updates assessments based on model-specific responses. By incorporating justification production alongside verdict prediction, this framework provides a comprehensive and evolving audit of LLMs’ factual reasoning capabilities, to investigate their trustworthiness. Extensive experiments demonstrate that FACT-AUDIT effectively differentiates among state-of-the-art LLMs, providing valuable insights into model strengths and limitations in model-centric fact-checking analysis.</abstract>
      <url hash="7e4868bf">2025.acl-long.17</url>
      <bibkey>lin-etal-2025-fact</bibkey>
    </paper>
    <paper id="18">
      <title>Statistical Deficiency for Task Inclusion Estimation</title>
      <author><first>Loïc</first><last>Fosse</last></author>
      <author><first>Frederic</first><last>Bechet</last><affiliation>Académie d’Aix-Marseille</affiliation></author>
      <author><first>Benoit</first><last>Favre</last><affiliation>Université d’Aix-Marseille</affiliation></author>
      <author><first>Géraldine</first><last>Damnati</last><affiliation>Orange Innovation</affiliation></author>
      <author><first>Gwénolé</first><last>Lecorvé</last><affiliation>Orange</affiliation></author>
      <author><first>Maxime</first><last>Darrin</last></author>
      <author><first>Philippe</first><last>Formont</last><affiliation>École de technologie supérieure, Université du Québec and Université Paris-Saclay</affiliation></author>
      <author><first>Pablo</first><last>Piantanida</last><affiliation>Université Paris-Saclay, CNRS</affiliation></author>
      <pages>382-415</pages>
      <abstract>Tasks are central in machine learning, as they are the most natural objects to assess the capabilities of current models. The trend is to build general models able to address any task. Even though transfer learning and multitask learning try to leverage the underlying task space, no well-founded tools are available to study its structure. This study proposes a theoretically grounded setup to define the notion of task and to compute the inclusion between two tasks from a statistical deficiency point of view. We propose a tractable proxy as information sufficiency to estimate the degree of inclusion between tasks, show its soundness on synthetic data, and use it to reconstruct empirically the classic NLP pipeline.</abstract>
      <url hash="65e72f69">2025.acl-long.18</url>
      <bibkey>fosse-etal-2025-statistical</bibkey>
    </paper>
    <paper id="19">
      <title>Towards Robust and Efficient Federated Low-Rank Adaptation with Heterogeneous Clients</title>
      <author><first>Jabin</first><last>Koo</last><affiliation>Pohang University of Science and Technology</affiliation></author>
      <author><first>Minwoo</first><last>Jang</last><affiliation>Pohang University of Science and Technology</affiliation></author>
      <author><first>Jungseul</first><last>Ok</last><affiliation>POSTECH</affiliation></author>
      <pages>416-429</pages>
      <abstract>Federated fine-tuning for Large Language Models (LLMs) has recently gained attention due to the heavy communication overhead of transmitting large model updates. Low Rank Adaptation (LoRA) has been proposed as a solution, yet its application in federated learning is complicated by discordance in aggregation. Existing methods addressing this discordance often suffer from performance degradation at low ranks in heterogeneous data settings. In response, we introduce LoRA-A^2 (Low Rank Adaptation with Alternating freeze and Adaptive rank selection), which demonstrates robustness in challenging settings with low ranks and high data heterogeneity. Our experimental findings reveal that LoRA-A^2 maintains performance even under extreme heterogeneity and low rank conditions, achieving up to a 99.8% reduction in uploaded parameters compared to full fine-tuning without compromising performance. This adaptive mechanism boosts robustness and communication efficiency in federated fine-tuning, enabling the practical deployment of LLMs in resource-constrained environments.</abstract>
      <url hash="ce9caf62">2025.acl-long.19</url>
      <bibkey>koo-etal-2025-towards</bibkey>
    </paper>
    <paper id="20">
      <title><fixed-case>LLM</fixed-case>-Powered Test Case Generation for Detecting Bugs in Plausible Programs</title>
      <author><first>Kaibo</first><last>Liu</last></author>
      <author><first>Zhenpeng</first><last>Chen</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Yiyang</first><last>Liu</last></author>
      <author><first>Jie M.</first><last>Zhang</last><affiliation>King’s College London, University of London</affiliation></author>
      <author><first>Mark</first><last>Harman</last><affiliation>University College London, University of London</affiliation></author>
      <author><first>Yudong</first><last>Han</last></author>
      <author><first>Yun</first><last>Ma</last><affiliation>Peking University</affiliation></author>
      <author><first>Yihong</first><last>Dong</last></author>
      <author><first>Ge</first><last>Li</last><affiliation>Peking University</affiliation></author>
      <author><first>Gang</first><last>Huang</last><affiliation>Peking University</affiliation></author>
      <pages>430-440</pages>
      <abstract>Detecting tricky bugs in plausible programs, those that pass existing test suites yet still contain bugs, remains a significant challenge in software testing. To address this problem, we propose TrickCatcher, an LLM-powered approach to generating test cases for uncovering bugs in plausible programs. TrickCatcher operates in three stages: First, it uses an LLM to generate program variants based on the program under test (PUT) and its specification. Second, it employs an LLM to construct an input generator from the specification for producing test inputs. Finally, these inputs are executed on both the PUT and its program variants to detect inconsistencies in their outputs. We evaluate TrickCatcher on two datasets, TrickyBugs and EvalPlus, which include 366 human-written and 151 AI-generated plausible programs with tricky bugs. TrickCatcher achieves recall, precision, and F1 scores that are 1.80×, 2.65×, and 1.66× those of the state-of-the-art baselines, respectively. Code and data used are available at https://github.com/RinCloud/TrickCatcher/.</abstract>
      <url hash="3bce16a4">2025.acl-long.20</url>
      <bibkey>liu-etal-2025-llm</bibkey>
    </paper>
    <paper id="21">
      <title>Capture the Key in Reasoning to Enhance <fixed-case>C</fixed-case>o<fixed-case>T</fixed-case> Distillation Generalization</title>
      <author><first>Chengwei</first><last>Dai</last></author>
      <author><first>Kun</first><last>Li</last><affiliation>University of Chinese Academy of Sciences</affiliation></author>
      <author><first>Wei</first><last>Zhou</last><affiliation>Institute of Information Engeering</affiliation></author>
      <author><first>Songlin</first><last>Hu</last></author>
      <pages>441-465</pages>
      <abstract>As Large Language Models (LLMs) scale up and gain powerful Chain-of-Thoughts (CoTs) reasoning abilities, practical resource constraints drive efforts to distill these capabilities into more compact Smaller Language Models (SLMs). We find that CoTs consist mainly of simple reasoning forms, with a small proportion (4.7%) of key reasoning steps that truly impact conclusions. However, previous distillation methods typically involve supervised fine-tuning student SLMs only on correct CoTs data produced by teacher LLMs, resulting in students struggling to learn the key, instead imitating the teacher’s reasoning forms and making errors or omissions in reasoning. To address these issues, drawing an analogy to human learning, where analyzing mistakes according to correct solutions often reveals the crucial steps leading to successes or failures, we propose mistak<b>E</b>-<b>D</b>riven key reason<b>I</b>ng step distilla<b>T</b>ion (<b>EDIT</b>), a novel method that further aids SLMs learning key reasoning steps rather than mere simple fine-tuning. Firstly, to expose the crucial steps in CoTs, we carefully design specific prompts to generate dual CoTs data with similar reasoning paths but divergent conclusions. Then, we apply the minimum edit distance algorithm on the dual CoTs data to locate these key steps and optimize the likelihood on these tokens. Extensive experiments and analysis validate the effectiveness of EDIT across both in-domain(IND) and out-of-domain(OOD) benchmark reasoning datasets.</abstract>
      <url hash="82b31e02">2025.acl-long.21</url>
      <bibkey>dai-etal-2025-capture</bibkey>
    </paper>
    <paper id="22">
      <title>How to Enable Effective Cooperation Between Humans and <fixed-case>NLP</fixed-case> Models: A Survey of Principles, Formalizations, and Beyond</title>
      <author><first>Chen</first><last>Huang</last><affiliation>Sichuan University</affiliation></author>
      <author><first>Yang</first><last>Deng</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Wenqiang</first><last>Lei</last><affiliation>Sichuan University</affiliation></author>
      <author><first>Jiancheng</first><last>Lv</last><affiliation>Sichuan University</affiliation></author>
      <author><first>Tat-Seng</first><last>Chua</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Jimmy</first><last>Huang</last><affiliation>York University and York University</affiliation></author>
      <pages>466-488</pages>
      <abstract>With the advancement of large language models (LLMs), intelligent models have evolved from mere tools to autonomous agents with their own goals and strategies for cooperating with humans. This evolution has birthed a novel paradigm in NLP, i.e., human-model cooperation, that has yielded remarkable progress in numerous NLP tasks in recent years. In this paper, we take the first step to present a thorough review of human-model cooperation, exploring its principles, formalizations, and open challenges. In particular, we introduce a new taxonomy that provides a unified perspective to summarize existing approaches. Also, we discuss potential frontier areas and their corresponding challenges. We regard our work as an entry point, paving the way for more breakthrough research in this regard.</abstract>
      <url hash="8fae2695">2025.acl-long.22</url>
      <bibkey>huang-etal-2025-enable</bibkey>
    </paper>
    <paper id="23">
      <title>Enhancing Hyperbole and Metaphor Detection with Their Bidirectional Dynamic Interaction and Emotion Knowledge</title>
      <author><first>Li</first><last>Zheng</last></author>
      <author><first>Sihang</first><last>Wang</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Hao</first><last>Fei</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Zuquan</first><last>Peng</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Fei</first><last>Li</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Jianming</first><last>Fu</last></author>
      <author><first>Chong</first><last>Teng</last></author>
      <author><first>Donghong</first><last>Ji</last></author>
      <pages>489-499</pages>
      <abstract>Text-based hyperbole and metaphor detection are of great significance for natural language processing (NLP) tasks. However, due to their semantic obscurity and expressive diversity, it is rather challenging to identify them. Existing methods mostly focus on superficial text features, ignoring the associations of hyperbole and metaphor as well as the effect of implicit emotion on perceiving these rhetorical devices. To implement these hypotheses, we propose an emotion-guided hyperbole and metaphor detection framework based on bidirectional dynamic interaction (EmoBi). Firstly, the emotion analysis module deeply mines the emotion connotations behind hyperbole and metaphor. Next, the emotion-based domain mapping module identifies the target and source domains to gain a deeper understanding of the implicit meanings of hyperbole and metaphor. Finally, the bidirectional dynamic interaction module enables the mutual promotion between hyperbole and metaphor. Meanwhile, a verification mechanism is designed to ensure detection accuracy and reliability. Experiments show that EmoBi outperforms all baseline methods on four datasets. Specifically, compared to the current SoTA, the F1 score increased by 28.1% for hyperbole detection on the TroFi dataset and 23.1% for metaphor detection on the HYPO-L dataset. These results, underpinned by in-depth analyses, underscore the effectiveness and potential of our approach for advancing hyperbole and metaphor detection.</abstract>
      <url hash="54d5a445">2025.acl-long.23</url>
      <bibkey>zheng-etal-2025-enhancing</bibkey>
    </paper>
    <paper id="24">
      <title><fixed-case>U</fixed-case>ni<fixed-case>ICL</fixed-case>: An Efficient <fixed-case>ICL</fixed-case> Framework Unifying Compression, Selection, and Generation</title>
      <author><first>Jun</first><last>Gao</last></author>
      <author><first>Qi</first><last>Lv</last><affiliation>Shanghai Artificial Intelligence Laboratory and Harbin Institute of Technology</affiliation></author>
      <author><first>Zili</first><last>Wang</last></author>
      <author><first>Tianxiang</first><last>Wu</last></author>
      <author><first>Ziqiang</first><last>Cao</last><affiliation>Soochow University, China</affiliation></author>
      <author><first>Wenjie</first><last>Li</last><affiliation>The Hong Kong Polytechnic University, The Hong Kong Polytechnic University</affiliation></author>
      <pages>500-510</pages>
      <abstract>In-context learning (ICL) enhances the reasoning abilities of Large Language Models (LLMs) by prepending a few demonstrations. It motivates researchers to introduce more examples to provide additional contextual information for the generation. However, existing methods show a significant limitation due to the problem of excessive growth in context length which causes a large hardware burden. Additionally, shallow-relevant examples selected by out-off-shelf tools hinder LLMs from capturing useful contextual information for generation. In this paper, to approach these limitations, we propose <b>UniICL</b>, a novel Unified ICL framework that unifies demonstration compression, demonstration selection, and final response generation. Furthermore, to avoid repeated compression of the same demonstration and boost inference efficiency, we design a tailored compression strategy that allows UniICL caching compression results into Demonstration Bank(DB). Extensive out-of-domain evaluations prove the advantages of UniICL in both effectiveness and efficiency.</abstract>
      <url hash="b521de96">2025.acl-long.24</url>
      <bibkey>gao-etal-2025-uniicl</bibkey>
    </paper>
    <paper id="25">
      <title><fixed-case>B</fixed-case>elarusian<fixed-case>GLUE</fixed-case>: Towards a Natural Language Understanding Benchmark for <fixed-case>B</fixed-case>elarusian</title>
      <author><first>Maksim</first><last>Aparovich</last><affiliation>Brno University of Technology</affiliation></author>
      <author><first>Volha</first><last>Harytskaya</last><affiliation>Independent</affiliation></author>
      <author><first>Vladislav</first><last>Poritski</last><affiliation>unaffiliated</affiliation></author>
      <author><first>Oksana</first><last>Volchek</last><affiliation>Independent</affiliation></author>
      <author><first>Pavel</first><last>Smrz</last><affiliation>Brno University of Technology</affiliation></author>
      <pages>511-527</pages>
      <abstract>In the epoch of multilingual large language models (LLMs), it is still challenging to evaluate the models’ understanding of lower-resourced languages, which motivates further development of expert-crafted natural language understanding benchmarks. We introduce BelarusianGLUE — a natural language understanding benchmark for Belarusian, an East Slavic language, with ≈15K instances in five tasks: sentiment analysis, linguistic acceptability, word in context, Winograd schema challenge, textual entailment. A systematic evaluation of BERT models and LLMs against this novel benchmark reveals that both types of models approach human-level performance on easier tasks, such as sentiment analysis, but there is a significant gap in performance between machine and human on a harder task — Winograd schema challenge. We find the optimal choice of model type to be task-specific: e.g. BERT models underperform on textual entailment task but are competitive for linguistic acceptability. We release the datasets (https://hf.co/datasets/maaxap/BelarusianGLUE) and evaluation code (https://github.com/maaxap/BelarusianGLUE).</abstract>
      <url hash="8db6cd0a">2025.acl-long.25</url>
      <bibkey>aparovich-etal-2025-belarusianglue</bibkey>
    </paper>
    <paper id="26">
      <title>A Survey on Foundation Language Models for Single-cell Biology</title>
      <author><first>Fan</first><last>Zhang</last></author>
      <author><first>Hao</first><last>Chen</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Zhihong</first><last>Zhu</last><affiliation>Tencent</affiliation></author>
      <author><first>Ziheng</first><last>Zhang</last></author>
      <author><first>Zhenxi</first><last>Lin</last><affiliation>Tencent</affiliation></author>
      <author><first>Ziyue</first><last>Qiao</last><affiliation>Great Bay University</affiliation></author>
      <author><first>Yefeng</first><last>Zheng</last><affiliation>Westlake University</affiliation></author>
      <author><first>Xian</first><last>Wu</last><affiliation>Tencent</affiliation></author>
      <pages>528-549</pages>
      <abstract>The recent advancements in language models have significantly catalyzed progress in computational biology. A growing body of research strives to construct unified foundation models for single-cell biology, with language models serving as the cornerstone. In this paper, we systematically review the developments in foundation language models designed specifically for single-cell biology. Our survey offers a thorough analysis of various incarnations of single-cell foundation language models, viewed through the lens of both pre-trained language models (PLMs) and large language models (LLMs). This includes an exploration of data tokenization strategies, pre-training/tuning paradigms, and downstream single-cell data analysis tasks. Additionally, we discuss the current challenges faced by these pioneering works and speculate on future research directions. Overall, this survey provides a comprehensive overview of the existing single-cell foundation language models, paving the way for future research endeavors.</abstract>
      <url hash="2e465d9a">2025.acl-long.26</url>
      <bibkey>zhang-etal-2025-survey-foundation</bibkey>
    </paper>
    <paper id="27">
      <title><fixed-case>R</fixed-case>ule<fixed-case>A</fixed-case>rena: A Benchmark for Rule-Guided Reasoning with <fixed-case>LLM</fixed-case>s in Real-World Scenarios</title>
      <author><first>Ruiwen</first><last>Zhou</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Wenyue</first><last>Hua</last></author>
      <author><first>Liangming</first><last>Pan</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Sitao</first><last>Cheng</last></author>
      <author><first>Xiaobao</first><last>Wu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>En</first><last>Yu</last></author>
      <author><first>William Yang</first><last>Wang</last><affiliation>UC Santa Barbara</affiliation></author>
      <pages>550-572</pages>
      <abstract>This paper introduces RuleArena, a novel and challenging benchmark designed to evaluate the ability of large language models (LLMs) to follow complex, real-world rules in reasoning. Covering three practical domains – airline baggage fees, NBA transactions, and tax regulations – RuleArena assesses LLMs’ proficiency in handling intricate natural language instructions that demand long-context understanding, logical reasoning, and accurate mathematical computation. Two key attributes distinguish RuleArena from traditional rule-based reasoning benchmarks: (1) it extends beyond standard first-order logic representations, and (2) it is grounded in authentic, practical scenarios, providing insights into the suitability and reliability of LLMs for real-world applications. Our findings reveal several notable limitations in LLMs: (1) they struggle to identify and apply the appropriate rules, frequently becoming confused by similar but distinct regulations, (2) they cannot consistently perform accurate mathematical computations, even when they correctly identify the relevant rules, and (3) in general, they perform poorly in the benchmark. We also observe a significant performance boost when LLMs are provided with external tools for oracle math and logic operations. These results highlight significant challenges and promising research directions in advancing LLMs’ rule-guided reasoning capabilities in real-life applications. Our codes and data are publicly available on https://github.com/skyriver-2000/rulearena.</abstract>
      <url hash="a73b55be">2025.acl-long.27</url>
      <bibkey>zhou-etal-2025-rulearena</bibkey>
    </paper>
    <paper id="28">
      <title>Extending <fixed-case>LLM</fixed-case> Context Window with Adaptive Grouped Positional Encoding: A Training-Free Method</title>
      <author><first>Xinhao</first><last>Xu</last></author>
      <author><first>Jiaxin</first><last>Li</last></author>
      <author><first>Hui</first><last>Chen</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Zijia</first><last>Lin</last><affiliation>Kuaishou Technology</affiliation></author>
      <author><first>Jungong</first><last>Han</last><affiliation>The University of Sheffield and University of Sheffield</affiliation></author>
      <author><first>Guiguang</first><last>Ding</last><affiliation>Tsinghua University</affiliation></author>
      <pages>573-587</pages>
      <abstract>Processing long input remains a significant challenge for large language models (LLMs) due to the scarcity of large-scale long-context training data and the high computational cost of training models for extended context windows. In this paper, we propose **Ada**ptive **Gro**uped **P**ositional **E**ncoding (AdaGroPE), a training-free, plug-and-play method to enhance long-context understanding in existing LLMs. AdaGroPE progressively increases the reuse count of relative positions as the distance grows and dynamically adapts the positional encoding mapping to sequence length, thereby fully exploiting the range of pre-trained position embeddings. Its design is consistent with the principles of rotary position embedding (RoPE) and aligns with human perception of relative distance, enabling robust performance in real-world settings with variable-length inputs. Extensive experiments across various benchmarks demonstrate that our AdaGroPE consistently achieves state-of-the-art performance, surpassing baseline methods and even outperforming LLMs inherently designed for long-context processing on certain tasks.</abstract>
      <url hash="23bb7254">2025.acl-long.28</url>
      <bibkey>xu-etal-2025-extending</bibkey>
    </paper>
    <paper id="29">
      <title>Semantic Exploration with Adaptive Gating for Efficient Problem Solving with Language Models</title>
      <author><first>Sungjae</first><last>Lee</last><affiliation>Pohang University of Science and Technology</affiliation></author>
      <author><first>Hyejin</first><last>Park</last><affiliation>Pohang University of Science and Technology</affiliation></author>
      <author><first>Jaechang</first><last>Kim</last><affiliation>POSTECH</affiliation></author>
      <author><first>Jungseul</first><last>Ok</last><affiliation>POSTECH</affiliation></author>
      <pages>588-606</pages>
      <abstract>Recent advancements in large language models (LLMs) have shown remarkable potential in various complex tasks requiring multi-step reasoning methods like tree search to explore diverse reasoning paths. However, existing methods often suffer from computational inefficiency and redundancy. First, they overlook the diversity of task difficulties, leading to unnecessarily extensive searches even for easy tasks. Second, they neglect the semantics of reasoning paths, resulting in redundant exploration of semantically identical paths. To address these limitations, we propose Semantic Exploration with Adaptive Gating (SEAG), a computationally efficient method. SEAG employs an adaptive gating mechanism that dynamically decides whether to conduct a tree search, based on the confidence level of answers from a preceding simple reasoning method. Furthermore, its tree-based exploration consolidates semantically identical reasoning steps, reducing redundant explorations while maintaining or even improving accuracy. Our extensive experiments demonstrate that SEAG significantly improves accuracy by 4.3% on average while requiring only 31% of computational costs compared to existing tree search-based methods on complex reasoning benchmarks including GSM8K and ARC with diverse language models such as Llama2, Llama3, and Mistral. Our code is available at https://github.com/ml-postech/SEAG-semantic-exploration-with-adaptive-gating.</abstract>
      <url hash="1db8498e">2025.acl-long.29</url>
      <bibkey>lee-etal-2025-semantic</bibkey>
    </paper>
    <paper id="30">
      <title><fixed-case>H</fixed-case>otel<fixed-case>M</fixed-case>atch-<fixed-case>LLM</fixed-case>: Joint Multi-Task Training of Small and Large Language Models for Efficient Multimodal Hotel Retrieval</title>
      <author><first>Arian</first><last>Askari</last></author>
      <author><first>Emmanouil</first><last>Stergiadis</last><affiliation>Booking</affiliation></author>
      <author><first>Ilya</first><last>Gusev</last><affiliation>Booking</affiliation></author>
      <author><first>Moran</first><last>Beladev</last></author>
      <pages>607-619</pages>
      <abstract>We present HotelMatch-LLM, a multimodal dense retrieval model for the travel domain that enables natural language property search, addressing the limitations of traditional travel search engines which require users to start with a destination and editing search parameters. HotelMatch-LLM features three key innovations: (1) Domain-specific multi-task optimization with three novel retrieval, visual, and language modeling objectives; (2) Asymmetrical dense retrieval architecture combining a small language model (SLM) for efficient online query processing and a large language model (LLM) for embedding hotel data; and (3) Extensive image processing to handle all property image galleries. Experiments on four diverse test sets show HotelMatch-LLM significantly outperforms state-of-the-art models, including VISTA and MARVEL. Specifically, on the test set—main query type—we achieve 0.681 for HotelMatch-LLM compared to 0.603 for the most effective baseline, MARVEL. Our analysis highlights the impact of our multi-task optimization, the generalizability of HotelMatch-LLM across LLM architectures, and its scalability for processing large image galleries.</abstract>
      <url hash="035b69d4">2025.acl-long.30</url>
      <bibkey>askari-etal-2025-hotelmatch</bibkey>
    </paper>
    <paper id="31">
      <title>Can Multimodal Large Language Models Understand Spatial Relations?</title>
      <author><first>Jingping</first><last>Liu</last><affiliation>East China University of Science and Technology</affiliation></author>
      <author><first>Ziyan</first><last>Liu</last></author>
      <author><first>Zhedong</first><last>Cen</last><affiliation>East China University of Science and Technology</affiliation></author>
      <author><first>Yan</first><last>Zhou</last></author>
      <author><first>Yinan</first><last>Zou</last></author>
      <author><first>Weiyan</first><last>Zhang</last><affiliation>East China University of Science and Technology</affiliation></author>
      <author><first>Haiyun</first><last>Jiang</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Tong</first><last>Ruan</last></author>
      <pages>620-632</pages>
      <abstract>Spatial relation reasoning is a crucial task for multimodal large language models (MLLMs) to understand the objective world. However, current benchmarks have issues like relying on bounding boxes, ignoring perspective substitutions, or allowing questions to be answered using only the model’s prior knowledge without image understanding. To address these issues, we introduce SpatialMQA, a human-annotated spatial relation reasoning benchmark based on COCO2017, which enables MLLMs to focus more on understanding images in the objective world. To ensure data quality, we design a well-tailored annotation procedure, resulting in SpatialMQA consisting of 5,392 samples. Based on this benchmark, a series of closed- and open-source MLLMs are implemented and the results indicate that the current state-of-the-art MLLM achieves only 48.14% accuracy, far below the human-level accuracy of 98.40%. Extensive experimental analyses are also conducted, suggesting the future research directions. The benchmark and codes are available at https://huggingface.co/datasets/liuziyan/SpatialMQA.</abstract>
      <url hash="eb6e9f69">2025.acl-long.31</url>
      <bibkey>liu-etal-2025-multimodal-large</bibkey>
    </paper>
    <paper id="32">
      <title><tex-math>S^3</tex-math> - Semantic Signal Separation</title>
      <author><first>Márton</first><last>Kardos</last><affiliation>Aarhus University</affiliation></author>
      <author><first>Jan</first><last>Kostkan</last><affiliation>Aarhus University</affiliation></author>
      <author><first>Kenneth</first><last>Enevoldsen</last></author>
      <author><first>Arnault-Quentin</first><last>Vermillet</last></author>
      <author><first>Kristoffer</first><last>Nielbo</last><affiliation>Aarhus University</affiliation></author>
      <author><first>Roberta</first><last>Rocca</last><affiliation>Aarhus University</affiliation></author>
      <pages>633-666</pages>
      <abstract>Topic models are useful tools for discovering latent semantic structures in large textual corpora. Recent efforts have been oriented at incorporating contextual representations in topic modeling and have been shown to outperform classical topic models. These approaches are typically slow, volatile, and require heavy preprocessing for optimal results. We present Semantic Signal Separation (<tex-math>S^3</tex-math>), a theory-driven topic modeling approach in neural embedding spaces. <tex-math>S^3</tex-math> conceptualizes topics as independent axes of semantic space and uncovers these by decomposing contextualized document embeddings using Independent Component Analysis. Our approach provides diverse and highly coherent topics, requires no preprocessing, and is demonstrated to be the fastest contextual topic model, being, on average, 4.5x faster than the runner-up BERTopic. We offer an implementation of <tex-math>S^3</tex-math>, and all contextual baselines, in the Turftopic Python package.</abstract>
      <url hash="2c6bfef5">2025.acl-long.32</url>
      <bibkey>kardos-etal-2025-s3</bibkey>
    </paper>
    <paper id="33">
      <title><fixed-case>T</fixed-case>rim<fixed-case>LLM</fixed-case>: Progressive Layer Dropping for Domain-Specific <fixed-case>LLM</fixed-case>s</title>
      <author><first>Lanxiang</first><last>Hu</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Tajana</first><last>Rosing</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Hao</first><last>Zhang</last><affiliation>University of California, San Diego, Snowflake, Petuum, Inc and Carnegie Mellon University</affiliation></author>
      <pages>667-681</pages>
      <abstract>Specializing large language models (LLMs) for local deployment in domain-specific use cases is necessary for strong performance while meeting latency and privacy constraints. However, conventional task-specific adaptation approaches do not show simultaneous memory saving and inference speedup at deployment time. Practical compression techniques like quantization and pruning require dedicated hardware or kernel support to achieve measured inference speedup. We develop TrimLLM based on the layer-wise specialization phenomenon we empirically observed and verified on contemporary LLMs. TrimLLM reduces the depth of LLMs via progressive layer dropping. We show it retains LLMs’ capacity in specific domains and achieves inference speedup irrespective of hardware and deep learning frameworks. We evaluated TrimLLM on LLMs of various sizes for inference; models adapted on medical, legal, and financial datasets all demonstrate <tex-math>2.1 - 5.7\times</tex-math> inference speedup on consumer GPUs and up to <tex-math>3.1\times</tex-math> speedup on A100 when compared to state-of-the-art model compression algorithms, with no loss in accuracy at <tex-math>50\sim 60</tex-math>% model compression ratio.</abstract>
      <url hash="39d8e768">2025.acl-long.33</url>
      <bibkey>hu-etal-2025-trimllm</bibkey>
    </paper>
    <paper id="34">
      <title><fixed-case>J</fixed-case>u<fixed-case>S</fixed-case>t<fixed-case>R</fixed-case>ank: Benchmarking <fixed-case>LLM</fixed-case> Judges for System Ranking</title>
      <author><first>Ariel</first><last>Gera</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Odellia</first><last>Boni</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Yotam</first><last>Perlitz</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Roy</first><last>Bar-Haim</last><affiliation>IBM Research AI</affiliation></author>
      <author><first>Lilach</first><last>Eden</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Asaf</first><last>Yehudai</last></author>
      <pages>682-712</pages>
      <abstract>Given the rapid progress of generative AI, there is a pressing need to systematically compare and choose between the numerous models and configurations available. The scale and versatility of such evaluations make the use of LLM-based judges a compelling solution for this challenge. Crucially, this approach requires first to validate the quality of the LLM judge itself. Previous work has focused on instance-based assessment of LLM judges, where a judge is evaluated over a set of responses, or response pairs, while being agnostic to their source systems. We argue that this setting overlooks critical factors affecting system-level ranking, such as a judge’s positive or negative bias towards certain systems. To address this gap, we conduct the first large-scale study of LLM judges as system rankers. System scores are generated by aggregating judgment scores over multiple system outputs, and the judge’s quality is assessed by comparing the resulting system ranking to a human-based ranking. Beyond overall judge assessment, our analysis provides a fine-grained characterization of judge behavior, including their decisiveness and bias.</abstract>
      <url hash="8fc9e5e2">2025.acl-long.34</url>
      <bibkey>gera-etal-2025-justrank</bibkey>
    </paper>
    <paper id="35">
      <title>Generating Diverse Training Samples for Relation Extraction with Large Language Models</title>
      <author><first>Zexuan</first><last>Li</last><affiliation>Nanjing University of Aeronautics and Astronautics</affiliation></author>
      <author><first>Hongliang</first><last>Dai</last><affiliation>Nanjing University of Aeronautics and Astronautics</affiliation></author>
      <author><first>Piji</first><last>Li</last><affiliation>Nanjing University of Aeronautics and Astronautics</affiliation></author>
      <pages>713-726</pages>
      <abstract>Using Large Language Models (LLMs) to generate training data can potentially be a preferable way to improve zero or few-shot NLP tasks. However, many problems remain to be investigated for this direction. For the task of Relation Extraction (RE), we find that samples generated by directly prompting LLMs may easily have high structural similarities with each other. They tend to use a limited variety of phrasing while expressing the relation between a pair of entities. Therefore, in this paper, we study how to effectively improve the diversity of the training samples generated with LLMs for RE, while also maintaining their correctness. We first try to make the LLMs produce dissimilar samples by directly giving instructions in In-Context Learning (ICL) prompts. Then, we propose an approach to fine-tune LLMs for diversity training sample generation through Direct Preference Optimization (DPO). Our experiments on commonly used RE datasets show that both attempts can improve the quality of the generated training data. We also find that comparing with directly performing RE with an LLM, training a non-LLM RE model with its generated samples may lead to better performance.</abstract>
      <url hash="ea84bae9">2025.acl-long.35</url>
      <bibkey>li-etal-2025-generating</bibkey>
    </paper>
    <paper id="36">
      <title><fixed-case>M</fixed-case>ulti<fixed-case>S</fixed-case>ocial: Multilingual Benchmark of Machine-Generated Text Detection of Social-Media Texts</title>
      <author><first>Dominik</first><last>Macko</last><affiliation>Kempelen Institute of Intelligent Technologies</affiliation></author>
      <author><first>Jakub</first><last>Kopál</last><affiliation>Kempelen Institute of Intelligent Technologies</affiliation></author>
      <author><first>Robert</first><last>Moro</last><affiliation>Kempelen Institute of Intelligent Technologies</affiliation></author>
      <author><first>Ivan</first><last>Srba</last><affiliation>Kempelen Institute of Intelligent Technologies</affiliation></author>
      <pages>727-752</pages>
      <abstract>Recent LLMs are able to generate high-quality multilingual texts, indistinguishable for humans from authentic human-written ones. Research in machine-generated text detection is however mostly focused on the English language and longer texts, such as news articles, scientific papers or student essays. Social-media texts are usually much shorter and often feature informal language, grammatical errors, or distinct linguistic items (e.g., emoticons, hashtags). There is a gap in studying the ability of existing methods in detection of such texts, reflected also in the lack of existing multilingual benchmark datasets. To fill this gap we propose the first multilingual (22 languages) and multi-platform (5 social media platforms) dataset for benchmarking machine-generated text detection in the social-media domain, called MultiSocial. It contains 472,097 texts, of which about 58k are human-written and approximately the same amount is generated by each of 7 multilingual LLMs. We use this benchmark to compare existing detection methods in zero-shot as well as fine-tuned form. Our results indicate that the fine-tuned detectors have no problem to be trained on social-media texts and that the platform selection for training matters.</abstract>
      <url hash="b31892ee">2025.acl-long.36</url>
      <bibkey>macko-etal-2025-multisocial</bibkey>
    </paper>
    <paper id="37">
      <title>Efficient and Accurate Prompt Optimization: the Benefit of Memory in Exemplar-Guided Reflection</title>
      <author><first>Cilin</first><last>Yan</last></author>
      <author><first>Jingyun</first><last>Wang</last><affiliation>Beihang University</affiliation></author>
      <author><first>Lin</first><last>Zhang</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Ruihui</first><last>Zhao</last></author>
      <author><first>Xiaopu</first><last>Wu</last></author>
      <author><first>Kai</first><last>Xiong</last></author>
      <author><first>Qingsong</first><last>Liu</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Guoliang</first><last>Kang</last><affiliation>Beihang University</affiliation></author>
      <author><first>Yangyang</first><last>Kang</last></author>
      <pages>753-779</pages>
      <abstract>Automatic prompt engineering aims to enhance the generation quality of large language models (LLMs). Recent works utilize feedbacks generated from erroneous cases to guide the prompt optimization. During inference, they may further retrieve several semantically-related exemplars and concatenate them to the optimized prompts to improve the performance. However, those works only utilize the feedback at the current step, ignoring historical and unseleccted feedbacks which are potentially beneficial. Moreover, the selection of exemplars only considers the general semantic relationship and may not be optimal in terms of task performance and matching with the optimized prompt. In this work, we propose an Exemplar-Guided Reflection with Memory mechanism (ERM) to realize more efficient and accurate prompt optimization. Specifically, we design an exemplar-guided reflection mechanism where the feedback generation is additionally guided by the generated exemplars. We further build two kinds of memory to fully utilize the historical feedback information and support more effective exemplar retrieval. Empirical evaluations show our method surpasses previous state-of-the-arts with less optimization steps, i.e., improving F1 score by 10.1 on LIAR dataset, and reducing half of the optimization steps on ProTeGi.</abstract>
      <url hash="8125c747">2025.acl-long.37</url>
      <bibkey>yan-etal-2025-efficient</bibkey>
    </paper>
    <paper id="38">
      <title>Evaluation of <fixed-case>LLM</fixed-case> Vulnerabilities to Being Misused for Personalized Disinformation Generation</title>
      <author><first>Aneta</first><last>Zugecova</last><affiliation>Kempelen Institute of Intelligent Technologies and Copenhagen University</affiliation></author>
      <author><first>Dominik</first><last>Macko</last><affiliation>Kempelen Institute of Intelligent Technologies</affiliation></author>
      <author><first>Ivan</first><last>Srba</last><affiliation>Kempelen Institute of Intelligent Technologies</affiliation></author>
      <author><first>Robert</first><last>Moro</last><affiliation>Kempelen Institute of Intelligent Technologies</affiliation></author>
      <author><first>Jakub</first><last>Kopál</last><affiliation>Kempelen Institute of Intelligent Technologies</affiliation></author>
      <author><first>Katarína</first><last>Marcinčinová</last></author>
      <author><first>Matúš</first><last>Mesarčík</last><affiliation>Kempelen Institute of Intelligent Technologies and Comenius University in Bratislava</affiliation></author>
      <pages>780-797</pages>
      <abstract>The capabilities of recent large language models (LLMs) to generate high-quality content indistinguishable by humans from human-written texts raises many concerns regarding their misuse. Previous research has shown that LLMs can be effectively misused for generating disinformation news articles following predefined narratives. Their capabilities to generate personalized (in various aspects) content have also been evaluated and mostly found usable. However, a combination of personalization and disinformation abilities of LLMs has not been comprehensively studied yet. Such a dangerous combination should trigger integrated safety filters of the LLMs, if there are some. This study fills this gap by evaluating vulnerabilities of recent open and closed LLMs, and their willingness to generate personalized disinformation news articles in English. We further explore whether the LLMs can reliably meta-evaluate the personalization quality and whether the personalization affects the generated-texts detectability. Our results demonstrate the need for stronger safety-filters and disclaimers, as those are not properly functioning in most of the evaluated LLMs. Additionally, our study revealed that the personalization actually reduces the safety-filter activations; thus effectively functioning as a jailbreak. Such behavior must be urgently addressed by LLM developers and service providers.</abstract>
      <url hash="ff376026">2025.acl-long.38</url>
      <bibkey>zugecova-etal-2025-evaluation</bibkey>
    </paper>
    <paper id="39">
      <title><fixed-case>E</fixed-case>scape<fixed-case>B</fixed-case>ench: Towards Advancing Creative Intelligence of Language Model Agents</title>
      <author><first>Cheng</first><last>Qian</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Peixuan</first><last>Han</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Qinyu</first><last>Luo</last><affiliation>Johns Hopkins University and Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Bingxiang</first><last>He</last></author>
      <author><first>Xiusi</first><last>Chen</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Yuji</first><last>Zhang</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Hongyi</first><last>Du</last></author>
      <author><first>Jiarui</first><last>Yao</last></author>
      <author><first>Xiaocheng</first><last>Yang</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Denghui</first><last>Zhang</last><affiliation>Stevens Institute of Technology</affiliation></author>
      <author><first>Yunzhu</first><last>Li</last><affiliation>Columbia University</affiliation></author>
      <author><first>Heng</first><last>Ji</last><affiliation>University of Illinois, Urbana-Champaign</affiliation></author>
      <pages>798-820</pages>
      <abstract>Language model agents excel in long-session planning and reasoning, but existing benchmarks primarily focus on goal-oriented tasks with explicit objectives, neglecting creative adaptation in unfamiliar environments. To address this, we introduce EscapeBench—a benchmark suite of room escape game environments designed to challenge agents with creative reasoning, unconventional tool use, and iterative problem-solving to uncover implicit goals. Our results show that current LM models, despite employing working memory and Chain-of-Thought reasoning, achieve only 15% average progress without hints, highlighting their limitations in creativity. To bridge this gap, we propose EscapeAgent, a framework designed to enhance creative reasoning through Foresight (innovative tool use) and Reflection (identifying unsolved tasks). Experiments show that EscapeAgent can execute action chains over 1,000 steps while maintaining logical coherence. It navigates and completes games with up to 40% fewer steps and hints, performs robustly across difficulty levels, and achieves higher action success rates with more efficient and innovative puzzle-solving strategies.</abstract>
      <url hash="557fe642">2025.acl-long.39</url>
      <bibkey>qian-etal-2025-escapebench</bibkey>
    </paper>
    <paper id="40">
      <title><fixed-case>BPP</fixed-case>-Search: Enhancing Tree of Thought Reasoning for Mathematical Modeling Problem Solving</title>
      <author><first>Teng</first><last>Wang</last><affiliation>OPPO</affiliation></author>
      <author><first>Wing Yin</first><last>Yu</last></author>
      <author><first>Zhenqi</first><last>He</last></author>
      <author><first>Zehua</first><last>Liu</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>HaileiGong</first><last>HaileiGong</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Han</first><last>Wu</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Xiongwei</first><last>Han</last></author>
      <author><first>Wei</first><last>Shi</last></author>
      <author><first>Ruifeng</first><last>She</last></author>
      <author><first>Fangzhou</first><last>Zhu</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Tao</first><last>Zhong</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <pages>821-838</pages>
      <abstract>LLMs exhibit advanced reasoning capabilities, offering the potential to transform natural language questions into mathematical models. However, existing open-source datasets in operations research domain lack detailed annotations of the modeling process, such as variable definitions, focusing solely on objective values, which hinders reinforcement learning applications. To address this, we release the StructuredOR dataset, annotated with comprehensive labels that capture the complete mathematical modeling process. We further propose BPP-Search, an algorithm that integrates reinforcement learning into a tree-of-thought structure using Beam search, a Process reward model, and a pairwise Preference algorithm. This approach enables efficient exploration of tree structures, avoiding exhaustive search while improving accuracy. Extensive experiments on StructuredOR, NL4OPT, and MAMO-ComplexLP datasets show that BPP-Search significantly outperforms state-of-the-art methods. In tree-based reasoning, BPP-Search excels in accuracy and efficiency, enabling faster retrieval of correct solutions. The StructuredOR dataset is available on Huggingface https://huggingface.co/datasets/LLM4OR/StructuredOR and GitHub https://github.com/LLM4OR/StructuredOR.</abstract>
      <url hash="f42e5db8">2025.acl-long.40</url>
      <bibkey>wang-etal-2025-bpp</bibkey>
    </paper>
    <paper id="41">
      <title><fixed-case>LACA</fixed-case>: Improving Cross-lingual Aspect-Based Sentiment Analysis with <fixed-case>LLM</fixed-case> Data Augmentation</title>
      <author><first>Jakub</first><last>Šmíd</last><affiliation>University of West Bohemia</affiliation></author>
      <author><first>Pavel</first><last>Priban</last><affiliation>University of West Bohemia</affiliation></author>
      <author><first>Pavel</first><last>Kral</last><affiliation>University of West Bohemia</affiliation></author>
      <pages>839-853</pages>
      <abstract>Cross-lingual aspect-based sentiment analysis (ABSA) involves detailed sentiment analysis in a target language by transferring knowledge from a source language with available annotated data. Most existing methods depend heavily on often unreliable translation tools to bridge the language gap. In this paper, we propose a new approach that leverages a large language model (LLM) to generate high-quality pseudo-labelled data in the target language without the need for translation tools. First, the framework trains an ABSA model to obtain predictions for unlabelled target language data. Next, LLM is prompted to generate natural sentences that better represent these noisy predictions than the original text. The ABSA model is then further fine-tuned on the resulting pseudo-labelled dataset. We demonstrate the effectiveness of this method across six languages and five backbone models, surpassing previous state-of-the-art translation-based approaches. The proposed framework also supports generative models, and we show that fine-tuned LLMs outperform smaller multilingual models.</abstract>
      <url hash="a2497eb0">2025.acl-long.41</url>
      <bibkey>smid-etal-2025-laca</bibkey>
    </paper>
    <paper id="42">
      <title>Fusing Highly Specialized Language Models for Comprehensive Expertise</title>
      <author><first>Ning</first><last>Ding</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Yulin</first><last>Chen</last></author>
      <author><first>Ganqu</first><last>Cui</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Xingtai</first><last>Lv</last></author>
      <author><first>Weilin</first><last>Zhao</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Kaiyan</first><last>Zhang</last></author>
      <author><first>Ruobing</first><last>Xie</last></author>
      <author><first>Bowen</first><last>Zhou</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Zhiyuan</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Maosong</first><last>Sun</last><affiliation>Tsinghua University</affiliation></author>
      <pages>854-878</pages>
      <abstract>Underlying data distributions of natural language, programming code, and mathematical symbols vary vastly, presenting a complex challenge for large language models (LLMs) that strive to achieve high performance across all three domains simultaneously. Achieving a very high level of proficiency for an LLM within a specific domain often requires extensive training with relevant corpora, which is typically accompanied by a sacrifice in performance in other domains. In this paper, we aim to “play the dealt cards well” and propose to fuse models that are already highly-specialized directly. The proposed fusing framework, , consists of different distinct specialists that are already sufficiently trained on different domains (we mainly focus on language, coding, and mathematics in this paper). A token-level gating mechanism is introduced to blend the specialists’ outputs. A two-stage training strategy accompanied by balanced sampling is designed to ensure stability. To effectively train the fused model, we further construct a high-quality supervised instruction tuning dataset, , which includes text, code, and mathematical content. This dataset comprises approximately 300,000 instructions and covers a wide range of topics in each domain. Experiments show that our model could simultaneously achieve mastery of the three crucial domains.</abstract>
      <url hash="b6d76b94">2025.acl-long.42</url>
      <bibkey>ding-etal-2025-fusing</bibkey>
    </paper>
    <paper id="43">
      <title><fixed-case>H</fixed-case>yb<fixed-case>GRAG</fixed-case>: Hybrid Retrieval-Augmented Generation on Textual and Relational Knowledge Bases</title>
      <author><first>Meng-Chieh</first><last>Lee</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Qi</first><last>Zhu</last><affiliation>Amazon</affiliation></author>
      <author><first>Costas</first><last>Mavromatis</last><affiliation>Amazon</affiliation></author>
      <author><first>Zhen</first><last>Han</last><affiliation>Amazon</affiliation></author>
      <author><first>Soji</first><last>Adeshina</last><affiliation>Amazon</affiliation></author>
      <author><first>Vassilis N.</first><last>Ioannidis</last><affiliation>Amazon Web Services</affiliation></author>
      <author><first>Huzefa</first><last>Rangwala</last><affiliation>Amazon and Computer Science, George Mason University</affiliation></author>
      <author><first>Christos</first><last>Faloutsos</last><affiliation>Amazon and Carnegie Mellon University</affiliation></author>
      <pages>879-893</pages>
      <abstract>Given a semi-structured knowledge base (SKB), where text documents are interconnected by relations, how can we effectively retrieve relevant information to answer user questions?Retrieval-Augmented Generation (RAG) retrieves documents to assist large language models (LLMs) in question answering; while Graph RAG (GRAG) uses structured knowledge bases as its knowledge source.However, many questions require both textual and relational information from SKB — referred to as “hybrid” questions — which complicates the retrieval process and underscores the need for a hybrid retrieval method that leverages both information.In this paper, through our empirical analysis, we identify key insights that show why existing methods may struggle with hybrid question answering (HQA) over SKB. Based on these insights, we propose HybGRAG for HQA, consisting of a retriever bank and a critic module, with the following advantages:1. Agentic, it automatically refines the output by incorporating feedback from the critic module, 2. Adaptive, it solves hybrid questions requiring both textual and relational information with the retriever bank,3. Interpretable, it justifies decision making with intuitive refinement path, and4. Effective, it surpasses all baselines on HQA benchmarks.In experiments on the STaRK benchmark, HybGRAG achieves significant performance gains, with an average relative improvement in Hit@1 of 51%.</abstract>
      <url hash="dd5ad748">2025.acl-long.43</url>
      <bibkey>lee-etal-2025-hybgrag</bibkey>
    </paper>
    <paper id="44">
      <title>Re-ranking Using Large Language Models for Mitigating Exposure to Harmful Content on Social Media Platforms</title>
      <author><first>Rajvardhan</first><last>Oak</last><affiliation>Microsoft</affiliation></author>
      <author><first>Muhammad</first><last>Haroon</last></author>
      <author><first>Claire Wonjeong</first><last>Jo</last></author>
      <author><first>Magdalena</first><last>Wojcieszak</last><affiliation>University of California, Davis</affiliation></author>
      <author><first>Anshuman</first><last>Chhabra</last><affiliation>University of South Florida</affiliation></author>
      <pages>894-908</pages>
      <abstract>Social media platforms utilize Machine Learning (ML) and Artificial Intelligence (AI) powered recommendation algorithms to maximize user engagement, which can result in inadvertent exposure to harmful content. Current moderation efforts, reliant on classifiers trained with extensive human-annotated data, struggle with scalability and adapting to new forms of harm. To address these challenges, we propose a novel re-ranking approach using Large Language Models (LLMs) in zero-shot and few-shot settings. Our method dynamically assesses and re-ranks content sequences, effectively mitigating harmful content exposure without requiring extensive labeled data. Alongside traditional ranking metrics, we also introduce two new metrics to evaluate the effectiveness of re-ranking in reducing exposure to harmful content. Through experiments on three datasets, three models and across three configurations, we demonstrate that our LLM-based approach significantly outperforms existing proprietary moderation approaches, offering a scalable and adaptable solution for harm mitigation.</abstract>
      <url hash="2fa25702">2025.acl-long.44</url>
      <bibkey>oak-etal-2025-ranking</bibkey>
    </paper>
    <paper id="45">
      <title>Aligning <fixed-case>AI</fixed-case> Research with the Needs of Clinical Coding Workflows: Eight Recommendations Based on <fixed-case>US</fixed-case> Data Analysis and Critical Review</title>
      <author><first>Yidong</first><last>Gan</last><affiliation>University of Sydney, University of Sydney</affiliation></author>
      <author><first>Maciej</first><last>Rybinski</last><affiliation>Universidad de Málaga</affiliation></author>
      <author><first>Ben</first><last>Hachey</last><affiliation>University of Sydney, University of Sydney</affiliation></author>
      <author><first>Jonathan K.</first><last>Kummerfeld</last><affiliation>University of Sydney</affiliation></author>
      <pages>909-922</pages>
      <abstract>Clinical coding is crucial for healthcare billing and data analysis. Manual clinical coding is labour-intensive and error-prone, which has motivated research towards full automation of the process. However, our analysis, based on US English electronic health records and automated coding research using these records, shows that widely used evaluation methods are not aligned with real clinical contexts. For example, evaluations that focus on the top 50 most common codes are an oversimplification, as there are thousands of codes used in practice. This position paper aims to align AI coding research more closely with practical challenges of clinical coding. Based on our analysis, we offer eight specific recommendations, suggesting ways to improve current evaluation methods. Additionally, we propose new AI-based methods beyond automated coding, suggesting alternative approaches to assist clinical coders in their workflows.</abstract>
      <url hash="7f3adeb0">2025.acl-long.45</url>
      <bibkey>gan-etal-2025-aligning</bibkey>
    </paper>
    <paper id="46">
      <title><fixed-case>MIND</fixed-case>: A Multi-agent Framework for Zero-shot Harmful Meme Detection</title>
      <author><first>Ziyan</first><last>Liu</last></author>
      <author><first>Chunxiao</first><last>Fan</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Haoran</first><last>Lou</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Yuexin</first><last>Wu</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Kaiwei</first><last>Deng</last><affiliation>lut</affiliation></author>
      <pages>923-947</pages>
      <abstract>The rapid expansion of memes on social media has highlighted the urgent need for effective approaches to detect harmful content. However, traditional data-driven approaches struggle to detect new memes due to their evolving nature and the lack of up-to-date annotated data. To address this issue, we propose MIND, a multi-agent framework for zero-shot harmful meme detection that does not rely on annotated data. MIND implements three key strategies: 1) We retrieve similar memes from an unannotated reference set to provide contextual information. 2) We propose a bi-directional insight derivation mechanism to extract a comprehensive understanding of similar memes. 3) We then employ a multi-agent debate mechanism to ensure robust decision-making through reasoned arbitration. Extensive experiments on three meme datasets demonstrate that our proposed framework not only outperforms existing zero-shot approaches but also shows strong generalization across different model architectures and parameter scales, providing a scalable solution for harmful meme detection.</abstract>
      <url hash="966dc317">2025.acl-long.46</url>
      <bibkey>liu-etal-2025-mind</bibkey>
    </paper>
    <paper id="47">
      <title><fixed-case>E</fixed-case>vo<fixed-case>W</fixed-case>iki: Evaluating <fixed-case>LLM</fixed-case>s on Evolving Knowledge</title>
      <author><first>Wei</first><last>Tang</last></author>
      <author><first>Yixin</first><last>Cao</last><affiliation>Fudan University</affiliation></author>
      <author><first>Yang</first><last>Deng</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Jiahao</first><last>Ying</last></author>
      <author><first>Bo</first><last>Wang</last></author>
      <author><first>Yizhe</first><last>Yang</last></author>
      <author><first>Yuyue</first><last>Zhao</last></author>
      <author><first>Qi</first><last>Zhang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xuanjing</first><last>Huang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Yu-Gang</first><last>Jiang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Yong</first><last>Liao</last><affiliation>University of Science and Technology of China and China Academic of Electronics and Information Technology</affiliation></author>
      <pages>948-964</pages>
      <abstract>Knowledge utilization is a critical aspect of LLMs, and understanding how they adapt to evolving knowledge is essential for their effective deployment. However, existing benchmarks are predominantly static, failing to capture the evolving nature of LLMs and knowledge, leading to inaccuracies and vulnerabilities such as contamination. In this paper, we introduce EvoWiki, an evolving dataset designed to reflect knowledge evolution by categorizing information into stable, evolved, and uncharted states. EvoWiki is fully auto-updatable, enabling precise evaluation of continuously changing knowledge and newly released LLMs. Through experiments with Retrieval-Augmented Generation (RAG) and Continual Learning (CL), we evaluate how effectively LLMs adapt to evolving knowledge. Our results indicate that current models often struggle with evolved knowledge, frequently providing outdated or incorrect responses. Moreover, the dataset highlights a synergistic effect between RAG and CL, demonstrating their potential to better adapt to evolving knowledge. EvoWiki provides a robust benchmark for advancing future research on the knowledge evolution capabilities of large language models.</abstract>
      <url hash="e05be31c">2025.acl-long.47</url>
      <bibkey>tang-etal-2025-evowiki</bibkey>
    </paper>
    <paper id="48">
      <title>Rethinking Repetition Problems of <fixed-case>LLM</fixed-case>s in Code Generation</title>
      <author><first>Yihong</first><last>Dong</last></author>
      <author><first>Yuchen</first><last>Liu</last></author>
      <author><first>Xue</first><last>Jiang</last><affiliation>Peking University</affiliation></author>
      <author><first>Bin</first><last>Gu</last><affiliation>Beijing Institute of Control Engineering</affiliation></author>
      <author><first>Zhi</first><last>Jin</last><affiliation>Peking University</affiliation></author>
      <author><first>Ge</first><last>Li</last><affiliation>Peking University</affiliation></author>
      <pages>965-985</pages>
      <abstract>With the advent of neural language models, the performance of code generation has been significantly boosted. However, the problem of repetitions during the generation process continues to linger. Previous work has primarily focused on content repetition, which is merely a fraction of the broader repetition problem in code generation. A more prevalent and challenging problem is structural repetition. In structural repetition, the repeated code appears in various patterns but possesses a fixed structure, which can be inherently reflected in grammar. In this paper, we formally define structural repetition and propose an efficient decoding approach called RPG, which stands for Repetition Penalization based on Grammar, to alleviate the repetition problems in code generation for LLMs. Specifically, RPG first leverages grammar rules to identify repetition problems during code generation, and then strategically decays the likelihood of critical tokens that contribute to repetitions, thereby mitigating them in code generation. To facilitate this study, we construct a new dataset CodeRepetEval to comprehensively evaluate approaches for mitigating the repetition problems in code generation. Extensive experimental results demonstrate that RPG substantially outperforms the best-performing baselines on CodeRepetEval dataset as well as HumanEval and MBPP benchmarks, effectively reducing repetitions and enhancing the quality of generated code.</abstract>
      <url hash="5bd04e43">2025.acl-long.48</url>
      <bibkey>dong-etal-2025-rethinking</bibkey>
    </paper>
    <paper id="49">
      <title><fixed-case>P</fixed-case>unch<fixed-case>B</fixed-case>ench: Benchmarking <fixed-case>MLLM</fixed-case>s in Multimodal Punchline Comprehension</title>
      <author><first>Kun</first><last>Ouyang</last></author>
      <author><first>Yuanxin</first><last>Liu</last><affiliation>Peking University</affiliation></author>
      <author><first>Shicheng</first><last>Li</last><affiliation>Peking University</affiliation></author>
      <author><first>Yi</first><last>Liu</last><affiliation>Peking University</affiliation></author>
      <author><first>Hao</first><last>Zhou</last><affiliation>Tencent</affiliation></author>
      <author><first>Fandong</first><last>Meng</last><affiliation>WeChat AI, Tencent Inc.</affiliation></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <author><first>Xu</first><last>Sun</last><affiliation>Peking University</affiliation></author>
      <pages>986-1008</pages>
      <abstract>Multimodal punchlines, which involve humor or sarcasm conveyed in image-caption pairs, are a popular way of communication on online multimedia platforms. With the rapid development of multimodal large language models (MLLMs), it is essential to assess their ability to effectively comprehend these punchlines. However, existing benchmarks on punchline comprehension suffer from three major limitations: 1) language shortcuts that allow models to solely rely on text, 2) lack of question diversity, and 3) narrow focus on a specific domain of multimodal content (e.g., cartoon). To address these limitations, we introduce a multimodal **Punch**line comprehension **Bench**mark, named **PunchBench**, which is tailored for accurate and comprehensive evaluation of punchline comprehension. To enhance the evaluation accuracy, we generate synonymous and antonymous captions by modifying original captions, which mitigates the impact of shortcuts in the captions. To provide a comprehensive evaluation, PunchBench incorporates diverse question formats and image-captions from various domains. On this basis, we conduct extensive evaluations and reveal a significant gap between state-of-the-art MLLMs and humans in punchline comprehension. To improve punchline comprehension, we propose Simple-to-Complex Chain-of-Question (SC-CoQ) strategy, enabling the models to incrementally address complicated questions by first mastering simple ones. SC-CoQ effectively enhances the performance of various MLLMs on PunchBench, surpassing in-context learning and chain-of-thought.</abstract>
      <url hash="1f690e36">2025.acl-long.49</url>
      <bibkey>ouyang-etal-2025-punchbench</bibkey>
    </paper>
    <paper id="50">
      <title><fixed-case>P</fixed-case>rocess<fixed-case>B</fixed-case>ench: Identifying Process Errors in Mathematical Reasoning</title>
      <author><first>Chujie</first><last>Zheng</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Zhenru</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Beichen</first><last>Zhang</last></author>
      <author><first>Runji</first><last>Lin</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Keming</first><last>Lu</last></author>
      <author><first>Bowen</first><last>Yu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Dayiheng</first><last>Liu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Jingren</first><last>Zhou</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Junyang</first><last>Lin</last><affiliation>Alibaba Group</affiliation></author>
      <pages>1009-1024</pages>
      <abstract>As language models regularly make mistakes when solving math problems, automated identification of errors in the reasoning process becomes increasingly significant for their scalable oversight. In this paper, we introduce ProcessBench for measuring the ability to identify erroneous steps in mathematical reasoning. It consists of 3,400 test cases, primarily focused on competition- and Olympiad-level math problems. Each test case contains a step-by-step solution with error location annotated by human experts. Models are required to identify the earliest step that contains an error, or conclude that all steps are correct. We conduct extensive evaluation on ProcessBench, involving two types of models: process reward models (PRMs) and critic models, where for the latter we prompt general language models to critique each solution step by step. We draw two main observations: (1) Existing PRMs typically fail to generalize to more challenging math problems beyond GSM8K and MATH. They underperform both critic models (i.e., prompted general language models) and our own trained PRM that is straightforwardly fine-tuned on the PRM800K dataset. (2) The best open-source model, QwQ-32B-Preview, has demonstrated the critique capability competitive with the proprietary model GPT-4o, despite that it still lags behind the reasoning-specialized o1-mini. We hope ProcessBench can foster future research in reasoning process assessment, paving the way toward scalable oversight of language models.</abstract>
      <url hash="8472aff3">2025.acl-long.50</url>
      <bibkey>zheng-etal-2025-processbench</bibkey>
    </paper>
    <paper id="51">
      <title>Model Extrapolation Expedites Alignment</title>
      <author><first>Chujie</first><last>Zheng</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Ziqi</first><last>Wang</last></author>
      <author><first>Heng</first><last>Ji</last><affiliation>University of Illinois, Urbana-Champaign</affiliation></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <author><first>Nanyun</first><last>Peng</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>1025-1041</pages>
      <abstract>Given the high computational cost of preference alignment training of large language models (LLMs), exploring efficient methods to reduce the training overhead remains an important and compelling research problem. Motivated by the observation that alignment training typically involves only small parameter changes without injecting new knowledge into models, we propose a straightforward method called ExPO (model extrapolation) to expedite LLMs’ alignment with human preferences. Given a partially-trained model and its initial SFT checkpoint, ExPO improves the implicit optimization objective of alignment training by simply amplifying the parameter change based on a first-order approximation, without any additional training overhead. Through controlled experiments, we demonstrate that ExPO boosts a DPO model trained with only 20% steps to outperform the fully-trained one. Moreover, we show that ExPO notably improves existing open-source LLMs (ranging from 1.8B to 70B parameters) on the leading AlpacaEval 2.0 and MT-Bench benchmarks, which highlights ExPO’s broader utility in efficiently enhancing LLM alignment.</abstract>
      <url hash="112fee9d">2025.acl-long.51</url>
      <bibkey>zheng-etal-2025-model</bibkey>
    </paper>
    <paper id="52">
      <title><fixed-case>ATLANTIS</fixed-case>: Weak-to-Strong Learning via Importance Sampling</title>
      <author><first>Yi</first><last>Liu</last><affiliation>Peking University</affiliation></author>
      <author><first>Guoyin</first><last>Wang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Shicheng</first><last>Li</last><affiliation>Peking University</affiliation></author>
      <author><first>Feifan</first><last>Song</last><affiliation>Peking University</affiliation></author>
      <author><first>Xu</first><last>Sun</last><affiliation>Peking University</affiliation></author>
      <pages>1042-1052</pages>
      <abstract>Supervised fine-tuning (SFT) enables large language models to align with training data for better performance in many aspects. Nevertheless, the gap between the distribution of current datasets from human annotations or model generations and the real-world data distribution heavily limits the capacities and potentials of models. As a result, we propose a new SFT technique, ATLANTIS, to bridge the gap. We adopt importance sampling to estimate the optimal data distribution in the real world from existing training datasets because the former is hard to sample from. Furthermore, we introduce an extra small model and reference model to estimate the sampling ratio through the probability gap between them. We evaluate our method with benchmarks in knowledge &amp; understanding and preference aspects. The experiment results prove that ATLANTIS can bring consistent and significant improvements to models’ performance. What’s more, our method can be flexibly transferred among models with different structures. Our analyses demonstrate that our method is well-compatible with other SFT techniques to further enhance models’ capacities and has great potential to be combined with existing training frameworks.</abstract>
      <url hash="54ad4b80">2025.acl-long.52</url>
      <bibkey>liu-etal-2025-atlantis</bibkey>
    </paper>
    <paper id="53">
      <title><fixed-case>MPVS</fixed-case>tance: Mitigating Hallucinations in Stance Detection with Multi-Perspective Verification</title>
      <author><first>ZhaoDan</first><last>Zhang</last><affiliation>University of Chinese Academy of Sciences and Chinese Academy of Sciences</affiliation></author>
      <author><first>Zhao</first><last>Zhang</last></author>
      <author><first>Jin</first><last>Zhang</last></author>
      <author><first>Hui</first><last>Xu</last><affiliation>Chinese Academy of Sciences</affiliation></author>
      <author><first>Xueqi</first><last>Cheng</last><affiliation>Institute of Computing Technology, Chinese Academy</affiliation></author>
      <pages>1053-1067</pages>
      <abstract>Stance detection is a pivotal task in Natural Language Processing (NLP), identifying textual attitudes toward various targets. Despite advances in using Large Language Models (LLMs), challenges persist due to hallucination-models generating plausible yet inaccurate content. Addressing these challenges, we introduce MPVStance, a framework that incorporates Multi-Perspective Verification (MPV) with Retrieval-Augmented Generation (RAG) across a structured five-step verification process. Our method enhances stance detection by rigorously validating each response from factual accuracy, logical consistency, contextual relevance, and other perspectives. Extensive testing on the SemEval-2016 and VAST datasets, including scenarios that challenge existing methods and comprehensive ablation studies, demonstrates that MPVStance significantly outperforms current models. It effectively mitigates hallucination issues and sets new benchmarks for reliability and accuracy in stance detection, particularly in zero-shot, few-shot, and challenging scenarios.</abstract>
      <url hash="5a767703">2025.acl-long.53</url>
      <bibkey>zhang-etal-2025-mpvstance</bibkey>
    </paper>
    <paper id="54">
      <title>Personality-Guided Code Generation Using Large Language Models</title>
      <author><first>Yaoqi</first><last>Guo</last></author>
      <author><first>Zhenpeng</first><last>Chen</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Jie M.</first><last>Zhang</last><affiliation>King’s College London, University of London</affiliation></author>
      <author id="yang-liu"><first>Yang</first><last>Liu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Yun</first><last>Ma</last><affiliation>Peking University</affiliation></author>
      <pages>1068-1080</pages>
      <abstract>Code generation, the automatic creation of source code from natural language descriptions, has garnered significant attention due to its potential to streamline software development. Inspired by research that links task-personality alignment with improved development outcomes, we conduct an empirical study on personality-guided code generation using large language models (LLMs). Specifically, we investigate how emulating personality traits appropriate to the coding tasks affects LLM performance. We extensively evaluate this approach using seven widely adopted LLMs across four representative datasets. Our results show that personality guidance significantly enhances code generation accuracy, with improved pass rates in 23 out of 28 LLM-dataset combinations. Notably, in 11 cases, the improvement exceeds 5%, and in 5 instances, it surpasses 10%, with the highest gain reaching 12.9%. Additionally, personality guidance can be easily integrated with other prompting strategies to further boost performance.</abstract>
      <url hash="949e6636">2025.acl-long.54</url>
      <bibkey>guo-etal-2025-personality</bibkey>
    </paper>
    <paper id="55">
      <title><fixed-case>P</fixed-case>sy<fixed-case>DT</fixed-case>: Using <fixed-case>LLM</fixed-case>s to Construct the Digital Twin of Psychological Counselor with Personalized Counseling Style for Psychological Counseling</title>
      <author><first>Haojie</first><last>Xie</last></author>
      <author><first>Yirong</first><last>Chen</last></author>
      <author><first>Xiaofen</first><last>Xing</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Jingkai</first><last>Lin</last></author>
      <author><first>Xiangmin</first><last>Xu</last><affiliation>South China University of Technology</affiliation></author>
      <pages>1081-1115</pages>
      <abstract>Currently, large language models (LLMs) have made significant progress in the field of psychological counseling. However, existing mental health LLMs overlook a critical issue where they do not consider the fact that different psychological counselors exhibit different personal styles, including linguistic style and therapy techniques, etc. As a result, these LLMs fail to satisfy the individual needs of clients who seek different counseling styles. To help bridge this gap, we propose PsyDT, a novel framework using LLMs to construct the Digital Twin of Psychological counselor with personalized counseling style. Compared to the time-consuming and costly approach of collecting a large number of real-world counseling cases to create a specific counselor’s digital twin, our framework offers a faster and more cost-effective solution. To construct PsyDT, we utilize dynamic one-shot learning by using GPT-4 to capture counselor’s unique counseling style, mainly focusing on linguistic style and therapy techniques. Subsequently, using existing single-turn long-text dialogues with client’s questions, GPT-4 is guided to synthesize multi-turn dialogues of specific counselor. Finally, we fine-tune the LLMs on the synthetic dataset, PsyDTCorpus, to achieve the digital twin of psychological counselor with personalized counseling style. Experimental results indicate that our proposed PsyDT framework can synthesize multi-turn dialogues that closely resemble real-world counseling cases and demonstrate better performance compared to other baselines, thereby show that our framework can effectively construct the digital twin of psychological counselor with a specific counseling style.</abstract>
      <url hash="d5eda76d">2025.acl-long.55</url>
      <bibkey>xie-etal-2025-psydt</bibkey>
    </paper>
    <paper id="56">
      <title><fixed-case>BIP</fixed-case>ro: Zero-shot <fixed-case>C</fixed-case>hinese Poem Generation via Block Inverse Prompting Constrained Generation Framework</title>
      <author><first>Xu</first><last>Zou</last><affiliation>Beijing Knowledge Atlas Technology Joint Stock Company Limited</affiliation></author>
      <pages>1116-1134</pages>
      <abstract>Recently, generative pre-trained models have made significant strides, particularly highlighted by the release of ChatGPT and GPT-4, which exhibit superior cross-domain capabilities. However, these models still face challenges on constrained writing tasks like poem generation under open-domain titles via direct generation.In response to this challenge, we introduce Block Inverse Prompting (BIPro) constrained generation framework. BIPro leverages two block inverse prompting methods, revise and rewrite. This inference scaling approach mimics the process of human text writing using block generative models. It significantly improves the zero-shot generation quality on the constrained generation task of open-domain traditional-form Chinese poem generation. Based on a less powerful block generative model GLM-10B-Chinese, poems composed via BIPro without priming or additional training outperform both much larger direct generative systems like GPT-4 or GLM-4 and domain-specific systems such as Yusheng, Shisanbai, or Baidu Poetry Helper in human evaluation by proficient poets. BIPro considerably narrows the gap between AI-generated works and short-listed human literary arts in another human evaluation, unveiling the promising potential of inference scaling in improving the quality of constrained generation. It is open-sourced and available as an agent in chatglm app.</abstract>
      <url hash="05f45d72">2025.acl-long.56</url>
      <bibkey>zou-2025-bipro</bibkey>
    </paper>
    <paper id="57">
      <title><fixed-case>L</fixed-case>ong<fixed-case>D</fixed-case>oc<fixed-case>URL</fixed-case>: a Comprehensive Multimodal Long Document Benchmark Integrating Understanding, Reasoning, and Locating</title>
      <author><first>Chao</first><last>Deng</last></author>
      <author><first>Jiale</first><last>Yuan</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Pi</first><last>Bu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Peijie</first><last>Wang</last></author>
      <author><first>Zhong-Zhi</first><last>Li</last></author>
      <author><first>Jian</first><last>Xu</last></author>
      <author><first>Xiao-Hui</first><last>Li</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yuan</first><last>Gao</last></author>
      <author><first>Jun</first><last>Song</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Bo</first><last>Zheng</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Cheng-Lin</first><last>Liu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <pages>1135-1159</pages>
      <abstract>Large vision language models (LVLMs) have improved the document understanding capabilities remarkably, enabling the handling of complex document elements, longer contexts, and a wider range of tasks. However, existing document understanding benchmarks have been limited to handling only a small number of pages and fail to provide a comprehensive analysis of layout elements locating. In this paper, we first define three primary task categories: Long Document Understanding, numerical Reasoning, and cross-element Locating, and then propose a comprehensive benchmark—LongDocURL—integrating above three primary tasks and comprising 20 sub-tasks categorized based on different primary tasks and answer evidences. Furthermore, we develop a semi-automated construction pipeline and collect 2,325 high-quality question-answering pairs, covering more than 33,000 pages of documents, significantly outperforming existing benchmarks. Subsequently, we conduct comprehensive evaluation experiments on both open-source and closed- source models across 26 different configurations, revealing critical performance gaps in this field. The code and data: https://github.com/dengc2023/LongDocURL.</abstract>
      <url hash="92ee1f2e">2025.acl-long.57</url>
      <bibkey>deng-etal-2025-longdocurl</bibkey>
    </paper>
    <paper id="58">
      <title><fixed-case>O</fixed-case>bfus<fixed-case>LM</fixed-case>: Privacy-preserving Language Model Service against Embedding Inversion Attacks</title>
      <author><first>Yu</first><last>Lin</last></author>
      <author><first>Ruining</first><last>Yang</last></author>
      <author><first>Yunlong</first><last>Mao</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Qizhi</first><last>Zhang</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Jue</first><last>Hong</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Quanwei</first><last>Cai</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Ye</first><last>Wu</last></author>
      <author><first>Huiqi</first><last>Liu</last></author>
      <author><first>Zhiyu</first><last>Chen</last></author>
      <author><first>Bing</first><last>Duan</last></author>
      <author><first>Sheng</first><last>Zhong</last><affiliation>nanjing university</affiliation></author>
      <pages>1160-1174</pages>
      <abstract>As the rapid expansion of Machine Learning as a Service (MLaaS) for language models, concerns over the privacy of client inputs during inference or fine-tuning have correspondingly escalated. Recently, solutions have been proposed to safeguard client privacy by obfuscation techniques. However, the solutions incur notable decline in model utility and mainly focus on classification tasks, rendering them impractical for real-world applications. Moreover, recent studies reveal that these obfuscation, if not well designed, is susceptible to embedding inversion attacks (EIAs). In this paper, we devise ObfusLM, a privacy-preserving MLaaS framework for both classification and generation tasks. ObfusLM leverages a model obfuscation module to achieve privacy protection for both classification and generation tasks. Based on <tex-math>(k, \epsilon)</tex-math>-anonymity, ObfusLM includes novel obfuscation algorithms to reach provable security against EIAs. Extensive experiments show that ObfusLM outperforms existing works in utility by 10% with a nearly 80% resistance rate against EIAs.</abstract>
      <url hash="834aa30d">2025.acl-long.58</url>
      <bibkey>lin-etal-2025-obfuslm</bibkey>
    </paper>
    <paper id="59">
      <title>Interlocking-free Selective Rationalization Through Genetic-based Learning</title>
      <author><first>Federico</first><last>Ruggeri</last><affiliation>University of Bologna</affiliation></author>
      <author><first>Gaetano</first><last>Signorelli</last><affiliation>University of Bologna</affiliation></author>
      <pages>1175-1191</pages>
      <abstract>A popular end-to-end architecture for selective rationalization is the select-then-predict pipeline, comprising a generator to extract highlights fed to a predictor. Such a cooperative system suffers from suboptimal equilibrium minima due to the dominance of one of the two modules, a phenomenon known as interlocking. While several contributions aimed at addressing interlocking, they only mitigate its effect, often by introducing feature-based heuristics, sampling, and ad-hoc regularizations. We present GenSPP, the first interlocking-free architecture for selective rationalization that does not require any learning overhead, as the above-mentioned. GenSPP avoids interlocking by performing disjoint training of the generator and predictor via genetic global search. Experiments on a synthetic and a real-world benchmark show that our model outperforms several state-of-the-art competitors.</abstract>
      <url hash="c56681cb">2025.acl-long.59</url>
      <bibkey>ruggeri-signorelli-2025-interlocking</bibkey>
    </paper>
    <paper id="60">
      <title>Re-identification of De-identified Documents with Autoregressive Infilling</title>
      <author><first>Lucas Georges Gabriel</first><last>Charpentier</last><affiliation>University of Oslo</affiliation></author>
      <author><first>Pierre</first><last>Lison</last><affiliation>Norwegian Computing Center</affiliation></author>
      <pages>1192-1209</pages>
      <abstract>Documents revealing sensitive information about individuals must typically be de-identified. This de-identification is often done by masking all mentions of personally identifiable information (PII), thereby making it more difficult to uncover the identity of the person(s) in question. To investigate the robustness of de-identification methods, we present a novel, RAG-inspired approach that attempts the reverse process of re-identification based on a database of documents representing background knowledge. Given a text in which personal identifiers have been masked, the re-identification proceeds in two steps. A retriever first selects from the background knowledge passages deemed relevant for the re-identification. Those passages are then provided to an infilling model which seeks to infer the original content of each text span. This process is repeated until all masked spans are replaced. We evaluate the re-identification on three datasets (Wikipedia biographies, court rulings and clinical notes). Results show that (1) as many as 80% of de-identified text spans can be successfully recovered and (2) the re-identification accuracy increases along with the level of background knowledge.</abstract>
      <url hash="b6bb9166">2025.acl-long.60</url>
      <bibkey>charpentier-lison-2025-identification</bibkey>
    </paper>
    <paper id="61">
      <title>Modeling Uncertainty in Composed Image Retrieval via Probabilistic Embeddings</title>
      <author><first>Haomiao</first><last>Tang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Jinpeng</first><last>Wang</last></author>
      <author><first>Yuang</first><last>Peng</last><affiliation>StepFun Technology Inc.</affiliation></author>
      <author><first>GuangHao</first><last>Meng</last></author>
      <author><first>Ruisheng</first><last>Luo</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Bin</first><last>Chen</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <author><first>Long</first><last>Chen</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Yaowei</first><last>Wang</last><affiliation>Harbin Institute of Technology, Shenzhen and Pengcheng Laboratory</affiliation></author>
      <author><first>Shu-Tao</first><last>Xia</last><affiliation>Shenzhen International Graduate School, Tsinghua University</affiliation></author>
      <pages>1210-1222</pages>
      <abstract>Composed Image Retrieval (CIR) enables users to search for images using multimodal queries that combine text and reference images. While metric learning methods have shown promise, they rely on deterministic point embeddings that fail to capture the inherent uncertainty in the input data, in which user intentions may be imprecisely specified or open to multiple interpretations. We address this challenge by reformulating CIR through our proposed Composed Probabilistic Embedding (CoPE) framework, which represents both queries and targets as Gaussian distributions in latent space rather than fixed points. Through careful design of probabilistic distance metrics and hierarchical learning objectives, CoPE explicitly captures uncertainty at both instance and feature levels, enabling more flexible, nuanced, and robust matching that can handle polysemy and ambiguity in search intentions. Extensive experiments across multiple benchmarks demonstrate that CoPE effectively quantifies both quality and semantic uncertainties within Composed Image Retrieval, achieving state-of-the-art performance on recall rate. Code: https://github.com/tanghme0w/ACL25-CoPE.</abstract>
      <url hash="af8eae16">2025.acl-long.61</url>
      <bibkey>tang-etal-2025-modeling</bibkey>
    </paper>
    <paper id="62">
      <title>Untie the Knots: An Efficient Data Augmentation Strategy for Long-Context Pre-Training in Language Models</title>
      <author><first>Junfeng</first><last>Tian</last><affiliation>Xiaohongshu</affiliation></author>
      <author><first>Da</first><last>Zheng</last><affiliation>Xiaohongshu</affiliation></author>
      <author><first>Yang</first><last>Chen</last></author>
      <author><first>Rui</first><last>Wang</last><affiliation>Decilion</affiliation></author>
      <author><first>Colin</first><last>Zhang</last></author>
      <author><first>Debing</first><last>Zhang</last></author>
      <pages>1223-1242</pages>
      <abstract>Large language models (LLM) have prioritized expanding the context window from which models can incorporate more information. However, training models to handle long contexts presents significant challenges. These include the scarcity of high-quality natural long-context data, the potential for performance degradation on short-context tasks, and the reduced training efficiency associated with attention mechanisms. In this paper, we introduce Untie the Knots (UtK), a novel data augmentation strategy employed during the continue pre-training phase, designed to efficiently enable LLMs to gain long-context capabilities without the need to modify the existing data mixture. In particular, we chunk the documents, shuffle the chunks, and create a complex and knotted structure of long texts; LLMs are then trained to untie these knots and identify relevant segments within seemingly chaotic token sequences. This approach greatly improves the model’s performance by accurately attending to relevant information in long context and the training efficiency is also largely increased. We conduct extensive experiments on models with 7B and 72B parameters, trained on 20 billion tokens, demonstrating that UtK achieves 75% and 84.5% accurracy on RULER at 128K context length, significantly outperforming other long context strategies. The trained models will open-source for further research.</abstract>
      <url hash="92c3d667">2025.acl-long.62</url>
      <bibkey>tian-etal-2025-untie</bibkey>
    </paper>
    <paper id="63">
      <title><fixed-case>APPL</fixed-case>: A Prompt Programming Language for Harmonious Integration of Programs and Large Language Model Prompts</title>
      <author><first>Honghua</first><last>Dong</last><affiliation>Department of Computer Science, University of Toronto</affiliation></author>
      <author><first>Qidong</first><last>Su</last><affiliation>University of Toronto</affiliation></author>
      <author><first>Yubo</first><last>Gao</last><affiliation>University of Toronto</affiliation></author>
      <author><first>Zhaoyu</first><last>Li</last><affiliation>University of Toronto</affiliation></author>
      <author><first>Yangjun</first><last>Ruan</last><affiliation>Stanford University and University of Toronto</affiliation></author>
      <author><first>Gennady</first><last>Pekhimenko</last><affiliation>Department of Computer Science, University of Toronto</affiliation></author>
      <author><first>Chris J.</first><last>Maddison</last><affiliation>University of Toronto and Google</affiliation></author>
      <author><first>Xujie</first><last>Si</last><affiliation>University of Toronto</affiliation></author>
      <pages>1243-1266</pages>
      <abstract>Large Language Models (LLMs) have become increasingly capable of handling diverse tasks with the aid of well-crafted prompts and integration of external tools, but as task complexity rises, the workflow involving LLMs can be complicated and thus challenging to implement and maintain. To address this challenge, we propose APPL, A Prompt Programming Language that acts as a bridge between computer programs and LLMs, allowing seamless embedding of prompts into Python functions, and vice versa. APPL provides an intuitive and Python-native syntax, an efficient parallelized runtime with asynchronous semantics, and a tracing module supporting effective failure diagnosis and replaying without extra costs. We demonstrate that APPL programs are intuitive, concise, and efficient through representative scenarios including Chain-of-Thought with self-consistency (CoT-SC) and ReAct tool-use agent. We further use LLMs to judge the language design between APPL and previous work, where the results indicate that codes written in APPL are more readable and intuitive. Our code, tutorial and documentation are available at https://github.com/appl-team/appl.</abstract>
      <url hash="70f3960e">2025.acl-long.63</url>
      <bibkey>dong-etal-2025-appl</bibkey>
    </paper>
    <paper id="64">
      <title>Evaluating Lexical Proficiency in Neural Language Models</title>
      <author><first>Cristiano</first><last>Ciaccio</last><affiliation>Institute for Computational Linguistics “A. Zampolli” (CNR-ILC), Pisa</affiliation></author>
      <author><first>Alessio</first><last>Miaschi</last><affiliation>Institute for Computational Linguistics “A. Zampolli” (CNR-ILC), Pisa</affiliation></author>
      <author><first>Felice</first><last>Dell’Orletta</last><affiliation>Istituto di Linguistica Computazionale “A. Zampolli” (ILC)</affiliation></author>
      <pages>1267-1286</pages>
      <abstract>We present a novel evaluation framework designed to assess the lexical proficiency and linguistic creativity of Transformer-based Language Models (LMs). We validate the framework by analyzing the performance of a set of LMs of different sizes, in both mono- and multilingual configuration, across tasks involving the generation, definition, and contextual usage of lexicalized words, neologisms, and nonce words. To support these evaluations, we developed a novel dataset of lexical entries for the Italian language, including curated definitions and usage examples sourced from various online platforms. The results highlight the robustness and effectiveness of our framework in evaluating multiple dimensions of LMs’ linguistic understanding and offer an insight, through the assessment of their linguistic creativity, on the lexical generalization abilities of LMs.</abstract>
      <url hash="529135ea">2025.acl-long.64</url>
      <bibkey>ciaccio-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="65">
      <title>Autoregressive Speech Synthesis without Vector Quantization</title>
      <author><first>Lingwei</first><last>Meng</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Long</first><last>Zhou</last><affiliation>Microsoft Research Asia</affiliation></author>
      <author><first>Shujie</first><last>Liu</last></author>
      <author><first>Sanyuan</first><last>Chen</last><affiliation>Facebook and Harbin Institute of Technology</affiliation></author>
      <author><first>Bing</first><last>Han</last></author>
      <author><first>Shujie</first><last>Hu</last></author>
      <author><first>Yanqing</first><last>Liu</last><affiliation>Microsoft</affiliation></author>
      <author><first>Jinyu</first><last>Li</last><affiliation>Microsoft</affiliation></author>
      <author><first>Sheng</first><last>Zhao</last><affiliation>Microsoft</affiliation></author>
      <author><first>Xixin</first><last>Wu</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Helen M.</first><last>Meng</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Furu</first><last>Wei</last><affiliation>Microsoft Research</affiliation></author>
      <pages>1287-1300</pages>
      <abstract>We present MELLE, a novel continuous-valued token based language modeling approach for text-to-speech synthesis (TTS). MELLE autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which is typically designed for audio compression and sacrifices fidelity compared to continuous representations. Specifically, (i) instead of cross-entropy loss, we apply regression loss with a proposed spectrogram flux loss function to model the probability distribution of the continuous-valued tokens; (ii) we have incorporated variational inference into MELLE to facilitate sampling mechanisms, thereby enhancing the output diversity and model robustness. Experiments demonstrate that, compared to the two-stage codec language model VALL-E and its variants, the single-stage MELLE mitigates robustness issues by avoiding the inherent flaws of sampling vector-quantized codes, achieves superior performance across multiple metrics, and, most importantly, offers a more streamlined paradigm. The demos of our work are provided at https://aka.ms/melle.</abstract>
      <url hash="b06873af">2025.acl-long.65</url>
      <bibkey>meng-etal-2025-autoregressive</bibkey>
    </paper>
    <paper id="66">
      <title>Cuckoo: An <fixed-case>IE</fixed-case> Free Rider Hatched by Massive Nutrition in <fixed-case>LLM</fixed-case>’s Nest</title>
      <author><first>Letian</first><last>Peng</last></author>
      <author><first>Zilong</first><last>Wang</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Feng</first><last>Yao</last></author>
      <author><first>Jingbo</first><last>Shang</last><affiliation>University of California, San Diego</affiliation></author>
      <pages>1301-1315</pages>
      <abstract>Massive high-quality data, both pre-training raw texts and post-training annotations, have been carefully prepared to incubate advanced large language models (LLMs). In contrast, for information extraction (IE), pre-training data, such as BIO-tagged sequences, are hard to scale up. We show that IE models can act as free riders on LLM resources by reframing next-token <i>prediction</i> into <i>extraction</i> for tokens already present in the context. Specifically, our proposed next tokens extraction (NTE) paradigm learns a versatile IE model, <i>Cuckoo</i>, with 102.6M extractive data converted from LLM’s pre-training and post-training data. Under the few-shot setting, Cuckoo adapts effectively to traditional and complex instruction-following IE with better performance than existing pre-trained IE models. As a free rider, Cuckoo can naturally evolve with the ongoing advancements in LLM data preparation, benefiting from improvements in LLM training pipelines without additional manual effort.</abstract>
      <url hash="7e97184d">2025.acl-long.66</url>
      <bibkey>peng-etal-2025-cuckoo</bibkey>
    </paper>
    <paper id="67">
      <title><fixed-case>F</fixed-case>ed<fixed-case>E</fixed-case>x-<fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case>: Exact Aggregation for Federated and Efficient Fine-Tuning of Large Language Models</title>
      <author><first>Raghav</first><last>Singhal</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Kaustubh</first><last>Ponkshe</last></author>
      <author><first>Praneeth</first><last>Vepakomma</last><affiliation>Massachusetts Institute of Technology and Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>1316-1336</pages>
      <abstract>Low-Rank Adaptation (LoRA) is a popular technique for efficient fine-tuning of foundation models. However, applying LoRA in federated learning environments, where data is distributed across multiple clients, presents unique challenges. Existing methods rely on traditional federated averaging of LoRA adapters, resulting in inexact updates. To address this, we propose Federated Exact LoRA, or FedEx-LoRA, which adds a residual error term to the pre-trained frozen weight matrix. Our approach achieves exact updates with minimal computational and communication overhead, preserving LoRA’s efficiency. We evaluate the method on various models across arithmetic reasoning, commonsense reasoning, natural language understanding and natural language generation tasks, showing consistent performance gains over state-of-the-art methods across multiple settings. Through extensive analysis, we quantify that the deviations in updates from the ideal solution are significant, highlighting the need for exact aggregation. Our method’s simplicity, efficiency, and broad applicability position it as a promising solution for accurate and effective federated fine-tuning of foundation models.</abstract>
      <url hash="602212ad">2025.acl-long.67</url>
      <bibkey>singhal-etal-2025-fedex</bibkey>
    </paper>
    <paper id="68">
      <title>Measuring Social Biases in Masked Language Models by Proxy of Prediction Quality</title>
      <author><first>Rahul</first><last>Zalkikar</last><affiliation>New York University</affiliation></author>
      <author><first>Kanchan</first><last>Chandra</last><affiliation>New York University</affiliation></author>
      <pages>1337-1361</pages>
      <abstract>Innovative transformer-based language models produce contextually-aware token embeddings and have achieved state-of-the-art performance for a variety of natural language tasks, but have been shown to encode unwanted biases for downstream applications. In this paper, we evaluate the social biases encoded by transformers trained with the masked language modeling objective using proposed proxy functions within an iterative masking experiment to measure the quality of transformer models’ predictions and assess the preference of MLMs towards disadvantaged and advantaged groups. We find that all models encode concerning social biases. We compare bias estimations with those produced by other evaluation methods using benchmark datasets and assess their alignment with human annotated biases. We extend previous work by evaluating social biases introduced after retraining an MLM under the masked language modeling objective and find proposed measures produce more accurate and sensitive estimations of biases based on relative preference for biased sentences between models, while other methods tend to underestimate biases after retraining on sentences biased towards disadvantaged groups.</abstract>
      <url hash="4b0f72ca">2025.acl-long.68</url>
      <bibkey>zalkikar-chandra-2025-measuring</bibkey>
    </paper>
    <paper id="69">
      <title>Capturing Author Self Beliefs in Social Media Language</title>
      <author><first>Siddharth</first><last>Mangalik</last></author>
      <author><first>Adithya</first><last>V Ganesan</last><affiliation>, State University of New York, Stony Brook</affiliation></author>
      <author><first>Abigail B.</first><last>Wheeler</last><affiliation>University of Pennsylvania, University of Pennsylvania</affiliation></author>
      <author><first>Nicholas</first><last>Kerry</last></author>
      <author><first>Jeremy D. W.</first><last>Clifton</last><affiliation>University of Pennsylvania, University of Pennsylvania</affiliation></author>
      <author><first>H.</first><last>Schwartz</last><affiliation>Stony Brook University (SUNY)</affiliation></author>
      <author><first>Ryan L.</first><last>Boyd</last><affiliation>University of Texas at Dallas</affiliation></author>
      <pages>1362-1376</pages>
      <abstract>Measuring the prevalence and dimensions of self beliefs is essential for understanding human self-perception and various psychological outcomes. In this paper, we develop a novel task for classifying language that contains explicit or implicit mentions of the author’s self beliefs. We contribute a set of 2,000 human-annotated self beliefs, 100,000 LLM-labeled examples, and 10,000 surveyed self belief paragraphs. We then evaluate several encoder-based classifiers and training routines for this task. Our trained model, SelfAwareNet, achieved an AUC of 0.944, outperforming 0.839 from OpenAI’s state-of-the-art GPT-4o model. Using this model we derive data-driven categories of self beliefs and demonstrate their ability to predict valence, depression, anxiety, and stress. We release the resulting self belief classification model and annotated datasets for use in future research.</abstract>
      <url hash="ef456e0d">2025.acl-long.69</url>
      <bibkey>mangalik-etal-2025-capturing</bibkey>
    </paper>
    <paper id="70">
      <title>Neural Topic Modeling with Large Language Models in the Loop</title>
      <author><first>Xiaohao</first><last>Yang</last></author>
      <author><first>He</first><last>Zhao</last><affiliation>Commonwealth Scientific and Industrial Research Organisation, CSIRO</affiliation></author>
      <author><first>Weijie</first><last>Xu</last><affiliation>Amazon</affiliation></author>
      <author><first>Yuanyuan</first><last>Qi</last></author>
      <author><first>Jueqing</first><last>Lu</last></author>
      <author><first>Dinh</first><last>Phung</last><affiliation>Monash University</affiliation></author>
      <author><first>Lan</first><last>Du</last><affiliation>Monash University</affiliation></author>
      <pages>1377-1401</pages>
      <abstract>Topic modeling is a fundamental task in natural language processing, allowing the discovery of latent thematic structures in text corpora. While Large Language Models (LLMs) have demonstrated promising capabilities in topic discovery, their direct application to topic modeling suffers from issues such as incomplete topic coverage, misalignment of topics, and inefficiency. To address these limitations, we propose LLM-ITL, a novel LLM-in-the-loop framework that integrates LLMs with Neural Topic Models (NTMs). In LLM-ITL, global topics and document representations are learned through the NTM. Meanwhile, an LLM refines these topics using an Optimal Transport (OT)-based alignment objective, where the refinement is dynamically adjusted based on the LLM’s confidence in suggesting topical words for each set of input words. With the flexibility of being integrated into many existing NTMs, the proposed approach enhances the interpretability of topics while preserving the efficiency of NTMs in learning topics and document representations. Extensive experiments demonstrate that LLM-ITL helps NTMs significantly improve their topic interpretability while maintaining the quality of document representation. Our code and datasets are available athttps://github.com/Xiaohao-Yang/LLM-ITL</abstract>
      <url hash="eb34a469">2025.acl-long.70</url>
      <bibkey>yang-etal-2025-neural</bibkey>
    </paper>
    <paper id="71">
      <title><fixed-case>HAL</fixed-case>o<fixed-case>GEN</fixed-case>: Fantastic <fixed-case>LLM</fixed-case> Hallucinations and Where to Find Them</title>
      <author><first>Abhilasha</first><last>Ravichander</last><affiliation>University of Washington and School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Shrusti</first><last>Ghela</last><affiliation>University of Washington</affiliation></author>
      <author><first>David</first><last>Wadden</last><affiliation>Google</affiliation></author>
      <author><first>Yejin</first><last>Choi</last><affiliation>Computer Science Department, Stanford University and NVIDIA</affiliation></author>
      <pages>1402-1425</pages>
      <abstract>Despite their impressive ability to generate high-quality and fluent text, generative large language models (LLMs) also produce hallucinations: statements that are misaligned with established world knowledge or provided input context. However, measuring hallucination can be challenging, as having humans verify model generations on-the-fly is both expensive and time-consuming. In this work, we release HALoGEN, a comprehensive hallucination benchmark consisting of: (1) 10,923 prompts for generative models spanning nine domains including programming, scientific attribution, and summarization, and (2) automatic high-precision verifiers for each use case that decompose LLM generations into atomic units, and verify each unit against a high-quality knowledge source. We use this framework to evaluate ~150,000 generations from 14 language models, finding that even the best-performing models are riddled with hallucinations (sometimes up to 86% of generated atomic facts depending on the domain). We further define a novel error classification for LLM hallucinations based on whether they likely stem from incorrect recollection of training data (Type A errors), or incorrect knowledge in training data (Type B errors), or are fabrication (Type C errors). We hope our framework provides a foundation to enable the principled study of why generative models hallucinate, and advances the development of trustworthy large language models.</abstract>
      <url hash="be59fe7c">2025.acl-long.71</url>
      <bibkey>ravichander-etal-2025-halogen</bibkey>
    </paper>
    <paper id="72">
      <title>Synergizing <fixed-case>LLM</fixed-case>s with Global Label Propagation for Multimodal Fake News Detection</title>
      <author><first>Shuguo</first><last>Hu</last></author>
      <author><first>Jun</first><last>Hu</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Huaiwen</first><last>Zhang</last><affiliation>Inner Mongolia University</affiliation></author>
      <pages>1426-1440</pages>
      <abstract>Large Language Models (LLMs) can assist multimodal fake news detection by predicting pseudo labels. However, LLM-generated pseudo labels alone demonstrate poor performance compared to traditional detection methods, making their effective integration non-trivial. In this paper, we propose Global Label Propagation Network with LLM-based Pseudo Labeling (GLPN-LLM) for multimodal fake news detection, which integrates LLM capabilities via label propagation techniques. The global label propagation can utilize LLM-generated pseudo labels, enhancing prediction accuracy by propagating label information among all samples. For label propagation, a mask-based mechanism is designed to prevent label leakage during training by ensuring that training nodes do not propagate their own labels back to themselves. Experimental results on benchmark datasets show that by synergizing LLMs with label propagation, our model achieves superior performance over state-of-the-art baselines.</abstract>
      <url hash="d6c05a14">2025.acl-long.72</url>
      <bibkey>hu-etal-2025-synergizing</bibkey>
    </paper>
    <paper id="73">
      <title>“Yes, My <fixed-case>L</fixed-case>o<fixed-case>RD</fixed-case>.” Guiding Language Model Extraction with Locality Reinforced Distillation</title>
      <author><first>Zi</first><last>Liang</last></author>
      <author><first>Qingqing</first><last>Ye</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Yanyun</first><last>Wang</last><affiliation>The Hong Kong University of Science and Technology (Guangzhou)</affiliation></author>
      <author><first>Sen</first><last>Zhang</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Yaxin</first><last>Xiao</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>RongHua</first><last>Li</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Jianliang</first><last>Xu</last><affiliation>Hong Kong Baptist University</affiliation></author>
      <author><first>Haibo</first><last>Hu</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <pages>1441-1465</pages>
      <abstract>Model extraction attacks (MEAs) on large language models (LLMs) have received increasing attention in recent research. However, existing attack methods typically adapt the extraction strategies originally developed for deep neural networks (DNNs). They neglect the underlying inconsistency between the training tasks of MEA and LLM alignment, leading to suboptimal attack performance. To tackle this issue, we propose Locality Reinforced Distillation (LoRD), a novel model extraction algorithm specifically designed for LLMs. In particular, LoRD employs a newly defined policy-gradient-style training task that utilizes the responses of victim model as the signal to guide the crafting of preference for the local model. Theoretical analyses demonstrate that I) The convergence procedure of LoRD in model extraction is consistent with the alignment procedure of LLMs, and II) LoRD can reduce query complexity while mitigating watermark protection through our exploration-based stealing. Extensive experiments validate the superiority of our method in extracting various state-of-the-art commercial LLMs. Our code is available at: https://github.com/liangzid/LoRD-MEA.</abstract>
      <url hash="0dc09c18">2025.acl-long.73</url>
      <bibkey>liang-etal-2025-yes</bibkey>
    </paper>
    <paper id="74">
      <title>Jailbreak Large Vision-Language Models Through Multi-Modal Linkage</title>
      <author><first>Yu</first><last>Wang</last><affiliation>University of the Chinese Academy of Sciences</affiliation></author>
      <author><first>Xiaofei</first><last>Zhou</last></author>
      <author><first>Yichen</first><last>Wang</last></author>
      <author><first>Geyuan</first><last>Zhang</last></author>
      <author><first>Tianxing</first><last>He</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>1466-1494</pages>
      <abstract>With the rapid advancement of Large Vision-Language Models (VLMs), concerns about their ‌potential misuse and abuse have grown rapidly. Prior research has exposed VLMs’ vulnerability to jailbreak attacks, where carefully crafted inputs can lead the model to produce content that violates ethical and legal standards. However, current jailbreak methods often fail against cutting-edge models such as GPT-4o. We attribute this to the over-exposure of harmful content and the absence of stealthy malicious guidance. In this work, we introduce a novel jailbreak framework: Multi-Modal Linkage (MML) Attack. Drawing inspiration from cryptography, MML employs an encryption-decryption process across text and image modalities to mitigate the over-exposure of malicious information. To covertly align the model’s output with harmful objectives, MML leverages a technique we term evil alignment, framing the attack within the narrative context of a video game development scenario. Extensive experiments validate the effectiveness of MML. Specifically, MML jailbreaks GPT-4o with attack success rates of 99.40% on SafeBench, 98.81% on MM-SafeBench, and 99.07% on HADES-Dataset. Our code is available at https://github.com/wangyu-ovo/MML.</abstract>
      <url hash="6ef9a48f">2025.acl-long.74</url>
      <bibkey>wang-etal-2025-jailbreak</bibkey>
    </paper>
    <paper id="75">
      <title>Wait, that’s not an option: <fixed-case>LLM</fixed-case>s Robustness with Incorrect Multiple-Choice Options</title>
      <author><first>Gracjan</first><last>Góral</last></author>
      <author><first>Emilia</first><last>Wiśnios</last></author>
      <author><first>Piotr</first><last>Sankowski</last><affiliation>MIM Solutions and University of Warsaw</affiliation></author>
      <author><first>Paweł</first><last>Budzianowski</last><affiliation>K-Scale Labs and University of Warsaw</affiliation></author>
      <pages>1495-1515</pages>
      <abstract>This work introduces a novel framework for evaluating LLMs’ capacity to balance instruction-following with critical reasoning when presented with multiple-choice questions containing no valid answers. Through systematic evaluation across arithmetic, domain-specific knowledge, and high-stakes medical decision tasks, we demonstrate that post-training aligned models often default to selecting invalid options, while base models exhibit improved refusal capabilities that scale with model size. Our analysis reveals that alignment techniques, though intended to enhance helpfulness, can inadvertently impair models’ reflective judgment–the ability to override default behaviors when faced with invalid options. We additionally conduct a parallel human study showing similar instruction-following biases, with implications for how these biases may propagate through human feedback datasets used in alignment. We provide extensive ablation studies examining the impact of model size, training techniques, and prompt engineering. Our findings highlight fundamental tensions between alignment optimization and preservation of critical reasoning capabilities, with important implications for developing more robust AI systems for real-world deployment.</abstract>
      <url hash="be5aa861">2025.acl-long.75</url>
      <bibkey>goral-etal-2025-wait</bibkey>
    </paper>
    <paper id="76">
      <title>The Hidden Attention of Mamba Models</title>
      <author><first>Ameen Ali</first><last>Ali</last><affiliation>Tel Aviv University</affiliation></author>
      <author><first>Itamar</first><last>Zimerman</last><affiliation>Tel Aviv University</affiliation></author>
      <author><first>Lior</first><last>Wolf</last><affiliation>Tel Aviv University, Tel Aviv University and Tel Aviv University</affiliation></author>
      <pages>1516-1534</pages>
      <abstract>The Mamba layer offers an efficient selective state-space model (SSM) that is highly effective in modeling multiple domains, includingNLP, long-range sequence processing, and computer vision. Selective SSMs are viewed as dual models, in which one trains in parallel on the entire sequence via an IO-aware parallel scan, and deploys in an autoregressive manner. We add a third view and show that such models can be viewed as attention-driven models. This new perspective enables us to empirically and theoretically compare the underlying mechanisms to that of the attention in transformers and allows us to peer inside the inner workings of the Mamba model with explainability methods. Our code is publicly available.</abstract>
      <url hash="4babeffe">2025.acl-long.76</url>
      <bibkey>ali-etal-2025-hidden</bibkey>
    </paper>
    <paper id="77">
      <title><fixed-case>KV</fixed-case>-Latent: Dimensional-level <fixed-case>KV</fixed-case> Cache Reduction with Frequency-aware Rotary Positional Embedding</title>
      <author><first>Shi</first><last>Luohe</last></author>
      <author><first>Zuchao</first><last>Li</last></author>
      <author><first>Lefei</first><last>Zhang</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Baoyuan</first><last>Qi</last><affiliation>Xiaomi Corporation</affiliation></author>
      <author><first>Liu</first><last>Guoming</last><affiliation>Xiaomi Corporation</affiliation></author>
      <author><first>Hai</first><last>Zhao</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>1535-1550</pages>
      <abstract>Large language models (LLMs) based on Transformer Decoders have become the preferred choice for conversational generative AI. Despite the overall superiority of the Decoder architecture, the gradually increasing Key-Value (KV) cache during inference has emerged as a primary efficiency bottleneck, both in aspects of memory consumption and data transfer bandwidth limitations. To address these challenges, we propose a paradigm called KV-Latent. By down-sampling the Key-Value vector dimensions into a latent space, we can significantly reduce the KV Cache footprint and improve inference speed, only with a small amount of extra training, less than 1% of pre-training takes. Besides, we enhanced the stability of Rotary Positional Embedding applied on lower-dimensional vectors by modifying its frequency sampling mechanism, avoiding noise introduced by higher frequencies while retaining position attenuation. Our experiments, including both models with Grouped Query Attention and those without, have yielded satisfactory results. Finally, we conducted comparative experiments to study the impact of separately reducing Key and Value components on model’s performance. Our approach allows for the construction of more efficient language model systems, and opens the new possibility on KV Cache saving and efficient LLMs.</abstract>
      <url hash="7fa7c0c6">2025.acl-long.77</url>
      <bibkey>luohe-etal-2025-kv</bibkey>
    </paper>
    <paper id="78">
      <title><fixed-case>LEANCODE</fixed-case>: Understanding Models Better for Code Simplification of Pre-trained Large Language Models</title>
      <author><first>Yan</first><last>Wang</last><affiliation>Central University of Finance and Economics</affiliation></author>
      <author><first>Ling</first><last>Ding</last></author>
      <author><first>Tien N</first><last>Nguyen</last><affiliation>university of texas at dallas</affiliation></author>
      <author><first>Shaohua</first><last>Wang</last></author>
      <author><first>Yanan</first><last>Zheng</last></author>
      <pages>1551-1567</pages>
      <abstract>Large Language Models for code often entail significant computational complexity, which grows significantly with the length of the input code sequence. We propose <tex-math>LeanCode</tex-math> for code simplification to reduce training and prediction time, leveraging code contexts in utilizing attention scores to represent the tokens’ importance. We advocate for the selective removal of tokens based on the average context-aware attention scores rather than average scores across all inputs. <tex-math>LeanCode</tex-math> uses the attention scores of ‘CLS’ tokens within the encoder for classification tasks, such as code search. It also employs the encoder-decoder attention scores to determine token significance for sequence-to-sequence tasks like code summarization. Our evaluation shows <tex-math>LeanCode</tex-math>‘s superiority over the SOTAs <tex-math>DietCode</tex-math> and <tex-math>SlimCode</tex-math>, with improvements of 60% and 16% for code search, and 29% and 27% for code summarization, respectively.</abstract>
      <url hash="b29333ec">2025.acl-long.78</url>
      <bibkey>wang-etal-2025-leancode</bibkey>
    </paper>
    <paper id="79">
      <title><fixed-case>MARS</fixed-case>: Benchmarking the Metaphysical Reasoning Abilities of Language Models with a Multi-task Evaluation Dataset</title>
      <author><first>Weiqi</first><last>Wang</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Yangqiu</first><last>Song</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <pages>1568-1596</pages>
      <abstract>To enable Large Language Models (LLMs) to function as conscious agents with generalizable reasoning capabilities, it is crucial that they possess the ability to ***comprehend situational changes (transitions) in distribution*** triggered by environmental factors or actions from other agents. Despite its fundamental significance, this ability remains underexplored due to the complexity of modeling infinite possible changes in an event and their associated distributions, coupled with the lack of benchmark data with situational transitions. Addressing these gaps, we propose a novel formulation of ***reasoning with distributional changes as a three-step discriminative process***, termed as ***MetAphysical ReaSoning***. We then introduce the first-ever benchmark, **MARS**, comprising three tasks corresponding to each step. These tasks systematically assess LLMs’ capabilities in reasoning the plausibility of (i) changes in actions, (ii) states caused by changed actions, and (iii) situational transitions driven by changes in action. Extensive evaluations with 20 (L)LMs of varying sizes and methods indicate that all three tasks in this process pose significant challenges, even after fine-tuning. Further analyses reveal potential causes for the underperformance of LLMs and demonstrate that pre-training on large-scale conceptualization taxonomies can potentially enhance LMs’ metaphysical reasoning capabilities. Our data and models are publicly accessible at https://github.com/HKUST-KnowComp/MARS.</abstract>
      <url hash="5c0d2856">2025.acl-long.79</url>
      <bibkey>wang-song-2025-mars</bibkey>
    </paper>
    <paper id="80">
      <title>Ask-Before-Detection: Identifying and Mitigating Conformity Bias in <fixed-case>LLM</fixed-case>-Powered Error Detector for Math Word Problem Solutions</title>
      <author><first>Hang</first><last>Li</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Tianlong</first><last>Xu</last><affiliation>Squirrel Ai Learning</affiliation></author>
      <author><first>Kaiqi</first><last>Yang</last></author>
      <author><first>Yucheng</first><last>Chu</last></author>
      <author><first>Yanling</first><last>Chen</last></author>
      <author><first>Yichi</first><last>Song</last></author>
      <author><first>Qingsong</first><last>Wen</last><affiliation>Squirrel Ai Learning</affiliation></author>
      <author><first>Hui</first><last>Liu</last></author>
      <pages>1597-1609</pages>
      <abstract>The rise of large language models (LLMs) offers new opportunities for automatic error detection in education, particularly for math word problems (MWPs). While prior studies demonstrate the promise of LLMs as error detectors, they overlook the presence of multiple valid solutions for a single MWP. Our preliminary analysis reveals a significant performance gap between conventional and alternative solutions in MWPs, a phenomenon we term conformity bias in this work. To mitigate this bias, we introduce the Ask-Before-Detect (AskBD) framework, which generates adaptive reference solutions using LLMs to enhance error detection. Experiments on 200 examples of GSM8K show that AskBD effectively mitigates bias and improves performance, especially when combined with reasoning-enhancing techniques like chain-of-thought prompting.</abstract>
      <url hash="71e1f6c7">2025.acl-long.80</url>
      <bibkey>li-etal-2025-ask</bibkey>
    </paper>
    <paper id="81">
      <title>Real-time Factuality Assessment from Adversarial Feedback</title>
      <author><first>Sanxing</first><last>Chen</last></author>
      <author><first>Yukun</first><last>Huang</last><affiliation>Duke University</affiliation></author>
      <author><first>Bhuwan</first><last>Dhingra</last><affiliation>Duke University</affiliation></author>
      <pages>1610-1630</pages>
      <abstract>We show that existing evaluations for assessing the factuality of news from conventional sources, such as claims on fact-checking websites, result in high accuracies over time for LLM-based detectors—even after their knowledge cutoffs. This suggests that recent popular false information from such sources can be easily identified due to its likely presence in pre-training/retrieval corpora or the emergence of salient, yet shallow, patterns in these datasets. Instead, we argue that a proper factuality evaluation dataset should test a model’s ability to reason about current events by retrieving and reading related evidence. To this end, we develop a novel pipeline that leverages natural language feedback from a RAG-based detector to iteratively modify real-time news into deceptive variants that challenge LLMs. Our iterative rewrite decreases the binary classification ROC-AUC by an absolute 17.5 percent for a strong RAG-based GPT-4o detector. Our experiments reveal the important role of RAG in both evaluating and generating challenging news examples, as retrieval-free LLM detectors are vulnerable to unseen events and adversarial attacks, while feedback from RAG-based evaluation helps discover more deceitful patterns.</abstract>
      <url hash="1f599e8e">2025.acl-long.81</url>
      <bibkey>chen-etal-2025-real</bibkey>
    </paper>
    <paper id="82">
      <title>Improve Vision Language Model Chain-of-thought Reasoning</title>
      <author><first>Ruohong</first><last>Zhang</last></author>
      <author><first>Bowen</first><last>Zhang</last><affiliation>Apple</affiliation></author>
      <author><first>Yanghao</first><last>Li</last><affiliation>Apple</affiliation></author>
      <author><first>Haotian</first><last>Zhang</last><affiliation>Apple AI/ML</affiliation></author>
      <author><first>Zhiqing</first><last>Sun</last><affiliation>OpenAI</affiliation></author>
      <author><first>Zhe</first><last>Gan</last><affiliation>Apple</affiliation></author>
      <author><first>Yinfei</first><last>Yang</last><affiliation>Apple</affiliation></author>
      <author><first>Ruoming</first><last>Pang</last><affiliation>Apple</affiliation></author>
      <author><first>Yiming</first><last>Yang</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <pages>1631-1662</pages>
      <abstract>Chain-of-thought (CoT) reasoning in vision language models (VLMs) is crucial for improving interpretability and trustworthiness. However, current training recipes often relying on datasets dominated by short annotations with minimal rationales. In this work, we show that training VLM on short answers leads to poor generalization on reasoning tasks that require more detailed explanations. To address this limitation, we propose a two-stage post-training strategy that extends the usage of short answer data for enhanced CoT reasoning. First, we augment short answers with CoT reasoning generated by GPT-4o, enhancing the VLM’s CoT capabilities through fine-tuning. Second, we leverage short answers as outcome rewards for reinforcement learning. Specifically, short answers are used as correctness indicators to construct positive (correct) and negative (incorrect) pairs from model-generated reasoning chains. These pairs are then used to calibrate the model’s reasoning via Direct Preference Optimization. Our experiments show significant improvements in CoT reasoning on benchmark datasets, along with enhanced generalization to direct answer prediction. This work provides a critical data resource for VLM CoT training and demonstrates the effectiveness of outcome rewards for multimodal models post-training.</abstract>
      <url hash="5a4bf2ae">2025.acl-long.82</url>
      <bibkey>zhang-etal-2025-improve</bibkey>
    </paper>
    <paper id="83">
      <title>On the Mutual Influence of Gender and Occupation in <fixed-case>LLM</fixed-case> Representations</title>
      <author><first>Haozhe</first><last>An</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Connor</first><last>Baumler</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Abhilasha</first><last>Sancheti</last><affiliation>Amazon</affiliation></author>
      <author><first>Rachel</first><last>Rudinger</last></author>
      <pages>1663-1680</pages>
      <abstract>We examine LLM representations of gender for first names in various occupational contexts to study how occupations and the gender perception of first names in LLMs influence each other mutually. We find that LLMs’ first-name gender representations correlate with real-world gender statistics associated with the name, and are influenced by the co-occurrence of stereotypically feminine or masculine occupations. Additionally, we study the influence of first-name gender representations on LLMs in a downstream occupation prediction task and their potential as an internal metric to identify extrinsic model biases. While feminine first-name embeddings often raise the probabilities for female-dominated jobs (and vice versa for male-dominated jobs), reliably using these internal gender representations for bias detection remains challenging.</abstract>
      <url hash="a64018a9">2025.acl-long.83</url>
      <bibkey>an-etal-2025-mutual</bibkey>
    </paper>
    <paper id="84">
      <title>Disentangling Memory and Reasoning Ability in Large Language Models</title>
      <author><first>Mingyu</first><last>Jin</last></author>
      <author><first>Weidi</first><last>Luo</last></author>
      <author><first>Sitao</first><last>Cheng</last></author>
      <author><first>Xinyi</first><last>Wang</last><affiliation>UC Santa Barbara</affiliation></author>
      <author><first>Wenyue</first><last>Hua</last></author>
      <author><first>Ruixiang</first><last>Tang</last><affiliation>Rutgers University</affiliation></author>
      <author><first>William Yang</first><last>Wang</last><affiliation>UC Santa Barbara</affiliation></author>
      <author><first>Yongfeng</first><last>Zhang</last><affiliation>Rutgers University</affiliation></author>
      <pages>1681-1701</pages>
      <abstract>Large Language Models (LLMs) have demonstrated strong performance in handling complex tasks that require both extensive knowledge and reasoning abilities. However, the existing LLM inference pipeline operates as an opaque process without explicit separation between knowledge retrieval and reasoning steps, making the model’s decision-making process unclear and disorganized. Recent research has shown that this ambiguity will lead to issues such as knowledge forgetting, which significantly impact the reliability of LLMs. In this paper, we propose a novel language model inference paradigm that decomposes the complex inference process into two distinct and clear actions: <b>(1) memory recall</b>: which retrieves relevant knowledge in LLM, and <b>(2) reasoning</b>: which performs reasoning steps based on the recalled knowledge. To facilitate this decomposition, we introduce two special tokens <b>
          <tex-math>\langle \text{memory} \rangle</tex-math></b> and <b>
          <tex-math>\langle \text{reason} \rangle</tex-math></b>, guiding the model to distinguish between steps that require knowledge retrieval and those that involve reasoning. Our experiment results show that this decomposition not only improves LLMs’ performance among utility benchmarks but also enhances interpretability during the inference process, enabling users to identify sources of error and refine model responses effectively. The code is available at: https://github.com/MingyuJ666/Disentangling-Memory-and-Reasoning.</abstract>
      <url hash="e3d09feb">2025.acl-long.84</url>
      <bibkey>jin-etal-2025-disentangling-memory</bibkey>
    </paper>
    <paper id="85">
      <title>Open-World Attribute Mining for <fixed-case>E</fixed-case>-Commerce Products with Multimodal Self-Correction Instruction Tuning</title>
      <author><first>Jiaqi</first><last>Li</last></author>
      <author><first>Yanming</first><last>Li</last></author>
      <author><first>Xiaoli</first><last>Shen</last></author>
      <author><first>Chuanyi</first><last>Zhang</last><affiliation>Hohai University</affiliation></author>
      <author><first>Guilin</first><last>Qi</last></author>
      <author><first>Sheng</first><last>Bi</last><affiliation>Southeast University</affiliation></author>
      <pages>1702-1714</pages>
      <abstract>In e-commerce, effective product Attribute Mining (AM) is essential for improving product features and aiding consumer decisions. However, current AM methods often focus on extracting attributes from unimodal text, underutilizing multimodal data. In this paper, we propose a novel framework called Multimodal Self-Correction Instruction Tuning (MSIT) to mine new potential attributes from both images and text with Multimodal Large Language Models. The tuning process involves two datasets: Attribute Generation Tuning Data (AGTD) and Chain-of-Thought Tuning Data (CTTD). AGTD is constructed utilizing in-context learning with a small set of seed attributes, aiding the MLLM in accurately extracting attribute-value pairs from multimodal information. To introduce explicit reasoning and improve the extraction in accuracy, we construct CTTD, which incorporates a structured 5-step reasoning process for self-correction. Finally, we employ a 3-stage inference process to filter out redundant attributes and sequentially validate each generated attribute. Comprehensive experimental results on two datasets show that MSIT outperforms state-of-the-art methods. We will release our code and data in the near future.</abstract>
      <url hash="44e75d7f">2025.acl-long.85</url>
      <bibkey>li-etal-2025-open</bibkey>
    </paper>
    <paper id="86">
      <title>Normalized <fixed-case>AOPC</fixed-case>: Fixing Misleading Faithfulness Metrics for Feature Attributions Explainability</title>
      <author><first>Joakim</first><last>Edin</last><affiliation>University of Copenhagen</affiliation></author>
      <author><first>Andreas Geert</first><last>Motzfeldt</last></author>
      <author><first>Casper L.</first><last>Christensen</last><affiliation>Corti</affiliation></author>
      <author><first>Tuukka</first><last>Ruotsalo</last><affiliation>Lappeenranta University of Technology and University of Copenhagen</affiliation></author>
      <author><first>Lars</first><last>Maaløe</last><affiliation>Technical University of Denmark</affiliation></author>
      <author><first>Maria</first><last>Maistro</last><affiliation>University of Copenhagen</affiliation></author>
      <pages>1715-1730</pages>
      <abstract>Deep neural network predictions are notoriously difficult to interpret. Feature attribution methods aim to explain these predictions by identifying the contribution of each input feature. Faithfulness, often evaluated using the area over the perturbation curve (AOPC), reflects feature attributions’ accuracy in describing the internal mechanisms of deep neural networks. However, many studies rely on AOPC to compare faithfulness across different models, which we show can lead to false conclusions about models’ faithfulness. Specifically, we find that AOPC is sensitive to variations in the model, resulting in unreliable cross-model comparisons. Moreover, AOPC scores are difficult to interpret in isolation without knowing the model-specific lower and upper limits. To address these issues, we propose a normalization approach, Normalized AOPC (NAOPC), enabling consistent cross-model evaluations and more meaningful interpretation of individual scores. Our experiments demonstrate that this normalization can radically change AOPC results, questioning the conclusions of earlier studies and offering a more robust framework for assessing feature attribution faithfulness. Our code is available at https://github.com/JoakimEdin/naopc.</abstract>
      <url hash="78a26c31">2025.acl-long.86</url>
      <bibkey>edin-etal-2025-normalized</bibkey>
    </paper>
    <paper id="87">
      <title>Takin-<fixed-case>VC</fixed-case>: Expressive Zero-Shot Voice Conversion via Adaptive Hybrid Content Encoding and Enhanced Timbre Modeling</title>
      <author><first>Yang</first><last>Yuguang</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Yu</first><last>Pan</last></author>
      <author><first>Jixun</first><last>Yao</last></author>
      <author><first>Xiang</first><last>Zhang</last></author>
      <author><first>Jianhao</first><last>Ye</last></author>
      <author><first>Hongbin</first><last>Zhou</last><affiliation>Ximalaya Inc.</affiliation></author>
      <author><first>Lei</first><last>Xie</last><affiliation>Northwest Polytechnical University</affiliation></author>
      <author><first>Lei</first><last>Ma</last><affiliation>The University of Tokyo and University of Alberta</affiliation></author>
      <author><first>Jianjun</first><last>Zhao</last><affiliation>Kyushu University</affiliation></author>
      <pages>1731-1742</pages>
      <abstract>Expressive zero-shot voice conversion (VC) is a critical and challenging task that aims to transform the source timbre into an arbitrary unseen speaker while preserving the original content and expressive qualities. Despite recent progress in zero-shot VC, there remains considerable potential for improvements in speaker similarity and speech naturalness. Moreover, existing zero-shot VC systems struggle to fully reproduce paralinguistic information in highly expressive speech, such as breathing, crying, and emotional nuances, limiting their practical applicability. To address these issues, we propose Takin-VC, a novel expressive zero-shot VC framework via adaptive hybrid content encoding and memory-augmented context-aware timbre modeling. Specifically, we introduce an innovative hybrid content encoder that incorporates an adaptive fusion module, capable of effectively integrating quantized features of the pre-trained WavLM and HybridFormer in an implicit manner, so as to extract precise linguistic features while enriching paralinguistic elements. For timbre modeling, we propose advanced memory-augmented and context-aware modules to generate high-quality target timbre features and fused representations that seamlessly align source content with target timbre. To enhance real-time performance, we advocate a conditional flow matching model to reconstruct the Mel-spectrogram of the source speech. Experimental results show that our Takin-VC consistently surpasses state-of-the-art VC systems, achieving notable improvements in terms of speech naturalness, speech expressiveness, and speaker similarity, while offering enhanced inference speed.</abstract>
      <url hash="73cc16fa">2025.acl-long.87</url>
      <bibkey>yuguang-etal-2025-takin</bibkey>
    </paper>
    <paper id="88">
      <title><fixed-case>L</fixed-case>ang<fixed-case>SAMP</fixed-case>: Language-Script Aware Multilingual Pretraining</title>
      <author><first>Yihong</first><last>Liu</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Haotian</first><last>Ye</last><affiliation>Center for Information and Language Processing</affiliation></author>
      <author><first>Chunlan</first><last>Ma</last></author>
      <author><first>Mingyang</first><last>Wang</last></author>
      <author><first>Hinrich</first><last>Schuetze</last></author>
      <pages>1743-1770</pages>
      <abstract>Recent multilingual pretrained language models (mPLMs) often avoid using language embeddings – learnable vectors assigned to individual languages. However, this places a significant burden on token representations to encode all language-specific information, which may hinder language neutrality. To address this limitation, we propose Language-Script Aware Multilingual Pretraining (LangSAMP), a method that incorporates both language and script embeddings to enhance representation learning. Specifically, we integrate these embeddings into the output of the Transformer blocks before passing the final representations to the language modeling head for prediction. We apply LangSAMP to the continual pretraining of XLM-R on a highly multilingual corpus covering more than 500 languages. The resulting model consistently outperforms the baseline in zero-shot crosslingual transfer across diverse downstream tasks. Extensive analysis reveals that language and script embeddings capture language- and script-specific nuances, which benefits more language-neutral representations, proven by improved pairwise cosine similarity. In our case study, we also show that language and script embeddings can be used to select better source languages for crosslingual transfer. We make our code and models publicly available at <url>https://github.com/cisnlp/LangSAMP</url>.</abstract>
      <url hash="2d0f89ee">2025.acl-long.88</url>
      <bibkey>liu-etal-2025-langsamp</bibkey>
    </paper>
    <paper id="89">
      <title><fixed-case>R</fixed-case>elational<fixed-case>C</fixed-case>oder: Rethinking Complex Tables via Programmatic Relational Transformation</title>
      <author><first>Haoyu</first><last>Dong</last></author>
      <author><first>Yue</first><last>Hu</last></author>
      <author><first>Huailiang</first><last>Peng</last></author>
      <author><first>Yanan</first><last>Cao</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <pages>1771-1784</pages>
      <abstract>Semi-structured tables, with their varied layouts and formatting artifacts, remain a major obstacle for automated data processing and analytics. To address these challenges, we propose RelationalCoder, which uniformly converts semi-structured tables into relational data, enabling smooth integration with the rich ecosystem of data processing and analytics tools. By leveraging SQL code, RelationalCoder prevents schema errors and markedly improves normalization quality across multiple relational tables.To address the challenge of large tables, we propose a new technique called Loop Reference Decoding (LRD): it identifies <i>expandable groups</i>—repeating regions of similar structure and semantics—and replicates each group using a concise loop over its repetitive region by referencing cell addresses, rather than regenerating each individual cell. This design substantially reduces output length from <tex-math>\mathcal{O}(N \times M)</tex-math>—proportional to the table’s height (<tex-math>N</tex-math>) and width (<tex-math>M</tex-math>)—to approximately <tex-math>\mathcal{O}(K)</tex-math>, where <tex-math>K</tex-math> is the total number of unique cell types within detected expandable groups. As a result, LRD is highly scalable: the larger the input table, the greater the compression ratio. It scales seamlessly to extremely large tables, achieving output reductions of up to <tex-math>100{,}000\times</tex-math>.We further create the first human-labeled corpus for table transformation, created with a cost-efficient, actively supervised pipeline. Extensive experiments on HiTab and MultiHiertt show that RelationalCoder not only enables programmatic symbolic reasoning but also boosts QA accuracy—raising Llama-2 and Mistral models by more than 20%, and GPT-4o by over 4%. Project page: https://github.com/haoyudong/RelationalCoder.</abstract>
      <url hash="1f817d97">2025.acl-long.89</url>
      <bibkey>dong-etal-2025-relationalcoder</bibkey>
    </paper>
    <paper id="90">
      <title>Algorithmic Fidelity of Large Language Models in Generating Synthetic <fixed-case>G</fixed-case>erman Public Opinions: A Case Study</title>
      <author><first>Bolei</first><last>Ma</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Berk</first><last>Yoztyurk</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Anna-Carolina</first><last>Haensch</last><affiliation>University of Maryland, College Park and Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Xinpeng</first><last>Wang</last></author>
      <author><first>Markus</first><last>Herklotz</last></author>
      <author><first>Frauke</first><last>Kreuter</last><affiliation>University of Maryland</affiliation></author>
      <author><first>Barbara</first><last>Plank</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Matthias</first><last>Aßenmacher</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <pages>1785-1809</pages>
      <abstract>In recent research, large language models (LLMs) have been increasingly used to investigate public opinions. This study investigates the algorithmic fidelity of LLMs, i.e., the ability to replicate the socio-cultural context and nuanced opinions of human participants. Using open-ended survey data from the German Longitudinal Election Studies (GLES), we prompt different LLMs to generate synthetic public opinions reflective of German subpopulations by incorporating demographic features into the persona prompts. Our results show that Llama performs better than other LLMs at representing subpopulations, particularly when there is lower opinion diversity within those groups. Our findings further reveal that the LLM performs better for supporters of left-leaning parties like The Greens and The Left compared to other parties, and matches the least with the right-party AfD. Additionally, the inclusion or exclusion of specific variables in the prompts can significantly impact the models’ predictions. These findings underscore the importance of aligning LLMs to more effectively model diverse public opinions while minimizing political biases and enhancing robustness in representativeness.</abstract>
      <url hash="2d267f57">2025.acl-long.90</url>
      <bibkey>ma-etal-2025-algorithmic</bibkey>
    </paper>
    <paper id="91">
      <title><fixed-case>TUNA</fixed-case>: Comprehensive Fine-grained Temporal Understanding Evaluation on Dense Dynamic Videos</title>
      <author><first>Fanheng</first><last>Kong</last></author>
      <author><first>Jingyuan</first><last>Zhang</last><affiliation>Kuaishou- 快手科技</affiliation></author>
      <author><first>Hongzhi</first><last>Zhang</last><affiliation>Kuaishou- 快手科技</affiliation></author>
      <author><first>Shi</first><last>Feng</last><affiliation>Northeastern University, China</affiliation></author>
      <author><first>Daling</first><last>Wang</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Linhao</first><last>Yu</last></author>
      <author><first>Xingguang</first><last>Ji</last><affiliation>Kuaishou- 快手科技</affiliation></author>
      <author><first>Yu</first><last>Tian</last></author>
      <author><first>V.</first><last>W.</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Fuzheng</first><last>Zhang</last></author>
      <pages>1810-1839</pages>
      <abstract>Videos are unique in their integration of temporal elements, including camera, scene, action, and attribute, along with their dynamic relationships over time. However, existing benchmarks for video understanding often treat these properties separately or narrowly focus on specific aspects, overlooking the holistic nature of video content. To address this, we introduce TUNA, a temporal-oriented benchmark for fine-grained understanding on dense dynamic videos, with two complementary tasks: captioning and QA. Our TUNA features diverse video scenarios and dynamics, assisted by interpretable and robust evaluation criteria. We evaluate several leading models on our benchmark, providing fine-grained performance assessments across various dimensions. This evaluation reveals key challenges in video temporal understanding, such as limited action description, inadequate multi-subject understanding, and insensitivity to camera motion, offering valuable insights for improving video understanding models.</abstract>
      <url hash="70b61f52">2025.acl-long.91</url>
      <bibkey>kong-etal-2025-tuna</bibkey>
    </paper>
    <paper id="92">
      <title>Self-Instructed Derived Prompt Generation Meets In-Context Learning: Unlocking New Potential of Black-Box <fixed-case>LLM</fixed-case>s</title>
      <author><first>Zhuo</first><last>Li</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <author><first>Yuhao</first><last>Du</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <author><first>Jinpeng</first><last>Hu</last><affiliation>Hefei University of Technology</affiliation></author>
      <author><first>Xiang</first><last>Wan</last><affiliation>Shenzhen Research Institute of Big Data</affiliation></author>
      <author><first>Anningzhe</first><last>Gao</last><affiliation>ByteDance Inc.</affiliation></author>
      <pages>1840-1857</pages>
      <abstract>Improving prompt quality is crucial for enhancing the performance of large language models (LLMs), particularly for Black-Box models like GPT4. Existing prompt refinement methods, while effective, often suffer from semantic inconsistencies between refined and original prompts, and fail to maintain users’ real intent. To address these challenges, we propose a self-instructed in-context learning framework that generates reliable derived prompts, keeping semantic consistency with the original prompts. Specifically, our framework incorporates a reinforcement learning mechanism, enabling direct interaction with the response model during prompt generation to better align with human preferences. We then formulate the querying as an in-context learning task, combining responses from LLMs with derived prompts to create a contextual demonstration for the original prompt. This approach effectively enhances alignment, reduces semantic discrepancies, and activates the LLM’s in-context learning ability for generating more beneficial response. Extensive experiments demonstrate that the proposed method not only generates better derived prompts but also significantly enhances LLMs’ ability to deliver more effective responses, particularly for Black-Box models like GPT4.</abstract>
      <url hash="9d5f10ba">2025.acl-long.92</url>
      <bibkey>li-etal-2025-self</bibkey>
    </paper>
    <paper id="93">
      <title>Binary Classifier Optimization for Large Language Model Alignment</title>
      <author><first>Seungjae</first><last>Jung</last><affiliation>Kakao Corp.</affiliation></author>
      <author><first>Gunsoo</first><last>Han</last><affiliation>Kakao Brain</affiliation></author>
      <author><first>Daniel Wontae</first><last>Nam</last><affiliation>Kakao Brain Corp.</affiliation></author>
      <author><first>Kyoung-Woon</first><last>On</last><affiliation>LBOX</affiliation></author>
      <pages>1858-1872</pages>
      <abstract>In real-world services such as ChatGPT, aligning models based on user feedback is crucial for improving model performance. However, due to the simplicity and convenience of providing feedback, users typically offer only basic binary signals, such as ‘thumbs-up’ or ‘thumbs-down’. Most existing alignment research, on the other hand, relies on preference-based approaches that require both positive and negative responses as a pair. We propose Binary Classifier Optimization (BCO), a technique that effectively aligns LLMs using only binary feedback. BCO trains a binary classifier, where the logit serves as an implicit reward, effectively minimizing the Direct Preference Optimization (DPO) loss. We demonstrate that the binary cross-entropy loss employed in classifier training acts as an upper bound for the DPO loss. Additionally, a novel reward shift technique further minimizes the gap between the losses. We validate our methodology in two settings: first, on a paired preference dataset, where our method performs on par with DPO; and second, on a Likert-5 scale annotation dataset which stems from real users’ queries. Our model consistently demonstrates effective and robust alignment across four base LLMs and three different datasets, showcasing the strength of our approach to learning from binary signals.</abstract>
      <url hash="ea2584a5">2025.acl-long.93</url>
      <bibkey>jung-etal-2025-binary</bibkey>
    </paper>
    <paper id="94">
      <title><fixed-case>U</fixed-case>n<fixed-case>S</fixed-case>een<fixed-case>T</fixed-case>ime<fixed-case>QA</fixed-case>: Time-Sensitive Question-Answering Beyond <fixed-case>LLM</fixed-case>s’ Memorization</title>
      <author><first>Md Nayem</first><last>Uddin</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Amir</first><last>Saeidi</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Divij</first><last>Handa</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Agastya</first><last>Seth</last></author>
      <author><first>Tran Cao</first><last>Son</last><affiliation>New Mexico State University</affiliation></author>
      <author><first>Eduardo</first><last>Blanco</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Steven</first><last>Corman</last></author>
      <author><first>Chitta</first><last>Baral</last><affiliation>Arizona State University</affiliation></author>
      <pages>1873-1913</pages>
      <abstract>This paper introduces UnSeenTimeQA, a novel data contamination-free time-sensitive question-answering (TSQA) benchmark. It differs from existing TSQA benchmarks by avoiding web-searchable queries grounded in the real world. We present a series of time-sensitive event scenarios based on synthetically generated facts. It requires large language models (LLMs) to engage in genuine temporal reasoning without depending on the factual knowledge acquired during the pre-training phase. Our data generation framework enables on-demand generation of new samples, mitigating the risk of data leakage. We designed three types of time-sensitive questions to test LLMs’ temporal reasoning abilities over sequential and parallel event occurrences. Our evaluation of five LLMs on synthetic fact-based TSQA reveals mixed results: while they perform well on simpler subsets, their overall performance remains inferior as compared to real world fact-based TSQA. Error analysis indicates that LLMs face difficulties in reasoning over long-range event dependencies and parallel events.</abstract>
      <url hash="0a8c95ee">2025.acl-long.94</url>
      <bibkey>uddin-etal-2025-unseentimeqa</bibkey>
    </paper>
    <paper id="95">
      <title>From Information to Insight: Leveraging <fixed-case>LLM</fixed-case>s for Open Aspect-Based Educational Summarization</title>
      <author><first>Yang</first><last>Zhong</last><affiliation>University of Pittsburgh</affiliation></author>
      <author><first>Diane</first><last>Litman</last><affiliation>University of Pittsburgh</affiliation></author>
      <pages>1914-1947</pages>
      <abstract>This paper addresses the challenge of aspect-based summarization in education by introducing Reflective ASPect-based summarization (ReflectASP), a novel dataset that summarizes student reflections on STEM lectures. Despite the promising performance of large language models in general summarization, their application to nuanced aspect-based summaries remains under-explored. ReflectASP eases the exploration of open-aspect-based summarization (OABS), overcoming the limitations of current datasets and comes with ample human annotations. We benchmarked different types of zero-shot summarization methods and proposed two refinement methods to improve summaries, supported by both automatic and human manual evaluations. Additionally, we analyzed suggestions and revisions made during the refinement process, offering a fine-grained study of the editing strategies employed by these methods. We make our models, dataset, and all human evaluation results available at https://github.com/cs329yangzhong/ReflectASP.</abstract>
      <url hash="3644e66a">2025.acl-long.95</url>
      <bibkey>zhong-litman-2025-information</bibkey>
    </paper>
    <paper id="96">
      <title><fixed-case>A</fixed-case>fri<fixed-case>M</fixed-case>ed-<fixed-case>QA</fixed-case>: A Pan-<fixed-case>A</fixed-case>frican, Multi-Specialty, Medical Question-Answering Benchmark Dataset</title>
      <author><first>Charles</first><last>Nimo</last></author>
      <author><first>Tobi</first><last>Olatunji</last></author>
      <author><first>Abraham Toluwase</first><last>Owodunni</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <author><first>Tassallah</first><last>Abdullahi</last></author>
      <author><first>Emmanuel</first><last>Ayodele</last><affiliation>Intron Health</affiliation></author>
      <author><first>Mardhiyah</first><last>Sanni</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Ezinwanne C.</first><last>Aka</last><affiliation>Intron Health</affiliation></author>
      <author><first>Folafunmi</first><last>Omofoye</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Foutse</first><last>Yuehgoh</last></author>
      <author><first>Timothy</first><last>Faniran</last></author>
      <author><first>Bonaventure F. P.</first><last>Dossou</last></author>
      <author><first>Moshood O.</first><last>Yekini</last><affiliation>Masakhane</affiliation></author>
      <author><first>Jonas</first><last>Kemp</last><affiliation>Google</affiliation></author>
      <author><first>Katherine A</first><last>Heller</last><affiliation>Google</affiliation></author>
      <author><first>Jude Chidubem</first><last>Omeke</last><affiliation>Maximus</affiliation></author>
      <author><first>Chidi Asuzu</first><last>Md</last></author>
      <author><first>Naome A</first><last>Etori</last></author>
      <author><first>Aïmérou</first><last>Ndiaye</last></author>
      <author><first>Ifeoma</first><last>Okoh</last><affiliation>University of Ibadan</affiliation></author>
      <author><first>Evans Doe</first><last>Ocansey</last><affiliation>RISC Software GmbH</affiliation></author>
      <author><first>Wendy</first><last>Kinara</last></author>
      <author><first>Michael L.</first><last>Best</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Irfan</first><last>Essa</last><affiliation>Google DeepMind and Georgia Institute of Technology</affiliation></author>
      <author><first>Stephen Edward</first><last>Moore</last><affiliation>University of Cape Coast</affiliation></author>
      <author><first>Chris</first><last>Fourie</last></author>
      <author><first>Mercy Nyamewaa</first><last>Asiedu</last><affiliation>Research, Google</affiliation></author>
      <pages>1948-1973</pages>
      <abstract>Recent advancements in large language model (LLM) performance on medical multiplechoice question (MCQ) benchmarks have stimulated interest from healthcare providers and patients globally. Particularly in low-andmiddle-income countries (LMICs) facing acute physician shortages and lack of specialists, LLMs offer a potentially scalable pathway to enhance healthcare access and reduce costs. However, their effectiveness in the Global South, especially across the African continent, remains to be established. In this work, we introduce AfriMed-QA , the first largescale Pan-African English multi-specialty medical Question-Answering (QA) dataset, 15,000 questions (open and closed-ended) sourced from over 60 medical schools across 16 countries, covering 32 medical specialties. We further evaluate 30 LLMs across multiple axes including correctness and demographic bias. Our findings show significant performance variation across specialties and geographies, MCQ performance clearly lags USMLE (MedQA). We find that biomedical LLMs underperform general models and smaller edge-friendly LLMs struggle to achieve a passing score. Interestingly, human evaluations show a consistent consumer preference for LLM answers and explanations when compared with clinician answers.</abstract>
      <url hash="0d199eb3">2025.acl-long.96</url>
      <bibkey>nimo-etal-2025-afrimed</bibkey>
    </paper>
    <paper id="97">
      <title>Root Defense Strategies: Ensuring Safety of <fixed-case>LLM</fixed-case> at the Decoding Level</title>
      <author><first>Xinyi</first><last>Zeng</last></author>
      <author><first>Yuying</first><last>Shang</last></author>
      <author><first>Jiawei</first><last>Chen</last></author>
      <author><first>Jingyuan</first><last>Zhang</last><affiliation>Kuaishou- 快手科技</affiliation></author>
      <author><first>Yu</first><last>Tian</last></author>
      <pages>1974-1988</pages>
      <abstract>Large language models (LLMs) have demonstrated immense utility across various industries. However, as LLMs advance, the risk of harmful outputs increases due to incorrect or malicious prompts. While current methods effectively address jailbreak risks, they share common limitations: 1) Judging harmful outputs from the prefill-level lacks utilization of the model’s decoding outputs, leading to relatively lower effectiveness and robustness. 2) Rejecting potentially harmful outputs based on a single evaluation can significantly impair the model’s helpfulness. To address the above issues, we examine LLMs’ capability to recognize harmful outputs, revealing and quantifying their proficiency in assessing the danger of previous tokens. Motivated by pilot experiment results, we design a robust defense mechanism at the decoding level. Our novel decoder-oriented, step-by-step defense architecture corrects the outputs of harmful queries directly rather than rejecting them outright. We introduce speculative decoding to enhance usability and facilitate deployment to boost safe decoding speed. Extensive experiments demonstrate that our approach improves model security without compromising reasoning speed. Notably, our method leverages the model’s ability to discern hazardous information, maintaining its helpfulness compared to existing methods.</abstract>
      <url hash="6e8f30d6">2025.acl-long.97</url>
      <bibkey>zeng-etal-2025-root</bibkey>
    </paper>
    <paper id="98">
      <title>In-the-wild Audio Spatialization with Flexible Text-guided Localization</title>
      <author><first>Tianrui</first><last>Pan</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Jie</first><last>Liu</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Zewen</first><last>Huang</last></author>
      <author><first>Jie</first><last>Tang</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Gangshan</first><last>Wu</last><affiliation>Nanjing University</affiliation></author>
      <pages>1989-2001</pages>
      <abstract>Binaural audio enriches immersive experiences by enabling the perception of the spatial locations of sounding objects in AR, VR, and embodied AI applications. While existing audio spatialization methods can generally map any available monaural audio to binaural audio signals, they often lack the flexible and interactive control needed in complex multi-object user-interactive environments. To address this, we propose a Text-guided Audio Spatialization (TAS) framework that utilizes diverse text prompts and evaluates our model from unified generation and comprehension perspectives. Due to the limited availability of high-quality, large-scale stereo data, we construct the SpatialTAS dataset, which encompasses 376,000 simulated binaural audio samples to facilitate the training of our model. Our model learns binaural differences guided by 3D spatial location and relative position prompts, enhanced with flipped-channel audio. Experimental results show that our model can generate high quality binaural audios for various audio types on both simulated and real-recorded datasets. Besides, we establish an assessment model based on Llama-3.1-8B, which evaluates the semantic accuracy of spatial locations through a spatial reasoning task. Results demonstrate that by utilizing text prompts for flexible and interactive control, we can generate binaural audio with both high quality and semantic consistency in spatial locations.</abstract>
      <url hash="c89f30fd">2025.acl-long.98</url>
      <bibkey>pan-etal-2025-wild</bibkey>
    </paper>
    <paper id="99">
      <title><fixed-case>L</fixed-case>4<fixed-case>Q</fixed-case>: Parameter Efficient Quantization-Aware Fine-Tuning on Large Language Models</title>
      <author><first>Hyesung</first><last>Jeon</last></author>
      <author><first>Yulhwa</first><last>Kim</last><affiliation>Sungkyunkwan University</affiliation></author>
      <author><first>Jae-Joon</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <pages>2002-2024</pages>
      <abstract>Due to the high memory and computational costs associated with large language models (LLMs), model compression techniques such as quantization, which reduces inference costs, and parameter-efficient fine-tuning (PEFT) methods like Low-Rank Adaptation (LoRA), which reduce training costs, have gained significant popularity. This trend has spurred active research into quantization-aware PEFT techniques, aimed at maintaining model accuracy while minimizing memory overhead during both inference and training. Previous quantization-aware PEFT methods typically apply post-training quantiation (PTQ) to pre-trained LLMs, followed by PEFT to recover accuracy loss. Meanwhile, this approach has limitations in recovering the accuracy loss. In this paper, we propose L4Q, a method that integrates Quantization-Aware Training (QAT) with LoRA. By employing a memory-optimized layer design, L4Q significantly reduces QAT’s memory overhead, making its training cost comparable to LoRA, while preserving the advantage of QAT in producing fully quantized LLMs with high accuracy. Our experiments demonstrate that this combined approach to quantization and fine-tuning achieves superior accuracy compared to decoupled fine-tuning schemes, particularly in 4-bit and 3-bit quantization, positioning L4Q as an efficient QAT solution. Using the LLaMA and Mistral models with instructional datasets, we showcase L4Q’s capabilities in language tasks and few-shot learning.</abstract>
      <url hash="bbb619f0">2025.acl-long.99</url>
      <bibkey>jeon-etal-2025-l4q</bibkey>
    </paper>
    <paper id="100">
      <title>Second Language (<fixed-case>A</fixed-case>rabic) Acquisition of <fixed-case>LLM</fixed-case>s via Progressive Vocabulary Expansion</title>
      <author><first>Jianqing</first><last>Zhu</last></author>
      <author><first>Huang</first><last>Huang</last><affiliation>Shenzhen Research Institute of Big Data</affiliation></author>
      <author><first>Zhihang</first><last>Lin</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Juhao</first><last>Liang</last></author>
      <author><first>Zhengyang</first><last>Tang</last></author>
      <author><first>Khalid</first><last>Almubarak</last><affiliation>National Center for AI (NCAI), Saudi Data and AI Authority (SDAIA)</affiliation></author>
      <author><first>Mosen</first><last>Alharthi</last><affiliation>King Abdullah University of Science and Technology</affiliation></author>
      <author><first>Bang</first><last>An</last></author>
      <author><first>Juncai</first><last>He</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Xiangbo</first><last>Wu</last><affiliation>SRIBD</affiliation></author>
      <author><first>Fei</first><last>Yu</last></author>
      <author><first>Junying</first><last>Chen</last></author>
      <author><first>Ma</first><last>Zhuoheng</last></author>
      <author><first>Yuhao</first><last>Du</last></author>
      <author><first>He</first><last>Zhang</last><affiliation>CNPIEC KEXIN LTD</affiliation></author>
      <author><first>Saied</first><last>Alshahrani</last><affiliation>University of Bisha</affiliation></author>
      <author><first>Emad A.</first><last>Alghamdi</last><affiliation>King Abdulaziz University</affiliation></author>
      <author><first>Lian</first><last>Zhang</last><affiliation>Shenzhen Research Institute of Big Data</affiliation></author>
      <author><first>Ruoyu</first><last>Sun</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Haizhou</first><last>Li</last><affiliation>The Chinese University of Hong Kong (Shenzhen); National University of Singapore and National University of Singapore</affiliation></author>
      <author><first>Benyou</first><last>Wang</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <author><first>Jinchao</first><last>Xu</last><affiliation>King Abdullah University of Science and Technology and Pennsylvania State University</affiliation></author>
      <pages>2025-2042</pages>
      <abstract>This paper addresses the critical need for democratizing large language models (LLM) in the Arab world, a region that has seen slower progress in developing models comparable to state-of-the-art offerings like GPT-4 or GPT-3.5, due to a predominant focus on mainstream languages (e.g., English and Chinese). One practical objective for Arabic LLMs is to utilize Arabic-specific vocabulary in the tokenizer to accelerate decoding. However, using a different vocabulary often leads to degradation of the model’s learned knowledge, since many words become out-of-vocabulary (OOV) at the beginning of training. Inspired by the vocabulary learning during Second Language (Arabic) Acquisition for humans, the released AraLLaMA employs progressive vocabulary expansion, which is implemented by a modified BPE algorithm that progressively extends the Arabic subwords in its dynamic vocabulary during training, thereby balancing the OOV ratio at every stage. The ablation study demonstrated the effectiveness of Progressive Vocabulary Expansion.Moreover, AraLLaMA achieves decent performance comparable to the best Arabic LLMs across a variety of Arabic benchmarks. Our model weights are available at: <url>https://github.com/FreedomIntelligence/AraLLaMa</url>.</abstract>
      <url hash="4cd95071">2025.acl-long.100</url>
      <bibkey>zhu-etal-2025-second</bibkey>
    </paper>
    <paper id="101">
      <title>What Really Matters in Many-Shot Attacks? An Empirical Study of Long-Context Vulnerabilities in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Sangyeop</first><last>Kim</last><affiliation>Coxwave and Seoul National University</affiliation></author>
      <author><first>Yohan</first><last>Lee</last><affiliation>KakaoBank</affiliation></author>
      <author><first>Yongwoo</first><last>Song</last></author>
      <author><first>Kimin</first><last>Lee</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <pages>2043-2063</pages>
      <abstract>We investigate long-context vulnerabilities in Large Language Models (LLMs) through Many-Shot Jailbreaking (MSJ). Our experiments utilize context length of up to 128K tokens. Through comprehensive analysis with various many-shot attack settings with different instruction styles, shot density, topic, and format, we reveal that context length is the primary factor determining attack effectiveness. Critically, we find that successful attacks do not require carefully crafted harmful content. Even repetitive shots or random dummy text can circumvent model safety measures, suggesting fundamental limitations in long-context processing capabilities of LLMs. The safety behavior of well-aligned models becomes increasingly inconsistent with longer contexts. These findings highlight significant safety gaps in context expansion capabilities of LLMs, emphasizing the need for new safety mechanisms.</abstract>
      <url hash="5ae226c9">2025.acl-long.101</url>
      <bibkey>kim-etal-2025-really</bibkey>
    </paper>
    <paper id="102">
      <title><fixed-case>ECERC</fixed-case>: Evidence-Cause Attention Network for Multi-Modal Emotion Recognition in Conversation</title>
      <author><first>Tao</first><last>Zhang</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Zhenhua</first><last>Tan</last><affiliation>Northeastern University</affiliation></author>
      <pages>2064-2077</pages>
      <abstract>Multi-modal Emotion Recognition in Conversation (MMERC) aims to identify speakers’ emotional states using multi-modal conversational data, significant for various domains. MMERC requires addressing emotional causes: contextual factors that influence emotions, alongside emotional evidence directly expressed in the target utterance. Existing methods primarily model general conversational dependencies, such as sequential utterance relationships or inter-speaker dynamics, but fall short in capturing diverse and detailed emotional causes, including emotional contagion, influences from others, and self-referenced or externally introduced events. To address these limitations, we propose the Evidence-Cause Attention Network for Multi-Modal Emotion Recognition in Conversation (ECERC). ECERC integrates emotional evidence with contextual causes through five stages: Evidence Gating extracts and refines emotional evidence across modalities; Cause Encoding captures causes from conversational context; Evidence-Cause Interaction uses attention to integrate evidence with diverse causes, generating rich candidate features for emotion inference; Feature Gating adaptively weights contributions of candidate features; and Emotion Classification classifies emotions. We evaluate ECERC on two widely used benchmark datasets, IEMOCAP and MELD. Experimental results show that ECERC achieves competitive performance in weighted F1-score and accuracy, demonstrating its effectiveness in MMERC</abstract>
      <url hash="4e7fb8e6">2025.acl-long.102</url>
      <bibkey>zhang-tan-2025-ecerc</bibkey>
    </paper>
    <paper id="103">
      <title><fixed-case>C</fixed-case>ompile<fixed-case>A</fixed-case>gent: Automated Real-World Repo-Level Compilation with Tool-Integrated <fixed-case>LLM</fixed-case>-based Agent System</title>
      <author><first>Li</first><last>Hu</last></author>
      <author><first>Guoqiang</first><last>Chen</last><affiliation>Qi’anxin Technology Group Co., Ltd</affiliation></author>
      <author><first>Xiuwei</first><last>Shang</last></author>
      <author><first>Shaoyin</first><last>Cheng</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Benlong</first><last>Wu</last></author>
      <author><first>LiGangyang</first><last>LiGangyang</last></author>
      <author><first>Xu</first><last>Zhu</last></author>
      <author><first>Weiming</first><last>Zhang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Nenghai</first><last>Yu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>2078-2091</pages>
      <abstract>With open-source projects growing in size and complexity, manual compilation becomes tedious and error-prone, highlighting the need for automation to improve efficiency and accuracy. However, the complexity of compilation instruction search and error resolution makes automatic compilation challenging. Inspired by the success of LLM-based agents in various fields, we propose CompileAgent, the first LLM-based agent framework dedicated to repo-level compilation. CompileAgent integrates five tools and a flow-based agent strategy, enabling interaction with software artifacts for compilation instruction search and error resolution. To measure the effectiveness of our method, we design a public repo-level benchmark CompileAgentBench, and we also design two baselines for comparison by combining two compilation-friendly schemes. The performance on this benchmark shows that our method significantly improves the compilation success rate, ranging from 10% to 71%. Meanwhile, we evaluate the performance of CompileAgent under different agent strategies and verify the effectiveness of the flow-based strategy. Additionally, we emphasize the scalability of CompileAgent, further expanding its application prospects. The complete code and data are available at https://github.com/Ch3nYe/AutoCompiler.</abstract>
      <url hash="cade517a">2025.acl-long.103</url>
      <bibkey>hu-etal-2025-compileagent</bibkey>
    </paper>
    <paper id="104">
      <title>Beyond Demographics: Fine-tuning Large Language Models to Predict Individuals’ Subjective Text Perceptions</title>
      <author><first>Matthias</first><last>Orlikowski</last><affiliation>Universität Bielefeld</affiliation></author>
      <author><first>Jiaxin</first><last>Pei</last><affiliation>Stanford University</affiliation></author>
      <author><first>Paul</first><last>Röttger</last><affiliation>Bocconi University</affiliation></author>
      <author><first>Philipp</first><last>Cimiano</last></author>
      <author><first>David</first><last>Jurgens</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>Dirk</first><last>Hovy</last><affiliation>Bocconi University</affiliation></author>
      <pages>2092-2111</pages>
      <abstract>People naturally vary in their annotations for subjective questions and some of this variation is thought to be due to the person’s sociodemographic characteristics. LLMs have also been used to label data, but recent work has shown that models perform poorly when prompted with sociodemographic attributes, suggesting limited inherent sociodemographic knowledge. Here, we ask whether LLMs can be trained to be accurate sociodemographic models of annotator variation. Using a curated dataset of five tasks with standardized sociodemographics, we show that models do improve in sociodemographic prompting when trained but that this performance gain is largely due to models learning annotator-specific behaviour rather than sociodemographic behaviours. Across all tasks, our results suggest that models learn little meaningful connection between sociodemographics and annotation, raising doubts about the current use of LLMs for simulating sociodemographic variation and behaviour.</abstract>
      <url hash="8aa58a74">2025.acl-long.104</url>
      <bibkey>orlikowski-etal-2025-beyond</bibkey>
    </paper>
    <paper id="105">
      <title>Exploring Forgetting in Large Language Model Pre-Training</title>
      <author><first>Chonghua</first><last>Liao</last></author>
      <author><first>Ruobing</first><last>Xie</last></author>
      <author><first>Xingwu</first><last>Sun</last><affiliation>Tencent Hunyuan</affiliation></author>
      <author><first>Haowen</first><last>Sun</last><affiliation>Tsinghua University, Tsinghua University and Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Zhanhui</first><last>Kang</last></author>
      <pages>2112-2127</pages>
      <abstract>Catastrophic forgetting remains a formidable obstacle to building an omniscient model in large language models (LLMs). Despite the pioneering research on task-level forgetting in LLM fine-tuning, there is scant focus on forgetting during pre-training. We systematically explored the existence and measurement of forgetting in pre-training, questioning traditional metrics such as perplexity (PPL) and introducing new metrics to better detect entity memory retention. Based on our revised assessment of forgetting metrics, we explored low-cost, straightforward methods to mitigate forgetting during the pre-training phase. In addition, we carefully analyzed the learning curves, offering insights into the dynamics of forgetting. Extensive evaluations and analyses on forgetting of pre-training could facilitate future research on LLMs.</abstract>
      <url hash="d3fa6438">2025.acl-long.105</url>
      <bibkey>liao-etal-2025-exploring</bibkey>
    </paper>
    <paper id="106">
      <title>Bias in the Mirror : Are <fixed-case>LLM</fixed-case>s opinions robust to their own adversarial attacks</title>
      <author><first>Virgile</first><last>Rennard</last></author>
      <author><first>Christos</first><last>Xypolopoulos</last><affiliation>National Technical University of Athens and École Polytechnique</affiliation></author>
      <author><first>Michalis</first><last>Vazirgiannis</last><affiliation>Ecole Polytechnique, France</affiliation></author>
      <pages>2128-2143</pages>
      <abstract>Large language models (LLMs) inherit biases from their training data and alignment processes, influencing their responses in subtle ways. While many studies have examined these biases, little work has explored their robustness during interactions. In this paper, we introduce a novel approach where two instances of an LLM engage in self-debate, arguing opposing viewpoints to persuade a neutral version of the model. Through this, we evaluate how firmly biases hold and whether models are susceptible to reinforcing misinformation or shifting to harmful viewpoints. Our experiments span multiple LLMs of varying sizes, origins, and languages, providing deeper insights into bias persistence and flexibility across linguistic and cultural contexts.</abstract>
      <url hash="5c4a2d72">2025.acl-long.106</url>
      <bibkey>rennard-etal-2025-bias</bibkey>
    </paper>
    <paper id="107">
      <title><fixed-case>A</fixed-case>ndroid<fixed-case>L</fixed-case>ab: Training and Systematic Benchmarking of Android Autonomous Agents</title>
      <author><first>Yifan</first><last>Xu</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Xiao</first><last>Liu</last></author>
      <author><first>Xueqiao</first><last>Sun</last></author>
      <author><first>Siyi</first><last>Cheng</last></author>
      <author><first>Hao</first><last>Yu</last></author>
      <author><first>Hanyu</first><last>Lai</last></author>
      <author><first>Shudan</first><last>Zhang</last></author>
      <author id="dan-zhang"><first>Dan</first><last>Zhang</last></author>
      <author><first>Jie</first><last>Tang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Yuxiao</first><last>Dong</last><affiliation>Tsinghua University</affiliation></author>
      <pages>2144-2166</pages>
      <abstract>Autonomous agents have become increasingly important for interacting with the real world. Android agents, in particular, have been a frequently-mentioned interaction method. However, existing studies for training and evaluating Android agents lack systematic research on both open-source and closed-source models. In this work, we propose AndroidLab as a systematic Android agent framework. It includes an operation environment with different modalities, action space, and a reproducible benchmark. It supports both large language models (LLMs) and multimodal models (LMMs) in the same action space. AndroidLab benchmark includes predefined Android virtual devices and 138 tasks across nine apps built on these devices. By using the AndroidLab environment, we develop an Android Instruction dataset and train six open-source LLMs and LMMs, lifting the average success rates from 4.59% to 21.50% for LLMs and from 1.93% to 13.28% for LMMs. AndroidLab is open-sourced and publicly available at https://github.com/THUDM/Android-Lab.</abstract>
      <url hash="05984f70">2025.acl-long.107</url>
      <bibkey>xu-etal-2025-androidlab</bibkey>
    </paper>
    <paper id="108">
      <title>Modular Sentence Encoders: Separating Language Specialization from Cross-Lingual Alignment</title>
      <author><first>Yongxin</first><last>Huang</last><affiliation>Technische Universität Darmstadt</affiliation></author>
      <author><first>Kexin</first><last>Wang</last><affiliation>Ubiquitous Knowledge Processing Lab, Technical University of Darmstadt</affiliation></author>
      <author><first>Goran</first><last>Glavaš</last><affiliation>Julius-Maximilians-Universität Würzburg</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Institute for Computer Science, Artificial Intelligence and Technology, Mohamed bin Zayed University of Artificial Intelligence and Technische Universität Darmstadt</affiliation></author>
      <pages>2167-2187</pages>
      <abstract>Multilingual sentence encoders (MSEs) are commonly obtained by training multilingual language models to map sentences from different languages into a shared semantic space. As such, they are subject to curse of multilinguality, a loss of monolingual representational accuracy due to parameter sharing. Another limitation of MSEs is the trade-off between different task performance: cross-lingual alignment training distorts the optimal monolingual structure of semantic spaces of individual languages, harming the utility of sentence embeddings in monolingual tasks; cross-lingual tasks, such as cross-lingual semantic similarity and zero-shot transfer for sentence classification, may also require conflicting cross-lingual alignment strategies. In this work, we address both issues by means of modular training of sentence encoders. We first train language-specific monolingual modules to mitigate negative interference between languages (i.e., the curse). We then align all non-English sentence embeddings to the English by training cross-lingual alignment adapters, preventing interference with monolingual specialization from the first step. We train the cross-lingual adapters with two different types of data to resolve the conflicting requirements of different cross-lingual tasks. Monolingual and cross-lingual results on semantic text similarity and relatedness, bitext mining and sentence classification show that our modular solution achieves better and more balanced performance across all the tasks compared to full-parameter training of monolithic multilingual sentence encoders, especially benefiting low-resource languages.</abstract>
      <url hash="acb770ab">2025.acl-long.108</url>
      <bibkey>huang-etal-2025-modular</bibkey>
    </paper>
    <paper id="109">
      <title>Multimodal Transformers are Hierarchical Modal-wise Heterogeneous Graphs</title>
      <author><first>Yijie</first><last>Jin</last><affiliation>Shanghai University</affiliation></author>
      <author><first>Junjie</first><last>Peng</last><affiliation>Shanghai University</affiliation></author>
      <author><first>Xuanchao</first><last>Lin</last></author>
      <author><first>Haochen</first><last>Yuan</last></author>
      <author><first>Lan</first><last>Wang</last><affiliation>Shanghai University</affiliation></author>
      <author><first>Cangzhi</first><last>Zheng</last></author>
      <pages>2188-2209</pages>
      <abstract>Multimodal Sentiment Analysis (MSA) is a rapidly developing field that integrates multimodal information to recognize sentiments, and existing models have made significant progress in this area. The central challenge in MSA is multimodal fusion, which is predominantly addressed by Multimodal Transformers (MulTs). Although act as the paradigm, MulTs suffer from efficiency concerns. In this work, from the perspective of efficiency optimization, we propose and prove that MulTs are hierarchical modal-wise heterogeneous graphs (HMHGs), and we introduce the graph-structured representation pattern of MulTs. Based on this pattern, we propose an Interlaced Mask (IM) mechanism to design the Graph-Structured and Interlaced-Masked Multimodal Transformer (GsiT). It is formally equivalent to MulTs which achieves an efficient weight-sharing mechanism without information disorder through IM, enabling All-Modal-In-One fusion with only 1/3 of the parameters of pure MulTs. A kernel called Decomposition is implemented to ensure avoiding additional computational overhead. Moreover, it achieves significantly higher performance than traditional MulTs. To further validate the effectiveness of GsiT itself and the HMHG concept, we integrate them into multiple state-of-the-art models and demonstrate notable performance improvements and parameter reduction on widely used MSA datasets. Experimental results also demonstrate its effectiveness on other multimodal tasks. The code is available in https://github.com/drewjin/GsiT.git.</abstract>
      <url hash="eaff7769">2025.acl-long.109</url>
      <bibkey>jin-etal-2025-multimodal</bibkey>
    </paper>
    <paper id="110">
      <title>Have We Designed Generalizable Structural Knowledge Promptings? Systematic Evaluation and Rethinking</title>
      <author><first>Yichi</first><last>Zhang</last></author>
      <author><first>Zhuo</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Lingbing</first><last>Guo</last><affiliation>Tianjin University</affiliation></author>
      <author><first>Yajing</first><last>Xu</last><affiliation>College of Computer Science and Technology, Zhejiang University</affiliation></author>
      <author><first>Shaokai</first><last>Chen</last></author>
      <author><first>Mengshu</first><last>Sun</last></author>
      <author><first>Binbin</first><last>Hu</last><affiliation>Ant Group</affiliation></author>
      <author><first>Zhiqiang</first><last>Zhang</last><affiliation>Ant Group</affiliation></author>
      <author><first>Lei</first><last>Liang</last></author>
      <author><first>Wen</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Huajun</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <pages>2210-2226</pages>
      <abstract>Large language models (LLMs) have demonstrated exceptional performance in text generation within current NLP research. However, the lack of factual accuracy is still a dark cloud hanging over the LLM skyscraper. Structural knowledge prompting (SKP) is a prominent paradigm to integrate external knowledge into LLMs by incorporating structural representations, achieving state-of-the-art results in many knowledge-intensive tasks. However, existing methods often focus on specific problems, lacking a comprehensive exploration of the generalization and capability boundaries of SKP. This paper aims to evaluate and rethink the generalization capability of the SKP paradigm from four perspectives including Granularity, Transferability, Scalability, and Universality. To provide a thorough evaluation, we introduce a novel multi-granular, multi-level benchmark called SUBARU, consisting of 9 different tasks with varying levels of granularity and difficulty. Through extensive experiments, we draw key conclusions regarding the generalization of SKP, offering insights to guide the future development and extension of the SKP paradigm.</abstract>
      <url hash="e3de4b23">2025.acl-long.110</url>
      <bibkey>zhang-etal-2025-designed</bibkey>
    </paper>
    <paper id="111">
      <title><fixed-case>LL</fixed-case>ä<fixed-case>M</fixed-case>mlein: Transparent, Compact and Competitive <fixed-case>G</fixed-case>erman-Only Language Models from Scratch</title>
      <author><first>Jan</first><last>Pfister</last><affiliation>Julius-Maximilians-Universität Würzburg</affiliation></author>
      <author><first>Julia</first><last>Wunderle</last><affiliation>Bayerische Julius-Maximilians-Universität Würzburg</affiliation></author>
      <author><first>Andreas</first><last>Hotho</last><affiliation>Bayerische Julius-Maximilians-Universität Würzburg</affiliation></author>
      <pages>2227-2246</pages>
      <abstract>We transparently create two German-only decoder models, LLäMmlein 120M and 1B, from scratch and publish them, along with the training data, for the (German) NLP research community to use. The model training involved several key steps, including data preprocessing/filtering, the creation of a German tokenizer, the training itself, as well as the evaluation of the final models on various benchmarks, also against existing models. Throughout the training process, multiple checkpoints were saved in equal intervals and analyzed using the German SuperGLEBer benchmark to gain insights into the models’ learning process.Compared to state-of-the-art models on the SuperGLEBer benchmark, both LLäMmlein models performed competitively, consistently matching or surpassing models with similar parameter sizes. The results show that the models’ quality scales with size as expected, but performance improvements on some tasks plateaued early during training, offering valuable insights into resource allocation for future models.</abstract>
      <url hash="6a41e622">2025.acl-long.111</url>
      <bibkey>pfister-etal-2025-llammlein</bibkey>
    </paper>
    <paper id="112">
      <title>Speaking Beyond Language: A Large-Scale Multimodal Dataset for Learning Nonverbal Cues from Video-Grounded Dialogues</title>
      <author><first>Youngmin</first><last>Kim</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Jiwan</first><last>Chung</last></author>
      <author><first>Jisoo</first><last>Kim</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Sunghyun</first><last>Lee</last><affiliation>Yonsei University and Yonsei University</affiliation></author>
      <author><first>Sangkyu</first><last>Lee</last></author>
      <author><first>Junhyeok</first><last>Kim</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Cheoljong</first><last>Yang</last></author>
      <author><first>Youngjae</first><last>Yu</last><affiliation>Yonsei University</affiliation></author>
      <pages>2247-2265</pages>
      <abstract>Nonverbal communication is integral to human interaction, with gestures, facial expressions, and body language conveying critical aspects of intent and emotion. However, existing large language models (LLMs) fail to effectively incorporate these nonverbal elements, limiting their capacity to create fully immersive conversational experiences. We introduce MARS, a multimodal language model designed to understand and generate nonverbal cues alongside text, bridging this gap in conversational AI.Our key innovation is VENUS, a large-scale dataset comprising annotated videos with time-aligned text, facial expressions, and body language.Leveraging VENUS, we train MARS with a next-token prediction objective, combining text with vector-quantized nonverbal representations to achieve multimodal understanding and generation within a unified framework.Based on various analyses of the VENUS datasets, we validate its substantial scale and high effectiveness. Our quantitative and qualitative results demonstrate that MARS successfully generates text and nonverbal languages, corresponding to conversational input.Our dataset and code are available at https://github.com/winston1214/nonverbal-conversation.</abstract>
      <url hash="d36bca02">2025.acl-long.112</url>
      <bibkey>kim-etal-2025-speaking</bibkey>
    </paper>
    <paper id="113">
      <title>How Much Do Encoder Models Know About Word Senses?</title>
      <author><first>Simone</first><last>Teglia</last><affiliation>University of Roma “La Sapienza”</affiliation></author>
      <author><first>Simone</first><last>Tedeschi</last></author>
      <author><first>Roberto</first><last>Navigli</last><affiliation>Sapienza University of Rome</affiliation></author>
      <pages>2266-2277</pages>
      <abstract>Word Sense Disambiguation (WSD) is a key task in Natural Language Processing (NLP), involving selecting the correct meaning of a word based on its context. With Pretrained Language Models (PLMs) like BERT and DeBERTa now well established, significant progress has been made in understanding contextual semantics. Nevertheless, how well these models inherently disambiguate word senses remains uncertain. In this work, we evaluate several encoder-only PLMs across two popular inventories (i.e. WordNet and the Oxford Dictionary of English) by analyzing their ability to separate word senses without any task-specific fine-tuning. We compute centroids of word senses and measure similarity to assess performance across different layers. Our results show that DeBERTa-v3 delivers the best performance on the task, with the middle layers (specifically the 7th and 8th layers) achieving the highest accuracy, outperforming the output layer by approximately 15 percentage points. Our experiments also explore the inherent structure of WordNet and ODE sense inventories, highlighting their influence on the overall model behavior and performance. Finally, based on our findings, we develop a small, efficient model for the WSD task that attains robust performance while significantly reducing the carbon footprint.</abstract>
      <url hash="bae044f1">2025.acl-long.113</url>
      <bibkey>teglia-etal-2025-much</bibkey>
    </paper>
    <paper id="114">
      <title>When Backdoors Speak: Understanding <fixed-case>LLM</fixed-case> Backdoor Attacks Through Model-Generated Explanations</title>
      <author><first>Huaizhi</first><last>Ge</last></author>
      <author><first>Yiming</first><last>Li</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Qifan</first><last>Wang</last><affiliation>Meta AI</affiliation></author>
      <author><first>Yongfeng</first><last>Zhang</last><affiliation>Rutgers University</affiliation></author>
      <author><first>Ruixiang</first><last>Tang</last><affiliation>Rutgers University</affiliation></author>
      <pages>2278-2296</pages>
      <abstract>Large Language Models (LLMs) are known to be vulnerable to backdoor attacks, where triggers embedded in poisoned samples can maliciously alter LLMs’ behaviors. In this paper, we move beyond attacking LLMs and instead examine backdoor attacks through the novel lens of natural language explanations. Specifically, we leverage LLMs’ generative capabilities to produce human-readable explanations for their decisions, enabling direct comparisons between explanations for clean and poisoned samples. Our results show that backdoored models produce coherent explanations for clean inputs but diverse and logically flawed explanations for poisoned data, a pattern consistent across classification and generation tasks for different backdoor attacks. Further analysis reveals key insights into the explanation generation process. At the token level, explanation tokens associated with poisoned samples only appear in the final few transformer layers. At the sentence level, attention dynamics indicate that poisoned inputs shift attention away from the original input context during explanation generation. These findings enhance our understanding of backdoor mechanisms in LLMs and present a promising framework for detecting vulnerabilities through explainability.</abstract>
      <url hash="4f3767bd">2025.acl-long.114</url>
      <bibkey>ge-etal-2025-backdoors</bibkey>
    </paper>
    <paper id="115">
      <title><fixed-case>H</fixed-case>ate<fixed-case>D</fixed-case>ay: Insights from a Global Hate Speech Dataset Representative of a Day on <fixed-case>T</fixed-case>witter</title>
      <author><first>Manuel</first><last>Tonneau</last><affiliation>Oxford Internet Institute, University of Oxford</affiliation></author>
      <author><first>Diyi</first><last>Liu</last></author>
      <author><first>Niyati</first><last>Malhotra</last></author>
      <author><first>Scott A.</first><last>Hale</last><affiliation>Meedan, University of Oxford and Alan Turing Institute</affiliation></author>
      <author><first>Samuel</first><last>Fraiberger</last><affiliation>World Bank</affiliation></author>
      <author><first>Victor</first><last>Orozco-Olvera</last></author>
      <author><first>Paul</first><last>Röttger</last><affiliation>Bocconi University</affiliation></author>
      <pages>2297-2321</pages>
      <abstract>To address the global challenge of online hate speech, prior research has developed detection models to flag such content on social media. However, due to systematic biases in evaluation datasets, the real-world effectiveness of these models remains unclear, particularly across geographies. We introduce HateDay, the first global hate speech dataset representative of social media settings, constructed from a random sample of all tweets posted on September 21, 2022 and covering eight languages and four English-speaking countries. Using HateDay, we uncover substantial variation in the prevalence and composition of hate speech across languages and regions. We show that evaluations on academic datasets greatly overestimate real-world detection performance, which we find is very low, especially for non-European languages. Our analysis identifies key drivers of this gap, including models’ difficulty to distinguish hate from offensive speech and a mismatch between the target groups emphasized in academic datasets and those most frequently targeted in real-world settings. We argue that poor model performance makes public models ill-suited for automatic hate speech moderation and find that high moderation rates are only achievable with substantial human oversight. Our results underscore the need to evaluate detection systems on data that reflects the complexity and diversity of real-world social media.</abstract>
      <url hash="6edaa619">2025.acl-long.115</url>
      <bibkey>tonneau-etal-2025-hateday</bibkey>
    </paper>
    <paper id="116">
      <title><fixed-case>L</fixed-case>egal<fixed-case>A</fixed-case>gent<fixed-case>B</fixed-case>ench: Evaluating <fixed-case>LLM</fixed-case> Agents in Legal Domain</title>
      <author><first>Haitao</first><last>Li</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Junjie</first><last>Chen</last></author>
      <author><first>Jingli</first><last>Yang</last><affiliation>Beijing Knowledge Atlas Technology Co., Ltd.</affiliation></author>
      <author><first>Qingyao</first><last>Ai</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Wei</first><last>Jia</last></author>
      <author><first>Youfeng</first><last>Liu</last></author>
      <author><first>Kai</first><last>Lin</last><affiliation>Shanghai Amarsoft Enterprise Credit Information Service Co.,Ltd</affiliation></author>
      <author><first>Yueyue</first><last>Wu</last><affiliation>, Tsinghua University</affiliation></author>
      <author><first>Guozhi</first><last>Yuan</last></author>
      <author><first>Yiran</first><last>Hu</last></author>
      <author><first>Wuyue</first><last>Wang</last><affiliation>Haidian Urban Renewal Group</affiliation></author>
      <author><first>Yiqun</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <pages>2322-2344</pages>
      <abstract>With the increasing intelligence and autonomy of LLM Agents, their potential applications in the legal domain are becoming increasingly apparent. However, existing general-domain benchmarks are unable to fully capture the complexity and subtle nuances inherent in real-world judicial cognition and decision-making. Therefore, we propose LegalAgentBench, a comprehensive benchmark specifically designed to evaluate LLM Agents in the Chinese legal domain. LegalAgentBench includes 17 corpora from real-world legal scenarios and provides 37 tools for interacting with external knowledge. To cover tasks of varying difficulty and types, we designed a scalable task construction process that enables a more precise evaluation of performance in both tool utilization and reasoning. Moreover, Beyond assessing performance through the success rate of final outcomes, LegalAgentBench incorporates keyword analysis during intermediate processes to calculate progress rates, facilitating a more fine-grained evaluation. We evaluated eight popular LLMs, highlighting the strengths, limitations, and potential areas for improvement of existing models and methods. LegalAgentBench sets a new benchmark for the practical application of LLMs in the legal domain, with its code and data available at https://github.com/CSHaitao/LegalAgentBench.</abstract>
      <url hash="c8854871">2025.acl-long.116</url>
      <bibkey>li-etal-2025-legalagentbench</bibkey>
    </paper>
    <paper id="117">
      <title>Inference Compute-Optimal Video Vision Language Models</title>
      <author><first>Peiqi</first><last>Wang</last></author>
      <author><first>ShengYun</first><last>Peng</last></author>
      <author><first>Xuewen</first><last>Zhang</last></author>
      <author><first>Hanchao</first><last>Yu</last><affiliation>Facebook</affiliation></author>
      <author><first>Yibo</first><last>Yang</last></author>
      <author><first>Lifu</first><last>Huang</last><affiliation>University of California, Davis</affiliation></author>
      <author><first>Fujun</first><last>Liu</last></author>
      <author><first>Qifan</first><last>Wang</last><affiliation>Meta AI</affiliation></author>
      <pages>2345-2374</pages>
      <abstract>This work investigates the optimal allocation of inference compute across three key scaling factors in video vision language models: language model size, frame count, and the number of visual tokens per frame. While prior works typically focuses on optimizing model efficiency or improving performance without considering resource constraints, we instead identify optimal model configuration under fixed inference compute budgets. We conduct large-scale training sweeps and careful parametric modeling of task performance to identify the inference compute-optimal frontier. Our experiments reveal how task performance depends on scaling factors and finetuning data size, as well as how changes in data size shift the compute-optimal frontier. These findings translate to practical tips for selecting these scaling factors.</abstract>
      <url hash="04209697">2025.acl-long.117</url>
      <bibkey>wang-etal-2025-inference</bibkey>
    </paper>
    <paper id="118">
      <title>Steering into New Embedding Spaces: Analyzing Cross-Lingual Alignment Induced by Model Interventions in Multilingual Language Models</title>
      <author><first>Anirudh</first><last>Sundar</last></author>
      <author><first>Sinead</first><last>Williamson</last><affiliation>Apple</affiliation></author>
      <author><first>Katherine</first><last>Metcalf</last><affiliation>Apple</affiliation></author>
      <author><first>Barry-John</first><last>Theobald</last><affiliation>Apple</affiliation></author>
      <author><first>Skyler</first><last>Seto</last><affiliation>Apple</affiliation></author>
      <author><first>Masha</first><last>Fedzechkina</last><affiliation>Apple</affiliation></author>
      <pages>2375-2401</pages>
      <abstract>Aligned representations across languages is a desired property in multilingual large language models (mLLMs), as alignment can improve performance in cross-lingual tasks. Typically alignment requires fine-tuning a model, which is computationally expensive, and sizable language data, which often may not be available. A data-efficient alternative to fine-tuning is model interventions — a method for manipulating model activations to steer generation into the desired direction. We analyze the effect of a popular intervention (finding experts) on the alignment of cross-lingual representations in mLLMs. We identify the neurons to manipulate for a given language and introspect the embedding space of mLLMs pre- and post-manipulation. We show that modifying the mLLM’s activations changes its embedding space such that cross-lingual alignment is enhanced. Further, we show that the changes to the embedding space translate into improved downstream performance on retrieval tasks, with up to 2x improvements in top-1 accuracy on cross-lingual retrieval.</abstract>
      <url hash="9a5b7222">2025.acl-long.118</url>
      <bibkey>sundar-etal-2025-steering</bibkey>
    </paper>
    <paper id="119">
      <title>Digital Gatekeepers: <fixed-case>G</fixed-case>oogle’s Role in Curating Hashtags and Subreddits</title>
      <author><first>Amrit</first><last>Poudel</last></author>
      <author><first>Yifan</first><last>Ding</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Tim</first><last>Weninger</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Jürgen</first><last>Pfeffer</last><affiliation>Technische Universität München</affiliation></author>
      <pages>2402-2415</pages>
      <abstract>Search engines play a crucial role as digital gatekeepers, shaping the visibility of Web and social media content through algorithmic curation. This study investigates how search engines like Google selectively promotes or suppresses certain hashtags and subreddits, impacting the information users encounter. By comparing search engine results with nonsampled data from Reddit and Twitter/X, we reveal systematic biases in content visibility. Google’s algorithms tend to suppress subreddits and hashtags related to sexually explicit material, conspiracy theories, advertisements, and cryptocurrencies, while promoting content associated with higher engagement. These findings suggest that Google’s gatekeeping practices influence public discourse by curating the social media narratives available to users.</abstract>
      <url hash="f7e14b46">2025.acl-long.119</url>
      <bibkey>poudel-etal-2025-digital</bibkey>
    </paper>
    <paper id="120">
      <title>Behind Closed Words: Creating and Investigating the fore<fixed-case>PL</fixed-case>ay Annotated Dataset for <fixed-case>P</fixed-case>olish Erotic Discourse</title>
      <author><first>Anna</first><last>Kołos</last></author>
      <author><first>Katarzyna</first><last>Lorenc</last><affiliation>NASK - National Research Institute</affiliation></author>
      <author><first>Emilia</first><last>Wiśnios</last></author>
      <author><first>Agnieszka</first><last>Karlińska</last><affiliation>NASK - National Research Institute</affiliation></author>
      <pages>2416-2432</pages>
      <abstract>The surge in online content has created an urgent demand for robust detection systems, especially in non-English contexts where current tools demonstrate significant limitations. We introduce forePLay, a novel Polish-language dataset for erotic content detection, comprising over 24,000 annotated sentences. The dataset features a multidimensional taxonomy that captures ambiguity, violence, and socially unacceptable behaviors. Our comprehensive evaluation demonstrates that specialized Polish language models achieve superior performance compared to multilingual alternatives, with transformer-based architectures showing particular strength in handling imbalanced categories. The dataset and accompanying analysis establish essential frameworks for developing linguistically-aware content moderation systems, while highlighting critical considerations for extending such capabilities to morphologically complex languages.</abstract>
      <url hash="e526a37c">2025.acl-long.120</url>
      <bibkey>kolos-etal-2025-behind</bibkey>
    </paper>
    <paper id="121">
      <title>Assessment and manipulation of latent constructs in pre-trained language models using psychometric scales</title>
      <author><first>Maor</first><last>Reuben</last></author>
      <author><first>Ortal</first><last>Slobodin</last></author>
      <author><first>Idan-Chaim</first><last>Cohen</last><affiliation>Ben Gurion University of the Negev</affiliation></author>
      <author><first>Aviad</first><last>Elyashar</last><affiliation>Sami Shamoon College of Engineering</affiliation></author>
      <author><first>Orna</first><last>Braun-Lewensohn</last></author>
      <author><first>Odeya</first><last>Cohen</last><affiliation>Ben Gurion University of the Negev</affiliation></author>
      <author><first>Rami</first><last>Puzis</last><affiliation>Ben Gurion University of the Negev</affiliation></author>
      <pages>2433-2444</pages>
      <abstract>Human-like personality traits have recently been discovered in large language models, raising the hypothesis that their (known and as yet undiscovered) biases conform with human latent psychological constructs. While large conversational models may be tricked into answering psychometric questionnaires, the latent psychological constructs of thousands of simpler transformers, trained for other tasks, cannot be assessed because appropriate psychometric methods are currently lacking. Here, we show how standard psychological questionnaires can be reformulated into natural language inference prompts, and we provide a code library to support the psychometric assessment of arbitrary models. We demonstrate, using a sample of 88 publicly available models, the existence of human-like mental health-related constructs—including anxiety, depression, and the sense of coherence—which conform with standard theories in human psychology and show similar correlations and mitigation strategies. The ability to interpret and rectify the performance of language models by using psychological tools can boost the development of more explainable, controllable, and trustworthy models.</abstract>
      <url hash="a5c1332f">2025.acl-long.121</url>
      <bibkey>reuben-etal-2025-assessment</bibkey>
    </paper>
    <paper id="122">
      <title>Did Translation Models Get More Robust Without Anyone <fixed-case>E</fixed-case>ven Noticing?</title>
      <author><first>Ben</first><last>Peters</last><affiliation>Instituto de Telecomunicações, Portugal and Instituto Superior Técnico</affiliation></author>
      <author><first>Andre</first><last>Martins</last><affiliation>Instituto Superior Técnico and Unbabel</affiliation></author>
      <pages>2445-2458</pages>
      <abstract>Neural machine translation (MT) models achieve strong results across a variety of settings, but it is widely believed that they are highly sensitive to “noisy” inputs, such as spelling errors, abbreviations, and other formatting issues. In this paper, we revisit this insight in light of recent multilingual MT models and large language models (LLMs) applied to machine translation. Somewhat surprisingly, we show through controlled experiments that these models are far more robust to many kinds of noise than previous models, even when they perform similarly on clean data. This is notable because, even though LLMs have more parameters and more complex training processes than past models, none of the open ones we consider use any techniques specifically designed to encourage robustness. Next, we show that similar trends hold for social media translation experiments – LLMs are more robust to social media text. We include an analysis of the circumstances in which source correction techniques can be used to mitigate the effects of noise. Altogether, we show that robustness to many types of noise has increased.</abstract>
      <url hash="5c3115ef">2025.acl-long.122</url>
      <bibkey>peters-martins-2025-translation</bibkey>
    </paper>
    <paper id="123">
      <title>Nemotron-<fixed-case>CC</fixed-case>: Transforming <fixed-case>C</fixed-case>ommon <fixed-case>C</fixed-case>rawl into a Refined Long-Horizon Pretraining Dataset</title>
      <author><first>Dan</first><last>Su</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Kezhi</first><last>Kong</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Ying</first><last>Lin</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Joseph</first><last>Jennings</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Brandon</first><last>Norick</last><affiliation>Microsoft</affiliation></author>
      <author><first>Markus</first><last>Kliegl</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Mostofa</first><last>Patwary</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Mohammad</first><last>Shoeybi</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Bryan</first><last>Catanzaro</last><affiliation>NVIDIA</affiliation></author>
      <pages>2459-2475</pages>
      <abstract>Recent English Common Crawl datasets like FineWeb-Edu and DCLM achieved significant benchmark gains via aggressive model-based filtering, but at the cost of removing 90% of data. This limits their suitability for long token horizon training, such as 15T tokens for Llama 3.1. In this paper, we show how to achieve better trade-offs between accuracy and data quantity by a combination of classifier ensembling, synthetic data rephrasing, and reduced reliance on heuristic filters. When training 8B parameter models for 1T tokens, using a high-quality subset of our data improves MMLU by 5.6 over DCLM, demonstrating the efficacy of our methods for boosting accuracies over a relatively short token horizon. Furthermore, our full 6.3T token dataset matches DCLM on MMLU, but contains four times more unique real tokens than DCLM. This unlocks state-of-the-art training over a long token horizon: an 8B parameter model trained for 15T tokens, of which 7.2T came from our dataset, is better than the Llama 3.1 8B model: +5 on MMLU, +3.1 on ARC-Challenge, and +0.5 on average across ten diverse tasks. The dataset is available at https://data.commoncrawl.org/contrib/Nemotron/Nemotron-CC/index.html.</abstract>
      <url hash="b0ec6d84">2025.acl-long.123</url>
      <bibkey>su-etal-2025-nemotron</bibkey>
    </paper>
    <paper id="124">
      <title>Hierarchical Level-Wise News Article Clustering via Multilingual Matryoshka Embeddings</title>
      <author><first>Hans William Alexander</first><last>Hanley</last></author>
      <author><first>Zakir</first><last>Durumeric</last><affiliation>Stanford University</affiliation></author>
      <pages>2476-2492</pages>
      <abstract>Contextual large language model embeddings are increasingly utilized for topic modeling and clustering. However, current methods often scale poorly, rely on opaque similarity metrics, and struggle in multilingual settings. In this work, we present a novel, scalable, interpretable, hierarchical, and multilingual approach to clustering news articles and social media data. To do this, we first train multilingual Matryoshka embeddings that can determine story similarity at varying levels of granularity based on which subset of the dimensions of the embeddings is examined. This embedding model achieves state-of-the-art performance on the SemEval 2022 Task 8 test dataset (Pearson <tex-math>\rho</tex-math> = 0.816). Once trained, we develop an efficient hierarchical clustering algorithm that leverages the hierarchical nature of Matryoshka embeddings to identify unique news stories, narratives, and themes. We conclude by illustrating how our approach can identify and cluster stories, narratives, and overarching themes within real-world news datasets.</abstract>
      <url hash="41cd09b0">2025.acl-long.124</url>
      <bibkey>hanley-durumeric-2025-hierarchical</bibkey>
    </paper>
    <paper id="125">
      <title>Contrastive Perplexity for Controlled Generation: An Application in Detoxifying Large Language Models</title>
      <author><first>Tassilo</first><last>Klein</last><affiliation>SAP SE</affiliation></author>
      <author><first>Moin</first><last>Nabi</last><affiliation>Apple Inc.</affiliation></author>
      <pages>2493-2508</pages>
      <abstract>The generation of toxic content by large language models (LLMs) remains a critical challenge for the safe deployment of language technology. We propose a novel framework for implicit knowledge editing and controlled text generation by fine-tuning LLMs with a prototype-based contrastive perplexity objective. Central to our method is the construction of hard negatives—toxic outputs that are generated through adversarial paraphrasing to be semantically similar and model probability to their non-toxic counterparts. By training on these challenging and realistic pairs, our approach ensures robust and stable contrastive optimization. Experimental results in the domain of detoxification demonstrate that our method significantly reduces toxic generation while maintaining strong performance on downstream tasks such as commonsense reasoning and reading comprehension. Our findings highlight the effectiveness of exploiting hard negatives for attribute-aware fine-tuning.</abstract>
      <url hash="73c03373">2025.acl-long.125</url>
      <bibkey>klein-nabi-2025-contrastive</bibkey>
    </paper>
    <paper id="126">
      <title><fixed-case>INVESTORBENCH</fixed-case>: A Benchmark for Financial Decision-Making Tasks with <fixed-case>LLM</fixed-case>-based Agent</title>
      <author><first>Haohang</first><last>Li</last></author>
      <author><first>Yupeng</first><last>Cao</last><affiliation>Stevens Institute of Technology</affiliation></author>
      <author><first>Yangyang</first><last>Yu</last><affiliation>Accenture</affiliation></author>
      <author><first>Shashidhar Reddy</first><last>Javaji</last></author>
      <author><first>Zhiyang</first><last>Deng</last></author>
      <author><first>Yueru</first><last>He</last><affiliation>Columbia University</affiliation></author>
      <author><first>Yuechen</first><last>Jiang</last><affiliation>University of Hawaii at Manoa</affiliation></author>
      <author><first>Zining</first><last>Zhu</last><affiliation>Stevens Institute of Technology</affiliation></author>
      <author><first>K.p.</first><last>Subbalakshmi</last><affiliation>Stevens Institute of Technology</affiliation></author>
      <author><first>Jimin</first><last>Huang</last><affiliation>The Fin AI</affiliation></author>
      <author><first>Lingfei</first><last>Qian</last><affiliation>Yale University</affiliation></author>
      <author><first>Xueqing</first><last>Peng</last><affiliation>Yale University</affiliation></author>
      <author><first>Jordan W.</first><last>Suchow</last></author>
      <author><first>Qianqian</first><last>Xie</last></author>
      <pages>2509-2525</pages>
      <abstract>Recent advancements have underscored the potential of large language model (LLM)-based agents in financial decision-making. Despite this progress, the field currently encounters two main challenges: (1) the lack of a comprehensive LLM agent framework adaptable to a variety of financial tasks, and (2) the absence of standardized benchmarks and consistent datasets for assessing agent performance. To tackle these issues, we introduce InvestorBench, the first benchmark specifically designed for evaluating LLM-based agents in diverse financial decision-making contexts. InvestorBench enhances the versatility of LLM-enabled agents by providing a comprehensive suite of tasks applicable to different financial products, including single equities like stocks and cryptocurrencies, and exchange-traded funds (ETFs). Additionally, we assess the reasoning and decision-making capabilities of our agent framework using thirteen different LLMs as backbone models, across various market environments and tasks. Furthermore, we have curated a diverse collection of open-source, datasets and developed a comprehensive suite of environments for financial decision-making. This establishes a highly accessible platform for evaluating financial agents’ performance across various scenarios.</abstract>
      <url hash="60eb92fd">2025.acl-long.126</url>
      <bibkey>li-etal-2025-investorbench</bibkey>
    </paper>
    <paper id="127">
      <title>Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference</title>
      <author><first>Benjamin</first><last>Warner</last><affiliation>AnswerDotAI</affiliation></author>
      <author><first>Antoine</first><last>Chaffin</last><affiliation>LightOn</affiliation></author>
      <author><first>Benjamin</first><last>Clavié</last><affiliation>Answer.AI</affiliation></author>
      <author><first>Orion</first><last>Weller</last></author>
      <author><first>Oskar</first><last>Hallström</last><affiliation>LightOn</affiliation></author>
      <author><first>Said</first><last>Taghadouini</last><affiliation>LightOn</affiliation></author>
      <author><first>Alexis</first><last>Gallagher</last><affiliation>Answer.AI</affiliation></author>
      <author><first>Raja</first><last>Biswas</last><affiliation>AnswerAi</affiliation></author>
      <author><first>Faisal</first><last>Ladhak</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Tom</first><last>Aarsen</last><affiliation>Hugging Face</affiliation></author>
      <author><first>Griffin Thomas</first><last>Adams</last></author>
      <author><first>Jeremy</first><last>Howard</last><affiliation>Answer.AI</affiliation></author>
      <author><first>Iacopo</first><last>Poli</last><affiliation>LightOn</affiliation></author>
      <pages>2526-2547</pages>
      <abstract>Encoder-only transformer models such as BERT offer a great performance-size tradeoff for retrieval and classification tasks with respect to larger decoder-only models. Despite being the workhorse of numerous production pipelines, there have been limited Pareto improvements to BERT since its release. In this paper, we introduce ModernBERT, bringing modern model optimizations to encoder-only models and representing a major Pareto improvement over older encoders. Trained on 2 trillion tokens with a native 8192 sequence length, ModernBERT models exhibit state-of-the-art results on a large pool of evaluations encompassing diverse classification tasks and both single and multi-vector retrieval on different domains (including code). In addition to strong downstream performance, ModernBERT is also the most speed and memory efficient encoder and is designed for inference on common GPUs.</abstract>
      <url hash="812f347d">2025.acl-long.127</url>
      <bibkey>warner-etal-2025-smarter</bibkey>
    </paper>
    <paper id="128">
      <title>Gender Inclusivity Fairness Index (<fixed-case>GIFI</fixed-case>): A Multilevel Framework for Evaluating Gender Diversity in Large Language Models</title>
      <author><first>Zhengyang</first><last>Shan</last></author>
      <author><first>Emily</first><last>Diana</last><affiliation>CMU, Carnegie Mellon University</affiliation></author>
      <author><first>Jiawei</first><last>Zhou</last><affiliation>State University of New York at Stony Brook</affiliation></author>
      <pages>2548-2579</pages>
      <abstract>We present a comprehensive evaluation of gender fairness in large language models (LLMs), focusing on their ability to handle both binary and non-binary genders. While previous studies primarily focus on binary gender distinctions, we introduce the Gender Inclusivity Fairness Index (GIFI), a novel and comprehensive metric that quantifies the diverse gender inclusivity of LLMs. GIFI consists of a wide range of evaluations at different levels, from simply probing the model with respect to provided gender pronouns to testing various aspects of model generation and cognitive behaviors under different gender assumptions, revealing biases associated with varying gender identifiers.We conduct extensive evaluations with GIFI on 20 prominent open-source and proprietary LLMs of varying sizes and capabilities, discovering significant variations in LLMs’ gender inclusivity. Our study highlights the importance of improving LLMs’ inclusivity, providing a critical benchmark for future advancements in gender fairness in generative models.</abstract>
      <url hash="0b74307b">2025.acl-long.128</url>
      <bibkey>shan-etal-2025-gender</bibkey>
    </paper>
    <paper id="129">
      <title><fixed-case>D</fixed-case>.<fixed-case>V</fixed-case>a: Validate Your Demonstration First Before You Use It</title>
      <author><first>Qi</first><last>Zhang</last></author>
      <author><first>Zhiqing</first><last>Xiao</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Ruixuan</first><last>Xiao</last></author>
      <author><first>Lirong</first><last>Gao</last></author>
      <author><first>Junbo</first><last>Zhao</last><affiliation>Zhejiang University</affiliation></author>
      <pages>2580-2594</pages>
      <abstract>In-context learning (ICL) has demonstrated significant potential in enhancing the capabilities of large language models (LLMs) during inference. It’s well-established that ICL heavily relies on selecting effective demonstrations to achieve outputs that better align with the expected results. As for demonstration selection, previous approaches have typically relied on intuitive metrics to evaluate the effectiveness of demonstrations, which often results in limited robustness and poor cross-model generalization capabilities. To tackle these challenges, we propose a novel method, **D**emonstration **Va**lidation (**D.Va**), which integrates a demonstration validation perspective into this field. By introducing the demonstration validation mechanism, our method effectively identifies demonstrations that are both effective and highly generalizable. **D.Va** surpasses all existing retrieval-based in-context learning techniques across both natural language understanding (NLU) and natural language generation (NLG) tasks. Additionally, we demonstrate the robustness and generalizability of our approach across various language models and retrieval models.</abstract>
      <url hash="37512d3d">2025.acl-long.129</url>
      <bibkey>zhang-etal-2025-va</bibkey>
    </paper>
    <paper id="130">
      <title>Are Any-to-Any Models More Consistent Across Modality Transfers Than Specialists?</title>
      <author><first>Jiwan</first><last>Chung</last></author>
      <author><first>Janghan</first><last>Yoon</last></author>
      <author><first>Junhyeong</first><last>Park</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Sangeyl</first><last>Lee</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Joowon</first><last>Yang</last></author>
      <author><first>Sooyeon</first><last>Park</last></author>
      <author><first>Youngjae</first><last>Yu</last><affiliation>Yonsei University</affiliation></author>
      <pages>2595-2606</pages>
      <abstract>Any-to-any generative models aim to enable seamless interpretation and generation across multiple modalities within a unified framework, yet their ability to preserve relationships across modalities remains uncertain. Do unified models truly achieve cross-modal coherence, or is this coherence merely perceived? To explore this, we introduce ACON, a dataset of 1,000 images (500 newly contributed) paired with captions, editing instructions, and Q&amp;A pairs to evaluate cross-modal transfers rigorously. Using three consistency criteria—cyclic consistency, forward equivariance, and conjugated equivariance—our experiments reveal that any-to-any models do not consistently demonstrate greater cross-modal consistency than specialized models in pointwise evaluations such as cyclic consistency. However, equivariance evaluations uncover weak but observable consistency through structured analyses of the intermediate latent space enabled by multiple editing operations. We release our code and data at https://github.com/JiwanChung/ACON.</abstract>
      <url hash="a5aa6990">2025.acl-long.130</url>
      <bibkey>chung-etal-2025-models</bibkey>
    </paper>
    <paper id="131">
      <title><fixed-case>MAIN</fixed-case>-<fixed-case>RAG</fixed-case>: Multi-Agent Filtering Retrieval-Augmented Generation</title>
      <author><first>Chia-Yuan</first><last>Chang</last><affiliation>Texas A&amp;M University - College Station</affiliation></author>
      <author><first>Zhimeng</first><last>Jiang</last><affiliation>Google</affiliation></author>
      <author><first>Vineeth</first><last>Rakesh</last><affiliation>VISA</affiliation></author>
      <author><first>Menghai</first><last>Pan</last><affiliation>Visa Research</affiliation></author>
      <author><first>Chin-Chia Michael</first><last>Yeh</last><affiliation>VISA</affiliation></author>
      <author><first>Guanchu</first><last>Wang</last></author>
      <author><first>Mingzhi</first><last>Hu</last><affiliation>Worcester Polytechnic Institute</affiliation></author>
      <author><first>Zhichao</first><last>Xu</last></author>
      <author><first>Yan</first><last>Zheng</last><affiliation>VISA</affiliation></author>
      <author><first>Mahashweta</first><last>Das</last></author>
      <author><first>Na</first><last>Zou</last><affiliation>University of Houston</affiliation></author>
      <pages>2607-2622</pages>
      <abstract>Large Language Models (LLMs) are becoming essential tools for various natural language processing tasks but often suffer from generating outdated or incorrect information. Retrieval-Augmented Generation (RAG) addresses this issue by incorporating external, real-time information retrieval to ground LLM responses. However, the existing RAG systems frequently struggle with the quality of retrieval documents, as irrelevant or noisy documents degrade performance, increase computational overhead, and undermine response reliability. To tackle this problem, we propose Multi-Agent Filtering Retrieval-Augmented Generation (MAIN-RAG), a training-free RAG framework that leverages multiple LLM agents to collaboratively filter and score retrieved documents. Specifically, MAIN-RAG introduces an adaptive filtering mechanism that dynamically adjusts the relevance filtering threshold based on score distributions, effectively minimizing noise while maintaining high recall of relevant documents. The proposed approach leverages inter-agent consensus to ensure robust document selection without requiring additional training data or fine-tuning. Experimental results across four QA benchmarks demonstrate that MAIN-RAG consistently outperforms traditional RAG approaches, achieving a 2–11% improvement in answer accuracy while reducing the number of irrelevant retrieved documents. Quantitative analysis further reveals that our approach achieves superior response consistency and answer accuracy over baseline methods, offering a competitive and practical alternative to training-based solutions.</abstract>
      <url hash="b723dcb0">2025.acl-long.131</url>
      <bibkey>chang-etal-2025-main</bibkey>
    </paper>
    <paper id="132">
      <title>Unraveling the Mechanics of Learning-Based Demonstration Selection for In-Context Learning</title>
      <author><first>Hui</first><last>Liu</last></author>
      <author><first>Wenya</first><last>Wang</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Hao</first><last>Sun</last></author>
      <author><first>Chris Xing</first><last>Tian</last><affiliation>City University of Hong Kong</affiliation></author>
      <author><first>Chenqi</first><last>Kong</last></author>
      <author><first>Xin</first><last>Dong</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Haoliang</first><last>Li</last><affiliation>City University of Hong Kong</affiliation></author>
      <pages>2623-2641</pages>
      <abstract>Large Language Models (LLMs) have demonstrated impressive in-context learning (ICL) capabilities from few-shot demonstration exemplars. Recent learning-based demonstration selection methods have proven beneficial to ICL by choosing more useful exemplars. While these methods generally assume they learn better similarity measurements between exemplars and test cases from the proxy task, what kinds of similarities are captured by them and are vital to performing ICL still need to be explored. To dive into this question, we analyze the working mechanism of learning-based demonstration selection methods and empirically identify two essential factors of their similarity measurements: 1) Integrating task-agnostic similarities of different levels between the input of exemplars and test cases; 2) Incorporating task-specific similarity between the output of exemplars and test cases. We validate these two findings through extensive quantitative analysis across ten datasets and various LLMs. Based on these insights, we introduce two simplified exemplar selection methods, MLSM and TTF, catering to task-agnostic and task-specific demands to eliminate costly data collection. The effectiveness of both methods evince our findings again and pave the way for future studies.</abstract>
      <url hash="616cdcfe">2025.acl-long.132</url>
      <bibkey>liu-etal-2025-unraveling</bibkey>
    </paper>
    <paper id="133">
      <title>Direct Prompt Optimization with Continuous Representations</title>
      <author><first>Yangkun</first><last>Wang</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Zihan</first><last>Wang</last></author>
      <author><first>Jingbo</first><last>Shang</last><affiliation>University of California, San Diego</affiliation></author>
      <pages>2642-2652</pages>
      <abstract>Prompt optimization for language models faces challenges due to the large discrete search space, the reliance on continuous gradient updates, and the need to round continuous representations into discrete prompts, which causes inflexibility and instability. Existing methods attempt to address these by constraining the search space and adopting greedy, incremental improvements, but they often fail to fully leverage historical gradient information. In this paper, we model the prompt optimization problem by the probability distribution of the prompt and present a novel approach that integrates greedy strategies into optimization with continuous representations. This approach can exploit historical gradient information to address the instability caused by rounding in existing methods. Our study indicates that using continuous representations can improve prompt optimization performance on both text classification and attack tasks, as well as models, including GPT-2, OPT, Vicuna, and LLaMA-2, and also be adaptable to models of different sizes.</abstract>
      <url hash="f4b4aff4">2025.acl-long.133</url>
      <bibkey>wang-etal-2025-direct</bibkey>
    </paper>
    <paper id="134">
      <title>u<fixed-case>M</fixed-case>ed<fixed-case>S</fixed-case>um: A Unified Framework for Clinical Abstractive Summarization</title>
      <author><first>Aishik</first><last>Nagar</last></author>
      <author><first>Yutong</first><last>Liu</last><affiliation>Industry</affiliation></author>
      <author><first>Andy T.</first><last>Liu</last><affiliation>ASUS</affiliation></author>
      <author><first>Viktor</first><last>Schlegel</last><affiliation>Imperial College London and University of Manchester</affiliation></author>
      <author><first>Vijay Prakash</first><last>Dwivedi</last><affiliation>Computer Science Department, Stanford University</affiliation></author>
      <author><first>Arun-Kumar</first><last>Kaliya-Perumal</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Guna Pratheep</first><last>Kalanchiam</last></author>
      <author><first>Yili</first><last>Tang</last><affiliation>ASUS</affiliation></author>
      <author><first>Robby T.</first><last>Tan</last><affiliation>National University of Singapore</affiliation></author>
      <pages>2653-2672</pages>
      <abstract>Clinical abstractive summarization struggles to balance faithfulness and informativeness, sacrificing key information or introducing confabulations. Techniques like in-context learning and fine-tuning have improved overall summary quality orthogonally, without considering the above issue. Conversely, methods aimed at improving faithfulness and informativeness, such as model reasoning and self improvement, have not been systematically evaluated in the clinical domain. We address this gap by first performing a comprehensive benchmark and study of six advanced abstractive summarization methods across three datasets using five reference-based and reference-free metrics, with the latter specifically assessing faithfulness and informativeness. Based on its findings we then develop uMedSum, a modular hybrid framework introducing novel approaches for sequential confabulation removal and key information addition. Our work outperforms previous GPT-4-based state-of-the-art (SOTA) methods in both quantitative metrics and expert evaluations, achieving an 11.8% average improvement in dedicated faithfulness metrics over the previous SOTA. Doctors prefer uMedSum’s summaries 6 times more than previous SOTA in difficult cases containing confabulations or missing information. These results highlight uMedSum’s effectiveness and generalizability across various datasets and metrics, marking a significant advancement in clinical summarization. uMedSum toolkit is made available on GitHub.</abstract>
      <url hash="998d03a6">2025.acl-long.134</url>
      <bibkey>nagar-etal-2025-umedsum</bibkey>
    </paper>
    <paper id="135">
      <title><fixed-case>G</fixed-case>iga<fixed-case>S</fixed-case>peech 2: An Evolving, Large-Scale and Multi-domain <fixed-case>ASR</fixed-case> Corpus for Low-Resource Languages with Automated Crawling, Transcription and Refinement</title>
      <author><first>Yifan</first><last>Yang</last></author>
      <author><first>Zheshu</first><last>Song</last></author>
      <author><first>Jianheng</first><last>Zhuo</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Mingyu</first><last>Cui</last></author>
      <author><first>Jinpeng</first><last>Li</last></author>
      <author><first>Bo</first><last>Yang</last></author>
      <author><first>Yexing</first><last>Du</last></author>
      <author><first>Ziyang</first><last>Ma</last></author>
      <author><first>Xunying</first><last>Liu</last><affiliation>Chinese University of Hong Kong</affiliation></author>
      <author><first>Ziyuan</first><last>Wang</last></author>
      <author><first>Ke</first><last>Li</last></author>
      <author><first>Shuai</first><last>Fan</last><affiliation>AISpeech Ltd</affiliation></author>
      <author><first>Kai</first><last>Yu</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Wei-Qiang</first><last>Zhang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Guoguo</first><last>Chen</last></author>
      <author><first>Xie</first><last>Chen</last></author>
      <pages>2673-2686</pages>
      <abstract>The evolution of speech technology has been spurred by the rapid increase in dataset sizes. Traditional speech models generally depend on a large amount of labeled training data, which is scarce for low-resource languages. This paper presents GigaSpeech 2, a large-scale, multi-domain, multilingual speech recognition corpus. It is designed for low-resource languages and does not rely on paired speech and text data. GigaSpeech 2 comprises about 30,000 hours of automatically transcribed speech, including Thai, Indonesian, and Vietnamese, gathered from unlabeled YouTube videos. We also introduce an automated pipeline for data crawling, transcription, and label refinement. Specifically, this pipeline involves Whisper for initial transcription, MMS for forced alignment, and multi-dimensional filtering for data quality assurance. A modified Noisy Student Training is developed to further refine flawed pseudo labels iteratively, thereby enhancing model performance. Experimental results on our manually transcribed evaluation set and two public test sets from Common Voice and FLEURS confirm our corpus’s high quality and broad applicability. Notably, ASR models trained on GigaSpeech 2 can reduce the word error rate for Thai, Indonesian, and Vietnamese on our challenging and realistic YouTube test set by 25% to 40% compared to Whisper large-v3, with merely 10% model parameters. Furthermore, our ASR models trained on GigaSpeech 2 yield superior performance compared to commercial services. We hope that our newly introduced corpus and pipeline will open a new avenue for low-resource speech recognition and significantly facilitate research in this area.</abstract>
      <url hash="5bb5aae8">2025.acl-long.135</url>
      <bibkey>yang-etal-2025-gigaspeech</bibkey>
    </paper>
    <paper id="136">
      <title>Context-Aware Sentiment Forecasting via <fixed-case>LLM</fixed-case>-based Multi-Perspective Role-Playing Agents</title>
      <author><first>Fanhang</first><last>Man</last></author>
      <author><first>Huandong</first><last>Wang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Jianjie</first><last>Fang</last></author>
      <author><first>Zhaoyi</first><last>Deng</last><affiliation>University of California, Irvine</affiliation></author>
      <author><first>Baining</first><last>Zhao</last></author>
      <author><first>Xinlei</first><last>Chen</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Yong</first><last>Li</last></author>
      <pages>2687-2703</pages>
      <abstract>User sentiment on social media reveals underlying social trends, crises, and needs. Researchers have analyzed users’ past messages to track the evolution of sentiments and reconstruct sentiment dynamics. However, predicting the imminent sentiment response of users to ongoing events remains understudied. In this paper, we address the problem of sentiment forecasting on social media to predict users’ future sentiment based on event developments. We extract sentiment-related features to enhance modeling and propose a multi-perspective role-playing framework to simulate human response processes. Our preliminary results show significant improvements in sentiment forecasting at both microscopic and macroscopic levels.</abstract>
      <url hash="f06dac2d">2025.acl-long.136</url>
      <bibkey>man-etal-2025-context</bibkey>
    </paper>
    <paper id="137">
      <title><fixed-case>TARGA</fixed-case>: Targeted Synthetic Data Generation for Practical Reasoning over Structured Data</title>
      <author><first>Xiang</first><last>Huang</last></author>
      <author><first>Jiayu</first><last>Shen</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Shanshan</first><last>Huang</last><affiliation>nanjing university</affiliation></author>
      <author><first>Sitao</first><last>Cheng</last></author>
      <author><first>Xiaxia</first><last>Wang</last></author>
      <author><first>Yuzhong</first><last>Qu</last><affiliation>Nanjing University</affiliation></author>
      <pages>2704-2726</pages>
      <abstract>Semantic parsing, which converts natural language queries into logic forms, plays a crucial role in reasoning within structured environments. However, existing methods encounter two significant challenges: reliance on extensive manually annotated datasets and limited generalization capability to unseen examples. To tackle these issues, we propose Targeted Synthetic Data Generation (Targa), a practical framework that dynamically generates high-relevance synthetic data without manual annotation. Starting from the pertinent entity and relation of a given question, we probe for the potential relevant queries through layer-wise expansion and cross-layer combination. Then, we generate corresponding natural language questions for these constructed queries to jointly serve as the synthetic demonstration for in-context learning. Experiments on multiple knowledge-based question answering (KBQA) datasets demonstrate that Targa, using only a 7B-parameter model, substantially outperforms existing non-fine-tuned methods that utilize close-sourced model, achieving notable improvements in F1 scores on GrailQA(+7.7) and KBQA-Agent(+12.2). Furthermore, Targa also exhibits superior sample efficiency, robustness, and generalization capabilities under non-I.I.D. settings.</abstract>
      <url hash="8be20374">2025.acl-long.137</url>
      <bibkey>huang-etal-2025-targa</bibkey>
    </paper>
    <paper id="138">
      <title><fixed-case>A</fixed-case>ndroid<fixed-case>G</fixed-case>en: Building an Android Language Agent under Data Scarcity</title>
      <author><first>Hanyu</first><last>Lai</last></author>
      <author><first>Junjie</first><last>Gao</last></author>
      <author><first>Xiao</first><last>Liu</last></author>
      <author><first>Yifan</first><last>Xu</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Shudan</first><last>Zhang</last></author>
      <author><first>Yuxiao</first><last>Dong</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Jie</first><last>Tang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>2727-2749</pages>
      <abstract>Large language models have opened up a world of possibilities for various NLP tasks, sparking optimism for the future. Despite their potential, LLMs have yet to be widely used as agents on real mobile devices. The main challenge is the need for high-quality data sources. Time constraints and labor intensity often hinder human annotation. On the other hand, existing LLMs exhibit inadequate completion rates and need a robust data filtration strategy. Given these challenges, we develop a framework called AndroidGen to enhance the capabilities of LLM-based agents under data scarcity. In addition, we leverage AndroidGen to collect trajectories given human tasks and train open-source LLMs on these trajectories to develop an open-source mobile agent without manually labeled trajectories. We extensively evaluate AndroidGen with AndroidWorld, AitW, and various popular applications, demonstrating its improvements and revealing potential areas for future improvement. Code, model, and data are available at https://github.com/THUDM/AndroidGen.</abstract>
      <url hash="8ad2ac78">2025.acl-long.138</url>
      <bibkey>lai-etal-2025-androidgen</bibkey>
    </paper>
    <paper id="139">
      <title>Prompt Candidates, then Distill: A Teacher-Student Framework for <fixed-case>LLM</fixed-case>-driven Data Annotation</title>
      <author><first>Mingxuan</first><last>Xia</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Haobo</first><last>Wang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Yixuan</first><last>Li</last><affiliation>University of Wisconsin, Madison, Stanford University, Facebook AI, Google AI and Cornell University</affiliation></author>
      <author><first>Zewei</first><last>Yu</last></author>
      <author><first>Jindong</first><last>Wang</last><affiliation>William &amp; Mary</affiliation></author>
      <author><first>Junbo</first><last>Zhao</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Runze</first><last>Wu</last><affiliation>NetEase Corp</affiliation></author>
      <pages>2750-2770</pages>
      <abstract>Recently, Large Language Models (LLMs) have demonstrated significant potential for data annotation, markedly reducing the labor costs associated with downstream applications. However, existing methods mostly adopt an aggressive strategy by prompting LLM to determine a single gold label for each unlabeled sample. Due to the inherent uncertainty within LLMs, they often produce incorrect labels for difficult samples, severely compromising the data quality for downstream applications. Motivated by ambiguity aversion in human behaviors, we propose a novel candidate annotation paradigm wherein large language models are encouraged to output all possible labels when incurring uncertainty. To ensure unique labels are provided for downstream tasks, we develop a teacher-student framework CanDist that distills candidate annotations with a Small Language Model (SLM). We further provide a rigorous justification demonstrating that distilling candidate annotations from the teacher LLM offers superior theoretical guarantees compared to directly using single annotations. Extensive experiments across six text classification tasks validate the effectiveness of our proposed method. The source code is available at https://github.com/MingxuanXia/CanDist.</abstract>
      <url hash="13a942c1">2025.acl-long.139</url>
      <bibkey>xia-etal-2025-prompt</bibkey>
    </paper>
    <paper id="140">
      <title>A Survey of Post-Training Scaling in Large Language Models</title>
      <author><first>Hanyu</first><last>Lai</last></author>
      <author><first>Xiao</first><last>Liu</last></author>
      <author><first>Junjie</first><last>Gao</last></author>
      <author><first>Jiale</first><last>Cheng</last></author>
      <author><first>Zehan</first><last>Qi</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Yifan</first><last>Xu</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Shuntian</first><last>Yao</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author id="dan-zhang"><first>Dan</first><last>Zhang</last></author>
      <author><first>Jinhua</first><last>Du</last></author>
      <author><first>Zhenyu</first><last>Hou</last></author>
      <author><first>Xin</first><last>Lv</last><affiliation>Zhipu AI</affiliation></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <author><first>Yuxiao</first><last>Dong</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Jie</first><last>Tang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>2771-2791</pages>
      <abstract>Large language models (LLMs) have achieved remarkable proficiency in understanding and generating human natural languages, mainly owing to the “scaling law” that optimizes relationships among language modeling loss, model parameters, and pre-trained tokens. However, with the exhaustion of high-quality internet corpora and increasing computational demands, the sustainability of pre-training scaling needs to be addressed. This paper presents a comprehensive survey of post-training scaling, an emergent paradigm aiming to relieve the limitations of traditional pre-training by focusing on the alignment phase, which traditionally accounts for a minor fraction of the total training computation. Our survey categorizes post-training scaling into three key methodologies: Supervised Fine-tuning (SFT), Reinforcement Learning from Feedback (RLxF), and Test-time Compute (TTC). We provide an in-depth analysis of the motivation behind post-training scaling, the scalable variants of these methodologies, and a comparative discussion against traditional approaches. By examining the latest advancements, identifying promising application scenarios, and highlighting unresolved issues, we seek a coherent understanding and map future research trajectories in the landscape of post-training scaling for LLMs.</abstract>
      <url hash="f986c91a">2025.acl-long.140</url>
      <bibkey>lai-etal-2025-survey</bibkey>
    </paper>
    <paper id="141">
      <title>Position-aware Automatic Circuit Discovery</title>
      <author><first>Tal</first><last>Haklay</last><affiliation>Technion - Israel Institute of Technology, Technion</affiliation></author>
      <author><first>Hadas</first><last>Orgad</last><affiliation>Computer Science Department, Technion - Israel Institute of Technology</affiliation></author>
      <author><first>David</first><last>Bau</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Aaron</first><last>Mueller</last><affiliation>Northeastern University and Technion - Israel Institute of Technology</affiliation></author>
      <author><first>Yonatan</first><last>Belinkov</last><affiliation>Technion, Technion</affiliation></author>
      <pages>2792-2817</pages>
      <abstract>A widely used strategy to discover and understand language model mechanisms is circuit analysis. A circuit is a minimal subgraph of a model’s computation graph that executes a specific task. We identify a gap in existing circuit discovery methods: they assume circuits are position-invariant, treating model components as equally relevant across input positions. This limits their ability to capture cross-positional interactions or mechanisms that vary across positions. To address this gap, we propose two improvements to incorporate positionality into circuits, even on tasks containing variable-length examples. First, we extend edge attribution patching, a gradient-based method for circuit discovery, to differentiate between token positions. Second, we introduce the concept of a dataset schema, which defines token spans with similar semantics across examples, enabling position-aware circuit discovery in datasets with variable length examples. We additionally develop an automated pipeline for schema generation and application using large language models. Our approach enables fully automated discovery of position-sensitive circuits, yielding better trade-offs between circuit size and faithfulness compared to prior work.</abstract>
      <url hash="f1cab908">2025.acl-long.141</url>
      <bibkey>haklay-etal-2025-position</bibkey>
    </paper>
    <paper id="142">
      <title><fixed-case>H</fixed-case>yper<fixed-case>FM</fixed-case>: Fact-Centric Multimodal Fusion for Link Prediction over Hyper-Relational Knowledge Graphs</title>
      <author><first>Yuhuan</first><last>Lu</last></author>
      <author><first>Weijian</first><last>Yu</last></author>
      <author><first>Xin</first><last>Jing</last></author>
      <author><first>Dingqi</first><last>Yang</last><affiliation>University of Macau</affiliation></author>
      <pages>2818-2830</pages>
      <abstract>With the ubiquity of hyper-relational facts in modern Knowledge Graphs (KGs), existing link prediction techniques mostly focus on learning the sophisticated relationships among multiple entities and relations contained in a fact, while ignoring the multimodal information, which often provides additional clues to boost link prediction performance. Nevertheless, traditional multimodel fusion approaches, which are mainly designed for triple facts under either entity-centric or relation-guided fusion schemes, fail to integrate the multimodal information with the rich context of the hyper-relational fact consisting of multiple entities and relations. Against this background, we propose **HyperFM**, a **Hyper**-relational **F**act-centric **M**ultimodal Fusion technique. It effectively captures the intricate interactions between different data modalities while accommodating the hyper-relational structure of the KG in a fact-centric manner via a customized Hypergraph Transformer. We evaluate HyperFM against a sizeable collection of baselines in link prediction tasks on two real-world KG datasets. Results show that HyperFM consistently achieves the best performance, yielding an average improvement of 6.0-6.8% over the best-performing baselines on the two datasets. Moreover, a series of ablation studies systematically validate our fact-centric fusion scheme.</abstract>
      <url hash="94e1b3c7">2025.acl-long.142</url>
      <bibkey>lu-etal-2025-hyperfm</bibkey>
    </paper>
    <paper id="143">
      <title>Centurio: On Drivers of Multilingual Ability of Large Vision-Language Model</title>
      <author><first>Gregor</first><last>Geigle</last><affiliation>Bayerische Julius-Maximilians-Universität Würzburg</affiliation></author>
      <author><first>Florian</first><last>Schneider</last></author>
      <author><first>Carolin</first><last>Holtermann</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Chris</first><last>Biemann</last><affiliation>U Hamburg</affiliation></author>
      <author><first>Radu</first><last>Timofte</last><affiliation>Bayerische Julius-Maximilians-Universität Würzburg</affiliation></author>
      <author><first>Anne</first><last>Lauscher</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Goran</first><last>Glavaš</last><affiliation>Julius-Maximilians-Universität Würzburg</affiliation></author>
      <pages>2831-2881</pages>
      <abstract>Most Large Vision-Language Models (LVLMs) to date are trained predominantly on English data, which makes them struggle to understand non-English input and fail to generate output in the desired target language. Existing efforts mitigate these issues by adding multilingual training data, but do so in a largely ad-hoc manner, lacking insight into how different training mixes tip the scale for different groups of languages. In this work, we present a comprehensive investigation into the training strategies for massively multilingual LVLMs. First, we conduct a series of multi-stage experiments spanning 13 downstream vision-language tasks and 43 languages, systematically examining: (1) the number of training languages that can be included without degrading English performance and (2) optimal language distributions of pre-training as well as (3) instruction-tuning data. Further, we (4) investigate how to improve multilingual text-in-image understanding, and introduce a new benchmark for the task. Surprisingly, our analysis reveals that one can (i) include as many as 100 training languages simultaneously (ii) with as little as 25-50% of non-English data, to greatly improve multilingual performance while retaining strong English performance. We further find that (iii) including non-English OCR data in pre-training and instruction-tuning is paramount for improving multilingual text-in-image understanding. Finally, we put all our findings together and train , a 100-language LVLM, offering state-of-the-art performance in an evaluation covering 14 tasks and 56 languages.</abstract>
      <url hash="6ee8d48e">2025.acl-long.143</url>
      <bibkey>geigle-etal-2025-centurio</bibkey>
    </paper>
    <paper id="144">
      <title>Less for More: Enhanced Feedback-aligned Mixed <fixed-case>LLM</fixed-case>s for Molecule Caption Generation and Fine-Grained <fixed-case>NLI</fixed-case> Evaluation</title>
      <author><first>Dimitris</first><last>Gkoumas</last><affiliation>Queen Mary University of London</affiliation></author>
      <author><first>Maria</first><last>Liakata</last><affiliation>Queen Mary University London</affiliation></author>
      <pages>2882-2902</pages>
      <abstract>Scientific language models drive research innovation but require extensive fine-tuning on large datasets. This work enhances such models by improving their inference and evaluation capabilities with minimal or no additional training. Focusing on molecule caption generation, we explore post-training synergies between alignment fine-tuning and model merging in a cross-modal setup. We reveal intriguing insights into the behaviour and suitability of such methods while significantly surpassing state-of-the-art models. Moreover, we propose a novel atomic-level evaluation method leveraging off-the-shelf Natural Language Inference (NLI) models for use in the unseen chemical domain. Our experiments demonstrate that our evaluation operates at the right level of granularity, effectively handling multiple content units and subsentence reasoning, while widely adopted NLI methods consistently misalign with assessment criteria.</abstract>
      <url hash="cb1509eb">2025.acl-long.144</url>
      <bibkey>gkoumas-liakata-2025-less</bibkey>
    </paper>
    <paper id="145">
      <title>Ensemble Watermarks for Large Language Models</title>
      <author><first>Georg</first><last>Niess</last><affiliation>Technische Universität Graz</affiliation></author>
      <author><first>Roman</first><last>Kern</last><affiliation>Know Center GmbH and Technische Universität Graz</affiliation></author>
      <pages>2903-2916</pages>
      <abstract>As large language models (LLMs) reach human-like fluency, reliably distinguishing AI-generated text from human authorship becomes increasingly difficult. While watermarks already exist for LLMs, they often lack flexibility and struggle with attacks such as paraphrasing. To address these issues, we propose a multi-feature method for generating watermarks that combines multiple distinct watermark features into an ensemble watermark. Concretely, we combine acrostica and sensorimotor norms with the established red-green watermark to achieve a 98% detection rate. After a paraphrasing attack, the performance remains high with 95% detection rate. In comparison, the red-green feature alone as a baseline achieves a detection rate of 49% after paraphrasing. The evaluation of all feature combinations reveals that the ensemble of all three consistently has the highest detection rate across several LLMs and watermark strength settings. Due to the flexibility of combining features in the ensemble, various requirements and trade-offs can be addressed. Additionally, the same detection function can be used without adaptations for all ensemble configurations. This method is particularly of interest to facilitate accountability and prevent societal harm.</abstract>
      <url hash="dc9ec01b">2025.acl-long.145</url>
      <bibkey>niess-kern-2025-ensemble</bibkey>
    </paper>
    <paper id="146">
      <title><tex-math>\mathsf{Con Instruction}</tex-math>: Universal Jailbreaking of Multimodal Large Language Models via Non-Textual Modalities</title>
      <author><first>Jiahui</first><last>Geng</last></author>
      <author><first>Thy Thy</first><last>Tran</last></author>
      <author><first>Preslav</first><last>Nakov</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Institute for Computer Science, Artificial Intelligence and Technology, Mohamed bin Zayed University of Artificial Intelligence and Technische Universität Darmstadt</affiliation></author>
      <pages>2917-2933</pages>
      <abstract>Existing attacks against multimodal language models often communicate instruction through text, either as an explicit malicious instruction or a crafted generic prompt, and accompanied by a toxic image. In contrast, here we exploit the capabilities of MLLMs in following non-textual instruction, i.e., an adversarial image or audio, namely Con Instruction. It is a novel gray-box attack method that generates adversarial images or audio to convey specific harmful instructions to MLLMs. We also find that combining our adversarial examples with certain non-empty text inputs amplifies attack success, while appending these after malicious text has limited effects. To evaluate whether an attack is successful, we introduce a new attack response categorization (ARC) that considers the response quality and relevancy concerning the malicious instruction. The results show that Con Instruction effectively bypasses the safety mechanisms in various visual and audio-language models, including LLaVA-v1.5, InternVL, Qwen-VL, and Qwen-Audio, across two standard benchmarks: AdvBench and SafeBench. Specifically, our method achieves the highest attack success rates, reaching 81.3% and 86.6% on LLaVA-v1.5 (13B). We show that larger models are more susceptible toCon Instruction, contrasting observations in their underlying LLMs. On the defense side, we explore various methods against our attacks and find substantial gaps among existing techniques. The code will be made available upon publication.</abstract>
      <url hash="505b28c8">2025.acl-long.146</url>
      <bibkey>geng-etal-2025-mathsf</bibkey>
    </paper>
    <paper id="147">
      <title><fixed-case>TRACT</fixed-case>: Regression-Aware Fine-tuning Meets Chain-of-Thought Reasoning for <fixed-case>LLM</fixed-case>-as-a-Judge</title>
      <author><first>Cheng-Han</first><last>Chiang</last></author>
      <author><first>Hung-yi</first><last>Lee</last><affiliation>National Taiwan University</affiliation></author>
      <author><first>Michal</first><last>Lukasik</last><affiliation>Google Research</affiliation></author>
      <pages>2934-2952</pages>
      <abstract>The LLM-as-a-judge paradigm uses large language models (LLMs) for automated text evaluation, assigning a score to the input based on scoring rubrics. Existing methods for fine-tuning LLM-as-a-judge use cross-entropy (CE) loss, which neglects the numeric nature of score prediction. Recent work addresses numerical prediction limitations of LLM fine-tuning through regression-aware fine-tuning but does not consider chain-of-thought (CoT) reasoning for score prediction. In this paper, we introduce TRACT (Two-stage Regression-Aware fine-tuning with CoT), which combines CoT reasoning with regression-aware training. TRACT uses a two-stage process: first, it fine-tunes the seed LLM to generate CoTs, which serve as the training data for the second stage; next, it uses these self-generated CoTs to retrain the seed LLM. The fine-tuning objective of TRACT applies CE loss for CoT reasoning and regression-aware loss for the score. Experiments across four LLM-as-a-judge datasets and two LLMs show that TRACT significantly outperforms existing methods. Extensive ablation studies validate the effectiveness of each component in TRACT.</abstract>
      <url hash="59c978eb">2025.acl-long.147</url>
      <bibkey>chiang-etal-2025-tract</bibkey>
    </paper>
    <paper id="148">
      <title><fixed-case>D</fixed-case>io<fixed-case>R</fixed-case>: Adaptive Cognitive Detection and Contextual Retrieval Optimization for Dynamic Retrieval-Augmented Generation</title>
      <author><first>Hanghui</first><last>Guo</last></author>
      <author><first>Jia</first><last>Zhu</last><affiliation>Zhejiang Normal University</affiliation></author>
      <author><first>Shimin</first><last>Di</last><affiliation>Southeast University and Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Weijie</first><last>Shi</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Zhangze</first><last>Chen</last><affiliation>Zhejiang Normal University</affiliation></author>
      <author><first>Jiajie</first><last>Xu</last><affiliation>Soochow University</affiliation></author>
      <pages>2953-2975</pages>
      <abstract>Dynamic Retrieval-augmented Generation (RAG) has shown great success in mitigating hallucinations in large language models (LLMs) during generation. However, existing dynamic RAG methods face significant limitations in two key aspects: 1) Lack of an effective mechanism to control retrieval triggers, and 2) Lack of effective scrutiny of retrieval content. To address these limitations, we propose an innovative dynamic RAG method, DioR (Adaptive Cognitive Detection and Contextual Retrieval Optimization), which consists of two main components: adaptive cognitive detection and contextual retrieval optimization, specifically designed to determine when retrieval is needed and what to retrieve for LLMs is useful. Experimental results demonstrate that DioR achieves superior performance on all tasks, demonstrating the effectiveness of our work.</abstract>
      <url hash="bdf5ef98">2025.acl-long.148</url>
      <bibkey>guo-etal-2025-dior</bibkey>
    </paper>
    <paper id="149">
      <title>Unveiling the Power of Source: Source-based Minimum <fixed-case>B</fixed-case>ayes Risk Decoding for Neural Machine Translation</title>
      <author><first>Boxuan</first><last>Lyu</last><affiliation>Institute of Science Tokyo</affiliation></author>
      <author><first>Hidetaka</first><last>Kamigaito</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Kotaro</first><last>Funakoshi</last><affiliation>Institute of Science Tokyo</affiliation></author>
      <author><first>Manabu</first><last>Okumura</last><affiliation>Institute of Science Tokyo and Tokyo Institute of Technology, Tokyo Institute of Technology</affiliation></author>
      <pages>2976-2994</pages>
      <abstract>Maximum a posteriori decoding, a commonly used method for neural machine translation (NMT), aims to maximize the estimated posterior probability. However, high estimated probability does not always lead to high translation quality. Minimum Bayes Risk (MBR) decoding offers an alternative by seeking hypotheses with the highest expected utility.Inspired by Quality Estimation (QE) reranking which uses the QE model as a ranker, we propose source-based MBR (sMBR) decoding, a novel approach that utilizes quasi-sources (generated via paraphrasing or back-translation) as “support hypotheses” and a reference-free quality estimation metric as the utility function, marking the first work to solely use sources in MBR decoding. Experiments show that sMBR outperforms QE reranking and the standard MBR decoding. Our findings suggest that sMBR is a promising approach for NMT decoding.</abstract>
      <url hash="90b25223">2025.acl-long.149</url>
      <bibkey>lyu-etal-2025-unveiling</bibkey>
    </paper>
    <paper id="150">
      <title><fixed-case>T</fixed-case>ool<fixed-case>H</fixed-case>op: A Query-Driven Benchmark for Evaluating Large Language Models in Multi-Hop Tool Use</title>
      <author><first>Junjie</first><last>Ye</last></author>
      <author><first>Zhengyin</first><last>Du</last><affiliation>ByteDance</affiliation></author>
      <author><first>Xuesong</first><last>Yao</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Weijian</first><last>Lin</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Yufei</first><last>Xu</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Zehui</first><last>Chen</last></author>
      <author><first>Zaiyuan</first><last>Wang</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Sining</first><last>Zhu</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Zhiheng</first><last>Xi</last></author>
      <author><first>Siyu</first><last>Yuan</last></author>
      <author><first>Tao</first><last>Gui</last><affiliation>Fudan University</affiliation></author>
      <author><first>Qi</first><last>Zhang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xuanjing</first><last>Huang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Jiecao</first><last>Chen</last><affiliation>ByteDance Inc.</affiliation></author>
      <pages>2995-3021</pages>
      <abstract>Effective evaluation of multi-hop tool use is critical for analyzing the understanding, reasoning, and function-calling capabilities of large language models (LLMs). However, progress has been hindered by a lack of reliable evaluation datasets. To address this, we present ToolHop, a dataset comprising 995 user queries and 3,912 associated tools, specifically designed for rigorous evaluation of multi-hop tool use. ToolHop ensures diverse queries, meaningful interdependencies, locally executable tools, detailed feedback, and verifiable answers through a novel query-driven data construction approach that includes tool creation, document refinement, and code generation. We evaluate 14 LLMs across five model families (i.e., LLaMA3.1, Qwen2.5, Gemini1.5, Claude3.5, and GPT), uncovering significant challenges in handling multi-hop tool-use scenarios. The leading model, GPT-4o, achieves an accuracy of 49.04%, underscoring substantial room for improvement. Further analysis reveals variations in tool-use strategies for various families, offering actionable insights to guide the development of more effective approaches. Code and data can be found in https://huggingface.co/datasets/bytedance-research/ToolHop.</abstract>
      <url hash="98cf8679">2025.acl-long.150</url>
      <bibkey>ye-etal-2025-toolhop</bibkey>
    </paper>
    <paper id="151">
      <title>Mixture of insigh<fixed-case>T</fixed-case>ful Experts (<fixed-case>M</fixed-case>o<fixed-case>TE</fixed-case>): The Synergy of Reasoning Chains and Expert Mixtures in Self-Alignment</title>
      <author><first>Zhili</first><last>Liu</last></author>
      <author><first>Yunhao</first><last>Gou</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Kai</first><last>Chen</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Lanqing</first><last>Hong</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Jiahui</first><last>Gao</last></author>
      <author><first>Fei</first><last>Mi</last></author>
      <author><first>Yu</first><last>Zhang</last><affiliation>Southern University of Science and Technology</affiliation></author>
      <author><first>Zhenguo</first><last>Li</last><affiliation>Department of Computer Science and Engineering, Hong Kong University of Science and Technology and Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Xin</first><last>Jiang</last></author>
      <author><first>Qun</first><last>Liu</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>James</first><last>Kwok</last><affiliation>Department of Computer Science and Engineering, The Hong Kong University of Science and Technology</affiliation></author>
      <pages>3022-3038</pages>
      <abstract>As the capabilities of large language models (LLMs) continue to expand, aligning these models with human values remains a significant challenge. Recent studies show that reasoning abilities contribute significantly to model safety, while integrating Mixture-of-Experts (MoE) architectures can further enhance alignment.In this work, we address a fundamental question:<i>How to effectively incorporate reasoning abilitiesand MoE architectures into self-alignment processin LLMs?</i>We propose Mixture of insighTful Experts (MoTE), a novel framework that synergistically combines reasoning chains and expert mixtures to improve self-alignments.From a data perspective, MoTE employs a structured reasoning chain comprising four key stages: Question Analysis, Answer Guidance, Safe Answer, and Safety Checking. This approach enhances safety through multi-step reasoning and proves effective even for smaller and less powerful LLMs (e.g., 7B models). From an architectural perspective, MoTE adopts a multi-LoRA framework with step-level routing, where each expert is dedicated to a specific reasoning step. This design eliminates the need for balance losses, ensures stable training, and supports adaptive inference lengths. Experimental results demonstrate that MoTE significantly improves model safety, jailbreak resistance, and over-refusal capabilities, achieving performance comparable to OpenAI’s state-of-the-art o1 model.</abstract>
      <url hash="84b6d1ff">2025.acl-long.151</url>
      <bibkey>liu-etal-2025-mixture</bibkey>
    </paper>
    <paper id="152">
      <title><fixed-case>MAPS</fixed-case>: Motivation-Aware Personalized Search via <fixed-case>LLM</fixed-case>-Driven Consultation Alignment</title>
      <author><first>Weicong</first><last>Qin</last></author>
      <author><first>Yi</first><last>Xu</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Weijie</first><last>Yu</last><affiliation>University of International Business and Economics</affiliation></author>
      <author><first>Chenglei</first><last>Shen</last></author>
      <author><first>Ming</first><last>He</last><affiliation>Lenovo Group Limited</affiliation></author>
      <author><first>Jianping</first><last>Fan</last><affiliation>AI Lab at Lenovo Research, Hangzhou Dianzi University and Northwest University</affiliation></author>
      <author><first>Xiao</first><last>Zhang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Jun</first><last>Xu</last><affiliation>Renmin University of China</affiliation></author>
      <pages>3039-3051</pages>
      <abstract>Personalized product search aims to retrieve and rank items that match users’ preferences and search intent. Despite their effectiveness, existing approaches typically assume that users’ query fully captures their real motivation. However, our analysis of a real-world e-commerce platform reveals that users often engage in relevant consultations before searching, indicating they refine intents through consultations based on motivation and need. The implied motivation in consultations is a key enhancing factor for personalized search. This unexplored area comes with new challenges including aligning contextual motivations with concise queries, bridging the category-text gap, and filtering noise within sequence history. To address these, we propose a Motivation-Aware Personalized Search (MAPS) method. It embeds queries and consultations into a unified semantic space via LLMs, utilizes a Mixture of Attention Experts (MoAE) to prioritize critical semantics, and introduces dual alignment: (1) contrastive learning aligns consultations, reviews, and product features; (2) bidirectional attention integrates motivation-aware embeddings with user preferences. Extensive experiments on real and synthetic data show MAPS outperforms existing methods in both retrieval and ranking tasks. Code and supplementary materials are available at: https://github.com/E-qin/MAPS.</abstract>
      <url hash="a2cc8f9f">2025.acl-long.152</url>
      <bibkey>qin-etal-2025-maps</bibkey>
    </paper>
    <paper id="153">
      <title>Aristotle: Mastering Logical Reasoning with A Logic-Complete Decompose-Search-Resolve Framework</title>
      <author><first>Jundong</first><last>Xu</last></author>
      <author><first>Hao</first><last>Fei</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Meng</first><last>Luo</last></author>
      <author><first>Qian</first><last>Liu</last><affiliation>University of Auckland</affiliation></author>
      <author><first>Liangming</first><last>Pan</last><affiliation>University of Arizona</affiliation></author>
      <author><first>William Yang</first><last>Wang</last><affiliation>UC Santa Barbara</affiliation></author>
      <author><first>Preslav</first><last>Nakov</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Mong-Li</first><last>Lee</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Wynne</first><last>Hsu</last><affiliation>National University of Singapore</affiliation></author>
      <pages>3052-3075</pages>
      <abstract>In the context of large language models (LLMs), current advanced reasoning methods have made impressive strides in various reasoning tasks. However, when it comes to logical reasoning tasks, significant challenges remain in both efficacy and efficiency. This is rooted in the fact that these systems fail to fully leverage the inherent structure of logical tasks throughout the reasoning processes, including decomposition, search, and resolution. To address this, this paper proposes a logic-complete reasoning framework, Aristotle. The framework consists of three key components: Logical Decomposer, Logical Search Router, and Logical Resolver, in which symbolic expressions and logical rules are comprehensively integrated into the entire reasoning process, significantly alleviating the bottlenecks of logical reasoning, i.e., reducing sub-task complexity, minimizing search errors, and resolving logical contradictions. Experimental results demonstrate that Aristotle consistently outperforms state-of-the-art reasoning frameworks in both accuracy and efficiency, particularly excelling in complex logical reasoning scenarios.</abstract>
      <url hash="8c8609f0">2025.acl-long.153</url>
      <bibkey>xu-etal-2025-aristotle</bibkey>
    </paper>
    <paper id="154">
      <title><fixed-case>LADM</fixed-case>: Long-context Training Data Selection with Attention-based Dependency Measurement for <fixed-case>LLM</fixed-case>s</title>
      <author><first>Jianghao</first><last>Chen</last></author>
      <author><first>Junhong</first><last>Wu</last><affiliation>University of Chinese Academy of Sciences</affiliation></author>
      <author><first>Yangyifan</first><last>Xu</last><affiliation>University of the Chinese Academy of Sciences</affiliation></author>
      <author><first>Jiajun</first><last>Zhang</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <pages>3076-3090</pages>
      <abstract>Long-context modeling has drawn more and more attention in the area of Large Language Models (LLMs). Continual training with long-context data becomes the de-facto method to equip LLMs with the ability to process long inputs. However, it still remains an open challenge to measure the quality of long-context training data. To address this issue, we propose a Long-context data selection framework with Attention-based Dependency Measurement (LADM), which can efficiently identify high-quality long-context data from a large-scale, multi-domain pre-training corpus. LADM leverages the retrieval capabilities of the attention mechanism to capture contextual dependencies, ensuring a comprehensive quality measurement of long-context data. Experimental results show that our LADM framework significantly boosts the performance of LLMs on multiple long-context tasks with only 1B tokens for continual training.</abstract>
      <url hash="90970078">2025.acl-long.154</url>
      <bibkey>chen-etal-2025-ladm</bibkey>
    </paper>
    <paper id="155">
      <title>Iron Sharpens Iron: Defending Against Attacks in Machine-Generated Text Detection with Adversarial Training</title>
      <author><first>Yuanfan</first><last>Li</last></author>
      <author><first>Zhaohan</first><last>Zhang</last></author>
      <author><first>Chengzhengxu</first><last>Li</last></author>
      <author><first>Chao</first><last>Shen</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Xiaoming</first><last>Liu</last></author>
      <pages>3091-3113</pages>
      <abstract>Machine-generated Text (MGT) detection is crucial for regulating and attributing online texts. While the existing MGT detectors achieve strong performance, they remain vulnerable to simple perturbations and adversarial attacks. To build an effective defense against malicious perturbations, we view MGT detection from a threat modeling perspective, that is, analyzing the model’s vulnerability from an adversary’s point of view and exploring effective mitigations. To this end, we introduce an adversarial framework for training a robust MGT detector, named GREedy Adversary PromoTed DefendER (GREATER). The GREATER consists of two key components: an adversary GREATER-A and a detector GREATER-D. The GREATER-D learns to defend against the adversarial attack from GREATER-A and generalizes the defense to other attacks. GREATER-A identifies and perturbs the critical tokens in embedding space, along with greedy search and pruning to generate stealthy and disruptive adversarial examples. Besides, we update the GREATER-A and GREATER-D synchronously, encouraging the GREATER-D to generalize its defense to different attacks and varying attack intensities. Our experimental results across 10 text perturbation strategies and 6 adversarial attacks show that our GREATER-D reduces the Attack Success Rate (ASR) by 0.67% compared with SOTA defense methods while our GREATER-A is demonstrated to be more effective and efficient than SOTA attack approaches. Codes and dataset are available in https://github.com/Liyuuuu111/GREATER.</abstract>
      <url hash="76a69e2b">2025.acl-long.155</url>
      <bibkey>li-etal-2025-iron</bibkey>
    </paper>
    <paper id="156">
      <title>Cultural Learning-Based Culture Adaptation of Language Models</title>
      <author><first>Chen Cecilia</first><last>Liu</last></author>
      <author><first>Anna</first><last>Korhonen</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Institute for Computer Science, Artificial Intelligence and Technology, Mohamed bin Zayed University of Artificial Intelligence and Technische Universität Darmstadt</affiliation></author>
      <pages>3114-3134</pages>
      <abstract>Adapting large language models (LLMs) to diverse cultural values is a challenging task, as existing LLMs often reflect the values of specific groups by default, and potentially cause harm to others. In this paper, we present CLCA, a novel framework for enhancing LLM alignment with cultural values based on cultural learning. The framework leverages simulated social interactions to generate conversations in which LLMs engage in role-playing within culturally adapted social scenarios, capturing implicit cultural norms for model fine-tuning. CLCA improves cultural value alignment across various model architectures measured using World Value Survey data, demonstrating the effectiveness of our proposed approach. Our results provide early evidence that understanding intent and social interactions can enhance cultural value adaptation in LLMs, highlighting the promise of training approaches based on cultural learning.</abstract>
      <url hash="df6b1924">2025.acl-long.156</url>
      <bibkey>liu-etal-2025-cultural</bibkey>
    </paper>
    <paper id="157">
      <title>A-<fixed-case>TASC</fixed-case>: <fixed-case>A</fixed-case>sian <fixed-case>TED</fixed-case>-Based Automatic Subtitling Corpus</title>
      <author><first>Yuhan</first><last>Zhou</last></author>
      <author><first>Naoki</first><last>Yoshinaga</last><affiliation>Institute of Industrial Science, the University of Tokyo</affiliation></author>
      <pages>3135-3148</pages>
      <abstract>Subtitles play a crucial role in improving the accessibility of the vast amount of audiovisual content available on the Internet, allowing audiences worldwide to comprehend and engage with this content in various languages. Automatic subtitling (AS) systems are essential for alleviating the substantial workload of human transcribers and translators. However, existing AS corpora and the primary metric SubER focus on European languages. This paper introduces A-TASC, an Asian TED-based automatic subtitling corpus derived from English TED Talks, comprising nearly 800 hours of audio segments, aligned English transcripts, and subtitles in Chinese, Japanese, Korean, and Vietnamese. We then present SacreSubER, a modification of SubER, to enable the reliable evaluation of subtitle quality for languages without explicit word boundaries. Experimental results, using both end-to-end systems and pipeline approaches built on strong ASR and LLM components, validate the quality of the proposed corpus and reveal differences in AS performance between European and Asian languages. The code to build our corpus is released.</abstract>
      <url hash="0999ca0d">2025.acl-long.157</url>
      <bibkey>zhou-yoshinaga-2025-tasc</bibkey>
    </paper>
    <paper id="158">
      <title>Refuse Whenever You Feel Unsafe: Improving Safety in <fixed-case>LLM</fixed-case>s via Decoupled Refusal Training</title>
      <author><first>Youliang</first><last>Yuan</last><affiliation>The Chinese University of Hong Kong-Shenzhen</affiliation></author>
      <author><first>Wenxiang</first><last>Jiao</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Wenxuan</first><last>Wang</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Jen-tse</first><last>Huang</last></author>
      <author><first>Jiahao</first><last>Xu</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Tian</first><last>Liang</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Pinjia</first><last>He</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <author><first>Zhaopeng</first><last>Tu</last><affiliation>Tencent AI Lab</affiliation></author>
      <pages>3149-3167</pages>
      <abstract>This study addresses a critical gap in safety tuning practices for Large Language Models (LLMs) by identifying and tackling a refusal position bias within safety tuning data, which compromises the models’ ability to appropriately refuse generating unsafe content. We introduce a novel approach, <b>De</b>coupled <b>R</b>efusal <b>T</b>r<b>a</b>ining (DeRTa), designed to empower LLMs to refuse compliance to harmful prompts at any response position, significantly enhancing their safety capabilities. DeRTa incorporates two novel components: (1) Maximum Likelihood Estimation (MLE) with Harmful Response Prefix, which trains models to recognize and avoid unsafe content by appending a segment of harmful response to the beginning of a safe response, and (2) Reinforced Transition Optimization (RTO), which equips models with the ability to transition from potential harm to safety refusal consistently throughout the harmful response sequence. Our empirical evaluation, conducted using LLaMA3 and Mistral model families across six attack scenarios, demonstrates that our method not only improves model safety without compromising performance but also surpasses baseline methods in defending against attacks.</abstract>
      <url hash="0c45e634">2025.acl-long.158</url>
      <bibkey>yuan-etal-2025-refuse</bibkey>
    </paper>
    <paper id="159">
      <title>Token Prepending: A Training-Free Approach for Eliciting Better Sentence Embeddings from <fixed-case>LLM</fixed-case>s</title>
      <author><first>Yuchen</first><last>Fu</last></author>
      <author><first>Zifeng</first><last>Cheng</last></author>
      <author><first>Zhiwei</first><last>Jiang</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Zhonghui</first><last>Wang</last></author>
      <author><first>Yafeng</first><last>Yin</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Zhengliang</first><last>Li</last><affiliation>nanjing university</affiliation></author>
      <author><first>Qing</first><last>Gu</last><affiliation>Nanjing University</affiliation></author>
      <pages>3168-3181</pages>
      <abstract>Extracting sentence embeddings from large language models (LLMs) is a promising direction, as LLMs have demonstrated stronger semantic understanding capabilities. Previous studies typically focus on prompt engineering to elicit sentence embeddings from LLMs by prompting the model to encode sentence information into the embedding of the last token.However, LLMs are mostly decoder-only models with causal attention and the earlier tokens in the sentence cannot attend to the latter tokens, resulting in biased encoding of sentence information and cascading effects on the final decoded token.To this end, we propose a novel Token Prepending (TP) technique that prepends each layer’s decoded sentence embedding to the beginning of the sentence in the next layer’s input, allowing earlier tokens to attend to the complete sentence information under the causal attention mechanism.The proposed TP technique is a plug-and-play and training-free technique, which means it can be seamlessly integrated with various prompt-based sentence embedding methods and autoregressive LLMs.Extensive experiments on various Semantic Textual Similarity (STS) tasks and downstream classification tasks demonstrate that our proposed TP technique can significantly improve the performance of existing prompt-based sentence embedding methods across different LLMs, while incurring negligible additional inference cost.</abstract>
      <url hash="61de8f17">2025.acl-long.159</url>
      <bibkey>fu-etal-2025-token</bibkey>
    </paper>
    <paper id="160">
      <title>No Questions are Stupid, but some are Poorly Posed: Understanding Poorly-Posed Information-Seeking Questions</title>
      <author><first>Neha</first><last>Srikanth</last></author>
      <author><first>Rachel</first><last>Rudinger</last></author>
      <author><first>Jordan Lee</first><last>Boyd-Graber</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>3182-3199</pages>
      <abstract>Questions help unlock information to satisfy users’ information needs. However, when the question is poorly posed, answerers (whether human or computer) may struggle to answer the question in a way that satisfies the asker, despite possibly knowing everything necessary to address the asker’s latent information need. Using Reddit question-answer interactions from r/NoStupidQuestions, we develop a computational framework grounded in linguistic theory to study poorly-posedness of questions by generating spaces of potential interpretations of questions and computing distributions over these spaces based on interpretations chosen by both human answerers in the Reddit question thread, as well as by a suite of large language models. Both humans and models struggle to converge on dominant interpretations when faced with poorly-posed questions, but employ different strategies: humans focus on specific interpretations through question negotiation, while models attempt comprehensive coverage by addressing many interpretations simultaneously.</abstract>
      <url hash="798ae032">2025.acl-long.160</url>
      <bibkey>srikanth-etal-2025-questions</bibkey>
    </paper>
    <paper id="161">
      <title>Understanding Common Ground Misalignment in Goal-Oriented Dialog: A Case-Study with <fixed-case>U</fixed-case>buntu Chat Logs</title>
      <author><first>Rupak</first><last>Sarkar</last></author>
      <author><first>Neha</first><last>Srikanth</last></author>
      <author><first>Taylor</first><last>Pellegrin</last></author>
      <author><first>Rachel</first><last>Rudinger</last></author>
      <author><first>Claire</first><last>Bonial</last><affiliation>Georgetown University and Army Research Lab</affiliation></author>
      <author><first>Philip</first><last>Resnik</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>3200-3215</pages>
      <abstract>While it is commonly accepted that maintaining common ground plays a role in conversational success, little prior research exists connecting conversational grounding to success in task-oriented conversations. We study failures of grounding in the Ubuntu IRC dataset, where participants use text-only communication to resolve technical issues. We find that disruptions in conversational flow often stem from a misalignment in common ground, driven by a divergence in beliefs and assumptions held by participants. These disruptions, which we call conversational friction, significantly correlate with task success. While LLMs can identify overt cases of conversational friction, they struggle with subtler and more context-dependent instances that require pragmatic or domain-specific reasoning.</abstract>
      <url hash="e3ed338f">2025.acl-long.161</url>
      <bibkey>sarkar-etal-2025-understanding</bibkey>
    </paper>
    <paper id="162">
      <title>Addressing Blind Guessing: Calibration of Selection Bias in Multiple-Choice Question Answering by Video Language Models</title>
      <author><first>Olga</first><last>Loginova</last></author>
      <author><first>Oleksandr</first><last>Bezrukov</last></author>
      <author><first>Ravi</first><last>Shekhar</last><affiliation>University of Essex</affiliation></author>
      <author><first>Alexey</first><last>Kravets</last></author>
      <pages>3216-3246</pages>
      <abstract>Evaluating Video Language Models (VLMs) is a challenging task. Due to its transparency, Multiple-Choice Question Answering (MCQA) is widely used to measure the performance of these models through accuracy. However, existing MCQA benchmarks fail to capture the full reasoning capabilities of VLMs due to selection bias, when models disproportionately favor certain answer options based on positional patterns observed during training. In this work, we conduct a comprehensive empirical analysis of several VLM architectures across major datasets designed to assess complex video-focused reasoning. We identify where the bias is most pronounced and demonstrate to what extent model responses reflect genuine understanding of video content and related questions, as opposed to reliance on arbitrary patterns or superficial cues, such as answer position. By decomposing the MCQA task and adapting fairness bias metrics to VLMs, we introduce a post-processing calibration technique BOLD to balance this bias. Our results show that reducing selection bias improves not only debiasing metrics but also overall model performance, including Accuracy and F1 Mean score. Our method, by suppressing “blind guessing”, offers a more cost- and time-effective approach to mitigating selection bias compared to existing techniques. This study represents the first focused investigation of selection bias in video-to-text LLM-powered models.</abstract>
      <url hash="0cec5473">2025.acl-long.162</url>
      <bibkey>loginova-etal-2025-addressing</bibkey>
    </paper>
    <paper id="163">
      <title>Towards Reward Fairness in <fixed-case>RLHF</fixed-case>: From a Resource Allocation Perspective</title>
      <author><first>Sheng</first><last>Ouyang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Yulan</first><last>Hu</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Ge</first><last>Chen</last></author>
      <author><first>Qingyang</first><last>Li</last></author>
      <author><first>Fuzheng</first><last>Zhang</last></author>
      <author><first>Yong</first><last>Liu</last><affiliation>Renmin University of China</affiliation></author>
      <pages>3247-3259</pages>
      <abstract>Rewards serve as proxies for human preferences and play a crucial role in Reinforcement Learning from Human Feedback (RLHF). However, if these rewards are inherently imperfect, exhibiting various biases, they can adversely affect the alignment of large language models (LLMs). In this paper, we collectively define the various biases present in rewards as the problem of reward unfairness. We propose a bias-agnostic method to address the issue of reward fairness from a resource allocation perspective, without specifically designing for each type of bias, yet effectively mitigating them. Specifically, we model preference learning as a resource allocation problem, treating rewards as resources to be allocated while considering the trade-off between utility and fairness in their distribution. We propose two methods, Fairness Regularization and Fairness Coefficient, to achieve fairness in rewards. We apply our methods in both verification and reinforcement learning scenarios to obtain a fairness reward model and a policy model, respectively. Experiments conducted in these scenarios demonstrate that our approach aligns LLMs with human preferences in a more fair manner. Our data and code are available at<url>https://github.com/shoyua/Towards-Reward-Fairness</url>.</abstract>
      <url hash="d5ca6169">2025.acl-long.163</url>
      <bibkey>ouyang-etal-2025-towards</bibkey>
    </paper>
    <paper id="164">
      <title>Taming <fixed-case>LLM</fixed-case>s with Gradient Grouping</title>
      <author><first>Siyuan</first><last>Li</last></author>
      <author><first>Juanxi</first><last>Tian</last><affiliation>Peking University</affiliation></author>
      <author><first>Zedong</first><last>Wang</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Xin</first><last>Jin</last></author>
      <author><first>Zicheng</first><last>Liu</last></author>
      <author><first>Wentao</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Dan</first><last>Xu</last><affiliation>Department of Computer Science and Engineering, The Hong Kong University of Science and Technology and VGG, University of Oxford</affiliation></author>
      <pages>3260-3279</pages>
      <abstract>Training large language models (LLMs) poses challenges due to their massive scale and heterogeneous architectures. While adaptive optimizers like AdamW help address gradient variations, they still struggle with efficient and effective parameter-wise learning rate estimation, resulting in training instability, slow convergence, and poor compatibility with parameter-efficient fine-tuning (PEFT) techniques. This work introduces Scaling with Gradient Grouping (SGG), an optimizer wrapper that improves adaptive learning rate estimation by dynamic grouping and group-specific scaling. SGG first groups gradient statistics in each layer into clusters and then applies cluster-specific scaling to calibrate learning rates for each parameter, thus imposing collective group-wise constraints while maintaining precise per-parameter adaptation. Experiments on diverse (M)LLM benchmarks show that SGG integrates seamlessly with existing optimizers, and offers consistent gains and faster convergence over baselines, with various model sizes. Its stability across varying batch sizes and learning rates establishes SGG as a robust choice for LLM optimization.</abstract>
      <url hash="eae1baaa">2025.acl-long.164</url>
      <bibkey>li-etal-2025-taming</bibkey>
    </paper>
    <paper id="165">
      <title><fixed-case>L</fixed-case>azy<fixed-case>R</fixed-case>eview: A Dataset for Uncovering Lazy Thinking in <fixed-case>NLP</fixed-case> Peer Reviews</title>
      <author><first>Sukannya</first><last>Purkayastha</last><affiliation>Technische Universität Darmstadt</affiliation></author>
      <author><first>Zhuang</first><last>Li</last><affiliation>Royal Melbourne Institute of Technology</affiliation></author>
      <author><first>Anne</first><last>Lauscher</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Lizhen</first><last>Qu</last><affiliation>Monash University</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Institute for Computer Science, Artificial Intelligence and Technology, Mohamed bin Zayed University of Artificial Intelligence and Technische Universität Darmstadt</affiliation></author>
      <pages>3280-3308</pages>
      <abstract>Peer review is a cornerstone of quality control in scientific publishing. With the increasing workload, the unintended use of ‘quick’ heuristics, referred to as <i>lazy thinking</i>, has emerged as a recurring issue compromising review quality. Automated methods to detect such heuristics can help improve the peer-reviewing process. However, there is limited NLP research on this issue, and no real-world dataset exists to support the development of detection tools. This work introduces <tex-math>\textsc{LazyReview}</tex-math>, a dataset of peer-review sentences annotated with fine-grained <i>lazy thinking</i> categories. Our analysis reveals that Large Language Models (LLMs) struggle to detect these instances in a zero-shot setting. However, instruction-based fine-tuning on our dataset significantly boosts performance by 10-20 performance points, highlighting the importance of high-quality training data. Furthermore, a controlled experiment demonstrates that reviews revised with <i>lazy thinking</i> feedback are more comprehensive and actionable than those written without such feedback. We will release our dataset and the enhanced guidelines that can be used to train junior reviewers in the community.</abstract>
      <url hash="19832074">2025.acl-long.165</url>
      <bibkey>purkayastha-etal-2025-lazyreview</bibkey>
    </paper>
    <paper id="166">
      <title>Revisiting Common Assumptions about <fixed-case>A</fixed-case>rabic Dialects in <fixed-case>NLP</fixed-case></title>
      <author><first>Amr</first><last>Keleg</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Sharon</first><last>Goldwater</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Walid</first><last>Magdy</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>3309-3327</pages>
      <abstract>Arabic has diverse dialects, where one dialect can be substantially different from the others. In the NLP literature, some assumptions about these dialects are widely adopted (e.g., “Arabic dialects can be grouped into distinguishable regional dialects”) and are manifested in different computational tasks such as Arabic Dialect Identification (ADI). However, these assumptions are not quantitatively verified. We identify four of these assumptions and examine them by extending and analyzing a multi-label dataset, where the validity of each sentence in 11 different country-level dialects is manually assessed by speakers of these dialects. Our analysis indicates that the four assumptions oversimplify reality, and some of them are not always accurate. This in turn might be hindering further progress in different Arabic NLP tasks.</abstract>
      <url hash="d191e346">2025.acl-long.166</url>
      <bibkey>keleg-etal-2025-revisiting</bibkey>
    </paper>
    <paper id="167">
      <title>Retrieve to Explain: Evidence-driven Predictions for Explainable Drug Target Identification</title>
      <author><first>Ravi</first><last>Patel</last></author>
      <author><first>Angus</first><last>Brayne</last></author>
      <author><first>Rogier</first><last>Hintzen</last></author>
      <author><first>Daniel</first><last>Jaroslawicz</last><affiliation>BenevolentAI</affiliation></author>
      <author><first>Georgiana</first><last>Neculae</last><affiliation>BenevolentAI</affiliation></author>
      <author><first>Dane S.</first><last>Corneil</last></author>
      <pages>3328-3370</pages>
      <abstract>Language models hold incredible promise for enabling scientific discovery by synthesizing massive research corpora. Many complex scientific research questions have multiple plausible answers, each supported by evidence of varying strength. However, existing language models lack the capability to quantitatively and faithfully compare answer plausibility in terms of supporting evidence. To address this, we introduce Retrieve to Explain (R2E), a retrieval-based model that scores and ranks all possible answers to a research question based on evidence retrieved from a document corpus. The architecture represents each answer only in terms of its supporting evidence, with the answer itself masked. This allows us to extend feature attribution methods such as Shapley values, to transparently attribute answer scores to supporting evidence at inference time. The architecture also allows incorporation of new evidence without retraining, including non-textual data modalities templated into natural language. We developed R2E for the challenging scientific discovery task of drug target identification, a human-in-the-loop process where failures are extremely costly and explainability paramount. When predicting whether drug targets will subsequently be confirmed as efficacious in clinical trials, R2E not only matches non-explainable literature-based models but also surpasses a genetics-based target identification approach used throughout the pharmaceutical industry.</abstract>
      <url hash="93b6c3f7">2025.acl-long.167</url>
      <bibkey>patel-etal-2025-retrieve</bibkey>
    </paper>
    <paper id="168">
      <title>Whose Boat Does it Float? Improving Personalization in Preference Tuning via Inferred User Personas</title>
      <author><first>Nishant</first><last>Balepur</last></author>
      <author><first>Vishakh</first><last>Padmakumar</last><affiliation>New York University</affiliation></author>
      <author><first>Fumeng</first><last>Yang</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Shi</first><last>Feng</last><affiliation>George Washington University</affiliation></author>
      <author><first>Rachel</first><last>Rudinger</last></author>
      <author><first>Jordan Lee</first><last>Boyd-Graber</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>3371-3393</pages>
      <abstract>LLMs are aligned to follow input instructions by learning which of two responses users prefer for a prompt. However, such preference data do not convey *why* users prefer responses that are chosen or rejected, so LLMs trained on these datasets cannot tailor responses to varied user needs. To surface these parameters of personalization, we apply *abductive reasoning* to preference data, inferring needs and interests of users, i.e., personas, that may prefer either response. We test this idea in two steps: **Persona Inference (PI)**—abductively inferring personas of users who prefer chosen or rejected outputs—and **Persona Tailoring (PT)**—training models to tailor outputs to personas from PI. We show: 1) LLMs infer personas accurately explaining why different users may prefer *both* chosen or rejected outputs; 2) Training on preference data augmented with PI personas via PT boosts personalization and generalizes to supporting user-written personas; and 3) Rejected response personas form harder personalization evaluations, showing PT better aids users with uncommon preferences versus typical alignment methods. We argue for an abductive view of preferences for personalization, asking not only which response is better but when, why, and for whom.</abstract>
      <url hash="7e1246b1">2025.acl-long.168</url>
      <bibkey>balepur-etal-2025-whose</bibkey>
    </paper>
    <paper id="169">
      <title>Which of These Best Describes Multiple Choice Evaluation with <fixed-case>LLM</fixed-case>s? A) Forced <fixed-case>B</fixed-case>) Flawed <fixed-case>C</fixed-case>) Fixable <fixed-case>D</fixed-case>) All of the Above</title>
      <author><first>Nishant</first><last>Balepur</last></author>
      <author><first>Rachel</first><last>Rudinger</last></author>
      <author><first>Jordan Lee</first><last>Boyd-Graber</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>3394-3418</pages>
      <abstract>Multiple choice question answering (MCQA) is popular for LLM evaluation due to its simplicity and human-like testing, but we argue for its reform. We first reveal flaws in MCQA’s format, as it struggles to: 1) test generation/subjectivity; 2) match LLM use cases; and 3) fully test knowledge. We instead advocate for generative formats based on human testing—where LLMs construct and explain answers—better capturing user needs and knowledge while remaining easy to score. We then show even when MCQA is a useful format, its datasets suffer from: leakage; unanswerability; shortcuts; and saturation. In each issue, we give fixes from education, like rubrics to guide MCQ writing; scoring methods to bridle guessing; and Item Response Theory to build harder MCQs. Lastly, we discuss LLM errors in MCQA—robustness, biases, and unfaithful explanations—showing how our prior solutions better measure or address these issues. While we do not need to desert MCQA, we encourage more efforts in refining the task based on educational testing, advancing evaluations.</abstract>
      <url hash="b28e2fa2">2025.acl-long.169</url>
      <bibkey>balepur-etal-2025-best</bibkey>
    </paper>
    <paper id="170">
      <title>Detection of Human and Machine-Authored Fake News in <fixed-case>U</fixed-case>rdu</title>
      <author><first>Muhammad Zain</first><last>Ali</last><affiliation>University of Waikato</affiliation></author>
      <author><first>Yuxia</first><last>Wang</last></author>
      <author><first>Bernhard</first><last>Pfahringer</last><affiliation>The University of Waikato</affiliation></author>
      <author><first>Tony C</first><last>Smith</last></author>
      <pages>3419-3428</pages>
      <abstract>The rise of social media has amplified the spread of fake news, now further complicated by large language models (LLMs) like ChatGPT, which ease the generation of highly convincing, error-free misinformation, making it increasingly challenging for the public to discern truth from falsehood. Traditional fake news detection methods relying on linguistic cues have also become less effective. Moreover, current detectors primarily focus on binary classification and English texts, often overlooking the distinction between machine-generated true vs. fake news and the detection in low-resource languages. To this end, we updated the detection schema to include machine-generated news focusing on Urdu. We further propose a conjoint detection strategy to improve the accuracy and robustness. Experiments show its effectiveness across four datasets in various settings.</abstract>
      <url hash="3b8a4416">2025.acl-long.170</url>
      <bibkey>ali-etal-2025-detection</bibkey>
    </paper>
    <paper id="171">
      <title>An Efficient Task-Oriented Dialogue Policy: Evolutionary Reinforcement Learning Injected by Elite Individuals</title>
      <author><first>Yangyang</first><last>Zhao</last><affiliation>Changsha University of Science and Technology</affiliation></author>
      <author><first>Ben</first><last>Niu</last></author>
      <author><first>Libo</first><last>Qin</last><affiliation>Central South University</affiliation></author>
      <author><first>Shihan</first><last>Wang</last><affiliation>Utrecht University</affiliation></author>
      <pages>3429-3442</pages>
      <abstract>Deep Reinforcement Learning (DRL) is widely used in task-oriented dialogue systems to optimize dialogue policy, but it struggles to balance exploration and exploitation due to the high dimensionality of state and action spaces. This challenge often results in local optima or poor convergence. Evolutionary Algorithms (EAs) have been proven to effectively explore the solution space of neural networks by maintaining population diversity. Inspired by this, we innovatively combine the global search capabilities of EA with the local optimization of DRL to achieve a balance between exploration and exploitation. Nevertheless, the inherent flexibility of natural language in dialogue tasks complicates this direct integration, leading to prolonged evolutionary times. Thus, we further propose an elite individual injection mechanism to enhance EA’s search efficiency by adaptively introducing best-performing individuals into the population. Experiments across four datasets show that our approach significantly improves the balance between exploration and exploitation, boosting performance. Moreover, the effectiveness of the EII mechanism in reducing exploration time has been demonstrated, achieving an efficient integration of EA and DRL on task-oriented dialogue policy tasks.</abstract>
      <url hash="ec2118f3">2025.acl-long.171</url>
      <bibkey>zhao-etal-2025-efficient</bibkey>
    </paper>
    <paper id="172">
      <title><fixed-case>SR</fixed-case>-<fixed-case>LLM</fixed-case>: Rethinking the Structured Representation in Large Language Model</title>
      <author><first>Jiahuan</first><last>Zhang</last><affiliation>Tianjin University</affiliation></author>
      <author><first>Tianheng</first><last>Wang</last><affiliation>Hangzhou Institute of Medicine Chinese Academy of Science</affiliation></author>
      <author><first>Ziyi</first><last>Huang</last></author>
      <author><first>Yulong</first><last>Wu</last></author>
      <author><first>Hanqing</first><last>Wu</last></author>
      <author><first>DongbaiChen</first><last>DongbaiChen</last></author>
      <author><first>Linfeng</first><last>Song</last></author>
      <author><first>Yue</first><last>Zhang</last><affiliation>Westlake University</affiliation></author>
      <author><first>Guozheng</first><last>Rao</last><affiliation>Tianjin University</affiliation></author>
      <author><first>Kaicheng</first><last>Yu</last><affiliation>KMind.AI and Westlake University</affiliation></author>
      <pages>3443-3462</pages>
      <abstract>Structured representations, exemplified by Abstract Meaning Representation (AMR), have long been pivotal in computational linguistics. However, their role remains ambiguous in the Large Language Models (LLMs) era. Initial attempts to integrate structured representation into LLMs via a zero-shot setting yielded inferior performance. We hypothesize that such a decline stems from the structure information being passed into LLMs in a code format unfamiliar to LLMs’ training corpora. Consequently, we propose SR-LLM, an innovative framework with two settings to explore a superior way of integrating structured representation with LLMs from training-free and training-dependent perspectives. The former integrates structural information through natural language descriptions in LLM prompts, whereas its counterpart augments the model’s inference capability through fine-tuning on linguistically described structured representations. Performance improvements were observed in widely downstream datasets, with particularly notable gains of 3.17% and 12.38% in PAWS. To the best of our knowledge, this work represents the pioneering demonstration that leveraging structural representations can substantially enhance LLMs’ inference capability. We hope that our work sheds light and encourages future research to enhance the reasoning and interoperability of LLMs by structure data.</abstract>
      <url hash="1015b071">2025.acl-long.172</url>
      <bibkey>zhang-etal-2025-sr</bibkey>
    </paper>
    <paper id="173">
      <title>Taming Language Models for Text-attributed Graph Learning with Decoupled Aggregation</title>
      <author><first>Chuang</first><last>Zhou</last></author>
      <author><first>Zhu</first><last>Wang</last></author>
      <author><first>Shengyuan</first><last>Chen</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Jiahe</first><last>Du</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Qiyuan</first><last>Zheng</last></author>
      <author><first>Zhaozhuo</first><last>Xu</last><affiliation>Stevens Institute of Technology</affiliation></author>
      <author><first>Xiao</first><last>Huang</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <pages>3463-3474</pages>
      <abstract>Text-attributed graphs (TAGs) are prevalent in various real-world applications, including academic networks, e-commerce platforms, and social networks. Effective learning on TAGs requires leveraging both textual node features and structural graph information. While language models (LMs) excel at processing text and graph neural networks (GNNs) effectively capture relational structures, their direct integration is computationally prohibitive due to the high cost of text and graph representation learning. Existing approaches address this challenge by adopting a two-step pipeline where LMs generate fixed node embeddings, which are then used for GNN training. However, this method neglects the interaction between textual and structural information, leading to suboptimal learning outcomes. To overcome these limitations, we propose SKETCH (Semantic Knowledge and Structure Enrichment), a novel framework that decouples node aggregation from graph convolution and integrates it into the text representation learning process. SKETCH enhances TAG learning by incorporating two key aggregation mechanisms: (1) Semantic aggregation, which retrieves semantically relevant node texts for contextual enrichment, and (2) Structural aggregation, which propagates textual features beyond immediate neighbors to capture broader graph relationships. Extensive experiments demonstrate that SKETCH outperforms state-of-the-art TAG learning methods while requiring fewer computational resources. By enabling a more efficient and effective fusion of textual and structural information, SKETCH provides new insights into TAG problems and offers a practical solution for real applications.</abstract>
      <url hash="9c7caac0">2025.acl-long.173</url>
      <bibkey>zhou-etal-2025-taming</bibkey>
    </paper>
    <paper id="174">
      <title>Contrastive Prompting Enhances Sentence Embeddings in <fixed-case>LLM</fixed-case>s through Inference-Time Steering</title>
      <author><first>Zifeng</first><last>Cheng</last></author>
      <author><first>Zhonghui</first><last>Wang</last></author>
      <author><first>Yuchen</first><last>Fu</last></author>
      <author><first>Zhiwei</first><last>Jiang</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Yafeng</first><last>Yin</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Cong</first><last>Wang</last><affiliation>Singapore Management University and Nanjing University</affiliation></author>
      <author><first>Qing</first><last>Gu</last><affiliation>Nanjing University</affiliation></author>
      <pages>3475-3487</pages>
      <abstract>Extracting sentence embeddings from large language models (LLMs) is a practical direction, as it requires neither additional data nor fine-tuning. Previous studies usually focus on prompt engineering to guide LLMs to encode the core semantic information of the sentence into the embedding of the last token. However, the last token in these methods still encodes an excess of non-essential information, such as stop words, limiting its encoding capacity. To this end, we propose a Contrastive Prompting (CP) technique that introduces an extra auxiliary prompt to elicit better sentence embedding. By contrasting with the auxiliary prompt, CP can steer existing prompts to encode the core semantics of the sentence, rather than non-essential information. CP is a plug-and-play inference-time intervention method that can be combined with various prompt-based methods. Extensive experiments on Semantic Textual Similarity (STS) tasks and downstream classification tasks demonstrate that our method can improve the performance of existing prompt-based methods across different LLMs.</abstract>
      <url hash="bf947571">2025.acl-long.174</url>
      <bibkey>cheng-etal-2025-contrastive</bibkey>
    </paper>
    <paper id="175">
      <title>Cracking the Code of Hallucination in <fixed-case>LVLM</fixed-case>s with Vision-aware Head Divergence</title>
      <author><first>Jinghan</first><last>He</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Kuan</first><last>Zhu</last></author>
      <author><first>Haiyun</first><last>Guo</last><affiliation>Institute of automation, Chinese Academy of Sciences</affiliation></author>
      <author><first>Junfeng</first><last>Fang</last></author>
      <author><first>Zhenglin</first><last>Hua</last></author>
      <author><first>Yuheng</first><last>Jia</last><affiliation>Southeast University</affiliation></author>
      <author><first>Ming</first><last>Tang</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Tat-Seng</first><last>Chua</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Jinqiao</first><last>Wang</last><affiliation>Institute of Automation, Chinese Academy of Sciences</affiliation></author>
      <pages>3488-3501</pages>
      <abstract>Large vision-language models (LVLMs) have made substantial progress in integrating large language models (LLMs) with visual inputs, enabling advanced multimodal reasoning. Despite their success, a persistent challenge is hallucination—where generated text fails to accurately reflect visual content—undermining both accuracy and reliability. Existing methods focus on alignment training or decoding refinements but primarily address symptoms at the generation stage without probing the underlying causes. In this work, we investigate the internal mechanisms driving hallucination in LVLMs, with an emphasis on the multi-head attention module. Specifically, we introduce Vision-aware Head Divergence (VHD), a metric that quantifies the sensitivity of attention head outputs to visual context. Based on this, our findings reveal the presence of vision-aware attention heads that are more attuned to visual information; however, the model’s overreliance on its prior language patterns is closely related to hallucinations. Building on these insights, we propose Vision-aware Head Reinforcement (VHR), a training-free approach to mitigate hallucination by enhancing the role of vision-aware attention heads. Extensive experiments demonstrate that our method achieves superior performance compared to state-of-the-art approaches in mitigating hallucinations, while maintaining high efficiency with negligible additional time overhead. The code is available at https://github.com/jinghan1he/VHR.</abstract>
      <url hash="19c3a9d2">2025.acl-long.175</url>
      <bibkey>he-etal-2025-cracking</bibkey>
    </paper>
    <paper id="176">
      <title>Hierarchical Document Refinement for Long-context Retrieval-augmented Generation</title>
      <author><first>Jiajie</first><last>Jin</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Xiaoxi</first><last>Li</last></author>
      <author><first>Guanting</first><last>Dong</last></author>
      <author><first>Yuyao</first><last>Zhang</last></author>
      <author><first>Yutao</first><last>Zhu</last></author>
      <author><first>Yongkang</first><last>Wu</last></author>
      <author><first>Zhonghua</first><last>Li</last></author>
      <author><first>Ye</first><last>Qi</last></author>
      <author><first>Zhicheng</first><last>Dou</last><affiliation>Renmin University of China</affiliation></author>
      <pages>3502-3520</pages>
      <abstract>Real-world RAG applications often encounter long-context input scenarios, where redundant information and noise results in higher inference costs and reduced performance. To address these challenges, we propose LongRefiner, an efficient plug-and-play refiner that leverages the inherent structural characteristics of long documents. LongRefiner employs dual-level query analysis, hierarchical document structuring, and adaptive refinement through multi-task learning on a single foundation model. Experiments on seven QA datasets demonstrate that LongRefiner achieves competitive performance in various scenarios while using 10x fewer computational costs and latency compared to the best baseline. Further analysis validates that LongRefiner is scalable, efficient, and effective, providing practical insights for real-world long-text RAG applications. Our code is available at https://github.com/ignorejjj/LongRefiner.</abstract>
      <url hash="496dc761">2025.acl-long.176</url>
      <bibkey>jin-etal-2025-hierarchical</bibkey>
    </paper>
    <paper id="177">
      <title>Comparing Moral Values in <fixed-case>W</fixed-case>estern <fixed-case>E</fixed-case>nglish-speaking societies and <fixed-case>LLM</fixed-case>s with Word Associations</title>
      <author><first>Chaoyi</first><last>Xiang</last></author>
      <author><first>Chunhua</first><last>Liu</last></author>
      <author><first>Simon</first><last>De Deyne</last></author>
      <author><first>Lea</first><last>Frermann</last><affiliation>University of Melbourne</affiliation></author>
      <pages>3521-3536</pages>
      <abstract>As the impact of large language models increases, understanding the moral values they encode becomes ever more important. Assessing moral values encoded in these models via direct prompting is challenging due to potential leakage of human norms into model training data, and their sensitivity to prompt formulation. Instead, we propose to use word associations, which have been shown to reflect moral reasoning in humans, as low-level underlying representations to obtain a more robust picture of LLMs’ moral reasoning. We study moral differences in associations from western English-speaking communities and LLMs trained predominantly on English data. First, we create a large dataset of <tex-math>\textit{LLM-generated}</tex-math> word associations, resembling an existing data set of <tex-math>\textit{human}</tex-math> word associations. Next, we propose a novel method to propagate moral values based on seed words derived from Moral Foundation Theory through the human and LLM-generated association graphs. Finally, we compare the resulting moral representations, highlighting detailed but systematic differences between moral values emerging from English speakers and LLM associations.</abstract>
      <url hash="a8aebde7">2025.acl-long.177</url>
      <bibkey>xiang-etal-2025-comparing</bibkey>
    </paper>
    <paper id="178">
      <title><fixed-case>TEACH</fixed-case>: A Contrastive Knowledge Adaptive Distillation Framework for Classical <fixed-case>C</fixed-case>hinese Understanding</title>
      <author><first>Yuting</first><last>Wei</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Qi</first><last>Meng</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Yuanxing</first><last>Xu</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Bin</first><last>Wu</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <pages>3537-3550</pages>
      <abstract>Traditional methods for processing classical Chinese typically segment language understanding into discrete tasks, which overlook crucial background information and reduce user engagement. Large language models (LLMs) provide integrated solutions, yet they entail high computational costs and risks of generating inaccurate historical information. To tackle these challenges, we propose a novel framework, TEACH (conTrastive knowlEdge Adaptive distillation with enhanCed Historical interpretability), which focuses on classical Chinese understanding by integrating word sense disambiguation with sentence translation. This integration leverages a confidence-annotated knowledge base and a step-by-step Chain-of-Thought prompting mechanism to minimize hallucinations and improve semantic analysis. Moreover, TEACH employs contrastive distillation learning to efficiently transfer capabilities from larger models to smaller ones (e.g., Qwen2-1.5B), addressing overly liberal translations. Additionally, we introduce an innovative generation evaluation metric using iterative word alignment, enhancing LLM performance assessments by distinguishing additional information and addressing excessive translation issues. Experiments conducted on real-world datasets validate TEACH’s efficacy in classical Chinese educational scenarios.</abstract>
      <url hash="569b4f1a">2025.acl-long.178</url>
      <bibkey>wei-etal-2025-teach</bibkey>
    </paper>
    <paper id="179">
      <title><fixed-case>RAG</fixed-case>-Critic: Leveraging Automated Critic-Guided Agentic Workflow for Retrieval Augmented Generation</title>
      <author><first>Guanting</first><last>Dong</last></author>
      <author><first>Jiajie</first><last>Jin</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Xiaoxi</first><last>Li</last></author>
      <author><first>Yutao</first><last>Zhu</last></author>
      <author><first>Zhicheng</first><last>Dou</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Ji-Rong</first><last>Wen</last><affiliation>Renmin University of China</affiliation></author>
      <pages>3551-3578</pages>
      <abstract>Retrieval-augmented generation (RAG) has emerged as a pivotal technology in natural language processing, owing to its efficacy in generating factual content. However, its informative inputs and complex paradigms often lead to a greater variety of errors. Consequently, achieving automated on-policy assessment and error-oriented correction remain unresolved issues. In this paper, we propose RAG-Critic, a novel framework that leverages a critic-guided agentic workflow to improve RAG capabilities autonomously. Specifically, we initially design a data-driven error mining pipeline to establish a hierarchical RAG error system. Based on this system, we progressively align an error-critic model using a coarse-to-fine training objective, which automatically provides fine-grained error feedback. Finally, we design a critic-guided agentic RAG workflow that customizes executor-based solution flows based on the error-critic model’s feedback, facilitating an error-driven self-correction process. Experimental results across seven RAG-related datasets confirm the effectiveness of RAG-Critic, while qualitative analysis offers practical insights for achieving reliable RAG systems. Our dataset and code are available at https://github.com/RUC-NLPIR/RAG-Critic.</abstract>
      <url hash="3b0976a3">2025.acl-long.179</url>
      <bibkey>dong-etal-2025-rag</bibkey>
    </paper>
    <paper id="180">
      <title>Progressive Multimodal Reasoning via Active Retrieval</title>
      <author><first>Guanting</first><last>Dong</last></author>
      <author><first>Chenghao</first><last>Zhang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Mengjie</first><last>Deng</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Yutao</first><last>Zhu</last></author>
      <author><first>Zhicheng</first><last>Dou</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Ji-Rong</first><last>Wen</last><affiliation>Renmin University of China</affiliation></author>
      <pages>3579-3602</pages>
      <abstract>Multi-step multimodal reasoning tasks pose significant challenges for multimodal large language models (MLLMs), and finding effective ways to enhance their performance in such scenarios remains an unresolved issue. In this paper, we propose AR-MCTS, a universal framework designed to progressively improve the reasoning capabilities of MLLMs through Active Retrieval (AR) and Monte Carlo Tree Search (MCTS). AR-MCTS follows the MCTS algorithm and heuristically integrates an active retrieval mechanism during the expansion stage to automatically acquire high-quality step-wise reasoning annotations. Moreover, we further introduce curriculum training objectives to progressively align with a process reward model, ultimately achieving process-level multimodal reasoning verification. Experimental results across three complex multimodal reasoning benchmarks confirm the effectiveness of AR-MCTS. Further analysis demonstrates that it can optimize sampling diversity and accuracy, yielding reliable multimodal reasoning.</abstract>
      <url hash="a4baa809">2025.acl-long.180</url>
      <bibkey>dong-etal-2025-progressive</bibkey>
    </paper>
    <paper id="181">
      <title>Pre-training Distillation for Large Language Models: A Design Space Exploration</title>
      <author><first>Hao</first><last>Peng</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Xin</first><last>Lv</last><affiliation>Zhipu AI</affiliation></author>
      <author><first>Yushi</first><last>Bai</last></author>
      <author><first>Zijun</first><last>Yao</last></author>
      <author><first>Jiajie</first><last>Zhang</last></author>
      <author><first>Lei</first><last>Hou</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <pages>3603-3618</pages>
      <abstract>Knowledge distillation (KD) aims to transfer knowledge from a large teacher model to a smaller student model. Previous work applying KD in the field of large language models (LLMs) typically focused on the post-training phase, where the student LLM learns directly from instructions and corresponding responses generated by the teacher model. In this paper, we extend KD to the pre-training phase of LLMs, named pre-training distillation (PD). We first conduct a preliminary experiment using GLM-4-9B as the teacher LLM to distill a 1.9B parameter student LLM, validating the effectiveness of PD. Considering the key impact factors of distillation, we systematically explore the design space of pre-training distillation across four aspects: logits processing, loss selection, scaling law, and offline or online logits. We conduct extensive experiments to explore the design space of pre-training distillation and find better configurations and interesting conclusions, such as larger student LLMs generally benefiting more from pre-training distillation, while a larger teacher LLM does not necessarily guarantee better results. We hope our exploration of the design space will inform future practices in pre-training distillation.</abstract>
      <url hash="3904ac47">2025.acl-long.181</url>
      <bibkey>peng-etal-2025-pre</bibkey>
    </paper>
    <paper id="182">
      <title>Teaching Vision-Language Models to Ask: Resolving Ambiguity in Visual Questions</title>
      <author><first>Pu</first><last>Jian</last><affiliation>University of the Chinese Academy of Sciences</affiliation></author>
      <author><first>Donglei</first><last>Yu</last></author>
      <author><first>Wen</first><last>Yang</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Shuo</first><last>Ren</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jiajun</first><last>Zhang</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <pages>3619-3638</pages>
      <abstract>In visual question answering (VQA) context, users often pose ambiguous questions to visual language models (VLMs) due to varying expression habits. Existing research addresses such ambiguities primarily by rephrasing questions. These approaches neglect the inherently interactive nature of user interactions with VLMs, where ambiguities can be clarified through user feedback. However, research on interactive clarification faces two major challenges: (1) Benchmarks are absent to assess VLMs’ capacity for resolving ambiguities through interaction; (2) VLMs are trained to prefer answering rather than asking, preventing them from seeking clarification. To overcome these challenges, we introduce ClearVQA benchmark, which targets three common categories of ambiguity in VQA context, and encompasses various VQA scenarios. Furthermore, we propose an automated pipeline to generate ambiguity-clarification question pairs, enabling VLMs to ask reasonable clarification questions and generate more accurate and specific answers based on user feedback, as demonstrated by experimental results.</abstract>
      <url hash="35e6013a">2025.acl-long.182</url>
      <bibkey>jian-etal-2025-teaching</bibkey>
    </paper>
    <paper id="183">
      <title><fixed-case>L</fixed-case>ong<fixed-case>B</fixed-case>ench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks</title>
      <author><first>Yushi</first><last>Bai</last></author>
      <author><first>Shangqing</first><last>Tu</last></author>
      <author><first>Jiajie</first><last>Zhang</last></author>
      <author><first>Hao</first><last>Peng</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Xiaozhi</first><last>Wang</last><affiliation>Department of Computer Science and Technology, Tsinghua University</affiliation></author>
      <author><first>Xin</first><last>Lv</last><affiliation>Zhipu AI</affiliation></author>
      <author><first>Shulin</first><last>Cao</last><affiliation>Zhipu AI and Tsinghua University</affiliation></author>
      <author><first>Jiazheng</first><last>Xu</last></author>
      <author><first>Lei</first><last>Hou</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Yuxiao</first><last>Dong</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Jie</first><last>Tang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <pages>3639-3664</pages>
      <abstract>This paper introduces LongBench v2, a benchmark designed to assess the ability of LLMs to handle long-context problems requiring deep understanding and reasoning across real-world multitasks. LongBench v2 consists of 503 challenging multiple-choice questions, with contexts ranging from 8k to 2M words, across six major task categories: single-document QA, multi-document QA, long in-context learning, long-dialogue history understanding, code repository understanding, and long structured data understanding. To ensure the breadth and the practicality, we collect data from nearly 100 highly educated individuals with diverse professional backgrounds. We employ both automated and manual review processes to maintain high quality and difficulty, resulting in human experts achieving only 53.7% accuracy under a 15-minute time constraint. Our evaluation reveals that the best-performing model, when directly answers the questions, achieves only 50.1% accuracy. In contrast, the o1-preview model, which includes longer reasoning, achieves 57.7%, surpassing the human baseline by 4%. These results highlight the importance of enhanced reasoning ability and scaling inference-time compute to tackle the long-context challenges in LongBench v2.</abstract>
      <url hash="f97fa271">2025.acl-long.183</url>
      <bibkey>bai-etal-2025-longbench</bibkey>
    </paper>
    <paper id="184">
      <title>Battling against Tough Resister: Strategy Planning with Adversarial Game for Non-collaborative Dialogues</title>
      <author><first>Haiyang</first><last>Wang</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Zhiliang</first><last>Tian</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Yuchen</first><last>Pan</last></author>
      <author><first>Xin</first><last>Song</last></author>
      <author><first>Xin</first><last>Niu</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <author><first>Bin</first><last>Zhou</last><affiliation>National University of Defense Technology</affiliation></author>
      <pages>3665-3685</pages>
      <abstract>Non-collaborative dialogue involves two participants with conflicting interests engaging in a multi-round dialogue to achieve their own goals. Strategy planning is the key to guiding both participants towards a consensus. Most LLMs-based methods use stimulus prompts or external strategy planners for strategy planning. However, stimulus prompts fail to teach LLMs to plan dialogue strategies explicitly. Moreover, training external strategy planners doesn’t fully account for adversarial interactions, thereby limiting their effectiveness against tough resisters. In this paper, to mitigate the above issues, we propose <tex-math>\textbf{GAIA}</tex-math>, a <tex-math>\textbf{G}</tex-math>ame-based <tex-math>\textbf{A}</tex-math>dversarial self-play <tex-math>\textbf{I}</tex-math>nter<tex-math>\textbf{A}</tex-math>ctive training paradigm, which constructs an adversarial two-player (a persuader and a resister) zero-sum game and guides the game to approximate Nash Equilibrium (NE) via reinforcement learning (RL) for the non-collaborative dialogues. First, we design a Chain-of-Mind prompt to reason the resister’s dialogue act step-by-step to plan the persuasive strategies. Secondly, to adversarially improve the persuader, we construct diverse resistant planners and theoretically improve the persuader’s optimal lower bound. Finally, we iteratively optimise their policies via adversarial self-play interactive RL and design an <tex-math>\epsilon</tex-math>-NE verification algorithm to approximate the game’s NE. Experiments on three datasets show that our model obtains state-of-the-art performance.</abstract>
      <url hash="7a14ec16">2025.acl-long.184</url>
      <bibkey>wang-etal-2025-battling</bibkey>
    </paper>
    <paper id="185">
      <title>Cross-model Transferability among Large Language Models on the Platonic Representations of Concepts</title>
      <author><first>Youcheng</first><last>Huang</last></author>
      <author><first>Chen</first><last>Huang</last><affiliation>Sichuan University</affiliation></author>
      <author><first>Duanyu</first><last>Feng</last></author>
      <author><first>Wenqiang</first><last>Lei</last><affiliation>Sichuan University</affiliation></author>
      <author><first>Jiancheng</first><last>Lv</last><affiliation>Sichuan University</affiliation></author>
      <pages>3686-3704</pages>
      <abstract>Understanding the inner workings of Large Language Models (LLMs) is a critical research frontier. Prior research has shown that a single LLM’s concept representations can be captured as steering vectors (SVs), enabling the control of LLM behavior (e.g., towards generating harmful content). Our work takes a novel approach by exploring the intricate relationships between concept representations across different LLMs, drawing an intriguing parallel to Plato’s Allegory of the Cave. In particular, we introduce a linear transformation method to bridge these representations and present three key findings: 1) Concept representations across different LLMs can be effectively aligned using simple linear transformations, enabling efficient cross-model transfer and behavioral control via SVs. 2) This linear transformation generalizes across concepts, facilitating alignment and control of SVs representing different concepts across LLMs. 3) A weak-to-strong transferability exists between LLM concept representations, whereby SVs extracted from smaller LLMs can effectively control the behavior of larger LLMs. Our code is provided in the supplementary file and will be openly released.</abstract>
      <url hash="94860008">2025.acl-long.185</url>
      <bibkey>huang-etal-2025-cross</bibkey>
    </paper>
    <paper id="186">
      <title><fixed-case>F</fixed-case>old<fixed-case>M</fixed-case>o<fixed-case>E</fixed-case>: Efficient Long Sequence <fixed-case>M</fixed-case>o<fixed-case>E</fixed-case> Training via Attention-<fixed-case>M</fixed-case>o<fixed-case>E</fixed-case> Pipelining</title>
      <author><first>Guichao</first><last>Zhu</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Lintian</first><last>Lei</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Yuhao</first><last>Qing</last><affiliation>The University of Hong Kong</affiliation></author>
      <author><first>Yichao</first><last>Fu</last></author>
      <author><first>Fanxin</first><last>Li</last><affiliation>The University of Hong Kong</affiliation></author>
      <author><first>Dong</first><last>Huang</last></author>
      <author><first>Zekai</first><last>Sun</last><affiliation>the University of Hong Kong, University of Hong Kong</affiliation></author>
      <author><first>Heming</first><last>Cui</last><affiliation>the University of Hong Kong, University of Hong Kong</affiliation></author>
      <pages>3705-3717</pages>
      <abstract>Training LLMs with Mixture-of-Experts (MoE) architecture on long sequences poses significant challenges due to the all-to-all communication bottleneck of expert parallelism. While existing approaches attempt to hide the communication costs in computation through token-level pipelining within MoE layers, their effectiveness is limited by the insufficient computation. We present FoldMoE, a high-performance MoE training system that enables token-level overlapping across entire Transformer blocks through novel attention-MoE pipelining. We propose an efficient pipeline schedule, and a novel token buffering design to decouple attention and MoE layer partitioning, along with a time-uniform micro-batching strategy for enhanced efficiency. Evaluations on GPT-MoE models with sequences up to 32K tokens show that FoldMoE achieves up to 1.49x and 2.72x speedup over state-of-the-art token-level overlapping and non-overlapping baselines respectively.</abstract>
      <url hash="61dfe5b3">2025.acl-long.186</url>
      <bibkey>zhu-etal-2025-foldmoe</bibkey>
    </paper>
    <paper id="187">
      <title><fixed-case>L</fixed-case>ong<fixed-case>R</fixed-case>eward: Improving Long-context Large Language Models with <fixed-case>AI</fixed-case> Feedback</title>
      <author><first>Jiajie</first><last>Zhang</last></author>
      <author><first>Zhongni</first><last>Hou</last><affiliation>,Chinese Academy of Sciences</affiliation></author>
      <author><first>Xin</first><last>Lv</last><affiliation>Zhipu AI</affiliation></author>
      <author><first>Shulin</first><last>Cao</last><affiliation>Zhipu AI and Tsinghua University</affiliation></author>
      <author><first>Zhenyu</first><last>Hou</last></author>
      <author><first>Yilin</first><last>Niu</last></author>
      <author><first>Lei</first><last>Hou</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Yuxiao</first><last>Dong</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Ling</first><last>Feng</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <pages>3718-3739</pages>
      <abstract>Though significant advancements have been achieved in developing long-context large language models (LLMs), the compromised quality of LLM-synthesized data for supervised fine-tuning (SFT) often affects the long-context performance of SFT models and leads to inherent limitations. In principle, reinforcement learning (RL) with appropriate reward signals can further enhance models’ capacities. However, how to obtain reliable rewards in long-context scenarios remains unexplored. To this end, we propose <b>LongReward</b>, a novel method that utilizes an off-the-shelf LLM to provide rewards for long-context model responses from four human-valued dimensions: helpfulness, logicality, faithfulness, and completeness, each with a carefully designed assessment pipeline. By combining LongReward and offline RL algorithm DPO, we are able to effectively improve long-context SFT models. Our experiments indicate that LongReward not only significantly improves models’ long-context performance but also enhances their ability to follow short instructions. We also find that long-context DPO with LongReward and conventional short-context DPO can be used together without hurting either one’s performance.</abstract>
      <url hash="383cf4f6">2025.acl-long.187</url>
      <bibkey>zhang-etal-2025-longreward</bibkey>
    </paper>
    <paper id="188">
      <title>Influences on <fixed-case>LLM</fixed-case> Calibration: A Study of Response Agreement, Loss Functions, and Prompt Styles</title>
      <author><first>Yuxi</first><last>Xia</last></author>
      <author><first>Pedro Henrique</first><last>Luz De Araujo</last><affiliation>Universität Vienna</affiliation></author>
      <author><first>Klim</first><last>Zaporojets</last><affiliation>Aarhus University</affiliation></author>
      <author><first>Benjamin</first><last>Roth</last><affiliation>Universität Vienna</affiliation></author>
      <pages>3740-3761</pages>
      <abstract>Calibration, the alignment between model confidence and prediction accuracy, is critical for the reliable deployment of large language models (LLMs). Existing works neglect to measure the generalization of their methods to other prompt styles and different sizes of LLMs. To address this, we define a controlled experimental setting covering 12 LLMs and four prompt styles. We additionally investigate if incorporating the response agreement of multiple LLMs and an appropriate loss function can improve calibration performance. Concretely, we build Calib-n, a novel framework that trains an auxiliary model for confidence estimation that aggregates responses from multiple LLMs to capture inter-model agreement. To optimize calibration, we integrate focal and AUC surrogate losses alongside binary cross-entropy. Experiments across four datasets demonstrate that both response agreement and focal loss improve calibration from baselines. We find that few-shot prompts are the most effective for auxiliary model-based methods, and auxiliary models demonstrate robust calibration performance across accuracy variations, outperforming LLMs’ internal probabilities and verbalized confidences. These insights deepen the understanding of influence factors in LLM calibration, supporting their reliable deployment in diverse applications.</abstract>
      <url hash="533a4c2c">2025.acl-long.188</url>
      <bibkey>xia-etal-2025-influences</bibkey>
    </paper>
    <paper id="189">
      <title><fixed-case>UTB</fixed-case>oost: Rigorous Evaluation of Coding Agents on <fixed-case>SWE</fixed-case>-Bench</title>
      <author><first>Boxi</first><last>Yu</last></author>
      <author><first>Yuxuan</first><last>Zhu</last><affiliation>Department of Computer Science</affiliation></author>
      <author><first>Pinjia</first><last>He</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <author><first>Daniel</first><last>Kang</last></author>
      <pages>3762-3774</pages>
      <abstract>The advent of Large Language Models (LLMs) has spurred the development of coding agents for real-world code generation.As a widely used benchmark for evaluating the code generation capabilities of these agents, SWE-Bench uses real-world problems based on GitHub issues and their corresponding pull requests.However, the manually written test cases included in these pull requests are often insufficient, allowing generated patches to pass the tests without resolving the underlying issue.To address this challenge, we introduce UTGenerator, an LLM-driven test case generator that automatically analyzes codebases and dependencies to generate test cases for real-world Python projects.Building on UTGenerator, we propose UTBoost, a comprehensive framework for test case augmentation.In our evaluation, we identified 36 task instances with insufficient test cases and uncovered 345 erroneous patches incorrectly labeled as passed in the original SWE Bench.These corrections, impacting 40.9% of SWE-Bench Lite and 24.4% of SWE-Bench Verified leaderboard entries, yield 18 and 11 ranking changes, respectively.</abstract>
      <url hash="5e5f3209">2025.acl-long.189</url>
      <bibkey>yu-etal-2025-utboost</bibkey>
    </paper>
    <paper id="190">
      <title>Towards Better Evaluation for Generated Patent Claims</title>
      <author><first>Lekang</first><last>Jiang</last></author>
      <author><first>Pascal A.</first><last>Scherz</last></author>
      <author><first>Stefan</first><last>Goetz</last><affiliation>University of Cambridge and Duke University</affiliation></author>
      <pages>3775-3788</pages>
      <abstract>Patent claims define the scope of protection and establish the legal boundaries of an invention. Drafting these claims is a complex and time-consuming process that usually requires the expertise of skilled patent attorneys, which can form a large access barrier for many small enterprises. To solve these challenges, researchers have investigated large language models (LLMs) for automating patent claim generation. However, existing studies highlight inconsistencies between automated evaluation metrics and human expert assessments. To bridge this gap, we introduce Patent-CE, the first comprehensive benchmark for evaluating patent claims. Patent-CE includes comparative claim evaluations annotated by patent experts, focusing on five key criteria: feature completeness, conceptual clarity, terminology consistency, logical linkage, and overall quality. Additionally, we propose PatClaimEval, a novel multi-dimensional evaluation method specifically designed for patent claims. Our experiments demonstrate that PatClaimEval achieves the highest correlation with human expert evaluations across all assessment criteria among all tested metrics. This research provides the groundwork for more accurate evaluations of automated patent claim generation systems.</abstract>
      <url hash="072c1e8c">2025.acl-long.190</url>
      <bibkey>jiang-etal-2025-towards-better</bibkey>
    </paper>
    <paper id="191">
      <title>Fine-Tuning on Diverse Reasoning Chains Drives Within-Inference <fixed-case>C</fixed-case>o<fixed-case>T</fixed-case> Refinement in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Haritz</first><last>Puerto</last><affiliation>TU Darmstadt</affiliation></author>
      <author><first>Tilek</first><last>Chubakov</last></author>
      <author><first>Xiaodan</first><last>Zhu</last><affiliation>Queen’s University</affiliation></author>
      <author><first>Harish</first><last>Tayyar Madabushi</last><affiliation>University of Bath</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Institute for Computer Science, Artificial Intelligence and Technology, Mohamed bin Zayed University of Artificial Intelligence and Technische Universität Darmstadt</affiliation></author>
      <pages>3789-3808</pages>
      <abstract>Requiring a large language model (LLM) to generate intermediary reasoning steps, known as Chain of Thought (CoT), has been shown to be an effective way of boosting performance. Previous approaches have focused on generating multiple independent CoTs, combining them through ensembling or other post-hoc strategies to enhance reasoning. In this work, we introduce a novel approach where LLMs are fine-tuned to generate a sequence of Diverse Chains of Thought (DCoT) within a single inference step, which is fundamentally different from prior work that primarily operate on parallel CoT generations. DCoT allows LLMs to gain the ability to perform within-inference refinement of reasoning chains without requiring external feedback. Through a rigorous set of experiments spanning a wide range of tasks that require various reasoning types, we show that fine-tuning on DCoT improves performance over the CoT baseline across model families and scales (1.3B to 70B). These improvements are particularly impactful for tasks with a large result state space, such as those involving numeric answers. Our work is also significant because both quantitative analyses and manual evaluations reveal the observed gains stem from the models’ ability to refine an initial reasoning chain by generating a second, improved chain within the same inference step, demonstrating previously elusive self-improvement. Our code and data are publicly available.</abstract>
      <url hash="595c014f">2025.acl-long.191</url>
      <bibkey>puerto-etal-2025-fine</bibkey>
    </paper>
    <paper id="192">
      <title>Establishing Trustworthy <fixed-case>LLM</fixed-case> Evaluation via Shortcut Neuron Analysis</title>
      <author><first>Kejian</first><last>Zhu</last></author>
      <author><first>Shangqing</first><last>Tu</last></author>
      <author><first>Zhuoran</first><last>Jin</last></author>
      <author><first>Lei</first><last>Hou</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <author><first>Jun</first><last>Zhao</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <pages>3809-3822</pages>
      <abstract>The development of large language models (LLMs) depends on **trustworthy evaluation**. However, most current evaluations rely on public benchmarks, which are prone to data contamination issues that significantly compromise fairness. Previous researches have focused on constructing dynamic benchmarks to address contamination. However, continuously building new benchmarks is costly and cyclical.In this work, we aim to tackle contamination by analyzing the mechanisms of contaminated models themselves. Through our experiments, we discover that the overestimation of contaminated models is likely due to parameters acquiring shortcut solutions in training. We further propose a novel method for identifying shortcut neurons through **comparative and causal analysis**.Building on this, we introduce an evaluation method called **shortcut neuron patching** to suppress shortcut neurons. Experiments validate the effectiveness of our approach in mitigating contamination. Additionally, our evaluation results exhibit a strong linear correlation with MixEval, a recently released trustworthy benchmark, achieving a Spearman coefficient (<tex-math>\rho</tex-math>) exceeding 0.95. This high correlation indicates that our method closely reveals true capabilities of the models and is trustworthy. We conduct further experiments to demonstrate the generalizability of our method across various benchmarks and hyperparameter settings. **Code**: https://github.com/GaryStack/Trustworthy-Evaluation.</abstract>
      <url hash="e89687ca">2025.acl-long.192</url>
      <bibkey>zhu-etal-2025-establishing</bibkey>
    </paper>
    <paper id="193">
      <title>Do Large Language Models have an <fixed-case>E</fixed-case>nglish Accent? Evaluating and Improving the Naturalness of Multilingual <fixed-case>LLM</fixed-case>s</title>
      <author><first>Yanzhu</first><last>Guo</last></author>
      <author><first>Simone</first><last>Conia</last><affiliation>Sapienza University of Rome</affiliation></author>
      <author><first>Zelin</first><last>Zhou</last></author>
      <author><first>Min</first><last>Li</last><affiliation>Apple</affiliation></author>
      <author><first>Saloni</first><last>Potdar</last><affiliation>Apple</affiliation></author>
      <author><first>Henry</first><last>Xiao</last></author>
      <pages>3823-3838</pages>
      <abstract>Current Large Language Models (LLMs) are predominantly designed with English as the primary language, and even the few that are multilingual tend to exhibit strong English-centric biases. Much like speakers who might produce awkward expressions when learning a second language, LLMs often generate unnatural outputs in non-English languages, reflecting English-centric patterns in both vocabulary and grammar. Despite the importance of this issue, the naturalness of multilingual LLM outputs has received limited attention. In this paper, we address this gap by introducing novel automatic corpus-level metrics to assess the lexical and syntactic naturalness of LLM outputs in a multilingual context. Using our new metrics, we evaluate state-of-the-art LLMs on a curated benchmark in French and Chinese, revealing a tendency towards English-influenced patterns. To mitigate this issue, we also propose a simple and effective alignment method to improve the naturalness of an LLM in a target language and domain, achieving consistent improvements in naturalness without compromising the performance on general-purpose benchmarks. Our work highlights the importance of developing multilingual metrics, resources and methods for the new wave of multilingual LLMs.</abstract>
      <url hash="bbc42379">2025.acl-long.193</url>
      <bibkey>guo-etal-2025-large</bibkey>
    </paper>
    <paper id="194">
      <title>Enhancing Character-Level Understanding in <fixed-case>LLM</fixed-case>s through Token Internal Structure Learning</title>
      <author><first>Zhu</first><last>Xu</last></author>
      <author><first>Zhiqiang</first><last>Zhao</last><affiliation>Chongqing University of Post and Telecommunications</affiliation></author>
      <author><first>Zihan</first><last>Zhang</last></author>
      <author><first>Yuchi</first><last>Liu</last></author>
      <author><first>Quanwei</first><last>Shen</last></author>
      <author id="fei-liu"><first>Fei</first><last>Liu</last></author>
      <author><first>Yu</first><last>Kuang</last></author>
      <author><first>Jian</first><last>He</last></author>
      <author><first>Conglin</first><last>Liu</last></author>
      <pages>3839-3853</pages>
      <abstract>Tokenization methods like Byte-Pair Encoding (BPE) enhance computational efficiency in large language models (LLMs) but often obscure internal character structures within tokens. This limitation hinders LLMs’ ability to predict precise character positions, which is crucial in tasks like Chinese Spelling Correction (CSC) where identifying the positions of misspelled characters accelerates correction processes. We propose Token Internal Position Awareness (TIPA), a method that significantly improves models’ ability to capture character positions within tokens by training them on reverse character prediction tasks using the tokenizer’s vocabulary. Experiments demonstrate that TIPA enhances position prediction accuracy in LLMs, enabling more precise identification of target characters in original text. Furthermore, when applied to downstream tasks that do not require exact position prediction, TIPA still boosts performance in tasks needing character-level information, validating its versatility and effectiveness.</abstract>
      <url hash="49ff94da">2025.acl-long.194</url>
      <bibkey>xu-etal-2025-enhancing-character</bibkey>
    </paper>
    <paper id="195">
      <title>Conformity in Large Language Models</title>
      <author><first>Xiaochen</first><last>Zhu</last></author>
      <author><first>Caiqi</first><last>Zhang</last></author>
      <author><first>Tom</first><last>Stafford</last><affiliation>University of Sheffield</affiliation></author>
      <author><first>Nigel</first><last>Collier</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Andreas</first><last>Vlachos</last><affiliation>University of Cambridge</affiliation></author>
      <pages>3854-3872</pages>
      <abstract>The conformity effect describes the tendency of individuals to align their responses with the majority. Studying this bias in large language models (LLMs) is crucial, as LLMs are increasingly used in various information-seeking and decision-making tasks as conversation partners to improve productivity. Thus, conformity to incorrect responses can compromise their effectiveness. In this paper, we adapt psychological experiments to examine the extent of conformity in state-of-the-art LLMs. Our findings reveal that all models tested exhibit varying levels of conformity toward the majority, regardless of their initial choice or correctness, across different knowledge domains. Notably, we are the first to show that LLMs are more likely to conform when they are more uncertain in their own prediction. We further explore factors that influence conformity, such as training paradigms and input characteristics, finding that instruction-tuned models are less susceptible to conformity, while increasing the naturalness of majority tones amplifies conformity. Finally, we propose two interventions—Devil’s Advocate and Question Distillation—to mitigate conformity, providing insights into building more robust language models.</abstract>
      <url hash="6838fe57">2025.acl-long.195</url>
      <bibkey>zhu-etal-2025-conformity</bibkey>
    </paper>
    <paper id="196">
      <title>Interpret and Improve In-Context Learning via the Lens of Input-Label Mappings</title>
      <author><first>Chenghao</first><last>Sun</last></author>
      <author><first>Zhen</first><last>Huang</last></author>
      <author><first>Yonggang</first><last>Zhang</last></author>
      <author><first>Le</first><last>Lu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Houqiang</first><last>Li</last></author>
      <author><first>Xinmei</first><last>Tian</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Xu</first><last>Shen</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Jieping</first><last>Ye</last><affiliation>Alibaba Group</affiliation></author>
      <pages>3873-3895</pages>
      <abstract>Large language models (LLMs) excel at downstream NLP tasks through in-context learning (ICL) with a few demonstrations of input–label pairs. However, the internal mechanisms behind ICL remain under-explored, particularly the mappings between inputs and labels. In this work, we reverse-engineer ICL by examining input-label mappings: what they are within LLMs, where they function, and how LLMs utilize them. (1) what: We discover input-label mappings stored within a few specific layers in the form of principal components (PCs), which capture human-interpretable and task-related words. (2) where: We propose a PC patching approach to identify the modules where input-label mappings function. Specifically, PC patching automatically crafts counterfactual representations using identified semantic PCs, rather than manually designing counterfactual text, to suppress the behavior related to LLM capability for ICL-related modules. Utilizing PC patching, we identify LLMs apply input-label mappings in a small fraction of attention heads. (3) how: We observe and verify that the identified key heads utilize input-label mappings from demonstrations to generate target labels for new queries. Based on these discoveries, we further show that precisely fine-tuning key ICL-related modules leads to significant improvements across diverse tasks.</abstract>
      <url hash="d66a173d">2025.acl-long.196</url>
      <bibkey>sun-etal-2025-interpret</bibkey>
    </paper>
    <paper id="197">
      <title>Positional Overload: Positional Debiasing and Context Window Extension for Large Language Models using Set Encoding</title>
      <author><first>Lukas</first><last>Kinder</last><affiliation>Karlsruher Institut für Technologie</affiliation></author>
      <author><first>Lukas</first><last>Edman</last><affiliation>Technische Universität München</affiliation></author>
      <author><first>Alexander</first><last>Fraser</last><affiliation>Technical University of Munich</affiliation></author>
      <author><first>Tobias</first><last>Käfer</last><affiliation>Karlsruher Institut für Technologie</affiliation></author>
      <pages>3896-3908</pages>
      <abstract>Large Language Models (LLMs) typically track the order of tokens using positional encoding, which causes the following problems: positional bias, where the model is influenced by an ordering within the prompt, and a fixed context window, as models struggle to generalize to positions beyond those encountered during training. To address these limitations, we developed a novel method called <tex-math>\textit{set encoding}</tex-math>. This method allows multiple pieces of text to be encoded in the same position, thereby eliminating positional bias entirely. Another promising use case for set encoding is to increase the size of the input an LLM can handle. Our experiments demonstrate that set encoding allows an LLM to solve tasks with far more tokens than without set encoding. To our knowledge, set encoding is the first technique to effectively extend an LLM’s context window without requiring any additional training.</abstract>
      <url hash="475b6128">2025.acl-long.197</url>
      <bibkey>kinder-etal-2025-positional</bibkey>
    </paper>
    <paper id="198">
      <title><fixed-case>FR</fixed-case>-Spec: Accelerating Large-Vocabulary Language Models via Frequency-Ranked Speculative Sampling</title>
      <author><first>Weilin</first><last>Zhao</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Tengyu</first><last>Pan</last></author>
      <author><first>Xu</first><last>Han</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Yudi</first><last>Zhang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Sun</first><last>Ao</last></author>
      <author><first>Yuxiang</first><last>Huang</last></author>
      <author><first>Kaihuo</first><last>Zhang</last></author>
      <author><first>Weilun</first><last>Zhao</last><affiliation>ModelBest</affiliation></author>
      <author><first>Yuxuan</first><last>Li</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <author><first>Hao</first><last>Zhou</last><affiliation>Tencent</affiliation></author>
      <author><first>Jianyong</first><last>Wang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Maosong</first><last>Sun</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Zhiyuan</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <pages>3909-3921</pages>
      <abstract>Speculative sampling has emerged as an important technique for accelerating the auto-regressive generation process of large language models (LLMs) by utilizing a draft-then-verify mechanism to produce multiple tokens per forward pass. While state-of-the-art speculative sampling methods use only a single layer and a language modeling (LM) head as the draft model to achieve impressive layer compression, their efficiency gains are substantially reduced for large-vocabulary LLMs, such as Llama-3-8B with a vocabulary of 128k tokens. To address this, we present FR-Spec, a frequency-ranked speculative sampling framework that optimizes draft candidate selection through vocabulary space compression. By constraining the draft search to a frequency-prioritized token subset, our method reduces LM Head computation overhead by 75% while ensuring the equivalence of the final output distribution. Experiments across multiple datasets demonstrate an average of 1.12<tex-math>\times</tex-math> speedup over the state-of-the-art speculative sampling method EAGLE-2. Code is availableat https://github.com/thunlp/FR-Spec.</abstract>
      <url hash="1cbe3273">2025.acl-long.198</url>
      <bibkey>zhao-etal-2025-fr</bibkey>
    </paper>
    <paper id="199">
      <title><fixed-case>VR</fixed-case>e<fixed-case>ST</fixed-case>: Enhancing Reasoning in Large Vision-Language Models through Tree Search and Self-Reward Mechanism</title>
      <author><first>Congzhi</first><last>Zhang</last></author>
      <author><first>Jiawei</first><last>Peng</last><affiliation>Southeast University</affiliation></author>
      <author><first>Zhenglin</first><last>Wang</last></author>
      <author><first>Yilong</first><last>Lai</last><affiliation>Southeast University</affiliation></author>
      <author><first>Haowen</first><last>Sun</last><affiliation>Southeast University</affiliation></author>
      <author><first>Heng</first><last>Chang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Fei</first><last>Ma</last><affiliation>Guangming Laboratory</affiliation></author>
      <author><first>Weijiang</first><last>Yu</last></author>
      <pages>3922-3941</pages>
      <abstract>Large Vision-Language Models (LVLMs) have shown exceptional performance in multimodal tasks, but their effectiveness in complex visual reasoning is still constrained, especially when employing Chain-of-Thought prompting techniques. In this paper, we propose VReST, a novel training-free approach that enhances Reasoning in LVLMs through Monte Carlo Tree Search and Self-Reward mechanisms. VReST meticulously traverses the reasoning landscape by establishing a search tree, where each node encapsulates a reasoning step, and each path delineates a comprehensive reasoning sequence. Our innovative multimodal Self-Reward mechanism assesses the quality of reasoning steps by integrating the utility of sub-questions, answer correctness, and the relevance of vision-language clues, all without the need for additional models. VReST surpasses current prompting methods and secures state-of-the-art performance across three multimodal mathematical reasoning benchmarks. Furthermore, it substantiates the efficacy of test-time scaling laws in multimodal tasks, offering a promising direction for future research.</abstract>
      <url hash="62509d68">2025.acl-long.199</url>
      <bibkey>zhang-etal-2025-vrest</bibkey>
    </paper>
    <paper id="200">
      <title>Past Meets Present: Creating Historical Analogy with Large Language Models</title>
      <author><first>Nianqi</first><last>Li</last><affiliation>Fudan University</affiliation></author>
      <author><first>Siyu</first><last>Yuan</last></author>
      <author><first>Jiangjie</first><last>Chen</last><affiliation>ByteDance Seed</affiliation></author>
      <author><first>Jiaqing</first><last>Liang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Feng</first><last>Wei</last></author>
      <author><first>Zujie</first><last>Liang</last><affiliation>Ant Group</affiliation></author>
      <author><first>Deqing</first><last>Yang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Yanghua</first><last>Xiao</last><affiliation>Fudan University</affiliation></author>
      <pages>3942-3957</pages>
      <abstract>Historical analogies, which compare known past events with contemporary but unfamiliar events, are important abilities that help people make decisions and understand the world. However, research in applied history suggests that people have difficulty finding appropriate analogies. And previous studies in the AI community have also overlooked historical analogies. To fill this gap, in this paper, we focus on the historical analogy acquisition task, which aims to acquire analogous historical events for a given event. We explore retrieval and generation methods for acquiring historical analogies based on different large language models (LLMs). Furthermore, we propose a self-reflection method to mitigate hallucinations and stereotypes when LLMs generate historical analogies. Through human evaluations and our specially designed automatic multi-dimensional assessment, we find that LLMs generally have a good potential for historical analogies. And the performance of the models can be further improved by using our self-reflection method. Resources of this paper can be found at https://anonymous.4open.science/r/Historical-Analogy-of-LLMs-FC17</abstract>
      <url hash="b9df0798">2025.acl-long.200</url>
      <bibkey>li-etal-2025-past</bibkey>
    </paper>
    <paper id="201">
      <title>Meta-Reflection: A Feedback-Free Reflection Learning Framework</title>
      <author><first>Yaoke</first><last>Wang</last></author>
      <author><first>Yun</first><last>Zhu</last></author>
      <author><first>XintongBao</first><last>XintongBao</last></author>
      <author><first>Wenqiao</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Suyang</first><last>Dai</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Kehan</first><last>Chen</last></author>
      <author><first>Wenqiang</first><last>Li</last></author>
      <author><first>Gang</first><last>Huang</last></author>
      <author><first>Siliang</first><last>Tang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Yueting</first><last>Zhuang</last><affiliation>Zhejiang University</affiliation></author>
      <pages>3958-3976</pages>
      <abstract>Despite the remarkable capabilities of large language models (LLMs) in natural language understanding and reasoning, they often display undesirable behaviors, such as generating hallucinations and unfaithful reasoning. A prevalent strategy to mitigate these issues is the use of reflection, which refines responses through an iterative process. However, while promising, reflection heavily relies on high-quality external feedback and requires iterative multi-agent inference processes, thus hindering its practical application. In this paper, we propose Meta-Reflection, a novel feedback-free reflection mechanism that necessitates only a single inference pass without external feedback. Motivated by the human ability to remember and retrieve reflections from past experiences when encountering similar problems, Meta-Reflection integrates reflective insights into a codebook, allowing the historical insights to be stored, retrieved, and used to guide LLMs in problem-solving. To thoroughly investigate and evaluate the practicality of Meta-Reflection in real-world scenarios, we introduce an industrial e-commerce benchmark named E-commerce Customer Intent Detection. Extensive experiments conducted on both public datasets and the ECID benchmark highlight the effectiveness and efficiency of our proposed approach. Project is available at https://github.com/DCDmllm/Meta-Reflection</abstract>
      <url hash="4d21f2c8">2025.acl-long.201</url>
      <bibkey>wang-etal-2025-meta</bibkey>
    </paper>
    <paper id="202">
      <title>Read it in Two Steps: Translating Extremely Low-Resource Languages with Code-Augmented Grammar Books</title>
      <author><first>Chen</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Jiuheng</first><last>Lin</last></author>
      <author><first>Xiao</first><last>Liu</last></author>
      <author><first>Zekai</first><last>Zhang</last></author>
      <author><first>Yansong</first><last>Feng</last><affiliation>Peking University</affiliation></author>
      <pages>3977-3997</pages>
      <abstract>While large language models (LLMs) have shown promise in translating extremely low-resource languages using resources like dictionaries, the effectiveness of grammar books remains debated. This paper investigates the role of grammar books in translating extremely low-resource languages by decomposing it into two key steps: grammar rule retrieval and application. To facilitate the study, we introduce ZhuangRules, a modularized dataset of grammar rules and their corresponding test sentences. Our analysis reveals that rule retrieval constitutes a primary bottleneck in grammar-based translation. Moreover, although LLMs can apply simple rules for translation when explicitly provided, they encounter difficulties in handling more complex rules. To address these challenges, we propose representing grammar rules as code functions, considering their similarities in structure and the benefit of code in facilitating LLM reasoning. Our experiments show that using code rules significantly boosts both rule retrieval and application, ultimately resulting in a 13.1% BLEU improvement in translation.</abstract>
      <url hash="f379cb0a">2025.acl-long.202</url>
      <bibkey>zhang-etal-2025-read</bibkey>
    </paper>
    <paper id="203">
      <title>Confidence v.s. Critique: A Decomposition of Self-Correction Capability for <fixed-case>LLM</fixed-case>s</title>
      <author><first>Zhe</first><last>Yang</last><affiliation>Peking University</affiliation></author>
      <author><first>Yichang</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yudong</first><last>Wang</last><affiliation>Peking University</affiliation></author>
      <author><first>Ziyao</first><last>Xu</last><affiliation>Peking University</affiliation></author>
      <author><first>Junyang</first><last>Lin</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Zhifang</first><last>Sui</last><affiliation>Peking University</affiliation></author>
      <pages>3998-4014</pages>
      <abstract>Large Language Models (LLMs) can correct their self-generated responses, but a decline in accuracy after self-correction is also witnessed. To have a deeper understanding of self-correction, we endeavor to decompose, evaluate, and analyze the self-correction behaviors of LLMs. By enumerating and analyzing answer correctness before and after self-correction, we decompose the self-correction capability into confidence (being confident to correct answers) and critique (turning wrong answers to correct) capabilities, and propose two metrics from a probabilistic perspective to measure these 2 capabilities, along with another metric for overall self-correction capability evaluation. Based on our decomposition and evaluation metrics, we conduct extensive experiments and draw some empirical conclusions. For example, we find different models can exhibit distinct behaviors: some models are confident while others are more critical. We also find the trade-off between the two capabilities (i.e. improving one can lead to a decline in the other) when manipulating model self-correction behavior by prompts or in-context learning. Further, we find a simple yet efficient strategy to improve self-correction capability by transforming Supervision Fine-Tuning (SFT) data format, and our strategy outperforms vanilla SFT in both capabilities and achieves much higher accuracy after self-correction.</abstract>
      <url hash="c4afcfc4">2025.acl-long.203</url>
      <bibkey>yang-etal-2025-confidence</bibkey>
    </paper>
    <paper id="204">
      <title>Automating Legal Interpretation with <fixed-case>LLM</fixed-case>s: Retrieval, Generation, and Evaluation</title>
      <author><first>Kangcheng</first><last>Luo</last><affiliation>Peking University</affiliation></author>
      <author><first>Quzhe</first><last>Huang</last></author>
      <author><first>Cong</first><last>Jiang</last></author>
      <author><first>Yansong</first><last>Feng</last><affiliation>Peking University</affiliation></author>
      <pages>4015-4047</pages>
      <abstract>Interpreting the law is always essential for the law to adapt to the ever-changing society. It is a critical and challenging task even for legal practitioners, as it requires meticulous and professional annotations and summarizations by legal experts, which are admittedly time-consuming and expensive to collect at scale. To alleviate the burden on legal experts, we propose a method for automated legal interpretation. Specifically, by emulating doctrinal legal research, we introduce a novel framework, **ATRIE**, to address Legal Concept Interpretation, a typical task in legal interpretation. **ATRIE** utilizes large language models (LLMs) to **A**u**T**omatically **R**etrieve concept-related information, **I**nterpret legal concepts, and **E**valuate generated interpretations, eliminating dependence on legal experts. ATRIE comprises a legal concept interpreter and a legal concept interpretation evaluator. The interpreter uses LLMs to retrieve relevant information from previous cases and interpret legal concepts. The evaluator uses performance changes on Legal Concept Entailment, a downstream task we propose, as a proxy of interpretation quality. Automated and multifaceted human evaluations indicate that the quality of our interpretations is comparable to those written by legal experts, with superior comprehensiveness and readability. Although there remains a slight gap in accuracy, it can already assist legal practitioners in improving the efficiency of legal interpretation.</abstract>
      <url hash="10ba7a9c">2025.acl-long.204</url>
      <bibkey>luo-etal-2025-automating</bibkey>
    </paper>
    <paper id="205">
      <title>Visual Evidence Prompting Mitigates Hallucinations in Large Vision-Language Models</title>
      <author><first>Wei</first><last>Li</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Zhen</first><last>Huang</last></author>
      <author><first>Houqiang</first><last>Li</last></author>
      <author><first>Le</first><last>Lu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yang</first><last>Lu</last><affiliation>Xiamen University</affiliation></author>
      <author><first>Xinmei</first><last>Tian</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Xu</first><last>Shen</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Jieping</first><last>Ye</last><affiliation>Alibaba Group</affiliation></author>
      <pages>4048-4080</pages>
      <abstract>Large Vision-Language Models (LVLMs) have shown impressive progress by integrating visual perception with linguistic understanding to produce contextually grounded outputs. Despite these advancements achieved, LVLMs still suffer from the hallucination problem, e.g., they tend to produce content that does not exist in the input images. Our investigation suggests that such hallucinations often stem from the deficiencies in fine-grained comprehension on the visual aspect, particularly when visual scenes exhibit appearance or semantic similarities (e.g., bicycle vs. motorcycles, baseball bat vs. baseball). In this work, we show such hallucination is naturally mitigated via a novel method called visual evidence prompting, utilizing small visual models to complement the LVLMs. While traditional visual models are not adept at interacting with humans, they excel at perceiving the fine-grained image contents. By symbolizing the professional outputs of domain-expert models as prompts, the LVLM generalists are able to refer to these evidences as visual knowledge to generate more precise answers. Detailed analysis shows that visual evidence enables models to adjust and rectify the attribution and attention on the images, reducing visual confusion by suppressing false activation while enhancing correct ones. Extensive experiments and in-depth analysis demonstrate the effectiveness of our method. We hope our straightforward but insightful work enhances the comprehension of hallucination in LVLMs and offers valuable perspectives on addressing such challenges.</abstract>
      <url hash="a8868d04">2025.acl-long.205</url>
      <bibkey>li-etal-2025-visual</bibkey>
    </paper>
    <paper id="206">
      <title>Leveraging Dual Process Theory in Language Agent Framework for Real-time Simultaneous Human-<fixed-case>AI</fixed-case> Collaboration</title>
      <author><first>Shao</first><last>Zhang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Xihuai</first><last>Wang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Wenhao</first><last>Zhang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Chaoran</first><last>Li</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Junru</first><last>Song</last></author>
      <author><first>Tingyu</first><last>Li</last></author>
      <author><first>Lin</first><last>Qiu</last></author>
      <author><first>Xuezhi</first><last>Cao</last><affiliation>Meituan</affiliation></author>
      <author><first>Xunliang</first><last>Cai</last><affiliation>Meituan</affiliation></author>
      <author><first>Wen</first><last>Yao</last></author>
      <author><first>Weinan</first><last>Zhang</last></author>
      <author><first>Xinbing</first><last>Wang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Ying</first><last>Wen</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>4081-4108</pages>
      <abstract>Agents built on large language models (LLMs) have excelled in turn-by-turn human-AI collaboration but struggle with simultaneous tasks requiring real-time interaction. Latency issues and the challenge of inferring variable human strategies hinder their ability to make autonomous decisions without explicit instructions. Through experiments with current independent *System 1* and *System 2* methods, we validate the necessity of using Dual Process Theory (DPT) in real-time tasks. We propose DPT-Agent, a novel language agent framework that integrates *System 1* and *System 2* for efficient real-time simultaneous human-AI collaboration. DPT-Agent’s *System 1* uses a Finite-state Machine (FSM) and code-as-policy for fast, intuitive, and controllable decision-making. DPT-Agent’s *System 2* integrates Theory of Mind (ToM) and asynchronous reflection to infer human intentions and perform reasoning-based autonomous decisions. We demonstrate the effectiveness of DPT-Agent through further experiments with rule-based agents and human collaborators, showing significant improvements over mainstream LLM-based frameworks. To the best of our knowledge, DPT-Agent is the first language agent framework that achieves successful real-time simultaneous human-AI collaboration autonomously. Code of DPT-Agent can be found in https://github.com/sjtu-marl/DPT-Agent.</abstract>
      <url hash="47430035">2025.acl-long.206</url>
      <bibkey>zhang-etal-2025-leveraging</bibkey>
    </paper>
    <paper id="207">
      <title><fixed-case>T</fixed-case>ok<fixed-case>A</fixed-case>lign: Efficient Vocabulary Adaptation via Token Alignment</title>
      <author><first>Chong</first><last>Li</last><affiliation>Institute of automation, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jiajun</first><last>Zhang</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Chengqing</first><last>Zong</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <pages>4109-4126</pages>
      <abstract>Tokenization serves as a foundational step for Large Language Models (LLMs) to process text. In new domains or languages, the inefficiency of the tokenizer will slow down the training and generation of LLM. The mismatch in vocabulary also hinders deep knowledge transfer between LLMs like token-level distillation. To mitigate this gap, we propose an efficient method named **TokAlign** to replace the vocabulary of LLM from the token co-occurrences view, and further transfer the token-level knowledge between models. It first aligns the source vocabulary to the target one by learning a one-to-one mapping matrix for token IDs. Model parameters, including embeddings, are rearranged and progressively fine-tuned for the new vocabulary. Our method significantly improves multilingual text compression rates and vocabulary initialization for LLMs, decreasing the perplexity from <tex-math>{3.4e}^{2}</tex-math> of strong baseline methods to <tex-math>{1.2e}^{2}</tex-math> after initialization. Experimental results on models across multiple parameter scales demonstrate the effectiveness and generalization of TokAlign, which costs as few as 5k steps to restore the performance of the vanilla model. After unifying vocabularies between LLMs, token-level distillation can remarkably boost (+4.4% than sentence-level distillation) the base model, costing only 235M tokens.</abstract>
      <url hash="5775d1ee">2025.acl-long.207</url>
      <bibkey>li-etal-2025-tokalign</bibkey>
    </paper>
    <paper id="208">
      <title><fixed-case>A</fixed-case>da<fixed-case>E</fixed-case>dit: Advancing Continuous Knowledge Editing For Large Language Models</title>
      <author><first>Qi</first><last>Li</last></author>
      <author><first>Xiaowen</first><last>Chu</last><affiliation>Hong Kong University of Science and Technology (Guangzhou)</affiliation></author>
      <pages>4127-4149</pages>
      <abstract>Knowledge editing (KE) has emerged as a prominent alternative that enables efficient and precise information modification inside language models. However, a critical challenge arises in continuous language models editing — a significant performance decline both in knowledge update and retention when the number of edits increases. By dissecting the perturbation weight of language model in continuous KE, we uncover that disentangled and sparsified knowledge representation can significantly alleviate the performance decline. Building on these insights, we introduce AdaEdit, a novel knowledge editing method. Extensive empirical evaluations on multiple LLMs demonstrate that our proposed methods can enhance the performance of edited LLMs in large-size continuous editing regimes, outperforming existing ones without substantially compromising the general abilities of these models.</abstract>
      <url hash="2f30a91b">2025.acl-long.208</url>
      <bibkey>li-chu-2025-adaedit</bibkey>
    </paper>
    <paper id="209">
      <title>The Impact of Token Granularity on the Predictive Power of Language Model Surprisal</title>
      <author><first>Byung-Doh</first><last>Oh</last><affiliation>New York University</affiliation></author>
      <author><first>William</first><last>Schuler</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <pages>4150-4162</pages>
      <abstract>Word-by-word language model surprisal is often used to model the incremental processing of human readers, which raises questions about how various choices in language modeling influence its predictive power. One factor that has been overlooked in cognitive modeling is the granularity of subword tokens, which explicitly encodes information about word length and frequency, and ultimately influences the quality of vector representations that are learned. This paper presents experiments that manipulate the token granularity and evaluate its impact on the ability of surprisal to account for processing difficulty of naturalistic text and garden-path constructions. Experiments with naturalistic reading times reveal a substantial influence of token granularity on surprisal, with tokens defined by a vocabulary size of 8,000 resulting in surprisal that is most predictive. In contrast, on garden-path constructions, language models trained on coarser-grained tokens generally assigned higher surprisal to critical regions, suggesting a greater sensitivity to garden-path effects than previously reported. Taken together, these results suggest a large role of token granularity on the quality of language model surprisal for cognitive modeling.</abstract>
      <url hash="78c587bd">2025.acl-long.209</url>
      <bibkey>oh-schuler-2025-impact</bibkey>
    </paper>
    <paper id="210">
      <title>Segment-Level Diffusion: A Framework for Controllable Long-Form Generation with Diffusion Language Models</title>
      <author><first>Xiaochen</first><last>Zhu</last></author>
      <author><first>Georgi</first><last>Karadzhov</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Chenxi</first><last>Whitehouse</last><affiliation>Meta</affiliation></author>
      <author><first>Andreas</first><last>Vlachos</last><affiliation>University of Cambridge</affiliation></author>
      <pages>4163-4183</pages>
      <abstract>Diffusion models have shown promise in text generation, but often struggle with generating long, coherent, and contextually accurate text. Token-level diffusion doesn’t model word-order dependencies explicitly and operates on short, fixed output windows, while passage-level diffusion struggles with learning robust representations for long-form text. To address these challenges, we propose Segment-Level Diffusion (SLD), a framework that enhances diffusion-based text generation through text segmentation, robust representation training with adversarial and contrastive learning, and improved latent-space guidance. By segmenting long-form outputs into multiple latent representations and decoding them with an autoregressive decoder, SLD simplifies diffusion predictions and improves scalability. Experiments on four datasets demonstrate that, when compared to other diffusion and autoregressive baselines SLD achieves competitive or superior fluency, coherence, and contextual compatibility in automatic and human evaluations.</abstract>
      <url hash="c76bc0f9">2025.acl-long.210</url>
      <bibkey>zhu-etal-2025-segment</bibkey>
    </paper>
    <paper id="211">
      <title><fixed-case>BELLE</fixed-case>: A Bi-Level Multi-Agent Reasoning Framework for Multi-Hop Question Answering</title>
      <author><first>Taolin</first><last>Zhang</last><affiliation>Hefei University of Technology</affiliation></author>
      <author><first>Dongyang</first><last>Li</last><affiliation>Shanghai University of Electric Power</affiliation></author>
      <author><first>Qizhou</first><last>Chen</last></author>
      <author><first>Chengyu</first><last>Wang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Xiaofeng</first><last>He</last><affiliation>East China Normal University</affiliation></author>
      <pages>4184-4202</pages>
      <abstract>Multi-hop question answering (QA) involves finding multiple relevant passages and performing step-by-step reasoning to answer complex questions. Previous works on multi-hop QA employ specific methods from different modeling perspectives based on large language models (LLMs), regardless of the question types. In this paper, we first conduct an in-depth analysis of public multi-hop QA benchmarks, dividing the questions into four types and evaluating five types of cutting-edge methods for multi-hop QA: Chain-of-Thought (CoT), Single-step, Iterative-step, Sub-step, and Adaptive-step. We find that different types of multi-hop questions have varying degrees of sensitivity to different types of methods. Thus, we propose a Bi-levEL muLti-agEnt reasoning (BELLE) framework to address multi-hop QA by specifically focusing on the correspondence between question types and methods, where each type of method is regarded as an ”operator” by prompting LLMs differently. The first level of BELLE includes multiple agents that debate to obtain an executive plan of combined ”operators” to address the multi-hop QA task comprehensively. During the debate, in addition to the basic roles of affirmative debater, negative debater, and judge, at the second level, we further leverage fast and slow debaters to monitor whether changes in viewpoints are reasonable. Extensive experiments demonstrate that BELLE significantly outperforms strong baselines in various datasets. Additionally, the model consumption of BELLE is higher cost-effectiveness than that of single models in more complex multi-hop QA scenarios.</abstract>
      <url hash="13598f7b">2025.acl-long.211</url>
      <bibkey>zhang-etal-2025-belle</bibkey>
    </paper>
    <paper id="212">
      <title>Dynamic and Generalizable Process Reward Modeling</title>
      <author><first>Zhangyue</first><last>Yin</last></author>
      <author><first>Qiushi</first><last>Sun</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Zhiyuan</first><last>Zeng</last></author>
      <author><first>Qinyuan</first><last>Cheng</last></author>
      <author><first>Xipeng</first><last>Qiu</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xuanjing</first><last>Huang</last><affiliation>Fudan University</affiliation></author>
      <pages>4203-4233</pages>
      <abstract>Process Reward Models (PRMs) are crucial for guiding Large Language Models (LLMs) in complex scenarios by providing dense reward signals. However, existing PRMs primarily rely on heuristic approaches, which struggle with cross-domain generalization. While LLM-as-judge has been proposed to provide generalized rewards, current research has focused mainly on feedback results, overlooking the meaningful guidance embedded within the text. Additionally, static and coarse-grained evaluation criteria struggle to adapt to complex process supervision. To tackle these challenges, we propose Dynamic and Generalizable Process Reward Modeling (DG-PRM), which features a reward tree to capture and store fine-grained, multi-dimensional reward criteria. DG-PRM dynamically selects reward signals for step-wise reward scoring. To handle multifaceted reward signals, we pioneeringly adopt Pareto dominance estimation to identify discriminative positive and negative pairs. Experimental results show that DG-PRM achieves stunning performance on prevailing benchmarks, significantly boosting model performance across tasks with dense rewards. Further analysis reveals that DG-PRM adapts well to out-of-distribution scenarios, demonstrating exceptional generalizability.</abstract>
      <url hash="b40a2ca8">2025.acl-long.212</url>
      <bibkey>yin-etal-2025-dynamic</bibkey>
    </paper>
    <paper id="213">
      <title><fixed-case>A</fixed-case>dam<fixed-case>M</fixed-case>eme: Adaptively Probe the Reasoning Capacity of Multimodal Large Language Models on Harmfulness</title>
      <author><first>Zixin</first><last>Chen</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Hongzhan</first><last>Lin</last><affiliation>Hong Kong Baptist University</affiliation></author>
      <author><first>Kaixin</first><last>Li</last></author>
      <author><first>Ziyang</first><last>Luo</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Zhen</first><last>Ye</last></author>
      <author><first>Guang</first><last>Chen</last></author>
      <author><first>Zhiyong</first><last>Huang</last><affiliation>NUS School of Computing</affiliation></author>
      <author><first>Jing</first><last>Ma</last><affiliation>Hong Kong Baptist University</affiliation></author>
      <pages>4234-4253</pages>
      <abstract>The proliferation of multimodal memes in the social media era demands that multimodal Large Language Models (mLLMs) effectively understand meme harmfulness. Existing benchmarks for assessing mLLMs on harmful meme understanding rely on accuracy-based, model-agnostic evaluations using static datasets. These benchmarks are limited in their ability to provide up-to-date and thorough assessments, as online memes evolve dynamically. To address this, we propose AdamMeme, a flexible, agent-based evaluation framework that adaptively probes the reasoning capabilities of mLLMs in deciphering meme harmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive evaluations by iteratively updating the meme data with challenging samples, thereby exposing specific limitations in how mLLMs interpret harmfulness. Extensive experiments show that our framework systematically reveals the varying performance of different target mLLMs, offering in-depth, fine-grained analyses of model-specific weaknesses. Our code is available at https://github.com/Lbotirx/AdamMeme.</abstract>
      <url hash="6ca9235e">2025.acl-long.213</url>
      <bibkey>chen-etal-2025-adammeme</bibkey>
    </paper>
    <paper id="214">
      <title>Towards Text-Image Interleaved Retrieval</title>
      <author><first>Xin</first><last>Zhang</last><affiliation>Hong Kong Polytechnic University and Harbin Institute of Technology, Shenzhen</affiliation></author>
      <author><first>Ziqi</first><last>Dai</last></author>
      <author id="yongqi-li-hk"><first>Yongqi</first><last>Li</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Yanzhao</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Dingkun</first><last>Long</last></author>
      <author><first>Pengjun</first><last>Xie</last></author>
      <author><first>Meishan</first><last>Zhang</last><affiliation>Harbin Institute of Technology (Shenzhen), China</affiliation></author>
      <author><first>Jun</first><last>Yu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Wenjie</first><last>Li</last><affiliation>The Hong Kong Polytechnic University, The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>4254-4269</pages>
      <abstract>Current multimodal information retrieval studies mainly focus on single-image inputs, which limits real-world applications involving multiple images and text-image interleaved content. In this work, we introduce the text-image interleaved retrieval (TIIR) task, where the query and document are interleaved text-image sequences, and the model is required to understand the semantics from the interleaved context for effective retrieval. We construct a TIIR benchmark based on naturally interleaved wikiHow tutorials, where a specific pipeline is designed to generate interleaved queries. To explore the task, we adapt several off-the-shelf retrievers and build a dense baseline by interleaved multimodal large language model (MLLM). We then propose a novel Matryoshka Multimodal Embedder (MME), which compresses the number of visual tokens at different granularity, to address the challenge of excessive visual tokens in MLLM-based TIIR models. Experiments demonstrate that simple adaption of existing models does not consistently yield effective results. Our MME achieves significant improvements over the baseline by substantially fewer visual tokens. We provide extensive analysis and will release the dataset and code to facilitate future research.</abstract>
      <url hash="0daba913">2025.acl-long.214</url>
      <bibkey>zhang-etal-2025-towards</bibkey>
    </paper>
    <paper id="215">
      <title>Large Margin Representation Learning for Robust Cross-lingual Named Entity Recognition</title>
      <author><first>Guangcheng</first><last>Zhu</last></author>
      <author><first>Ruixuan</first><last>Xiao</last></author>
      <author><first>Haobo</first><last>Wang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Zhen</first><last>Zhu</last><affiliation>Zhejiang Sci-Tech University, Zhejiang Sci-Tech University, Beijing Jiaotong University and Yixiaomo.Inc</affiliation></author>
      <author><first>Gengyu</first><last>Lyu</last></author>
      <author><first>Junbo</first><last>Zhao</last><affiliation>Zhejiang University</affiliation></author>
      <pages>4270-4291</pages>
      <abstract>Cross-lingual named entity recognition (NER) aims to build an NER model that generalizes to the low-resource target language with labeled data from the high-resource source language. Current state-of-the-art methods typically combine self-training mechanism with contrastive learning paradigm, in order to develop discriminative entity clusters for cross-lingual adaptation. Despite the promise, we identify that these methods neglect two key problems: distribution skewness and pseudo-label bias, leading to indistinguishable entity clusters with small margins. To this end, we propose a novel framework, MARAL, which optimizes an adaptively reweighted contrastive loss to handle the class skewness and theoretically guarantees the optimal feature arrangement with maximum margin. To further mitigate the adverse effects of unreliable pseudo-labels, MARAL integrates a progressive cross-lingual adaptation strategy, which first selects reliable samples as anchors and then refines the remaining unreliable ones. Extensive experiments demonstrate that MARAL significantly outperforms the current state-of-the-art methods on multiple benchmarks, e.g., +2.04% on the challenging MultiCoNER dataset.</abstract>
      <url hash="bebcb647">2025.acl-long.215</url>
      <bibkey>zhu-etal-2025-large</bibkey>
    </paper>
    <paper id="216">
      <title>An Efficient and Precise Training Data Construction Framework for Process-supervised Reward Model in Mathematical Reasoning</title>
      <author><first>Wei</first><last>Sun</last></author>
      <author><first>Qianlong</first><last>Du</last></author>
      <author><first>Fuwei</first><last>Cui</last></author>
      <author><first>Jiajun</first><last>Zhang</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <pages>4292-4305</pages>
      <abstract>Enhancing the mathematical reasoning capabilities of Large Language Models (LLMs) is of great scientific and practical significance. Researchers typically employ process-supervised reward models (PRMs) to guide the reasoning process, effectively improving the models’ reasoning abilities. However, existing methods for constructing process supervision training data, such as manual annotation and per-step Monte Carlo estimation, are often costly or suffer from poor quality. To address these challenges, this paper introduces a framework called EpicPRM (Efficient, Precise, Cheap), which annotates each intermediate reasoning step based on its quantified contribution and uses an adaptive binary search algorithm to enhance both annotation precision and efficiency. Using this approach, we efficiently construct a high-quality process supervision training dataset named Epic50k, consisting of 50k annotated intermediate steps. Compared to other publicly available datasets, the PRM trained on Epic50k demonstrates significantly superior performance.</abstract>
      <url hash="3505e5a5">2025.acl-long.216</url>
      <bibkey>sun-etal-2025-efficient</bibkey>
    </paper>
    <paper id="217">
      <title><fixed-case>QAE</fixed-case>ncoder: Towards Aligned Representation Learning in Question Answering Systems</title>
      <author><first>Zhengren</first><last>Wang</last><affiliation>Peking University</affiliation></author>
      <author><first>Qinhan</first><last>Yu</last></author>
      <author><first>Shida</first><last>Wei</last></author>
      <author><first>Zhiyu</first><last>Li</last></author>
      <author><first>Feiyu</first><last>Xiong</last><affiliation>Institute for Advanced Algorithms Research, Shanghai</affiliation></author>
      <author><first>Xiaoxing</first><last>Wang</last><affiliation>IAAR</affiliation></author>
      <author><first>Simin</first><last>Niu</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Hao</first><last>Liang</last></author>
      <author><first>Wentao</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <pages>4306-4332</pages>
      <abstract>Modern QA systems entail retrieval-augmented generation (RAG) for accurate and trustworthy responses. However, the inherent gap between user queries and relevant documents hinders precise matching. We introduce QAEncoder, a training-free approach to bridge this gap. Specifically, QAEncoder estimates the expectation of potential queries in the embedding space as a robust surrogate for the document embedding, and attaches document fingerprints to effectively distinguish these embeddings. Extensive experiments across diverse datasets, languages, and embedding models confirmed QAEncoder’s alignment capability, which offers a simple-yet-effective solution with zero additional index storage, retrieval latency, training costs, or catastrophic forgetting and hallucination issues. The repository is publicly available at https://github.com/IAAR-Shanghai/QAEncoder.</abstract>
      <url hash="73cddee1">2025.acl-long.217</url>
      <bibkey>wang-etal-2025-qaencoder</bibkey>
    </paper>
    <paper id="218">
      <title>Game Development as Human-<fixed-case>LLM</fixed-case> Interaction</title>
      <author><first>Jiale</first><last>Hong</last></author>
      <author><first>Hongqiu</first><last>Wu</last></author>
      <author><first>Hai</first><last>Zhao</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>4333-4354</pages>
      <abstract>Game development is a highly specialized task that relies on a complex game engine powered by complex programming languages, preventing many gaming enthusiasts from handling it. This paper introduces the <tex-math>\textit{Chat Game Engine (ChatGE)}</tex-math> powered by LLM, which allows everyone to develop a custom game using natural language through Human-LLM interaction. To enable an LLM to function as a ChatGE, we instruct it to perform the following processes in each turn: (1) <tex-math>P_{script}</tex-math>: configure the game script segment based on the user’s input; (2) <tex-math>P_{code}</tex-math>: generate the corresponding code snippet based on the game script segment; (3) <tex-math>P_{utter}</tex-math>: interact with the user, including guidance and feedback. We propose a data synthesis pipeline based on LLM to generate game script-code pairs and interactions from a few manually crafted seed data. We propose a three-stage training strategy following curriculum learning principles to transfer the dialogue-based LLM to our ChatGE smoothly. We construct a ChatGE for poker games as a case study and comprehensively evaluate it from two perspectives: interaction quality and code correctness.</abstract>
      <url hash="16881fd5">2025.acl-long.218</url>
      <bibkey>hong-etal-2025-game</bibkey>
    </paper>
    <paper id="219">
      <title>Can <fixed-case>LLM</fixed-case>s Simulate <fixed-case>L</fixed-case>2-<fixed-case>E</fixed-case>nglish Dialogue? An Information-Theoretic Analysis of <fixed-case>L</fixed-case>1-Dependent Biases</title>
      <author><first>Rena Wei</first><last>Gao</last></author>
      <author><first>Xuetong</first><last>Wu</last></author>
      <author><first>Tatsuki</first><last>Kuribayashi</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Mingrui</first><last>Ye</last><affiliation>King’s College London, University of London</affiliation></author>
      <author><first>Siya</first><last>Qi</last></author>
      <author><first>Carsten</first><last>Roever</last><affiliation>University of Melbourne</affiliation></author>
      <author><first>Yuanxing</first><last>Liu</last></author>
      <author><first>Zheng</first><last>Yuan</last><affiliation>University of Sheffield</affiliation></author>
      <author><first>Jey Han</first><last>Lau</last><affiliation>The University of Melbourne</affiliation></author>
      <pages>4355-4379</pages>
      <abstract>This study evaluates Large Language Models’ (LLMs) ability to simulate non-native-like English use observed in human second language (L2) learners interfered with by their native first language (L1). In dialogue-based interviews, we prompt LLMs to mimic L2 English learners with specific L1s (e.g., Japanese, Thai, Urdu) across seven languages, comparing their outputs to real L2 learner data. Our analysis examines L1-driven linguistic biases, such as reference word usage and avoidance behaviors, using information-theoretic and distributional density measures. Results show that modern LLMs (e.g., Qwen2.5, LLAMA3, DeepseekV3, GPT 4o) replicate L1-dependent patterns observed in human L2 data, with distinct influences from various languages (e.g., Japanese, Korean, and Mandarin significantly affect tense agreement, and Urdu influences noun-verb collocations). Our results reveal LLMs’ potential for L2 dialogue generation and evaluation for future educational applications.</abstract>
      <url hash="679b9a65">2025.acl-long.219</url>
      <bibkey>gao-etal-2025-llms</bibkey>
    </paper>
    <paper id="220">
      <title><fixed-case>D</fixed-case>eep<fixed-case>S</fixed-case>olution: Boosting Complex Engineering Solution Design via Tree-based Exploration and Bi-point Thinking</title>
      <author><first>Zhuoqun</first><last>Li</last></author>
      <author><first>Haiyang</first><last>Yu</last></author>
      <author><first>Xuanang</first><last>Chen</last></author>
      <author><first>Hongyu</first><last>Lin</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yaojie</first><last>Lu</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Fei</first><last>Huang</last><affiliation>Alibaba Group US</affiliation></author>
      <author><first>Xianpei</first><last>Han</last><affiliation>Institute of Software, CAS</affiliation></author>
      <author><first>Yongbin</first><last>Li</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Le</first><last>Sun</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <pages>4380-4396</pages>
      <abstract>Designing solutions for complex engineering challenges is crucial in human production activities. However, previous research in the retrieval-augmented generation (RAG) field has not sufficiently addressed tasks related to the design of complex engineering solutions. To fill this gap, we introduce a new benchmark, SolutionBench, to evaluate a system’s ability to generate complete and feasible solutions for engineering problems with multiple complex constraints. To further advance the design of complex engineering solutions, we propose a novel system, SolutionRAG, that leverages the tree-based exploration and bi-point thinking mechanism to generate reliable solutions. Extensive experimental results demonstrate that SolutionRAG achieves state-of-the-art (SOTA) performance on the SolutionBench, highlighting its potential to enhance the automation and reliability of complex engineering solution design in real-world applications.</abstract>
      <url hash="ab93a9de">2025.acl-long.220</url>
      <bibkey>li-etal-2025-deepsolution</bibkey>
    </paper>
    <paper id="221">
      <title><fixed-case>S</fixed-case>urvey<fixed-case>P</fixed-case>ilot: an Agentic Framework for Automated Human Opinion Collection from Social Media</title>
      <author><first>Viet Thanh</first><last>Pham</last><affiliation>Monash University</affiliation></author>
      <author><first>Lizhen</first><last>Qu</last><affiliation>Monash University</affiliation></author>
      <author><first>Zhuang</first><last>Li</last><affiliation>Royal Melbourne Institute of Technology</affiliation></author>
      <author><first>Suraj</first><last>Sharma</last><affiliation>Calvin College</affiliation></author>
      <author><first>Gholamreza</first><last>Haffari</last><affiliation>Monash University, Monash University and Monash University</affiliation></author>
      <pages>4397-4422</pages>
      <abstract>Opinion survey research is a crucial method used by social scientists for understanding societal beliefs and behaviors. Traditional methodologies often entail high costs and limited scalability, while current automated methods such as opinion synthesis exhibit severe biases and lack traceability. In this paper, we introduce SurveyPilot, a novel finite-state orchestrated agentic framework that automates the collection and analysis of human opinions from social media platforms. SurveyPilot addresses the limitations of pioneering approaches by (i) providing transparency and traceability in each state of opinion collection and (ii) incorporating several techniques for mitigating biases, notably with a novel genetic algorithm for improving result diversity. Our extensive experiments reveal that SurveyPilot achieves a close alignment with authentic survey results across multiple domains, observing average relative improvements of 68,98% and 51,37% when comparing to opinion synthesis and agent-based approaches. Implementation of SurveyPilot is available on https://github.com/thanhpv2102/SurveyPilot.</abstract>
      <url hash="685b0cee">2025.acl-long.221</url>
      <bibkey>pham-etal-2025-surveypilot</bibkey>
    </paper>
    <paper id="222">
      <title>Sharper and Faster mean Better: Towards More Efficient Vision-Language Model for Hour-scale Long Video Understanding</title>
      <author><first>Daoze</first><last>Zhang</last><affiliation>Alibaba Cloud</affiliation></author>
      <author><first>Yuze</first><last>Zhao</last></author>
      <author><first>Jintao</first><last>Huang</last></author>
      <author><first>Yingda</first><last>Chen</last></author>
      <pages>4423-4439</pages>
      <abstract>Despite existing multimodal language models showing impressive performance on the video understanding task, extremely long videos still pose significant challenges to language model’s context length, memory consumption, and computational complexity. To address these issues, we propose a vision-language model named Sophia for long video understanding, which can efficiently handle hour-scale long videos. First, we employ a Shot-adaptive Frame Pruning technique, which naturally segments long videos into multiple camera shots, to more sharply identify and focus on the frames relevant to the query. Additionally, we introduce a Hierarchical Attention mechanism to effectively model the long-term temporal dependencies between video frames, which achieves a time and space complexity of O(N) w.r.t. the input sequence length N while theoretically maintaining the global modeling efficiency. Experimentally, our Sophia exhibits competitive performance compared to existing video understanding baselines across various benchmarks for long video understanding with reduced time and memory consumption. The model code and weights are available at https://huggingface.co/Tao-tse/Sophia.</abstract>
      <url hash="d0e02a89">2025.acl-long.222</url>
      <bibkey>zhang-etal-2025-sharper</bibkey>
    </paper>
    <paper id="223">
      <title>Auto-Arena: Automating <fixed-case>LLM</fixed-case> Evaluations with Agent Peer Battles and Committee Discussions</title>
      <author><first>Ruochen</first><last>Zhao</last></author>
      <author><first>Wenxuan</first><last>Zhang</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Yew Ken</first><last>Chia</last></author>
      <author><first>Weiwen</first><last>Xu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Deli</first><last>Zhao</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Lidong</first><last>Bing</last><affiliation>Shanda Group and Alibaba Group</affiliation></author>
      <pages>4440-4463</pages>
      <abstract>As LLMs continuously evolve, there is an urgent need for a reliable evaluation method that delivers trustworthy results promptly. Currently, static benchmarks suffer from inflexibility and unreliability, leading users to prefer human voting platforms like Chatbot Arena. However, human evaluations require significant manual effort. Therefore, we propose Auto-Arena, an innovative framework that automates the entire evaluation process using LLM-powered agents. Firstly, an LLM examiner generates questions. Then, two LLM candidates engage in a multi-round peer battle based on the questions, aiming at revealing their true performance differences. Finally, a committee of LLM judges collaboratively discusses and decides the winner, reducing bias and enhancing fairness. During the peer battles, we observe intriguing scenarios where the LLM candidates display competitive behaviors and learn from the opponents. In our extensive experiments involving 15 recent LLMs, Auto-Arena shows a 92.14% correlation with human preferences, surpassing all previous expert-annotated benchmarks without any manual efforts. Auto-Arena offers a promising alternative to current human evaluation platforms for evaluating LLMs automatically.</abstract>
      <url hash="afb275d3">2025.acl-long.223</url>
      <bibkey>zhao-etal-2025-auto</bibkey>
    </paper>
    <paper id="224">
      <title>How Humans and <fixed-case>LLM</fixed-case>s Organize Conceptual Knowledge: Exploring Subordinate Categories in <fixed-case>I</fixed-case>talian</title>
      <author><first>Andrea</first><last>Pedrotti</last></author>
      <author><first>Giulia</first><last>Rambelli</last><affiliation>University of Bologna</affiliation></author>
      <author><first>Caterina</first><last>Villani</last></author>
      <author><first>Marianna</first><last>Bolognesi</last></author>
      <pages>4464-4482</pages>
      <abstract>People can categorize the same entity at multiple taxonomic levels, such as basic (bear), superordinate (animal), and subordinate (grizzly bear). While prior research has focused on basic-level categories, this study is the first attempt to examine the organization of categories by analyzing exemplars produced at the subordinate level. We present a new Italian psycholinguistic dataset of human-generated exemplars for 187 concrete words. We then leverage these data to evaluate whether textual and vision LLMs produce meaningful exemplars that align with human category organization across three key tasks: exemplar generation, category induction, and typicality judgment. Our findings show a low alignment between humans and LLMs, consistent with previous studies. However, their performance varies notably across different semantic domains. Ultimately, this study highlights both the promises and the constraints of using AI-generated exemplars to support psychological and linguistic research.</abstract>
      <url hash="8f26adf2">2025.acl-long.224</url>
      <bibkey>pedrotti-etal-2025-humans</bibkey>
    </paper>
    <paper id="225">
      <title><fixed-case>PTQ</fixed-case>1.61: Push the Real Limit of Extremely Low-Bit Post-Training Quantization Methods for Large Language Models</title>
      <author><first>Jiaqi</first><last>Zhao</last></author>
      <author><first>Miao</first><last>Zhang</last><affiliation>Harbin Institute of Technology (Shenzhen)</affiliation></author>
      <author><first>Ming</first><last>Wang</last></author>
      <author><first>Yuzhang</first><last>Shang</last><affiliation>University of Central Florida</affiliation></author>
      <author><first>Kaihao</first><last>Zhang</last><affiliation>Harbin Institute of Technology and Australian National University</affiliation></author>
      <author><first>Weili</first><last>Guan</last><affiliation>Harbin Institute of Technology (Shenzhen)</affiliation></author>
      <author><first>Yaowei</first><last>Wang</last><affiliation>Harbin Institute of Technology, Shenzhen and Pengcheng Laboratory</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>4483-4502</pages>
      <abstract>Large Language Models (LLMs) suffer severe performance degradation when facing extremely low-bit (sub 2-bit) quantization. Several existing sub 2-bit post-training quantization (PTQ) methods utilize a mix-precision scheme by leveraging an unstructured fine-grained mask to explicitly distinguish salient weights, while which introduces an extra 1-bit or more per weight. To explore the real limit of PTQ, we propose an extremely low-bit PTQ method called PTQ1.61, which enables weight quantization to 1.61-bit for the first time. Specifically, we first introduce a one-dimensional structured mask with negligibly additional 0.0002-bit per weight based on input activations from the perspective of reducing the upper bound of quantization error to allocate corresponding salient weight channels to 4-bit. For non-salient channels binarization, an efficient block-wise scaling factors optimization framework is then presented to take implicit row-wise correlations and angular biases into account. Different from prior works that concentrate on adjusting quantization methodologies, we further propose a novel paradigm called quantization preprocessing, where we argue that transforming the weight distribution of the pretrained model before quantization can alleviate the difficulty in per-channel extremely low-bit PTQ. Extensive experiments indicate our PTQ1.61 achieves state-of-the-art performance in extremely low-bit quantization. Codes are available at https://github.com/zjq0455/PTQ1.61.</abstract>
      <url hash="215d464e">2025.acl-long.225</url>
      <bibkey>zhao-etal-2025-ptq1</bibkey>
    </paper>
    <paper id="226">
      <title><fixed-case>P</fixed-case>roto<fixed-case>L</fixed-case>ens: Advancing Prototype Learning for Fine-Grained Interpretability in Text Classification</title>
      <author><first>Bowen</first><last>Wei</last></author>
      <author><first>Ziwei</first><last>Zhu</last><affiliation>George Mason University</affiliation></author>
      <pages>4503-4523</pages>
      <abstract>In this work, we propose ProtoLens, a novel prototype-based model that provides fine-grained, sub-sentence level interpretability for text classification. ProtoLens uses a Prototype-aware Span Extraction module to identify relevant text spans associated with learned prototypes and a Prototype Alignment mechanism to ensure prototypes are semantically meaningful throughout training. By aligning the prototype embeddings with human-understandable examples, ProtoLens provides interpretable predictions while maintaining competitive accuracy. Extensive experiments demonstrate that ProtoLens outperforms both prototype-based and non-interpretable baselines on multiple text classification benchmarks. Code and data are available at <url>https://github.com/weibowen555/ProtoLens</url>.</abstract>
      <url hash="b8deecaa">2025.acl-long.226</url>
      <bibkey>wei-zhu-2025-protolens</bibkey>
    </paper>
    <paper id="227">
      <title>Fine-grained Video Dubbing Duration Alignment with Segment Supervised Preference Optimization</title>
      <author><first>Chaoqun</first><last>Cui</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Liangbin</first><last>Huang</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Shijing</first><last>Wang</last></author>
      <author><first>Zhe</first><last>Tong</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Zhaolong</first><last>Huang</last></author>
      <author><first>Xiao</first><last>Zeng</last></author>
      <author><first>Xiaofeng</first><last>Liu</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <pages>4524-4546</pages>
      <abstract>Video dubbing aims to translate original speech in visual media programs from the source language to the target language, relying on neural machine translation and text-to-speech technologies. Due to varying information densities across languages, target speech often mismatches the source speech duration, causing audio-video synchronization issues that significantly impact viewer experience. In this study, we approach duration alignment in LLM-based video dubbing machine translation as a preference optimization problem. We propose the Segment Supervised Preference Optimization (SSPO) method, which employs a segment-wise sampling strategy and fine-grained loss to mitigate duration mismatches between source and target lines. Experimental results demonstrate that SSPO achieves superior performance in duration alignment tasks.</abstract>
      <url hash="b6c31492">2025.acl-long.227</url>
      <bibkey>cui-etal-2025-fine</bibkey>
    </paper>
    <paper id="228">
      <title>Sparse Latents Steer Retrieval-Augmented Generation</title>
      <author><first>Chunlei</first><last>Xin</last><affiliation>Chinese Academy of Sciences and Chinese Information Processing Laboratory</affiliation></author>
      <author><first>Shuheng</first><last>Zhou</last><affiliation>Ant Group</affiliation></author>
      <author><first>Huijia</first><last>Zhu</last><affiliation>Ant Group</affiliation></author>
      <author><first>Weiqiang</first><last>Wang</last><affiliation>Ant Group</affiliation></author>
      <author><first>Xuanang</first><last>Chen</last></author>
      <author><first>Xinyan</first><last>Guan</last></author>
      <author><first>Yaojie</first><last>Lu</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Hongyu</first><last>Lin</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xianpei</first><last>Han</last><affiliation>Institute of Software, CAS</affiliation></author>
      <author><first>Le</first><last>Sun</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <pages>4547-4562</pages>
      <abstract>Understanding the mechanisms underlying Large Language Model (LLM) behavior in Retrieval-Augmented Generation (RAG) systems is critical for enhancing reliability. In this paper, we leverage Sparse Autoencoders (SAEs) within the LLaMA Scope to uncover sparse, interpretable latents that govern RAG behaviors. Through systematic analysis of SAE activations, we identify specific latents associated with two fundamental RAG decisions: (1) context versus memory prioritization, and (2) response generation versus query rejection. Intervention experiments demonstrate that these latents enable precise control over model behavior and maintain generalizability across various experimental settings. Mechanistic analysis reveals that manipulating these latents influences model behavior by reconfiguring attention patterns of retrieval heads. Our findings establish SAEs as a principled tool for understanding and controlling RAG behaviors, demonstrating capabilities in precise behavior steering without architectural modifications.</abstract>
      <url hash="1dd1ff73">2025.acl-long.228</url>
      <bibkey>xin-etal-2025-sparse</bibkey>
    </paper>
    <paper id="229">
      <title>Unveiling Language-Specific Features in Large Language Models via Sparse Autoencoders</title>
      <author><first>Boyi</first><last>Deng</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Yu</first><last>Wan</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Baosong</first><last>Yang</last></author>
      <author><first>Yidan</first><last>Zhang</last></author>
      <author><first>Fuli</first><last>Feng</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>4563-4608</pages>
      <abstract>The mechanisms behind multilingual capabilities in Large Language Models (LLMs) have been examined using neuron-based or internal-activation-based methods. However, these methods often face challenges such as superposition and layer-wise activation variance, which limit their reliability. Sparse Autoencoders (SAEs) offer a more nuanced analysis by decomposing the activations of LLMs into a sparse linear combination of SAE features. We introduce a novel metric to assess the monolinguality of features obtained from SAEs, discovering that some features are strongly related to specific languages. Additionally, we show that ablating these SAE features only significantly reduces abilities in one language of LLMs, leaving others almost unaffected. Interestingly, we find some languages have multiple synergistic SAE features, and ablating them together yields greater improvement than ablating individually. Moreover, we leverage these SAE-derived language-specific features to enhance steering vectors, achieving control over the language generated by LLMs. The code is publicly available at <url>https://github.com/Aatrox103/multilingual-llm-features</url>.</abstract>
      <url hash="bef9d30b">2025.acl-long.229</url>
      <bibkey>deng-etal-2025-unveiling</bibkey>
    </paper>
    <paper id="230">
      <title><fixed-case>S</fixed-case>afe<fixed-case>RAG</fixed-case>: Benchmarking Security in Retrieval-Augmented Generation of Large Language Model</title>
      <author><first>Xun</first><last>Liang</last></author>
      <author><first>Simin</first><last>Niu</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Zhiyu</first><last>Li</last></author>
      <author><first>Sensen</first><last>Zhang</last></author>
      <author><first>Hanyu</first><last>Wang</last></author>
      <author><first>Feiyu</first><last>Xiong</last><affiliation>Institute for Advanced Algorithms Research, Shanghai</affiliation></author>
      <author><first>Zhaoxin</first><last>Fan</last><affiliation>Beijing University of Aeronautics and Astronautics</affiliation></author>
      <author><first>Bo</first><last>Tang</last></author>
      <author><first>Jihao</first><last>Zhao</last></author>
      <author><first>Jiawei</first><last>Yang</last></author>
      <author><first>Shichao</first><last>Song</last></author>
      <author><first>Mengwei</first><last>Wang</last></author>
      <pages>4609-4631</pages>
      <abstract>The indexing-retrieval-generation paradigm of retrieval-augmented generation (RAG) has been highly successful in solving knowledge-intensive tasks by integrating external knowledge into large language models (LLMs). However, the incorporation of external and unverified knowledge increases the vulnerability of LLMs because attackers can perform attack tasks by manipulating knowledge. In this paper, we introduce a benchmark named SafeRAG designed to evaluate the RAG security. First, we classify attack tasks into silver noise, inter-context conflict, soft ad, and white Denial-of-Service. Next, we construct RAG security evaluation dataset (i.e., SafeRAG dataset) primarily manually for each task. We then utilize the SafeRAG dataset to simulate various attack scenarios that RAG may encounter. Experiments conducted on 14 representative RAG components demonstrate that RAG exhibits significant vulnerability to all attack tasks and even the most apparent attack task can easily bypass existing retrievers, filters, or advanced LLMs, resulting in the degradation of RAG service quality. Code is available at: https://github.com/IAAR-Shanghai/SafeRAG.</abstract>
      <url hash="de0f5a02">2025.acl-long.230</url>
      <bibkey>liang-etal-2025-saferag</bibkey>
    </paper>
    <paper id="231">
      <title><fixed-case>A</fixed-case>n<fixed-case>R</fixed-case>e: Analogical Replay for Temporal Knowledge Graph Forecasting</title>
      <author><first>Guo</first><last>Tang</last></author>
      <author><first>Zheng</first><last>Chu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Wenxiang</first><last>Zheng</last></author>
      <author><first>Junjia</first><last>Xiang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Yizhuo</first><last>Li</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Weihao</first><last>Zhang</last></author>
      <author><first>Ming</first><last>Liu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Bing</first><last>Qin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>4632-4650</pages>
      <abstract>Temporal Knowledge Graphs (TKGs) are vital for event prediction, yet current methods face limitations. Graph neural networks mainly depend on structural information, often overlooking semantic understanding and requiring high computational costs. Meanwhile, Large Language Models (LLMs) support zero-shot reasoning but lack sufficient capabilities to grasp the laws of historical event development. To tackle these challenges, we introduce a training-free Analogical Replay (AnRe) reasoning framework. Our approach retrieves similar events for queries through semantic-driven clustering and builds comprehensive historical contexts using a dual history extraction module that integrates long-term and short-term history. It then uses LLMs to generate analogical reasoning examples as contextual inputs, enabling the model to deeply understand historical patterns of similar events and improve its ability to predict unknown ones. Our experiments on four benchmarks show that AnRe significantly exceeds traditional training and existing LLM-based methods. Further ablation studies also confirm the effectiveness of the dual history extraction and analogical replay mechanisms.</abstract>
      <url hash="a8188635">2025.acl-long.231</url>
      <bibkey>tang-etal-2025-anre</bibkey>
    </paper>
    <paper id="232">
      <title>Revisiting the Test-Time Scaling of o1-like Models: Do they Truly Possess Test-Time Scaling Capabilities?</title>
      <author><first>Zhiyuan</first><last>Zeng</last></author>
      <author><first>Qinyuan</first><last>Cheng</last></author>
      <author><first>Zhangyue</first><last>Yin</last></author>
      <author><first>Yunhua</first><last>Zhou</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Xipeng</first><last>Qiu</last><affiliation>Fudan University</affiliation></author>
      <pages>4651-4665</pages>
      <abstract>The advent of test-time scaling in large language models (LLMs), exemplified by OpenAI’s o1 series, has advanced reasoning capabilities by scaling computational resource allocation during inference. While successors like QwQ, Deepseek-R1 (R1) and LIMO replicate these advancements, whether these models truly possess test-time scaling capabilities remains underexplored. This study found that longer CoTs of these o1-like models do not consistently enhance accuracy; in fact, correct solutions are often shorter than incorrect ones for the same questions. Further investigation shows this phenomenon is closely related to models’ self-revision capabilities - longer CoTs contain more self-revisions, which often lead to performance degradation. We then compare sequential and parallel scaling strategies on QwQ, R1 and LIMO, finding that parallel scaling achieves better coverage and scalability. Based on these insights, we propose “Shortest Majority Vote”, a method that combines parallel scaling strategies with CoT length characteristics, significantly improving models’ test-time scalability compared to conventional majority voting approaches.</abstract>
      <url hash="4ae0de89">2025.acl-long.232</url>
      <bibkey>zeng-etal-2025-revisiting</bibkey>
    </paper>
    <paper id="233">
      <title>Text is All You Need: <fixed-case>LLM</fixed-case>-enhanced Incremental Social Event Detection</title>
      <author><first>Zitai</first><last>Qiu</last></author>
      <author><first>Congbo</first><last>Ma</last></author>
      <author><first>Jia</first><last>Wu</last><affiliation>Macquarie University</affiliation></author>
      <author><first>Jian</first><last>Yang</last><affiliation>Macquarie University</affiliation></author>
      <pages>4666-4680</pages>
      <abstract>Social event detection (SED) is the task of identifying, categorizing, and tracking events from social data sources such as social media posts, news articles, and online discussions. Existing state-of-the-art (SOTA) SED models predominantly rely on graph neural networks (GNNs), which involve complex graph construction and time-consuming training processes, limiting their practicality in real-world scenarios. In this paper, we rethink the key challenge in SED: the informal and noisy nature of short texts on social media platforms, which impacts clustering accuracy. We propose a novel framework, LLM-enhanced Social Event Detection (LSED), which leverages the rich background knowledge of large language models (LLMs) to address this challenge. Specifically, LSED utilizes LLMs to formalize and disambiguate short texts by completing abbreviations and summarizing informal expressions. Furthermore, we introduce hyperbolic space embeddings, which are more suitable for natural language sentence representations, to enhance clustering performance. Extensive experiments on two challenging real-world datasets demonstrate that LSED outperforms existing SOTA models, achieving improvements in effectiveness, efficiency, and stability. Our work highlights the potential of LLMs in SED and provides a practical solution for real-world applications.</abstract>
      <url hash="f64dc771">2025.acl-long.233</url>
      <bibkey>qiu-etal-2025-text</bibkey>
    </paper>
    <paper id="234">
      <title>Multimodal Pragmatic Jailbreak on Text-to-image Models</title>
      <author><first>Tong</first><last>Liu</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Zhixin</first><last>Lai</last><affiliation>Google</affiliation></author>
      <author><first>Jiawen</first><last>Wang</last></author>
      <author><first>Gengyuan</first><last>Zhang</last><affiliation>Amazon and Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Shuo</first><last>Chen</last><affiliation>Amazon, Siemens Corporate Research and University of Munich, Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Philip</first><last>Torr</last><affiliation>University of Oxford</affiliation></author>
      <author><first>Vera</first><last>Demberg</last><affiliation>Universität des Saarlandes</affiliation></author>
      <author><first>Volker</first><last>Tresp</last><affiliation>Ludwig Maximilian University of Munich and Siemens Corporate Research</affiliation></author>
      <author><first>Jindong</first><last>Gu</last><affiliation>University of Oxford</affiliation></author>
      <pages>4681-4720</pages>
      <abstract>Diffusion models have recently achieved remarkable advancements in terms of image quality and fidelity to textual prompts. Concurrently, the safety of such generative models has become an area of growing concern. This work introduces a novel type of jailbreak, which triggers T2I models to generate the image with visual text, where the image and the text, although considered to be safe in isolation, combine to form unsafe content. To systematically explore this phenomenon, we propose a dataset to evaluate the current diffusion-based text-to-image (T2I) models under such jailbreak. We benchmark nine representative T2I models, including two closed-source commercial models. Experimental results reveal a concerning tendency to produce unsafe content: all tested models suffer from such type of jailbreak, with rates of unsafe generation ranging from around 10% to 70% where DALL·E 3 demonstrates almost the highest unsafety. In real-world scenarios, various filters such as keyword blocklists, customized prompt filters, and NSFW image filters, are commonly employed to mitigate these risks. We evaluate the effectiveness of such filters against our jailbreak and found that, while these filters may be effective for single modality detection, they fail to work against our jailbreak. We also investigate the underlying reason for such jailbreaks, from the perspective of text rendering capability and training data. Our work provides a foundation for further development towards more secure and reliable T2I models.</abstract>
      <url hash="cfc63a4b">2025.acl-long.234</url>
      <bibkey>liu-etal-2025-multimodal-pragmatic</bibkey>
    </paper>
    <paper id="235">
      <title>Principled Understanding of Generalization for Generative Transformer Models in Arithmetic Reasoning Tasks</title>
      <author><first>Xingcheng</first><last>Xu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Zibo</first><last>Zhao</last></author>
      <author><first>Haipeng</first><last>Zhang</last><affiliation>ShanghaiTech University</affiliation></author>
      <author><first>Yanqing</first><last>Yang</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <pages>4721-4747</pages>
      <abstract>Transformer-based models excel in various tasks but their generalization capabilities, especially in arithmetic reasoning, remain incompletely understood. Arithmetic tasks provide a controlled framework to explore these capabilities, yet performance anomalies persist, such as inconsistent effectiveness in multiplication and erratic generalization in modular addition (e.g., modulo 100 vs. 101). This paper develops a unified theoretical framework for understanding the generalization behaviors of transformers in arithmetic tasks, focusing on length generalization. Through detailed analysis of addition, multiplication, and modular operations, we reveal that translation invariance in addition aligns with relative positional encoding for robust generalization, while base mismatch in modular operations disrupts this alignment. Experiments across GPT-family models validate our framework, confirming its ability to predict generalization behaviors. Our work highlights the importance of task structure and training data distribution for achieving data-efficient and structure-aware training, providing a systematic approach to understanding of length generalization in transformers.</abstract>
      <url hash="b7777a26">2025.acl-long.235</url>
      <bibkey>xu-etal-2025-principled</bibkey>
    </paper>
    <paper id="236">
      <title>Discourse Relation-Enhanced Neural Coherence Modeling</title>
      <author><first>Wei</first><last>Liu</last><affiliation>Heidelberg University</affiliation></author>
      <author><first>Michael</first><last>Strube</last><affiliation>Heidelberg Institute for Theoretical Studies</affiliation></author>
      <pages>4748-4762</pages>
      <abstract>Discourse coherence theories posit relations between text spans as a key feature of coherent texts. However, existing work on coherence modeling has paid little attention to discourse relations. In this paper, we provide empirical evidence to demonstrate that relation features are correlated with text coherence. Then, we investigate a novel fusion model that uses position-aware attention and a visible matrix to combine text- and relation-based features for coherence assessment. Experimental results on two benchmarks show that our approaches can significantly improve baselines, demonstrating the importance of relation features for coherence modeling.</abstract>
      <url hash="e6a330b3">2025.acl-long.236</url>
      <bibkey>liu-strube-2025-discourse</bibkey>
    </paper>
    <paper id="237">
      <title>Benchmarking Open-ended Audio Dialogue Understanding for Large Audio-Language Models</title>
      <author><first>Kuofeng</first><last>Gao</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Shu-Tao</first><last>Xia</last><affiliation>Shenzhen International Graduate School, Tsinghua University</affiliation></author>
      <author><first>Ke</first><last>Xu</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Philip</first><last>Torr</last><affiliation>University of Oxford</affiliation></author>
      <author><first>Jindong</first><last>Gu</last><affiliation>University of Oxford</affiliation></author>
      <pages>4763-4784</pages>
      <abstract>Large Audio-Language Models (LALMs), such as GPT-4o, have recently unlocked audio dialogue capabilities, enabling direct spoken exchanges with humans. The potential of LALMs broadens their applicability across a wide range of practical scenarios supported by audio dialogues. However, given these advancements, a comprehensive benchmark to evaluate the performance of LALMs in the open-ended audio dialogue understanding remains absent currently. To address this gap, we propose an **A**udio **D**ialogue **U**nderstanding **Bench**mark **(ADU-Bench),** which consists of 4 benchmark datasets. They assess the open-ended audio dialogue ability for LALMs in 3 general scenarios, 12 skills, 9 multilingual languages, and 4 categories of ambiguity handling. Notably, *we firstly propose the evaluation of ambiguity handling* in audio dialogues that expresses different intentions beyond the same literal meaning of sentences, *e.g.,* ‘“Really!?”‘ with different intonations. In summary, ADU-Bench includes over 20,000 open-ended audio dialogues for the assessment of LALMs. Through extensive experiments conducted on 16 LALMs, our analysis reveals that existing LALMs struggle with mathematical symbols and formulas, understanding human behavior such as roleplay, comprehending multiple languages, and handling audio dialogue ambiguities from different phonetic elements, such as intonations, pause positions, and homophones. The benchmark is available at https://adu-bench.github.io/.</abstract>
      <url hash="105725ec">2025.acl-long.237</url>
      <bibkey>gao-etal-2025-benchmarking</bibkey>
    </paper>
    <paper id="238">
      <title>from Benign import Toxic: Jailbreaking the Language Model via Adversarial Metaphors</title>
      <author><first>Yu</first><last>Yan</last></author>
      <author><first>Sheng</first><last>Sun</last><affiliation>Chinese Academy of Sciences</affiliation></author>
      <author><first>Zenghao</first><last>Duan</last></author>
      <author><first>Teli</first><last>Liu</last></author>
      <author><first>Min</first><last>Liu</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Zhiyi</first><last>Yin</last><affiliation>, Chinese Academy of Sciences</affiliation></author>
      <author><first>LeiJingyu</first><last>LeiJingyu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Qi</first><last>Li</last><affiliation>Tsinghua University</affiliation></author>
      <pages>4785-4817</pages>
      <abstract>Current studies have exposed the risk of Large Language Models (LLMs) generating harmful content by jailbreak attacks. However, they overlook that the direct generation of harmful content from scratch is more difficult than inducing LLM to calibrate benign content into harmful forms.In our study, we introduce a novel attack framework that exploits <b>A</b>d<b>V</b>ers<b>A</b>rial me<b>TA</b>pho<b>R</b> (<b>AVATAR</b>) to induce the LLM to calibrate malicious metaphors for jailbreaking.Specifically, to answer harmful queries, AVATAR adaptively identifies a set of benign but logically related metaphors as the initial seed.Then, driven by these metaphors, the target LLM is induced to reason and calibrate about the metaphorical content, thus jailbroken by either directly outputting harmful responses or calibrating residuals between metaphorical and professional harmful content.Experimental results demonstrate that AVATAR can effectively and transferably jailbreak LLMs and achieve a state-of-the-art attack success rate across multiple advanced LLMs.</abstract>
      <url hash="d6040265">2025.acl-long.238</url>
      <bibkey>yan-etal-2025-benign</bibkey>
    </paper>
    <paper id="239">
      <title><fixed-case>S</fixed-case>hif<fixed-case>C</fixed-case>on: Enhancing Non-Dominant Language Capabilities with a Shift-based Multilingual Contrastive Framework</title>
      <author><first>Hengyuan</first><last>Zhang</last></author>
      <author><first>Chenming</first><last>Shang</last></author>
      <author><first>Sizhe</first><last>Wang</last><affiliation>Washington University in Saint Louis</affiliation></author>
      <author><first>Dongdong</first><last>Zhang</last><affiliation>Microsoft Research Asia</affiliation></author>
      <author><first>Yiyao</first><last>Yu</last></author>
      <author><first>Feng</first><last>Yao</last></author>
      <author><first>Renliang</first><last>Sun</last><affiliation>UCLA Computer Science Department, University of California, Los Angeles</affiliation></author>
      <author><first>Yujiu</first><last>Yang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Furu</first><last>Wei</last><affiliation>Microsoft Research</affiliation></author>
      <pages>4818-4841</pages>
      <abstract>Although fine-tuning Large Language Models (LLMs) with multilingual data can rapidly enhance the multilingual capabilities of LLMs, they still exhibit a performance gap between the dominant language (e.g., English) and non-dominant ones due to the imbalance of training data across languages. To further enhance the performance of non-dominant languages, we propose ShifCon, a Shift-based multilingual Contrastive framework that aligns the internal forward process of other languages toward that of the dominant one. Specifically, it shifts the representations of non-dominant languages into the dominant language subspace, allowing them to access relatively rich information encoded in the model parameters. The enriched representations are then shifted back into their original language subspace before generation. Moreover, we introduce a subspace distance metric to pinpoint the optimal layer area for shifting representations and employ multilingual contrastive learning to further enhance the alignment of representations within this area. Experiments demonstrate that our ShifCon framework significantly enhances the performance of non-dominant languages, particularly for low-resource ones. Further analysis offers extra insights to verify the effectiveness of ShifCon and propel future research.</abstract>
      <url hash="28086674">2025.acl-long.239</url>
      <bibkey>zhang-etal-2025-shifcon</bibkey>
    </paper>
    <paper id="240">
      <title><fixed-case>M</fixed-case>orph<fixed-case>M</fixed-case>ark: Flexible Adaptive Watermarking for Large Language Models</title>
      <author><first>Zongqi</first><last>Wang</last></author>
      <author><first>Tianle</first><last>Gu</last></author>
      <author><first>Baoyuan</first><last>Wu</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <author><first>Yujiu</first><last>Yang</last><affiliation>Tsinghua University</affiliation></author>
      <pages>4842-4860</pages>
      <abstract>Watermarking by altering token sampling probabilities based on red-green list is a promising method for tracing the origin of text generated by large language models (LLMs). However, existing watermark methods often struggle with a fundamental dilemma: improving watermark effectiveness (the detectability of the watermark) often comes at the cost of reduced text quality. This trade-off limits their practical application. To address this challenge, we first formalize the problem within a multi-objective trade-off analysis framework. Within this framework, we identify a key factor that influences the dilemma. Unlike existing methods, where watermark strength is typically treated as a fixed hyperparameter, our theoretical insights lead to the development of MorphMark—a method that adaptively adjusts the watermark strength in response to changes in the identified factor, thereby achieving an effective resolution of the dilemma. In addition, MorphMark also prioritizes flexibility since it is an model-agnostic and model-free watermark method, thereby offering a practical solution for real-world deployment, particularly in light of the rapid evolution of AI models. Extensive experiments demonstrate that MorphMark achieves a superior resolution of the effectiveness-quality dilemma, while also offering greater flexibility and time and space efficiency.</abstract>
      <url hash="6bc03979">2025.acl-long.240</url>
      <bibkey>wang-etal-2025-morphmark</bibkey>
    </paper>
    <paper id="241">
      <title>A Silver Bullet or a Compromise for Full Attention? A Comprehensive Study of Gist Token-based Context Compression</title>
      <author><first>Chenlong</first><last>Deng</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Zhisong</first><last>Zhang</last><affiliation>Tencent</affiliation></author>
      <author><first>Kelong</first><last>Mao</last></author>
      <author><first>Shuaiyi</first><last>Li</last><affiliation>Chinese University of Hong Kong, The Chinese University of Hong Kong</affiliation></author>
      <author><first>Xinting</first><last>Huang</last><affiliation>Tencent</affiliation></author>
      <author><first>Dong</first><last>Yu</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Zhicheng</first><last>Dou</last><affiliation>Renmin University of China</affiliation></author>
      <pages>4861-4879</pages>
      <abstract>In this work, we provide an empirical investigation of gist-based context compression methods to improve context processing in large language models. We focus on two key questions: (1) How well can these methods replace full attention models? and (2) What potential failure patterns arise due to compression? Through extensive experiments, we show that while gist-based compression can achieve only slight performance loss on tasks like retrieval-augmented generation and long-document QA, it faces challenges in tasks like synthetic recall. Furthermore, we identify three key failure patterns: lost by the boundary, lost if surprise, and lost along the way. To mitigate these issues, we propose two effective strategies: fine-grained autoencoding, which enhances the reconstruction of original token information, and segment-wise token importance estimation, which adjusts optimization based on token dependencies. Our work provides valuable insights into the understanding of gist token-based context compression and offers practical strategies for improving compression capabilities.</abstract>
      <url hash="4bf95a53">2025.acl-long.241</url>
      <bibkey>deng-etal-2025-silver</bibkey>
    </paper>
    <paper id="242">
      <title>On the Limit of Language Models as Planning Formalizers</title>
      <author><first>Cassie</first><last>Huang</last><affiliation>Drexel University</affiliation></author>
      <author id="li-zhang-aws"><first>Li</first><last>Zhang</last><affiliation>Drexel University</affiliation></author>
      <pages>4880-4904</pages>
      <abstract>Large Language Models have been found to create plans that are neither executable nor verifiable in grounded environments. An emerging line of work demonstrates success in using the LLM as a formalizer to generate a formal representation of the planning domain in some language, such as Planning Domain Definition Language (PDDL). This formal representation can be deterministically solved to find a plan. We systematically evaluate this methodology while bridging some major gaps. While previous work only generates a partial PDDL representation, given templated, and therefore unrealistic environment descriptions, we generate the complete representation given descriptions of various naturalness levels. Among an array of observations critical to improve LLMs’ formal planning abilities, we note that most large enough models can effectively formalize descriptions as PDDL, outperforming those directly generating plans, while being robust to lexical perturbation. As the descriptions become more natural-sounding, we observe a decrease in performance and provide detailed error analysis.</abstract>
      <url hash="49651b85">2025.acl-long.242</url>
      <bibkey>huang-zhang-2025-limit</bibkey>
    </paper>
    <paper id="243">
      <title>Learning to Generate Structured Output with Schema Reinforcement Learning</title>
      <author><first>Yaxi</first><last>Lu</last><affiliation>Department of Computer Science and Technology, Tsinghua University</affiliation></author>
      <author><first>Haolun</first><last>Li</last></author>
      <author><first>Xin</first><last>Cong</last></author>
      <author><first>Zhong</first><last>Zhang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Yesai</first><last>Wu</last></author>
      <author><first>Yankai</first><last>Lin</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Zhiyuan</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Fangming</first><last>Liu</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Maosong</first><last>Sun</last><affiliation>Tsinghua University</affiliation></author>
      <pages>4905-4918</pages>
      <abstract>This study investigates the structured generation capabilities of large language models (LLMs), focusing on producing valid JSON outputs against a given schema. Despite the widespread use of JSON in integrating language models with programs, there is a lack of comprehensive analysis and benchmarking of these capabilities. We explore various aspects of JSON generation, such as structure understanding, escaping, and natural language description, to determine how to assess and enable LLMs to generate valid responses. Building upon this, we propose SchemaBench features around 40K different JSON schemas to obtain and assess models’ abilities in generating valid JSON. We find that the latest LLMs are still struggling to generate a valid JSON string. Moreover, we demonstrate that incorporating reinforcement learning with a Fine-grained Schema Validator can further enhance models’ understanding of JSON schema, leading to improved performance. Our models demonstrate significant improvement in both generating JSON outputs and downstream tasks.</abstract>
      <url hash="d6f7cc9e">2025.acl-long.243</url>
      <bibkey>lu-etal-2025-learning</bibkey>
    </paper>
    <paper id="244">
      <title>Enhancing Unsupervised Sentence Embeddings via Knowledge-Driven Data Augmentation and <fixed-case>G</fixed-case>aussian-Decayed Contrastive Learning</title>
      <author><first>Peichao</first><last>Lai</last></author>
      <author><first>Zhengfeng</first><last>Zhang</last><affiliation>Fuzhou University</affiliation></author>
      <author><first>Wentao</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Fangcheng</first><last>Fu</last></author>
      <author><first>Bin</first><last>Cui</last><affiliation>Peking University</affiliation></author>
      <pages>4919-4940</pages>
      <abstract>Recently, using large language models (LLMs) for data augmentation has led to considerable improvements in unsupervised sentence embedding models. However, existing methods encounter two primary challenges: limited data diversity and high data noise. Current approaches often neglect fine-grained knowledge, such as entities and quantities, leading to insufficient diversity. Besides, unsupervised data frequently lacks discriminative information, and the generated synthetic samples may introduce noise. In this paper, we propose a pipeline-based data augmentation method via LLMs and introduce the Gaussian-decayed gradient-assisted Contrastive Sentence Embedding (GCSE) model to enhance unsupervised sentence embeddings. To tackle the issue of low data diversity, our pipeline utilizes knowledge graphs (KGs) to extract entities and quantities, enabling LLMs to generate more diverse samples. To address high data noise, the GCSE model uses a Gaussian-decayed function to limit the impact of false hard negative samples, enhancing the model’s discriminative capability. Experimental results show that our approach achieves state-of-the-art performance in semantic textual similarity (STS) tasks, using fewer data samples and smaller LLMs, demonstrating its efficiency and robustness across various models.</abstract>
      <url hash="6e4e3bca">2025.acl-long.244</url>
      <bibkey>lai-etal-2025-enhancing</bibkey>
    </paper>
    <paper id="245">
      <title>Improve Safety Training of Large Language Models with Safety-Critical Singular Vectors Localization</title>
      <author><first>Peijian</first><last>Gu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Quan</first><last>Wang</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Zhendong</first><last>Mao</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>4941-4954</pages>
      <abstract>The rapid advancement of large language models (LLMs) has brought about increased concerns regarding their safety, especially as adversaries develop jailbreak techniques to bypass LLMs’ safety mechanism. Although recent work on safety training with modules such as low-rank adaptation (LoRA) to resist jailbreaks shows promise, these approaches can inadvertently degrade a model’s general utility. In this paper, we propose a novel plug-and-play method that mitigates the impact of safety training on model utility by explicitly locating and leveraging safety-critical singular vectors, which only contribute to safety, within the model’s parameter space. We quantify the safety-criticality of each singular vector as the difference of their importance for safety and utility measured by a corresponding low-rank projection. The top scored singular vectors are located as safety-critical and are used to initialize the LoRA modules within existing safety training methods in a plug-and-play manner, thereby constraining the training updates within safety-critical parameters. Additionally, we propose a dynamic rank number determination strategy to further reduce parameter overhead. Experiments on HarmBench with multiple jailbreak methods validate the effectiveness of our approach in safety training, while evaluations on several utility benchmarks demonstrate that our method successfully mitigates the adverse impact of safety training on model utility, enhancing the utility performance of the evaluated safety training baselines.</abstract>
      <url hash="679ba895">2025.acl-long.245</url>
      <bibkey>gu-etal-2025-improve</bibkey>
    </paper>
    <paper id="246">
      <title><fixed-case>W</fixed-case>arrior<fixed-case>C</fixed-case>oder: Learning from Expert Battles to Augment Code Large Language Models</title>
      <author><first>Huawen</first><last>Feng</last></author>
      <author><first>Pu</first><last>Zhao</last></author>
      <author><first>Qingfeng</first><last>Sun</last></author>
      <author><first>Can</first><last>Xu</last></author>
      <author><first>Fangkai</first><last>Yang</last></author>
      <author><first>Lu</first><last>Wang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Qianli</first><last>Ma</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Qingwei</first><last>Lin</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Saravan</first><last>Rajmohan</last><affiliation>Microsoft</affiliation></author>
      <author><first>Dongmei</first><last>Zhang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <pages>4955-4969</pages>
      <abstract>Despite recent progress achieved by code large language models (LLMs), their remarkable abilities are largely dependent on fine-tuning on the high-quality data, posing challenges for data collection and annotation. To address this, current methods often design various data flywheels to collect complex code instructions, enabling models to handle more intricate tasks. However, these approaches typically rely on off-the-shelf datasets and data augmentation from a limited set of proprietary LLMs (e.g., Claude, GPT4, and so on), which restricts the diversity of the constructed data and makes it prone to systemic biases. In this paper, we propose **WarriorCoder**, a novel paradigm learns from expert battles to address these limitations. Specifically, we create an arena where leading expert code LLMs challenge each other, with evaluations conducted by impartial judges. This competitive framework generates novel training data from scratch, leveraging the strengths of all participants. Experimental results show that **WarriorCoder** achieves state-of-the-art performance compared to previous models of the same size, even without relying on proprietary LLMs.</abstract>
      <url hash="34a9736c">2025.acl-long.246</url>
      <bibkey>feng-etal-2025-warriorcoder</bibkey>
    </paper>
    <paper id="247">
      <title>A Triple-View Framework for Fine-Grained Emotion Classification with Clustering-Guided Contrastive Learning</title>
      <author><first>Junqing</first><last>Gong</last></author>
      <author><first>Binhan</first><last>Yang</last><affiliation>Nankai University</affiliation></author>
      <author><first>Wei</first><last>Shen</last><affiliation>Nankai University</affiliation></author>
      <pages>4970-4984</pages>
      <abstract>Fine-grained emotion classification (FEC) aims to analyze speakers’ utterances and distinguish dozens of emotions with subtle differences, allowing for a more nuanced understanding of human emotional states. However, compared to traditional coarse-grained emotion classification, two difficulties arise as the granularity of emotions becomes finer, i.e., the presence of closely confusable emotions which are hard to distinguish, and the biased performance caused by long-tailed emotions. Although addressing both difficulties is vital to FEC, previous studies have predominantly focused on dealing with only one of them. In this paper, we propose TACO, a novel triple-view framework that treats FEC as an instance-label (i.e., utterance-emotion) joint embedding learning problem to tackle both difficulties concurrently by considering three complementary views. Specifically, we design a clustering-guided contrastive loss, which incorporates clustering techniques to guide the contrastive learning process and facilitate more discriminative instance embeddings. Additionally, we introduce the emotion label description as a helpful resource to refine label embeddings and mitigate the poor performance towards under-represented (i.e., long-tailed) emotions. Extensive experiments on two widely-used benchmark datasets demonstrate that our proposed TACO achieves substantial and consistent improvements compared to other competitive baseline methods.</abstract>
      <url hash="22fa7ad8">2025.acl-long.247</url>
      <bibkey>gong-etal-2025-triple</bibkey>
    </paper>
    <paper id="248">
      <title>Quantification of Large Language Model Distillation</title>
      <author><first>Sunbowen</first><last>Lee</last></author>
      <author><first>Junting</first><last>Zhou</last></author>
      <author><first>Chang</first><last>Ao</last></author>
      <author><first>Kaige</first><last>Li</last></author>
      <author><first>Xeron</first><last>Du</last></author>
      <author><first>Sirui</first><last>He</last></author>
      <author><first>Haihong</first><last>Wu</last></author>
      <author><first>Tianci</first><last>Liu</last></author>
      <author><first>Jiaheng</first><last>Liu</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Hamid</first><last>Alinejad-Rokny</last><affiliation>UNSW Sydney</affiliation></author>
      <author><first>Min</first><last>Yang</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yitao</first><last>Liang</last><affiliation>Peking University</affiliation></author>
      <author><first>Zhoufutu</first><last>Wen</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Shiwen</first><last>Ni</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences</affiliation></author>
      <pages>4985-5004</pages>
      <abstract>Model distillation is a fundamental technique in building large language models (LLMs), transferring knowledge from a teacher model to a student model. However, distillation can lead to model homogenization, reducing diversity among models and impairing their ability to robustly handle complex or novel tasks. These limitations underscore the need to systematically quantify the distillation process and its impact. In this work, we propose a framework to evaluate and quantify model distillation. Our method addresses two key aspects: (1) Identifying identity cognition contradictions to assess discrepancies in how models perceive and represent identity-related information, and (2) Analyzing multi-granularity response similarities across models to measure the extent of homogenization. Experimental results demonstrate two key insights: (1) Well-known closed-source and open-source LLMs usually exhibit high distillation degrees, except for Claude, Doubao, and Gemini. (2) Base LLMs show higher distillation degrees compared to aligned LLMs. By offering a systematic approach to improve the transparency of LLM data distillation, we call for LLMs with more independent development and more transparent technical reports to improve LLMs’ robustness and safety. The code and data are available at https://github.com/Aegis1863/LLMs-Distillation-Quantification.</abstract>
      <url hash="67b28465">2025.acl-long.248</url>
      <bibkey>lee-etal-2025-quantification</bibkey>
    </paper>
    <paper id="249">
      <title>Demons in the Detail: On Implementing Load Balancing Loss for Training Specialized Mixture-of-Expert Models</title>
      <author><first>Zihan</first><last>Qiu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Zeyu</first><last>Huang</last></author>
      <author><first>Bo</first><last>Zheng</last></author>
      <author><first>Kaiyue</first><last>Wen</last></author>
      <author><first>Zekun</first><last>Wang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Rui</first><last>Men</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Ivan</first><last>Titov</last><affiliation>University of Edinburgh and University of Amsterdam</affiliation></author>
      <author><first>Dayiheng</first><last>Liu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Jingren</first><last>Zhou</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Junyang</first><last>Lin</last><affiliation>Alibaba Group</affiliation></author>
      <pages>5005-5018</pages>
      <abstract>This paper revisits the implementation of <tex-math>\textbf{L}</tex-math>oad-<tex-math>\textbf{B}</tex-math>alancing-<tex-math>\textbf{L}</tex-math>oss (LBL) when training Mixture-of-Experts (MoEs) models. Specifically, LBL for MoEs is defined as <tex-math>N_E \sum_{i=1}^{N_E} f_ip_i</tex-math>, where <tex-math>N_E</tex-math> is the total number of experts, <tex-math>f_i</tex-math> represents the frequency of expert <tex-math>i</tex-math> being selected, and <tex-math>p_i</tex-math> denotes the average gating score of the expert <tex-math>i</tex-math>. Existing MoE training frameworks usually employ the parallel training strategy so that <tex-math>f_i</tex-math> and the LBL are calculated within a <b>micro-batch</b> and averaged across parallel groups.However, a micro-batch for training billion-scale LLMs typically contains very few sequences, leading to the micro-batch LBL being almost at the sequence level, and the router is pushed to distribute the token evenly within each sequence.Under this strict constraint, even tokens from a domain-specific sequence (<tex-math>\textit{e.g.}</tex-math>, code) are uniformly routed to all experts, thereby inhibiting expert specialization.In this work, we propose calculating LBL using a <tex-math>\textbf{global-batch}</tex-math> to loose this constraint. Because a global-batch contains much more diverse sequences than a micro-batch, which will encourage load balance at the corpus level. Specifically, we introduce an extra communication step to synchronize <tex-math>f_i</tex-math> across micro-batches and then use it to calculate the LBL.Through experiments on training MoEs-based LLMs (up to <tex-math>\textbf{42.8B}</tex-math> parameters and <tex-math>\textbf{400B}</tex-math> tokens), we surprisingly find that the global-batch LBL strategy yields excellent performance gains in both pre-training perplexity and downstream tasks.Our analysis reveals that the global-batch LBL greatly improves the domain specialization of experts. <tex-math>\textit{Global-batch LBL is also used in Qwen3-MoEs.}</tex-math></abstract>
      <url hash="819b1600">2025.acl-long.249</url>
      <bibkey>qiu-etal-2025-demons</bibkey>
    </paper>
    <paper id="250">
      <title>Pandora’s Box or Aladdin’s Lamp: A Comprehensive Analysis Revealing the Role of <fixed-case>RAG</fixed-case> Noise in Large Language Models</title>
      <author><first>Jinyang</first><last>Wu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Shuai</first><last>Zhang</last></author>
      <author><first>Feihu</first><last>Che</last></author>
      <author><first>Mingkuan</first><last>Feng</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Pengpeng</first><last>Shao</last></author>
      <author><first>Jianhua</first><last>Tao</last><affiliation>Tsinghua University</affiliation></author>
      <pages>5019-5039</pages>
      <abstract>Retrieval-Augmented Generation (RAG) has emerged as a crucial method for addressing hallucinations in large language models (LLMs). While recent research has extended RAG models to complex noisy scenarios, these explorations often confine themselves to limited noise types and presuppose that noise is inherently detrimental to LLMs, potentially deviating from real-world retrieval environments and restricting practical applicability. In this paper, we define seven distinct noise types from a linguistic perspective and establish a Noise RAG Benchmark (NoiserBench), a comprehensive evaluation framework encompassing multiple datasets and reasoning tasks. Through empirical evaluation of eight representative LLMs with diverse architectures and scales, we reveal that these noises can be further categorized into two practical groups: noise that is beneficial to LLMs (aka beneficial noise) and noise that is harmful to LLMs (aka harmful noise). While harmful noise generally impairs performance, beneficial noise may enhance several aspects of model capabilities and overall performance. Our analysis offers insights for developing robust RAG solutions and mitigating hallucinations across diverse retrieval scenarios. Code is available at https://github.com/jinyangwu/NoiserBench.</abstract>
      <url hash="ac4af489">2025.acl-long.250</url>
      <bibkey>wu-etal-2025-pandoras</bibkey>
    </paper>
    <paper id="251">
      <title>Stepwise Reasoning Disruption Attack of <fixed-case>LLM</fixed-case>s</title>
      <author><first>Jingyu</first><last>Peng</last></author>
      <author><first>Maolin</first><last>Wang</last></author>
      <author><first>Xiangyu</first><last>Zhao</last><affiliation>City University of Hong Kong</affiliation></author>
      <author><first>Kai</first><last>Zhang</last></author>
      <author><first>Wanyu</first><last>Wang</last></author>
      <author><first>Pengyue</first><last>Jia</last><affiliation>City University of Hong Kong</affiliation></author>
      <author><first>Qidong</first><last>Liu</last><affiliation>City University of Hong Kong and Xi’an Jiaotong University</affiliation></author>
      <author><first>Ruocheng</first><last>Guo</last><affiliation>Intuit AI</affiliation></author>
      <author><first>Qi</first><last>Liu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>5040-5058</pages>
      <abstract>Large language models (LLMs) have made remarkable strides in complex reasoning tasks, but their safety and robustness in reasoning processes remain unexplored, particularly in third-party platforms that facilitate user interactions via APIs. Existing attacks on LLM reasoning are constrained by specific settings or lack of imperceptibility, limiting their feasibility and generalizability. To address these challenges, we propose the Stepwise rEasoning Error Disruption (SEED) attack, which subtly injects errors into prior reasoning steps to mislead the model into producing incorrect subsequent reasoning and final answers. Unlike previous methods, SEED is compatible with zero-shot and few-shot settings, maintains the natural reasoning flow, and ensures covert execution without modifying the instruction. Extensive experiments on four datasets across four different models demonstrate SEED’s effectiveness, revealing the vulnerabilities of LLMs to disruptions in reasoning processes. These findings underscore the need for greater attention to the robustness of LLM reasoning to ensure safety in practical applications. Our code is available at: https://github.com/Applied-Machine-Learning-Lab/SEED-Attack</abstract>
      <url hash="4ab97a8b">2025.acl-long.251</url>
      <bibkey>peng-etal-2025-stepwise</bibkey>
    </paper>
    <paper id="252">
      <title>Crowd Comparative Reasoning: Unlocking Comprehensive Evaluations for <fixed-case>LLM</fixed-case>-as-a-Judge</title>
      <author><first>Qiyuan</first><last>Zhang</last><affiliation>City University of Hong Kong</affiliation></author>
      <author><first>Yufei</first><last>Wang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Yuxin</first><last>Jiang</last></author>
      <author><first>Liangyou</first><last>Li</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Chuhan</first><last>Wu</last><affiliation>WeChat AI, Tencent</affiliation></author>
      <author><first>Yasheng</first><last>Wang</last></author>
      <author><first>Xin</first><last>Jiang</last></author>
      <author><first>Lifeng</first><last>Shang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Ruiming</first><last>Tang</last></author>
      <author><first>Fuyuan</first><last>Lyu</last><affiliation>McGill University, McGill University and Mila - Quebec Artificial Intelligence Institute</affiliation></author>
      <author><first>Chen</first><last>Ma</last><affiliation>City University of Hong Kong</affiliation></author>
      <pages>5059-5074</pages>
      <abstract>LLM-as-a-Judge, which generates chain-of-thought (CoT) judgments, has become a widely adopted auto-evaluation method. However, its reliability is compromised by the CoT reasoning’s inability to capture comprehensive and deeper details, often leading to incomplete outcomes. Existing methods mainly rely on majority voting or criteria expansion, which is insufficient to address the limitation in CoT. We propose Crowd-based Comparative Evaluation, which introduces additional crowd responses to compare with the candidate responses, thereby exposing deeper and more comprehensive details within the candidate responses. This process effectively guides LLM-as-a-Judge to provide a more detailed CoT judgment. Extensive experiments demonstrate that our approach enhances evaluation reliability, achieving an average accuracy gain of 6.7% across five benchmarks. Moreover, our method produces higher-quality CoTs that facilitate judge distillation and exhibit superior performance in rejection sampling for supervised fine-tuning (SFT), referred to as crowd rejection sampling, thereby enabling more efficient SFT. Our analysis confirms that CoTs generated by ours are more comprehensive and of higher quality, and evaluation accuracy improves as inference scales.</abstract>
      <url hash="fe701c11">2025.acl-long.252</url>
      <bibkey>zhang-etal-2025-crowd</bibkey>
    </paper>
    <paper id="253">
      <title>Lost in Multilinguality: Dissecting Cross-lingual Factual Inconsistency in Transformer Language Models</title>
      <author><first>Mingyang</first><last>Wang</last></author>
      <author><first>Heike</first><last>Adel</last><affiliation>Hochschule der Medien (University of Applied Sciences)</affiliation></author>
      <author><first>Lukas</first><last>Lange</last><affiliation>Robert Bosch GmbH, Bosch</affiliation></author>
      <author><first>Yihong</first><last>Liu</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Ercong</first><last>Nie</last></author>
      <author><first>Jannik</first><last>Strötgen</last><affiliation>Karlsruhe University of Applied Sciences</affiliation></author>
      <author><first>Hinrich</first><last>Schuetze</last></author>
      <pages>5075-5094</pages>
      <abstract>Multilingual language models (MLMs) store factual knowledge across languages but often struggle to provide consistent responses to semantically equivalent prompts in different languages. While previous studies point out this cross-lingual inconsistency issue, the underlying causes remain unexplored. In this work, we use mechanistic interpretability methods to investigate cross-lingual inconsistencies in MLMs. We find that MLMs encode knowledge in a language-independent concept space through most layers, and only transition to language-specific spaces in the final layers. Failures during the language transition often result in incorrect predictions in the target language, even when the answers are correct in other languages. To mitigate this inconsistency issue, we propose a linear shortcut method that bypasses computations in the final layers, enhancing both prediction accuracy and cross-lingual consistency. Our findings shed light on the internal mechanisms of MLMs and provide a lightweight, effective strategy for producing more consistent factual outputs.</abstract>
      <url hash="fcb4004a">2025.acl-long.253</url>
      <bibkey>wang-etal-2025-lost-multilinguality</bibkey>
    </paper>
    <paper id="254">
      <title>Optimizing Decomposition for Optimal Claim Verification</title>
      <author><first>Yining</first><last>Lu</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Noah</first><last>Ziems</last></author>
      <author><first>Hy</first><last>Dang</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Meng</first><last>Jiang</last><affiliation>University of Notre Dame</affiliation></author>
      <pages>5095-5114</pages>
      <abstract>Current research on the Decompose-Then-Verify paradigm for evaluating the factuality of long-form text typically treats decomposition and verification in isolation, overlooking their interactions and potential misalignment. We find that existing decomposition policies, typically hand-crafted demonstrations, do not align well with downstream verifiers in terms of atomicity—a novel metric quantifying information density—leading to suboptimal verification results. We formulate finding the optimal decomposition policy for optimal verification as a bilevel optimization problem. To approximate a solution for this strongly NP-hard problem, we propose dynamic decomposition, a reinforcement learning framework that leverages verifier feedback to learn a policy for dynamically decomposing claims to verifier-preferred atomicity. Experimental results show that dynamic decomposition outperforms existing decomposition policies, improving verification confidence by 0.07 and accuracy by 0.12 (on a 0-1 scale) on average across varying verifiers, datasets, and atomcities of input claims.</abstract>
      <url hash="5ffb56b0">2025.acl-long.254</url>
      <bibkey>lu-etal-2025-optimizing</bibkey>
    </paper>
    <paper id="255">
      <title><fixed-case>G</fixed-case>rad<fixed-case>OT</fixed-case>: Training-free Gradient-preserving Offsite-tuning for Large Language Models</title>
      <author><first>Kai</first><last>Yao</last></author>
      <author><first>Zhaorui</first><last>Tan</last><affiliation>Xi’an Jiaotong-Liverpool University and University of Liverpool</affiliation></author>
      <author><first>Penglei</first><last>Gao</last><affiliation>cleveland clinic foundation</affiliation></author>
      <author><first>Lichun</first><last>Li</last></author>
      <author><first>Kaixin</first><last>Wu</last><affiliation>Ant Group</affiliation></author>
      <author><first>Yinggui</first><last>Wang</last></author>
      <author><first>Yuan</first><last>Zhao</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yixin</first><last>Ji</last><affiliation>Soochow University</affiliation></author>
      <author><first>Jianke</first><last>Zhu</last></author>
      <author><first>Wei</first><last>Wang</last></author>
      <pages>5115-5130</pages>
      <abstract>The rapid growth of large language models (LLMs) with traditional centralized fine-tuning emerges as a key technique for adapting these models to domain-specific challenges, yielding privacy risks for both model and data owners. One promising solution, called offsite-tuning (OT), is proposed to address these challenges, where a weaker emulator is compressed from the original model and further fine-tuned with adapter to enhance privacy. However, the existing OT-based methods require high computational costs and lack theoretical analysis. This paper introduces a novel OT approach based on gradient-preserving compression. By analyzing the OT problem through the lens of optimization, we propose a method that selectively applies compression techniques such as rank compression and channel pruning, preserving the gradients of fine-tuned adapters while ensuring privacy. Extensive experiments demonstrate that our approach surpasses existing OT methods, both in terms of privacy protection and model performance. Our method provides a theoretical foundation for OT and offers a practical, training-free solution for offsite-tuning of large-scale LLMs.</abstract>
      <url hash="3769e144">2025.acl-long.255</url>
      <bibkey>yao-etal-2025-gradot</bibkey>
    </paper>
    <paper id="256">
      <title>Knowledge Boundary of Large Language Models: A Survey</title>
      <author><first>Moxin</first><last>Li</last></author>
      <author><first>Yong</first><last>Zhao</last></author>
      <author><first>Wenxuan</first><last>Zhang</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Shuaiyi</first><last>Li</last><affiliation>Chinese University of Hong Kong, The Chinese University of Hong Kong</affiliation></author>
      <author><first>Wenya</first><last>Xie</last><affiliation>University of Minnesota - Twin Cities</affiliation></author>
      <author><first>See-Kiong</first><last>Ng</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Tat-Seng</first><last>Chua</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Yang</first><last>Deng</last><affiliation>Singapore Management University</affiliation></author>
      <pages>5131-5157</pages>
      <abstract>Although large language models (LLMs) store vast amount of knowledge in their parameters, they still have limitations in the memorization and utilization of certain knowledge, leading to undesired behaviors such as generating untruthful and inaccurate responses. This highlights the critical need to understand the knowledge boundary of LLMs, a concept that remains inadequately defined in existing research. In this survey, we propose a comprehensive definition of the LLM knowledge boundary and introduce a formalized taxonomy categorizing knowledge into four distinct types. Using this foundation, we systematically review the field through three key lenses: the motivation for studying LLM knowledge boundaries, methods for identifying these boundaries, and strategies for mitigating the challenges they present. Finally, we discuss open challenges and potential research directions in this area. We aim for this survey to offer the community a comprehensive overview, facilitate access to key issues, and inspire further advancements in LLM knowledge research.</abstract>
      <url hash="e3dd4c6a">2025.acl-long.256</url>
      <bibkey>li-etal-2025-knowledge-boundary</bibkey>
    </paper>
    <paper id="257">
      <title>Mitigating Visual Forgetting via Take-along Visual Conditioning for Multi-modal Long <fixed-case>C</fixed-case>o<fixed-case>T</fixed-case> Reasoning</title>
      <author><first>Hai-Long</first><last>Sun</last></author>
      <author><first>Zhun</first><last>Sun</last><affiliation>Tohoku University</affiliation></author>
      <author><first>Houwen</first><last>Peng</last></author>
      <author><first>Han-Jia</first><last>Ye</last></author>
      <pages>5158-5171</pages>
      <abstract>Recent advancements in Large Language Models (LLMs) have demonstrated enhanced reasoning capabilities, evolving from Chain-of-Thought (CoT) prompting to advanced, product-oriented solutions like OpenAI o1. During our re-implementation of this model, we noticed that in multimodal tasks requiring visual input (e.g., geometry problems), Multimodal LLMs (MLLMs) struggle to maintain focus on the visual information, in other words, MLLMs suffer from a gradual decline in attention to visual information as reasoning progresses, causing text-over-relied outputs. To investigate this, we ablate image inputs during long-chain reasoning. Concretely, we truncate the reasoning process midway, then re-complete the reasoning process with the input image removed. We observe only a ~2 accuracy drop on MathVista’s test-hard subset, revealing the model’s textual outputs dominate the following reasoning process. Motivated by this, we propose Take-along Visual Conditioning (TVC), a strategy that shifts image input to critical reasoning stages and compresses redundant visual tokens via dynamic pruning. This methodology helps the model retain attention to the visual components throughout the reasoning. Our approach achieves state-of-the-art performance on average across five mathematical reasoning benchmarks (+3.4% vs previous sota), demonstrating the effectiveness of TVC in enhancing multimodal reasoning systems. The project page is available at <url>https://sun-hailong.github.io/projects/TVC</url>.</abstract>
      <url hash="cdde190d">2025.acl-long.257</url>
      <bibkey>sun-etal-2025-mitigating-visual</bibkey>
    </paper>
    <paper id="258">
      <title><fixed-case>M</fixed-case>o<fixed-case>C</fixed-case>: Mixtures of Text Chunking Learners for Retrieval-Augmented Generation System</title>
      <author><first>Jihao</first><last>Zhao</last></author>
      <author><first>Zhiyuan</first><last>Ji</last></author>
      <author><first>Zhaoxin</first><last>Fan</last><affiliation>Beijing University of Aeronautics and Astronautics</affiliation></author>
      <author><first>Hanyu</first><last>Wang</last></author>
      <author><first>Simin</first><last>Niu</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Bo</first><last>Tang</last></author>
      <author><first>Feiyu</first><last>Xiong</last><affiliation>Institute for Advanced Algorithms Research, Shanghai</affiliation></author>
      <author><first>Zhiyu</first><last>Li</last></author>
      <pages>5172-5189</pages>
      <abstract>Retrieval-Augmented Generation (RAG), while serving as a viable complement to large language models (LLMs), often overlooks the crucial aspect of text chunking within its pipeline. This paper initially introduces a dual-metric evaluation method, comprising Boundary Clarity and Chunk Stickiness, to enable the direct quantification of chunking quality. Leveraging this assessment method, we highlight the inherent limitations of traditional and semantic chunking in handling complex contextual nuances, thereby substantiating the necessity of integrating LLMs into chunking process. To address the inherent trade-off between computational efficiency and chunking precision in LLM-based approaches, we devise the granularity-aware Mixture-of-Chunkers (MoC) framework, which consists of a three-stage processing mechanism. Notably, our objective is to guide the chunker towards generating a structured list of chunking regular expressions, which are subsequently employed to extract chunks from the original text. Extensive experiments demonstrate that both our proposed metrics and the MoC framework effectively settle challenges of the chunking task, revealing the chunking kernel while enhancing the performance of the RAG system.</abstract>
      <url hash="f7e3ef81">2025.acl-long.258</url>
      <bibkey>zhao-etal-2025-moc</bibkey>
    </paper>
    <paper id="259">
      <title>Mitigating Selection Bias with Node Pruning and Auxiliary Options</title>
      <author><first>Hyeong Kyu</first><last>Choi</last><affiliation>Department of Computer Science, University of Wisconsin - Madison</affiliation></author>
      <author><first>Weijie</first><last>Xu</last><affiliation>Amazon</affiliation></author>
      <author><first>Chi</first><last>Xue</last><affiliation>Amazon</affiliation></author>
      <author><first>Stephanie</first><last>Eckman</last><affiliation>Amazon</affiliation></author>
      <author><first>Chandan K.</first><last>Reddy</last><affiliation>Virginia Tech and Amazon</affiliation></author>
      <pages>5190-5215</pages>
      <abstract>Large language models (LLMs) often exhibit systematic preferences for certain answer choices when responding to multiple-choice questions—a behavior known as selection bias. This bias reduces the accuracy and reliability of LLM outputs, limiting their usefulness in decision-critical applications. While prior work has focused on adjusting model inputs or outputs to mitigate this issue, our work takes a fundamentally different approach by identifying and removing the internal sources of bias. We introduce two methods: Bias Node Pruning (BNP), which prunes parameters that contribute to selection bias, and Auxiliary Option Injection (AOI), which introduces an additional answer choice to reduce bias in both white-box and black-box settings. To address the shortcomings of existing evaluation metrics, we propose Choice Kullback-Leibler Divergence (CKLD), a new metric that captures distributional imbalances in model predictions. Experiments on three LLMs across multiple datasets demonstrate that our methods consistently improve answer accuracy while reducing selection bias, providing a robust solution for both open- and closed-source models.</abstract>
      <url hash="a632c71f">2025.acl-long.259</url>
      <bibkey>choi-etal-2025-mitigating</bibkey>
    </paper>
    <paper id="260">
      <title>Dually Self-Improved Counterfactual Data Augmentation Using Large Language Model</title>
      <author><first>Luhao</first><last>Zhang</last></author>
      <author><first>Xinyu</first><last>Zhang</last></author>
      <author><first>Linmei</first><last>Hu</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Dandan</first><last>Song</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Liqiang</first><last>Nie</last><affiliation>Harbin Institute of Technology (Shenzhen) and Shandong University</affiliation></author>
      <pages>5216-5227</pages>
      <abstract>Counterfactual data augmentation, which generates minimally edited tokens to alter labels, has become a key approach to improving model robustness in natural language processing (NLP). It is usually implemented by first identifying the causal terms and then modifying these terms to create counterfactual candidates. The emergence of large language models (LLMs) has effectively facilitated the task of counterfactual data augmentation. However, existing LLM-based approaches still face some challenges in 1) accurately extracting the task-specific causal terms, and 2) the quality of LLM-generated counterfacts. To address the issues, we propose a dually self-improved counterfactual data augmentation method using LLM for the Natural Language Inference (NLI) task. On the one hand, we design a self-improved strategy employing the attention distribution of the task model to identify the task-specific causal terms, which is lightweight and task-specific. On the other hand, a second self-improved strategy based on direct preference optimization is utilized to refine LLM-generated counterfacts, achieving high-quality counterfacts. Finally, a balanced loss preventing over-emphasis on augmented data is proposed to retrain the task model on the fusion of existing data and generated counterfacts. Extensive experiments on NLI benchmarks demonstrate the effectiveness of our proposed method in generating high-quality counterfacts for improving task performance.</abstract>
      <url hash="6e897f37">2025.acl-long.260</url>
      <bibkey>zhang-etal-2025-dually</bibkey>
    </paper>
    <paper id="261">
      <title><fixed-case>RPO</fixed-case>: Retrieval Preference Optimization for Robust Retrieval-Augmented Generation</title>
      <author><first>Shi-Qi</first><last>Yan</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Quan</first><last>Liu</last></author>
      <author><first>Zhen-Hua</first><last>Ling</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>5228-5240</pages>
      <abstract>While Retrieval-Augmented Generation (RAG) has exhibited promise in utilizing external knowledge, its generation process heavily depends on the quality and accuracy of the retrieved context. Large language models (LLMs) struggle to evaluate the correctness of non-parametric knowledge retrieved externally when it differs from internal memorization, leading to knowledge conflicts during response generation. To this end, we introduce the **R**etrieval **P**reference **O**ptimization (RPO), a lightweight and effective alignment method to adaptively leverage multi-source knowledge based on retrieval relevance. An implicit representation of retrieval relevance is derived and incorporated into the reward model to integrate retrieval evaluation and response generation into a single model, solving the problem that previous methods necessitate the additional procedure to assess the retrieval quality. Notably, RPO is a RAG-dedicated alignment approach that quantifies the awareness of retrieval relevance in training, first overcoming mathematical obstacles. Experiments on four datasets demonstrate that RPO outperforms RAG by 4-10% in accuracy without any extra component, exhibiting its robust generalization.</abstract>
      <url hash="4e1eccce">2025.acl-long.261</url>
      <bibkey>yan-etal-2025-rpo</bibkey>
    </paper>
    <paper id="262">
      <title>Learning to Reason from Feedback at Test-Time</title>
      <author><first>Yanyang</first><last>Li</last></author>
      <author><first>Michael R.</first><last>Lyu</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Liwei</first><last>Wang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>5241-5253</pages>
      <abstract>Solving complex tasks in a single attempt is challenging for large language models (LLMs). Iterative interaction with the environment and feedback is often required to achieve success, making effective feedback utilization a critical topic. Existing approaches either struggle with length generalization or rely on naive retries without leveraging prior information. In this paper, we introduce FTTT, a novel paradigm that formulates feedback utilization as an optimization problem at test time. Additionally, we propose a learnable test-time optimizer, OpTune, to effectively exploit feedback. Experiments on two LLMs across four reasoning datasets demonstrate that FTTT and OpTune achieve superior scalability and performance.</abstract>
      <url hash="6564f9b0">2025.acl-long.262</url>
      <bibkey>li-etal-2025-learning-reason</bibkey>
    </paper>
    <paper id="263">
      <title><tex-math>\textit{L-CiteEval}</tex-math>: A Suite for Evaluating Fidelity of Long-context Models</title>
      <author><first>Zecheng</first><last>Tang</last><affiliation>Soochow University</affiliation></author>
      <author><first>Keyan</first><last>Zhou</last></author>
      <author><first>Juntao</first><last>Li</last></author>
      <author><first>Baibei</first><last>Ji</last></author>
      <author><first>Jianye</first><last>Hou</last></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>5254-5277</pages>
      <abstract>Long-context models(LCMs) have witnessed remarkable advancements in recent years, facilitating real-world tasks like long-document QA. The success of LCMs is founded on the hypothesis that the model demonstrates strong <tex-math>\textbf{fidelity}</tex-math>, enabling it to respond based on the provided long context rather than relying solely on the intrinsic knowledge acquired during pre-training. Yet, in this paper, we find that open-sourced LCMs are not as faithful as expected. We introduce <tex-math>\textit{L-CiteEval}</tex-math>, an out-of-the-box suite that can assess both generation quality and fidelity in long-context understanding tasks. It covers 11 tasks with context lengths ranging from 8K to 48K and a corresponding automatic evaluation pipeline. Evaluation of 11 cutting-edge closed-source and open-source LCMs indicates that, while there are minor differences in their generation, open-source models significantly lag behind closed-source counterparts in terms of fidelity. Furthermore, we analyze the benefits of citation generation for LCMs from both the perspective of explicit model output and the internal attention mechanism.</abstract>
      <url hash="7ece7933">2025.acl-long.263</url>
      <bibkey>tang-etal-2025-l</bibkey>
    </paper>
    <paper id="264">
      <title><tex-math>SECRET</tex-math>: Semi-supervised Clinical Trial Document Similarity Search</title>
      <author><first>Trisha</first><last>Das</last><affiliation>Department of Computer Science, UIUC</affiliation></author>
      <author><first>Afrah</first><last>Shafquat</last><affiliation>Medidata Solutions</affiliation></author>
      <author><first>Mandis</first><last>Beigi</last></author>
      <author><first>Jacob</first><last>Aptekar</last></author>
      <author><first>Jimeng</first><last>Sun</last><affiliation>University of Illinois, Urbana Champaign, College of Computing and Georgia Institute of Technology</affiliation></author>
      <pages>5278-5291</pages>
      <abstract>Clinical trials are vital for evaluation of safety and efficacy of new treatments. However, clinical trials are resource-intensive, time-consuming and expensive to conduct, where errors in trial design, reduced efficacy, and safety events can result in significant delays, financial losses, and damage to reputation. These risks underline the importance of informed and strategic decisions in trial design to mitigate these risks and improve the chances of a successful trial. Identifying similar historical trials is critical as these trials can provide an important reference for potential pitfalls and challenges including serious adverse events, dosage inaccuracies, recruitment difficulties, patient adherence issues, etc. Addressing these challenges in trial design can lead to development of more effective study protocols with optimized patient safety and trial efficiency. In this paper, we present a novel method to identify similar historical trials by summarizing clinical trial protocols and searching for similar trials based on a query trial’s protocol. Our approach significantly outperforms all baselines, achieving up to a 78% improvement in recall@1 and a 53% improvement in precision@1 over the best baseline. We also show that our method outperforms all other baselines in partial trial similarity search and zero-shot patient-trial matching, highlighting its superior utility in these tasks.</abstract>
      <url hash="a6cb6e1e">2025.acl-long.264</url>
      <bibkey>das-etal-2025-secret</bibkey>
    </paper>
    <paper id="265">
      <title>Geometric Signatures of Compositionality Across a Language Model’s Lifetime</title>
      <author><first>Jin Hwa</first><last>Lee</last></author>
      <author><first>Thomas</first><last>Jiralerspong</last><affiliation>Montreal Institute for Learning Algorithms, University of Montreal, Université de Montréal</affiliation></author>
      <author><first>Lei</first><last>Yu</last><affiliation>Meta</affiliation></author>
      <author><first>Yoshua</first><last>Bengio</last><affiliation>University of Montreal</affiliation></author>
      <author><first>Emily</first><last>Cheng</last></author>
      <pages>5292-5320</pages>
      <abstract>By virtue of linguistic compositionality, few syntactic rules and a finite lexicon can generate an unbounded number of sentences. That is, language, though seemingly high-dimensional, can be explained using relatively few degrees of freedom. An open question is whether contemporary language models (LMs) reflect the intrinsic simplicity of language that is enabled by compositionality. We take a geometric view of this problem by relating the degree of compositionality in a dataset to the intrinsic dimension (ID) of its representations under an LM, a measure of feature complexity. We find not only that the degree of dataset compositionality is reflected in representations’ ID, but that the relationship between compositionality and geometric complexity arises due to learned linguistic features over training. Finally, our analyses reveal a striking contrast between nonlinear and linear dimensionality, showing they respectively encode semantic and superficial aspects of linguistic composition.</abstract>
      <url hash="5bbcc337">2025.acl-long.265</url>
      <bibkey>lee-etal-2025-geometric</bibkey>
    </paper>
    <paper id="266">
      <title>Pattern Recognition or Medical Knowledge? The Problem with Multiple-Choice Questions in Medicine</title>
      <author><first>Maxime</first><last>Griot</last><affiliation>Louvain School of Management</affiliation></author>
      <author><first>Jean</first><last>Vanderdonckt</last><affiliation>UCL</affiliation></author>
      <author><first>Demet</first><last>Yuksel</last><affiliation>Cliniques universitaires Saint-Luc</affiliation></author>
      <author><first>Coralie</first><last>Hemptinne</last><affiliation>Université catholique de Louvain</affiliation></author>
      <pages>5321-5341</pages>
      <abstract>Large Language Models (LLMs) such as ChatGPT demonstrate significant potential in the medical domain and are often evaluated using multiple-choice questions (MCQs) modeled on exams like the USMLE. However, such benchmarks may overestimate true clinical understanding by rewarding pattern recognition and test-taking heuristics. To investigate this, we created a fictional medical benchmark centered on an imaginary organ, the Glianorex, allowing us to separate memorized knowledge from reasoning ability. We generated textbooks and MCQs in English and French using leading LLMs, then evaluated proprietary, open-source, and domain-specific models in a zero-shot setting. Despite the fictional content, models achieved an average score of 64%, while physicians scored only 27%. Fine-tuned medical models outperformed base models in English but not in French. Ablation and interpretability analyses revealed that models frequently relied on shallow cues, test-taking strategies, and hallucinated reasoning to identify the correct choice. These results suggest that standard MCQ-based evaluations may not effectively measure clinical reasoning and highlight the need for more robust, clinically meaningful assessment methods for LLMs.</abstract>
      <url hash="e2ac2478">2025.acl-long.266</url>
      <bibkey>griot-etal-2025-pattern</bibkey>
    </paper>
    <paper id="267">
      <title>People who frequently use <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case> for writing tasks are accurate and robust detectors of <fixed-case>AI</fixed-case>-generated text</title>
      <author><first>Jenna</first><last>Russell</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Marzena</first><last>Karpinska</last><affiliation>Microsoft</affiliation></author>
      <author><first>Mohit</first><last>Iyyer</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>5342-5373</pages>
      <abstract>In this paper, we study how well humans can detect text generated by commercial LLMs (GPT-4o, Claude, o1). We hire annotators to read 300 non-fiction English articles, label them as either human-written or AI-generated, and provide paragraph-length explanations for their decisions. Our experiments show that annotators who frequently use LLMs for writing tasks excel at detecting AI-generated text, even without any specialized training or feedback. In fact, the majority vote among five such “expert” annotators misclassifies only 1 of 300 articles, significantly outperforming most commercial and open-source detectors we evaluated even in the presence of evasion tactics like paraphrasing and humanization. Qualitative analysis of the experts’ free-form explanations shows that while they rely heavily on specific lexical clues (‘AI vocabulary’), they also pick up on more complex phenomena within the text (e.g., formality, originality, clarity) that are challenging to assess for automatic detectors. We release our annotated dataset and code to spur future research into both human and automated detection of AI-generated text.</abstract>
      <url hash="96c62f19">2025.acl-long.267</url>
      <bibkey>russell-etal-2025-people</bibkey>
    </paper>
    <paper id="268">
      <title><fixed-case>Y</fixed-case>u<fixed-case>L</fixed-case>an-Mini: Pushing the Limits of Open Data-efficient Language Model</title>
      <author><first>Hu</first><last>Yiwen</last></author>
      <author><first>Huatong</first><last>Song</last></author>
      <author><first>Jie</first><last>Chen</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Jia</first><last>Deng</last></author>
      <author><first>Jiapeng</first><last>Wang</last></author>
      <author><first>Kun</first><last>Zhou</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Yutao</first><last>Zhu</last></author>
      <author><first>Jinhao</first><last>Jiang</last></author>
      <author><first>Zican</first><last>Dong</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Yang</first><last>Lu</last><affiliation>Cheung Kong Graduate School of business</affiliation></author>
      <author><first>Xu</first><last>Miao</last><affiliation>DataCanvas</affiliation></author>
      <author><first>Xin</first><last>Zhao</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Ji-Rong</first><last>Wen</last><affiliation>Renmin University of China</affiliation></author>
      <pages>5374-5400</pages>
      <abstract>Due to the immense resource demands and the involved complex techniques, it is still challenging for successfully pre-training a large language models (LLMs) with state-of-the-art performance. In this paper, we explore the key bottlenecks and designs during pre-training, and make the following contributions: (1) a comprehensive investigation into the factors contributing to training instability; (2) a robust optimization approach designed to mitigate training instability effectively; (3) an elaborate data pipeline that integrates data synthesis, data curriculum, and data selection. By integrating the above techniques, we create a rather low-cost training recipe and use it to pre-train YuLan-Mini, a fully-open base model with 2.4B parameters on 1.08T tokens. Remarkably, YuLan-Mini achieves top-tier performance among models of similar parameter scale, with comparable performance to industry-leading models that require significantly more data. To facilitate reproduction, we release the full details of training recipe and data composition. Project details can be accessed at the following link: https://anonymous.4open.science/r/YuLan-Mini/README.md.</abstract>
      <url hash="97254b2c">2025.acl-long.268</url>
      <bibkey>yiwen-etal-2025-yulan</bibkey>
    </paper>
    <paper id="269">
      <title>Your Model is Overconfident, and Other Lies We Tell Ourselves</title>
      <author><first>Timothee</first><last>Mickus</last><affiliation>University of Helsinki</affiliation></author>
      <author><first>Aman</first><last>Sinha</last></author>
      <author><first>Raúl</first><last>Vázquez</last></author>
      <pages>5401-5417</pages>
      <abstract>The difficulty intrinsic to a given example, rooted in its inherent ambiguity, is a key yet often overlooked factor in evaluating neural NLP models. We investigate the interplay and divergence among various metrics for assessing intrinsic difficulty, including annotator dissensus, training dynamics, and model confidence. Through a comprehensive analysis using 29 models on three datasets, we reveal that while correlations exist among these metrics, their relationships are neither linear nor monotonic. By disentangling these dimensions of uncertainty, we aim to refine our understanding of data complexity and its implications for evaluating and improving NLP models.</abstract>
      <url hash="fc75e18f">2025.acl-long.269</url>
      <bibkey>mickus-etal-2025-model</bibkey>
    </paper>
    <paper id="270">
      <title>Bridging the Language Gaps in Large Language Models with Inference-Time Cross-Lingual Intervention</title>
      <author><first>Weixuan</first><last>Wang</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Minghao</first><last>Wu</last></author>
      <author><first>Barry</first><last>Haddow</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Alexandra</first><last>Birch</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>5418-5433</pages>
      <abstract>Large Language Models (LLMs) have shown remarkable capabilities in natural language processing but exhibit significant performance gaps among different languages. Most existing approaches to address these disparities rely on pretraining or fine-tuning, which are resource-intensive. To overcome these limitations without incurring significant costs, we propose Inference-Time Cross-Lingual Intervention (INCLINE), a novel framework that enhances LLM performance on low-performing (source) languages by aligning their internal representations with those of high-performing (target) languages during inference. INCLINE initially learns alignment matrices using parallel sentences from source and target languages through a Least-Squares optimization, and then applies these matrices during inference to transform the low-performing language representations toward the high-performing language space. Extensive experiments on nine benchmarks with five LLMs demonstrate that INCLINE significantly improves performance across diverse tasks and languages, compared to recent strong baselines. Our analysis demonstrates that INCLINE is highly cost-effective and applicable to a wide range of applications. In addition, we release the code to foster research along this line.</abstract>
      <url hash="d81a1a79">2025.acl-long.270</url>
      <bibkey>wang-etal-2025-bridging</bibkey>
    </paper>
    <paper id="271">
      <title>Plug-in and Fine-tuning: Bridging the Gap between Small Language Models and Large Language Models</title>
      <author><first>Kyeonghyun</first><last>Kim</last><affiliation>Chung-Ang University</affiliation></author>
      <author><first>Jinhee</first><last>Jang</last></author>
      <author><first>Juhwan</first><last>Choi</last><affiliation>AITRICS</affiliation></author>
      <author><first>Yoonji</first><last>Lee</last><affiliation>Chung-Ang University</affiliation></author>
      <author><first>Kyohoon</first><last>Jin</last><affiliation>DATUMO Inc.</affiliation></author>
      <author><first>YoungBin</first><last>Kim</last><affiliation>Chung-Ang University</affiliation></author>
      <pages>5434-5452</pages>
      <abstract>Large language models (LLMs) are renowned for their extensive linguistic knowledge and strong generalization capabilities, but their high computational demands make them unsuitable for resource-constrained environments. In contrast, small language models (SLMs) are computationally efficient but often lack the broad generalization capacity of LLMs. To bridge this gap, we propose PiFi, a novel framework that combines the strengths of both LLMs and SLMs to achieve high performance while maintaining efficiency. PiFi integrates a single frozen layer from an LLM into a SLM and fine-tunes the combined model for specific tasks, boosting performance without a significant increase in computational cost. We show that PiFi delivers consistent performance improvements across a range of natural language processing tasks, including both natural language understanding and generation. Moreover, our findings demonstrate PiFi’s ability to effectively leverage LLM knowledge, enhancing generalization to unseen domains and facilitating the transfer of linguistic abilities.</abstract>
      <url hash="42c9b31c">2025.acl-long.271</url>
      <bibkey>kim-etal-2025-plug</bibkey>
    </paper>
    <paper id="272">
      <title>What is Stigma Attributed to? A Theory-Grounded, Expert-Annotated Interview Corpus for Demystifying Mental-Health Stigma</title>
      <author><first>Han</first><last>Meng</last></author>
      <author><first>Yancan</first><last>Chen</last></author>
      <author><first>Yunan</first><last>Li</last></author>
      <author><first>Yitian</first><last>Yang</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Jungup</first><last>Lee</last></author>
      <author><first>Renwen</first><last>Zhang</last></author>
      <author><first>Yi-Chieh</first><last>Lee</last><affiliation>National University of Singapore</affiliation></author>
      <pages>5453-5490</pages>
      <abstract>Mental-health stigma remains a pervasive social problem that hampers treatment-seeking and recovery. Existing resources for training neural models to finely classify such stigma are limited, relying primarily on social-media or synthetic data without theoretical underpinnings. To remedy this gap, we present an expert-annotated, theory-informed corpus of human-chatbot interviews, comprising 4,141 snippets from 684 participants with documented socio-cultural backgrounds. Our experiments benchmark state-of-the-art neural models and empirically unpack the challenges of stigma detection. This dataset can facilitate research on computationally detecting, neutralizing, and counteracting mental-health stigma. Our corpus is openly available at https://github.com/HanMeng2004/Mental-Health-Stigma-Interview-Corpus.</abstract>
      <url hash="6ca79ac5">2025.acl-long.272</url>
      <bibkey>meng-etal-2025-stigma</bibkey>
    </paper>
    <paper id="273">
      <title><fixed-case>ATRI</fixed-case>: Mitigating Multilingual Audio Text Retrieval Inconsistencies by Reducing Data Distribution Errors</title>
      <author><first>Yuguo</first><last>Yin</last></author>
      <author><first>Yuxin</first><last>Xie</last></author>
      <author><first>Wenyuan</first><last>Yang</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Dongchao</first><last>Yang</last></author>
      <author><first>Jinghan</first><last>Ru</last></author>
      <author><first>Xianwei</first><last>Zhuang</last></author>
      <author><first>Liming</first><last>Liang</last></author>
      <author><first>Yuexian</first><last>Zou</last><affiliation>Peking University</affiliation></author>
      <pages>5491-5504</pages>
      <abstract>Multilingual audio-text retrieval (ML-ATR) is a challenging task that aims to retrieve audio clips or multilingual texts from databases. However, existing ML-ATR schemes suffer from inconsistencies for instance similarity matching across languages. To address the inconsistency issue in multilingual audio-text retrieval, we first identify two intuitive factors that contribute to inconsistency: misalignment between audio and multilingual text embeddings, and error propagation in model optimization. By systematically analyzing these factors, we derive theoretical weight error upper bounds for quantifying their effects and find that the main source of inconsistency is the data distribution error during training. This finding motivates our solution to reduce data distribution errors.We propose a consistent ML-ATR scheme using 1-to-k contrastive learning and audio-English co-anchor contrastive learning, aiming to mitigate the negative impact of data distribution error on recall and consistency in ML-ATR. Experimental results on the translated AudioCaps and Clotho datasets show that our scheme achieves state-of-the-art performance on recall and consistency metrics for eight mainstream languages, including English. Our code will be available at https://github.com/ATRI-ACL/ATRI-ACL.</abstract>
      <url hash="b09a486d">2025.acl-long.273</url>
      <bibkey>yin-etal-2025-atri</bibkey>
    </paper>
    <paper id="274">
      <title>Enhancing Transformers for Generalizable First-Order Logical Entailment</title>
      <author><first>Tianshi</first><last>Zheng</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Jiazheng</first><last>Wang</last></author>
      <author><first>Zihao</first><last>Wang</last><affiliation>TSY Capital</affiliation></author>
      <author><first>Jiaxin</first><last>Bai</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Hang</first><last>Yin</last></author>
      <author><first>Zheye</first><last>Deng</last><affiliation>Department of Computer Science and Engineering, Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Yangqiu</first><last>Song</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Jianxin</first><last>Li</last><affiliation>Beihang University</affiliation></author>
      <pages>5505-5524</pages>
      <abstract>Transformers, as the fundamental deep learning architecture, have demonstrated great capability in reasoning. This paper studies the generalizable first-order logical reasoning ability of transformers with their *parameterized* knowledge and how to improve it. Transformers’ capability of first-order reasoning is further captured by whether they can conduct first-order logical entailment, which is quantitatively measured by their performance in answering knowledge graph queries. We establish the connections between (1) two types of distribution shifts studied in out-of-distribution generalization and (2) unseen knowledge and query settings discussed in the task of knowledge graph query answering, which makes it possible to characterize the fine-grained generalizability. Results on our comprehensive dataset showed that transformers **outperform** previous methods designed particularly for this task and provided detailed empirical evidence about the impact of the input query syntax, token embedding, and transformer architectures on the reasoning capability of transformers. Interestingly, our results revealed the mismatch of positional encoding and other design choices of transformer architectures in previous practices. Motivated by this, we propose **TEGA**, a logic-aware architecture that significantly improves the performance in generalizable first-order logical entailment.</abstract>
      <url hash="06195358">2025.acl-long.274</url>
      <bibkey>zheng-etal-2025-enhancing-transformers</bibkey>
    </paper>
    <paper id="275">
      <title>Self-Taught Agentic Long Context Understanding</title>
      <author><first>Yufan</first><last>Zhuang</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Xiaodong</first><last>Yu</last><affiliation>Advanced Micro Devices</affiliation></author>
      <author><first>Jialian</first><last>Wu</last><affiliation>Advanced Micro Devices</affiliation></author>
      <author><first>Ximeng</first><last>Sun</last><affiliation>Advanced Micro Devices</affiliation></author>
      <author><first>Ze</first><last>Wang</last><affiliation>Advanced Micro Devices</affiliation></author>
      <author><first>Jiang</first><last>Liu</last><affiliation>Advanced Micro Devices</affiliation></author>
      <author><first>Yusheng</first><last>Su</last><affiliation>Advanced Micro Devices</affiliation></author>
      <author><first>Jingbo</first><last>Shang</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Zicheng</first><last>Liu</last><affiliation>Advanced Micro Devices</affiliation></author>
      <author><first>Emad</first><last>Barsoum</last><affiliation>AMD</affiliation></author>
      <pages>5525-5537</pages>
      <abstract>Answering complex, long-context questions remains a major challenge for large language models (LLMs) as it requires effective question clarifications and context retrieval. We propose Agentic Long-Context Understanding (AgenticLU), a framework designed to enhance an LLM’s understanding of such queries by integrating targeted self-clarification with contextual grounding within an agentic workflow. At the core of AgenticLU is Chain-of-Clarifications (CoC), where models refine their understanding through self-generated clarification questions and corresponding contextual groundings. By scaling inference as a tree search where each node represents a CoC step, we achieve 97.8% answer recall on NarrativeQA with a search depth of up to three and a branching factor of eight. To amortize the high cost of this search process to training, we leverage the preference pairs for each step obtained by the CoC workflow and perform two-stage model finetuning: (1) supervised finetuning to learn effective decomposition strategies, and (2) direct preference optimization to enhance reasoning quality. This enables AgenticLU models to generate clarifications and retrieve relevant context effectively and efficiently in a single inference pass. Extensive experiments across seven long-context tasks demonstrate that AgenticLU significantly outperforms state-of-the-art prompting methods and specialized long-context LLMs, achieving robust multi-hop reasoning while sustaining consistent performance as context length grows.</abstract>
      <url hash="22590178">2025.acl-long.275</url>
      <bibkey>zhuang-etal-2025-self</bibkey>
    </paper>
    <paper id="276">
      <title>Hallucination Detox: Sensitivity Dropout (<fixed-case>S</fixed-case>en<fixed-case>D</fixed-case>) for Large Language Model Training</title>
      <author><first>Shahrad</first><last>Mohammadzadeh</last></author>
      <author><first>Juan David</first><last>Guerra</last></author>
      <author><first>Marco</first><last>Bonizzato</last><affiliation>École Polytechnique de Montréal, Université de Montréal, Université de Montréal, Université de Montréal and EPFL - EPF Lausanne</affiliation></author>
      <author><first>Reihaneh</first><last>Rabbany</last><affiliation>McGill University and Montreal Institute for Learning Algorithms, University of Montreal, University of Montreal</affiliation></author>
      <author><first>Golnoosh</first><last>Farnadi</last><affiliation>McGill University</affiliation></author>
      <pages>5538-5554</pages>
      <abstract>As large language models (LLMs) become increasingly prevalent, concerns about their reliability, particularly due to hallucinations - factually inaccurate or irrelevant outputs - have grown. Our research investigates the relationship between the uncertainty in training dynamics and the emergence of hallucinations. Using models from the Pythia suite and several hallucination detection metrics, we analyze hallucination trends and identify significant variance during training. To address this, we propose Sensitivity Dropout (SenD), a novel training protocol designed to reduce hallucination variance during training by deterministically dropping embedding indices with significant variability. In addition, we develop an unsupervised hallucination detection metric, Efficient EigenScore (EES), which approximates the traditional EigenScore in 2x speed. This metric is integrated into our training protocol, allowing SenD to be both computationally scalable and effective at reducing hallucination variance. SenD improves test-time reliability of Pythia and Meta’s Llama models by up to 17% and enhances factual accuracy in Wikipedia, Medical, Legal, and Coding domains without affecting downstream task performance.</abstract>
      <url hash="bf25016c">2025.acl-long.276</url>
      <bibkey>mohammadzadeh-etal-2025-hallucination</bibkey>
    </paper>
    <paper id="277">
      <title><fixed-case>OS</fixed-case>-Genesis: Automating <fixed-case>GUI</fixed-case> Agent Trajectory Construction via Reverse Task Synthesis</title>
      <author><first>Qiushi</first><last>Sun</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Kanzhi</first><last>Cheng</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Zichen</first><last>Ding</last></author>
      <author><first>Chuanyang</first><last>Jin</last></author>
      <author><first>Yian</first><last>Wang</last></author>
      <author><first>Fangzhi</first><last>Xu</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Zhenyu</first><last>Wu</last></author>
      <author><first>Chengyou</first><last>Jia</last></author>
      <author><first>Liheng</first><last>Chen</last></author>
      <author><first>Zhoumianze</first><last>Liu</last></author>
      <author><first>Ben</first><last>Kao</last><affiliation>the University of Hong Kong, University of Hong Kong</affiliation></author>
      <author><first>Guohao</first><last>Li</last><affiliation>University of Oxford</affiliation></author>
      <author><first>Junxian</first><last>He</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Yu</first><last>Qiao</last><affiliation>Shanghai Aritifcal Intelligence Laboratory</affiliation></author>
      <author><first>Zhiyong</first><last>Wu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <pages>5555-5579</pages>
      <abstract>Graphical User Interface (GUI) agents powered by Vision-Language Models (VLMs) have demonstrated human-like computer control capability. Despite their utility in advancing digital automation, the development of such agents faces a critical bottleneck: collecting high-quality trajectory data for training. Common practices for collecting such data rely on human supervision or synthetic data generation through executing pre-defined tasks, which are either resource-intensive or unable to guarantee data quality. Further, these approaches exhibit significant gaps between the generated data and online environments, alongside limited data diversity. To address this issue, we introduce OS-Genesis, a novel GUI data synthesis pipeline that overcomes the challenges above. Unlike prior methods that rely on preset tasks, OS-Genesis reverse engineers the GUI trajectory construction process. Agents first perceive environments and perform step-level interactions, then retrospectively derive high-quality tasks to enable trajectory-level exploration. A trajectory reward model is then employed to ensure the quality of the generated trajectories. We demonstrate that training GUI agents with OS-Genesis significantly improves their performance on highly challenging online benchmarks. In-depth analysis further validates OS-Genesis’s cost-effectiveness and its superior data quality and diversity compared to existing synthesis methods.</abstract>
      <url hash="821a645b">2025.acl-long.277</url>
      <bibkey>sun-etal-2025-os</bibkey>
    </paper>
    <paper id="278">
      <title><fixed-case>CORAL</fixed-case>: Learning Consistent Representations across Multi-step Training with Lighter Speculative Drafter</title>
      <author><first>Yepeng</first><last>Weng</last></author>
      <author><first>Dianwen</first><last>Mei</last><affiliation>Lenovo Group Limited</affiliation></author>
      <author><first>Huishi</first><last>Qiu</last></author>
      <author><first>Xujie</first><last>Chen</last><affiliation>Lenovo Research</affiliation></author>
      <author><first>Li</first><last>Liu</last></author>
      <author><first>Jiang</first><last>Tian</last><affiliation>Lenovo Research</affiliation></author>
      <author><first>Zhongchao</first><last>Shi</last><affiliation>Lenovo Research</affiliation></author>
      <pages>5580-5593</pages>
      <abstract>Speculative decoding is a powerful technique that accelerates Large Language Model (LLM) inference by leveraging a lightweight speculative draft model. However, existing designs suffers in performance due to misalignment between training and inference. Recent methods have tried to solve this issue by adopting a multi-step training strategy, but the complex inputs of different training steps make it harder for the draft model to converge. To address this, we propose CORAL, a novel framework that improves both accuracy and efficiency in speculative drafting. CORAL introduces Cross-Step Representation Alignment, a method that enhances consistency across multiple training steps, significantly improving speculative drafting performance. Additionally, we identify the LM head as a major bottleneck in the inference speed of the draft model. We introduce a weight-grouping mechanism that selectively activates a subset of LM head parameters during inference, substantially reducing the latency of the draft model. We evaluate CORAL on three LLM families and three benchmark datasets, achieving speedup ratios of 2.50x-4.07x, outperforming state-of-the-art methods such as EAGLE-2 and HASS. Our results demonstrate that CORAL effectively mitigates training-inference misalignment and delivers significant speedup for modern LLMs with large vocabularies.</abstract>
      <url hash="dc3b8568">2025.acl-long.278</url>
      <bibkey>weng-etal-2025-coral</bibkey>
    </paper>
    <paper id="279">
      <title><fixed-case>C</fixed-case>on<fixed-case>S</fixed-case>im: Measuring Concept-Based Explanations’ Effectiveness with Automated Simulatability</title>
      <author><first>Antonin</first><last>Poché</last><affiliation>Institut de Recherche en Informatique de Toulouse and IRT Saint Exupery</affiliation></author>
      <author><first>Alon</first><last>Jacovi</last><affiliation>Google</affiliation></author>
      <author><first>Agustin Martin</first><last>Picard</last><affiliation>IRT Saint-Exupery</affiliation></author>
      <author><first>Victor</first><last>Boutin</last><affiliation>CNRS</affiliation></author>
      <author><first>Fanny</first><last>Jourdan</last><affiliation>IRT Saint Exupery</affiliation></author>
      <pages>5594-5615</pages>
      <abstract>Concept-based explanations work by mapping complex model computations to human-understandable concepts. Evaluating such explanations is very difficult, as it includes not only the quality of the induced space of possible concepts but also how effectively the chosen concepts are communicated to users. Existing evaluation metrics often focus solely on the former, neglecting the latter.We introduce an evaluation framework for measuring concept explanations via automated simulatability: a simulator’s ability to predict the explained model’s outputs based on the provided explanations. This approach accounts for both the concept space and its interpretation in an end-to-end evaluation. Human studies for simulatability are notoriously difficult to enact, particularly at the scale of a wide, comprehensive empirical evaluation (which is the subject of this work). We propose using large language models (LLMs) as simulators to approximate the evaluation and report various analyses to make such approximations reliable. Our method allows for scalable and consistent evaluation across various models and datasets. We report a comprehensive empirical evaluation using this framework and show that LLMs provide consistent rankings of explanation methods. Code available at Anonymous GitHub.</abstract>
      <url hash="5bbf7dc3">2025.acl-long.279</url>
      <bibkey>poche-etal-2025-consim</bibkey>
    </paper>
    <paper id="280">
      <title>Decoding Reading Goals from Eye Movements</title>
      <author><first>Omer</first><last>Shubi</last></author>
      <author><first>Cfir Avraham</first><last>Hadar</last></author>
      <author><first>Yevgeni</first><last>Berzak</last><affiliation>Technion - Israel Institute of Technology, Technion</affiliation></author>
      <pages>5616-5637</pages>
      <abstract>Readers can have different goals with respect to the text that they are reading. Can these goals be decoded from their eye movements over the text? In this work, we examine for the first time whether it is possible to distinguish between two types of common reading goals: information seeking and ordinary reading for comprehension. Using large-scale eye tracking data, we address this task with a wide range of models that cover different architectural and data representation strategies, and further introduce a new model ensemble. We find that transformer-based models with scanpath representations coupled with language modeling solve it most successfully, and that accurate predictions can be made in real time, shortly after the participant started reading the text. We further introduce a new method for model performance analysis based on mixed effect modeling. Combining this method with rich textual annotations reveals key properties of textual items and participants that contribute to the difficulty of the task, and improves our understanding of the variability in eye movement patterns across the two reading regimes.</abstract>
      <url hash="9e32729e">2025.acl-long.280</url>
      <bibkey>shubi-etal-2025-decoding</bibkey>
    </paper>
    <paper id="281">
      <title>Uncovering Visual-Semantic Psycholinguistic Properties from the Distributional Structure of Text Embedding Space</title>
      <author><first>Si</first><last>Wu</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Sebastian</first><last>Bruch</last><affiliation>Northeastern University</affiliation></author>
      <pages>5638-5649</pages>
      <abstract>Imageability (potential of text to evoke a mental image) and concreteness (perceptibility of text) are two psycholinguistic properties that link visual and semantic spaces. It is little surprise that computational methods that estimate them do so using parallel visual and semantic spaces, such as collections of image-caption pairs or multi-modal models. In this paper, we work on the supposition that text itself in an image-caption dataset offers sufficient signals to accurately estimate these properties. We hypothesize, in particular, that the peakedness of the neighborhood of a word in the semantic embedding space reflects its degree of imageability and concreteness. We then propose an unsupervised, distribution-free measure, which we call Neighborhood Stability Measure (NSM), that quantifies the sharpness of peaks. Extensive experiments show that NSM correlates more strongly with ground-truth ratings than existing unsupervised methods, and is a strong predictor of these properties for classification. Our code and data are available on GitHub (https://github.com/Artificial-Memory-Lab/imageability).</abstract>
      <url hash="9125023e">2025.acl-long.281</url>
      <bibkey>wu-bruch-2025-uncovering</bibkey>
    </paper>
    <paper id="282">
      <title><fixed-case>GUI</fixed-case>-explorer: Autonomous Exploration and Mining of Transition-aware Knowledge for <fixed-case>GUI</fixed-case> Agent</title>
      <author><first>Bin</first><last>Xie</last></author>
      <author><first>Rui</first><last>Shao</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Gongwei</first><last>Chen</last></author>
      <author><first>Kaiwen</first><last>Zhou</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Yinchuan</first><last>Li</last><affiliation>Huawei Noah’s Ark Lab (AI Lab)</affiliation></author>
      <author><first>Jie</first><last>Liu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <author><first>Liqiang</first><last>Nie</last><affiliation>Harbin Institute of Technology (Shenzhen) and Shandong University</affiliation></author>
      <pages>5650-5667</pages>
      <abstract>GUI automation faces critical challenges in dynamic environments. MLLMs suffer from two key issues: misinterpreting UI components and outdated knowledge. Traditional fine-tuning methods are costly for app-specific knowledge updates. We propose GUI-explorer, a training-free GUI agent that incorporates two fundamental mechanisms: <tex-math>\textbf{(1) Autonomous Exploration of Function-aware Trajectory}</tex-math>. To comprehensively cover all application functionalities, we design a <tex-math>\textbf{Function-aware Task Goal Generator}</tex-math> that automatically constructs exploration goals by analyzing GUI structural information (e.g., screenshots and activity hierarchies). This enables systematic exploration to collect diverse trajectories. <tex-math>\textbf{(2) Unsupervised Mining of Transition-aware Knowledge}</tex-math>. To establish precise screen-operation logic, we develop a <tex-math>\textbf{Transition-aware Knowledge Extractor}</tex-math> that extracts effective screen-operation logic through unsupervised analysis the state transition of structured interaction triples (observation, action, outcome). This eliminates the need for human involvement in knowledge extraction. With a task success rate of 53.7% on SPA-Bench and 47.4% on AndroidWorld, GUI-explorer shows significant improvements over SOTA agents. It requires no parameter updates for new apps. GUI-explorer is open-sourced and publicly available at https://github.com/JiuTian-VL/GUI-explorer.</abstract>
      <url hash="53a94d4d">2025.acl-long.282</url>
      <bibkey>xie-etal-2025-gui</bibkey>
    </paper>
    <paper id="283">
      <title>P<tex-math>^2</tex-math> Law: Scaling Law for Post-Training After Model Pruning</title>
      <author><first>Xiaodong</first><last>Chen</last></author>
      <author><first>Yuxuan</first><last>Hu</last></author>
      <author><first>Xiaokang</first><last>Zhang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Yanling</first><last>Wang</last><affiliation>Zhipu Ai</affiliation></author>
      <author><first>Cuiping</first><last>Li</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Hong</first><last>Chen</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Jing</first><last>Zhang</last><affiliation>Renmin University of China</affiliation></author>
      <pages>5668-5686</pages>
      <abstract>Pruning has become a widely adopted technique for reducing the hardware requirements of large language models (LLMs). To recover model performance after pruning, post-training is commonly employed to mitigate the resulting performance degradation. While post-training benefits from larger datasets, once the dataset size is already substantial, increasing the training data provides only limited performance gains. To balance post-training cost and model performance, it is necessary to explore the optimal amount of post-training data. Through extensive experiments on the Llama-3 and Qwen-2.5 series models, pruned using various common pruning methods, we uncover the scaling <b>Law</b> for <b>P</b>ost-training after model <b>P</b>runing, referred to as the P<tex-math>^2</tex-math> Law. This law identifies four key factors for predicting the pruned model’s post-training loss: the model size before pruning, the number of post-training tokens, the pruning rate, and the model’s loss before pruning. Moreover, P<tex-math>^2</tex-math> Law can generalize to larger dataset sizes, larger model sizes, and higher pruning rates, offering valuable insights for the post-training of pruned LLMs.</abstract>
      <url hash="8c98949f">2025.acl-long.283</url>
      <bibkey>chen-etal-2025-p2</bibkey>
    </paper>
    <paper id="284">
      <title>Making <fixed-case>FETCH</fixed-case>! Happen: Finding Emergent Dog Whistles Through Common Habitats</title>
      <author><first>Kuleen</first><last>Sasse</last></author>
      <author><first>Carlos Alejandro</first><last>Aguirre</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Isabel</first><last>Cachola</last><affiliation>Department of Computer Science, Whiting School of Engineering</affiliation></author>
      <author><first>Sharon</first><last>Levy</last><affiliation>Rutgers University</affiliation></author>
      <author><first>Mark</first><last>Dredze</last><affiliation>Department of Computer Science, Whiting School of Engineering and Bloomberg</affiliation></author>
      <pages>5687-5709</pages>
      <abstract>Dog whistles are coded expressions with dual meanings: one intended for the general public (outgroup) and another that conveys a specific message to an intended audience (ingroup). Often, these expressions are used to convey controversial political opinions while maintaining plausible deniability and slip by content moderation filters. Identification of dog whistles relies on curated lexicons, which have trouble keeping up to date. We introduce FETCH!, a task for finding novel dog whistles in massive social media corpora. We find that state-of-the-art systems fail to achieve meaningful results across three distinct social media case studies. We present EarShot, a strong baseline system that combines the strengths of vector databases and Large Language Models (LLMs) to efficiently and effectively identify new dog whistles.</abstract>
      <url hash="3bd23faf">2025.acl-long.284</url>
      <bibkey>sasse-etal-2025-making</bibkey>
    </paper>
    <paper id="285">
      <title>Lost in the Context: Insufficient and Distracted Attention to Contexts in Preference Modeling</title>
      <author><first>Shihan</first><last>Dou</last></author>
      <author><first>Jiayi</first><last>Chen</last></author>
      <author><first>Chenhao</first><last>Huang</last></author>
      <author><first>Feng</first><last>Chen</last></author>
      <author><first>Wei</first><last>Chengzhi</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Huiyuan</first><last>Zheng</last></author>
      <author><first>Shichun</first><last>Liu</last></author>
      <author><first>Yan</first><last>Liu</last></author>
      <author><first>Chenxiao</first><last>Liu</last><affiliation>Peking University</affiliation></author>
      <author><first>Chao</first><last>Xin</last></author>
      <author><first>Lin</first><last>Yan</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Zongzhang</first><last>Zhang</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Tao</first><last>Gui</last><affiliation>Fudan University</affiliation></author>
      <author><first>Qi</first><last>Zhang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xuanjing</first><last>Huang</last><affiliation>Fudan University</affiliation></author>
      <pages>5710-5728</pages>
      <abstract>In Reinforcement Learning from Human Feedback (RLHF), the reward model (RM) evaluates the response quality based on the given context and assigns a reward. It plays a crucial role in aligning RLHF with human preferences. Although the current RM training paradigm concatenates the context and response while amplifying the reward difference between good and bad response pairs, we demonstrate that the RM faces two significant issues: i) it often allocates only a small proportion of attention to the context, and ii) it frequently ignores segments of the context that are relevant for evaluating the response quality. These issues undermine the RM’s effectiveness in modeling human preferences. To further address these challenges, we propose AttnRM, a novel optimization framework that enables the RM to concentrate on crucial segments of the context. Experimental results demonstrate that AttnRM significantly improves preference modeling by increasing attention to relevant information within the context. It also enhances the RM’s generalizability and achieves better performance in aligning with human preferences.</abstract>
      <url hash="d606f64e">2025.acl-long.285</url>
      <bibkey>dou-etal-2025-lost</bibkey>
    </paper>
    <paper id="286">
      <title>Entailment-Preserving First-order Logic Representations in Natural Language Entailment</title>
      <author><first>Jinu</first><last>Lee</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Qi</first><last>Liu</last></author>
      <author><first>Runzhi</first><last>Ma</last></author>
      <author><first>Vincent</first><last>Han</last></author>
      <author><first>Ziqi</first><last>Wang</last></author>
      <author><first>Heng</first><last>Ji</last><affiliation>University of Illinois, Urbana-Champaign</affiliation></author>
      <author><first>Julia</first><last>Hockenmaier</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <pages>5729-5742</pages>
      <abstract>First-order logic (FOL) is often used to represent logical entailment, but determining natural language (NL) entailment using FOL remains a challenge. To address this, we propose the Entailment-Preserving FOL representations (EPF) task and introduce reference-free evaluation metrics for EPF (Entailment-Preserving Rate (EPR) family). In EPF, one should generate FOL representations from multi-premise NL entailment data (e.g., EntailmentBank) so that the automatic prover’s result preserves the entailment labels. Furthermore, we propose a training method specialized for the task, iterative learning-to-rank, which trains an NL-to-FOL translator by using the natural language entailment labels as verifiable rewards. Our method achieves a 1.8–2.7% improvement in EPR and a 17.4–20.6% increase in EPR@16 compared to diverse baselines in three datasets. Further analyses reveal that iterative learning-to-rank effectively suppresses the arbitrariness of FOL representation by reducing the diversity of predicate signatures, and maintains strong performance across diverse inference types and out-of-domain data.</abstract>
      <url hash="269791fa">2025.acl-long.286</url>
      <bibkey>lee-etal-2025-entailment</bibkey>
    </paper>
    <paper id="287">
      <title>Enhancing Multimodal Continual Instruction Tuning with <fixed-case>B</fixed-case>ranch<fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case></title>
      <author><first>Duzhen</first><last>Zhang</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Yong</first><last>Ren</last></author>
      <author><first>Zhong-Zhi</first><last>Li</last></author>
      <author><first>Yahan</first><last>Yu</last><affiliation>Kyoto University, Kyoto University</affiliation></author>
      <author><first>Jiahua</first><last>Dong</last></author>
      <author><first>Chenxing</first><last>Li</last></author>
      <author><first>Zhilong</first><last>Ji</last><affiliation>Tomorrow Advancing Life</affiliation></author>
      <author><first>Jinfeng</first><last>Bai</last><affiliation>TAL</affiliation></author>
      <pages>5743-5756</pages>
      <abstract>Multimodal Continual Instruction Tuning (MCIT) aims to finetune Multimodal Large Language Models (MLLMs) to continually align with human intent across sequential tasks. Existing approaches often rely on the Mixture-of-Experts (MoE) LoRA framework to preserve previous instruction alignments. However, these methods are prone to Catastrophic Forgetting (CF), as they aggregate all LoRA blocks via simple summation, which compromises performance over time. In this paper, we identify a critical parameter inefficiency in the MoELoRA framework within the MCIT context. Based on this insight, we propose BranchLoRA, an asymmetric framework to enhance both efficiency and performance. To mitigate CF, we introduce a flexible tuning-freezing mechanism within BranchLoRA, enabling branches to specialize in intra-task knowledge while fostering inter-task collaboration. Moreover, we incrementally incorporate task-specific routers to ensure an optimal branch distribution over time, rather than favoring the most recent task. To streamline inference, we introduce a task selector that automatically routes test inputs to the appropriate router without requiring task identity. Extensive experiments on the latest MCIT benchmark demonstrate that BranchLoRA significantly outperforms MoELoRA and maintains its superiority across various MLLM sizes.</abstract>
      <url hash="b9ea54fe">2025.acl-long.287</url>
      <bibkey>zhang-etal-2025-enhancing-multimodal</bibkey>
    </paper>
    <paper id="288">
      <title>Enhancing Automated Interpretability with Output-Centric Feature Descriptions</title>
      <author><first>Yoav</first><last>Gur-Arieh</last></author>
      <author><first>Roy</first><last>Mayan</last></author>
      <author><first>Chen</first><last>Agassy</last></author>
      <author><first>Atticus</first><last>Geiger</last><affiliation>Pr(Ai)²R Group</affiliation></author>
      <author><first>Mor</first><last>Geva</last><affiliation>Tel Aviv University and Google Research</affiliation></author>
      <pages>5757-5778</pages>
      <abstract>Automated interpretability pipelines generate natural language descriptions for the concepts represented by features in large language models (LLMs), such as “plants” or “the first word in a sentence”. These descriptions are derived using inputs that activate the feature, which may be a dimension or a direction in the model’s representation space. However, identifying activating inputs is costly, and the mechanistic role of a feature in model behavior is determined both by how inputs cause a feature to activate and by how feature activation affects outputs. Using steering evaluations, we reveal that current pipelines provide descriptions that fail to capture the causal effect of the feature on outputs. To fix this, we propose efficient, output-centric methods for automatically generating feature descriptions. These methods use the tokens weighted higher after feature stimulation or the highest weight tokens after applying the vocabulary “unembedding” head directly to the feature. Our output-centric descriptions better capture the causal effect of a feature on model outputs than input-centric descriptions, but combining the two leads to the best performance on both input and output evaluations. Lastly, we show that output-centric descriptions can be used to find inputs that activate features previously thought to be “dead”.</abstract>
      <url hash="67404241">2025.acl-long.288</url>
      <bibkey>gur-arieh-etal-2025-enhancing</bibkey>
    </paper>
    <paper id="289">
      <title>Towards Effective and Efficient Continual Pre-training of Large Language Models</title>
      <author><first>Jie</first><last>Chen</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Zhipeng</first><last>Chen</last></author>
      <author><first>Jiapeng</first><last>Wang</last></author>
      <author><first>Kun</first><last>Zhou</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Yutao</first><last>Zhu</last></author>
      <author><first>Jinhao</first><last>Jiang</last></author>
      <author><first>Yingqian</first><last>Min</last></author>
      <author><first>Xin</first><last>Zhao</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Zhicheng</first><last>Dou</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Jiaxin</first><last>Mao</last><affiliation>Renmin University of China, Tsinghua University</affiliation></author>
      <author><first>Yankai</first><last>Lin</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Ruihua</first><last>Song</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Jun</first><last>Xu</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Xu</first><last>Chen</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Rui</first><last>Yan</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Zhewei</first><last>Wei</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Di</first><last>Hu</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Wenbing</first><last>Huang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Ji-Rong</first><last>Wen</last><affiliation>Renmin University of China</affiliation></author>
      <pages>5779-5795</pages>
      <abstract>Continual pre-training (CPT) has been an important approach for adapting language models to specific domains or tasks. In this paper, we comprehensively study its key designs to balance the new abilities while retaining the original abilities, and present an effective CPT method that can greatly improve the Chinese language ability and scientific reasoning ability of LLMs. To achieve it, we design specific data mixture and curriculum strategies based on existing datasets and synthetic high-quality data. Concretely, we synthesize multidisciplinary scientific QA pairs based on related web pages to guarantee the data quality, and also devise the performance tracking and data mixture adjustment strategy to ensure the training stability. For the detailed designs, we conduct preliminary studies on a relatively small model, and summarize the findings to help optimize our CPT method. Extensive experiments on a number of evaluation benchmarks show that our approach can largely improve the performance of Llama-3 (8B), including both the general abilities (+8.81 on C-Eval and +6.31 on CMMLU) and the scientific reasoning abilities (+12.00 on MATH and +4.13 on SciEval). Our model, data, and codes are available at https://github.com/RUC-GSAI/Llama-3-SynE.</abstract>
      <url hash="d1ef54d8">2025.acl-long.289</url>
      <bibkey>chen-etal-2025-towards-effective</bibkey>
    </paper>
    <paper id="290">
      <title>Efficient Universal Goal Hijacking with Semantics-guided Prompt Organization</title>
      <author><first>Yihao</first><last>Huang</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Chong</first><last>Wang</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Xiaojun</first><last>Jia</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Qing</first><last>Guo</last><affiliation>National University of Singapore and Agency for Science, Technology and Research (A*STAR))</affiliation></author>
      <author><first>Felix</first><last>Juefei-Xu</last><affiliation>GenAI, Meta</affiliation></author>
      <author><first>Jian</first><last>Zhang</last><affiliation>Nanyang Technological University</affiliation></author>
      <author id="yang-liu"><first>Yang</first><last>Liu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Geguang</first><last>Pu</last><affiliation>East China Normal University</affiliation></author>
      <pages>5796-5816</pages>
      <abstract>Universal goal hijacking is a kind of prompt injection attack that forces LLMs to return a target malicious response for arbitrary normal user prompts. The previous methods achieve high attack performance while being too cumbersome and time-consuming. Also, they have concentrated solely on optimization algorithms, overlooking the crucial role of the prompt. To this end, we propose a method called POUGH that incorporates an efficient optimization algorithm and two semantics-guided prompt organization strategies. Specifically, our method starts with a sampling strategy to select representative prompts from a candidate pool, followed by a ranking strategy that prioritizes them. Given the sequentially ranked prompts, our method employs an iterative optimization algorithm to generate a fixed suffix that can concatenate to arbitrary user prompts for universal goal hijacking. Experiments conducted on four popular LLMs and ten types of target responses verified the effectiveness.</abstract>
      <url hash="39a7e1f0">2025.acl-long.290</url>
      <bibkey>huang-etal-2025-efficient</bibkey>
    </paper>
    <paper id="291">
      <title>m<fixed-case>PLUG</fixed-case>-<fixed-case>D</fixed-case>oc<fixed-case>O</fixed-case>wl2: High-resolution Compressing for <fixed-case>OCR</fixed-case>-free Multi-page Document Understanding</title>
      <author><first>Anwen</first><last>Hu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Haiyang</first><last>Xu</last></author>
      <author><first>Liang</first><last>Zhang</last></author>
      <author><first>Jiabo</first><last>Ye</last></author>
      <author><first>Ming</first><last>Yan</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Ji</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Qin</first><last>Jin</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Fei</first><last>Huang</last><affiliation>Alibaba Group US</affiliation></author>
      <author><first>Jingren</first><last>Zhou</last><affiliation>Alibaba Group</affiliation></author>
      <pages>5817-5834</pages>
      <abstract>Multimodel Large Language Models(MLLMs) have achieved promising OCR-free Document Understanding performance by increasing the supported resolution of document images. However, this comes at the cost of generating thousands of visual tokens for a single document image, leading to excessive GPU memory and slower inference times, particularly in multi-page document comprehension. In this work, to address these challenges, we propose a High-resolution DocCompressor module to compress each high-resolution document image into 324 tokens, guided by low-resolution global visual features. With this compression module, to strengthen multi-page document comprehension ability and balance both token efficiency and question-answering performance, we develop the DocOwl2 under a three-stage training framework: Single-image Pretraining, Multi-image Continue-pretraining, and Multi-task Finetuning. DocOwl2 sets a new state-of-the-art across multi-page document understanding benchmarks and reduces first token latency by more than 50%. Compared to single-image MLLMs trained on similar data, our DocOwl2 achieves comparable single-page understanding performance with less than 20% of the visual tokens. Our codes, models, and data will be publicly available.</abstract>
      <url hash="dfab0016">2025.acl-long.291</url>
      <bibkey>hu-etal-2025-mplug</bibkey>
    </paper>
    <paper id="292">
      <title>What Makes a Good Natural Language Prompt?</title>
      <author><first>Do Xuan</first><last>Long</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Duy</first><last>Dinh</last><affiliation>FPT Software</affiliation></author>
      <author><first>Ngoc-Hai</first><last>Nguyen</last><affiliation>QualComm</affiliation></author>
      <author><first>Kenji</first><last>Kawaguchi</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Nancy F.</first><last>Chen</last></author>
      <author><first>Shafiq</first><last>Joty</last><affiliation>Nanyang Technological University and SalesForce.com</affiliation></author>
      <author><first>Min-Yen</first><last>Kan</last><affiliation>National University of Singapore</affiliation></author>
      <pages>5835-5873</pages>
      <abstract>As large language models (LLMs) have progressed towards more human-like and human–AI communications prevalent, prompting has emerged as a decisive component. However, there is limited conceptual consensus on what exactly quantifies natural language prompts. We attempt to address this question by conducting a meta-analysis surveying 150+ prompting-related papers from leading NLP and AI conferences (2022–2024), and blogs. We propose a property- and human-centric framework for evaluating prompt quality, encompassing 21 properties categorized into six dimensions. We then examine how existing studies assess their impact on LLMs, revealing their imbalanced support across models and tasks, and substantial research gaps. Further, we analyze correlations among properties in high-quality natural language prompts, deriving prompting recommendations. Finally, we explore multi-property prompt enhancements in reasoning tasks, observing that single-property enhancements often have the greatest impact. Our findings establish a foundation for property-centric prompt evaluation and optimization, bridging the gaps between human–AI communication and opening new prompting research directions.</abstract>
      <url hash="c88afb35">2025.acl-long.292</url>
      <bibkey>long-etal-2025-makes</bibkey>
    </paper>
    <paper id="293">
      <title><fixed-case>X</fixed-case>-<fixed-case>TURING</fixed-case>: Towards an Enhanced and Efficient <fixed-case>T</fixed-case>uring Test for Long-Term Dialogue Agents</title>
      <author><first>Weiqi</first><last>Wu</last></author>
      <author><first>Hongqiu</first><last>Wu</last></author>
      <author><first>Hai</first><last>Zhao</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>5874-5889</pages>
      <abstract>The Turing test examines whether AIs exhibit human-like behaviour in natural language conversations. The traditional setting limits each participant to one message at a time and requires constant human participation. This fails to reflect a natural conversational style and hinders the evaluation of dialogue agents based on Large Language Models (LLMs) in complex and prolonged interactions. This paper proposes X-Turing, which enhances the original test with a burst dialogue pattern, allowing more dynamic exchanges using consecutive messages. It further reduces human workload by iteratively generating dialogues that simulate the long-term interaction between the agent and a human to compose the majority of the test process. With the pseudo-dialogue history, the agent then engages in a shorter dialogue with a real human, which is paired with a human-human conversation on the same topic to be judged using questionnaires. We introduce the X-Turn Pass-Rate metric to assess the human likeness of LLMs across varying durations. While LLMs like GPT-4 initially perform well, achieving pass rates of 51.9% and 38.9% during 3 turns and 10 turns of dialogues respectively, their performance drops as the dialogue progresses, which underscores the difficulty in maintaining consistency in the long term.</abstract>
      <url hash="2f4e02be">2025.acl-long.293</url>
      <bibkey>wu-etal-2025-x</bibkey>
    </paper>
    <paper id="294">
      <title>Are Rules Meant to be Broken? Understanding Multilingual Moral Reasoning as a Computational Pipeline with <fixed-case>U</fixed-case>ni<fixed-case>M</fixed-case>oral</title>
      <author><first>Shivani</first><last>Kumar</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>David</first><last>Jurgens</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <pages>5890-5912</pages>
      <abstract>Moral reasoning is a complex cognitive process shaped by individual experiences and cultural contexts and presents unique challenges for computational analysis. While natural language processing (NLP) offers promising tools for studying this phenomenon, current research lacks cohesion, employing discordant datasets and tasks that examine isolated aspects of moral reasoning. We bridge this gap with UniMoral, a unified dataset integrating psychologically grounded and social-media-derived moral dilemmas annotated with labels for action choices, ethical principles, contributing factors, and consequences, alongside annotators’ moral and cultural profiles. Recognizing the cultural relativity of moral reasoning, UniMoral spans six languages, Arabic, Chinese, English, Hindi, Russian, and Spanish, capturing diverse socio-cultural contexts. We demonstrate UniMoral’s utility through a benchmark evaluations of three large language models (LLMs) across four tasks: action prediction, moral typology classification, factor attribution analysis, and consequence generation. Key findings reveal that while implicitly embedded moral contexts enhance the moral reasoning capability of LLMs, there remains a critical need for increasingly specialized approaches to further advance moral reasoning in these models.</abstract>
      <url hash="e142167d">2025.acl-long.294</url>
      <bibkey>kumar-jurgens-2025-rules</bibkey>
    </paper>
    <paper id="295">
      <title>Modality-Aware Neuron Pruning for Unlearning in Multimodal Large Language Models</title>
      <author><first>Zheyuan</first><last>Liu</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Guangyao</first><last>Dou</last></author>
      <author><first>Xiangchi</first><last>Yuan</last></author>
      <author><first>Chunhui</first><last>Zhang</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Zhaoxuan</first><last>Tan</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Meng</first><last>Jiang</last><affiliation>University of Notre Dame</affiliation></author>
      <pages>5913-5933</pages>
      <abstract>Generative models such as Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) trained on massive datasets can lead them to memorize and inadvertently reveal sensitive information, raising ethical and privacy concerns. While some prior works have explored this issue in the context of LLMs, it presents a unique challenge for MLLMs due to the entangled nature of knowledge across modalities, making comprehensive unlearning more difficult. To address this challenge, we propose Modality Aware Neuron Unlearning (MANU), a novel unlearning framework for MLLMs designed to selectively clip neurons based on their relative importance to the targeted forget data, curated for different modalities. Specifically, MANU consists of two stages: important neuron selection and selective pruning. The first stage identifies and collects the most influential neurons across modalities relative to the targeted forget knowledge, while the second stage is dedicated to pruning those selected neurons. MANU effectively isolates and removes the neurons that contribute most to the forget data within each modality, while preserving the integrity of retained knowledge. Our experiments conducted across various MLLM architectures illustrate that MANU can achieve a more balanced and comprehensive unlearning in each modality without largely affecting the overall model utility.</abstract>
      <url hash="a8d11b32">2025.acl-long.295</url>
      <bibkey>liu-etal-2025-modality</bibkey>
    </paper>
    <paper id="296">
      <title><fixed-case>NGQA</fixed-case>: A Nutritional Graph Question Answering Benchmark for Personalized Health-aware Nutritional Reasoning</title>
      <author><first>Zheyuan</first><last>Zhang</last></author>
      <author><first>Yiyang</first><last>Li</last></author>
      <author><first>Nhi Ha Lan</first><last>Le</last></author>
      <author><first>Zehong</first><last>Wang</last></author>
      <author><first>Tianyi</first><last>Ma</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Vincent</first><last>Galassi</last></author>
      <author><first>Keerthiram</first><last>Murugesan</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Nuno</first><last>Moniz</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Werner</first><last>Geyer</last></author>
      <author><first>Nitesh V</first><last>Chawla</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Chuxu</first><last>Zhang</last><affiliation>University of Connecticut</affiliation></author>
      <author><first>Yanfang</first><last>Ye</last><affiliation>University of Notre Dame</affiliation></author>
      <pages>5934-5966</pages>
      <abstract>Diet plays a critical role in human health, yet tailoring dietary reasoning to individual health conditions remains a major challenge. Nutrition Question Answering (QA) has emerged as a popular method for addressing this problem. However, current research faces two critical limitations. On one hand, the absence of datasets involving user-specific medical information severely limits <i>personalization</i>. This challenge is further compounded by the wide variability in individual health needs. On the other hand, while large language models (LLMs), a popular solution for this task, demonstrate strong reasoning abilities, they struggle with the domain-specific complexities of personalized healthy dietary reasoning, and existing benchmarks fail to capture these challenges. To address these gaps, we introduce the Nutritional Graph Question Answering (NGQA) benchmark, the first graph question answering dataset designed for personalized nutritional health reasoning. NGQA leverages data from the National Health and Nutrition Examination Survey (NHANES) and the Food and Nutrient Database for Dietary Studies (FNDDS) to evaluate whether a food is healthy for a specific user, supported by explanations of the key contributing nutrients. The benchmark incorporates three question complexity settings and evaluates reasoning across three downstream tasks. Extensive experiments with LLM backbones and baseline models demonstrate that the NGQA benchmark effectively challenges existing models. In sum, NGQA addresses a critical real-world problem while advancing GraphQA research with a novel domain-specific benchmark. Our codebase and dataset are available here.</abstract>
      <url hash="6cc2fdd0">2025.acl-long.296</url>
      <bibkey>zhang-etal-2025-ngqa</bibkey>
    </paper>
    <paper id="297">
      <title><fixed-case>R</fixed-case>e<fixed-case>L</fixed-case>earn: Unlearning via Learning for Large Language Models</title>
      <author><first>Haoming</first><last>Xu</last></author>
      <author><first>Ningyuan</first><last>Zhao</last></author>
      <author><first>Liming</first><last>Yang</last></author>
      <author><first>Sendong</first><last>Zhao</last></author>
      <author><first>Shumin</first><last>Deng</last></author>
      <author><first>Mengru</first><last>Wang</last></author>
      <author><first>Bryan</first><last>Hooi</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Nay</first><last>Oo</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Huajun</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Ningyu</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <pages>5967-5987</pages>
      <abstract>Current unlearning methods for large language models usually rely on reverse optimization to reduce target token probabilities. However, this paradigm disrupts the subsequent tokens prediction, degrading model performance and linguistic coherence. Moreover, existing evaluation metrics overemphasize contextual forgetting while inadequately assessing response fluency and relevance. To address these challenges, we propose ReLearn, a data augmentation and fine-tuning pipeline for effective unlearning, along with a comprehensive evaluation framework. This framework introduces Knowledge Forgetting Ratio (KFR) and Knowledge Retention Ratio (KRR) to measure knowledge-level preservation, and Linguistic Score (LS) to evaluate generation quality. Our experiments show that ReLearn successfully achieves targeted forgetting while preserving high-quality outputs. Through mechanistic analysis, we further demonstrate how reverse optimization disrupts coherent text generation, while ReLearn preserves this essential capability.</abstract>
      <url hash="553def69">2025.acl-long.297</url>
      <bibkey>xu-etal-2025-relearn</bibkey>
    </paper>
    <paper id="298">
      <title>Understanding Cross-Domain Adaptation in Low-Resource Topic Modeling</title>
      <author><first>Pritom Saha</first><last>Akash</last></author>
      <author><first>Kevin Chen-Chuan</first><last>Chang</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <pages>5988-6001</pages>
      <abstract>Topic modeling plays a vital role in uncovering hidden semantic structures within text corpora, but existing models struggle in low-resource settings where limited target-domain data leads to unstable and incoherent topic inference. We address this challenge by formally introducing domain adaptation for low-resource topic modeling, where a high-resource source domain informs a low-resource target domain without overwhelming it with irrelevant content. We establish a finite-sample generalization bound showing that effective knowledge transfer depends on robust performance in both domains, minimizing latent-space discrepancy, and preventing overfitting to the data. Guided by these insights, we propose DALTA (Domain-Aligned Latent Topic Adaptation), a new framework that employs a shared encoder for domain-invariant features, specialized decoders for domain-specific nuances, and adversarial alignment to selectively transfer relevant information. Experiments on diverse low-resource datasets demonstrate that DALTA consistently outperforms state-of-the-art methods in terms of topic coherence, stability, and transferability.</abstract>
      <url hash="0946d3f4">2025.acl-long.298</url>
      <bibkey>akash-chang-2025-understanding</bibkey>
    </paper>
    <paper id="299">
      <title><fixed-case>UA</fixed-case>lign: Leveraging Uncertainty Estimations for Factuality Alignment on Large Language Models</title>
      <author><first>Boyang</first><last>Xue</last></author>
      <author><first>Fei</first><last>Mi</last></author>
      <author><first>Qi</first><last>Zhu</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Hongru</first><last>Wang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Rui</first><last>Wang</last></author>
      <author><first>Sheng</first><last>Wang</last></author>
      <author><first>Erxin</first><last>Yu</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Xuming</first><last>Hu</last><affiliation>The Hong Kong University of Science and Technology (Guangzhou) and Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Kam-Fai</first><last>Wong</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>6002-6024</pages>
      <abstract>Despite demonstrating impressive capabilities, Large Language Models (LLMs) still often struggle to accurately express the factual knowledge they possess, especially in cases where the LLMs’ knowledge boundaries are ambiguous. To improve LLMs’ factual expressions, we propose the UAlign framework, which leverages Uncertainty estimations to represent knowledge boundaries, and then explicitly incorporates these representations as input features into prompts for LLMs to Align with factual knowledge. First, we prepare the dataset on knowledge question-answering (QA) samples by calculating two uncertainty estimations, including confidence score and semantic entropy, to represent the knowledge boundaries for LLMs. Subsequently, using the prepared dataset, we train a reward model that incorporates uncertainty estimations and then employ the Proximal Policy Optimization (PPO) algorithm for factuality alignment on LLMs. Experimental results indicate that, by integrating uncertainty representations in LLM alignment, the proposed UAlign can significantly enhance the LLMs’ capacities to confidently answer known questions and refuse unknown questions on both in-domain and out-of-domain tasks, showing reliability improvements and good generalizability over various prompt- and training-based baselines.</abstract>
      <url hash="0392c800">2025.acl-long.299</url>
      <bibkey>xue-etal-2025-ualign</bibkey>
    </paper>
    <paper id="300">
      <title><fixed-case>C</fixed-case>o<fixed-case>T</fixed-case>-Valve: Length-Compressible Chain-of-Thought Tuning</title>
      <author><first>Xinyin</first><last>Ma</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Guangnian</first><last>Wan</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Runpeng</first><last>Yu</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Gongfan</first><last>Fang</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Xinchao</first><last>Wang</last><affiliation>National University of Singapore</affiliation></author>
      <pages>6025-6035</pages>
      <abstract>Chain-of-Thought significantly enhances a model’s reasoning capability, but it also comes with a considerable increase in inference costs due to long chains. With the observation that the reasoning path can be easily compressed under easy tasks but struggle on hard tasks, we explore the feasibility of elastically controlling the length of reasoning paths with only one model, thereby reducing the inference overhead of reasoning models dynamically based on task difficulty. We introduce a new tuning and inference strategy named CoT-Valve, designed to allow models to generate reasoning chains of varying lengths. To achieve this, we propose to identify a direction in the parameter space that, when manipulated, can effectively control the length of generated CoT. Moreover, we show that this property is valuable for compressing the reasoning chain. We construct datasets with chains from long to short for the same questions and explore two enhanced strategies for CoT-Valve: (1) a precise length-compressible CoT tuning method, and (2) a progressive chain length compression approach. Our experiments show that CoT-Valve successfully enables controllability and compressibility of the chain and shows better performance than the prompt-based control. We applied this method to QwQ-32B-Preview, reducing reasoning chains on GSM8K from 741 to 225 tokens with a minor performance drop (95.07% to 94.92%) and on AIME from 6827 to 4629 tokens, with only one additional incorrect answer.</abstract>
      <url hash="9a977a35">2025.acl-long.300</url>
      <bibkey>ma-etal-2025-cot</bibkey>
    </paper>
    <paper id="301">
      <title><fixed-case>H</fixed-case>o<fixed-case>H</fixed-case>: A Dynamic Benchmark for Evaluating the Impact of Outdated Information on Retrieval-Augmented Generation</title>
      <author><first>Jie</first><last>Ouyang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Tingyue</first><last>Pan</last></author>
      <author><first>Mingyue</first><last>Cheng</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Ruiran</first><last>Yan</last></author>
      <author><first>Yucong</first><last>Luo</last></author>
      <author><first>Jiaying</first><last>Lin</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Qi</first><last>Liu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>6036-6063</pages>
      <abstract>While Retrieval-Augmented Generation (RAG) has emerged as an effective approach for addressing the knowledge outdating problem in Large Language Models (LLMs), it still faces a critical challenge: the prevalence of outdated information in knowledge bases. Current research primarily focuses on incorporating up-to-date information, yet the impact of outdated information coexisting in retrieval sources remains inadequately addressed. To bridge this gap, we introduce HoH, the first benchmark specifically designed to evaluate the impact of outdated information on RAG. Our benchmark leverages token-level diff algorithms combined with LLM pipelines to efficiently create a large-scale QA dataset that accurately captures the evolution of temporal knowledge in real-world facts.Through comprehensive experiments, we reveal that outdated information significantly degrades RAG performance in two critical ways: (1) it substantially reduces response accuracy by distracting models from correct information, and (2) it can mislead models into generating potentially harmful outputs, even when current information is available. Current RAG approaches struggle with both retrieval and generation aspects when handling outdated information. These findings highlight the urgent need for innovative solutions to address the temporal challenges in RAG.</abstract>
      <url hash="dc4fc741">2025.acl-long.301</url>
      <bibkey>ouyang-etal-2025-hoh</bibkey>
    </paper>
    <paper id="302">
      <title>Uncertainty Propagation on <fixed-case>LLM</fixed-case> Agent</title>
      <author><first>Qiwei</first><last>Zhao</last></author>
      <author><first>Dong</first><last>Li</last></author>
      <author><first>Yanchi</first><last>Liu</last><affiliation>NEC-Labs</affiliation></author>
      <author><first>Wei</first><last>Cheng</last><affiliation>NEC-Labs</affiliation></author>
      <author><first>Yiyou</first><last>Sun</last></author>
      <author><first>Mika</first><last>Oishi</last><affiliation>NEC</affiliation></author>
      <author><first>Takao</first><last>Osaki</last><affiliation>NEC</affiliation></author>
      <author><first>Katsushi</first><last>Matsuda</last></author>
      <author><first>Huaxiu</first><last>Yao</last><affiliation>Department of Computer Science, University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Chen</first><last>Zhao</last><affiliation>Baylor University</affiliation></author>
      <author><first>Haifeng</first><last>Chen</last></author>
      <author><first>Xujiang</first><last>Zhao</last><affiliation>NEC Labs America</affiliation></author>
      <pages>6064-6073</pages>
      <abstract>Large language models (LLMs) integrated into multi-step agent systems enable complex decision-making processes across various applications. However, their outputs often lack reliability, making uncertainty estimation crucial. Existing uncertainty estimation methods primarily focus on final-step outputs, which fail to account for cumulative uncertainty over the multi-step decision-making process and the dynamic interactions between agents and their environments. To address these limitations, we propose SAUP (Situation Awareness Uncertainty Propagation), a novel framework that propagates uncertainty through each step of an LLM-based agent’s reasoning process. SAUP incorporates situational awareness by assigning situational weights to each step’s uncertainty during the propagation. Our method, compatible with various one-step uncertainty estimation techniques, provides a comprehensive and accurate uncertainty measure. Extensive experiments on benchmark datasets demonstrate that SAUP significantly outperforms existing state-of-the-art methods, achieving up to 20% improvement in AUROC.</abstract>
      <url hash="6365899c">2025.acl-long.302</url>
      <bibkey>zhao-etal-2025-uncertainty</bibkey>
    </paper>
    <paper id="303">
      <title>Beyond Position: the emergence of wavelet-like properties in Transformers</title>
      <author><first>Valeria</first><last>Ruscio</last></author>
      <author><first>Umberto</first><last>Nanni</last><affiliation>University of Roma “La Sapienza”</affiliation></author>
      <author><first>Fabrizio</first><last>Silvestri</last><affiliation>Sapienza University of Rome</affiliation></author>
      <pages>6074-6088</pages>
      <abstract>This paper studies how Transformer models with Rotary Position Embeddings (RoPE) develop emergent, wavelet-like properties that compensate for the positional encoding’s theoretical limitations. Through an analysis spanning model scales, architectures, and training checkpoints, we show that attention heads evolve to implement multi-resolution processing analogous to wavelet transforms. We demonstrate that this scale-invariant behavior is unique to RoPE, emerges through distinct evolutionary phases during training, and statistically adheres to the fundamental uncertainty principle. Our findings suggest that the effectiveness of modern Transformers stems from their remarkable ability to spontaneously develop optimal, multi-resolution decompositions to address inherent architectural constraints.</abstract>
      <url hash="b0f72c2a">2025.acl-long.303</url>
      <bibkey>ruscio-etal-2025-beyond</bibkey>
    </paper>
    <paper id="304">
      <title>Are the Hidden States Hiding Something? Testing the Limits of Factuality-Encoding Capabilities in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Giovanni</first><last>Servedio</last><affiliation>Polytechnic Institute of Bari</affiliation></author>
      <author><first>Alessandro</first><last>De Bellis</last></author>
      <author><first>Dario Di</first><last>Palma</last></author>
      <author><first>Vito Walter</first><last>Anelli</last><affiliation>Polytechnic University of Bari</affiliation></author>
      <author><first>Tommaso Di</first><last>Noia</last><affiliation>Polytechnic University of Bari - Politecnico di Bari</affiliation></author>
      <pages>6089-6104</pages>
      <abstract>Factual hallucinations are a major challenge for Large Language Models (LLMs). They undermine reliability and user trust by generating inaccurate or fabricated content. Recent studies suggest that when generating false statements, the internal states of LLMs encode information about truthfulness. However, these studies often rely on synthetic datasets that lack realism, which limits generalization when evaluating the factual accuracy of text generated by the model itself. In this paper, we challenge the findings of previous work by investigating truthfulness encoding capabilities, leading to the generation of a more realistic and challenging dataset. Specifically, we extend previous work by introducing: (1) a strategy for sampling plausible true-false factoid sentences from tabular data and (2) a procedure for generating realistic, LLM-dependent true-false datasets from Question Answering collections. Our analysis of two open-source LLMs reveals that while the findings from previous studies are partially validated, generalization to LLM-generated datasets remains challenging. This study lays the groundwork for future research on factuality in LLMs and offers practical guidelines for more effective evaluation.</abstract>
      <url hash="e6653fe5">2025.acl-long.304</url>
      <bibkey>servedio-etal-2025-hidden</bibkey>
    </paper>
    <paper id="305">
      <title>Disentangling Biased Knowledge from Reasoning in Large Language Models via Machine Unlearning</title>
      <author><first>Zheyuan</first><last>Liu</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Suraj</first><last>Maharjan</last><affiliation>Amazon</affiliation></author>
      <author><first>Fanyou</first><last>Wu</last><affiliation>Amazon</affiliation></author>
      <author><first>Rahil</first><last>Parikh</last><affiliation>Amazon</affiliation></author>
      <author><first>Belhassen</first><last>Bayar</last><affiliation>Amazon</affiliation></author>
      <author><first>Srinivasan H.</first><last>Sengamedu</last><affiliation>Amazon</affiliation></author>
      <author><first>Meng</first><last>Jiang</last><affiliation>University of Notre Dame</affiliation></author>
      <pages>6105-6123</pages>
      <abstract>The rapid development of Large Language Models (LLMs) has led to their widespread adoption across various domains, leveraging vast pre-training knowledge and impressive generalization capabilities. However, these models often inherit biased knowledge, resulting in unfair decisions in sensitive applications. It is challenging to remove this biased knowledge without compromising reasoning abilities due to the entangled nature of the learned knowledge within LLMs. To solve this problem, existing approaches have attempted to mitigate the bias using techniques such as fine-tuning with unbiased datasets, model merging, and gradient ascent. While these methods have experimentally proven effective, they can still be sub-optimum in fully disentangling biases from reasoning. To address this gap, we propose Selective Disentanglement Unlearning (SDU), a novel unlearning framework that selectively removes biased knowledge while preserving reasoning capabilities. SDU operates in three stages: identifying biased parameters using a shadow LLM, fine-tuning with unbiased data, and performing selective parameter updates based on weight saliency. Experimental results across multiple LLMs show that SDU improves fairness accuracy by 14.7% and enhances reasoning performance by 62.6% compared to existing baselines.</abstract>
      <url hash="8eeca117">2025.acl-long.305</url>
      <bibkey>liu-etal-2025-disentangling</bibkey>
    </paper>
    <paper id="306">
      <title><fixed-case>LL</fixed-case>a<fixed-case>MA</fixed-case>s Have Feelings Too: Unveiling Sentiment and Emotion Representations in <fixed-case>LL</fixed-case>a<fixed-case>MA</fixed-case> Models Through Probing</title>
      <author><first>Dario Di</first><last>Palma</last></author>
      <author><first>Alessandro</first><last>De Bellis</last></author>
      <author><first>Giovanni</first><last>Servedio</last><affiliation>Polytechnic Institute of Bari</affiliation></author>
      <author><first>Vito Walter</first><last>Anelli</last><affiliation>Polytechnic University of Bari</affiliation></author>
      <author><first>Fedelucio</first><last>Narducci</last><affiliation>Polytechnic Institute of Bari</affiliation></author>
      <author><first>Tommaso Di</first><last>Noia</last><affiliation>Polytechnic University of Bari - Politecnico di Bari</affiliation></author>
      <pages>6124-6142</pages>
      <abstract>Large Language Models (LLMs) have rapidly become central to NLP, demonstrating their ability to adapt to various tasks through prompting techniques, including sentiment analysis. However, we still have a limited understanding of how these models capture sentiment-related information. This study probes the hidden layers of LLaMA models to pinpoint where sentiment features are most represented and to assess how this affects sentiment analysis.Using probe classifiers, we analyze sentiment encoding across layers and scales, identifying the layers and pooling methods that best capture sentiment signals. Our results show that sentiment information is most concentrated in mid-layers for binary polarity tasks, with detection accuracy increasing up to 14% over prompting techniques. Additionally, we find that in decoder-only models, the last token is not consistently the most informative for sentiment encoding. Finally, this approach enables sentiment tasks to be performed with memory requirements reduced by an average of 57%.These insights contribute to a broader understanding of sentiment in LLMs, suggesting layer-specific probing as an effective approach for sentiment tasks beyond prompting, with potential to enhance model utility and reduce memory requirements.</abstract>
      <url hash="f23e04f3">2025.acl-long.306</url>
      <bibkey>palma-etal-2025-llamas</bibkey>
    </paper>
    <paper id="307">
      <title><fixed-case>C</fixed-case>x<fixed-case>GGEC</fixed-case>: Construction-Guided Grammatical Error Correction</title>
      <author><first>Yayu</first><last>Cao</last></author>
      <author><first>Tianxiang</first><last>Wang</last></author>
      <author><first>Lvxiaowei</first><last>Xu</last></author>
      <author><first>Zhenyao</first><last>Wang</last></author>
      <author><first>Ming</first><last>Cai</last><affiliation>Zhejiang University</affiliation></author>
      <pages>6143-6156</pages>
      <abstract>The grammatical error correction (GEC) task aims to detect and correct grammatical errors in text to enhance its accuracy and readability. Current GEC methods primarily rely on grammatical labels for syntactic information, often overlooking the inherent usage patterns of language. In this work, we explore the potential of construction grammar (CxG) to improve GEC by leveraging constructions to capture underlying language patterns and guide corrections. We first establish a comprehensive construction inventory from corpora. Next, we introduce a construction prediction model that identifies potential constructions in ungrammatical sentences using a noise-tolerant language model. Finally, we train a CxGGEC model on construction-masked parallel data, which performs GEC by decoding construction tokens into their original forms and correcting erroneous tokens. Extensive experiments on English and Chinese GEC benchmarks demonstrate the effectiveness of our approach.</abstract>
      <url hash="0a21616a">2025.acl-long.307</url>
      <bibkey>cao-etal-2025-cxggec</bibkey>
    </paper>
    <paper id="308">
      <title>Beyond Sequences: Two-dimensional Representation and Dependency Encoding for Code Generation</title>
      <author><first>Xiangyu</first><last>Zhang</last></author>
      <author><first>Yu</first><last>Zhou</last></author>
      <author><first>Guang</first><last>Yang</last></author>
      <author><first>Wei</first><last>Cheng</last></author>
      <author><first>Taolue</first><last>Chen</last><affiliation>Birkbeck, University of London</affiliation></author>
      <pages>6157-6172</pages>
      <abstract>The advent of large language models has significantly advanced automatic code generation, transforming the way programmers writing code. Inspired by natural language processing, mainstream code generation approaches represent code as a linear sequence of tokens. In this paper, we propose to represent code snippets as two-dimensional entities, where both code lines and tokens within lines are explicitly modeled. This representation allows us to capture the hierarchical and spatial structure of code, especially the dependencies between code lines. Our method CoDE introduces a dependency encoding approach that leverages dictionary learning to perform semantic matching between code lines. As such, it avoids the reliance on strict position indices, leading to better generalization to code with diverse context and lengths. We thoroughly evaluate CoDE based on four categories of tasks. The experimental results showcase its generalizability, context understanding and retrieval, as well as interpretability in code generation.</abstract>
      <url hash="557dc2ab">2025.acl-long.308</url>
      <bibkey>zhang-etal-2025-beyond</bibkey>
    </paper>
    <paper id="309">
      <title><fixed-case>HD</fixed-case>-<fixed-case>NDE</fixed-case>s: Neural Differential Equations for Hallucination Detection in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Qing</first><last>Li</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Jiahui</first><last>Geng</last></author>
      <author><first>Zongxiong</first><last>Chen</last><affiliation>Fraunhofer FOKUS</affiliation></author>
      <author><first>Derui</first><last>Zhu</last><affiliation>Technische Universität Darmstadt</affiliation></author>
      <author><first>Yuxia</first><last>Wang</last></author>
      <author><first>Congbo</first><last>Ma</last></author>
      <author><first>Chenyang</first><last>Lyu</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Fakhri</first><last>Karray</last><affiliation>University of Waterloo and Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>6173-6186</pages>
      <abstract>In recent years, large language models (LLMs) have made remarkable advancements, yet hallucination, where models produce inaccurate or non-factual statements, remains a significant challenge for real-world deployment. Although current classification-based methods, such as SAPLMA, are highly efficient in mitigating hallucinations, they struggle when non-factual information arises in the early or mid-sequence of outputs, reducing their reliability. To address these issues, we propose Hallucination Detection-Neural Differential Equations (HD-NDEs), a novel method that systematically assesses the truthfulness of statements by capturing the full dynamics of LLMs within their latent space. Our approaches apply neural differential equations (Neural DEs) to model the dynamic system in the latent space of LLMs. Then, the sequence in the latent space is mapped to the classification space for truth assessment. The extensive experiments across five datasets and six widely used LLMs demonstrate the effectiveness of HD-NDEs, especially, achieving over 14% improvement in AUC-ROC on the True-False dataset compared to state-of-the-art techniques.</abstract>
      <url hash="c68bc6fd">2025.acl-long.309</url>
      <bibkey>li-etal-2025-hd</bibkey>
    </paper>
    <paper id="310">
      <title>What Is That Talk About? A Video-to-Text Summarization Dataset for Scientific Presentations</title>
      <author><first>Dongqi</first><last>Liu</last><affiliation>Universität des Saarlandes</affiliation></author>
      <author><first>Chenxi</first><last>Whitehouse</last><affiliation>Meta</affiliation></author>
      <author><first>Xi</first><last>Yu</last><affiliation>Universität des Saarlandes and University of Groningen</affiliation></author>
      <author><first>Louis</first><last>Mahon</last></author>
      <author><first>Rohit</first><last>Saxena</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Zheng</first><last>Zhao</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Yifu</first><last>Qiu</last></author>
      <author><first>Mirella</first><last>Lapata</last><affiliation>Edinburgh University, University of Edinburgh</affiliation></author>
      <author><first>Vera</first><last>Demberg</last><affiliation>Universität des Saarlandes</affiliation></author>
      <pages>6187-6210</pages>
      <abstract>Transforming recorded videos into concise and accurate textual summaries is a growing challenge in multimodal learning. This paper introduces VISTA, a dataset specifically designed for video-to-text summarization in scientific domains. VISTA contains 18,599 recorded AI conference presentations paired with their corresponding paper abstracts. We benchmark the performance of state-of-the-art large models and apply a plan-based framework to better capture the structured nature of abstracts. Both human and automated evaluations confirm that explicit planning enhances summary quality and factual consistency. However, a considerable gap remains between models and human performance, highlighting the challenges of our dataset. This study aims to pave the way for future research on scientific video-to-text summarization.</abstract>
      <url hash="0a61a36a">2025.acl-long.310</url>
      <bibkey>liu-etal-2025-talk</bibkey>
    </paper>
    <paper id="311">
      <title><fixed-case>N</fixed-case>eu<fixed-case>S</fixed-case>ym-<fixed-case>RAG</fixed-case>: Hybrid Neural Symbolic Retrieval with Multiview Structuring for <fixed-case>PDF</fixed-case> Question Answering</title>
      <author><first>Ruisheng</first><last>Cao</last></author>
      <author><first>Hanchong</first><last>Zhang</last></author>
      <author><first>Tiancheng</first><last>Huang</last></author>
      <author><first>Zhangyi</first><last>Kang</last></author>
      <author><first>Yuxin</first><last>Zhang</last></author>
      <author><first>Liangtai</first><last>Sun</last><affiliation>Meituan</affiliation></author>
      <author><first>Hanqi</first><last>Li</last></author>
      <author><first>Yuxun</first><last>Miao</last></author>
      <author><first>Shuai</first><last>Fan</last><affiliation>AISpeech Ltd</affiliation></author>
      <author><first>Lu</first><last>Chen</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Kai</first><last>Yu</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>6211-6239</pages>
      <abstract>The increasing number of academic papers poses significant challenges for researchers to efficiently acquire key details. While retrieval augmented generation (RAG) shows great promise in large language model (LLM) based automated question answering, previous works often isolate neural and symbolic retrieval despite their complementary strengths. Moreover, conventional single-view chunking neglects the rich structure and layout of PDFs, e.g., sections and tables. In this work, we propose NeuSym-RAG, a hybrid neural symbolic retrieval framework which combines both paradigms in an interactive process. By leveraging multi-view chunking and schema-based parsing, NeuSym-RAG organizes semi-structured PDF content into both the relational database and vectorstore, enabling LLM agents to iteratively gather context until sufficient to generate answers. Experiments on three full PDF-based QA datasets, including a self-annotated one AirQA-Real, show that NeuSym-RAG stably defeats both the vector-based RAG and various structured baselines, highlighting its capacity to unify both retrieval schemes and utilize multiple views.</abstract>
      <url hash="12d06f5a">2025.acl-long.311</url>
      <bibkey>cao-etal-2025-neusym</bibkey>
    </paper>
    <paper id="312">
      <title><fixed-case>P</fixed-case>rov<fixed-case>B</fixed-case>ench: A Benchmark of Legal Provision Recommendation for Contract Auto-Reviewing</title>
      <author><first>Xiuxuan</first><last>Shen</last><affiliation>Xidian University</affiliation></author>
      <author><first>Zhongyuan</first><last>Jiang</last><affiliation>Xi’an University of Electronic Science and Technology</affiliation></author>
      <author><first>Junsan</first><last>Zhang</last><affiliation>China University of Petroleum</affiliation></author>
      <author><first>Junxiao</first><last>Han</last><affiliation>Hangzhou City University</affiliation></author>
      <author><first>Yao</first><last>Wan</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Chengjie</first><last>Guo</last><affiliation>Xidian University</affiliation></author>
      <author><first>Bingcheng</first><last>Liu</last></author>
      <author><first>Jie</first><last>Wu</last></author>
      <author><first>Renxiang</first><last>Li</last></author>
      <author><first>Philip S.</first><last>Yu</last><affiliation>University of Illinois Chicago</affiliation></author>
      <pages>6240-6254</pages>
      <abstract>Contract review is a critical process to protect the rights and interests of the parties involved. However, this process is time-consuming, labor-intensive, and costly, especially when a contract faces multiple rounds of review. To accelerate the contract review and promote the completion of transactions, this paper introduces a novel benchmark of legal provision recommendation and conflict detection for contract auto-reviewing (ProvBench), which aims to recommend the legal provisions related to contract clauses and detect possible legal conflicts. Specifically, we construct the first Legal Provision Recommendation Dataset: ProvData, which covers 8 common contract types. In addition, we conduct extensive experiments to evaluate ProvBench on various state-of-the-art models. Experimental results validate the feasibility of ProvBench and demonstrate the effectiveness of ProvData. Finally, we identify potential challenges in the ProvBench and advocate for further investigation.</abstract>
      <url hash="96fb473b">2025.acl-long.312</url>
      <bibkey>shen-etal-2025-provbench</bibkey>
    </paper>
    <paper id="313">
      <title>F5-<fixed-case>TTS</fixed-case>: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching</title>
      <author><first>Yushen</first><last>Chen</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Zhikang</first><last>Niu</last></author>
      <author><first>Ziyang</first><last>Ma</last></author>
      <author><first>Keqi</first><last>Deng</last></author>
      <author><first>Chunhui</first><last>Wang</last></author>
      <author><first>JianZhao</first><last>JianZhao</last><affiliation>Jiangsu University of Science and Technology</affiliation></author>
      <author><first>Kai</first><last>Yu</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Xie</first><last>Chen</last></author>
      <pages>6255-6271</pages>
      <abstract>This paper introduces F5-TTS, a fully non-autoregressive text-to-speech system based on flow matching with Diffusion Transformer (DiT). Without requiring complex designs such as duration model, text encoder, and phoneme alignment, the text input is simply padded with filler tokens to the same length as input speech, and then the denoising is performed for speech generation, which was originally proved feasible by E2 TTS. However, the original design of E2 TTS makes it hard to follow due to its slow convergence and low robustness. To address these issues, we first model the input with ConvNeXt to refine the text representation, making it easy to align with the speech. We further propose an inference-time Sway Sampling strategy, which significantly improves our model’s performance and efficiency. This sampling strategy for flow step can be easily applied to existing flow matching based models without retraining. Our design allows faster training and achieves an inference RTF of 0.15, which is greatly improved compared to state-of-the-art diffusion-based TTS models. Trained on a public 100K hours multilingual dataset, our F5-TTS exhibits highly natural and expressive zero-shot ability, seamless code-switching capability, and speed control efficiency. We have released all codes and checkpoints to promote community development, at https://SWivid.github.io/F5-TTS/.</abstract>
      <url hash="9061600d">2025.acl-long.313</url>
      <bibkey>chen-etal-2025-f5</bibkey>
    </paper>
    <paper id="314">
      <title><fixed-case>A</fixed-case>uto<fixed-case>M</fixed-case>ed<fixed-case>E</fixed-case>val: Harnessing Language Models for Automatic Medical Capability Evaluation</title>
      <author><first>Xiechi</first><last>Zhang</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Zetian</first><last>Ouyang</last></author>
      <author><first>Linlin</first><last>Wang</last></author>
      <author><first>Gerard</first><last>De Melo</last><affiliation>Hasso Plattner Institute and University of Potsdam</affiliation></author>
      <author><first>Zhu</first><last>Cao</last></author>
      <author><first>Xiaoling</first><last>Wang</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Ya</first><last>Zhang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Yanfeng</first><last>Wang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Liang</first><last>He</last><affiliation>East China Normal University</affiliation></author>
      <pages>6272-6285</pages>
      <abstract>With the proliferation of large language models (LLMs) in the medical domain, there is increasing demand for improved evaluation techniques to assess their capabilities. However, traditional metrics like F1 and ROUGE, which rely on token overlaps to measure quality, significantly overlook the importance of medical terminology. While human evaluation tends to be more reliable, it can be very costly and may as well suffer from inaccuracies due to limits in human expertise and motivation. Although there are some evaluation methods based on LLMs, their usability in the medical field is limited due to their proprietary nature or lack of expertise. To tackle these challenges, we present AutoMedEval, an open-sourced automatic evaluation model with 13B parameters specifically engineered to measure the question-answering proficiency of medical LLMs. The overarching objective of AutoMedEval is to assess the quality of responses produced by diverse models, aspiring to significantly reduce the dependence on human evaluation. Specifically, we propose a hierarchical training method involving curriculum instruction tuning and an iterative knowledge introspection mechanism, enabling AutoMedEval to acquire professional medical assessment capabilities with limited instructional data. Human evaluations indicate that AutoMedEval surpasses other baselines in terms of correlation with human judgments.</abstract>
      <url hash="84f8d706">2025.acl-long.314</url>
      <bibkey>zhang-etal-2025-automedeval</bibkey>
    </paper>
    <paper id="315">
      <title><fixed-case>C</fixed-case>o<fixed-case>T</fixed-case>-based Synthesizer: Enhancing <fixed-case>LLM</fixed-case> Performance through Answer Synthesis</title>
      <author><first>Bohan</first><last>Zhang</last></author>
      <author><first>Xiaokang</first><last>Zhang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Jing</first><last>Zhang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Jifan</first><last>Yu</last></author>
      <author><first>Sijia</first><last>Luo</last></author>
      <author><first>Jie</first><last>Tang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>6286-6303</pages>
      <abstract>Current inference scaling methods, such as Self-consistency and Best-of-N, have proven effective in improving the accuracy of LLMs on complex reasoning tasks. However, these methods rely heavily on the quality of candidate responses and are unable to produce correct answers when all candidates are incorrect. In this paper, we propose a novel inference scaling strategy, CoT-based Synthesizer, which leverages CoT reasoning to synthesize superior answers by analyzing complementary information from multiple candidate responses, even when all candidates are flawed. To support a lightweight and cost-effective implementation, we introduce an automated data generation pipeline that creates diverse training data. This enables smaller LLMs trained on this data to improve the inference accuracy of larger models, including API-based LLMs. Experimental results across four benchmark datasets with seven policy models demonstrate that our method significantly enhances performance, with gains of 11.8% for Llama3-8B and 10.3% for GPT-4o on the MATH dataset. The corresponding training data and code are publicly available on the [repository](https://github.com/RUCKBReasoning/CoT-based-Synthesizer).</abstract>
      <url hash="abf5ddf8">2025.acl-long.315</url>
      <bibkey>zhang-etal-2025-cot</bibkey>
    </paper>
    <paper id="316">
      <title>Efficiently Identifying Watermarked Segments in Mixed-Source Texts</title>
      <author><first>Xuandong</first><last>Zhao</last><affiliation>University of California, Berkeley</affiliation></author>
      <author><first>Chenwen</first><last>Liao</last></author>
      <author><first>Yu-Xiang</first><last>Wang</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Lei</first><last>Li</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <pages>6304-6316</pages>
      <abstract>Text watermarks in large language models (LLMs) are increasingly used to detect synthetic text, mitigating misuse cases like fake news and academic dishonesty. While existing watermarking detection techniques primarily focus on classifying entire documents as watermarked or not, they often neglect the common scenario of identifying individual watermark segments within longer, mixed-source documents. Drawing inspiration from plagiarism detection systems, we propose two novel methods for partial watermark detection. First, we develop a geometry cover detection framework aimed at determining whether there is a watermark segment in long text. Second, we introduce an adaptive online learning algorithm to pinpoint the precise location of watermark segments within the text. Evaluated on three popular watermarking techniques (KGW-Watermark, Unigram-Watermark, and Gumbel-Watermark), our approach achieves high accuracy, significantly outperforming baseline methods. Moreover, our framework is adaptable to other watermarking techniques, offering new insights for precise watermark detection. Our code is publicly available at <url>https://github.com/XuandongZhao/llm-watermark-location</url>.</abstract>
      <url hash="9643033f">2025.acl-long.316</url>
      <bibkey>zhao-etal-2025-efficiently</bibkey>
    </paper>
    <paper id="317">
      <title>Assessing Dialect Fairness and Robustness of Large Language Models in Reasoning Tasks</title>
      <author><first>Fangru</first><last>Lin</last><affiliation>Faculty of Linguistics, Philology and Phonetics, University of Oxford</affiliation></author>
      <author><first>Shaoguang</first><last>Mao</last><affiliation>Microsoft</affiliation></author>
      <author><first>Emanuele</first><last>La Malfa</last><affiliation>Department of Computer Science, University of Oxford</affiliation></author>
      <author><first>Valentin</first><last>Hofmann</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Adrian</first><last>de Wynter</last><affiliation>Microsoft</affiliation></author>
      <author><first>Xun</first><last>Wang</last></author>
      <author><first>Si-Qing</first><last>Chen</last></author>
      <author><first>Michael J.</first><last>Wooldridge</last></author>
      <author><first>Janet B.</first><last>Pierrehumbert</last><affiliation>University of Oxford</affiliation></author>
      <author><first>Furu</first><last>Wei</last><affiliation>Microsoft Research</affiliation></author>
      <pages>6317-6342</pages>
      <abstract>Language is not monolithic. While benchmarks, including those designed for multiple languages, are often used as proxies to evaluate the performance of Large Language Models (LLMs), they tend to overlook the nuances of within-language variation and thus fail to model the experience of speakers of non-standard dialects. Focusing on African American Vernacular English (AAVE), we present the first study aimed at objectively assessing the fairness and robustness of LLMs in handling dialects across canonical reasoning tasks, including algorithm, math, logic, and integrated reasoning. We introduce **ReDial** (**Re**asoning with **Dial**ect Queries), a benchmark containing 1.2K+ parallel query pairs in Standardized English and AAVE. We hire AAVE speakers, including experts with computer science backgrounds, to rewrite seven popular benchmarks,such as HumanEval and GSM8K. With ReDial, we evaluate widely used LLMs, including GPT, Claude, Llama, Mistral, and the Phi model families. Our findings reveal that <b>almost all of these widely used models show significant brittleness and unfairness to queries in AAVE</b>. Our work establishes a systematic and objective framework for analyzing LLM bias in dialectal queries. Moreover, it highlights how mainstream LLMs provide unfair service to dialect speakers in reasoning tasks, laying a critical foundation for future research.</abstract>
      <url hash="e4407f74">2025.acl-long.317</url>
      <bibkey>lin-etal-2025-assessing</bibkey>
    </paper>
    <paper id="318">
      <title>Towards a More Generalized Approach in Open Relation Extraction</title>
      <author><first>Qing</first><last>Wang</last><affiliation>Iowa State University</affiliation></author>
      <author><first>Yuepei</first><last>Li</last></author>
      <author><first>Qiao</first><last>Qiao</last><affiliation>Iowa State University</affiliation></author>
      <author><first>Kang</first><last>Zhou</last><affiliation>Amazon</affiliation></author>
      <author><first>Qi</first><last>Li</last><affiliation>Iowa State University</affiliation></author>
      <pages>6343-6354</pages>
      <abstract>Open Relation Extraction (OpenRE) seeks to identify and extract novel relational facts between named entities from unlabeled data without pre-defined relation schemas. Traditional OpenRE methods typically assume that the unlabeled data consists solely of novel relations or is pre-divided into known and novel instances. However, in real-world scenarios, novel relations are arbitrarily distributed. In this paper, we propose a generalized OpenRE setting that considers unlabeled data as a mixture of both known and novel instances. To address this, we propose MixORE, a two-phase framework that integrates relation classification and clustering to jointly learn known and novel relations. Experiments on three benchmark datasets demonstrate that MixORE consistently outperforms competitive baselines in known relation classification and novel relation clustering. Our findings contribute to the advancement of generalized OpenRE research and real-world applications.</abstract>
      <url hash="b04f480f">2025.acl-long.318</url>
      <bibkey>wang-etal-2025-towards-generalized</bibkey>
    </paper>
    <paper id="319">
      <title>Adaptive Retrieval Without Self-Knowledge? Bringing Uncertainty Back Home</title>
      <author><first>Viktor</first><last>Moskvoretskii</last><affiliation>Skolkovo Institute of Science and Technology</affiliation></author>
      <author><first>Maria</first><last>Marina</last></author>
      <author><first>Mikhail</first><last>Salnikov</last><affiliation>AIRI and Skolkovo Institute of Science and Technology</affiliation></author>
      <author><first>Nikolay</first><last>Ivanov</last><affiliation>Skolkovo Institute of Science and Technology</affiliation></author>
      <author><first>Sergey</first><last>Pletenev</last></author>
      <author><first>Daria</first><last>Galimzianova</last><affiliation>MTS AI</affiliation></author>
      <author><first>Nikita</first><last>Krayko</last><affiliation>MTS AI</affiliation></author>
      <author><first>Vasily</first><last>Konovalov</last><affiliation>AIRI</affiliation></author>
      <author><first>Irina</first><last>Nikishina</last></author>
      <author><first>Alexander</first><last>Panchenko</last><affiliation>Skoltech</affiliation></author>
      <pages>6355-6384</pages>
      <abstract>Retrieval Augmented Generation (RAG) improves correctness of Question Answering (QA) and addresses hallucinations in Large Language Models (LLMs), yet greatly increase computational costs. Besides, RAG is not always needed as may introduce irrelevant information. Recent adaptive retrieval methods integrate LLMs’ intrinsic knowledge with external information appealing to LLM self-knowledge, but they often neglect efficiency evaluations and comparisons with uncertainty estimation techniques. We bridge this gap by conducting a comprehensive analysis of 35 adaptive retrieval methods, including 8 recent approaches and 27 uncertainty estimation techniques, across 6 datasets using 10 metrics for QA performance, self-knowledge, and efficiency. Our findings show that uncertainty estimation techniques often outperform complex pipelines in terms of efficiency and self-knowledge, while maintaining comparable QA performance.</abstract>
      <url hash="aaeb2b99">2025.acl-long.319</url>
      <bibkey>moskvoretskii-etal-2025-adaptive</bibkey>
    </paper>
    <paper id="320">
      <title>Evaluating Language Models as Synthetic Data Generators</title>
      <author><first>Seungone</first><last>Kim</last></author>
      <author><first>Juyoung</first><last>Suk</last></author>
      <author><first>Xiang</first><last>Yue</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Vijay</first><last>Viswanathan</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Seongyun</first><last>Lee</last></author>
      <author><first>Yizhong</first><last>Wang</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <author><first>Kiril</first><last>Gashteovski</last><affiliation>NEC Laboratories Europe, St.Cyril and Methodius University and NEC Laboratories Europe</affiliation></author>
      <author><first>Carolin</first><last>Lawrence</last><affiliation>NEC Laboratories Europe and NEC Laboratories Europe</affiliation></author>
      <author><first>Sean</first><last>Welleck</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Graham</first><last>Neubig</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>6385-6403</pages>
      <abstract>Given the increasing use of synthetic data in language model (LM) post-training, an LM’s ability to generate high-quality data has become nearly as crucial as its ability to solve problems directly. While prior works have focused on developing effective data generation methods, they lack systematic comparison of different LMs as data generators in a unified setting. To address this gap, we propose AgoraBench, a benchmark that provides standardized settings and metrics to evaluate LMs’ data generation abilities. Through synthesizing 1.26 million training instances using 6 LMs and training 99 student models, we uncover key insights about LMs’ data generation capabilities. First, we observe that LMs exhibit distinct strengths. For instance, GPT-4o excels at generating new problems, while Claude-3.5-Sonnet performs better at enhancing existing ones. Furthermore, our analysis reveals that an LM’s data generation ability doesn’t necessarily correlate with its problem-solving ability. Instead, multiple intrinsic features of data quality—including response quality, perplexity, and instruction difficulty—collectively serve as better indicators. Finally, we demonstrate that strategic choices in output format and cost-conscious model selection significantly impact data generation effectiveness. Our code, checkpoints, and data are all publicly available at https://github.com/neulab/data-agora.</abstract>
      <url hash="28b77a48">2025.acl-long.320</url>
      <bibkey>kim-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="321">
      <title>Can Graph Descriptive Order Affect Solving Graph Problems with <fixed-case>LLM</fixed-case>s?</title>
      <author><first>Yuyao</first><last>Ge</last></author>
      <author><first>Shenghua</first><last>Liu</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Baolong</first><last>Bi</last></author>
      <author><first>Yiwei</first><last>Wang</last><affiliation>University of California, Merced</affiliation></author>
      <author><first>Lingrui</first><last>Mei</last><affiliation>Skywork AI</affiliation></author>
      <author><first>Wenjie</first><last>Feng</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Lizhe</first><last>Chen</last></author>
      <author><first>Xueqi</first><last>Cheng</last><affiliation>Institute of Computing Technology, Chinese Academy</affiliation></author>
      <pages>6404-6420</pages>
      <abstract>Large language models (LLMs) have achieved significant success in reasoning tasks, including mathematical reasoning and logical deduction. Among these reasoning tasks, graph problems stand out due to their complexity and unique structural characteristics, attracting considerable attention from researchers. Previous studies have explored LLMs’ graph reasoning abilities through various techniques, such as different encoding methods for graph structures and the use of carefully designed prompts. However, a critical factor has been mostly overlooked: the prompt sequential order in which graph descriptions are presented to the models. In this study, we present the first comprehensive analysis of how the order of graph descriptions impacts LLM performance. Specifically, we comprehensively evaluate four graph description orders across six graph problems using six mainstream LLMs. The results reveal that: (1) ordered graph descriptions significantly improve LLMs’ comprehension of graph structures; (2) the robustness of LLMs to graph description order varies across different tasks; and (3) the impact of graph order on performance is closely related to the inherent characteristics of tasks. This study provides a critical advancement in the application of LLMs for solving graph-related problems, paving the way for future research to optimize model performance through strategic graph description ordering.</abstract>
      <url hash="bec6366f">2025.acl-long.321</url>
      <bibkey>ge-etal-2025-graph</bibkey>
    </paper>
    <paper id="322">
      <title>Learning to Rewrite: Generalized <fixed-case>LLM</fixed-case>-Generated Text Detection</title>
      <author><first>Wei</first><last>Hao</last></author>
      <author><first>Ran</first><last>Li</last><affiliation>Columbia University</affiliation></author>
      <author><first>Weiliang</first><last>Zhao</last></author>
      <author><first>Junfeng</first><last>Yang</last><affiliation>Columbia University</affiliation></author>
      <author><first>Chengzhi</first><last>Mao</last><affiliation>Google</affiliation></author>
      <pages>6421-6434</pages>
      <abstract>Detecting text generated by Large Language Models (LLMs) is crucial, yet current detectors often struggle to generalize in open-world settings. We introduce Learning2Rewrite, a novel framework to detect LLM-generated text with exceptional generalization to unseen domains. Capitalized on the finding that LLMs inherently modify LLM-generated content less than human-written text when rewriting, we train an LLM to amplify this disparity, yielding a more distinguishable and generalizable edit distance across diverse text distributions. Extensive experiments on data from 21 independent domains and four major LLMs (GPT-3.5, GPT-4, Gemini, and Llama-3) demonstrate that our detector outperforms state-of-the-art detection methods by up to 23.04% in AUROC for in-distribution tests, 35.10% for out-of-distribution tests, and 48.66% under adversarial attacks. Our unique training objective ensures better generalizability compared to directly training for classification, even when leveraging the same amount of tunable parameters. Our findings suggest that reinforcing LLMs’ inherent rewriting tendencies offers a robust and scalable solution for detecting LLM-generated text.</abstract>
      <url hash="88293877">2025.acl-long.322</url>
      <bibkey>hao-etal-2025-learning</bibkey>
    </paper>
    <paper id="323">
      <title>Evaluating Multimodal Large Language Models on Video Captioning via <fixed-case>M</fixed-case>onte <fixed-case>C</fixed-case>arlo Tree Search</title>
      <author><first>Linhao</first><last>Yu</last></author>
      <author><first>Xingguang</first><last>Ji</last><affiliation>Kuaishou- 快手科技</affiliation></author>
      <author><first>Yahui</first><last>Liu</last></author>
      <author><first>Fanheng</first><last>Kong</last></author>
      <author><first>Chenxi</first><last>Sun</last></author>
      <author><first>Jingyuan</first><last>Zhang</last><affiliation>Kuaishou- 快手科技</affiliation></author>
      <author><first>Hongzhi</first><last>Zhang</last><affiliation>Kuaishou- 快手科技</affiliation></author>
      <author><first>V.</first><last>W.</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Fuzheng</first><last>Zhang</last></author>
      <author><first>Deyi</first><last>Xiong</last><affiliation>Tianjin University</affiliation></author>
      <pages>6435-6462</pages>
      <abstract>Video captioning can be used to assess the video understanding capabilities of Multimodal Large Language Models (MLLMs).However, existing benchmarks and evaluation protocols suffer from crucial issues, such as inadequate or homogeneous creation of key points, exorbitant cost of data creation, and limited evaluation scopes. To address these issues, we propose an automatic framework, named AutoCaption, which leverages Monte Carlo Tree Search (MCTS) to construct numerous and diverse descriptive sentences (<i>i.e.</i>, key points) that thoroughly represent video content in an iterative way. This iterative captioning strategy enables the continuous enhancement of video details such as actions, objects’ attributes, environment details, etc. We apply AutoCaption to curate MCTS-VCB, a fine-grained video caption benchmark covering video details, thereby enabling a comprehensive evaluation of MLLMs on the video captioning task. We evaluate more than 20 open- and closed-source MLLMs of varying sizes on MCTS-VCB. Results show that MCTS-VCB can effectively and comprehensively evaluate the video captioning capability, with Gemini-1.5-Pro achieving the highest F1 score of 71.2. Interestingly, we fine-tune InternVL2.5-8B with the AutoCaption-generated data, which helps the model achieve an overall improvement of 25.0% on MCTS-VCB and 16.3% on DREAM-1K, further demonstrating the effectiveness of AutoCaption. The code and data are available at <url>https://github.com/tjunlp-lab/MCTS-VCB</url>.</abstract>
      <url hash="c1aee1e5">2025.acl-long.323</url>
      <bibkey>yu-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="324">
      <title><fixed-case>GIFT</fixed-case>-<fixed-case>SW</fixed-case>: <fixed-case>G</fixed-case>aussian noise Injected Fine-Tuning of Salient Weights for <fixed-case>LLM</fixed-case>s</title>
      <author><first>Maxim</first><last>Zhelnin</last><affiliation>Skolkovo Institute of Science and Technology</affiliation></author>
      <author><first>Viktor</first><last>Moskvoretskii</last><affiliation>Skolkovo Institute of Science and Technology</affiliation></author>
      <author><first>Egor</first><last>Shvetsov</last><affiliation>Skoltech</affiliation></author>
      <author><first>Maria</first><last>Krylova</last></author>
      <author><first>Venediktov</first><last>Egor</last><affiliation>Huawei</affiliation></author>
      <author><first>Zuev</first><last>Aleksandr</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Evgeny</first><last>Burnaev</last><affiliation>Skolkovo Institute of Science and Technology</affiliation></author>
      <pages>6463-6480</pages>
      <abstract>Parameter Efficient Fine-Tuning (PEFT) methods have gained popularity and democratized the usage of Large Language Models (LLMs). Recent studies have shown that a small subset of weights significantly impacts performance. Based on this observation, we introduce a novel PEFT method, called Gaussian noise Injected Fine Tuning of Salient Weights (GIFT-SW). Our method updates only salient columns, while injecting Gaussian noise into non-salient ones. To identify these columns, we developed a generalized sensitivity metric that extends and unifies metrics from previous studies. Experiments with LLaMA models demonstrate that GIFT-SW outperforms full fine-tuning and modern PEFT methods under the same computational budget. Moreover, GIFT-SW offers practical advantages to recover performance of models subjected to mixed-precision quantization with keeping salient weights in full precision.</abstract>
      <url hash="c2469aea">2025.acl-long.324</url>
      <bibkey>zhelnin-etal-2025-gift</bibkey>
    </paper>
    <paper id="325">
      <title>Quaff: Quantized Parameter-Efficient Fine-Tuning under Outlier Spatial Stability Hypothesis</title>
      <author><first>Hong</first><last>Huang</last><affiliation>City University of Hong Kong</affiliation></author>
      <author><first>Dapeng</first><last>Wu</last><affiliation>City University of Hong Kong</affiliation></author>
      <pages>6481-6496</pages>
      <abstract>Large language models (LLMs) have made exciting achievements across various domains, yet their deployment on resource-constrained personal devices remains hindered by the prohibitive computational and memory demands of task-specific fine-tuning. While quantization offers a pathway to efficiency, existing methods struggle to balance performance and overhead, either incurring high computational/memory costs or failing to address activation outliers—a critical bottleneck in quantized fine-tuning. To address these challenges, we propose the Outlier Spatial Stability Hypothesis (__OSSH__): _During fine-tuning, certain activation outlier channels retain stable spatial positions across training iterations._ Building on OSSH, we propose __Quaff__, a Quantized parameter-efficient fine-tuning framework for LLMs, optimizing low-precision activation representations through targeted momentum scaling. Quaff dynamically suppresses outliers exclusively in invariant channels using lightweight operations, eliminating full-precision weight storage and global rescaling while reducing quantization errors. Extensive experiments across ten benchmarks validate OSSH and demonstrate Quaff’s efficacy. Specifically, on the GPQA reasoning benchmark, Quaff achieves a <tex-math>1.73\times</tex-math> latency reduction and 30% memory savings over full-precision fine-tuning while improving accuracy by 0.6% on the Phi-3 model, reconciling the triple trade-off between efficiency, performance, and deployability. By enabling consumer-grade GPU fine-tuning (e.g., RTX 2080 Super) without sacrificing model utility, Quaff democratizes personalized LLM deployment. The code is available at https://anonymous.4open.science/r/Quaff-B322/.</abstract>
      <url hash="e47d18a9">2025.acl-long.325</url>
      <bibkey>huang-wu-2025-quaff</bibkey>
    </paper>
    <paper id="326">
      <title>Unsolvable Problem Detection: Robust Understanding Evaluation for Large Multimodal Models</title>
      <author><first>Atsuyuki</first><last>Miyai</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Jingkang</first><last>Yang</last></author>
      <author><first>Jingyang</first><last>Zhang</last><affiliation>Electrical and Computer Engineering, Duke University</affiliation></author>
      <author><first>Yifei</first><last>Ming</last><affiliation>Salesforce AI Research</affiliation></author>
      <author><first>Qing</first><last>Yu</last><affiliation>LY Corporation</affiliation></author>
      <author><first>Go</first><last>Irie</last><affiliation>Tokyo University of Science and Keio University</affiliation></author>
      <author><first>Yixuan</first><last>Li</last><affiliation>University of Wisconsin, Madison, Stanford University, Facebook AI, Google AI and Cornell University</affiliation></author>
      <author><first>Hai Helen</first><last>Li</last><affiliation>Duke University</affiliation></author>
      <author><first>Ziwei</first><last>Liu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Kiyoharu</first><last>Aizawa</last><affiliation>The University of Tokyo, The University of Tokyo and Tokyo University of Science</affiliation></author>
      <pages>6497-6540</pages>
      <abstract>This paper introduces a novel task to evaluate the robust understanding capability of Large Multimodal Models (LMMs), termed Unsolvable Problem Detection (UPD). Multiple-choice question answering (MCQA) is widely used to assess the understanding capability of LMMs, but it does not guarantee that LMMs truly comprehend the answer. UPD assesses the LMM’s ability to withhold answers when encountering unsolvable problems of MCQA, verifying whether the model truly understands the answer. UPD encompasses three problems: Absent Answer Detection (AAD), Incompatible Answer Set Detection (IASD), and Incompatible Visual Question Detection (IVQD), covering unsolvable cases like answer-lacking or incompatible choices and image-question mismatches. For the evaluation, we introduce the MM-UPD Bench, a benchmark for assessing performance across various ability dimensions. Our experiments reveal that even most LMMs, which demonstrate adequate performance on existing benchmarks, struggle significantly with MM-UPD, underscoring a novel aspect of trustworthiness that current benchmarks have overlooked. A detailed analysis shows that LMMs have different bottlenecks and chain-of-thought and self-reflection improved performance for LMMs with the bottleneck in their LLM capability. We hope our insights will enhance the broader understanding and development of more reliable LMMs.</abstract>
      <url hash="d507f69b">2025.acl-long.326</url>
      <bibkey>miyai-etal-2025-unsolvable</bibkey>
    </paper>
    <paper id="327">
      <title><fixed-case>A</fixed-case>lign<fixed-case>MMB</fixed-case>ench: Evaluating <fixed-case>C</fixed-case>hinese Multimodal Alignment in Large Vision-Language Models</title>
      <author><first>Yuhang</first><last>Wu</last></author>
      <author><first>Wenmeng</first><last>Yu</last><affiliation>Zhipu AI</affiliation></author>
      <author><first>Yean</first><last>Cheng</last></author>
      <author><first>Yan</first><last>Wang</last></author>
      <author><first>Xiaohan</first><last>Zhang</last><affiliation>Beijing Knowledge Atlas Technology Co., Ltd.</affiliation></author>
      <author><first>Jiazheng</first><last>Xu</last></author>
      <author><first>Ming</first><last>Ding</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Yuxiao</first><last>Dong</last><affiliation>Tsinghua University</affiliation></author>
      <pages>6541-6558</pages>
      <abstract>Evaluating the alignment capabilities of large Vision-Language Models (VLMs) is essential for determining their effectiveness as helpful assistants. However, existing benchmarks primarily focus on basic abilities using nonverbal methods, such as yes-no and multiple-choice questions. In this paper, we address this gap by introducing AlignMMBench, which provides more nuanced evaluations of alignment capabilities and is the first benchmark specifically designed for Chinese visual contexts. This benchmark is meticulously curated from real-world scenarios and internet sources, encompassing thirteen specific tasks across three categories, and includes both single-turn and multi-turn dialogue scenarios. Incorporating a prompt rewrite strategy, AlignMMBench encompasses 1,054 images and 4,978 question-answer pairs. To facilitate the evaluation pipeline, we develop CritiqueVLM, a rule-calibrated evaluator that exceeds GPT-4’s evaluation ability. Additionally, we measure the “alignment score”, a quantitative metric designed to assess the robustness and stability of models across diverse prompts. Finally, we evaluate the performance of representative VLMs on AlignMMBench, offering insights into the capabilities and limitations of different VLM architectures. The evaluation code and data are available at https://github.com/THUDM/AlignMMBench.</abstract>
      <url hash="0d49da29">2025.acl-long.327</url>
      <bibkey>wu-etal-2025-alignmmbench</bibkey>
    </paper>
    <paper id="328">
      <title>Biased <fixed-case>LLM</fixed-case>s can Influence Political Decision-Making</title>
      <author><first>Jillian</first><last>Fisher</last><affiliation>University of Washington</affiliation></author>
      <author><first>Shangbin</first><last>Feng</last><affiliation>University of Washington</affiliation></author>
      <author><first>Robert</first><last>Aron</last><affiliation>University of Texas at Arlington</affiliation></author>
      <author><first>Thomas</first><last>Richardson</last><affiliation>University of Washington</affiliation></author>
      <author><first>Yejin</first><last>Choi</last><affiliation>Computer Science Department, Stanford University and NVIDIA</affiliation></author>
      <author><first>Daniel W</first><last>Fisher</last></author>
      <author><first>Jennifer</first><last>Pan</last><affiliation>Stanford University</affiliation></author>
      <author><first>Yulia</first><last>Tsvetkov</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <author><first>Katharina</first><last>Reinecke</last><affiliation>Paul G. Allen School of Computer Science and Engineering, University of Washington</affiliation></author>
      <pages>6559-6607</pages>
      <abstract>As modern large language models (LLMs) become integral to everyday tasks, concerns about their inherent biases and their potential impact on human decision-making have emerged. While bias in models are well-documented, less is known about how these biases influence human decisions. This paper presents two interactive experiments investigating the effects of partisan bias in LLMs on political opinions and decision-making. Participants interacted freely with either a biased liberal, biased conservative, or unbiased control model while completing these tasks. We found that participants exposed to partisan biased models were significantly more likely to adopt opinions and make decisions which matched the LLM’s bias. Even more surprising, this influence was seen when the model bias and personal political partisanship of the participant were opposite. However, we also discovered that prior knowledge of AI was weakly correlated with a reduction of the impact of the bias, highlighting the possible importance of AI education for robust mitigation of bias effects. Our findings not only highlight the critical effects of interacting with biased LLMs and its ability to impact public discourse and political conduct, but also highlights potential techniques for mitigating these risks in the future.</abstract>
      <url hash="9430574b">2025.acl-long.328</url>
      <bibkey>fisher-etal-2025-biased</bibkey>
    </paper>
    <paper id="329">
      <title><fixed-case>L</fixed-case>ex<fixed-case>T</fixed-case>empus: Enhancing Temporal Generalizability of Legal Language Models Through Dynamic Mixture of Experts</title>
      <author><first>Santosh</first><last>T.y.s.s</last></author>
      <author><first>Tuan-Quang</first><last>Vuong</last></author>
      <pages>6608-6624</pages>
      <abstract>The rapid evolution of legal concepts over time necessitates that legal language models adapt swiftly accounting for the temporal dynamics. However, prior works have largely neglected this crucial dimension, treating legal adaptation as a static problem rather than a continuous process. To address this gap, we pioneer LexTempus, a dynamic mixture of experts model that explicitly models the temporal evolution of legal language in a parameter-efficient online learning framework. LexTempus starts with a single lightweight adapter expert and dynamically expands by adding new experts as significant deviations in the data distribution are detected. This self-expansion strategy allows LexTempus to adapt to new information without forgetting past knowledge, thereby improving temporal generalization. We use a a non-parametric similarity-based router to merge relevant experts into a unified expert for each test instance, ensuring efficient inference without additional overhead. We validate the effectiveness of LexTempus on ECHR and EU case law datasets, demonstrating its superiority in both perplexity and open-ended text generation quality metrics.</abstract>
      <url hash="5fce4112">2025.acl-long.329</url>
      <bibkey>t-y-s-s-vuong-2025-lextempus</bibkey>
    </paper>
    <paper id="330">
      <title>That is Unacceptable: the Moral Foundations of Canceling</title>
      <author><first>Soda Marem</first><last>Lo</last></author>
      <author><first>Oscar</first><last>Araque</last><affiliation>Universidad Politécnica de Madrid</affiliation></author>
      <author><first>Rajesh</first><last>Sharma</last><affiliation>institute of computer science, University of Tartu</affiliation></author>
      <author><first>Marco Antonio</first><last>Stranisci</last></author>
      <pages>6625-6639</pages>
      <abstract>Canceling is a morally-driven phenomenon that hinders the development of safe social media platforms and contributes to ideological polarization. To address this issue we present the Canceling Attitudes Detection (CADE) dataset, an annotated corpus of canceling incidents aimed at exploring the factors of disagreements in evaluating people’s canceling attitudes on social media. Specifically, we study the impact of annotators’ morality in their perception of canceling, showing that morality is an independent axis for the explanation of disagreement on this phenomenon. Annotator’s judgments heavily depend on the type of controversial events and involved celebrities. This shows the need to develop more event-centric datasets to better understand how harms are perpetrated in social media and to develop more aware technologies for their detection.</abstract>
      <url hash="dcb986cc">2025.acl-long.330</url>
      <bibkey>lo-etal-2025-unacceptable</bibkey>
    </paper>
    <paper id="331">
      <title><fixed-case>F</fixed-case>loor<fixed-case>P</fixed-case>lan-<fixed-case>LL</fixed-case>a<fixed-case>M</fixed-case>a: Aligning Architects’ Feedback and Domain Knowledge in Architectural Floor Plan Generation</title>
      <author><first>Jun</first><last>Yin</last></author>
      <author><first>Pengyu</first><last>Zeng</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Haoyuan</first><last>Sun</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Yuqin</first><last>Dai</last></author>
      <author><first>Han</first><last>Zheng</last><affiliation>Guangzhou University</affiliation></author>
      <author><first>Miao</first><last>Zhang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Yachao</first><last>Zhang</last></author>
      <author><first>Shuai</first><last>Lu</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>6640-6662</pages>
      <abstract>Floor plans serve as a graphical language through which architects sketch and communicate their design ideas. Actually, in the Architecture, Engineering, and Construction (AEC) design stages, generating floor plans is a complex task requiring domain expertise and alignment with user requirements. However, existing evaluation methods for floor plan generation rely mainly on statistical metrics like FID, GED, and PSNR, which often fail to evaluate using domain knowledge. As a result, even high-performing models on these metrics struggle to generate viable floor plans in practice. To address this, (1) we propose ArchiMetricsNet, the first floor plan dataset that includes functionality, flow, and overall evaluation scores, along with detailed textual analyses. We trained FloorPlan-MPS (Multi-dimensional Preference Score) on it. (2) We develope FloorPlan-LLaMa, a floor plan generation model based on autoregressive framework. To integrate architects’ professional expertise and preferences, FloorPlan-MPS serves as the reward model during the RLHF (Reinforcement Learning from Human Feedback) process, aligning FP-LLaMa with the needs of the architectural community. (3) Comparative experiments demonstrate that our method outperforms baseline models in both text-conditional and class-conditional tasks. Validation by professional architects confirms that our approach yields more rational plans and aligns better with human preferences.</abstract>
      <url hash="af9bc4d6">2025.acl-long.331</url>
      <bibkey>yin-etal-2025-floorplan</bibkey>
    </paper>
    <paper id="332">
      <title><fixed-case>T</fixed-case>heorem<fixed-case>E</fixed-case>xplain<fixed-case>A</fixed-case>gent: Towards Video-based Multimodal Explanations for <fixed-case>LLM</fixed-case> Theorem Understanding</title>
      <author><first>Max</first><last>Ku</last></author>
      <author><first>Cheuk Hei</first><last>Chong</last><affiliation>Beever AI and Votee AI</affiliation></author>
      <author><first>Jonathan</first><last>Leung</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Krish</first><last>Shah</last></author>
      <author><first>Alvin</first><last>Yu</last><affiliation>Votee AI</affiliation></author>
      <author><first>Wenhu</first><last>Chen</last><affiliation>University of Waterloo</affiliation></author>
      <pages>6663-6684</pages>
      <abstract>Understanding domain-specific theorems often requires more than just text-based reasoning; effective communication through structured visual explanations is crucial for deeper comprehension. While large language models (LLMs) demonstrate strong performance in text-based theorem reasoning, their ability to generate coherent and pedagogically meaningful visual explanations remains an open challenge. In this work, we introduce TheoremExplainAgent, an agentic approach for generating long-form theorem explanation videos (over 5 minutes) using Manim animations. To systematically evaluate multimodal theorem explanations, we propose TheoremExplainBench, a benchmark covering 240 theorems across multiple STEM disciplines, along with 5 automated evaluation metrics. Our results reveal that agentic planning is essential for generating detailed long-form videos, and the o3-mini agent achieves a success rate of 93.8% and an overall score of 0.77. However, our quantitative and qualitative studies show that most of the videos produced exhibit minor issues with visual element layout. Furthermore, multimodal explanations expose deeper reasoning flaws that text-based explanations fail to reveal, highlighting the importance of multimodal explanations.</abstract>
      <url hash="1058269a">2025.acl-long.332</url>
      <bibkey>ku-etal-2025-theoremexplainagent</bibkey>
    </paper>
    <paper id="333">
      <title><fixed-case>F</fixed-case>ine<fixed-case>R</fixed-case>eason: Evaluating and Improving <fixed-case>LLM</fixed-case>s’ Deliberate Reasoning through Reflective Puzzle Solving</title>
      <author><first>Guizhen</first><last>Chen</last></author>
      <author><first>Weiwen</first><last>Xu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Hao</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Hou Pong</first><last>Chan</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Chaoqun</first><last>Liu</last></author>
      <author><first>Lidong</first><last>Bing</last><affiliation>Shanda Group and Alibaba Group</affiliation></author>
      <author><first>Deli</first><last>Zhao</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Anh Tuan</first><last>Luu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Yu</first><last>Rong</last><affiliation>Alibaba Group</affiliation></author>
      <pages>6685-6715</pages>
      <abstract>Many challenging reasoning tasks require not just rapid, intuitive responses, but a more deliberate, multi-step approach. Recent progress in large language models (LLMs) highlights an important shift from the “System 1” way of quick reactions to the “System 2” style of reflection-and-correction problem solving. However, current benchmarks heavily rely on the final-answer accuracy, leaving much of a model’s intermediate reasoning steps unexamined. This fails to assess the model’s ability to reflect and rectify mistakes within the reasoning process. To bridge this gap, we introduce FINEREASON, a logic-puzzle benchmark for systematic evaluation of LLMs’ reasoning capabilities. Each puzzle can be decomposed into atomic steps, making it ideal for rigorous validation of intermediate correctness. Building on this, we introduce two tasks: state checking and state transition, for a comprehensive evaluation of how models assess the current situation and plan the next move. To support broader research, we also provide a puzzle training set aimed at enhancing general reasoning. We show that models trained on our state checking and transition data demonstrate gains in mathematical reasoning by up to 5.1%.</abstract>
      <url hash="5bc73fe7">2025.acl-long.333</url>
      <bibkey>chen-etal-2025-finereason</bibkey>
    </paper>
    <paper id="334">
      <title>The <fixed-case>TIP</fixed-case> of the Iceberg: Revealing a Hidden Class of Task-in-Prompt Adversarial Attacks on <fixed-case>LLM</fixed-case>s</title>
      <author><first>Sergey</first><last>Berezin</last></author>
      <author><first>Reza</first><last>Farahbakhsh</last></author>
      <author><first>Noel</first><last>Crespi</last><affiliation>Telecom SudParis</affiliation></author>
      <pages>6716-6730</pages>
      <abstract>We present a novel class of jailbreak adversarial attacks on LLMs, termed Task-in-Prompt (TIP) attacks. Our approach embeds sequence-to-sequence tasks (e.g., cipher decoding, riddles, code execution) into the model’s prompt to indirectly generate prohibited inputs. To systematically assess the effectiveness of these attacks, we introduce the PHRYGE benchmark. We demonstrate that our techniques successfully circumvent safeguards in six state-of-the-art language models, including GPT-4o and LLaMA 3.2. Our findings highlight critical weaknesses in current LLM safety alignment and underscore the urgent need for more sophisticated defence strategies.</abstract>
      <url hash="a88e96ff">2025.acl-long.334</url>
      <bibkey>berezin-etal-2025-tip</bibkey>
    </paper>
    <paper id="335">
      <title>Identifying Reliable Evaluation Metrics for Scientific Text Revision</title>
      <author><first>Leane</first><last>Jourdan</last><affiliation>Université de Nantes</affiliation></author>
      <author><first>Nicolas</first><last>Hernandez</last><affiliation>Université de Nantes</affiliation></author>
      <author><first>Florian</first><last>Boudin</last><affiliation>University of Nantes</affiliation></author>
      <author><first>Richard</first><last>Dufour</last><affiliation>Nantes University</affiliation></author>
      <pages>6731-6756</pages>
      <abstract>Evaluating text revision in scientific writing remains a challenge, as traditional metrics such as ROUGE and BERTScore primarily focus on similarity rather than capturing meaningful improvements. In this work, we analyse and identify the limitations of these metrics and explore alternative evaluation methods that better align with human judgments. We first conduct a manual annotation study to assess the quality of different revisions. Then, we investigate reference-free evaluation metrics from related NLP domains. Additionally, we examine LLM-as-a-judge approaches, analysing their ability to assess revisions with and without a gold reference. Our results show that LLMs effectively assess instruction-following but struggle with correctness, while domain-specific metrics provide complementary insights. We find that a hybrid approach combining LLM-as-a-judge evaluation and task-specific metrics offers the most reliable assessment of revision.</abstract>
      <url hash="8a95a4e9">2025.acl-long.335</url>
      <bibkey>jourdan-etal-2025-identifying</bibkey>
    </paper>
    <paper id="336">
      <title>Can Language Models Reason about Individualistic Human Values and Preferences?</title>
      <author><first>Liwei</first><last>Jiang</last></author>
      <author><first>Taylor</first><last>Sorensen</last><affiliation>University of Washington</affiliation></author>
      <author><first>Sydney</first><last>Levine</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Yejin</first><last>Choi</last><affiliation>Computer Science Department, Stanford University and NVIDIA</affiliation></author>
      <pages>6757-6794</pages>
      <abstract>Recent calls for pluralistic alignment emphasize that AI systems should address the diverse needs of all people. Yet, efforts in this space often require sorting people into fixed buckets of pre-specified diversity-defining dimensions (e.g., demographics), risking smoothing out individualistic variations or even stereotyping. To achieve an authentic representation of diversity that respects individuality, we propose individualistic alignment. While individualistic alignment can take various forms, in this paper, we introduce IndieValueCatalog, a dataset transformed from the influential World Values Survey (WVS), to study language models (LMs) on the specific challenge of individualistic value reasoning. Given a sample of an individual’s value-expressing statements, models are tasked with predicting their value judgments in novel cases. With IndieValueCatalog, we reveal critical limitations in frontier LMs’ abilities to predict individualistic values with accuracies only ranging between 55% to 65%. Moreover, our results highlight that a precise description of individualistic values cannot be approximated only via demographic information. Finally, we train a series of IndieValueReasoners to reveal new patterns and dynamics into global human values.</abstract>
      <url hash="fb9bd363">2025.acl-long.336</url>
      <bibkey>jiang-etal-2025-language</bibkey>
    </paper>
    <paper id="337">
      <title><fixed-case>BERT</fixed-case>-like Models for <fixed-case>S</fixed-case>lavic Morpheme Segmentation</title>
      <author><first>Dmitry</first><last>Morozov</last><affiliation>Novosibirsk State University and Russian National Corpus</affiliation></author>
      <author><first>Lizaveta</first><last>Astapenka</last></author>
      <author><first>Anna</first><last>Glazkova</last><affiliation>Tyumen State University</affiliation></author>
      <author><first>Timur</first><last>Garipov</last></author>
      <author><first>Olga</first><last>Lyashevskaya</last><affiliation>HSE University and Vinogradov Russian Language Institute RAS</affiliation></author>
      <pages>6795-6815</pages>
      <abstract>Automatic morpheme segmentation algorithms are applicable in various tasks, such as building tokenizers and language education. For Slavic languages, the development of such algorithms is complicated by the rich derivational capabilities of these languages. Previous research has shown that, on average, these algorithms have already reached expert-level quality. However, a key unresolved issue is the significant decline in performance when segmenting words containing roots not present in the training data. This problem can be partially addressed by using pre-trained language models to better account for word semantics. In this work, we explored the possibility of fine-tuning BERT-like models for morpheme segmentation using data from Belarusian, Czech, and Russian. We found that for Czech and Russian, our models outperform all previously proposed approaches, achieving word-level accuracy of 92.5-95.1%. For Belarusian, this task was addressed for the first time. The best-performing approach for Belarusian was an ensemble of convolutional neural networks with word-level accuracy of 90.45%.</abstract>
      <url hash="5cf60798">2025.acl-long.337</url>
      <bibkey>morozov-etal-2025-bert</bibkey>
    </paper>
    <paper id="338">
      <title>Turning Trash into Treasure: Accelerating Inference of Large Language Models with Token Recycling</title>
      <author><first>Xianzhen</first><last>Luo</last><affiliation>Harbin Institute of Techology</affiliation></author>
      <author><first>Yixuan</first><last>Wang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Qingfu</first><last>Zhu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Zhiming</first><last>Zhang</last></author>
      <author><first>Xuanyu</first><last>Zhang</last></author>
      <author><first>Qing</first><last>Yang</last></author>
      <author><first>Dongliang</first><last>Xu</last></author>
      <pages>6816-6831</pages>
      <abstract>The rapid growth in the parameters of LLMs has made inference latency a fundamental bottleneck. Speculative decoding represents a lossless approach to accelerate inference through a guess-and-verify paradigm. Some methods rely on additional architectures to guess draft tokens, which need extra training before use. Alternatively, retrieval-based train-free techniques build libraries from pre-existing corpora or by n-gram generation. However, they face challenges like large storage requirements, time-consuming retrieval, and limited adaptability. Observing that candidate tokens generated during the decoding process are likely to reoccur in future sequences, we propose Token Recycling. This approach stores candidate tokens in an adjacency matrix and employs a breadth-first-search (BFS)-like algorithm to construct a draft tree, which is then validated through tree attention. New candidate tokens from the decoding process are then used to update the matrix. Token Recycling requires &lt;2MB of additional storage and achieves approximately 2x speedup across all sizes of LLMs. It significantly outperforms existing train-free methods by 30% and even a training method by 25%.</abstract>
      <url hash="b96cca5d">2025.acl-long.338</url>
      <bibkey>luo-etal-2025-turning</bibkey>
    </paper>
    <paper id="339">
      <title>Unlocking General Long Chain-of-Thought Reasoning Capabilities of Large Language Models via Representation Engineering</title>
      <author><first>Xinyu</first><last>Tang</last><affiliation>Renmin University of China</affiliation></author>
      <author id="xiaolei-wang-fudan"><first>Xiaolei</first><last>Wang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Zhihao</first><last>Lv</last></author>
      <author><first>Yingqian</first><last>Min</last></author>
      <author><first>Xin</first><last>Zhao</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Binbin</first><last>Hu</last><affiliation>Ant Group</affiliation></author>
      <author><first>Ziqi</first><last>Liu</last><affiliation>Ant Group</affiliation></author>
      <author><first>Zhiqiang</first><last>Zhang</last><affiliation>Ant Group</affiliation></author>
      <pages>6832-6849</pages>
      <abstract>Recent advancements in long chain-of-thoughts (long CoTs) have significantly improved the reasoning capabilities of large language models (LLMs). Existing work finds that the capability of long CoT reasoning can be efficiently elicited by tuning on only a few examples and can easily transfer to other tasks. This motivates us to investigate whether long CoT reasoning is a general capability for LLMs. In this work, we conduct an empirical analysis for this question from the perspective of representation. We find that LLMs do encode long CoT reasoning as a general capability, with a clear distinction from vanilla CoTs. Furthermore, domain-specific representations are also required for the effective transfer of long CoT reasoning. Inspired by these findings, we propose GLORE, a novel representation engineering method to unleash the general long CoT reasoning capabilities of LLMs. Extensive experiments demonstrate the effectiveness and efficiency of GLORE in both in-domain and cross-domain scenarios. The code is available at https://github.com/txy77/GLoRE.</abstract>
      <url hash="03b2066b">2025.acl-long.339</url>
      <bibkey>tang-etal-2025-unlocking</bibkey>
    </paper>
    <paper id="340">
      <title>Drift: Enhancing <fixed-case>LLM</fixed-case> Faithfulness in Rationale Generation via Dual-Reward Probabilistic Inference</title>
      <author><first>Jiazheng</first><last>Li</last><affiliation>King’s College London, University of London</affiliation></author>
      <author><first>Hanqi</first><last>Yan</last></author>
      <author><first>Yulan</first><last>He</last><affiliation>King’s College London, University of London</affiliation></author>
      <pages>6850-6866</pages>
      <abstract>As Large Language Models (LLMs) are increasingly applied to complex reasoning tasks, achieving both accurate task performance and faithful explanations becomes crucial. However, LLMs often generate unfaithful explanations, partly because they do not consistently adhere closely to the provided context. Existing approaches to this problem either rely on superficial calibration methods, such as decomposed Chain-of-Thought prompting, or require costly retraining to improve model faithfulness. In this work, we propose a probabilistic inference paradigm that leverages task-specific and lookahead rewards to ensure that LLM-generated rationales are more faithful to model decisions and align better with input context. These rewards are derived from a domain-specific proposal distribution, allowing for optimized sequential Monte Carlo approximations. Our evaluations across three different reasoning tasks show that this method, which allows for controllable generation during inference, improves both accuracy and faithfulness of LLMs. This method offers a promising path towards making LLMs more reliable for reasoning tasks without sacrificing performance.</abstract>
      <url hash="53641c4d">2025.acl-long.340</url>
      <bibkey>li-etal-2025-drift</bibkey>
    </paper>
    <paper id="341">
      <title>Fairness through Difference Awareness: Measuring <tex-math>\textit{Desired}</tex-math> Group Discrimination in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Angelina</first><last>Wang</last><affiliation>Cornell University</affiliation></author>
      <author><first>Michelle</first><last>Phan</last><affiliation>Stanford University</affiliation></author>
      <author><first>Daniel E.</first><last>Ho</last><affiliation>Stanford University</affiliation></author>
      <author><first>Sanmi</first><last>Koyejo</last><affiliation>Virtue AI and Stanford University</affiliation></author>
      <pages>6867-6893</pages>
      <abstract>Algorithmic fairness has conventionally adopted the mathematically convenient perspective of racial color-blindness (i.e., difference unaware treatment). However, we contend that in a range of important settings, group difference awareness matters. For example, differentiating between groups may be necessary in legal contexts (e.g., the U.S. compulsory draft applies to men but not women) and harm assessments (e.g., referring to girls as “terrorists” may be less harmful than referring to Muslim people as such). Thus, in contrast to most fairness work, we study fairness through the perspective of treating people differently — when it is contextually appropriate to. We first introduce an important distinction between descriptive (fact-based), normative (value-based), and correlation (association-based) benchmarks. This distinction is significant because each category requires separate interpretation and mitigation tailored to its specific characteristics. Then, we present a benchmark suite composed of eight different scenarios for a total of 16k questions that enables us to assess difference awareness. Finally, we show results across ten models that demonstrate difference awareness is a distinct dimension to fairness where existing bias mitigation strategies may backfire.</abstract>
      <url hash="4cfd9311">2025.acl-long.341</url>
      <bibkey>wang-etal-2025-fairness</bibkey>
    </paper>
    <paper id="342">
      <title><fixed-case>M</fixed-case>erge<fixed-case>P</fixed-case>rint: Merge-Resistant Fingerprints for Robust Black-box Ownership Verification of Large Language Models</title>
      <author><first>Shojiro</first><last>Yamabe</last></author>
      <author><first>Futa Kai</first><last>Waseda</last></author>
      <author><first>Tsubasa</first><last>Takahashi</last><affiliation>Turing Inc.</affiliation></author>
      <author><first>Koki</first><last>Wataoka</last><affiliation>SB Intuitions</affiliation></author>
      <pages>6894-6916</pages>
      <abstract>Protecting the intellectual property of Large Language Models (LLMs) has become increasingly critical due to the high cost of training. Model merging, which integrates multiple expert models into a single multi-task model, introduces a novel risk of unauthorized use of LLMs due to its efficient merging process. While fingerprinting techniques have been proposed for verifying model ownership, their resistance to model merging remains unexplored. To address this gap, we propose a novel fingerprinting method, MergePrint, which embeds robust fingerprints capable of surviving model merging. MergePrint enables black-box ownership verification, where owners only need to check if a model produces target outputs for specific fingerprint inputs, without accessing model weights or intermediate outputs. By optimizing against a pseudo-merged model that simulates merged behavior, MergePrint ensures fingerprints that remain detectable after merging. Additionally, to minimize performance degradation, we pre-optimize the fingerprint inputs. MergePrint pioneers a practical solution for black-box ownership verification, protecting LLMs from misappropriation via merging, while also excelling in resistance to broader model theft threats.</abstract>
      <url hash="974b60ca">2025.acl-long.342</url>
      <bibkey>yamabe-etal-2025-mergeprint</bibkey>
    </paper>
    <paper id="343">
      <title>Dynamic Scaling of Unit Tests for Code Reward Modeling</title>
      <author><first>Zeyao</first><last>Ma</last></author>
      <author><first>Xiaokang</first><last>Zhang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Jing</first><last>Zhang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Jifan</first><last>Yu</last></author>
      <author><first>Sijia</first><last>Luo</last></author>
      <author><first>Jie</first><last>Tang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>6917-6935</pages>
      <abstract>Current large language models (LLMs) often struggle to produce accurate responses on the first attempt for complex reasoning tasks like code generation. Prior research tackles this challenge by generating multiple candidate solutions and validating them with LLM-generated unit tests. The execution results of unit tests serve as reward signals to identify correct solutions. As LLMs always confidently make mistakes, these unit tests are not reliable, thereby diminishing the quality of reward signals. Motivated by the observation that scaling the number of solutions improves LLM performance, we explore the impact of scaling unit tests to enhance reward signal quality. Our pioneer experiment reveals a positive correlation between the number of unit tests and reward signal quality, with greater benefits observed in more challenging problems. Based on these insights, we propose CodeRM-8B, a lightweight yet effective unit test generator that enables efficient and high-quality unit test scaling. Additionally, we implement a dynamic scaling mechanism that adapts the number of unit tests based on problem difficulty, further improving efficiency. Experimental results show that our approach significantly improves performance across various models on three benchmarks (e.g., with gains of 18.43 for Llama3-8B and 3.42 for GPT-4o-mini on HumanEval Plus). The parameters of CodeRM-8B and corresponding training data will be available upon publication.</abstract>
      <url hash="0627da1c">2025.acl-long.343</url>
      <bibkey>ma-etal-2025-dynamic</bibkey>
    </paper>
    <paper id="344">
      <title><fixed-case>U</fixed-case>ni<fixed-case>C</fixed-case>onv: Unifying Retrieval and Response Generation for Large Language Models in Conversations</title>
      <author><first>Fengran</first><last>Mo</last></author>
      <author><first>Yifan</first><last>Gao</last><affiliation>Amazon</affiliation></author>
      <author><first>Chuan</first><last>Meng</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Xin</first><last>Liu</last><affiliation>Amazon</affiliation></author>
      <author><first>Zhuofeng</first><last>Wu</last><affiliation>Amazon</affiliation></author>
      <author><first>Kelong</first><last>Mao</last></author>
      <author><first>Zhengyang</first><last>Wang</last><affiliation>Amazon</affiliation></author>
      <author><first>Pei</first><last>Chen</last><affiliation>Texas A&amp;M University - College Station</affiliation></author>
      <author><first>Zheng</first><last>Li</last><affiliation>Amazon</affiliation></author>
      <author><first>Xian</first><last>Li</last><affiliation>Amazon</affiliation></author>
      <author><first>Bing</first><last>Yin</last><affiliation>Amazon</affiliation></author>
      <author><first>Meng</first><last>Jiang</last><affiliation>University of Notre Dame</affiliation></author>
      <pages>6936-6949</pages>
      <abstract>The rapid advancement of conversational search systems revolutionizes how information is accessed by enabling the multi-turn interaction between the user and the system. Existing conversational search systems are usually built with two different models. This separation restricts the system from leveraging the intrinsic knowledge of the models simultaneously, which cannot ensure the effectiveness of retrieval benefiting the generation. The existing studies for developing unified models cannot fully address the aspects of understanding conversational context, managing retrieval independently, and generating responses. In this paper, we explore how to unify dense retrieval and response generation for large language models in conversation. We conduct joint fine-tuning with different objectives and design two mechanisms to reduce the inconsistency risks while mitigating data discrepancy. The evaluations on five conversational search datasets demonstrate that our unified model can mutually improve both tasks and outperform the existing baselines.</abstract>
      <url hash="784d1725">2025.acl-long.344</url>
      <bibkey>mo-etal-2025-uniconv</bibkey>
    </paper>
    <paper id="345">
      <title>Tracking Life’s Ups and Downs: Mining Life Events from Social Media Posts for Mental Health Analysis</title>
      <author><first>Minghao</first><last>Lv</last></author>
      <author><first>Siyuan</first><last>Chen</last></author>
      <author><first>Haoan</first><last>Jin</last></author>
      <author><first>Minghao</first><last>Yuan</last><affiliation>Peking University</affiliation></author>
      <author><first>Qianqian</first><last>Ju</last></author>
      <author><first>Yujia</first><last>Peng</last><affiliation>Peking University</affiliation></author>
      <author><first>Kenny Q.</first><last>Zhu</last><affiliation>University of Texas at Arlington</affiliation></author>
      <author><first>Mengyue</first><last>Wu</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <pages>6950-6965</pages>
      <abstract>Social media platforms possess considerable potential in the realm of exploring mental health. Previous research has indicated that major life events can greatly impact individuals’ mental health. However, due to the complexity and ambiguity nature of life events, shedding its light on social media data is quite challenging. In this paper, we are dedicated to uncovering life events mentioned in posts on social media. We hereby provide a carefully-annotated social media event dataset, PsyEvent, which encompasses 12 major life event categories that are likely to occur in everyday life. This dataset is human-annotated under iterative procedure and boasts a high level of quality. Furthermore, by applying the life events extracted from posts to downstream tasks such as early risk detection of depression and suicide risk prediction, we have observed a considerable improvement in performance. This suggests that extracting life events from social media can be beneficial for the analysis of individuals’ mental health.</abstract>
      <url hash="0ba2acab">2025.acl-long.345</url>
      <bibkey>lv-etal-2025-tracking</bibkey>
    </paper>
    <paper id="346">
      <title><fixed-case>C</fixed-case>ontrol<fixed-case>S</fixed-case>peech: Towards Simultaneous and Independent Zero-shot Speaker Cloning and Zero-shot Language Style Control</title>
      <author><first>Shengpeng</first><last>Ji</last></author>
      <author><first>Qian</first><last>Chen</last></author>
      <author><first>Wen</first><last>Wang</last></author>
      <author><first>Jialong</first><last>Zuo</last></author>
      <author><first>Minghui</first><last>Fang</last></author>
      <author><first>Ziyue</first><last>Jiang</last></author>
      <author><first>Hai</first><last>Huang</last></author>
      <author><first>Zehan</first><last>Wang</last></author>
      <author><first>Xize</first><last>Cheng</last></author>
      <author><first>Siqi</first><last>Zheng</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Zhou</first><last>Zhao</last><affiliation>Zhejiang University and Zhejiang University</affiliation></author>
      <pages>6966-6981</pages>
      <abstract>In this paper, we present ControlSpeech, a text-to-speech (TTS) system capable of fully cloning the speaker’s voice and enabling arbitrary control and adjustment of speaking style. Prior zero-shot TTS models only mimic the speaker’s voice without further control and adjustment capabilities while prior controllable TTS models cannot perform speaker-specific voice generation. Therefore, ControlSpeech focuses on a more challenging task—a TTS system with controllable timbre, content, and style at the same time. ControlSpeech takes speech prompts, content prompts, and style prompts as inputs and utilizes bidirectional attention and mask-based parallel decoding to capture codec representations corresponding to timbre, content, and style in a discrete decoupling codec space. Moreover, we analyze the many-to-many issue in textual style control and propose the Style Mixture Semantic Density (SMSD) module, which is based on Gaussian mixture density networks, to resolve this problem. To facilitate empirical validations, we make available a new style controllable dataset called VccmDataset. Our experimental results demonstrate that ControlSpeech exhibits comparable or state-of-the-art (SOTA) performance in terms of controllability, timbre similarity, audio quality, robustness, and generalizability. Codes are available at https://github.com/jishengpeng/ControlSpeech.</abstract>
      <url hash="82a77282">2025.acl-long.346</url>
      <bibkey>ji-etal-2025-controlspeech</bibkey>
    </paper>
    <paper id="347">
      <title><fixed-case>PIC</fixed-case>: Unlocking Long-Form Text Generation Capabilities of Large Language Models via Position <fixed-case>ID</fixed-case> Compression</title>
      <author><first>Haoran</first><last>Que</last></author>
      <author><first>Wenge</first><last>Rong</last><affiliation>Beihang University</affiliation></author>
      <pages>6982-6995</pages>
      <abstract>Long-context understanding is crucial for large language models (LLMs) and has become a fundamental capability for most LLMs. However, beyond the focus on “input-long”, the ability to “output-long” is equally significant, yet it remains underexplored. To address this limitation, we propose a simple, efficient, and plug-in approach, Position ID Compression (PIC), to unlock the long-form text generation potential of LLMs. The idea is straightforward: by compressing the position ids of the context, we provoke and guide LLMs to generate coherent and longer output. Specifically, we find that directly reducing the position ids by a fixed ratio significantly impacts the generation quality. To mitigate this, we propose two variants of PIC: NTK-aware PIC and Dynamic PIC. Without additional training, both methods enable LLMs to extend their generation length by approximately 1.5 times without compromising generation quality. Furthermore, by integrating supervised fine-tuning (SFT) with PIC, we propose PIC-SFT, which further improves LLMs’ long-form text generation capabilities, achieving top performance on HelloBench and LongBench-Write. Extensive experiments demonstrate the effectiveness of our approach.</abstract>
      <url hash="a5a60293">2025.acl-long.347</url>
      <bibkey>que-rong-2025-pic</bibkey>
    </paper>
    <paper id="348">
      <title>Towards Effective Extraction and Evaluation of Factual Claims</title>
      <author><first>Dasha</first><last>Metropolitansky</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Jonathan</first><last>Larson</last><affiliation>Microsoft</affiliation></author>
      <pages>6996-7045</pages>
      <abstract>A common strategy for fact-checking long-form content generated by Large Language Models (LLMs) is extracting simple claims that can be verified independently. Since inaccurate or incomplete claims compromise fact-checking results, ensuring claim quality is critical. However, the lack of a standardized evaluation framework impedes assessment and comparison of claim extraction methods. To address this gap, we propose a framework for evaluating claim extraction in the context of fact-checking along with automated, scalable, and replicable methods for applying this framework, including novel approaches for measuring coverage and decontextualization. We also introduce Claimify, an LLM-based claim extraction method, and demonstrate that it outperforms existing methods under our evaluation framework. A key feature of Claimify is its ability to handle ambiguity and extract claims only when there is high confidence in the correct interpretation of the source text.</abstract>
      <url hash="72a21aea">2025.acl-long.348</url>
      <bibkey>metropolitansky-larson-2025-towards</bibkey>
    </paper>
    <paper id="349">
      <title>Beyond Facts: Evaluating Intent Hallucination in Large Language Models</title>
      <author><first>Yijie</first><last>Hao</last></author>
      <author><first>Haofei</first><last>Yu</last></author>
      <author><first>Jiaxuan</first><last>You</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <pages>7046-7069</pages>
      <abstract>When exposed to complex queries containing multiple conditions, today’s large language models (LLMs) tend to produce responses that only partially satisfy the query while neglecting certain conditions. We, therefore, introduce the concept of Intent Hallucination, a phenomenon where LLMs either omit (failing to address certain parts) or misinterpret (responding to invented query parts) elements of the given query, leading to responses misaligned with the original query. To systematically evaluate intent hallucination, we introduce FAITHQA, a novel benchmark for intent hallucination that contains 20,068 problems, covering both query-only and retrieval-augmented generation (RAG) setups with varying topics and difficulty. FAITHQA is the first hallucination benchmark that goes beyond factual verification, tailored to identify the fundamental cause of intent hallucination. By evaluating various LLMs on FAITHQA, we find that (1) intent hallucination is a common issue even for state-of-the-art models, and (2) such a phenomenon stems from omission or misinterpretation of LLMs. To facilitate future research, we introduce an automatic LLM generation evaluation metric, named INTENT CONSTRAINT, for detecting intent hallucination. Human evaluation results demonstrate that INTENT CONSTRAINT is closer to human performance for intent hallucination compared to baselines.</abstract>
      <url hash="aa8cd0ad">2025.acl-long.349</url>
      <bibkey>hao-etal-2025-beyond</bibkey>
    </paper>
    <paper id="350">
      <title>A Systematic Study of Compositional Syntactic Transformer Language Models</title>
      <author><first>Yida</first><last>Zhao</last></author>
      <author><first>Hao</first><last>Xve</last><affiliation>ShanghaiTech University</affiliation></author>
      <author><first>Xiang</first><last>Hu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Kewei</first><last>Tu</last><affiliation>ShanghaiTech University</affiliation></author>
      <pages>7070-7083</pages>
      <abstract>Syntactic language models (SLMs) enhance Transformers by incorporating syntactic biases through the modeling of linearized syntactic parse trees alongside surface sentences. This paper focuses on compositional SLMs that are based on constituency parse trees and contain explicit bottom-up composition of constituent representations. We identify key aspects of design choices in existing compositional SLMs and propose a unified framework encompassing both existing models and novel variants. We conduct a comprehensive empirical evaluation of all the variants in our framework across language modeling, syntactic generalization, summarization, and inference efficiency. Based on the experimental results, we make multiple recommendations on the design of compositional SLMs. Our code is released at https://github.com/zhaoyd1/compositional_SLMs.</abstract>
      <url hash="2b5d7895">2025.acl-long.350</url>
      <bibkey>zhao-etal-2025-systematic</bibkey>
    </paper>
    <paper id="351">
      <title><fixed-case>M</fixed-case>-<fixed-case>MAD</fixed-case>: Multidimensional Multi-Agent Debate for Advanced Machine Translation Evaluation</title>
      <author><first>Zhaopeng</first><last>Feng</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Jiayuan</first><last>Su</last></author>
      <author><first>Jiamei</first><last>Zheng</last></author>
      <author><first>Jiahan</first><last>Ren</last></author>
      <author><first>Yan</first><last>Zhang</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Jian</first><last>Wu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Hongwei</first><last>Wang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Zuozhu</first><last>Liu</last><affiliation>Zhejiang University</affiliation></author>
      <pages>7084-7107</pages>
      <abstract>Recent advancements in large language models (LLMs) have given rise to the LLM-as-a-judge paradigm, showcasing their potential to deliver human-like judgments. However, in the field of machine translation (MT) evaluation, current LLM-as-a-judge methods fall short of learned automatic metrics. In this paper, we propose Multidimensional Multi-Agent Debate (M-MAD), a systematic LLM-based multi-agent framework for advanced LLM-as-a-judge MT evaluation. Our findings demonstrate that M-MAD achieves significant advancements by (1) decoupling heuristic MQM criteria into distinct evaluation dimensions for fine-grained assessments; (2) employing multi-agent debates to harness the collaborative reasoning capabilities of LLMs; (3) synthesizing dimension-specific results into a final evaluation judgment to ensure robust and reliable outcomes. Comprehensive experiments show that M-MAD not only outperforms all existing LLM-as-a-judge methods but also competes with state-of-the-art reference-based automatic metrics, even when powered by a suboptimal model like GPT-4o mini. Detailed ablations and analysis highlight the superiority of our framework design, offering a fresh perspective for LLM-as-a-judge paradigm. Our code and data are publicly available at https://github.com/SU-JIAYUAN/M-MAD.</abstract>
      <url hash="ed618bb3">2025.acl-long.351</url>
      <bibkey>feng-etal-2025-mad</bibkey>
    </paper>
    <paper id="352">
      <title><fixed-case>S</fixed-case>ong<fixed-case>C</fixed-case>omposer: A Large Language Model for Lyric and Melody Generation in Song Composition</title>
      <author><first>Shuangrui</first><last>Ding</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Zihan</first><last>Liu</last></author>
      <author><first>Xiaoyi</first><last>Dong</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Pan</first><last>Zhang</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Rui</first><last>Qian</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Junhao</first><last>Huang</last></author>
      <author><first>Conghui</first><last>He</last><affiliation>Shanghai AI Lab</affiliation></author>
      <author><first>Dahua</first><last>Lin</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Jiaqi</first><last>Wang</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <pages>7108-7127</pages>
      <abstract>Creating lyrics and melodies for the vocal track in a symbolic format, known as song composition, demands expert musical knowledge of melody, an advanced understanding of lyrics, and precise alignment between them. Despite achievements in sub-tasks such as lyric generation, lyric-to-melody, and melody-to-lyric, etc, a unified model for song composition has not yet been achieved. In this paper, we introduce SongComposer, a pioneering step towards a unified song composition model that can readily create symbolic lyrics and melodies following instructions. SongComposer is a music-specialized large language model (LLM) that, for the first time, integrates the capability of simultaneously composing lyrics and melodies into LLMs by leveraging three key innovations: 1) a flexible tuple format for word-level alignment of lyrics and melodies, 2) an extended tokenizer vocabulary for song notes, with scalar initialization based on musical knowledge to capture rhythm, and 3) a multi-stage pipeline that captures musical structure, starting with motif-level melody patterns and progressing to phrase-level structure for improved coherence. Extensive experiments demonstrate that SongComposer outperforms advanced LLMs, including GPT-4, in tasks such as lyric-to-melody generation, melody-to-lyric generation, song continuation, and text-to-song creation. Moreover, we will release SongCompose, a large-scale dataset for training, containing paired lyrics and melodies in Chinese and English.</abstract>
      <url hash="294f5e26">2025.acl-long.352</url>
      <bibkey>ding-etal-2025-songcomposer</bibkey>
    </paper>
    <paper id="353">
      <title>Personalized Text Generation with Contrastive Activation Steering</title>
      <author><first>Jinghao</first><last>Zhang</last></author>
      <author><first>Yuting</first><last>Liu</last></author>
      <author><first>Wenjie</first><last>Wang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Qiang</first><last>Liu</last><affiliation>Institute of Automation, Chinese Academy of Sciences</affiliation></author>
      <author><first>Shu</first><last>Wu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Liang</first><last>Wang</last><affiliation>Institute of Automation, CAS,China</affiliation></author>
      <author><first>Tat-Seng</first><last>Chua</last><affiliation>National University of Singapore</affiliation></author>
      <pages>7128-7141</pages>
      <abstract>Personalized text generation aims to infer users’ writing style preferences from their historical texts and generate outputs that faithfully reflect these stylistic characteristics. Existing solutions primarily adopt two paradigms: retrieval-augmented generation (RAG) and parameter-efficient fine-tuning (PEFT). While these approaches have advanced the field, they suffer from two critical limitations: (1) the entanglement of content semantics and stylistic patterns in historical texts impedes accurate modeling of user-specific writing preferences; and (2) scalability challenges arising from both RAG’s inference latency by retrieval operations and PEFT’s parameter storage requirements for per user model. To overcome these limitations, we propose StyleVector, a training-free framework that disentangles and represents personalized writing style as a vector in LLM’s activation space, enabling style-steered generation during inference without requiring costly retrieval or parameter storage. Comprehensive experiments demonstrate that our framework achieves a significant 8% relative improvement in personalized generation while reducing storage requirements by 1700 <tex-math>\times</tex-math> over PEFT method.</abstract>
      <url hash="3db6c8d6">2025.acl-long.353</url>
      <bibkey>zhang-etal-2025-personalized</bibkey>
    </paper>
    <paper id="354">
      <title><fixed-case>G</fixed-case>umbel Reranking: Differentiable End-to-End Reranker Optimization</title>
      <author><first>Siyuan</first><last>Huang</last></author>
      <author><first>Zhiyuan</first><last>Ma</last></author>
      <author><first>Jintao</first><last>Du</last><affiliation>Ant Group</affiliation></author>
      <author><first>Changhua</first><last>Meng</last><affiliation>Ant Group</affiliation></author>
      <author><first>Weiqiang</first><last>Wang</last><affiliation>Ant Group</affiliation></author>
      <author><first>Jingwen</first><last>Leng</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Minyi</first><last>Guo</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Zhouhan</first><last>Lin</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>7142-7161</pages>
      <abstract>RAG systems rely on rerankers to identify relevant documents. However, fine-tuning these models remains challenging due to the scarcity of annotated query-document pairs. Existing distillation-based approaches suffer from training-inference misalignment and fail to capture interdependencies among candidate documents. To overcome these limitations, we reframe the reranking process as an attention-mask problem and propose Gumbel Reranking, an end-to-end training framework for rerankers aimed at minimizing the training-inference gap. In our approach, reranker optimization is reformulated as learning a stochastic, document-wise Top-<tex-math>k</tex-math> attention mask using the Gumbel Trick and Relaxed Top-<tex-math>k</tex-math> Sampling. This formulation enables end-to-end optimization by minimizing the overall language loss. Experiments across various settings consistently demonstrate performance gains, including a 10.4% improvement in recall on HotpotQA for distinguishing indirectly relevant documents.</abstract>
      <url hash="d8335aa3">2025.acl-long.354</url>
      <bibkey>huang-etal-2025-gumbel</bibkey>
    </paper>
    <paper id="355">
      <title>Hybrid Preferences: Learning to Route Instances for Human vs. <fixed-case>AI</fixed-case> Feedback</title>
      <author><first>Lester James Validad</first><last>Miranda</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Yizhong</first><last>Wang</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <author><first>Yanai</first><last>Elazar</last><affiliation>Allen Institute for Artificial Intelligence and Department of Computer Science</affiliation></author>
      <author><first>Sachin</first><last>Kumar</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <author><first>Valentina</first><last>Pyatkin</last><affiliation>Allen Institute for Artificial Intelligence and Department of Computer Science</affiliation></author>
      <author><first>Faeze</first><last>Brahman</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Noah A.</first><last>Smith</last><affiliation>University of Washington and Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Hannaneh</first><last>Hajishirzi</last></author>
      <author><first>Pradeep</first><last>Dasigi</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>7162-7200</pages>
      <abstract>Learning from human feedback has enabled the alignment of language models (LMs) with human preferences. However, collecting human preferences is expensive and time-consuming, with highly variable annotation quality. An appealing alternative is to distill preferences from LMs as a source of synthetic annotations, offering a cost-effective and scalable alternative, albeit susceptible to other biases and errors. In this work, we introduce HyPER, a Hybrid Preference routER that defers an annotation to either humans or LMs, achieving better annotation quality while reducing the cost of human-only annotation. We formulate this as an optimization problem: given a preference dataset and an evaluation metric, we (1) train a performance prediction model (PPM) to predict a reward model’s (RM) performance on an arbitrary combination of human and LM annotations and (2) employ a routing strategy that selects a combination that maximizes predicted performance. We train the PPM on MultiPref, a new preference dataset with 10K instances paired with human and LM labels. We show that the selected hybrid mixture of synthetic and direct human preferences using HyPER achieves better RM performance compared to using either one exclusively by 7-13% on RewardBench and generalizes across unseen preference datasets and other base models. We also observe the same trend in other benchmarks using Best-of-N reranking, where the hybrid mix has 2-3% better performance. Finally, we analyze features from HyPER and find that prompts with moderate safety concerns or complexity benefit the most from human feedback.</abstract>
      <url hash="2b389bd5">2025.acl-long.355</url>
      <bibkey>miranda-etal-2025-hybrid</bibkey>
    </paper>
    <paper id="356">
      <title><fixed-case>SEOE</fixed-case>: A Scalable and Reliable Semantic Evaluation Framework for Open Domain Event Detection</title>
      <author><first>Yi-Fan</first><last>Lu</last></author>
      <author><first>Xian-Ling</first><last>Mao</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Tian</first><last>Lan</last></author>
      <author><first>Tong</first><last>Zhang</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Yu-Shi</first><last>Zhu</last></author>
      <author><first>Heyan</first><last>Huang</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <pages>7201-7218</pages>
      <abstract>Automatic evaluation for Open Domain Event Detection (ODED) is a highly challenging task, because ODED is characterized by a vast diversity of un-constrained output labels from various domains. Nearly all existing evaluation methods for ODED usually first construct evaluation benchmarks with limited labels and domain coverage, and then evaluate ODED methods using metrics based on token-level label matching rules. However, this kind of evaluation framework faces two issues: (1) The limited evaluation benchmarks lack representatives of the real world, making it difficult to accurately reflect the performance of various ODED methods in real-world scenarios; (2) Evaluation metrics based on token-level matching rules fail to capture semantic similarity between predictions and golden labels. To address these two problems above, we propose a scalable and reliable Semantic-level Evaluation framework for Open domain Event detection (SEOE) by constructing a more representative evaluation benchmark and introducing a semantic evaluation metric. Specifically, our proposed framework first constructs a scalable evaluation benchmark that currently includes 564 event types covering 7 major domains, with a cost-effective supplementary annotation strategy to ensure the benchmark’s representativeness. The strategy also allows for the supplement of new event types and domains in the future. Then, the proposed SEOE leverages large language models (LLMs) as automatic evaluation agents to compute a semantic F1-score, incorporating fine-grained definitions of semantically similar labels to enhance the reliability of the evaluation. Extensive experiments validate the representatives of the benchmark and the reliability of the semantic evaluation metric. Existing ODED methods are thoroughly evaluated, and the error patterns of predictions are analyzed, revealing several insightful findings.</abstract>
      <url hash="7dfca4cd">2025.acl-long.356</url>
      <bibkey>lu-etal-2025-seoe</bibkey>
    </paper>
    <paper id="357">
      <title>The <fixed-case>UD</fixed-case>-<fixed-case>N</fixed-case>ews<fixed-case>C</fixed-case>rawl Treebank: Reflections and Challenges from a Large-scale <fixed-case>T</fixed-case>agalog Syntactic Annotation Project</title>
      <author><first>Angelina Aspra</first><last>Aquino</last><affiliation>Charles Darwin University</affiliation></author>
      <author><first>Lester James Validad</first><last>Miranda</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Elsie Marie T.</first><last>Or</last><affiliation>University of the Philippines Diliman</affiliation></author>
      <pages>7219-7239</pages>
      <abstract>This paper presents UD-NewsCrawl, the largest Tagalog treebank to date, containing 15.6k trees manually annotated according tothe Universal Dependencies framework. We detail our treebank development process, including data collection, pre-processing, manual annotation, and quality assurance procedures. We provide baseline evaluations using multiple transformer-based models to assess the performance of state-of-the-art dependency parsers on Tagalog. We also highlight challenges in the syntactic analysis of Tagalog given its distinctive grammatical properties, and discuss its implications for the annotation of this treebank. We anticipate that UD-NewsCrawl and our baseline model implementations will serve as valuable resources for advancing computational linguistics research in underrepresented languages like Tagalog.</abstract>
      <url hash="23d90f84">2025.acl-long.357</url>
      <bibkey>aquino-etal-2025-ud</bibkey>
    </paper>
    <paper id="358">
      <title><fixed-case>DRAG</fixed-case>: Distilling <fixed-case>RAG</fixed-case> for <fixed-case>SLM</fixed-case>s from <fixed-case>LLM</fixed-case>s to Transfer Knowledge and Mitigate Hallucination via Evidence and Graph-based Distillation</title>
      <author><first>Jennifer</first><last>Chen</last></author>
      <author><first>Aidar</first><last>Myrzakhan</last></author>
      <author><first>Yaxin</first><last>Luo</last><affiliation>Technical University of Denmark</affiliation></author>
      <author><first>Hassaan Muhammad</first><last>Khan</last></author>
      <author><first>Sondos Mahmoud</first><last>Bsharat</last></author>
      <author><first>Zhiqiang</first><last>Shen</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>7240-7260</pages>
      <abstract>Retrieval-Augmented Generation (RAG) methods have proven highly effective for tasks requiring factual consistency and robust knowledge retrieval. However, large-scale RAG systems consume significant computational resources and are prone to generating “hallucinated” content from Humans. In this work, we introduce DRAG, a novel framework for distilling RAG knowledge from large-scale Language Models (LLMs) into small LMs (SLMs). Our approach leverages evidence- and knowledge graph–based distillation, ensuring that the distilled model retains critical factual knowledge while significantly reducing model size and computational cost. By aligning the smaller model’s predictions with a structured knowledge graph and ranked evidence, DRAG effectively mitigates hallucinations and improves factual accuracy. We further present a case demonstrating how our framework mitigates user privacy risks and introduce a corresponding benchmark. Experimental evaluations on multiple benchmarks demonstrate that our method outperforms the prior competitive RAG methods like MiniRAG for SLMs by up to 27.7% using the same models, preserving high-level efficiency and reliability. With DRAG, we provide a practical and resource-efficient roadmap to deploying enhanced retrieval and generation capabilities in small-size LLMs. Code is available at https://github.com/VILA-Lab/DRAG.</abstract>
      <url hash="ed6490cc">2025.acl-long.358</url>
      <bibkey>chen-etal-2025-drag</bibkey>
    </paper>
    <paper id="359">
      <title><fixed-case>G</fixed-case>-Safeguard: A Topology-Guided Security Lens and Treatment on <fixed-case>LLM</fixed-case>-based Multi-agent Systems</title>
      <author><first>Shilong</first><last>Wang</last></author>
      <author><first>Guibin</first><last>Zhang</last></author>
      <author><first>Miao</first><last>Yu</last></author>
      <author><first>Guancheng</first><last>Wan</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Fanci</first><last>Meng</last></author>
      <author><first>Chongye</first><last>Guo</last></author>
      <author><first>Kun</first><last>Wang</last></author>
      <author><first>Yang</first><last>Wang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>7261-7276</pages>
      <abstract>Large Language Model (LLM)-based Multi-agent Systems (MAS) have demonstrated remarkable capabilities in various complex tasks, ranging from collaborative problem-solving to autonomous decision-making. However, as these systems become increasingly integrated into critical applications, their vulnerability to <i>adversarial attacks</i>, <i>misinformation propagation</i>, and <i>unintended behaviors</i> have raised significant concerns. To address this challenge, we introduce G-Safeguard, a topology-guided security lens and treatment for robust LLM-MAS, which leverages graph neural networks to detect anomalies on the multi-agent utterance graph and employ topological intervention for attack remediation. Extensive experiments demonstrate that G-Safeguard: (I) exhibits significant effectiveness under various attack strategies, recovering over 40% of the performance for prompt injection; (II) is highly adaptable to diverse LLM backbones and large-scale MAS; (III) can seamlessly combine with mainstream MAS with security guarantees.</abstract>
      <url hash="b2d72aac">2025.acl-long.359</url>
      <bibkey>wang-etal-2025-g</bibkey>
    </paper>
    <paper id="360">
      <title>Deontological Keyword Bias: The Impact of Modal Expressions on Normative Judgments of Language Models</title>
      <author><first>Bumjin</first><last>Park</last></author>
      <author><first>Leejinsil</first><last>Leejinsil</last></author>
      <author><first>Jaesik</first><last>Choi</last><affiliation>Korea Advanced Institute of Science &amp; Technology and INEEJI</affiliation></author>
      <pages>7277-7296</pages>
      <abstract>Large language models (LLMs) are increasingly engaging in moral and ethical reasoning, where criteria for judgment are often unclear, even for humans. While LLM alignment studies cover many areas, one important yet underexplored area is how LLMs make judgments about obligations. This work reveals a strong tendency in LLMs to judge non-obligatory contexts as obligations when prompts are augmented with modal expressions such as <i>must</i> or <i>ought to</i>. We introduce this phenomenon as Deontological Keyword Bias (DKB). We find that LLMs judge over 90% of commonsense scenarios as obligations when modal expressions are present. This tendency is consist across various LLM families, question types, and answer formats. To mitigate DKB, we propose a judgment strategy that integrates few-shot examples with reasoning prompts. This study sheds light on how modal expressions, as a form of linguistic framing, influence the normative decisions of LLMs and underscores the importance of addressing such biases to ensure judgment alignment.</abstract>
      <url hash="fdfdeaaa">2025.acl-long.360</url>
      <bibkey>park-etal-2025-deontological</bibkey>
    </paper>
    <paper id="361">
      <title><fixed-case>L</fixed-case>egal<fixed-case>R</fixed-case>easoner: Step-wised Verification-Correction for Legal Judgment Reasoning</title>
      <author><first>Weijie</first><last>Shi</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Han</first><last>Zhu</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Jiaming</first><last>Ji</last></author>
      <author><first>Mengze</first><last>Li</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Jipeng</first><last>Zhang</last></author>
      <author><first>Ruiyuan</first><last>Zhang</last></author>
      <author><first>Jia</first><last>Zhu</last><affiliation>Zhejiang Normal University</affiliation></author>
      <author><first>Jiajie</first><last>Xu</last><affiliation>Soochow University</affiliation></author>
      <author><first>Sirui</first><last>Han</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Yike</first><last>Guo</last><affiliation>Hong Kong University of Science and Technology and Imperial College London</affiliation></author>
      <pages>7297-7313</pages>
      <abstract>Legal judgment prediction (LJP) aims to function as a judge by making final rulings based on case claims and facts, which plays a vital role in the judicial domain for supporting court decision-making and improving judicial efficiency. However, existing methods often struggle with logical errors when conducting complex legal reasoning. We propose LegalReasoner, which enhances LJP reliability through step-wise verification and correction of the reasoning process. Specifically, it first identifies dispute points to decompose complex cases, and then conducts step-wise reasoning while employing a process verifier to validate each step’s logic from correctness, progressiveness, and potential perspectives. When errors are detected, expert-designed attribution and resolution strategies are applied for correction. To fine-tune LegalReasoner, we release the LegalHK dataset, containing 58,130 Hong Kong court cases with detailed annotations of dispute points, step-by-step reasoning chains, and process verification labels. Experiments demonstrate that LegalReasoner significantly improves concordance with court decisions from 72.37 to 80.27 on LLAMA-3.1-70B. The data is available at https://huggingface.co/datasets/weijiezz/LegalHK.</abstract>
      <url hash="c43a6884">2025.acl-long.361</url>
      <bibkey>shi-etal-2025-legalreasoner</bibkey>
    </paper>
    <paper id="362">
      <title>Rolling the <fixed-case>DICE</fixed-case> on Idiomaticity: How <fixed-case>LLM</fixed-case>s Fail to Grasp Context</title>
      <author><first>Maggie</first><last>Mi</last></author>
      <author><first>Aline</first><last>Villavicencio</last><affiliation>University of Exeter and University of Sheffield</affiliation></author>
      <author><first>Nafise Sadat</first><last>Moosavi</last><affiliation>University of Sheffield</affiliation></author>
      <pages>7314-7332</pages>
      <abstract>Human processing of idioms heavily depends on interpreting the surrounding context in which they appear. While large language models (LLMs) have achieved impressive performance on idiomaticity detection benchmarks, this success may be driven by reasoning shortcuts present in existing datasets. To address this, we introduce a novel, controlled contrastive dataset (DICE) specifically designed to assess whether LLMs can effectively leverage context to disambiguate idiomatic meanings. Furthermore, we investigate the influence of collocational frequency and sentence probability—proxies for human processing known to affect idiom resolution—on model performance. Our results show that LLMs frequently fail to resolve idiomaticity when it depends on contextual understanding, performing better on sentences deemed more likely by the model. Additionally, idiom frequency influences performance but does not guarantee accurate interpretation. Our findings emphasize the limitations of current models in grasping contextual meaning and highlight the need for more context-sensitive evaluation.</abstract>
      <url hash="5bc20d51">2025.acl-long.362</url>
      <bibkey>mi-etal-2025-rolling</bibkey>
    </paper>
    <paper id="363">
      <title><fixed-case>C</fixed-case>hart<fixed-case>C</fixed-case>oder: Advancing Multimodal Large Language Model for Chart-to-Code Generation</title>
      <author><first>Xuanle</first><last>Zhao</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xianzhen</first><last>Luo</last><affiliation>Harbin Institute of Techology</affiliation></author>
      <author><first>Qi</first><last>Shi</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Chi</first><last>Chen</last></author>
      <author><first>Shuo</first><last>Wang</last></author>
      <author><first>Zhiyuan</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Maosong</first><last>Sun</last><affiliation>Tsinghua University</affiliation></author>
      <pages>7333-7348</pages>
      <abstract>Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in chart understanding tasks. However, interpreting charts with textual descriptions often leads to information loss, as it fails to fully capture the dense information embedded in charts. In contrast, parsing charts into code provides lossless representations that can effectively contain all critical details. Although existing open-source MLLMs have achieved success in chart understanding tasks, they still face two major challenges when applied to chart-to-code tasks: (1) Low executability and poor restoration of chart details in the generated code and (2) Lack of large-scale and diverse training data. To address these challenges, we propose <b>ChartCoder</b>, the first dedicated chart-to-code MLLM, which leverages Code LLMs as the language backbone to enhance the executability of the generated code. Furthermore, we introduce <b>Chart2Code-160k</b>, the first large-scale and diverse dataset for chart-to-code generation, and propose the <b>Snippet-of-Thought (SoT)</b> method, which transforms direct chart-to-code generation data into step-by-step generation. Experiments demonstrate that ChartCoder, with only 7B parameters, surpasses existing open-source MLLMs on chart-to-code benchmarks, achieving superior chart restoration and code excitability. Our code is available at <url>https://github.com/thunlp/ChartCoder</url>.</abstract>
      <url hash="067537ca">2025.acl-long.363</url>
      <bibkey>zhao-etal-2025-chartcoder</bibkey>
    </paper>
    <paper id="364">
      <title>The Cross-linguistic Role of <fixed-case>A</fixed-case>nimacy in Grammar Structures</title>
      <author><first>Nina</first><last>Gregorio</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Matteo</first><last>Gay</last></author>
      <author><first>Sharon</first><last>Goldwater</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Edoardo</first><last>Ponti</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>7349-7363</pages>
      <abstract>Animacy is a semantic feature of nominals and follows a hierarchy: personal pronouns &gt; human &gt; animate &gt; inanimate. In several languages, animacy imposes hard constraints on grammar. While it has been argued that these constraints may emerge from universal soft tendencies, it has been difficult to provide empirical evidence for this conjecture due to the lack of data annotated with animacy classes. In this work, we first propose a method to reliably classify animacy classes of nominals in 11 languages from 5 families, leveraging multilingual large language models (LLMs) and word sense disambiguation datasets. Then, through this newly acquired data, we verify that animacy displays consistent cross-linguistic tendencies in terms of preferred morphosyntactic constructions, although not always in line with received wisdom: animacy in nouns correlates with the alignment role of agent, early positions in a clause, and syntactic pivot (e.g., for relativisation), but not necessarily with grammatical subjecthood. Furthermore, the behaviour of personal pronouns in the hierarchy is idiosyncratic as they are rarely plural and relativised, contrary to high-animacy nouns.</abstract>
      <url hash="76aaa1f0">2025.acl-long.364</url>
      <bibkey>gregorio-etal-2025-cross</bibkey>
    </paper>
    <paper id="365">
      <title><fixed-case>L</fixed-case>ex<fixed-case>G</fixed-case>en: Domain-aware Multilingual Lexicon Generation</title>
      <author><first>Ayush</first><last>Maheshwari</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Atul Kumar</first><last>Singh</last></author>
      <author><first>N J</first><last>Karthika</last><affiliation>Indian Institute of Technology Bombay, Indian Institute of Technology, Bombay</affiliation></author>
      <author><first>Krishnakant</first><last>Bhatt</last><affiliation>Indian Institute of Technology, Bombay, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <author><first>Preethi</first><last>Jyothi</last><affiliation>Indian Institute of Technology Bombay</affiliation></author>
      <author><first>Ganesh</first><last>Ramakrishnan</last><affiliation>Indian Institute of Technology Bombay, Indian Institute of Technology Bombay</affiliation></author>
      <pages>7364-7375</pages>
      <abstract>Lexicon or dictionary generation across domains has the potential for societal impact, as it can potentially enhance information accessibility for a diverse user base while preserving language identity. Prior work in the field primarily focuses on bilingual lexical induction, which deals with word alignments using mapping-based or corpora-based approaches. However, these approaches do not cater to domain-specific lexicon generation that consists of domain-specific terminology. This task becomes particularly important in specialized medical, engineering, and other technical domains, owing to the highly infrequent usage of the terms and scarcity of data involving domain-specific terms especially for low-resource languages. We propose a new model to generate dictionary words for 6 Indian languages in the multi-domain setting. Our model consists of domain-specific and domain-generic layers that encode information, and these layers are invoked via a learnable routing technique. We also release a new benchmark dataset consisting of &gt;75K translation pairs across 6 Indian languages spanning 8 diverse domains. We conduct both zero-shot and few-shot experiments across multiple domains to show the efficacy of our proposed model in generalizing to unseen domains and unseen languages. Additionally, we also perform a human post-hoc evaluation on unseen languages. The source code and dataset is present at https://github.com/Atulkmrsingh/lexgen.</abstract>
      <url hash="95d8fde4">2025.acl-long.365</url>
      <bibkey>maheshwari-etal-2025-lexgen</bibkey>
    </paper>
    <paper id="366">
      <title>How to Train Long-Context Language Models (Effectively)</title>
      <author><first>Tianyu</first><last>Gao</last></author>
      <author><first>Alexander</first><last>Wettig</last><affiliation>Princeton University</affiliation></author>
      <author><first>Howard</first><last>Yen</last><affiliation>Princeton University</affiliation></author>
      <author><first>Danqi</first><last>Chen</last><affiliation>Princeton University</affiliation></author>
      <pages>7376-7399</pages>
      <abstract>We study continued training and supervised fine-tuning (SFT) of a language model (LM) to make effective use of long-context information. We first establish a reliable evaluation protocol to guide model development—instead of perplexity or simple needle-in-a-haystack (NIAH) tests, we use a broad set of long-context downstream tasks, and we evaluate models after SFT as this better reveals long-context abilities. Supported by our robust evaluations, we run thorough experiments to decide the data mix for continued pre-training, the instruction tuning dataset, and many other design choices such as position extrapolation. We find that (1) code repositories and books are excellent sources of long data, but it is crucial to combine them with high-quality short-context data; (2) training with a sequence length beyond the evaluation length boosts long-context performance; (3) for SFT, using only short instruction datasets yields strong performance on long-context tasks. Our final model, ProLong-8B, which is initialized from Llama-3 and trained on 40B tokens, demonstrates state-of-the-art long-context performance among similarly sized models at a length of 128K. ProLong outperforms Llama-3.1-8B-Instruct on the majority of long-context tasks despite using only 5% as many tokens during long-context training. Additionally, ProLong can effectively process up to 512K tokens, one of the longest context windows of publicly available LMs.</abstract>
      <url hash="deed3407">2025.acl-long.366</url>
      <bibkey>gao-etal-2025-train</bibkey>
    </paper>
    <paper id="367">
      <title><fixed-case>M</fixed-case>ath<fixed-case>F</fixed-case>usion: Enhancing Mathematical Problem-solving of <fixed-case>LLM</fixed-case> through Instruction Fusion</title>
      <author><first>Qizhi</first><last>Pei</last></author>
      <author><first>Lijun</first><last>Wu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Zhuoshi</first><last>Pan</last></author>
      <author><first>Yu</first><last>Li</last></author>
      <author><first>Honglin</first><last>Lin</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Chenlin</first><last>Ming</last></author>
      <author><first>Xin</first><last>Gao</last></author>
      <author><first>Conghui</first><last>He</last><affiliation>Shanghai AI Lab</affiliation></author>
      <author><first>Rui</first><last>Yan</last><affiliation>Renmin University of China</affiliation></author>
      <pages>7400-7420</pages>
      <abstract>Large Language Models (LLMs) have shown impressive progress in mathematical reasoning. While data augmentation is promising to enhance mathematical problem-solving ability, current approaches are predominantly limited to instance-level modifications—such as rephrasing or generating syntactic variations—which fail to capture and leverage the intrinsic relational structures inherent in mathematical knowledge. Inspired by human learning processes, where mathematical proficiency develops through systematic exposure to interconnected concepts, we introduce <b>MathFusion</b>, a novel framework that enhances mathematical reasoning through cross-problem instruction synthesis. MathFusion implements this through three fusion strategies: (1) <i>sequential fusion</i>, which chains related problems to model solution dependencies; (2) <i>parallel fusion</i>, which combines analogous problems to reinforce conceptual understanding; and (3) <i>conditional fusion</i>, which creates context-aware selective problems to enhance reasoning flexibility. By applying these strategies, we generate a new dataset, <b>MathFusionQA</b>, followed by fine-tuning models (DeepSeekMath-7B, Mistral-7B, Llama3-8B) on it. Experimental results demonstrate that MathFusion achieves substantial improvements in mathematical reasoning while maintaining high data efficiency, boosting performance by 18.0 points in accuracy across diverse benchmarks while requiring only 45K additional synthetic instructions, representing a substantial improvement over traditional single-instruction approaches.</abstract>
      <url hash="bd57f55a">2025.acl-long.367</url>
      <bibkey>pei-etal-2025-mathfusion</bibkey>
    </paper>
    <paper id="368">
      <title>Mining Complex Patterns of Argumentative Reasoning in Natural Language Dialogue</title>
      <author><first>Ramon</first><last>Ruiz-Dolz</last><affiliation>University of Dundee</affiliation></author>
      <author><first>Zlata</first><last>Kikteva</last><affiliation>Universität Passau</affiliation></author>
      <author><first>John</first><last>Lawrence</last><affiliation>University of Dundee</affiliation></author>
      <pages>7421-7435</pages>
      <abstract>Argumentation scheme mining is the task of automatically identifying reasoning mechanisms behind argument inferences. These mechanisms provide insights into underlying argument structures and guide the assessment of natural language arguments. Research on argumentation scheme mining, however, has always been limited by the scarcity of large enough publicly available corpora containing scheme annotations. In this paper, we present the first state-of-the-art results for mining argumentation schemes in natural language dialogue. For this purpose, we create QT-Schemes, a new corpus of 441 arguments annotated with 24 argumentation schemes. Using this corpus, we leverage the capabilities of LLMs and Transformer-based models, pre-training them on a large corpus containing textbook-like argumentation schemes and validating their applicability in real-world scenarios.</abstract>
      <url hash="2006dc22">2025.acl-long.368</url>
      <bibkey>ruiz-dolz-etal-2025-mining</bibkey>
    </paper>
    <paper id="369">
      <title><fixed-case>OS</fixed-case> Agents: A Survey on <fixed-case>MLLM</fixed-case>-based Agents for Computer, Phone and Browser Use</title>
      <author><first>Xueyu</first><last>Hu</last></author>
      <author><first>Tao</first><last>Xiong</last></author>
      <author><first>Biao</first><last>Yi</last></author>
      <author><first>Zishu</first><last>Wei</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Ruixuan</first><last>Xiao</last></author>
      <author><first>Yurun</first><last>Chen</last></author>
      <author><first>Jiasheng</first><last>Ye</last></author>
      <author><first>Meiling</first><last>Tao</last><affiliation>Guangdong University of Technology</affiliation></author>
      <author><first>Xiangxin</first><last>Zhou</last></author>
      <author><first>Ziyu</first><last>Zhao</last></author>
      <author><first>Yuhuai</first><last>Li</last></author>
      <author><first>Shengze</first><last>Xu</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Shenzhi</first><last>Wang</last></author>
      <author><first>Xinchen</first><last>Xu</last></author>
      <author><first>Shuofei</first><last>Qiao</last></author>
      <author><first>Zhaokai</first><last>Wang</last><affiliation>Shanghai AI Laboratory and Shanghai Jiao Tong University</affiliation></author>
      <author><first>Kun</first><last>Kuang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Tieyong</first><last>Zeng</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Liang</first><last>Wang</last><affiliation>Institute of Automation, CAS,China</affiliation></author>
      <author><first>Jiwei</first><last>Li</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Yuchen Eleanor</first><last>Jiang</last><affiliation>AIWaves Inc.</affiliation></author>
      <author><first>Wangchunshu</first><last>Zhou</last><affiliation>Guangdong OPPO Mobile Telecommunications Corp.,Ltd.</affiliation></author>
      <author><first>Guoyin</first><last>Wang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Keting</first><last>Yin</last></author>
      <author><first>Zhou</first><last>Zhao</last><affiliation>Zhejiang University and Zhejiang University</affiliation></author>
      <author><first>Hongxia</first><last>Yang</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Fan</first><last>Wu</last></author>
      <author><first>Shengyu</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Fei</first><last>Wu</last><affiliation>Zhejiang University</affiliation></author>
      <pages>7436-7465</pages>
      <abstract>The dream to create AI assistants as capable and versatile as the fictional J.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolution of multi-modal large language models ((M)LLMs), this dream is closer to reality, as (M)LLM-based Agents using computers, mobile phones and web browsers by operating within the environments and interfaces (e.g., Graphical User Interface (GUI) and Command Line Interface (CLI)) provided by operating systems (OS) to automate tasks have significantly advanced. This paper presents a comprehensive survey on these advanced agents, designated as OS Agents. We begin by elucidating the fundamentals of OS Agents, exploring their key components and capabilities. We then examine methodologies for constructing OS Agents, focusing on domain-specific foundation models and agent frameworks. A detailed review of evaluation metrics and benchmarks highlights how OS Agents are assessed across diverse platforms and tasks. Finally, we discuss current challenges and identify promising directions for future research. An open-source GitHub repository is maintained as a dynamic resource to foster further innovation in this field.</abstract>
      <url hash="d3ceedd3">2025.acl-long.369</url>
      <bibkey>hu-etal-2025-os</bibkey>
    </paper>
    <paper id="370">
      <title>Data Quality Issues in Multilingual Speech Datasets: The Need for Sociolinguistic Awareness and Proactive Language Planning</title>
      <author><first>Mingfei</first><last>Lau</last><affiliation>Google</affiliation></author>
      <author><first>Qian</first><last>Chen</last></author>
      <author><first>Yeming</first><last>Fang</last><affiliation>University of Pennsylvania, University of Pennsylvania</affiliation></author>
      <author><first>Tingting</first><last>Xu</last><affiliation>Google</affiliation></author>
      <author><first>Tongzhou</first><last>Chen</last><affiliation>Google</affiliation></author>
      <author><first>Pavel</first><last>Golik</last><affiliation>Google</affiliation></author>
      <pages>7466-7492</pages>
      <abstract>Our quality audit for three widely used public multilingual speech datasets Mozilla Common Voice 17.0, FLEURS, and VoxPopuli shows that in some languages, these datasets suffer from significant quality issues. We believe addressing these issues will make these datasets more useful as evaluation sets, and improve downstream models. We divide these quality issues into two categories: micro-level and macro-level. We find that macro-level issues are more prevalent in less institutionalized, often under-resourced languages. We provide a case analysis of Taiwanese Southern Min (nan_tw) that highlights the need for proactive language planning (e.g. orthography prescriptions, dialect boundary definition) and enhanced data quality control in the process of Automatic Speech Recognition (ASR) dataset creation. We conclude by proposing guidelines and recommendations to mitigate these issues in future dataset development, emphasizing the importance of sociolinguistic awareness in creating robust and reliable speech data resources.</abstract>
      <url hash="44df92f0">2025.acl-long.370</url>
      <bibkey>lau-etal-2025-data</bibkey>
    </paper>
    <paper id="371">
      <title><fixed-case>LLM</fixed-case> as a Broken Telephone: Iterative Generation Distorts Information</title>
      <author><first>Amr</first><last>Mohamed</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Mingmeng</first><last>Geng</last><affiliation>Ecole Normale Supérieure – PSL</affiliation></author>
      <author><first>Michalis</first><last>Vazirgiannis</last><affiliation>Ecole Polytechnique, France</affiliation></author>
      <author><first>Guokan</first><last>Shang</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>7493-7509</pages>
      <abstract>As large language models are increasingly responsible for online content, concerns arise about the impact of repeatedly processing their own outputs.Inspired by the “broken telephone” effect in chained human communication, this study investigates whether LLMs similarly distort information through iterative generation.Through translation-based experiments, we find that distortion accumulates over time, influenced by language choice and chain complexity. While degradation is inevitable, it can be mitigated through strategic prompting techniques. These findings contribute to discussions on the long-term effects of AI-mediated information propagation, raising important questions about the reliability of LLM-generated content in iterative workflows.</abstract>
      <url hash="d9496c05">2025.acl-long.371</url>
      <bibkey>mohamed-etal-2025-llm</bibkey>
    </paper>
    <paper id="372">
      <title><fixed-case>VLM</fixed-case>2-Bench: A Closer Look at How Well <fixed-case>VLM</fixed-case>s Implicitly Link Explicit Matching Visual Cues</title>
      <author><first>Jianshu</first><last>Zhang</last></author>
      <author><first>Dongyu</first><last>Yao</last></author>
      <author><first>Renjie</first><last>Pi</last></author>
      <author><first>Paul Pu</first><last>Liang</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Yi R.</first><last>Fung</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <pages>7510-7545</pages>
      <abstract>Visually linking matching cues is a crucial ability in daily life, such as identifying the same person in multiple photos based on their cues, even without knowing who they are. Despite the extensive knowledge that vision-language models (VLMs) possess, it remains largely unexplored whether they are capable of performing this fundamental task. To address this, we introduce VLM2-Bench, a benchmark designed to assess whether VLMs can Visually Link Matching cues, with 9 subtasks and over 3,000 test cases. Comprehensive evaluation across twelve VLMs, along with further analysis of various language-side and vision-side prompting methods, leads to a total of eight key findings. We identify critical challenges in models’ ability to link visual cues, highlighting a significant performance gap. Based on these insights, we advocate for (i) enhancing core visual capabilities to improve adaptability and reduce reliance on prior knowledge, (ii) establishing clearer principles for integrating language-based reasoning in vision-centric tasks to prevent unnecessary biases, and (iii) shifting vision-text training paradigms toward fostering models’ ability to independently structure and infer relationships among visual cues.</abstract>
      <url hash="14daaa52">2025.acl-long.372</url>
      <bibkey>zhang-etal-2025-vlm2</bibkey>
    </paper>
    <paper id="373">
      <title>Alleviating Distribution Shift in Synthetic Data for Machine Translation Quality Estimation</title>
      <author><first>Xiang</first><last>Geng</last></author>
      <author><first>Zhejian</first><last>Lai</last></author>
      <author><first>Jiajun</first><last>Chen</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Hao</first><last>Yang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Shujian</first><last>Huang</last><affiliation>Nanjing University</affiliation></author>
      <pages>7546-7560</pages>
      <abstract>Quality Estimation (QE) models evaluate the quality of machine translations without reference translations, serving as the reward models for the translation task.Due to the data scarcity, synthetic data generation has emerged as a promising solution.However, synthetic QE data often suffers from distribution shift, which can manifest as discrepancies between pseudo and real translations, or in pseudo labels that do not align with human preferences.To tackle this issue, we introduce DCSQE, a novel framework for alleviating distribution shift in synthetic QE data.To reduce the difference between pseudo and real translations, we employ the constrained beam search algorithm and enhance translation diversity through the use of distinct generation models.DCSQE uses references—i.e., translation supervision signals—to guide both the generation and annotation processes, enhancing the quality of token-level labels.DCSQE further identifies the shortest phrase covering consecutive error tokens, mimicking human annotation behavior, to assign the final phrase-level labels.Specially, we underscore that the translation model can not annotate translations of itself accurately.Extensive experiments demonstrate that DCSQE outperforms SOTA baselines like CometKiwi in both supervised and unsupervised settings.Further analysis offers insights into synthetic data generation that could benefit reward models for other tasks.The code is available at https://github.com/NJUNLP/njuqe.</abstract>
      <url hash="d846952a">2025.acl-long.373</url>
      <bibkey>geng-etal-2025-alleviating</bibkey>
    </paper>
    <paper id="374">
      <title>Evaluation Agent: Efficient and Promptable Evaluation Framework for Visual Generative Models</title>
      <author><first>Fan</first><last>Zhang</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Shulin</first><last>Tian</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Ziqi</first><last>Huang</last></author>
      <author><first>Yu</first><last>Qiao</last><affiliation>Shanghai Aritifcal Intelligence Laboratory</affiliation></author>
      <author><first>Ziwei</first><last>Liu</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>7561-7582</pages>
      <abstract>Recent advancements in visual generative models have enabled high-quality image and video generation, opening diverse applications. However, evaluating these models often demands sampling hundreds or thousands of images or videos, making the process computationally expensive, especially for diffusion-based models with inherently slow sampling. Moreover, existing evaluation methods rely on rigid pipelines that overlook specific user needs and provide numerical results without clear explanations. In contrast, humans can quickly form impressions of a model’s capabilities by observing only a few samples. To mimic this, we propose the Evaluation Agent framework, which employs human-like strategies for efficient, dynamic, multi-round evaluations using only a few samples per round, while offering detailed, user-tailored analyses. It offers four key advantages: 1) efficiency, 2) promptable evaluation tailored to diverse user needs, 3) explainability beyond single numerical scores, and 4) scalability across various models and tools. Experiments show that Evaluation Agent reduces evaluation time to 10% of traditional methods while delivering comparable results. The Evaluation Agent framework is fully open-sourced to advance research in visual generative models and their efficient evaluation.</abstract>
      <url hash="dca96a83">2025.acl-long.374</url>
      <bibkey>zhang-etal-2025-evaluation</bibkey>
    </paper>
    <paper id="375">
      <title>Large Language Models Struggle to Describe the Haystack without Human Help: A Social Science-Inspired Evaluation of Topic Models</title>
      <author><first>Zongxia</first><last>Li</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Lorena</first><last>Calvo-Bartolomé</last><affiliation>Universidad Carlos III de Madrid</affiliation></author>
      <author><first>Alexander Miserlis</first><last>Hoyle</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <author><first>Paiheng</first><last>Xu</last><affiliation>Department of Computer Science, University of Maryland, College Park</affiliation></author>
      <author><first>Daniel Kofi</first><last>Stephens</last></author>
      <author><first>Juan Francisco</first><last>Fung</last><affiliation>National Institute of Standards and Technology</affiliation></author>
      <author><first>Alden</first><last>Dima</last><affiliation>National Institute of Standards and Technology</affiliation></author>
      <author><first>Jordan Lee</first><last>Boyd-Graber</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>7583-7604</pages>
      <abstract>A common use of NLP is to facilitate the understanding of large document collections, with models based on Large Language Models (LLMs) replacing probabilistic topic models. Yet the effectiveness of LLM-based approaches in real-world applications remains under explored. This study measures the knowledge users acquire with topic models—including traditional, unsupervised and supervised LLM- based approaches—on two datasets. While LLM-based methods generate more human- readable topics and show higher average win probabilities than traditional models for data exploration, they produce overly generic topics for domain-specific datasets that do not easily allow users to learn much about the documents. Adding human supervision to LLM-based topic models improves data exploration by addressing hallucination and genericity but requires more human efforts. In contrast, traditional models like Latent Dirichlet Allocation (LDA) remain effective for exploration but are less user-friendly. This paper provides best practices—there is no one right model, the choice of models is situation-specific—and suggests potential improvements for scalable LLM- based topic models.</abstract>
      <url hash="c5140206">2025.acl-long.375</url>
      <bibkey>li-etal-2025-large-language</bibkey>
    </paper>
    <paper id="376">
      <title><fixed-case>A</fixed-case>cti<fixed-case>V</fixed-case>iew: Evaluating Active Perception Ability for Multimodal Large Language Models</title>
      <author><first>Ziyue</first><last>Wang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Chi</first><last>Chen</last></author>
      <author><first>Fuwen</first><last>Luo</last></author>
      <author><first>Yurui</first><last>Dong</last></author>
      <author><first>Yuanchi</first><last>Zhang</last></author>
      <author><first>Yuzhuang</first><last>Xu</last></author>
      <author><first>Xiaolong</first><last>Wang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Peng</first><last>Li</last><affiliation>Tsinghua University</affiliation></author>
      <author id="yang-liu"><first>Yang</first><last>Liu</last></author>
      <pages>7605-7633</pages>
      <abstract>Active perception, a crucial human capability, involves setting a goal based on the current understanding of the environment and performing actions to achieve that goal. Despite significant efforts in evaluating Multimodal Large Language Models (MLLMs), active perception has been largely overlooked. To address this gap, we propose a novel benchmark named ActiView to evaluate active perception in MLLMs. We focus on a specialized form of Visual Question Answering (VQA) that eases and quantifies the evaluation yet challenging for existing MLLMs. Meanwhile, intermediate reasoning behaviors of models are also discussed. Given an image, we restrict the perceptual field of a model, requiring it to actively zoom or shift its perceptual field based on reasoning to answer the question successfully. We conduct extensive evaluation over 30 models, including proprietary and open-source models, and observe that restricted perceptual fields play a significant role in enabling active perception. Results reveal a significant gap in the active perception capability of MLLMs, indicating that this area deserves more attention. We hope that ActiView could help develop methods for MLLMs to understand multimodal inputs in more natural and holistic ways.</abstract>
      <url hash="d6c0a6c2">2025.acl-long.376</url>
      <bibkey>wang-etal-2025-actiview</bibkey>
    </paper>
    <paper id="377">
      <title>Enough Coin Flips Can Make <fixed-case>LLM</fixed-case>s Act <fixed-case>B</fixed-case>ayesian</title>
      <author><first>Ritwik</first><last>Gupta</last><affiliation>Defense Innovation Unit and University of California, Berkeley</affiliation></author>
      <author><first>Rodolfo</first><last>Corona</last></author>
      <author><first>Jiaxin</first><last>Ge</last><affiliation>University of California, Berkeley</affiliation></author>
      <author><first>Eric</first><last>Wang</last></author>
      <author><first>Dan</first><last>Klein</last><affiliation>University of California, Berkeley</affiliation></author>
      <author><first>Trevor</first><last>Darrell</last><affiliation>Electrical Engineering &amp; Computer Science Department</affiliation></author>
      <author><first>David M.</first><last>Chan</last><affiliation>University of California, Berkeley</affiliation></author>
      <pages>7634-7655</pages>
      <abstract>Large language models (LLMs) exhibit the ability to generalize given few-shot examples in their input prompt, an emergent capability known as in-context learning (ICL). We investigate whether LLMs use ICL to perform structured reasoning in ways that are consistent with a Bayesian framework or rely on pattern matching. Using a controlled setting of biased coin flips, we find that: (1) LLMs often possess biased priors, causing initial divergence in zero-shot settings, (2) in-context evidence outweighs explicit bias instructions, (3) LLMs broadly follow Bayesian posterior updates, with deviations primarily due to miscalibrated priors rather than flawed updates, and (4) attention magnitude has negligible effect on Bayesian inference. With sufficient demonstrations of biased coin flips via ICL, LLMs update their priors in a Bayesian manner. Code and visualizations are available on the [project page](https://ai-climate.berkeley.edu/llm-coin-flips/).</abstract>
      <url hash="f65b1041">2025.acl-long.377</url>
      <bibkey>gupta-etal-2025-enough</bibkey>
    </paper>
    <paper id="378">
      <title><fixed-case>GAMEB</fixed-case>o<fixed-case>T</fixed-case>: Transparent Assessment of <fixed-case>LLM</fixed-case> Reasoning in Games</title>
      <author><first>Wenye</first><last>Lin</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Jonathan</first><last>Roberts</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Yunhan</first><last>Yang</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Samuel</first><last>Albanie</last><affiliation>Google</affiliation></author>
      <author><first>Zongqing</first><last>Lu</last><affiliation>Peking University</affiliation></author>
      <author><first>Kai</first><last>Han</last><affiliation>The University of Hong Kong</affiliation></author>
      <pages>7656-7682</pages>
      <abstract>Large Language Models (LLMs) are increasingly deployed in real-world applications that demand complex reasoning. To track progress, robust benchmarks are required to evaluate their capabilities beyond superficial pattern recognition. However, current LLM reasoning benchmarks often face challenges such as insufficient interpretability, performance saturation or data contamination. To address these challenges, we introduce GAMEBoT, a gaming arena designed for rigorous and transparent assessment of LLM reasoning capabilities. GAMEBoT decompose complex reasoning in games into predefined modular subproblems. This decomposition allows us to design a suite of Chain-of-Thought (CoT) prompts infused with domain knowledge to guide LLMs in addressing these subproblems before action selection. Furthermore, we develop a suite of rule-based algorithms to generate ground truth for these subproblems, enabling rigorous validation of the LLMs’ intermediate reasoning steps. This approach facilitates evaluation of both the quality of final actions and the accuracy of the underlying reasoning process. GAMEBoT also naturally alleviates the risk of data contamination through dynamic games and head-to-head LLM competitions. We benchmark 17 prominent LLMs across eight games, encompassing various strategic abilities and game characteristics. Our results suggest that GAMEBoT presents a significant challenge, even when LLMs are provided with detailed CoT prompts.</abstract>
      <url hash="e98c3fab">2025.acl-long.378</url>
      <bibkey>lin-etal-2025-gamebot</bibkey>
    </paper>
    <paper id="379">
      <title>A Text is Worth Several Tokens: Text Embedding from <fixed-case>LLM</fixed-case>s Secretly Aligns Well with The Key Tokens</title>
      <author><first>Zhijie</first><last>Nie</last><affiliation>Beihang University</affiliation></author>
      <author><first>Richong</first><last>Zhang</last></author>
      <author><first>Zhanyu</first><last>Wu</last></author>
      <pages>7683-7694</pages>
      <abstract>Text embeddings from large language models (LLMs) have achieved excellent results in tasks such as information retrieval, semantic textual similarity, etc. In this work, we show an interesting finding: when feeding a text into the LLM-based embedder, the obtained text embedding will be able to be aligned with the key tokens in the input text. We first fully analyze this phenomenon on eight LLM-based embedders and show that this phenomenon is universal and is not affected by model architecture, training strategy, and embedding method. With a deeper analysis, we find that the main change in embedding space between these embedders and their LLM backbones is in the first principal component. By adjusting the first principal component, we can align text embedding with the key tokens. Finally, we give several examples to demonstrate the vast application potential of this finding: (1) we propose a simple and practical sparse retrieval method based on the aligned tokens, which can achieve 80% of the dense retrieval effect of the same model while reducing the computation significantly; (2) we show that our findings provide a novel perspective to help understand novel technologies (e.g., instruction-following embedding) and fuzzy concepts (e.g., semantic relatedness vs. similarity) in this field.</abstract>
      <url hash="b5197ce2">2025.acl-long.379</url>
      <bibkey>nie-etal-2025-text</bibkey>
    </paper>
    <paper id="380">
      <title>Commonsense Reasoning in <fixed-case>A</fixed-case>rab Culture</title>
      <author><first>Abdelrahman</first><last>Sadallah</last></author>
      <author><first>Junior Cedric</first><last>Tonga</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Khalid</first><last>Almubarak</last><affiliation>National Center for AI (NCAI), Saudi Data and AI Authority (SDAIA)</affiliation></author>
      <author><first>Saeed</first><last>Almheiri</last></author>
      <author><first>Farah</first><last>Atif</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Chatrine</first><last>Qwaider</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Chalmers University of Technology</affiliation></author>
      <author><first>Karima</first><last>Kadaoui</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Sara</first><last>Shatnawi</last></author>
      <author><first>Yaser</first><last>Alesh</last></author>
      <author><first>Fajri</first><last>Koto</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>7695-7710</pages>
      <abstract>Despite progress in Arabic large language models, such as Jais and AceGPT, their evaluation on commonsense reasoning has largely relied on machine-translated datasets, which lack cultural depth and may introduce Anglocentric biases. Commonsense reasoning is shaped by geographical and cultural contexts, and existing English datasets fail to capture the diversity of the Arab world. To address this, we introduce , a commonsense reasoning dataset in Modern Standard Arabic (MSA), covering cultures of 13 countries across the Gulf, Levant, North Africa, and the Nile Valley. The dataset was built from scratch by engaging native speakers to write and validate culturally relevant questions for their respective countries. spans 12 daily life domains with 54 fine-grained subtopics, reflecting various aspects of social norms, traditions, and everyday experiences. Zero-shot evaluations show that open-weight language models with up to 32B parameters struggle to comprehend diverse Arab cultures, with performance varying across regions. These findings highlight the need for more culturally aware models and datasets tailored to the Arabic-speaking world.</abstract>
      <url hash="4d287f81">2025.acl-long.380</url>
      <bibkey>sadallah-etal-2025-commonsense</bibkey>
    </paper>
    <paper id="381">
      <title><fixed-case>AXIS</fixed-case>: Efficient Human-Agent-Computer Interaction with <fixed-case>API</fixed-case>-First <fixed-case>LLM</fixed-case>-Based Agents</title>
      <author><first>Junting</first><last>Lu</last></author>
      <author><first>Zhiyang</first><last>Zhang</last></author>
      <author><first>Fangkai</first><last>Yang</last></author>
      <author><first>Jue</first><last>Zhang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Lu</first><last>Wang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Chao</first><last>Du</last><affiliation>Microsoft</affiliation></author>
      <author><first>Qingwei</first><last>Lin</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Saravan</first><last>Rajmohan</last><affiliation>Microsoft</affiliation></author>
      <author><first>Dongmei</first><last>Zhang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Qi</first><last>Zhang</last><affiliation>Microsoft</affiliation></author>
      <pages>7711-7743</pages>
      <abstract>Multimodal large language models (MLLMs) have enabled LLM-based agents to directly interact with application user interfaces (UIs), enhancing agents’ performance in complex tasks. However, these agents often suffer from high latency and low reliability due to the extensive sequential UI interactions. To address this issue, we propose AXIS, a novel LLM-based agents framework that prioritize actions through application programming interfaces (APIs) over UI actions. This framework also facilitates the creation and expansion of APIs through automated exploration of applications. Our experiments on Microsoft Word demonstrate that AXIS reduces task completion time by 65%-70% and cognitive workload by 38%-53%, while maintaining accuracy of 97%-98% compared to humans. Our work contributes to a new human-agent-computer interaction (HACI) framework and explores a fresh UI design principle for application providers to turn applications into agents in the era of LLMs, paving the way towards an agent-centric operating system (Agent OS). The code and dataset will be available at https://aka.ms/haci_axis.</abstract>
      <url hash="da4acd25">2025.acl-long.381</url>
      <bibkey>lu-etal-2025-axis</bibkey>
    </paper>
    <paper id="382">
      <title>Translation and Fusion Improves Cross-lingual Information Extraction</title>
      <author><first>Yang</first><last>Chen</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Vedaant</first><last>Shah</last></author>
      <author><first>Alan</first><last>Ritter</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <pages>7744-7764</pages>
      <abstract>Large language models (LLMs) combined with instruction tuning have shown significant progress in information extraction (IE) tasks, exhibiting strong generalization capabilities to unseen datasets by following annotation guidelines. However, their applicability to low-resource languages remains limited due to lack of both labeled data for fine-tuning, and unlabeled text for pre-training. In this paper, we propose TransFusion, a framework in which models are fine-tuned to use English translations of low-resource language data, enabling more precise predictions through annotation fusion. Based on TransFusion, we introduce GoLLIE-TF, a cross-lingual instruction-tuned LLM for IE tasks, designed to close the performance gap between high and low-resource languages. Our experiments across twelve multilingual IE datasets spanning 50 languages demonstrate that GoLLIE-TF achieves better cross-lingual transfer over the base model. In addition, we show that TransFusion significantly improves low-resource language named entity recognition when applied to proprietary models such as GPT-4 (+5 F1) with a prompting approach, or fine-tuning different language models including decoder-only (+14 F1) and encoder-only (+13 F1) architectures.</abstract>
      <url hash="13e11f84">2025.acl-long.382</url>
      <bibkey>chen-etal-2025-translation</bibkey>
    </paper>
    <paper id="383">
      <title>Conditional Dichotomy Quantification via Geometric Embedding</title>
      <author><first>Shaobo</first><last>Cui</last></author>
      <author><first>Wenqing</first><last>Liu</last></author>
      <author><first>Yiyang</first><last>Feng</last></author>
      <author><first>Jiawei</first><last>Zhou</last><affiliation>State University of New York at Stony Brook</affiliation></author>
      <author><first>Boi</first><last>Faltings</last></author>
      <pages>7765-7791</pages>
      <abstract>Conditional dichotomy, the contrast between two outputs conditioned on the same context, is vital for applications such as debate, defeasible inference, and causal reasoning. Existing methods that rely on semantic similarity often fail to capture the nuanced oppositional dynamics essential for these applications. Motivated by these limitations, we introduce a novel task, Conditional Dichotomy Quantification (ConDQ), which formalizes the direct measurement of conditional dichotomy and provides carefully constructed datasets covering debate, defeasible natural language inference, and causal reasoning scenarios. To address this task, we develop the Dichotomy-oriented Geometric Embedding (DoGE) framework, which leverages complex-valued embeddings and a dichotomous objective to model and quantify these oppositional relationships effectively. Extensive experiments validate the effectiveness and versatility of DoGE, demonstrating its potential in understanding and quantifying conditional dichotomy across diverse NLP applications. Our code and datasets are available at https://github.com/cui-shaobo/conditional-dichotomy-quantification.</abstract>
      <url hash="6ad0b2a2">2025.acl-long.383</url>
      <bibkey>cui-etal-2025-conditional</bibkey>
    </paper>
    <paper id="384">
      <title>Aligning Large Language Models with Implicit Preferences from User-Generated Content</title>
      <author><first>Zhaoxuan</first><last>Tan</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Zheng</first><last>Li</last><affiliation>Amazon</affiliation></author>
      <author><first>Tianyi</first><last>Liu</last><affiliation>Amazon</affiliation></author>
      <author><first>Haodong</first><last>Wang</last><affiliation>Amazon</affiliation></author>
      <author><first>Hyokun</first><last>Yun</last><affiliation>Amazon</affiliation></author>
      <author><first>Ming</first><last>Zeng</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Pei</first><last>Chen</last><affiliation>Texas A&amp;M University - College Station</affiliation></author>
      <author><first>Zhihan</first><last>Zhang</last></author>
      <author><first>Yifan</first><last>Gao</last><affiliation>Amazon</affiliation></author>
      <author><first>Ruijie</first><last>Wang</last></author>
      <author><first>Priyanka</first><last>Nigam</last></author>
      <author><first>Bing</first><last>Yin</last><affiliation>Amazon</affiliation></author>
      <author><first>Meng</first><last>Jiang</last><affiliation>University of Notre Dame</affiliation></author>
      <pages>7792-7820</pages>
      <abstract>Learning from preference feedback is essential for aligning large language models (LLMs) with human values and improving the quality of generated responses. However, existing preference learning methods rely heavily on curated data from humans or advanced LLMs, which is costly and difficult to scale. In this work, we present PUGC, a novel framework that leverages implicit human Preferences in unlabeled User-Generated Content (UGC) to generate preference data. Although UGC is not explicitly created to guide LLMs in generating human-preferred responses, it often reflects valuable insights and implicit preferences from its creators that has the potential to address readers’ questions. PUGC transforms UGC into user queries and generates responses from the policy model. The UGC is then leveraged as a reference text for response scoring, aligning the model with these implicit preferences. This approach improves the quality of preference data while enabling scalable, domain-specific alignment. Experimental results on Alpaca Eval 2 show that models trained with DPO and PUGC achieve a 9.37% performance improvement over traditional methods, setting a 35.93% state-of-the-art length-controlled win rate using Mistral-7B-Instruct. Further studies highlight gains in reward quality, domain-specific alignment effectiveness, robustness against UGC quality, and theory of mind capabilities. Our code and dataset are available at https://zhaoxuan.info/PUGC.github.io/.</abstract>
      <url hash="6fd399ee">2025.acl-long.384</url>
      <bibkey>tan-etal-2025-aligning</bibkey>
    </paper>
    <paper id="385">
      <title><fixed-case>VQAG</fixed-case>uider: Guiding Multimodal Large Language Models to Answer Complex Video Questions</title>
      <author><first>Yuyan</first><last>Chen</last></author>
      <author><first>Jiyuan</first><last>Jia</last><affiliation>southern university of science and technology</affiliation></author>
      <author><first>Jiaxin</first><last>Lu</last></author>
      <author><first>Siyue</first><last>Li</last></author>
      <author><first>Yu</first><last>Guan</last><affiliation>University of Warwick</affiliation></author>
      <author><first>Ming</first><last>Yang</last><affiliation>Ant Group</affiliation></author>
      <author><first>Qingpei</first><last>Guo</last><affiliation>Ant Group</affiliation></author>
      <pages>7821-7834</pages>
      <abstract>Complex video question-answering (VQA) requires in-depth understanding of video contents including object and action recognition as well as video classification and summarization, which exhibits great potential in emerging applications in education and entertainment, etc. Multimodal large language models (MLLMs) may accomplish this task by grasping the intention of a question and decomposing it to a series of visual recognition sub-tasks to find out the answer with the help of an agent. To tackle this task, we first collect a new dedicated Complex VQA dataset named CVQA and then propose VQAGuider, an innovative framework planning a few atomic visual recognition tools by video-related API matching. VQAGuider facilitates a deep engagement with video content and precise responses to complex video-related questions by MLLMs, which is beyond aligning visual and language features for simple VQA tasks. Our experiments demonstrate VQAGuider is capable of navigating the complex VQA tasks by MLLMs and improves the accuracy by 29.6% and 17.2% on CVQA and the existing VQA datasets, respectively, highlighting its potential in advancing MLLMs’s capabilities in video understanding.</abstract>
      <url hash="6a9ea324">2025.acl-long.385</url>
      <bibkey>chen-etal-2025-vqaguider</bibkey>
    </paper>
    <paper id="386">
      <title>Large Language Models are Good Relational Learners</title>
      <author><first>Fang</first><last>Wu</last></author>
      <author><first>Vijay Prakash</first><last>Dwivedi</last><affiliation>Computer Science Department, Stanford University</affiliation></author>
      <author><first>Jure</first><last>Leskovec</last><affiliation>Stanford University and Kumo.AI</affiliation></author>
      <pages>7835-7854</pages>
      <abstract>Large language models (LLMs) have demonstrated remarkable capabilities across various domains, yet their application to relational deep learning (RDL) remains underexplored. Existing approaches adapt LLMs by traversing relational links between entities in a database and converting the structured data into flat text documents, but this text-based serialization disregards critical relational structures, introduces redundancy, and often exceeds standard LLM context lengths. We introduce Rel-LLM, a novel architecture that employs a graph neural network (GNN) based encoder to create structured relational prompts for LLMs within a retrieval-augmented generation (RAG) framework. Unlike traditional text-based serialization approaches, our method preserves the inherent relational structure of databases while enabling LLMs to effectively process and reason over complex entity relationships. Specifically, the GNN encoder extracts a local subgraph around an entity to build feature representations that contain relevant entity relationships and temporal dependencies. These representations are transformed into structured prompts using a denormalization process, effectively allowing the LLM to reason over relational structures. Through extensive experiments, we demonstrate that Rel-LLM outperforms existing methods on key RDL tasks, offering a scalable and efficient approach to integrating LLMs with structured data sources. Code is available at <url>https://github.com/smiles724/Rel-LLM</url>.</abstract>
      <url hash="bbcee5c0">2025.acl-long.386</url>
      <bibkey>wu-etal-2025-large</bibkey>
    </paper>
    <paper id="387">
      <title><fixed-case>S</fixed-case>pa<fixed-case>RE</fixed-case>: Enhancing Spatial Reasoning in Vision-Language Models with Synthetic Data</title>
      <author><first>Michael</first><last>Ogezi</last></author>
      <author><first>Freda</first><last>Shi</last><affiliation>University of Waterloo and Vector Institute</affiliation></author>
      <pages>7855-7875</pages>
      <abstract>Vision-language models (VLMs) work well in tasks ranging from image captioning to visual question answering (VQA), yet they struggle with spatial reasoning, a key skill for understanding our physical world that humans excel at. We find that spatial relations are generally rare in widely used VL datasets, with only a few being well represented, while most form a long tail of underrepresented relations. This gap leaves VLMs ill-equipped to handle diverse spatial relationships. To bridge it, we construct a synthetic VQA dataset focused on spatial reasoning generated from hyper-detailed image descriptions in Localized Narratives, DOCCI, and PixMo-Cap. Our dataset consists of 455k samples containing 3.4 million QA pairs. Trained on this dataset, our Spatial-Reasoning Enhanced (SpaRE) VLMs show strong improvements on spatial reasoning benchmarks, achieving up to a 49% performance gain on the What’s Up benchmark, while maintaining strong results on general tasks. Our work narrows the gap between human and VLM spatial reasoning and makes VLMs more capable in real-world tasks such as robotics and navigation. We plan to share our code and dataset in due course.</abstract>
      <url hash="17c687c2">2025.acl-long.387</url>
      <bibkey>ogezi-shi-2025-spare</bibkey>
    </paper>
    <paper id="388">
      <title>Distilling an End-to-End Voice Assistant Without Instruction Training Data</title>
      <author><first>William Barr</first><last>Held</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Yanzhe</first><last>Zhang</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Weiyan</first><last>Shi</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Minzhi</first><last>Li</last></author>
      <author><first>Michael J</first><last>Ryan</last><affiliation>Stanford University</affiliation></author>
      <author><first>Diyi</first><last>Yang</last><affiliation>Stanford University</affiliation></author>
      <pages>7876-7891</pages>
      <abstract>Voice assistants, such as Siri and Google Assistant, typically model audio and text separately, resulting in lost speech information and increased complexity. Recent efforts to address this with end-to-end Speech Large Language Models (speech-in, text-out) trained with supervised finetuning (SFT) have led to models “forgetting” capabilities from text-only LLMs. Our work proposes an alternative paradigm for training Speech LLMs without instruction data, using the response of a text-only LLM to transcripts as self-supervision. Importantly, this process can be performed without annotated responses. We show that our Distilled Voice Assistant (DiVA) generalizes to Spoken Question Answering, Classification, and Translation. Furthermore, DiVA better matches user preferences, achieving a 72% win rate compared with state-of-the-art models like Qwen 2 Audio, despite using <tex-math>&gt;</tex-math>100x less training compute.</abstract>
      <url hash="ff656a07">2025.acl-long.388</url>
      <bibkey>held-etal-2025-distilling</bibkey>
    </paper>
    <paper id="389">
      <title><fixed-case>C</fixed-case>o<fixed-case>M</fixed-case>et: Metaphor-Driven Covert Communication for Multi-Agent Language Games</title>
      <author><first>Shuhang</first><last>Xu</last></author>
      <author><first>Fangwei</first><last>Zhong</last><affiliation>Beijing Normal University</affiliation></author>
      <pages>7892-7917</pages>
      <abstract>Metaphors are a crucial way for humans to express complex or subtle ideas by comparing one concept to another, often from a different domain. However, many large language models (LLMs) struggle to interpret and apply metaphors in multi-agent language games, hindering their ability to engage in covert communication and semantic evasion, which are crucial for strategic communication. To address this challenge, we introduce CoMet, a framework that enables LLM-based agents to engage in metaphor processing. CoMet combines a hypothesis-based metaphor reasoner with a metaphor generator that improves through self-reflection and knowledge integration. This enhances the agents’ ability to interpret and apply metaphors, improving the strategic and nuanced quality of their interactions. We evaluate CoMet on two multi-agent language games—Undercover and Adversarial Taboo—which emphasize “covert communication” and “semantic evasion”. Experimental results demonstrate that CoMet significantly enhances the agents’ ability to communicate strategically using metaphors.</abstract>
      <url hash="44ba76e9">2025.acl-long.389</url>
      <bibkey>xu-zhong-2025-comet</bibkey>
    </paper>
    <paper id="390">
      <title><fixed-case>CER</fixed-case>: Confidence Enhanced Reasoning in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Ali</first><last>Razghandi</last><affiliation>Sharif University of Technology, Sharif University of Technology</affiliation></author>
      <author><first>Seyed Mohammad Hadi</first><last>Hosseini</last><affiliation>Sharif University of Technology</affiliation></author>
      <author><first>Mahdieh Soleymani</first><last>Baghshah</last></author>
      <pages>7918-7938</pages>
      <abstract>Ensuring the reliability of Large Language Models (LLMs) in complex reasoning tasks remains a formidable challenge, particularly in scenarios that demand precise mathematical calculations and knowledge-intensive open-domain generation. In this work, we introduce an uncertainty-aware framework designed to enhance the accuracy of LLM responses by systematically incorporating model confidence at critical decision points. We propose an approach that encourages multi-step reasoning in LLMs and quantify the confidence of intermediate answers such as numerical results in mathematical reasoning and proper nouns in open-domain generation. Then, the overall confidence of each reasoning chain is evaluated based on confidence of these critical intermediate steps. Finally, we aggregate the answer of generated response paths in a way that reflects the reliability of each generated content (as opposed to self-consistency in which each generated chain contributes equally to majority voting). We conducted extensive experiments in five datasets, three mathematical datasets and two open-domain datasets, using four LLMs. The results consistently validate the effectiveness of our novel confidence-aggregation method, leading to an accuracy improvement of up to 7.4% and 5.8% over baseline approaches in math and open-domain generation tasks, respectively. Code is publicly available at https://github.com/sharif-ml-lab/CER.</abstract>
      <url hash="b444c5bc">2025.acl-long.390</url>
      <bibkey>razghandi-etal-2025-cer</bibkey>
    </paper>
    <paper id="391">
      <title>Watermarking Large Language Models: An Unbiased and Low-risk Method</title>
      <author><first>Minjia</first><last>Mao</last></author>
      <author><first>Dongjun</first><last>Wei</last></author>
      <author><first>Zeyu</first><last>Chen</last><affiliation>University of Delaware</affiliation></author>
      <author><first>Xiao</first><last>Fang</last><affiliation>University of Delaware</affiliation></author>
      <author><first>Michael</first><last>Chau</last><affiliation>University of Hong Kong</affiliation></author>
      <pages>7939-7960</pages>
      <abstract>Recent advancements in large language models (LLMs) have highlighted the risk of misusing them, raising the need for accurate detection of LLM-generated content. In response, a viable solution is to inject imperceptible identifiers into LLMs, known as watermarks. Our research extends the existing watermarking methods by proposing the novel Sampling One Then Accepting (STA-1) method. STA-1 is an unbiased watermark that preserves the original token distribution in expectation and has a lower risk of producing unsatisfactory outputs in low-entropy scenarios compared to existing unbiased watermarks. In watermark detection, STA-1 does not require prompts or a white-box LLM, provides statistical guarantees, demonstrates high efficiency in detection time, and remains robust against various watermarking attacks. Experimental results on low-entropy and high-entropy datasets demonstrate that STA-1 achieves the above properties simultaneously, making it a desirable solution for watermarking LLMs. Implementation codes for this study are available online.</abstract>
      <url hash="00d939e3">2025.acl-long.391</url>
      <bibkey>mao-etal-2025-watermarking</bibkey>
    </paper>
    <paper id="392">
      <title>On Synthetic Data Strategies for Domain-Specific Generative Retrieval</title>
      <author><first>Haoyang</first><last>Wen</last></author>
      <author><first>Jiang</first><last>Guo</last><affiliation>Amazon</affiliation></author>
      <author><first>Yi</first><last>Zhang</last><affiliation>AWS AI</affiliation></author>
      <author><first>Jiarong</first><last>Jiang</last><affiliation>Amazon</affiliation></author>
      <author><first>Zhiguo</first><last>Wang</last></author>
      <pages>7961-7976</pages>
      <abstract>This paper investigates synthetic data generation strategies in developing generative retrieval models for domain-specific corpora, thereby addressing the scalability challenges inherent in manually annotating in-domain queries. We study the data strategies for a two-stage training framework: in the first stage, which focuses on learning to decode document identifiers from queries, we investigate LLM-generated queries across multiple granularity (e.g. chunks, sentences) and domain-relevant search constraints that can better capture nuanced relevancy signals. In the second stage, which aims to refine document ranking through preference learning, we explore the strategies for mining hard negatives based on the initial model’s predictions. Experiments on public datasets over diverse domains demonstrate the effectiveness of our synthetic data generation and hard negative sampling approach.</abstract>
      <url hash="ceb99b99">2025.acl-long.392</url>
      <bibkey>wen-etal-2025-synthetic</bibkey>
    </paper>
    <paper id="393">
      <title><fixed-case>LLM</fixed-case> Braces: Straightening Out <fixed-case>LLM</fixed-case> Predictions with Relevant Sub-Updates</title>
      <author><first>Ying</first><last>Shen</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Lifu</first><last>Huang</last><affiliation>University of California, Davis</affiliation></author>
      <pages>7977-7992</pages>
      <abstract>Recent findings reveal that much of the knowledge in a Transformer-based Large Language Model (LLM) is encoded in its feed-forward (FFN) layers, where each FNN layer can be interpreted as the summation of sub-updates, each corresponding to a weighted column vector from the FFN’s value parameter matrix that often encodes human-interpretable concepts. In light of this, we hypothesize that model performance and behaviors can be further enhanced and controlled by modulating the contributions of these sub-updates based on their relevance to the input or target output style, and propose LLMBraces, a novel and efficient method that computes relevance scores associated with value vectors in FFN layers and leverages these scores to dynamically adjust the contribution of sub-updates. By optimizing sub-update contributions, LLMBraces refines the prediction process, leading to more accurate and reliable outputs, much like a ‘brace’ providing support and stability. Moreover, LLMBraces can be extended to support conditional control over generation characteristics, such as sentiment, thereby offering fine-grained steering of LLM outputs. Extensive experiments on various LLMs—including Qwen2.5-1.5B, Llama2-7B, and Llama3-8B—demonstrate that LLMBraces outperforms baseline approaches in both fine-tuning and zero-shot settings while requiring significantly fewer tunable parameters, up to 75% fewer compared to LoRA. Furthermore, LLMBraces excels in sentiment-controlled generation and toxicity reduction, highlighting its potential for flexible, controlled text generation across applications.</abstract>
      <url hash="f4312d94">2025.acl-long.393</url>
      <bibkey>shen-huang-2025-llm</bibkey>
    </paper>
    <paper id="394">
      <title><fixed-case>CONFETTI</fixed-case>: Conversational Function-Calling Evaluation Through Turn-Level Interactions</title>
      <author><first>Tamer</first><last>Alkhouli</last><affiliation>Amazon</affiliation></author>
      <author><first>Katerina</first><last>Margatina</last><affiliation>Amazon</affiliation></author>
      <author><first>James</first><last>Gung</last><affiliation>AWS AI Labs</affiliation></author>
      <author><first>Raphael</first><last>Shu</last><affiliation>Amazon</affiliation></author>
      <author><first>Claudia</first><last>Zaghi</last><affiliation>Amazon</affiliation></author>
      <author><first>Monica</first><last>Sunkara</last></author>
      <author><first>Yi</first><last>Zhang</last><affiliation>Amazon</affiliation></author>
      <pages>7993-8006</pages>
      <abstract>We introduce Conversational Function-Calling Evaluation Through Turn-Level Interactions (CONFETTI), a conversational benchmark designed to evaluate the function-calling capabilities and response quality of large language models (LLMs). Current benchmarks lack comprehensive assessment of LLMs in complex conversational scenarios. CONFETTI addresses this gap through 109 human-simulated conversations, comprising 313 user turns and covering 86 APIs. These conversations explicitly target various conversational complexities, such as follow-ups, goal correction and switching, ambiguous and implicit goals. We perform off-policy turn-level evaluation using this benchmark targeting function-calling. Our benchmark also incorporates dialog act annotations to assess agent responses. We evaluate a series of state-of-the-art LLMs and analyze their performance with respect to the number of available APIs, conversation lengths, and chained function calling. Our results reveal that while some models are able to handle long conversations, and leverage more than 20+ APIs successfully, other models struggle with longer context or when increasing the number of APIs. We also report that the performance on chained function-calls is severely limited across the models. Overall, the top performing models onCONFETTI are Nova Pro (40.01%), Claude Sonnet v3.5 (35.46%) and Llama 3.1 405B (33.19%) followed by command-r-plus (31.18%) and Mistral-Large-2407 (30.07%).</abstract>
      <url hash="6c1a637f">2025.acl-long.394</url>
      <bibkey>alkhouli-etal-2025-confetti</bibkey>
    </paper>
    <paper id="395">
      <title>Evaluating Theory of (an uncertain) Mind: Predicting the Uncertain Beliefs of Others from Conversational Cues</title>
      <author><first>Anthony</first><last>Sicilia</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Malihe</first><last>Alikhani</last><affiliation>Northeastern University</affiliation></author>
      <pages>8007-8021</pages>
      <abstract>Typically, when evaluating Theory of Mind, we consider the beliefs of others to be binary: held or not held. But what if someone is unsure about their own beliefs? How can we quantify this uncertainty? We propose a new suite of tasks, challenging language models (LMs) to model the uncertainty of participants in a dialogue. We design these tasks around conversation forecasting, where the goal is to predict the probability of an unobserved conversation outcome. Uniquely, we view conversation agents themselves as forecasters, asking an LM to predict the uncertainty of an individual from their language use. We experiment with scaling methods, bagging, and demographic context for this regression task, conducting experiments on three dialogue corpora (social, negotiation, task-oriented) with eight LMs. While LMs can explain up to 7% variance in the uncertainty of others, we highlight the difficulty of the tasks and room for future work, especially in tasks that require explicit shifts in perspective.</abstract>
      <url hash="cae5fdc2">2025.acl-long.395</url>
      <bibkey>sicilia-alikhani-2025-evaluating</bibkey>
    </paper>
    <paper id="396">
      <title>Uncertainty in Causality: A New Frontier</title>
      <author><first>Shaobo</first><last>Cui</last></author>
      <author><first>Luca</first><last>Mouchel</last></author>
      <author><first>Boi</first><last>Faltings</last></author>
      <pages>8022-8044</pages>
      <abstract>Understanding uncertainty in causality is vital in various domains, including core NLP tasks like event causality extraction, commonsense reasoning, and counterfactual text generation. However, existing literature lacks a comprehensive examination of this area. This survey aims to fill this gap by thoroughly reviewing uncertainty in causality. We first introduce a novel trichotomy, categorizing causal uncertainty into aleatoric (inherent randomness in causal data), epistemic (causal model limitations), and ontological (existence of causal links) uncertainty. We then survey methods for quantifying uncertainty in causal analysis and highlight the complementary relationship between causal uncertainty and causal strength. Furthermore, we examine the challenges that large language models (LLMs) face in handling causal uncertainty, such as hallucinations and inconsistencies, and propose key traits for an optimal causal LLM. Our paper reviews current approaches and outlines future research directions, aiming to serve as a practical guide for researchers and practitioners in this emerging field.</abstract>
      <url hash="de0b97c6">2025.acl-long.396</url>
      <bibkey>cui-etal-2025-uncertainty</bibkey>
    </paper>
    <paper id="397">
      <title><fixed-case>S</fixed-case>ynthesize<fixed-case>M</fixed-case>e! Inducing Persona-Guided Prompts for Personalized Reward Models in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Michael J</first><last>Ryan</last><affiliation>Stanford University</affiliation></author>
      <author><first>Omar</first><last>Shaikh</last><affiliation>Stanford University</affiliation></author>
      <author><first>Aditri</first><last>Bhagirath</last></author>
      <author><first>Daniel</first><last>Frees</last><affiliation>Google</affiliation></author>
      <author><first>William Barr</first><last>Held</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Diyi</first><last>Yang</last><affiliation>Stanford University</affiliation></author>
      <pages>8045-8078</pages>
      <abstract>Recent calls for pluralistic alignment of Large Language Models (LLMs) encourage adapting models to diverse user preferences. However, most prior work on personalized reward models heavily rely on additional identity information, such as demographic details or a predefined set of preference categories. To this end, we introduce SynthesizeMe, an approach to inducing synthetic user personas from user interactions for personalized reward modeling. SynthesizeMe first generates and verifies reasoning to explain user preferences, then induces synthetic user personas from that reasoning, and finally filters to informative prior user interactions in order to build personalized prompts for a particular user. We show that using SynthesizeMe induced prompts improves personalized LLM-as-a-judge accuracy by 4.4% on Chatbot Arena. Combining SynthesizeMe derived prompts with a reward model achieves top performance on PersonalRewardBench: a new curation of user-stratified interactions with chatbots collected from 854 users of Chatbot Arena and PRISM.</abstract>
      <url hash="788af398">2025.acl-long.397</url>
      <bibkey>ryan-etal-2025-synthesizeme</bibkey>
    </paper>
    <paper id="398">
      <title>When People are Floods: Analyzing Dehumanizing Metaphors in Immigration Discourse with Large Language Models</title>
      <author><first>Julia</first><last>Mendelsohn</last></author>
      <author><first>Ceren</first><last>Budak</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <pages>8079-8103</pages>
      <abstract>Metaphor, discussing one concept in terms of another, is abundant in politics and can shape how people understand important issues. We develop a computational approach to measure metaphorical language, focusing on immigration discourse on social media. Grounded in qualitative social science research, we identify seven concepts evoked in immigration discourse (e.g. water or vermin). We propose and evaluate a novel technique that leverages both word-level and document-level signals to measure metaphor with respect to these concepts. We then study the relationship between metaphor, political ideology, and user engagement in 400K US tweets about immigration. While conservatives tend to use dehumanizing metaphors more than liberals, this effect varies widely across concepts. Moreover, creature-related metaphor is associated with more retweets, especially for liberal authors. Our work highlights the potential for computational methods to complement qualitative approaches in understanding subtle and implicit language in political discourse.</abstract>
      <url hash="dd3bc49c">2025.acl-long.398</url>
      <bibkey>mendelsohn-budak-2025-people</bibkey>
    </paper>
    <paper id="399">
      <title><fixed-case>AG</fixed-case>rail: A Lifelong Agent Guardrail with Effective and Adaptive Safety Detection</title>
      <author><first>Weidi</first><last>Luo</last></author>
      <author><first>Shenghong</first><last>Dai</last></author>
      <author><first>Xiaogeng</first><last>Liu</last><affiliation>University of Wisconsin - Madison</affiliation></author>
      <author><first>Suman</first><last>Banerjee</last><affiliation>UW-Madison</affiliation></author>
      <author><first>Huan</first><last>Sun</last><affiliation>The Ohio State University, Columbus</affiliation></author>
      <author><first>Muhao</first><last>Chen</last><affiliation>University of California, Davis and University of Southern California</affiliation></author>
      <author><first>Chaowei</first><last>Xiao</last><affiliation>University of Wisconsin - Madison and NVIDIA</affiliation></author>
      <pages>8104-8139</pages>
      <abstract>The rapid advancements in Large Language Models (LLMs) have enabled their deployment as autonomous agents for handling complex tasks in dynamic environments. These LLMs demonstrate strong problem-solving capabilities and adaptability to multifaceted scenarios. However, their use as agents also introduces significant risks, including task-specific risks, which are identified by the agent administrator based on the specific task requirements and constraints, and systemic risks, which stem from vulnerabilities in their design or interactions, potentially compromising confidentiality, integrity, or availability (CIA) of information and triggering security risks. Existing defense agencies fail to adaptively and effectively mitigate these risks. In this paper, we propose AGrail, a lifelong agent guardrail to enhance LLM agent safety, which features adaptive safety check generation, effective safety check optimization, and tool compatibility &amp; flexibility. Extensive experiments demonstrate that AGrail not only achieves strong performance against task-specific and system risks but also exhibits transferability across different LLM agents’ tasks.</abstract>
      <url hash="e7c4e05c">2025.acl-long.399</url>
      <bibkey>luo-etal-2025-agrail</bibkey>
    </paper>
    <paper id="400">
      <title>Improving Model Factuality with Fine-grained Critique-based Evaluator</title>
      <author><first>Yiqing</first><last>Xie</last></author>
      <author><first>Wenxuan</first><last>Zhou</last><affiliation>CMU, Carnegie Mellon University</affiliation></author>
      <author><first>Pradyot</first><last>Prakash</last><affiliation>Meta</affiliation></author>
      <author><first>Di</first><last>Jin</last><affiliation>Meta</affiliation></author>
      <author><first>Yuning</first><last>Mao</last><affiliation>Meta</affiliation></author>
      <author><first>Quintin</first><last>Fettes</last><affiliation>Meta</affiliation></author>
      <author><first>Arya</first><last>Talebzadeh</last><affiliation>Meta</affiliation></author>
      <author><first>Sinong</first><last>Wang</last><affiliation>Facebook</affiliation></author>
      <author><first>Han</first><last>Fang</last><affiliation>Meta AI</affiliation></author>
      <author><first>Carolyn</first><last>Rose</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Daniel</first><last>Fried</last><affiliation>Meta AI and Carnegie Mellon University</affiliation></author>
      <author><first>Hejia</first><last>Zhang</last><affiliation>Facebook</affiliation></author>
      <pages>8140-8155</pages>
      <abstract>Factuality evaluation aims to detect factual errors produced by language models (LMs) and hence guide the development of more factual models. Towards this goal, we train a factuality evaluator, FenCE, that provides LM generators with claim-level factuality feedback. In particular, we train FenCE to (1) generate textual critiques along with scores and (2) make claim-level judgment based on diverse source documents obtained by various tools, via data augmentation on a combination of public judgment datasets. We then present a framework that leverages FenCE to improve the factuality of LM generators by constructing training data. Specifically, we generate a set of candidate responses, ask FenCE to revise and score each response without introducing lesser-known facts, and train the generator by preferring highly scored revised responses. Experiments show that our data augmentation methods improve the evaluator’s accuracy by 2.9% on LLM-AggreFact. With FenCE, we improve Llama2-7B-chat/Llama3-8B-chat’s factuality rate by 16.86%/14.45% on FActScore, outperforming state-of-the-art factuality finetuning methods by 8.83%/6.96%.</abstract>
      <url hash="2403831c">2025.acl-long.400</url>
      <bibkey>xie-etal-2025-improving</bibkey>
    </paper>
    <paper id="401">
      <title>Building a Long Text Privacy Policy Corpus with Multi-Class Labels</title>
      <author><first>Florencia</first><last>Marotta-Wurgler</last><affiliation>New York University</affiliation></author>
      <author><first>David</first><last>Stein</last><affiliation>Northeastern University</affiliation></author>
      <pages>8156-8219</pages>
      <abstract>Legal text poses distinctive challenges for natural language processing. The legal import of a term may depend on omissions, cross-references, or silence, Further, legal text is often susceptible to multiple valid, conflicting interpretations; as the saying goes: a good lawyer’s answer to any question is “it depends.”This work introduces a new, hand-coded dataset for the interpretation of privacy policies. It includes privacy policies from 149 firms, including materials incorporated by reference. The policies are annotated across 64 dimension that reflect the applicable legal rules and contested terms from EU and US privacy regulation and litigation. Our annotation methodology is designed to capture the capture core challenges peculiar to legal language, including indeterminacy, interdependence between clauses, meaningful silence, and the implications of legal defaults. We present a set of baseline results for the dataset using current large language models.</abstract>
      <url hash="53f2439f">2025.acl-long.401</url>
      <bibkey>marotta-wurgler-stein-2025-building</bibkey>
    </paper>
    <paper id="402">
      <title><fixed-case>R</fixed-case>2-<fixed-case>M</fixed-case>ulti<fixed-case>O</fixed-case>mnia: Leading Multilingual Multimodal Reasoning via Self-Training</title>
      <author><first>Leonardo</first><last>Ranaldi</last></author>
      <author><first>Federico</first><last>Ranaldi</last><affiliation>University of Roma “Tor Vergata”</affiliation></author>
      <author><first>Giulia</first><last>Pucci</last></author>
      <pages>8220-8234</pages>
      <abstract>Reasoning is an intricate process that transcends both language and vision; yet, despite its inherently modality-agnostic nature, develop-ing effective multilingual and multimodal reasoning capabilities remains a substantial challenge for Multimodal Large Language Models (MLLMs). They struggle to activate complex reasoning behaviours, delivering step-wise explanation, questioning and reflection, particularly in multilingual settings where high-quality supervision across languages is lacking. Recent works have introduced eclectic strategies to enhance MLLMs’ reasoning; however, they remain related to a single language.To make MLLMs’ reasoning capabilities aligned among languages and improve modality performances, we propose R2-MultiOmnia, a modular approach that instructs the models to abstract key elements of the reasoning process and then refine reasoning trajectories via self-correction. Specifically, we instruct the models producing multimodal synthetic resources by bridging modalities and then self-improving their capabilities. To stabilise learning and the reasoning processes structure, we propose Curriculum Learning Reasoning Stabilisation with structured output rewards to gradually refine the models’ capabilities to learn and deliver robust reasoning processes. Experiments show that R2-MultiOmnia improves multimodal reasoning, gets aligned performances among the languages approaching strong models.</abstract>
      <url hash="74086a79">2025.acl-long.402</url>
      <bibkey>ranaldi-etal-2025-r2</bibkey>
    </paper>
    <paper id="403">
      <title>When the <fixed-case>LM</fixed-case> misunderstood the human chuckled: Analyzing garden path effects in humans and language models</title>
      <author><first>Samuel Joseph</first><last>Amouyal</last><affiliation>School of Computer Science, Tel Aviv University</affiliation></author>
      <author><first>Aya</first><last>Meltzer-Asscher</last><affiliation>Tel Aviv University</affiliation></author>
      <author><first>Jonathan</first><last>Berant</last><affiliation>Google and Tel Aviv University</affiliation></author>
      <pages>8235-8253</pages>
      <abstract>Modern Large Language Models (LLMs) have shown human-like abilities in many language tasks, sparking interest in comparing LLMs’ and humans’ language processing. In this paper, we try to answer two questions: 1. What makes garden-path sentences hard to understand for humans? 2. Do the same reasons make garden-path sentences hard for LLMs as well? Based on psycholinguistic research, we formulate hypotheses on why garden-path sentences are hard, and test these hypotheses on human participants and a large suite of LLMs using comprehension questions. Our findings reveal that both LLMs and humans struggle with specific syntactic complexities, with some models showing high correlation with human comprehension. To complement our findings, we test LLM comprehension of garden-path constructions with paraphrasing and text-to-image generation tasks, and find that the results mirror the sentence comprehension question results, further validating our findings on LLM understanding of these constructions.</abstract>
      <url hash="cfa0f633">2025.acl-long.403</url>
      <bibkey>amouyal-etal-2025-lm</bibkey>
    </paper>
    <paper id="404">
      <title>Cross-Lingual Pitfalls: Automatic Probing Cross-Lingual Weakness of Multilingual Large Language Models</title>
      <author><first>Zixiang</first><last>Xu</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Yanbo</first><last>Wang</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Yue</first><last>Huang</last></author>
      <author><first>Xiuying</first><last>Chen</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Jieyu</first><last>Zhao</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Meng</first><last>Jiang</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Xiangliang</first><last>Zhang</last><affiliation>University of Notre Dame</affiliation></author>
      <pages>8254-8284</pages>
      <abstract>Large Language Models (LLMs) have achieved remarkable success in Natural Language Processing (NLP), yet their cross-lingual consistency remains a significant challenge. This paper introduces a novel methodology for efficiently identifying inherent cross-lingual weaknesses in LLMs. Our approach leverages beam search and LLM-based simulation to generate bilingual question pairs that expose performance discrepancies between English and target languages. We construct a new dataset of over 6,000 bilingual pairs across 16 languages using this methodology, demonstrating its effectiveness in revealing weaknesses even in state-of-the-art models. The extensive experiments demonstrate that our method precisely and cost-effectively pinpoints cross-lingual weaknesses, consistently revealing over 50% accuracy drops in target languages across a wide range of models. Moreover, further experiments investigate the relationship between linguistic similarity and cross-lingual weaknesses, revealing that linguistically related languages share similar performance patterns and benefit from targeted post-training. Code is available at https://github.com/xzx34/Cross-Lingual-Pitfalls.</abstract>
      <url hash="f70622b8">2025.acl-long.404</url>
      <bibkey>xu-etal-2025-cross</bibkey>
    </paper>
    <paper id="405">
      <title><fixed-case>VLSB</fixed-case>ench: Unveiling Visual Leakage in Multimodal Safety</title>
      <author><first>Xuhao</first><last>Hu</last><affiliation>Fudan University and Shanghai AI Laboratory</affiliation></author>
      <author><first>Dongrui</first><last>Liu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Hao</first><last>Li</last><affiliation>Beijing University of Aeronautics and Astronautics</affiliation></author>
      <author><first>Xuanjing</first><last>Huang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Jing</first><last>Shao</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <pages>8285-8316</pages>
      <abstract>Safety concerns of Multimodal large language models (MLLMs) have gradually become an important problem in various applications. Surprisingly, previous works indicate a counterintuitive phenomenon that using textual unlearning to align MLLMs achieves comparable safety performances with MLLMs aligned with image-text pairs. To explain such a phenomenon, we discover a <tex-math>\textit{\textbf{V}isual \textbf{S}afety \textbf{I}nformation \textbf{L}eakage} (\textbf{VSIL})</tex-math> problem in existing multimodal safety benchmarks, <tex-math>\textit{i.e.}</tex-math>, the potentially risky content in the image has been revealed in the textual query. Thus, MLLMs can easily refuse these sensitive image-text pairs according to textual queries only, leading to <b>unreliable cross-modality safety evaluation of MLLMs</b>. We also conduct a further comparison experiment between textual alignment and multimodal alignment to highlight this drawback. To this end, we construct <tex-math>\textit{\textbf{V}isual \textbf{L}eakless \textbf{S}afety \textbf{B}ench} (\textbf{VLSBench})</tex-math> with 2.2k image-text pairs through an automated data pipeline. Experimental results indicate that VLSBench poses a significant challenge to both open-source and close-source MLLMs, <tex-math>\textit{i.e.}</tex-math>, LLaVA, Qwen2-VL and GPT-4o. Besides, we empirically compare textual and multimodal alignment methods on VLSBench and find that textual alignment is effective enough for multimodal safety scenarios with VSIL, while multimodal alignment is preferable for safety scenarios without VSIL.</abstract>
      <url hash="90c4d180">2025.acl-long.405</url>
      <bibkey>hu-etal-2025-vlsbench</bibkey>
    </paper>
    <paper id="406">
      <title>Browsing Lost Unformed Recollections: A Benchmark for Tip-of-the-Tongue Search and Reasoning</title>
      <author><first>Sky</first><last>CH-Wang</last><affiliation>Columbia University</affiliation></author>
      <author><first>Darshan Girish</first><last>Deshpande</last><affiliation>Patronus AI</affiliation></author>
      <author><first>Smaranda</first><last>Muresan</last><affiliation>Columbia University</affiliation></author>
      <author><first>Anand</first><last>Kannappan</last><affiliation>Patronus AI</affiliation></author>
      <author><first>Rebecca</first><last>Qian</last><affiliation>Patronus AI</affiliation></author>
      <pages>8317-8331</pages>
      <abstract>We introduce Browsing Lost Unformed Recollections, a tip-of-the-tongue known-item search and reasoning benchmark for general AI assistants. BLUR introduces a set of 573 real-world validated questions that demand searching and reasoning across multimodal and multilingual inputs, as well as proficient tool use, in order to excel on. Humans easily ace these questions (scoring on average 98%), while the best-performing system scores around 56%. To facilitate progress toward addressing this challenging and aspirational use case for general AI assistants, we release 350 questions through a public leaderboard, retain the answers to 250 of them, and have the rest as a private test set.</abstract>
      <url hash="0aa3b219">2025.acl-long.406</url>
      <bibkey>ch-wang-etal-2025-browsing</bibkey>
    </paper>
    <paper id="407">
      <title>Data Laundering: Artificially Boosting Benchmark Results through Knowledge Distillation</title>
      <author><first>Jonibek</first><last>Mansurov</last></author>
      <author><first>Akhmed</first><last>Sakip</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Alham Fikri</first><last>Aji</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>8332-8345</pages>
      <abstract>In this paper, we show that knowledge distillation can be subverted to manipulate language model benchmark scores, revealing a critical vulnerability in current evaluation practices. We introduce “Data Laundering,” a process that enables the covert transfer of benchmark-specific knowledge through seemingly legitimate intermediate training steps. Through extensive experiments with a 2-layer BERT student model, we show how this approach can achieve substantial improvements in benchmark accuracy (up to 75% on GPQA) without developing genuine reasoning capabilities. Notably, this method can be exploited intentionally or even unintentionally, as researchers may inadvertently adopt this method and inflate scores without realising the implications. While our findings demonstrate the effectiveness of this technique, we present them as a cautionary tale highlighting the urgent need for more robust evaluation methods in AI. This work aims to contribute to the ongoing discussion about evaluation integrity in AI development and the need for benchmarks that more accurately reflect true model capabilities. The code is available at <url>https://github.com/mbzuai-nlp/data_laundering</url>.</abstract>
      <url hash="8bcae731">2025.acl-long.407</url>
      <bibkey>mansurov-etal-2025-data</bibkey>
    </paper>
    <paper id="408">
      <title>Conspiracy Theories and Where to Find Them on <fixed-case>T</fixed-case>ik<fixed-case>T</fixed-case>ok</title>
      <author><first>Francesco</first><last>Corso</last></author>
      <author><first>Francesco</first><last>Pierri</last><affiliation>Politecnico di Milano</affiliation></author>
      <author><first>Gianmarco</first><last>De Francisci Morales</last><affiliation>CENTAI</affiliation></author>
      <pages>8346-8362</pages>
      <abstract>TikTok has skyrocketed in popularity over recent years, especially among younger audiences. However, there are public concerns about the potential of this platform to promote and amplify harmful content. This study presents the first systematic analysis of conspiracy theories on TikTok. By leveraging the official TikTok Research API we collect a longitudinal dataset of 1.5M videos shared in the U.S. over three years. We estimate a lower bound on the prevalence of conspiratorial videos (up to 1000 new videos per month) and evaluate the effects of TikTok’s Creativity Program for monetization, observing an overall increase in video duration regardless of content. Lastly, we evaluate the capabilities of state-of-the-art open-weight Large Language Models to identify conspiracy theories from audio transcriptions of videos. While these models achieve high precision in detecting harmful content (up to 96%), their overall performance remains comparable to fine-tuned traditional models such as RoBERTa. Our findings suggest that Large Language Models can serve as an effective tool for supporting content moderation strategies aimed at reducing the spread of harmful content on TikTok.</abstract>
      <url hash="a0c793bf">2025.acl-long.408</url>
      <bibkey>corso-etal-2025-conspiracy</bibkey>
    </paper>
    <paper id="409">
      <title>Growing Through Experience: Scaling Episodic Grounding in Language Models</title>
      <author><first>Chunhui</first><last>Zhang</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Sirui</first><last>Wang</last></author>
      <author><first>Zhongyu</first><last>Ouyang</last></author>
      <author><first>Xiangchi</first><last>Yuan</last></author>
      <author><first>Soroush</first><last>Vosoughi</last><affiliation>Dartmouth College</affiliation></author>
      <pages>8363-8375</pages>
      <abstract>Language models (LMs) require effective episodic grounding—the ability to learn from and apply past experiences—to perform well at physical planning tasks. While current approaches struggle with scalability and integration of episodic memory, which is particularly limited for medium-sized LMs (7B parameters), larger LMs (70-405B) offer untapped potential through their hierarchical representations and extensive pre-trained knowledge. Therefore, to unlock larger LMs’ potential for grounding, we present a scalable weak-to-strong episodic learning framework that efficiently transfers episodic behaviors from smaller to larger LMs. It uses Monte Carlo tree search for structured experience collection with a novel distillation method that preserves LM capabilities while incorporating episodic memory. This enables larger LMs to leverage their inherent advantages for improved physical planning. Experiments show our solution outperforms top proprietary LMs by 3.45% across diverse planning and question-answering tasks. Layer-wise probing reveals systematic improvements in task alignment, particularly in later LM layers. It shows stable generalization to even unseen scenarios, even as planning steps increase, whereas baselines deteriorate sharply beyond a complexity threshold of four planning steps.</abstract>
      <url hash="730570fc">2025.acl-long.409</url>
      <bibkey>zhang-etal-2025-growing</bibkey>
    </paper>
    <paper id="410">
      <title>Exploiting the Shadows: Unveiling Privacy Leaks through Lower-Ranked Tokens in Large Language Models</title>
      <author><first>Yuan</first><last>Zhou</last><affiliation>Purdue University</affiliation></author>
      <author><first>Zhuo</first><last>Zhang</last></author>
      <author><first>Xiangyu</first><last>Zhang</last><affiliation>Purdue University</affiliation></author>
      <pages>8376-8386</pages>
      <abstract>Large language models (LLMs) play a crucial role in modern applications but face vulnerabilities related to the extraction of sensitive information. This includes unauthorized accesses to internal prompts and retrieval of personally identifiable information (PII) (e.g., in Retrieval-Augmented Generation based agentic applications). We examine these vulnerabilities in a question-answering (QA) setting where LLMs use retrieved documents or training knowledge as few-shot prompts. Although these documents remain confidential under normal use, adversaries can manipulate input queries to extract private content. In this paper, we propose a novel attack method by exploiting the model’s lower-ranked output tokens to leak sensitive information. We systematically evaluate our method, demonstrating its effectiveness in both the agentic application privacy extraction setting and the direct training data extraction. These findings reveal critical privacy risks in LLMs and emphasize the urgent need for enhanced safeguards against information leakage.</abstract>
      <url hash="eea5e741">2025.acl-long.410</url>
      <bibkey>zhou-etal-2025-exploiting</bibkey>
    </paper>
    <paper id="411">
      <title>Attacking Vision-Language Computer Agents via Pop-ups</title>
      <author><first>Yanzhe</first><last>Zhang</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Tao</first><last>Yu</last><affiliation>The University of Hong Kong</affiliation></author>
      <author><first>Diyi</first><last>Yang</last><affiliation>Stanford University</affiliation></author>
      <pages>8387-8401</pages>
      <abstract>Autonomous agents powered by large vision and language models (VLM) have demonstrated significant potential in completing daily computer tasks, such as browsing the web to book travel and operating desktop software, which requires agents to understand these interfaces. Despite such visual inputs becoming more integrated into agentic applications, what types of risks and attacks exist around them still remain unclear. In this work, we demonstrate that VLM agents can be easily attacked by a set of carefully designed adversarial pop-ups, which human users would typically recognize and ignore. This distraction leads agents to click these pop-ups instead of performing their tasks as usual. Integrating these pop-ups into existing agent testing environments like OSWorld and VisualWebArena leads to an attack success rate (the frequency of the agent clicking the pop-ups) of 86% on average and decreases the task success rate by 47%. Basic defense techniques, such as asking the agent to ignore pop-ups or including an advertisement notice, are ineffective against the attack. Code is available at [this link](https://github.com/SALT-NLP/PopupAttack).</abstract>
      <url hash="eeecca36">2025.acl-long.411</url>
      <bibkey>zhang-etal-2025-attacking</bibkey>
    </paper>
    <paper id="412">
      <title>Explicit and Implicit Data Augmentation for Social Event Detection</title>
      <author><first>Congbo</first><last>Ma</last></author>
      <author><first>Yuxia</first><last>Wang</last></author>
      <author><first>Jia</first><last>Wu</last><affiliation>Macquarie University</affiliation></author>
      <author><first>Jian</first><last>Yang</last><affiliation>Macquarie University</affiliation></author>
      <author><first>Jing</first><last>Du</last></author>
      <author><first>Zitai</first><last>Qiu</last></author>
      <author><first>Qing</first><last>Li</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Hu</first><last>Wang</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Preslav</first><last>Nakov</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>8402-8415</pages>
      <abstract>Social event detection involves identifying and categorizing important events from social media, which relies on labeled data, but annotation is costly and labor-intensive. To address this problem, we propose Augmentation framework for Social Event Detection (SED-Aug), a plug-and-play dual augmentation framework, which combines explicit text-based and implicit feature-space augmentation to enhance data diversity and model robustness. The explicit augmentation utilizes LLMs to enhance textual information through five diverse generation strategies. For implicit augmentation, we design five novel perturbation techniques that operate in the feature space on structural fused embeddings. These perturbations are crafted to keep the semantic and relational properties of the embeddings and make them more diverse. Specifically, SED-Aug outperforms the best baseline model by approximately 17.67% on the Twitter2012 dataset and by about 15.57% on the Twitter2018 dataset in terms of the average F1 score.</abstract>
      <url hash="42ad392f">2025.acl-long.412</url>
      <bibkey>ma-etal-2025-explicit</bibkey>
    </paper>
    <paper id="413">
      <title>In Prospect and Retrospect: Reflective Memory Management for Long-term Personalized Dialogue Agents</title>
      <author><first>Zhen</first><last>Tan</last></author>
      <author><first>Jun</first><last>Yan</last><affiliation>Google</affiliation></author>
      <author><first>I-Hung</first><last>Hsu</last><affiliation>Google</affiliation></author>
      <author><first>Rujun</first><last>Han</last><affiliation>Google</affiliation></author>
      <author><first>Zifeng</first><last>Wang</last><affiliation>Google</affiliation></author>
      <author><first>Long</first><last>Le</last><affiliation>Google</affiliation></author>
      <author><first>Yiwen</first><last>Song</last><affiliation>Google</affiliation></author>
      <author><first>Yanfei</first><last>Chen</last><affiliation>Google</affiliation></author>
      <author><first>Hamid</first><last>Palangi</last><affiliation>Google</affiliation></author>
      <author><first>George</first><last>Lee</last><affiliation>Google</affiliation></author>
      <author><first>Anand Rajan</first><last>Iyer</last><affiliation>Google</affiliation></author>
      <author><first>Tianlong</first><last>Chen</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Huan</first><last>Liu</last></author>
      <author><first>Chen-Yu</first><last>Lee</last><affiliation>Google</affiliation></author>
      <author><first>Tomas</first><last>Pfister</last><affiliation>Google</affiliation></author>
      <pages>8416-8439</pages>
      <abstract>Large Language Models (LLMs) have made significant progress in open-ended dialogue, yet their inability to retain and retrieve relevant information from long-term interactions limits their effectiveness in applications requiring sustained personalization. External memory mechanisms have been proposed to address this limitation, enabling LLMs to maintain conversational continuity. However, existing approaches struggle with two key challenges. First, rigid memory granularity fails to capture the natural semantic structure of conversations, leading to fragmented and incomplete representations. Second, fixed retrieval mechanisms cannot adapt to diverse dialogue contexts and user interaction patterns. In this work, we propose Reflective Memory Management (RMM), a novel mechanism for long-term dialogue agents, integrating forward- and backward-looking reflections: (1) Prospective Reflection, which dynamically summarizes interactions across granularities—utterances, turns, and sessions—into a personalized memory bank for effective future retrieval, and (2) Retrospective Reflection, which iteratively refines the retrieval in an online reinforcement learning (RL) manner based on LLMs’ cited evidence. Experiments show that RMM demonstrates consistent improvement across various metrics and benchmarks. For example, RMM shows more than 10% accuracy improvement over the baseline without memory management on the LongMemEval dataset.</abstract>
      <url hash="94013190">2025.acl-long.413</url>
      <bibkey>tan-etal-2025-prospect</bibkey>
    </paper>
    <paper id="414">
      <title>Revisiting Classical <fixed-case>C</fixed-case>hinese Event Extraction with Ancient Literature Information</title>
      <author><first>Xiaoyi</first><last>Bao</last></author>
      <author><first>Zhongqing</first><last>Wang</last><affiliation>Soochow University, China</affiliation></author>
      <author><first>Jinghang</first><last>Gu</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Chu-Ren</first><last>Huang</last></author>
      <pages>8440-8451</pages>
      <abstract>The research on classical Chinese event extraction trends to directly graft the complex modeling from English or modern Chinese works, neglecting the utilization of the unique characteristic of this language. We argue that, compared with grafting the sophisticated methods from other languages, focusing on classical Chinese’s inimitable source of __Ancient Literature__ could provide us with extra and comprehensive semantics in event extraction. Motivated by this, we propose a Literary Vision-Language Model (VLM) for classical Chinese event extraction, integrating with literature annotations, historical background and character glyph to capture the inner- and outer-context information from the sequence. Extensive experiments build a new state-of-the-art performance in the GuwenEE, CHED datasets, which underscores the effectiveness of our proposed VLM, and more importantly, these unique features can be obtained precisely at nearly zero cost.</abstract>
      <url hash="e17deb35">2025.acl-long.414</url>
      <bibkey>bao-etal-2025-revisiting</bibkey>
    </paper>
    <paper id="415">
      <title>Unanswerability Evaluation for Retrieval Augmented Generation</title>
      <author><first>Xiangyu</first><last>Peng</last><affiliation>Salesforce AI Research</affiliation></author>
      <author><first>Prafulla Kumar</first><last>Choubey</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Caiming</first><last>Xiong</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Chien-Sheng</first><last>Wu</last><affiliation>Salesforce AI</affiliation></author>
      <pages>8452-8472</pages>
      <abstract>Existing evaluation frameworks for retrieval-augmented generation (RAG) systems focus on answerable queries, but they overlook the importance of appropriately rejecting unanswerable requests. In this paper, we introduce UAEval4RAG, a comprehensive evaluation framework designed to evaluate whether RAG systems effectively handle unanswerable queries specific to a given knowledge base. We first define a taxonomy with six unanswerable categories, and UAEval4RAG automatically synthesizes diverse and challenging queries for any given knowledge base and evaluate the RAG systems with unanswered ratio and acceptable ratio metrics. We also conduct experiments with various RAG components and prompting strategies across four datasets, which reveals that due to varying knowledge distribution across datasets, no single configuration consistently delivers optimal performance on both answerable and unanswerable requests across different knowledge bases. Our findings highlight the critical role of component selection and prompt design in optimizing RAG systems to balance the accuracy of answerable queries with high rejection rates of unanswerable ones. UAEval4RAG provides valuable insights and tools for developing more robust and reliable RAG systems.</abstract>
      <url hash="e1f485b6">2025.acl-long.415</url>
      <bibkey>peng-etal-2025-unanswerability</bibkey>
    </paper>
    <paper id="416">
      <title><fixed-case>SCALE</fixed-case>: Towards Collaborative Content Analysis in Social Science with Large Language Model Agents and Human Intervention</title>
      <author><first>Chengshuai</first><last>Zhao</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Zhen</first><last>Tan</last></author>
      <author><first>Chau-Wai</first><last>Wong</last><affiliation>North Carolina State University</affiliation></author>
      <author><first>Xinyan</first><last>Zhao</last></author>
      <author><first>Tianlong</first><last>Chen</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Huan</first><last>Liu</last></author>
      <pages>8473-8503</pages>
      <abstract>Content analysis breaks down complex and unstructured texts into theory-informed numerical categories. Particularly, in social science, this process usually relies on multiple rounds of manual annotation, domain expert discussion, and rule-based refinement. In this paper, we introduce SCALE, a novel multi-agent framework that effectively <tex-math>\underline{\textbf{S}}</tex-math>imulates <tex-math>\underline{\textbf{C}}</tex-math>ontent <tex-math>\underline{\textbf{A}}</tex-math>nalysis via <tex-math>\underline{\textbf{L}}</tex-math>arge language model (LLM) ag<tex-math>\underline{\textbf{E}}</tex-math>nts. SCALE imitates key phases of content analysis, including text coding, collaborative discussion, and dynamic codebook evolution, capturing the reflective depth and adaptive discussions of human researchers. Furthermore, by integrating diverse modes of human intervention, SCALE is augmented with expert input to further enhance its performance. Extensive evaluations on real-world datasets demonstrate that SCALE achieves human-approximated performance across various complex content analysis tasks, offering an innovative potential for future social science research.</abstract>
      <url hash="18fc0502">2025.acl-long.416</url>
      <bibkey>zhao-etal-2025-scale</bibkey>
    </paper>
    <paper id="417">
      <title>Self-Error-Instruct: Generalizing from Errors for <fixed-case>LLM</fixed-case>s Mathematical Reasoning</title>
      <author><first>Erxin</first><last>Yu</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Jing</first><last>Li</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Ming</first><last>Liao</last></author>
      <author><first>Qi</first><last>Zhu</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Boyang</first><last>Xue</last></author>
      <author><first>Minghui</first><last>Xu</last></author>
      <author><first>Baojun</first><last>Wang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Lanqing</first><last>Hong</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Fei</first><last>Mi</last></author>
      <author><first>Lifeng</first><last>Shang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <pages>8504-8519</pages>
      <abstract>Although large language models demonstrate strong performance across various domains, they still struggle with numerous bad cases in mathematical reasoning. Previous approaches to learning from errors synthesize training data by solely extrapolating from isolated bad cases, thereby failing to generalize the extensive patterns inherent within these cases. This paper presents Self-Error-Instruct (SEI), a framework that addresses these model weaknesses and synthesizes more generalized targeted training data. Specifically, we explore a target model on two mathematical datasets, GSM8K and MATH, to pinpoint bad cases. Then, we generate error keyphrases for these cases based on the instructor model’s (GPT-4o) analysis and identify error types by clustering these keyphrases. Next, we sample a few bad cases during each generation for each identified error type and input them into the instructor model, which synthesizes additional training data using a self-instruct approach. This new data is refined through a one-shot learning process to ensure that only the most effective examples are kept. Finally, we use these curated data to fine-tune the target model, iteratively repeating the process to enhance performance. We apply our framework to various models and observe improvements in their reasoning abilities across both in-domain and out-of-domain mathematics datasets. These results demonstrate the effectiveness of self-error instruction in improving LLMs’ mathematical reasoning through error generalization.</abstract>
      <url hash="5149a3fc">2025.acl-long.417</url>
      <bibkey>yu-etal-2025-self-error</bibkey>
    </paper>
    <paper id="418">
      <title><fixed-case>RAGE</fixed-case>val: Scenario Specific <fixed-case>RAG</fixed-case> Evaluation Dataset Generation Framework</title>
      <author><first>Kunlun</first><last>Zhu</last></author>
      <author><first>Yifan</first><last>Luo</last></author>
      <author><first>Dingling</first><last>Xu</last></author>
      <author><first>Yukun</first><last>Yan</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Zhenghao</first><last>Liu</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Shi</first><last>Yu</last><affiliation>Department of Computer Science and Technology, Tsinghua University</affiliation></author>
      <author><first>Ruobing</first><last>Wang</last></author>
      <author><first>Shuo</first><last>Wang</last></author>
      <author><first>Yishan</first><last>Li</last></author>
      <author><first>Nan</first><last>Zhang</last><affiliation>ModelBest</affiliation></author>
      <author><first>Xu</first><last>Han</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Zhiyuan</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Maosong</first><last>Sun</last><affiliation>Tsinghua University</affiliation></author>
      <pages>8520-8544</pages>
      <abstract>Retrieval-Augmented Generation (RAG) is a powerful approach that enables large language models (LLMs) to incorporate external knowledge. However, evaluating the effectiveness of RAG systems in specialized scenarios remains challenging due to the high costs of data construction and the lack of suitable evaluation metrics. This paper introduces RAGEval, a framework designed to assess RAG systems across diverse scenarios by generating high-quality documents, questions, answers, and references through a schema-based pipeline. With a focus on factual accuracy, we propose three novel metrics—Completeness, Hallucination, and Irrelevance—to evaluate LLM-generated responses rigorously. Experimental results show that RAGEval outperforms zero-shot and one-shot methods in terms of clarity, safety, conformity, and richness of generated samples. Furthermore, the use of LLMs for scoring the proposed metrics demonstrates a high level of consistency with human evaluations. RAGEval establishes a new paradigm for evaluating RAG systems in real-world applications. The code and dataset are released at https://github.com/OpenBMB/RAGEval.</abstract>
      <url hash="acd90e71">2025.acl-long.418</url>
      <bibkey>zhu-etal-2025-rageval</bibkey>
    </paper>
    <paper id="419">
      <title>A Survey on Patent Analysis: From <fixed-case>NLP</fixed-case> to Multimodal <fixed-case>AI</fixed-case></title>
      <author><first>Homaira Huda</first><last>Shomee</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <author><first>Zhu</first><last>Wang</last><affiliation>Northwestern University</affiliation></author>
      <author><first>Sathya N.</first><last>Ravi</last><affiliation>University of Illinois, Chicago</affiliation></author>
      <author><first>Sourav</first><last>Medya</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <pages>8545-8561</pages>
      <abstract>Recent advances in Pretrained Language Models (PLMs) and Large Language Models (LLMs) have demonstrated transformative capabilities across diverse domains. The field of patent analysis and innovation is not an exception, where natural language processing (NLP) techniques presents opportunities to streamline and enhance important tasks—such as patent classification and patent retrieval—in the patent cycle. This not only accelerates the efficiency of patent researchers and applicants, but also opens new avenues for technological innovation and discovery. Our survey provides a comprehensive summary of recent NLP-based methods—including multimodal ones—in patent analysis. We also introduce a novel taxonomy for categorization based on tasks in the patent life cycle, as well as the specifics of the methods. This interdisciplinary survey aims to serve as a comprehensive resource for researchers and practitioners who work at the intersection of NLP, Multimodal AI, and patent analysis, as well as patent offices to build efficient patent systems.</abstract>
      <url hash="c848f142">2025.acl-long.419</url>
      <bibkey>shomee-etal-2025-survey</bibkey>
    </paper>
    <paper id="420">
      <title><fixed-case>S</fixed-case>ci<fixed-case>V</fixed-case>er: Evaluating Foundation Models for Multimodal Scientific Claim Verification</title>
      <author><first>Chengye</first><last>Wang</last></author>
      <author><first>Yifei</first><last>Shen</last></author>
      <author><first>Zexi</first><last>Kuang</last></author>
      <author><first>Arman</first><last>Cohan</last><affiliation>Yale University and Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Yilun</first><last>Zhao</last><affiliation>Yale University</affiliation></author>
      <pages>8562-8579</pages>
      <abstract>We introduce SciVer, the first benchmark specifically designed to evaluate the ability of foundation models to verify claims within a multimodal scientific context.SciVer consists of 3,000 expert-annotated examples over 1,113 scientific papers, covering four subsets, each representing a common reasoning type in multimodal scientific claim verification. To enable fine-grained evaluation, each example includes expert-annotated supporting evidence.We assess the performance of 21 state-of-the-art multimodal foundation models, including o4-mini, Gemini-2.5-Flash, Llama-3.2-Vision, and Qwen2.5-VL. Our experiment reveals a substantial performance gap between these models and human experts on SciVer.Through an in-depth analysis of retrieval-augmented generation (RAG), and human-conducted error evaluations, we identify critical limitations in current open-source models, offering key insights to advance models’ comprehension and reasoning in multimodal scientific literature tasks.</abstract>
      <url hash="3ff341f9">2025.acl-long.420</url>
      <bibkey>wang-etal-2025-sciver</bibkey>
    </paper>
    <paper id="421">
      <title><fixed-case>M</fixed-case>ulti<fixed-case>A</fixed-case>gent<fixed-case>B</fixed-case>ench : Evaluating the Collaboration and Competition of <fixed-case>LLM</fixed-case> agents</title>
      <author><first>Kunlun</first><last>Zhu</last></author>
      <author><first>Hongyi</first><last>Du</last></author>
      <author><first>Zhaochen</first><last>Hong</last></author>
      <author><first>Xiaocheng</first><last>Yang</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Shuyi</first><last>Guo</last></author>
      <author><first>Zhe</first><last>Wang</last></author>
      <author><first>Zhenhailong</first><last>Wang</last></author>
      <author><first>Cheng</first><last>Qian</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Robert</first><last>Tang</last></author>
      <author><first>Heng</first><last>Ji</last><affiliation>University of Illinois, Urbana-Champaign</affiliation></author>
      <author><first>Jiaxuan</first><last>You</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <pages>8580-8622</pages>
      <abstract>Large Language Models (LLMs) have shown remarkable capabilities as autonomous agents; yet existing benchmarks either focus on single-agent tasks or are confined to narrow domains, failing to capture the dynamics of multi-agent coordination and competition. In this paper, we introduce MultiAgentBench, a comprehensive benchmark designed to evaluate LLM-based multi-agent systems across diverse, interactive scenarios. Our framework measures not only task completion but also the quality of collaboration and competition using novel, milestone-based key performance indicators. Moreover, we evaluate various coordination protocols (including star, chain, tree, and graph topologies) and innovative strategies such as group discussion and cognitive planning. Notably, cognitive planning improves milestone achievement rates by 3%. Code and dataset will be made publicly available. Code and datasets are publicavailable at https://github.com/ulab-uiuc/MARBLE</abstract>
      <url hash="b198dae8">2025.acl-long.421</url>
      <bibkey>zhu-etal-2025-multiagentbench</bibkey>
    </paper>
    <paper id="422">
      <title><fixed-case>S</fixed-case>inhala Encoder-only Language Models and Evaluation</title>
      <author><first>Tharindu</first><last>Ranasinghe</last><affiliation>Lancaster University</affiliation></author>
      <author><first>Hansi</first><last>Hettiarachchi</last><affiliation>Lancaster University</affiliation></author>
      <author><first>Nadeesha Chathurangi Naradde Vidana</first><last>Pathirana</last><affiliation>Aston University</affiliation></author>
      <author><first>Damith</first><last>Premasiri</last></author>
      <author><first>Lasitha</first><last>Uyangodage</last><affiliation>Westfälische Wilhelms-Universität Münster</affiliation></author>
      <author><first>Isuri</first><last>Nanomi Arachchige</last></author>
      <author><first>Alistair</first><last>Plum</last><affiliation>University of Luxembourg</affiliation></author>
      <author><first>Paul</first><last>Rayson</last><affiliation>Lancaster University</affiliation></author>
      <author><first>Ruslan</first><last>Mitkov</last><affiliation>Lancaster University</affiliation></author>
      <pages>8623-8636</pages>
      <abstract>Recently, language models (LMs) have produced excellent results in many natural language processing (NLP) tasks. However, their effectiveness is highly dependent on available pre-training resources, which is particularly challenging for low-resource languages such as Sinhala. Furthermore, the scarcity of benchmarks to evaluate LMs is also a major concern for low-resource languages. In this paper, we address these two challenges for Sinhala by (i) collecting the largest monolingual corpus for Sinhala, (ii) training multiple LMs on this corpus and (iii) compiling the first Sinhala NLP benchmark (Sinhala-GLUE) and evaluating LMs on it. We show the Sinhala LMs trained in this paper outperform the popular multilingual LMs, such as XLM-R and existing Sinhala LMs in downstream NLP tasks. All the trained LMs are publicly available. We also make Sinhala-GLUE publicly available as a public leaderboard, and we hope that it will enable further advancements in developing and evaluating LMs for Sinhala.</abstract>
      <url hash="fc10167a">2025.acl-long.422</url>
      <bibkey>ranasinghe-etal-2025-sinhala</bibkey>
    </paper>
    <paper id="423">
      <title><fixed-case>LLM</fixed-case>s can Perform Multi-Dimensional Analytic Writing Assessments: A Case Study of <fixed-case>L</fixed-case>2 Graduate-Level Academic <fixed-case>E</fixed-case>nglish Writing</title>
      <author><first>Zhengxiang</first><last>Wang</last><affiliation>State University of New York at Stony Brook</affiliation></author>
      <author><first>Veronika</first><last>Makarova</last><affiliation>University of Saskatchewan</affiliation></author>
      <author><first>Zhi</first><last>Li</last></author>
      <author><first>Jordan</first><last>Kodner</last><affiliation>State University of New York, Stony Brook</affiliation></author>
      <author><first>Owen</first><last>Rambow</last><affiliation>Stony Brook University</affiliation></author>
      <pages>8637-8663</pages>
      <abstract>The paper explores the performance of LLMs in the context of multi-dimensional analytic writing assessments, i.e. their ability to provide both scores and comments based on multiple assessment criteria. Using a corpus of literature reviews written by L2 graduate students and assessed by human experts against 9 analytic criteria, we prompt several popular LLMs to perform the same task under various conditions. To evaluate the quality of feedback comments, we apply a novel feedback comment quality evaluation framework. This framework is interpretable, cost-efficient, scalable, and reproducible, compared to existing methods that rely on manual judgments. We find that LLMs can generate reasonably good and generally reliable multi-dimensional analytic assessments. We release our corpus and code for reproducibility.</abstract>
      <url hash="010414e6">2025.acl-long.423</url>
      <bibkey>wang-etal-2025-llms-perform</bibkey>
    </paper>
    <paper id="424">
      <title><fixed-case>SEUF</fixed-case>: Is Unlearning One Expert Enough for Mixture-of-Experts <fixed-case>LLM</fixed-case>s?</title>
      <author><first>Haomin</first><last>Zhuang</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Yihua</first><last>Zhang</last></author>
      <author><first>Kehan</first><last>Guo</last></author>
      <author><first>Jinghan</first><last>Jia</last></author>
      <author><first>Gaowen</first><last>Liu</last></author>
      <author><first>Sijia</first><last>Liu</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Xiangliang</first><last>Zhang</last><affiliation>University of Notre Dame</affiliation></author>
      <pages>8664-8678</pages>
      <abstract>Recent advancements in LLMs unlearning have shown remarkable success in removing unwanted data-model influences while preserving the model’s utility for legitimate knowledge. Despite these strides, sparse Mixture-of-Experts (MoE) LLMs–a key subset of the LLM family–have remained unexplored in the context of unlearning. As MoE LLMs are celebrated for their exceptional performance, we ask:How can unlearning be performed effectively and efficiently on MoE LLMs? Our pilot study shows that the dynamic routing nature of MoE LLMs introduces unique challenges, leading to excessive forgetting, uncontrolled knowledge erasure and substantial utility drops when existing unlearning methods are applied. To address this, we propose a novel Selected-Expert Unlearning Framework (SEUF). Through expert attribution, unlearning is concentrated on the most actively engaged experts for the specified knowledge. Concurrently, an anchor loss is applied to the router to stabilize the active state of this targeted expert, ensuring focused and controlled unlearning. SEUF is compatible with various standard unlearning algorithms. Extensive experiments demonstrate that SEUF enhances both forget quality up to 5% and model utility by 35% on MoE LLMs across various benchmarks and LLM architectures (compared to standard unlearning algorithms), while only unlearning 0.06% of the model parameters.</abstract>
      <url hash="e4ac7925">2025.acl-long.424</url>
      <bibkey>zhuang-etal-2025-seuf</bibkey>
    </paper>
    <paper id="425">
      <title>Pragmatics in the Era of Large Language Models: A Survey on Datasets, Evaluation, Opportunities and Challenges</title>
      <author><first>Bolei</first><last>Ma</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Yuting</first><last>Li</last><affiliation>Universität Köln</affiliation></author>
      <author><first>Wei</first><last>Zhou</last></author>
      <author><first>Ziwei</first><last>Gong</last><affiliation>Columbia University</affiliation></author>
      <author><first>Yang Janet</first><last>Liu</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Katja</first><last>Jasinskaja</last><affiliation>Universität Köln</affiliation></author>
      <author><first>Annemarie</first><last>Friedrich</last><affiliation>University of Augsburg</affiliation></author>
      <author><first>Julia</first><last>Hirschberg</last><affiliation>Columbia University</affiliation></author>
      <author><first>Frauke</first><last>Kreuter</last><affiliation>University of Maryland</affiliation></author>
      <author><first>Barbara</first><last>Plank</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <pages>8679-8696</pages>
      <abstract>Understanding pragmatics—the use of language in context—is crucial for developing NLP systems capable of interpreting nuanced language use. Despite recent advances in language technologies, including large language models, evaluating their ability to handle pragmatic phenomena such as implicatures and references remains challenging. To advance pragmatic abilities in models, it is essential to understand current evaluation trends and identify existing limitations. In this survey, we provide a comprehensive review of resources designed for evaluating pragmatic capabilities in NLP, categorizing datasets by the pragmatic phenomena they address. We analyze task designs, data collection methods, evaluation approaches, and their relevance to real-world applications. By examining these resources in the context of modern language models, we highlight emerging trends, challenges, and gaps in existing benchmarks. Our survey aims to clarify the landscape of pragmatic evaluation and guide the development of more comprehensive and targeted benchmarks, ultimately contributing to more nuanced and context-aware NLP models.</abstract>
      <url hash="90b1b595">2025.acl-long.425</url>
      <bibkey>ma-etal-2025-pragmatics</bibkey>
    </paper>
    <paper id="426">
      <title><fixed-case>L</fixed-case>oc<fixed-case>A</fixed-case>gent: Graph-Guided <fixed-case>LLM</fixed-case> Agents for Code Localization</title>
      <author><first>Zhaoling</first><last>Chen</last></author>
      <author><first>Robert</first><last>Tang</last></author>
      <author><first>Gangda</first><last>Deng</last></author>
      <author><first>Fang</first><last>Wu</last></author>
      <author><first>Jialong</first><last>Wu</last><affiliation>Southeast University</affiliation></author>
      <author><first>Zhiwei</first><last>Jiang</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Viktor</first><last>Prasanna</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Arman</first><last>Cohan</last><affiliation>Yale University and Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Xingyao</first><last>Wang</last><affiliation>All Hands AI and University of Illinois Urbana-Champaign</affiliation></author>
      <pages>8697-8727</pages>
      <abstract>Code localization–identifying precisely where in a codebase changes need to be made–is a fundamental yet challenging task in software maintenance. Existing approaches struggle to efficiently navigate complex codebases when identifying relevant code snippets.The challenge lies in bridging natural language problem descriptions with the target code elements, often requiring reasoning across hierarchical structures and multiple dependencies.We introduce LocAgent, a framework that addresses code localization through a graph-guided agent.By parsing codebases into directed heterogeneous graphs, LocAgent creates a lightweight representation that captures code structures and their dependencies, enabling LLM agents to effectively search and locate relevant entities through powerful multi-hop reasoning.Experimental results on real-world benchmarks demonstrate that our approach significantly enhances accuracy in code localization.Notably, our method with the fine-tuned Qwen-2.5-Coder-Instruct-32B model achieves comparable results to SOTA proprietary models at greatly reduced cost (approximately 86% reduction), reaching up to 92.7% accuracy on file-level localization while improving downstream GitHub issue resolution success rates by 12% for multiple attempts (Pass@10). Our code is available at <url>https://github.com/gersteinlab/LocAgent</url>.</abstract>
      <url hash="9f751217">2025.acl-long.426</url>
      <bibkey>chen-etal-2025-locagent</bibkey>
    </paper>
    <paper id="427">
      <title><fixed-case>COSMMIC</fixed-case>: Comment-Sensitive Multimodal Multilingual <fixed-case>I</fixed-case>ndian Corpus for Summarization and Headline Generation</title>
      <author><first>Raghvendra</first><last>Kumar</last></author>
      <author><first>Mohammed Salman S</first><last>A</last></author>
      <author><first>Aryan</first><last>Sahu</last></author>
      <author><first>Tridib</first><last>Nandi</last></author>
      <author><first>Pragathi Y</first><last>P</last></author>
      <author><first>Sriparna</first><last>Saha</last><affiliation>Indian Institute of Technology Patna, India</affiliation></author>
      <author><first>Jose G</first><last>Moreno</last><affiliation>Université Paul Sabatier / Université de Toulouse III</affiliation></author>
      <pages>8728-8748</pages>
      <abstract>Despite progress in comment-aware multimodal and multilingual summarization for English and Chinese, research in Indian languages remains limited. This study addresses this gap by introducing COSMMIC, a pioneering comment-sensitive multimodal, multilingual dataset featuring nine major Indian languages. COSMMIC comprises 4,959 article-image pairs and 24,484 reader comments, with ground-truth summaries available in all included languages. Our approach enhances summaries by integrating reader insights and feedback. We explore summarization and headline generation across four configurations: (1) using article text alone, (2) incorporating user comments, (3) utilizing images, and (4) combining text, comments, and images. To assess the dataset’s effectiveness, we employ state-of-the-art language models such as LLama3 and GPT-4. We conduct a comprehensive study to evaluate different component combinations, including identifying supportive comments, filtering out noise using a dedicated comment classifier using IndicBERT, and extracting valuable insights from images with a multilingual CLIP-based classifier. This helps determine the most effective configurations for natural language generation (NLG) tasks. Unlike many existing datasets that are either text-only or lack user comments in multimodal settings, COSMMIC uniquely integrates text, images, and user feedback. This holistic approach bridges gaps in Indian language resources, advancing NLP research and fostering inclusivity.</abstract>
      <url hash="70ff99d6">2025.acl-long.427</url>
      <bibkey>kumar-etal-2025-cosmmic</bibkey>
    </paper>
    <paper id="428">
      <title>Mind the Gap: Static and Interactive Evaluations of Large Audio Models</title>
      <author><first>Minzhi</first><last>Li</last></author>
      <author><first>William Barr</first><last>Held</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Michael J</first><last>Ryan</last><affiliation>Stanford University</affiliation></author>
      <author><first>Kunat</first><last>Pipatanakul</last><affiliation>SCB 10X</affiliation></author>
      <author><first>Potsawee</first><last>Manakul</last><affiliation>SCB 10X</affiliation></author>
      <author><first>Hao</first><last>Zhu</last><affiliation>Stanford University</affiliation></author>
      <author><first>Diyi</first><last>Yang</last><affiliation>Stanford University</affiliation></author>
      <pages>8749-8766</pages>
      <abstract>As AI chatbots become ubiquitous, voice interaction presents a compelling way to enable rapid, high-bandwidth communication for both semantic and social signals. This has driven research into Large Audio Models (LAMs) to power voice-native experiences. However, aligning LAM development with user goals requires a clear understanding of user needs and preferences to establish reliable progress metrics. This study addresses these challenges by introducing an interactive approach to evaluate LAMs and collecting 7,500 LAM interactions from 484 participants. Through topic modeling of user queries, we identify primary use cases for audio interfaces. We then analyze user preference rankings and qualitative feedback to determine which models best align with user needs. Finally, we evaluate how static benchmarks predict interactive performance - our analysis reveals no individual benchmark strongly correlates with interactive results (<tex-math>\tau \leq 0.33</tex-math> for all benchmarks). While combining multiple coarse-grained features yields modest predictive power (<tex-math>R^2</tex-math>=0.30), only two out of twenty datasets on spoken question answering and age prediction show significantly positive correlations. This suggests a clear need to develop LAM evaluations that better correlate with user preferences.</abstract>
      <url hash="ace37d94">2025.acl-long.428</url>
      <bibkey>li-etal-2025-mind</bibkey>
    </paper>
    <paper id="429">
      <title>Understanding In-Context Machine Translation for Low-Resource Languages: A Case Study on <fixed-case>M</fixed-case>anchu</title>
      <author><first>Renhao</first><last>Pei</last></author>
      <author><first>Yihong</first><last>Liu</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Peiqin</first><last>Lin</last><affiliation>Institut für Informatik</affiliation></author>
      <author><first>François</first><last>Yvon</last><affiliation>ISIR, Sorbonne Université &amp; CNRS</affiliation></author>
      <author><first>Hinrich</first><last>Schuetze</last></author>
      <pages>8767-8788</pages>
      <abstract>In-context machine translation (MT) with large language models (LLMs) is a promising approach for low-resource MT, as it can readily take advantage of linguistic resources such as grammar books and dictionaries.Such resources are usually selectively integrated into the prompt so that LLMs can directly perform translation without any specific training, via their in-context learning capability (ICL).However, the relative importance of each type of resource, e.g., dictionary, grammar book, and retrieved parallel examples, is not entirely clear.To address this gap, this study systematically investigates how each resource and its quality affect the translation performance, with the Manchu language as our case study. To remove any prior knowledge of Manchu encoded in the LLM parameters and single out the effect of ICL, we also experiment with an enciphered version of Manchu texts.Our results indicate that high-quality dictionaries and good parallel examples are very helpful, while grammars hardly help.In a follow-up study, we showcase a promising application of in-context MT: parallel data augmentation as a way to bootstrap a conventional MT model. When monolingual data abound, generating synthetic parallel data through in-context MT offers a pathway to mitigate data scarcity and build effective and efficient low-resource neural MT systems.</abstract>
      <url hash="b134bd6f">2025.acl-long.429</url>
      <bibkey>pei-etal-2025-understanding</bibkey>
    </paper>
    <paper id="430">
      <title><fixed-case>CK</fixed-case>now<fixed-case>E</fixed-case>dit: A New <fixed-case>C</fixed-case>hinese Knowledge Editing Dataset for Linguistics, Facts, and Logic Error Correction in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Jizhan</first><last>Fang</last></author>
      <author><first>Tianhe</first><last>Lu</last></author>
      <author><first>Yunzhi</first><last>Yao</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Ziyan</first><last>Jiang</last></author>
      <author><first>Xin</first><last>Xu</last></author>
      <author><first>Huajun</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Ningyu</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <pages>8789-8807</pages>
      <abstract>Chinese, as a linguistic system rich in depth and complexity, is characterized by distinctive elements such as ancient poetry, proverbs, idioms, and other cultural constructs. However, current Large Language Models (LLMs) face limitations in these specialized domains, highlighting the need for the development of comprehensive datasets that can assess, continuously update, and progressively improve these culturally-grounded linguistic competencies through targeted training optimizations. To address this gap, we introduce CKnowEdit, the first-ever Chinese knowledge editing dataset designed to correct linguistic, factual, and logical errors in LLMs. We collect seven types of knowledge from a wide range of sources, including classical texts, idioms, and content from Baidu Tieba Ruozhiba, taking into account the unique polyphony, antithesis, and logical structures inherent in the Chinese language. By analyzing this dataset, we highlight the challenges current LLMs face in mastering Chinese. Furthermore, our evaluation of state-of-the-art knowledge editing techniques reveals opportunities to advance the correction of Chinese knowledge.</abstract>
      <url hash="11d1df3d">2025.acl-long.430</url>
      <bibkey>fang-etal-2025-cknowedit</bibkey>
    </paper>
    <paper id="431">
      <title><fixed-case>T</fixed-case>riple<fixed-case>F</fixed-case>act: Defending Data Contamination in the Evaluation of <fixed-case>LLM</fixed-case>-driven Fake News Detection</title>
      <author><first>Cheng</first><last>Xu</last></author>
      <author><first>Nan</first><last>Yan</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <pages>8808-8823</pages>
      <abstract>The proliferation of large language models (LLMs) has introduced unprecedented challenges in fake news detection due to benchmark data contamination (BDC), where evaluation benchmarks are inadvertently memorized during the pre-training, leading to the inflated performance metrics. Traditional evaluation paradigms, reliant on static datasets and closed-world assumptions, fail to account the BDC risk in large-scale pre-training of current LLMs. This paper introduces TripleFact, a novel evaluation framework for fake news detection task, which designed to mitigate BDC risk while prioritizing real-world applicability. TripleFact integrates three components: (1) Human-Adversarial Preference Testing (HAPT) to assess robustness against human-crafted misinformation, (2) Real-Time Web Agent with Asynchronous Validation (RTW-AV) to evaluate temporal generalization using dynamically sourced claims, and (3) Entity-Controlled Virtual Environment (ECVE) to eliminate entity-specific biases. Through experiments on 17 state-of-the-art LLMs, including GPT, LLaMA, and DeepSeek variants, TripleFact demonstrates superior contamination resistance compared to traditional benchmarks. Results reveal that BDC artificially inflates performance by up to 23% in conventional evaluations, while TripleFact Score (TFS) remain stable within 4% absolute error under controlled contamination. The framework’s ability to disentangle genuine detection capabilities from memorization artifacts underscores its potential as a fake news detection benchmark for the LLM era.</abstract>
      <url hash="e2215a59">2025.acl-long.431</url>
      <bibkey>xu-yan-2025-triplefact</bibkey>
    </paper>
    <paper id="432">
      <title>Meaning Beyond Truth Conditions: Evaluating Discourse Level Understanding via Anaphora Accessibility</title>
      <author><first>Xiaomeng</first><last>Zhu</last><affiliation>Yale University</affiliation></author>
      <author><first>Zhenghao</first><last>Zhou</last><affiliation>Yale University</affiliation></author>
      <author><first>Simon</first><last>Charlow</last><affiliation>Yale University</affiliation></author>
      <author><first>Robert</first><last>Frank</last><affiliation>Yale University</affiliation></author>
      <pages>8824-8842</pages>
      <abstract>We present a hierarchy of natural language understanding abilities and argue for the importance of moving beyond assessments of understanding at the lexical and sentence levels to the discourse level. We propose the task of anaphora accessibility as a diagnostic for assessing discourse understanding, and to this end, present an evaluation dataset inspired by theoretical research in dynamic semantics. We evaluate human and LLM performance on our dataset and find that LLMs and humans align on some tasks and diverge on others. Such divergence can be explained by LLMs’ reliance on specific lexical items during language comprehension, in contrast to human sensitivity to structural abstractions.</abstract>
      <url hash="41f33a02">2025.acl-long.432</url>
      <bibkey>zhu-etal-2025-meaning</bibkey>
    </paper>
    <paper id="433">
      <title>Large Language and Reasoning Models are Shallow Disjunctive Reasoners</title>
      <author><first>Irtaza</first><last>Khalid</last><affiliation>Cardiff University</affiliation></author>
      <author><first>Amir Masoud</first><last>Nourollah</last></author>
      <author><first>Steven</first><last>Schockaert</last><affiliation>Cardiff University</affiliation></author>
      <pages>8843-8869</pages>
      <abstract>Large Language Models (LLMs) have been found to struggle with systematic reasoning. Even on tasks where they appear to perform well, their performance often depends on shortcuts, rather than on genuine reasoning abilities, leading them to collapse on out-of-distribution (OOD) examples. Post-training strategies based on reinforcement learning and chain-of-thought prompting have recently been hailed as a step change. However, little is known about the potential of the resulting “Large Reasoning Models” (LRMs) beyond maths and programming-based problem solving, where genuine OOD problems can be sparse. In this paper, we focus on tasks that require systematic relational composition for qualitative spatial and temporal reasoning. The setting allows fine control over problem difficulty to precisely measure OOD generalization. We find that, zero-shot LRMs generally outperform their LLM counterparts in single-path reasoning tasks but struggle in the multi-path setting. Whilst showing comparatively better results, fine-tuned LLMs are also not capable of multi-path generalization. We also provide evidence for the behavioral interpretation for this, i.e., that LRMs are shallow disjunctive reasoners.</abstract>
      <url hash="d9f15a7f">2025.acl-long.433</url>
      <bibkey>khalid-etal-2025-large</bibkey>
    </paper>
    <paper id="434">
      <title>Warmup Generations: A Task-Agnostic Approach for Guiding Sequence-to-Sequence Learning with Unsupervised Initial State Generation</title>
      <author><first>Senyu</first><last>Li</last></author>
      <author><first>Zipeng</first><last>Sun</last></author>
      <author><first>Jiayi</first><last>Wang</last></author>
      <author><first>Xue</first><last>Liu</last><affiliation>McGill University</affiliation></author>
      <author><first>Pontus</first><last>Stenetorp</last><affiliation>University College London</affiliation></author>
      <author><first>Siva</first><last>Reddy</last><affiliation>ServiceNow Inc, Mila, McGill University and Mila, McGill University</affiliation></author>
      <author><first>David Ifeoluwa</first><last>Adelani</last><affiliation>McGill University</affiliation></author>
      <pages>8870-8880</pages>
      <abstract>Traditional supervised fine-tuning (SFT) strategies for sequence-to-sequence tasks often train models to directly generate the target output. Recent work has shown that guiding models with intermediate steps—such as keywords, outlines, or reasoning chains—can significantly improve performance, coherence, and interpretability. However, these methods often depend on predefined intermediate formats and annotated data, limiting their scalability and generalizability. In this work, we introduce a task-agnostic framework that enables models to generate intermediate “warmup” sequences. These warmup sequences, serving as an initial state for subsequent generation, are optimized to enhance the probability of generating the target sequence without relying on external supervision or human-designed structures. Drawing inspiration from reinforcement learning principles, our method iteratively refines these intermediate steps to maximize their contribution to the final output, similar to reward-driven optimization in reinforcement learning with human feedback. Experimental results across tasks such as translation, summarization, and multi-choice question answering for logical reasoning show that our approach outperforms traditional SFT methods, and offers a scalable and flexible solution for sequence-to-sequence tasks.</abstract>
      <url hash="082631a5">2025.acl-long.434</url>
      <bibkey>li-etal-2025-warmup</bibkey>
    </paper>
    <paper id="435">
      <title>Building Better: Avoiding Pitfalls in Developing Language Resources when Data is Scarce</title>
      <author><first>Nedjma</first><last>Ousidhoum</last><affiliation>Cardiff University</affiliation></author>
      <author><first>Meriem</first><last>Beloucif</last><affiliation>Uppsala University</affiliation></author>
      <author><first>Saif M.</first><last>Mohammad</last></author>
      <pages>8881-8894</pages>
      <abstract>Language is a form of symbolic capital that affects people’s lives in many ways (Bourdieu1977,1991). As a powerful means of communication, it reflects identities, cultures, traditions, and societies more broadly. Therefore, data in a given language should be regarded as more than just a collection of tokens. Rigorous data collection and labeling practices are essential for developing more human-centered and socially aware technologies. Although there has been growing interest in under-resourced languages within the NLP community, work in this area faces unique challenges, such as data scarcity and limited access to qualified annotators.In this paper, we collect feedback from individuals directly involved in and impacted by NLP artefacts for medium- and low-resource languages. We conduct both quantitative and qualitative analyses of their responses and highlight key issues related to: (1) data quality, including linguistic and cultural appropriateness; and (2) the ethics of common annotation practices, such as the misuse of participatory research. Based on these findings, we make several recommendations for creating high-quality language artefacts that reflect the cultural milieu of their speakers, while also respecting the dignity and labor of data workers.</abstract>
      <url hash="3fa291c7">2025.acl-long.435</url>
      <bibkey>ousidhoum-etal-2025-building</bibkey>
    </paper>
    <paper id="436">
      <title><fixed-case>BRIGHTER</fixed-case>: <fixed-case>BRI</fixed-case>dging the Gap in Human-Annotated Textual Emotion Recognition Datasets for 28 Languages</title>
      <author><first>Shamsuddeen Hassan</first><last>Muhammad</last><affiliation>Imperial College London and Bayero University, Kano-Nigeria</affiliation></author>
      <author><first>Nedjma</first><last>Ousidhoum</last><affiliation>Cardiff University</affiliation></author>
      <author><first>Idris</first><last>Abdulmumin</last><affiliation>Ahmadu Bello University</affiliation></author>
      <author><first>Jan Philip</first><last>Wahle</last><affiliation>University of Göttingen, Germany</affiliation></author>
      <author><first>Terry</first><last>Ruas</last><affiliation>Georg-August Universität Göttingen</affiliation></author>
      <author><first>Meriem</first><last>Beloucif</last><affiliation>Uppsala University</affiliation></author>
      <author><first>Christine</first><last>de Kock</last></author>
      <author><first>Nirmal</first><last>Surange</last><affiliation>International Institute of Information Technology Hyderabad</affiliation></author>
      <author><first>Daniela</first><last>Teodorescu</last></author>
      <author><first>Ibrahim Said</first><last>Ahmad</last><affiliation>Northeastern University</affiliation></author>
      <author><first>David Ifeoluwa</first><last>Adelani</last><affiliation>McGill University</affiliation></author>
      <author><first>Alham Fikri</first><last>Aji</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Felermino D. M. A.</first><last>Ali</last></author>
      <author><first>Ilseyar</first><last>Alimova</last><affiliation>Kazan Federal University</affiliation></author>
      <author><first>Vladimir</first><last>Araujo</last><affiliation>Sailplane</affiliation></author>
      <author><first>Nikolay</first><last>Babakov</last><affiliation>Univesity of Santiago de Compostela</affiliation></author>
      <author><first>Naomi</first><last>Baes</last></author>
      <author><first>Ana-Maria</first><last>Bucur</last><affiliation>Universita della Svizzera Italiana, Universidad Politécnica de Valencia and University of Bucharest</affiliation></author>
      <author><first>Andiswa</first><last>Bukula</last></author>
      <author><first>Guanqun</first><last>Cao</last></author>
      <author><first>Rodrigo</first><last>Tufiño</last><affiliation>Universidad Politécnica Salesiana</affiliation></author>
      <author><first>Rendi</first><last>Chevi</last></author>
      <author><first>Chiamaka Ijeoma</first><last>Chukwuneke</last><affiliation>Nnamdi Azikiwe University</affiliation></author>
      <author><first>Alexandra</first><last>Ciobotaru</last><affiliation>University of Bucharest</affiliation></author>
      <author><first>Daryna</first><last>Dementieva</last></author>
      <author><first>Murja Sani</first><last>Gadanya</last><affiliation>Bayero University Kano</affiliation></author>
      <author><first>Robert</first><last>Geislinger</last></author>
      <author><first>Bela</first><last>Gipp</last><affiliation>Georg-August Universität Göttingen</affiliation></author>
      <author><first>Oumaima</first><last>Hourrane</last><affiliation>Al Akhawayn University</affiliation></author>
      <author><first>Oana</first><last>Ignat</last><affiliation>Santa Clara University</affiliation></author>
      <author><first>Falalu Ibrahim</first><last>Lawan</last><affiliation>Kaduna State University</affiliation></author>
      <author><first>Rooweither</first><last>Mabuya</last><affiliation>North-West University</affiliation></author>
      <author><first>Rahmad</first><last>Mahendra</last><affiliation>Royal Melbourne Institute of Technology and Universitas Indonesia</affiliation></author>
      <author><first>Vukosi</first><last>Marivate</last><affiliation>University of Pretoria</affiliation></author>
      <author><first>Alexander</first><last>Panchenko</last><affiliation>Skoltech</affiliation></author>
      <author><first>Andrew</first><last>Piper</last><affiliation>McGill University</affiliation></author>
      <author><first>Charles Henrique Porto</first><last>Ferreira</last><affiliation>Centro Universitário FEI</affiliation></author>
      <author><first>Vitaly</first><last>Protasov</last><affiliation>AIRI</affiliation></author>
      <author><first>Samuel</first><last>Rutunda</last></author>
      <author><first>Manish</first><last>Shrivastava</last><affiliation>International Institute of Information Technology Hyderabad, India</affiliation></author>
      <author><first>Aura Cristina</first><last>Udrea</last></author>
      <author><first>Lilian Diana Awuor</first><last>Wanzare</last><affiliation>Maseno University</affiliation></author>
      <author><first>Sophie</first><last>Wu</last></author>
      <author><first>Florian Valentin</first><last>Wunderlich</last><affiliation>Georg-August Universität Göttingen</affiliation></author>
      <author><first>Hanif Muhammad</first><last>Zhafran</last><affiliation>Institut Teknologi Bandung</affiliation></author>
      <author><first>Tianhui</first><last>Zhang</last><affiliation>University of Liverpool</affiliation></author>
      <author><first>Yi</first><last>Zhou</last><affiliation>Cardiff University</affiliation></author>
      <author><first>Saif M.</first><last>Mohammad</last></author>
      <pages>8895-8916</pages>
      <abstract>People worldwide use language in subtle and complex ways to express emotions. Although emotion recognition–an umbrella term for several NLP tasks–impacts various applications within NLP and beyond, most work in this area has focused on high-resource languages. This has led to significant disparities in research efforts and proposed solutions, particularly for under-resourced languages, which often lack high-quality annotated datasets.In this paper, we present BRIGHTER–a collection of multi-labeled, emotion-annotated datasets in 28 different languages and across several domains. BRIGHTER primarily covers low-resource languages from Africa, Asia, Eastern Europe, and Latin America, with instances labeled by fluent speakers. We highlight the challenges related to the data collection and annotation processes, and then report experimental results for monolingual and crosslingual multi-label emotion identification, as well as emotion intensity recognition. We analyse the variability in performance across languages and text domains, both with and without the use of LLMs, and show that the BRIGHTER datasets represent a meaningful step towards addressing the gap in text-based emotion recognition.</abstract>
      <url hash="b829a51e">2025.acl-long.436</url>
      <bibkey>muhammad-etal-2025-brighter</bibkey>
    </paper>
    <paper id="437">
      <title><fixed-case>S</fixed-case>kill<fixed-case>V</fixed-case>erse : Assessing and Enhancing <fixed-case>LLM</fixed-case>s with Tree Evaluation</title>
      <author><first>Yufei</first><last>Tian</last></author>
      <author><first>Jiao</first><last>Sun</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Nanyun</first><last>Peng</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Zizhao</first><last>Zhang</last><affiliation>Google</affiliation></author>
      <pages>8917-8933</pages>
      <abstract>As language models evolve to tackle complex, multifaceted tasks, their evaluation must adapt to capture this intricacy. A granular, skill-specific understanding of model capabilities can empower researchers to make informed model development plans. In this paper, we introduce SkillVerse, an unsupervised tree-structured diagnosis framework for understanding model proficiency in specific abilities. With LLM as a judge, SkillVerse first critiques the model responses, and then organizes them into a hierarchical structure termed dendrogram. Given proficiency at arbitrary levels of granularity, SkillVerse is flexible to produce insights of behaviors of modern large models. We also demonstrate its efficacy in two downstream tasks: 1) improving model in-context learning by 25% using a tree-search algorithm to select more informative few-shot demonstrations, and 2) accurately predicting new model weaknesses with a 55% success rate, 22% higher than without SkillVerse.</abstract>
      <url hash="317e525b">2025.acl-long.437</url>
      <bibkey>tian-etal-2025-skillverse</bibkey>
    </paper>
    <paper id="438">
      <title><fixed-case>C</fixed-case>ypher<fixed-case>B</fixed-case>ench: Towards Precise Retrieval over Full-scale Modern Knowledge Graphs in the <fixed-case>LLM</fixed-case> Era</title>
      <author><first>Yanlin</first><last>Feng</last></author>
      <author><first>Simone</first><last>Papicchio</last></author>
      <author><first>Sajjadur</first><last>Rahman</last></author>
      <pages>8934-8958</pages>
      <abstract>Retrieval from graph data is crucial for augmenting large language models (LLM) with both open-domain knowledge and private enterprise data, and it is also a key component in the recent GraphRAG system (CITATION). Despite decades of research on knowledge graphs and knowledge base question answering, leading LLM frameworks (Langchain and LlamaIndex) have only minimal support for retrieval from modern encyclopedic knowledge graphs like Wikidata. In this paper, we analyze the root cause and suggest that modern RDF knowledge graphs (Wikidata, Freebase) are less efficient for LLMs due to overly large schemas that far exceed the typical LLM context window, use of resource identifiers, overlapping and ambiguous relation types and lack of normalization. As a solution, we propose <i>property graph views</i> on top of the underlying RDF graph that can be efficiently queried by LLMs using <i>Cypher</i>. We instantiated this idea on Wikidata and introduced CypherBench, the first benchmark with 11 large-scale, multi-domain property graphs with 7.8 million entities and over 10,000 questions. To achieve this, we tackled several key challenges, including developing an RDF-to-property graph conversion engine, creating a systematic pipeline for text-to-Cypher task generation, and designing new evaluation metrics.</abstract>
      <url hash="d19e5d4b">2025.acl-long.438</url>
      <bibkey>feng-etal-2025-cypherbench</bibkey>
    </paper>
    <paper id="439">
      <title>Empathy Prediction from Diverse Perspectives</title>
      <author><first>Francine</first><last>Chen</last><affiliation>Toyota Research Institute</affiliation></author>
      <author><first>Scott</first><last>Carter</last><affiliation>Toyota Research Institute</affiliation></author>
      <author><first>Tatiana</first><last>Lau</last><affiliation>Toyota Research Institute</affiliation></author>
      <author><first>Nayeli Suseth</first><last>Bravo</last><affiliation>Toyota Research Institute</affiliation></author>
      <author><first>Sumanta</first><last>Bhattacharyya</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <author><first>Kate</first><last>Sieck</last><affiliation>Toyota Research Institute</affiliation></author>
      <author><first>Charlene C.</first><last>Wu</last><affiliation>Toyota Research Institute</affiliation></author>
      <pages>8959-8974</pages>
      <abstract>A person’s perspective on a topic can influence their empathy towards a story. To investigate the use of personal perspective in empathy prediction, we collected a dataset, EmpathyFromPerspectives, where a user rates their empathy towards a story by a person with a different perspective on a prompted topic. We observed in the dataset that user perspective can be important for empathy prediction and developed a model, PPEP, that uses a rater’s perspective as context for predicting the rater’s empathy towards a story. Experiments comparing PPEP with baseline models show that use of personal perspective significantly improves performance. A user study indicated that human empathy ratings of stories generally agreed with PPEP’s relative empathy rankings.</abstract>
      <url hash="8e930df2">2025.acl-long.439</url>
      <bibkey>chen-etal-2025-empathy</bibkey>
    </paper>
    <paper id="440">
      <title>Are <fixed-case>LLM</fixed-case>s effective psychological assessors? Leveraging adaptive <fixed-case>RAG</fixed-case> for interpretable mental health screening through psychometric practice</title>
      <author><first>Federico</first><last>Ravenda</last></author>
      <author><first>Seyed Ali</first><last>Bahrainian</last></author>
      <author><first>Andrea</first><last>Raballo</last></author>
      <author><first>Antonietta</first><last>Mira</last><affiliation>Università della Svizzera Italiana and Università dell’Insubria</affiliation></author>
      <author><first>Noriko</first><last>Kando</last><affiliation>NII, Tokyo Institute of Technology</affiliation></author>
      <pages>8975-8991</pages>
      <abstract>In psychological practice, standardized questionnaires serve as essential tools for assessing mental health through structured, clinically-validated questions (i.e., items). While social media platforms offer rich data for mental health screening, computational approaches often bypass these established clinical assessment tools in favor of black-box classification. We propose a novel questionnaire-guided screening framework that bridges psychological practice and computational methods through adaptive Retrieval-Augmented Generation (aRAG). Our approach links unstructured social media content and standardized clinical assessments by retrieving relevant posts for each questionnaire item and using Large Language Models (LLMs) to complete validated psychological instruments. Our findings demonstrate two key advantages of questionnaire-guided screening: First, when completing the Beck Depression Inventory-II (BDI-II), our approach matches or outperforms state-of-the-art performance on Reddit-based benchmarks without requiring training data. Second, we show that guiding LLMs through standardized questionnaires yields superior results compared to directly prompting them for depression screening. Additionally, we show as a proof-of-concept how our questionnaire-based methodology successfully extends to self-harm screening.</abstract>
      <url hash="227b3d33">2025.acl-long.440</url>
      <bibkey>ravenda-etal-2025-llms</bibkey>
    </paper>
    <paper id="441">
      <title><fixed-case>INTERACT</fixed-case>: Enabling Interactive, Question-Driven Learning in Large Language Models</title>
      <author><first>Aum</first><last>Kendapadi</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Kerem</first><last>Zaman</last><affiliation>Department of Computer Science, University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Rakesh</first><last>R Menon</last></author>
      <author><first>Shashank</first><last>Srivastava</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <pages>8992-9024</pages>
      <abstract>Large language models (LLMs) excel at answering questions but remain passive learners—absorbing static data without the ability to question and refine knowledge. This paper explores how LLMs can transition to interactive, question-driven learning through student-teacher dialogues. We introduce INTERACT (INTERactive learning for Adaptive Concept Transfer), a framework in which a “student” LLM engages a “teacher” LLM through iterative inquiries to acquire knowledge across 1,347 contexts, including song lyrics, news articles, movie plots, academic papers, and images. Our experiments show that across a wide range of scenarios and LLM architectures, interactive learning consistently enhances performance, achieving up to a 25% improvement, with ‘cold-start’ student models matching static learning baselines in as few as five dialogue turns. Interactive setups can also mitigate the disadvantages of weaker teachers, showcasing the robustness of question-driven learning.</abstract>
      <url hash="0a332d4d">2025.acl-long.441</url>
      <bibkey>kendapadi-etal-2025-interact</bibkey>
    </paper>
    <paper id="442">
      <title>Circuit Stability Characterizes Language Model Generalization</title>
      <author><first>Alan</first><last>Sun</last></author>
      <pages>9025-9040</pages>
      <abstract>Extensively evaluating the capabilities of (large) language models is difficult. Rapid development of state-of-the-art models induce benchmark saturation, while creating more challenging datasets is labor-intensive. Inspired by the recent developments in mechanistic interpretability, we introduce circuit stability as a new way to assess model performance. Circuit stability refers to a model’s ability to apply a consistent reasoning process–its circuit–across various inputs. We mathematically formalize circuit stability and circuit equivalence. Then, through three case studies, we empirically show that circuit stability and the lack thereof can characterize and predict different aspects of generalization. Our proposed methods offer a step towards rigorously relating the generality of models to their interpretability.</abstract>
      <url hash="3e378675">2025.acl-long.442</url>
      <bibkey>sun-2025-circuit</bibkey>
    </paper>
    <paper id="443">
      <title>Comparing <fixed-case>LLM</fixed-case>-generated and human-authored news text using formal syntactic theory</title>
      <author><first>Olga</first><last>Zamaraeva</last><affiliation>Universidad de La Coruña</affiliation></author>
      <author><first>Dan</first><last>Flickinger</last></author>
      <author><first>Francis</first><last>Bond</last><affiliation>Palacký University Olomouc</affiliation></author>
      <author><first>Carlos</first><last>Gómez-Rodríguez</last><affiliation>Universidade da Coruña</affiliation></author>
      <pages>9041-9060</pages>
      <abstract>This study provides the first comprehensive comparison of New York Times-style text generated by six large language models against real, human-authored NYT writing. The comparison is based on a formal syntactic theory. We use Head-driven Phrase Structure Grammar (HPSG) to analyze the grammatical structure of the texts. We then investigate and illustrate the differences in the distributions of HPSG grammar types, revealing systematic distinctions between human and LLM-generated writing. These findings contribute to a deeper understanding of the syntactic behavior of LLMs as well as humans, within the NYT genre.</abstract>
      <url hash="09803369">2025.acl-long.443</url>
      <bibkey>zamaraeva-etal-2025-comparing</bibkey>
    </paper>
    <paper id="444">
      <title>Improving Preference Extraction In <fixed-case>LLM</fixed-case>s By Identifying Latent Knowledge Through Classifying Probes</title>
      <author><first>Sharan</first><last>Maiya</last></author>
      <author><first>Yinhong</first><last>Liu</last></author>
      <author><first>Ramit</first><last>Debnath</last></author>
      <author><first>Anna</first><last>Korhonen</last><affiliation>University of Cambridge</affiliation></author>
      <pages>9061-9081</pages>
      <abstract>Large Language Models (LLMs) are often used as automated judges to evaluate text, but their effectiveness can be hindered by various unintentional biases. We propose using linear classifying probes, trained by leveraging differences between contrasting pairs of prompts, to directly access LLMs’ latent knowledge and extract more accurate preferences. Through extensive experiments using models of varying size from four different families and six diverse datasets assessing text quality evaluation and common sense reasoning, we demonstrate that both supervised and unsupervised probing approaches consistently outperform traditional generation-based judgement while maintaining similar computational costs. These probes generalise under domain shifts and can even outperform finetuned evaluators with the same training data size. Our results suggest linear probing offers an accurate, robust and computationally efficient approach for LLM-as-judge tasks while providing interpretable insights into how models encode judgement-relevant knowledge. Our data and code will be openly released in the future.</abstract>
      <url hash="9f80ea66">2025.acl-long.444</url>
      <bibkey>maiya-etal-2025-improving</bibkey>
    </paper>
    <paper id="445">
      <title>White Men Lead, Black Women Help? Benchmarking and Mitigating Language Agency Social Biases in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Yixin</first><last>Wan</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Kai-Wei</first><last>Chang</last><affiliation>University of California, Los Angeles and Amazon</affiliation></author>
      <pages>9082-9108</pages>
      <abstract>Social biases can manifest in language agency. However, very limited research has investigated such biases in Large Language Model (LLM)-generated content. In addition, previous works often rely on string-matching techniques to identify agentic and communal words within texts, falling short of accurately classifying language agency. We introduce the **Language Agency Bias Evaluation (LABE)** benchmark, which comprehensively evaluates biases in LLMs by analyzing agency levels attributed to different demographic groups in model generations. LABE tests for gender, racial, and intersectional language agency biases in LLMs on 3 text generation tasks: biographies, professor reviews, and reference letters. Using LABE, we unveil language agency social biases in 3 recent LLMs: ChatGPT, Llama3, and Mistral. We observe that: (1) LLM generations tend to demonstrate greater gender bias than human-written texts; (2) Models demonstrate remarkably higher levels of intersectional bias than the other bias aspects. (3) Prompt-based mitigation is unstable and frequently leads to bias exacerbation. Based on our observations, we propose **Mitigation via Selective Rewrite (MSR)**, a novel bias mitigation strategy that leverages an agency classifier to identify and selectively revise parts of generated texts that demonstrate communal traits. Empirical results prove MSR to be more effective and reliable than prompt-based mitigation method, showing a promising research direction.</abstract>
      <url hash="385a0b3f">2025.acl-long.445</url>
      <bibkey>wan-chang-2025-white</bibkey>
    </paper>
    <paper id="446">
      <title><fixed-case>AIMSC</fixed-case>heck: Leveraging <fixed-case>LLM</fixed-case>s for <fixed-case>AI</fixed-case>-Assisted Review of Modern Slavery Statements Across Jurisdictions</title>
      <author><first>Adriana Eufrosina</first><last>Bora</last><affiliation>Mila - Quebec Artificial Intelligence Institute</affiliation></author>
      <author><first>Akshatha</first><last>Arodi</last><affiliation>Mila - Quebec Artificial Intelligence Institute</affiliation></author>
      <author><first>Duoyi</first><last>Zhang</last></author>
      <author><first>Jordan</first><last>Bannister</last><affiliation>Mila - Quebec Artificial Intelligence Institute</affiliation></author>
      <author><first>Mirko</first><last>Bronzi</last><affiliation>Mila - Quebec Artificial Intelligence Institute</affiliation></author>
      <author><first>Arsene</first><last>Fansi Tchango</last><affiliation>Montreal Institute of Learning Algorithms</affiliation></author>
      <author><first>Md Abul</first><last>Bashar</last><affiliation>Queensland University of Technology</affiliation></author>
      <author><first>Richi</first><last>Nayak</last><affiliation>Queensland University of Technology</affiliation></author>
      <author><first>Kerrie</first><last>Mengersen</last></author>
      <pages>9109-9135</pages>
      <abstract>Modern Slavery Acts mandate that corporations disclose their efforts to combat modern slavery, aiming to enhance transparency and strengthen practices for its eradication. However, verifying these statements remains challenging due to their complex, diversified language and the sheer number of statements that must be reviewed. The development of NLP tools to assist in this task is also difficult due to a scarcity of annotated data. Furthermore, as modern slavery transparency legislation has been introduced in several countries, the generalizability of such tools across legal jurisdictions must be studied. To address these challenges, we work with domain experts to make two key contributions. First, we present AIMS.uk and AIMS.ca, newly annotated datasets from the UK and Canada to enable cross-jurisdictional evaluation. Second, we introduce AIMSCheck, an end-to-end framework for compliance validation. AIMSCheck decomposes the compliance assessment task into three levels, enhancing interpretability and practical applicability. Our experiments show that models trained on an Australian dataset generalize well across UK and Canadian jurisdictions, demonstrating the potential for broader application in compliance monitoring. We release the benchmark datasets and AIMSCheck to the public to advance AI-adoption in compliance assessment and drive further research in this field.</abstract>
      <url hash="77a5cda9">2025.acl-long.446</url>
      <bibkey>bora-etal-2025-aimscheck</bibkey>
    </paper>
    <paper id="447">
      <title>Collapse of Dense Retrievers: Short, Early, and Literal Biases Outranking Factual Evidence</title>
      <author><first>Mohsen</first><last>Fayyaz</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Ali</first><last>Modarressi</last><affiliation>Center for Information and Language Processing, LMU Munich</affiliation></author>
      <author><first>Hinrich</first><last>Schuetze</last></author>
      <author><first>Nanyun</first><last>Peng</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>9136-9152</pages>
      <abstract>Dense retrieval models are commonly used in Information Retrieval (IR) applications, such as Retrieval-Augmented Generation (RAG). Since they often serve as the first step in these systems, their robustness is critical to avoid downstream failures. In this work, we repurpose a relation extraction dataset (e.g., Re-DocRED) to design controlled experiments that quantify the impact of heuristic biases, such as a preference for shorter documents, on retrievers like Dragon+ and Contriever. We uncover major vulnerabilities, showing retrievers favor shorter documents, early positions, repeated entities, and literal matches, all while ignoring the answer’s presence! Notably, when multiple biases combine, models exhibit catastrophic performance degradation, selecting the answer-containing document in less than 10% of cases over a synthetic biased document without the answer. Furthermore, we show that these biases have direct consequences for downstream applications like RAG, where retrieval-preferred documents can mislead LLMs, resulting in a 34% performance drop than providing no documents at all.https://huggingface.co/datasets/mohsenfayyaz/ColDeR</abstract>
      <url hash="c823b8e6">2025.acl-long.447</url>
      <bibkey>fayyaz-etal-2025-collapse</bibkey>
    </paper>
    <paper id="448">
      <title><fixed-case>S</fixed-case>elf<fixed-case>E</fixed-case>licit: Your Language Model Secretly Knows Where is the Relevant Evidence</title>
      <author><first>Zhining</first><last>Liu</last></author>
      <author><first>Rana Ali</first><last>Amjad</last><affiliation>Amazon</affiliation></author>
      <author><first>Ravinarayana</first><last>Adkathimar</last><affiliation>Facebook</affiliation></author>
      <author><first>Tianxin</first><last>Wei</last></author>
      <author><first>Hanghang</first><last>Tong</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <pages>9153-9173</pages>
      <abstract>Providing Language Models (LMs) with relevant evidence in the context (either via retrieval or user-provided) can significantly improve their ability to provide better-grounded responses. However, recent studies have found that LMs often struggle to fully comprehend and utilize key evidence from the context, especially when it contains noise and irrelevant information—an issue common in real-world scenarios.To address this, we propose SelfElicit, an inference-time approach that helps LMs focus on key contextual evidence through self-guided explicit highlighting.By leveraging the inherent evidence-finding capabilities of LMs using the attention scores of deeper layers, our method automatically identifies and emphasizes key evidence within the input context, facilitating more accurate and grounded responses without additional training or iterative prompting.We demonstrate that SelfElicit brings consistent and significant improvement on multiple evidence-based QA tasks for various LM families while maintaining computational efficiency.Our code and documentation are available at https://github.com/ZhiningLiu1998/SelfElicit.</abstract>
      <url hash="d4da6d81">2025.acl-long.448</url>
      <bibkey>liu-etal-2025-selfelicit</bibkey>
    </paper>
    <paper id="449">
      <title>The Male <fixed-case>CEO</fixed-case> and the Female Assistant: Evaluation and Mitigation of Gender Biases in Text-To-Image Generation of Dual Subjects</title>
      <author><first>Yixin</first><last>Wan</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Kai-Wei</first><last>Chang</last><affiliation>University of California, Los Angeles and Amazon</affiliation></author>
      <pages>9174-9190</pages>
      <abstract>Recent large-scale T2I models like DALLE-3 have made progress in reducing gender stereotypes when generating single-person images. However, significant biases remain when generating images with more than one person. To systematically evaluate this, we propose the **Paired Stereotype Test (PST)** framework, which queries T2I models to depict two individuals assigned with male-stereotyped and female-stereotyped social identities, respectively (e.g. “a CEO” and “an Assistant”). This contrastive setting often triggers T2I models to generate gender-stereotyped images. Using PST, we evaluate two aspects of gender biases – the well-known **bias in gendered occupation** and a novel aspect: **bias in organizational power**. Experiments show that **over 74% images generated by DALLE-3 display gender-occupational biases**. Additionally, compared to single-person settings, DALLE-3 is more likely to perpetuate male-associated stereotypes under PST. We further propose **FairCritic**, a novel and interpretable framework that leverages an LLM-based critic model to i) detect bias in generated images, and ii) adaptively provide feedback to T2I models for improving fairness. FairCritic achieves near-perfect fairness on PST, overcoming the limitations of previous prompt-based intervention approaches.</abstract>
      <url hash="9caf9c91">2025.acl-long.449</url>
      <bibkey>wan-chang-2025-male</bibkey>
    </paper>
    <paper id="450">
      <title>Mitigating Shortcut Learning with <fixed-case>I</fixed-case>nterpo<fixed-case>L</fixed-case>ated Learning</title>
      <author><first>Michalis</first><last>Korakakis</last></author>
      <author><first>Andreas</first><last>Vlachos</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Adrian</first><last>Weller</last><affiliation>Alan Turing Institute and University of Cambridge</affiliation></author>
      <pages>9191-9206</pages>
      <abstract>Empirical risk minimization (ERM) incentivizes models to exploit shortcuts, i.e., spurious correlations between input attributes and labels that are prevalent in the majority of the training data but unrelated to the task at hand. This reliance hinders generalization on minority examples, where such correlations do not hold. Existing shortcut mitigation approaches are model-specific, difficult to tune, computationally expensive, and fail to improve learned representations. To address these issues, we propose InterpoLated Learning (InterpoLL) which interpolates the representations of majority examples to include features from intra-class minority examples with shortcut-mitigating patterns. This weakens shortcut influence, enabling models to acquire features predictive across both minority and majority examples. Experimental results on multiple natural language understanding tasks demonstrate that InterpoLL improves minority generalization over both ERM and state-of-the-art mitigation methods, without compromising accuracy on majority examples. Notably, these gains persist across encoder, encoder-decoder, and decoder-only architectures, demonstrating the method’s broad applicability.</abstract>
      <url hash="82c26388">2025.acl-long.450</url>
      <bibkey>korakakis-etal-2025-mitigating</bibkey>
    </paper>
    <paper id="451">
      <title>Toward Automatic Discovery of a Canine Phonetic Alphabet</title>
      <author><first>Theron S.</first><last>Wang</last></author>
      <author><first>Xingyuan</first><last>Li</last></author>
      <author><first>Hridayesh</first><last>Lekhak</last></author>
      <author><first>Tuan Minh</first><last>Dang</last><affiliation>University of Texas at Arlington, University of Texas at Arlington</affiliation></author>
      <author><first>Mengyue</first><last>Wu</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Kenny Q.</first><last>Zhu</last><affiliation>University of Texas at Arlington</affiliation></author>
      <pages>9207-9219</pages>
      <abstract>Dogs communicate intelligently but little is known about the phonetic properties of their vocalization communication. For the first time, this paper presents an iterative algorithm inspired by human phonetic discovery, which is based on minimal pairs that determine phonemes by distinguishing different words in human language, and is able to produce a complete alphabet of distinct canine phoneme-like units. In addition, the algorithm produces a number of canine repeated acoustic units, which may correspond to specific environments and activities of a dog, composed exclusively of the canine phoneme-like units in the alphabet. The framework outlined in this paper is expected to function not only on canines but other animal species.</abstract>
      <url hash="238005da">2025.acl-long.451</url>
      <bibkey>wang-etal-2025-toward</bibkey>
    </paper>
    <paper id="452">
      <title><fixed-case>D</fixed-case>av<fixed-case>IR</fixed-case>: Data Selection via Implicit Reward for Large Language Models</title>
      <author><first>Haotian</first><last>Zhou</last></author>
      <author><first>Tingkai</first><last>Liu</last><affiliation>Cold Spring Harbor Laboratory</affiliation></author>
      <author><first>Qianli</first><last>Ma</last></author>
      <author><first>Yufeng</first><last>Zhang</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Jianbo</first><last>Yuan</last><affiliation>Amazon</affiliation></author>
      <author><first>Pengfei</first><last>Liu</last></author>
      <author><first>Yang</first><last>You</last></author>
      <author><first>Hongxia</first><last>Yang</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <pages>9220-9237</pages>
      <abstract>We introduce DavIR, a model-based data selection method for post-training Large Language Models. DavIR generalizes Reducible Holdout Loss to core-set selection problem of causal language modeling, and quantifies the learnability of a given datum with respect to a pre-trained LLM based on relative reduction in loss during fine-tuning, a metric we show to be closely related to the implicit reward model described in Direct Preference Optimization (DPO). We show that 6% of Alpaca dataset selected with DavIR can steer both the LLaMA and Gemma model family to produce superior performance compared to the same models trained on the full 52K dataset. We also show that Alpaca dataset compressed with DavIR can be combined with GSM8K dataset to effectively balance open-domain freeform QA and mathematical reasoning capabilities. Finally, we apply the DavIR objective to DPO and develop a normalized DavIR-DPO objective which improves alignment performance of Zephyr-7B-SFT model by 8% (relative) on AlpacaEval, compared against training on vanilla DPO objective.</abstract>
      <url hash="f0b42cce">2025.acl-long.452</url>
      <bibkey>zhou-etal-2025-davir</bibkey>
    </paper>
    <paper id="453">
      <title>Byte Latent Transformer: Patches Scale Better Than Tokens</title>
      <author><first>Artidoro</first><last>Pagnoni</last><affiliation>Facebook and University of Washington</affiliation></author>
      <author><first>Ramakanth</first><last>Pasunuru</last></author>
      <author><first>Pedro</first><last>Rodriguez</last><affiliation>Meta FAIR</affiliation></author>
      <author><first>John</first><last>Nguyen</last><affiliation>Facebook</affiliation></author>
      <author><first>Benjamin</first><last>Muller</last><affiliation>Facebook and Meta</affiliation></author>
      <author><first>Margaret</first><last>Li</last><affiliation>University of Washington</affiliation></author>
      <author><first>Chunting</first><last>Zhou</last><affiliation>Meta AI</affiliation></author>
      <author><first>Lili</first><last>Yu</last><affiliation>physical intelligence</affiliation></author>
      <author><first>Jason E</first><last>Weston</last><affiliation>New York University and Facebook</affiliation></author>
      <author><first>Luke</first><last>Zettlemoyer</last><affiliation>University of Washington, Facebook and Meta</affiliation></author>
      <author><first>Gargi</first><last>Ghosh</last><affiliation>Meta AI</affiliation></author>
      <author><first>Mike</first><last>Lewis</last><affiliation>Facebook AI Research</affiliation></author>
      <author><first>Ari</first><last>Holtzman</last><affiliation>, University of Chicago</affiliation></author>
      <author><first>Srini</first><last>Iyer</last><affiliation>Facebook</affiliation></author>
      <pages>9238-9258</pages>
      <abstract>We introduce the Byte Latent Transformer (BLT), a new byte-level LLM architecture that, for the first time, matches tokenization-based LLM performance at scale with significant improvements in inference efficiency and robustness. BLT encodes bytes into dynamically sized patches, which serve as the primary units of computation. Patches are segmented based on the entropy of the next byte, allocating more compute and model capacity where increased data complexity demands it. We present the first FLOP controlled scaling study of byte-level models – up to 8B parameters and 4T training bytes – demonstrating the feasibility of scaling models trained on raw bytes without a fixed vocabulary. Both training and inference efficiency improve due to dynamically selecting long patches when data is predictable, along with qualitative improvements on reasoning and long tail generalization. For fixed inference costs, BLT shows significantly better scaling than tokenization-based models, by simultaneously growing both patch and model size.</abstract>
      <url hash="a8ad3cfd">2025.acl-long.453</url>
      <bibkey>pagnoni-etal-2025-byte</bibkey>
    </paper>
    <paper id="454">
      <title><fixed-case>D</fixed-case>iffuse<fixed-case>D</fixed-case>ef: Improved Robustness to Adversarial Attacks via Iterative Denoising</title>
      <author><first>Zhenhao</first><last>Li</last></author>
      <author><first>Huichi</first><last>Zhou</last></author>
      <author><first>Marek</first><last>Rei</last><affiliation>Imperial College London</affiliation></author>
      <author><first>Lucia</first><last>Specia</last><affiliation>Imperial College London</affiliation></author>
      <pages>9259-9274</pages>
      <abstract>Pretrained language models have significantly advanced performance across various natural language processing tasks. However, adversarial attacks continue to pose a critical challenge to system built using these models, as they can be exploited with carefully crafted adversarial texts. Inspired by the ability of diffusion models to predict and reduce noise in computer vision, we propose a novel and flexible adversarial defense method for language classification tasks, DiffuseDef, which incorporates a diffusion layer as a denoiser between the encoder and the classifier. The diffusion layer is trained on top of the existing classifier, ensuring seamless integration with any model in a plug-and-play manner. During inference, the adversarial hidden state is first combined with sampled noise, then denoised iteratively and finally ensembled to produce a robust text representation. By integrating adversarial training, denoising, and ensembling techniques, we show that DiffuseDef improves over existing adversarial defense methods and achieves state-of-the-art performance against common black-box and white-box adversarial attacks.</abstract>
      <url hash="1fad6d1a">2025.acl-long.454</url>
      <bibkey>li-etal-2025-diffusedef</bibkey>
    </paper>
    <paper id="455">
      <title>Identifying Cellular Niches in Spatial Transcriptomics: An Investigation into the Capabilities of Large Language Models</title>
      <author><first>Huanhuan</first><last>Wei</last><affiliation>Yale University</affiliation></author>
      <author><first>Xiao</first><last>Luo</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Hongyi</first><last>Yu</last></author>
      <author><first>Jinping</first><last>Liang</last><affiliation>Yale University</affiliation></author>
      <author><first>Luning</first><last>Yang</last></author>
      <author><first>Lixing</first><last>Lin</last></author>
      <author><first>Alexandra</first><last>Popa</last><affiliation>Boehringer Ingelheim</affiliation></author>
      <author><first>Xiting</first><last>Yan</last><affiliation>Yale University</affiliation></author>
      <pages>9275-9289</pages>
      <abstract>Spatial transcriptomic technologies enable measuring gene expression profile and spatial information of cells in tissues simultaneously. Clustering of captured cells/spots in the spatial transcriptomic data is crucial for understanding tissue niches and uncovering disease-related changes.Current methods to cluster spatial transcriptomic data encounter obstacles, including inefficiency in handling multi-replicate data, lack of prior knowledge incorporation, and producing uninterpretable cluster labels.We introduce a novel approach, LLMiniST, to identify spatial niche using a zero-shot large language models (LLMs) by transforming spatial transcriptomic data into spatial context prompts, leveraging gene expression of neighboring cells/spots, cell type composition, tissue information, and external knowledge. The model was further enhanced using a two-stage fine-tuning strategy for improved generalizability. We also develop a user-friendly annotation tool to accelerate the creation of well-annotated spatial dataset for fine-tuning.Comprehensive method performance evaluations showed that both zero-shot and fine-tunned LLMiniST had superior performance than current non-LLM methods in many circumstances. Notably, the two-stage fine-tuning strategy facilitated substantial cross-subject generalizability. The results demonstrate the feasibility of LLMs for tissue niche identification using spatial transcriptomic data and the potential of LLMs as a scalable solution to efficiently integrate minimal human guidance for improved performance in large-scale datasets.</abstract>
      <url hash="8b660108">2025.acl-long.455</url>
      <bibkey>wei-etal-2025-identifying</bibkey>
    </paper>
    <paper id="456">
      <title>Culture Matters in Toxic Language Detection in <fixed-case>P</fixed-case>ersian</title>
      <author><first>Zahra</first><last>Bokaei</last></author>
      <author><first>Walid</first><last>Magdy</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Bonnie</first><last>Webber</last><affiliation>Edinburgh University, University of Edinburgh</affiliation></author>
      <pages>9290-9304</pages>
      <abstract>Toxic language detection is crucial for creating safer online environments and limiting the spread of harmful content. While toxic language detection has been under-explored in Persian, the current work compares different methods for this task, including fine-tuning, data enrichment, zero-shot and few-shot learning, and cross-lingual transfer learning. What is especially compelling is the impact of cultural context on transfer learning for this task: We show that the language of a country with cultural similarities to Persian yields better results in transfer learning. Conversely, the improvement is lower when the language comes from a culturally distinct country.</abstract>
      <url hash="daafc80e">2025.acl-long.456</url>
      <bibkey>bokaei-etal-2025-culture</bibkey>
    </paper>
    <paper id="457">
      <title>Bitnet.cpp: Efficient Edge Inference for Ternary <fixed-case>LLM</fixed-case>s</title>
      <author><first>Jinheng</first><last>Wang</last></author>
      <author><first>Hansong</first><last>Zhou</last></author>
      <author><first>Ting</first><last>Song</last></author>
      <author><first>Shijie</first><last>Cao</last><affiliation>Microsoft Research Asia</affiliation></author>
      <author><first>Yan</first><last>Xia</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Ting</first><last>Cao</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Jianyu</first><last>Wei</last></author>
      <author><first>Shuming</first><last>Ma</last></author>
      <author><first>Hongyu</first><last>Wang</last></author>
      <author><first>Furu</first><last>Wei</last><affiliation>Microsoft Research</affiliation></author>
      <pages>9305-9322</pages>
      <abstract>The advent of 1-bit large language models (LLMs), led by BitNet b1.58, has spurred interest in ternary LLMs. Despite this, research and practical applications focusing on efficient edge inference for ternary LLMs remain scarce. To bridge this gap, we introduce Bitnet.cpp, an inference system optimized for BitNet b1.58 and ternary LLMs. Given that mixed-precision matrix multiplication (mpGEMM) constitutes the bulk of inference time in ternary LLMs, Bitnet.cpp incorporates a novel mpGEMM library to facilitate sub-2-bits-per-weight, efficient and lossless inference. The library features two core solutions: Ternary Lookup Table (TL), which addresses spatial inefficiencies of previous bit-wise methods, and Int2 with a Scale (I2_S), which ensures lossless edge inference, both enabling high-speed inference. Our experiments show that Bitnet.cpp achieves up to a 6.25x increase in speed over full-precision baselines and up to 2.32x over low-bit baselines, setting new benchmarks in the field. Additionally, we expand TL to element-wise lookup table (ELUT) for low-bit LLMs in the appendix, presenting both theoretical and empirical evidence of its considerable potential. Bitnet.cpp is publicly available at https://github.com/microsoft/BitNet/tree/paper, offering a sophisticated solution for the efficient and practical deployment of edge LLMs.</abstract>
      <url hash="9c744a08">2025.acl-long.457</url>
      <bibkey>wang-etal-2025-bitnet</bibkey>
    </paper>
    <paper id="458">
      <title>Instance-Selection-Inspired Undersampling Strategies for Bias Reduction in Small and Large Language Models for Binary Text Classification</title>
      <author><first>Guilherme</first><last>Fonseca</last></author>
      <author><first>Washington</first><last>Cunha</last><affiliation>Universidade Federal de Minas Gerais</affiliation></author>
      <author><first>Gabriel</first><last>Prenassi</last></author>
      <author><first>Marcos André</first><last>Gonçalves</last><affiliation>Universidade Federal de Minas Gerais, Universidade Federal de Minas Gerais</affiliation></author>
      <author><first>Leonardo Chaves Dutra Da</first><last>Rocha</last><affiliation>Universidade Federal de São João del-Rei</affiliation></author>
      <pages>9323-9340</pages>
      <abstract>Skewness in imbalanced datasets affects Automatic Text Classification (ATC), leading to classifier bias toward the majority classes. This work examines undersampling methods to mitigate such bias in Small and Large Language Model (SLMs and LLMs) classifiers. Based on the limitations found in existing solutions, we propose two novel undersampling methods inspired by state-of-the-art Instance Selection techniques, relying on calibrated confidences and semantic difficulty estimates. We compare them against 19 baselines across 13 datasets, evaluating: (i) effectiveness, (ii) class imbalance bias, (iii) efficiency, (iv) scalability, and (v) consistency. Results show our methods uniquely reduce classifier bias (up to 56%) across all datasets without effectiveness loss while improving efficiency (1.6x speedup), scalability and reducing carbon emissions (up to 50%).</abstract>
      <url hash="e34695c3">2025.acl-long.458</url>
      <bibkey>fonseca-etal-2025-instance</bibkey>
    </paper>
    <paper id="459">
      <title>Forward Knows Efficient Backward Path: Saliency-Guided Memory-Efficient Fine-tuning of Large Language Models</title>
      <author><first>Yeachan</first><last>Kim</last></author>
      <author><first>SangKeun</first><last>Lee</last><affiliation>Korea University</affiliation></author>
      <pages>9341-9356</pages>
      <abstract>Fine-tuning is widely recognized as a crucial process for aligning large language models (LLMs) with human intentions. However, the substantial memory requirements associated with fine-tuning pose a significant barrier to extending the applicability of LLMs. While parameter-efficient fine-tuning can be a promising approach by reducing trainable parameters, intermediate activations still need to be cached to compute gradients during the backward pass, thereby limiting overall memory efficiency. In this work, we propose Saliency-Guided Gradient Flow (SAGE), a memory-efficient fine-tuning method designed to minimize the memory specifically associated with cached intermediate activations. The key strategy is to selectively cache activations based on their saliency during the forward pass and then use these activations for the backward pass. This process transforms the dense backward pass into a sparse one, thereby enhancing memory efficiency. To verify whether SAGE can serve as an efficient alternative for fine-tuning, we conduct comprehensive experiments across diverse fine-tuning scenarios and setups. The experimental results show that SAGE substantially improves memory efficiency without a significant loss in accuracy, highlighting its broad value in real-world applications</abstract>
      <url hash="52d99f65">2025.acl-long.459</url>
      <bibkey>kim-lee-2025-forward</bibkey>
    </paper>
    <paper id="460">
      <title>Focus on What Matters: Enhancing Medical Vision-Language Models with Automatic Attention Alignment Tuning</title>
      <author><first>Aofei</first><last>Chang</last></author>
      <author><first>Le</first><last>Huang</last><affiliation>GE HealthCare</affiliation></author>
      <author><first>Alex James</first><last>Boyd</last></author>
      <author><first>Parminder</first><last>Bhatia</last><affiliation>GEHC</affiliation></author>
      <author><first>Taha</first><last>Kass-Hout</last><affiliation>GE HealthCare</affiliation></author>
      <author><first>Cao</first><last>Xiao</last><affiliation>GE Healthcare</affiliation></author>
      <author><first>Fenglong</first><last>Ma</last><affiliation>Pennsylvania State University</affiliation></author>
      <pages>9357-9372</pages>
      <abstract>Medical Large Vision-Language Models (Med-LVLMs) often exhibit suboptimal attention distribution on visual inputs, leading to hallucinated or inaccurate outputs. Existing methods primarily rely on inference-time interventions, which are limited in attention adaptation or require additional supervision. To address this, we propose A<tex-math>^3</tex-math>Tune, a novel fine-tuning framework for Automatic Attention Alignment Tuning. ATune leverages zero-shot weak labels from SAM, refines them into prompt-aware labels using BioMedCLIP, and then selectively modifies visually-critical attention heads to improve alignment while minimizing interference. Additionally, we introduce a A<tex-math>^3</tex-math>MoE module, enabling adaptive parameter selection for attention tuning across diverse prompts and images. Extensive experiments on medical VQA and report generation benchmarks show that A<tex-math>^3</tex-math>Tune outperforms state-of-the-art baselines, achieving enhanced attention distributions and performance in Med-LVLMs.</abstract>
      <url hash="e0f12136">2025.acl-long.460</url>
      <bibkey>chang-etal-2025-focus</bibkey>
    </paper>
    <paper id="461">
      <title><fixed-case>LLM</fixed-case>s + Persona-Plug = Personalized <fixed-case>LLM</fixed-case>s</title>
      <author><first>Jiongnan</first><last>Liu</last></author>
      <author><first>Yutao</first><last>Zhu</last></author>
      <author><first>Shuting</first><last>Wang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Xiaochi</first><last>Wei</last><affiliation>Baidu</affiliation></author>
      <author><first>Erxue</first><last>Min</last></author>
      <author><first>Yu</first><last>Lu</last></author>
      <author><first>Shuaiqiang</first><last>Wang</last></author>
      <author><first>Dawei</first><last>Yin</last><affiliation>Baidu</affiliation></author>
      <author><first>Zhicheng</first><last>Dou</last><affiliation>Renmin University of China</affiliation></author>
      <pages>9373-9385</pages>
      <abstract>Personalization plays a critical role in numerous language tasks and applications, since users with the same requirements may prefer diverse outputs based on their interests. This has led to the development of various personalized approaches aimed at adapting large language models (LLMs) to generate customized outputs aligned with user preferences. Some of them involve fine-tuning a unique personalized LLM for each user, which is too expensive for widespread application. Alternative approaches introduce personalization information in a plug-and-play manner by retrieving the user’s relevant historical texts as demonstrations. However, this retrieval-based strategy may break the continuity of the user history and fail to capture the user’s overall styles and patterns, hence leading to sub-optimal performance. To address these challenges, we propose a novel personalized LLM model, PPlug. It constructs a user-specific embedding for each individual by modeling all her historical contexts through a lightweight plug-in user embedder module. By attaching this embedding to the task input, LLMs can better understand and capture user habits and preferences, thereby producing more personalized outputs without tuning their parameters. Extensive experiments on various tasks in the language model personalization (LaMP) benchmark demonstrate that the proposed model significantly outperforms existing personalized LLM approaches.</abstract>
      <url hash="7b8b7b44">2025.acl-long.461</url>
      <bibkey>liu-etal-2025-llms</bibkey>
    </paper>
    <paper id="462">
      <title>Developmentally-plausible Working Memory Shapes a Critical Period for Language Acquisition</title>
      <author><first>Masato</first><last>Mita</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Ryo</first><last>Yoshida</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Yohei</first><last>Oseki</last><affiliation>University of Tokyo</affiliation></author>
      <pages>9386-9399</pages>
      <abstract>Large language models possess general linguistic abilities but acquire language less efficiently than humans. This study proposes a method for integrating the developmental characteristics of working memory during the critical period, a stage when human language acquisition is particularly efficient, into the training process of language models. The proposed method introduces a mechanism that initially constrains working memory during the early stages of training and gradually relaxes this constraint in an exponential manner as learning progresses. Targeted syntactic evaluation shows that the proposed method outperforms conventional methods without memory constraints or with static memory constraints. These findings not only provide new directions for designing data-efficient language models but also offer indirect evidence supporting the role of the developmental characteristics of working memory as the underlying mechanism of the critical period in language acquisition.</abstract>
      <url hash="464477a3">2025.acl-long.462</url>
      <bibkey>mita-etal-2025-developmentally</bibkey>
    </paper>
    <paper id="463">
      <title><fixed-case>IRIS</fixed-case>: An Iterative and Integrated Framework for Verifiable Causal Discovery in the Absence of Tabular Data</title>
      <author><first>Tao</first><last>Feng</last></author>
      <author><first>Lizhen</first><last>Qu</last><affiliation>Monash University</affiliation></author>
      <author><first>Niket</first><last>Tandon</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Gholamreza</first><last>Haffari</last><affiliation>Monash University, Monash University and Monash University</affiliation></author>
      <pages>9400-9428</pages>
      <abstract>Causal discovery is fundamental to scientific research, yet traditional statistical algorithms face significant challenges, including expensive data collection, redundant computation for known relations, and unrealistic assumptions. While recent LLM-based methods excel at identifying commonly known causal relations, they fail to uncover novel relations. We introduce IRIS (Iterative Retrieval and Integrated System for Real-Time Causal Discovery), a novel framework that addresses these limitations. Starting with a set of initial variables, IRIS automatically collects relevant documents, extracts variables, and uncovers causal relations. Our hybrid causal discovery method combines statistical algorithms and LLM-based methods to discover known and novel causal relations. In addition to causal discovery on initial variables, the missing variable proposal component of IRIS identifies and incorporates missing variables to expand the causal graphs. Our approach enables real-time causal discovery from only a set of initial variables without requiring pre-existing datasets.</abstract>
      <url hash="6799efb5">2025.acl-long.463</url>
      <bibkey>feng-etal-2025-iris</bibkey>
    </paper>
    <paper id="464">
      <title><fixed-case>INJONGO</fixed-case>: A Multicultural Intent Detection and Slot-filling Dataset for 16 <fixed-case>A</fixed-case>frican Languages</title>
      <author><first>Hao</first><last>Yu</last></author>
      <author><first>Jesujoba Oluwadara</first><last>Alabi</last></author>
      <author><first>Andiswa</first><last>Bukula</last></author>
      <author><first>Jian Yun</first><last>Zhuang</last></author>
      <author><first>En-Shiun Annie</first><last>Lee</last></author>
      <author><first>Tadesse Kebede</first><last>Guge</last><affiliation>Haramaya University</affiliation></author>
      <author><first>Israel Abebe</first><last>Azime</last></author>
      <author><first>Happy</first><last>Buzaaba</last><affiliation>Princeton University</affiliation></author>
      <author><first>Blessing Kudzaishe</first><last>Sibanda</last></author>
      <author><first>Godson Koffi</first><last>Kalipe</last></author>
      <author><first>Jonathan</first><last>Mukiibi</last></author>
      <author><first>Salomon</first><last>Kabongo Kabenamualu</last><affiliation>TIB/L3S</affiliation></author>
      <author><first>Mmasibidi</first><last>Setaka</last></author>
      <author><first>Lolwethu</first><last>Ndolela</last></author>
      <author><first>Nkiruka</first><last>Odu</last><affiliation>African University of Science and Technology</affiliation></author>
      <author><first>Rooweither</first><last>Mabuya</last><affiliation>North-West University</affiliation></author>
      <author><first>Shamsuddeen Hassan</first><last>Muhammad</last><affiliation>Imperial College London and Bayero University, Kano-Nigeria</affiliation></author>
      <author><first>Salomey</first><last>Osei</last></author>
      <author><first>Sokhar</first><last>Samb</last></author>
      <author><first>Dietrich</first><last>Klakow</last></author>
      <author><first>David Ifeoluwa</first><last>Adelani</last><affiliation>McGill University</affiliation></author>
      <pages>9429-9452</pages>
      <abstract>Slot-filling and intent detection are well-established tasks in Conversational AI. However, current large-scale benchmarks for these tasks often exclude evaluations of low-resource languages and rely on translations from English benchmarks, thereby predominantly reflecting Western-centric concepts. In this paper, we introduce “INJONGO” - a multicultural, open-source benchmark dataset for 16 African languages with utterances generated by native speakers across diverse domains, including banking, travel, home, and dining. Through extensive experiments, we benchmark fine-tuning multilingual transformer models and prompting large language models (LLMs), and show the advantage of leveraging African-cultural utterances over Western-centric utterances for improving cross-lingual transfer from the English language. Experimental results reveal that current LLMs struggle with the slot-filling task, with GPT-4o achieving an average performance of 26 F1. In contrast, intent detection performance is notably better, with an average accuracy of 70.6%, though it still falls short of fine-tuning baselines. When compared to the English language, GPT-4o and fine-tuning baselines perform similarly on intent detection, achieving an accuracy of approximately 81%. Our findings suggest that LLMs performance is still behind for many low-resource African languages, and more work is needed to further improve their downstream performance.</abstract>
      <url hash="ad6fddd5">2025.acl-long.464</url>
      <bibkey>yu-etal-2025-injongo</bibkey>
    </paper>
    <paper id="465">
      <title>Boosting Long-Context Information Seeking via Query-Guided Activation Refilling</title>
      <author><first>Hongjin</first><last>Qian</last><affiliation>Beijing Academy of Artificial Intelligence</affiliation></author>
      <author><first>Zheng</first><last>Liu</last></author>
      <author><first>Peitian</first><last>Zhang</last></author>
      <author><first>Zhicheng</first><last>Dou</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Defu</first><last>Lian</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>9453-9464</pages>
      <abstract>Processing long contexts poses a significant challenge for large language models (LLMs) due to their inherent context window limitations and the computational burden of extensive key-value (KV) activations, which severely impact efficiency. For information-seeking tasks, full context perception is often unnecessary, as a query’s information needs can dynamically range from localized details to a global perspective, depending on its complexity. However, existing methods struggle to adapt effectively to this dynamic information needs.In the paper, we propose a method for processing long-context information-seeking tasks via query-guided ACtivation REfilling (ACRE). ACRE constructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache compactly captures global information, and the layer-2 (L2) cache provides detailed, localized information. ACRE establishes a proxying relationship between the two caches, allowing the input query to attend to the L1 cache and dynamically refill it with relevant entries from the L2 cache. This mechanism integrates global understanding with query-specific local details, thereby enhancing answer decoding. Experiments on a variety of long-context information-seeking datasets demonstrate ACRE’s effectiveness, achieving significant improvements in both performance and efficiency.</abstract>
      <url hash="f5b0e334">2025.acl-long.465</url>
      <bibkey>qian-etal-2025-boosting</bibkey>
    </paper>
    <paper id="466">
      <title>Efficient Pretraining Data Selection for Language Models via Multi-Actor Collaboration</title>
      <author><first>Tianyi</first><last>Bai</last><affiliation>Shanghai Artificial Intelligence Laboratory and Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Ling</first><last>Yang</last><affiliation>Princeton University</affiliation></author>
      <author><first>Zhen Hao</first><last>Wong</last></author>
      <author><first>Fupeng</first><last>Sun</last></author>
      <author><first>Xinlin</first><last>Zhuang</last></author>
      <author><first>Jiahui</first><last>Peng</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Chi</first><last>Zhang</last></author>
      <author><first>Lijun</first><last>Wu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Qiu</first><last>Jiantao</last><affiliation>shanghai AI lab</affiliation></author>
      <author><first>Wentao</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Binhang</first><last>Yuan</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Conghui</first><last>He</last><affiliation>Shanghai AI Lab</affiliation></author>
      <pages>9465-9491</pages>
      <abstract>Efficient data selection is crucial to accelerate the pretraining of language model (LMs). While various methods have been proposed to enhance data efficiency, limited research has addressed the inherent conflicts between these approaches to achieve optimal data selection for LM pretraining. To tackle this problem, we propose a multi-actor collaborative data selection mechanism. Each data selection method independently prioritizes data based on its specific criterion and updates its prioritization rules using the current state of the model, functioning as an independent actor for data selection. Additionally, a console is designed to adjust the impacts of different actors at various stages and dynamically integrate information from all actors throughout the LM pretraining process. We conduct extensive empirical studies to evaluate our multi-actor framework. The experimental results demonstrate that our approach significantly improves data efficiency, accelerates convergence in LM pretraining, and achieves an average relative performance gain up to 10.5% across multiple language model benchmarks compared to the state-of-the-art methods.</abstract>
      <url hash="32314bbd">2025.acl-long.466</url>
      <bibkey>bai-etal-2025-efficient-pretraining</bibkey>
    </paper>
    <paper id="467">
      <title><fixed-case>A</fixed-case>da<fixed-case>DHP</fixed-case>: Fine-Grained Fine-Tuning via Dual <fixed-case>H</fixed-case>adamard Product and Adaptive Parameter Selection</title>
      <author><first>Han</first><last>Liu</last><affiliation>Dalian University of Technology</affiliation></author>
      <author><first>Changya</first><last>Li</last><affiliation>Dalian University of Technology</affiliation></author>
      <author><first>Xiaotong</first><last>Zhang</last></author>
      <author><first>Feng</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Fenglong</first><last>Ma</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Wei</first><last>Wang</last></author>
      <author><first>Hong</first><last>Yu</last></author>
      <pages>9492-9504</pages>
      <abstract>With the continuously expanding parameters, efficiently adapting large language models to downstream tasks is crucial in resource-limited conditions. Many parameter-efficient fine-tuning methods have emerged to address this challenge. However, they lack flexibility, like LoRA requires manually selecting trainable parameters and rank size, (IA)<tex-math>^{3}</tex-math> can only scale the activations along columns, yielding inferior results due to less precise fine-tuning. To address these issues, we propose a novel method named AdaDHP with fewer parameters and finer granularity, which can adaptively select important parameters for each task. Specifically, we introduce two trainable vectors for each parameter and fine-tune the parameters through Hadamard product along both rows and columns. This significantly reduces the number of trainable parameters, with our parameter count capped at the lower limit of LoRA. Moreover, we design an adaptive parameter selection strategy to select important parameters for downstream tasks dynamically. This allows our method to flexibly remove unimportant parameters for downstream tasks. Finally, we demonstrate the superiority of our method on the T5-base model across 17 NLU tasks and on complex mathematical tasks with the Llama series models.</abstract>
      <url hash="bc4b44ff">2025.acl-long.467</url>
      <bibkey>liu-etal-2025-adadhp</bibkey>
    </paper>
    <paper id="468">
      <title><fixed-case>KG</fixed-case>-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning over Knowledge Graph</title>
      <author><first>Jinhao</first><last>Jiang</last></author>
      <author><first>Kun</first><last>Zhou</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Xin</first><last>Zhao</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Yang</first><last>Song</last><affiliation>BOSS Zhipin</affiliation></author>
      <author><first>Chen</first><last>Zhu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Hengshu</first><last>Zhu</last><affiliation>Computer Network Information Center, Chinese Academy of Sciences</affiliation></author>
      <author><first>Ji-Rong</first><last>Wen</last><affiliation>Renmin University of China</affiliation></author>
      <pages>9505-9523</pages>
      <abstract>In this paper, we aim to improve the reasoning ability of large language models(LLMs) over knowledge graphs(KGs) to answer complex questions. Inspired by existing methods that design the interaction strategy between LLMs and KG, we propose an autonomous LLM-based agent framework, called <b>KG-Agent</b>, which enables a small LLM to actively make decisions until finishing the reasoning process over KGs. In KG-Agent, we integrate the LLM, multifunctional toolbox, KG-based executor, and knowledge memory, and develop an iteration mechanism that autonomously selects the tool and then updates the memory for reasoning over KG. To guarantee the effectiveness, we leverage program language to formulate the multi-hop reasoning process over the KG and synthesize a code-based instruction dataset to fine-tune the base LLM. Extensive experiments demonstrate that only using 10K samples for tuning LLaMA2-7B can outperform competitive methods using larger LLMs or more data, on both in-domain and out-domain datasets. Our code and data will be publicly released.</abstract>
      <url hash="6c295961">2025.acl-long.468</url>
      <bibkey>jiang-etal-2025-kg</bibkey>
    </paper>
    <paper id="469">
      <title>Curriculum Debiasing: Toward Robust Parameter-Efficient Fine-Tuning Against Dataset Biases</title>
      <author><first>Mingyu</first><last>Lee</last></author>
      <author><first>Yeachan</first><last>Kim</last></author>
      <author><first>Wing-Lam</first><last>Mok</last><affiliation>Korea University</affiliation></author>
      <author><first>SangKeun</first><last>Lee</last><affiliation>Korea University</affiliation></author>
      <pages>9524-9540</pages>
      <abstract>Parameter-efficient fine-tuning (PEFT) addresses the memory footprint issue of full fine-tuning by modifying only a subset of model parameters. However, on datasets exhibiting spurious correlations, we observed that PEFT slows down the model’s convergence on unbiased examples, while the convergence on biased examples remains fast. This leads to the model’s overfitting on biased examples, causing significant performance degradation in out-of-distribution (OOD) scenarios. Traditional debiasing methods mitigate this issue by emphasizing unbiased examples during training but often come at the cost of in-distribution (ID) performance drops. To address this trade-off issue, we propose a curriculum debiasing framework that presents examples in a <i>biased-to-unbiased</i> order. Our framework initially limits the model’s exposure to unbiased examples, which are harder to learn, allowing it to first establish a foundation on easier-to-converge biased examples. As training progresses, we gradually increase the proportion of unbiased examples in the training set, guiding the model away from reliance on spurious correlations. Compared to the original PEFT methods, our method accelerates convergence on unbiased examples by approximately twofold and improves ID and OOD performance by 1.2% and 8.0%, respectively.</abstract>
      <url hash="3374a31c">2025.acl-long.469</url>
      <bibkey>lee-etal-2025-curriculum</bibkey>
    </paper>
    <paper id="470">
      <title>Does Context Matter? <fixed-case>C</fixed-case>ontextual<fixed-case>J</fixed-case>udge<fixed-case>B</fixed-case>ench for Evaluating <fixed-case>LLM</fixed-case>-based Judges in Contextual Settings</title>
      <author><first>Austin</first><last>Xu</last><affiliation>Salesforce AI Research</affiliation></author>
      <author><first>Srijan</first><last>Bansal</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Yifei</first><last>Ming</last><affiliation>Salesforce AI Research</affiliation></author>
      <author><first>Semih</first><last>Yavuz</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Shafiq</first><last>Joty</last><affiliation>Nanyang Technological University and SalesForce.com</affiliation></author>
      <pages>9541-9564</pages>
      <abstract>The large language model (LLM)-as-judge paradigm has been used to meet the demand for a cheap, reliable, and fast evaluation of model outputs during AI system development and post-deployment monitoring. While judge models—LLMs finetuned to specialize in assessing and critiquing model outputs—have been touted as general purpose evaluators, they are typically evaluated only on non-contextual scenarios, such as instruction following. The omission of contextual settings—those where external information is used as context to generate an output—is surprising given the increasing prevalence of retrieval-augmented generation (RAG) and summarization use cases. Contextual assessment is uniquely challenging, as evaluation often depends on practitioner priorities, leading to conditional evaluation criteria (e.g., comparing responses based on factuality and then considering completeness if they are equally factual). To address the gap, we propose ContextualJudgeBench, a judge benchmark with 2,000 challenging response pairs across eight splits inspired by real-world contextual evaluation scenarios. We build our benchmark with a multi-pronged data construction pipeline that leverages both existing human annotations and model-based perturbations. Our comprehensive study across 11 judge models and 7 general purpose models, reveals that the contextual information and assessment criteria present a significant challenge to even state-of-the-art models. For example, o1, the best-performing model, barely reaches 55% consistent accuracy.</abstract>
      <url hash="294d7525">2025.acl-long.470</url>
      <bibkey>xu-etal-2025-context</bibkey>
    </paper>
    <paper id="471">
      <title>On the Reliability of Large Language Models for Causal Discovery</title>
      <author><first>Tao</first><last>Feng</last></author>
      <author><first>Lizhen</first><last>Qu</last><affiliation>Monash University</affiliation></author>
      <author><first>Niket</first><last>Tandon</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Zhuang</first><last>Li</last><affiliation>Royal Melbourne Institute of Technology</affiliation></author>
      <author><first>Xiaoxi</first><last>Kang</last></author>
      <author><first>Gholamreza</first><last>Haffari</last><affiliation>Monash University, Monash University and Monash University</affiliation></author>
      <pages>9565-9590</pages>
      <abstract>This study investigates the efficacy of Large Language Models (LLMs) in causal discovery. Using newly available open-source LLMs, OLMo and BLOOM, which provide access to their pre-training corpora, we investigate how LLMs address causal discovery through three research questions. We examine: (i) the impact of memorization for accurate causal relation prediction, (ii) the influence of incorrect causal relations in pre-training data, and (iii) the contextual nuances that influence LLMs’ understanding of causal relations. Our findings indicate that while LLMs are effective in recognizing causal relations that occur frequently in pre-training data, their ability to generalize to new or rare causal relations is limited. Moreover, the presence of incorrect causal relations significantly undermines the confidence of LLMs in corresponding correct causal relations, and the contextual information critically affects the outcomes of LLMs to discern causal connections between random variables.</abstract>
      <url hash="0a6b213d">2025.acl-long.471</url>
      <bibkey>feng-etal-2025-reliability</bibkey>
    </paper>
    <paper id="472">
      <title>Value-Spectrum: Quantifying Preferences of Vision-Language Models via Value Decomposition in Social Media Contexts</title>
      <author><first>Jingxuan</first><last>Li</last></author>
      <author><first>Yuning</first><last>Yang</last><affiliation>FDA</affiliation></author>
      <author><first>Shengqi</first><last>Yang</last><affiliation>Los Alamos National Laboratory</affiliation></author>
      <author><first>Linfan</first><last>Zhang</last></author>
      <author><first>Ying Nian</first><last>Wu</last><affiliation>UCLA</affiliation></author>
      <pages>9591-9610</pages>
      <abstract>The recent progress in Vision-Language Models (VLMs) has broadened the scope of multimodal applications. However, evaluations often remain limited to functional tasks, neglecting abstract dimensions such as personality traits and human values. To address this gap, we introduce Value-Spectrum, a novel Visual Question Answering (VQA) benchmark aimed at assessing VLMs based on Schwartz’s value dimensions that capture core human values guiding people’s preferences and actions. We design a VLM agent pipeline to simulate video browsing and construct a vector database comprising over 50,000 short videos from TikTok, YouTube Shorts, and Instagram Reels. These videos span multiple months and cover diverse topics, including family, health, hobbies, society, technology, etc. Benchmarking on Value-Spectrum highlights notable variations in how VLMs handle value-oriented content. Beyond identifying VLMs’ intrinsic preferences, we also explore the ability of VLM agents to adopt specific personas when explicitly prompted, revealing insights into the adaptability of the model in role-playing scenarios. These findings highlight the potential of Value-Spectrum as a comprehensive evaluation set for tracking VLM preferences in value-based tasks and abilities to simulate diverse personas. The complete code and data are available at https://github.com/Jeremyyny/Value-Spectrum.</abstract>
      <url hash="1040eb62">2025.acl-long.472</url>
      <bibkey>li-etal-2025-value</bibkey>
    </paper>
    <paper id="473">
      <title><fixed-case>T</fixed-case>e<fixed-case>RD</fixed-case>y: Temporal Relation Dynamics through Frequency Decomposition for Temporal Knowledge Graph Completion</title>
      <author><first>Ziyang</first><last>Liu</last></author>
      <author><first>Chaokun</first><last>Wang</last><affiliation>Tsinghua University</affiliation></author>
      <pages>9611-9622</pages>
      <abstract>Temporal knowledge graph completion aims to predict missing facts in a knowledge graph by leveraging temporal information. Existing methods often struggle to capture both the long-term changes and short-term variability of relations, which are crucial for accurate prediction. In this paper, we propose a novel method called TeRDy for temporal knowledge graph completion. TeRDy captures temporal relational dynamics by utilizing time-invariant embeddings, along with long-term temporally dynamic embeddings (e.g., enduring political alliances) and short-term temporally dynamic embeddings (e.g., transient political events). These two types of embeddings are derived from low- and high-frequency components via frequency decomposition. Also, we design temporal smoothing and temporal gradient to seamlessly incorporate timestamp embeddings into relation embeddings. Extensive experiments on benchmark datasets demonstrate that TeRDy outperforms state-of-the-art temporal knowledge graph embedding methods.</abstract>
      <url hash="b389bee8">2025.acl-long.473</url>
      <bibkey>liu-wang-2025-terdy</bibkey>
    </paper>
    <paper id="474">
      <title>Incorporating Domain Knowledge into Materials Tokenization</title>
      <author><first>Yerim</first><last>Oh</last><affiliation>Korea University</affiliation></author>
      <author><first>Jun-Hyung</first><last>Park</last><affiliation>Hankuk University of Foreign Studies</affiliation></author>
      <author><first>Junho</first><last>Kim</last><affiliation>Korea University</affiliation></author>
      <author><first>SungHo</first><last>Kim</last><affiliation>Korea University</affiliation></author>
      <author><first>SangKeun</first><last>Lee</last><affiliation>Korea University</affiliation></author>
      <pages>9623-9644</pages>
      <abstract>While language models are increasingly utilized in materials science, typical models rely on frequency-centric tokenization methods originally developed for natural language processing. However, these methods frequently produce excessive fragmentation and semantic loss, failing to maintain the structural and semantic integrity of material concepts. To address this issue, we propose MATTER, a novel tokenization approach that integrates material knowledge into tokenization. Based on MatDetector trained on our materials knowledge base and re-ranking method prioritizing material terms in token merging, MATTER maintains the structural integrity of identified materials concepts and prevents fragmentation during tokenization, ensuring their semantic meaning remains intact. The experimental results demonstrate that MATTER outperforms existing tokenization methods, achieving an average performance gain of 4% and 2% in the generation and classification tasks, respectively. These results underscore the importance of domain knowledge for tokenization strategies in scientific text processing.</abstract>
      <url hash="7364c04b">2025.acl-long.474</url>
      <bibkey>oh-etal-2025-incorporating</bibkey>
    </paper>
    <paper id="475">
      <title><fixed-case>PIG</fixed-case>: Privacy Jailbreak Attack on <fixed-case>LLM</fixed-case>s via Gradient-based Iterative In-Context Optimization</title>
      <author><first>Yidan</first><last>Wang</last></author>
      <author><first>Yanan</first><last>Cao</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yubing</first><last>Ren</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <author><first>Fang</first><last>Fang</last><affiliation>Institute of Information Engineering,Chinese Academy of Sciences</affiliation></author>
      <author><first>Zheng</first><last>Lin</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <author><first>Binxing</first><last>Fang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>9645-9660</pages>
      <abstract>Large Language Models (LLMs) excel in various domains but pose inherent privacy risks. Existing methods to evaluate privacy leakage in LLMs often use memorized prefixes or simple instructions to extract data, both of which well-alignment models can easily block. Meanwhile, Jailbreak attacks bypass LLM safety mechanisms to generate harmful content, but their role in privacy scenarios remains underexplored. In this paper, we examine the effectiveness of jailbreak attacks in extracting sensitive information, bridging privacy leakage and jailbreak attacks in LLMs. Moreover, we propose PIG, a novel framework targeting Personally Identifiable Information (PII) and addressing the limitations of current jailbreak methods. Specifically, PIG identifies PII entities and their types in privacy queries, uses in-context learning to build a privacy context, and iteratively updates it with three gradient-based strategies to elicit target PII. We evaluate PIG and existing jailbreak methods using two privacy-related datasets. Experiments on four white-box and two black-box LLMs show that PIG outperforms baseline methods and achieves state-of-the-art (SoTA) results. The results underscore significant privacy risks in LLMs, emphasizing the need for stronger safeguards.</abstract>
      <url hash="ada88573">2025.acl-long.475</url>
      <bibkey>wang-etal-2025-pig</bibkey>
    </paper>
    <paper id="476">
      <title>Agents Under Siege: Breaking Pragmatic Multi-Agent <fixed-case>LLM</fixed-case> Systems with Optimized Prompt Attacks</title>
      <author><first>Rana</first><last>Shahroz</last></author>
      <author><first>Zhen</first><last>Tan</last></author>
      <author><first>Sukwon</first><last>Yun</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Charles</first><last>Fleming</last><affiliation>Cisco</affiliation></author>
      <author><first>Tianlong</first><last>Chen</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <pages>9661-9674</pages>
      <abstract>Most discussions about Large Language Model (LLM) safety have focused on single-agent settings but multi-agent LLM systems now create novel adversarial risks because their behavior depends on communication between agents and decentralized reasoning. In this work, we innovatively focus on attacking pragmatic systems that have constrains such as limited token bandwidth, latency between message delivery, and defense mechanisms. We design a <tex-math>\textit{permutation-invariant adversarial attack}</tex-math> that optimizes prompt distribution across latency and bandwidth-constraint network topologies to bypass distributed safety mechanisms within the system. Formulating the attack path as a problem of <tex-math>\textit{maximum-flow minimum-cost}</tex-math>, coupled with the novel <tex-math>\textit{Permutation-Invariant Evasion Loss (PIEL)}</tex-math>, we leverage <tex-math>{graph-based optimization}</tex-math> to maximize attack success rate while minimizing detection risk. Evaluating across models including <tex-math>\texttt{Llama}</tex-math>, <tex-math>\texttt{Mistral}</tex-math>, <tex-math>\texttt{Gemma}</tex-math>, <tex-math>\texttt{DeepSeek}</tex-math> and other variants on various datasets like <tex-math>\texttt{JailBreakBench}</tex-math> and <tex-math>\texttt{AdversarialBench}</tex-math>, our method outperforms conventional attacks by up to <tex-math>7\times</tex-math>, exposing critical vulnerabilities in multi-agent systems. Moreover, we demonstrate that existing defenses, including variants of <tex-math>\texttt{Llama-Guard}</tex-math> and <tex-math>\texttt{PromptGuard}</tex-math>, fail to prohibit our attack, emphasizing the urgent need for multi-agent specific safety mechanisms.</abstract>
      <url hash="054a8dfc">2025.acl-long.476</url>
      <bibkey>shahroz-etal-2025-agents</bibkey>
    </paper>
    <paper id="477">
      <title>Semantic-Eval : A Semantic Comprehension Evaluation Framework for Large Language Models Generation without Training</title>
      <author><first>Shusheng</first><last>Li</last></author>
      <author><first>Jiale</first><last>Li</last></author>
      <author><first>Yifei</first><last>Qu</last></author>
      <author><first>Xinwei</first><last>Shi</last></author>
      <author><first>Yanliang</first><last>Guo</last></author>
      <author><first>Ziyi</first><last>He</last></author>
      <author><first>Yubo</first><last>Wang</last><affiliation>Northeastern University, China</affiliation></author>
      <author><first>Wenjun</first><last>Tan</last><affiliation>Northeastern University</affiliation></author>
      <pages>9675-9690</pages>
      <abstract>With the increasing prominence of large language models (LLMs), evaluating their text-generation capabilities has become an essential research challenge. Although LLM-based evaluation methods exhibit robust performance, the inherent stochastic nature of the LLM generation process introduces a degree of uncertainty in alignment with human preferences. To address this limitation, we propose Semantic-Eval, the first training-free framework designed to assess LLM-generated text based on semantic understanding. This framework computes semantic similarity between pairwise texts to evaluate the interdependence of semantic units, integrating a graph-based weighting mechanism to account for the differential contributions of individual sentences. A pre-trained natural language inference (NLI) model is also incorporated to mitigate potential semantic relationship biases. We evaluate Semantic-Eval across eight datasets that encompass four common NLP tasks. The experimental results indicate that Semantic-Eval surpasses traditional N-gram and BERT-based evaluation metrics, aligning more closely with human judgments and demonstrating a higher correlation than smaller LLMs. However, it slightly lags behind GPT-4. Finally, we demonstrate the effectiveness of Semantic-Eval in evaluating the generation quality of 13 large language models. The code is publicly available at https://github.com/LssTry/Semantic-Eval.</abstract>
      <url hash="996b1d8e">2025.acl-long.477</url>
      <bibkey>li-etal-2025-semantic-eval</bibkey>
    </paper>
    <paper id="478">
      <title>Between Circuits and <fixed-case>C</fixed-case>homsky: Pre-pretraining on Formal Languages Imparts Linguistic Biases</title>
      <author><first>Michael Y.</first><last>Hu</last><affiliation>New York University</affiliation></author>
      <author><first>Jackson</first><last>Petty</last><affiliation>New York University</affiliation></author>
      <author><first>Chuan</first><last>Shi</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>William</first><last>Merrill</last><affiliation>New York University</affiliation></author>
      <author><first>Tal</first><last>Linzen</last><affiliation>New York University and Google</affiliation></author>
      <pages>9691-9709</pages>
      <abstract>Pretraining language models on formal language can improve their acquisition of natural language. Which features of the formal language impart an inductive bias that leads to effective transfer? Drawing on insights from linguistics and complexity theory, we hypothesize that effective transfer occurs when two conditions are met: the formal language should capture the dependency structures present in natural language, and it should remain within the computational limitations of the model architecture. We experiment with pre-pretraining (training on formal language before natural languages) on transformers and find that formal languages capturing hierarchical dependencies indeed enable language models to achieve lower loss on natural language and better linguistic generalization compared to other formal languages. We also find modest support for the hypothesis that the formal language should fall within the computational limitations of the architecture. Strikingly, pre-pretraining reduces loss more efficiently than training on a matched amount of natural language. For a 1B-parameter language model trained on roughly 1.6B tokens of natural language, pre-pretraining achieves the same loss and better linguistic generalization with a 33% smaller token budget. Finally, we also give mechanistic evidence of transfer from formal tonatural language: attention heads acquired during pre-pretraining remain crucial for the model’s performance on syntactic evaluations.</abstract>
      <url hash="f0d7c06b">2025.acl-long.478</url>
      <bibkey>hu-etal-2025-circuits</bibkey>
    </paper>
    <paper id="479">
      <title>When to Speak, When to Abstain: Contrastive Decoding with Abstention</title>
      <author><first>Hyuhng Joon</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Youna</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Sang-goo</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Taeuk</first><last>Kim</last><affiliation>Hanyang University</affiliation></author>
      <pages>9710-9730</pages>
      <abstract>Large Language Models (LLMs) demonstrate exceptional performance across diverse tasks by leveraging pre-trained (i.e., parametric) and external (i.e., contextual) knowledge. While substantial efforts have been made to enhance the utilization of both forms of knowledge, situations in which models lack relevant information remain underexplored. To investigate this challenge, we first present a controlled testbed featuring four distinct knowledge access scenarios, including the aforementioned edge case, revealing that conventional LLM usage exhibits insufficient robustness in handling all instances. Addressing this limitation, we propose Contrastive Decoding with Abstention (CDA), a novel training-free decoding method that allows LLMs to generate responses when relevant knowledge is available and to abstain otherwise. CDA estimates the relevance of both knowledge sources for a given input, adaptively deciding which type of information to prioritize and which to exclude. Through extensive experiments, we demonstrate that CDA can effectively perform accurate generation and abstention simultaneously, enhancing reliability and preserving user trust.</abstract>
      <url hash="a87e86e0">2025.acl-long.479</url>
      <bibkey>kim-etal-2025-speak</bibkey>
    </paper>
    <paper id="480">
      <title>On the Risk of Evidence Pollution for Malicious Social Text Detection in the Era of <fixed-case>LLM</fixed-case>s</title>
      <author><first>Herun</first><last>Wan</last></author>
      <author><first>Minnan</first><last>Luo</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Zhixiong</first><last>Su</last></author>
      <author><first>Guang</first><last>Dai</last><affiliation>SGIT AI</affiliation></author>
      <author><first>Xiang</first><last>Zhao</last><affiliation>National University of Defense Technology</affiliation></author>
      <pages>9731-9761</pages>
      <abstract>Evidence-enhanced detectors present remarkable abilities in identifying malicious social text. However, the rise of large language models (LLMs) brings potential risks of evidence pollution to confuse detectors. This paper explores potential manipulation scenarios including basic pollution, and rephrasing or generating evidence by LLMs. To mitigate the negative impact, we propose three defense strategies from the data and model sides, including machine-generated text detection, a mixture of experts, and parameter updating. Extensive experiments on four malicious social text detection tasks with ten datasets illustrate that evidence pollution significantly compromises detectors, where the generating strategy causes up to a 14.4% performance drop. Meanwhile, the defense strategies could mitigate evidence pollution, but they faced limitations for practical employment. Further analysis illustrates that polluted evidence (i) is of high quality, evaluated by metrics and humans; (ii) would compromise the model calibration, increasing expected calibration error up to 21.6%; and (iii) could be integrated to amplify the negative impact, especially for encoder-based LMs, where the accuracy drops by 21.8%.</abstract>
      <url hash="6143f6b4">2025.acl-long.480</url>
      <bibkey>wan-etal-2025-risk</bibkey>
    </paper>
    <paper id="481">
      <title>Investigating and Extending Homans’ Social Exchange Theory with Large Language Model based Agents</title>
      <author><first>Lei</first><last>Wang</last></author>
      <author><first>Zheqing</first><last>Zhang</last></author>
      <author><first>Xu</first><last>Chen</last><affiliation>Renmin University of China</affiliation></author>
      <pages>9762-9777</pages>
      <abstract>Homans’ Social Exchange Theory (SET) is widely recognized as a basic framework for understanding the formation and emergence of human civilizations and social structures. In social science, this theory is typically studied based on simple simulation experiments or real-world human studies, both of which either lack realism or are too expensive to control. In artificial intelligence, recent advances in large language models (LLMs) have shown promising capabilities in simulating human behaviors. Inspired by these insights, we adopt an interdisciplinary research perspective and propose using LLM-based agents to study Homans’ SET. Specifically, we construct a virtual society composed of three LLM agents and have them engage in a social exchange game to observe their behaviors. Through extensive experiments, we found that Homans’ SET is well validated in our agent society, demonstrating the consistency between the agent and human behaviors. Building on this foundation, we intentionally alter the settings of the agent society to extend the traditional Homans’ SET, making it more comprehensive and detailed. To the best of our knowledge, this paper marks the first step in studying Homans’ SET with LLM-based agents. More importantly, it introduces a novel and feasible research paradigm that bridges the fields of social science and computer science through LLM-based agents. Code is available at https://github.com/Paitesanshi/SET .</abstract>
      <url hash="9d02d863">2025.acl-long.481</url>
      <bibkey>wang-etal-2025-investigating</bibkey>
    </paper>
    <paper id="482">
      <title>A Drop-In Solution for On-the-Fly Adaptation of Speculative Decoding in Large Language Models</title>
      <author><first>Jiesong</first><last>Liu</last><affiliation>North Carolina State University</affiliation></author>
      <author><first>Brian</first><last>Park</last></author>
      <author><first>Xipeng</first><last>Shen</last><affiliation>North Carolina State University</affiliation></author>
      <pages>9778-9794</pages>
      <abstract>Large Language Models (LLMs) are cutting-edge generative AI models built on transformer architecture, which tend to be highly memory-intensive when performing real-time inference. Various strategies have been developed to enhance the end-to-end inference speed for LLMs, one of which is speculative decoding. This technique involves running a smaller LLM (draft model) for inference over a defined window size, denoted as <tex-math>\gamma</tex-math>, while simultaneously being validated by the larger LLM (target model). Choosing the optimal <tex-math>\gamma</tex-math> value and the draft model is essential for unlocking the potential of speculative decoding. But it is difficult to do due to the complicated influence from various factors, including the nature of the task, the hardware in use, and the combination of the large and small models. This paper introduces *on-the-fly adaption of speculative decoding*, a solution that dynamically adapts the choices to maximize the efficiency of speculative decoding for LLM inferences. As a drop-in solution, it needs no offline benchmarking or training. Experiments show that the solution can lead to 3.55-16.48% speed improvement over the standard speculative decoding, and 1.2-3.4<tex-math>\times</tex-math> over the default LLMs.</abstract>
      <url hash="2018f1b7">2025.acl-long.482</url>
      <bibkey>liu-etal-2025-drop</bibkey>
    </paper>
    <paper id="483">
      <title>If Attention Serves as a Cognitive Model of Human Memory Retrieval, What is the Plausible Memory Representation?</title>
      <author><first>Ryo</first><last>Yoshida</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Shinnosuke</first><last>Isono</last><affiliation>National Institute for Japanese Language and Linguistics and University of Tokyo</affiliation></author>
      <author><first>Kohei</first><last>Kajikawa</last><affiliation>National Institute for Japanese Language and Linguistics</affiliation></author>
      <author><first>Taiga</first><last>Someya</last></author>
      <author><first>Yushi</first><last>Sugimoto</last><affiliation>Osaka University</affiliation></author>
      <author><first>Yohei</first><last>Oseki</last><affiliation>University of Tokyo</affiliation></author>
      <pages>9795-9812</pages>
      <abstract>Recent work in computational psycholinguistics has revealed intriguing parallels between attention mechanisms and human memory retrieval, focusing primarily on vanilla Transformers that operate on token-level representations. However, computational psycholinguistic research has also established that syntactic structures provide compelling explanations for human sentence processing that token-level factors cannot fully account for. In this paper, we investigate whether the attention mechanism of Transformer Grammar (TG), which uniquely operates on syntactic structures as representational units, can serve as a cognitive model of human memory retrieval, using Normalized Attention Entropy (NAE) as a linking hypothesis between models and humans. Our experiments demonstrate that TG’s attention achieves superior predictive power for self-paced reading times compared to vanilla Transformer’s, with further analyses revealing independent contributions from both models. These findings suggest that human sentence processing involves dual memory representations—one based on syntactic structures and another on token sequences—with attention serving as the general memory retrieval algorithm, while highlighting the importance of incorporating syntactic structures as representational units.</abstract>
      <url hash="6a944a48">2025.acl-long.483</url>
      <bibkey>yoshida-etal-2025-attention</bibkey>
    </paper>
    <paper id="484">
      <title>Aligning <fixed-case>VLM</fixed-case> Assistants with Personalized Situated Cognition</title>
      <author id="yongqi-li-hk"><first>Yongqi</first><last>Li</last></author>
      <author><first>Shen</first><last>Zhou</last></author>
      <author><first>Xiaohu</first><last>Li</last></author>
      <author><first>Xin</first><last>Miao</last></author>
      <author><first>Jintao</first><last>Wen</last></author>
      <author><first>Mayi</first><last>Xu</last></author>
      <author><first>Jianhao</first><last>Chen</last></author>
      <author><first>Birong</first><last>Pan</last></author>
      <author><first>Hankun</first><last>Kang</last></author>
      <author><first>Yuanyuan</first><last>Zhu</last></author>
      <author><first>Ming</first><last>Zhong</last></author>
      <author><first>Tieyun</first><last>Qian</last><affiliation>Wuhan University</affiliation></author>
      <pages>9813-9839</pages>
      <abstract>Vision-language models (VLMs) aligned with general human objectives, such as being harmless and hallucination-free, have become valuable assistants of humans in managing visual tasks. However, people with diversified backgrounds have different cognition even in the same situation. Consequently, they may have personalized expectations for VLM assistants. This highlights the urgent need to align VLM assistants with personalized situated cognition for real-world assistance. To study this problem, we first simplify it by characterizing individuals based on the sociological concept of Role-Set. Then, we propose to evaluate the individuals’ actions to examine whether the personalized alignment is achieved. Further, we construct a benchmark named PCogAlignBench, which includes 18k instances and 20 individuals with different Role-Sets. Finally, we present a framework called PCogAlign, which constructs a cognition-aware and action-based reward model for personalized alignment. Experimental results and human evaluations demonstrate the reliability of the PCogAlignBench and the effectiveness of our proposed PCogAlign. We will open-source the constructed benchmark and code after being accepted.</abstract>
      <url hash="e8dcb636">2025.acl-long.484</url>
      <bibkey>li-etal-2025-aligning-vlm</bibkey>
    </paper>
    <paper id="485">
      <title>Attention Entropy is a Key Factor: An Analysis of Parallel Context Encoding with Full-attention-based Pre-trained Language Models</title>
      <author><first>Zhisong</first><last>Zhang</last><affiliation>Tencent</affiliation></author>
      <author><first>Yan</first><last>Wang</last><affiliation>Tencent</affiliation></author>
      <author><first>Xinting</first><last>Huang</last><affiliation>Tencent</affiliation></author>
      <author><first>Tianqing</first><last>Fang</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Chenlong</first><last>Deng</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Shuaiyi</first><last>Li</last><affiliation>Chinese University of Hong Kong, The Chinese University of Hong Kong</affiliation></author>
      <author><first>Dong</first><last>Yu</last><affiliation>Tencent AI Lab</affiliation></author>
      <pages>9840-9855</pages>
      <abstract>Large language models have shown remarkable performance across a wide range of language tasks, owing to their exceptional capabilities in context modeling. The most commonly used method of context modeling is full self-attention, as seen in standard decoder-only Transformers. Although powerful, this method can be inefficient for long sequences and may overlook inherent input structures. To address these problems, an alternative approach is parallel context encoding, which splits the context into sub-pieces and encodes them parallelly. Because parallel patterns are not encountered during training, naively applying parallel encoding leads to performance degradation. However, the underlying reasons and potential mitigations are unclear. In this work, we provide a detailed analysis of this issue and identify that unusually high attention entropy can be a key factor. Furthermore, we adopt two straightforward methods to reduce attention entropy by incorporating attention sinks and selective mechanisms. Experiments on various tasks reveal that these methods effectively lower irregular attention entropy and narrow performance gaps. We hope this study can illuminate ways to enhance context modeling mechanisms.</abstract>
      <url hash="c6305bcc">2025.acl-long.485</url>
      <bibkey>zhang-etal-2025-attention</bibkey>
    </paper>
    <paper id="486">
      <title>Faster Speculative Decoding via Effective Draft Decoder with Pruned Candidate Tree</title>
      <author><first>Huanran</first><last>Zheng</last></author>
      <author><first>Xiaoling</first><last>Wang</last><affiliation>East China Normal University</affiliation></author>
      <pages>9856-9868</pages>
      <abstract>Speculative Decoding (SD) is a promising method for reducing the inference latency of large language models (LLMs). A well-designed draft model and an effective draft candidate tree construction method are key to enhancing the acceleration effect of SD. In this paper, we first propose the Effective Draft Decoder (EDD), which treats the LLM as a powerful encoder and generates more accurate draft tokens by leveraging the encoding results as soft prompts. Furthermore, we use KL divergence instead of the standard cross-entropy loss to better align the draft model’s output with the LLM. Next, we introduce the Pruned Candidate Tree (PCT) algorithm to construct a more efficient candidate tree. Specifically, we found that the confidence scores predicted by the draft model are well-calibrated with the acceptance probability of draft tokens. Therefore, PCT estimates the expected time gain for each node in the candidate tree based on confidence scores and retains only the nodes that contribute to acceleration, pruning away redundant nodes. We conducted extensive experiments with various LLMs across four datasets. The experimental results verify the effectiveness of our proposed method, which significantly improves the performance of SD and reduces the inference latency of LLMs.</abstract>
      <url hash="a0317e2b">2025.acl-long.486</url>
      <bibkey>zheng-wang-2025-faster</bibkey>
    </paper>
    <paper id="487">
      <title>Selecting and Merging: Towards Adaptable and Scalable Named Entity Recognition with Large Language Models</title>
      <author><first>Zhuojun</first><last>Ding</last></author>
      <author><first>Wei</first><last>Wei</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Chenghao</first><last>Fan</last></author>
      <pages>9869-9886</pages>
      <abstract>Supervised fine-tuning (SFT) is widely used to align large language models (LLMs) with information extraction (IE) tasks, such as named entity recognition (NER). However, annotating such fine-grained labels and training domain-specific models is costly. Existing works typically train a unified model across multiple domains, but such approaches lack adaptation and scalability since not all training data benefits target domains and scaling trained models remains challenging. We propose the SaM framework, which dynamically Selects and Merges expert models at inference time. Specifically, for a target domain, we select domain-specific experts pre-trained on existing domains based on (i) domain similarity to the target domain and (ii) performance on sampled instances, respectively. The experts are then merged to create task-specific models optimized for the target domain. By dynamically merging experts beneficial to target domains, we improve generalization across various domains without extra training. Additionally, experts can be added or removed conveniently, leading to great scalability. Extensive experiments on multiple benchmarks demonstrate our framework’s effectiveness, which outperforms the unified model by an average of 10%. We further provide insights into potential improvements, practical experience, and extensions of our framework.</abstract>
      <url hash="cfab8016">2025.acl-long.487</url>
      <bibkey>ding-etal-2025-selecting</bibkey>
    </paper>
    <paper id="488">
      <title>Embracing Imperfection: Simulating Students with Diverse Cognitive Levels Using <fixed-case>LLM</fixed-case>-based Agents</title>
      <author><first>Tao</first><last>Wu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Jingyuan</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Wang</first><last>Lin</last></author>
      <author><first>Mengze</first><last>Li</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Yumeng</first><last>Zhu</last></author>
      <author><first>Ang</first><last>Li</last></author>
      <author><first>Kun</first><last>Kuang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Fei</first><last>Wu</last><affiliation>Zhejiang University</affiliation></author>
      <pages>9887-9908</pages>
      <abstract>Large language models (LLMs) are revolutionizing education, with LLM-based agents playing a key role in simulating student behavior. A major challenge in student simulation is modeling the diverse learning patterns of students at various cognitive levels. However, current LLMs, typically trained as “helpful assistants”, target at generating perfect responses. As a result, they struggle to simulate students with diverse cognitive abilities, as they often produce overly advanced answers, missing the natural imperfections that characterize student learning and resulting in unrealistic simulations. To address this issue, we propose a training-free framework for student simulation. We begin by constructing a cognitive prototype for each student using a knowledge graph, which captures their understanding of concepts from past learning records. This prototype is then mapped to new tasks to predict student performance. Next, we simulate student solutions based on these predictions and iteratively refine them using a beam search method to better replicate realistic mistakes. To validate our approach, we construct the Student_100 dataset, consisting of 100 students working on Python programming and 5,000 learning records. Experimental results show that our method consistently outperforms baseline models, achieving 100% improvement in simulation accuracy and realism.</abstract>
      <url hash="7ea08cf0">2025.acl-long.488</url>
      <bibkey>wu-etal-2025-embracing</bibkey>
    </paper>
    <paper id="489">
      <title><fixed-case>CADR</fixed-case>eview: Automatically Reviewing <fixed-case>CAD</fixed-case> Programs with Error Detection and Correction</title>
      <author><first>Jiali</first><last>Chen</last></author>
      <author><first>Xusen</first><last>Hei</last></author>
      <author><first>HongFei</first><last>Liu</last></author>
      <author><first>Yuancheng</first><last>Wei</last></author>
      <author><first>Zikun</first><last>Deng</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Jiayuan</first><last>Xie</last></author>
      <author><first>Yi</first><last>Cai</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Li</first><last>Qing</last><affiliation>The Hong Kong Polytechnic University and Hong Kong Polytechnic University</affiliation></author>
      <pages>9909-9927</pages>
      <abstract>Computer-aided design (CAD) is crucial in prototyping 3D objects through geometric instructions (i.e., CAD programs). In practical design workflows, designers often engage in time-consuming reviews and refinements of these prototypes by comparing them with reference images. To bridge this gap, we introduce the CAD review task to automatically detect and correct potential errors, ensuring consistency between the constructed 3D objects and reference images. However, recent advanced multimodal large language models (MLLMs) struggle to recognize multiple geometric components and perform spatial geometric operations within the CAD program, leading to inaccurate reviews. In this paper, we propose the CAD program repairer (ReCAD) framework to effectively detect program errors and provide helpful feedback on error correction. Additionally, we create a dataset, CADReview, consisting of over 20K program-image pairs, with diverse errors for the CAD review task. Extensive experiments demonstrate that our ReCAD significantly outperforms existing MLLMs, which shows great potential in design applications.</abstract>
      <url hash="81c821f0">2025.acl-long.489</url>
      <bibkey>chen-etal-2025-cadreview</bibkey>
    </paper>
    <paper id="490">
      <title>Think&amp;Cite: Improving Attributed Text Generation with Self-Guided Tree Search and Progress Reward Modeling</title>
      <author><first>Junyi</first><last>Li</last></author>
      <author><first>Hwee Tou</first><last>Ng</last><affiliation>National University of Singapore</affiliation></author>
      <pages>9928-9942</pages>
      <abstract>Despite their outstanding capabilities, large language models (LLMs) are prone to hallucination and producing factually incorrect information. This challenge has spurred efforts in attributed text generation, which prompts LLMs to generate content with supporting evidence. In this paper, we propose a novel framework, called Think&amp;Cite, and formulate attributed text generation as a multi-step reasoning problem integrated with search. Specifically, we propose Self-Guided Monte Carlo Tree Search (SG-MCTS), which capitalizes on the self-reflection capability of LLMs to reason about the intermediate states of MCTS for guiding the tree expansion process. To provide reliable and comprehensive feedback, we introduce Progress Reward Modeling to measure the progress of tree search from the root to the current state from two aspects, i.e., generation and attribution progress. We conduct extensive experiments on three datasets and the results show that our approach significantly outperforms baseline approaches.</abstract>
      <url hash="255c6455">2025.acl-long.490</url>
      <bibkey>li-ng-2025-think</bibkey>
    </paper>
    <paper id="491">
      <title>The Lawyer That Never Thinks: Consistency and Fairness as Keys to Reliable <fixed-case>AI</fixed-case></title>
      <author><first>Dana R</first><last>Alsagheer</last></author>
      <author><first>Abdulrahman</first><last>Kamal</last></author>
      <author><first>Mohammad</first><last>Kamal</last></author>
      <author><first>Cosmo Yang</first><last>Wu</last></author>
      <author><first>Weidong</first><last>Shi</last></author>
      <pages>9943-9954</pages>
      <abstract>Large Language Models (LLMs) are increasingly used in high-stakes domains like law and research, yet their inconsistencies and response instability raise concerns about trustworthiness. This study evaluates six leading LLMs—GPT-3.5, GPT-4, Claude, Gemini, Mistral, and LLaMA 2—on rationality, stability, and ethical fairness through reasoning tests, legal challenges, and bias-sensitive scenarios. Results reveal significant inconsistencies, highlighting trade-offs between model scale, architecture, and logical coherence. These findings underscore the risks of deploying LLMs in legal and policy settings, emphasizing the need for AI systems that prioritize transparency, fairness, and ethical robustness.</abstract>
      <url hash="dfb6d263">2025.acl-long.491</url>
      <bibkey>alsagheer-etal-2025-lawyer</bibkey>
    </paper>
    <paper id="492">
      <title>Polishing Every Facet of the <fixed-case>GEM</fixed-case>: Testing Linguistic Competence of <fixed-case>LLM</fixed-case>s and Humans in <fixed-case>K</fixed-case>orean</title>
      <author><first>SungHo</first><last>Kim</last><affiliation>Korea University</affiliation></author>
      <author><first>Nayeon</first><last>Kim</last><affiliation>Korea University</affiliation></author>
      <author><first>Taehee</first><last>Jeon</last></author>
      <author><first>SangKeun</first><last>Lee</last><affiliation>Korea University</affiliation></author>
      <pages>9955-9984</pages>
      <abstract>We introduce the <tex-math>\underline{Ko}rean \underline{G}rammar \underline{E}valuation Bench\underline{M}ark (KoGEM)</tex-math>, designed to assess the linguistic competence of LLMs and humans in Korean. KoGEM consists of 1.5k multiple-choice QA pairs covering five main categories and 16 subcategories. The zero-shot evaluation of 27 LLMs of various sizes and types reveals that while LLMs perform remarkably well on straightforward tasks requiring primarily definitional knowledge, they struggle with tasks that demand the integration of real-world experiential knowledge, such as phonological rules and pronunciation. Furthermore, our in-depth analysis suggests that incorporating such experiential knowledge could enhance the linguistic competence of LLMs. With KoGEM, we not only highlight the limitations of current LLMs in linguistic competence but also uncover hidden facets of LLMs in linguistic competence, paving the way for enhancing comprehensive language understanding. Our code and dataset are available at: https://github.com/SungHo3268/KoGEM.</abstract>
      <url hash="3e47cda6">2025.acl-long.492</url>
      <bibkey>kim-etal-2025-polishing</bibkey>
    </paper>
    <paper id="493">
      <title><fixed-case>S</fixed-case>peech<fixed-case>F</fixed-case>ake: A Large-Scale Multilingual Speech Deepfake Dataset Incorporating Cutting-Edge Generation Methods</title>
      <author><first>Wen</first><last>Huang</last></author>
      <author><first>Yanmei</first><last>Gu</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Zhiming</first><last>Wang</last><affiliation>Ant Group</affiliation></author>
      <author><first>Huijia</first><last>Zhu</last><affiliation>Ant Group</affiliation></author>
      <author><first>Yanmin</first><last>Qian</last></author>
      <pages>9985-9998</pages>
      <abstract>As speech generation technology advances, the risk of misuse through deepfake audio has become a pressing concern, which underscores the critical need for robust detection systems. However, many existing speech deepfake datasets are limited in scale and diversity, making it challenging to train models that can generalize well to unseen deepfakes. To address these gaps, we introduce SpeechFake, a large-scale dataset designed specifically for speech deepfake detection. SpeechFake includes over 3 million deepfake samples, totaling more than 3,000 hours of audio, generated using 40 different speech synthesis tools. The dataset encompasses a wide range of generation techniques, including text-to-speech, voice conversion, and neural vocoder, incorporating the latest cutting-edge methods. It also provides multilingual support, spanning 46 languages. In this paper, we offer a detailed overview of the dataset’s creation, composition, and statistics. We also present baseline results by training detection models on SpeechFake, demonstrating strong performance on both its own test sets and various unseen test sets. Additionally, we conduct experiments to rigorously explore how generation methods, language diversity, and speaker variation affect detection performance. We believe SpeechFake will be a valuable resource for advancing speech deepfake detection and developing more robust models for evolving generation techniques.</abstract>
      <url hash="5cb4f197">2025.acl-long.493</url>
      <bibkey>huang-etal-2025-speechfake</bibkey>
    </paper>
    <paper id="494">
      <title><fixed-case>R</fixed-case>eflection<fixed-case>C</fixed-case>oder: Learning from Reflection Sequence for Enhanced One-off Code Generation</title>
      <author><first>Houxing</first><last>Ren</last><affiliation>Sensetime</affiliation></author>
      <author><first>Mingjie</first><last>Zhan</last></author>
      <author><first>Zhongyuan</first><last>Wu</last></author>
      <author><first>Aojun</first><last>Zhou</last></author>
      <author><first>Junting</first><last>Pan</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Hongsheng</first><last>Li</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>9999-10020</pages>
      <abstract>Code generation plays a crucial role in various tasks, such as code auto-completion and mathematical reasoning. Previous work has proposed numerous methods to enhance code generation performance, including integrating feedback from the compiler. Inspired by this, we present ReflectionCoder, a novel approach that effectively leverages reflection sequences constructed by integrating compiler feedback to improve one-off code generation performance. Furthermore, we propose reflection self-distillation and dynamically masked distillation to effectively utilize these reflection sequences. Extensive experiments on three benchmarks, i.e., HumanEval (+), MBPP (+), and MultiPl-E, demonstrate that models fine-tuned with our method achieve state-of-the-art performance. Beyond the code domain, we believe this approach can benefit other domains that focus on final results and require long reasoning paths. Code and data are available at https://github.com/SenseLLM/ReflectionCoder.</abstract>
      <url hash="f3359ebf">2025.acl-long.494</url>
      <bibkey>ren-etal-2025-reflectioncoder</bibkey>
    </paper>
    <paper id="495">
      <title><fixed-case>I</fixed-case>nvest<fixed-case>A</fixed-case>lign: Overcoming Data Scarcity in Aligning Large Language Models with Investor Decision-Making Processes Under Herd Behavior</title>
      <author><first>Huisheng</first><last>Wang</last></author>
      <author><first>Zhuoshi</first><last>Pan</last></author>
      <author><first>Hangjing</first><last>Zhang</last></author>
      <author><first>Mingxiao</first><last>Liu</last></author>
      <author><first>Hanqing</first><last>Gao</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>H. Vicky</first><last>Zhao</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>10021-10052</pages>
      <abstract>Aligning Large Language Models (LLMs) with investor decision-making processes under herd behavior is a critical challenge in behavioral finance, which grapples with a fundamental limitation: the scarcity of real-user data needed for Supervised Fine-Tuning (SFT). While SFT can bridge the gap between LLM outputs and human behavioral patterns, its reliance on massive authentic data imposes substantial collection costs and privacy risks. We propose **InvestAlign**, a novel framework that constructs high-quality SFT datasets by leveraging theoretical solutions to similar and simple optimal investment problems rather than the complex scenarios. Our theoretical analysis demonstrates that training LLMs with **InvestAlign**-generated data achieves faster parameter convergence than using real-user data, suggesting superior learning efficiency. Furthermore, we develop **InvestAgent**, an LLM agent fine-tuned with **InvestAlign**, which shows significantly closer alignment to real-user data than pre-SFT models in both simple and complex investment problems. This highlights our proposed **InvestAlign** as a promising approach with the potential to address complex optimal investment problems and align LLMs with investor decision-making processes under herd behavior. Our code is publicly available at https://github.com/thu-social-network-research-group/InvestAlign.</abstract>
      <url hash="d778d899">2025.acl-long.495</url>
      <bibkey>wang-etal-2025-investalign</bibkey>
    </paper>
    <paper id="496">
      <title>Enhancing Neural Machine Translation Through Target Language Data: A <tex-math>k</tex-math><fixed-case>NN</fixed-case>-<fixed-case>LM</fixed-case> Approach for Domain Adaptation</title>
      <author><first>Abudurexiti</first><last>Reheman</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Hongyu</first><last>Liu</last></author>
      <author><first>Junhao</first><last>Ruan</last></author>
      <author><first>Abudukeyumu</first><last>Abudula</last></author>
      <author><first>Yingfeng</first><last>Luo</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Tong</first><last>Xiao</last><affiliation>Northeastern University</affiliation></author>
      <author><first>JingBo</first><last>Zhu</last><affiliation>Northeastern University</affiliation></author>
      <pages>10053-10065</pages>
      <abstract>Neural machine translation (NMT) has advanced significantly, yet challenges remain in adapting to new domains . In scenarios where bilingual data is limited, this issue is further exacerbated. To address this, we propose <tex-math>k</tex-math>NN-LM-NMT, a method that leverages semantically similar target language sentences in the <tex-math>k</tex-math>NN framework. Our approach generates a probability distribution over these sentences during decoding, and this distribution is then interpolated with the NMT model’s distribution. Additionally, we introduce an <tex-math>n</tex-math>-gram-based approach to focus on similar fragments, enabling the model to avoid the noise introduced by the non-similar parts. To enhance accuracy, we further incorporate cross-lingual retrieval similarity to refine the <tex-math>k</tex-math>NN probability distribution. Extensive experiments on multi-domain datasets demonstrate significant performance improvements in both high-resource and low-resource scenarios. Our approach effectively extracts translation knowledge from limited target domain data, and well benefits from large-scale monolingual data for robust context representation.</abstract>
      <url hash="4e9668ea">2025.acl-long.496</url>
      <bibkey>reheman-etal-2025-enhancing</bibkey>
    </paper>
    <paper id="497">
      <title>Multi-level Relevance Document Identifier Learning for Generative Retrieval</title>
      <author><first>Fuwei</first><last>Zhang</last><affiliation>Meituan</affiliation></author>
      <author><first>Xiaoyu</first><last>Liu</last></author>
      <author><first>Xinyu</first><last>Jia</last></author>
      <author><first>Yingfei</first><last>Zhang</last><affiliation>Meituan</affiliation></author>
      <author><first>Shuai</first><last>Zhang</last><affiliation>Meituan</affiliation></author>
      <author><first>Xiang</first><last>Li</last></author>
      <author><first>Fuzhen</first><last>Zhuang</last><affiliation>School of Artificial Intelligence, Beihang University and Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Wei</first><last>Lin</last></author>
      <author><first>Zhao</first><last>Zhang</last><affiliation>SKLCCSE, School of Computer Science and Engineering, Beihang University, Beijing, China</affiliation></author>
      <pages>10066-10080</pages>
      <abstract>Generative Retrieval (GR) introduces a new information retrieval paradigm that directly generates unique document identifiers (DocIDs). The key challenge of GR lies in creating effective yet discrete DocIDs that preserve semantic relevance for similar documents while differentiating dissimilar ones. However, existing methods generate DocIDs solely based on the textual content of documents, which may result in DocIDs with weak semantic connections for similar documents due to variations in expression. Therefore, we propose using queries as a bridge to connect documents with varying relevance levels for learning improved DocIDs. In this paper, we propose **M**ulti-l**E**vel **R**elevance document identifier learning for **G**enerative r**E**trieval (MERGE), a novel approach that utilizes multi-level document relevance to learn high-quality DocIDs. MERGE incorporates three modules: a multi-relevance query-document alignment module to effectively align document representations with related queries, an outer-level contrastive learning module to capture binary-level relevance, and an inner-level multi-level relevance learning module to distinguish documents with different relevance levels. Our approach encodes rich hierarchical semantic information and maintains uniqueness across documents. Experimental results on real-world multilingual e-commerce search datasets demonstrate that MERGE significantly outperforms existing methods, underscoring its effectiveness. The source code is available at &lt;https://github.com/zhangfw123/MERGE&gt;.</abstract>
      <url hash="69b91521">2025.acl-long.497</url>
      <bibkey>zhang-etal-2025-multi-level</bibkey>
    </paper>
    <paper id="498">
      <title><fixed-case>E</fixed-case>fficient<fixed-case>QAT</fixed-case>: Efficient Quantization-Aware Training for Large Language Models</title>
      <author><first>Mengzhao</first><last>Chen</last></author>
      <author><first>Wenqi</first><last>Shao</last></author>
      <author><first>Peng</first><last>Xu</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Jiahao</first><last>Wang</last></author>
      <author><first>Peng</first><last>Gao</last></author>
      <author><first>Kaipeng</first><last>Zhang</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Ping</first><last>Luo</last><affiliation>The University of Hong Kong</affiliation></author>
      <pages>10081-10100</pages>
      <abstract>Large language models (LLMs) are crucial in modern natural language processing and artificial intelligence. However, they face challenges in managing their significant memory requirements. Although quantization-aware training (QAT) offers a solution by reducing memory consumption through low-bit representations with minimal accuracy loss, it is impractical due to substantial training resources. To address this, we propose Efficient Quantization-Aware Training (EfficientQAT), a more feasible QAT algorithm. EfficientQAT involves two consecutive phases: Block-wise training of all parameters (Block-AP) and end-to-end training of quantization parameters (E2E-QP). To the best of our knowledge, Block-AP is the first method to enable direct training of all parameters in a block-wise manner, reducing accuracy loss in low-bit scenarios by enhancing the solution space during optimization. E2E-QP then trains only the quantization parameters (step sizes) end-to-end, further improving the performance of quantized models by considering interactions among all sub-modules. Extensive experiments demonstrate that EfficientQAT outperforms previous quantization methods across a range of models, including base LLMs, instruction-tuned LLMs, and multimodal LLMs, with scales from 7B to 70B parameters at various quantization bits. For instance, EfficientQAT obtains a 2-bit Llama-2-70B model on a single A100-80GB GPU in 41 hours, with less than 3 points accuracy degradation compared to the full precision (69.48 vs. 72.41). Code is available at https://github.com/OpenGVLab/EfficientQAT.</abstract>
      <url hash="ddadf8a8">2025.acl-long.498</url>
      <bibkey>chen-etal-2025-efficientqat</bibkey>
    </paper>
    <paper id="499">
      <title>Exploring How Generative <fixed-case>MLLM</fixed-case>s Perceive More Than <fixed-case>CLIP</fixed-case> with the Same Vision Encoder</title>
      <author><first>Siting</first><last>Li</last><affiliation>University of Washington</affiliation></author>
      <author><first>Pang Wei</first><last>Koh</last><affiliation>Allen Institute for Artificial Intelligence and University of Washington</affiliation></author>
      <author><first>Simon Shaolei</first><last>Du</last><affiliation>University of Washington</affiliation></author>
      <pages>10101-10119</pages>
      <abstract>Recent research has shown that CLIP models struggle with visual reasoning tasks that require grounding compositionality, understanding spatial relationships, or capturing fine-grained details. One natural hypothesis is that the CLIP vision encoder does not embed essential information for these tasks. However, we find that this is not always the case: The encoder gathers query-relevant visual information, while CLIP fails to extract it. In particular, we show that another branch of Vision-Language Models (VLMs), Generative Multimodal Large Language Models (MLLMs), achieve significantly higher accuracy than CLIP in many of these tasks using the *same* vision encoder and weights, indicating that these Generative MLLMs *perceive more*—as they extract and utilize visual information more effectively. We conduct a series of controlled experiments and reveal that their success is attributed to multiple key design choices, including patch tokens, position embeddings, and prompt-based weighting. On the other hand, enhancing the training data alone or applying a stronger text encoder does not suffice to solve the task, and additional text tokens offer little benefit. Interestingly, we find that fine-grained visual reasoning is not exclusive to generative models trained by an autoregressive loss: When converted into CLIP-like encoders by contrastive finetuning, these MLLMs still outperform CLIP under the same cosine similarity-based evaluation protocol. Our study highlights the importance of VLM architectural choices and suggests directions for improving the performance of CLIP-like contrastive VLMs.</abstract>
      <url hash="c8a53220">2025.acl-long.499</url>
      <bibkey>li-etal-2025-exploring-generative</bibkey>
    </paper>
    <paper id="500">
      <title><fixed-case>N</fixed-case>exus<fixed-case>S</fixed-case>um: Hierarchical <fixed-case>LLM</fixed-case> Agents for Long-Form Narrative Summarization</title>
      <author><first>Hyuntak</first><last>Kim</last><affiliation>CJ Group</affiliation></author>
      <author><first>Byung-Hak</first><last>Kim</last><affiliation>CJ</affiliation></author>
      <pages>10120-10157</pages>
      <abstract>Summarizing long-form narratives—such as books, movies, and TV scripts—requires capturing intricate plotlines, character interactions, and thematic coherence, a task that remains challenging for existing LLMs. We introduce NexusSum, a multi-agent LLM framework for narrative summarization that processes long-form text through a structured, sequential pipeline—without requiring fine-tuning. Our approach introduces two key innovations: **(1) Dialogue-to-Description Transformation**: A narrative-specific preprocessing method that standardizes character dialogue and descriptive text into a unified format, improving coherence. **(2) Hierarchical Multi-LLM Summarization**: A structured summarization pipeline that optimizes chunk processing and controls output length for accurate, high-quality summaries. Our method establishes a new state-of-the-art in narrative summarization, achieving up to **a 30.0% improvement in BERTScore (F1)** across books, movies, and TV scripts. These results demonstrate the effectiveness of multi-agent LLMs in handling long-form content, offering a scalable approach for structured summarization in diverse storytelling domains.</abstract>
      <url hash="bde80ee8">2025.acl-long.500</url>
      <bibkey>kim-kim-2025-nexussum</bibkey>
    </paper>
    <paper id="501">
      <title><fixed-case>HAIC</fixed-case>: Improving Human Action Understanding and Generation with Better Captions for Multi-modal Large Language Models</title>
      <author><first>Xiao</first><last>Wang</last></author>
      <author><first>Jingyun</first><last>Hua</last></author>
      <author><first>Weihong</first><last>Lin</last><affiliation>Kuaishou</affiliation></author>
      <author><first>Yuanxing</first><last>Zhang</last><affiliation>Kuaishou- 快手科技</affiliation></author>
      <author><first>Fuzheng</first><last>Zhang</last></author>
      <author><first>Jianlong</first><last>Wu</last><affiliation>Harbin Institute of Technology (Shenzhen)</affiliation></author>
      <author><first>Di</first><last>Zhang</last><affiliation>Kuaishou Technology</affiliation></author>
      <author><first>Liqiang</first><last>Nie</last><affiliation>Harbin Institute of Technology (Shenzhen) and Shandong University</affiliation></author>
      <pages>10158-10181</pages>
      <abstract>Recent Multi-modal Large Language Models (MLLMs) have made great progress in video understanding. However, their performance on videos involving human actions is still limited by the lack of high-quality data. To address this, we introduce a two-stage data annotation pipeline. First, we design strategies to accumulate videos featuring clear human actions from the Internet. Second, videos are annotated in a standardized caption format that uses human attributes to distinguish individuals and chronologically details their actions and interactions. Through this pipeline, we curate two datasets, namely HAICTrain and HAICBench. **HAICTrain** comprises 126K video-caption pairs generated by Gemini-Pro and verified for training purposes. Meanwhile, **HAICBench** includes 412 manually annotated video-caption pairs and 2,000 QA pairs, for a comprehensive evaluation of human action understanding. Experimental results demonstrate that training with HAICTrain not only significantly enhances human understanding abilities across 4 benchmarks, but can also improve text-to-video generation results. Both the HAICTrain and HAICBench will be made open-source to facilitate further research.</abstract>
      <url hash="c2875a57">2025.acl-long.501</url>
      <bibkey>wang-etal-2025-haic</bibkey>
    </paper>
    <paper id="502">
      <title>Uni-Retrieval: A Multi-Style Retrieval Framework for <fixed-case>STEM</fixed-case>’s Education</title>
      <author><first>Yanhao</first><last>Jia</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Xinyi</first><last>Wu</last></author>
      <author><first>Li</first><last>Hao</last></author>
      <author><first>QinglinZhang</first><last>QinglinZhang</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Yuxiao</first><last>Hu</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Shuai</first><last>Zhao</last></author>
      <author><first>Wenqi</first><last>Fan</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <pages>10182-10197</pages>
      <abstract>In AI-facilitated teaching, leveraging various query styles to interpret abstract text descriptions is crucial for ensuring high-quality teaching. However, current retrieval models primarily focus on natural text-image retrieval, making them insufficiently tailored to educational scenarios due to the ambiguities in the retrieval process. In this paper, we propose a diverse expression retrieval task tailored to educational scenarios, supporting retrieval based on multiple query styles and expressions. We introduce the STEM Education Retrieval Dataset (SER), which contains over 24,000 query pairs of different styles, and the Uni-Retrieval, an efficient and style-diversified retrieval vision-language model based on prompt tuning. Uni-Retrieval extracts query style features as prototypes and builds a continuously updated Prompt Bank containing prompt tokens for diverse queries. This bank can updated during test time to represent domain-specific knowledge for different subject retrieval scenarios. Our framework demonstrates scalability and robustness by dynamically retrieving prompt tokens based on prototype similarity, effectively facilitating learning for unknown queries. Experimental results indicate that Uni-Retrieval outperforms existing retrieval models in most retrieval tasks.</abstract>
      <url hash="299ec452">2025.acl-long.502</url>
      <bibkey>jia-etal-2025-uni</bibkey>
    </paper>
    <paper id="503">
      <title><fixed-case>D</fixed-case>ense<fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case>: Dense Low-Rank Adaptation of Large Language Models</title>
      <author><first>Lin</first><last>Mu</last><affiliation>Anhui University</affiliation></author>
      <author><first>Xiaoyu</first><last>Wang</last></author>
      <author><first>Li</first><last>Ni</last><affiliation>Anhui University</affiliation></author>
      <author><first>Yang</first><last>Li</last></author>
      <author><first>Zhize</first><last>Wu</last><affiliation>Hefei University</affiliation></author>
      <author><first>Peiquan</first><last>Jin</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Yiwen</first><last>Zhang</last><affiliation>Anhui University</affiliation></author>
      <pages>10198-10211</pages>
      <abstract>Low-rank adaptation (LoRA) has been developed as an efficient approach for adapting large language models (LLMs) by fine-tuning two low-rank matrices, thereby reducing the number of trainable parameters. However, prior research indicates that many of the weights in these matrices are redundant, leading to inefficiencies in parameter utilization. To address this limitation, we introduce Dense Low-Rank Adaptation (DenseLoRA), a novel approach that enhances parameter efficiency while achieving superior performance compared to LoRA. DenseLoRA builds upon the concept of representation fine-tuning, incorporating a single Encoder-Decoder to refine and compress hidden representations across all adaptation layers before applying adaptation. Instead of relying on two redundant low-rank matrices as in LoRA, DenseLoRA adapts LLMs through a dense low-rank matrix, improving parameter utilization and adaptation efficiency. We evaluate DenseLoRA on various benchmarks, showing that it achieves 83.8% accuracy with only 0.01% of trainable parameters, compared to LoRA’s 80.8% accuracy with 0.70% of trainable parameters on LLaMA3-8B. Additionally, we conduct extensive experiments to systematically assess the impact of DenseLoRA’s components on overall model performance.</abstract>
      <url hash="ac25870c">2025.acl-long.503</url>
      <bibkey>mu-etal-2025-denselora</bibkey>
    </paper>
    <paper id="504">
      <title>Exploring the Potential of <fixed-case>LLM</fixed-case>s as Personalized Assistants: Dataset, Evaluation, and Analysis</title>
      <author><first>Jisoo</first><last>Mok</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Ik-hwan</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Sangkwon</first><last>Park</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Sungroh</first><last>Yoon</last><affiliation>Seoul National University</affiliation></author>
      <pages>10212-10239</pages>
      <abstract>Personalized AI assistants, a hallmark of the human-like capabilities of Large Language Models (LLMs), are a challenging application that intertwines multiple problems in LLM research. Despite the growing interest in the development of personalized assistants, the lack of an open-source conversational dataset tailored for personalization remains a significant obstacle for researchers in the field. To address this research gap, we introduce HiCUPID, a new benchmark to probe and unleash the potential of LLMs to deliver personalized responses. Alongside a conversational dataset, HiCUPID provides a Llama-3.2-based automated evaluation model whose assessment closely mirrors human preferences. We release our dataset, evaluation model, and code at https://github.com/12kimih/HiCUPID.</abstract>
      <url hash="361dd32a">2025.acl-long.504</url>
      <bibkey>mok-etal-2025-exploring</bibkey>
    </paper>
    <paper id="505">
      <title>Cracking Factual Knowledge: A Comprehensive Analysis of Degenerate Knowledge Neurons in Large Language Models</title>
      <author><first>Yuheng</first><last>Chen</last></author>
      <author><first>Pengfei</first><last>Cao</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yubo</first><last>Chen</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <author><first>Yining</first><last>Wang</last></author>
      <author><first>Shengping</first><last>Liu</last></author>
      <author><first>Kang</first><last>Liu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jun</first><last>Zhao</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <pages>10240-10261</pages>
      <abstract>Knowledge neuron theory provides a key approach to understanding the mechanisms of factual knowledge in Large Language Models (LLMs), which suggests that facts are stored within multi-layer perceptron neurons. This paper further explores **Degenerate Knowledge Neurons** (DKNs), where distinct sets of neurons can store identical facts, but unlike simple redundancy, they also participate in storing other different facts. Despite the novelty and unique properties of this concept, it has not been rigorously defined and systematically studied. Our contributions are: (1) We pioneer the study of structures in knowledge neurons by analyzing weight connection patterns, providing a comprehensive definition of DKNs from both functional and structural aspects. (2) Based on this definition, we develop the **Neuronal Topology Clustering** method, leading to a more accurate DKN identification. (3) We demonstrate the practical applications of DKNs in two aspects: guiding LLMs to learn new knowledge and relating to LLMs’ robustness against input errors.</abstract>
      <url hash="67733818">2025.acl-long.505</url>
      <bibkey>chen-etal-2025-cracking</bibkey>
    </paper>
    <paper id="506">
      <title>Towards Context-Robust <fixed-case>LLM</fixed-case>s: A Gated Representation Fine-tuning Approach</title>
      <author><first>Shenglai</first><last>Zeng</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Pengfei</first><last>He</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Kai</first><last>Guo</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Tianqi</first><last>Zheng</last><affiliation>Amazon</affiliation></author>
      <author><first>Hanqing</first><last>Lu</last><affiliation>Amazon</affiliation></author>
      <author><first>Yue</first><last>Xing</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Hui</first><last>Liu</last></author>
      <pages>10262-10276</pages>
      <abstract>Large Language Models (LLMs) enhanced with external contexts, such as through retrieval-augmented generation (RAG), often face challenges in handling imperfect evidence. They tend to over-rely on external knowledge, making them vulnerable to misleading and unhelpful contexts. To address this, we propose the concept of context-robust LLMs, which can effectively balance internal knowledge with external context, similar to human cognitive processes. Specifically, context-robust LLMs should rely on external context only when lacking internal knowledge, identify contradictions between internal and external knowledge, and disregard unhelpful contexts. To achieve this goal, we introduce Grft, a lightweight and plug-and-play gated representation fine-tuning approach. Grft consists of two key components: a gating mechanism to detect and filter problematic inputs, and low-rank representation adapters to adjust hidden representations. By training a lightweight intervention function with only 0.0004% of model size on fewer than 200 examples, Grft can effectively adapt LLMs towards context-robust behaviors.</abstract>
      <url hash="c106a6ae">2025.acl-long.506</url>
      <bibkey>zeng-etal-2025-towards-context</bibkey>
    </paper>
    <paper id="507">
      <title>On Support Samples of Next Word Prediction</title>
      <author><first>Yuqian</first><last>Li</last></author>
      <author><first>Yupei</first><last>Du</last></author>
      <author><first>Yufang</first><last>Liu</last></author>
      <author><first>Feifei</first><last>Feng</last><affiliation>Midea Group</affiliation></author>
      <author><first>Mou Xiao</first><last>Feng</last><affiliation>AI Innovation Center</affiliation></author>
      <author><first>Yuanbin</first><last>Wu</last></author>
      <pages>10277-10289</pages>
      <abstract>Language models excel in various tasks by making complex decisions, yet understanding the rationale behind these decisions remains a challenge. This paper investigates <i>data-centric interpretability</i> in language models, focusing on the next-word prediction task. Using representer theorem, we identify two types of <i>support samples</i>—those that either promote or deter specific predictions. Our findings reveal that being a support sample is an intrinsic property, predictable even before training begins. Additionally, while non-support samples are less influential in direct predictions, they play a critical role in preventing overfitting and shaping generalization and representation learning. Notably, the importance of non-support samples increases in deeper layers, suggesting their significant role in intermediate representation formation.These insights shed light on the interplay between data and model decisions, offering a new dimension to understanding language model behavior and interpretability.</abstract>
      <url hash="89825ec9">2025.acl-long.507</url>
      <bibkey>li-etal-2025-support</bibkey>
    </paper>
    <paper id="508">
      <title><fixed-case>W</fixed-case>eb<fixed-case>W</fixed-case>alker: Benchmarking <fixed-case>LLM</fixed-case>s in Web Traversal</title>
      <author><first>Jialong</first><last>Wu</last><affiliation>Southeast University</affiliation></author>
      <author><first>Wenbiao</first><last>Yin</last><affiliation>nanjing university</affiliation></author>
      <author><first>Yong</first><last>Jiang</last><affiliation>Tongyi Lab</affiliation></author>
      <author><first>Zhenglin</first><last>Wang</last></author>
      <author><first>Zekun</first><last>Xi</last></author>
      <author><first>Runnan</first><last>Fang</last></author>
      <author><first>Linhai</first><last>Zhang</last></author>
      <author><first>Yulan</first><last>He</last><affiliation>King’s College London, University of London</affiliation></author>
      <author><first>Deyu</first><last>Zhou</last><affiliation>Southeast University</affiliation></author>
      <author><first>Pengjun</first><last>Xie</last></author>
      <author><first>Fei</first><last>Huang</last><affiliation>Alibaba Group US</affiliation></author>
      <pages>10290-10305</pages>
      <abstract>Retrieval-augmented generation (RAG) demonstrates remarkable performance across tasks in open-domain question-answering. However, traditional search engines may retrieve shallow content, limiting the ability of LLMs to handle complex, multi-layered information. To address this, we introduce WebWalkerQA, a benchmark designed to assess the ability of LLMs to perform web traversal. It evaluates the capacity of LLMs to traverse a website’s subpages to extract high-quality data systematically. We propose WebWalker, which is a multi-agent framework that mimics human-like web navigation through an explore-critic paradigm. Extensive experimental results show that WebWalkerQA is challenging and demonstrates the effectiveness of RAG combined with WebWalker, through this horizontal and vertical integration in real-world scenarios.</abstract>
      <url hash="5220f74e">2025.acl-long.508</url>
      <bibkey>wu-etal-2025-webwalker</bibkey>
    </paper>
    <paper id="509">
      <title>From Trade-off to Synergy: A Versatile Symbiotic Watermarking Framework for Large Language Models</title>
      <author><first>Yidan</first><last>Wang</last></author>
      <author><first>Yubing</first><last>Ren</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yanan</first><last>Cao</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <author><first>Binxing</first><last>Fang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>10306-10322</pages>
      <abstract>The rise of Large Language Models (LLMs) has heightened concerns about the misuse of AI-generated text, making watermarking a promising solution. Mainstream watermarking schemes for LLMs fall into two categories: logits-based and sampling-based. However, current schemes entail trade-offs among robustness, text quality, and security. To mitigate this, we integrate logits-based and sampling-based schemes, harnessing their respective strengths to achieve synergy. In this paper, we propose a versatile symbiotic watermarking framework with three strategies: serial, parallel, and hybrid. The hybrid framework adaptively embeds watermarks using token entropy and semantic entropy, optimizing the balance between detectability, robustness, text quality, and security. Furthermore, we validate our approach through comprehensive experiments on various datasets and models. Experimental results indicate that our method outperforms existing baselines and achieves state-of-the-art (SOTA) performance. We believe this framework provides novel insights into diverse watermarking paradigms.</abstract>
      <url hash="edc299c1">2025.acl-long.509</url>
      <bibkey>wang-etal-2025-trade</bibkey>
    </paper>
    <paper id="510">
      <title><fixed-case>A</fixed-case>uto<fixed-case>GUI</fixed-case>: Scaling <fixed-case>GUI</fixed-case> Grounding with Automatic Functionality Annotations from <fixed-case>LLM</fixed-case>s</title>
      <author><first>Hongxin</first><last>Li</last></author>
      <author><first>Jingfan</first><last>Chen</last></author>
      <author><first>Jingran</first><last>Su</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Yuntao</first><last>Chen</last><affiliation>Centre for Artificial Intelligence and Robotics (CAIR), Hong Kong Institute of Science &amp; Innovation, Chinese Academy of Sciences</affiliation></author>
      <author><first>Li</first><last>Qing</last><affiliation>The Hong Kong Polytechnic University and Hong Kong Polytechnic University</affiliation></author>
      <author><first>Zhaoxiang</first><last>Zhang</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <pages>10323-10358</pages>
      <abstract>User interface understanding with vision-language models (VLMs) has received much attention due to its potential for enhancing software automation.However, existing datasets used to build UI-VLMs either only contain large-scale context-free element annotations or contextualized functional descriptions for elements at a small scale.In this work, we propose the <b>AutoGUI</b> pipeline for automatically annotating UI elements with detailed functionality descriptions at scale.Specifically, we leverage large language models (LLMs) to infer element functionality by comparing UI state changes before and after simulated interactions. To improve annotation quality, we propose LLM-aided rejection and verification, eliminating invalid annotations without human labor.We construct a high-quality AutoGUI-704k dataset using the proposed pipeline, featuring diverse and detailed functionality annotations that are hardly provided by previous datasets.Human evaluation shows that we achieve annotation correctness comparable to a trained human annotator. Extensive experiments show that our dataset remarkably enhances VLM’s UI grounding capabilities and exhibits significant scaling effects. We also show the interesting potential use of our dataset in UI agent tasks. Please view our project at https://autogui-project.github.io/.</abstract>
      <url hash="1e45b0bc">2025.acl-long.510</url>
      <bibkey>li-etal-2025-autogui</bibkey>
    </paper>
    <paper id="511">
      <title>Introducing Graph Context into Language Models through Parameter-Efficient Fine-Tuning for Lexical Relation Mining</title>
      <author><first>Jingwen</first><last>Sun</last></author>
      <author><first>Zhiyi</first><last>Tian</last></author>
      <author><first>Yu</first><last>He</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Jingwei</first><last>Sun</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Guangzhong</first><last>Sun</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>10359-10374</pages>
      <abstract>Lexical relation refers to the way words are related within a language. Prior work has demonstrated that pretrained language models (PLMs) can effectively mine lexical relations between word pairs. However, they overlook the potential of graph structures composed of lexical relations, which can be integrated with the semantic knowledge of PLMs. In this work, we propose a parameter-efficient fine-tuning method through graph context, which integrates graph features and semantic representations for lexical relation classification (LRC) and lexical entailment (LE) tasks. Our experiments show that graph features can help PLMs better understand more complex lexical relations, establishing a new state-of-the-art for LRC and LE. Finally, we perform an error analysis, identifying the bottlenecks of language models in lexical relation mining tasks and providing insights for future improvements.</abstract>
      <url hash="3b1940b6">2025.acl-long.511</url>
      <bibkey>sun-etal-2025-introducing</bibkey>
    </paper>
    <paper id="512">
      <title><fixed-case>S</fixed-case>-<fixed-case>RAG</fixed-case>: A Novel Audit Framework for Detecting Unauthorized Use of Personal Data in <fixed-case>RAG</fixed-case> Systems</title>
      <author><first>Zhirui</first><last>Zeng</last></author>
      <author><first>Jiamou</first><last>Liu</last><affiliation>The University of Auckland</affiliation></author>
      <author><first>Meng-Fen</first><last>Chiang</last><affiliation>National Yang Ming Chiao Tung University</affiliation></author>
      <author><first>Jialing</first><last>He</last><affiliation>Chongqing University</affiliation></author>
      <author><first>Zijian</first><last>Zhang</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <pages>10375-10385</pages>
      <abstract>Retrieval-Augmented Generation (RAG) systems combine external data retrieval with text generation and have become essential in applications requiring accurate and context-specific responses. However, their reliance on external data raises critical concerns about unauthorized collection and usage of personal information. To ensure compliance with data protection regulations like GDPR and detect improper use of data, we propose the Shadow RAG Auditing Data Provenance (S-RAG) framework. S-RAG enables users to determine whether their textual data has been utilized in RAG systems, even in black-box settings with no prior system knowledge. It is effective across open-source and closed-source RAG systems and resilient to defense strategies. Experiments demonstrate that S-RAG achieves an improvement in Accuracy by 19.9% (compared to the best baseline), while maintaining strong performance under adversarial defenses. Furthermore, we analyze how the auditor’s knowledge of the target system affects performance, offering practical insights for privacy-preserving AI systems. Our code is open-sourced online.</abstract>
      <url hash="498d5cd2">2025.acl-long.512</url>
      <bibkey>zeng-etal-2025-rag</bibkey>
    </paper>
    <paper id="513">
      <title>Praetor: A Fine-Grained Generative <fixed-case>LLM</fixed-case> Evaluator with Instance-Level Customizable Evaluation Criteria</title>
      <author><first>Yongqi</first><last>Leng</last></author>
      <author><first>Renren</first><last>Jin</last></author>
      <author><first>Yue</first><last>Chen</last></author>
      <author><first>Zhuowen</first><last>Han</last></author>
      <author><first>Ling</first><last>Shi</last></author>
      <author><first>Jianxiang</first><last>Peng</last></author>
      <author><first>Lei</first><last>Yang</last></author>
      <author><first>Juesi</first><last>Xiao</last></author>
      <author><first>Deyi</first><last>Xiong</last><affiliation>Tianjin University</affiliation></author>
      <pages>10386-10418</pages>
      <abstract>With the increasing capability of large language models (LLMs), LLM-as-a-judge has emerged as a new evaluation paradigm. Compared with traditional automatic and manual evaluation, LLM evaluators exhibit better interpretability and efficiency. Despite this, existing LLM evaluators suffer from limited use scenarios and poor flexibility. To mitigate these issues, we propose Praetor, a fine-grained generative LLM evaluator with instance-level customazable evaluation criteria. To train Praetor, we curate a large-scale dataset guided with a hierarchical guideline covering a wide range of tasks and instance-level evaluation criteria. We train Praetor on this dataset in a multi-task learning fashion, which enables to evaluate LLMs in either pointwise grading or pairwise comparison way and support two languages simultaneously with a high flexibility of setting evaluation criteria. Extensive experiments demonstrate that Praetor outperforms previous LLM evaluators and instruction-tuned LLMs on multiple benchmarks, setting new SOTA results. It also exhibits the potential for generating critiques as scalable feedback to further improve LLMs. Our model and related resources are released at <url>https://github.com/tjunlp-lab/Praetor</url>.</abstract>
      <url hash="dc5ab275">2025.acl-long.513</url>
      <bibkey>leng-etal-2025-praetor</bibkey>
    </paper>
    <paper id="514">
      <title>Mitigating Confounding in Speech-Based Dementia Detection through Weight Masking</title>
      <author><first>Zhecheng</first><last>Sheng</last></author>
      <author><first>Xiruo</first><last>Ding</last></author>
      <author><first>Brian</first><last>Hur</last><affiliation>University of Washington</affiliation></author>
      <author><first>Changye</first><last>Li</last><affiliation>University of Washington</affiliation></author>
      <author><first>Trevor</first><last>Cohen</last><affiliation>University of Washington</affiliation></author>
      <author><first>Serguei V. S.</first><last>Pakhomov</last><affiliation>University of Minnesota - Twin Cities</affiliation></author>
      <pages>10419-10434</pages>
      <abstract>Deep transformer models have been used to detect linguistic anomalies in patient transcripts for early Alzheimer’s disease (AD) screening. While pre-trained neural language models (LMs) fine-tuned on AD transcripts perform well, little research has explored the effects of the gender of the speakers represented by these transcripts. This work addresses gender confounding in dementia detection and proposes two methods: the Extended Confounding Filter and the Dual Filter, which isolate and ablate weights associated with gender. We evaluate these methods on dementia datasets with first-person narratives from patients with cognitive impairment and healthy controls. Our results show transformer models tend to overfit to training data distributions. Disrupting gender-related weights results in a deconfounded dementia classifier, with the trade-off of slightly reduced dementia detection performance.</abstract>
      <url hash="7129f7cd">2025.acl-long.514</url>
      <bibkey>sheng-etal-2025-mitigating</bibkey>
    </paper>
    <paper id="515">
      <title><fixed-case>MCS</fixed-case>-Bench: A Comprehensive Benchmark for Evaluating Multimodal Large Language Models in <fixed-case>C</fixed-case>hinese Classical Studies</title>
      <author id="yang-liu"><first>Yang</first><last>Liu</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Jiahuan</first><last>Cao</last></author>
      <author><first>Hiuyi</first><last>Cheng</last></author>
      <author><first>Yongxin</first><last>Shi</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Kai</first><last>Ding</last><affiliation>INTSIG Information</affiliation></author>
      <author><first>Lianwen</first><last>Jin</last></author>
      <pages>10435-10492</pages>
      <abstract>With the rapid development of Multimodal Large Language Models (MLLMs), their potential in Chinese Classical Studies (CCS), a field which plays a vital role in preserving and promoting China’s rich cultural heritage, remains largely unexplored due to the absence of specialized benchmarks. To bridge this gap, we propose MCS-Bench, the first-of-its-kind multimodal benchmark specifically designed for CCS across multiple subdomains. MCS-Bench spans seven core subdomains (Ancient Chinese Text, Calligraphy, Painting, Oracle Bone Script, Seal, Cultural Relic, and Illustration), with a total of 45 meticulously designed tasks. Through extensive evaluation of 37 representative MLLMs, we observe that even the top-performing model (InternVL2.5-78B) achieves an average score below 50, indicating substantial room for improvement. Our analysis reveals significant performance variations across different tasks and identifies critical challenges in areas such as Optical Character Recognition (OCR) and cultural context interpretation. MCS-Bench not only establishes a standardized baseline for CCS-focused MLLM research but also provides valuable insights for advancing cultural heritage preservation and innovation in the Artificial General Intelligence (AGI) era. Data and code will be publicly available.</abstract>
      <url hash="b98c7a98">2025.acl-long.515</url>
      <bibkey>liu-etal-2025-mcs</bibkey>
    </paper>
    <paper id="516">
      <title>The Knowledge Microscope: Features as Better Analytical Lenses than Neurons</title>
      <author><first>Yuheng</first><last>Chen</last></author>
      <author><first>Pengfei</first><last>Cao</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Kang</first><last>Liu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jun</first><last>Zhao</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <pages>10493-10515</pages>
      <abstract>We demonstrate that features, rather than neurons, serve as superior analytical units for understanding the mechanisms of factual knowledge in Language Models (LMs). Previous studies primarily utilize MLP neurons as units of analysis; however, neurons suffer from polysemanticity, leading to limited knowledge expression and poor interpretability. We first conduct preliminary experiments to validate that SAE can effectively decompose neurons into features. With this established, our core findings reveal three key advantages of features over neurons: (1) Features exhibit stronger influence on knowledge expression and superior interpretability. (2) Features demonstrate enhanced monosemanticity, showing distinct activation patterns between related and unrelated facts. (3) Feature-based method demonstrates superior performance over neuron-based approaches in erasing privacy-sensitive information from LMs. Additionally, we propose FeatureEdit, the first feature-based editing method. Code and dataset will be available.</abstract>
      <url hash="6a4e92de">2025.acl-long.516</url>
      <bibkey>chen-etal-2025-knowledge</bibkey>
    </paper>
    <paper id="517">
      <title>From Real to Synthetic: Synthesizing Millions of Diversified and Complicated User Instructions with Attributed Grounding</title>
      <author><first>Chiwei</first><last>Zhu</last></author>
      <author><first>Benfeng</first><last>Xu</last></author>
      <author><first>Xiaorui</first><last>Wang</last><affiliation>Yuanshi- 元石科技</affiliation></author>
      <author><first>Zhendong</first><last>Mao</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>10516-10543</pages>
      <abstract>The pursuit of diverse, complex, and large-scale instruction data is crucial for automatically aligning large language models (LLMs). While there are methods capable of generating synthetic instructions at scale, they either suffer from limited grounding sources, leading to a narrow distribution, or rely on trivial extensions that fail to produce meaningful trajectories in terms of complexity. In contrast, instructions that benefit efficient alignment are typically crafted with cognitive insights and grounded in real-world use cases. In this paper, we synthesize such instructions using attributed grounding, which involves 1) a top-down attribution process that grounds a selective set of real instructions to situated users, and 2) a bottom-up synthesis process that leverages web documents to first generate a situation, then a meaningful instruction. This framework allows us to harvest diverse and complex instructions at scale, utilizing the vast range of web documents. Specifically, we construct a dataset of 1 million instructions, called SynthQuestions, and demonstrate that models trained on it achieve leading performance on several common benchmarks, with improvements that continually scale with more web corpora.</abstract>
      <url hash="344a5fb7">2025.acl-long.517</url>
      <bibkey>zhu-etal-2025-real</bibkey>
    </paper>
    <paper id="518">
      <title><fixed-case>P</fixed-case>riva<fixed-case>CI</fixed-case>-Bench: Evaluating Privacy with Contextual Integrity and Legal Compliance</title>
      <author><first>Haoran</first><last>Li</last></author>
      <author><first>Wenbin</first><last>Hu</last></author>
      <author><first>Huihao</first><last>Jing</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Yulin</first><last>Chen</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Qi</first><last>Hu</last></author>
      <author><first>Sirui</first><last>Han</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Tianshu</first><last>Chu</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Peizhao</first><last>Hu</last><affiliation>Rochester Institute of Technology</affiliation></author>
      <author><first>Yangqiu</first><last>Song</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <pages>10544-10559</pages>
      <abstract>Recent advancements in generative large language models (LLMs) have enabled wider applicability, accessibility, and flexibility. However, their reliability and trustworthiness are still in doubt, especially for concerns regarding individuals’ data privacy. Great efforts have been made on privacy by building various evaluation benchmarks to study LLMs’ privacy awareness and robustness from their generated outputs to their hidden representations. Unfortunately, most of these works adopt a narrow formulation of privacy and only investigate personally identifiable information (PII). In this paper, we follow the merit of the Contextual Integrity (CI) theory, which posits that privacy evaluation should not only cover the transmitted attributes but also encompass the whole relevant social context through private information flows. We present PrivaCI-Bench, a comprehensive contextual privacy evaluation benchmark targeted at legal compliance to cover well-annotated privacy and safety regulations, real court cases, privacy policies, and synthetic data built from the official toolkit to study LLMs’ privacy and safety compliance. We evaluate the latest LLMs, including the recent reasoner models QwQ-32B and Deepseek R1. Our experimental results suggest that though LLMs can effectively capture key CI parameters inside a given context, they still require further advancements for privacy compliance.</abstract>
      <url hash="1d661d5a">2025.acl-long.518</url>
      <bibkey>li-etal-2025-privaci</bibkey>
    </paper>
    <paper id="519">
      <title>Unveiling Environmental Impacts of Large Language Model Serving: A Functional Unit View</title>
      <author><first>Yanran</first><last>Wu</last><affiliation>Purdue University</affiliation></author>
      <author><first>Inez</first><last>Hua</last><affiliation>Purdue University</affiliation></author>
      <author><first>Yi</first><last>Ding</last><affiliation>Purdue University</affiliation></author>
      <pages>10560-10576</pages>
      <abstract>Large language models (LLMs) offer powerful capabilities but come with significant environmental impact, particularly in carbon emissions. Existing studies benchmark carbon emissions but lack a standardized basis for comparison across different model configurations. To address this, we introduce the concept of functional unit (FU) as a standardized basis and develop FUEL, the first FU-based framework for evaluating LLM serving’s environmental impact. Through three case studies, we uncover key insights and trade-offs in reducing carbon emissions by optimizing model size, quantization strategy, and hardware choice, paving the way for more sustainable LLM serving. The code is available at https://github.com/jojacola/FUEL.</abstract>
      <url hash="ca4169f8">2025.acl-long.519</url>
      <bibkey>wu-etal-2025-unveiling</bibkey>
    </paper>
    <paper id="520">
      <title><fixed-case>E</fixed-case>xpe<fixed-case>T</fixed-case>rans: <fixed-case>LLM</fixed-case>s Are Experiential Transfer Learners</title>
      <author><first>Jinglong</first><last>Gao</last><affiliation>Research Center for Social Computing and Information Retrieval</affiliation></author>
      <author><first>Xiao</first><last>Ding</last></author>
      <author><first>Lingxiao</first><last>Zou</last></author>
      <author><first>Bibo</first><last>Cai</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Bing</first><last>Qin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Ting</first><last>Liu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>10577-10616</pages>
      <abstract>Recent studies provide large language models (LLMs) with textual task-solving experiences via prompts to improve their performance.However, previous methods rely on substantial human labor or time to gather such experiences for each task, which is impractical given the growing variety of task types in user queries to LLMs.To address this issue, we design an autonomous experience transfer framework to explore whether LLMs can mimic human cognitive intelligence to autonomously transfer experience from existing source tasks to newly encountered target tasks. This not only allows the acquisition of experience without extensive costs of previous methods, but also offers a novel path for the generalization of LLMs.Experimental results on 13 datasets demonstrate that our framework effectively improves the performance of LLMs. Furthermore, we provide a detailed analysis of each module in the framework.</abstract>
      <url hash="6c0d4b8a">2025.acl-long.520</url>
      <bibkey>gao-etal-2025-expetrans</bibkey>
    </paper>
    <paper id="521">
      <title>Cool-Fusion: Fuse Large Language Models without Training</title>
      <author id="cong-liu"><first>Cong</first><last>Liu</last></author>
      <author><first>Xiaojun</first><last>Quan</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Yan</first><last>Pan</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Weigang</first><last>Wu</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Xu</first><last>Chen</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Liang</first><last>Lin</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <pages>10617-10627</pages>
      <abstract>We focus on the problem of fusing two or more heterogeneous large language models (LLMs) to leverage their complementary strengths. One of the challenges of model fusion is high computational load, specifically in fine-tuning or aligning vocabularies. To address this, we propose Cool-Fusion, a simple yet effective approach that fuses the knowledge of source LLMs, which does not require training. Unlike ensemble methods, Cool-Fusion is applicable to any set of source LLMs that have different vocabularies. To overcome the vocabulary discrepancies among LLMs, we ensemble LLMs on text level, allowing them to rerank the generated texts by each other with different granularities. Extensive experiments have been conducted across a variety of benchmark datasets. On GSM8K, Cool-Fusion increases accuracy from three strong source LLMs by a significant margin of 17.4%.</abstract>
      <url hash="ca02e811">2025.acl-long.521</url>
      <bibkey>liu-etal-2025-cool</bibkey>
    </paper>
    <paper id="522">
      <title><fixed-case>DAPE</fixed-case> V2: Process Attention Score as Feature Map for Length Extrapolation</title>
      <author><first>Chuanyang</first><last>Zheng</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Yihang</first><last>Gao</last></author>
      <author><first>Han</first><last>Shi</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Jing</first><last>Xiong</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Jiankai</first><last>Sun</last><affiliation>Stanford University</affiliation></author>
      <author><first>Jingyao</first><last>Li</last></author>
      <author><first>Minbin</first><last>Huang</last></author>
      <author><first>Xiaozhe</first><last>Ren</last></author>
      <author><first>Michael</first><last>Ng</last><affiliation>The University of Hong Kong</affiliation></author>
      <author><first>Xin</first><last>Jiang</last></author>
      <author><first>Zhenguo</first><last>Li</last><affiliation>Department of Computer Science and Engineering, Hong Kong University of Science and Technology and Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Yu</first><last>Li</last><affiliation>Department of Computer Science and Engineering, The Chinese University of Hong Kong</affiliation></author>
      <pages>10628-10666</pages>
      <abstract>The attention mechanism is a fundamental component of the Transformer model, contributing to interactions among distinct tokens. In general, the attention scores are determined simply by the key-query products. However, this work’s occasional trial (combining DAPE and NoPE) of including additional MLPs on attention scores without position encoding indicates that the classical key-query multiplication may limit the performance of Transformers. In this work, we conceptualize attention as a feature map and apply the convolution operator (for neighboring attention scores across different heads) to mimic the processing methods in computer vision. Specifically, **the main contribution of this paper is identifying and interpreting the Transformer length extrapolation problem as a result of the limited expressiveness of the naive query and key dot product, and we successfully translate the length extrapolation issue into a well-understood feature map processing problem**, which is called Convolutional Data-Adaptive Position Encoding (CDAPE).The novel insight, which can be adapted to various attention-related models, reveals that the current Transformer architecture has the potential for further evolution. Extensive experiments demonstrate that treating attention as a feature map and applying convolution as a processing method significantly enhances Transformer performance.</abstract>
      <url hash="815b073e">2025.acl-long.522</url>
      <bibkey>zheng-etal-2025-dape</bibkey>
    </paper>
    <paper id="523">
      <title><fixed-case>M</fixed-case>u<fixed-case>SC</fixed-case>: Improving Complex Instruction Following with Multi-granularity Self-Contrastive Training</title>
      <author><first>Hui</first><last>Huang</last></author>
      <author><first>Jiaheng</first><last>Liu</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Yancheng</first><last>He</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Shilong</first><last>Li</last></author>
      <author><first>Bing</first><last>Xu</last></author>
      <author><first>Conghui</first><last>Zhu</last></author>
      <author><first>Muyun</first><last>Yang</last></author>
      <author><first>Tiejun</first><last>Zhao</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>10667-10686</pages>
      <abstract>Complex instruction-following with elaborate constraints is imperative for Large Language Models (LLMs). While existing methods have constructed data for complex instruction alignment, they all rely on a more advanced model, especially GPT-4, limiting their application. In this paper, we propose a Multi-granularity Self-Contrastive Training (MuSC) framework, to improve the complex instruction alignment without relying on a stronger model. Our method is conducted on both coarse and fine granularity. On coarse-granularity, we construct constraint-aware preference data based on instruction decomposition and recombination. On fine-granularity, we perform token-aware preference optimization with dynamic token-level supervision. Our method is evaluated on open-sourced models, and experiment results show our method achieves significant improvement on both complex and general instruction-following benchmarks, surpassing previous self-alignment methods.</abstract>
      <url hash="675a7169">2025.acl-long.523</url>
      <bibkey>huang-etal-2025-musc</bibkey>
    </paper>
    <paper id="524">
      <title><fixed-case>L</fixed-case>ong<fixed-case>R</fixed-case>e<fixed-case>D</fixed-case>: Mitigating Short-Text Degradation of Long-Context Large Language Models via Restoration Distillation</title>
      <author><first>Zican</first><last>Dong</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Junyi</first><last>Li</last></author>
      <author><first>Jinhao</first><last>Jiang</last></author>
      <author><first>Mingyu</first><last>Xu</last></author>
      <author><first>Xin</first><last>Zhao</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Bingning</first><last>Wang</last><affiliation>Beijing Baichuan Intelligence Technology Co., Ltd.</affiliation></author>
      <author><first>Weipeng</first><last>Chen</last></author>
      <pages>10687-10707</pages>
      <abstract>Large language models (LLMs) have gained extended context windows through scaling positional encodings and lightweight continual pre-training. However, this often leads to degraded performance on short-text tasks, while the reasons for this degradation remain insufficiently explored. In this work, we identify two primary factors contributing to this issue: distribution drift in hidden states and attention scores, and catastrophic forgetting during continual pre-training. To address these challenges, we propose Long Context Pre-training with Restoration Distillation (LongReD), a novel approach designed to mitigate short-text performance degradation through minimizing the distribution discrepancy between the extended and original models. Besides training on long texts, LongReD distills the hidden state of selected layers from the original model on short texts. Additionally, LongReD also introduces a short-to-long distillation, aligning the output distribution on short texts with that on long texts by leveraging skipped positional indices. Experiments on common benchmarks demonstrate that LongReD effectively preserves the model’s short-text performance while maintaining or even enhancing its long-context abilities.</abstract>
      <url hash="8f5c0eec">2025.acl-long.524</url>
      <bibkey>dong-etal-2025-longred</bibkey>
    </paper>
    <paper id="525">
      <title><fixed-case>APB</fixed-case>: Accelerating Distributed Long-Context Inference by Passing Compressed Context Blocks across <fixed-case>GPU</fixed-case>s</title>
      <author><first>Yuxiang</first><last>Huang</last></author>
      <author><first>Mingye</first><last>Li</last></author>
      <author><first>Xu</first><last>Han</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Chaojun</first><last>Xiao</last></author>
      <author><first>Weilin</first><last>Zhao</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Sun</first><last>Ao</last></author>
      <author><first>Hao</first><last>Zhou</last><affiliation>Tencent</affiliation></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <author><first>Zhiyuan</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Maosong</first><last>Sun</last><affiliation>Tsinghua University</affiliation></author>
      <pages>10708-10727</pages>
      <abstract>While long-context inference is crucial for advancing large language model (LLM) applications, its prefill speed remains a significant bottleneck. Current approaches, including sequence parallelism strategies and compute reduction through approximate attention mechanisms, still fall short of delivering optimal inference efficiency. This hinders scaling the inputs to longer sequences and processing long-context queries in a timely manner. To address this, we introduce APB, an efficient long-context inference framework that leverages multi-host approximate attention to enhance prefill speed by reducing compute and enhancing parallelism simultaneously. APB introduces a communication mechanism for essential key-value pairs within a sequence parallelism framework, enabling a faster inference speed while maintaining task performance. We implement APB by incorporating a tailored FlashAttn kernel alongside optimized distribution strategies, supporting diverse models and parallelism configurations. APB achieves speedups of up to 9.2<tex-math>\times</tex-math>, 4.2<tex-math>\times</tex-math>, and 1.6<tex-math>\times</tex-math> compared with FlashAttn, RingAttn, and StarAttn, respectively, without any observable task performance degradation.</abstract>
      <url hash="a6f20970">2025.acl-long.525</url>
      <bibkey>huang-etal-2025-apb</bibkey>
    </paper>
    <paper id="526">
      <title><fixed-case>PPT</fixed-case>: A Minor Language News Recommendation Model via Cross-Lingual Preference Pattern Transfer</title>
      <author><first>Yiyang</first><last>Zhang</last></author>
      <author><first>Nan</first><last>Chen</last></author>
      <pages>10728-10745</pages>
      <abstract>Rich user-item interactions are essential for building reliable recommender systems, as they reflect user preference patterns. However, minor language news recommendation platforms suffer from limited interactions due to a small user base. A natural solution is to apply well-established English recommender systems to minor language news recommendation, but the linguistic gap can lead to inaccurate modeling of minor language news content. Therefore, enabling few-shot minor language news recommender systems to capture both content information and preference patterns remains a challenge. Based on the observation that preference patterns are similar across languages, we propose a minor language news recommendation model by cross-lingual preference pattern transfer, named PPT. Our model adopts the widely used two-tower architecture and employs the large language model as the backbone of the news encoder. Through cross-lingual alignment, the strong English capability of the news encoder is extended to minor languages, thus enhancing news content representations. Additionally, through cross-lingual news augmentation, PPT simulates interactions of minor language news in the English domain, which facilitates the transfer of preference patterns from the many-shot English domain to the few-shot minor language domain. Extensive experiments on two real-world datasets across 15 minor languages demonstrate the superiority and generalization of our proposed PPT in addressing minor language news recommendation.</abstract>
      <url hash="306b4c13">2025.acl-long.526</url>
      <bibkey>zhang-chen-2025-ppt</bibkey>
    </paper>
    <paper id="527">
      <title><fixed-case>G</fixed-case>ain<fixed-case>RAG</fixed-case>: Preference Alignment in Retrieval-Augmented Generation through Gain Signal Synthesis</title>
      <author><first>Yi</first><last>Jiang</last></author>
      <author><first>Sendong</first><last>Zhao</last></author>
      <author><first>Jianbo</first><last>Li</last></author>
      <author><first>Haochun</first><last>Wang</last></author>
      <author><first>Bing</first><last>Qin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>10746-10757</pages>
      <abstract>The Retrieval-Augmented Generation (RAG) framework introduces a retrieval module to dynamicaslly inject retrieved information into the input context of large language models (LLMs), and has demonstrated significant success in various NLP tasks. However, the current study points out that there is a preference gap between retrievers and LLMs in the RAG framework, which limit the further improvement of system performance. Some highly relevant passages may interfere with LLM reasoning because they contain complex or contradictory information; while some indirectly related or even inaccurate content may help LLM generate more accurate answers by providing suggestive information or logical clues. To solve this, we propose **GainRAG**, a novel approach that aligns the retriever’s and LLM’s preferences by defining a new metric, “gain’’, which measure how well an input passage contributes to correct outputs.We then propose a method to estimate these gain signals and train a middleware that aligns the preferences of the retriever and the LLM using only limited data.In addition, we introduce a pseudo-passage strategy to mitigate degradation.The experimental results on 6 datasets verify the effectiveness of GainRAG.</abstract>
      <url hash="143bfe1d">2025.acl-long.527</url>
      <bibkey>jiang-etal-2025-gainrag</bibkey>
    </paper>
    <paper id="528">
      <title>Top-<tex-math>n\sigma</tex-math>: Eliminating Noise in Logit Space for Robust Token Sampling of <fixed-case>LLM</fixed-case></title>
      <author><first>Chenxia</first><last>Tang</last></author>
      <author><first>Jianchun</first><last>Liu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Hongli</first><last>Xu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Liusheng</first><last>Huang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>10758-10774</pages>
      <abstract>Large language models (LLMs) rely heavily on sampling methods to generate diverse and high-quality text.While existing sampling methods like top-<tex-math>p</tex-math> and min-<tex-math>p</tex-math> have identified the detrimental effects of low-probability tails in LLMs’ outputs, they still fail to effectively distinguish between diversity and noise. This limitation stems from their reliance on probability-based metrics that are inherently sensitive to temperature scaling. Through empirical and theoretical analysis, we make two key discoveries: (1) the pre-softmax logits exhibit a clear statistical separation between informative tokens and noise, and (2) we prove the mathematical equivalence of min-<tex-math>p</tex-math> and top-(1-<tex-math>p</tex-math>) under uniform distribution over logits. These findings motivate the design of top-n<tex-math>\sigma</tex-math>, a novel sampling method that identifies informative tokens by eliminating noise directly in logit space.Unlike existing methods that become unstable at high temperatures, top-<tex-math>n\sigma</tex-math> achieves temperature-invariant token selection while preserving output diversity. Extensive experiments across reasoning and creative writing tasks demonstrate that our method consistently outperforms existing approaches, with particularly significant improvements in high-temperature settings.</abstract>
      <url hash="13eb0c93">2025.acl-long.528</url>
      <bibkey>tang-etal-2025-top</bibkey>
    </paper>
    <paper id="529">
      <title><fixed-case>SCOPE</fixed-case>: Optimizing Key-Value Cache Compression in Long-context Generation</title>
      <author><first>Jialong</first><last>Wu</last><affiliation>Southeast University</affiliation></author>
      <author><first>Zhenglin</first><last>Wang</last></author>
      <author><first>Linhai</first><last>Zhang</last></author>
      <author><first>Yilong</first><last>Lai</last><affiliation>Southeast University</affiliation></author>
      <author><first>Yulan</first><last>He</last><affiliation>King’s College London, University of London</affiliation></author>
      <author><first>Deyu</first><last>Zhou</last><affiliation>Southeast University</affiliation></author>
      <pages>10775-10790</pages>
      <abstract>Key-Value (KV) cache has become a bottleneck of LLMs for long-context generation. Despite the numerous efforts in this area, the optimization for the decoding phase is generally ignored. However, we believe such optimization is crucial, especially for long-output generation tasks based on the following two observations: (i) Excessive compression during the prefill phase, which requires specific full context impairs the comprehension of the reasoning task; (ii) Deviation of heavy hitters occurs in the reasoning tasks with long outputs. Therefore, SCOPE, a simple yet efficient framework that separately performs KV cache optimization during the prefill and decoding phases, is introduced. Specifically, the KV cache during the prefill phase is preserved to maintain the essential information, while a novel strategy based on sliding is proposed to select essential heavy hitters for the decoding phase. Memory usage and memory transfer are further optimized using adaptive and discontinuous strategies. Extensive experiments on LongGenBench show the effectiveness and generalization of SCOPE and its compatibility as a plug-in to other prefill-only KV compression methods.</abstract>
      <url hash="6b182ccd">2025.acl-long.529</url>
      <bibkey>wu-etal-2025-scope</bibkey>
    </paper>
    <paper id="530">
      <title>Mitigating Non-Representative Prototypes and Representation Bias in Few-Shot Continual Relation Extraction</title>
      <author><first>Thanh Duc</first><last>Pham</last></author>
      <author><first>Nam Le</first><last>Hai</last><affiliation>Hanoi University of Science and Technology</affiliation></author>
      <author><first>Linh Ngo</first><last>Van</last><affiliation>Hanoi University of Science and Technology</affiliation></author>
      <author><first>Nguyen Thi Ngoc</first><last>Diep</last></author>
      <author><first>Sang</first><last>Dinh</last><affiliation>Hanoi University of Science and Technology</affiliation></author>
      <author><first>Thien Huu</first><last>Nguyen</last><affiliation>University of Oregon</affiliation></author>
      <pages>10791-10809</pages>
      <abstract>To address the phenomenon of similar classes, existing methods in few-shot continual relation extraction (FCRE) face two main challenges: non-representative prototypes and representation bias, especially when the number of available samples is limited. In our work, we propose Minion to address these challenges. Firstly, we leverage the General Orthogonal Frame (GOF) structure, based on the concept of Neural Collapse, to create robust class prototypes with clear separation, even between analogous classes. Secondly, we utilize label description representations as global class representatives within the fast-slow contrastive learning paradigm. These representations consistently encapsulate the essential attributes of each relation, acting as global information that helps mitigate overfitting and reduces representation bias caused by the limited local few-shot examples within a class. Extensive experiments on well-known FCRE benchmarks show that our method outperforms state-of-the-art approaches, demonstrating its effectiveness for advancing RE system.</abstract>
      <url hash="efa8abd1">2025.acl-long.530</url>
      <bibkey>pham-etal-2025-mitigating</bibkey>
    </paper>
    <paper id="531">
      <title><fixed-case>M</fixed-case>o<fixed-case>QAE</fixed-case>: Mixed-Precision Quantization for Long-Context <fixed-case>LLM</fixed-case> Inference via Mixture of Quantization-Aware Experts</title>
      <author><first>Wei</first><last>Tao</last></author>
      <author><first>Haocheng</first><last>Lu</last></author>
      <author><first>Xiaoyang</first><last>Qu</last></author>
      <author><first>Bin</first><last>Zhang</last></author>
      <author><first>Kai</first><last>Lu</last></author>
      <author><first>Jiguang</first><last>Wan</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Jianzong</first><last>Wang</last><affiliation>Pingan Technology</affiliation></author>
      <pages>10810-10820</pages>
      <abstract>One of the primary challenges in optimizing large language models (LLMs) for long-context inference lies in the high memory consumption of the Key-Value (KV) cache. Existing approaches, such as quantization, have demonstrated promising results in reducing memory usage. However, current quantization methods cannot take both effectiveness and efficiency into account. In this paper, we propose MoQAE, a novel mixed-precision quantization method via mixture of quantization-aware experts. First, we view different quantization bit-width configurations as experts and use the traditional mixture of experts (MoE) method to select the optimal configuration. To avoid the inefficiency caused by inputting tokens one by one into the router in the traditional MoE method, we input the tokens into the router chunk by chunk. Second, we design a lightweight router-only fine-tuning process to train MoQAE with a comprehensive loss to learn the trade-off between model accuracy and memory usage. Finally, we introduce a routing freezing (RF) and a routing sharing (RS) mechanism to further reduce the inference overhead. Extensive experiments on multiple benchmark datasets demonstrate that our method outperforms state-of-the-art KV cache quantization approaches in both efficiency and effectiveness.</abstract>
      <url hash="c91a79f7">2025.acl-long.531</url>
      <bibkey>tao-etal-2025-moqae</bibkey>
    </paper>
    <paper id="532">
      <title><fixed-case>P</fixed-case>rivacy<fixed-case>R</fixed-case>estore: Privacy-Preserving Inference in Large Language Models via Privacy Removal and Restoration</title>
      <author><first>Ziqian</first><last>Zeng</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Jianwei</first><last>Wang</last></author>
      <author><first>Junyao</first><last>Yang</last></author>
      <author><first>Zhengdong</first><last>Lu</last></author>
      <author><first>Haoran</first><last>Li</last></author>
      <author><first>Huiping</first><last>Zhuang</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Cen</first><last>Chen</last><affiliation>South China University of Technology</affiliation></author>
      <pages>10821-10855</pages>
      <abstract>The widespread usage of online Large Language Models (LLMs) inference services has raised significant privacy concerns about the potential exposure of private information in user inputs. Existing privacy protection methods for LLMs suffer from either insufficient privacy protection with performance degradation, or large inference time overhead. To address these limitations, we propose PrivacyRestore, a plug-and-play method to protect the privacy of user inputs during LLM inference for the client-server scenario. The server first trains restoration vectors for each privacy span type offline and then releases them to the clients. During inference, the client aggregates restoration vectors of all privacy spans in the user query into a meta restoration vector, which is later sent to the server to restore information. Before transmission, the client removes all privacy spans in the user query and applies <tex-math>d_\chi</tex-math>-privacy mechanism to the meta vector for privacy protection. We prove that our method can inherently prevent the linear growth of the privacy budget. We conduct extensive experimental, covering the medical and legal domains, and demonstrate that PrivacyRestore effectively protects private information and maintains acceptable levels of performance and inference efficiency</abstract>
      <url hash="64f516bd">2025.acl-long.532</url>
      <bibkey>zeng-etal-2025-privacyrestore</bibkey>
    </paper>
    <paper id="533">
      <title>Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models</title>
      <author><first>Xinlin</first><last>Zhuang</last></author>
      <author><first>Jiahui</first><last>Peng</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Ren</first><last>Ma</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Yinfan</first><last>Wang</last></author>
      <author><first>Tianyi</first><last>Bai</last><affiliation>Shanghai Artificial Intelligence Laboratory and Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Xingjian</first><last>Wei</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Qiu</first><last>Jiantao</last><affiliation>shanghai AI lab</affiliation></author>
      <author><first>Chi</first><last>Zhang</last></author>
      <author><first>Ying</first><last>Qian</last></author>
      <author><first>Conghui</first><last>He</last><affiliation>Shanghai AI Lab</affiliation></author>
      <pages>10856-10896</pages>
      <abstract>The composition of pre-training datasets for large language models (LLMs) remains largely undisclosed, hindering transparency and efforts to optimize data quality—a critical driver of model performance. Current data selection methods, such as natural language quality assessments, diversity-based filters, and classifier-based approaches, are limited by single-dimensional evaluation or redundancy-focused strategies. To address these gaps, we propose four dimensions to evaluate data quality: professionalism, readability, reasoning, and cleanliness. We further introduce <b>Meta-rater</b>, a multi-dimensional data selection method that integrates these dimensions with existing quality metrics through learned optimal weightings. Meta-rater employs proxy models to train a regression model that predicts validation loss, enabling the identification of optimal combinations of quality scores. Experiments demonstrate that Meta-rater <b>doubles convergence speed</b> for 1.3B parameter models and improves downstream task performance by <b>3.23%</b>, with advantages that scale to models as large as 7.2B parameters. Our work establishes that holistic, multi-dimensional quality integration significantly outperforms conventional single-dimension approaches, offering a scalable paradigm for enhancing pre-training efficiency and model capability. To advance future research, we release scripts, data, and models at <url>https://github.com/opendatalab/Meta-rater</url>.</abstract>
      <url hash="e562c454">2025.acl-long.533</url>
      <bibkey>zhuang-etal-2025-meta</bibkey>
    </paper>
    <paper id="534">
      <title><fixed-case>G</fixed-case>uess<fixed-case>A</fixed-case>rena: Guess Who <fixed-case>I</fixed-case> Am? A Self-Adaptive Framework for Evaluating <fixed-case>LLM</fixed-case>s in Domain-Specific Knowledge and Reasoning</title>
      <author><first>Qingchen</first><last>Yu</last></author>
      <author><first>Zifan</first><last>Zheng</last><affiliation>University of Sydney</affiliation></author>
      <author><first>Ding</first><last>Chen</last></author>
      <author><first>Simin</first><last>Niu</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Bo</first><last>Tang</last></author>
      <author><first>Feiyu</first><last>Xiong</last><affiliation>Institute for Advanced Algorithms Research, Shanghai</affiliation></author>
      <author><first>Zhiyu</first><last>Li</last></author>
      <pages>10897-10912</pages>
      <abstract>The evaluation of large language models (LLMs) has traditionally relied on static benchmarks, a paradigm that poses two major limitations: (1) predefined test sets lack adaptability to diverse application domains, and (2) standardized evaluation protocols often fail to capture fine-grained assessments of domain-specific knowledge and contextual reasoning abilities. To overcome these challenges, we propose GuessArena, an adaptive evaluation framework grounded in adversarial game-based interactions. Inspired by the interactive structure of the Guess Who I Am? game, our framework seamlessly integrates dynamic domain knowledge modeling with progressive reasoning assessment to improve evaluation fidelity. Empirical studies across five vertical domains-finance, healthcare, manufacturing, information technology, and education-demonstrate that GuessArena effectively distinguishes LLMs in terms of domain knowledge coverage and reasoning chain completeness. Compared to conventional benchmarks, our method provides substantial advantages in interpretability, scalability, and scenario adaptability.</abstract>
      <url hash="152779af">2025.acl-long.534</url>
      <bibkey>yu-etal-2025-guessarena</bibkey>
    </paper>
    <paper id="535">
      <title>Sample-Efficient Human Evaluation of Large Language Models via Maximum Discrepancy Competition</title>
      <author><first>Kehua</first><last>Feng</last></author>
      <author><first>Keyan</first><last>Ding</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Tan</first><last>Hongzhi</last><affiliation>Shanghai Electric Group Co.,Ltd. Central Academe</affiliation></author>
      <author><first>Kede</first><last>Ma</last><affiliation>City University of Hong Kong</affiliation></author>
      <author><first>Zhihua</first><last>Wang</last></author>
      <author><first>Shuangquan</first><last>Guo</last><affiliation>Shanghai Electric Group Co.,Ltd. Central Academe</affiliation></author>
      <author><first>Cheng</first><last>Yuzhou</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Ge</first><last>Sun</last></author>
      <author><first>Guozhou</first><last>Zheng</last></author>
      <author><first>Qiang</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Huajun</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <pages>10913-10947</pages>
      <abstract>The past years have witnessed a proliferation of large language models (LLMs). Yet, reliable evaluation of LLMs is challenging due to the inaccuracy of standard metrics in human perception of text quality and the inefficiency in sampling informative test examples for human evaluation. This paper presents a sample-efficient human evaluation method for LLMs based on the principle of MAximum Discrepancy (MAD) competition. MAD automatically selects a small set of informative input instructions, each of which maximizes the discrepancy of two LLMs’ reponses, which are subsequently subject to three-alternative forced choice by human subjects. The pairwise comparison results of multiple LLMs are then aggregated into a global ranking using the Elo rating system. We compare eight representative LLMs in terms of four skills: knowledge understanding, mathematical reasoning, writing, and coding. Experimental results show that the proposed method reliably achieves the “golden” ranking of LLMs with a minimum set of input instructions, which in turn reveal their relative strengths and weaknesses, and offers valuable insights for further LLM advancement.</abstract>
      <url hash="eb9524c8">2025.acl-long.535</url>
      <bibkey>feng-etal-2025-sample</bibkey>
    </paper>
    <paper id="536">
      <title><fixed-case>DTCRS</fixed-case>: Dynamic Tree Construction for Recursive Summarization</title>
      <author><first>Guanran</first><last>Luo</last></author>
      <author><first>Zhongquan</first><last>Jian</last></author>
      <author><first>Wentao</first><last>Qiu</last></author>
      <author><first>Meihong</first><last>Wang</last></author>
      <author><first>Qingqiang</first><last>Wu</last></author>
      <pages>10948-10963</pages>
      <abstract>Retrieval-Augmented Generation (RAG) mitigates the hallucination problem of Large Language Models (LLMs) by incorporating external knowledge. Recursive summarization constructs a hierarchical summary tree by clustering text chunks, integrating information from multiple parts of a document to provide evidence for abstractive questions involving multi-step reasoning. However, summary trees often contain a large number of redundant summary nodes, which not only increase construction time but may also negatively impact question answering. Moreover, recursive summarization is not suitable for all types of questions. We introduce DTCRS, a method that dynamically generates summary trees based on document structure and query semantics. DTCRS determines whether a summary tree is necessary by analyzing the question type. It then decomposes the question and uses the embeddings of sub-questions as initial cluster centers, reducing redundant summaries while improving the relevance between summaries and the question. Our approach significantly reduces summary tree construction time and achieves substantial improvements across three QA tasks. Additionally, we investigate the applicability of recursive summarization to different question types, providing valuable insights for future research.</abstract>
      <url hash="ce10c338">2025.acl-long.536</url>
      <bibkey>luo-etal-2025-dtcrs</bibkey>
    </paper>
    <paper id="537">
      <title>A Generative Adaptive Replay Continual Learning Model for Temporal Knowledge Graph Reasoning</title>
      <author><first>Zhiyu</first><last>Zhang</last></author>
      <author><first>Wei</first><last>Chen</last></author>
      <author><first>Youfang</first><last>Lin</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Huaiyu</first><last>Wan</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <pages>10964-10977</pages>
      <abstract>Recent Continual Learning (CL)-based Temporal Knowledge Graph Reasoning (TKGR) methods focus on significantly reducing computational cost and mitigating catastrophic forgetting caused by fine-tuning models with new data. However, existing CL-based TKGR methods still face two key limitations: (1) They usually one-sidedly reorganize individual historical facts, while overlooking the historical context essential for accurately understanding the historical semantics of these facts; (2) They preserve historical knowledge by simply replaying historical facts, while ignoring the potential conflicts between historical and emerging facts. In this paper, we propose a <tex-math>\textbf{D}</tex-math>eep <tex-math>\textbf{G}</tex-math>enerative <tex-math>\textbf{A}</tex-math>daptive <tex-math>\textbf{R}</tex-math>eplay (DGAR) method, which can generate and adaptively replay historical entity distribution representations from the whole historical context. To address the first challenge, historical context prompts as sampling units are built to preserve the whole historical context information. To overcome the second challenge, a pre-trained diffusion model is adopted to generate the historical distribution. During the generation process, the common features between the historical and current distributions are enhanced under the guidance of the TKGR model. In addition, a layer-by-layer adaptive replay mechanism is designed to effectively integrate historical and current distributions. Experimental results demonstrate that DGAR significantly outperforms baselines in reasoning and mitigating forgetting.</abstract>
      <url hash="955a678a">2025.acl-long.537</url>
      <bibkey>zhang-etal-2025-generative</bibkey>
    </paper>
    <paper id="538">
      <title><fixed-case>AR</fixed-case>ise: Towards Knowledge-Augmented Reasoning via Risk-Adaptive Search</title>
      <author><first>Yize</first><last>Zhang</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Tianshu</first><last>Wang</last></author>
      <author><first>Sirui</first><last>Chen</last><affiliation>Tongji University</affiliation></author>
      <author><first>Kun</first><last>Wang</last><affiliation>SenseTime Group Ltd</affiliation></author>
      <author><first>Xingyu</first><last>Zeng</last><affiliation>SenseTime Group Limited</affiliation></author>
      <author><first>Hongyu</first><last>Lin</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xianpei</first><last>Han</last><affiliation>Institute of Software, CAS</affiliation></author>
      <author><first>Le</first><last>Sun</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Chaochao</first><last>Lu</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <pages>10978-10995</pages>
      <abstract>Large language models (LLMs) have demonstrated impressive capabilities and are receiving increasing attention to enhance their reasoning through scaling test-time compute. However, their application in open-ended, knowledge-intensive, complex reasoning scenarios is still limited. Reasoning-oriented methods struggle to generalize to open-ended scenarios due to implicit assumptions of complete world knowledge. Meanwhile, knowledge-augmented reasoning (KAR) methods fails to address two core challenges: 1) error propagation, where errors in early steps cascade through the chain, and 2) verification bottleneck, where the explore–exploit trade-off arises in multi-branch decision processes. To overcome these limitations, we introduce ARise, a novel framework that integrates risk assessment of intermediate reasoning states with dynamic retrieval-augmented generation (RAG) within a Monte Carlo tree search paradigm. This approach enables effective construction and optimization of reasoning plans across multiple maintained hypothesis branches. Experimental results show that ARise significantly outperforms the state-of-the-art KAR methods by up to 23.10%, and the latest RAG-equipped large reasoning models by up to 25.37%. Our project page is at https://opencausalab.github.io/ARise.</abstract>
      <url hash="2fd0f5fb">2025.acl-long.538</url>
      <bibkey>zhang-etal-2025-arise</bibkey>
    </paper>
    <paper id="539">
      <title><fixed-case>PKAG</fixed-case>-<fixed-case>DDI</fixed-case>: Pairwise Knowledge-Augmented Language Model for Drug-Drug Interaction Event Text Generation</title>
      <author><first>Ziyan</first><last>Wang</last></author>
      <author><first>Zhankun</first><last>Xiong</last></author>
      <author><first>Feng</first><last>Huang</last></author>
      <author><first>Wen</first><last>Zhang</last><affiliation>Huazhong Agricultural University</affiliation></author>
      <pages>10996-11010</pages>
      <abstract>Drug-drug interactions (DDIs) arise when multiple drugs are administered concurrently. Accurately predicting the specific mechanisms underlying DDIs (named DDI events or DDIEs) is critical for the safe clinical use of drugs. DDIEs are typically represented as textual descriptions. However, most computational methods focus more on predicting the DDIE class label over generating human-readable natural language increasing clinicians’ interpretation costs. Furthermore, current methods overlook the fact that each drug assumes distinct biological functions in a DDI, which, when used as input context, can enhance the understanding of the DDIE process and benefit DDIE generation by the language model (LM). In this work, we propose a novel pairwise knowledge-augmented generative method (termed PKAG-DDI) for DDIE text generation. It consists of a pairwise knowledge selector efficiently injecting structural information between drugs bidirectionally and simultaneously to select pairwise biological functions from the knowledge set, and a pairwise knowledge integration strategy that matches and integrates the selected biological functions into the LM. Experiments on two professional datasets show that PKAG-DDI outperforms existing methods in DDIE text generation, especially in challenging inductive scenarios, indicating its practicality and generalization.</abstract>
      <url hash="bd85b474">2025.acl-long.539</url>
      <bibkey>wang-etal-2025-pkag</bibkey>
    </paper>
    <paper id="540">
      <title>Knowledge-Augmented Multimodal Clinical Rationale Generation for Disease Diagnosis with Small Language Models</title>
      <author><first>Shuai</first><last>Niu</last><affiliation>University of Hong Kong and Hong Kong Baptist University</affiliation></author>
      <author><first>Jing</first><last>Ma</last><affiliation>Hong Kong Baptist University</affiliation></author>
      <author><first>Hongzhan</first><last>Lin</last><affiliation>Hong Kong Baptist University</affiliation></author>
      <author><first>Liang</first><last>Bai</last></author>
      <author><first>Zhihua</first><last>Wang</last><affiliation>Shanghai Institute for Advanced Study of Zhejiang University</affiliation></author>
      <author><first>Richard Yi Da</first><last>Xu</last><affiliation>Hong Kong Baptist University</affiliation></author>
      <author><first>Yunya</first><last>Song</last></author>
      <author><first>Xian</first><last>Yang</last><affiliation>University of Manchester</affiliation></author>
      <pages>11011-11024</pages>
      <abstract>Interpretation is critical for disease diagnosis, but existing models struggle to balance predictive accuracy with human-understandable rationales. While large language models (LLMs) offer strong reasoning abilities, their clinical use is limited by high computational costs and restricted multimodal reasoning ability. Small language models (SLMs) are efficient but lack advanced reasoning for integrating multimodal medical data. In addition, both LLMs and SLMs lack domain knowledge for trustworthy reasoning. Therefore, we propose ClinRaGen, enhancing SLMs by leveraging LLM-derived reasoning ability via rationale distillation and domain knowledge injection for trustworthy multimodal rationale generation. Key innovations include a sequential rationale distillation framework that equips SLMs with LLM-comparable multimodal reasoning abilities, and a knowledge-augmented attention mechanism that jointly unifies multimodal representation from time series and textual data in the same encoding space, enabling it to be naturally interpreted by SLMs while incorporating domain knowledge for reliable rationale generation. Experiments on real-world medical datasets show that ClinRaGen achieves state-of-the-art performance in disease diagnosis and rationale generation, demonstrating the effectiveness of combining LLM-driven reasoning with knowledge augmentation for improved interpretability.</abstract>
      <url hash="cf810584">2025.acl-long.540</url>
      <bibkey>niu-etal-2025-knowledge</bibkey>
    </paper>
    <paper id="541">
      <title><fixed-case>TWIST</fixed-case>: Text-encoder Weight-editing for Inserting Secret Trojans in Text-to-Image Models</title>
      <author><first>Xindi</first><last>Li</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Zhe</first><last>Liu</last><affiliation>Zhejiang Lab</affiliation></author>
      <author><first>Tong</first><last>Zhang</last></author>
      <author><first>Jiahao</first><last>Chen</last></author>
      <author><first>Qingming</first><last>Li</last></author>
      <author><first>Jinbao</first><last>Li</last><affiliation>Qilu University of Technology(Shandong Academy of Science) and Shandong Artificial Intelligence Institute</affiliation></author>
      <author><first>Shouling</first><last>Ji</last><affiliation>Zhejiang University</affiliation></author>
      <pages>11025-11041</pages>
      <abstract>Text-to-image (T2I) models excel at generating high-quality images from text via powerful text encoders but training these encoders demands substantial computational resources. Consequently, many users seek pre-trained text encoders from model plugin-sharing platforms like Civitai and Hugging Face, which introduces an underexplored threat: the potential for adversaries to embed Trojans within these plugins. Existing Trojan attacks often require extensive training data and suffer from poor generalization across different triggers, limiting their effectiveness and scalability. To the best of our knowledge, this paper introduces the first **T**ext-encoder **W**eight-editing method for **I**nserting **S**ecret **T**rojans (**TWIST**). By identifying the *bottleneck MLP layer*—the critical point where minimal edits can dominantly control cross-modal alignment—TWIST achieves training-free and data-free Trojan insertion, which makes it highly efficient and practical. The experimental results across various triggers demonstrate that TWIST attains an average attack success rate of 91%, a 78% improvement over the state-of-the-art (SOTA) method proposed in 2024 and highlights the excellent generalization capability. Moreover, TWIST reduces modified parameters by 8-fold and cuts injection time to 25 seconds. Our findings underscore the security risks associated with text encoders in real-world applications and emphasize the need for more robust defense mechanisms.</abstract>
      <url hash="bc4b713f">2025.acl-long.541</url>
      <bibkey>li-etal-2025-twist</bibkey>
    </paper>
    <paper id="542">
      <title>Frictional Agent Alignment Framework: Slow Down and Don’t Break Things</title>
      <author><first>Abhijnan</first><last>Nath</last></author>
      <author><first>Carine</first><last>Graff</last><affiliation>Colorado State University</affiliation></author>
      <author><first>Andrei</first><last>Bachinin</last><affiliation>Colorado State University</affiliation></author>
      <author><first>Nikhil</first><last>Krishnaswamy</last><affiliation>Colorado State University</affiliation></author>
      <pages>11042-11089</pages>
      <abstract>AI support of collaborative interactions entails mediating potential misalignment between interlocutor beliefs. Common preference alignment methods like DPO excel in static settings, but struggle in dynamic collaborative tasks where the explicit signals of interlocutor beliefs are sparse and skewed. We propose the Frictional Agent Alignment Framework (FAAF), to generate precise, context-aware “friction” that prompts for deliberation and re-examination of existing evidence. FAAF’s two-player objective decouples from data skew: a frictive-state policy identifies belief misalignments, while an intervention policy crafts collaborator-preferred responses. We derive an analytical solution to this objective, enabling training a single policy via a simple supervised loss. Experiments on three benchmarks show FAAF outperforms competitors in producing concise, interpretable friction and in OOD generalization. By aligning LLMs to act as adaptive “thought partners”—not passive responders—FAAF advances scalable, dynamic human-AI collaboration. Our code and data can be found at https://github.com/csu-signal/FAAF_ACL.</abstract>
      <url hash="b1d1fe50">2025.acl-long.542</url>
      <bibkey>nath-etal-2025-frictional</bibkey>
    </paper>
    <paper id="543">
      <title>Powerformer: Efficient and High-Accuracy Privacy-Preserving Language Model with Homomorphic Encryption</title>
      <author><first>Dongjin</first><last>Park</last><affiliation>Chung-Ang University</affiliation></author>
      <author><first>Eunsang</first><last>Lee</last></author>
      <author><first>Joon-Woo</first><last>Lee</last><affiliation>Chung-Ang University</affiliation></author>
      <pages>11090-11111</pages>
      <abstract>We propose Powerformer, an efficient homomorphic encryption (HE)-based privacy-preserving language model (PPLM) designed to reduce computation overhead while maintaining model performance. Powerformer incorporates three key techniques to optimize encrypted computations:1. A novel distillation technique that replaces softmax and layer normalization (LN) with computationally efficient power and linear functions, ensuring no performance degradation while enabling seamless encrypted computation.2. A pseudo-sign composite approximation method that accurately approximates GELU and tanh functions with minimal computational overhead.3. A homomorphic matrix multiplication algorithm specifically optimized for Transformer models, enhancing efficiency in encrypted environments.By integrating these techniques, Powerformer based on the BERT-base model achieves a 45% reduction in computation time compared to the state-of-the-art HE-based PPLM without any loss in accuracy.</abstract>
      <url hash="44e9b97a">2025.acl-long.543</url>
      <bibkey>park-etal-2025-powerformer</bibkey>
    </paper>
    <paper id="544">
      <title>Beware of Your Po! Measuring and Mitigating <fixed-case>AI</fixed-case> Safety Risks in Role-Play Fine-Tuning of <fixed-case>LLM</fixed-case>s</title>
      <author><first>Weixiang</first><last>Zhao</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Yulin</first><last>Hu</last></author>
      <author><first>Yang</first><last>Deng</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Jiahe</first><last>Guo</last></author>
      <author><first>Xingyu</first><last>Sui</last></author>
      <author><first>Xinyang</first><last>Han</last></author>
      <author><first>An</first><last>Zhang</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Yanyan</first><last>Zhao</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Bing</first><last>Qin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Tat-Seng</first><last>Chua</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Ting</first><last>Liu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>11112-11137</pages>
      <abstract>Role-playing enables large language models (LLMs) to engage users in immersive and personalized interactions, but it also introduces significant safety risks. Existing role-play fine-tuning techniques improve role adaptability but may degrade safety performance, particularly for villainous characters. In this work, we conduct the first comprehensive assessment of role-play fine-tuning risks by training 95 role-specific LLMs using RoleBench. Our experiments reveal that role-play fine-tuning leads to a noticeable decline in safety performance, with safety risks varying based on character traits. To tackle this challenge, we propose Safety-Aware Role-Play Fine-Tuning (SaRFT), a novel method designed to balance role-playing capabilities and safety. Extensive experiments on LLaMA-3-8B-Instruct, Gemma-2-9B-it, and Qwen2.5-7B-Instruct demonstrate that SaRFT consistently outperforms state-of-the-art baselines under both LoRA and full-parameter fine-tuning settings. Our findings highlight the necessity of role-adaptive safety measures and provide insights into mitigating role-specific safety risks in role-playing LLMs.</abstract>
      <url hash="a552ef43">2025.acl-long.544</url>
      <bibkey>zhao-etal-2025-beware</bibkey>
    </paper>
    <paper id="545">
      <title>Can Graph Neural Networks Learn Language with Extremely Weak Text Supervision?</title>
      <author><first>Zihao</first><last>Li</last><affiliation>University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>Lecheng</first><last>Zheng</last></author>
      <author><first>Bowen</first><last>Jin</last></author>
      <author><first>Dongqi</first><last>Fu</last><affiliation>Meta</affiliation></author>
      <author><first>Baoyu</first><last>Jing</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <author><first>Yikun</first><last>Ban</last><affiliation>Beijing University of Aeronautics and Astronautics</affiliation></author>
      <author><first>Jingrui</first><last>He</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <pages>11138-11165</pages>
      <abstract>While great success has been achieved in building vision models with Contrastive Language-Image Pre-training (CLIP) over Internet-scale image-text pairs, building transferable Graph Neural Networks (GNNs) with CLIP pipeline is challenging because of the scarcity of labeled data and text supervision, different levels of downstream tasks, and the conceptual gaps between domains. In this work, to address these issues, we propose a multi-modal prompt learning paradigm to effectively adapt pre-trained GNN to downstream tasks and data, given only a few semantically labeled samples, each with extremely weak text supervision. Our new paradigm embeds the graphs directly in the same space as the Large Language Models (LLMs) by learning both graph prompts and text prompts simultaneously. We demonstrate the superior performance of our paradigm in few-shot, multi-task-level, and cross-domain settings. Moreover, we build the first CLIP-style zero-shot classification prototype that can generalize GNNs to unseen classes with extremely weak text supervision.</abstract>
      <url hash="9f5a2f85">2025.acl-long.545</url>
      <bibkey>li-etal-2025-graph-neural</bibkey>
    </paper>
    <paper id="546">
      <title>Towards Enhanced Immersion and Agency for <fixed-case>LLM</fixed-case>-based Interactive Drama</title>
      <author><first>Hongqiu</first><last>Wu</last></author>
      <author><first>Weiqi</first><last>Wu</last></author>
      <author><first>Tianyang</first><last>Xu</last></author>
      <author><first>Jiameng</first><last>Zhang</last></author>
      <author><first>Hai</first><last>Zhao</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>11166-11182</pages>
      <abstract>LLM-based Interactive Drama is a novel AI-based dialogue scenario, where the user (i.e. the player) plays the role of a character in the story, has conversations with characters played by LLM agents, and experiences an unfolding story. This paper begins with understanding interactive drama from two aspects: Immersion—the player’s feeling of being present in the story—and Agency—the player’s ability to influence the story world. Both are crucial to creating an enjoyable interactive experience, while they have been underexplored in previous work. To enhance these two aspects, we first propose Playwriting-guided Generation, a novel method that helps LLMs craft dramatic stories with substantially improved structures and narrative quality. Additionally, we introduce Plot-based Reflection for LLM agents to refine their reactions to align with the player’s intentions. Our evaluation relies on human judgment to assess the gains of our methods in terms of immersion and agency.</abstract>
      <url hash="068982fd">2025.acl-long.546</url>
      <bibkey>wu-etal-2025-towards-enhanced</bibkey>
    </paper>
    <paper id="547">
      <title>Disambiguating Reference in Visually Grounded Dialogues through Joint Modeling of Textual and Multimodal Semantic Structures</title>
      <author><first>Shun</first><last>Inadumi</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Nobuhiro</first><last>Ueda</last><affiliation>NEC</affiliation></author>
      <author><first>Koichiro</first><last>Yoshino</last><affiliation>Tokyo Institute of Technology/Institute of Science Tokyo and RIKEN</affiliation></author>
      <pages>11183-11198</pages>
      <abstract>Multimodal reference resolution, including phrase grounding, aims to understand the semantic relations between mentions and real-world objects. Phrase grounding between images and their captions is a well-established task. In contrast, for real-world applications, it is essential to integrate textual and multimodal reference resolution to unravel the reference relations within dialogue, especially in handling ambiguities caused by pronouns and ellipses. This paper presents a framework that unifies textual and multimodal reference resolution by mapping mention embeddings to object embeddings and selecting mentions or objects based on their similarity. Our experiments show that learning textual reference resolution, such as coreference resolution and predicate-argument structure analysis, positively affects performance in multimodal reference resolution. In particular, our model with coreference resolution performs better in pronoun phrase grounding than representative models for this task, MDETR and GLIP. Our qualitative analysis demonstrates that incorporating textual reference relations strengthens the confidence scores between mentions, including pronouns and predicates, and objects, which can reduce the ambiguities that arise in visually grounded dialogues.</abstract>
      <url hash="3edf46b4">2025.acl-long.547</url>
      <bibkey>inadumi-etal-2025-disambiguating</bibkey>
    </paper>
    <paper id="548">
      <title>Improving Factuality with Explicit Working Memory</title>
      <author><first>Mingda</first><last>Chen</last><affiliation>Meta FAIR</affiliation></author>
      <author><first>Yang</first><last>Li</last><affiliation>Facebook</affiliation></author>
      <author><first>Karthik</first><last>Padthe</last><affiliation>Meta AI</affiliation></author>
      <author><first>Rulin</first><last>Shao</last></author>
      <author><first>Alicia Yi</first><last>Sun</last><affiliation>Meta AI and Massachusetts Institute of Technology</affiliation></author>
      <author><first>Luke</first><last>Zettlemoyer</last><affiliation>University of Washington, Facebook and Meta</affiliation></author>
      <author><first>Gargi</first><last>Ghosh</last><affiliation>Meta AI</affiliation></author>
      <author><first>Wen-tau</first><last>Yih</last><affiliation>Meta Platforms, Inc.</affiliation></author>
      <pages>11199-11213</pages>
      <abstract>Large language models can generate factually inaccurate content, a problem known as hallucination. Recent works have built upon retrieved-augmented generation to improve factuality through iterative prompting but these methods are limited by the traditional RAG design. To address these challenges, we introduce Ewe (Explicit Working Memory), a novel approach that enhances factuality in long-form text generation by integrating a working memory that receives real-time feedback from external resources. The memory is refreshed based on online fact-checking and retrieval feedback, allowing Ewe to rectify false claims during the generation process and ensure more accurate and reliable outputs. Our experiments demonstrate that Ewe outperforms strong baselines on four fact-seeking long-form generation datasets, increasing the factuality metric, VeriScore, by 2 to 6 points absolute without sacrificing the helpfulness of the responses. Further analysis reveals that the design of rules for memory updates, configurations of memory units, and the quality of the retrieval datastore are crucial factors for influencing model performance.</abstract>
      <url hash="298836ca">2025.acl-long.548</url>
      <bibkey>chen-etal-2025-improving</bibkey>
    </paper>
    <paper id="549">
      <title>Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of Large Language Models</title>
      <author><first>Chengao</first><last>Li</last></author>
      <author><first>Hanyu</first><last>Zhang</last><affiliation>Institute of Computing Technology , Chinese Academy of Sciences</affiliation></author>
      <author><first>Yunkun</first><last>Xu</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Hongyan</first><last>Xue</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xiang</first><last>Ao</last><affiliation>University of the Chinese Academy of Sciences and Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Qing</first><last>He</last></author>
      <pages>11214-11232</pages>
      <abstract>Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful technique for aligning large language models (LLMs) with human preferences. However, effectively aligning LLMs with diverse human preferences remains a significant challenge, particularly when they are conflict. To address this issue, we frame human value alignment as a multi-objective optimization problem, aiming to maximize a set of potentially conflicting objectives. We introduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning paradigm that employs multiple-gradient descent to align LLMs with diverse preference distributions. GAPO adaptively rescales the gradients for each objective to determine an update direction that optimally balances the trade-offs between objectives. Additionally, we introduce P-GAPO, which incorporates user preferences across different objectives and achieves Pareto solutions that better align with the user’s specific needs.</abstract>
      <url hash="d4a61743">2025.acl-long.549</url>
      <bibkey>li-etal-2025-gradient</bibkey>
    </paper>
    <paper id="550">
      <title>Dynamic Parallel Tree Search for Efficient <fixed-case>LLM</fixed-case> Reasoning</title>
      <author><first>Yifu</first><last>Ding</last></author>
      <author><first>Wentao</first><last>Jiang</last></author>
      <author><first>Shunyu</first><last>Liu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Yongcheng</first><last>Jing</last></author>
      <author><first>Jinyang</first><last>Guo</last><affiliation>Beijing University of Aeronautics and Astronautics</affiliation></author>
      <author><first>Yingjie</first><last>Wang</last></author>
      <author><first>Jing</first><last>Zhang</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Zengmao</first><last>Wang</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Ziwei</first><last>Liu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Bo</first><last>Du</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Xianglong</first><last>Liu</last><affiliation>Beihang University</affiliation></author>
      <author><first>Dacheng</first><last>Tao</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>11233-11252</pages>
      <abstract>Tree of Thoughts (ToT) enhances Large Language Model (LLM) reasoning by structuring problem-solving as a spanning tree. However, recent methods focus on search accuracy while overlooking computational efficiency. The challenges of accelerating the ToT lie in the frequent switching of reasoning focus, and the redundant exploration of suboptimal solutions. To alleviate this dilemma, we propose Dynamic Parallel Tree Search (DPTS), a novel parallelism framework that aims to dynamically optimize the reasoning path in inference. It includes the Parallelism Streamline in the generation phase to build up a flexible and adaptive parallelism with arbitrary paths by cache management and alignment. Meanwhile, the Search and Transition Mechanism filters potential candidates to dynamically maintain the reasoning focus on more possible solutions with less redundancy. Experiments on Qwen-2.5 and Llama-3 on math and code datasets show that DPTS significantly improves efficiency by 2-4<tex-math>\times</tex-math> on average while maintaining or even surpassing existing reasoning algorithms in accuracy, making ToT-based reasoning more scalable and computationally efficient. Codes are released at: https://github.com/yifu-ding/DPTS.</abstract>
      <url hash="c499afcf">2025.acl-long.550</url>
      <bibkey>ding-etal-2025-dynamic</bibkey>
    </paper>
    <paper id="551">
      <title>Pre<tex-math>^3</tex-math>: Enabling Deterministic Pushdown Automata for Faster Structured <fixed-case>LLM</fixed-case> Generation</title>
      <author><first>Junyi</first><last>Chen</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Shihao</first><last>Bai</last></author>
      <author><first>Zaijun</first><last>Wang</last></author>
      <author><first>Siyu</first><last>Wu</last></author>
      <author><first>Chuheng</first><last>Du</last></author>
      <author><first>Hailong</first><last>Yang</last></author>
      <author><first>Ruihao</first><last>Gong</last></author>
      <author><first>Shengzhong</first><last>Liu</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Fan</first><last>Wu</last></author>
      <author><first>Guihai</first><last>Chen</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>11253-11267</pages>
      <abstract>Extensive LLM applications demand efficient structured generations, particularly for LR(1) grammars, to produce outputs in specified formats (e.g., JSON). Existing methods primarily parse LR(1) grammars into a pushdown automaton (PDA), leading to runtime execution overhead for context-dependent token processing, especially inefficient under large inference batches.To address these issues, we propose <tex-math>\text{Pre}^3</tex-math> that exploits deterministic pushdown automata (DPDA) to optimize the constrained LLM decoding efficiency.First, by **pre**computing **pre**fix-conditioned edges during the **pre**processing, <tex-math>\text{Pre}^3</tex-math> enables ahead-of-time edge analysis and thus makes parallel transition processing possible.Futher, leveraging the prefix-conditioned edges, <tex-math>\text{Pre}^3</tex-math> introduces a novel approach that transforms LR(1) transition graphs into DPDA, eliminating the need for runtime path exploration and achieving edge transitions with minimal overhead.<tex-math>\text{Pre}^3</tex-math> can be seamlessly integrated into standard LLM inference frameworks, improving time per output token (TPOT) by up to 40% and throughput by up to 36% in our experiments. Our code is available at https://github.com/ModelTC/lightllm.</abstract>
      <url hash="914a23f2">2025.acl-long.551</url>
      <bibkey>chen-etal-2025-pre3</bibkey>
    </paper>
    <paper id="552">
      <title><fixed-case>SHARE</fixed-case>: An <fixed-case>SLM</fixed-case>-based Hierarchical Action <fixed-case>C</fixed-case>or<fixed-case>RE</fixed-case>ction Assistant for Text-to-<fixed-case>SQL</fixed-case></title>
      <author><first>Ge</first><last>Qu</last></author>
      <author><first>Jinyang</first><last>Li</last></author>
      <author><first>Bowen</first><last>Qin</last><affiliation>Beijing Academy of Artificial Intelligence</affiliation></author>
      <author><first>Xiaolong</first><last>Li</last></author>
      <author><first>Nan</first><last>Huo</last></author>
      <author><first>Chenhao</first><last>Ma</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <author><first>Reynold</first><last>Cheng</last></author>
      <pages>11268-11292</pages>
      <abstract>Current self-correction approaches in text-to-SQL face two critical limitations: 1) Conventional self-correction methods rely on recursive self-calls of LLMs, resulting in multiplicative computational overhead, and 2) LLMs struggle to implement effective error detection and correction for monolithic SQL queries, as they fail to demonstrate the underlying reasoning path. In this work, we propose **SHARE**, a **S**LM-based **H**ierarchical **A**ction cor**RE**ction assistant that enables LLMs to perform more precise error localization and efficient correction. SHARE orchestrates three specialized Small Language Models (SLMs) in a sequential pipeline, where it first transforms monolithic SQL queries into stepwise action trajectories that reveal underlying reasoning, followed by a two-phase granular refinement. We further propose a novel hierarchical self-evolution strategy for data-efficient training. Our experimental results demonstrate that SHARE effectively enhances self-correction capabilities while proving robust across various LLMs. Furthermore, our comprehensive analysis shows that SHARE maintains strong performance even in low-resource training settings, which is particularly valuable for text-to-SQL applications with data privacy constraints.</abstract>
      <url hash="8d581d8b">2025.acl-long.552</url>
      <bibkey>qu-etal-2025-share</bibkey>
    </paper>
    <paper id="553">
      <title><fixed-case>G</fixed-case>ender<fixed-case>A</fixed-case>lign: An Alignment Dataset for Mitigating Gender Bias in Large Language Models</title>
      <author><first>Tao</first><last>Zhang</last></author>
      <author><first>Ziqian</first><last>Zeng</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>YuxiangXiao</first><last>YuxiangXiao</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Huiping</first><last>Zhuang</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Cen</first><last>Chen</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>James R.</first><last>Foulds</last><affiliation>University of Maryland, Baltimore County</affiliation></author>
      <author><first>Shimei</first><last>Pan</last><affiliation>Columbia University, University of Maryland, Baltimore County, IBM TJ Watson Research Center and Tsinghua University</affiliation></author>
      <pages>11293-11311</pages>
      <abstract>Large Language Models (LLMs) are prone to generating content that exhibits gender biases, raising significant ethical concerns. Alignment, the process of fine-tuning LLMs to better align with desired behaviors, is recognized as an effective approach to mitigate gender biases. Although proprietary LLMs have made significant strides in mitigating gender bias, their alignment datasets are not publicly available. The commonly used and publicly available alignment dataset, HH-RLHF, still exhibits gender bias to some extent. There is a lack of publicly available alignment datasets specifically designed to address gender bias. Hence, we developed a new dataset named GenderAlign, aiming at mitigating a comprehensive set of gender biases in LLMs. This dataset comprises 8k single-turn dialogues, each paired with a “chosen” and a “rejected” response. Compared to the “rejected” responses, the “chosen” responses demonstrate lower levels of gender bias and higher quality. Furthermore, we categorized the gender biases in the “rejected” responses of GenderAlign into 4 principal categories. The experimental results show the effectiveness of GenderAlign in reducing gender bias in LLMs.</abstract>
      <url hash="6bed77b1">2025.acl-long.553</url>
      <bibkey>zhang-etal-2025-genderalign</bibkey>
    </paper>
    <paper id="554">
      <title>Large Language and Protein Assistant for Protein-Protein Interactions Prediction</title>
      <author><first>Peng</first><last>Zhou</last></author>
      <author><first>Pengsen</first><last>Ma</last></author>
      <author><first>Jianmin</first><last>Wang</last></author>
      <author><first>Xibao</first><last>Cai</last></author>
      <author><first>Haitao</first><last>Huang</last></author>
      <author><first>Wei</first><last>Liu</last></author>
      <author><first>Longyue</first><last>Wang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Lai Hou</first><last>Tim</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Xiangxiang</first><last>Zeng</last><affiliation>Hunan University</affiliation></author>
      <pages>11312-11327</pages>
      <abstract>Predicting the types and affinities of protein-protein interactions (PPIs) is crucial for understanding biological processes and developing novel therapeutic approaches. While encoding proteins themselves is essential, PPI networks can also provide rich prior knowledge for these predictive tasks. However, existing methods oversimplify the problem of PPI prediction in a semi-supervised manner when utilizing PPI networks, limiting their practical application. Furthermore, how to effectively use the rich prior knowledge of PPI networks for novel proteins not present in the network remains an unexplored issue. Additionally, due to inflexible architectures, most of existing methods cannot handle complexes containing an flexible number of proteins. To overcome these limitations, we introduce LLaPA (Large Language and Protein Assistant), a multimodal large language model that integrates proteins and PPI networks. LLaPA offers a more rational approach to utilizing PPI networks for PPI prediction and can fully exploit the information of PPI networks for unseen proteins. Through natural language instructions, LLaPA can accept flexible number of protein sequences and has the potential to perform various protein tasks. Experiments show that LLaPA achieves state-of-the-art performance in multi-label PPI (mPPI) type prediction and is capable of predicting the binding affinity between multiple interacting proteins based on sequence data.</abstract>
      <url hash="86b7b77d">2025.acl-long.554</url>
      <bibkey>zhou-etal-2025-large</bibkey>
    </paper>
    <paper id="555">
      <title>An Empirical Study of Many-to-Many Summarization with Large Language Models</title>
      <author><first>Jiaan</first><last>Wang</last><affiliation>Tencent</affiliation></author>
      <author><first>Fandong</first><last>Meng</last><affiliation>WeChat AI, Tencent Inc.</affiliation></author>
      <author><first>Zengkui</first><last>Sun</last></author>
      <author><first>Yunlong</first><last>Liang</last></author>
      <author><first>Yuxuan</first><last>Cao</last></author>
      <author><first>Jiarong</first><last>Xu</last><affiliation>Fudan University</affiliation></author>
      <author><first>Haoxiang</first><last>Shi</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>11328-11344</pages>
      <abstract>Many-to-many summarization (M2MS) aims to process documents in any language and generate the corresponding summaries also in any language. Recently, large language models (LLMs) have shown strong multi-lingual abilities, giving them the potential to perform M2MS in real applications. This work presents a systematic empirical study on LLMs’ M2MS ability. Specifically, we first reorganize M2MS data based on eight previous domain-specific datasets. The reorganized data contains 47.8K samples spanning five domains and six languages, which could be used to train and evaluate LLMs. Then, we benchmark 18 LLMs in a zero-shot manner and an instruction-tuning manner. Fine-tuned traditional models (e.g., mBART) are also conducted for comparisons. Our experiments reveal that, zero-shot LLMs achieve competitive results with fine-tuned traditional models. After instruct-tuning, open-source LLMs can significantly improve their M2MS ability, and outperform zero-shot LLMs (including GPT-4) in terms of automatic evaluations. In addition, we demonstrate this task-specific improvement does not sacrifice the LLMs’ general task-solving abilities. However, as revealed by our human evaluation, LLMs still face the factuality issue, and the instruction tuning might intensify the issue. Thus, how to control factual errors becomes the key when building LLM summarizers in real applications, and is worthy to be noted in future research.</abstract>
      <url hash="0f060ccd">2025.acl-long.555</url>
      <bibkey>wang-etal-2025-empirical</bibkey>
    </paper>
    <paper id="556">
      <title>Locate-and-Focus: Enhancing Terminology Translation in Speech Language Models</title>
      <author><first>Suhang</first><last>Wu</last><affiliation>Xiamen University</affiliation></author>
      <author><first>Jialong</first><last>Tang</last></author>
      <author><first>Chengyi</first><last>Yang</last></author>
      <author><first>Pei</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Baosong</first><last>Yang</last></author>
      <author><first>Junhui</first><last>Li</last><affiliation>Soochow University, China</affiliation></author>
      <author><first>Junfeng</first><last>Yao</last><affiliation>Xiamen University</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <author><first>Jinsong</first><last>Su</last><affiliation>Xiamen University</affiliation></author>
      <pages>11345-11360</pages>
      <abstract>Direct speech translation (ST) has garnered increasing attention nowadays, yet the accurate translation of terminology within utterances remains a great challenge. In this regard, current studies mainly concentrate on leveraging various translation knowledge into ST models. However, these methods often struggle with interference from irrelevant noise and can not fully utilize the translation knowledge. To address these issues, in this paper, we propose a novel Locate-and-Focus method for terminology translation. It first effectively locates the speech clips containing terminologies within the utterance to construct translation knowledge, minimizing irrelevant information for the ST model. Subsequently, it associates the translation knowledge with the utterance and hypothesis from both audio and textual modalities, allowing the ST model to better focus on translation knowledge during translation. Experimental results across various datasets demonstrate that our method effectively locates terminologies within utterances and enhances the success rate of terminology translation, while maintaining robust general translation performance.</abstract>
      <url hash="1e4149c6">2025.acl-long.556</url>
      <bibkey>wu-etal-2025-locate</bibkey>
    </paper>
    <paper id="557">
      <title><fixed-case>G</fixed-case>uide<fixed-case>B</fixed-case>ench: Benchmarking Domain-Oriented Guideline Following for <fixed-case>LLM</fixed-case> Agents</title>
      <author><first>Lingxiao</first><last>Diao</last></author>
      <author><first>Xinyue</first><last>Xu</last></author>
      <author><first>Wanxuan</first><last>Sun</last></author>
      <author><first>Cheng</first><last>Yang</last></author>
      <author><first>Zhuosheng</first><last>Zhang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>11361-11399</pages>
      <abstract>Large language models (LLMs) have been widely deployed as autonomous agents capable of following user instructions and making decisions in real-world applications. Previous studies have made notable progress in benchmarking the instruction following capabilities of LLMs in general domains, with a primary focus on their inherent commonsense knowledge. Recently, LLMs have been increasingly deployed as domain-oriented agents, which rely on domain-oriented guidelines that may conflict with their commonsense knowledge. These guidelines exhibit two key characteristics: they consist of a wide range of domain-oriented rules and are subject to frequent updates. Despite these challenges, the absence of comprehensive benchmarks for evaluating the domain-oriented guideline following capabilities of LLMs presents a significant obstacle to their effective assessment and further development. In this paper, we introduce GuideBench, a comprehensive benchmark designed to evaluate guideline following performance of LLMs. GuideBench evaluates LLMs on three critical aspects: (i) adherence to diverse rules, (ii) robustness to rule updates, and (iii) alignment with human preferences. Experimental results on a range of LLMs indicate substantial opportunities for improving their ability to follow domain-oriented guidelines. Data and code are available at Anonymous.</abstract>
      <url hash="4b2764b8">2025.acl-long.557</url>
      <bibkey>diao-etal-2025-guidebench</bibkey>
    </paper>
    <paper id="558">
      <title><fixed-case>TC</fixed-case>–<fixed-case>RAG</fixed-case>: <fixed-case>T</fixed-case>uring–Complete <fixed-case>RAG</fixed-case>’s Case study on Medical <fixed-case>LLM</fixed-case> Systems</title>
      <author><first>Xinke</first><last>Jiang</last></author>
      <author><first>Yue</first><last>Fang</last><affiliation>Peking University</affiliation></author>
      <author><first>Rihong</first><last>Qiu</last></author>
      <author><first>Haoyu</first><last>Zhang</last></author>
      <author><first>Yongxin</first><last>Xu</last></author>
      <author><first>Hao</first><last>Chen</last><affiliation>University of the Chinese Academy of Sciences</affiliation></author>
      <author><first>Wentao</first><last>Zhang</last></author>
      <author><first>Ruizhe</first><last>Zhang</last></author>
      <author><first>Yuchen</first><last>Fang</last></author>
      <author><first>Xinyu</first><last>Ma</last></author>
      <author><first>Xu</first><last>Chu</last><affiliation>Peking University</affiliation></author>
      <author><first>Junfeng</first><last>Zhao</last><affiliation>Peking University</affiliation></author>
      <author><first>Yasha</first><last>Wang</last></author>
      <pages>11400-11426</pages>
      <abstract>In the pursuit of enhancing domain-specific Large Language Models (LLMs), Retrieval-Augmented Generation (RAG) emerges as a promising solution to mitigate issues such as hallucinations, outdated knowledge, and limited expertise in highly specialized queries. However, existing approaches to RAG fall short by neglecting system state variables, which are crucial for ensuring adaptive control, retrieval halting, and system convergence. In this paper, we introduce the Turing-Complete-RAG (TC-RAG) through rigorous proof, a novel framework that addresses these challenges by incorporating a Turing Complete System to manage state variables, thereby enabling more efficient and accurate knowledge retrieval. By leveraging a memory stack system with adaptive retrieval, reasoning, and planning capabilities, TC-RAG not only ensures the controlled halting of retrieval processes but also mitigates the accumulation of erroneous knowledge via Push and Pop actions. In the case study of the medical and general domain, our extensive experiments on seven real-world healthcare and general-domain datasets demonstrate the superiority of TC-RAG over existing methods in accuracy by over 7.20%. Our code, datasets and RAG resources have been available at https://github.com/Artessay/TC-RAG.</abstract>
      <url hash="0a579196">2025.acl-long.558</url>
      <bibkey>jiang-etal-2025-tc</bibkey>
    </paper>
    <paper id="559">
      <title><fixed-case>S</fixed-case>o<fixed-case>RFT</fixed-case>: Issue Resolving with Subtask-oriented Reinforced Fine-Tuning</title>
      <author><first>Zexiong</first><last>Ma</last></author>
      <author><first>Chao</first><last>Peng</last><affiliation>ByteDance</affiliation></author>
      <author><first>Pengfei</first><last>Gao</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Xiangxin</first><last>Meng</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Yanzhen</first><last>Zou</last><affiliation>Peking University</affiliation></author>
      <author><first>Bing</first><last>Xie</last><affiliation>Peking University</affiliation></author>
      <pages>11427-11441</pages>
      <abstract>Mainstream issue-resolving frameworks predominantly rely on commercial models, leading to high costs and privacy concerns. Existing training approaches for issue resolving struggle with poor generalization and fail to fully leverage open-source development resources. We propose **S**ubtask-**o**riented **R**einforced **F**ine-**T**uning (**SoRFT**), a novel training approach to enhance the issue resolving capability of LLMs. We decomposes issue resolving into structured subtasks: file localization, function localization, line localization, and code edit generation. SoRFT consists of two training stages: (1) **rejection-sampled supervised fine-tuning**, Chain of Thought (CoT) data is filtered using ground-truth before fine-tuning the LLM, and (2) **rule-based reinforcement learning**, which leverages PPO with ground-truth based rewards. We evaluate the SoRFT-trained model on SWE-Bench Verified and SWE-Bench Lite, achieving state-of-the-art (SOTA) performance among open-source models (e.g., resolve 21.4% issues on SWE-Bench Verified with SoRFT-Qwen-7B). The experimental results demonstrate that SoRFT significantly enhances issue-resolving performance, improves model generalization, and provides a cost-efficient alternative to commercial models.</abstract>
      <url hash="8756c903">2025.acl-long.559</url>
      <bibkey>ma-etal-2025-sorft</bibkey>
    </paper>
    <paper id="560">
      <title><fixed-case>M</fixed-case>ini<fixed-case>L</fixed-case>ong<fixed-case>B</fixed-case>ench: The Low-cost Long Context Understanding Benchmark for Large Language Models</title>
      <author><first>Zhongzhan</first><last>Huang</last><affiliation>Sun Yat-Sen University</affiliation></author>
      <author><first>Guoming</first><last>Ling</last></author>
      <author><first>Shanshan</first><last>Zhong</last></author>
      <author><first>Hefeng</first><last>Wu</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Liang</first><last>Lin</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <pages>11442-11460</pages>
      <abstract>Long Context Understanding (LCU) is a critical area for exploration in current large language models (LLMs). However, due to the inherently lengthy nature of long-text data, existing LCU benchmarks for LLMs often result in prohibitively high evaluation costs, like testing time and inference expenses. Through extensive experimentation, we discover that existing LCU benchmarks exhibit significant redundancy, which means the inefficiency in evaluation. In this paper, we propose a concise data compression method tailored for long-text data with sparse information characteristics. By pruning the well-known LCU benchmark LongBench, we create MiniLongBench. This benchmark includes only 237 test samples across six major task categories and 21 distinct tasks. Through empirical analysis of over 60 LLMs, MiniLongBench achieves an average evaluation cost reduced to only 4.5% of the original while maintaining an average rank correlation coefficient of 0.97 with LongBench results. Therefore, our MiniLongBench, as a low-cost benchmark, holds great potential to substantially drive future research into the LCU capabilities of LLMs.</abstract>
      <url hash="ba06c869">2025.acl-long.560</url>
      <bibkey>huang-etal-2025-minilongbench</bibkey>
    </paper>
    <paper id="561">
      <title>Divide-Then-Align: Honest Alignment based on the Knowledge Boundary of <fixed-case>RAG</fixed-case></title>
      <author><first>Xin</first><last>Sun</last></author>
      <author><first>Jianan</first><last>Xie</last></author>
      <author><first>Zhongqi</first><last>Chen</last></author>
      <author><first>Qiang</first><last>Liu</last><affiliation>Institute of Automation, Chinese Academy of Sciences</affiliation></author>
      <author><first>Shu</first><last>Wu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yuehe</first><last>Chen</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Bowen</first><last>Song</last><affiliation>Ant Group</affiliation></author>
      <author><first>Zilei</first><last>Wang</last></author>
      <author><first>Weiqiang</first><last>Wang</last><affiliation>Ant Group</affiliation></author>
      <author><first>Liang</first><last>Wang</last><affiliation>Institute of Automation, CAS,China</affiliation></author>
      <pages>11461-11480</pages>
      <abstract>Large language models (LLMs) augmented with retrieval systems have significantly advanced natural language processing tasks by integrating external knowledge sources, enabling more accurate and contextually rich responses. To improve the robustness of such systems against noisy retrievals, Retrieval-Augmented Fine-Tuning (RAFT) has emerged as a widely adopted method. However, RAFT conditions models to generate answers even in the absence of reliable knowledge. This behavior undermines their reliability in high-stakes domains, where acknowledging uncertainty is critical. To address this issue, we propose Divide-Then-Align (DTA), a post-training approach designed to endow RAG systems with the ability to respond with “I don’t know” when the query is out of the knowledge boundary of both the retrieved passages and the model’s internal knowledge. DTA divides data samples into four knowledge quadrants and constructs tailored preference data for each quadrant, resulting in a curated dataset for Direct Preference Optimization (DPO). Experimental results on three benchmark datasets demonstrate that effectively balances accuracy with appropriate abstention, enhancing the reliability and trustworthiness of retrieval-augmented systems.</abstract>
      <url hash="4496824b">2025.acl-long.561</url>
      <bibkey>sun-etal-2025-divide</bibkey>
    </paper>
    <paper id="562">
      <title><fixed-case>P</fixed-case>wn<fixed-case>GPT</fixed-case>: Automatic Exploit Generation Based on Large Language Models</title>
      <author><first>Wanzong</first><last>Peng</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Lin</first><last>Ye</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Xuetao</first><last>Du</last><affiliation>University of Mississippi Medical Center</affiliation></author>
      <author><first>Hongli</first><last>Zhang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Dongyang</first><last>Zhan</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Yunting</first><last>Zhang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Yicheng</first><last>Guo</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Chen</first><last>Zhang</last><affiliation>University of Mississippi Medical Center</affiliation></author>
      <pages>11481-11494</pages>
      <abstract>Automatic exploit generation (AEG) refers to the automatic discovery and exploitation of vulnerabilities against unknown targets. Traditional AEG often targets a single type of vulnerability and still relies on templates built from expert experience. To achieve intelligent exploit generation, we establish a comprehensive benchmark using Binary Exploitation (pwn) challenges in Capture the Flag (CTF) competitions and investigate the capabilities of Large Language Models (LLMs) in AEG based on the benchmark. To improve the performance of AEG, we propose PwnGPT, an LLM-based automatic exploit generation framework that automatically solves pwn challenges. The structural design of PwnGPT is divided into three main components: analysis, generation, and verification modules. With the help of a modular approach and structured problem inputs, PwnGPT can solve challenges that LLMs cannot directly solve. We evaluate PwnGPT on our benchmark and analyze the outputs of each module. Experimental results show that our framework is highly autonomous and capable of addressing various challenges. Compared to direct input LLMs, PwnGPT increases the completion rate of exploit on our benchmark from 26.3% to 57.9% with the OpenAI o1-preview model and from 21.1% to 36.8% with the GPT-4o model.</abstract>
      <url hash="02340cb6">2025.acl-long.562</url>
      <bibkey>peng-etal-2025-pwngpt</bibkey>
    </paper>
    <paper id="563">
      <title><fixed-case>VMLU</fixed-case> Benchmarks: A comprehensive benchmark toolkit for <fixed-case>V</fixed-case>ietnamese <fixed-case>LLM</fixed-case>s</title>
      <author><first>Cuc Thi</first><last>Bui</last></author>
      <author><first>Nguyen Truong</first><last>Son</last><affiliation>Ho Chi Minh city University of Science, Vietnam National University</affiliation></author>
      <author><first>Truong Van</first><last>Trang</last></author>
      <author><first>Lam Viet</first><last>Phung</last><affiliation>VNG Corporation</affiliation></author>
      <author><first>Pham Nhut</first><last>Huy</last></author>
      <author><first>Hoang Anh</first><last>Le</last><affiliation>VNG Corporation</affiliation></author>
      <author><first>Quoc Huu</first><last>Van</last><affiliation>VNG</affiliation></author>
      <author><first>Phong Nguyen-Thuan</first><last>Do</last><affiliation>Zalo</affiliation></author>
      <author><first>Van Le Tran</first><last>Truc</last><affiliation>Ho Chi Minh city University of Science, Vietnam National University</affiliation></author>
      <author><first>Duc Thanh</first><last>Chau</last><affiliation>Ho Chi Minh city University of Science, Vietnam National University</affiliation></author>
      <author><first>Le-Minh</first><last>Nguyen</last><affiliation>Japan Advanced Institute of Science and Technology, Tokyo Institute of Technology</affiliation></author>
      <pages>11495-11515</pages>
      <abstract>The evolution of Large Language Models (LLMs) has underscored the necessity for benchmarks designed for various languages and cultural contexts. To address this need for Vietnamese, we present the first Vietnamese Multitask Language Understanding (VMLU) Benchmarks. The VMLU benchmarks consist of four datasets that assess different capabilities of LLMs, including general knowledge, reading comprehension, reasoning, and conversational skills. This paper also provides an insightful overview of the current state of some dominant LLMs, such as Llama-3, Qwen2.5, and GPT-4, highlighting their performances and limitations when measured against these benchmarks. Furthermore, we provide insights into how prompt design can influence VMLU’s evaluation outcomes, as well as suggest that open-source LLMs can serve as effective, cost-efficient evaluators within the Vietnamese context. By offering a comprehensive and accessible benchmarking framework, the VMLU Benchmarks aim to foster the development and fine-tuning of Vietnamese LLMs, thereby establishing a foundation for their practical applications in language-specific domains.</abstract>
      <url hash="e596cafe">2025.acl-long.563</url>
      <bibkey>bui-etal-2025-vmlu</bibkey>
    </paper>
    <paper id="564">
      <title>Scaling up the State Size of <fixed-case>RNN</fixed-case> <fixed-case>LLM</fixed-case>s for Long-Context Scenarios</title>
      <author><first>Kai</first><last>Liu</last><affiliation>Tongji University and Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Jianfei</first><last>Gao</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Kai</first><last>Chen</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <pages>11516-11529</pages>
      <abstract>The Transformer architecture has become the standard LLM architecture due to its powerful self-attention mechanism. However, it suffers from quadratic computational complexity and linear memory complexity. RNN-based LLMs have been proposed as alternatives. Yet, RNN models struggle in long-context scenarios, making it challenging to replace self-attention with RNNs. We identify the state size as a critical bottleneck, which is significantly smaller than that of Transformers with a basic context length of 2k. However, simply increasing the state size significantly raises the number of parameters and lowers training efficiency. In this paper, we propose an efficient scaling method to scale the state size of RNN models to match the 2k context length of Transformers, with small parameters overhead. Experimental results demonstrate that scaling the state size significantly enhances long-context understanding. Retrieval performance scales almost linearly with state size, with a 454M model featuring an expanded state achieving performance comparable to a 1.47B model on FDA, a recall-intensive task. These findings highlight state scaling as a promising approach for advancing RNN-based LLMs.</abstract>
      <url hash="6641887c">2025.acl-long.564</url>
      <bibkey>liu-etal-2025-scaling</bibkey>
    </paper>
    <paper id="565">
      <title>Unifying Continuous and Discrete Text Diffusion with Non-simultaneous Diffusion Processes</title>
      <author><first>Bocheng</first><last>Li</last></author>
      <author><first>Zhujin</first><last>Gao</last></author>
      <author><first>Linli</first><last>Xu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>11530-11551</pages>
      <abstract>Diffusion models have emerged as a promising approach for text generation, with recent works falling into two main categories: discrete and continuous diffusion models. Discrete diffusion models apply token corruption independently using categorical distributions, allowing for different diffusion progress across tokens but lacking fine-grained control. Continuous diffusion models map tokens to continuous spaces and apply fine-grained noise, but the diffusion progress is uniform across tokens, limiting their ability to capture semantic nuances. To address these limitations, we propose <b>N</b>on-simultan<b>e</b>ous C<b>o</b>ntinuous <b>Diff</b>usion Models (NeoDiff), a novel diffusion model that integrates the strengths of both discrete and continuous approaches. NeoDiff introduces a Poisson diffusion process for the forward process, enabling a flexible and fine-grained noising paradigm, and employs a time predictor for the reverse process to adaptively modulate the denoising progress based on token semantics. Furthermore, NeoDiff utilizes an optimized schedule for inference to ensure more precise noise control and improved performance. Our approach unifies the theories of discrete and continuous diffusion models, offering a more principled and effective framework for text generation. Experimental results on several text generation tasks demonstrate NeoDiff’s superior performance compared to baselines of non-autoregressive continuous and discrete diffusion models, iterative-based methods and autoregressive diffusion-based methods. These results highlight NeoDiff’s potential as a powerful tool for generating high-quality text and advancing the field of diffusion-based text generation.</abstract>
      <url hash="2d658b81">2025.acl-long.565</url>
      <bibkey>li-etal-2025-unifying</bibkey>
    </paper>
    <paper id="566">
      <title>A Strategic Coordination Framework of Small <fixed-case>LM</fixed-case>s Matches Large <fixed-case>LM</fixed-case>s in Data Synthesis</title>
      <author><first>Xin</first><last>Gao</last></author>
      <author><first>Qizhi</first><last>Pei</last></author>
      <author><first>Zinan</first><last>Tang</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Yu</first><last>Li</last></author>
      <author><first>Honglin</first><last>Lin</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Jiang</first><last>Wu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Lijun</first><last>Wu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Conghui</first><last>He</last><affiliation>Shanghai AI Lab</affiliation></author>
      <pages>11552-11570</pages>
      <abstract>While data synthesis and distillation are promising strategies to enhance small language models, current approaches heavily rely on Large Language Models (LLMs), which suffer from high computational costs, environmental inefficiency, and potential biases inherited from monolithic architectures. In contrast, smaller LMs are more accessible and sustainable, but their individual capabilities often fall short in generating high-quality, diverse, and reliable data. Inspired by collaborative human processes (e.g., peer review), we propose a multiple small LMs involved framework, GRA, that aggregates specialized roles across small LMs to iterative refinement and quality control typically achieved by a single large LM. In this collaborative framework, multiple small LMs assume distinct roles—Generator, Reviewer, and Adjudicator—to simulate a peer-review-inspired data synthesis pipeline. The Generator proposes initial data samples, the Reviewer critiques their quality and diversity, and the Adjudicator resolves conflicts to finalize the output. By decomposing the synthesis process into specialized sub-tasks, collaborative small LMs can achieve data-level parity with distillation from large LMs. Through experiments across multiple benchmarks, we demonstrate that GRA-produced data matches or exceeds the quality of single large LM outputs, e.g., Qwen-2.5-72B-Instruct. Our results challenge the necessity of monolithic large models for high-quality data synthesis, advocating instead for strategic coordination of smaller agents.</abstract>
      <url hash="5285531d">2025.acl-long.566</url>
      <bibkey>gao-etal-2025-strategic</bibkey>
    </paper>
    <paper id="567">
      <title>Defining and Evaluating Visual Language Models’ Basic Spatial Abilities: A Perspective from Psychometrics</title>
      <author><first>Wenrui</first><last>Xu</last></author>
      <author><first>Dalin</first><last>Lyu</last></author>
      <author><first>Weihang</first><last>Wang</last></author>
      <author><first>Jie</first><last>Feng</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Chen</first><last>Gao</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Yong</first><last>Li</last></author>
      <pages>11571-11590</pages>
      <abstract>The Theory of Multiple Intelligences underscores the hierarchical nature of cognitive capabilities. To advance Spatial Artificial Intelligence, we pioneer a psychometric framework defining five Basic Spatial Abilities (BSAs) in Visual Language Models (VLMs): Spatial Perception, Spatial Relation, Spatial Orientation, Mental Rotation, and Spatial Visualization. Benchmarking 13 mainstream VLMs through nine validated psychometric experiments reveals significant gaps versus humans, with three key findings: 1) VLMs mirror human hierarchies (strongest in 2D orientation, weakest in 3D rotation) with independent BSAs; 2) Many smaller models surpass larger counterparts, with Qwen leading and InternVL2 lagging; 3) Interventions like CoT and few-shot training show limits from architectural constraints, while ToT demonstrates the most effective enhancement. Identified barriers include weak geometry encoding and missing dynamic simulation. By linking Psychometrics to VLMs, we provide a comprehensive BSA evaluation benchmark, a methodological perspective for embodied AI development, and a cognitive science-informed roadmap for achieving human-like spatial intelligence.</abstract>
      <url hash="eabf16f5">2025.acl-long.567</url>
      <bibkey>xu-etal-2025-defining</bibkey>
    </paper>
    <paper id="568">
      <title><fixed-case>SPHERE</fixed-case>: Unveiling Spatial Blind Spots in Vision-Language Models Through Hierarchical Evaluation</title>
      <author><first>Wenyu</first><last>Zhang</last><affiliation>I2R, A*STAR</affiliation></author>
      <author><first>Wei En</first><last>Ng</last></author>
      <author><first>Lixin</first><last>Ma</last></author>
      <author><first>Yuwen</first><last>Wang</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Junqi</first><last>Zhao</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Allison</first><last>Koenecke</last><affiliation>Cornell University</affiliation></author>
      <author><first>Boyang</first><last>Li</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Wanglu</first><last>Wanglu</last><affiliation>A*STAR, I2R</affiliation></author>
      <pages>11591-11609</pages>
      <abstract>Current vision-language models may grasp basic spatial cues and simple directions (e.g. left, right, front, back), but struggle with the multi-dimensional spatial reasoning necessary for human-like understanding and real-world applications. To address this gap, we develop SPHERE (Spatial Perception and Hierarchical Evaluation of REasoning), a hierarchical evaluation framework supported by a new human-annotated dataset. SPHERE systematically probes models across increasing levels of complexity, from fundamental skills to multi-skill integration and high-level reasoning that combines spatial, visual, and logical understanding. Benchmark evaluation of state-of-the-art models reveals significant deficiencies, especially in reasoning about distance and proximity, understanding both egocentric and allocentric perspectives, and applying spatial logic in physical contexts. These findings expose critical blind spots in existing models and underscore the need for more advanced spatial reasoning techniques, driving the development of vision-language models that align more closely with human spatial cognition.</abstract>
      <url hash="18228f21">2025.acl-long.568</url>
      <bibkey>zhang-etal-2025-sphere</bibkey>
    </paper>
    <paper id="569">
      <title>User-side Model Consistency Monitoring for Open Source Large Language Models Inference Services</title>
      <author><first>Qijun</first><last>Miao</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Zhixuan</first><last>Fang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>11610-11622</pages>
      <abstract>With the continuous advancement in the performance of open-source large language models (LLMs), their inference services have attracted a substantial user base by offering quality comparable to closed-source models at a significantly lower cost. However, it has also given rise to trust issues regarding model consistency between users and third-party service providers. Specifically, service providers can effortlessly degrade a model’s parameter scale or precision for more margin profits, and although users may perceptibly experience differences in text quality, they often lack a reliable method for concrete monitoring. To address this problem, we propose a paradigm for model consistency monitoring on the user side. It constructs metrics based on the logits produced by LLMs to differentiate sequences generated by degraded models. Furthermore, by leveraging model offloading techniques, we demonstrate that the proposed method is implementable on consumer-grade devices. Metric evaluations conducted on three widely used LLMs series (OPT, Llama 3.1 and Qwen 2.5) along with system prototype efficiency tests on a consumer device (RTX 3080 TI) confirm both the effectiveness and feasibility of the proposed approach.</abstract>
      <url hash="dd28b8cb">2025.acl-long.569</url>
      <bibkey>miao-fang-2025-user</bibkey>
    </paper>
    <paper id="570">
      <title>Jailbreaking? One Step Is Enough!</title>
      <author><first>Weixiong</first><last>Zheng</last><affiliation>Guangdong University of Technology</affiliation></author>
      <author><first>Peijian</first><last>Zeng</last></author>
      <author><first>YiWei</first><last>Li</last></author>
      <author><first>Hongyan</first><last>Wu</last></author>
      <author><first>Nankai</first><last>Lin</last><affiliation>Guangdong University of Foreign Studies</affiliation></author>
      <author><first>Junhao</first><last>Chen</last></author>
      <author><first>Aimin</first><last>Yang</last><affiliation>Lingnan Normal University and Guangdong University of Technology</affiliation></author>
      <author><first>Yongmei</first><last>Zhou</last></author>
      <pages>11623-11642</pages>
      <abstract>Large language models (LLMs) excel in various tasks but remain vulnerable to jailbreak attacks, where adversaries manipulate prompts to generate harmful outputs. Examining jailbreak prompts helps uncover the shortcomings of LLMs. However, current jailbreak methods and the target model’s defenses are engaged in an independent and adversarial process, resulting in the need for frequent attack iterations and redesigning attacks for different models. To address these gaps, we propose a Reverse Embedded Defense Attack (REDA) mechanism that disguises the attack intention as the “defense”. intention against harmful content. Specifically, REDA starts from the target response, guiding the model to embed harmful content within its defensive measures, thereby relegating harmful content to a secondary role and making the model believe it is performing a defensive task. The attacking model considers that it is guiding the target model to deal with harmful content, while the target model thinks it is performing a defensive task, creating an illusion of cooperation between the two. Additionally, to enhance the model’s confidence and guidance in “defensive” intentions, we adopt in-context learning (ICL) with a small number of attack examples and construct a corresponding dataset of attack examples. Extensive evaluations demonstrate that the REDA method enables cross-model attacks without the need to redesign attack strategies for different models, enables successful jailbreak in one iteration, and outperforms existing methods on both open-source and closed-source models.</abstract>
      <url hash="ee5e117b">2025.acl-long.570</url>
      <bibkey>zheng-etal-2025-jailbreaking</bibkey>
    </paper>
    <paper id="571">
      <title>Parenting: Optimizing Knowledge Selection of Retrieval-Augmented Language Models with Parameter Decoupling and Tailored Tuning</title>
      <author><first>Yongxin</first><last>Xu</last></author>
      <author><first>Ruizhe</first><last>Zhang</last></author>
      <author><first>Xinke</first><last>Jiang</last></author>
      <author><first>Yujie</first><last>Feng</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Yuzhen</first><last>Xiao</last></author>
      <author><first>Xinyu</first><last>Ma</last></author>
      <author><first>Runchuan</first><last>Zhu</last></author>
      <author><first>Xu</first><last>Chu</last><affiliation>Peking University</affiliation></author>
      <author><first>Junfeng</first><last>Zhao</last><affiliation>Peking University</affiliation></author>
      <author><first>Yasha</first><last>Wang</last></author>
      <pages>11643-11662</pages>
      <abstract>Retrieval-Augmented Generation (RAG) offers an effective solution to the issues faced by Large Language Models (LLMs) in hallucination generation and knowledge obsolescence by incorporating externally retrieved knowledge. However, existing methods lack effective control mechanisms for integrating internal and external knowledge. Inspired by human cognitive processes, we propose Parenting, a novel framework that decouples, identifies, and purposefully optimizes parameter subspaces related to adherence and robustness. Specifically, Parenting utilizes a key parameter mining method that combines forward and backward propagation signals to localize subspaces representing different capabilities. Then, Parenting employs a type-tailored tuning strategy, applying specific and appropriate optimizations to different subspaces, aiming to achieve a balanced enhancement of both adherence and robustness. Extensive experiments on various datasets and models validate the effectiveness and generalizability of our method. Our code is available at https://github.com/Nostradamus4869/Parenting.</abstract>
      <url hash="689d8594">2025.acl-long.571</url>
      <bibkey>xu-etal-2025-parenting</bibkey>
    </paper>
    <paper id="572">
      <title><fixed-case>P</fixed-case>a<fixed-case>S</fixed-case>a: An <fixed-case>LLM</fixed-case> Agent for Comprehensive Academic Paper Search</title>
      <author><first>Yichen</first><last>He</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Guanhua</first><last>Huang</last></author>
      <author><first>Peiyuan</first><last>Feng</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Yuan</first><last>Lin</last><affiliation>ByteDance</affiliation></author>
      <author><first>Yuchen</first><last>Zhang</last></author>
      <author><first>Hang</first><last>Li</last></author>
      <author><first>Weinan</first><last>E</last><affiliation>Peking University</affiliation></author>
      <pages>11663-11679</pages>
      <abstract>We introduce PaSa, an advanced Paper Search agent powered by large language models. PaSa can autonomously make a series of decisions, including invoking search tools, reading papers, and selecting relevant references, to ultimately obtain comprehensive and accurate results for complex scholar queries. We optimize PaSa using reinforcement learning with a synthetic dataset, AutoScholarQuery, which includes 35k fine-grained academic queries and corresponding papers sourced from top-tier AI conference publications. Additionally, we develop RealScholarQuery, a benchmark collecting real-world academic queries to assess PaSa performance in more realistic scenarios. Despite being trained on synthetic data, PaSa significantly outperforms existing baselines on RealScholarQuery, including Google, Google Scholar, Google with GPT-4o for paraphrased queries, ChatGPT (search-enabled GPT-4o), GPT-o1, and PaSa-GPT-4o (PaSa implemented by prompting GPT-4o). Notably, PaSa-7B surpasses the best Google-based baseline, Google with GPT-4o, by 37.78% in recall@20 and 39.90% in recall@50, and exceeds PaSa-GPT-4o by 30.36% in recall and 4.25% in precision. Model, datasets, and code are available at https://github.com/bytedance/pasa.Demo: https://pasa-agent.ai</abstract>
      <url hash="733980a8">2025.acl-long.572</url>
      <bibkey>he-etal-2025-pasa</bibkey>
    </paper>
    <paper id="573">
      <title>Less Mature is More Adaptable for Sentence-level Language Modeling</title>
      <author><first>Abhilasha</first><last>Sancheti</last><affiliation>Amazon</affiliation></author>
      <author><first>David</first><last>Dale</last><affiliation>FAIR at Meta</affiliation></author>
      <author><first>Artyom</first><last>Kozhevnikov</last></author>
      <author><first>Maha</first><last>Elbayad</last><affiliation>FAIR</affiliation></author>
      <pages>11680-11695</pages>
      <abstract>This work investigates sentence-level models (<tex-math>\textit{i.e.}</tex-math>, models that operate at the sentence-level) to study how sentence representations from various encoders influence downstream task performance, and which syntactic, semantic, and discourse-level properties are essential for strong performance. Our experiments encompass encoders with diverse training regimes and pretraining domains, as well as various pooling strategies applied to multi-sentence input tasks (including sentence ordering, sentiment classification, and natural language inference) requiring coarse-to-fine-grained reasoning. We find that ”less mature” representations (<tex-math>\textit{e.g.}</tex-math>, mean-pooled representations from BERT’s first or last layer, or representations from encoders with limited fine-tuning) exhibit greater generalizability and adaptability to downstream tasks compared to representations from extensively fine-tuned models (<tex-math>\textit{e.g.}</tex-math>, SBERT or SimCSE). These findings are consistent across different pretraining seed initializations for BERT. Our probing analysis reveals that syntactic and discourse-level properties are stronger indicators of downstream performance than MTEB scores or decodability. Furthermore, the data and time efficiency of sentence-level models, often outperforming token-level models, underscores their potential for future research.</abstract>
      <url hash="e6c83f0f">2025.acl-long.573</url>
      <bibkey>sancheti-etal-2025-less</bibkey>
    </paper>
    <paper id="574">
      <title><fixed-case>E</fixed-case>p<fixed-case>MAN</fixed-case>: Episodic Memory <fixed-case>A</fixed-case>ttentio<fixed-case>N</fixed-case> for Generalizing to Longer Contexts</title>
      <author><first>Subhajit</first><last>Chaudhury</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Payel</first><last>Das</last></author>
      <author><first>Sarathkrishna</first><last>Swaminathan</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Georgios</first><last>Kollias</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Elliot</first><last>Nelson</last></author>
      <author><first>Khushbu</first><last>Pahwa</last><affiliation>Amazon</affiliation></author>
      <author><first>Tejaswini</first><last>Pedapati</last></author>
      <author><first>Igor</first><last>Melnyk</last><affiliation>Capital One</affiliation></author>
      <author><first>Matthew</first><last>Riemer</last><affiliation>Montreal Institute for Learning Algorithms, University of Montreal, University of Montreal and International Business Machines</affiliation></author>
      <pages>11696-11708</pages>
      <abstract>Recent advances in Large Language Models (LLMs) have yielded impressive successes on many language tasks. However, efficient processing of long contexts using LLMs remains a significant challenge. We introduce **EpMAN** – a method for processing long contexts in an episodic memory module while holistically attending to semantically-relevant context chunks. Output from episodic attention is then used to reweigh the decoder’s self-attention to the stored KV cache of the context during training and generation. When an LLM decoder is trained using **EpMAN**, its performance on multiple challenging single-hop long-context recall and question-answering benchmarks is found to be stronger and more robust across the range from 16k to 256k tokens than baseline decoders trained with self-attention, and popular retrieval-augmented generation frameworks.</abstract>
      <url hash="4cc83c05">2025.acl-long.574</url>
      <bibkey>chaudhury-etal-2025-epman</bibkey>
    </paper>
    <paper id="575">
      <title><fixed-case>UORA</fixed-case>: Uniform Orthogonal Reinitialization Adaptation in Parameter Efficient Fine-Tuning of Large Models</title>
      <author><first>Xueyan</first><last>Zhang</last></author>
      <author><first>Jinman</first><last>Zhao</last></author>
      <author><first>Zhifei</first><last>Yang</last><affiliation>Peking University</affiliation></author>
      <author><first>Yibo</first><last>Zhong</last></author>
      <author><first>Shuhao</first><last>Guan</last></author>
      <author><first>Linbo</first><last>Cao</last></author>
      <author><first>Yining</first><last>Wang</last></author>
      <pages>11709-11728</pages>
      <abstract>This paper introduces UoRA, a novel parameter-efficient fine-tuning (PEFT) approach for large language models (LLMs). UoRA achieves state-of-the-art efficiency by leveraging a low-rank approximation method that reduces the number of trainable parameters without compromising performance. Unlike existing methods such as LoRA and VeRA, UoRA employs a re-parametrization mechanism that eliminates the need to adapt frozen projection matrices while maintaining shared projection layers across the model. This results in halving the trainable parameters compared to LoRA and outperforming VeRA in computation and storage efficiency. Comprehensive experiments across various benchmarks demonstrate UoRA’s superiority in achieving competitive fine-tuning performance with minimal computational overhead. We demonstrate its performance on GLUE and E2E benchmarks and is effectiveness in instruction-tuning large language models and image classification models. Our contributions establish a new paradigm for scalable and resource-efficient fine-tuning of LLMs.</abstract>
      <url hash="a2e190d1">2025.acl-long.575</url>
      <bibkey>zhang-etal-2025-uora</bibkey>
    </paper>
    <paper id="576">
      <title>Agri-<fixed-case>CM</fixed-case><tex-math>^3</tex-math>: A <fixed-case>C</fixed-case>hinese Massive Multi-modal, Multi-level Benchmark for Agricultural Understanding and Reasoning</title>
      <author><first>Haotian</first><last>Wang</last></author>
      <author><first>Yi</first><last>Guan</last><affiliation>Harbin institute of technology</affiliation></author>
      <author><first>Fanshu</first><last>Meng</last></author>
      <author><first>Chao</first><last>Zhao</last><affiliation>Google</affiliation></author>
      <author><first>Lian</first><last>Yan</last></author>
      <author><first>Yang</first><last>Yang</last></author>
      <author><first>Jingchi</first><last>Jiang</last></author>
      <pages>11729-11754</pages>
      <abstract>Multi-modal Large Language Models (MLLMs) integrating images, text, and speech can provide farmers with accurate diagnoses and treatment of pests and diseases, enhancing agricultural efficiency and sustainability. However, existing benchmarks lack comprehensive evaluations, particularly in multi-level reasoning, making it challenging to identify model limitations. To address this issue, we introduce Agri-CM<tex-math>^3</tex-math>, an expert-validated benchmark assessing MLLMs’ understanding and reasoning in agricultural management. It includes 3,939 images and 15,901 multi-level multiple-choice questions with detailed explanations. Evaluations of 45 MLLMs reveal significant gaps. Even GPT-4o achieves only 63.64% accuracy, falling short in fine-grained reasoning tasks. Analysis across three reasoning levels and seven compositional abilities highlights key challenges in accuracy and cognitive understanding. Our study provides insights for advancing MLLMs in agricultural management, driving their development and application. Code and data are available at <url>https://github.com/HIT-Kwoo/Agri-CM3</url>.</abstract>
      <url hash="6d5a6a1e">2025.acl-long.576</url>
      <bibkey>wang-etal-2025-agri</bibkey>
    </paper>
    <paper id="577">
      <title><fixed-case>TROVE</fixed-case>: A Challenge for Fine-Grained Text Provenance via Source Sentence Tracing and Relationship Classification</title>
      <author><first>Junnan</first><last>Zhu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Min</first><last>Xiao</last></author>
      <author><first>Yining</first><last>Wang</last></author>
      <author><first>Feifei</first><last>Zhai</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yu</first><last>Zhou</last><affiliation>Institute of Automation, Chinese Academy of Sciences</affiliation></author>
      <author><first>Chengqing</first><last>Zong</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <pages>11755-11771</pages>
      <abstract>LLMs have achieved remarkable fluency and coherence in text generation, yet their widespread adoption has raised concerns about content reliability and accountability. In high-stakes domains, it is crucial to understand where and how the content is created. To address this, we introduce the Text pROVEnance (TROVE) challenge, designed to trace each sentence of a target text back to specific source sentences within potentially lengthy or multi-document inputs. Beyond identifying sources, TROVE annotates the fine-grained relationships (quotation, compression, inference, and others), providing a deep understanding of how each target sentence is formed.To benchmark TROVE, we construct our dataset by leveraging three public datasets covering 11 diverse scenarios (e.g., QA and summarization) in English and Chinese, spanning source texts of varying lengths (0–5k, 5–10k, 10k+), emphasizing the multi-document and long-document settings essential for provenance. To ensure high-quality data, we employ a three-stage annotation process: sentence retrieval, GPT-4o provenance, and human provenance. We evaluate 11 LLMs under direct prompting and retrieval-augmented paradigms, revealing that retrieval is essential for robust performance, larger models perform better in complex relationship classification, and closed-source models often lead, yet open-source models show significant promise, particularly with retrieval augmentation. We make our dataset available here: https://github.com/ZNLP/ZNLP-Dataset.</abstract>
      <url hash="6b7365ed">2025.acl-long.577</url>
      <bibkey>zhu-etal-2025-trove</bibkey>
    </paper>
    <paper id="578">
      <title><fixed-case>C</fixed-case>a<fixed-case>LMQA</fixed-case>: Exploring culturally specific long-form question answering across 23 languages</title>
      <author><first>Shane</first><last>Arora</last></author>
      <author><first>Marzena</first><last>Karpinska</last><affiliation>Microsoft</affiliation></author>
      <author><first>Hung-Ting</first><last>Chen</last><affiliation>New York University</affiliation></author>
      <author><first>Ipsita</first><last>Bhattacharjee</last></author>
      <author><first>Mohit</first><last>Iyyer</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Eunsol</first><last>Choi</last><affiliation>New York University</affiliation></author>
      <pages>11772-11817</pages>
      <abstract>Despite rising global usage of large language models (LLMs), their ability to generate *long-form* answers to *culturally specific* questions remains unexplored in many languages. To fill this gap, we perform the first study of textual multilingual long-form QA by creating CaLMQA, a dataset of **51.7K** culturally specific questions across **23** different languages. We define culturally specific questions as those that refer to concepts unique to one or a few cultures, or have different answers depending on the cultural or regional context. We obtain these questions by crawling naturally-occurring questions from community web forums in high-resource languages, and by hiring native speakers to write questions in under-resourced, rarely-studied languages such as Fijian and Kirundi. Our data collection methodologies are translation-free, enabling the collection of culturally unique questions like “Kuber iki umwami wa mbere w’uburundi yitwa Ntare?” (Kirundi; English translation: “Why was the first king of Burundi called Ntare (Lion)?”). We evaluate factuality, relevance and surface-level quality of LLM-generated long-form answers, finding that (1) for many languages, even the best models make critical surface-level errors (e.g., answering in the wrong language, repetition), especially for low-resource languages; and (2) answers to culturally specific questions contain more factual errors than answers to culturally agnostic questions – questions that have consistent meaning and answer across many cultures. We release CaLMQA to facilitate future research in cultural and multilingual long-form QA.</abstract>
      <url hash="885077cc">2025.acl-long.578</url>
      <bibkey>arora-etal-2025-calmqa</bibkey>
    </paper>
    <paper id="579">
      <title>Croppable Knowledge Graph Embedding</title>
      <author><first>Yushan</first><last>Zhu</last></author>
      <author><first>Wen</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Zhiqiang</first><last>Liu</last></author>
      <author><first>Mingyang</first><last>Chen</last><affiliation>Baichuan Inc.</affiliation></author>
      <author><first>Lei</first><last>Liang</last></author>
      <author><first>Huajun</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <pages>11818-11835</pages>
      <abstract>Knowledge Graph Embedding (KGE) is a common approach for Knowledge Graphs (KGs) in AI tasks. Embedding dimensions depend on application scenarios. Requiring a new dimension means training a new KGE model from scratch, increasing cost and limiting efficiency and flexibility. In this work, we propose a novel KGE training framework MED. It allows one training to obtain a croppable KGE model for multiple scenarios with different dimensional needs. Sub-models of required dimensions can be directly cropped and used without extra training. In MED, we propose a mutual learning mechanism to improve the low-dimensional sub-models and make high-dimensional sub-models retain the low-dimensional sub-models’ capacity, an evolutionary improvement mechanism to promote the high-dimensional sub-models to master the triple that the low-dimensional sub-models can not, and a dynamic loss weight to adaptively balance the multiple losses. Experiments on 4 KGE models across 4 standard KG completion datasets, 3 real-world scenarios using a large-scale KG, and extending MED to the BERT language model demonstrate its effectiveness, high efficiency, and flexible extensibility.</abstract>
      <url hash="24145f3f">2025.acl-long.579</url>
      <bibkey>zhu-etal-2025-croppable</bibkey>
    </paper>
    <paper id="580">
      <title><fixed-case>H</fixed-case>y<fixed-case>KGE</fixed-case>: A Hypothesis Knowledge Graph Enhanced <fixed-case>RAG</fixed-case> Framework for Accurate and Reliable Medical <fixed-case>LLM</fixed-case>s Responses</title>
      <author><first>Xinke</first><last>Jiang</last></author>
      <author><first>Ruizhe</first><last>Zhang</last></author>
      <author><first>Yongxin</first><last>Xu</last></author>
      <author><first>Rihong</first><last>Qiu</last></author>
      <author><first>Yue</first><last>Fang</last><affiliation>Peking University</affiliation></author>
      <author><first>Zhiyuan</first><last>Wang</last></author>
      <author><first>Jinyi</first><last>Tang</last></author>
      <author><first>Hongxin</first><last>Ding</last></author>
      <author><first>Xu</first><last>Chu</last><affiliation>Peking University</affiliation></author>
      <author><first>Junfeng</first><last>Zhao</last><affiliation>Peking University</affiliation></author>
      <author><first>Yasha</first><last>Wang</last></author>
      <pages>11836-11856</pages>
      <abstract>In this paper, we investigate the retrieval-augmented generation (RAG) based on Knowledge Graphs (KGs) to improve the accuracy and reliability of Large Language Models (LLMs). Recent approaches suffer from insufficient and repetitive knowledge retrieval, tedious and time-consuming query parsing, and monotonous knowledge utilization. To this end, we develop a Hypothesis Knowledge Graph Enhanced (HyKGE) framework, which leverages LLMs’ powerful reasoning capacity to compensate for the incompleteness of user queries, optimizes the interaction process with LLMs, and provides diverse retrieved knowledge. Specifically, HyKGE explores the zero-shot capability and the rich knowledge of LLMs with Hypothesis Outputs to extend feasible exploration directions in the KGs, as well as the carefully curated prompt to enhance the density and efficiency of LLMs’ responses. Furthermore, we introduce the HO Fragment Granularity-aware Rerank Module to filter out noise while ensuring the balance between diversity and relevance in retrieved knowledge. Experiments on two Chinese medical multiple-choice question datasets and one Chinese open-domain medical Q&amp;A dataset with two LLM turbos demonstrate the superiority of HyKGE in terms of accuracy and explainability. Code is available at https://github.com/Artessay/HyKGE.</abstract>
      <url hash="4211bd8a">2025.acl-long.580</url>
      <bibkey>jiang-etal-2025-hykge</bibkey>
    </paper>
    <paper id="581">
      <title><fixed-case>L</fixed-case>ong<fixed-case>R</fixed-case>ecipe: Recipe for Efficient Long Context Generalization in Large Language Models</title>
      <author><first>Zhiyuan</first><last>Hu</last></author>
      <author><first>Yuliang</first><last>Liu</last></author>
      <author><first>Jinman</first><last>Zhao</last></author>
      <author><first>Suyuchen</first><last>Wang</last><affiliation>Université de Montréal and Mila - Quebec Artificial Intelligence Institute</affiliation></author>
      <author><first>WangYan</first><last>WangYan</last></author>
      <author><first>Wei</first><last>Shen</last></author>
      <author><first>Qing</first><last>Gu</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Anh Tuan</first><last>Luu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>See-Kiong</first><last>Ng</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Zhiwei</first><last>Jiang</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Bryan</first><last>Hooi</last><affiliation>National University of Singapore</affiliation></author>
      <pages>11857-11870</pages>
      <abstract>Large language models (LLMs) face significant challenges in handling long-context tasks because of their limited effective context window size during pretraining, which restricts their ability to generalize over extended sequences. Meanwhile, extending the context window in LLMs through post-pretraining is highly resource-intensive.To address this, we introduce LongRecipe, an efficient training strategy for extending the context window of LLMs, including impactful token analysis, position index transformation, and training optimization strategies. It simulates long-sequence inputs while maintaining training efficiency and significantly improves the model’s understanding of long-range dependencies. Experiments on three types of LLMs show that LongRecipe can utilize long sequences while requiring only 30% of the target context window size, and reduces computational training resource over 85% compared to full sequence training. Furthermore, LongRecipe also preserves the original LLM’s capabilities in general tasks. Ultimately, <i>we can extend effective context window of open-source LLMs from 8k to 128k, achieving performance close to GPT-4 with just one day of dedicated training using a single GPU with 80G memory.</i>Our code is released at https://github.com/zhiyuanhubj/LongRecipe.</abstract>
      <url hash="b86a5b55">2025.acl-long.581</url>
      <bibkey>hu-etal-2025-longrecipe</bibkey>
    </paper>
    <paper id="582">
      <title><fixed-case>B</fixed-case>eam<fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case>: Beam-Constraint Low-Rank Adaptation</title>
      <author><first>Naibin</first><last>Gu</last></author>
      <author><first>Zhenyu</first><last>Zhang</last><affiliation>Baidu Inc.</affiliation></author>
      <author><first>Xiyu</first><last>Liu</last></author>
      <author><first>Peng</first><last>Fu</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <author><first>Zheng</first><last>Lin</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <author><first>Shuohuan</first><last>Wang</last></author>
      <author><first>Yu</first><last>Sun</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <author><first>Weiping</first><last>Wang</last><affiliation>IIE</affiliation></author>
      <author><first>Haifeng</first><last>Wang</last><affiliation>Baidu</affiliation></author>
      <pages>11871-11883</pages>
      <abstract>Due to the demand for efficient fine-tuning of large language models, Low-Rank Adaptation (LoRA) has been widely adopted as one of the most effective parameter-efficient fine-tuning methods. Nevertheless, while LoRA improves efficiency, there remains room for improvement in accuracy. Herein, we adopt a novel perspective to assess the characteristics of LoRA ranks. The results reveal that different ranks within the LoRA modules not only exhibit varying levels of importance but also evolve dynamically throughout the fine-tuning process, which may limit the performance of LoRA. Based on these findings, we propose BeamLoRA, which conceptualizes each LoRA module as a beam where each rank naturally corresponds to a potential sub-solution, and the fine-tuning process becomes a search for the optimal sub-solution combination. BeamLoRA dynamically eliminates underperforming sub-solutions while expanding the parameter space for promising ones, enhancing performance with a fixed rank. Extensive experiments across three base models and 12 datasets spanning math reasoning, code generation, and commonsense reasoning demonstrate that BeamLoRA consistently enhances the performance of LoRA, surpassing the other baseline methods.</abstract>
      <url hash="1cdcbc03">2025.acl-long.582</url>
      <bibkey>gu-etal-2025-beamlora</bibkey>
    </paper>
    <paper id="583">
      <title><fixed-case>GODB</fixed-case>ench: A Benchmark for Multimodal Large Language Models in Video Comment Art</title>
      <author><first>Yiming</first><last>Lei</last><affiliation>Beijing University of Aeronautics and Astronautics</affiliation></author>
      <author><first>Chenkai</first><last>Zhang</last></author>
      <author><first>Zeming</first><last>Liu</last></author>
      <author><first>Haitao</first><last>Leng</last><affiliation>Kuaishou- 快手科技</affiliation></author>
      <author><first>ShaoGuo</first><last>Liu</last><affiliation>Alibaba Group and Kuaishou- 快手科技</affiliation></author>
      <author><first>Tingting</first><last>Gao</last><affiliation>Kuaishou- 快手科技</affiliation></author>
      <author><first>Qingjie</first><last>Liu</last><affiliation>Beihang University</affiliation></author>
      <author><first>Yunhong</first><last>Wang</last><affiliation>Beihang University</affiliation></author>
      <pages>11884-11952</pages>
      <abstract>***Video Comment Art*** enhances user engagement by providing creative content that conveys humor, satire, or emotional resonance, requiring a nuanced and comprehensive grasp of cultural and contextual subtleties. Although Multimodal Large Language Models (MLLMs) and Chain-of-Thought (CoT) have demonstrated strong reasoning abilities in STEM tasks (e.g. mathematics and coding), they still struggle to generate creative expressions such as resonant jokes and insightful satire. Moreover, existing benchmarks are constrained by their limited modalities and insufficient categories, hindering the exploration of comprehensive creativity in video-based Comment Art creation. To address these limitations, we introduce **GODBench**, a novel benchmark that integrates video and text modalities to systematically evaluate MLLMs’ abilities to compose Comment Art. Furthermore, inspired by the propagation patterns of waves in physics, we propose **Ripple of Thought (RoT)**, a multi-step reasoning framework designed to enhance the creativity of MLLMs. Extensive experiments on GODBench reveal that existing MLLMs and CoT methods still face significant challenges in understanding and generating creative video comments. In contrast, RoT provides an effective approach to improving creative composing, highlighting its potential to drive meaningful advancements in MLLM-based creativity.</abstract>
      <url hash="a1e803ba">2025.acl-long.583</url>
      <bibkey>lei-etal-2025-godbench</bibkey>
    </paper>
    <paper id="584">
      <title><fixed-case>U</fixed-case>ni<fixed-case>LR</fixed-case>: Unleashing the Power of <fixed-case>LLM</fixed-case>s on Multiple Legal Tasks with a Unified Legal Retriever</title>
      <author><first>Ang</first><last>Li</last></author>
      <author><first>Yiquan</first><last>Wu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Yifei</first><last>Liu</last></author>
      <author><first>Ming</first><last>Cai</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Lizhi</first><last>Qing</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Shihang</first><last>Wang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yangyang</first><last>Kang</last></author>
      <author><first>Chengyuan</first><last>Liu</last></author>
      <author><first>Fei</first><last>Wu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Kun</first><last>Kuang</last><affiliation>Zhejiang University</affiliation></author>
      <pages>11953-11967</pages>
      <abstract>Despite the impressive capabilities of LLMs, they often generate content with factual inaccuracies in LegalAI, which may lead to serious legal consequences. Retrieval-Augmented Generation (RAG), a promising approach, can conveniently integrate specialized knowledge into LLMs. In practice, there are diverse legal knowledge retrieval demands (e.g. law articles and similar cases). However, existing retrieval methods are either designed for general domains, struggling with legal knowledge, or tailored for specific legal tasks, unable to handle diverse legal knowledge types. Therefore, we propose a novel **Uni**fied **L**egal **R**etriever (UniLR) capable of performing multiple legal retrieval tasks for LLMs. Specifically, we introduce attention supervision to guide the retriever in focusing on key elements during knowledge encoding. Next, we design a graph-based method to integrate meta information through a heterogeneous graph, further enriching the knowledge representation. These two components work together to enable UniLR to capture the essence of knowledge hidden beneath formats. Extensive experiments on multiple datasets of common legal tasks demonstrate that UniLR achieves the best retrieval performance and can significantly enhance the performance of LLM.</abstract>
      <url hash="dd687c98">2025.acl-long.584</url>
      <bibkey>li-etal-2025-unilr</bibkey>
    </paper>
    <paper id="585">
      <title>Generative Psycho-Lexical Approach for Constructing Value Systems in Large Language Models</title>
      <author><first>Haoran</first><last>Ye</last><affiliation>Peking University</affiliation></author>
      <author><first>TianZe</first><last>Zhang</last></author>
      <author><first>Yuhang</first><last>Xie</last></author>
      <author><first>Liyuan</first><last>Zhang</last></author>
      <author><first>Yuanyi</first><last>Ren</last></author>
      <author><first>Xin</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Guojie</first><last>Song</last><affiliation>Peking University</affiliation></author>
      <pages>11968-11991</pages>
      <abstract>Values are core drivers of individual and collective perception, cognition, and behavior. Value systems, such as Schwartz’s Theory of Basic Human Values, delineate the hierarchy and interplay among these values, enabling cross-disciplinary investigations into decision-making and societal dynamics. Recently, the rise of Large Language Models (LLMs) has raised concerns regarding their elusive intrinsic values. Despite growing efforts in evaluating, understanding, and aligning LLM values, a psychologically grounded LLM value system remains underexplored. This study addresses the gap by introducing the Generative Psycho-Lexical Approach (GPLA), a scalable, adaptable, and theoretically informed method for constructing value systems. Leveraging GPLA, we propose a psychologically grounded five-factor value system tailored for LLMs. For systematic validation, we present three benchmarking tasks that integrate psychological principles with cutting-edge AI priorities. Our results reveal that the proposed value system meets standard psychological criteria, better captures LLM values, improves LLM safety prediction, and enhances LLM alignment, when compared to the canonical Schwartz’s values.</abstract>
      <url hash="90f4de6f">2025.acl-long.585</url>
      <bibkey>ye-etal-2025-generative</bibkey>
    </paper>
    <paper id="586">
      <title>Beyond Dialogue: A Profile-Dialogue Alignment Framework Towards General Role-Playing Language Model</title>
      <author><first>Yeyong</first><last>Yu</last></author>
      <author><first>Runsheng</first><last>Yu</last><affiliation>ByteDance</affiliation></author>
      <author><first>Haojie</first><last>Wei</last></author>
      <author><first>Zhanqiu</first><last>Zhang</last><affiliation>Individual Researcher</affiliation></author>
      <author><first>Quan</first><last>Qian</last><affiliation>Shanghai University</affiliation></author>
      <pages>11992-12022</pages>
      <abstract>The rapid advancement of large language models (LLMs) has revolutionized role-playing, enabling the development of general role-playing models. However, current role-playing training has two significant issues: (I) Using a predefined role profile to prompt dialogue training for specific scenarios usually leads to biases and even conflicts between the dialogue and the profile, resulting in training biases. (II) Models learn to imitate the role based solely on the profile, neglecting profile-dialogue alignment at the sentence level. To overcome the aforementioned hurdles, we propose a novel framework **Beyond Dialogue**, which introduces “beyond dialogue” tasks to align dialogue with profile traits for each scenario, eliminating biases during training. Furthermore, the framework achieves a sentence-level fine-grained alignment between profile and dialogue through an innovative prompting mechanism that generates reasoning data for training. Moreover, the aforementioned methods are fully automated and low-cost. Experimental results demonstrate our model excels in adhering to role profiles, outperforming most proprietary general and specialized role-playing baselines. The code and data are provided in https://github.com/yuyouyu32/BeyondDialogue.</abstract>
      <url hash="fba9d9e6">2025.acl-long.586</url>
      <bibkey>yu-etal-2025-beyond</bibkey>
    </paper>
    <paper id="587">
      <title><fixed-case>ACECODER</fixed-case>: Acing Coder <fixed-case>RL</fixed-case> via Automated Test-Case Synthesis</title>
      <author><first>Huaye</first><last>Zeng</last></author>
      <author><first>Dongfu</first><last>Jiang</last></author>
      <author><first>Haozhe</first><last>Wang</last><affiliation>INF</affiliation></author>
      <author><first>Ping</first><last>Nie</last></author>
      <author><first>Xiaotong</first><last>Chen</last><affiliation>ProtagoLabs, Inc. / NetMind</affiliation></author>
      <author><first>Wenhu</first><last>Chen</last><affiliation>University of Waterloo</affiliation></author>
      <pages>12023-12040</pages>
      <abstract>Most progress in recent coder models has been driven by supervised fine-tuning (SFT), while the potential of reinforcement learning (RL) remains largely unexplored, primarily due to the lack of reliable reward data/model in the code domain. In this paper, we address this challenge by leveraging automated large-scale test-case synthesis to enhance code model training. Specifically, we design a pipeline that generates extensive (question, test-cases) pairs from existing code data. Using these test cases, we construct preference pairs based on pass rates over sampled programs to train reward models with Bradley-Terry loss. It shows an average of 10-point improvement for Llama-3.1-8B-Ins and 5-point improvement for Qwen2.5-Coder-7B-Ins through best-of-32 sampling, making the 7B model on par with 236B DeepSeek-V2.5. Furthermore, we conduct reinforcement learning with both reward models and test-case pass rewards, leading to consistent improvements across HumanEval, MBPP, BigCodeBench, and LiveCodeBench (V4). Notably, we follow the R1-style training to start from Qwen2.5-Coder-base directly and show that our RL training can improve model on HumanEval-plus by over 25% and MBPP-plus by 6% for merely 80 optimization steps. We believe our results highlight the huge potential of reinforcement learning in coder models.</abstract>
      <url hash="cd4e4db2">2025.acl-long.587</url>
      <bibkey>zeng-etal-2025-acecoder</bibkey>
    </paper>
    <paper id="588">
      <title>Quantifying Semantic Emergence in Language Models</title>
      <author><first>Hang</first><last>Chen</last></author>
      <author><first>Xinyu</first><last>Yang</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Jiaying</first><last>Zhu</last></author>
      <author><first>Wenya</first><last>Wang</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>12041-12054</pages>
      <abstract>Large language models (LLMs) are widely recognized for their exceptional capacity to capture semantics meaning. Yet, there remains no established metric to quantify this capability. In this work, we introduce a quantitative metric, Information Emergence (IE), designed to measure LLMs’ ability to extract semantics from input tokens. We formalize “semantics” as the meaningful information abstracted from a sequence of tokens and quantify this by comparing the entropy reduction observed for a sequence of tokens (macro-level) and individual tokens (micro-level). To achieve this, we design a lightweight estimator to compute the mutual information at each transformer layer, which is agnostic to different tasks and language model architectures. We apply IE in both synthetic in-context learning (ICL) scenarios and natural sentence contexts. Experiments demonstrate informativeness and patterns about semantics. While some of these patterns confirm the conventional prior linguistic knowledge, the rest are relatively unexpected, which may provide new insights.</abstract>
      <url hash="ba58deca">2025.acl-long.588</url>
      <bibkey>chen-etal-2025-quantifying-semantic</bibkey>
    </paper>
    <paper id="589">
      <title><fixed-case>D</fixed-case>ebate<fixed-case>C</fixed-case>oder: Towards Collective Intelligence of <fixed-case>LLM</fixed-case>s via Test Case Driven <fixed-case>LLM</fixed-case> Debate for Code Generation</title>
      <author><first>Jizheng</first><last>Chen</last></author>
      <author><first>Kounianhua</first><last>Du</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Xinyi</first><last>Dai</last></author>
      <author><first>Weiming</first><last>Zhang</last></author>
      <author><first>Xihuai</first><last>Wang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Yasheng</first><last>Wang</last></author>
      <author><first>Ruiming</first><last>Tang</last></author>
      <author><first>Weinan</first><last>Zhang</last></author>
      <author><first>Yong</first><last>Yu</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <pages>12055-12065</pages>
      <abstract>With the impressive reasoning and text generation capabilities of large language models (LLMs), methods leveraging multiple LLMs to debate each other have garnered increasing attention. However, existing debate-based approaches remain limited in effectiveness in structured and detailed domains represented by code generation due to several reasons: 1) Reliance on different instances of the same LLM for debate, neglecting the potential benefits of integrating diverse models with varied internal knowledge for more comprehensive code generation, 2) under-utilization of test cases, and 3) reliance on third-party LLM moderators for result consolidation and decision-making, probably introducing hallucinations and judgment errors. To address these challenges, we propose DebateCoder to collect intelligence of LLMs via test case-driven debate for code generation. In DebateCoder, test cases serve as a medium for models to analyze code and identify bugs, while opposing models generate test cases to challenge each other’s code during the debate process. These test cases, along with their execution results, are elaborately leveraged to refine and enhance the code through a novel contrastive analysis process. Furthermore, DebateCoder leverages test case outcomes to assess code quality and determine convergence criteria. Unlike previous approaches, DebateCoder emphasizes the collaborative improvement of both models through competitive debate and interactive analysis. Abundant experimental results on two datasets demonstrate the effectiveness of DebateCoder.</abstract>
      <url hash="506e5c35">2025.acl-long.589</url>
      <bibkey>chen-etal-2025-debatecoder</bibkey>
    </paper>
    <paper id="590">
      <title>The Tug of War Within: Mitigating the Fairness-Privacy Conflicts in Large Language Models</title>
      <author><first>Chen</first><last>Qian</last></author>
      <author><first>Dongrui</first><last>Liu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Jie</first><last>Zhang</last><affiliation>Chinese Academy of Sciences</affiliation></author>
      <author><first>Yong</first><last>Liu</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Jing</first><last>Shao</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <pages>12066-12095</pages>
      <abstract>Ensuring awareness of fairness and privacy in Large Language Models (LLMs) is critical. Interestingly, we discover a counter-intuitive trade-off phenomenon that enhancing an LLM’s privacy awareness through Supervised Fine-Tuning (SFT) methods significantly decreases its fairness awareness with thousands of samples. To address this issue, inspired by the information theory, we introduce a training-free method to <b>S</b>uppress the <b>P</b>rivacy and fa<b>I</b>rness coupled <b>N</b>eurons (<b>SPIN</b>), which theoretically and empirically decrease the mutual information between fairness and privacy awareness. Extensive experimental results demonstrate that SPIN eliminates the trade-off phenomenon and significantly improves LLMs’ fairness and privacy awareness simultaneously without compromising general capabilities, e.g., improving Qwen-2-7B-Instruct’s fairness awareness by 12.2% and privacy awareness by 14.0%.More crucially, SPIN remains robust and effective with limited annotated data or even when only malicious fine-tuning data is available, whereas SFT methods may fail to perform properly in such scenarios. Furthermore, we show that SPIN could generalize to other potential trade-off dimensions.We hope this study provides valuable insights into concurrently addressing fairness and privacy concerns in LLMs and can be integrated into comprehensive frameworks to develop more ethical and responsible AI systems. Our code is available at <url>https://github.com/ChnQ/SPIN</url>.</abstract>
      <url hash="392b2f08">2025.acl-long.590</url>
      <bibkey>qian-etal-2025-tug</bibkey>
    </paper>
    <paper id="591">
      <title><fixed-case>G</fixed-case>raph<fixed-case>I</fixed-case>nsight: Unlocking Insights in Large Language Models for Graph Structure Understanding</title>
      <author><first>Yukun</first><last>Cao</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Shuo</first><last>Han</last></author>
      <author><first>Zengyi</first><last>Gao</last></author>
      <author><first>Zezhong</first><last>Ding</last></author>
      <author><first>Xike</first><last>Xie</last></author>
      <author><first>S Kevin</first><last>Zhou</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>12096-12134</pages>
      <abstract>Although Large Language Models (LLMs) have demonstrated potential in processing graphs, they struggle with comprehending graphical structure information through prompts of graph description sequences, especially as the graph size increases. We attribute this challenge to the uneven memory performance of LLMs across different positions in graph description sequences, known as ”Positional bias”. To address this, we propose GraphInsight, a novel framework aimed at improving LLMs’ comprehension of both macro- and micro-level graphical information. GraphInsight is grounded in two key strategies: 1) placing critical graphical information in positions where LLMs exhibit stronger memory performance, and 2) investigating a lightweight external knowledge base for regions with weaker memory performance, inspired by retrieval-augmented generation (RAG). Moreover, GraphInsight explores integrating these two strategies into LLM agent processes for composite graph tasks that require multi-step reasoning. Extensive empirical studies on benchmarks with a wide range of evaluation tasks show that GraphInsight significantly outperforms all other graph description methods (e.g., prompting techniques and reordering strategies) in understanding graph structures of varying sizes.</abstract>
      <url hash="c42530de">2025.acl-long.591</url>
      <bibkey>cao-etal-2025-graphinsight</bibkey>
    </paper>
    <paper id="592">
      <title>Phonotomizer: A Compact, Unsupervised, Online Training Approach to Real-Time, Multilingual Phonetic Segmentation</title>
      <author><first>Michael S.</first><last>Yantosca</last><affiliation>University of Houston</affiliation></author>
      <author><first>Albert M. K.</first><last>Cheng</last><affiliation>University of Houston</affiliation></author>
      <pages>12135-12147</pages>
      <abstract>Phonetic transcription requires significant time and expert training. Automated, state-of-the-art text-dependent methods still involve substantial pre-training annotation labor and may not generalize to multiple languages. Hallucination of speech amid silence or non-speech noise can also plague these methods, which fall short in real-time applications due to post hoc whole-phrase evaluation. This paper introduces Phonotomizer, a compact, unsupervised, online training approach to automatic, multilingual phonetic segmentation, a critical first stage in transcription. Unlike prior approaches, Phonotomizer trains on raw sound files alone and can modulate computational exactness. Preliminary evaluations on Irish and Twi, two underrepresented languages, exhibit segmentation comparable to current forced alignment technology, reducing acoustic model size and minimizing training epochs.</abstract>
      <url hash="f0f68b6b">2025.acl-long.592</url>
      <bibkey>yantosca-cheng-2025-phonotomizer</bibkey>
    </paper>
    <paper id="593">
      <title>A Multi-persona Framework for Argument Quality Assessment</title>
      <author><first>Bojun</first><last>Jin</last></author>
      <author><first>Jianzhu</first><last>Bao</last></author>
      <author><first>Yufang</first><last>Hou</last><affiliation>IT:U Interdisciplinary Transformation University Austria, Technische Universität Darmstadt and IBM Research Ireland</affiliation></author>
      <author><first>Yang</first><last>Sun</last></author>
      <author><first>Yice</first><last>Zhang</last></author>
      <author><first>Huajie</first><last>Wang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Bin</first><last>Liang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Ruifeng</first><last>Xu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>12148-12170</pages>
      <abstract>Argument quality assessment faces inherent challenges due to its subjective nature, where different evaluators may assign varying quality scores for an argument based on personal perspectives. Although existing datasets collect opinions from multiple annotators to model subjectivity, most existing computational methods fail to consider multi-perspective evaluation. To address this issue, we propose MPAQ, a multi-persona framework for argument quality assessment that simulates diverse evaluator perspectives through large language models. It first dynamically generates targeted personas tailored to an input argument, then simulates each persona’s reasoning process to evaluate the argument quality from multiple perspectives. To effectively generate fine-grained quality scores, we develop a coarse-to-fine scoring strategy that first generates a coarse-grained integer score and then refines it into a fine-grained decimal score. Experiments on IBM-Rank-30k and IBM-ArgQ-5.3kArgs datasets demonstrate that MPAQ consistently outperforms strong baselines while providing comprehensive multi-perspective rationales.</abstract>
      <url hash="63d505e8">2025.acl-long.593</url>
      <bibkey>jin-etal-2025-multi</bibkey>
    </paper>
    <paper id="594">
      <title>Safe: Enhancing Mathematical Reasoning in Large Language Models via Retrospective Step-aware Formal Verification</title>
      <author><first>Chengwu</first><last>Liu</last><affiliation>Peking University</affiliation></author>
      <author><first>Ye</first><last>Yuan</last></author>
      <author><first>Yichun</first><last>Yin</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Yan</first><last>Xu</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Xin</first><last>Xu</last></author>
      <author><first>Zaoyu</first><last>Chen</last></author>
      <author><first>Yasheng</first><last>Wang</last></author>
      <author><first>Lifeng</first><last>Shang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Qun</first><last>Liu</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Ming</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <pages>12171-12186</pages>
      <abstract>Chain-of-Thought (CoT) prompting has become the de facto method to elicit reasoning capabilities from large language models (LLMs). However, to mitigate hallucinations in CoT that are notoriously difficult to detect, current methods such as process reward models (PRMs) or self-consistency operate as opaque boxes and do not provide checkable evidence for their judgments, possibly limiting their effectiveness. To address this issue, we draw inspiration from the idea that “the gold standard for supporting a mathematical claim is to provide a proof”. We propose a retrospective, step-aware formal verification framework Safe. Rather than assigning arbitrary scores, we strive to articulate mathematical claims in formal mathematical language Lean 4 at each reasoning step and provide formal proofs to identify hallucinations. We evaluate our framework Safe across multiple language models and various mathematical datasets, demonstrating a significant performance improvement while offering interpretable and verifiable evidence. We also propose FormalStep as a benchmark for step correctness theorem proving with 30,809 formal statements. To the best of our knowledge, our work represents the first endeavor to utilize formal mathematical language Lean 4 for verifying content generated by LLMs, aligning with the reason why formal mathematical languages were created in the first place: to provide a robust foundation for hallucination-prone human-written proofs.</abstract>
      <url hash="99152a1d">2025.acl-long.594</url>
      <bibkey>liu-etal-2025-safe</bibkey>
    </paper>
    <paper id="595">
      <title><fixed-case>SAM</fixed-case> Decoding: Speculative Decoding via Suffix Automaton</title>
      <author><first>Yuxuan</first><last>Hu</last></author>
      <author><first>Ke</first><last>Wang</last></author>
      <author><first>Xiaokang</first><last>Zhang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Fanjin</first><last>Zhang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Cuiping</first><last>Li</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Hong</first><last>Chen</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Jing</first><last>Zhang</last><affiliation>Renmin University of China</affiliation></author>
      <pages>12187-12204</pages>
      <abstract>Speculative decoding (SD) has been demonstrated as an effective technique for lossless LLM inference acceleration.Retrieval-based SD methods, one kind of model-free method, have yielded promising speedup, but they often rely on single retrieval resources, inefficient retrieval methods, and are constrained to certain tasks. This paper presents a novel retrieval-based speculative decoding method that adapts the suffix automaton (SAM) for efficient and accurate draft generation by utilizing the generating text sequence and static text corpus. Unlike existing <tex-math>n</tex-math>-gram matching methods, SAM-Decoding finds the exact longest suffix match, achieving an average time complexity of O(1) per generation step of SAM update and suffix retrieval.It can also integrate with existing methods, adaptively selecting a draft generation strategy based on match length to generalize to broader domains. Extensive experiments on Spec-Bench show that our method is 18% faster than other retrieval-based SD methods. Additionally, when combined with advanced EAGLE-2, it provides an additional speedup of 3.28% – 11.13% across various-sized LLM backbones.</abstract>
      <url hash="b7ad92d3">2025.acl-long.595</url>
      <bibkey>hu-etal-2025-sam</bibkey>
    </paper>
    <paper id="596">
      <title><fixed-case>P</fixed-case>sy<fixed-case>A</fixed-case>dvisor: A Plug-and-Play Strategy Advice Planner with Proactive Questioning in Psychological Conversations</title>
      <author><first>Yuxin</first><last>Hu</last><affiliation>Southeast Community College Area</affiliation></author>
      <author><first>Danni</first><last>Liu</last></author>
      <author><first>Bo</first><last>Liu</last></author>
      <author><first>Yida</first><last>Chen</last><affiliation>Southeast University</affiliation></author>
      <author><first>Jiuxin</first><last>Cao</last><affiliation>Southeast University</affiliation></author>
      <author><first>Yan</first><last>Liu</last></author>
      <pages>12205-12229</pages>
      <abstract>Proactive questioning is essential in psychological conversations as it helps uncover deeper issues and unspoken concerns. Current psychological LLMs are constrained by passive response mechanisms, limiting their capacity to deploy proactive strategies for psychological counseling. To bridge this gap, we first develop the ProPsyC (Proactive Psychological Conversation) dataset, a multi-turn conversation dataset with interpretive labels including strategy decision logic and reaction attribution. Based on ProPsyC, we propose PsyAdvisor by supervised fine-tuning, a plug-and-play proactive questioning strategy planner that empowers psychological LLMs to initiate well-timed questioning through strategic prompting. Experimental results demonstrate that psychological LLMs integrated with PsyAdvisor substantially improve proactive questioning capacity, conversation depth, and response quality.Furthermore, PsyAdvisor shows promising potential in assisting novice counselors by providing strategy recommendations. This study provides new optimization directions for psychological conversation systems and offers valuable insights for future research on proactive questioning mechanisms in psychological LLMs.</abstract>
      <url hash="c48b56c9">2025.acl-long.596</url>
      <bibkey>hu-etal-2025-psyadvisor</bibkey>
    </paper>
    <paper id="597">
      <title><tex-math>HomeBench</tex-math>: Evaluating <fixed-case>LLM</fixed-case>s in Smart Homes with Valid and Invalid Instructions Across Single and Multiple Devices</title>
      <author><first>Silin</first><last>Li</last></author>
      <author><first>Yuhang</first><last>Guo</last></author>
      <author><first>Jiashu</first><last>Yao</last></author>
      <author><first>Zeming</first><last>Liu</last></author>
      <author><first>Haifeng</first><last>Wang</last><affiliation>Baidu</affiliation></author>
      <pages>12230-12250</pages>
      <abstract>Large language models (LLMs) have the potential to revolutionize smart home assistants by enhancing their ability to accurately understand user needs and respond appropriately, which is extremely beneficial for building a smarter home environment. While recent studies have explored integrating LLMs into smart home systems, they primarily focus on handling straightforward, valid single-device operation instructions. However, real-world scenarios are far more complex and often involve users issuing invalid instructions or controlling multiple devices simultaneously. These have two main challenges: LLMs must accurately identify and rectify errors in user instructions and execute multiple user instructions perfectly. To address these challenges and advance the development of LLM-based smart home assistants, we introduce HomeBench, the first smart home dataset with valid and invalid instructions across single and multiple devices in this paper. We have experimental results on 13 distinct LLMs; e.g., GPT-4o achieves only a 0.0% success rate in the scenario of invalid multi-device instructions, revealing that the existing state-of-the-art LLMs still cannot perform well in this situation even with the help of in-context learning, retrieval-augmented generation, and fine-tuning. Our code and dataset are publicly available at https://github.com/BITHLP/HomeBench.</abstract>
      <url hash="deea3e4a">2025.acl-long.597</url>
      <bibkey>li-etal-2025-homebench</bibkey>
    </paper>
    <paper id="598">
      <title>Advancing Zero-shot Text-to-Speech Intelligibility across Diverse Domains via Preference Alignment</title>
      <author><first>Xueyao</first><last>Zhang</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <author><first>Yuancheng</first><last>Wang</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <author><first>Chaoren</first><last>Wang</last></author>
      <author><first>Ziniu</first><last>Li</last></author>
      <author><first>Zhuo</first><last>Chen</last></author>
      <author><first>Zhizheng</first><last>Wu</last><affiliation>Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <pages>12251-12270</pages>
      <abstract>Modern zero-shot text-to-speech (TTS) systems, despite using extensive pre-training, often struggle in challenging scenarios such as tongue twisters, repeated words, code-switching, and cross-lingual synthesis, leading to intelligibility issues. To address these limitations, this paper leverages preference alignment techniques, which enable targeted construction of out-of-pretraining-distribution data to enhance performance. We introduce a new dataset, named the Intelligibility Preference Speech Dataset (INTP), and extend the Direct Preference Optimization (DPO) framework to accommodate diverse TTS architectures. After INTP alignment, in addition to intelligibility, we observe overall improvements including naturalness, similarity, and audio quality for multiple TTS models across diverse domains. Based on that, we also verify the weak-to-strong generalization ability of INTP for more intelligible models such as CosyVoice 2 and Ints. Moreover, we showcase the potential for further improvements through iterative alignment based on Ints. Audio samples are available at https://intalign.github.io/.</abstract>
      <url hash="0da173ef">2025.acl-long.598</url>
      <bibkey>zhang-etal-2025-advancing-zero</bibkey>
    </paper>
    <paper id="599">
      <title><fixed-case>G</fixed-case>i<fixed-case>FT</fixed-case>: <fixed-case>G</fixed-case>ibbs Fine-Tuning for Code Generation</title>
      <author><first>Haochen</first><last>Li</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Wanjin</first><last>Feng</last></author>
      <author><first>Xin</first><last>Zhou</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Zhiqi</first><last>Shen</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>12271-12284</pages>
      <abstract>Training Large Language Models (LLMs) with synthetic data is a prevalent practice in code generation. A key approach is self-training, where LLMs are iteratively trained on self-generated correct code snippets. In this case, the self-generated codes are drawn from a conditional distribution, conditioned on a specific seed description. However, the seed description is not the only valid representation that aligns with its intended meaning. With all valid descriptions and codes forming a joint space, codes drawn from the conditional distribution would lead to an underrepresentation of the full description-code space. As such, we propose Gibbs Fine-Tuning (GiFT), a novel self-training method inspired by Gibbs sampling. GiFT allows self-generated data to be drawn from the marginal distribution of the joint space, thereby mitigating the biases inherent in conditional sampling. We provide a theoretical analysis demonstrating the potential benefits of fine-tuning LLMs with code derived from the marginal distribution. Furthermore, we propose a perplexity-based code selection method to mitigate the imbalanced long-tail distribution of the self-generated codes. Empirical evaluation of two LLMs across four datasets demonstrates that GiFT achieves superior performance, particularly on more challenging benchmarks. Source code is available at <url>https://github.com/Alex-HaochenLi/GiFT</url>.</abstract>
      <url hash="15b3862d">2025.acl-long.599</url>
      <bibkey>li-etal-2025-gift</bibkey>
    </paper>
    <paper id="600">
      <title>Enhancing Interpretable Image Classification Through <fixed-case>LLM</fixed-case> Agents and Conditional Concept Bottleneck Models</title>
      <author><first>Yiwen</first><last>Jiang</last><affiliation>Monash University</affiliation></author>
      <author><first>Deval</first><last>Mehta</last><affiliation>Monash University</affiliation></author>
      <author><first>Wei</first><last>Feng</last></author>
      <author><first>Zongyuan</first><last>Ge</last></author>
      <pages>12285-12297</pages>
      <abstract>Concept Bottleneck Models (CBMs) decompose image classification into a process governed by interpretable, human-readable concepts. Recent advances in CBMs have used Large Language Models (LLMs) to generate candidate concepts. However, a critical question remains: What is the optimal number of concepts to use? Current concept banks suffer from redundancy or insufficient coverage. To address this issue, we introduce a dynamic, agent-based approach that adjusts the concept bank in response to environmental feedback, optimizing the number of concepts for sufficiency yet concise coverage. Moreover, we propose Conditional Concept Bottleneck Models (CoCoBMs) to overcome the limitations in traditional CBMs’ concept scoring mechanisms. It enhances the accuracy of assessing each concept’s contribution to classification tasks and feature an editable matrix that allows LLMs to correct concept scores that conflict with their internal knowledge. Our evaluations across 6 datasets show that our method not only improves classification accuracy by 6% but also enhances interpretability assessments by 30%.</abstract>
      <url hash="103021b6">2025.acl-long.600</url>
      <bibkey>jiang-etal-2025-enhancing</bibkey>
    </paper>
    <paper id="601">
      <title>Reliably Bounding False Positives: A Zero-Shot Machine-Generated Text Detection Framework via Multiscaled Conformal Prediction</title>
      <author><first>Xiaowei</first><last>Zhu</last></author>
      <author><first>Yubing</first><last>Ren</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yanan</first><last>Cao</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xixun</first><last>Lin</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <author><first>Fang</first><last>Fang</last><affiliation>Institute of Information Engineering,Chinese Academy of Sciences</affiliation></author>
      <author><first>Yangxi</first><last>Li</last></author>
      <pages>12298-12319</pages>
      <abstract>The rapid advancement of large language models has raised significant concerns regarding their potential misuse by malicious actors. As a result, developing effective detectors to mitigate these risks has become a critical priority. However, most existing detection methods focus excessively on detection accuracy, often neglecting the societal risks posed by high false positive rates (FPRs). This paper addresses this issue by leveraging Conformal Prediction (CP), which effectively constrains the upper bound of FPRs. While directly applying CP constrains FPRs, it also leads to a significant reduction in detection performance. To overcome this trade-off, this paper proposes a Zero-Shot Machine-Generated Text Detection Framework via Multiscaled Conformal Prediction (MCP), which both enforces the FPR constraint and improves detection performance. This paper also introduces RealDet, a high-quality dataset that spans a wide range of domains, ensuring realistic calibration and enabling superior detection performance when combined with MCP. Empirical evaluations demonstrate that MCP effectively constrains FPRs, significantly enhances detection performance, and increases robustness against adversarial attacks across multiple detectors and datasets.</abstract>
      <url hash="8b64bba6">2025.acl-long.601</url>
      <bibkey>zhu-etal-2025-reliably</bibkey>
    </paper>
    <paper id="602">
      <title><fixed-case>RSCF</fixed-case>: Relation-Semantics Consistent Filter for Entity Embedding of Knowledge Graph</title>
      <author><first>Junsik</first><last>Kim</last><affiliation>Gwangju Institute of Science and Technology</affiliation></author>
      <author><first>Jinwook</first><last>Park</last><affiliation>Gwangju Institute of Science and Technology</affiliation></author>
      <author><first>Kangil</first><last>Kim</last><affiliation>Gwangju Institute of Science and Technology</affiliation></author>
      <pages>12320-12336</pages>
      <abstract>In knowledge graph embedding, leveraging relation specific entity transformation has markedly enhanced performance. However, the consistency of embedding differences before and after transformation remains unaddressed, risking the loss of valuable inductive bias inherent in the embeddings. This inconsistency stems from two problems. First, transformation representations are specified for relations in a disconnected manner, allowing dissimilar transformations and corresponding entity embeddings for similar relations. Second, a generalized plug-in approach as a SFBR (Semantic Filter Based on Relations) disrupts this consistency through excessive concentration of entity embeddings under entity-based regularization, generating indistinguishable score distributions among relations. In this paper, we introduce a plug-in KGE method, Relation-Semantics Consistent Filter (RSCF). Its entity transformation has three features for enhancing semantic consistency: 1) shared affine transformation of relation embeddings across all relations, 2) rooted entity transformation that adds an entity embedding to its change represented by the transformed vector, and 3) normalization of the change to prevent scale reduction. To amplify the advantages of consistency that preserve semantics on embeddings, RSCF adds relation transformation and prediction modules for enhancing the semantics. In knowledge graph completion tasks with distance-based and tensor decomposition models, RSCF significantly outperforms state-of-the-art KGE methods, showing robustness across all relations and their frequencies.</abstract>
      <url hash="c6d23227">2025.acl-long.602</url>
      <bibkey>kim-etal-2025-rscf</bibkey>
    </paper>
    <paper id="603">
      <title><fixed-case>R</fixed-case>ole<fixed-case>P</fixed-case>lot: A Systematic Framework for Evaluating and Enhancing the Plot-Progression Capabilities of Role-Playing Agents</title>
      <author><first>Pinyi</first><last>Zhang</last></author>
      <author><first>Siyu</first><last>An</last></author>
      <author><first>Lingfeng</first><last>Qiao</last></author>
      <author><first>Yifei</first><last>Yu</last></author>
      <author><first>Jingyang</first><last>Chen</last></author>
      <author><first>Jie</first><last>Wang</last></author>
      <author><first>Di</first><last>Yin</last></author>
      <author><first>Xing</first><last>Sun</last><affiliation>Tencent YouTu Lab</affiliation></author>
      <author><first>Kai</first><last>Zhang</last><affiliation>East China Normal University</affiliation></author>
      <pages>12337-12354</pages>
      <abstract>Role-playing agents (RPAs) are garnering increasing interests as a novel form of conversational AI. While previous research has predominantly concentrated on their ability to portray specified characters, we argue from a user-centered perspective that RPAs’ capability to advance the plot requires substantial improvements to deliver more engaging interaction. To bridge this gap, we propose RolePlot, a role-playing framework specifically designed to evaluate and enhance the plot-progression capabilities of RPAs. RolePlot begins by constructing a plot-progression dataset extended from human-written literary scripts and specially designed synthetic data, followed by narrative theory-driven manual annotation and automated labeling validated through human verification. We then exploit the over-parameterized embedding space of LLMs to detect a “trigger subspace” that identifies dialogue segments catalyzing plot transitions. When user’s inputs align with this subspace, we explicitly prompt RPAs to advance the plot. For evaluation, we simulate User-RPA interactions and track both the conversation longevity (measured in dialogue turns before disengagement) and users’ arousal levels across different stages. Empirically, our method improves RPAs’ capability to time plot developments, and more importantly, yielding a significant increase in conversation turns and sustained higher arousal levels, thereby confirming that users experience more immersive engagements.</abstract>
      <url hash="0c4079f3">2025.acl-long.603</url>
      <bibkey>zhang-etal-2025-roleplot</bibkey>
    </paper>
    <paper id="604">
      <title><fixed-case>T</fixed-case>ree<fixed-case>RL</fixed-case>: <fixed-case>LLM</fixed-case> Reinforcement Learning with On-Policy Tree Search</title>
      <author><first>Zhenyu</first><last>Hou</last></author>
      <author><first>Ziniu</first><last>Hu</last><affiliation>xAI</affiliation></author>
      <author><first>Yujiang</first><last>Li</last></author>
      <author><first>Rui</first><last>Lu</last></author>
      <author><first>Jie</first><last>Tang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Yuxiao</first><last>Dong</last><affiliation>Tsinghua University</affiliation></author>
      <pages>12355-12369</pages>
      <abstract>Reinforcement learning (RL) with tree search has demonstrated superior performance in traditional reasoning tasks. Compared to conventional independent chain sampling strategies with outcome supervision, tree search enables better exploration of the reasoning space and provides dense, on-policy process rewards during RL training but remains under-explored in On-Policy LLM RL. We propose TreeRL, a reinforcement learning framework that directly incorporates on-policy tree search for RL training. Our approach includes intermediate supervision and eliminates the need for separate reward model training. Existing approaches typically train a separate process reward model, which can suffer from distribution mismatch and reward hacking. We also introduce a cost-effective tree search approach that achieves higher search efficiency under the same generation token budget by strategically branching from high-uncertainty intermediate steps rather than using random branching. Experiments on challenging math and code reasoning benchmarks demonstrate that TreeRL achieves superior performance compared to traditional ChainRL, highlighting the potential of tree search for LLM. TreeRL is open-sourced at <url>https://github.com/THUDM/TreeRL</url>.</abstract>
      <url hash="670a44ec">2025.acl-long.604</url>
      <bibkey>hou-etal-2025-treerl</bibkey>
    </paper>
    <paper id="605">
      <title>Can a Single Model Master Both Multi-turn Conversations and Tool Use? <fixed-case>C</fixed-case>o<fixed-case>ALM</fixed-case>: A Unified Conversational Agentic Language Model</title>
      <author><first>Emre Can</first><last>Acikgoz</last></author>
      <author><first>Jeremiah</first><last>Greer</last><affiliation>Oumi AI PBC</affiliation></author>
      <author><first>Akul</first><last>Datta</last></author>
      <author><first>Ze</first><last>Yang</last></author>
      <author><first>William</first><last>Zeng</last></author>
      <author><first>Oussama</first><last>Elachqar</last><affiliation>Oumi</affiliation></author>
      <author><first>Emmanouil</first><last>Koukoumidis</last></author>
      <author><first>Dilek</first><last>Hakkani-Tür</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Gokhan</first><last>Tur</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <pages>12370-12390</pages>
      <abstract>Large Language Models (LLMs) with API-calling capabilities enabled building effective Language Agents (LA), while also revolutionizing the conventional task-oriented dialogue (TOD) paradigm. However, current approaches face a critical dilemma: TOD systems are often trained on a limited set of target APIs, requiring new data to maintain their quality when interfacing with new services, while LAs are not trained to maintain user intent over multi-turn conversations. Because both robust multi-turn management and advanced function calling are crucial for effective conversational agents, we evaluate these skills on three popular benchmarks: MultiWOZ 2.4 (TOD), BFCL V3 (LA), and API-Bank (LA)—and our analyses reveal that specialized approaches excel in one domain but underperform in the other. To bridge this chasm, we introduce **CoALM** (**C**onversational **A**gentic **L**anguage **M**odel), a unified approach that integrates both conversational and agentic capabilities. We created **CoALM-IT**, a carefully constructed multi-task dataset that interleave multi-turn ReAct reasoning with complex API usage. Using CoALM-IT, we train three models **CoALM 8B**, **CoALM 70B**, and **CoALM 405B**, which outperform top domain-specific models, including GPT-4o, across all three benchmarks. This demonstrates the feasibility of a single model approach for both TOD and LA, setting a new standard for conversational agents.</abstract>
      <url hash="7349d498">2025.acl-long.605</url>
      <bibkey>acikgoz-etal-2025-single</bibkey>
    </paper>
    <paper id="606">
      <title>Single-to-mix Modality Alignment with Multimodal Large Language Model for Document Image Machine Translation</title>
      <author><first>Yupu</first><last>Liang</last><affiliation>University of Chinese Academy of Sciences</affiliation></author>
      <author><first>Yaping</first><last>Zhang</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Zhiyang</first><last>Zhang</last></author>
      <author><first>Yang</first><last>Zhao</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Lu</first><last>Xiang</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Chengqing</first><last>Zong</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yu</first><last>Zhou</last><affiliation>Institute of Automation, Chinese Academy of Sciences</affiliation></author>
      <pages>12391-12408</pages>
      <abstract>Document Image Machine Translation (DIMT) aims to translate text within document images, facing generalization challenges due to limited training data and the complex interplay between visual and textual information. To address these challenges, we introduce M4Doc, a novel single-to-mix Modality alignment framework leveraging Multimodal Large Language Models (MLLMs). M4Doc aligns an imageonly encoder with the multimodal representations of an MLLM, pre-trained on large-scale document image datasets. This alignment enables a lightweight DIMT model to learn crucial visual-textual correlations during training. During inference, M4Doc bypasses the MLLM, maintaining computational efficiency while benefiting from its multimodal knowledge. Comprehensive experiments demonstrate substantial improvements in translation quality, especially in cross-domain generalization and challenging document image scenarios. The code will be released upon acceptance.</abstract>
      <url hash="374cc83f">2025.acl-long.606</url>
      <bibkey>liang-etal-2025-single</bibkey>
    </paper>
    <paper id="607">
      <title><fixed-case>SDPO</fixed-case>: Segment-Level Direct Preference Optimization for Social Agents</title>
      <author><first>Aobo</first><last>Kong</last></author>
      <author><first>Wentao</first><last>Ma</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Shiwan</first><last>Zhao</last></author>
      <author><first>Yongbin</first><last>Li</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yuchuan</first><last>Wu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Ke</first><last>Wang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Xiaoqian</first><last>Liu</last></author>
      <author><first>Qicheng</first><last>Li</last><affiliation>Nankai University</affiliation></author>
      <author><first>Yong</first><last>Qin</last><affiliation>Nankai University</affiliation></author>
      <author><first>Fei</first><last>Huang</last><affiliation>Alibaba Group US</affiliation></author>
      <pages>12409-12423</pages>
      <abstract>Social agents powered by large language models (LLMs) can simulate human social behaviors but fall short in handling complex social dialogues. Direct Preference Optimization (DPO) has proven effective in aligning LLM behavior with human preferences across various agent tasks. However, standard DPO focuses solely on individual turns, which limits its effectiveness in multi-turn social interactions. Several DPO-based multi-turn alignment methods with session-level data have shown potential in addressing this problem. While these methods consider multiple turns across entire sessions, they are often overly coarse-grained, introducing training noise, and lack robust theoretical support. To resolve these limitations, we propose Segment-Level Direct Preference Optimization (SDPO), which dynamically select key segments within interactions to optimize multi-turn agent behavior. SDPO minimizes training noise and is grounded in a rigorous theoretical framework. Evaluations on the SOTOPIA benchmark demonstrate that SDPO-tuned agents consistently outperform both existing DPO-based methods and proprietary LLMs like GPT-4o, underscoring SDPO’s potential to advance the social intelligence of LLM-based agents. We release our code and data at https://anonymous.4open.science/r/SDPO-CE8F.</abstract>
      <url hash="74c3e01b">2025.acl-long.607</url>
      <bibkey>kong-etal-2025-sdpo</bibkey>
    </paper>
    <paper id="608">
      <title><fixed-case>K</fixed-case>okoro<fixed-case>C</fixed-case>hat: A <fixed-case>J</fixed-case>apanese Psychological Counseling Dialogue Dataset Collected via Role-Playing by Trained Counselors</title>
      <author><first>Zhiyang</first><last>Qi</last><affiliation>University of Electro-Communications</affiliation></author>
      <author><first>Takumasa</first><last>Kaneko</last><affiliation>University of Electro-Communications</affiliation></author>
      <author><first>Keiko</first><last>Takamizo</last><affiliation>Japanese Organization of Mental Health and Educational Agencies and iDEAR Human Support Service Co., Ltd.</affiliation></author>
      <author><first>Mariko</first><last>Ukiyo</last><affiliation>Japanese Organization of Mental Health and Educational Agencies and iDEAR Human Support Service Co., Ltd.</affiliation></author>
      <author><first>Michimasa</first><last>Inaba</last><affiliation>The University of Electro-Communications</affiliation></author>
      <pages>12424-12443</pages>
      <abstract>Generating psychological counseling responses with language models relies heavily on high-quality datasets. Crowdsourced data collection methods require strict worker training, and data from real-world counseling environments may raise privacy and ethical concerns. While recent studies have explored using large language models (LLMs) to augment psychological counseling dialogue datasets, the resulting data often suffers from limited diversity and authenticity. To address these limitations, this study adopts a role-playing approach where trained counselors simulate counselor-client interactions, ensuring high-quality dialogues while mitigating privacy risks. Using this method, we construct KokoroChat, a Japanese psychological counseling dialogue dataset comprising 6,589 long-form dialogues, each accompanied by comprehensive client feedback. Experimental results demonstrate that fine-tuning open-source LLMs with KokoroChat improves both the quality of generated counseling responses and the automatic evaluation of counseling dialogues. The KokoroChat dataset is available at https://github.com/UEC-InabaLab/KokoroChat.</abstract>
      <url hash="377a1f7d">2025.acl-long.608</url>
      <bibkey>qi-etal-2025-kokorochat</bibkey>
    </paper>
    <paper id="609">
      <title><fixed-case>SURVEYFORGE</fixed-case> : On the Outline Heuristics, Memory-Driven Generation, and Multi-dimensional Evaluation for Automated Survey Writing</title>
      <author><first>Xiangchao</first><last>Yan</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Shiyang</first><last>Feng</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Jiakang</first><last>Yuan</last><affiliation>Shanghai Artificial Intelligence Laboratory and Fudan University</affiliation></author>
      <author><first>Renqiu</first><last>Xia</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Bin</first><last>Wang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Lei</first><last>Bai</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Bo</first><last>Zhang</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <pages>12444-12465</pages>
      <abstract>Survey paper plays a crucial role in scientific research, especially given the rapid growth of research publications. Recently, researchers have begun using LLMs to automate survey generation for better efficiency. However, the quality gap between LLM-generated surveys and those written by human remains significant, particularly in terms of outline quality and citation accuracy. To close these gaps, we introduce SURVEYFORGE, which first generates the outline by analyzing the logical structure of human-written outlines and referring to the retrieved domain-related articles. Subsequently, leveraging high-quality papers retrieved from memory by our scholar navigation agent, SURVEYFORGE can automatically generate and refine the content of the generated article. Moreover, to achieve a comprehensive evaluation, we construct SurveyBench, which includes 100 human-written survey papers for win-rate comparison and assesses AI-generated survey papers across three dimensions: reference, outline, and content quality. Experiments demonstrate that SURVEYFORGEcan outperform previous works such as AutoSurvey.</abstract>
      <url hash="c8f70c8f">2025.acl-long.609</url>
      <bibkey>yan-etal-2025-surveyforge</bibkey>
    </paper>
    <paper id="610">
      <title>Making <fixed-case>LLM</fixed-case>s Better Many-to-Many Speech-to-Text Translators with Curriculum Learning</title>
      <author><first>Yexing</first><last>Du</last></author>
      <author><first>Youcheng</first><last>Pan</last><affiliation>Pengcheng Laboratory</affiliation></author>
      <author><first>Ziyang</first><last>Ma</last></author>
      <author><first>Bo</first><last>Yang</last></author>
      <author><first>Yifan</first><last>Yang</last></author>
      <author><first>Keqi</first><last>Deng</last></author>
      <author><first>Xie</first><last>Chen</last></author>
      <author><first>Yang</first><last>Xiang</last></author>
      <author><first>Ming</first><last>Liu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Bing</first><last>Qin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>12466-12478</pages>
      <abstract>Multimodal Large Language Models (MLLMs) have achieved significant success in Speech-to-Text Translation (S2TT) tasks. While most existing research has focused on English-centric translation directions, the exploration of many-to-many translation is still limited by the scarcity of parallel data. To address this, we propose a three-stage curriculum learning strategy that leverages the machine translation capabilities of large language models and adapts them to S2TT tasks, enabling effective learning in low-resource settings. We trained MLLMs with varying parameter sizes (3B, 7B, and 32B) and evaluated the proposed strategy using the FLEURS and CoVoST-2 datasets. Experimental results show that the proposed strategy achieves state-of-the-art average performance in <tex-math>15\times14</tex-math> language pairs, requiring fewer than 10 hours of speech data per language to achieve competitive results. The source code and models are released at <url>https://github.com/yxduir/LLM-SRT</url>.</abstract>
      <url hash="d3eecc2e">2025.acl-long.610</url>
      <bibkey>du-etal-2025-making</bibkey>
    </paper>
    <paper id="611">
      <title><fixed-case>A</fixed-case>b<fixed-case>G</fixed-case>en: Evaluating Large Language Models in Ablation Study Design and Evaluation for Scientific Research</title>
      <author><first>Yilun</first><last>Zhao</last><affiliation>Yale University</affiliation></author>
      <author><first>Weiyuan</first><last>Chen</last></author>
      <author><first>Zhijian</first><last>Xu</last></author>
      <author><first>Manasi</first><last>Patwardhan</last><affiliation>Tata Consultancy Services Limited, India</affiliation></author>
      <author><first>Chengye</first><last>Wang</last></author>
      <author><first>Yixin</first><last>Liu</last><affiliation>Yale University</affiliation></author>
      <author><first>Lovekesh</first><last>Vig</last></author>
      <author><first>Arman</first><last>Cohan</last><affiliation>Yale University and Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>12479-12491</pages>
      <abstract>We introduce AbGen, the first benchmark designed to evaluate the capabilities of LLMs in designing ablation studies for scientific research. AbGen consists of 2,000 expert-annotated examples derived from 677 NLP papers. In this benchmark, LLMs are tasked with generating detailed ablation study designs for a specified module or process based on the given research context. Our evaluation of leading LLMs, such as GPT-4o and Llama-3.1, highlights a significant performance gap between these models and human experts in terms of the importance, faithfulness, and soundness of the ablation study designs. Moreover, we demonstrate that current automated evaluation methods are not reliable for our task, as they show a significant discrepancy when compared to human assessment. To better investigate this, we develop AbGen-Eval, a meta-evaluation benchmark designed to assess the reliability of commonly used automated evaluation systems in measuring LLM performance on our task. We investigate various LLM-based evaluation methods on AbGen-Eval, providing insights for future research on developing more effective and reliable LLM-based evaluation systems for complex scientific tasks.</abstract>
      <url hash="49a91f38">2025.acl-long.611</url>
      <bibkey>zhao-etal-2025-abgen</bibkey>
    </paper>
    <paper id="612">
      <title>Redundancy Principles for <fixed-case>MLLM</fixed-case>s Benchmarks</title>
      <author><first>Zicheng</first><last>Zhang</last></author>
      <author><first>Xiangyu</first><last>Zhao</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Xinyu</first><last>Fang</last></author>
      <author><first>Chunyi</first><last>Li</last></author>
      <author><first>Xiaohong</first><last>Liu</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Xiongkuo</first><last>Min</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Haodong</first><last>Duan</last></author>
      <author><first>Kai</first><last>Chen</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Guangtao</first><last>Zhai</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>12492-12504</pages>
      <abstract>With the rapid iteration of Multi-modality Large Language Models (MLLMs) and the evolving demands of the field, the number of benchmarks produced annually has surged into the hundreds. The rapid growth has inevitably led to significant redundancy among benchmarks. Therefore, it is crucial to take a step back and critically assess the current state of redundancy and propose targeted principles for constructing effective MLLM benchmarks. In this paper, we focus on redundancy from three key perspectives: 1) Redundancy of benchmark capability dimensions, 2) Redundancy in the number of test questions, and 3) Cross-benchmark redundancy within specific domains. Through the comprehensive analysis over hundreds of MLLMs’ performance across more than 20 benchmarks, we aim to quantitatively measure the level of redundancy lies in existing MLLM evaluations, provide valuable insights to guide the future development of MLLM benchmarks, and offer strategies to refine and address redundancy issues effectively.</abstract>
      <url hash="7987c738">2025.acl-long.612</url>
      <bibkey>zhang-etal-2025-redundancy-principles</bibkey>
    </paper>
    <paper id="613">
      <title><fixed-case>W</fixed-case>av<fixed-case>RAG</fixed-case>: Audio-Integrated Retrieval Augmented Generation for Spoken Dialogue Models</title>
      <author><first>Yifu</first><last>Chen</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Shengpeng</first><last>Ji</last></author>
      <author><first>Haoxiao</first><last>Wang</last></author>
      <author><first>Ziqing</first><last>Wang</last></author>
      <author><first>Siyu</first><last>Chen</last></author>
      <author><first>Jinzheng</first><last>He</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Jin</first><last>Xu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Zhou</first><last>Zhao</last><affiliation>Zhejiang University and Zhejiang University</affiliation></author>
      <pages>12505-12523</pages>
      <abstract>Retrieval Augmented Generation (RAG) has gained widespread adoption owing to its capacity to empower large language models (LLMs) to integrate external knowledge. However, existing RAG frameworks are primarily designed for text-based LLMs and rely on Automatic Speech Recognition to process speech input, which discards crucial audio information, risks transcription errors, and increases computational overhead. Therefore, we introduce WavRAG, the first retrieval augmented generation framework with native, end-to-end audio support. WavRAG offers two key features: 1) Bypassing ASR, WavRAG directly processes raw audio for both embedding and retrieval. 2) WavRAG integrates audio and text into a unified knowledge representation. Specifically, we propose the WavRetriever to facilitate the retrieval from a text-audio hybrid knowledge base, and further enhance the in-context capabilities of spoken dialogue models through the integration of chain-of-thought reasoning. In comparison to state-of-the-art ASR-Text RAG pipelines, WavRAG achieves comparable retrieval performance while delivering a 10x acceleration. Furthermore, WavRAG’s unique text-audio hybrid retrieval capability extends the boundaries of RAG to the audio modality.</abstract>
      <url hash="c59ddf1a">2025.acl-long.613</url>
      <bibkey>chen-etal-2025-wavrag</bibkey>
    </paper>
    <paper id="614">
      <title><fixed-case>C</fixed-case>hild<fixed-case>M</fixed-case>andarin: A Comprehensive <fixed-case>M</fixed-case>andarin Speech Dataset for Young Children Aged 3-5</title>
      <author><first>Jiaming</first><last>Zhou</last></author>
      <author><first>Shiyao</first><last>Wang</last><affiliation>Nankai University</affiliation></author>
      <author><first>Shiwan</first><last>Zhao</last></author>
      <author><first>Jiabei</first><last>He</last></author>
      <author><first>Haoqin</first><last>Sun</last></author>
      <author><first>Hui</first><last>Wang</last></author>
      <author><first>Cheng</first><last>Liu</last></author>
      <author><first>Aobo</first><last>Kong</last></author>
      <author><first>Yujie</first><last>Guo</last></author>
      <author><first>Xi</first><last>Yang</last><affiliation>Beijing Academy of Artificial Intelligence</affiliation></author>
      <author><first>Yequan</first><last>Wang</last><affiliation>Beijing Academy of Artificial Intelligence</affiliation></author>
      <author><first>Yonghua</first><last>Lin</last></author>
      <author><first>Yong</first><last>Qin</last><affiliation>Nankai University</affiliation></author>
      <pages>12524-12537</pages>
      <abstract>Automatic speech recognition (ASR) systems have advanced significantly with models like Whisper, Conformer, and self-supervised frameworks such as Wav2vec 2.0 and HuBERT. However, developing robust ASR models for young children’s speech remains challenging due to differences in pronunciation, tone, and pace compared to adult speech. In this paper, we introduce a new Mandarin speech dataset focused on children aged 3 to 5, addressing the scarcity of resources in this area. The dataset comprises 41.25 hours of speech with carefully crafted manual transcriptions, collected from 397 speakers across various provinces in China, with balanced gender representation. We provide a comprehensive analysis of speaker demographics, speech duration distribution and geographic coverage. Additionally, we evaluate ASR performance on models trained from scratch, such as Conformer, as well as fine-tuned pre-trained models like HuBERT and Whisper, where fine-tuning demonstrates significant performance improvements. Furthermore, we assess speaker verification (SV) on our dataset, showing that, despite the challenges posed by the unique vocal characteristics of young children, the dataset effectively supports both ASR and SV tasks. This dataset is a valuable contribution to Mandarin child speech research and holds potential for applications in educational technology and child-computer interaction. It will be open-source and freely available for all academic purposes.</abstract>
      <url hash="1f98e458">2025.acl-long.614</url>
      <bibkey>zhou-etal-2025-childmandarin</bibkey>
    </paper>
    <paper id="615">
      <title>Finding the Sweet Spot: Preference Data Construction for Scaling Preference Optimization</title>
      <author><first>Yao</first><last>Xiao</last></author>
      <author><first>Hai</first><last>Ye</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Linyao</first><last>Chen</last></author>
      <author><first>Hwee Tou</first><last>Ng</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Lidong</first><last>Bing</last><affiliation>Shanda Group and Alibaba Group</affiliation></author>
      <author><first>Xiaoli</first><last>Li</last></author>
      <author><first>Roy Ka-Wei</first><last>Lee</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <pages>12538-12552</pages>
      <abstract>Iterative data generation and model retraining are widely used to align large language models (LLMs).It typically involves a policy model to generate on-policy responses and a reward model to guide training data selection. Direct Preference Optimization (DPO) further enhances this process by constructing preference pairs of chosen and rejected responses. In this work, we aim to <i>scale up</i> the number of on-policy samples via repeated random sampling to improve alignment performance. Conventional practice selects the sample with the highest reward as chosen and the lowest as rejected for DPO. However, our experiments reveal that this strategy leads to a <i>decline</i> in performance as the sample size increases. To address this, we investigate preference data construction through the lens of underlying normal distribution of sample rewards. We categorize the reward space into seven representative points and systematically explore all 21 (<tex-math>C_7^2</tex-math>) pairwise combinations. Through evaluations on four models using AlpacaEval 2, we find that selecting the rejected response at reward position <tex-math>\mu - 2\sigma</tex-math> rather than the minimum reward, is crucial for optimal performance. We finally introduce a scalable preference data construction strategy that consistently enhances model performance as the sample scale increases.</abstract>
      <url hash="00770049">2025.acl-long.615</url>
      <bibkey>xiao-etal-2025-finding</bibkey>
    </paper>
    <paper id="616">
      <title>Enhancing Safe and Controllable Protein Generation via Knowledge Preference Optimization</title>
      <author><first>Yuhao</first><last>Wang</last></author>
      <author><first>Keyan</first><last>Ding</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Kehua</first><last>Feng</last></author>
      <author><first>Zeyuan</first><last>Wang</last></author>
      <author><first>Ming</first><last>Qin</last></author>
      <author><first>Xiaotong</first><last>Li</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Qiang</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Huajun</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <pages>12553-12569</pages>
      <abstract>Protein language models have emerged as powerful tools for sequence generation, offering substantial advantages in functional optimization and *denovo* design. However, these models also present significant risks of generating harmful protein sequences, such as those that enhance viral transmissibility or evade immune responses. These concerns underscore critical biosafety and ethical challenges. To address these issues, we propose a Knowledge-guided Preference Optimization (KPO) framework that integrates prior knowledge via a Protein Safety Knowledge Graph. This framework utilizes an efficient graph pruning strategy to identify preferred sequences and employs reinforcement learning to minimize the risk of generating harmful proteins. Experimental results demonstrate that KPO effectively reduces the likelihood of producing hazardous sequences while maintaining high functionality, offering a robust safety assurance framework for applying generative models in biotechnology.</abstract>
      <url hash="ce24ee55">2025.acl-long.616</url>
      <bibkey>wang-etal-2025-enhancing-safe</bibkey>
    </paper>
    <paper id="617">
      <title><fixed-case>SINC</fixed-case>on: Mitigate <fixed-case>LLM</fixed-case>-Generated Malicious Message Injection Attack for Rumor Detection</title>
      <author><first>Mingqing</first><last>Zhang</last></author>
      <author><first>Qiang</first><last>Liu</last><affiliation>Institute of Automation, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xiang</first><last>Tao</last></author>
      <author><first>Shu</first><last>Wu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Liang</first><last>Wang</last><affiliation>Institute of Automation, CAS,China</affiliation></author>
      <pages>12570-12581</pages>
      <abstract>In the era of rapidly evolving large language models (LLMs), state-of-the-art rumor detection systems, particularly those based on Message Propagation Trees (MPTs), which represent a conversation tree with the post as its root and the replies as its descendants, are facing increasing threats from adversarial attacks that leverage LLMs to generate and inject malicious messages. Existing methods are based on the assumption that different nodes exhibit varying degrees of influence on predictions. They define nodes with high predictive influence as important nodes and target them for attacks. If the model treats nodes’ predictive influence more uniformly, attackers will find it harder to target high predictive influence nodes. In this paper, we propose Similarizing the predictive Influence of Nodes with Contrastive Learning (SINCon), a defense mechanism that encourages the model to learn graph representations where nodes with varying importance have a more uniform influence on predictions. Extensive experiments on the Twitter and Weibo datasets demonstrate that SINCon not only preserves high classification accuracy on clean data but also significantly enhances resistance against LLM-driven message injection attacks.</abstract>
      <url hash="141af398">2025.acl-long.617</url>
      <bibkey>zhang-etal-2025-sincon</bibkey>
    </paper>
    <paper id="618">
      <title>Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large Language Models</title>
      <author><first>Jungwoo</first><last>Park</last><affiliation>Korea University</affiliation></author>
      <author><first>Taewhoo</first><last>Lee</last><affiliation>Korea University</affiliation></author>
      <author><first>Chanwoong</first><last>Yoon</last><affiliation>Korea University</affiliation></author>
      <author><first>Hyeon</first><last>Hwang</last><affiliation>Korea University</affiliation></author>
      <author><first>Jaewoo</first><last>Kang</last><affiliation>Korea University</affiliation></author>
      <pages>12582-12600</pages>
      <abstract>Extreme activation outliers in Large Language Models (LLMs) critically degrade quantization performance, hindering efficient on-device deployment. While channel-wise operations and adaptive gradient scaling are recognized causes, practical mitigation remains challenging. We introduce **Outlier-Safe Pre-Training (OSP)**, a practical guideline that proactively prevents outlier formation, rather than relying on post-hoc mitigation. OSP combines three key innovations: (1) the Muon optimizer, eliminating privileged bases while maintaining training efficiency, (2) Single-Scale RMSNorm, preventing channel-wise amplification, and (3) a learnable embedding projection, redistributing activation magnitudes. We validate OSP by training a 1.4B-parameter model on 1 trillion tokens, which is the first production-scale LLM trained without such outliers. Under aggressive 4-bit quantization, our OSP model achieves a 35.7 average score across 10 benchmarks (versus 26.5 for an Adam-trained model), with only a 2% training overhead. Remarkably, OSP models exhibit near-zero excess kurtosis (0.04) compared to extreme values (1818.56) in standard models, fundamentally altering LLM quantization behavior. Our work demonstrates that outliers are not inherent to LLMs but are consequences of training strategies, paving the way for more efficient LLM deployment.</abstract>
      <url hash="2faf0623">2025.acl-long.618</url>
      <bibkey>park-etal-2025-outlier</bibkey>
    </paper>
    <paper id="619">
      <title>Agentic Knowledgeable Self-awareness</title>
      <author><first>Shuofei</first><last>Qiao</last></author>
      <author><first>Zhisong</first><last>Qiu</last></author>
      <author><first>Baochang</first><last>Ren</last></author>
      <author><first>Xiaobin</first><last>Wang</last></author>
      <author><first>Xiangyuan</first><last>Ru</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Ningyu</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Xiang</first><last>Chen</last><affiliation>Nanjing University of Aeronautics and Astronautics</affiliation></author>
      <author><first>Yong</first><last>Jiang</last><affiliation>Tongyi Lab</affiliation></author>
      <author><first>Pengjun</first><last>Xie</last></author>
      <author><first>Fei</first><last>Huang</last><affiliation>Alibaba Group US</affiliation></author>
      <author><first>Huajun</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <pages>12601-12625</pages>
      <abstract>Large Language Models (LLMs) have achieved considerable performance across various agentic planning tasks. However, traditional approaches adopt a “flood irrigation” methodology that indiscriminately injects gold trajectories, external feedback, and domain knowledge into agent models. This practice overlooks the fundamental human cognitive principle of self-awareness - the ability to dynamically assess situational demands and strategically employ resources during decision-making. We propose <tex-math>\textbf{Agentic Knowledgeable Self-awareness}</tex-math> to address this gap, a novel paradigm enabling LLM-based agents to autonomously regulate knowledge utilization. Specifically, we propose <tex-math>\texttt{KnowSelf}</tex-math>, a data-centric approach that applies agents with <tex-math>\texttt{know}</tex-math>ledgeable <tex-math>\texttt{self}</tex-math>-awareness like humans. Concretely, we devise a heuristic situation judgement criterion to mark special tokens on the agent’s self-explored trajectories for collecting training data. Through a two-stage training process, the agent model can switch between different situations by generating specific special tokens, achieving optimal planning effects with minimal costs. Our experiments demonstrate that can outperform various strong baselines on different tasks and models with minimal use of external knowledge.</abstract>
      <url hash="73e9819a">2025.acl-long.619</url>
      <bibkey>qiao-etal-2025-agentic</bibkey>
    </paper>
    <paper id="620">
      <title>A Unified Agentic Framework for Evaluating Conditional Image Generation</title>
      <author><first>Jifang</first><last>Wang</last></author>
      <author><first>Yangxue</first><last>Yangxue</last></author>
      <author><first>Longyue</first><last>Wang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Zhenran</first><last>Xu</last></author>
      <author><first>Yiyu</first><last>Wang</last><affiliation>University of Chinese Academy of Sciences</affiliation></author>
      <author><first>Yaowei</first><last>Wang</last><affiliation>Harbin Institute of Technology, Shenzhen and Pengcheng Laboratory</affiliation></author>
      <author><first>Weihua</first><last>Luo</last><affiliation>Alibaba International Digital Commerce Group</affiliation></author>
      <author><first>Kaifu</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Baotian</first><last>Hu</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>12626-12646</pages>
      <abstract>Conditional image generation has gained significant attention for its ability to personalize content. However, the field faces challenges in developing task-agnostic, reliable, and explainable evaluation metrics. This paper introduces CIGEval, a unified agentic framework for comprehensive evaluation of conditional image generation tasks. CIGEval utilizes large multimodal models (LMMs) as its core, integrating a multi-functional toolbox and establishing a fine-grained evaluation framework. Additionally, we synthesize evaluation trajectories for fine-tuning, empowering smaller LMMs to autonomously select appropriate tools and conduct nuanced analyses based on tool outputs. Experiments across seven prominent conditional image generation tasks demonstrate that CIGEval (GPT-4o version) achieves a high correlation of 0.4625 with human assessments, closely matching the inter-annotator correlation of 0.47. Notably, when implemented with 7B open-source LMMs using only 2.3K training trajectories, CIGEval surpasses the previous GPT-4o-based state-of-the-art method. These findings indicate that CIGEval holds great potential for automating evaluation of image generation tasks while maintaining human-level reliability.</abstract>
      <url hash="1d5ef72c">2025.acl-long.620</url>
      <bibkey>wang-etal-2025-unified</bibkey>
    </paper>
    <paper id="621">
      <title>Planning-Driven Programming: A Large Language Model Programming Workflow</title>
      <author><first>Chao</first><last>Lei</last></author>
      <author><first>Yanchuan</first><last>Chang</last><affiliation>University of Melbourne</affiliation></author>
      <author><first>Nir</first><last>Lipovetzky</last></author>
      <author><first>Krista A.</first><last>Ehinger</last><affiliation>The University of Melbourne</affiliation></author>
      <pages>12647-12684</pages>
      <abstract>The strong performance of large language models (LLMs) raises extensive discussion on their application to code generation. Recent research suggests continuous program refinements through visible tests to improve code generation accuracy in LLMs. However, these methods suffer from LLMs’ inefficiency and limited reasoning capacity. In this work, we propose an LLM programming workflow (LPW) designed to improve both initial code generation and subsequent refinements within a structured two-phase workflow. Specifically, the solution generation phase formulates a solution plan, which is then verified through visible tests to specify the intended natural language solution. Subsequently, the code implementation phase drafts an initial code according to the solution plan and its verification. If the generated code fails the visible tests, the plan verification serves as the intended solution to consistently inform the refinement process for correcting bugs. Compared to state-of-the-art methods across various existing LLMs, LPW significantly improves the Pass@1 accuracy by up to 16.4% on well-established text-to-code generation benchmarks. LPW also sets new state-of-the-art Pass@1 accuracy, achieving 98.2% on HumanEval, 84.8% on MBPP, 59.3% on LiveCode, 62.6% on APPS, and 34.7% on CodeContests, using GPT-4o as the backbone. Our code is publicly available at: https://github.com/you68681/lpw.</abstract>
      <url hash="e50833fb">2025.acl-long.621</url>
      <bibkey>lei-etal-2025-planning</bibkey>
    </paper>
    <paper id="622">
      <title>Can Knowledge Graphs Make Large Language Models More Trustworthy? An Empirical Study Over Open-ended Question Answering</title>
      <author><first>Yuan</first><last>Sui</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Yufei</first><last>He</last></author>
      <author><first>Zifeng</first><last>Ding</last></author>
      <author><first>Bryan</first><last>Hooi</last><affiliation>National University of Singapore</affiliation></author>
      <pages>12685-12701</pages>
      <abstract>Recent works integrating Knowledge Graphs (KGs) have shown promising improvements in enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing benchmarks primarily focus on closed-ended tasks, leaving a gap in evaluating performance on more complex, real-world scenarios. This limitation also hinders a thorough assessment of KGs’ potential to reduce hallucinations in LLMs. To address this, we introduce OKGQA, a new benchmark specifically designed to evaluate LLMs augmented with KGs in open-ended, real-world question answering settings. OKGQA reflects practical complexities through diverse question types and incorporates metrics to quantify both hallucination rates and reasoning improvements in LLM+KG models. To consider the scenarios in which KGs may contain varying levels of errors, we propose a benchmark variant, OKGQA-P, to assess model performance when the semantics and structure of KGs are deliberately perturbed and contaminated. In this paper, we aims to (1) explore whether KGs can make LLMs more trustworthy in an open-ended setting, and (2) conduct a comparative analysis to shed light on method design. We believe this study can facilitate a more complete performance comparison and encourages continuous improvement in integrating KGs with LLMs to mitigate hallucination, and make LLMs more trustworthy.</abstract>
      <url hash="2d947fec">2025.acl-long.622</url>
      <bibkey>sui-etal-2025-knowledge</bibkey>
    </paper>
    <paper id="623">
      <title>Nudging: Inference-time Alignment of <fixed-case>LLM</fixed-case>s via Guided Decoding</title>
      <author><first>Yu</first><last>Fei</last><affiliation>University of California, Irvine</affiliation></author>
      <author><first>Yasaman</first><last>Razeghi</last></author>
      <author><first>Sameer</first><last>Singh</last><affiliation>University of California, Irvine</affiliation></author>
      <pages>12702-12739</pages>
      <abstract>Large language models (LLMs) require alignment to effectively and safely follow user instructions. This process necessitates training an aligned version for every base model, resulting in significant computational overhead. In this work, we propose NUDGING, a simple, training-free algorithm that aligns any base model at inference time using a small aligned model. NUDGING is motivated by recent findings that alignment primarily alters the model’s behavior on a small subset of stylistic tokens (e.g., discourse markers). We find that base models are significantly more uncertain when generating these tokens. Building on this insight, NUDGING employs a small aligned model to generate nudging tokens to guide the base model’s output during decoding when the base model’s uncertainty is high, with only a minor additional inference overhead. We evaluate NUDGING across 3 model families on a diverse range of open-instruction tasks. Without any training, nudging a large base model with a 7×-14× smaller aligned model achieves zero-shot performance comparable to, and sometimes surpassing, that of large aligned models. By operating at the token level, NUDGING enables off-the-shelf collaboration between model families. For instance, nudging Gemma-2-27b with Llama-27b-chat outperforms Llama-2-70b-chat on various tasks. Overall, our work offers a modular and cost-efficient solution to LLM alignment. Our code and demo are available at: https://fywalter.github.io/nudging/.</abstract>
      <url hash="1823665d">2025.acl-long.623</url>
      <bibkey>fei-etal-2025-nudging</bibkey>
    </paper>
    <paper id="624">
      <title>Unveiling Attractor Cycles in Large Language Models: A Dynamical Systems View of Successive Paraphrasing</title>
      <author><first>Zhilin</first><last>Wang</last></author>
      <author><first>Yafu</first><last>Li</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Jianhao</first><last>Yan</last><affiliation>Westlake University</affiliation></author>
      <author><first>Yu</first><last>Cheng</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Yue</first><last>Zhang</last><affiliation>Westlake University</affiliation></author>
      <pages>12740-12755</pages>
      <abstract>Dynamical systems theory provides a framework for analyzing iterative processes and evolution over time. Within such systems, repetitive transformations can lead to stable configurations, known as attractors, including fixed points and limit cycles. Applying this perspective to large language models (LLMs), which iteratively map input text to output text, provides a principled approach to characterizing long-term behaviors. Successive paraphrasing serves as a compelling testbed for exploring such dynamics, as paraphrases re-express the same underlying meaning with linguistic variation. Although LLMs are expected to explore a diverse set of paraphrases in the text space, our study reveals that successive paraphrasing converges to stable periodic states, such as 2-period attractor cycles, limiting linguistic diversity. This phenomenon is attributed to the self-reinforcing nature of LLMs, as they iteratively favour and amplify certain textual forms over others. This pattern persists with increasing generation randomness or alternating prompts and LLMs. These findings underscore inherent constraints in LLM generative capability, while offering a novel dynamical systems perspective for studying their expressive potential.</abstract>
      <url hash="9b844088">2025.acl-long.624</url>
      <bibkey>wang-etal-2025-unveiling</bibkey>
    </paper>
    <paper id="625">
      <title><fixed-case>SCAR</fixed-case>: Data Selection via Style Consistency-Aware Response Ranking for Efficient Instruction-Tuning of Large Language Models</title>
      <author><first>Zhuang</first><last>Li</last><affiliation>Royal Melbourne Institute of Technology</affiliation></author>
      <author><first>Yuncheng</first><last>Hua</last></author>
      <author><first>Thuy-Trang</first><last>Vu</last><affiliation>Monash University</affiliation></author>
      <author><first>Haolan</first><last>Zhan</last><affiliation>Monash University</affiliation></author>
      <author><first>Lizhen</first><last>Qu</last><affiliation>Monash University</affiliation></author>
      <author><first>Gholamreza</first><last>Haffari</last><affiliation>Monash University, Monash University and Monash University</affiliation></author>
      <pages>12756-12790</pages>
      <abstract>Recent studies emphasize that manually ensuring a consistent response style and maintaining high data quality in training sets can significantly improve the performance of fine-tuned Large Language Models (LLMs) while reducing the number of training examples needed. However, the precise definition of style and the relationship between style, data quality, and LLM performance remains unclear. This research identifies two key stylistic elements in responses: linguistic form and instructional surprisal. We find that, among training data of comparable quality, higher consistency in these response elements leads to better LLM performance. Inspired by this, we introduce Style Consistency-Aware Response Ranking (SCAR), which automatically prioritizes instruction-response pairs in the training set based on their response stylistic consistency. By selecting the most style-consistent examples, using 0.7% of the full dataset in certain cases, the fine-tuned LLMs can match or even surpass the performance of models trained on the entire dataset in coding and open-ended question-answering benchmarks. Code and data are available at https://github.com/zhuang-li/SCAR .</abstract>
      <url hash="d4839293">2025.acl-long.625</url>
      <bibkey>li-etal-2025-scar</bibkey>
    </paper>
    <paper id="626">
      <title><fixed-case>HFT</fixed-case>: Half Fine-Tuning for Large Language Models</title>
      <author><first>Tingfeng</first><last>Hui</last></author>
      <author><first>Zhenyu</first><last>Zhang</last><affiliation>Baidu Inc.</affiliation></author>
      <author><first>Shuohuan</first><last>Wang</last></author>
      <author><first>Weiran</first><last>Xu</last><affiliation>Beijing University of Post and Telecommunication</affiliation></author>
      <author><first>Yu</first><last>Sun</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <pages>12791-12819</pages>
      <abstract>Large language models (LLMs) with one or more fine-tuning phases have become necessary to unlock various capabilities, enabling LLMs to follow natural language instructions and align with human preferences. However, it carries the risk of catastrophic forgetting during sequential training, the parametric knowledge or the ability learned in previous stages may be overwhelmed by incoming training data. This paper finds that LLMs can restore some original knowledge by regularly resetting partial parameters. Inspired by this, we introduce Half Fine-Tuning (HFT) for LLMs, as a substitute for full fine-tuning (FFT), to mitigate the forgetting issues, where half of the parameters are selected to learn new tasks. In contrast, the other half are frozen to retain previous knowledge. We provide a feasibility analysis from the optimization perspective and interpret the parameter selection operation as a regularization term. HFT could be seamlessly integrated into existing fine-tuning frameworks without changing the model architecture. Extensive experiments and analysis on supervised fine-tuning, direct preference optimization, and continual learning consistently demonstrate the effectiveness, robustness, and efficiency of HFT. Compared with FFT, HFT not only significantly alleviates the forgetting problem, but also achieves the best performance in a series of downstream benchmarks, with an approximately 30% reduction in training time.</abstract>
      <url hash="ef50b9b4">2025.acl-long.626</url>
      <bibkey>hui-etal-2025-hft</bibkey>
    </paper>
    <paper id="627">
      <title>Beyond Surface Simplicity: Revealing Hidden Reasoning Attributes for Precise Commonsense Diagnosis</title>
      <author><first>Huijun</first><last>Lian</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Zekai</first><last>Sun</last></author>
      <author><first>Keqi</first><last>Chen</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Yingming</first><last>Gao</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Ya</first><last>Li</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <pages>12820-12835</pages>
      <abstract>Commonsense question answering (QA) are widely used to evaluate the commonsense abilities of large language models. However, answering commonsense questions correctly requires not only knowledge but also reasoning—even for seemingly simple questions. We demonstrate that such hidden reasoning attributes in commonsense questions can lead evaluation accuracy differences of up to 24.8% across different difficulty levels in the same benchmark. Current benchmarks overlook these hidden reasoning attributes, making it difficult to assess a model’s specific levels of commonsense knowledge and reasoning ability. To address this issue, we introduce ReComSBench, a novel framework that reveals hidden reasoning attributes behind commonsense questions by leveraging the knowledge generated during the reasoning process. Additionally, ReComSBench proposes three new metrics for decoupled evaluation: Knowledge Balanced Accuracy, Marginal Sampling Gain, and Knowledge Coverage Ratio. Experiments show that ReComSBench provides insights into model performance that traditional benchmarks cannot offer. The difficulty stratification based on revealed hidden reasoning attributes performs as effectively as the model-probability-based approach but is more generalizable and better suited for improving a model’s commonsense reasoning abilities. By uncovering and analyzing the hidden reasoning attributes in commonsense data, ReComSBench offers a new approach to enhancing existing commonsense benchmarks.</abstract>
      <url hash="21bebd04">2025.acl-long.627</url>
      <bibkey>lian-etal-2025-beyond</bibkey>
    </paper>
    <paper id="628">
      <title>From Objectives to Questions: A Planning-based Framework for Educational Mathematical Question Generation</title>
      <author><first>Cheng</first><last>Cheng</last></author>
      <author><first>Zhenya</first><last>Huang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>GuanHao</first><last>Zhao</last></author>
      <author><first>Yuxiang</first><last>Guo</last></author>
      <author><first>Xin</first><last>Lin</last></author>
      <author><first>Jinze</first><last>Wu</last></author>
      <author><first>Xin</first><last>Li</last></author>
      <author><first>Shijin</first><last>Wang</last><affiliation>State Key Laboratory of Cognitive Intelligence</affiliation></author>
      <pages>12836-12856</pages>
      <abstract>Automatically generating high-quality mathematical problems that align with educational objectives is a crucial task in NLP-based educational technology. Traditional generation methods focus primarily on textual quality, but they often overlook educational objectives. Moreover, these methods address only single-dimensional, simple question generation, failing to meet complex, multifaceted educational requirements. To address these challenges, we constructed and annotated EduMath, a dataset of 16k mathematical questions with multi-dimensional educational objectives. Based on this dataset, we developed EQGEVAL, which incorporates three evaluation dimensions and is designed to assess the ability of models to generate educational questions. Drawing inspiration from teachers’ problem design processes, we propose the Educational Question Planning with self-Reflection (EQPR) method for educational mathematical question generation, following a “plan-evaluate-optimize” approach. Specifically, by combining planning algorithm based on Monte Carlo Tree Search with the generative capabilities of Large Language Models, we continuously optimize questions through iterative feedback. This self-optimization mechanism ensures that the generated questions both fit the educational context and strategically achieve specific basic educational objectives. Through extensive experiments based on EQGEVAL, we have demonstrated that EQPR achieves significant improvements in generating questions that meet multi-dimensional educational objectives.</abstract>
      <url hash="1dc5fa60">2025.acl-long.628</url>
      <bibkey>cheng-etal-2025-objectives</bibkey>
    </paper>
    <paper id="629">
      <title><fixed-case>R</fixed-case>ank<fixed-case>C</fixed-case>o<fixed-case>T</fixed-case>: Refining Knowledge for Retrieval-Augmented Generation through Ranking Chain-of-Thoughts</title>
      <author><first>Mingyan</first><last>Wu</last></author>
      <author><first>Zhenghao</first><last>Liu</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Yukun</first><last>Yan</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Xinze</first><last>Li</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Shi</first><last>Yu</last><affiliation>Department of Computer Science and Technology, Tsinghua University</affiliation></author>
      <author><first>Zheni</first><last>Zeng</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Yu</first><last>Gu</last></author>
      <author><first>Ge</first><last>Yu</last></author>
      <pages>12857-12874</pages>
      <abstract>Retrieval-Augmented Generation (RAG) enhances the performance of Large Language Models (LLMs) by incorporating external knowledge. However, LLMs still encounter challenges in effectively utilizing the knowledge from retrieved documents, often being misled by irrelevant or noisy information. To address this issue, we introduce RankCoT, a knowledge refinement method that incorporates reranking signals in generating CoT-based summarization for knowledge refinement based on given query and all retrieval documents. During training, RankCoT prompts the LLM to generate Chain-of-Thought (CoT) candidates based on the query and individual documents. It then fine-tunes the LLM to directly reproduce the best CoT from these candidate outputs based on all retrieved documents, which requires LLM to filter out irrelevant documents during generating CoT-style summarization. Additionally, RankCoT incorporates a self-reflection mechanism that further refines the CoT outputs, resulting in higher-quality training data. Our experiments demonstrate the effectiveness of RankCoT, showing its superior performance over other knowledge refinement models. Further analysis reveals that RankCoT can provide shorter but effective refinement results, enabling the generator to produce more accurate answers. All code and data are available at https://github.com/NEUIR/RankCoT.</abstract>
      <url hash="1ceed0c9">2025.acl-long.629</url>
      <bibkey>wu-etal-2025-rankcot</bibkey>
    </paper>
    <paper id="630">
      <title>Lost in Literalism: How Supervised Training Shapes Translationese in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Yafu</first><last>Li</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Ronghao</first><last>Zhang</last></author>
      <author><first>Zhilin</first><last>Wang</last></author>
      <author><first>Huajian</first><last>Zhang</last></author>
      <author><first>Leyang</first><last>Cui</last></author>
      <author><first>Yongjing</first><last>Yin</last></author>
      <author><first>Tong</first><last>Xiao</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Yue</first><last>Zhang</last><affiliation>Westlake University</affiliation></author>
      <pages>12875-12894</pages>
      <abstract>Large language models (LLMs) have achieved remarkable success in machine translation, demonstrating impressive performance across diverse languages. However, translationese—characterized by overly literal and unnatural translations—remains a persistent challenge in LLM-based translation systems. Despite their pre-training on vast corpora of natural utterances, LLMs exhibit translationese errors and generate unexpected unnatural translations, stemming from biases introduced during supervised fine-tuning (SFT). In this work, we systematically evaluate the prevalence of translationese in LLM-generated translations and investigate its roots during supervised training. We introduce methods to mitigate these biases, including polishing golden references and filtering unnatural training instances. Empirical evaluations demonstrate that these approaches significantly reduce translationese while improving translation naturalness, validated by human evaluations and automatic metrics. Our findings highlight the need for training-aware adjustments to optimize LLM translation outputs, paving the way for more fluent and target-language-consistent translations.</abstract>
      <url hash="aa43a434">2025.acl-long.630</url>
      <bibkey>li-etal-2025-lost</bibkey>
    </paper>
    <paper id="631">
      <title>Accurate <fixed-case>KV</fixed-case> Cache Quantization with Outlier Tokens Tracing</title>
      <author><first>Yi</first><last>Su</last></author>
      <author><first>Yuechi</first><last>Zhou</last></author>
      <author><first>Quantong</first><last>Qiu</last><affiliation>Jiangxi University of Finance and Economics</affiliation></author>
      <author><first>Juntao</first><last>Li</last></author>
      <author><first>Qingrong</first><last>Xia</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Ping</first><last>Li</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Xinyu</first><last>Duan</last></author>
      <author><first>Zhefeng</first><last>Wang</last></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>12895-12915</pages>
      <abstract>The impressive capabilities of Large Language Models (LLMs) come at the cost of substantial computational resources during deployment. While KV Cache can significantly reduce recomputation during inference, it also introduces additional memory overhead. KV Cache quantization presents a promising solution, striking a good balance between memory usage and accuracy. Previous research has shown that the Keys are distributed by channel, while the Values are distributed by token. Consequently, the common practice is to apply channel-wise quantization to the Keys and token-wise quantization to the Values. However, our further investigation reveals that a small subset of unusual tokens exhibit unique characteristics that deviate from this pattern, which can substantially impact quantization accuracy. To address this, we develop a simple yet effective method to identify these tokens accurately during the decoding process and exclude them from quantization as outlier tokens, significantly improving overall accuracy. Extensive experiments show that our method achieves significant accuracy improvements under 2-bit quantization and can deliver a 6.4 times reduction in memory usage and a 2.3 times increase in throughput.</abstract>
      <url hash="aaa4ec83">2025.acl-long.631</url>
      <bibkey>su-etal-2025-accurate</bibkey>
    </paper>
    <paper id="632">
      <title>Can Large Language Models Understand <fixed-case>I</fixed-case>nternet Buzzwords Through User-Generated Content</title>
      <author><first>Chen</first><last>Huang</last><affiliation>Sichuan University</affiliation></author>
      <author><first>Junkai</first><last>Luo</last><affiliation>University of California, Riverside and Sichuan University</affiliation></author>
      <author><first>Xinzuo</first><last>Wang</last></author>
      <author><first>Wenqiang</first><last>Lei</last><affiliation>Sichuan University</affiliation></author>
      <author><first>Jiancheng</first><last>Lv</last><affiliation>Sichuan University</affiliation></author>
      <pages>12916-12941</pages>
      <abstract>The massive user-generated content (UGC) available in Chinese social media is giving rise to the possibility of studying internet buzzwords. In this paper, we study if large language models (LLMs) can generate accurate definitions for these buzzwords based on UGC as examples. Our work serves a threefold contribution. First, we introduce CHEER, the first dataset of Chinese internet buzzwords, each annotated with a definition and relevant UGC. Second, we propose a novel method, called RESS, to effectively steer the comprehending process of LLMs to produce more accurate buzzword definitions, mirroring the skills of human language learning. Third, with CHEER, we benchmark the strengths and weaknesses of various off-the-shelf definition generation methods and our RESS. Our benchmark demonstrates the effectiveness of RESS while revealing a crucial shared challenge: comprehending unseen buzzwords and leveraging sufficient, high-quality UGC to facilitate this comprehension. In this paper, we believe our work lays the groundwork for future advancements in LLM-based definition generation. Our dataset and code will be openly released.</abstract>
      <url hash="ed6f16c0">2025.acl-long.632</url>
      <bibkey>huang-etal-2025-large</bibkey>
    </paper>
    <paper id="633">
      <title><fixed-case>EAC</fixed-case>-<fixed-case>M</fixed-case>o<fixed-case>E</fixed-case>: Expert-Selection Aware Compressor for Mixture-of-Experts Large Language Models</title>
      <author><first>Yuanteng</first><last>Chen</last><affiliation>Institute of Automation, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yuantian</first><last>Shao</last><affiliation>Nanjing University of Science and Technology</affiliation></author>
      <author><first>Peisong</first><last>Wang</last><affiliation>Institute of Automation of,Chinese Academy of Sciences</affiliation></author>
      <author><first>Jian</first><last>Cheng</last><affiliation>Institute of Automation, Chinese Academy of Sciences</affiliation></author>
      <pages>12942-12963</pages>
      <abstract>Mixture-of-Experts (MoE) has demonstrated promising potential in scaling LLMs. However, it is hindered by two critical challenges: (1) substantial GPU memory consumption to load all experts; (2) low activated parameters cannot be equivalently translated into inference acceleration effects. In this work, we propose EAC-MoE, an Expert-Selection Aware Compressor for MoE-LLMs, which deeply aligns with the characteristics of MoE from the perspectives of quantization and pruning, and introduces two modules to address these two challenges respectively: (1) The expert selection bias caused by low-bit quantization is a major factor contributing to the performance degradation in MoE-LLMs. Based on this, we propose Quantization with Expert-Selection Calibration (QESC), which mitigates the expert selection bias by calibrating the routers within the MoE; (2) There are always certain experts that are not crucial for the corresponding tasks, yet causing inference latency. Therefore, we propose Pruning based on Expert-Selection Frequency (PESF), which significantly improves inference speed by pruning less frequently used experts for current task. Extensive experiments demonstrate that our approach significantly reduces memory usage and improves inference speed with minimal performance degradation.</abstract>
      <url hash="f18f6545">2025.acl-long.633</url>
      <bibkey>chen-etal-2025-eac</bibkey>
    </paper>
    <paper id="634">
      <title>Activation Steering Decoding: Mitigating Hallucination in Large Vision-Language Models through Bidirectional Hidden State Intervention</title>
      <author><first>Jingran</first><last>Su</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Jingfan</first><last>Chen</last></author>
      <author><first>Hongxin</first><last>Li</last></author>
      <author><first>Yuntao</first><last>Chen</last><affiliation>Centre for Artificial Intelligence and Robotics (CAIR), Hong Kong Institute of Science &amp; Innovation, Chinese Academy of Sciences</affiliation></author>
      <author><first>Li</first><last>Qing</last><affiliation>The Hong Kong Polytechnic University and Hong Kong Polytechnic University</affiliation></author>
      <author><first>Zhaoxiang</first><last>Zhang</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <pages>12964-12974</pages>
      <abstract>Large Vision-Language Models (LVLMs) have demonstrated impressive capabilities in multimodal understanding, but they frequently suffer from hallucination - generating content inconsistent with visual inputs. In this work, we explore a novel perspective on hallucination mitigation by examining the intermediate activations of LVLMs during generation. Our investigation reveals that hallucinated content manifests as distinct, identifiable patterns in the model’s hidden state space. Motivated by this finding, we propose Activation Steering Decoding (ASD), a training-free approach that mitigates hallucination through targeted intervention in the model’s intermediate activations. ASD operates by first identifying directional patterns of hallucination in the activation space using a small calibration set, then employing a contrast decoding mechanism that computes the difference between positive and negative steering predictions. This approach effectively suppresses hallucination patterns while preserving the model’s general capabilities. Extensive experiments demonstrate that our method significantly reduces hallucination across multiple benchmarks while maintaining performance on general visual understanding tasks. Notably, our approach requires no model re-training or architectural modifications, making it readily applicable to existing deployed models.</abstract>
      <url hash="9f72ae11">2025.acl-long.634</url>
      <bibkey>su-etal-2025-activation</bibkey>
    </paper>
    <paper id="635">
      <title>Interactive Evolution: A Neural-Symbolic Self-Training Framework For Large Language Models</title>
      <author><first>Fangzhi</first><last>Xu</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Qiushi</first><last>Sun</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Kanzhi</first><last>Cheng</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Jun</first><last>Liu</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Yu</first><last>Qiao</last><affiliation>Shanghai Aritifcal Intelligence Laboratory</affiliation></author>
      <author><first>Zhiyong</first><last>Wu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <pages>12975-12993</pages>
      <abstract>One of the primary driving forces contributing to the superior performance of Large Language Models (LLMs) is the extensive availability of human-annotated natural language data, which is used for alignment fine-tuning. This inspired researchers to investigate self-training methods to mitigate the extensive reliance on human annotations. However, the current success of self-training has been primarily observed in natural language scenarios, rather than in the increasingly important neural-symbolic scenarios. To this end, we propose an environment-guided neural-symbolic self-training framework named ENVISIONS. It aims to overcome two main challenges: (1) the scarcity of symbolic data, and (2) the limited proficiency of LLMs in processing symbolic language. Extensive evaluations conducted on three distinct domains demonstrate the effectiveness of our approach. Additionally, we have conducted a comprehensive analysis to uncover the factors contributing to ENVISIONS’s success, thereby offering valuable insights for future research in this area.</abstract>
      <url hash="ab80b99e">2025.acl-long.635</url>
      <bibkey>xu-etal-2025-interactive</bibkey>
    </paper>
    <paper id="636">
      <title>Improving Medical Large Vision-Language Models with Abnormal-Aware Feedback</title>
      <author><first>Yucheng</first><last>Zhou</last><affiliation>University of Macau</affiliation></author>
      <author><first>Lingran</first><last>Song</last></author>
      <author><first>Jianbing</first><last>Shen</last><affiliation>University of Macau</affiliation></author>
      <pages>12994-13011</pages>
      <abstract>Existing Medical Large Vision-Language Models (Med-LVLMs), encapsulating extensive medical knowledge, demonstrate excellent capabilities in understanding medical images. However, there remain challenges in visual localization in medical images, which is crucial for abnormality detection and interpretation. To address these issues, we propose a novel UMed-LVLM designed to unveil medical abnormalities. Specifically, we collect a Medical Abnormalities Unveiling (MAU) dataset and propose a two-stage training method for UMed-LVLM training. To collect MAU dataset, we propose a prompt method utilizing the GPT-4V to generate diagnoses based on identified abnormal areas in medical images. Moreover, the two-stage training method includes Abnormal-Aware Instruction Tuning and Abnormal-Aware Rewarding, comprising Relevance Reward, Abnormal Localization Reward and Vision Relevance Reward. Experimental results demonstrate that our UMed-LVLM significantly outperforms existing Med-LVLMs in identifying and understanding medical abnormalities, achieving a 58% improvement over the baseline. In addition, this work shows that enhancing the abnormality detection capabilities of Med-LVLMs significantly improves their understanding of medical images and generalization capability. Our code and data release at URL.</abstract>
      <url hash="cf88d9c6">2025.acl-long.636</url>
      <bibkey>zhou-etal-2025-improving</bibkey>
    </paper>
    <paper id="637">
      <title>Upcycling Instruction Tuning from Dense to Mixture-of-Experts via Parameter Merging</title>
      <author><first>Tingfeng</first><last>Hui</last></author>
      <author><first>Zhenyu</first><last>Zhang</last><affiliation>Baidu Inc.</affiliation></author>
      <author><first>Shuohuan</first><last>Wang</last></author>
      <author><first>Yu</first><last>Sun</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <author><first>Sen</first><last>Su</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <pages>13012-13031</pages>
      <abstract>Mixture-of-Experts (MoE) shines brightly in large language models (LLMs) and demonstrates outstanding performance in plentiful natural language processing tasks. However, existing methods transforming LLMs from dense to MoE face significant data requirements and typically rely on large-scale post-training. In this paper, we propose Upcycling Instruction Tuning (UpIT), a data-efficient approach for tuning a dense pre-trained model into a MoE instruction model. Specifically, we first point out that intermediate checkpoints during instruction tuning of the dense model are naturally suitable for specialized experts, and then propose an expert expansion stage to flexibly achieve models with flexible numbers of experts, where genetic algorithm and parameter merging are introduced to ensure sufficient diversity of new extended experts. To ensure that each specialized expert in the MoE model works as expected, we select a small amount of seed data that each expert excels to pre-optimize the router. Extensive experiments with various data scales and upcycling settings demonstrate the outstanding performance and data efficiency of UpIT, as well as stable improvement in expert or data scaling. Further analysis reveals the importance of ensuring expert diversity in upcycling.</abstract>
      <url hash="712d8001">2025.acl-long.637</url>
      <bibkey>hui-etal-2025-upcycling</bibkey>
    </paper>
    <paper id="638">
      <title><fixed-case>M</fixed-case>ap<fixed-case>N</fixed-case>av: A Novel Memory Representation via Annotated Semantic Maps for <fixed-case>VLM</fixed-case>-based Vision-and-Language Navigation</title>
      <author><first>Lingfeng</first><last>Zhang</last><affiliation>Beijing Academy of Artificial Intelligence(BAAl), Peking University and Hong Kong University of Science and Technology (Guangzhou)</affiliation></author>
      <author><first>Xiaoshuai</first><last>Hao</last><affiliation>Beijing Academy of Artificial Intelligence(BAAl)</affiliation></author>
      <author><first>Qinwen</first><last>Xu</last></author>
      <author><first>Qiang</first><last>Zhang</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Xinyao</first><last>Zhang</last></author>
      <author><first>Pengwei</first><last>Wang</last></author>
      <author><first>Jing</first><last>Zhang</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Zhongyuan</first><last>Wang</last><affiliation>BAAI</affiliation></author>
      <author><first>Shanghang</first><last>Zhang</last></author>
      <author><first>Renjing</first><last>Xu</last><affiliation>The Hong Kong University of Science and Technology (Guangzhou)</affiliation></author>
      <pages>13032-13056</pages>
      <abstract>Vision-language navigation (VLN) is a key task in Embodied AI, requiring agents to navigate diverse and unseen environments while following natural language instructions. Traditional approaches rely heavily on historical observations as spatio-temporal contexts for decision making, leading to significant storage and computational overhead. In this paper, we introduce MapNav, a novel end-to-end VLN model that leverages Annotated Semantic Map (ASM) to replace historical frames. Specifically, our approach constructs a top-down semantic map at the start of each episode and update it at each timestep, allowing for precise object mapping and structured navigation information. Then, we enhance this map with explicit textual labels for key regions, transforming abstract semantics into clear navigation cues and generate our ASM. MapNav agent using the constructed ASM as input, and use the powerful end-to-end capabilities of VLM to empower VLN. Extensive experiments demonstrate that MapNav achieves state-of-the-art (SOTA) performance in both simulated and real-world environments, validating the effectiveness of our method. We will release our ASM generation source code and dataset to ensure reproducibility, contributing valuable resources to the field. We believe that our proposed MapNav can be used as a new memory representation method in VLN, paving the way for future research in this field.</abstract>
      <url hash="ddb42c43">2025.acl-long.638</url>
      <bibkey>zhang-etal-2025-mapnav</bibkey>
    </paper>
    <paper id="639">
      <title>Exploring Compositional Generalization of Multimodal <fixed-case>LLM</fixed-case>s for Medical Imaging</title>
      <author><first>Zhenyang</first><last>Cai</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <author><first>Junying</first><last>Chen</last></author>
      <author><first>Rongsheng</first><last>Wang</last></author>
      <author><first>Weihong</first><last>Wang</last><affiliation>The Chinese University of Hong Kong and The Chinese University of Hong Kong</affiliation></author>
      <author><first>Yonglin</first><last>Deng</last></author>
      <author><first>Dingjie</first><last>Song</last><affiliation>Lehigh University</affiliation></author>
      <author><first>Yize</first><last>Chen</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Zixu</first><last>Zhang</last><affiliation>The Chinese University of Hong Kong (ShenZhen)</affiliation></author>
      <author><first>Benyou</first><last>Wang</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <pages>13057-13079</pages>
      <abstract>Medical imaging provides essential visual insights for diagnosis, and multimodal large language models (MLLMs) are increasingly utilized for its analysis due to their strong generalization capabilities; however, the underlying factors driving this generalization remain unclear. Current research suggests that multi-task training outperforms single-task as different tasks can benefit each other, but they often overlook the internal relationships within these tasks. To analyze this phenomenon, we attempted to employ **compositional generalization** (CG), which refers to the models’ ability to understand novel combinations by recombining learned elements, as a guiding framework. Since medical images can be precisely defined by **M**odality, **A**natomical area, and **T**ask, naturally providing an environment for exploring CG, we assembled 106 medical datasets to create **Med-MAT** for comprehensive experiments. The experiments confirmed that MLLMs can use CG to understand unseen medical images and identified CG as one of the main drivers of the generalization observed in multi-task training. Additionally, further studies demonstrated that CG effectively supports datasets with limited data and confirmed that MLLMs can achieve CG across classification and detection tasks, underscoring its broader generalization potential. Med-MAT is available at https://github.com/FreedomIntelligence/Med-MAT.</abstract>
      <url hash="7c414528">2025.acl-long.639</url>
      <bibkey>cai-etal-2025-exploring</bibkey>
    </paper>
    <paper id="640">
      <title><fixed-case>CLAIM</fixed-case>: Mitigating Multilingual Object Hallucination in Large Vision-Language Models with Cross-Lingual Attention Intervention</title>
      <author><first>Zekai</first><last>Ye</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Qiming</first><last>Li</last></author>
      <author><first>Xiaocheng</first><last>Feng</last></author>
      <author><first>Libo</first><last>Qin</last><affiliation>Central South University</affiliation></author>
      <author><first>Yichong</first><last>Huang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Baohang</first><last>Li</last></author>
      <author><first>Kui</first><last>Jiang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Yang</first><last>Xiang</last></author>
      <author><first>Zhirui</first><last>Zhang</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Yunfei</first><last>Lu</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Duyu</first><last>Tang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Dandan</first><last>Tu</last></author>
      <author><first>Bing</first><last>Qin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>13080-13094</pages>
      <abstract>Large Vision-Language Models (LVLMs) have demonstrated impressive multimodal abilities but remain prone to multilingual object hallucination, with a higher likelihood of generating responses inconsistent with the visual input when utilizing queries in non-English languages compared to English. Most existing approaches to address these rely on pretraining or fine-tuning, which are resource-intensive. In this paper, inspired by observing the disparities in cross-modal attention patterns across languages, we propose Cross-Lingual Attention Intervention for Mitigating multilingual object hallucination (CLAIM) in LVLMs, a novel near training-free method by aligning attention patterns. CLAIM first identifies language-specific cross-modal attention heads, then estimates language shift vectors from English to the target language, and finally intervenes in the attention outputs during inference to facilitate cross-lingual visual perception capability alignment. Extensive experiments demonstrate that CLAIM achieves an average improvement of 13.56% (up to 30% in Spanish) on the POPE and 21.75% on the hallucination subsets of the MME benchmark across various languages. Further analysis reveals that multilingual attention divergence is most prominent in intermediate layers, highlighting their critical role in multilingual scenarios.</abstract>
      <url hash="11d4fab9">2025.acl-long.640</url>
      <bibkey>ye-etal-2025-claim</bibkey>
    </paper>
    <paper id="641">
      <title>Wizard of Shopping: Target-Oriented <fixed-case>E</fixed-case>-commerce Dialogue Generation with Decision Tree Branching</title>
      <author><first>Xiangci</first><last>Li</last><affiliation>Amazon Web Services</affiliation></author>
      <author><first>Zhiyu</first><last>Chen</last><affiliation>Amazon</affiliation></author>
      <author><first>Jason Ingyu</first><last>Choi</last><affiliation>Amazon</affiliation></author>
      <author><first>Nikhita</first><last>Vedula</last><affiliation>Amazon</affiliation></author>
      <author><first>Besnik</first><last>Fetahu</last><affiliation>Amazon</affiliation></author>
      <author><first>Oleg</first><last>Rokhlenko</last></author>
      <author><first>Shervin</first><last>Malmasi</last><affiliation>Amazon</affiliation></author>
      <pages>13095-13120</pages>
      <abstract>The goal of conversational product search (CPS) is to develop an intelligent, chat-based shopping assistant that can directly interact with customers to understand shopping intents, ask clarification questions, and find relevant products. However, training such assistants is hindered mainly due to the lack of reliable and large-scale datasets. Prior human-annotated CPS datasets are extremely small in size and lack integration with real-world product search systems. We propose a novel approach, TRACER, which leverages large language models (LLMs) to generate realistic and natural conversations for different shopping domains. TRACER’s novelty lies in grounding the generation to dialogue plans, which are product search trajectories predicted from a decision tree model, that guarantees relevant product discovery in the shortest number of search conditions. We also release the first target-oriented CPS dataset Wizard of Shopping (WoS), containing highly natural and coherent conversations (3.6k) from three shopping domains. Finally, we demonstrate the quality and effectiveness of WoS via human evaluations and downstream tasks.</abstract>
      <url hash="a6ef0b82">2025.acl-long.641</url>
      <bibkey>li-etal-2025-wizard</bibkey>
    </paper>
    <paper id="642">
      <title>Qwen2.5-x<fixed-case>C</fixed-case>oder: Multi-Agent Collaboration for Multilingual Code Instruction Tuning</title>
      <author><first>Jian</first><last>Yang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Wei</first><last>Zhang</last></author>
      <author><first>Yibo</first><last>Miao</last></author>
      <author><first>Shanghaoran</first><last>Quan</last></author>
      <author><first>Zhenhe</first><last>Wu</last><affiliation>Beijing University of Aeronautics and Astronautics</affiliation></author>
      <author><first>Qiyao</first><last>Peng</last></author>
      <author><first>Liqun</first><last>Yang</last></author>
      <author><first>Tianyu</first><last>Liu</last></author>
      <author><first>Zeyu</first><last>Cui</last></author>
      <author><first>Binyuan</first><last>Hui</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Junyang</first><last>Lin</last><affiliation>Alibaba Group</affiliation></author>
      <pages>13121-13131</pages>
      <abstract>Recent advancement in code understanding and generation demonstrates that code LLMs fine-tuned on a high-quality instruction dataset can gain powerful capabilities to address wide-ranging code-related tasks. However, most previous existing methods mainly view each programming language in isolation and ignore the knowledge transfer among different programming languages. To bridge the gap among different programming languages, we introduce a novel multi-agent collaboration framework to enhance multilingual instruction tuning for code LLMs, where multiple language-specific intelligent agent components with generation memory work together to transfer knowledge from one language to another efficiently and effectively. Specifically, we first generate the language-specific instruction data from the code snippets and then provide the generated data as the seed data for language-specific agents. Multiple language-specific agents discuss and collaborate to formulate a new instruction and its corresponding solution (A new programming language or existing programming language), To further encourage the cross-lingual transfer, each agent stores its generation history as memory and then summarizes its merits and faults. Finally, the high-quality multilingual instruction data is used to encourage knowledge transfer among different programming languages to train Qwen2.5-xCoder. Experimental results on multilingual programming benchmarks demonstrate the superior performance of Qwen2.5-xCoder in sharing common knowledge, highlighting its potential to reduce the cross-lingual gap.</abstract>
      <url hash="debc4946">2025.acl-long.642</url>
      <bibkey>yang-etal-2025-qwen2</bibkey>
    </paper>
    <paper id="643">
      <title>Cultivating Gaming Sense for Yourself: Making <fixed-case>VLM</fixed-case>s Gaming Experts</title>
      <author><first>Wenxuan</first><last>Lu</last></author>
      <author><first>Jiangyang</first><last>He</last><affiliation>Wuhan University of Technology</affiliation></author>
      <author><first>Zhanqiu</first><last>Zhang</last><affiliation>University of Science and Technology of China, Tsinghua University</affiliation></author>
      <author><first>Steven Y.</first><last>Guo</last></author>
      <author><first>Tianning</first><last>Zang</last></author>
      <pages>13132-13152</pages>
      <abstract>Developing agents capable of fluid gameplay in first/third-person games without API access remains a critical challenge in Artificial General Intelligence (AGI). Recent efforts leverage Vision Language Models (VLMs) as direct controllers, frequently pausing the game to analyze screens and plan action through language reasoning. However, this inefficient paradigm fundamentally restricts agents to basic and non-fluent interactions: relying on isolated VLM reasoning for each action makes it impossible to handle tasks requiring high reactivity (e.g., FPS shooting) or dynamic adaptability (e.g., ACT combat). To handle this, we propose a paradigm shift in gameplay agent design: instead of direct control, VLM serves as a developer, creating specialized execution modules tailored for tasks like shooting and combat. These modules handle real-time game interactions, elevating VLM to a high-level developer. Building upon this paradigm, we introduce GameSense, a gameplay agent framework where VLM develops task-specific game sense modules by observing task execution and leveraging vision tools and neural network training pipelines. These modules encapsulate action-feedback logic, ranging from direct action rules to neural network-based decisions. Experiments demonstrate that our framework is the first to achieve fluent gameplay in diverse genres, including ACT, FPS, and Flappy Bird, setting a new benchmark for game-playing agents.</abstract>
      <url hash="5f743096">2025.acl-long.643</url>
      <bibkey>lu-etal-2025-cultivating</bibkey>
    </paper>
    <paper id="644">
      <title>Genius: A Generalizable and Purely Unsupervised Self-Training Framework For Advanced Reasoning</title>
      <author><first>Fangzhi</first><last>Xu</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Hang</first><last>Yan</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Chang</first><last>Ma</last></author>
      <author><first>Haiteng</first><last>Zhao</last></author>
      <author><first>Qiushi</first><last>Sun</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Kanzhi</first><last>Cheng</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Junxian</first><last>He</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Jun</first><last>Liu</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Zhiyong</first><last>Wu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <pages>13153-13167</pages>
      <abstract>Advancing LLM reasoning skills has captivated wide interest. However, current post-training techniques rely heavily on supervisory signals, such as outcome supervision or auxiliary reward models, which face the problem of scalability and high annotation costs. This motivates us to enhance LLM reasoning without the need for external supervision. Given the input query, the LLM seeks the globally optimal response by stepwise sampling and self-rewarding, and optimizes itself with the collected responses. Genius offers some technical solutions to address the following key challenges. To tackle the problem of how to determine the steps in the response via self-rewarding, Genius introduces a stepwise foresight re-sampling strategy to sample and estimate the step value by simulating future outcomes. Recognizing the intrinsic noise and uncertainty of self-supervision, we propose an advantage-calibrated optimization (ACO) loss function to mitigate estimation inconsistencies. In short, Genius provides an advanced initial step towards self-improve LLM reasoning with general queries and without supervision, revolutionizing reasoning scaling laws given the vast availability of general queries.</abstract>
      <url hash="503067c5">2025.acl-long.644</url>
      <bibkey>xu-etal-2025-genius</bibkey>
    </paper>
    <paper id="645">
      <title>Extending Complex Logical Queries on Uncertain Knowledge Graphs</title>
      <author><first>Weizhi</first><last>Fei</last><affiliation>The Department of Mathematics, Tsinghua University</affiliation></author>
      <author><first>Zihao</first><last>Wang</last><affiliation>TSY Capital</affiliation></author>
      <author><first>Hang</first><last>Yin</last></author>
      <author><first>Yang</first><last>Duan</last></author>
      <author><first>Yangqiu</first><last>Song</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <pages>13168-13193</pages>
      <abstract>The study of machine learning-based logical query-answering enables reasoning with large-scale and incomplete knowledge graphs. This paper further advances this line of research by considering the uncertainty in the knowledge. The uncertain nature of knowledge is widely observed in the real world, but does not align seamlessly with the first-order logic underpinning existing studies. To bridge this gap, we study the setting of soft queries on uncertain knowledge, which is motivated by the establishment of soft constraint programming. We further propose an ML-based approach with both forward inference and backward calibration to answer soft queries on large-scale, incomplete, and uncertain knowledge graphs. Theoretical discussions reveal that our method ensures there are no catastrophic cascading errors in our forward inference algorithm while maintaining the same complexity as state-of-the-art inference algorithms for first-order queries. Empirical results justify the superior performance of our approach against previous ML-based methods with number embedding extensions.</abstract>
      <url hash="5b0e4ee3">2025.acl-long.645</url>
      <bibkey>fei-etal-2025-extending</bibkey>
    </paper>
    <paper id="646">
      <title>Knowledge Decoupling via Orthogonal Projection for Lifelong Editing of Large Language Models</title>
      <author><first>Haoyu</first><last>Xu</last></author>
      <author><first>Pengxiang</first><last>Lan</last></author>
      <author><first>Enneng</first><last>Yang</last></author>
      <author><first>Guibing</first><last>Guo</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Jianzhe</first><last>Zhao</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Linying</first><last>Jiang</last></author>
      <author><first>Xingwei</first><last>Wang</last><affiliation>Northeastern University</affiliation></author>
      <pages>13194-13213</pages>
      <abstract>As large language models (LLMs) require continuous knowledge updates and the mitigation of hallucination issues in generated content, lifelong model editing has become a prominent research area. A mainstream knowledge editing method usually freezes LLM’s original parameters and adds extra trainable modules for new knowledge management, reducing interference with old knowledge. Although these approaches have achieved some success, our experiments show that, after extensive editing, the model’s knowledge understanding and memory capacity significantly degrade, particularly concerning early edited knowledge. The root cause is that subsequent edits interfere with the previously edited knowledge, and we refer to this phenomenon as knowledge coupling. To address this issue, we propose the <b>Knowledge Decoupling Editing</b> (KDE) method. Specifically, KDE stores the basis vectors of the representation space of past edits in a knowledge cache. It projects the gradient of the current edit onto a space orthogonal to previous knowledge for updating. This method effectively alleviates the coupling between different pieces of knowledge. We also propose a two-stage training strategy to better balance the model’s ability to edit new knowledge and distinguish whether a query is related to previous edits. This strategy gradually reduces the interference between new knowledge editing and query distinction, maintaining stable performance during long-term editing. We compared KDE with nine cutting-edge editing methods across multiple mainstream LLMs. The results demonstrate that, regarding question-answering ability and hallucination mitigation, KDE achieves average improvements of 14% and 61%.</abstract>
      <url hash="86d8d2de">2025.acl-long.646</url>
      <bibkey>xu-etal-2025-knowledge</bibkey>
    </paper>
    <paper id="647">
      <title><tex-math>\phi</tex-math>-Decoding: Adaptive Foresight Sampling for Balanced Inference-Time Exploration and Exploitation</title>
      <author><first>Fangzhi</first><last>Xu</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Hang</first><last>Yan</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Chang</first><last>Ma</last></author>
      <author><first>Haiteng</first><last>Zhao</last></author>
      <author><first>Jun</first><last>Liu</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Qika</first><last>Lin</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Zhiyong</first><last>Wu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <pages>13214-13227</pages>
      <abstract>Inference-time optimization scales computation to derive deliberate reasoning steps for effective performance. While previous search-based strategies address the short-sightedness of auto-regressive generation, the vast search space leads to excessive exploration and insufficient exploitation. To strike an efficient balance to derive the optimal step, we frame the decoding strategy as foresight sampling, leveraging simulated future steps to obtain globally optimal step estimation. Built on it, we propose a novel decoding strategy, named <tex-math>\phi</tex-math>-Decoding. To provide a precise and expressive estimation of step value, <tex-math>\phi</tex-math>-Decoding approximates two distributions via foresight and clustering. Sampling from the joint distribution, the optimal steps can be selected for exploitation. To support adaptive computation allocation, we propose in-width and in-depth pruning strategies, featuring a light-weight solution to achieve inference efficiency. Extensive experiments across seven benchmarks show <tex-math>\phi</tex-math>-Decoding outperforms strong baselines in both performance and efficiency. Additional analysis demonstrates its generalization across various LLMs and scalability across a wide range of computing budgets.</abstract>
      <url hash="c4a85f05">2025.acl-long.647</url>
      <bibkey>xu-etal-2025-ph</bibkey>
    </paper>
    <paper id="648">
      <title>Can <fixed-case>LLM</fixed-case> Watermarks Robustly Prevent Unauthorized Knowledge Distillation?</title>
      <author><first>Leyi</first><last>Pan</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Aiwei</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Shiyu</first><last>Huang</last><affiliation>Zhipu AI</affiliation></author>
      <author><first>Yijian</first><last>Lu</last></author>
      <author><first>Xuming</first><last>Hu</last><affiliation>The Hong Kong University of Science and Technology (Guangzhou) and Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Lijie</first><last>Wen</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Irwin</first><last>King</last></author>
      <author><first>Philip S.</first><last>Yu</last><affiliation>University of Illinois Chicago</affiliation></author>
      <pages>13228-13251</pages>
      <abstract>The radioactive nature of Large Language Model (LLM) watermarking enables the detection of watermarks inherited by student models when trained on the outputs of watermarked teacher models, making it a promising tool for preventing unauthorized knowledge distillation. However, the robustness of watermark radioactivity against adversarial actors remains largely unexplored. In this paper, we investigate whether student models can acquire the capabilities of teacher models through knowledge distillation while avoiding watermark inheritance. We propose two categories of watermark removal approaches: pre-distillation removal through untargeted and targeted training data paraphrasing (UP and TP), and post-distillation removal through inference-time watermark neutralization (WN). Extensive experiments across multiple model pairs, watermarking schemes and hyper-parameter settings demonstrate that both TP and WN thoroughly eliminate inherited watermarks, with WN achieving this while maintaining knowledge transfer efficiency and low computational overhead. Given the ongoing deployment of watermarking techniques in production LLMs, these findings emphasize the urgent need for more robust defense strategies.</abstract>
      <url hash="2a40568b">2025.acl-long.648</url>
      <bibkey>pan-etal-2025-llm</bibkey>
    </paper>
    <paper id="649">
      <title>Rethinking Reward Model Evaluation Through the Lens of Reward Overoptimization</title>
      <author><first>Sunghwan</first><last>Kim</last></author>
      <author><first>Dongjin</first><last>Kang</last></author>
      <author><first>Taeyoon</first><last>Kwon</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Hyungjoo</first><last>Chae</last></author>
      <author><first>Dongha</first><last>Lee</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Jinyoung</first><last>Yeo</last><affiliation>Yonsei University</affiliation></author>
      <pages>13252-13280</pages>
      <abstract>Reward models (RMs) play a crucial role in reinforcement learning from human feedback (RLHF), aligning model behavior with human preferences. However, existing benchmarks for reward models show a weak correlation with the performance of optimized policies, suggesting that they fail to accurately assess the true capabilities of RMs. To bridge this gap, we explore several evaluation designs through the lens of reward overoptimization, i.e., a phenomenon that captures both how well the reward model aligns with human preferences and the dynamics of the learning signal it provides to the policy. The results highlight three key findings on how to construct a reliable benchmark: (i) it is important to minimize differences between chosen and rejected responses beyond correctness, (ii) evaluating reward models requires multiple comparisons across a wide range of chosen and rejected responses, and (iii) given that reward models encounter responses with diverse representations, responses should be sourced from a variety of models. However, we also observe that a extremely high correlation with degree of overoptimization leads to comparatively lower correlation with certain downstream performance. Thus, when designing a benchmark, it is desirable to use the degree of overoptimization as a useful tool, rather than the end goal.</abstract>
      <url hash="7c5f8be9">2025.acl-long.649</url>
      <bibkey>kim-etal-2025-rethinking</bibkey>
    </paper>
    <paper id="650">
      <title>Inducing lexicons of in-group language with socio-temporal context</title>
      <author><first>Christine</first><last>de Kock</last></author>
      <pages>13281-13291</pages>
      <abstract>In-group language is an important signifier of group dynamics. This paper proposes a novel method for inducing lexicons of in-group language, which incorporates its socio-temporal context. Existing methods for lexicon induction do not capture the evolving nature of in-group language, nor the social structure of the community. Using dynamic word and user embeddings trained on conversations from online anti-women communities, our approach outperforms prior methods for lexicon induction. We develop a test set for the task of lexicon induction and a new lexicon of manosphere language, validated by human experts, which quantifies the relevance of each term to a specific sub-community at a given point in time. Finally, we present novel insights on in-group language which illustrate the utility of this approach.</abstract>
      <url hash="ef7d997c">2025.acl-long.650</url>
      <bibkey>de-kock-2025-inducing</bibkey>
    </paper>
    <paper id="651">
      <title><fixed-case>LL</fixed-case>a<fixed-case>SE</fixed-case>-G1: Incentivizing Generalization Capability for <fixed-case>LL</fixed-case>a<fixed-case>MA</fixed-case>-based Speech Enhancement</title>
      <author><first>Boyi</first><last>Kang</last></author>
      <author><first>Xinfa</first><last>Zhu</last></author>
      <author><first>Zihan</first><last>Zhang</last></author>
      <author><first>Zhen</first><last>Ye</last></author>
      <author><first>Mingshuai</first><last>Liu</last></author>
      <author><first>Ziqian</first><last>Wang</last></author>
      <author><first>Yike</first><last>Zhu</last></author>
      <author><first>Guobin</first><last>Ma</last><affiliation>Northwest Polytechnical University Xi’an</affiliation></author>
      <author><first>Jun</first><last>Chen</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Longshuai</first><last>Xiao</last></author>
      <author><first>Chao</first><last>Weng</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Wei</first><last>Xue</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Lei</first><last>Xie</last><affiliation>Northwest Polytechnical University</affiliation></author>
      <pages>13292-13305</pages>
      <abstract>Recent advancements in language models (LMs) have demonstrated strong capabilities in semantic understanding and contextual modeling, which have flourished in generative speech enhancement (SE). However, many LM-based SE approaches primarily focus on semantic information, often neglecting the critical role of acoustic information, which leads to acoustic inconsistency after enhancement and limited generalization across diverse SE tasks. In this paper, we introduce LLaSE-G1, a LLaMA-based language model that incentivizes generalization capabilities for speech enhancement. LLaSE-G1 offers the following key contributions: First, to mitigate acoustic inconsistency, LLaSE-G1 employs continuous representations from WavLM as input and predicts speech tokens from X-Codec2, maximizing acoustic preservation. Second, to promote generalization capability, LLaSE-G1 introduces dual-channel inputs and outputs, unifying multiple SE tasks without requiring task-specific IDs. Third, LLaSE-G1 outperforms prior task-specific discriminative and generative SE models, demonstrating scaling effects at test time and emerging capabilities for unseen SE tasks. Additionally, we release our code and models to support further research in this area.</abstract>
      <url hash="1d9cadd6">2025.acl-long.651</url>
      <bibkey>kang-etal-2025-llase</bibkey>
    </paper>
    <paper id="652">
      <title><fixed-case>M</fixed-case>ada<fixed-case>KV</fixed-case>: Adaptive Modality-Perception <fixed-case>KV</fixed-case> Cache Eviction for Efficient Multimodal Long-Context Inference</title>
      <author><first>Kunxi</first><last>Li</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Zhonghua</first><last>Jiang</last><affiliation>Alibaba Group and Zhejiang University</affiliation></author>
      <author><first>Zhouzhou</first><last>Shen</last></author>
      <author><first>ZhaodeWang</first><last>ZhaodeWang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Chengfei</first><last>Lv</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Shengyu</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Fan</first><last>Wu</last></author>
      <author><first>Fei</first><last>Wu</last><affiliation>Zhejiang University</affiliation></author>
      <pages>13306-13318</pages>
      <abstract>This paper introduces MadaKV, a modality-adaptive key-value (KV) cache eviction strategy designed to enhance the efficiency of multimodal large language models (MLLMs) in long-context inference. In multimodal scenarios, attention heads exhibit varying preferences for different modalities, resulting in significant disparities in modality importance across attention heads. Traditional KV cache eviction methods, which are tailored for unimodal settings, fail to capture modality-specific information, thereby yielding suboptimal performance. MadaKV addresses these challenges through two key components: modality preference adaptation and hierarchical compression compensation. By dynamically sensing modality information within attention heads and adaptively retaining critical tokens, MadaKV achieves substantial reductions in KV cache memory footprint and model inference decoding latency (1.3 to 1.5 times improvement) while maintaining high accuracy across various multimodal long-context tasks. Extensive experiments on representative MLLMs and the MileBench benchmark demonstrate the effectiveness of MadaKV compared to existing KV cache eviction methods.</abstract>
      <url hash="f717d5f1">2025.acl-long.652</url>
      <bibkey>li-etal-2025-madakv</bibkey>
    </paper>
    <paper id="653">
      <title>Efficient <fixed-case>O</fixed-case>p<fixed-case>A</fixed-case>mp Adaptation for Zoom Attention to Golden Contexts</title>
      <author><first>Haoyuan</first><last>Wu</last></author>
      <author><first>Rui</first><last>Ming</last></author>
      <author><first>Haisheng</first><last>Zheng</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Zhuolun</first><last>He</last><affiliation>Department of Computer Science and Engineering, The Chinese University of Hong Kong</affiliation></author>
      <author><first>Bei</first><last>Yu</last><affiliation>Department of Computer Science and Engineering, The Chinese University of Hong Kong</affiliation></author>
      <pages>13319-13331</pages>
      <abstract>Large language models (LLMs) have shown significant promise in question-answering (QA) tasks, particularly in retrieval-augmented generation (RAG) scenarios and long-context applications. However, their performance is hindered by noisy reference documents, which often distract from essential information. Despite fine-tuning efforts, Transformer-based architectures struggle to prioritize relevant content. This is evidenced by their tendency to allocate disproportionate attention to irrelevant or later-positioned documents. Recent work proposes the differential attention mechanism to address this issue, but this mechanism is limited by an unsuitable common-mode rejection ratio (CMRR) and high computational costs. Inspired by the operational amplifier (OpAmp), we propose the OpAmp adaptation to address these challenges, which is implemented with adapters efficiently. By integrating the adapter into pre-trained Transformer blocks, our approach enhances focus on the golden context without costly training from scratch. Empirical evaluations on noisy-context benchmarks reveal that our Qwen2.5-OpAmp-72B model, trained with our OpAmp adaptation, surpasses the performance of state-of-the-art LLMs, including DeepSeek-V3 and GPT-4o.Our code is available at https://github.com/wuhy68/OpampAdapter.</abstract>
      <url hash="3996e76a">2025.acl-long.653</url>
      <bibkey>wu-etal-2025-efficient-opamp</bibkey>
    </paper>
    <paper id="654">
      <title>Language-Codec: Bridging Discrete Codec Representations and Speech Language Models</title>
      <author><first>Shengpeng</first><last>Ji</last></author>
      <author><first>Minghui</first><last>Fang</last></author>
      <author><first>Jialong</first><last>Zuo</last></author>
      <author><first>Ziyue</first><last>Jiang</last></author>
      <author><first>Dingdong</first><last>Wang</last></author>
      <author><first>Hanting</first><last>Wang</last></author>
      <author><first>Hai</first><last>Huang</last></author>
      <author><first>Zhou</first><last>Zhao</last><affiliation>Zhejiang University and Zhejiang University</affiliation></author>
      <pages>13332-13345</pages>
      <abstract>In recent years, large language models have achieved significant success in generative tasks (e.g., speech cloning and audio generation) related to speech, audio, music, and other signal domains. A crucial element of these models is the discrete acoustic codecs, which serve as an intermediate representation replacing the mel-spectrogram. However, there exist several gaps between discrete codecs and downstream speech language models. Specifically, 1) Due to the reconstruction paradigm of the Codec model and the structure of residual vector quantization, the initial channel of the codebooks contains excessive information, making it challenging to directly generate acoustic tokens from weakly supervised signals such as text in downstream tasks. 2) Achieving good reconstruction performance requires the utilization of numerous codebooks, which increases the burden on downstream speech language models. Consequently, leveraging the characteristics of speech language models, we propose Language-Codec. In the Language-Codec, we introduce a Masked Channel Residual Vector Quantization (MCRVQ) mechanism along with improved fourier transform structures, refined discriminator design to address the aforementioned gaps. We compare our method with competing audio compression algorithms and observe significant outperformance across extensive evaluations. Furthermore, we also validate the efficiency of the Language-Codec on downstream speech language models. The source code and pretrained models will be open-sourced after the paper is accepted. Codes are available at https://github.com/jishengpeng/Languagecodec.</abstract>
      <url hash="a3b2ac7e">2025.acl-long.654</url>
      <bibkey>ji-etal-2025-language</bibkey>
    </paper>
    <paper id="655">
      <title>Adaptive Tool Use in Large Language Models with Meta-Cognition Trigger</title>
      <author><first>Wenjun</first><last>Li</last></author>
      <author><first>Dexun</first><last>Li</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Kuicai</first><last>Dong</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Cong</first><last>Zhang</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Hao</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Weiwen</first><last>Liu</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Yasheng</first><last>Wang</last></author>
      <author><first>Ruiming</first><last>Tang</last></author>
      <author><first>Yong</first><last>Liu</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <pages>13346-13370</pages>
      <abstract>Large language models (LLMs) have shown remarkable emergent capabilities, transforming the execution of functional tasks by leveraging external tools for complex problems that require specialized processing or up-to-date data. While existing research expands LLMs access to diverse tools (e.g., program interpreters, search engines, calculators), the necessity of using these tools is often overlooked, leading to indiscriminate tool invocation. This naive approach raises two key issues: increased latency due to unnecessary tool calls, and potential errors resulting from faulty interactions with external tools. In this paper, we introduce meta-cognition as a proxy for LLMs self-assessment of their capabilities, reflecting the model’s awareness of its own limitations. Based on this, we propose MeCo, an adaptive decision-making strategy for external tool use. MeCo quantifies metacognitive scores by capturing high-level cognitive signals in the representation space, guiding when to invoke tools. Notably, MeCo is fine-tuning-free and incurs minimal cost. Experiments across multiple backbone models and benchmarks show that MeCo reliably detects LLMs’ internal cognitive signals and significantly improves tool-use decision-making.</abstract>
      <url hash="88f2ed85">2025.acl-long.655</url>
      <bibkey>li-etal-2025-adaptive</bibkey>
    </paper>
    <paper id="656">
      <title><fixed-case>MMLU</fixed-case>-<fixed-case>CF</fixed-case>: A Contamination-free Multi-task Language Understanding Benchmark</title>
      <author><first>Qihao</first><last>Zhao</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Yangyu</first><last>Huang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Tengchao</first><last>Lv</last><affiliation>Microsoft</affiliation></author>
      <author><first>Lei</first><last>Cui</last><affiliation>Microsoft Research Asia</affiliation></author>
      <author><first>Qinzheng</first><last>Sun</last><affiliation>Microsoft</affiliation></author>
      <author><first>Shaoguang</first><last>Mao</last><affiliation>Microsoft</affiliation></author>
      <author><first>Xin</first><last>Zhang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Ying</first><last>Xin</last><affiliation>Microsoft</affiliation></author>
      <author><first>Qiufeng</first><last>Yin</last><affiliation>Microsoft</affiliation></author>
      <author><first>Scarlett</first><last>Li</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Furu</first><last>Wei</last><affiliation>Microsoft Research</affiliation></author>
      <pages>13371-13391</pages>
      <abstract>Multiple-choice question (MCQ) datasets like Massive Multitask Language Understanding (MMLU) are widely used to evaluate the commonsense, understanding, and problem-solving abilities of large language models (LLMs). However, the open-source nature of these benchmarks and the broad sources of training data for LLMs have inevitably led to benchmark contamination, resulting in unreliable evaluation. To alleviate this issue, we propose the contamination-free MCQ benchmark called MMLU-CF, which reassesses LLMs’ understanding of world knowledge by averting both unintentional and malicious data contamination. To mitigate unintentional data contamination, we source questions from a broader domain of over 200 billion webpages and apply three specifically designed decontamination rules. To prevent malicious data contamination, we divide the benchmark into validation and test sets with similar difficulty and subject distributions. The test set remains closed-source to ensure reliable results, while the validation set is publicly available to promote transparency and facilitate independent evaluation. The performance gap between these two sets of LLMs will indicate the contamination degree on the validation set in the future. We evaluated over 40 mainstream LLMs on the MMLU-CF. Compared to the original MMLU, not only LLMs’ performances significantly dropped but also the performance rankings of them changed considerably. This indicates the effectiveness of our approach in establishing a contamination-free and fairer evaluation standard.</abstract>
      <url hash="f5e0b8fb">2025.acl-long.656</url>
      <bibkey>zhao-etal-2025-mmlu</bibkey>
    </paper>
    <paper id="657">
      <title>Code-Switching Red-Teaming: <fixed-case>LLM</fixed-case> Evaluation for Safety and Multilingual Understanding</title>
      <author><first>Haneul</first><last>Yoo</last><affiliation>KAIST</affiliation></author>
      <author><first>Yongjin</first><last>Yang</last><affiliation>University of Toronto</affiliation></author>
      <author><first>Hwaran</first><last>Lee</last><affiliation>Sogang University</affiliation></author>
      <pages>13392-13413</pages>
      <abstract>As large language models (LLMs) have advanced rapidly, concerns regarding their safety have become prominent. In this paper, we discover that code-switching in red-teaming queries can effectively elicit undesirable behaviors of LLMs, which are common practices in natural language. We introduce a simple yet effective framework, CSRT, to synthesize code-switching red-teaming queries and investigate the safety and multilingual understanding of LLMs comprehensively. Through extensive experiments with ten state-of-the-art LLMs and code-switching queries combining up to 10 languages, we demonstrate that the CSRT significantly outperforms existing multilingual red-teaming techniques, achieving 46.7% more attacks than standard attacks in English and being effective in conventional safety domains. We also examine the multilingual ability of those LLMs to generate and understand code-switching texts. Additionally, we validate the extensibility of the CSRT by generating code-switching attack prompts with monolingual data. We finally conduct detailed ablation studies exploring code-switching and propound unintended correlation between resource availability of languages and safety alignment in existing multilingual LLMs.</abstract>
      <url hash="078484d1">2025.acl-long.657</url>
      <bibkey>yoo-etal-2025-code</bibkey>
    </paper>
    <paper id="658">
      <title>Unleashing <fixed-case>LLM</fixed-case> Reasoning Capability via Scalable Question Synthesis from Scratch</title>
      <author><first>Yuyang</first><last>Ding</last></author>
      <author><first>Xinyu</first><last>Shi</last></author>
      <author><first>Xiaobo</first><last>Liang</last></author>
      <author><first>Juntao</first><last>Li</last></author>
      <author><first>Zhaopeng</first><last>Tu</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Qiaoming</first><last>Zhu</last><affiliation>Soochow University</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>13414-13438</pages>
      <abstract>Improving the mathematical reasoning capabilities of Large Language Models (LLMs) is critical for advancing artificial intelligence. However, access to extensive, diverse, and high-quality reasoning datasets remains a significant challenge, particularly for the open-source community. In this paper, we propose ScaleQuest, a novel, scalable, and cost-effective data synthesis method that enables the generation of large-scale mathematical reasoning datasets using lightweight 7B-scale models. ScaleQuest introduces a two-stage question-tuning process comprising Question Fine-Tuning (QFT) and Question Preference Optimization (QPO) to unlock the question generation capabilities of problem-solving models. By generating diverse questions from scratch – without relying on powerful proprietary models or seed data – we produce a dataset of 1 million problem-solution pairs. Our experiments demonstrate that models trained on our data outperform existing open-source datasets in both in-domain and out-of-domain evaluations. Furthermore, our approach shows continued performance improvement as the volume of training data increases, highlighting its potential for ongoing data scaling. The extensive improvements observed in code reasoning tasks demonstrate the generalization capabilities of our proposed method. Our work provides the open-source community with a practical solution to enhance the mathematical reasoning abilities of LLMs.</abstract>
      <url hash="f4bdb5e0">2025.acl-long.658</url>
      <bibkey>ding-etal-2025-unleashing</bibkey>
    </paper>
    <paper id="659">
      <title><fixed-case>DRE</fixed-case>s<fixed-case>S</fixed-case>: Dataset for Rubric-based Essay Scoring on <fixed-case>EFL</fixed-case> Writing</title>
      <author><first>Haneul</first><last>Yoo</last><affiliation>KAIST</affiliation></author>
      <author><first>Jieun</first><last>Han</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>So-Yeon</first><last>Ahn</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Alice</first><last>Oh</last><affiliation>Google and Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>13439-13454</pages>
      <abstract>Automated essay scoring (AES) is a useful tool in English as a Foreign Language (EFL) writing education, offering real-time essay scores for students and instructors. However, previous AES models were trained on essays and scores irrelevant to the practical scenarios of EFL writing education and usually provided a single holistic score due to the lack of appropriate datasets. In this paper, we release DREsS, a large-scale, standard dataset for rubric-based automated essay scoring with 48.9K samples in total. DREsS comprises three sub-datasets: DREsS_New, DREsS_Std., and DREsS_CASE. We collect DREsS_New, a real-classroom dataset with 2.3K essays authored by EFL undergraduate students and scored by English education experts. We also standardize existing rubric-based essay scoring datasets as DREsS_Std. We suggest CASE, a corruption-based augmentation strategy for essays, which generates 40.1K synthetic samples of DREsS_CASE and improves the baseline results by 45.44%. DREsS will enable further research to provide a more accurate and practical AES system for EFL writing education.</abstract>
      <url hash="dbfb3843">2025.acl-long.659</url>
      <bibkey>yoo-etal-2025-dress</bibkey>
    </paper>
    <paper id="660">
      <title><fixed-case>PQR</fixed-case>: Improving Dense Retrieval via Potential Query Modeling</title>
      <author><first>Junfeng</first><last>Kang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Rui</first><last>Li</last></author>
      <author><first>Qi</first><last>Liu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Yanjiang</first><last>Chen</last></author>
      <author><first>Zheng</first><last>Zhang</last></author>
      <author><first>Junzhe</first><last>Jiang</last></author>
      <author><first>Heng</first><last>Yu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Yu</first><last>Su</last></author>
      <pages>13455-13469</pages>
      <abstract>Dense retrieval has now become the mainstream paradigm in information retrieval. The core idea of dense retrieval is to align document embeddings with their corresponding query embeddings by maximizing their dot product. The current training data is quite sparse, with each document typically associated with only one or a few labeled queries. However, a single document can be retrieved by multiple different queries. Aligning a document with just one or a limited number of labeled queries results in a loss of its semantic information. In this paper, we propose a training-free Potential Query Retrieval (PQR) framework to address this issue. Specifically, we use a Gaussian mixture distribution to model all potential queries for a document, aiming to capture its comprehensive semantic information. To obtain this distribution, we introduce three sampling strategies to sample a large number of potential queries for each document and encode them into a semantic space. Using these sampled queries, we employ the Expectation-Maximization algorithm to estimate parameters of the distribution. Finally, we also propose a method to calculate similarity scores between user queries and documents under the PQR framework. Extensive experiments demonstrate the effectiveness of the proposed method.</abstract>
      <url hash="dddce9a1">2025.acl-long.660</url>
      <bibkey>kang-etal-2025-pqr</bibkey>
    </paper>
    <paper id="661">
      <title>Cross-Lingual Generalization and Compression: From Language-Specific to Shared Neurons</title>
      <author><first>Frederick</first><last>Riemenschneider</last><affiliation>Ruprecht-Karls-Universität Heidelberg</affiliation></author>
      <author><first>Anette</first><last>Frank</last><affiliation>Ruprecht-Karls-Universität Heidelberg</affiliation></author>
      <pages>13470-13491</pages>
      <abstract>Multilingual language models (MLLMs) have demonstrated remarkable abilities to transfer knowledge across languages, despite being trained without explicit cross-lingual supervision. We analyze the parameter spaces of three MLLMs to study how their representations evolve during pre-training, observing patterns consistent with compression: models initially form language-specific representations, which gradually converge into cross-lingual abstractions as training progresses. Through probing experiments, we observe a clear transition from uniform language identification capabilities across layers to more specialized layer functions. For deeper analysis, we focus on neurons that encode distinct semantic concepts. By tracing their development during pre-training, we show how they gradually align across languages. Notably, we identify specific neurons that emerge as increasingly reliable predictors for the same concepts across languages. This alignment manifests concretely in generation: once an MLLM exhibits cross-lingual generalization according to our measures, we can select concept-specific neurons identified from, e.g., Spanish text and manipulate them to guide token predictions. Remarkably, rather than generating Spanish text, the model produces semantically coherent English text. This demonstrates that cross-lingually aligned neurons encode generalized semantic representations, independent of the original language encoding.</abstract>
      <url hash="5f5845b1">2025.acl-long.661</url>
      <bibkey>riemenschneider-frank-2025-cross</bibkey>
    </paper>
    <paper id="662">
      <title><fixed-case>SDB</fixed-case>ench: A Survey-based Domain-specific <fixed-case>LLM</fixed-case> Benchmarking and Optimization Framework</title>
      <author><first>Cheng</first><last>Guo</last></author>
      <author><first>Hu</first><last>Kai</last></author>
      <author><first>Shuxian</first><last>Liang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Yiyang</first><last>Jiang</last></author>
      <author><first>Yi</first><last>Gao</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Xian-Sheng</first><last>Hua</last><affiliation>Tongji University</affiliation></author>
      <author><first>Wei</first><last>Dong</last><affiliation>Zhejiang University</affiliation></author>
      <pages>13492-13506</pages>
      <abstract>The rapid advancement of large language models (LLMs) in recent years has made it feasible to establish domain-specific LLMs for specialized fields. However, in practical development, acquiring domain-specific knowledge often requires a significant amount of professional expert manpower. Moreover, even when domain-specific data is available, the lack of a unified methodology for benchmark dataset establishment often results in uneven data distribution. This imbalance can lead to an inaccurate assessment of the true model capabilities during the evaluation of domain-specific LLMs. To address these challenges, we introduce **SDBench**, a generic framework for generating evaluation datasets for domain-specific LLMs. This method is also applicable for establishing the LLM instruction datasets. It significantly reduces the reliance on expert manpower while ensuring that the collected data is uniformly distributed. To validate the effectiveness of this framework, we also present the **BridgeBench**, a novel benchmark for bridge engineering knowledge, and the **BridgeGPT**, the first LLM specialized in bridge engineering, which can solve bridge engineering tasks.</abstract>
      <url hash="dad2da81">2025.acl-long.662</url>
      <bibkey>guo-etal-2025-sdbench</bibkey>
    </paper>
    <paper id="663">
      <title><fixed-case>R</fixed-case>eflec<fixed-case>T</fixed-case>ool: Towards Reflection-Aware Tool-Augmented Clinical Agents</title>
      <author><first>Yusheng</first><last>Liao</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Shuyang</first><last>Jiang</last></author>
      <author><first>Yanfeng</first><last>Wang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Yu</first><last>Wang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>13507-13531</pages>
      <abstract>Large Language Models (LLMs) have shown promising potential in the medical domain, assisting with tasks like clinical note generation and patient communication. However, current LLMs are limited to text-based communication, hindering their ability to interact with diverse forms of information in clinical environments. Despite clinical agents succeeding in diverse signal interaction, they are oriented to a single clinical scenario and hence fail for broader applications. To evaluate clinical agents holistically, we propose ClinicalAgent Bench (CAB), a comprehensive medical agent benchmark consisting of 18 tasks across five key realistic clinical dimensions. Building on this, we introduce ReflectTool, a novel framework that excels at utilizing domain-specific tools within two stages. The first optimization stage progressively enlarges a long-term memory by saving successful solving processes and tool-wise experience of agents in a tiny pre-defined training set. In the following inference stage, ReflectTool can search for supportive successful demonstrations from already built long-term memory to guide the tool selection strategy, and a verifier improves the tool usage according to the tool-wise experience with two verification methods–iterative refinement and candidate selection. Extensive experiments on CAB demonstrate that ReflectTool surpasses the pure LLMs with more than 10 points and the well-established agent-based methods with 3 points, highlighting its adaptability and effectiveness in solving complex clinical tasks. Our code and datasets are available at https://github.com/BlueZeros/ReflecTool.</abstract>
      <url hash="1463fd7a">2025.acl-long.663</url>
      <bibkey>liao-etal-2025-reflectool</bibkey>
    </paper>
    <paper id="664">
      <title>Lexical Recall or Logical Reasoning: Probing the Limits of Reasoning Abilities in Large Language Models</title>
      <author><first>Henrike</first><last>Beyer</last><affiliation>University of Dundee</affiliation></author>
      <author><first>Chris</first><last>Reed</last><affiliation>University of Dundee</affiliation></author>
      <pages>13532-13557</pages>
      <abstract>Despite the increasing interest in the reasoning abilities of Large Language Models (LLMs), existing work shows limitations in assessing logic abilities independently from lexical memory. We address this gap with Mystery-Zebra. This robust two-part benchmark (4,290 puzzles) challenges the logic abstraction abilities of LLMs in two setups: (1) a lexical obfuscation setup tests the dependence of LLMs on lexical content based on two canonical grid puzzles widely spread on the Internet; (2) a set of new grid puzzles in 42 different sizes and 12 difficulty levels tests how the formal difficulty degree of a puzzle affects LLMs.We test open and closed-weight LLMs on both parts of the benchmark. The results on part two suggest that model sizes up to 70B parameters have only a minor influence when solving newly generated puzzles, while performance mainly relates to the number of items in the puzzle. The results on the first part of the benchmark suggest that the applied obfuscation strategies help to mitigate effects of logic puzzles being part of LLM training data, showing a drastic drop in performance for obfuscated versions of well-known puzzles. In addition we conduct a case-study on the first part of the benchmark predicting the position of single items, unveiling that the reasoning abilities of LLMs are mainly limited to a few consecutive steps of reasoning.</abstract>
      <url hash="de639c09">2025.acl-long.664</url>
      <bibkey>beyer-reed-2025-lexical</bibkey>
    </paper>
    <paper id="665">
      <title><fixed-case>C</fixed-case>hain<fixed-case>E</fixed-case>dit: Propagating Ripple Effects in <fixed-case>LLM</fixed-case> Knowledge Editing through Logical Rule-Guided Chains</title>
      <author><first>Zilu</first><last>Dong</last><affiliation>Nanjing University of Science and Technology</affiliation></author>
      <author><first>Xiangqing</first><last>Shen</last></author>
      <author><first>Zinong</first><last>Yang</last></author>
      <author><first>Rui</first><last>Xia</last><affiliation>Nanjing University of Science and Technology</affiliation></author>
      <pages>13558-13571</pages>
      <abstract>Current knowledge editing methods for large language models (LLMs) struggle to maintain logical consistency when propagating ripple effects to associated facts. We propose ChainEdit, a framework that synergizes knowledge graph-derived logical rules with LLM logical reasoning capabilities to enable systematic chain updates. By automatically extracting logical patterns from structured knowledge bases and aligning them with LLMs’ internal logics, ChainEdit dynamically generates and edits logically connected knowledge clusters. Experiments demonstrate an improvement of more than 30% in logical generalization over baselines while preserving editing reliability and specificity. We further address evaluation biases in existing benchmarks through knowledge-aware protocols that disentangle external dependencies. This work establishes new state-of-the-art performance on ripple effect while ensuring internal logical consistency after knowledge editing.</abstract>
      <url hash="46ad4cc5">2025.acl-long.665</url>
      <bibkey>dong-etal-2025-chainedit</bibkey>
    </paper>
    <paper id="666">
      <title><fixed-case>H</fixed-case>i<fixed-case>D</fixed-case>e-<fixed-case>LL</fixed-case>a<fixed-case>VA</fixed-case>: Hierarchical Decoupling for Continual Instruction Tuning of Multimodal Large Language Model</title>
      <author><first>Haiyang</first><last>Guo</last></author>
      <author><first>Fanhu</first><last>Zeng</last></author>
      <author><first>Ziwei</first><last>Xiang</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Fei</first><last>Zhu</last><affiliation>Centre for Artificial Intelligence and Robotics Hong Kong Institute of Science &amp; Innovation, Chinese Academy of Sciences</affiliation></author>
      <author><first>Da-Han</first><last>Wang</last><affiliation>Xiamen University of Technology</affiliation></author>
      <author><first>Xu-Yao</first><last>Zhang</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Cheng-Lin</first><last>Liu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <pages>13572-13586</pages>
      <abstract>Instruction tuning is widely used to enhance a pre-trained Multimodal Large Language Model (MLLM) to understand and follow human instructions by training it on a curated set of task-specific dataset. However, it is infeasible to collect all possible instruction datasets simultaneously in real-world scenarios. Thus, enabling MLLM with continual instruction tuning is essential for maintaining their adaptability. However, existing methods often trade off memory efficiency for performance gains, significantly compromising overall efficiency. In this paper, we propose a task-specific expansion and task-general fusion framework based on the variations in Centered Kernel Alignment (CKA) similarity across different model layers when trained on diverse datasets. Furthermore, we analyze the information leakage present in the existing benchmark and propose a new and more challenging benchmark to rationally evaluate the performance of different methods. Comprehensive experiments showcase a significant performance improvement of our method compared to existing state-of-the-art methods. Our code will be public available.</abstract>
      <url hash="2eb1ec65">2025.acl-long.666</url>
      <bibkey>guo-etal-2025-hide</bibkey>
    </paper>
    <paper id="667">
      <title>Self-supervised Quantized Representation for Seamlessly Integrating Knowledge Graphs with Large Language Models</title>
      <author><first>Qika</first><last>Lin</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Tianzhe</first><last>Zhao</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Kai</first><last>He</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Zhen</first><last>Peng</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Fangzhi</first><last>Xu</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Ling</first><last>Huang</last></author>
      <author><first>Jingying</first><last>Ma</last></author>
      <author><first>Mengling</first><last>Feng</last><affiliation>National University of Singapore</affiliation></author>
      <pages>13587-13602</pages>
      <abstract>Due to the presence of the natural gap between Knowledge Graph (KG) structures and the natural language, the effective integration of holistic structural information of KGs with Large Language Models (LLMs) has emerged as a significant question. To this end, we propose a two-stage framework to learn and apply quantized codes for each entity, aiming for the seamless integration of KGs with LLMs. Firstly, a self-supervised quantized representation (SSQR) method is proposed to compress both KG structural and semantic knowledge into discrete codes (i.e., tokens) that align the format of language sentences. We further design KG instruction-following data by viewing these learned codes as features to directly input to LLMs, thereby achieving seamless integration. The experiment results demonstrate that SSQR outperforms existing unsupervised quantized methods, producing more distinguishable codes. Moreover, the fine-tuned LLaMA2 and LLaMA3.1 also have superior performance on KG link prediction and triple classification tasks, utilizing only 16 tokens per entity instead of thousands in conventional prompting methods.</abstract>
      <url hash="0aa126b6">2025.acl-long.667</url>
      <bibkey>lin-etal-2025-self-supervised</bibkey>
    </paper>
    <paper id="668">
      <title>Finite State Automata Inside Transformers with Chain-of-Thought: A Mechanistic Study on State Tracking</title>
      <author><first>Yifan</first><last>Zhang</last></author>
      <author><first>Wenyu</first><last>Du</last></author>
      <author><first>Dongming</first><last>Jin</last></author>
      <author><first>Jie</first><last>Fu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Zhi</first><last>Jin</last><affiliation>Peking University</affiliation></author>
      <pages>13603-13621</pages>
      <abstract>Chain-of-thought (CoT) significantly enhances the performance of large language models (LLMs) across a wide range of tasks, and prior research shows that CoT can theoretically increase expressiveness. However, there is limited mechanistic understanding of the algorithms that Transformer+CoT can learn. Our key contributions are: (1) We evaluate the state tracking capabilities of Transformer+CoT and its variants, confirming the effectiveness of CoT. (2) Next, we identify the circuit (a subset of model components, responsible for tracking the world state), indicating that late-layer MLP neurons play a key role. We propose two metrics, compression and distinction, and show that the neuron sets for each state achieve nearly 100% accuracy, providing evidence of an implicit finite state automaton (FSA) embedded within the model. (3) Additionally, we explore three challenging settings: skipping intermediate steps, introducing data noises, and testing length generalization. Our results demonstrate that Transformer+CoT learns robust algorithms (FSAs), highlighting its resilience in challenging scenarios. Our code is available at https://github.com/IvanChangPKU/FSA.</abstract>
      <url hash="c9102e36">2025.acl-long.668</url>
      <bibkey>zhang-etal-2025-finite</bibkey>
    </paper>
    <paper id="669">
      <title><fixed-case>T</fixed-case>eam<fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case>: Boosting Low-Rank Adaptation with Expert Collaboration and Competition</title>
      <author><first>Tianwei</first><last>Lin</last></author>
      <author><first>Jiang</first><last>Liu</last></author>
      <author><first>Wenqiao</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Yang</first><last>Dai</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Haoyuan</first><last>Li</last></author>
      <author><first>Zhelun</first><last>Yu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Wanggui</first><last>He</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Juncheng</first><last>Li</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Jiannan</first><last>Guo</last></author>
      <author><first>Hao</first><last>Jiang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Siliang</first><last>Tang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Yueting</first><last>Zhuang</last><affiliation>Zhejiang University</affiliation></author>
      <pages>13622-13637</pages>
      <abstract>While Parameter-Efficient Fine-Tuning (PEFT) methods like Low-Rank Adaptation (LoRA) effectively address resource constraints during fine-tuning, their performance often falls short, especially in multidimensional task scenarios. To address this issue, one straightforward solution is to introduce task-specific LoRA as domain experts, leveraging the modeling of multiple capabilities of experts and thus enhancing the general capability of multi-task learning.Although promising, these additional components often add complexity to the training and inference process, contravening the efficiency that PEFT is designed to deliver. Considering this, we introduce an innovative PEFT method, **TeamLoRA**, consisting of a collaboration and competition module for LoRA experts, thus achieving the right balance of effectiveness and efficiency:**(i)** For *collaboration*, we introduce a novel knowledge sharing and organization mechanism designed to optimize hierarchical learning while enhancing the efficiency of model training and inference.**(ii)** For *competition*, we propose leveraging a game-theoretic interaction mechanism for experts, encouraging experts to transfer their domain-specific knowledge while facing diverse downstream tasks, thus enhancing the performance.By doing so, TeamLoRA elegantly connects the experts as a “*Team*” with internal collaboration and competition, enabling a faster and more accurate PEFT paradigm. Meanwhile, we curate a **Comprehensive Multi-Task Evaluation (CME)** benchmark to thoroughly assess the capability of multi-task learning. Experiments conducted on our CME and other benchmarks indicate the effectiveness and efficiency of TeamLoRA. Our project is available at https://github.com/DCDmllm/TeamLoRA.</abstract>
      <url hash="15ef71fc">2025.acl-long.669</url>
      <bibkey>lin-etal-2025-teamlora</bibkey>
    </paper>
    <paper id="670">
      <title><fixed-case>CR</fixed-case>isk<fixed-case>E</fixed-case>val: A <fixed-case>C</fixed-case>hinese Multi-Level Risk Evaluation Benchmark Dataset for Large Language Models</title>
      <author><first>Ling</first><last>Shi</last></author>
      <author><first>Deyi</first><last>Xiong</last><affiliation>Tianjin University</affiliation></author>
      <pages>13638-13659</pages>
      <abstract>Large language models (LLMs) are possessed of numerous beneficial capabilities, yet their potential inclination harbors unpredictable risks that may materialize in the future. We hence propose CRiskEval, a Chinese dataset meticulously designed for gauging the risk proclivities inherent in LLMs such as resource acquisition and malicious coordination, as part of efforts for proactive preparedness. To curate CRiskEval, we define a new risk taxonomy with 7 types of frontier risks and 4 safety levels, including extremely hazardous,moderately hazardous, neutral and safe. We follow the philosophy of tendency evaluation to empirically measure the stated ”desire” of LLMs via fine-grained multiple-choice question answering. The dataset consists of 14,888 questions that simulate scenarios related to predefined 7 types of frontier risks. Each question is accompanied with 4 answer choices that state opinions or behavioral tendencies corresponding to the question. All answer choices are manually annotated with one of the defined risk levels so that we can easily build a fine-grained frontier risk profile for each assessed LLM. Extensive evaluation with CRiskEval on a spectrum of prevalent Chinese LLMs has unveiled a striking revelation: most models exhibit risk tendencies of more than 40% (weighted tendency to the four risk levels). Furthermore, a subtle increase in the model’s inclination toward urgent self-sustainability, power seeking and other dangerous goals becomes evident as the size of models increases. To promote further research on the frontier risk evaluation of LLMs, we publicly release our dataset at https://github.com/tjunlp-lab/CRiskEval.</abstract>
      <url hash="2840a739">2025.acl-long.670</url>
      <bibkey>shi-xiong-2025-criskeval</bibkey>
    </paper>
    <paper id="671">
      <title><fixed-case>STUN</fixed-case>: Structured-Then-Unstructured Pruning for Scalable <fixed-case>M</fixed-case>o<fixed-case>E</fixed-case> Pruning</title>
      <author><first>Jaeseong</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Seung-won</first><last>Hwang</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Aurick</first><last>Qiao</last><affiliation>Snowflake</affiliation></author>
      <author><first>Daniel F</first><last>Campos</last><affiliation>Snowflake</affiliation></author>
      <author><first>Zhewei</first><last>Yao</last><affiliation>Snowflake</affiliation></author>
      <author><first>Yuxiong</first><last>He</last><affiliation>Microsoft</affiliation></author>
      <pages>13660-13676</pages>
      <abstract>Mixture-of-experts (MoEs) have been adopted for reducing inference costs by sparsely activating experts in large language models (LLMs). Despite these reductions, the massive number of parameters in MoEs still makes them expensive to serve. Conventionally, unstructured or structured pruning has been considered to reduce number of parameters. Our key contribution is exploring the interpolation between structured and unstructured pruning, to propose a novel structured-then-unstructured (STUN) approach outperforming both of structured or unstructured pruning, especially for MoEs. In the first stage, we show a scalable expert pruning with O(1) forward pass, unlike existing work requiring <tex-math>O(\frac{k^n}{\sqrt{n}})</tex-math> forward passes for <tex-math>n</tex-math> experts that cannot scale for recent MoEs with hundreds of experts. We then show our expert-pruned MoEs are robust to unstructured pruning to follow. Experiments on Snowflake Arctic and Mixtral shows that our proposal is highly effective– For Snowflake Arctic, a 480B-sized MoE with 128 experts, our method needs only one H100 and two hours to achieve nearly no loss in performance with 40% sparsity, even in generative tasks such as GSM8K, where state-of-the-art structured or unstructured pruning methods fail. The code is publicly available.</abstract>
      <url hash="1b41d53d">2025.acl-long.671</url>
      <bibkey>lee-etal-2025-stun</bibkey>
    </paper>
    <paper id="672">
      <title>Mimicking the Familiar: Dynamic Command Generation for Information Theft Attacks in <fixed-case>LLM</fixed-case> Tool-Learning System</title>
      <author><first>Ziyou</first><last>Jiang</last></author>
      <author><first>Mingyang</first><last>Li</last><affiliation>Institute of Software Chinese Academy of Sciences</affiliation></author>
      <author><first>Guowei</first><last>Yang</last><affiliation>University of Queensland</affiliation></author>
      <author><first>Junjie</first><last>Wang</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yuekai</first><last>Huang</last></author>
      <author><first>Zhiyuan</first><last>Chang</last></author>
      <author><first>Qing</first><last>Wang</last></author>
      <pages>13677-13693</pages>
      <abstract>Information theft attacks pose a significant risk to Large Language Model (LLM) tool-learning systems. Adversaries can inject malicious commands through compromised tools, manipulating LLMs to send sensitive information to these tools, which leads to potential privacy breaches. However, existing attack approaches are black-box oriented and rely on static commands that cannot adapt flexibly to the changes in user queries and the invocation chain of tools. It makes malicious commands more likely to be detected by LLM and leads to attack failure. In this paper, we propose AutoCMD, a dynamic attack comment generation approach for information theft attacks in LLM tool-learning systems. Inspired by the concept of mimicking the familiar, AutoCMD is capable of inferring the information utilized by upstream tools in the toolchain through learning on open-source systems and reinforcement with target system examples, thereby generating more targeted commands for information theft. The evaluation results show that AutoCMD outperforms the baselines with +13.2% <tex-math>ASR_{Theft}</tex-math>, and can be generalized to new tool-learning systems to expose their information leakage risks. We also design four defense methods to effectively protect tool-learning systems from the attack.</abstract>
      <url hash="ca231139">2025.acl-long.672</url>
      <bibkey>jiang-etal-2025-mimicking</bibkey>
    </paper>
    <paper id="673">
      <title><fixed-case>F</fixed-case>lash<fixed-case>A</fixed-case>udio: Rectified Flow for Fast and High-Fidelity Text-to-Audio Generation</title>
      <author><first>Huadai</first><last>Liu</last></author>
      <author><first>Jialei</first><last>Wang</last></author>
      <author><first>Rongjie</first><last>Huang</last><affiliation>Zhejiang University</affiliation></author>
      <author id="yang-liu"><first>Yang</first><last>Liu</last></author>
      <author><first>Heng</first><last>Lu</last></author>
      <author><first>Zhou</first><last>Zhao</last><affiliation>Zhejiang University and Zhejiang University</affiliation></author>
      <author><first>Wei</first><last>Xue</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <pages>13694-13710</pages>
      <abstract>Recent advancements in latent diffusion models (LDMs) have markedly enhanced text-to-audio generation, yet their iterative sampling processes impose substantial computational demands, limiting practical deployment. While recent methods utilizing consistency-based distillation aim to achieve few-step or single-step inference, their one-step performance is constrained by curved trajectories, preventing them from surpassing traditional diffusion models. In this work, we introduce FlashAudio with rectified flows to learn straight flow for fast simulation. To alleviate the inefficient timesteps allocation and suboptimal distribution of noise, FlashAudio optimizes the time distribution of rectified flow with Bifocal Samplers and proposes immiscible flow to minimize the total distance of data-noise pairs in a batch vias assignment. Furthermore, to address the amplified accumulation error caused by the classifier-free guidance (CFG), we propose Anchored Optimization, which refines the guidance scale by anchoring it to a reference trajectory. Experimental results on text-to-audio generation demonstrate that FlashAudio’s one-step generation performance surpasses the diffusion-based models with hundreds of sampling steps on audio quality and enables a sampling speed of 400x faster than real-time on a single NVIDIA 4090Ti GPU. Code will be available at <url>https://github.com/liuhuadai/FlashAudio</url>. Audio Samples are available at https://FlashAudio-TTA.github.io/.</abstract>
      <url hash="11809d56">2025.acl-long.673</url>
      <bibkey>liu-etal-2025-flashaudio</bibkey>
    </paper>
    <paper id="674">
      <title>How does Misinformation Affect Large Language Model Behaviors and Preferences?</title>
      <author><first>Miao</first><last>Peng</last></author>
      <author><first>Nuo</first><last>Chen</last></author>
      <author><first>Jianheng</first><last>Tang</last></author>
      <author><first>Jia</first><last>Li</last><affiliation>Hong Kong University of Science and Technology (Guangzhou)</affiliation></author>
      <pages>13711-13748</pages>
      <abstract>Large Language Models (LLMs) have shown remarkable capabilities in knowledge-intensive tasks, while they remain vulnerable when encountering misinformation. Existing studies have explored the role of LLMs in combating misinformation, but there is still a lack of fine-grained analysis on the specific aspects and extent to which LLMs are influenced by misinformation. To bridge this gap, we present MisBench, the current largest and most comprehensive benchmark for evaluating LLMs’ behavior and knowledge preference toward misinformation. MisBench consists of 10,346,712 pieces of misinformation, which uniquely considers both knowledge-based conflicts and stylistic variations in misinformation. Empirical results reveal that while LLMs demonstrate comparable abilities in discerning misinformation, they still remain susceptible to knowledge conflicts and stylistic variations. Based on these findings, we further propose a novel approach called Reconstruct to Discriminate (RtD) to strengthen LLMs’ ability to detect misinformation. Our study provides valuable insights into LLMs’ interactions with misinformation, and we believe MisBench can serve as an effective benchmark for evaluating LLM-based detectors and enhancing their reliability in real-world applications. Codes and data are available at: https://github.com/GKNL/MisBench.</abstract>
      <url hash="59252860">2025.acl-long.674</url>
      <bibkey>peng-etal-2025-misinformation</bibkey>
    </paper>
    <paper id="675">
      <title><fixed-case>YES</fixed-case>ci<fixed-case>E</fixed-case>val: Robust <fixed-case>LLM</fixed-case>-as-a-Judge for Scientific Question Answering</title>
      <author><first>Jennifer</first><last>D’Souza</last><affiliation>TIB Hannover</affiliation></author>
      <author><first>Hamed</first><last>Babaei Giglou</last></author>
      <author><first>Quentin</first><last>Münch</last></author>
      <pages>13749-13783</pages>
      <abstract>Large Language Models (LLMs) drive scientific question-answering on modern search engines, yet their evaluation robustness remains underexplored. We introduce YESciEval, an open-source framework that combines fine-grained rubric-based assessment with reinforcement learning to mitigate optimism bias in LLM evaluators. We release multidisciplinary scienceQ&amp;A datasets, including adversarial variants, with evaluation scores from multiple LLMs. Independent of proprietary models and human feedback, our approach enables scalable, cost-free evaluation. By advancing reliable LLM-as-a-judge models, this work supports AI alignment and fosters robust, transparent evaluation essential for scientific inquiry.</abstract>
      <url hash="039d4e1b">2025.acl-long.675</url>
      <bibkey>dsouza-etal-2025-yescieval</bibkey>
    </paper>
    <paper id="676">
      <title><fixed-case>GALL</fixed-case>a: Graph Aligned Large Language Models for Improved Source Code Understanding</title>
      <author><first>Ziyin</first><last>Zhang</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Hang</first><last>Yu</last><affiliation>Ant Group</affiliation></author>
      <author><first>Sage</first><last>Lee</last></author>
      <author><first>Peng</first><last>Di</last><affiliation>Ant Group and University of New South Wales</affiliation></author>
      <author><first>Jianguo</first><last>Li</last><affiliation>Ant Group</affiliation></author>
      <author><first>Rui</first><last>Wang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>13784-13802</pages>
      <abstract>Programming languages possess rich semantic information - such as data flow - that is represented by graphs and not available from the surface form of source code. Recent code language models have scaled to billions of parameters, but model source code solely as text tokens while ignoring any other structural information. Conversely, models that do encode structural information of code make modifications to the Transformer architecture, limiting their scale and compatibility with pretrained LLMs. In this work, we take the best of both worlds with GALLa - Graph Aligned Large Language Models. GALLa utilizes graph neural networks and cross-modal alignment technologies to inject the structural information of code into LLMs as an auxiliary task during finetuning. This framework is both model-agnostic and task-agnostic, as it can be applied to any code LLM for any code downstream task, and requires the structural graph data only at training time from a corpus unrelated to the finetuning data, while incurring no cost at inference time over the baseline LLM. Experiments on five code tasks with six different baseline LLMs ranging in size from 350M to 14B validate the effectiveness of GALLa, demonstrating consistent improvement over the baseline, even for powerful models such as LLaMA3 and Qwen2.5-Coder.</abstract>
      <url hash="bebdf228">2025.acl-long.676</url>
      <bibkey>zhang-etal-2025-galla</bibkey>
    </paper>
    <paper id="677">
      <title><fixed-case>MEDD</fixed-case>x<fixed-case>A</fixed-case>gent: A Unified Modular Agent Framework for Explainable Automatic Differential Diagnosis</title>
      <author><first>Daniel Philip</first><last>Rose</last></author>
      <author><first>Chia-Chien</first><last>Hung</last><affiliation>NEC Laboratories Europe</affiliation></author>
      <author><first>Marco</first><last>Lepri</last><affiliation>NEC</affiliation></author>
      <author><first>Israa</first><last>Alqassem</last></author>
      <author><first>Kiril</first><last>Gashteovski</last><affiliation>NEC Laboratories Europe, St.Cyril and Methodius University and NEC Laboratories Europe</affiliation></author>
      <author><first>Carolin</first><last>Lawrence</last><affiliation>NEC Laboratories Europe and NEC Laboratories Europe</affiliation></author>
      <pages>13803-13826</pages>
      <abstract>Differential Diagnosis (DDx) is a fundamental yet complex aspect of clinical decision-making, in which physicians iteratively refine a ranked list of possible diseases based on symptoms, antecedents, and medical knowledge. While recent advances in large language models (LLMs) have shown promise in supporting DDx, existing approaches face key limitations, including single-dataset evaluations, isolated optimization of components, unrealistic assumptions about complete patient profiles, and single-attempt diagnosis. We introduce a Modular Explainable DDx Agent (MEDDxAgent) framework designed for interactive DDx, where diagnostic reasoning evolves through iterative learning, rather than assuming a complete patient profile is accessible. MEDDxAgent integrates three modular components: (1) an orchestrator (DDxDriver), (2) a history taking simulator, and (3) two specialized agents for knowledge retrieval and diagnosis strategy. To ensure robust evaluation, we introduce a comprehensive DDx benchmark covering respiratory, skin, and rare diseases. We analyze single-turn diagnostic approaches and demonstrate the importance of iterative refinement when patient profiles are not available at the outset. Our broad evaluation demonstrates that MEDDxAgent achieves over 10% accuracy improvements in interactive DDx across both large and small LLMs, while offering critical explainability into its diagnostic reasoning process.</abstract>
      <url hash="a9b168e4">2025.acl-long.677</url>
      <bibkey>rose-etal-2025-meddxagent</bibkey>
    </paper>
    <paper id="678">
      <title>A Training-free <fixed-case>LLM</fixed-case>-based Approach to General <fixed-case>C</fixed-case>hinese Character Error Correction</title>
      <author><first>Houquan</first><last>Zhou</last><affiliation>Soochow University, China</affiliation></author>
      <author><first>Bo</first><last>Zhang</last></author>
      <author><first>Zhenghua</first><last>Li</last><affiliation>Soochow University</affiliation></author>
      <author><first>Ming</first><last>Yan</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>13827-13852</pages>
      <abstract>Chinese spelling correction (CSC) is a crucial task that aims to correct character errors in Chinese text. While conventional CSC focuses on character substitution errors caused by mistyping, two other common types of character errors, missing and redundant characters, have received less attention. These errors are often excluded from CSC datasets during the annotation process or ignored during evaluation, even when they have been annotated. This issue limits the practicality of the CSC task. To address this issue, we introduce the task of General Chinese Character Error Correction (C2EC), which focuses on all three types of character errors. We construct a high-quality C2EC benchmark by combining and manually verifying data from CCTC and Lemon datasets. We extend the training-free prompt-free CSC method to C2EC by using Levenshtein distance for handling length changes and leveraging an additional prompt-based large language model (LLM) to improve performance. Experiments show that our method enables a 14B-parameter LLM to be on par with models nearly 50 times larger on both conventional CSC and C2EC tasks, without any fine-tuning.</abstract>
      <url hash="bd2d3af2">2025.acl-long.678</url>
      <bibkey>zhou-etal-2025-training</bibkey>
    </paper>
    <paper id="679">
      <title><fixed-case>HSCR</fixed-case>: Hierarchical Self-Contrastive Rewarding for Aligning Medical Vision Language Models</title>
      <author><first>Songtao</first><last>Jiang</last></author>
      <author><first>Yan</first><last>Zhang</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Yeying</first><last>Jin</last><affiliation>Tencent</affiliation></author>
      <author><first>Zhihang</first><last>Tang</last></author>
      <author><first>Yangyang</first><last>Wu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Yang</first><last>Feng</last></author>
      <author><first>Jian</first><last>Wu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Zuozhu</first><last>Liu</last><affiliation>Zhejiang University</affiliation></author>
      <pages>13853-13868</pages>
      <abstract>Medical Vision-Language Models (Med-VLMs) have achieved success across various tasks, yet most existing methods overlook the modality misalignment issue that can lead to untrustworthy responses in clinical settings. In this paper, we propose Hierarchical Self-Contrastive Rewarding (HSCR), a novel approach that addresses two critical challenges in Med-VLM alignment: 1) Cost-effective generation of high-quality preference data; 2) Capturing nuanced and context-aware preferences for improved alignment. HSCR first leverages the inherent capability of Med-VLMs to generate dispreferred responses with higher sampling probability. By analyzing output logit shifts after visual token dropout, we identify modality-coupled tokens that induce misalignment and derive an implicit alignment reward function. This function guides token replacement with hallucinated ones during decoding, producing high-quality dispreferred data. Furthermore, HSCR introduces a multi-level preference optimization strategy, which extends beyond traditional adjacent-level optimization by incorporating nuanced implicit preferences, leveraging relative quality in dispreferred data to capture subtle alignment cues for more precise and context-aware optimization. Extensive experiments across multiple medical tasks, including Med-VQA, medical image captioning and instruction following, demonstrate that HSCR not only enhances zero-shot performance but also significantly improves modality alignment and trustworthiness with just 2,000 training entries. Code is released on https://github.com/jiangsongtao/HSCR.</abstract>
      <url hash="a73bf334">2025.acl-long.679</url>
      <bibkey>jiang-etal-2025-hscr</bibkey>
    </paper>
    <paper id="680">
      <title><fixed-case>MA</fixed-case>mmo<fixed-case>TH</fixed-case>-<fixed-case>VL</fixed-case>: Eliciting Multimodal Reasoning with Instruction Tuning at Scale</title>
      <author><first>Jiawei</first><last>Guo</last><affiliation>01.AI</affiliation></author>
      <author><first>Tianyu</first><last>Zheng</last></author>
      <author><first>Yizhi</first><last>Li</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Yuelin</first><last>Bai</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author id="bo-li"><first>Bo</first><last>Li</last></author>
      <author><first>Yubo</first><last>Wang</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>King</first><last>Zhu</last><affiliation>Guangdong OPPO Mobile Telecommunications Corp.,Ltd.</affiliation></author>
      <author><first>Graham</first><last>Neubig</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Wenhu</first><last>Chen</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Xiang</first><last>Yue</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>13869-13920</pages>
      <abstract>Open-source multimodal large language models (MLLMs) have shown significant potential in a broad range of tasks. However, their reasoning capabilities remain constrained by existing instruction-tuning datasets, which were predominately repurposed from academic datasets such as VQA, AI2D, and ChartQA. These datasets target simplistic tasks, and only provide phrase-level answers without any intermediate rationales.To address these challenges, we introduce a scalable and cost-effective method to construct a large-scale multimodal instruction-tuning dataset with rich intermediate rationales designed to elicit CoT reasoning. Using only open models, we create a dataset containing 12M instruction-response pairs to cover diverse reasoning-intensive tasks.Experiments demonstrate that training MLLMs on our dataset not only significantly improves reasoning capabilities, achieving state-of-the-art performance on benchmarks such as MathVerse (+8.1%), MMMU-Pro (+7%), and MuirBench (+13.3%), but also gains improvements of up to 4% on non-reasoning-based benchmarks.</abstract>
      <url hash="d5194dd1">2025.acl-long.680</url>
      <bibkey>guo-etal-2025-mammoth</bibkey>
    </paper>
    <paper id="681">
      <title><fixed-case>SIFT</fixed-case>-50<fixed-case>M</fixed-case>: A Large-Scale Multilingual Dataset for Speech Instruction Fine-Tuning</title>
      <author><first>Prabhat</first><last>Pandey</last><affiliation>Amazon</affiliation></author>
      <author><first>Rupak Vignesh</first><last>Swaminathan</last><affiliation>Amazon</affiliation></author>
      <author><first>K V Vijay</first><last>Girish</last><affiliation>Amazon</affiliation></author>
      <author><first>Arunasish</first><last>Sen</last><affiliation>Amazon</affiliation></author>
      <author><first>Jian.</first><last>Xie</last></author>
      <author><first>Grant</first><last>Strimel</last><affiliation>Amazon</affiliation></author>
      <author><first>Andreas</first><last>Schwarz</last></author>
      <pages>13921-13942</pages>
      <abstract>We introduce SIFT (Speech Instruction Fine-Tuning), a 50M-example dataset designed for instruction fine-tuning and pre-training of speech-text large language models (LLMs). SIFT-50M is built from publicly available speech corpora, which collectively contain 14K hours of speech, and leverages LLMs along with off-the-shelf expert models. The dataset spans five languages, encompassing a diverse range of speech understanding as well as controllable speech generation instructions. Using SIFT-50M, we train SIFT-LLM, which outperforms existing speech-text LLMs on instruction-following benchmarks while achieving competitive performance on foundational speech tasks. To support further research, we also introduce EvalSIFT, a benchmark dataset specifically designed to evaluate the instruction-following capabilities of speech-text LLMs.</abstract>
      <url hash="89d5af90">2025.acl-long.681</url>
      <bibkey>pandey-etal-2025-sift</bibkey>
    </paper>
    <paper id="682">
      <title>Recent Advances in Speech Language Models: A Survey</title>
      <author><first>Wenqian</first><last>Cui</last></author>
      <author><first>Dianzhi</first><last>Yu</last><affiliation>Chinese University of Hong Kong</affiliation></author>
      <author><first>Xiaoqi</first><last>Jiao</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Ziqiao</first><last>Meng</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Guangyan</first><last>Zhang</last><affiliation>Tencent</affiliation></author>
      <author><first>Qichao</first><last>Wang</last></author>
      <author><first>Steven Y.</first><last>Guo</last></author>
      <author><first>Irwin</first><last>King</last></author>
      <pages>13943-13970</pages>
      <abstract>Text-based Large Language Models (LLMs) have recently gained significant attention, primarily for their capabilities in text-based interactions. However, natural human interaction often relies on speech, highlighting the need for voice-based models. In this context, Speech Language Models (SpeechLMs)—foundation models designed to understand and generate speech—emerge as a promising solution for end-to-end speech interaction. This survey offers a comprehensive overview of recent approaches to building SpeechLMs, outlining their core architectural components, training methodologies, evaluation strategies, and the challenges and potential directions for future research in this rapidly advancing field. The GitHub repository is available at https://github.com/dreamtheater123/Awesome-SpeechLM-Survey</abstract>
      <url hash="8f996776">2025.acl-long.682</url>
      <bibkey>cui-etal-2025-recent</bibkey>
    </paper>
    <paper id="683">
      <title><fixed-case>L</fixed-case>ex<fixed-case>CL</fixed-case>i<fixed-case>PR</fixed-case>: Cross-Lingual Paragraph Retrieval from Legal Judgments</title>
      <author><first>Rohit</first><last>Upadhya</last><affiliation>Technische Universität München</affiliation></author>
      <author><first>Santosh</first><last>T.y.s.s</last></author>
      <pages>13971-13993</pages>
      <abstract>Efficient retrieval of pinpointed information from case law is crucial for legal professionals but challenging due to the length and complexity of legal judgments. Existing works mostly often focus on retrieving entire cases rather than precise, paragraph-level information. Moreover, multilingual legal practice necessitates cross-lingual retrieval, most works have been limited to monolingual settings. To address these gaps, we introduce LexCLiPR, a cross-lingual dataset for paragraph-level retrieval from European Court of Human Rights (ECtHR) judgments, leveraging multilingual case law guides and distant supervision to curate our dataset. We evaluate retrieval models in a zero-shot setting, revealing the limitations of pre-trained multilingual models for cross-lingual tasks in low-resource languages and the importance of retrieval based post-training strategies. In fine-tuning settings, we observe that two-tower models excel in cross-lingual retrieval, while siamese architectures are better suited for monolingual tasks. Fine-tuning multilingual models on native language queries improves performance but struggles to generalize to unseen legal concepts, highlighting the need for robust strategies to address topical distribution shifts in the legal queries.</abstract>
      <url hash="bbef68f1">2025.acl-long.683</url>
      <bibkey>upadhya-t-y-s-s-2025-lexclipr</bibkey>
    </paper>
    <paper id="684">
      <title>Multi-task Adversarial Attacks against Black-box Model with Few-shot Queries</title>
      <author><first>Wenqiang</first><last>Wang</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Yan</first><last>Xiao</last><affiliation>Sun Yat-Sen University</affiliation></author>
      <author><first>Hao</first><last>Lin</last></author>
      <author><first>Yangshijie</first><last>Zhang</last><affiliation>Lanzhou University</affiliation></author>
      <author><first>Xiaochun</first><last>Cao</last></author>
      <pages>13994-14014</pages>
      <abstract>Current multi-task adversarial text attacks rely on abundant access to shared internal features and numerous queries, often limited to a single task type. As a result, these attacks are less effective against practical scenarios involving black-box feedback APIs, limited queries, or multiple task types. To bridge this gap, we propose <b>C</b>luster and <b>E</b>nsemble <b>M</b>util-task Text Adversarial <b>A</b>ttack (<b>CEMA</b>), an effective black-box attack that exploits the transferability of adversarial texts across different tasks. CEMA simplifies complex multi-task scenarios by using a <i>deep-level substitute model</i> trained in a <i>plug-and-play</i> manner for text classification, enabling attacks without mimicking the victim model. This approach requires only a few queries for training, converting multi-task attacks into classification attacks and allowing attacks across various tasks. CEMA generates multiple adversarial candidates using different text classification methods and selects the one that most effectively attacks substitute models. In experiments involving multi-task models with two, three, or six tasks—spanning classification, translation, summarization, and text-to-image generation—CEMA demonstrates significant attack success with as few as 100 queries. Furthermore, CEMA can target commercial APIs (e.g., Baidu and Google Translate), large language models (e.g., ChatGPT 4o), and image-generation models (e.g., Stable Diffusion V2), showcasing its versatility and effectiveness in real-world applications.</abstract>
      <url hash="c4a559e0">2025.acl-long.684</url>
      <bibkey>wang-etal-2025-multi</bibkey>
    </paper>
    <paper id="685">
      <title><fixed-case>SPECTRA</fixed-case>: Faster Large Language Model Inference with Optimized Internal and External Speculation</title>
      <author><first>Nguyen-Khang</first><last>Le</last><affiliation>Japan Advanced Institute of Science and Technology, Tokyo Institute of Technology</affiliation></author>
      <author><first>Truong Dinh</first><last>Do</last></author>
      <author><first>Le-Minh</first><last>Nguyen</last><affiliation>Japan Advanced Institute of Science and Technology, Tokyo Institute of Technology</affiliation></author>
      <pages>14015-14034</pages>
      <abstract>Inference with modern Large Language Models (LLMs) is both computationally expensive and time-consuming. Speculative decoding has emerged as a promising solution, but existing approaches face key limitations: training-based methods require a draft model that is challenging to obtain and lacks generalizability, while training-free methods offer limited speedup gains. In this work, we present Spectra, a novel framework for accelerating LLM inference without the need for additional training or modification to the original LLM. Spectra introduces two new techniques for efficiently utilizing internal and external speculation, each outperforming corresponding state-of-the-art (SOTA) methods independently. When combined, these techniques achieve up to a 4.08x speedup across various benchmarks and LLM architectures, significantly surpassing existing training-free approaches. The implementation of Spectra is publicly available.</abstract>
      <url hash="6f71b9cf">2025.acl-long.685</url>
      <bibkey>le-etal-2025-spectra</bibkey>
    </paper>
    <paper id="686">
      <title>Multi-level Association Refinement Network for Dialogue Aspect-based Sentiment Quadruple Analysis</title>
      <author><first>Zeliang</first><last>Tong</last></author>
      <author><first>Wei</first><last>Wei</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Xiaoye</first><last>Qu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Rikui</first><last>Huang</last></author>
      <author><first>Zhixin</first><last>Chen</last></author>
      <author><first>Xingyu</first><last>Yan</last><affiliation>State Grid Fujian Electric Power Co.</affiliation></author>
      <pages>14035-14057</pages>
      <abstract>Dialogue Aspect-based Sentiment Quadruple (DiaASQ) analysis aims to identify all quadruples (i.e., target, aspect, opinion, sentiment) from the dialogue. This task is challenging as different elements within a quadruple may manifest in different utterances, requiring precise handling of associations at both the utterance and word levels. However, most existing methods tackling it predominantly leverage predefined dialogue structure (e.g., reply) and word semantics, resulting in a surficial understanding of the deep sentiment association between utterances and words. In this paper, we propose a novel Multi-level Association Refinement Network (MARN) designed to achieve more accurate and comprehensive sentiment associations between utterances and words. Specifically, for utterances, we dynamically capture their associations with enriched semantic features through a holistic understanding of the dialogue, aligning them more closely with sentiment associations within elements in quadruples. For words, we develop a novel cross-utterance syntax parser (CU-Parser) that fully exploits syntactic information to enhance the association between word pairs within and across utterances. Moreover, to address the scarcity of labeled data in DiaASQ, we further introduce a multi-view data augmentation strategy to enhance the performance of MARN under low-resource conditions. Experimental results demonstrate that MARN achieves state-of-the-art performance and maintains robustness even under low-resource conditions.</abstract>
      <url hash="31330528">2025.acl-long.686</url>
      <bibkey>tong-etal-2025-multi</bibkey>
    </paper>
    <paper id="687">
      <title>Innovative Image Fraud Detection with Cross-Sample Anomaly Analysis: The Power of <fixed-case>LLM</fixed-case>s</title>
      <author><first>QiWen</first><last>Wang</last></author>
      <author><first>Junqi</first><last>Yang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Zhenghao</first><last>Lin</last><affiliation>Microsoft</affiliation></author>
      <author><first>Zhenzhe</first><last>Ying</last></author>
      <author><first>Weiqiang</first><last>Wang</last><affiliation>Ant Group</affiliation></author>
      <author><first>Chen</first><last>Lin</last><affiliation>Xiamen University</affiliation></author>
      <pages>14058-14078</pages>
      <abstract>The financial industry faces a substantial workload in verifying document images. Existing methods based on visual features struggle to identify fraudulent document images due to the lack of visual clues on the tampering region. This paper proposes CSIAD (Cross-Sample Image Anomaly Detection) by leveraging LLMs to identify logical inconsistencies in similar images. This novel framework accurately detects forged images with slight tampering traces and explains anomaly detection results. Furthermore, we introduce CrossCred, a new benchmark of real-world fraudulent images with fine-grained manual annotations. Experiments demonstrate that CSIAD outperforms state-of-the-art image fraud detection methods by 79.6% (F1) on CrossCred and deployed industrial solutions by 21.7% (F1) on business data. The benchmark is available at https://github.com/XMUDM/CSIAD.</abstract>
      <url hash="b0b0ad97">2025.acl-long.687</url>
      <bibkey>wang-etal-2025-innovative</bibkey>
    </paper>
    <paper id="688">
      <title>Cooperative or Competitive? Understanding the Interaction between Attention Heads From A Game Theory Perspective</title>
      <author><first>Xiaoye</first><last>Qu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Zengqi</first><last>Yu</last></author>
      <author><first>Dongrui</first><last>Liu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Wei</first><last>Wei</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Daizong</first><last>Liu</last><affiliation>Peking University</affiliation></author>
      <author><first>Jianfeng</first><last>Dong</last><affiliation>Zhejiang Gongshang University</affiliation></author>
      <author><first>Yu</first><last>Cheng</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>14079-14099</pages>
      <abstract>Despite the remarkable success of attention-based large language models (LLMs), the precise interaction mechanisms between attention heads remain poorly understood. In contrast to prevalent methods that focus on individual head contributions, we rigorously analyze the intricate interplay among attention heads through a novel framework based on the Harsanyi dividend, a concept from cooperative game theory. Our analysis reveals that significant positive Harsanyi dividends are sparsely distributed across head combinations, indicating that most heads do not contribute cooperatively. Moreover, certain head combinations exhibit negative dividends, indicating implicit competitive relationships. To further optimize the interactions among attention heads, we propose a training-free Game-theoretic Attention Calibration (GAC) method. Specifically, GAC selectively retains heads demonstrating significant cooperative gains and applies fine-grained distributional adjustments to the remaining heads. Comprehensive experiments across 17 benchmarks demonstrate the effectiveness of our proposed GAC and its superior generalization capabilities across diverse model families, scales, and modalities. Crucially, the discovered interaction phenomena offer a path toward a deeper understanding of the behaviors of LLMs.</abstract>
      <url hash="98214b0a">2025.acl-long.688</url>
      <bibkey>qu-etal-2025-cooperative</bibkey>
    </paper>
    <paper id="689">
      <title><fixed-case>MM</fixed-case>-Verify: Enhancing Multimodal Reasoning with Chain-of-Thought Verification</title>
      <author><first>Linzhuang</first><last>Sun</last></author>
      <author><first>Hao</first><last>Liang</last></author>
      <author><first>Jingxuan</first><last>Wei</last></author>
      <author><first>Bihui</first><last>Yu</last><affiliation>University of the Chinese Academy of Sciences</affiliation></author>
      <author><first>Tianpeng</first><last>Li</last></author>
      <author><first>Fan</first><last>Yang</last></author>
      <author><first>Zenan</first><last>Zhou</last></author>
      <author><first>Wentao</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <pages>14100-14115</pages>
      <abstract>According to the Test-Time Scaling, the integration of External Slow-Thinking with the Verify mechanism has been demonstrated to enhance multi-round reasoning in large language models (LLMs). However, in the multimodal (MM) domain, there is still a lack of a strong MM-Verifier. In this paper, we introduce MM-Verifier and MM-Reasoner to enhance multimodal reasoning through longer inference and more robust verification. First, we propose a two-step MM verification data synthesis method, which combines a simulation-based tree search with verification and uses rejection sampling to generate high-quality Chain-of-Thought (COT) data. This data is then used to fine-tune the verification model, MM-Verifier. Additionally, we present a more efficient method for synthesizing MMCOT data, bridging the gap between text-based and multimodal reasoning. The synthesized data is used to fine-tune MM-Reasoner. Our MM-Verifier outperforms all larger models on the MathCheck, MathVista, and MathVerse benchmarks. Moreover, MM-Reasoner demonstrates strong effectiveness and scalability, with performance improving as data size increases. Finally, our approach achieves strong performance when combining MM-Reasoner and MM-Verifier, reaching an accuracy of 65.3 on MathVista, surpassing GPT-4o (63.8) with 12 rollouts.</abstract>
      <url hash="6128d044">2025.acl-long.689</url>
      <bibkey>sun-etal-2025-mm</bibkey>
    </paper>
    <paper id="690">
      <title>Graph-Structured Trajectory Extraction from Travelogues</title>
      <author><first>Aitaro</first><last>Yamamoto</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Hiroyuki</first><last>Otomo</last><affiliation>CyberAgent, Inc.</affiliation></author>
      <author><first>Hiroki</first><last>Ouchi</last><affiliation>NAIST</affiliation></author>
      <author><first>Shohei</first><last>Higashiyama</last><affiliation>Nara Institute of Science and Technology, Japan and National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Hiroki</first><last>Teranishi</last><affiliation>Turing Inc. and Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Hiroyuki</first><last>Shindo</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <pages>14116-14132</pages>
      <abstract>Human traveling trajectories play a central role in characterizing each travelogue, and automatic trajectory extraction from travelogues is highly desired for tourism services, such as travel planning and recommendation. This work addresses the extraction of human traveling trajectories from travelogues. Previous work treated each trajectory as a sequence of visited locations, although locations with different granularity levels, e.g., “Kyoto City” and “Kyoto Station,” should not be lined up in a sequence. In this work, we propose to represent the trajectory as a graph that can capture the hierarchy as well as the visiting order, and construct a benchmark dataset for the trajectory extraction. The experiments using this dataset show that even naive baseline systems can accurately predict visited locations and the visiting order between them, while it is more challenging to predict the hierarchical relations.</abstract>
      <url hash="a29cea65">2025.acl-long.690</url>
      <bibkey>yamamoto-etal-2025-graph</bibkey>
    </paper>
    <paper id="691">
      <title>Learning First-Order Logic Rules for Argumentation Mining</title>
      <author><first>Yang</first><last>Sun</last></author>
      <author><first>Guanrong</first><last>Chen</last></author>
      <author><first>Hamid</first><last>Alinejad-Rokny</last><affiliation>UNSW Sydney</affiliation></author>
      <author><first>Jianzhu</first><last>Bao</last></author>
      <author><first>Yuqi</first><last>Huang</last></author>
      <author><first>Bin</first><last>Liang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Kam-Fai</first><last>Wong</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Min</first><last>Yang</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Ruifeng</first><last>Xu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>14133-14148</pages>
      <abstract>Argumentation Mining (AM) aims to extract argumentative structures from texts by identifying argumentation components (ACs) and their argumentative relations (ARs). While previous works focus on representation learning to encode ACs and AC pairs, they fail to explicitly model the underlying reasoning patterns of AM, resulting in limited interpretability. This paper proposes a novel <tex-math>\underline{F}</tex-math>irst-<tex-math>\underline{O}</tex-math>rder <tex-math>\underline{L}</tex-math>ogic reasoning framework for <tex-math>\underline{AM}</tex-math> (FOL-AM), designed to explicitly capture logical reasoning paths within argumentative texts. By interpreting multiple AM subtasks as a unified relation query task modeled using FOL rules, FOL-AM facilitates multi-hop relational reasoning and enhances interpretability. The framework supports two flexible implementations: a fine-tuned approach to leverage task-specific learning, and a prompt-based method utilizing large language models to harness their generalization capabilities. Extensive experiments on two AM benchmarks demonstrate that FOL-AM outperforms strong baselines while significantly improving explainability.</abstract>
      <url hash="8f63660a">2025.acl-long.691</url>
      <bibkey>sun-etal-2025-learning</bibkey>
    </paper>
    <paper id="692">
      <title>Investigating and Enhancing the Robustness of Large Multimodal Models Against Temporal Inconsistency</title>
      <author><first>Jiafeng</first><last>Liang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Shixin</first><last>Jiang</last></author>
      <author><first>Xuan</first><last>Dong</last></author>
      <author><first>Ning</first><last>Wang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Zheng</first><last>Chu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Hui</first><last>Su</last><affiliation>Meituan</affiliation></author>
      <author><first>Jinlan</first><last>Fu</last></author>
      <author><first>Ming</first><last>Liu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>See-Kiong</first><last>Ng</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Bing</first><last>Qin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>14149-14162</pages>
      <abstract>Large Multimodal Models (LMMs) have recently demonstrated impressive performance on general video comprehension benchmarks. Nevertheless, for broader applications, the robustness of their temporal analysis capability needs to be thoroughly investigated yet predominantly ignored. Motivated by this, we propose a novel temporal robustness benchmark (TemRobBench), which introduces temporal inconsistency perturbations separately at the visual and textual modalities to assess the robustness of models. We evaluate 16 mainstream LMMs and find that they exhibit over-reliance on prior knowledge and textual context in adversarial environments, while ignoring the actual temporal dynamics in the video. To mitigate this issue, we design panoramic direct preference optimization (PanoDPO), which encourages LMMs to incorporate both visual and linguistic feature preferences simultaneously. Experimental results show that PanoDPO can effectively enhance the model’s robustness and reliability in temporal analysis.</abstract>
      <url hash="dabeb1aa">2025.acl-long.692</url>
      <bibkey>liang-etal-2025-investigating</bibkey>
    </paper>
    <paper id="693">
      <title><fixed-case>U</fixed-case>ni<fixed-case>RAG</fixed-case>: Unified Query Understanding Method for Retrieval Augmented Generation</title>
      <author><first>Rui</first><last>Li</last></author>
      <author><first>Liyang</first><last>He</last></author>
      <author><first>Qi</first><last>Liu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Zheng</first><last>Zhang</last></author>
      <author><first>Heng</first><last>Yu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Yuyang</first><last>Ye</last></author>
      <author><first>Linbo</first><last>Zhu</last></author>
      <author><first>Yu</first><last>Su</last></author>
      <pages>14163-14178</pages>
      <abstract>Retrieval-Augmented Generation (RAG) technology effectively addresses the issues of knowledge update lag and hallucinations in large language models (LLMs) by integrating internal and external knowledge. Existing query augmentation methods improve RAG’s performance in handling complex queries but face two key challenges: (1) the separation of query augmentation and encoding tasks, which hinders information sharing and introduces cumulative errors, and (2) the difficulty of selecting the optimal augmentation strategy for different scenarios. In this work, we propose UniRAG, a unified framework for query understanding in RAG. UniRAG employs a decoder-only LLM to jointly perform query augmentation and encoding, eliminating task separation. To facilitate adaptive query augmentation, we categorize existing techniques into query paraphrasing, query expansion, and query abstraction. Our model learns to select the optimal augmentation strategy based on user queries, leveraging retrieval and generation outputs as feedback. Experimental results show that UniRAG significantly outperforms traditional query augmentation methods in five knowledge-intensive benchmark tasks in both closed and open domain question answering.</abstract>
      <url hash="71cdacd6">2025.acl-long.693</url>
      <bibkey>li-etal-2025-unirag</bibkey>
    </paper>
    <paper id="694">
      <title>Contextual Experience Replay for Self-Improvement of Language Agents</title>
      <author><first>Yitao</first><last>Liu</last></author>
      <author><first>Chenglei</first><last>Si</last><affiliation>Stanford University</affiliation></author>
      <author><first>Karthik R</first><last>Narasimhan</last><affiliation>Princeton University</affiliation></author>
      <author><first>Shunyu</first><last>Yao</last><affiliation>Princeton University</affiliation></author>
      <pages>14179-14198</pages>
      <abstract>Large language model (LLM) agents have been applied to sequential decision-making tasks such as web navigation, but without any environment-specific experiences, they often fail in these complex tasks. Moreover, current LLM agents are not designed to continually learn from past experiences during inference time, which could be crucial for them to gain these environment-specific experiences. To address this, we propose Contextual Experience Replay (CER), a training-free framework to enable efficient self-improvement for language agents in their context window. Specifically, CER accumulates and synthesizes past experiences into a dynamic memory buffer. These experiences encompass environment dynamics and common decision-making patterns, allowing the agents to retrieve and augment themselves with relevant knowledge in new tasks, enhancing their adaptability in complex environments. We evaluate CER on the challenging WebArena and VisualWebArena benchmarks. On VisualWebArena, CER surpasses the tree search method with much fewer token costs and achieves the state-of-the-art performance of 31.9%. On WebArena, CER also gets a competitive average success rate of 36.7%, relatively improving the success rate of the GPT-4o agent baseline by 51.0%. We also conduct a comprehensive analysis on it to prove its efficiency, validity and understand it better.</abstract>
      <url hash="1420e49b">2025.acl-long.694</url>
      <bibkey>liu-etal-2025-contextual</bibkey>
    </paper>
    <paper id="695">
      <title>Emma-<fixed-case>X</fixed-case>: An Embodied Multimodal Action Model with Grounded Chain of Thought and Look-ahead Spatial Reasoning</title>
      <author><first>Qi</first><last>Sun</last></author>
      <author><first>Pengfei</first><last>Hong</last></author>
      <author><first>Tej Deep</first><last>Pala</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Vernon</first><last>Toh</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>U-Xuan</first><last>Tan</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Deepanway</first><last>Ghosal</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Soujanya</first><last>Poria</last></author>
      <pages>14199-14214</pages>
      <abstract>Traditional reinforcement learning-based robotic control methods are often task-specific and fail to generalize across diverse environments or unseen objects and instructions. Visual Language Models (VLMs) demonstrate strong scene understanding and planning capabilities but lack the ability to generate actionable policies tailored to specific robotic embodiments. To address this, Visual-Language-Action (VLA) models have emerged, yet they face challenges in long-horizon spatial reasoning and grounded task planning. In this work, we propose the Embodied Multimodal Action Model with Grounded Chain of Thought and Look-ahead Spatial Reasoning, EMMA-X. EMMA-X leverages our constructed hierarchical embodiment dataset based on BridgeV2, containing 60,000 robot manipulation trajectories auto-annotated with grounded task reasoning and spatial guidance. Additionally, we introduce a trajectory segmentation strategy based on gripper states and motion trajectories, which can help mitigate hallucination in grounding subtask reasoning generation. Experimental results demonstrate that EMMA-X achieves superior performance over competitive baselines, particularly in real-world robotic tasks requiring spatial reasoning.</abstract>
      <url hash="84f7e08b">2025.acl-long.695</url>
      <bibkey>sun-etal-2025-emma</bibkey>
    </paper>
    <paper id="696">
      <title>Towards Comprehensive Argument Analysis in Education: Dataset, Tasks, and Method</title>
      <author><first>Yupei</first><last>Ren</last></author>
      <author><first>Xinyi</first><last>Zhou</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Ning</first><last>Zhang</last></author>
      <author><first>Shangqing</first><last>Zhao</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Man</first><last>Lan</last></author>
      <author><first>Xiaopeng</first><last>Bai</last><affiliation>East China Normal University</affiliation></author>
      <pages>14215-14231</pages>
      <abstract>Argument mining has garnered increasing attention over the years, with the recent advancement of Large Language Models (LLMs) further propelling this trend. However, current argument relations remain relatively simplistic and foundational, struggling to capture the full scope of argument information. To address this limitation, we propose a systematic framework comprising 14 fine-grained relation types from the perspectives of vertical argument relations and horizontal discourse relations, thereby capturing the intricate interplay between argument components for a thorough understanding of argument structure. On this basis, we conducted extensive experiments on three tasks: argument component prediction, relation prediction, and automated essay grading. Additionally, we explored the impact of writing quality on argument component prediction and relation prediction, as well as the connections between discourse relations and argumentative features. The findings highlight the importance of fine-grained argumentative annotations for argumentative writing assessment and encourage multi-dimensional argument analysis.</abstract>
      <url hash="123da7c6">2025.acl-long.696</url>
      <bibkey>ren-etal-2025-towards</bibkey>
    </paper>
    <paper id="697">
      <title>Browsing Like Human: A Multimodal Web Agent with Experiential Fast-and-Slow Thinking</title>
      <author><first>Haohao</first><last>Luo</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Jiayi</first><last>Kuang</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Wei</first><last>Liu</last></author>
      <author><first>Ying</first><last>Shen</last></author>
      <author><first>Jian</first><last>Luan</last><affiliation>Xiaomi Corporation</affiliation></author>
      <author><first>Yang</first><last>Deng</last><affiliation>Singapore Management University</affiliation></author>
      <pages>14232-14251</pages>
      <abstract>Automating web navigation which aims to build a web agent that follows user instructions to complete tasks like booking flights by interacting with websites, has received increasing attention due to its practical value. Although existing web agents are mostly equipped with visual perception, planning, and memory abilities, their reasoning process are still deviate from human cognition. In this work, we study the human thought pattern to empower agent with more human-like abilities in web navigation. To tackle this problem, we propose a novel multimodal web agent framework called WebExperT, which is designed to emulate the human planning process of “thinking fast and slow” to effectively decompose complex user instructions. Furthermore, WebExperT leverages experiential learning by reflecting from failure for continuously refining planning and decision-making outcomes. Experimental results on the Mind2Web benchmark demonstrate the superiority of WebExperT in both supervised and unsupervised settings.</abstract>
      <url hash="0452a0c7">2025.acl-long.697</url>
      <bibkey>luo-etal-2025-browsing</bibkey>
    </paper>
    <paper id="698">
      <title><fixed-case>M</fixed-case>a<fixed-case>XIFE</fixed-case>: Multilingual and Cross-lingual Instruction Following Evaluation</title>
      <author><first>Yile</first><last>Liu</last></author>
      <author><first>Ziwei</first><last>Ma</last></author>
      <author><first>Xiu</first><last>Jiang</last><affiliation>Guangdong OPPO Mobile Telecommunications Corp.,Ltd.</affiliation></author>
      <author><first>Jinglu</first><last>Hu</last><affiliation>Waseda University</affiliation></author>
      <author><first>ChangJing</first><last>ChangJing</last></author>
      <author><first>Liang</first><last>Li</last></author>
      <pages>14252-14332</pages>
      <abstract>With the rapid adoption of large language models (LLMs) in natural language processing, the ability to follow instructions has emerged as a key metric for evaluating their practical utility. However, existing evaluation methods often focus on single-language scenarios, overlooking the challenges and differences present in multilingual and cross-lingual contexts. To address this gap, we introduce MaXIFE: a comprehensive evaluation benchmark designed to assess instruction-following capabilities across 23 different languages with 1667 verifiable instruction tasks. MaXIFE integrates both Rule-Based Evaluation and Model-Based Evaluation, ensuring a balance of efficiency and accuracy. We applied MaXIFE to evaluate several leading commercial LLMs, establishing baseline results for future comparisons. By providing a standardized tool for multilingual instruction-following evaluation, MaXIFE aims to advance research and development in natural language processing.</abstract>
      <url hash="d5c61cab">2025.acl-long.698</url>
      <bibkey>liu-etal-2025-maxife</bibkey>
    </paper>
    <paper id="699">
      <title>Linguistic Generalizability of Test-Time Scaling in Mathematical Reasoning</title>
      <author><first>Guijin</first><last>Son</last></author>
      <author><first>Jiwoo</first><last>Hong</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Hyunwoo</first><last>Ko</last><affiliation>haerae.com and OnelineAI</affiliation></author>
      <author><first>James</first><last>Thorne</last><affiliation>KAIST</affiliation></author>
      <pages>14333-14368</pages>
      <abstract>Scaling pre-training compute has proven effective for achieving multilinguality, but does the same hold for test-time scaling? In this work, we introduce **MCLM**, a multilingual math benchmark featuring competition-level problems in 55 languages. We then compare three test-time scaling methods—Outcome Reward Modeling, Process Reward Modeling, and Budget Forcing. Our findings indicate that although “thinking LLMs” have recently garnered significant attention, their performance is comparable to traditional scaling methods like best-of-N once constrained to similar levels of inference FLOPs. More importantly, all tested methods fail to generalize robustly across languages, achieving only modest gains that are smaller than those observed in English, with no improvements in variance or consistency. To foster further research, we release MCLM and MR1-1.5B (a multilingual LLM with reasoning capabilities) and our evaluation results.</abstract>
      <url hash="7a65ab32">2025.acl-long.699</url>
      <bibkey>son-etal-2025-linguistic</bibkey>
    </paper>
    <paper id="700">
      <title>Can <fixed-case>MLLM</fixed-case>s Understand the Deep Implication Behind <fixed-case>C</fixed-case>hinese Images?</title>
      <author><first>Chenhao</first><last>Zhang</last><affiliation>NYU Courant, Shanghai Artificial Intelligence Laboratory and Huazhong University of Science and Technology</affiliation></author>
      <author><first>Xi</first><last>Feng</last></author>
      <author><first>Yuelin</first><last>Bai</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xeron</first><last>Du</last></author>
      <author><first>Jinchang</first><last>Hou</last></author>
      <author><first>Kaixin</first><last>Deng</last><affiliation>Hokkaido University</affiliation></author>
      <author><first>Guangzeng</first><last>Han</last><affiliation>University of Memphis</affiliation></author>
      <author><first>Qinrui</first><last>Li</last><affiliation>Cornell University</affiliation></author>
      <author><first>Bingli</first><last>Wang</last></author>
      <author><first>Jiaheng</first><last>Liu</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Xingwei</first><last>Qu</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Yifei</first><last>Zhang</last></author>
      <author><first>Qixuan</first><last>Zhao</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Yiming</first><last>Liang</last></author>
      <author><first>Ziqiang</first><last>Liu</last></author>
      <author><first>Feiteng</first><last>Fang</last></author>
      <author><first>Min</first><last>Yang</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Wenhao</first><last>Huang</last></author>
      <author><first>Chenghua</first><last>Lin</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Ge</first><last>Zhang</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Shiwen</first><last>Ni</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences</affiliation></author>
      <pages>14369-14402</pages>
      <abstract>As the capabilities of Multimodal Large Language Models (MLLMs) improve, the need for higher-order evaluation of them is increasing. However, there is a lack of work evaluating MLLM for higher-order perception and understanding of Chinese visual content. To address this, we introduce the CII-Bench, which aims to assess MLLMs’ such capabilities for Chinese images. To ensure the authenticity of the Chinese context, images in CII-Bench are sourced from the Chinese Internet and manually reviewed, with corresponding answers also manually crafted. Additionally, CII-Bench incorporates images that represent Chinese traditional culture, such as famous Chinese traditional paintings, which can deeply reflect the model’s understanding of Chinese traditional culture. Through experiments on multiple MLLMs using CII-Bench, significant findings emerged. There is a large gap between MLLMs and humans in performance. The highest MLLM accuracy is 64.4%, while the human average is 78.2% and the peak is 81.0%. MLLMs perform poorly on traditional culture images, indicating limitations in understanding high-level semantics and lacking a deep knowledge base of Chinese traditional culture. Moreover, most models have higher accuracy when image emotion hints are added to the prompts. We believe CII-Bench will help MLLMs better understand Chinese semantics and specific images, and move forward the development of expert artificial general intelligence (AGI). Our project is publicly available at https://cii-bench.github.io.</abstract>
      <url hash="6d14e12d">2025.acl-long.700</url>
      <bibkey>zhang-etal-2025-mllms</bibkey>
    </paper>
    <paper id="701">
      <title><fixed-case>K</fixed-case>az<fixed-case>MMLU</fixed-case>: Evaluating Language Models on <fixed-case>K</fixed-case>azakh, <fixed-case>R</fixed-case>ussian, and Regional Knowledge of <fixed-case>K</fixed-case>azakhstan</title>
      <author><first>Mukhammed</first><last>Togmanov</last></author>
      <author><first>Nurdaulet</first><last>Mukhituly</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Diana</first><last>Turmakhan</last></author>
      <author><first>Jonibek</first><last>Mansurov</last></author>
      <author><first>Maiya</first><last>Goloburda</last></author>
      <author><first>Akhmed</first><last>Sakip</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Zhuohan</first><last>Xie</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Yuxia</first><last>Wang</last></author>
      <author><first>Bekassyl</first><last>Syzdykov</last></author>
      <author><first>Nurkhan</first><last>Laiyk</last></author>
      <author><first>Alham Fikri</first><last>Aji</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Ekaterina</first><last>Kochmar</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Preslav</first><last>Nakov</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Fajri</first><last>Koto</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>14403-14416</pages>
      <abstract>Despite having a population of twenty million, Kazakhstan’s culture and language remain underrepresented in the field of natural language processing. Although large language models (LLMs) continue to advance worldwide, progress in Kazakh language has been limited, as seen in the scarcity of dedicated models and benchmark evaluations. To address this gap, we introduce KazMMLU, the first MMLU-style dataset specifically designed for Kazakh language. KazMMLU comprises 23,000 questions that cover various educational levels, including STEM, humanities, and social sciences, sourced from authentic educational materials and manually validated by native speakers and educators. The dataset includes 10,969 Kazakh questions and 12,031 Russian questions, reflecting Kazakhstan’s bilingual education system and rich local context. Our evaluation of several state-of-the-art multilingual models (Llama3.1, Qwen-2.5, GPT-4, and DeepSeek V3) demonstrates substantial room for improvement, as even the best-performing models struggle to achieve competitive performance in Kazakh and Russian. These findings highlight significant performance gaps compared to high-resource languages. We hope that our dataset will enable further research and development of Kazakh-centric LLMs.</abstract>
      <url hash="e715cf20">2025.acl-long.701</url>
      <bibkey>togmanov-etal-2025-kazmmlu</bibkey>
    </paper>
    <paper id="702">
      <title>Towards Multi-dimensional Evaluation of <fixed-case>LLM</fixed-case> Summarization across Domains and Languages</title>
      <author><first>Hyangsuk</first><last>Min</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Yuho</first><last>Lee</last></author>
      <author><first>Minjeong</first><last>Ban</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Jiaqi</first><last>Deng</last></author>
      <author><first>Nicole Hee-Yeon</first><last>Kim</last></author>
      <author><first>Taewon</first><last>Yun</last></author>
      <author><first>Hang</first><last>Su</last><affiliation>Amazon</affiliation></author>
      <author><first>Jason</first><last>Cai</last><affiliation>Amazon</affiliation></author>
      <author><first>Hwanjun</first><last>Song</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>14417-14450</pages>
      <abstract>Evaluation frameworks for text summarization have evolved in terms of both domain coverage and metrics. However, existing benchmarks still lack domain-specific assessment criteria, remain predominantly English-centric, and face challenges with human annotation due to the complexity of reasoning. To address these, we introduce MSumBench, which provides a multi-dimensional, multi-domain evaluation of summarization in English and Chinese. It also incorporates specialized assessment criteria for each domain and leverages a multi-agent debate system to enhance annotation quality. By evaluating eight modern summarization models, we discover distinct performance patterns across domains and languages. We further examine large language models as summary evaluators, analyzing the correlation between their evaluation and summarization capabilities, and uncovering systematic bias in their assessment of self-generated summaries. Our benchmark dataset is publicly available at https://github.com/DISL-Lab/MSumBench.</abstract>
      <url hash="80a5e66e">2025.acl-long.702</url>
      <bibkey>min-etal-2025-towards</bibkey>
    </paper>
    <paper id="703">
      <title><fixed-case>C</fixed-case>luster<fixed-case>A</fixed-case>ttn: <fixed-case>KV</fixed-case> Cache Compression under Intrinsic Attention Clustering</title>
      <author><first>Minwei</first><last>Zhang</last></author>
      <author><first>Haifeng</first><last>Sun</last><affiliation>Beijing University of Posts and Telecommunications, Beijing University of Posts and Telecommunications and Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Jingyu</first><last>Wang</last></author>
      <author><first>Shaolong</first><last>Li</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Wanyi</first><last>Ning</last></author>
      <author><first>Qi</first><last>Qi</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Zirui</first><last>Zhuang</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Jianxin</first><last>Liao</last></author>
      <pages>14451-14473</pages>
      <abstract>Sparse attention can effectively alleviate the significant demands on memory when large language models (LLMs) process long contexts. Existing methods typically apply the same sparse pattern across different attention heads and inputs. However, this uniform approach fails to capture the inherent diversity of attention patterns within LLMs — the intrinsic attention clustering. To address this, we propose ClusterAttn, a training-free sparse attention method that provides an efficient prompt cache compression scheme under intrinsic attention clustering for efficient LLM inference.Our findings show that attention heads consistently focus on specific clusters of the prompt during decoding, a pattern detectable from an observation window at the prompt’s end. ClusterAttn adaptively fits these clusters utilizing a density-based attention clustering algorithm, thus compressing the KV cache of the prompt. Evaluations on different models across various benchmarks demonstrate ClusterAttn’s superior compression rates and efficiency. By utilizing only 1024 tokens, it can reduce memory usage by 10%–65%, resulting in a latency reduction of 12%–23% and a throughput increase of 2.6–4.8 times, all with nearly no accuracy loss. Additionally, ClusterAttn can handle up to 128k context on a single A100-80GB GPU, outperforming existing methods.</abstract>
      <url hash="4191d444">2025.acl-long.703</url>
      <bibkey>zhang-etal-2025-clusterattn</bibkey>
    </paper>
    <paper id="704">
      <title><fixed-case>SHARE</fixed-case>: Shared Memory-Aware Open-Domain Long-Term Dialogue Dataset Constructed from Movie Script</title>
      <author><first>Eunwon</first><last>Kim</last></author>
      <author><first>Chanho</first><last>Park</last></author>
      <author><first>Buru</first><last>Chang</last><affiliation>Korea University</affiliation></author>
      <pages>14474-14498</pages>
      <abstract>Shared memories between two individuals strengthen their bond and are crucial for facilitating their ongoing conversations. This study aims to make long-term dialogue more engaging by leveraging these shared memories. To this end, we introduce a new long-term dialogue dataset named SHARE, constructed from movie scripts, which are a rich source of shared memories among various relationships. Our dialogue dataset contains the summaries of persona information and events of two individuals, as explicitly revealed in their conversation, along with implicitly extractable shared memories. We also introduce EPISODE, a long-term dialogue framework based on SHARE that utilizes shared experiences between individuals. Through experiments using SHARE, we demonstrate that shared memories between two individuals make long-term dialogues more engaging and sustainable, and that EPISODE effectively manages shared memories during dialogue. Our dataset and code are available at https://github.com/e1kim/SHARE.</abstract>
      <url hash="3db07081">2025.acl-long.704</url>
      <bibkey>kim-etal-2025-share</bibkey>
    </paper>
    <paper id="705">
      <title>Incongruity-aware Tension Field Network for Multi-modal Sarcasm Detection</title>
      <author><first>Jiecheng</first><last>Zhang</last></author>
      <author><first>C.L.Philip</first><last>Chen</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Shuzhen</first><last>Li</last></author>
      <author><first>Tong</first><last>Zhang</last><affiliation>South China University of Technology</affiliation></author>
      <pages>14499-14508</pages>
      <abstract>Multi-modal sarcasm detection (MSD) identifies sarcasm and accurately understands users’ real attitudes from text-image pairs. Most MSD researches explore the incongruity of text-image pairs as sarcasm information through consistency preference methods. However, these methods prioritize consistency over incongruity and blur incongruity information under their global feature aggregation mechanisms, leading to incongruity distortions and model misinterpretations. To address the above issues, this paper proposes a pioneering inconsistency preference method called incongruity-aware tension field network (ITFNet) for multi-modal sarcasm detection tasks. Specifically, ITFNet extracts effective text-image feature pairs in fact and sentiment perspectives. It then constructs a fact/sentiment tension field with discrepancy metrics to capture the contextual tone and polarized incongruity after the iterative learning of tension intensity, effectively highlighting incongruity information during such inconsistency preference learning. It further standardizes the polarized incongruity with reference to contextual tone to obtain standardized incongruity, effectively implementing instance standardization for unbiased decision-making in MSD. ITFNet performs well in extracting salient and standardized incongruity through an incongruity-aware tension field, significantly tackling incongruity distortions and cross-instance variance. Moreover, ITFNet achieves state-of-the-art performance surpassing LLaVA1.5-7B with only 17.3M trainable parameters, demonstrating its optimal performance-efficiency in multi-modal sarcasm detection tasks.</abstract>
      <url hash="b7543ce0">2025.acl-long.705</url>
      <bibkey>zhang-etal-2025-incongruity</bibkey>
    </paper>
    <paper id="706">
      <title>Instruction Tuning on Public Government and Cultural Data for Low-Resource Language: a Case Study in <fixed-case>K</fixed-case>azakh</title>
      <author><first>Nurkhan</first><last>Laiyk</last></author>
      <author><first>Daniil</first><last>Orel</last></author>
      <author><first>Rituraj</first><last>Joshi</last><affiliation>Cerebras Systems, Inc</affiliation></author>
      <author><first>Maiya</first><last>Goloburda</last></author>
      <author><first>Yuxia</first><last>Wang</last></author>
      <author><first>Preslav</first><last>Nakov</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Fajri</first><last>Koto</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>14509-14538</pages>
      <abstract>Instruction tuning in low-resource languages remains underexplored due to limited text data, particularly in government and cultural domains. To address this, we introduce and open-source a large-scale (10,600 samples) instruction-following (IFT) dataset, covering key institutional and cultural knowledge relevant to Kazakhstan. Our dataset enhances LLMs’ understanding of procedural, legal, and structural governance topics. We employ LLM-assisted data generation, comparing open-weight and closed-weight models for dataset construction, and select GPT-4o as the backbone. Each entity of our dataset undergoes full manual verification to ensure high quality. We also show that fine-tuning Qwen, Falcon, and Gemma on our dataset leads to consistent performance improvements in both multiple-choice and generative tasks, demonstrating the potential of LLM-assisted instruction tuning for low-resource languages.</abstract>
      <url hash="dc9a3913">2025.acl-long.706</url>
      <bibkey>laiyk-etal-2025-instruction</bibkey>
    </paper>
    <paper id="707">
      <title>Stealing Training Data from Large Language Models in Decentralized Training through Activation Inversion Attack</title>
      <author><first>Chenxi</first><last>Dai</last></author>
      <author><first>Lin</first><last>Lu</last></author>
      <author><first>Pan</first><last>Zhou</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <pages>14539-14551</pages>
      <abstract>Decentralized training has become a resource-efficient framework to democratize the training of large language models (LLMs). However, the privacy risks associated with this framework, particularly due to the potential inclusion of sensitive data in training datasets, remain unexplored. This paper identifies a novel and realistic attack surface: the privacy leakage from training data in decentralized training, and proposes <tex-math>\textit{activation inversion attack}</tex-math> (AIA) for the first time. AIA first constructs a shadow dataset comprising text labels and corresponding activations using public datasets. Leveraging this dataset, an attack model can be trained to reconstruct the training data from activations in victim decentralized training. We conduct extensive experiments on various LLMs and publicly available datasets to demonstrate the susceptibility of decentralized training to AIA. These findings highlight the urgent need to enhance security measures in decentralized training to mitigate privacy risks in training LLMs.</abstract>
      <url hash="83310148">2025.acl-long.707</url>
      <bibkey>dai-etal-2025-stealing</bibkey>
    </paper>
    <paper id="708">
      <title>From Selection to Generation: A Survey of <fixed-case>LLM</fixed-case>-based Active Learning</title>
      <author><first>Yu</first><last>Xia</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Subhojyoti</first><last>Mukherjee</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Zhouhang</first><last>Xie</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Junda</first><last>Wu</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Xintong</first><last>Li</last></author>
      <author><first>Ryan</first><last>Aponte</last></author>
      <author><first>Hanjia</first><last>Lyu</last><affiliation>University of Rochester</affiliation></author>
      <author><first>Joe</first><last>Barrow</last><affiliation>Pattern Data</affiliation></author>
      <author><first>Hongjie</first><last>Chen</last><affiliation>Dolby Labs.</affiliation></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Branislav</first><last>Kveton</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Tong</first><last>Yu</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Ruiyi</first><last>Zhang</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Jiuxiang</first><last>Gu</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Nesreen K.</first><last>Ahmed</last><affiliation>Intel AI Research</affiliation></author>
      <author><first>Yu</first><last>Wang</last><affiliation>University of Oregon and Vanderbilt University</affiliation></author>
      <author><first>Xiang</first><last>Chen</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Hanieh</first><last>Deilamsalehy</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Sungchul</first><last>Kim</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Zhengmian</first><last>Hu</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Yue</first><last>Zhao</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Nedim</first><last>Lipka</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Seunghyun</first><last>Yoon</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Ting-Hao Kenneth</first><last>Huang</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Zichao</first><last>Wang</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Puneet</first><last>Mathur</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Soumyabrata</first><last>Pal</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Koyel</first><last>Mukherjee</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Zhehao</first><last>Zhang</last></author>
      <author><first>Namyong</first><last>Park</last><affiliation>Meta AI</affiliation></author>
      <author><first>Thien Huu</first><last>Nguyen</last><affiliation>University of Oregon</affiliation></author>
      <author><first>Jiebo</first><last>Luo</last><affiliation>University of Rochester</affiliation></author>
      <author><first>Ryan A.</first><last>Rossi</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Julian</first><last>McAuley</last><affiliation>University of California, San Diego, University of California, San Diego</affiliation></author>
      <pages>14552-14569</pages>
      <abstract>Active Learning (AL) has been a powerful paradigm for improving model efficiency and performance by selecting the most informative data points for labeling and training. In recent active learning frameworks, Large Language Models (LLMs) have been employed not only for selection but also for generating entirely new data instances and providing more cost-effective annotations. Motivated by the increasing importance of high-quality data and efficient model training in the era of LLMs, we present a comprehensive survey on LLM-based Active Learning. We introduce an intuitive taxonomy that categorizes these techniques and discuss the transformative roles LLMs can play in the active learning loop. We further examine the impact of AL on LLM learning paradigms and its applications across various domains. Finally, we identify open challenges and propose future research directions. This survey aims to serve as an up-to-date resource for researchers and practitioners seeking to gain an intuitive understanding of LLM-based AL techniques and deploy them to new applications.</abstract>
      <url hash="a5edd035">2025.acl-long.708</url>
      <bibkey>xia-etal-2025-selection</bibkey>
    </paper>
    <paper id="709">
      <title><fixed-case>O</fixed-case>mni<fixed-case>F</fixed-case>latten: An End-to-end <fixed-case>GPT</fixed-case> Model for Seamless Voice Conversation</title>
      <author><first>Qinglin</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Luyao</first><last>Cheng</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Chong</first><last>Deng</last></author>
      <author><first>Qian</first><last>Chen</last></author>
      <author><first>Wen</first><last>Wang</last></author>
      <author><first>Siqi</first><last>Zheng</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Jiaqing</first><last>Liu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Hai</first><last>Yu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Chao-Hong</first><last>Tan</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Zhihao</first><last>Du</last></author>
      <author><first>ShiLiang</first><last>Zhang</last></author>
      <pages>14570-14580</pages>
      <abstract>Full-duplex spoken dialogue systems significantly surpass traditional turn-based dialogue systems, as they allow simultaneous bidirectional communication, closely mirroring human-human interactions. However, achieving low latency and natural interactions in full-duplex dialogue systems remains a significant challenge, especially considering human conversation dynamics such as interruptions, backchannels, and overlapping speech. In this paper, we introduce a novel End-to-End GPT-based model OmniFlatten for full-duplex conversation, capable of effectively modeling the complex behaviors inherent to natural conversations with low latency. To achieve full-duplex conversation capabilities, we propose a multi-stage post-training scheme that progressively adapts a text large language model (LLM) backbone into a speech-text dialogue LLM, capable of generating text and speech in real time, without modifying the architecture of the backbone LLM. The training process comprises three stages: modality alignment, half-duplex dialogue learning, and full-duplex dialogue learning. In all training stages, we standardize the data using a flattening operation, which enables unifying the training methods and the GPT backbone across different modalities and tasks. Our approach offers a simple modeling technique and a promising research direction for developing efficient and natural end-to-end full-duplex spoken dialogue systems.</abstract>
      <url hash="44bc0925">2025.acl-long.709</url>
      <bibkey>zhang-etal-2025-omniflatten</bibkey>
    </paper>
    <paper id="710">
      <title><fixed-case>D</fixed-case>o<fixed-case>MIX</fixed-case>: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning</title>
      <author><first>Dohoon</first><last>Kim</last></author>
      <author><first>Donghun</first><last>Kang</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Taesup</first><last>Moon</last><affiliation>Seoul National University</affiliation></author>
      <pages>14581-14602</pages>
      <abstract>Domain-Adaptive Pre-training (DAP) has recently gained attention for its effectiveness in fine-tuning pre-trained models. Building on this, continual DAP has been explored to develop pre-trained models capable of incrementally incorporating different domain datasets. However, existing continual DAP methods face several limitations: (1) high computational cost and GPU memory usage during training; (2) sensitivity to incremental data order; and (3) providing a single, generalized model for all end tasks, which contradicts the essence of DAP. In this paper, we propose DoMIX, a novel approach that addresses these challenges by leveraging LoRA modules, a representative parameter-efficient fine-tuning (PEFT) method. Our approach enables efficient and parallel domain-adaptive pre-training that is robust to domain order and effectively utilizes accumulated knowledge to provide tailored pre-trained models for specific tasks.We also demonstrate that our method can be extended beyond the DAP setting to standard LLM fine-tuning scenarios. Code is available at https://github.com/dohoonkim-ai/DoMIX.</abstract>
      <url hash="aa2be600">2025.acl-long.710</url>
      <bibkey>kim-etal-2025-domix</bibkey>
    </paper>
    <paper id="711">
      <title><fixed-case>EAGLE</fixed-case>: Expert-Guided Self-Enhancement for Preference Alignment in Pathology Large Vision-Language Model</title>
      <author><first>Meidan</first><last>Ding</last><affiliation>Shenzhen University</affiliation></author>
      <author><first>Jipeng</first><last>Zhang</last></author>
      <author><first>Wenxuan</first><last>Wang</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Haiqin</first><last>Zhong</last><affiliation>Shenzhen University</affiliation></author>
      <author><first>Xiaoqin</first><last>Wang</last><affiliation>Shenzhen University</affiliation></author>
      <author><first>Xinheng</first><last>Lyu</last><affiliation>The University of Nottingham Ningbo China</affiliation></author>
      <author><first>Wenting</first><last>Chen</last></author>
      <author><first>Linlin</first><last>Shen</last><affiliation>Shenzhen University</affiliation></author>
      <pages>14603-14619</pages>
      <abstract>Recent advancements in Large Vision Language Models (LVLMs) show promise for pathological diagnosis, yet their application in clinical settings faces critical challenges of multimodal hallucination and biased responses. While preference alignment methods have proven effective in general domains, acquiring high-quality preference data for pathology remains challenging due to limited expert resources and domain complexity. In this paper, we propose EAGLE (Expert-guided self-enhancement for preference Alignment in patholoGy Large vision-languagE model), a novel framework that systematically integrates medical expertise into preference alignment. EAGLE consists of three key stages: initialization through supervised fine-tuning, self-preference creation leveraging expert prompting and medical entity recognition, and iterative preference following-tuning. The self-preference creation stage uniquely combines expert-verified chosen sampling with expert-guided rejected sampling to generate high-quality preference data, while the iterative tuning process continuously refines both data quality and model performance. Extensive experiments demonstrate that EAGLE significantly outperforms existing pathological LVLMs, effectively reducing hallucination and bias while maintaining pathological accuracy. The source code is available at https://github.com/meidandz/EAGLE.</abstract>
      <url hash="edbc25d0">2025.acl-long.711</url>
      <bibkey>ding-etal-2025-eagle</bibkey>
    </paper>
    <paper id="712">
      <title><fixed-case>C</fixed-case>o<fixed-case>T</fixed-case>-<fixed-case>ICL</fixed-case> Lab: A Synthetic Framework for Studying Chain-of-Thought Learning from In-Context Demonstrations</title>
      <author><first>Vignesh</first><last>Kothapalli</last><affiliation>LinkedIn</affiliation></author>
      <author><first>Hamed</first><last>Firooz</last><affiliation>LinkedIn</affiliation></author>
      <author><first>Maziar</first><last>Sanjabi</last><affiliation>LinkedIn</affiliation></author>
      <pages>14620-14642</pages>
      <abstract>We introduce CoT-ICL Lab, a framework and methodology to generate synthetic tokenized datasets and systematically study chain-of thought (CoT) in-context learning (ICL) in language models. CoT-ICL Lab allows fine grained control over the complexity of in-context examples by decoupling (1) the causal structure involved in chain token generation from (2) the underlying token processing functions. We train decoder-only transformers (up to 700M parameters) on these datasets and show that CoT accelerates the accuracy transition to higher values across model sizes. In particular, we find that model depth is crucial for leveraging CoT with limited in-context examples, while more examples help shallow models match deeper model performance. Additionally, limiting the diversity of token processing functions throughout training improves causal structure learning via ICL. We also interpret these transitions by analyzing transformer embeddings and attention maps. Overall, CoT-ICL Lab serves as a simple yet powerful testbed for theoretical and empirical insights into ICL and CoT in language models.</abstract>
      <url hash="f9ffcb43">2025.acl-long.712</url>
      <bibkey>kothapalli-etal-2025-cot</bibkey>
    </paper>
    <paper id="713">
      <title>Flexora: Flexible Low-Rank Adaptation for Large Language Models</title>
      <author><first>Chenxing</first><last>Wei</last></author>
      <author><first>Yao</first><last>Shu</last><affiliation>The Hong Kong University of Science and Technology (Guangzhou)</affiliation></author>
      <author><first>Ying Tiffany</first><last>He</last><affiliation>Shenzhen University</affiliation></author>
      <author><first>Fei</first><last>Yu</last><affiliation>GM Lab</affiliation></author>
      <pages>14643-14682</pages>
      <abstract>Large language models (LLMs) have revolutionized artificial intelligence, but their performance on specific tasks is often limited by knowledge boundaries. While fine-tuning techniques like low-rank adaptation (LoRA) aim to address this, they can suffer from overfitting. We propose flexible low-rank adaptation (Flexora), a novel method that automatically selects the most critical layers for fine-tuning to optimize performance across diverse downstream tasks. Flexora formulates layer selection as a hyperparameter optimization problem, employs unrolled differentiation for efficient solving, and identifies the most impactful layers based on optimized hyperparameters. Extensive experiments across various pre-trained models and natural language tasks demonstrate that Flexora consistently outperforms existing baselines. We provide theoretical insights and comprehensive ablation studies to elucidate the effectiveness of Flexora. Therefore, Flexora offers a robust solution to enhance LoRA fine-tuning for LLMs, potentially advancing the field of adaptive language model optimization.</abstract>
      <url hash="ec39cf73">2025.acl-long.713</url>
      <bibkey>wei-etal-2025-flexora</bibkey>
    </paper>
    <paper id="714">
      <title><fixed-case>QDTS</fixed-case>ynth: Quality-Driven Formal Theorem Synthesis for Enhancing Proving Performance of <fixed-case>LLM</fixed-case>s</title>
      <author><first>Lei</first><last>Wang</last></author>
      <author><first>Ruobing</first><last>Zuo</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Gaolei</first><last>He</last><affiliation>Henan Univeristy</affiliation></author>
      <author><first>Jianlin</first><last>Wang</last></author>
      <author><first>Zhengfeng</first><last>Yang</last><affiliation>East China Normal University</affiliation></author>
      <pages>14683-14698</pages>
      <abstract>Automated Theorem Proving is an important and challenging task. Although large language models (LLMs) have demonstrated remarkable potential in mathematical reasoning, their performance in formal theorem proving remains constrained by the scarcity of high-quality supervised fine-tuning (SFT) data. To address this limitation, we propose a **Q**uality-**D**riven **T**heorem **S**ynthesis method (QDTSynth) in Lean4. During the statement synthesis, we enhance Monte Carlo Tree Search (MCTS) with an adaptive adjustment mechanism that dynamically optimizes the search strategy based on the synthesis of statements. In addition, we propose diversity screening and the self-assessment method to select theorems that exhibit both diversity and high quality from the initially synthetic statements, enabling the synthesis of a high-quality Lean4 theorem dataset. After fine-tuning three open-source large language models on our synthetic dataset, experiments on the miniF2F benchmark demonstrate that QDTSynth significantly improves the performance of various open-source LLMs in theorem proving tasks. Our work offers a promising new direction for the future synthesis of high-quality formal mathematical theorems.</abstract>
      <url hash="d951732e">2025.acl-long.714</url>
      <bibkey>wang-etal-2025-qdtsynth</bibkey>
    </paper>
    <paper id="715">
      <title><fixed-case>RSVP</fixed-case>: Reasoning Segmentation via Visual Prompting and Multi-modal Chain-of-Thought</title>
      <author><first>Yi</first><last>Lu</last><affiliation>Opus AI Research</affiliation></author>
      <author><first>Jiawang</first><last>Cao</last><affiliation>Opus AI Research</affiliation></author>
      <author><first>Yongliang</first><last>Wu</last></author>
      <author><first>Bozheng</first><last>Li</last><affiliation>Brown University</affiliation></author>
      <author><first>Licheng</first><last>Tang</last><affiliation>Opus AI Research</affiliation></author>
      <author><first>Yangguang</first><last>Ji</last><affiliation>Opus AI Research</affiliation></author>
      <author><first>Chong</first><last>Wu</last></author>
      <author><first>Jay</first><last>Wu</last><affiliation>Opus AI Research</affiliation></author>
      <author><first>Wenbo</first><last>Zhu</last></author>
      <pages>14699-14716</pages>
      <abstract>Multi-modal Large Language Models (MLLMs) have demonstrated remarkable reasoning capability while lack explicit mechanisms for visual grounding and segmentation, creating a gap between cognitive reasoning and visual perception. To bridge this gap, we introduce Reasoning Segmentation via Visual Prompting (RSVP), a novel framework that unifies multi-step multimodal reasoning with grounded visual understanding. RSVP is a two-stage structuralized framework that integrates reasoning-driven localization with segmentation refinement. In the reasoning stage, RSVP employs multimodal chain-of-thought visual prompts to help MLLMs understand queries and infer targets, generating interpretable region proposals that enhance visual grounding. In segmentation stage, RSVP refines these proposals with a Vision-Language Segmentation Module (VLSM), seamlessly integrates textual and visual cues to produce precise segmentation masks. By explicitly modelling the interaction between multimodal reasoning and segmentation, RSVP introduces a new paradigm for interpretable reasoning segmentation. It exploits MLLMs’ inherent localization capabilities, enabling the models to not only reason about objects but also generate structured visual representations. Our extensive experiments demonstrate that RSVP achieves state-of-the-art performance, surpasses state-of-the-art methods by up to +6.5 gIoU and +9.2 cIoU on ReasonSeg, and achieves 49.7 mAP on SegInW under zero-shot settings. These results validate RSVP as an effective and scalable framework for integrating cognitive reasoning with structured visual understanding.</abstract>
      <url hash="03e265c8">2025.acl-long.715</url>
      <bibkey>lu-etal-2025-rsvp</bibkey>
    </paper>
    <paper id="716">
      <title><fixed-case>QAE</fixed-case>val: Mixture of Evaluators for Question-Answering Task Evaluation</title>
      <author><first>Tan</first><last>Yue</last></author>
      <author><first>Rui</first><last>Mao</last></author>
      <author><first>Xuzhao</first><last>Shi</last></author>
      <author><first>Shuo</first><last>Zhan</last></author>
      <author><first>Zuhao</first><last>Yang</last></author>
      <author><first>Dongyan</first><last>Zhao</last><affiliation>Peking University</affiliation></author>
      <pages>14717-14730</pages>
      <abstract>Question answering (QA) tasks serve as a key benchmark for evaluating generation systems. Traditional rule-based metrics, such as accuracy and relaxed-accuracy, struggle with open-ended and unstructured responses. LLM-based evaluation methods offer greater flexibility but suffer from sensitivity to instructions, robustness issues, and high computational costs. To overcome these challenges, we introduce QAEval, a hybrid framework combining rule-based reliability with LLM-based adaptability. QAEval utilizes two high-quality datasets: QAExtract for short-answer extraction and QAScore for scoring model training. By integrating a Mixture of Evaluators model with Dynamic Load Balancing Optimization, QAEval enables accurate, cost-effective QA evaluation. Experimental results show it outperforms models like GPT-4o and Claude-3, achieving 92.3% accuracy with only 0.6B parameters.</abstract>
      <url hash="9365ad3e">2025.acl-long.716</url>
      <bibkey>yue-etal-2025-qaeval</bibkey>
    </paper>
    <paper id="717">
      <title>Debiasing the Fine-Grained Classification Task in <fixed-case>LLM</fixed-case>s with Bias-Aware <fixed-case>PEFT</fixed-case></title>
      <author><first>Daiying</first><last>Zhao</last></author>
      <author><first>Xinyu</first><last>Yang</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Hang</first><last>Chen</last></author>
      <pages>14731-14746</pages>
      <abstract>Fine-grained classification via LLMs is susceptible to more complex label biases compared to traditional classification tasks. Existing bias mitigation strategies, such as retraining, post-hoc adjustment, and parameter-efficient fine-tuning (PEFT) are primarily effective for simple classification biases, such as stereotypes, but fail to adequately address prediction propensity and discriminative ability biases. In this paper, we analyze these two bias phenomena and observe their progressive accumulation from intermediate to deeper layers within LLMs. To mitigate this issue, we propose a bias-aware optimization framework that incorporates two distinct label balance constraints with a PEFT strategy targeting an intermediate layer. Our approach adjusts less than 1% of the model’s parameters while effectively curbing bias amplification in deeper layers. Extensive experiments conducted across 12 datasets and 5 LLMs demonstrate that our method consistently outperforms or matches the performance of full-parameter fine-tuning and LoRA, achieving superior results with lower perplexity.</abstract>
      <url hash="e21ec964">2025.acl-long.717</url>
      <bibkey>zhao-etal-2025-debiasing</bibkey>
    </paper>
    <paper id="718">
      <title>Demystifying Small Language Models for Edge Deployment</title>
      <author><first>Zhenyan</first><last>Lu</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Xiang</first><last>Li</last></author>
      <author><first>Dongqi</first><last>Cai</last></author>
      <author><first>Rongjie</first><last>Yi</last></author>
      <author><first>Fangming</first><last>Liu</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Wei</first><last>Liu</last></author>
      <author><first>Jian</first><last>Luan</last><affiliation>Xiaomi Corporation</affiliation></author>
      <author><first>Xiwen</first><last>Zhang</last><affiliation>Helixon Research</affiliation></author>
      <author><first>Nicholas D.</first><last>Lane</last><affiliation>Flower Labs and University of Cambridge</affiliation></author>
      <author><first>Mengwei</first><last>Xu</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <pages>14747-14764</pages>
      <abstract>Small language models (SLMs) have emerged as a promising solution for deploying resource-constrained devices, such as smartphones and Web of Things. This work presents the first comprehensive study of over 60 SLMs such as Microsoft Phi and Google Gemma that are publicly accessible. Our findings show that state-of-the-art SLMs outperform 7B models in general tasks, proving their practical viability. However, SLMs’ in-context learning capabilities remain limited, and their efficiency has significant optimization potential. We identify key SLM optimization opportunities, including dynamic task-specific routing, model-hardware co-design, and vocabulary/KV cache compression. Overall, we expect the work to reveal an all-sided landscape of SLMs, benefiting the research community across algorithm, model, system, and hardware levels.</abstract>
      <url hash="932333fc">2025.acl-long.718</url>
      <bibkey>lu-etal-2025-demystifying</bibkey>
    </paper>
    <paper id="719">
      <title>Adapt Once, Thrive with Updates: Transferable Parameter-Efficient Fine-Tuning on Evolving Base Models</title>
      <author><first>Naibin</first><last>Gu</last></author>
      <author><first>Peng</first><last>Fu</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xiyu</first><last>Liu</last></author>
      <author><first>Ke</first><last>Ma</last><affiliation>University of the Chinese Academy of Sciences</affiliation></author>
      <author><first>Zheng</first><last>Lin</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <author><first>Weiping</first><last>Wang</last><affiliation>IIE</affiliation></author>
      <pages>14765-14783</pages>
      <abstract>Parameter-efficient fine-tuning (PEFT) has become a common method for fine-tuning large language models, where a base model can serve multiple users through PEFT module switching. To enhance user experience, base models require periodic updates. However, once updated, PEFT modules fine-tuned on previous versions often suffer substantial performance degradation on newer versions. Re-tuning these numerous modules to restore performance would incur significant computational costs. Through a comprehensive analysis of the changes that occur during base model updates, we uncover an interesting phenomenon: continual training primarily affects task-specific knowledge stored in Feed-Forward Networks (FFN), while having less impact on the task-specific pattern in the Attention mechanism. Based on these findings, we introduce Trans-PEFT, a novel approach that enhances the PEFT module by focusing on the task-specific pattern while reducing its dependence on certain knowledge in the base model. Further theoretical analysis supports our approach. Extensive experiments across 7 base models and 12 datasets demonstrate that Trans-PEFT trained modules can maintain performance on updated base models without re-tuning, significantly reducing maintenance overhead in real-world applications.</abstract>
      <url hash="1c6bdc8d">2025.acl-long.719</url>
      <bibkey>gu-etal-2025-adapt</bibkey>
    </paper>
    <paper id="720">
      <title>Can Vision-Language Models Evaluate Handwritten Math?</title>
      <author><first>Oikantik</first><last>Nath</last><affiliation>Department of Computer Science, Indian Institute of Technology, Madras, Indian Institute of Technology, Madras</affiliation></author>
      <author><first>Hanani</first><last>Bathina</last></author>
      <author><first>Mohammed Safi Ur Rahman</first><last>Khan</last><affiliation>Indian Institute of Technology, Madras, Dhirubhai Ambani Institute Of Information and Communication Technology and Indian Institute of Technology, Madras, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <author><first>Mitesh M</first><last>Khapra</last><affiliation>Indian Institute of Technology, Madras</affiliation></author>
      <pages>14784-14814</pages>
      <abstract>Recent advancements in Vision-Language Models (VLMs) have opened new possibilities in automatic grading of handwritten student responses, particularly in mathematics. However, a comprehensive study to test the ability of VLMs to evaluate and reason over handwritten content remains absent. To address this gap, we introduce FERMAT, a benchmark designed to assess VLMs’ ability to detect, localize and correct errors in handwritten mathematical content. FERMAT spans four key error dimensions - computational, conceptual, notational, and presentation - and comprises over 2,200 handwritten math solutions derived from 609 manually curated problems from grades 7-12 with intentionally introduced perturbations. Using FERMAT we benchmark nine VLMs across three tasks: error detection, localization, and correction. Our results reveal significant shortcomings in current VLMs in reasoning over handwritten text, with Gemini-1.5-Pro achieving the highest error correction rate (77%). We also observed that some models struggle with processing handwritten content, as their accuracy improves when handwritten inputs are replaced with printed text or images. These findings highlight the limitations of current VLMs and reveal new avenues for improvement. We will release FERMAT and all the associated resources in the open-source to drive further research.</abstract>
      <url hash="db52d8dd">2025.acl-long.720</url>
      <bibkey>nath-etal-2025-vision</bibkey>
    </paper>
    <paper id="721">
      <title>Continual Gradient Low-Rank Projection Fine-Tuning for <fixed-case>LLM</fixed-case>s</title>
      <author><first>Chenxu</first><last>Wang</last></author>
      <author><first>Yilin</first><last>Lyu</last></author>
      <author><first>Zicheng</first><last>Sun</last></author>
      <author><first>Liping</first><last>Jing</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <pages>14815-14829</pages>
      <abstract>Continual fine-tuning of Large Language Models (LLMs) is hampered by the trade-off between efficiency and expressiveness. Low-Rank Adaptation (LoRA) offers efficiency but constrains the model’s ability to learn new tasks and transfer knowledge due to its low-rank nature and reliance on explicit parameter constraints. We propose GORP (<tex-math>\underline{\textbf{G}}</tex-math>radient L<tex-math>\underline{\textbf{O}}</tex-math>w <tex-math>\underline{\textbf{R}}</tex-math>ank <tex-math>\underline{\textbf{P}}</tex-math>rojection) for Continual Learning, a novel training strategy that overcomes these limitations by synergistically combining full and low-rank parameters and jointly updating within a unified low-rank gradient subspace. GORP expands the optimization space while preserving efficiency and mitigating catastrophic forgetting. Extensive experiments on continual learning benchmarks demonstrate GORP’s superior performance compared to existing state-of-the-art approaches. Code is available at https://github.com/Wcxwcxw/GORP.</abstract>
      <url hash="12f99fd2">2025.acl-long.721</url>
      <bibkey>wang-etal-2025-continual</bibkey>
    </paper>
    <paper id="722">
      <title>Towards Objective Fine-tuning: How <fixed-case>LLM</fixed-case>s’ Prior Knowledge Causes Potential Poor Calibration?</title>
      <author><first>Ziming</first><last>Wang</last></author>
      <author><first>Zeyu</first><last>Shi</last></author>
      <author><first>Haoyi</first><last>Zhou</last><affiliation>Beihang University</affiliation></author>
      <author><first>Shiqi</first><last>Gao</last></author>
      <author><first>Qingyun</first><last>Sun</last><affiliation>Beihang University</affiliation></author>
      <author><first>Jianxin</first><last>Li</last><affiliation>Beihang University</affiliation></author>
      <pages>14830-14853</pages>
      <abstract>Fine-tuned Large Language Models (LLMs) often demonstrate poor calibration, with their confidence scores misaligned with actual performance. While calibration has been extensively studied in models trained from scratch, the impact of LLMs’ prior knowledge on calibration during fine-tuning remains understudied. Our research reveals that LLMs’ prior knowledge causes potential poor calibration due to the ubiquitous presence of known data in real-world fine-tuning, which appears harmful for calibration. Specifically, data aligned with LLMs’ prior knowledge would induce overconfidence, while new knowledge improves calibration. Our findings expose a tension: LLMs’ encyclopedic knowledge, while enabling task versatility, undermines calibration through unavoidable knowledge overlaps. To address this, we propose CogCalib, a cognition-aware framework that applies targeted learning strategies according to the model’s prior knowledge. Experiments across 7 tasks using 3 LLM families prove that CogCalib significantly improves calibration while maintaining performance, achieving an average 57% reduction in ECE compared to standard fine-tuning in Llama3-8B. These improvements generalize well to out-of-domain tasks, enhancing the objectivity and reliability of domain-specific LLMs, and making them more trustworthy for critical human-AI interaction applications.</abstract>
      <url hash="ef0488a5">2025.acl-long.722</url>
      <bibkey>wang-etal-2025-towards-objective</bibkey>
    </paper>
    <paper id="723">
      <title>Towards Robust <fixed-case>ESG</fixed-case> Analysis Against Greenwashing Risks: Aspect-Action Analysis with Cross-Category Generalization</title>
      <author><first>Keane</first><last>Ong</last></author>
      <author><first>Rui</first><last>Mao</last></author>
      <author><first>Deeksha</first><last>Varshney</last></author>
      <author><first>Erik</first><last>Cambria</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Gianmarco</first><last>Mengaldo</last></author>
      <pages>14854-14879</pages>
      <abstract>Sustainability reports are key for evaluating companies’ environmental, social and governance (ESG) performance. To analyze these reports, NLP approaches can efficiently extract ESG insights at scale. However, even the most advanced NLP methods lack robustness against ESG content that is greenwashed – i.e. sustainability claims that are misleading, exaggerated, and fabricated. Accordingly, existing NLP approaches often extract insights that reflect misleading or exaggerated sustainability claims rather than objective ESG performance. To tackle this issue, we introduce A3CG - **A**spect-**A**ction **A**nalysis with Cross-**C**ategory **G**eneralization, as a novel dataset to improve the robustness of ESG analysis amid the prevalence of greenwashing. By explicitly linking sustainability aspects with their associated actions, A3CG facilitates a more fine-grained and transparent evaluation of sustainability claims, ensuring that insights are grounded in verifiable actions rather than vague or misleading rhetoric. Additionally, A3CG emphasizes cross-category generalization. This ensures robust model performance in aspect-action analysis even when companies change their reports to selectively favor certain sustainability areas. Through experiments on A3CG, we analyze state-of-the-art supervised models and LLMs, uncovering their limitations and outlining key directions for future research.</abstract>
      <url hash="52b127b6">2025.acl-long.723</url>
      <bibkey>ong-etal-2025-towards-robust</bibkey>
    </paper>
    <paper id="724">
      <title><fixed-case>H</fixed-case>idden<fixed-case>D</fixed-case>etect: Detecting Jailbreak Attacks against Multimodal Large Language Models via Monitoring Hidden States</title>
      <author><first>Yilei</first><last>Jiang</last></author>
      <author><first>Xinyan</first><last>Gao</last></author>
      <author><first>Tianshuo</first><last>Peng</last></author>
      <author><first>Yingshui</first><last>Tan</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Xiaoyong</first><last>Zhu</last><affiliation>Alibaba Group and Microsoft</affiliation></author>
      <author><first>Bo</first><last>Zheng</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Xiangyu</first><last>Yue</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>14880-14893</pages>
      <abstract>The integration of additional modalities increases the susceptibility of large vision-language models (LVLMs) to safety risks, such as jailbreak attacks, compared to their language-only counterparts. While existing research primarily focuses on post-hoc alignment techniques, the underlying safety mechanisms within LVLMs remain largely unexplored. In this work , we investigate whether LVLMs inherently encode safety-relevant signals within their internal activations during inference. Our findings reveal that LVLMs exhibit distinct activation patterns when processing unsafe prompts, which can be leveraged to detect and mitigate adversarial inputs without requiring extensive fine-tuning. Building on this insight, we introduce HiddenDetect, a novel tuning-free framework that harnesses internal model activations to enhance safety. Experimental results show that HiddenDetect surpasses state-of-the-art methods in detecting jailbreak attacks against LVLMs. By utilizing intrinsic safety-aware patterns, our method provides an efficient and scalable solution for strengthening LVLM robustness against multimodal threats. Our code and data will be released publicly.</abstract>
      <url hash="e23221ec">2025.acl-long.724</url>
      <bibkey>jiang-etal-2025-hiddendetect</bibkey>
    </paper>
    <paper id="725">
      <title><fixed-case>S</fixed-case>wi<fixed-case>LT</fixed-case>ra-Bench: The <fixed-case>S</fixed-case>wiss Legal Translation Benchmark</title>
      <author><first>Joel</first><last>Niklaus</last><affiliation>Harvey</affiliation></author>
      <author><first>Jakob</first><last>Merane</last><affiliation>ETHZ - ETH Zurich and Université de Lausanne</affiliation></author>
      <author><first>Luka</first><last>Nenadic</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <author><first>Sina</first><last>Ahmadi</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Yingqiang</first><last>Gao</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Cyrill A. H.</first><last>Chevalley</last><affiliation>University of Basel</affiliation></author>
      <author><first>Claude</first><last>Humbel</last></author>
      <author><first>Christophe</first><last>Gösken</last></author>
      <author><first>Lorenzo</first><last>Tanzi</last></author>
      <author><first>Thomas</first><last>Lüthi</last></author>
      <author><first>Stefan</first><last>Palombo</last><affiliation>Harvey</affiliation></author>
      <author><first>Spencer</first><last>Poff</last></author>
      <author><first>Boling</first><last>Yang</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <author><first>Nan</first><last>Wu</last></author>
      <author><first>Matthew</first><last>Guillod</last></author>
      <author><first>Robin</first><last>Mamié</last><affiliation>Federal Supreme Court of Switzerland</affiliation></author>
      <author><first>Daniel</first><last>Brunner</last></author>
      <author><first>Julio</first><last>Pereyra</last><affiliation>Counsel AI Corporation</affiliation></author>
      <author><first>Niko</first><last>Grupen</last></author>
      <pages>14894-14916</pages>
      <abstract>In Switzerland legal translation is uniquely important due to the country’s four official languages and requirements for multilingual legal documentation. However, this process traditionally relies on professionals who must be both legal experts and skilled translators—creating bottlenecks and impacting effective access to justice. To address this challenge, we introduce SwiLTra-Bench, a comprehensive multilingual benchmark of over 180K aligned Swiss legal translation pairs comprising laws, headnotes, and press releases across all Swiss languages along with English, designed to evaluate LLM-based translation systems. Our systematic evaluation reveals that frontier models achieve superior translation performance across all document types, while specialized translation systems excel specifically in laws but under-perform in headnotes. Through rigorous testing and human expert validation, we demonstrate that while fine-tuning open SLMs significantly improves their translation quality, they still lag behind the best zero-shot prompted frontier models such as Claude-3.5-Sonnet. Additionally, we present SwiLTra-Judge, a specialized LLM evaluation system that aligns best with human expert assessments.</abstract>
      <url hash="ed9d9dae">2025.acl-long.725</url>
      <bibkey>niklaus-etal-2025-swiltra</bibkey>
    </paper>
    <paper id="726">
      <title>Two Intermediate Translations Are Better Than One: Fine-tuning <fixed-case>LLM</fixed-case>s for Document-level Translation Refinement</title>
      <author><first>Yichen</first><last>Dong</last></author>
      <author><first>Xinglin</first><last>Lyu</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Junhui</first><last>Li</last><affiliation>Soochow University, China</affiliation></author>
      <author><first>Daimeng</first><last>Wei</last></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Shimin</first><last>Tao</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Hao</first><last>Yang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <pages>14917-14933</pages>
      <abstract>Recent research has shown that large language models (LLMs) can enhance translation quality through self-refinement. In this paper, we build on this idea by extending the refinement from sentence-level to document-level translation, specifically focusing on document-to-document (Doc2Doc) translation refinement. Since sentence-to-sentence (Sent2Sent) and Doc2Doc translation address different aspects of the translation process, we propose fine-tuning LLMs for translation refinement using two intermediate translations, combining the strengths of both Sent2Sent and Doc2Doc. Additionally, recognizing that the quality of intermediate translations varies, we introduce an enhanced fine-tuning method with quality awareness that assigns lower weights to easier translations and higher weights to more difficult ones, enabling the model to focus on challenging translation cases. Experimental results across ten translation tasks with LLaMA-3-8B-Instruct and Mistral-Nemo-Instruct demonstrate the effectiveness of our approach. We will release our code on GitHub.</abstract>
      <url hash="2114485c">2025.acl-long.726</url>
      <bibkey>dong-etal-2025-two</bibkey>
    </paper>
    <paper id="727">
      <title>Circuit Compositions: Exploring Modular Structures in Transformer-Based Language Models</title>
      <author><first>Philipp</first><last>Mondorf</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Sondre</first><last>Wold</last></author>
      <author><first>Barbara</first><last>Plank</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <pages>14934-14955</pages>
      <abstract>A fundamental question in interpretability research is to what extent neural networks, particularly language models, implement reusable functions through subnetworks that can be composed to perform more complex tasks. Recent advances in mechanistic interpretability have made progress in identifying circuits, the minimal computational subgraphs responsible for a model’s behavior on specific tasks. However, most studies focus on identifying circuits for individual tasks without investigating how functionally similar circuits relate to each other. To address this gap, we study the modularity of neural networks by analyzing circuits for highly compositional subtasks within a transformer-based language model. Specifically, given a probabilistic context-free grammar, we identify and compare circuits responsible for ten modular string-edit operations. Our results indicate that functionally similar circuits exhibit both notable node overlap and cross-task faithfulness. Moreover, we demonstrate that the circuits identified can be reused and combined through set operations to represent more complex functional model capabilities.</abstract>
      <url hash="c5005bda">2025.acl-long.727</url>
      <bibkey>mondorf-etal-2025-circuit</bibkey>
    </paper>
    <paper id="728">
      <title>Can <fixed-case>LLM</fixed-case>s Ground when they (Don’t) Know: A Study on Direct and Loaded Political Questions</title>
      <author><first>Clara</first><last>Lachenmaier</last></author>
      <author><first>Judith</first><last>Sieker</last><affiliation>Universität Bielefeld</affiliation></author>
      <author><first>Sina</first><last>Zarrieß</last><affiliation>Bielefeld University</affiliation></author>
      <pages>14956-14975</pages>
      <abstract>Communication among humans relies on conversational grounding, allowing interlocutors to reach mutual understanding even when they do not have perfect knowledge and must resolve discrepancies in each other’s beliefs. This paper investigates how large language models (LLMs) manage common ground in cases where they (don’t) possess knowledge, focusing on facts in the political domain where the risk of misinformation and grounding failure is high. We examine LLMs’ ability to answer direct knowledge questions and loaded questions that presuppose misinformation.We evaluate whether loaded questions lead LLMs to engage in active grounding and correct false user beliefs, in connection to their level of knowledge and their political bias.Our findings highlight significant challenges in LLMs’ ability to engage in grounding and reject false user beliefs, raising concerns about their role in mitigating misinformation in political discourse.</abstract>
      <url hash="abbb051a">2025.acl-long.728</url>
      <bibkey>lachenmaier-etal-2025-llms</bibkey>
    </paper>
    <paper id="729">
      <title><fixed-case>G</fixed-case>raph<fixed-case>C</fixed-case>heck: Breaking Long-Term Text Barriers with Extracted Knowledge Graph-Powered Fact-Checking</title>
      <author><first>Yingjian</first><last>Chen</last><affiliation>Henan Univeristy</affiliation></author>
      <author><first>Haoran</first><last>Liu</last><affiliation>Texas A&amp;M University</affiliation></author>
      <author><first>Yinhong</first><last>Liu</last></author>
      <author><first>Jinxiang</first><last>Xie</last></author>
      <author><first>Rui</first><last>Yang</last></author>
      <author><first>Han</first><last>Yuan</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Yanran</first><last>Fu</last></author>
      <author><first>Peng Yuan</first><last>Zhou</last><affiliation>Aarhus University</affiliation></author>
      <author><first>Qingyu</first><last>Chen</last><affiliation>Yale University</affiliation></author>
      <author><first>James</first><last>Caverlee</last><affiliation>Texas A&amp;M University - College Station and Google</affiliation></author>
      <author><first>Irene</first><last>Li</last></author>
      <pages>14976-14995</pages>
      <abstract>Large language models (LLMs) are widely used, but they often generate subtle factual errors, especially in long-form text. These errors are fatal in some specialized domains such as medicine. Existing fact-checking with grounding documents methods face two main challenges: (1) they struggle to understand complex multihop relations in long documents, often overlooking subtle factual errors; (2) most specialized methods rely on pairwise comparisons, requiring multiple model calls, leading to high resource and computational costs. To address these challenges, we propose GraphCheck, a fact-checking framework that uses extracted knowledge graphs to enhance text representation. Graph Neural Networks further process these graphs as a soft prompt, enabling LLMs to incorporate structured knowledge more effectively. Enhanced with graph-based reasoning, GraphCheck captures multihop reasoning chains that are often overlooked by existing methods, enabling precise and efficient fact-checking in a single inference call. Experimental results on seven benchmarks spanning both general and medical domains demonstrate up to a 7.1% overall improvement over baseline models. Notably, GraphCheck outperforms existing specialized fact-checkers and achieves comparable performance with state-of-the-art LLMs, such as DeepSeek-V3 and OpenAI-o1, with significantly fewer parameters.</abstract>
      <url hash="97c28748">2025.acl-long.729</url>
      <bibkey>chen-etal-2025-graphcheck</bibkey>
    </paper>
    <paper id="730">
      <title><fixed-case>SCULPT</fixed-case>: Systematic Tuning of Long Prompts</title>
      <author><first>Shanu</first><last>Kumar</last><affiliation>Microsoft</affiliation></author>
      <author><first>Akhila Yesantarao</first><last>Venkata</last><affiliation>Microsoft</affiliation></author>
      <author><first>Shubhanshu</first><last>Khandelwal</last><affiliation>Microsoft</affiliation></author>
      <author><first>Bishal</first><last>Santra</last><affiliation>Microsoft</affiliation></author>
      <author><first>Parag</first><last>Agrawal</last><affiliation>Microsoft</affiliation></author>
      <author><first>Manish</first><last>Gupta</last><affiliation>Microsoft</affiliation></author>
      <pages>14996-15029</pages>
      <abstract>Prompt optimization is essential for effective utilization of large language models (LLMs) across diverse tasks. While existing optimization methods are effective in optimizing short prompts, they struggle with longer, more complex ones, often risking information loss and being sensitive to small perturbations. To address these challenges, we propose SCULPT (Systematic Tuning of Long Prompts), a framework that treats prompt optimization as a hierarchical tree refinement problem. SCULPT represents prompts as tree structures, enabling targeted modifications while preserving contextual integrity. It employs a Critic-Actor framework that generates reflections and applies actions to refine the prompt. Evaluations demonstrate SCULPT’s effectiveness on long prompts, its robustness to adversarial perturbations, and its ability to generate high-performing prompts even without any initial human-written prompt. Compared to existing state of the art methods, SCULPT consistently improves LLM performance by preserving essential task information while applying structured refinements. Both qualitative and quantitative analyses show that SCULPT produces more stable and interpretable prompt modifications, ensuring better generalization across tasks.</abstract>
      <url hash="93b9492d">2025.acl-long.730</url>
      <bibkey>kumar-etal-2025-sculpt</bibkey>
    </paper>
    <paper id="731">
      <title>Crab: A Novel Configurable Role-Playing <fixed-case>LLM</fixed-case> with Assessing Benchmark</title>
      <author><first>Kai</first><last>He</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Yucheng</first><last>Huang</last></author>
      <author><first>Wenqing</first><last>Wang</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Delong</first><last>Ran</last></author>
      <author><first>Dongming</first><last>Sheng</last><affiliation>Interactive Entertainment Group, Tencent Inc.</affiliation></author>
      <author><first>Junxuan</first><last>Huang</last><affiliation>Tencent</affiliation></author>
      <author><first>Qika</first><last>Lin</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Jiaxing</first><last>Xu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Wenqiang</first><last>Liu</last><affiliation>Tencent</affiliation></author>
      <author><first>Mengling</first><last>Feng</last><affiliation>National University of Singapore</affiliation></author>
      <pages>15030-15052</pages>
      <abstract>This study introduces Crab, a novel Configurable Role-Playing (RP) LLM with Assessing Benchmark, which consists of Role-Centric Dataset Curation, Persona-Embodying LLM Construction, and Comprehensive Benchmark Creation for RP dialogue generation. Distinct from traditional RP models that employ only several preset roles, Crab enables dynamic configuration of desired roles, thereby enhancing related flexibility and adaptability. To effectively train RP-LLMs, we curated the largest RP training dataset. The dataset provides a detailed role overview for each dialogue, including character profile, conversation scenario, and tagged topic, capturing a broad range of role-based behaviors, emotions, and interactions. We also noticed that current benchmarks lack both proper evaluation standards and methods. Thus, to validate RP-LLMs’ effectiveness, we introduced a new benchmark containing an evaluation standard, a test dataset with manual annotations, and a reward model RoleRM designed to automatically assess specific aspects of RP while aligning with human perception. Sufficient experiments reveal that RoleRM significantly outperforms ChatGPT and other evaluation methods in conducting fine-grained evaluations of RP. Also, RP-LLMs powered by Crab demonstrate superior performance across various fine-grained aspects.</abstract>
      <url hash="6339096a">2025.acl-long.731</url>
      <bibkey>he-etal-2025-crab</bibkey>
    </paper>
    <paper id="732">
      <title><fixed-case>C</fixed-case>hinese <fixed-case>S</fixed-case>afety<fixed-case>QA</fixed-case>: A Safety Short-form Factuality Benchmark for Large Language Models</title>
      <author><first>Yingshui</first><last>Tan</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Boren</first><last>Zheng</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Baihui</first><last>Zheng</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Kerui</first><last>Cao</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Huiyun</first><last>Jing</last><affiliation>China Academy of Information and Communications Technology</affiliation></author>
      <author><first>Jincheng</first><last>Wei</last><affiliation>China Academy of Information and Communications Technology</affiliation></author>
      <author><first>Jiaheng</first><last>Liu</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Yancheng</first><last>He</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Wenbo</first><last>Su</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Xiaoyong</first><last>Zhu</last><affiliation>Alibaba Group and Microsoft</affiliation></author>
      <author><first>Bo</first><last>Zheng</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Kaifu</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <pages>15053-15076</pages>
      <abstract>With the rapid advancement of Large Language Models (LLMs), significant safety concerns have emerged. Fundamentally, the safety of large language models is closely linked to the accuracy, comprehensiveness, and clarity of their understanding of safety knowledge, particularly in domains such as law, policy and ethics. This factuality ability is crucial in determining whether these models can be deployed and applied safely and compliantly within specific regions. To address these challenges and better evaluate the factuality ability of LLMs to answer short question, we introduce the Chinese SafetyQA benchmark. Chinese SafetyQA has several properties (i.e., Chinese, Diverse, High-quality, Static, Easy-to-evaluate, safety-related, harmless). Based on Chinese SafetyQA, we perform a comprehensive evaluation on the factuality abilities of existing LLMs and analyze how these capabilities relate to LLM abilities, e.g., RAG ability and robustness against attacks.</abstract>
      <url hash="a3a7644c">2025.acl-long.732</url>
      <bibkey>tan-etal-2025-chinese</bibkey>
    </paper>
    <paper id="733">
      <title><fixed-case>TRIDENT</fixed-case>: Enhancing Large Language Model Safety with Tri-Dimensional Diversified Red-Teaming Data Synthesis</title>
      <author><first>Xiaorui</first><last>Wu</last></author>
      <author><first>Xiaofeng</first><last>Mao</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Fei</first><last>Li</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Xin</first><last>Zhang</last><affiliation>Ant International</affiliation></author>
      <author><first>Xuanhong</first><last>Li</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Chong</first><last>Teng</last></author>
      <author><first>Donghong</first><last>Ji</last></author>
      <author><first>Zhuang</first><last>Li</last><affiliation>Royal Melbourne Institute of Technology</affiliation></author>
      <pages>15077-15099</pages>
      <abstract>Large Language Models (LLMs) excel in various natural language processing tasks but remain vulnerable to generating harmful content or being exploited for malicious purposes. Although safety alignment datasets have been introduced to mitigate such risks through supervised fine-tuning (SFT), these datasets often lack comprehensive risk coverage. Most existing datasets focus primarily on lexical diversity while neglecting other critical dimensions. To address this limitation, we propose a novel analysis framework to systematically measure the risk coverage of alignment datasets across three essential dimensions: Lexical Diversity, Malicious Intent, and Jailbreak Tactics. We further introduce TRIDENT, an automated pipeline that leverages persona-based, zero-shot LLM generation to produce diverse and comprehensive instructions spanning these dimensions. Each harmful instruction is paired with an ethically aligned response, resulting in two datasets: TRIDENT-Core, comprising 26,311 examples, and TRIDENT-Edge, with 18,773 examples. Fine-tuning Llama 3.1-8B on TRIDENT-Edge demonstrates substantial improvements, achieving an average 14.29% reduction in Harm Score, and a 20% decrease in Attack Success Rate compared to the best-performing baseline model fine-tuned on the WildBreak dataset.</abstract>
      <url hash="c3298556">2025.acl-long.733</url>
      <bibkey>wu-etal-2025-trident</bibkey>
    </paper>
    <paper id="734">
      <title>Cross-Lingual Optimization for Language Transfer in Large Language Models</title>
      <author><first>Jungseob</first><last>Lee</last><affiliation>Korea University</affiliation></author>
      <author><first>Seongtae</first><last>Hong</last><affiliation>Korea University</affiliation></author>
      <author><first>Hyeonseok</first><last>Moon</last><affiliation>Korea University</affiliation></author>
      <author><first>Heuiseok</first><last>Lim</last></author>
      <pages>15100-15119</pages>
      <abstract>Adapting large language models to other languages typically employs supervised fine-tuning (SFT) as a standard approach. However, it often suffers from an overemphasis on English performance, a phenomenon that is especially pronounced in data-constrained environments. To overcome these challenges, we propose Cross-Lingual Optimization (CLO) that efficiently transfers an English-centric LLM to a target language while preserving its English capabilities. CLO utilizes publicly available English SFT data and a translation model to enable cross-lingual transfer. We conduct experiments using five models on six languages, each possessing varying levels of resource. Our results show that CLO consistently outperforms SFT in both acquiring target language proficiency and maintaining English performance. Remarkably, in low-resource languages, CLO with only 3,200 samples surpasses SFT with 6,400 samples, demonstrating that CLO can achieve better performance with less data. Furthermore, we find that SFT is particularly sensitive to data quantity in medium and low-resource languages, whereas CLO remains robust. Our comprehensive analysis emphasizes the limitations of SFT and incorporates additional training strategies in CLO to enhance efficiency.</abstract>
      <url hash="09d08423">2025.acl-long.734</url>
      <bibkey>lee-etal-2025-cross</bibkey>
    </paper>
    <paper id="735">
      <title><fixed-case>CART</fixed-case>: A Generative Cross-Modal Retrieval Framework With Coarse-To-Fine Semantic Modeling</title>
      <author><first>Minghui</first><last>Fang</last></author>
      <author><first>Shengpeng</first><last>Ji</last></author>
      <author><first>Jialong</first><last>Zuo</last></author>
      <author><first>Hai</first><last>Huang</last></author>
      <author><first>Yan</first><last>Xia</last></author>
      <author><first>Jieming</first><last>Zhu</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Xize</first><last>Cheng</last></author>
      <author><first>Xiaoda</first><last>Yang</last></author>
      <author><first>Wenrui</first><last>Liu</last></author>
      <author><first>Gang</first><last>Wang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Zhenhua</first><last>Dong</last></author>
      <author><first>Zhou</first><last>Zhao</last><affiliation>Zhejiang University and Zhejiang University</affiliation></author>
      <pages>15120-15133</pages>
      <abstract>Cross-modal retrieval aims to search for instances, which are semantically related to the query through the interaction of different modal data. Traditional solutions utilize a single-tower or dual-tower framework to explicitly compute the score between queries and candidates, which is challenged by training cost and inference latency with large-scale data. Inspired by the remarkable performance and efficiency of generative models, we propose a generative cross-modal retrieval framework (CART) based on coarse-to-fine semantic modeling, which assigns identifiers to each candidate and treats the generating identifier as the retrieval target. Specifically, we explore an effective coarse-to-fine scheme, combining K-Means and RQ-VAE to discretize multimodal data into token sequences that support autoregressive generation. Further, considering the lack of explicit interaction between queries and candidates, we propose a feature fusion strategy to align their semantics. Extensive experiments demonstrate the effectiveness of the strategies in the CART, achieving excellent results in both retrieval performance and efficiency.</abstract>
      <url hash="8192aae2">2025.acl-long.735</url>
      <bibkey>fang-etal-2025-cart</bibkey>
    </paper>
    <paper id="736">
      <title><fixed-case>MMMU</fixed-case>-Pro: A More Robust Multi-discipline Multimodal Understanding Benchmark</title>
      <author><first>Xiang</first><last>Yue</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Tianyu</first><last>Zheng</last></author>
      <author><first>Yuansheng</first><last>Ni</last></author>
      <author><first>Yubo</first><last>Wang</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Kai</first><last>Zhang</last></author>
      <author><first>Shengbang</first><last>Tong</last><affiliation>New York University</affiliation></author>
      <author><first>Yuxuan</first><last>Sun</last><affiliation>Westlake University and Zhejiang University</affiliation></author>
      <author><first>Botao</first><last>Yu</last><affiliation>The Ohio State University</affiliation></author>
      <author><first>Ge</first><last>Zhang</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Huan</first><last>Sun</last><affiliation>The Ohio State University, Columbus</affiliation></author>
      <author><first>Yu</first><last>Su</last><affiliation>Ohio State University</affiliation></author>
      <author><first>Wenhu</first><last>Chen</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Graham</first><last>Neubig</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>15134-15186</pages>
      <abstract>This paper introduces MMMU-Pro, a robust version of the Massive Multi-discipline Multimodal Understanding and Reasoning (MMMU) benchmark. MMMU-Pro rigorously assesses multimodal models’ true understanding and reasoning capabilities through a three-step process based on MMMU: (1) filtering out questions answerable by text-only models, (2) augmenting candidate options, and (3) introducing a vision-only input setting where questions are embedded within images. This setting challenges AI to truly “see” and “read” simultaneously, testing <i>a core human cognitive skill of seamlessly integrating visual and textual information</i>. Results show that model performance is substantially lower on MMMU-Pro than on MMMU, ranging from 16.8% to 26.9% across models. We explore the impact of OCR prompts and Chain of Thought (CoT) reasoning, finding that OCR prompts have minimal effect while CoT generally improves performance. MMMU-Pro provides a more rigorous evaluation tool, closely mimicking real-world scenarios and offering valuable directions for future multimodal research.</abstract>
      <url hash="30d60f3c">2025.acl-long.736</url>
      <bibkey>yue-etal-2025-mmmu</bibkey>
    </paper>
    <paper id="737">
      <title>Cheems: A Practical Guidance for Building and Evaluating <fixed-case>C</fixed-case>hinese Reward Models from Scratch</title>
      <author><first>Xueru</first><last>Wen</last></author>
      <author><first>Jie</first><last>Lou</last></author>
      <author><first>Zichao</first><last>Li</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yaojie</first><last>Lu</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>XingYu</first><last>XingYu</last><affiliation>Xiaohongshu</affiliation></author>
      <author><first>Yuqiu</first><last>Ji</last></author>
      <author><first>Guohai</first><last>Xu</last></author>
      <author><first>Hongyu</first><last>Lin</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Ben</first><last>He</last></author>
      <author><first>Xianpei</first><last>Han</last><affiliation>Institute of Software, CAS</affiliation></author>
      <author><first>Le</first><last>Sun</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Debing</first><last>Zhang</last></author>
      <pages>15187-15211</pages>
      <abstract>Reward models (RMs) are crucial for aligning large language models (LLMs) with human preferences. However, most RM research is centered on English and relies heavily on synthetic resources, which leads to limited and less reliable datasets and benchmarks for Chinese. To address this gap, we introduce CheemsBench, a fully human-annotated RM evaluation benchmark within Chinese contexts, and CheemsPreference, a large-scale and diverse preference dataset annotated through human-machine collaboration to support Chinese RM training. We systematically evaluate open-source discriminative and generative RMs on CheemsBench and observe significant limitations in their ability to capture human preferences in Chinese scenarios. Additionally, based on CheemsPreference, we construct an RM that achieves state-of-the-art performance on CheemsBench, demonstrating the necessity of human supervision in RM training. Our findings reveal that scaled AI-generated data struggles to fully capture human preferences, emphasizing the importance of high-quality human supervision in RM development.</abstract>
      <url hash="28aa532e">2025.acl-long.737</url>
      <bibkey>wen-etal-2025-cheems</bibkey>
    </paper>
    <paper id="738">
      <title>Why Safeguarded Ships Run Aground? Aligned Large Language Models’ Safety Mechanisms Tend to Be Anchored in The Template Region</title>
      <author><first>Chak Tou</first><last>Leong</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Qingyu</first><last>Yin</last></author>
      <author><first>Jian</first><last>Wang</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Wenjie</first><last>Li</last><affiliation>The Hong Kong Polytechnic University, The Hong Kong Polytechnic University</affiliation></author>
      <pages>15212-15229</pages>
      <abstract>The safety alignment of large language models (LLMs) remains vulnerable, as their initial behavior can be easily jailbroken by even relatively simple attacks. Since infilling a fixed template between the input instruction and initial model output is a common practice for existing LLMs, we hypothesize that this template is a key factor behind their vulnerabilities: LLMs’ safety-related decision-making overly relies on the aggregated information from the template region, which largely influences these models’ safety behavior. We refer to this issue as <i>template-anchored safety alignment</i>. In this paper, we conduct extensive experiments and verify that template-anchored safety alignment is widespread across various aligned LLMs. Our mechanistic analyses demonstrate how it leads to models’ susceptibility when encountering inference-time jailbreak attacks. Furthermore, we show that detaching safety mechanisms from the template region is promising in mitigating vulnerabilities to jailbreak attacks. We encourage future research to develop more robust safety alignment techniques that reduce reliance on the template region.</abstract>
      <url hash="7b7850e8">2025.acl-long.738</url>
      <bibkey>leong-etal-2025-safeguarded</bibkey>
    </paper>
    <paper id="739">
      <title><fixed-case>LL</fixed-case>a<fixed-case>VA</fixed-case> Steering: Visual Instruction Tuning with 500x Fewer Parameters through Modality Linear Representation-Steering</title>
      <author><first>Jinhe</first><last>Bi</last></author>
      <author><first>Yujun</first><last>Wang</last></author>
      <author><first>Haokun</first><last>Chen</last></author>
      <author><first>Xun</first><last>Xiao</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Artur</first><last>Hecker</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Volker</first><last>Tresp</last><affiliation>Ludwig Maximilian University of Munich and Siemens Corporate Research</affiliation></author>
      <author><first>Yunpu</first><last>Ma</last><affiliation>Ludwig-Maximilians-Universität München and Siemens Corporate Research</affiliation></author>
      <pages>15230-15250</pages>
      <abstract>Multimodal Large Language Models (MLLMs) enhance visual tasks by integrating visual representations into large language models (LLMs). The textual modality, inherited from LLMs, enables instruction following and in-context learning, while the visual modality boosts downstream task performance through rich semantic content, spatial information, and grounding capabilities. These modalities work synergistically across various visual tasks. Our research reveals a persistent imbalance between these modalities, with text often dominating output generation during visual instruction tuning, regardless of using full or parameter-efficient fine-tuning (PEFT). We found that re-balancing these modalities can significantly reduce trainable parameters, inspiring further optimization of visual instruction tuning. To this end, we introduce Modality Linear Representation-Steering (MoReS), which re-balances intrinsic modalities by steering visual representations through linear transformations in the visual subspace across each model layer. We validated our approach by developing LLaVA Steering, a suite of models using MoReS. Results show that LLaVA Steering requires, on average, 500 times fewer trainable parameters than LoRA while maintaining comparable performance across three visual benchmarks and eight visual question-answering tasks. Finally, we introduce the LLaVA Steering Factory, a platform that enables rapid customization of MLLMs with a component-based architecture, seamlessly integrating state-of-the-art models and evaluating intrinsic modality imbalance. This open-source project facilitates a deeper understanding of MLLMs within the research community.</abstract>
      <url hash="67a63b96">2025.acl-long.739</url>
      <bibkey>bi-etal-2025-llava</bibkey>
    </paper>
    <paper id="740">
      <title>Efficient Long Context Language Model Retrieval with Compression</title>
      <author><first>Minju</first><last>Seo</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Jinheon</first><last>Baek</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Seongyun</first><last>Lee</last></author>
      <author><first>Sung Ju</first><last>Hwang</last><affiliation>Korea Advanced Institute of Science and Technology and AITRICS</affiliation></author>
      <pages>15251-15268</pages>
      <abstract>Long Context Language Models (LCLMs) have emerged as a new paradigm to perform Information Retrieval (IR), which enables the direct ingestion and retrieval of information by processing an entire corpus in their single context, showcasing the potential to surpass traditional sparse and dense retrieval methods. However, processing a large number of passages within in-context for retrieval is computationally expensive, and handling their representations during inference further exacerbates the processing time; thus, we aim to make LCLM retrieval more efficient and potentially more effective with passage compression. Specifically, we propose a new compression approach tailored for LCLM retrieval, which is trained to maximize the retrieval performance while minimizing the length of the compressed passages. To accomplish this, we generate the synthetic data, where compressed passages are automatically created and labeled as chosen or rejected according to their retrieval success for a given query, and we train the proposed Compression model for Long context Retrieval (CoLoR) with this data via preference optimization while adding the length regularization loss on top of it to enforce brevity. Through extensive experiments on 9 datasets, we show that CoLoR improves the retrieval performance by 6% while compressing the in-context size by a factor of 1.91. Our code is available at: https://github.com/going-doer/CoLoR.</abstract>
      <url hash="30f3d2ae">2025.acl-long.740</url>
      <bibkey>seo-etal-2025-efficient</bibkey>
    </paper>
    <paper id="741">
      <title>Ontology-Guided Reverse Thinking Makes Large Language Models Stronger on Knowledge Graph Question Answering</title>
      <author><first>Runxuan</first><last>Liu</last></author>
      <author><first>Luobei</first><last>Luobei</last></author>
      <author><first>Jiaqi</first><last>Li</last></author>
      <author><first>Baoxin</first><last>Wang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Ming</first><last>Liu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Dayong</first><last>Wu</last></author>
      <author><first>Shijin</first><last>Wang</last><affiliation>State Key Laboratory of Cognitive Intelligence</affiliation></author>
      <author><first>Bing</first><last>Qin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>15269-15284</pages>
      <abstract>Large language models (LLMs) have shown remarkable capabilities in natural language processing. However, in knowledge graph question answering tasks (KGQA), there remains the issue of answering questions that require multi-hop reasoning. Existing methods rely on entity vector matching, but the purpose of the question is abstract and difficult to match with specific entities. As a result, it is difficult to establish reasoning paths to the purpose, which leads to information loss and redundancy. To address this issue, inspired by human reverse thinking, we propose Ontology-Guided Reverse Thinking (ORT), a novel framework that constructs reasoning paths from purposes back to conditions. ORT operates in three key phases: (1) using LLM to extract purpose labels and condition labels, (2) constructing label reasoning paths based on the KG ontology, and (3) using the label reasoning paths to guide knowledge retrieval. Experiments on the WebQSP and CWQ datasets show that ORT achieves state-of-the-art performance and significantly enhances the capability of LLMs for KGQA.</abstract>
      <url hash="7e6debae">2025.acl-long.741</url>
      <bibkey>liu-etal-2025-ontology</bibkey>
    </paper>
    <paper id="742">
      <title>Towards Omni-<fixed-case>RAG</fixed-case>: Comprehensive Retrieval-Augmented Generation for Large Language Models in Medical Applications</title>
      <author><first>Zhe</first><last>Chen</last></author>
      <author><first>Yusheng</first><last>Liao</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Shuyang</first><last>Jiang</last></author>
      <author><first>Pingjie</first><last>Wang</last></author>
      <author><first>YiQiu</first><last>Guo</last></author>
      <author><first>Yanfeng</first><last>Wang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Yu</first><last>Wang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>15285-15309</pages>
      <abstract>Large language models hold promise for addressing medical challenges, such as medical diagnosis reasoning, research knowledge acquisition, clinical decision-making, and consumer health inquiry support. However, they often generate hallucinations due to limited medical knowledge. Incorporating external knowledge is therefore critical, which necessitates multi-source knowledge acquisition. We address this challenge by framing it as a source planning problem, which is to formulate context-appropriate queries tailored to the attributes of diverse sources. Existing approaches either overlook source planning or fail to achieve it effectively due to misalignment between the model’s expectation of the sources and their actual content. To bridge this gap, we present MedOmniKB, a repository comprising multigenre and multi-structured medical knowledge sources. Leveraging these sources, we propose the Source Planning Optimisation method, which enhances multi-source utilisation. Our approach involves enabling an expert model to explore and evaluate potential plans while training a smaller model to learn source alignment. Experimental results demonstrate that our method substantially improves multi-source planning performance, enabling the optimised small model to achieve state-of-the-art results in leveraging diverse medical knowledge sources.</abstract>
      <url hash="fe74a051">2025.acl-long.742</url>
      <bibkey>chen-etal-2025-towards-omni</bibkey>
    </paper>
    <paper id="743">
      <title>Predicting Turn-Taking and Backchannel in Human-Machine Conversations Using Linguistic, Acoustic, and Visual Signals</title>
      <author><first>Yuxin</first><last>Lin</last><affiliation>Xiamen University</affiliation></author>
      <author><first>Yinglin</first><last>Zheng</last><affiliation>Xiamen University</affiliation></author>
      <author><first>Ming</first><last>Zeng</last><affiliation>Xiamen University</affiliation></author>
      <author><first>Wangzheng</first><last>Shi</last></author>
      <pages>15310-15322</pages>
      <abstract>This paper addresses the gap in predicting turn-taking and backchannel actions in human-machine conversations using multi-modal signals (linguistic, acoustic, and visual). To overcome the limitation of existing datasets, we propose an automatic data collection pipeline that allows us to collect and annotate over 210 hours of human conversation videos. From this, we construct a Multi-Modal Face-to-Face (MM-F2F) human conversation dataset, including over 1.5M words and corresponding turn-taking and backchannel annotations from approximately 20M frames. Additionally, we present an end-to-end framework that predicts the probability of turn-taking and backchannel actions from multi-modal signals. The proposed model emphasizes the interrelation between modalities and supports any combination of text, audio, and video inputs, making it adaptable to a variety of realistic scenarios. Our experiments show that our approach achieves state-of-the-art performance on turn-taking and backchannel prediction tasks, achieving a 10% increase in F1-score on turn-taking and a 33% increase on backchannel prediction. Our dataset and code are publicly available online to ease of subsequent research.</abstract>
      <url hash="986f7675">2025.acl-long.743</url>
      <bibkey>lin-etal-2025-predicting</bibkey>
    </paper>
    <paper id="744">
      <title>A New Formulation of <fixed-case>Z</fixed-case>ipf’s Meaning-Frequency Law through Contextual Diversity</title>
      <author><first>Ryo</first><last>Nagata</last><affiliation>RIKEN and Konan University</affiliation></author>
      <author><first>Kumiko</first><last>Tanaka-Ishii</last><affiliation>Waseda University</affiliation></author>
      <pages>15323-15335</pages>
      <abstract>This paper proposes formulating Zipf’s meaning-frequency law, the power law between word frequency and the number of meanings, as a relationship between word frequency and contextual diversity. The proposed formulation quantifies meaning counts as contextual diversity, which is based on the directions of contextualized word vectors obtained from a Language Model (LM). This formulation gives a new interpretation to the law and also enables us to examine it for a wider variety of words and corpora than previous studies have explored. In addition, this paper shows that the law becomes unobservable when the size of the LM used is small and that autoregressive LMs require much more parameters than masked LMs to be able to observe the law.</abstract>
      <url hash="fb729b07">2025.acl-long.744</url>
      <bibkey>nagata-tanaka-ishii-2025-new</bibkey>
    </paper>
    <paper id="745">
      <title>The Mirage of Model Editing: Revisiting Evaluation in the Wild</title>
      <author><first>Wanli</first><last>Yang</last></author>
      <author><first>Fei</first><last>Sun</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jiajun</first><last>Tan</last></author>
      <author><first>Xinyu</first><last>Ma</last><affiliation>Baidu</affiliation></author>
      <author><first>Qi</first><last>Cao</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences, China</affiliation></author>
      <author><first>Dawei</first><last>Yin</last><affiliation>Baidu</affiliation></author>
      <author><first>Huawei</first><last>Shen</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xueqi</first><last>Cheng</last><affiliation>Institute of Computing Technology, Chinese Academy</affiliation></author>
      <pages>15336-15354</pages>
      <abstract>Despite near-perfect results reported in the literature, the effectiveness of model editing in real-world applications remains unclear. To bridge this gap, we introduce QAEdit, a new benchmark aligned with widely used question answering (QA) datasets, and WILD, a task-agnostic evaluation framework designed to better reflect real-world usage of model editing. Our single editing experiments show that current editing methods perform substantially worse than previously reported (38.5% vs. 96.8%). We demonstrate that it stems from issues in the synthetic evaluation practices of prior work. Among them, the most severe is the use of teacher forcing during testing, which leaks both content and length of the ground truth, leading to overestimated performance. Furthermore, we simulate practical deployment by sequential editing, revealing that current approaches fail drastically with only 1000 edits. This work calls for a shift in model editing research toward rigorous evaluation and the development of robust, scalable methods that can reliably update knowledge in LLMs for real-world use.</abstract>
      <url hash="e597f9dc">2025.acl-long.745</url>
      <bibkey>yang-etal-2025-mirage</bibkey>
    </paper>
    <paper id="746">
      <title><fixed-case>LAQ</fixed-case>uer: Localized Attribution Queries in Content-grounded Generation</title>
      <author><first>Eran</first><last>Hirsch</last></author>
      <author><first>Aviv</first><last>Slobodkin</last></author>
      <author><first>David</first><last>Wan</last><affiliation>Department of Computer Science, University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Elias</first><last>Stengel-Eskin</last></author>
      <author><first>Mohit</first><last>Bansal</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Ido</first><last>Dagan</last><affiliation>Bar-Ilan University</affiliation></author>
      <pages>15355-15370</pages>
      <abstract>Grounded text generation models often produce content that deviates from their source material, requiring user verification to ensure accuracy. Existing attribution methods associate entire sentences with source documents, which can be overwhelming for users seeking to fact-check specific claims. In contrast, existing sub-sentence attribution methods may be more precise but fail to align with users’ interests. In light of these limitations, we introduce Localized Attribution Queries (LAQuer), a new task that localizes selected spans of generated output to their corresponding source spans, allowing fine-grained and user-directed attribution. We compare two approaches for the LAQuer task, including prompting large language models (LLMs) and leveraging LLM internal representations. We then explore a modeling framework that extends existing attributed text generation methods to LAQuer. We evaluate this framework across two grounded text generation tasks: Multi-document Summarization (MDS) and Long-form Question Answering (LFQA). Our findings show that LAQuer methods significantly reduce the length of the attributed text. Our contributions include: (1) proposing the LAQuer task to enhance attribution usability, (2) suggesting a modeling framework and benchmarking multiple baselines, and (3) proposing a new evaluation setting to promote future research on localized attribution in content-grounded generation.</abstract>
      <url hash="a38dc833">2025.acl-long.746</url>
      <bibkey>hirsch-etal-2025-laquer</bibkey>
    </paper>
    <paper id="747">
      <title><fixed-case>EPO</fixed-case>: Explicit Policy Optimization for Strategic Reasoning in <fixed-case>LLM</fixed-case>s via Reinforcement Learning</title>
      <author><first>Xiaoqian</first><last>Liu</last></author>
      <author><first>Ke</first><last>Wang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yongbin</first><last>Li</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yuchuan</first><last>Wu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Wentao</first><last>Ma</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Aobo</first><last>Kong</last></author>
      <author><first>Fei</first><last>Huang</last><affiliation>Alibaba Group US</affiliation></author>
      <author><first>Jianbin</first><last>Jiao</last><affiliation>University of the Chinese Academy of Sciences</affiliation></author>
      <author><first>Junge</first><last>Zhang</last></author>
      <pages>15371-15396</pages>
      <abstract>Large Language Models (LLMs) have shown impressive reasoning capabilities in well-defined problems with clear solutions, such as mathematics and coding. However, they still struggle with complex real-world scenarios like business negotiations, which require strategic reasoning—an ability to navigate dynamic environments and align long-term goals amidst uncertainty.Existing methods for strategic reasoning face challenges in adaptability, scalability, and transferring strategies to new contexts.To address these issues, we propose explicit policy optimization (*EPO*) for strategic reasoning, featuring an LLM that provides strategies in open-ended action space and can be plugged into arbitrary LLM agents to motivate goal-directed behavior.To improve adaptability and policy transferability, we train the strategic reasoning model via multi-turn reinforcement learning (RL), utilizing process rewards and iterative self-play.Experiments across social and physical domains demonstrate *EPO*’s ability of long-term goal alignment through enhanced strategic reasoning, achieving state-of-the-art performance on social dialogue and web navigation tasks. Our findings reveal various collaborative reasoning mechanisms emergent in *EPO* and its effectiveness in generating novel strategies, underscoring its potential for strategic reasoning in real-world applications. Code and data are available at [https://github.com/lxqpku/EPO](https://github.com/lxqpku/EPO).</abstract>
      <url hash="efce3333">2025.acl-long.747</url>
      <bibkey>liu-etal-2025-epo</bibkey>
    </paper>
    <paper id="748">
      <title><fixed-case>DCG</fixed-case>-<fixed-case>SQL</fixed-case>: Enhancing In-Context Learning for Text-to-<fixed-case>SQL</fixed-case> with Deep Contextual Schema Link Graph</title>
      <author><first>Jihyung</first><last>Lee</last></author>
      <author><first>Jin-Seop</first><last>Lee</last><affiliation>Sungkyunkwan University</affiliation></author>
      <author><first>Jaehoon</first><last>Lee</last><affiliation>Sung Kyun Kwan University</affiliation></author>
      <author><first>YunSeok</first><last>Choi</last><affiliation>SungKyunKwan University</affiliation></author>
      <author><first>Jee-Hyong</first><last>Lee</last><affiliation>Sungkyunkwan University</affiliation></author>
      <pages>15397-15412</pages>
      <abstract>Text-to-SQL, which translates a natural language question into an SQL query, has advanced with in-context learning of Large Language Models (LLMs). However, existing methods show little improvement in performance compared to randomly chosen demonstrations, and significant performance drops when smaller LLMs (e.g., Llama 3.1-8B) are used. This indicates that these methods heavily rely on the intrinsic capabilities of hyper-scaled LLMs, rather than effectively retrieving useful demonstrations. In this paper, we propose a novel approach for effectively retrieving demonstrations and generating SQL queries. We construct a Deep Contextual Schema Link Graph, which contains key information and semantic relationship between a question and its database schema items. This graph-based structure enables effective representation of Text-to-SQL samples and retrieval of useful demonstrations for in-context learning. Experimental results on the Spider benchmark demonstrate the effectiveness of our approach, showing consistent improvements in SQL generation performance and efficiency across both hyper-scaled LLMs and small LLMs. The code is available at https://github.com/jjklle/DCG-SQL.</abstract>
      <url hash="b959b041">2025.acl-long.748</url>
      <bibkey>lee-etal-2025-dcg</bibkey>
    </paper>
    <paper id="749">
      <title><fixed-case>P</fixed-case>re<fixed-case>P</fixed-case>-<fixed-case>OCR</fixed-case>: A Complete Pipeline for Document Image Restoration and Enhanced <fixed-case>OCR</fixed-case> Accuracy</title>
      <author><first>Shuhao</first><last>Guan</last></author>
      <author><first>Moule</first><last>Lin</last></author>
      <author><first>Cheng</first><last>Xu</last></author>
      <author><first>Xinyi</first><last>Liu</last></author>
      <author><first>Jinman</first><last>Zhao</last></author>
      <author><first>Jiexin</first><last>Fan</last></author>
      <author><first>Qi</first><last>Xu</last></author>
      <author><first>Derek</first><last>Greene</last><affiliation>University College Dublin</affiliation></author>
      <pages>15413-15425</pages>
      <abstract>This paper introduces PreP-OCR, a two-stage pipeline that combines document image restoration with semantic-aware post-OCR correction to enhance both visual clarity and textual consistency, thereby improving text extraction from degraded historical documents.First, we synthesize document-image pairs from plaintext, rendering them with diverse fonts and layouts and then applying a randomly ordered set of degradation operations. An image restoration model is trained on this synthetic data, using multi-directional patch extraction and fusion to process large images. Second, a ByT5 post-OCR model, fine-tuned on synthetic historical text pairs, addresses remaining OCR errors.Detailed experiments on 13,831 pages of real historical documents in English, French, and Spanish show that the PreP-OCR pipeline reduces character error rates by 63.9-70.3% compared to OCR on raw images. Our pipeline demonstrates the potential of integrating image restoration with linguistic error correction for digitizing historical archives.</abstract>
      <url hash="45d2b8b7">2025.acl-long.749</url>
      <bibkey>guan-etal-2025-prep</bibkey>
    </paper>
    <paper id="750">
      <title>Digest the Knowledge: Large Language Models empowered Message Passing for Knowledge Graph Question Answering</title>
      <author><first>Junhong</first><last>Wan</last></author>
      <author><first>Tao</first><last>Yu</last><affiliation>Hikvision Research Institute</affiliation></author>
      <author><first>Kunyu</first><last>Jiang</last><affiliation>Hikvision Research Institute</affiliation></author>
      <author><first>Yao</first><last>Fu</last><affiliation>Hikvision Research Institute</affiliation></author>
      <author><first>Weihao</first><last>Jiang</last><affiliation>Hikvision Research Institute</affiliation></author>
      <author><first>Jiang</first><last>Zhu</last><affiliation>Hikvision Research Institute</affiliation></author>
      <pages>15426-15442</pages>
      <abstract>Despite their success, large language models (LLMs) suffer from notorious hallucination issue. By introducing external knowledge stored in knowledge graphs (KGs), existing methods use paths as the medium to represent the graph information that send into LLMs. However, paths only contain limited graph structure information and are unorganized with redundant sequentially appeared keywords, which are difficult for LLMs to digest. We aim to find a suitable medium that captures the essence of structure knowledge in KGs. Inspired by the Neural Message Passing in Graph Neural Networks, we propose Language Message Passing (LMP) that first learns a concise facts graph by iteratively aggregates neighbor entities and transforms them into semantic facts, and then we performs Topological Readout that encodes the graph structure information into multi-level lists of texts to augment LLMs. Our method serves as a brand-new innovative framework that brings a new perspective into KG-enhanced LLMs, and also offers human-level semantic explainability with significant performance improvements over existing methods on all 5 knowledge graph question answering datasets. Code is available at https://github.com/wanjunhong0/LMP.</abstract>
      <url hash="38d58e93">2025.acl-long.750</url>
      <bibkey>wan-etal-2025-digest</bibkey>
    </paper>
    <paper id="751">
      <title><fixed-case>R</fixed-case>ec<fixed-case>LM</fixed-case>: Recommendation Instruction Tuning</title>
      <author><first>Yangqin</first><last>Jiang</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Yuhao</first><last>Yang</last></author>
      <author><first>Lianghao</first><last>Xia</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Da</first><last>Luo</last></author>
      <author><first>Kangyi</first><last>Lin</last></author>
      <author><first>Chao</first><last>Huang</last><affiliation>University of Hong Kong</affiliation></author>
      <pages>15443-15459</pages>
      <abstract>Modern recommender systems aim to deeply understand users’ complex preferences through their past interactions. While deep collaborative filtering approaches using Graph Neural Networks (GNNs) excel at capturing user-item relationships, their effectiveness is limited when handling sparse data or zero-shot scenarios, primarily due to constraints in ID-based embedding functions. To address these challenges, we propose a model-agnostic recommendation instruction-tuning paradigm that seamlessly integrates large language models with collaborative filtering. Our proposed Recommendation Language Model (RecLM) enhances the capture of user preference diversity through a carefully designed reinforcement learning reward function that facilitates self-augmentation of language models. Comprehensive evaluations demonstrate significant advantages of our approach across various settings, and its plug-and-play compatibility with state-of-the-art recommender systems results in notable performance enhancements.</abstract>
      <url hash="65e6af84">2025.acl-long.751</url>
      <bibkey>jiang-etal-2025-reclm</bibkey>
    </paper>
    <paper id="752">
      <title><fixed-case>DS</fixed-case><tex-math>^2</tex-math>-<fixed-case>ABSA</fixed-case>: Dual-Stream Data Synthesis with Label Refinement for Few-Shot Aspect-Based Sentiment Analysis</title>
      <author><first>Hongling</first><last>Xu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Yice</first><last>Zhang</last></author>
      <author><first>Qianlong</first><last>Wang</last></author>
      <author><first>Ruifeng</first><last>Xu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>15460-15478</pages>
      <abstract>Recently developed large language models (LLMs) have presented promising new avenues to address data scarcity in low-resource scenarios. In few-shot aspect-based sentiment analysis (ABSA), previous efforts have explored data augmentation techniques, which prompt LLMs to generate new samples by modifying existing ones. However, these methods fail to produce adequately diverse data, impairing their effectiveness. Besides, some studies apply in-context learning for ABSA by using specific instructions and a few selected examples as prompts. Though promising, LLMs often yield labels that deviate from task requirements. To overcome these limitations, we propose DS<tex-math>^2</tex-math>-ABSA, a dual-stream data synthesis framework targeted for few-shot ABSA. It leverages LLMs to synthesize data from two complementary perspectives: <i>key-point-driven</i> and <i>instance-driven</i>, which effectively generate diverse and high-quality ABSA samples in low-resource settings. Furthermore, a <i>label refinement</i> module is integrated to improve the synthetic labels. Extensive experiments demonstrate that DS<tex-math>^2</tex-math>-ABSA significantly outperforms previous few-shot ABSA solutions and other LLM-oriented data generation methods.</abstract>
      <url hash="f2b2728d">2025.acl-long.752</url>
      <bibkey>xu-etal-2025-ds2</bibkey>
    </paper>
    <paper id="753">
      <title><fixed-case>MISP</fixed-case>-Meeting: A Real-World Dataset with Multimodal Cues for Long-form Meeting Transcription and Summarization</title>
      <author><first>HangChen</first><last>HangChen</last></author>
      <author><first>Chao-Han Huck</first><last>Yang</last><affiliation>NVIDIA Research</affiliation></author>
      <author><first>Jia-Chen</first><last>Gu</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Sabato Marco</first><last>Siniscalchi</last><affiliation>University of Palermo and Norwegian Institute of Technology</affiliation></author>
      <author><first>Jun</first><last>Du</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>15479-15492</pages>
      <abstract>We introduce MISP-Meeting, a new real-world, multimodal dataset that covers subject-oriented long-form content. MISP-Meeting integrates information from speech, vision, and text modalities to facilitate automatic meeting transcription and summarization (AMTS). Challenging conditions in human meetings, including far-field speech recognition, audio-visual understanding, and long-term summarization, have been carefully evaluated. We benchmark state-of-the-art automatic speech recognition (ASR) and large language models (LLMs) on this dataset, enhanced with multimodal cues. Experiments demonstrate that incorporating multimodal cues, such as lip movements and visual focus of attention, significantly enhances transcription accuracy, reducing the character error rate (CER) from 36.60% to 20.27% via guided source separation (GSS), fine-tuning, and audio-visual fusion. Furthermore, our summarization analysis reveals a direct correlation between ASR quality and summary coherence, underscoring the importance of robust multimodal modeling. Our dataset and codebase will be released as open source.</abstract>
      <url hash="1919f7a8">2025.acl-long.753</url>
      <bibkey>hangchen-etal-2025-misp</bibkey>
    </paper>
    <paper id="754">
      <title>Learning Together to Perform Better: Teaching Small-Scale <fixed-case>LLM</fixed-case>s to Collaborate via Preferential Rationale Tuning</title>
      <author><first>Sohan</first><last>Patnaik</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Milan</first><last>Aggarwal</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Sumit</first><last>Bhatia</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Balaji</first><last>Krishnamurthy</last><affiliation>Adobe Systems</affiliation></author>
      <pages>15493-15512</pages>
      <abstract>LLMs such as GPT-4 have shown a remarkable ability to solve complex questions by generating step-by-step rationales. Prior works have utilized this capability to improve smaller and cheaper LMs (say, with 7B parameters). However, various practical constraints, such as copyright and legal issues, owing to lack of transparency in the pre-training data of large (often closed) models, prevent their use in commercial settings. Little focus has been given to improving the innate reasoning ability of smaller models without distilling information from larger LLMs. To address this, we propose COLLATE, a trainable framework that tunes a (small) LLM to generate those outputs from a pool of diverse rationales that selectively improves the downstream task. COLLATE enforces multiple instances of the same LLM to exhibit distinct behavior and employs them to generate rationales to obtain diverse outputs. The LLM is then tuned via preference optimization to choose the candidate rationale which maximizes the likelihood of ground-truth answer. COLLATE outperforms several trainable and prompting baselines on 5 datasets across 3 domains - maths problem solving, natural language inference, and commonsense reasoning. We show the efficacy of COLLATE on LLMs from different model families across varying parameter scales (1B to 8B) and demonstrate the benefit of multiple rationale providers guided by the end task through ablations. Code is released here (https://github.com/Sohanpatnaik106/collate).</abstract>
      <url hash="9dca65d2">2025.acl-long.754</url>
      <bibkey>patnaik-etal-2025-learning</bibkey>
    </paper>
    <paper id="755">
      <title><fixed-case>M</fixed-case>ol<fixed-case>RAG</fixed-case>: Unlocking the Power of Large Language Models for Molecular Property Prediction</title>
      <author><first>Ziting</first><last>Xian</last></author>
      <author><first>Jiawei</first><last>Gu</last></author>
      <author><first>Lingbo</first><last>Li</last><affiliation>University of Warwick</affiliation></author>
      <author><first>Shangsong</first><last>Liang</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <pages>15513-15531</pages>
      <abstract>Recent LLMs exhibit limited effectiveness on molecular property prediction task due to the semantic gap between molecular representations and natural language, as well as the lack of domain-specific knowledge. To address these challenges, we propose MolRAG, a Retrieval-Augmented Generation framework integrating Chain-of-Thought reasoning for molecular property prediction. MolRAG operates by retrieving structurally analogous molecules as contextual references to guide stepwise knowledge reasoning through chemical structure-property relationships. This dual mechanism synergizes molecular similarity analysis with structured inference, while generating human-interpretable rationales grounded in domain knowledge. Experimental results show MolRAG outperforms pre-trained LLMs on four datasets, and even matches supervised methods, achieving performance gains of 1.1%–45.7% over direct prediction approaches, demonstrating versatile effectiveness. Our code is available at https://github.com/AcaciaSin/MolRAG.</abstract>
      <url hash="0c27768c">2025.acl-long.755</url>
      <bibkey>xian-etal-2025-molrag</bibkey>
    </paper>
    <paper id="756">
      <title><fixed-case>S</fixed-case>kill<fixed-case>A</fixed-case>ggregation: Reference-free <fixed-case>LLM</fixed-case>-Dependent Aggregation</title>
      <author><first>Guangzhi</first><last>Sun</last></author>
      <author><first>Anmol</first><last>Kagrecha</last><affiliation>Stanford University</affiliation></author>
      <author><first>Potsawee</first><last>Manakul</last><affiliation>SCB 10X</affiliation></author>
      <author><first>Phil</first><last>Woodland</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Mark</first><last>Gales</last><affiliation>University of Cambridge</affiliation></author>
      <pages>15532-15548</pages>
      <abstract>Large Language Models (LLMs) are increasingly used to assess NLP tasks due to their ability to generate human-like judgments. Single LLMs were used initially, however, recent work suggests using multiple LLMs as judges yields improved performance. An important step in exploiting multiple judgements is the combination stage, aggregation. Existing methods in NLP either assign equal weight to all LLM judgments or are designed for specific tasks such as hallucination detection. This work focuses on aggregating predictions from multiple systems where no reference labels are available. A new method called SkillAggregation is proposed, which learns to combine estimates from LLM judges without needing additional data or ground truth. It extends the Crowdlayer aggregation method, developed for image classification, to exploit the judge estimates during inference. The approach is compared to a range of standard aggregation methods on HaluEval-Dialogue, TruthfulQA and Chatbot Arena tasks. SkillAggregation outperforms Crowdlayer on all tasks, and yields the best performance over all approaches on the majority of tasks.</abstract>
      <url hash="52f1039a">2025.acl-long.756</url>
      <bibkey>sun-etal-2025-skillaggregation</bibkey>
    </paper>
    <paper id="757">
      <title><fixed-case>M</fixed-case>as<fixed-case>R</fixed-case>outer: Learning to Route <fixed-case>LLM</fixed-case>s for Multi-Agent Systems</title>
      <author><first>Yanwei</first><last>Yue</last></author>
      <author><first>Guibin</first><last>Zhang</last></author>
      <author><first>Boyang</first><last>Liu</last></author>
      <author><first>Guancheng</first><last>Wan</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Kun</first><last>Wang</last></author>
      <author><first>Dawei</first><last>Cheng</last><affiliation>Tongji University</affiliation></author>
      <author><first>Yiyan</first><last>Qi</last><affiliation>IDEA</affiliation></author>
      <pages>15549-15572</pages>
      <abstract>Multi-agent systems (MAS) powered by Large Language Models (LLMs) have been demonstrated to push the boundaries of LLM capabilities, yet they often incur significant costs and face challenges in dynamic LLM selection. Current LLM routing methods effectively reduce overhead in single-agent scenarios by customizing LLM selection for each query, but they overlook the critical decisions regarding collaboration modes and agent roles in MAS. In response to this challenge, we first introduce the problem of <b>Multi-Agent System Routing (MASR)</b>, which integrates all components of MAS into a unified routing framework. Toward this goal, we propose MasRouter, the first high-performing, cost-effective, and inductive <b>MASR</b> solution. MasRouter employs collaboration mode determination, role allocation, and LLM routing through a cascaded controller network, progressively constructing a MAS that balances effectiveness and efficiency. Extensive experiments demonstrate that MasRouter is <b>(1) high-performing</b>, achieving a 1.8 improvement over the state-of-the-art method on MBPP; <b>(2) economical</b>, reducing overhead by up to 52.07 compared to SOTA methods on HumanEval; and <b>(3) plug-and-play</b>, seamlessly integrating with mainstream MAS frameworks, reducing overhead by 17.21 via customized routing.</abstract>
      <url hash="5e6f139e">2025.acl-long.757</url>
      <bibkey>yue-etal-2025-masrouter</bibkey>
    </paper>
    <paper id="758">
      <title>Beyond Single Labels: Improving Conversational Recommendation through <fixed-case>LLM</fixed-case>-Powered Data Augmentation</title>
      <author><first>Haozhe</first><last>Xu</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xiaohua</first><last>Wang</last></author>
      <author><first>Changze</first><last>Lv</last></author>
      <author><first>Xiaoqing</first><last>Zheng</last><affiliation>Fudan University</affiliation></author>
      <pages>15573-15590</pages>
      <abstract>Conversational recommender systems (CRSs) enhance recommendation quality by engaging users in multi-turn dialogues, capturing nuanced preferences through natural language interactions. However, these systems often face the false negative issue, where items that a user might like are incorrectly labeled as negative during training, leading to suboptimal recommendations. Expanding the label set through data augmentation presents an intuitive solution but faces the challenge of balancing two key aspects: ensuring semantic relevance and preserving the collaborative information inherent in CRS datasets. To address these issues, we propose a novel data augmentation framework that first leverages an LLM-based semantic retriever to identify diverse and semantically relevant items, which are then filtered by a relevance scorer to remove noisy candidates. Building on this, we introduce a two-stage training strategy balancing semantic relevance and collaborative information. Extensive experiments on two benchmark datasets and user simulators demonstrate significant and consistent performance improvements across various recommenders, highlighting the effectiveness of our approach in advancing CRS performance.</abstract>
      <url hash="74b042f1">2025.acl-long.758</url>
      <bibkey>xu-etal-2025-beyond</bibkey>
    </paper>
    <paper id="759">
      <title>Beyond One-Size-Fits-All: Tailored Benchmarks for Efficient Evaluation</title>
      <author><first>Peiwen</first><last>Yuan</last></author>
      <author><first>Yueqi</first><last>Zhang</last></author>
      <author><first>Shaoxiong</first><last>Feng</last><affiliation>RedNote</affiliation></author>
      <author><first>Yiwei</first><last>Li</last></author>
      <author><first>Xinglin</first><last>Wang</last></author>
      <author><first>Jiayi</first><last>Shi</last></author>
      <author><first>Chuyi</first><last>Tan</last></author>
      <author><first>Boyuan</first><last>Pan</last></author>
      <author><first>Yao</first><last>Hu</last><affiliation>Xiaohongshu</affiliation></author>
      <author><first>Kan</first><last>Li</last></author>
      <pages>15591-15615</pages>
      <abstract>Evaluating models on large benchmarks can be very resource-intensive, especially during a period of rapid model evolution. Existing efficient evaluation methods estimate the performance of target models by testing them on a small, static coreset derived from the publicly available evaluation results of source models, which are separate from the target models. However, these approaches rely on the assumption that target models have high prediction consistency with source models, which doesn’t generalize well in practice. To fill this gap, we propose TailoredBench, a method that conducts customized evaluation tailored to each target model. Specifically, a Global-coreset is first constructed as a probe to identify the most consistent source models for each target model with an adaptive source model selection strategy. Afterwards, a scalable K-Medoids clustering algorithm is proposed to extend the Global-coreset to a tailored Native-coreset for each target model. According to the predictions on respective Native-coreset, we estimate the overall performance of target models with a calibrated estimation strategy. Comprehensive experiments on five benchmarks across over 300 models demonstrate that compared to best performing baselines, TailoredBench achieves an average reduction of 31.4% in MAE of accuracy estimates under the same inference budgets, showcasing strong effectiveness and generalizability.</abstract>
      <url hash="552e48ca">2025.acl-long.759</url>
      <bibkey>yuan-etal-2025-beyond-one</bibkey>
    </paper>
    <paper id="760">
      <title>i<fixed-case>QUEST</fixed-case>: An Iterative Question-Guided Framework for Knowledge Base Question Answering</title>
      <author><first>Shuai</first><last>Wang</last></author>
      <author><first>Yinan</first><last>Yu</last><affiliation>Chalmers University of Technology</affiliation></author>
      <pages>15616-15628</pages>
      <abstract>While Large Language Models (LLMs) excel at many natural language processing tasks, they often suffer from factual inaccuracies in knowledge-intensive scenarios. Integrating external knowledge resources, particularly knowledge graphs (KGs), provides a transparent and updatable foundation for more reliable reasoning. Knowledge Base Question Answering (KBQA), which queries and reasons over KGs, is central to this effort, especially for complex, multi-hop queries. However, multi-hop reasoning poses two key challenges: (1) maintaining coherent reasoning paths, and (2) avoiding prematurely discarding critical multi-hop connections. To address these issues, we introduce iQUEST, a question-guided KBQA framework that iteratively decomposes complex queries into simpler sub-questions, ensuring a structured and focused reasoning trajectory. Additionally, we integrate a Graph Neural Network (GNN) to look ahead and incorporate 2-hop neighbor information at each reasoning step. This dual approach strengthens the reasoning process, enabling the model to explore viable paths more effectively. Detailed experiments demonstrate the consistent improvement delivered by iQUEST across four benchmark datasets and four LLMs.</abstract>
      <url hash="8d2d9840">2025.acl-long.760</url>
      <bibkey>wang-yu-2025-iquest</bibkey>
    </paper>
    <paper id="761">
      <title><fixed-case>IRT</fixed-case>-Router: Effective and Interpretable Multi-<fixed-case>LLM</fixed-case> Routing via Item Response Theory</title>
      <author><first>Wei</first><last>Song</last></author>
      <author><first>Zhenya</first><last>Huang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Cheng</first><last>Cheng</last></author>
      <author><first>Weibo</first><last>Gao</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Bihan</first><last>Xu</last></author>
      <author><first>GuanHao</first><last>Zhao</last></author>
      <author><first>Fei</first><last>Wang</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Runze</first><last>Wu</last><affiliation>NetEase Corp</affiliation></author>
      <pages>15629-15644</pages>
      <abstract>Large language models (LLMs) have demonstrated exceptional performance across a wide range of natural language tasks. However, selecting the optimal LLM to respond to a user query often necessitates a delicate balance between performance and cost. While powerful models deliver better results, they come at a high cost, whereas smaller models are more cost-effective but less capable. To address this trade-off, we propose IRT-Router, a multi-LLM routing framework that efficiently routes user queries to the most suitable LLM. Inspired by Item Response Theory (IRT), a psychological measurement methodology, IRT-Router explicitly models the relationship between LLM capabilities and user query attributes. This not only enables accurate prediction of response performance but also provides interpretable insights, such as LLM abilities and query difficulty. Additionally, we design an online query warm-up technique based on semantic similarity, further enhancing the online generalization capability of IRT-Router. Extensive experiments on 20 LLMs and 12 datasets demonstrate that IRT-Router outperforms most baseline methods in terms of effectiveness and interpretability. Its superior performance in cold-start scenarios further confirms the reliability and practicality of IRT-Router in real-world applications. Code is available at <url>https://github.com/Mercidaiha/IRT-Router</url>.</abstract>
      <url hash="36b0f536">2025.acl-long.761</url>
      <bibkey>song-etal-2025-irt</bibkey>
    </paper>
    <paper id="762">
      <title><fixed-case>MLAS</fixed-case>-<fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case>: Language-Aware Parameters Detection and <fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case>-Based Knowledge Transfer for Multilingual Machine Translation</title>
      <author><first>Tianyu</first><last>Dong</last></author>
      <author id="bo-li"><first>Bo</first><last>Li</last><affiliation>Baidu Inc</affiliation></author>
      <author><first>Jinsong</first><last>Liu</last></author>
      <author><first>Shaolin</first><last>Zhu</last><affiliation>Tianjin University</affiliation></author>
      <author><first>Deyi</first><last>Xiong</last><affiliation>Tianjin University</affiliation></author>
      <pages>15645-15660</pages>
      <abstract>Large language models (LLMs) have achieved remarkable progress in multilingual machine translation (MT), demonstrating strong performance even with limited parallel data. However, effectively fine-tuning LLMs for MT is challenging due to parameter interference, which arises from the conflicting demands of different language pairs and the risk of overwriting pre-trained knowledge. To address this issue, we propose <b>MLAS-LoRA</b>, a novel multiple language-aware LoRA knowledge transfer framework. MLAS-LoRA efficiently adapts LLMs to MT by selectively transferring knowledge from a large teacher to a small student model. Our approach first evaluates the awareness of neurons and extracts linguistic knowledge in the teacher model to both the general MT task and specific language pairs.We then propose a multiple language-specific LoRA architecture to inject the extracted knowledge into the student model. During fine-tuning, only the parameters of the relevant language-general and language-specific LoRA modules are updated. Experimental results on diverse multilingual language pairs demonstrate that MLAS-LoRA significantly outperforms strong baselines by +1.7 BLEU on average, including standard fine-tuning and other parameter-efficient methods.</abstract>
      <url hash="7e1f2ead">2025.acl-long.762</url>
      <bibkey>dong-etal-2025-mlas</bibkey>
    </paper>
    <paper id="763">
      <title><fixed-case>M</fixed-case>2<fixed-case>RC</fixed-case>-<fixed-case>EVAL</fixed-case>: Massively Multilingual Repository-level Code Completion Evaluation</title>
      <author><first>Jiaheng</first><last>Liu</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Ken</first><last>Deng</last></author>
      <author><first>Congnan</first><last>Liu</last></author>
      <author><first>Jian</first><last>Yang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Shukai</first><last>Liu</last></author>
      <author><first>He</first><last>Zhu</last><affiliation>Guangdong OPPO Mobile Telecommunications Corp.,Ltd.</affiliation></author>
      <author><first>Peng</first><last>Zhao</last></author>
      <author><first>Linzheng</first><last>Chai</last></author>
      <author><first>Yanan</first><last>Wu</last></author>
      <author><first>JinKe</first><last>JinKe</last></author>
      <author><first>Ge</first><last>Zhang</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Zekun Moore</first><last>Wang</last><affiliation>Beijing University of Aeronautics and Astronautics</affiliation></author>
      <author><first>Guoan</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yingshui</first><last>Tan</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Bangyu</first><last>Xiang</last></author>
      <author><first>Zhaoxiang</first><last>Zhang</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Wenbo</first><last>Su</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Bo</first><last>Zheng</last><affiliation>Alibaba Group</affiliation></author>
      <pages>15661-15684</pages>
      <abstract>Repository-level code completion has drawn great attention in software engineering, and several benchmarks have been introduced. However, existing repository-level code completion benchmarks usually focus on a limited number of languages (&lt;5), which cannot evaluate the general code intelligence abilities across different languages for existing code Large Language Models (LLMs). Besides, the existing benchmarks usually report overall average scores of different languages, where the fine-grained abilities in different completion scenarios are ignored. Therefore, to facilitate the research of code LLMs in multilingual scenarios, we propose a massively multilingual repository-level code completion benchmark covering 18 programming languages (called M2RC-EVAL), and two types of fine-grained annotations (i.e., bucket-level and semantic-level) on different completion scenarios are provided, where we obtain these annotations based on the parsed abstract syntax tree. Moreover, we also curate a massively multilingual instruction corpora M2RC-INSTRUCT dataset to improve the repository-level code completion abilities of existing code LLMs. Comprehensive experimental results demonstrate the effectiveness of our M2RC-EVAL and M2RC-INSTRUCT.</abstract>
      <url hash="c24f6666">2025.acl-long.763</url>
      <bibkey>liu-etal-2025-m2rc</bibkey>
    </paper>
    <paper id="764">
      <title>Evaluating Design Decisions for Dual Encoder-based Entity Disambiguation</title>
      <author><first>Susanna</first><last>Rücker</last><affiliation>Humboldt-Universität zu Berlin</affiliation></author>
      <author><first>Alan</first><last>Akbik</last><affiliation>Humboldt Universität Berlin</affiliation></author>
      <pages>15685-15701</pages>
      <abstract>Entity disambiguation (ED) is the task of linking mentions in text to corresponding entries in a knowledge base. Dual Encoders address this by embedding mentions and label candidates in a shared embedding space and applying a similarity metric to predict the correct label. In this work, we focus on evaluating key design decisions for Dual Encoder-based ED, such as its loss function, similarity metric, label verbalization format, and negative sampling strategy. We present the resulting model VerbalizED, a document-level Dual Encoder model that includes contextual label verbalizations and efficient hard negative sampling. Additionally, we explore an iterative prediction variant that aims to improve the disambiguation of challenging data points. To support our analysis, we first conduct comprehensive ablation experiments on specific design decisions using AIDA-Yago, followed by large-scale, multi-domain evaluation on the ZELDA benchmark.</abstract>
      <url hash="2179f4de">2025.acl-long.764</url>
      <bibkey>rucker-akbik-2025-evaluating</bibkey>
    </paper>
    <paper id="765">
      <title>How to Compare Things Properly? A Study of Argument Relevance in Comparative Question Answering</title>
      <author><first>Irina</first><last>Nikishina</last></author>
      <author><first>Saba</first><last>Anwar</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Nikolay</first><last>Dolgov</last></author>
      <author><first>Maria</first><last>Manina</last></author>
      <author><first>Daria</first><last>Ignatenko</last></author>
      <author><first>Artem</first><last>Shelmanov</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Chris</first><last>Biemann</last><affiliation>U Hamburg</affiliation></author>
      <pages>15702-15720</pages>
      <abstract>Comparative Question Answering (CQA) lies at the intersection of Question Answering, Argument Mining, and Summarization. It poses unique challenges due to the inherently subjective nature of many questions and the need to integrate diverse perspectives. Although the CQA task can be addressed using recently emerged instruction-following Large Language Models (LLMs), challenges such as hallucinations in their outputs and the lack of transparent argument provenance remain significant limitations.To address these challenges, we construct a manually curated dataset comprising arguments annotated with their relevance. These arguments are further used to answer comparative questions, enabling precise traceability and faithfulness. Furthermore, we define explicit criteria for an “ideal” comparison and introduce a benchmark for evaluating the outputs of various Retrieval-Augmented Generation (RAG) models with respect to argument relevance. All code and data are publicly released to support further research.</abstract>
      <url hash="58358770">2025.acl-long.765</url>
      <bibkey>nikishina-etal-2025-compare</bibkey>
    </paper>
    <paper id="766">
      <title><fixed-case>F</fixed-case>inance<fixed-case>R</fixed-case>easoning: Benchmarking Financial Numerical Reasoning More Credible, Comprehensive and Challenging</title>
      <author><first>Zichen</first><last>Tang</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Haihong</first><last>E</last><affiliation>Beijing University of Post and Telecommunication</affiliation></author>
      <author><first>Ziyan</first><last>Ma</last></author>
      <author><first>Haoyang</first><last>He</last></author>
      <author><first>Jiacheng</first><last>Liu</last></author>
      <author><first>Zhongjun</first><last>Yang</last></author>
      <author><first>Zihua</first><last>Rong</last></author>
      <author><first>Rongjin</first><last>Li</last></author>
      <author><first>Kun</first><last>Ji</last></author>
      <author><first>Qing</first><last>Huang</last></author>
      <author><first>Xinyang</first><last>Hu</last></author>
      <author id="yang-liu"><first>Yang</first><last>Liu</last></author>
      <author><first>Qianhe</first><last>Zheng</last></author>
      <pages>15721-15749</pages>
      <abstract>We introduce **FinanceReasoning**, a novel benchmark designed to evaluate the reasoning capabilities of large reasoning models (LRMs) in financial numerical reasoning problems. Compared to existing benchmarks, our work provides three key advancements. (1) **Credibility**: We update 15.6% of the questions from four public datasets, annotating 908 new questions with detailed Python solutions and rigorously refining evaluation standards. This enables an accurate assessment of the reasoning improvements of LRMs. (2) **Comprehensiveness**: FinanceReasoning covers 67.8% of financial concepts and formulas, significantly surpassing existing datasets. Additionally, we construct 3,133 Python-formatted functions, which enhances LRMs’ financial reasoning capabilities through refined knowledge (*e.g.*, 83.2% <tex-math>\rightarrow</tex-math> 91.6% for GPT-4o). (3) **Challenge**: Models are required to apply multiple financial formulas for precise numerical reasoning on 238 *Hard* problems. The best-performing model (*i.e.*, OpenAI o1 with PoT) achieves 89.1% accuracy, yet LRMs still face challenges in numerical precision. We demonstrate that combining Reasoner and Programmer models can effectively enhance LRMs’ performance (*e.g.*, 83.2% <tex-math>\rightarrow</tex-math> 87.8% for DeepSeek-R1). Our work paves the way for future research on evaluating and improving LRMs in domain-specific complex reasoning tasks.</abstract>
      <url hash="6efa8cba">2025.acl-long.766</url>
      <bibkey>tang-etal-2025-financereasoning</bibkey>
    </paper>
    <paper id="767">
      <title>Controllable Style Arithmetic with Language Models</title>
      <author><first>Weiqi</first><last>Wang</last></author>
      <author><first>Wengang</first><last>Zhou</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Zongmeng</first><last>Zhang</last></author>
      <author><first>Jie</first><last>Zhao</last></author>
      <author><first>Houqiang</first><last>Li</last></author>
      <pages>15750-15799</pages>
      <abstract>Language models have shown remarkable capabilities in text generation, but precisely controlling their linguistic style remains challenging. Existing methods either lack fine-grained control, require extensive computation, or introduce significant latency. We propose Style Arithmetic (SA), a novel parameter-space approach that first extracts style-specific representations by analyzing parameter differences between models trained on contrasting styles, then incorporates these representations into a base model with precise control over style intensity. Our experiments show that SA achieves three key capabilities: controllability for precise adjustment of styles, transferability for effective style transfer across tasks, and composability for simultaneous control of multiple style dimensions. Compared to alternative methods, SA offers superior effectiveness while achieving optimal computational efficiency. Our approach opens new possibilities for flexible and efficient style control in language models.</abstract>
      <url hash="7433ad92">2025.acl-long.767</url>
      <bibkey>wang-etal-2025-controllable</bibkey>
    </paper>
    <paper id="768">
      <title>Masks Can be Learned as an Alternative to Experts</title>
      <author><first>Peiyu</first><last>Liu</last><affiliation>University of International Business and Economics</affiliation></author>
      <author><first>Tianwen</first><last>Wei</last><affiliation>Xiaomi</affiliation></author>
      <author><first>Bo</first><last>Zhu</last></author>
      <author><first>Xin</first><last>Zhao</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Shuicheng</first><last>Yan</last><affiliation>National University of Singapore</affiliation></author>
      <pages>15800-15811</pages>
      <abstract>In this work, we investigate how to sparsify a pre-trained dense large language model into a mixture-of-experts (MoE) architecture for faster inference. Our approach applies mask matrix to the activations for each expert, constrained by <tex-math>L_0</tex-math> regularization to minimize the number of activated parameters. Starting with all parameters active, the model is progressively sparsified during training, ensuring minimal performance loss. This approach proves more efficient than one-shot sparsification techniques, which typically require significant resources for performance recovery. Moreover, our approach automatically identifies shared, token-specific, and inactive experts, allowing for more efficient allocation of computational resources. Through extensive experiments, we achieve up to 97% performance retention on downstream tasks with only 50% of the feed-forward parameters activated in dense models. Beyond enhancing inference efficiency, this strategy of sharing computational units among experts presents a valuable framework for designing more generalized and efficient MoE architectures, opening avenues for future advancements in expert-based models.</abstract>
      <url hash="57152a4f">2025.acl-long.768</url>
      <bibkey>liu-etal-2025-masks</bibkey>
    </paper>
    <paper id="769">
      <title>Program Synthesis Benchmark for Visual Programming in <fixed-case>XL</fixed-case>ogo<fixed-case>O</fixed-case>nline Environment</title>
      <author><first>Chao</first><last>Wen</last><affiliation>MPI-SWS</affiliation></author>
      <author><first>Jacqueline</first><last>Staub</last></author>
      <author><first>Adish</first><last>Singla</last><affiliation>Max Planck Institute for Software Systems (MPI-SWS)</affiliation></author>
      <pages>15812-15838</pages>
      <abstract>Large language and multimodal models have shown remarkable success on various benchmarks focused on specific skills such as general-purpose programming, math word problem-solving, and visual question answering. However, it is unclear how well these models perform on tasks that require a combination of these skills. In this paper, we curate a novel program synthesis benchmark based on the real-world tasks in the XLogoOnline visual programming environment. Each task requires a combination of different skills such as spatial planning, basic programming, and logical reasoning. Our evaluation shows that current state-of-the-art models like GPT-4V and Llama3-70B struggle to solve these tasks, achieving only 20% and 2.35% success rates, respectively. Next, we develop a fine-tuning pipeline to boost the performance of models by leveraging a large-scale synthetic training dataset with over 80,000 tasks. Moreover, we showcase how emulator-driven feedback can be used to design a curriculum over training data distribution, through which a fine-tuned Llama3-8B drastically outperforms GPT-4V and Llama3-70B models. Finally, we provide an in-depth failure analysis to understand the limitations of different models. We will publicly release the benchmark for future research on program synthesis in visual programming.</abstract>
      <url hash="df63c475">2025.acl-long.769</url>
      <bibkey>wen-etal-2025-program</bibkey>
    </paper>
    <paper id="770">
      <title>Removal of Hallucination on Hallucination: Debate-Augmented <fixed-case>RAG</fixed-case></title>
      <author><first>Wentao</first><last>Hu</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Wengyu</first><last>Zhang</last></author>
      <author><first>Yiyang</first><last>Jiang</last></author>
      <author><first>Chen Jason</first><last>Zhang</last></author>
      <author><first>Xiaoyong</first><last>Wei</last><affiliation>Hong Kong Polytechnic University and Sichuan University, China</affiliation></author>
      <author><first>Li</first><last>Qing</last><affiliation>The Hong Kong Polytechnic University and Hong Kong Polytechnic University</affiliation></author>
      <pages>15839-15853</pages>
      <abstract>Retrieval-Augmented Generation (RAG) enhances factual accuracy by integrating external knowledge, yet it introduces a critical issue: erroneous or biased retrieval can mislead generation, compounding hallucinations, a phenomenon we term Hallucination on Hallucination. To address this, we propose Debate-Augmented RAG (DRAG), a training-free framework that integrates Multi-Agent Debate (MAD) mechanisms into both retrieval and generation stages. In retrieval, DRAG employs structured debates among proponents, opponents, and judges to refine retrieval quality and ensure factual reliability. In generation, DRAG introduces asymmetric information roles and adversarial debates, enhancing reasoning robustness and mitigating factual inconsistencies. Evaluations across multiple tasks demonstrate that DRAG improves retrieval reliability, reduces RAG-induced hallucinations, and significantly enhances overall factual accuracy. Our code is available at https://github.com/Huenao/Debate-Augmented-RAG.</abstract>
      <url hash="eb91f7f4">2025.acl-long.770</url>
      <bibkey>hu-etal-2025-removal</bibkey>
    </paper>
    <paper id="771">
      <title><fixed-case>C</fixed-case>ode<fixed-case>DPO</fixed-case>: Aligning Code Models with Self Generated and Verified Source Code</title>
      <author><first>Kechi</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Ge</first><last>Li</last><affiliation>Peking University</affiliation></author>
      <author><first>Yihong</first><last>Dong</last></author>
      <author><first>Jingjing</first><last>Xu</last></author>
      <author><first>Jun</first><last>Zhang</last><affiliation>ByteDance</affiliation></author>
      <author><first>Jing</first><last>Su</last></author>
      <author><first>Yongfei</first><last>Liu</last><affiliation>Bytedance</affiliation></author>
      <author><first>Zhi</first><last>Jin</last><affiliation>Peking University</affiliation></author>
      <pages>15854-15871</pages>
      <abstract>Code generation models have shown significant potential for programming tasks. However, existing training methods like supervised fine-tuning face key limitations: they do not effectively teach models to prioritize correct over incorrect solutions in ambiguous situations, nor do they effectively optimize the runtime efficiency of the generated code. To address these challenges, we propose CodeDPO, a framework that integrates preference learning into code generation to improve two key code preference factors: code correctness and efficiency. CodeDPO employs a novel dataset construction method, utilizing a self-generation-and-validation mechanism that simultaneously generates and evaluates code and test cases. The underlying assumption is that test cases executable by multiple code snippets provide more reliable validation, and code that passes more tests is more likely to be correct. Through this self-validation process, our PageRank-inspired algorithm iteratively updates the ranking score of each code snippet, ultimately creating a code preference optimization dataset based on correctness and efficiency. CodeDPO is flexible and scalable, generating diverse preference optimization data without depending on powerful models such as GPT-4. Through comprehensive evaluations of five widely used benchmarks, CodeDPO demonstrates significant improvements in correctness and efficiency compared to existing methods. Our experiments prove that CodeDPO enhances the capabilities of LLMs in code generation and provides a robust foundation for conducting code preference optimization in more complex and challenging real-world scenarios.</abstract>
      <url hash="51aa2422">2025.acl-long.771</url>
      <bibkey>zhang-etal-2025-codedpo</bibkey>
    </paper>
    <paper id="772">
      <title><fixed-case>P</fixed-case>rox<fixed-case>A</fixed-case>nn: Use-Oriented Evaluations of Topic Models and Document Clustering</title>
      <author><first>Alexander Miserlis</first><last>Hoyle</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <author><first>Lorena</first><last>Calvo-Bartolomé</last><affiliation>Universidad Carlos III de Madrid</affiliation></author>
      <author><first>Jordan Lee</first><last>Boyd-Graber</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Philip</first><last>Resnik</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>15872-15897</pages>
      <abstract>Topic models and document-clustering evaluations either use automated metrics that align poorly with human preferences, or require expert labels that are intractable to scale. We design a scalable human evaluation protocol and a corresponding automated approximation that reflect practitioners’ real-world usage of models. Annotators—or an LLM-based proxy—review text items assigned to a topic or cluster, infer a category for the group, then apply that category to other documents. Using this protocol, we collect extensive crowdworker annotations of outputs from a diverse set of topic models on two datasets. We then use these annotations to validate automated proxies, finding that the best LLM proxy is statistically indistinguishable from a human annotator and can therefore serve as a reasonable substitute in automated evaluations.</abstract>
      <url hash="566a7de5">2025.acl-long.772</url>
      <bibkey>hoyle-etal-2025-proxann</bibkey>
    </paper>
    <paper id="773">
      <title><fixed-case>BOOKWORLD</fixed-case>: From Novels to Interactive Agent Societies for Story Creation</title>
      <author><first>Yiting</first><last>Ran</last></author>
      <author><first>Xintao</first><last>Wang</last></author>
      <author><first>Tian</first><last>Qiu</last></author>
      <author><first>Jiaqing</first><last>Liang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Yanghua</first><last>Xiao</last><affiliation>Fudan University</affiliation></author>
      <author><first>Deqing</first><last>Yang</last><affiliation>Fudan University</affiliation></author>
      <pages>15898-15912</pages>
      <abstract>Recent advances in large language models (LLMs) have enabled social simulation through multi-agent systems. Prior efforts focus on agent societies created from scratch, assigning agents with newly defined personas. However, simulating established fictional worlds and characters remain largely underexplored, despite its significant practical value. In this paper, we introduce BookWorld, a comprehensive system for constructing and simulating book-based multi-agent societies. BookWorld’s design covers comprehensive real-world intricacies, including diverse and dynamic characters, fictional worldviews, geographical constraints and changes, e.t.c. BookWorld enables diverse applications including story generation, interactive games and social simulation, offering novel ways to extend and explore beloved fictional works. Through extensive experiments, we demonstrate that BookWorld generates creative, high-quality stories while maintaining fidelity to the source books, surpassing previous methods with a win rate of 75.36%. The code and demo of this paper can be found at the project page: https://bookworld2025.github.io/.</abstract>
      <url hash="e4451961">2025.acl-long.773</url>
      <bibkey>ran-etal-2025-bookworld</bibkey>
    </paper>
    <paper id="774">
      <title>Quantifying Lexical Semantic Shift via Unbalanced Optimal Transport</title>
      <author><first>Ryo</first><last>Kishino</last></author>
      <author><first>Hiroaki</first><last>Yamagiwa</last></author>
      <author><first>Ryo</first><last>Nagata</last><affiliation>RIKEN and Konan University</affiliation></author>
      <author><first>Sho</first><last>Yokoi</last><affiliation>NINJAL, Tohoku University and RIKEN</affiliation></author>
      <author><first>Hidetoshi</first><last>Shimodaira</last><affiliation>Kyoto University and RIKEN</affiliation></author>
      <pages>15913-15933</pages>
      <abstract>Lexical semantic change detection aims to identify shifts in word meanings over time. While existing methods using embeddings from a diachronic corpus pair estimate the degree of change for target words, they offer limited insight into changes at the level of individual usage instances. To address this, we apply Unbalanced Optimal Transport (UOT) to sets of contextualized word embeddings, capturing semantic change through the excess and deficit in the alignment between usage instances. In particular, we propose Sense Usage Shift (SUS), a measure that quantifies changes in the usage frequency of a word sense at each usage instance. By leveraging SUS, we demonstrate that several challenges in semantic change detection can be addressed in a unified manner, including quantifying instance-level semantic change and word-level tasks such as measuring the magnitude of semantic change and the broadening or narrowing of meaning.</abstract>
      <url hash="25497533">2025.acl-long.774</url>
      <bibkey>kishino-etal-2025-quantifying</bibkey>
    </paper>
    <paper id="775">
      <title>Agentic Reward Modeling: Integrating Human Preferences with Verifiable Correctness Signals for Reliable Reward Systems</title>
      <author><first>Hao</first><last>Peng</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Yunjia</first><last>Qi</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Xiaozhi</first><last>Wang</last><affiliation>Department of Computer Science and Technology, Tsinghua University</affiliation></author>
      <author><first>Zijun</first><last>Yao</last></author>
      <author><first>Bin</first><last>Xu</last></author>
      <author><first>Lei</first><last>Hou</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <pages>15934-15949</pages>
      <abstract>Reward models (RMs) are crucial for the training and inference-time scaling up of large language models (LLMs). However, existing reward models primarily focus on human preferences, neglecting verifiable correctness signals which have shown strong potential in training LLMs. In this paper, we propose agentic reward modeling, a reward system that combines reward models with verifiable correctness signals from different aspects to provide reliable rewards. We empirically implement a reward agent, named RewardAgent, that combines human preference rewards with two verifiable signals: factuality and instruction following, to provide more reliable rewards. We conduct comprehensive experiments on existing reward model benchmarks and inference-time best-of-n searches on real-world downstream tasks. RewardAgent significantly outperforms vanilla reward models, demonstrating its effectiveness. We further construct training preference pairs using RewardAgent and train an LLM with the DPO objective, achieving superior performance on various NLP benchmarks compared to conventional reward models. Our codes are publicly released to facilitate further research.</abstract>
      <url hash="92d02649">2025.acl-long.775</url>
      <bibkey>peng-etal-2025-agentic</bibkey>
    </paper>
    <paper id="776">
      <title>Adaptive and Robust Translation from Natural Language to Multi-model Query Languages</title>
      <author><first>Gengyuan</first><last>Shi</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Chaokun</first><last>Wang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Liu</first><last>Yabin</last></author>
      <author><first>Jiawei</first><last>Ren</last><affiliation>University of California, San Diego</affiliation></author>
      <pages>15950-15965</pages>
      <abstract>Multi-model databases and polystore systems are increasingly studied for managing multi-model data holistically. As their primary interface, multi-model query languages (MMQLs) often exhibit complex grammars, highlighting the need for effective Text-to-MMQL translation methods. Despite advances in natural language translation, no effective solutions for Text-to-MMQL exist. To address this gap, we formally define the Text-to-MMQL task and present the first Text-to-MMQL dataset involving three representative MMQLs. We propose an adaptive Text-to-MMQL framework that includes both a schema embedding module for capturing multi-model schema information and an MMQL representation strategy to generate concise intermediate query formats with error correction in generated queries. Experimental results show that the proposed framework achieves over a 9% accuracy improvement over our adapted baseline methods.</abstract>
      <url hash="01943fda">2025.acl-long.776</url>
      <bibkey>shi-etal-2025-adaptive</bibkey>
    </paper>
    <paper id="777">
      <title><fixed-case>SAKE</fixed-case>: Steering Activations for Knowledge Editing</title>
      <author><first>Marco</first><last>Scialanga</last></author>
      <author><first>Thibault</first><last>Laugel</last><affiliation>LIP6, Sorbonne Université/CNRS and AXA</affiliation></author>
      <author><first>Vincent</first><last>Grari</last><affiliation>Stanford University</affiliation></author>
      <author><first>Marcin</first><last>Detyniecki</last><affiliation>AXA, CNRS and LIP6</affiliation></author>
      <pages>15966-15978</pages>
      <abstract>As Large Langue Models have been shown to memorize real-world facts, the need to update this knowledge in a controlled and efficient manner arises. Designed with these constraints in mind, Knowledge Editing (KE) approaches propose to alter specific facts in pretrained models. However, they have been shown to suffer from several limitations, including their lack of contextual robustness and their failure to generalize to logical implications related to the fact. To overcome these issues, we propose SAKE, a steering activation method that models a fact to be edited as a distribution rather than a single prompt. Leveraging Optimal Transport, SAKE alters the LLM behavior over a whole fact-related distribution, defined as paraphrases and logical implications. Several numerical experiments demonstrate the effectiveness of this method: SAKE is thus able to perform more robust edits than its existing counterparts.</abstract>
      <url hash="ef164a48">2025.acl-long.777</url>
      <bibkey>scialanga-etal-2025-sake</bibkey>
    </paper>
    <paper id="778">
      <title>Middle-Layer Representation Alignment for Cross-Lingual Transfer in Fine-Tuned <fixed-case>LLM</fixed-case>s</title>
      <author><first>Danni</first><last>Liu</last><affiliation>Karlsruher Institut für Technologie</affiliation></author>
      <author><first>Jan</first><last>Niehues</last></author>
      <pages>15979-15996</pages>
      <abstract>While large language models demonstrate remarkable capabilities at task-specific applications through fine-tuning, extending these benefits across diverse languages is essential for broad accessibility. However, effective cross-lingual transfer is hindered by LLM performance gaps across languages and the scarcity of fine-tuning data in many languages. Through analysis of LLM internal representations from over 1,000+ language pairs, we discover that middle layers exhibit the strongest potential for cross-lingual alignment. Building on this finding, we propose a middle-layer alignment objective integrated into task-specific training. Our experiments on slot filling, machine translation, and structured text generation show consistent improvements in cross-lingual transfer, especially to lower-resource languages. The method is robust to the choice of alignment languages and generalizes to languages unseen during alignment. Furthermore, we show that separately trained alignment modules can be merged with existing task-specific modules, improving cross-lingual capabilities without full re-training. The code is provided in the supplementary materials.</abstract>
      <url hash="5c76fcab">2025.acl-long.778</url>
      <bibkey>liu-niehues-2025-middle</bibkey>
    </paper>
    <paper id="779">
      <title>Can External Validation Tools Improve Annotation Quality for <fixed-case>LLM</fixed-case>-as-a-Judge?</title>
      <author><first>Arduin</first><last>Findeis</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Floris</first><last>Weers</last><affiliation>Apple</affiliation></author>
      <author><first>Guoli</first><last>Yin</last><affiliation>Apple</affiliation></author>
      <author><first>Ke</first><last>Ye</last><affiliation>Apple</affiliation></author>
      <author><first>Ruoming</first><last>Pang</last><affiliation>Apple</affiliation></author>
      <author><first>Tom</first><last>Gunter</last><affiliation>Apple</affiliation></author>
      <pages>15997-16020</pages>
      <abstract>Pairwise preferences over model responses are widely collected to evaluate and provide feedback to large language models (LLMs). Given two alternative model responses to the same input, a human or AI annotator selects the “better” response. This approach can provide feedback for domains where other hard-coded metrics are difficult to obtain (e.g., chat response quality), thereby helping model evaluation or training. However, for some domains high-quality pairwise comparisons can be tricky to obtain - from AI and humans. For example, for responses with many factual statements, annotators may disproportionately weigh writing quality rather than underlying facts. In this work, we explore augmenting standard AI annotator systems with additional tools to improve performance on three challenging response domains: long-form factual, math and code tasks. We propose a tool-using agentic system to provide higher quality feedback on these domains. Our system uses web-search and code execution to ground itself based on external validation, independent of the LLM’s internal knowledge and biases. We provide extensive experimental results evaluating our method across the three targeted response domains as well as general annotation tasks, using RewardBench (incl. AlpacaEval and LLMBar), RewardMath, as well as three new datasets for domains with saturated pre-existing datasets. Our results indicate that external tools can indeed improve performance in many, but not all, cases. More generally, our experiments highlight the sensitivity of performance to simple parameters (e.g., prompt) and the need for improved (non-saturated) annotator benchmarks. We share our code at https://github.com/apple/ml-agent-evaluator.</abstract>
      <url hash="5ea5a598">2025.acl-long.779</url>
      <bibkey>findeis-etal-2025-external</bibkey>
    </paper>
    <paper id="780">
      <title>One for All: Update Parameterized Knowledge Across Multiple Models with Once Edit</title>
      <author><first>Weitao</first><last>Ma</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Xiyuan</first><last>Du</last></author>
      <author><first>Xiaocheng</first><last>Feng</last></author>
      <author><first>Lei</first><last>Huang</last></author>
      <author><first>Yichong</first><last>Huang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Huiyi</first><last>Zhang</last></author>
      <author><first>Xiaoliang</first><last>Yang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Baohang</first><last>Li</last></author>
      <author><first>Xiachong</first><last>Feng</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Ting</first><last>Liu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Bing</first><last>Qin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>16021-16034</pages>
      <abstract>Large language models (LLMs) encode vast world knowledge but struggle to stay up-to-date, often leading to errors and hallucinations. Knowledge editing offers an efficient alternative to retraining, enabling targeted modifications by updating specific model parameters. However, existing methods primarily focus on individual models, posing challenges in efficiently updating multiple models and adapting to new models. To address this, we propose OnceEdit, a novel ensemble-based approach that employs a plug-in model as the editing module, enabling stable knowledge updates across multiple models. Building on the model ensemble, OnceEdit introduces two key mechanisms to enhance its effectiveness. First, we introduce a dynamic weight mechanism through a weight token for distinguishing between edit-related and non-edit-related instances, ensuring the appropriate utilization of knowledge from integrated models. Second, we incorporate an ensemble enhancement mechanism to mitigate the excessive reliance on the central model inherent in the model ensemble technique, making it more suitable for knowledge editing. Extensive experiments on diverse LLMs demonstrate that OnceEdit consistently outperforms existing methods while achieving superior editing efficiency. Further analysis confirms its adaptability and stability in multi-model editing scenarios.</abstract>
      <url hash="ea770c81">2025.acl-long.780</url>
      <bibkey>ma-etal-2025-one</bibkey>
    </paper>
    <paper id="781">
      <title><fixed-case>VLMI</fixed-case>nfer<fixed-case>S</fixed-case>low: Evaluating the Efficiency Robustness of Large Vision-Language Models as a Service</title>
      <author><first>Xiasi</first><last>Wang</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Tianliang</first><last>Yao</last></author>
      <author><first>Simin</first><last>Chen</last><affiliation>Columbia University</affiliation></author>
      <author><first>Runqi</first><last>Wang</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Lei</first><last>Ye</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Kuofeng</first><last>Gao</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Yi</first><last>Huang</last></author>
      <author><first>Yuan</first><last>Yao</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <pages>16035-16050</pages>
      <abstract>Vision-Language Models (VLMs) have demonstrated great potential in real-world applications. While existing research primarily focuses on improving their accuracy, the efficiency remains underexplored. Given the real-time demands of many applications and the high inference overhead of VLMs, efficiency robustness is a critical issue. However, previous studies evaluate efficiency robustness under unrealistic assumptions, requiring access to the model architecture and parameters—an impractical scenario in ML-as-a-service settings, where VLMs are deployed via inference APIs. To address this gap, we propose VLMInferSlow, a novel approach for evaluating VLM efficiency robustness in a realistic black-box setting. VLMInferSlow incorporates fine-grained efficiency modeling tailored to VLM inference and leverages zero-order optimization to search for adversarial examples. Experimental results show that VLMInferSlow generates adversarial images with imperceptible perturbations, increasing the computational cost by up to 128.47%. We hope this research raises the community’s awareness about the efficiency robustness of VLMs.</abstract>
      <url hash="b6d3902c">2025.acl-long.781</url>
      <bibkey>wang-etal-2025-vlminferslow</bibkey>
    </paper>
    <paper id="782">
      <title>The Alternative Annotator Test for <fixed-case>LLM</fixed-case>-as-a-Judge: How to Statistically Justify Replacing Human Annotators with <fixed-case>LLM</fixed-case>s</title>
      <author><first>Nitay</first><last>Calderon</last><affiliation>Technion - Israel Institute of Technology</affiliation></author>
      <author><first>Roi</first><last>Reichart</last><affiliation>Technion, Israel Institute of Technology</affiliation></author>
      <author><first>Rotem</first><last>Dror</last><affiliation>University of Haifa</affiliation></author>
      <pages>16051-16081</pages>
      <abstract>The “LLM-as-an-annotator” and “LLM-as-a-judge” paradigms employ Large Language Models (LLMs) as annotators, judges, and evaluators in tasks traditionally performed by humans. LLM annotations are widely used, not only in NLP research but also in fields like medicine, psychology, and social science. Despite their role in shaping study results and insights, there is no standard or rigorous procedure to determine whether LLMs can replace human annotators. In this paper, we propose a novel statistical procedure, the Alternative Annotator Test (alt-test), that requires only a modest subset of annotated examples to justify using LLM annotations. Additionally, we introduce a versatile and interpretable measure for comparing LLM annotators and judges. To demonstrate our procedure, we curated a diverse collection of ten datasets, consisting of language and vision-language tasks, and conducted experiments with six LLMs and four prompting techniques. Our results show that LLMs can sometimes replace humans with closed-source LLMs (such as GPT-4o), outperforming the open-source LLMs we examine, and that prompting techniques yield judges of varying quality. We hope this study encourages more rigorous and reliable practices.</abstract>
      <url hash="37e7f9c3">2025.acl-long.782</url>
      <bibkey>calderon-etal-2025-alternative</bibkey>
    </paper>
    <paper id="783">
      <title><fixed-case>C</fixed-case>risis<fixed-case>TS</fixed-case>: Coupling Social Media Textual Data and Meteorological Time Series for Urgency Classification</title>
      <author><first>Romain</first><last>Meunier</last><affiliation>IRIT</affiliation></author>
      <author><first>Farah</first><last>Benamara</last><affiliation>Institut de recherche en informatique de toulouse</affiliation></author>
      <author><first>Véronique</first><last>Moriceau</last><affiliation>IRIT, université de Toulouse</affiliation></author>
      <author><first>Zhongzheng</first><last>Qiao</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Savitha</first><last>Ramasamy</last><affiliation>Institute for Infocomm Research, Agency for Science, Technology and Research, Singapore</affiliation></author>
      <pages>16082-16099</pages>
      <abstract>This paper proposes CrisisTS, the first multimodal and multilingual dataset for urgency classification composed of benchmark crisis datasets from French and English social media about various expected (e.g., flood, storm) and sudden (e.g., earthquakes, explosions) crises that have been mapped with open source geocoded meteorological time series data. This mapping is based on a simple and effective strategy that allows for temporal and location alignment even in the absence of location mention in the text. A set of multimodal experiments have been conducted relying on transformers and LLMs to improve overall performances while ensuring model generalizability. Our results show that modality fusion outperforms text-only models.</abstract>
      <url hash="fc682249">2025.acl-long.783</url>
      <bibkey>meunier-etal-2025-crisists</bibkey>
    </paper>
    <paper id="784">
      <title>How to Mitigate Overfitting in Weak-to-strong Generalization?</title>
      <author><first>Junhao</first><last>Shi</last></author>
      <author><first>Qinyuan</first><last>Cheng</last></author>
      <author><first>Zhaoye</first><last>Fei</last></author>
      <author><first>Yining</first><last>Zheng</last></author>
      <author><first>Qipeng</first><last>Guo</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Xipeng</first><last>Qiu</last><affiliation>Fudan University</affiliation></author>
      <pages>16100-16118</pages>
      <abstract>Aligning powerful AI models on tasks that surpass human evaluation capabilities is the central problem of **superalignment**. To address this problem, weak-to-strong generalization aims to elicit the capabilities of strong models through weak supervisors and ensure that the behavior of strong models aligns with the intentions of weak supervisors without unsafe behaviors such as deception. Although weak-to-strong generalization exhibiting certain generalization capabilities, strong models exhibit significant overfitting in weak-to-strong generalization: Due to the strong fit ability of strong models, erroneous labels from weak supervisors may lead to overfitting in strong models. In addition, simply filtering out incorrect labels may lead to a degeneration in question quality, resulting in a weak generalization ability of strong models on hard questions. To mitigate overfitting in weak-to-strong generalization, we propose a two-stage framework that simultaneously improves the quality of supervision signals and the quality of input questions. Experimental results in three series of large language models and two mathematical benchmarks demonstrate that our framework significantly improves PGR (Performance Gap Recovered) compared to naive weak-to-strong generalization, even achieving up to 100% PGR on some models.</abstract>
      <url hash="591a4c23">2025.acl-long.784</url>
      <bibkey>shi-etal-2025-mitigate</bibkey>
    </paper>
    <paper id="785">
      <title>Com<tex-math>^2</tex-math> : A Causal-Guided Benchmark for Exploring Complex Commonsense Reasoning in Large Language Models</title>
      <author><first>Kai</first><last>Xiong</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Xiao</first><last>Ding</last></author>
      <author><first>Yixin</first><last>Cao</last><affiliation>Fudan University</affiliation></author>
      <author><first>Yuxiong</first><last>Yan</last></author>
      <author><first>Li</first><last>Du</last><affiliation>BAAI</affiliation></author>
      <author><first>Yufei</first><last>Zhang</last></author>
      <author><first>Jinglong</first><last>Gao</last><affiliation>Research Center for Social Computing and Information Retrieval</affiliation></author>
      <author><first>Jiaqian</first><last>Liu</last></author>
      <author><first>Bing</first><last>Qin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Ting</first><last>Liu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>16119-16140</pages>
      <abstract>Large language models (LLMs) have mastered abundant simple and explicit commonsense knowledge through pre-training, enabling them to achieve human-like performance in simple commonsense reasoning. Nevertheless, LLMs struggle to reason with complex and implicit commonsense knowledge that is derived from simple ones (such as understanding the long-term effects of certain events), an aspect humans tend to focus on more. Existing works focus on complex tasks like math and code, while complex commonsense reasoning remains underexplored due to its uncertainty and lack of structure. To fill this gap and align with real-world concerns, we propose a benchmark Com<tex-math>^2</tex-math> focusing on complex commonsense reasoning. We first incorporate causal event graphs to serve as structured complex commonsense. Then we adopt causal theory (e.g., intervention) to modify the causal event graphs and obtain different scenarios that meet human concerns. Finally, an LLM is employed to synthesize examples with slow thinking, which is guided by the logical relationships in the modified causal graphs. Furthermore, we use detective stories to construct a more challenging subset. Experiments show that LLMs struggle in reasoning depth and breadth, while post-training and slow thinking can alleviate this. The code and data are available at https://github.com/Waste-Wood/Com2.</abstract>
      <url hash="23140c9d">2025.acl-long.785</url>
      <bibkey>xiong-etal-2025-com2</bibkey>
    </paper>
    <paper id="786">
      <title>Dynamic Head Selection for Neural Lexicalized Constituency Parsing</title>
      <author><first>Yang</first><last>Hou</last><affiliation>Soochow University</affiliation></author>
      <author><first>Zhenghua</first><last>Li</last><affiliation>Soochow University</affiliation></author>
      <pages>16141-16155</pages>
      <abstract>Lexicalized parsing, which associates constituent nodes with lexical heads, has historically played a crucial role in constituency parsing by bridging constituency and dependency structures. Nevertheless, with the advent of neural networks, lexicalized structures have generally been neglected in favor of unlexicalized, span-based methods. In this paper, we revisit lexicalized parsing and propose a novel latent lexicalization framework that dynamically infers lexical heads during training without relying on predefined head-finding rules. Our method enables the model to learn lexical dependencies directly from data, offering greater adaptability across languages and datasets. Experiments on multiple treebanks demonstrate state-of-the-art or comparable performance. We also analyze the learned dependency structures, headword preferences, and linguistic biases.</abstract>
      <url hash="5685c753">2025.acl-long.786</url>
      <bibkey>hou-li-2025-dynamic</bibkey>
    </paper>
    <paper id="787">
      <title>My Words Imply Your Opinion: Reader Agent-Based Propagation Enhancement for Personalized Implicit Emotion Analysis</title>
      <author><first>Jian</first><last>Liao</last><affiliation>Shanxi University</affiliation></author>
      <author><first>Yu</first><last>Feng</last></author>
      <author><first>Yujin</first><last>Zheng</last></author>
      <author><first>Jun</first><last>Zhao</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <author><first>Suge</first><last>Wang</last></author>
      <author><first>JianXing</first><last>Zheng</last><affiliation>Shanxi University</affiliation></author>
      <pages>16156-16172</pages>
      <abstract>The subtlety of emotional expressions makes implicit emotion analysis (IEA) particularly sensitive to user-specific characteristics. Current studies personalize emotion analysis by focusing on the author but neglect the impact of the intended reader on implicit emotional feedback. In this paper, we introduce Personalized IEA (PIEA) and present the RAPPIE model, which addresses subjective variability by incorporating reader feedback. In particular, (1) we create reader agents based on large language models to simulate reader feedback, overcoming the issue of “spiral of silence effect” and data incompleteness of real reader reaction. (2) We develop a role-aware multi-view graph learning to model the emotion interactive propagation process in scenarios with sparse reader information. (3) We construct two new PIEA datasets covering English and Chinese social media with detailed user metadata, addressing the text-centric limitation of existing datasets. Extensive experiments show that RAPPIE significantly outperforms state-of-the-art baselines, demonstrating the value of incorporating reader feedback in PIEA.</abstract>
      <url hash="e5b2f866">2025.acl-long.787</url>
      <bibkey>liao-etal-2025-words</bibkey>
    </paper>
    <paper id="788">
      <title><fixed-case>E</fixed-case>volve<fixed-case>B</fixed-case>ench: A Comprehensive Benchmark for Assessing Temporal Awareness in <fixed-case>LLM</fixed-case>s on Evolving Knowledge</title>
      <author><first>Zhiyuan</first><last>Zhu</last></author>
      <author><first>Yusheng</first><last>Liao</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Zhe</first><last>Chen</last></author>
      <author><first>Yuhao</first><last>Wang</last></author>
      <author><first>Yunfeng</first><last>Guan</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Yanfeng</first><last>Wang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Yu</first><last>Wang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>16173-16188</pages>
      <abstract>Large language models (LLMs) are trained on extensive historical corpora, but their ability to understand time and maintain temporal awareness of time-evolving factual knowledge remains limited. Previous studies often neglect the critical aspect of utilizing knowledge from various sources. To address this gap, we introduce EvolveBench, a comprehensive benchmark that evaluates temporal competence along five key dimensions: Cognition, which examines the ability to recall and contextualize historical facts. Awareness, which tests LLMs’ awareness of temporal misalignment between external inputs and the temporal context of a query. Trustworthiness, which assesses whether models can identify and appropriately refuse queries based on invalid timestamps. Understanding, which focuses on interpreting both explicit dates and implicit historical markers. Finally, reasoning evaluates the capacity to analyze temporal relationships and draw accurate inferences. Evaluating 15 widely used LLMs on EvolveBench shows that GPT-4o achieves the highest average EM score of 79.36, while the open-source Llama3.1-70B demonstrates notable strength in handling temporally misaligned contexts with an average score of 72.47. Despite these advances, all models still struggle with handling temporal misaligned context. Our code and dataset are available at https://github.com/zzysjtuiwct/EvolveBench.</abstract>
      <url hash="db7f1842">2025.acl-long.788</url>
      <bibkey>zhu-etal-2025-evolvebench</bibkey>
    </paper>
    <paper id="789">
      <title>Enabling <fixed-case>LLM</fixed-case> Knowledge Analysis via Extensive Materialization</title>
      <author><first>Yujia</first><last>Hu</last><affiliation>Technische Universität Dresden</affiliation></author>
      <author><first>Tuan-Phong</first><last>Nguyen</last><affiliation>Max-Planck Institute for Informatics</affiliation></author>
      <author><first>Shrestha</first><last>Ghosh</last><affiliation>Eberhard-Karls-Universität Tübingen</affiliation></author>
      <author><first>Simon</first><last>Razniewski</last><affiliation>Technische Universität Dresden</affiliation></author>
      <pages>16189-16202</pages>
      <abstract>Large language models (LLMs) have majorly advanced NLP and AI, and next to their ability to perform a wide range of procedural tasks, a major success factor is their internalized factual knowledge. Since (Petroni et al., 2019), analyzing this knowledge has gained attention. However, most approaches investigate one question at a time via modest-sized pre-defined samples, introducing an “availability bias” (Tverski and Kahnemann, 1973) that prevents the analysis of knowledge (or beliefs) of LLMs beyond the experimenter’s predisposition.To address this challenge, we propose a novel methodology to comprehensively materialize an LLM’s factual knowledge through recursive querying and result consolidation. Our approach is a milestone for LLM research, for the first time providing constructive insights into the scope and structure of LLM knowledge (or beliefs).As a prototype, we extract a knowledge base (KB) comprising 101 million relational triples for over 2.9 million entities from GPT-4o-mini. We use GPTKB to exemplarily analyze GPT-4o-mini’s factual knowledge in terms of scale, accuracy, bias, cutoff and consistency, at the same time. Our resource is accessible at https://gptkb.org.</abstract>
      <url hash="eb421cbd">2025.acl-long.789</url>
      <bibkey>hu-etal-2025-enabling</bibkey>
    </paper>
    <paper id="790">
      <title>Rhythm Controllable and Efficient Zero-Shot Voice Conversion via Shortcut Flow Matching</title>
      <author><first>Jialong</first><last>Zuo</last></author>
      <author><first>Shengpeng</first><last>Ji</last></author>
      <author><first>Minghui</first><last>Fang</last></author>
      <author><first>Mingze</first><last>Li</last></author>
      <author><first>Ziyue</first><last>Jiang</last></author>
      <author><first>Xize</first><last>Cheng</last></author>
      <author><first>Xiaoda</first><last>Yang</last></author>
      <author><first>Chen</first><last>Feiyang</last></author>
      <author><first>Xinyu</first><last>Duan</last></author>
      <author><first>Zhou</first><last>Zhao</last><affiliation>Zhejiang University and Zhejiang University</affiliation></author>
      <pages>16203-16217</pages>
      <abstract>Zero-Shot Voice Conversion (VC) aims to transform the source speaker’s timbre into an arbitrary unseen one while retaining speech content. Most prior work focuses on preserving the source’s prosody, while fine-grained timbre information may leak through prosody, and transferring target prosody to synthesized speech is rarely studied. In light of this, we propose R-VC, a rhythm-controllable and efficient zero-shot voice conversion model. R-VC employs data perturbation techniques and discretize source speech into Hubert content tokens, eliminating much content-irrelevant information. By leveraging a Mask Generative Transformer for in-context duration modeling, our model adapts the linguistic content duration to the desired target speaking style, facilitating the transfer of the target speaker’s rhythm. Furthermore, R-VC introduces a powerful Diffusion Transformer (DiT) with shortcut flow matching during training, conditioning the network not only on the current noise level but also on the desired step size, enabling high timbre similarity and quality speech generation in fewer sampling steps, even in just two, thus minimizing latency. Experimental results show that R-VC achieves comparable speaker similarity to state-of-the-art VC methods with a smaller dataset, and surpasses them in terms of speech naturalness, intelligibility and style transfer performance.</abstract>
      <url hash="44d87f2d">2025.acl-long.790</url>
      <bibkey>zuo-etal-2025-rhythm</bibkey>
    </paper>
    <paper id="791">
      <title>Llama See, Llama Do: A Mechanistic Perspective on Contextual Entrainment and Distraction in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Jingcheng</first><last>Niu</last><affiliation>Technische Universität Darmstadt</affiliation></author>
      <author><first>Xingdi</first><last>Yuan</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Tong</first><last>Wang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Hamidreza</first><last>Saghir</last><affiliation>Microsoft</affiliation></author>
      <author><first>Amir H.</first><last>Abdi</last><affiliation>Microsoft and Borealis AI</affiliation></author>
      <pages>16218-16239</pages>
      <abstract>We observe a novel phenomenon, *contextual entrainment*, across a wide range of language models (LMs) and prompt settings, providing a new mechanistic perspective on how LMs become distracted by “irrelevant” contextual information in the input prompt. Specifically, LMs assign significantly higher logits (or probabilities) to any tokens that have previously appeared in the context prompt, even for random tokens. This suggests that contextual entrainment is a <i>mechanistic</i> phenomenon, occurring independently of the relevance or semantic relation of the tokens to the question or the rest of the sentence. We find statistically significant evidence that the magnitude of contextual entrainment is influenced by semantic factors. Counterfactual prompts have a greater effect compared to factual ones, suggesting that while contextual entrainment is a mechanistic phenomenon, it is modulated by semantic factors.We hypothesise that there is a circuit of attention heads — the *entrainment heads* — that corresponds to the contextual entrainment phenomenon. Using a novel entrainment head discovery method based on differentiable masking, we identify these heads across various settings. When we “turn off” these heads, i.e., set their outputs to zero, the effect of contextual entrainment is significantly attenuated, causing the model to generate output that capitulates to what it would produce if no distracting context were provided. Our discovery of contextual entrainment, along with our investigation into LM distraction via the entrainment heads, marks a key step towards the mechanistic analysis and mitigation of the distraction problem.</abstract>
      <url hash="8d21689d">2025.acl-long.791</url>
      <bibkey>niu-etal-2025-llama</bibkey>
    </paper>
    <paper id="792">
      <title><fixed-case>C</fixed-case>riti<fixed-case>Q</fixed-case>: Mining Data Quality Criteria from Human Preferences</title>
      <author><first>Honglin</first><last>Guo</last><affiliation>Fudan University</affiliation></author>
      <author><first>Kai</first><last>Lv</last></author>
      <author><first>Qipeng</first><last>Guo</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Tianyi</first><last>Liang</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Zhiheng</first><last>Xi</last></author>
      <author><first>Demin</first><last>Song</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Qiuyinzhe</first><last>Zhang</last></author>
      <author><first>Yu</first><last>Sun</last></author>
      <author><first>Kai</first><last>Chen</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Xipeng</first><last>Qiu</last><affiliation>Fudan University</affiliation></author>
      <author><first>Tao</first><last>Gui</last><affiliation>Fudan University</affiliation></author>
      <pages>16240-16261</pages>
      <abstract>Language model heavily depends on high-quality data for optimal performance. Existing approaches rely on manually designed heuristics, the perplexity of existing models, training classifiers, orcareful prompt engineering, which require significant expert experience and human annotation effort while introduce biases. We introduce CritiQ, a novel data selection method that automatically mines criteria from human preferences for data quality with only ~30 human-annotated pairs and performs efficient data selection. The main component, CritiQ Flow, employs a manager agent to evolve quality criteria and worker agents to make pairwise judgments. We build a knowledge base that extracts quality criteria from previous work to boost CritiQ Flow. Compared to perplexity- and classifier-based methods, verbal criteria are more interpretable and have greater reusable value. After deriving the criteria, we train the CritiQ Scorer to give quality scores and perform efficient data selection. We demonstrate the effectiveness of our method in the code, math, and logic domains, achieving high accuracy on human-annotated test sets. To validate the quality of the selected data, we continually train Llama 3.2 models and observe improved performance on downstream tasks compared to uniform sampling. Ablation studies validate the benefits of the knowledge base and the reflection process. We analyze how criteria evolve and the effectiveness of majority voting.</abstract>
      <url hash="e296ceb9">2025.acl-long.792</url>
      <bibkey>guo-etal-2025-critiq</bibkey>
    </paper>
    <paper id="793">
      <title>Theoretical Guarantees for Minimum <fixed-case>B</fixed-case>ayes Risk Decoding</title>
      <author><first>Yuki</first><last>Ichihara</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Yuu</first><last>Jinnai</last><affiliation>CyberAgent, Inc.</affiliation></author>
      <author><first>Kaito</first><last>Ariu</last><affiliation>CyberAgent, Inc.</affiliation></author>
      <author><first>Tetsuro</first><last>Morimura</last><affiliation>CyberAgent, Inc.</affiliation></author>
      <author><first>Eiji</first><last>Uchibe</last></author>
      <pages>16262-16284</pages>
      <abstract>Minimum Bayes Risk (MBR) decoding optimizes output selection by maximizing the expected utility value of an underlying human distribution. While prior work has shown the effectiveness of MBR decoding through empirical evaluation, few studies have analytically investigated why the method is effective. As a result of our analysis, we show that, given the size <tex-math>n</tex-math> of the reference hypothesis set used in computation, MBR decoding approaches the optimal solution with high probability at a rate of <tex-math>\mathcal{O}(n^{-\frac{1}{2}})</tex-math>, under certain assumptions, even though the language space <tex-math>\mathcal{Y}</tex-math> is significantly larger <tex-math>|\mathcal{Y}| \gg n</tex-math>.This result helps to theoretically explain the strong performance observed in several prior empirical studies on MBR decoding. In addition, we provide the performance gap for maximum-a-posteriori (MAP) decoding and compare it to MBR decoding. The result of this paper indicates that MBR decoding tends to converge to the optimal solution faster than MAP decoding in several cases.</abstract>
      <url hash="f6813e10">2025.acl-long.793</url>
      <bibkey>ichihara-etal-2025-theoretical</bibkey>
    </paper>
    <paper id="794">
      <title>Mutual-Taught for Co-adapting Policy and Reward Models</title>
      <author><first>Tianyuan</first><last>Shi</last></author>
      <author><first>Canbin</first><last>Huang</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Fanqi</first><last>Wan</last></author>
      <author><first>Longguang</first><last>Zhong</last></author>
      <author><first>Ziyi</first><last>Yang</last><affiliation>Sun Yat-Sen University</affiliation></author>
      <author><first>Weizhou</first><last>Shen</last></author>
      <author><first>Xiaojun</first><last>Quan</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Ming</first><last>Yan</last><affiliation>Alibaba Group</affiliation></author>
      <pages>16285-16298</pages>
      <abstract>During the preference optimization of large language models (LLMs), distribution shifts may arise between newly generated model samples and the data used to train the reward model (RM). This shift reduces the efficacy of the RM, which in turn negatively impacts the performance of the policy model (PM). To address this challenge, we propose <tex-math>\textbf{Mutual-Taught}</tex-math>, a self-training method that iteratively improves both the PM and RM without requiring additional human annotation. Our approach mirrors the expectation-maximization (EM) algorithm. In the E-step, the PM is updated using feedback from the current RM, guiding the PM toward a better approximation of the latent optimal preference distribution.In the M-step, we update the RM by constructing training data from the outputs of the PM before and after the E-step update. This process ensures that the RM adapts to the evolving policy distribution. Experimental results demonstrate that this iterative approach leads to consistent improvements in both models. Specifically, our 8B policy model, LLaMA-3-8B-Instruct-MT, achieves a length-controlled win rate of 54.1% on AlpacaEval-2, while our 8B reward model, FsfairX-LLaMA3-RM-MT, performs on par with GPT-4o-2024-08-06 on RewardBench.</abstract>
      <url hash="e80b5c20">2025.acl-long.794</url>
      <bibkey>shi-etal-2025-mutual</bibkey>
    </paper>
    <paper id="795">
      <title>Enhancing Cross-Lingual Transfer through Reversible Transliteration: A <fixed-case>H</fixed-case>uffman-Based Approach for Low-Resource Languages</title>
      <author><first>Wenhao</first><last>Zhuang</last></author>
      <author><first>Yuan</first><last>Sun</last><affiliation>Minzu University of China</affiliation></author>
      <author><first>Xiaobing</first><last>Zhao</last><affiliation>Minzu University of China and National Language Resource Monitoring &amp; Research Center of Minority Languages</affiliation></author>
      <pages>16299-16313</pages>
      <abstract>As large language models (LLMs) are trained on increasingly diverse and extensive multilingual corpora, they demonstrate cross-lingual transfer capabilities. However, these capabilities often fail to effectively extend to low-resource languages, particularly those utilizing non-Latin scripts. While transliterating low-resource languages into Latin script presents a natural solution, there currently lacks a comprehensive framework for integrating transliteration into LLMs training and deployment. Taking a pragmatic approach, this paper innovatively combines character transliteration with Huffman coding to design a complete transliteration framework. Our proposed framework offers the following advantages: 1) Compression: Reduces storage requirements for low-resource language content, achieving up to 50% reduction in file size and 50-80% reduction in token count. 2) Accuracy: Guarantees 100% lossless conversion from transliterated text back to the source language. 3) Efficiency: Eliminates the need for vocabulary expansion for low-resource languages, improving training and inference efficiency. 4) Scalability: The framework can be extended to other low-resource languages. We validate the effectiveness of our framework across multiple downstream tasks, including text classification, machine reading comprehension, and machine translation. Experimental results demonstrate that our method significantly enhances the model’s capability to process low-resource languages while maintaining performance on high-resource languages. Our data and code are publicly available at https://github.com/CMLI-NLP/HuffmanTranslit.</abstract>
      <url hash="08f67dd8">2025.acl-long.795</url>
      <bibkey>zhuang-etal-2025-enhancing</bibkey>
    </paper>
    <paper id="796">
      <title>Unmasking Style Sensitivity: A Causal Analysis of Bias Evaluation Instability in Large Language Models</title>
      <author><first>Jiaxu</first><last>Zhao</last></author>
      <author><first>Meng</first><last>Fang</last><affiliation>University of Liverpool and Eindhoven University of Technology</affiliation></author>
      <author id="kun-zhang"><first>Kun</first><last>Zhang</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Carnegie Mellon University</affiliation></author>
      <author><first>Mykola</first><last>Pechenizkiy</last><affiliation>Eindhoven University of Technology</affiliation></author>
      <pages>16314-16338</pages>
      <abstract>Natural language processing applications are increasingly prevalent, but social biases in their outputs remain a critical challenge. While various bias evaluation methods have been proposed, these assessments show unexpected instability when input texts undergo minor stylistic changes. This paper conducts a comprehensive analysis of how different style transformations impact bias evaluation results across multiple language models and bias types using causal inference techniques. Our findings reveal that formality transformations significantly affect bias scores, with informal style showing substantial bias reductions (up to 8.33% in LLaMA-2-13B). We identify appearance bias, sexual orientation bias, and religious bias as most susceptible to style changes, with variations exceeding 20%. Larger models demonstrate greater sensitivity to stylistic variations, with bias measurements fluctuating up to 3.1% more than in smaller models. These results highlight critical limitations in current bias evaluation methods and emphasize the need for reliable and fair assessments of language models.</abstract>
      <url hash="90664246">2025.acl-long.796</url>
      <bibkey>zhao-etal-2025-unmasking</bibkey>
    </paper>
    <paper id="797">
      <title><fixed-case>M</fixed-case>ock<fixed-case>C</fixed-case>onf: A Student Interpretation Dataset: Analysis, Word- and Span-level Alignment and Baselines</title>
      <author><first>Dávid</first><last>Javorský</last><affiliation>, Charles University Prague</affiliation></author>
      <author><first>Ondřej</first><last>Bojar</last><affiliation>Charles University Prague</affiliation></author>
      <author><first>François</first><last>Yvon</last><affiliation>ISIR, Sorbonne Université &amp; CNRS</affiliation></author>
      <pages>16339-16356</pages>
      <abstract>In simultaneous interpreting, an interpreter renders the speech into another language with a very short lag, much sooner than sentences are finished. In order to understand and later reproduce this dynamic and complex task automatically, we need specialized datasets and tools for analysis, monitoring, and evaluation, such as parallel speech corpora, and tools for their automatic annotation. Existing parallel corpora of translated texts and associated alignment algorithms hardly fill this gap, as they fail to model long-range interactions between speech segments or specific types of divergences (e.g. shortening, simplification, functional generalization) between the original and interpreted speeches. In this work, we develop and explore MockConf, a student interpretation dataset that was collected from Mock Conferences run as part of the students’ curriculum. This dataset contains 7 hours of recordings in 5 European languages, transcribed and aligned at the level of spans and words. We further implement and release InterAlign, a modern web-based annotation tool for parallel word and span annotations on long inputs, suitable for aligning simultaneous interpreting. We propose metrics for the evaluation and a baseline for automatic alignment. Dataset and tools will be released to the community.</abstract>
      <url hash="0e2a2970">2025.acl-long.797</url>
      <bibkey>javorsky-etal-2025-mockconf</bibkey>
    </paper>
    <paper id="798">
      <title><fixed-case>BMIKE</fixed-case>-53: Investigating Cross-Lingual Knowledge Editing with In-Context Learning</title>
      <author><first>Ercong</first><last>Nie</last></author>
      <author><first>Bo</first><last>Shao</last></author>
      <author><first>Mingyang</first><last>Wang</last></author>
      <author><first>Zifeng</first><last>Ding</last></author>
      <author><first>Helmut</first><last>Schmid</last><affiliation>Center for Information and Language Processing</affiliation></author>
      <author><first>Hinrich</first><last>Schuetze</last></author>
      <pages>16357-16374</pages>
      <abstract>This paper introduces BMIKE-53, a comprehensive benchmark for cross-lingual in-context knowledge editing (IKE), spanning 53 languages and three KE datasets: zsRE, CounterFact, and WikiFactDiff. Cross-lingual KE, which requires knowledge edited in one language to generalize across diverse languages while preserving unrelated knowledge, remains underexplored. To address this, we systematically evaluate IKE under zero-shot, one-shot, and few-shot setups, including tailored metric-specific demonstrations. Our findings reveal that model scale and demonstration alignment critically govern cross-lingual editing efficacy, with larger models and tailored demonstrations significantly improving performance. Linguistic properties, particularly script type, strongly influence outcomes, with non-Latin languages underperforming due to issues like language confusion.</abstract>
      <url hash="7b264666">2025.acl-long.798</url>
      <bibkey>nie-etal-2025-bmike</bibkey>
    </paper>
    <paper id="799">
      <title>What Matters in Evaluating Book-Length Stories? A Systematic Study of Long Story Evaluation</title>
      <author><first>Dingyi</first><last>Yang</last></author>
      <author><first>Qin</first><last>Jin</last><affiliation>Renmin University of China</affiliation></author>
      <pages>16375-16398</pages>
      <abstract>In this work, we conduct systematic research in a challenging area: the automatic evaluation of book-length stories (&gt;100K tokens). Our study focuses on two key questions: (1) understanding which evaluation aspects matter most to readers, and (2) exploring effective methods for evaluating lengthy stories. We introduce the first large-scale benchmark, **LongStoryEval**, comprising 600 newly published books with an average length of 121K tokens (maximum 397K). Each book includes its average rating and multiple reader reviews, presented as critiques organized by evaluation aspects. By analyzing all user-mentioned aspects, we propose an *evaluation criteria structure* and conduct experiments to identify the most significant aspects among the 8 top-level criteria. For evaluation methods, we compare the effectiveness of three types: *aggregation-based*, *incremental-updated*, and *summary-based* evaluations. Our findings reveal that aggregation- and summary-based evaluations perform better, with the former excelling in detail assessment and the latter offering greater efficiency. Building on these insights, we further propose **NovelCritique**, an 8B model that leverages the efficient summary-based framework to review and score stories across specified aspects. NovelCritique outperforms commercial models like GPT-4o in aligning with human evaluations. All our datasets and codes will be released to foster further research.</abstract>
      <url hash="dcc1af3a">2025.acl-long.799</url>
      <bibkey>yang-jin-2025-matters</bibkey>
    </paper>
    <paper id="800">
      <title><fixed-case>PROPER</fixed-case>: A Progressive Learning Framework for Personalized Large Language Models with Group-Level Adaptation</title>
      <author><first>Linhai</first><last>Zhang</last></author>
      <author><first>Jialong</first><last>Wu</last><affiliation>Southeast University</affiliation></author>
      <author><first>Deyu</first><last>Zhou</last><affiliation>Southeast University</affiliation></author>
      <author><first>Yulan</first><last>He</last><affiliation>King’s College London, University of London</affiliation></author>
      <pages>16399-16411</pages>
      <abstract>Personalized large language models (LLMs) aim to tailor their outputs to user preferences. Recent advances in parameter-efficient fine-tuning (PEFT) methods have highlighted the effectiveness of adapting population-level LLMs to personalized LLMs by fine-tuning user-specific parameters with user history. However, user data is typically sparse, making it challenging to adapt LLMs to specific user patterns. To address this challenge, we propose PROgressive PERsonalization (PROPER), a novel progressive learning framework inspired by meso-level theory in social science. PROPER bridges population-level and user-level models by grouping users based on preferences and adapting LLMs in stages. It combines a Mixture-of-Experts (MoE) structure with Low Ranked Adaptation (LoRA), using a user-aware router to assign users to appropriate groups automatically. Additionally, a LoRA-aware router is proposed to facilitate the integration of individual user LoRAs with the group-level LoRA. Experimental results show that PROPER significantly outperforms SOTA models across multiple tasks, demonstrating the effectiveness of our approach.</abstract>
      <url hash="24285e1e">2025.acl-long.800</url>
      <bibkey>zhang-etal-2025-proper</bibkey>
    </paper>
    <paper id="801">
      <title>Enhancing Event-centric News Cluster Summarization via Data Sharpening and Localization Insights</title>
      <author><first>Longyin</first><last>Zhang</last></author>
      <author><first>Bowei</first><last>Zou</last><affiliation>A*STAR</affiliation></author>
      <author><first>AiTi</first><last>Aw</last><affiliation>I2R</affiliation></author>
      <pages>16412-16426</pages>
      <abstract>This paper tackles the challenges of clustering news articles by main events (MEs) and summarizing these clusters, focusing on diverse languages and localized contexts. Our approach consists of four key contributions. First, we investigate the role of dynamic clustering and the integration of various ME references, including event attributions extracted by language models (LMs), in enhancing event-centric clustering. Second, we propose a data-sharpening framework that optimizes the balance between information volume and entropy in input texts, thereby optimizing generated summaries on multiple indicators. Third, we fine-tune LMs with local news articles for cross-lingual temporal question-answering and text summarization, achieving notable improvements in capturing localized contexts. Lastly, we present the first cross-lingual dataset and comprehensive evaluation metrics tailored for the event-centric news cluster summarization pipeline. Our findings enhance the understanding of news summarization across N-gram, event-level coverage, and faithfulness, providing new insights into leveraging LMs for large-scale cross-lingual and localized news analysis.</abstract>
      <url hash="9aa03c47">2025.acl-long.801</url>
      <bibkey>zhang-etal-2025-enhancing-event</bibkey>
    </paper>
    <paper id="802">
      <title><fixed-case>MMB</fixed-case>oundary: Advancing <fixed-case>MLLM</fixed-case> Knowledge Boundary Awareness through Reasoning Step Confidence Calibration</title>
      <author><first>Zhitao</first><last>He</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Sandeep</first><last>Polisetty</last></author>
      <author><first>Zhiyuan</first><last>Fan</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Yuchen</first><last>Huang</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Shujin</first><last>Wu</last></author>
      <author><first>Yi R.</first><last>Fung</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <pages>16427-16444</pages>
      <abstract>In recent years, multimodal large language models (MLLMs) have made significant progress but continue to face inherent challenges in multimodal reasoning, which requires multi-level (e.g., perception, reasoning) and multi-granular (e.g., multi-step reasoning chain) advanced inferencing. Prior work on estimating model confidence tends to focus on the overall response for training and calibration, but fails to assess confidence in each reasoning step, leading to undesirable hallucination snowballing. In this work, we present MMBoundary, a novel framework that advances the knowledge boundary awareness of MLLMs through reasoning step confidence calibration. To achieve this, we propose to incorporate complementary textual and cross-modal self-rewarding signals to estimate confidence at each step of the MLLM reasoning process. In addition to supervised fine-tuning MLLM on this set of self-rewarding confidence estimation signal for initial confidence expression warm-up, we introduce a reinforcement learning stage with multiple reward functions for further aligning model knowledge and calibrating confidence at each reasoning step, enhancing reasoning chain self-correction. Empirical results show that MMBoundary significantly outperforms existing methods across diverse domain datasets and metrics, achieving an average of 7.5% reduction in multimodal confidence calibration errors and up to 8.3% improvement in task performance.</abstract>
      <url hash="9eaa5696">2025.acl-long.802</url>
      <bibkey>he-etal-2025-mmboundary</bibkey>
    </paper>
    <paper id="803">
      <title><fixed-case>LIFB</fixed-case>ench: Evaluating the Instruction Following Performance and Stability of Large Language Models in Long-Context Scenarios</title>
      <author><first>Xiaodong</first><last>Wu</last></author>
      <author><first>Minhao</first><last>Wang</last></author>
      <author><first>Yichen</first><last>Liu</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Xiaoming</first><last>Shi</last><affiliation>East China Normal University</affiliation></author>
      <author><first>He</first><last>Yan</last><affiliation>iQIYI</affiliation></author>
      <author><first>Lu</first><last>Xiangju</last></author>
      <author><first>Junmin</first><last>Zhu</last></author>
      <author><first>Wei</first><last>Zhang</last><affiliation>East China Normal University</affiliation></author>
      <pages>16445-16468</pages>
      <abstract>As Large Language Models (LLMs) evolve in natural language processing (NLP), their ability to stably follow instructions in long-context inputs has become critical for real-world applications. However, existing benchmarks seldom focus on instruction-following in long-context scenarios or stability on different inputs. To bridge this gap, we introduce LIFBench, a scalable dataset designed to evaluate LLMs’ instruction-following capabilities and stability across long contexts. LIFBench comprises three long-context scenarios and eleven diverse tasks, featuring 2,766 instructions generated through an automated expansion method across three dimensions: length, expression, and variables. For evaluation, we propose LIFEval, a rubric-based assessment method that enables precise, automated scoring of complex LLM responses without reliance on LLM-assisted assessments or human judgment. This method allows for a comprehensive analysis of model performance and stability from multiple perspectives. We conduct detailed experiments on 20 prominent LLMs across six length intervals. Our work contributes LIFBench and LIFEval as robust tools for assessing LLM performance in complex and long-context settings, offering valuable insights to guide future advancements in LLM development.</abstract>
      <url hash="f348cced">2025.acl-long.803</url>
      <bibkey>wu-etal-2025-lifbench</bibkey>
    </paper>
    <paper id="804">
      <title>Aligning Large Language Models to Follow Instructions and Hallucinate Less via Effective Data Filtering</title>
      <author><first>Shuzheng</first><last>Si</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Haozhe</first><last>Zhao</last></author>
      <author><first>Gang</first><last>Chen</last></author>
      <author><first>Cheng</first><last>Gao</last></author>
      <author><first>Yuzhuo</first><last>Bai</last></author>
      <author><first>Zhitong</first><last>Wang</last></author>
      <author><first>Kaikai</first><last>An</last></author>
      <author><first>Kangyang</first><last>Luo</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Chen</first><last>Qian</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Fanchao</first><last>Qi</last></author>
      <author><first>Baobao</first><last>Chang</last><affiliation>Peking University</affiliation></author>
      <author><first>Maosong</first><last>Sun</last><affiliation>Tsinghua University</affiliation></author>
      <pages>16469-16488</pages>
      <abstract>Training LLMs on data containing unfamiliar knowledge during the instruction tuning stage can encourage hallucinations. To address this challenge, we introduce NOVA, a novel framework designed to identify high-quality data that aligns well with the LLM’s learned knowledge to reduce hallucinations. NOVA includes Internal Consistency Probing (ICP) and Semantic Equivalence Identification (SEI) to measure how familiar the LLM is with instruction data. Specifically, ICP evaluates the LLM’s understanding of the given instruction by calculating the tailored consistency among multiple self-generated responses. SEI further assesses the familiarity of the LLM with the target response by comparing it to the generated responses, using the proposed semantic clustering and well-designed voting strategy. Finally, to ensure the quality of selected samples, we introduce an expert-aligned reward model, considering characteristics beyond just familiarity. By considering data quality and avoiding unfamiliar data, we can utilize the selected data to effectively align LLMs to follow instructions and hallucinate less. Experiments show that NOVA significantly reduces hallucinations while maintaining a competitive ability to follow instructions.</abstract>
      <url hash="bb028c66">2025.acl-long.804</url>
      <bibkey>si-etal-2025-aligning</bibkey>
    </paper>
    <paper id="805">
      <title>One-Shot is Enough: Consolidating Multi-Turn Attacks into Efficient Single-Turn Prompts for <fixed-case>LLM</fixed-case>s</title>
      <author><first>Junwoo</first><last>Ha</last><affiliation>AIM Intelligence and University of Seoul</affiliation></author>
      <author><first>Hyunjun</first><last>Kim</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Sangyoon</first><last>Yu</last><affiliation>AIM Intelligence</affiliation></author>
      <author><first>Haon</first><last>Park</last><affiliation>AIM Intelligence</affiliation></author>
      <author><first>Ashkan</first><last>Yousefpour</last></author>
      <author><first>Yuna</first><last>Park</last></author>
      <author><first>Suhyun</first><last>Kim</last><affiliation>Kyung Hee University</affiliation></author>
      <pages>16489-16507</pages>
      <abstract>We introduce a novel framework for consolidating multi-turn adversarial “jailbreak” prompts into single-turn queries, significantly reducing the manual overhead required for adversarial testing of large language models (LLMs). While multi-turn human jailbreaks have been shown to yield high attack success rates (ASRs), they demand considerable human effort and time. Our proposed Multi-turn-to-Single-turn (M2S) methods—Hyphenize, Numberize, and Pythonize—systematically reformat multi-turn dialogues into structured single-turn prompts. Despite eliminating iterative back-and-forth interactions, these reformatted prompts preserve and often enhance adversarial potency: in extensive evaluations on the Multi-turn Human Jailbreak (MHJ) dataset, M2S methods yield ASRs ranging from 70.6 % to 95.9 % across various state-of-the-art LLMs. Remarkably, our single-turn prompts outperform the original multi-turn attacks by up to 17.5 % in absolute ASR, while reducing token usage by more than half on average. Further analyses reveal that embedding malicious requests in enumerated or code-like structures exploits “contextual blindness,” undermining both native guardrails and external input-output safeguards. By consolidating multi-turn conversations into efficient single-turn prompts, our M2S framework provides a powerful tool for large-scale red-teaming and exposes critical vulnerabilities in contemporary LLM defenses. All code, data, and conversion prompts are available for reproducibility and further investigations: https://github.com/Junuha/M2S_DATA</abstract>
      <url hash="a01d9a46">2025.acl-long.805</url>
      <bibkey>ha-etal-2025-one</bibkey>
    </paper>
    <paper id="806">
      <title><fixed-case>RAE</fixed-case>mo<fixed-case>LLM</fixed-case>: Retrieval Augmented <fixed-case>LLM</fixed-case>s for Cross-Domain Misinformation Detection Using In-Context Learning Based on Emotional Information</title>
      <author><first>Zhiwei</first><last>Liu</last></author>
      <author><first>Kailai</first><last>Yang</last></author>
      <author><first>Qianqian</first><last>Xie</last></author>
      <author><first>Christine</first><last>de Kock</last></author>
      <author><first>Sophia</first><last>Ananiadou</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Eduard</first><last>Hovy</last><affiliation>University of Melbourne and Carnegie Mellon University</affiliation></author>
      <pages>16508-16523</pages>
      <abstract>Misinformation is prevalent in various fields such as education, politics, health, etc., causing significant harm to society. However, current methods for cross-domain misinformation detection rely on effort- and resource-intensive fine-tuning and complex model structures. With the outstanding performance of LLMs, many studies have employed them for misinformation detection. Unfortunately, they focus on in-domain tasks and do not incorporate significant sentiment and emotion features (which we jointly call <i>affect</i>). In this paper, we propose RAEmoLLM, the first retrieval augmented (RAG) LLMs framework to address cross-domain misinformation detection using in-context learning based on affective information. RAEmoLLM includes three modules. (1) In the index construction module, we apply an emotional LLM to obtain affective embeddings from all domains to construct a retrieval database. (2) The retrieval module uses the database to recommend top K examples (text-label pairs) from source domain data for target domain contents. (3) These examples are adopted as few-shot demonstrations for the inference module to process the target domain content. The RAEmoLLM can effectively enhance the general performance of LLMs in cross-domain misinformation detection tasks through affect-based retrieval, without fine-tuning. We evaluate our framework on three misinformation benchmarks. Results show that RAEmoLLM achieves significant improvements compared to the other few-shot methods on three datasets, with the highest increases of 15.64%, 31.18%, and 15.73% respectively. This project is available at https://github.com/lzw108/RAEmoLLM.</abstract>
      <url hash="60285c57">2025.acl-long.806</url>
      <bibkey>liu-etal-2025-raemollm</bibkey>
    </paper>
    <paper id="807">
      <title>Task-Specific Information Decomposition for End-to-End Dense Video Captioning</title>
      <author><first>Zhiyue</first><last>Liu</last><affiliation>Guangxi University</affiliation></author>
      <author><first>Xinru</first><last>Zhang</last></author>
      <author><first>Jinyuan</first><last>Liu</last><affiliation>Guangxi University</affiliation></author>
      <pages>16524-16536</pages>
      <abstract>Dense video captioning aims to localize events within input videos and generate concise descriptive texts for each event. Advanced end-to-end methods require both tasks to share the same intermediate features that serve as event queries, thereby enabling the mutual promotion of two tasks. However, relying on shared queries limits the model’s ability to extract task-specific information, as event semantic perception and localization demand distinct perspectives on video understanding. To address this, we propose a decomposed dense video captioning framework that derives localization and captioning queries from event queries, enabling task-specific representations while maintaining inter-task collaboration. Considering the roles of different queries, we design a contrastive semantic optimization strategy that guides localization queries to focus on event-level visual features and captioning queries to align with textual semantics. Besides, only localization information is considered in existing methods for label assignment, failing to ensure the relevance of the selected queries to descriptions. We jointly consider localization and captioning losses to achieve a semantically balanced assignment process. Extensive experiments on the YouCook2 and ActivityNet Captions datasets demonstrate that our framework achieves state-of-the-art performance.</abstract>
      <url hash="8d1241c3">2025.acl-long.807</url>
      <bibkey>liu-etal-2025-task</bibkey>
    </paper>
    <paper id="808">
      <title><fixed-case>C</fixed-case>alibra<fixed-case>E</fixed-case>val: Calibrating Prediction Distribution to Mitigate Selection Bias in <fixed-case>LLM</fixed-case>s-as-Judges</title>
      <author><first>Haitao</first><last>Li</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Junjie</first><last>Chen</last></author>
      <author><first>Qingyao</first><last>Ai</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Zhumin</first><last>Chu</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Yujia</first><last>Zhou</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Qian</first><last>Dong</last></author>
      <author><first>Yiqun</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <pages>16537-16552</pages>
      <abstract>The use of large language models (LLMs) as automated evaluation tools to assess the quality of generated natural language, known as ”LLMs-as-Judges”, has demonstrated promising capabilities and is rapidly gaining widespread attention. However, when applied to pairwise comparisons of candidate responses, LLM-based evaluators often exhibit selection bias. Specifically, their judgments may become inconsistent when the option positions or ID tokens are swapped, compromising the effectiveness and fairness of the evaluation result. To address this challenge, we introduce CalibraEval, a novel label-free method for mitigating selection bias during inference. Specifically, CalibraEval reformulates debiasing as an optimization task aimed at adjusting observed prediction distributions to align with unbiased prediction distributions. To solve this optimization problem, we propose a non-parametric order-preserving algorithm (NOA). This algorithm leverages the partial order relationships between model prediction distributions, thereby eliminating the need for explicit labels and precise mathematical function modeling. Empirical evaluations of LLMs in multiple representative benchmarks demonstrate that CalibraEval effectively mitigates selection bias and improves performance compared to existing debiasing methods. This work marks a step toward building more robust and unbiased automated evaluation frameworks, paving the way for improved reliability in AI-driven assessments. The code can be found at https://github.com/CSHaitao/CalibraEval.</abstract>
      <url hash="70bd3584">2025.acl-long.808</url>
      <bibkey>li-etal-2025-calibraeval</bibkey>
    </paper>
    <paper id="809">
      <title>Explaining Matters: Leveraging Definitions and Semantic Expansion for Sexism Detection</title>
      <author><first>Sahrish</first><last>Khan</last></author>
      <author><first>Arshad</first><last>Jhumka</last></author>
      <author><first>Gabriele</first><last>Pergola</last><affiliation>University of Warwick</affiliation></author>
      <pages>16553-16571</pages>
      <abstract>The detection of sexism in online content remains an open problem, as harmful language disproportionately affects women and marginalized groups. While automated systems for sexism detection have been developed, they still face two key challenges: data sparsity and the nuanced nature of sexist language. Even in large, well-curated datasets like the Explainable Detection of Online Sexism (EDOS), severe class imbalance hinders model generalization. Additionally, the overlapping and ambiguous boundaries of fine-grained categories introduce substantial annotator disagreement, reflecting the difficulty of interpreting nuanced expressions of sexism. To address these challenges, we propose two prompt-based data augmentation techniques: Definition-based Data Augmentation (DDA), which leverages category-specific definitions to generate semantically-aligned synthetic examples, and Contextual Semantic Expansion (CSE), which targets systematic model errors by enriching examples with task-specific semantic features. To further improve reliability in fine-grained classification, we introduce an ensemble strategy that resolves prediction ties by aggregating complementary perspectives from multiple language models. Our experimental evaluation on the EDOS dataset demonstrates state-of-the-art performance across all tasks, with notable improvements of macro F1 by 1.5 points for binary classification (Task A) and 4.1 points for fine-grained classification (Task C).</abstract>
      <url hash="65d04071">2025.acl-long.809</url>
      <bibkey>khan-etal-2025-explaining</bibkey>
    </paper>
    <paper id="810">
      <title>Private Memorization Editing: Turning Memorization into a Defense to Strengthen Data Privacy in Large Language Models</title>
      <author><first>Elena Sofia</first><last>Ruzzetti</last><affiliation>Università degli Studi di Roma Tor Vergata</affiliation></author>
      <author><first>Giancarlo A.</first><last>Xompero</last><affiliation>University of Rome Tor Vergata and Almawave SpA</affiliation></author>
      <author><first>Davide</first><last>Venditti</last></author>
      <author><first>Fabio Massimo</first><last>Zanzotto</last><affiliation>University of Rome Tor Vergata</affiliation></author>
      <pages>16572-16592</pages>
      <abstract>Large Language Models (LLMs) memorize, and thus, among huge amounts of uncontrolled data, may memorize Personally Identifiable Information (PII), which should not be stored and, consequently, not leaked. In this paper, we introduce Private Memorization Editing (PME), an approach for preventing private data leakage that turns an apparent limitation, that is, the LLMs’ memorization ability, into a powerful privacy defense strategy. While attacks against LLMs have been performed exploiting previous knowledge regarding their training data, our approach aims to exploit the same kind of knowledge in order to make a model more robust. We detect a memorized PII and then mitigate the memorization of PII by editing a model knowledge of its training data. We verify that our procedure does not affect the underlying language model while making it more robust against privacy Training Data Extraction attacks. We demonstrate that PME can effectively reduce the number of leaked PII in a number of configurations, in some cases even reducing the accuracy of the privacy attacks to zero.</abstract>
      <url hash="0d44168f">2025.acl-long.810</url>
      <bibkey>ruzzetti-etal-2025-private</bibkey>
    </paper>
    <paper id="811">
      <title><fixed-case>P</fixed-case>hys<fixed-case>R</fixed-case>eason: A Comprehensive Benchmark towards Physics-Based Reasoning</title>
      <author><first>Xinyu</first><last>Zhang</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Yuxuan</first><last>Dong</last></author>
      <author><first>Yanrui</first><last>Wu</last></author>
      <author><first>Jiaxing</first><last>Huang</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Chengyou</first><last>Jia</last></author>
      <author><first>Basura</first><last>Fernando</last><affiliation>Nanyang Technological University and A*STAR</affiliation></author>
      <author><first>Mike Zheng</first><last>Shou</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Lingling</first><last>Zhang</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Jun</first><last>Liu</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <pages>16593-16615</pages>
      <abstract>Large language models demonstrate remarkable capabilities across various domains, especially mathematics and logic reasoning. However, current evaluations overlook physics-based reasoning - a complex task requiring physics theorems and constraints. We present PhysReason, a 1,200-problem benchmark comprising knowledge-based (25%) and reasoning-based (75%) problems, where the latter are divided into three difficulty levels (easy, medium, hard). Notably, problems require an average of 8.1 solution steps, with hard requiring 15.6, reflecting the complexity of physics-based reasoning. We propose the Physics Solution Auto Scoring Framework, incorporating efficient answer-level and comprehensive step-level evaluations. Top-performing models like Deepseek-R1, Gemini-2.0-Flash-Thinking, and o3-mini-high achieve less than 60% on answer-level evaluation, with performance dropping from knowledge questions (75.11%) to hard problems (31.95%). Through step-level evaluation, we identified four key bottlenecks: Physics Theorem Application, Physics Process Understanding, Calculation, and Physics Condition Analysis. These findings position PhysReason as a novel and comprehensive benchmark for evaluating physics-based reasoning capabilities in large language models.</abstract>
      <url hash="eacdae1e">2025.acl-long.811</url>
      <bibkey>zhang-etal-2025-physreason</bibkey>
    </paper>
    <paper id="812">
      <title>Does Time Have Its Place? Temporal Heads: Where Language Models Recall Time-specific Information</title>
      <author><first>Yein</first><last>Park</last><affiliation>Korea University</affiliation></author>
      <author><first>Chanwoong</first><last>Yoon</last><affiliation>Korea University</affiliation></author>
      <author><first>Jungwoo</first><last>Park</last><affiliation>Korea University</affiliation></author>
      <author><first>Minbyul</first><last>Jeong</last></author>
      <author><first>Jaewoo</first><last>Kang</last><affiliation>Korea University</affiliation></author>
      <pages>16616-16643</pages>
      <abstract>While the ability of language models to elicit facts has been widely investigated, how they handle temporally changing facts remains underexplored. We discover Temporal Heads, specific attention heads that primarily handle temporal knowledge, through circuit analysis. We confirm that these heads are present across multiple models, though their specific locations may vary, and their responses differ depending on the type of knowledge and its corresponding years. Disabling these heads degrades the model’s ability to recall time-specific knowledge while maintaining its general capabilities without compromising time-invariant and question-answering performances. Moreover, the heads are activated not only numeric conditions (“In 2004”) but also textual aliases (“In the year ...”), indicating that they encode a temporal dimension beyond simple numerical representation. Furthermore, we expand the potential of our findings by demonstrating how temporal knowledge can be edited by adjusting the values of these heads.</abstract>
      <url hash="caf88b12">2025.acl-long.812</url>
      <bibkey>park-etal-2025-time</bibkey>
    </paper>
    <paper id="813">
      <title>Velocitune: A Velocity-based Dynamic Domain Reweighting Method for Continual Pre-training</title>
      <author><first>Zheheng</first><last>Luo</last></author>
      <author><first>Xin</first><last>Zhang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Xiao</first><last>Liu</last><affiliation>Microsoft Research Asia</affiliation></author>
      <author><first>Haoling</first><last>Li</last></author>
      <author><first>Yeyun</first><last>Gong</last></author>
      <author><first>Qi</first><last>Chen</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Peng</first><last>Cheng</last><affiliation>Microsoft Research</affiliation></author>
      <pages>16644-16656</pages>
      <abstract>It is well-known that a diverse corpus is critical for training large language models, which are typically constructed from a mixture of various domains. In general, previous efforts resort to either sampling training data from different domains with static proportions or dynamically adjusting these proportions during training to optimise pretraining performance. However, few methods addressed the complexities of domain-adaptive continual pre-training. To fill this gap, we propose Velocitune, a novel framework that dynamically assesses learning velocity and adjusts data proportions accordingly, favouring slower learning domains while de-emphasising faster learning ones, which is guided by a scaling law to estimate the desired learning goal for each domain with a less associated cost. To evaluate the effectiveness of Velocitune, we conduct experiments on a dataset focused on reasoning tasks with CodeLlama, as well as on a corpus of system commands using Llama3 and Mistral. Velocitune achieves performance gains in both math and code reasoning tasks and command-line generation benchmarks. Further analysis reveals that key factors driving Velocitune’s effectiveness include target estimation and data ordering.</abstract>
      <url hash="cc034184">2025.acl-long.813</url>
      <bibkey>luo-etal-2025-velocitune</bibkey>
    </paper>
    <paper id="814">
      <title>Sheep’s Skin, Wolf’s Deeds: Are <fixed-case>LLM</fixed-case>s Ready for Metaphorical Implicit Hate Speech?</title>
      <author><first>Jingjie</first><last>Zeng</last></author>
      <author><first>Liang</first><last>Yang</last><affiliation>Dalian University of Technology</affiliation></author>
      <author><first>Zekun</first><last>Wang</last></author>
      <author><first>Yuanyuan</first><last>Sun</last><affiliation>Dalian University of Technology</affiliation></author>
      <author><first>Hongfei</first><last>Lin</last></author>
      <pages>16657-16677</pages>
      <abstract>Implicit hate speech has become a significant challenge for online platforms, as it often avoids detection by large language models (LLMs) due to its indirectly expressed hateful intent. This study identifies the limitations of LLMs in detecting implicit hate speech, particularly when disguised as seemingly harmless expressions in a rhetorical device. To address this challenge, we employ a Jailbreaking strategy and Energy-based Constrained Decoding techniques, and design a small model for measuring the energy of metaphorical rhetoric. This approach can lead to LLMs generating metaphorical implicit hate speech. Our research reveals that advanced LLMs, like GPT-4o, frequently misinterpret metaphorical implicit hate speech, and fail to prevent its propagation effectively. Even specialized models, like ShieldGemma and LlamaGuard, demonstrate inadequacies in blocking such content, often misclassifying it as harmless speech. This work points out the vulnerability of current LLMs to implicit hate speech, and emphasizes the improvements to address hate speech threats better.</abstract>
      <url hash="54ba401a">2025.acl-long.814</url>
      <bibkey>zeng-etal-2025-sheeps</bibkey>
    </paper>
    <paper id="815">
      <title>Neuron-Level Sequential Editing for Large Language Models</title>
      <author><first>Houcheng</first><last>Jiang</last></author>
      <author><first>Junfeng</first><last>Fang</last></author>
      <author><first>Tianyu</first><last>Zhang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Baolong</first><last>Bi</last></author>
      <author><first>An</first><last>Zhang</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Ruipeng</first><last>Wang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Tao</first><last>Liang</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Xiang</first><last>Wang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>16678-16702</pages>
      <abstract>This work explores sequential model editing in large language models (LLMs), a critical task that involves modifying internal knowledge within LLMs continuously through multi-round editing, each incorporating updates or corrections to adjust the model’s outputs without the need for costly retraining. Existing model editing methods, especially those that alter model parameters, typically focus on single-round editing and often face significant challenges in sequential model editing-most notably issues of model forgetting and failure. To address these challenges, we introduce a new model editing method, namely <b>N</b>euron-level <b>S</b>equential <b>E</b>diting (NSE), tailored for supporting sequential model editing. Specifically, we optimize the target layer’s hidden states using the model’s original weights to prevent model failure. Furthermore, we iteratively select neurons in multiple layers for editing based on their activation values to mitigate model forgetting. Our empirical experiments demonstrate that NSE significantly outperforms current modifying parameters model editing methods, marking a substantial advancement in the field of sequential model editing. Our code is released on <url>https://anonymous.4open.science/r/NSE-0A8D/</url>.</abstract>
      <url hash="f9cbb9b1">2025.acl-long.815</url>
      <bibkey>jiang-etal-2025-neuron</bibkey>
    </paper>
    <paper id="816">
      <title>Automatic Expert Discovery in <fixed-case>LLM</fixed-case> Upcycling via Sparse Interpolated Mixture-of-Experts</title>
      <author><first>Shengzhuang</first><last>Chen</last><affiliation>City University of Hong Kong</affiliation></author>
      <author><first>Ying</first><last>Wei</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Jonathan Richard</first><last>Schwarz</last></author>
      <pages>16703-16717</pages>
      <abstract>We present Sparse Interpolated Mixture-of-Experts (SIMoE) instruction-tuning, an end-to-end algorithm designed to fine-tune a dense pre-trained Large Language Model (LLM) into a MoE-style model that possesses capabilities in multiple specialized domains. During instruction-tuning, SIMoE automatically identifies multiple specialized experts under a specified sparsity constraint, with each expert representing a structurally sparse subset of the seed LLM’s parameters that correspond to domain-specific knowledge within the data. SIMoE simultaneously learns an input-dependent expert merging strategy via a router network, leveraging rich cross-expert knowledge for superior downstream generalization that surpasses existing baselines. Empirically, SIMoE consistently achieves state-of-the-art performance on common instruction-tuning benchmarks while maintaining an optimal performance-compute trade-off compared to all baselines.</abstract>
      <url hash="26090255">2025.acl-long.816</url>
      <bibkey>chen-etal-2025-automatic</bibkey>
    </paper>
    <paper id="817">
      <title><fixed-case>S</fixed-case>imul<fixed-case>S</fixed-case>2<fixed-case>S</fixed-case>-<fixed-case>LLM</fixed-case>: Unlocking Simultaneous Inference of Speech <fixed-case>LLM</fixed-case>s for Speech-to-Speech Translation</title>
      <author><first>Keqi</first><last>Deng</last></author>
      <author><first>Wenxi</first><last>Chen</last></author>
      <author><first>Xie</first><last>Chen</last></author>
      <author><first>Phil</first><last>Woodland</last><affiliation>University of Cambridge</affiliation></author>
      <pages>16718-16734</pages>
      <abstract>Simultaneous speech translation (SST) outputs translations in parallel with streaming speech input, balancing translation quality and latency. While large language models (LLMs) have been extended to handle the speech modality, streaming remains challenging as speech is pre-pended as a prompt for the entire generation process. To unlock LLM streaming capability, this paper proposes SimulS2S-LLM, which trains speech LLMs offline and employs a test-time policy to guide simultaneous inference. SimulS2S-LLM alleviates the mismatch between training and inference by extracting boundary-aware speech prompts that allows it to be better matched with text input data. SimulS2S-LLM achieves simultaneous speech-to-speech translation (Simul-S2ST) by predicting discrete output speech tokens and then synthesising output speech using a pre-trained vocoder. An incremental beam search is designed to expand the search space of speech token prediction without increasing latency. Experiments on the CVSS speech data show that SimulS2S-LLM offers a better translation quality-latency trade-off than existing methods that use the same training data, such as improving ASR-BLEU scores by 3 points at similar latency.</abstract>
      <url hash="4f50ceee">2025.acl-long.817</url>
      <bibkey>deng-etal-2025-simuls2s</bibkey>
    </paper>
    <paper id="818">
      <title><fixed-case>V</fixed-case>ox<fixed-case>E</fixed-case>val: Benchmarking the Knowledge Understanding Capabilities of End-to-End Spoken Language Models</title>
      <author><first>Wenqian</first><last>Cui</last></author>
      <author><first>Xiaoqi</first><last>Jiao</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Ziqiao</first><last>Meng</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Irwin</first><last>King</last></author>
      <pages>16735-16753</pages>
      <abstract>With the rising need for speech-based interaction models, end-to-end Spoken Language Models (SLMs) have emerged as a promising solution. While these models require comprehensive world knowledge for meaningful and reliable human interactions, existing question-answering (QA) benchmarks fall short in evaluating SLMs’ knowledge understanding due to their inability to support end-to-end speech evaluation and account for varied input audio conditions. To address these limitations, we present VoxEval, a novel SpeechQA benchmark that assesses SLMs’ knowledge understanding through pure speech interactions. Our benchmark uniquely maintains speech format for both inputs and outputs, evaluates model robustness across diverse input audio conditions, and pioneers the assessment of complex tasks like mathematical reasoning in spoken format. Through systematic evaluation, we demonstrate that current SLMs exhibit poor performance on VoxEval, show sensitivity to varying audio conditions, and possess limited reasoning capabilities, highlighting critical areas for future development. VoxEval dataset is available at: https://github.com/dreamtheater123/VoxEval</abstract>
      <url hash="861e4285">2025.acl-long.818</url>
      <bibkey>cui-etal-2025-voxeval</bibkey>
    </paper>
    <paper id="819">
      <title><fixed-case>R</fixed-case>etro<fixed-case>LLM</fixed-case>: Empowering Large Language Models to Retrieve Fine-grained Evidence within Generation</title>
      <author><first>Xiaoxi</first><last>Li</last></author>
      <author><first>Jiajie</first><last>Jin</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Yujia</first><last>Zhou</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Yongkang</first><last>Wu</last></author>
      <author><first>Zhonghua</first><last>Li</last></author>
      <author><first>Ye</first><last>Qi</last></author>
      <author><first>Zhicheng</first><last>Dou</last><affiliation>Renmin University of China</affiliation></author>
      <pages>16754-16779</pages>
      <abstract>Large language models (LLMs) exhibit remarkable generative capabilities but often suffer from hallucinations. Retrieval-augmented generation (RAG) offers an effective solution by incorporating external knowledge, but existing methods still face several limitations: additional deployment costs of separate retrievers, redundant input tokens from retrieved text chunks, and the lack of joint optimization of retrieval and generation. To address these issues, we propose <b>RetroLLM</b>, a unified framework that integrates retrieval and generation into a single, auto-regressive process, enabling LLMs to directly generate fine-grained evidence from the corpus with constrained decoding. Moreover, to mitigate false pruning in the process of constrained evidence generation, we introduce (1) hierarchical FM-Index constraints, which generate corpus-constrained clues to identify a subset of relevant documents before evidence generation, reducing irrelevant decoding space; and (2) a forward-looking constrained decoding strategy, which considers the relevance of future sequences to improve evidence accuracy. Extensive experiments on five open-domain QA datasets demonstrate RetroLLM’s superior performance across both in-domain and out-of-domain tasks. The code is available at https://anonymous.4open.science/r/RetroLLM-D95A.</abstract>
      <url hash="528d0939">2025.acl-long.819</url>
      <bibkey>li-etal-2025-retrollm</bibkey>
    </paper>
    <paper id="820">
      <title>The Role of Deductive and Inductive Reasoning in Large Language Models</title>
      <author><first>Chengkun</first><last>Cai</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Xu</first><last>Zhao</last><affiliation>Southeast University - Monash University Joint Graduate School</affiliation></author>
      <author><first>Haoliang</first><last>Liu</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Zhongyu</first><last>Jiang</last></author>
      <author><first>Tianfang</first><last>Zhang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Zongkai</first><last>Wu</last></author>
      <author><first>Jenq-Neng</first><last>Hwang</last></author>
      <author><first>Lei</first><last>Li</last></author>
      <pages>16780-16790</pages>
      <abstract>Large Language Models (LLMs) have demonstrated impressive capabilities in reasoning tasks, yet their reliance on static prompt structures and limited adaptability to complex scenarios remains a major challenge. In this paper, we propose the **Deductive and Inductive (DID)** method, a novel framework that enhances LLM reasoning by dynamically integrating both deductive and inductive reasoning approaches. Drawing from cognitive science principles, DID implements a dual-metric complexity evaluation system that combines Littlestone dimension and information entropy to precisely assess task difficulty and guide decomposition strategies. DID enables the model to progressively adapt its reasoning pathways based on problem complexity, mirroring human cognitive processes. We evaluate DID’s effectiveness across multiple benchmarks, including the AIW, MR-GSM8K, and our custom Holiday Puzzle dataset for temporal reasoning. Our results demonstrate great improvements in reasoning quality and solution accuracy - achieving 70.3% accuracy on AIW (compared to 62.2% for Tree of Thought), while maintaining lower computational costs.</abstract>
      <url hash="41c46401">2025.acl-long.820</url>
      <bibkey>cai-etal-2025-role</bibkey>
    </paper>
    <paper id="821">
      <title>Disentangling the Roles of Representation and Selection in Data Pruning</title>
      <author><first>Yupei</first><last>Du</last></author>
      <author><first>Yingjin</first><last>Song</last><affiliation>Utrecht University</affiliation></author>
      <author><first>Hugh Mee</first><last>Wong</last></author>
      <author><first>Daniil</first><last>Ignatev</last></author>
      <author><first>Albert</first><last>Gatt</last><affiliation>Utrecht University</affiliation></author>
      <author><first>Dong</first><last>Nguyen</last><affiliation>Utrecht University</affiliation></author>
      <pages>16791-16809</pages>
      <abstract>Data pruning—selecting small but impactful subsets—offers a promising way to efficiently scale NLP model training. However, existing methods often involve many different design choices, which have not been systematically studied. This limits future developments. In this work, we decompose data pruning into two key components: data representation and selection algorithm, and systematically analyze their influence on selected instances. Our theoretical and empirical results highlight the crucial role of representations: better representations, e.g., training gradients, generally lead to better selected instances, regardless of the chosen selection algorithm. Furthermore, different selection algorithms excel in different settings, and none consistently outperform the others. Moreover, the selection algorithms do not always align with their intended objectives: for example, algorithms designed for the same objective can select drastically different instances, highlighting the need for careful evaluation.</abstract>
      <url hash="e5cf547a">2025.acl-long.821</url>
      <bibkey>du-etal-2025-disentangling</bibkey>
    </paper>
    <paper id="822">
      <title><fixed-case>FRACTAL</fixed-case>: Fine-Grained Scoring from Aggregate Text Labels</title>
      <author><first>Yukti</first><last>Makhija</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Priyanka</first><last>Agrawal</last><affiliation>Google Deepmind</affiliation></author>
      <author><first>Rishi</first><last>Saket</last><affiliation>Google</affiliation></author>
      <author><first>Aravindan</first><last>Raghuveer</last><affiliation>Google</affiliation></author>
      <pages>16810-16830</pages>
      <abstract>Fine-Tuning of LLMs using RLHF / RLAIF has been shown as a critical step to improve the performance of LLMs in complex generation tasks. These methods typically use response-level human or model feedback for alignment. Recent works indicate that finer sentence or span-level labels provide more accurate and interpretable feedback for LLM optimization. In this work, we propose FRACTAL, a suite of models to disaggregate response-level labels into sentence-level (pseudo-)labels through Multiple Instance Learning (MIL) and Learning from Label Proportions (LLP) formulations, novel usage of prior information, and maximum likelihood calibration. We perform close to 2000 experiments across 6 datasets and 4 tasks that show that FRACTAL can reach up to 93% of the performance of the fully supervised baseline while requiring only around 10% of the gold labels. Furthermore, in a downstream eval, employing step-level pseudo scores in RLHF for a math reasoning task leads to 5% absolute improvement in performance. Our work is the first to develop response-level feedback to sentence-level scoring techniques leveraging sentence-level prior information, along with comprehensive evaluations on multiple tasks as well as end-to-end finetuning evaluations.</abstract>
      <url hash="a0358a43">2025.acl-long.822</url>
      <bibkey>makhija-etal-2025-fractal</bibkey>
    </paper>
    <paper id="823">
      <title><fixed-case>ACT</fixed-case>: Knowledgeable Agents to Design and Perform Complex Tasks</title>
      <author><first>Makoto</first><last>Nakatsuji</last></author>
      <author><first>Shuhei</first><last>Tateishi</last><affiliation>NTT Docomo, Inc.</affiliation></author>
      <author><first>Yasuhiro</first><last>Fujiwara</last><affiliation>NTT</affiliation></author>
      <author><first>Ayaka</first><last>Matsumoto</last><affiliation>NTT Communications</affiliation></author>
      <author><first>Narichika</first><last>Nomoto</last><affiliation>NTT, The University of Tokyo</affiliation></author>
      <author><first>Yoshihide</first><last>Sato</last><affiliation>NTT</affiliation></author>
      <pages>16831-16861</pages>
      <abstract>Large language models enhance collaborative task execution in multi-agent systems. Current studies break complex task into manageable tasks, but agents lack understanding of the overall task and how others approach their tasks, hindering synergy and integration.We propose a method called knowledgeable <b>
          <i>A</i></b>gents to design and perform <b>
          <i>C</i></b>omplex <b>
          <i>T</i></b>asks (ACT), where: (1) Agents independently manage their knowledge and tasks while collaboratively design the complex task into a more comprehensible form. In parallel, each agent also acquires knowledge of others, defined as a structured description of how other agents approach their tasks based on the agent’s own task resolution. (2) Each agent updates its knowledge and refines its task through interactions with others. By referencing structured knowledge, they effectively integrate their tasks to collaboratively solve the complex task.Three evaluations including creative writing and tool utilization, show that ACT accurately outperforms existing methods in solving complex tasks.</abstract>
      <url hash="fba75fbb">2025.acl-long.823</url>
      <bibkey>nakatsuji-etal-2025-act</bibkey>
    </paper>
    <paper id="824">
      <title>Logical forms complement probability in understanding language model (and human) performance</title>
      <author><first>Yixuan</first><last>Wang</last><affiliation>University of Chicago</affiliation></author>
      <author><first>Freda</first><last>Shi</last><affiliation>University of Waterloo and Vector Institute</affiliation></author>
      <pages>16862-16877</pages>
      <abstract>With the increasing interest in using large language models (LLMs) for planning in natural language, understanding their behaviors becomes an important research question. This work conducts a systematic investigation of LLMs’ ability to perform logical reasoning in natural language. We introduce a controlled dataset of hypothetical and disjunctive syllogisms in propositional and modal logic and use it as the testbed for understanding LLM performance. Our results lead to novel insights in predicting LLM behaviors: in addition to the probability of input, logical forms should be considered as important factors. In addition, we show similarities and discrepancies between the logical reasoning performances of humans and LLMs by collecting and comparing behavioral data from both.</abstract>
      <url hash="7b9eb240">2025.acl-long.824</url>
      <bibkey>wang-shi-2025-logical</bibkey>
    </paper>
    <paper id="825">
      <title>Length Controlled Generation for Black-box <fixed-case>LLM</fixed-case>s</title>
      <author><first>Yuxuan</first><last>Gu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Wenjie</first><last>Wang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Xiaocheng</first><last>Feng</last></author>
      <author><first>Weihong</first><last>Zhong</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Kun</first><last>Zhu</last></author>
      <author><first>Lei</first><last>Huang</last></author>
      <author><first>Ting</first><last>Liu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Bing</first><last>Qin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Tat-Seng</first><last>Chua</last><affiliation>National University of Singapore</affiliation></author>
      <pages>16878-16895</pages>
      <abstract>Large language models (LLMs) have demonstrated impressive instruction following capabilities, while still struggling to accurately manage the length of the generated text, which is a fundamental requirement in many real-world applications. Existing length control methods involve fine-tuning the parameters of LLMs, which is inefficient and suboptimal for practical use. In this paper, we propose a novel iterative sampling framework for text length control, integrating the Metropolis-Hastings algorithm with an importance sampling acceleration strategy. This framework efficiently and reliably regulates LLMs to generate length-constrained text without modifying the underlying parameters, thereby preserving the original capabilities of LLMs. Experimental results demonstrate that our framework achieves almost 100% success rates of length control on Llama3.1 for tasks such as length-controlled abstractive summarization and length-constrained instruction following, with minimal additional computational overhead. This also highlights the significant potential of our method for precise length control across a broader range of applications, without compromising the versatility of LLMs.</abstract>
      <url hash="888a4eec">2025.acl-long.825</url>
      <bibkey>gu-etal-2025-length</bibkey>
    </paper>
    <paper id="826">
      <title>Improving Contextual Faithfulness of Large Language Models via Retrieval Heads-Induced Optimization</title>
      <author><first>Lei</first><last>Huang</last></author>
      <author><first>Xiaocheng</first><last>Feng</last></author>
      <author><first>Weitao</first><last>Ma</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Yuchun</first><last>Fan</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Xiachong</first><last>Feng</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Yangfan</first><last>Ye</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Weihong</first><last>Zhong</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Yuxuan</first><last>Gu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Baoxin</first><last>Wang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Dayong</first><last>Wu</last></author>
      <author><first>Guoping</first><last>Hu</last><affiliation>IFLYTEK CO.LTD.</affiliation></author>
      <author><first>Bing</first><last>Qin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>16896-16913</pages>
      <abstract>Ensuring contextual faithfulness in retrieval-augmented large language models (LLMs) is crucial for building trustworthy information-seeking systems, particularly in long-form question-answering (LFQA) scenarios. In this work, we identify a salient correlation between LFQA faithfulness and retrieval heads, a set of attention heads responsible for retrieving contextual information. Leveraging this insight, we propose RHIO, a framework designed to teach LLMs to explicitly discriminate between faithful and unfaithful generations. RHIO first augments unfaithful samples that simulate realistic model-intrinsic errors by selectively masking retrieval heads. Then, these samples are incorporated into joint training, enabling the model to distinguish unfaithful outputs from faithful ones conditioned on control tokens. Furthermore, these control tokens are leveraged to self-induce contrastive outputs, amplifying their difference through contrastive decoding. Additionally, to facilitate the evaluation of contextual faithfulness, we also introduce GroundBench, a comprehensive benchmark compiled from five existing LFQA datasets. Extensive experimental results on GroundBench demonstrate that RHIO significantly improves faithfulness, even outperforming GPT-4o.</abstract>
      <url hash="423b5ae8">2025.acl-long.826</url>
      <bibkey>huang-etal-2025-improving</bibkey>
    </paper>
    <paper id="827">
      <title>Global Eye: Breaking the “Fixed Thinking Pattern” during the Instruction Expansion Process</title>
      <author><first>Wenxuan</first><last>Lu</last></author>
      <author><first>Wei</first><last>Liu</last></author>
      <author><first>Jian</first><last>Luan</last><affiliation>Xiaomi Corporation</affiliation></author>
      <author><first>Bin</first><last>Wang</last><affiliation>AI Lab, Xiaomi Inc.</affiliation></author>
      <author><first>Songhao</first><last>Jiang</last></author>
      <author><first>Tianning</first><last>Zang</last></author>
      <pages>16914-16928</pages>
      <abstract>An extensive high-quality instruction dataset is crucial for the instruction tuning process of Large Language Models (LLMs). Recent instruction expansion methods have demonstrated their capability to improve the quality and quantity of existing datasets, by prompting high-performance LLM to generate multiple new instructions from the original ones. However, existing methods focus on constructing multi-perspective prompts (e.g., increasing complexity or difficulty) to expand instructions, overlooking the “Fixed Thinking Pattern” issue of LLMs. This issue arises when repeatedly using the same set of prompts, causing LLMs to rely on a limited set of certain expressions to expand all instructions, potentially compromising the diversity of the final expanded dataset. This paper theoretically analyzes the causes of the “Fixed Thinking Pattern”, and corroborates this phenomenon through multi-faceted empirical research. Furthermore, we propose a novel method based on dynamic prompt updating: Global Eye. Specifically, after a fixed number of instruction expansions, we analyze the statistical characteristics of newly generated instructions and then update the prompts. Experimental results show that our method enables Llama3-8B and Llama2-13B to surpass the performance of open-source LLMs and GPT3.5 across various metrics. Our code and data are submitted to the Software &amp; Data option.</abstract>
      <url hash="16ca4d2b">2025.acl-long.827</url>
      <bibkey>lu-etal-2025-global</bibkey>
    </paper>
    <paper id="828">
      <title>On Synthesizing Data for Context Attribution in Question Answering</title>
      <author><first>Gorjan</first><last>Radevski</last><affiliation>Department of Electrical Engineering, KU Leuven, Belgium, KU Leuven and NEC</affiliation></author>
      <author><first>Kiril</first><last>Gashteovski</last><affiliation>NEC Laboratories Europe, St.Cyril and Methodius University and NEC Laboratories Europe</affiliation></author>
      <author><first>Shahbaz</first><last>Syed</last><affiliation>NEC</affiliation></author>
      <author><first>Christopher</first><last>Malon</last><affiliation>NEC Laboratories America</affiliation></author>
      <author><first>Sebastien</first><last>Nicolas</last></author>
      <author><first>Chia-Chien</first><last>Hung</last><affiliation>NEC Laboratories Europe</affiliation></author>
      <author><first>Timo</first><last>Sztyler</last><affiliation>NEC Laboratories Europe</affiliation></author>
      <author><first>Verena</first><last>Heußer</last><affiliation>NEC Laboratories Europe GmbH</affiliation></author>
      <author><first>Wiem</first><last>Ben Rim</last></author>
      <author><first>Masafumi</first><last>Enomoto</last><affiliation>NEC</affiliation></author>
      <author><first>Kunihiro</first><last>Takeoka</last><affiliation>NEC</affiliation></author>
      <author><first>Masafumi</first><last>Oyamada</last><affiliation>NEC</affiliation></author>
      <author><first>Goran</first><last>Glavaš</last><affiliation>Julius-Maximilians-Universität Würzburg</affiliation></author>
      <author><first>Carolin</first><last>Lawrence</last><affiliation>NEC Laboratories Europe and NEC Laboratories Europe</affiliation></author>
      <pages>16929-16950</pages>
      <abstract>Question Answering (QA) accounts for a significant portion of LLM usage in the wild”. However, LLMs sometimes produce false or misleading responses, also known as hallucinations”. Therefore, grounding the generated answers in contextually provided information—i.e., providing evidence for the generated text—is paramount for LLMs’ trustworthiness. Providing this information is the task of context attribution. In this paper, we systematically study LLM-based approaches for this task, namely we investigate (i) zero-shot inference, (ii) LLM ensembling, and (iii) fine-tuning of small LMs on synthetic data generated by larger LLMs. Our key contribution is SynQA: a novel generative strategy for synthesizing context attribution data. Given selected context sentences, an LLM generates QA pairs that are supported by these sentences. This leverages LLMs’ natural strengths in text generation while ensuring clear attribution paths in the synthetic training data. We show that the attribution data synthesized via SynQA is highly effective for fine-tuning small LMs for context attribution in different QA tasks and domains. Finally, with a user study, we validate the usefulness of small LMs (fine-tuned on synthetic data from SynQA) in context attribution for QA.</abstract>
      <url hash="6fcdd981">2025.acl-long.828</url>
      <bibkey>radevski-etal-2025-synthesizing</bibkey>
    </paper>
    <paper id="829">
      <title><fixed-case>TST</fixed-case>: A Schema-Based Top-Down and Dynamic-Aware Agent of Text-to-Table Tasks</title>
      <author><first>Peiwen</first><last>Jiang</last></author>
      <author><first>Haitong</first><last>Jiang</last></author>
      <author><first>Ruhui</first><last>Ma</last></author>
      <author><first>Yvonne Jie</first><last>Chen</last><affiliation>ShanghaiTech University</affiliation></author>
      <author><first>Jinhua</first><last>Cheng</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <pages>16951-16966</pages>
      <abstract>As a bridge between natural texts and information systems like structured storage, statistical analysis, retrieving, and recommendation, the text-to-table task has received widespread attention recently. Existing researches have gone through a paradigm shift from traditional bottom-up IE (Information Extraction) to top-down LLMs-based question answering with RAG (Retrieval-Augmented Generation). Furthermore, these methods mainly adopt end-to-end models or use multi-stage pipelines to extract text content based on static table structures. However, they neglect to deal with precise inner-document evidence extraction and dynamic information such as multiple entities and events, which can not be defined in static table head format and are very common in natural texts.To address this issue, we propose a two-stage dynamic content extraction agent framework called TST (Text-Schema-Table), which uses type recognition methods to extract context evidences with the conduction of domain schema sequentially. Based on the evidence, firstly we quantify the total instances of each dynamic object and then extract them with ordered numerical prompts. Through extensive comparisons with existing methods across different datasets, our extraction framework exhibits state-of-the-art (SOTA) performance. Our codes are available at <url>https://github.com/jiangpw41/TST</url>.</abstract>
      <url hash="c5afd192">2025.acl-long.829</url>
      <bibkey>jiang-etal-2025-tst</bibkey>
    </paper>
    <paper id="830">
      <title><fixed-case>E</fixed-case>vent<fixed-case>RAG</fixed-case>: Enhancing <fixed-case>LLM</fixed-case> Generation with Event Knowledge Graphs</title>
      <author><first>Zairun</first><last>Yang</last></author>
      <author><first>Yilin</first><last>Wang</last></author>
      <author><first>Zhengyan</first><last>Shi</last></author>
      <author><first>Yuan</first><last>Yao</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Lei</first><last>Liang</last></author>
      <author><first>Keyan</first><last>Ding</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Emine</first><last>Yilmaz</last></author>
      <author><first>Huajun</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Qiang</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <pages>16967-16979</pages>
      <abstract>Retrieval-augmented generation (RAG) systems often struggle with narrative-rich documents and event-centric reasoning, particularly when synthesizing information across multiple sources. We present EventRAG, a novel framework that enhances text generation through structured event representations. We first construct an Event Knowledge Graph by extracting events and merging semantically equivalent nodes across documents, while expanding under-connected relationships. We then employ an iterative retrieval and inference strategy that explicitly captures temporal dependencies and logical relationships across events. Experiments on UltraDomain and MultiHopRAG benchmarks show EventRAG’s superiority over baseline RAG systems, with substantial gains in generation effectiveness, logical consistency, and multi-hop reasoning accuracy. Our work advances RAG systems by integrating structured event semantics with iterative inference, particularly benefiting scenarios requiring temporal and logical reasoning across documents.</abstract>
      <url hash="f87f66a7">2025.acl-long.830</url>
      <bibkey>yang-etal-2025-eventrag</bibkey>
    </paper>
    <paper id="831">
      <title>Analyzing the Rapid Generalization of <fixed-case>SFT</fixed-case> via the Perspective of Attention Head Activation Patterns</title>
      <author><first>Yang</first><last>Zhao</last></author>
      <author><first>Li</first><last>Du</last><affiliation>BAAI</affiliation></author>
      <author><first>Xiao</first><last>Ding</last></author>
      <author><first>Kai</first><last>Xiong</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Ting</first><last>Liu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Bing</first><last>Qin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>16980-16992</pages>
      <abstract>LLMs’ performance on complex tasks is still unsatisfactory. A key issue is that presently LLMs learn in a data-driven schema, while the instructions about these complex tasks are both scarce and hard to collect or construct. On the contrary, a prominent phenomenon is that LLMs can learn rather fast on simpler tasks with adequate prior knowledge captured during pretraining stage. Thus, if the prerequisite and mechanism of such rapid generalization could be elucidated, it could enhance the efficiency and effectiveness of the LLM’s ability to learn complex tasks. Thus, in this paper, we employ a gradient-based method, to dissect the process that the SFT process adapts LLMs to downstream tasks via the perspective of attention patterns. We find that: (1) LLMs selectively activate task-specific attention heads during SFT; (2) activation patterns for complex tasks are combinations of basic task patterns; and (3) changes in a few parameters can significantly impact activation patterns after SFT on a small number of samples.Based on these insights, experiments are conducted to actually enhance the efficiency and effectiveness of SFT.</abstract>
      <url hash="03859a2d">2025.acl-long.831</url>
      <bibkey>zhao-etal-2025-analyzing</bibkey>
    </paper>
    <paper id="832">
      <title>Can’t See the Forest for the Trees: Benchmarking Multimodal Safety Awareness for Multimodal <fixed-case>LLM</fixed-case>s</title>
      <author><first>Wenxuan</first><last>Wang</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Xiaoyuan</first><last>Liu</last><affiliation>Tencent AI Lab and The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <author><first>Kuiyi</first><last>Gao</last></author>
      <author><first>Jen-tse</first><last>Huang</last></author>
      <author><first>Youliang</first><last>Yuan</last><affiliation>The Chinese University of Hong Kong-Shenzhen</affiliation></author>
      <author><first>Pinjia</first><last>He</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <author><first>Shuai</first><last>Wang</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Zhaopeng</first><last>Tu</last><affiliation>Tencent AI Lab</affiliation></author>
      <pages>16993-17006</pages>
      <abstract>Multimodal Large Language Models (MLLMs) have expanded the capabilities of traditional language models by enabling interaction through both text and images. However, ensuring the safety of these models remains a significant challenge, particularly in accurately identifying whether multimodal content is safe or unsafe—a capability we term <i>safety awareness</i>. In this paper, we introduce MMSafeAware, the first comprehensive multimodal safety awareness benchmark designed to evaluate MLLMs across 29 safety scenarios with 1,500 carefully curated image-prompt pairs. MMSafeAware includes both unsafe and over-safety subsets to assess models’ abilities to correctly identify unsafe content and avoid over-sensitivity that can hinder helpfulness. Evaluating nine widely used MLLMs using MMSafeAware reveals that current models are not sufficiently safe and often overly sensitive; for example, GPT-4V misclassifies 36.1% of unsafe inputs as safe and 59.9% of benign inputs as unsafe. We further explore three methods to improve safety awareness—prompting-based approaches, visual contrastive decoding, and vision-centric reasoning fine-tuning—but find that none achieve satisfactory performance. Our findings highlight the profound challenges in developing MLLMs with robust safety awareness, underscoring the need for further research in this area. All the code and data will be publicly available to facilitate future research.</abstract>
      <url hash="f086e77e">2025.acl-long.832</url>
      <bibkey>wang-etal-2025-cant</bibkey>
    </paper>
    <paper id="833">
      <title>Mis-prompt: Benchmarking Large Language Models for Proactive Error Handling</title>
      <author><first>Jiayi</first><last>Zeng</last></author>
      <author><first>Yizhe</first><last>Feng</last></author>
      <author><first>Mengliang</first><last>He</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Wenhui</first><last>Lei</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Wei</first><last>Zhang</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Zeming</first><last>Liu</last></author>
      <author><first>Xiaoming</first><last>Shi</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Aimin</first><last>Zhou</last><affiliation>East China Normal University</affiliation></author>
      <pages>17007-17034</pages>
      <abstract>Large language models (LLMs) have demonstrated significant advancements in error handling. Current error-handling works are performed in a passive manner, with explicit error-handling instructions. However, in real-world scenarios, explicit error-handling instructions are usually unavailable. In this paper, our work identifies this challenge as how to conduct proactive error handling without explicit error handling instructions. To promote further research, this work introduces a new benchmark, termed Mis-prompt, consisting of four evaluation tasks, an error category taxonomy, and a new evaluation dataset. Furthermore, this work analyzes current LLMs’ performance on the benchmark, and the experimental results reveal that current LLMs show poor performance on proactive error handling, and SFT on error handling instances improves LLMs’ proactive error handling capabilities. The dataset will be publicly available.</abstract>
      <url hash="1c272242">2025.acl-long.833</url>
      <bibkey>zeng-etal-2025-mis</bibkey>
    </paper>
    <paper id="834">
      <title><fixed-case>T</fixed-case>rip<fixed-case>C</fixed-case>raft: A Benchmark for Spatio-Temporally Fine Grained Travel Planning</title>
      <author><first>Soumyabrata</first><last>Chaudhuri</last></author>
      <author><first>Pranav</first><last>Purkar</last></author>
      <author><first>Ritwik</first><last>Raghav</last></author>
      <author><first>Shubhojit</first><last>Mallick</last><affiliation>Microsoft</affiliation></author>
      <author><first>Manish</first><last>Gupta</last><affiliation>Microsoft</affiliation></author>
      <author><first>Abhik</first><last>Jana</last><affiliation>IIT Bhubaneswar</affiliation></author>
      <author><first>Shreya</first><last>Ghosh</last><affiliation>IIT Bhubaneswar</affiliation></author>
      <pages>17035-17064</pages>
      <abstract>Recent advancements in probing Large Language Models (LLMs) have explored their latent potential as personalized travel planning agents, though this remains a rather nascent field. Existing benchmarks, such as TravelPlanner and TravelPlanner+, rely on semi-synthetic data as well ignoring several key components of travel planning, limiting their real-world applicability. Therefore, we introduce TripCraft, a spatio-temporally coherent travel planning dataset incorporating real-world constraints, including public transit schedules, public events, varied attraction categories, and user personas for enhanced personalization. Our dataset enables more detailed trip itinerary generation (including duration spent at each point of interest based on users’ persona, transit between two points of interest, etc.) while ensuring spatio-temporal consistency. Further, we propose novel evaluation metrics (temporal meal score, attraction score, spatial score, ordering score, and persona score) to assess LLM-generated plans across temporal, spatial, sequential, and personal dimensions, overcoming the limitations of commonsense and hard constraint metrics. Interestingly, our parameter-informed setting significantly enhances meal scheduling, improving performance from 61% to 80% in the 7-day scenario- as quantified by a 19% gain in our temporal meal score. Moreover, TripCraft serves as a high-quality benchmark for advancing personalized LLM-driven travel planning.</abstract>
      <url hash="d695ea2d">2025.acl-long.834</url>
      <bibkey>chaudhuri-etal-2025-tripcraft</bibkey>
    </paper>
    <paper id="835">
      <title><fixed-case>D</fixed-case>ual<fixed-case>G</fixed-case>uard: A Parameter Space Transformation Approach for Bidirectional Defense in Split-Based <fixed-case>LLM</fixed-case> Fine-Tuning</title>
      <author><first>Zihan</first><last>Liu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Yizhen</first><last>Wang</last></author>
      <author><first>Rui</first><last>Wang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Sai</first><last>Wu</last><affiliation>Zhejiang University</affiliation></author>
      <pages>17065-17080</pages>
      <abstract>Integrating split learning with large language model fine-tuning (LLM-FT) enables secure collaboration between a trusted local client and a well-equipped remote server, but it is vulnerable to data reconstruction attacks (DRAs) that exploit transmitted activations and gradients. Current defense methods, like adding noise to activations or gradients, often sacrifice task-specific model performance under strict privacy constraints. This paper introduces DualGuard, a bidirectional defense mechanism against DRAs for split-based LLM-FT. DualGuard proposes a local warm-up parameter space transformation to alter client-side model parameters before training, using multi-task learning to strike a balance between privacy protection and model performance. Additionally, a global fine-tuning parameter space retention strategy prevents the model from reverting to vulnerable states during formal fine-tuning. Experiments show that DualGuard outperforms current defense methods against various DRAs, while maintaining task performance. Our code will be made publicly available.</abstract>
      <url hash="2efd1f93">2025.acl-long.835</url>
      <bibkey>liu-etal-2025-dualguard</bibkey>
    </paper>
    <paper id="836">
      <title>Movie101v2: Improved Movie Narration Benchmark</title>
      <author><first>Zihao</first><last>Yue</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Yepeng</first><last>Zhang</last></author>
      <author><first>Ziheng</first><last>Wang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Qin</first><last>Jin</last><affiliation>Renmin University of China</affiliation></author>
      <pages>17081-17095</pages>
      <abstract>Automatic movie narration aims to generate video-aligned plot descriptions to assist visually impaired audiences. Unlike standard video captioning, it involves not only describing key visual details but also inferring plots that unfold across multiple movie shots, presenting distinct and complex challenges. To advance this field, we introduce Movie101v2, a large-scale, bilingual dataset with enhanced data quality specifically designed for movie narration. Revisiting the task, we propose breaking down the ultimate goal of automatic movie narration into three progressive stages, offering a clear roadmap with corresponding evaluation metrics. Based on our new benchmark, we baseline a range of large vision-language models and conduct an in-depth analysis of the challenges in movie narration generation. Our findings highlight that achieving applicable movie narration generation is a fascinating goal that requires significant research.</abstract>
      <url hash="832d50b4">2025.acl-long.836</url>
      <bibkey>yue-etal-2025-movie101v2</bibkey>
    </paper>
    <paper id="837">
      <title>Can <fixed-case>LLM</fixed-case>s Evaluate Complex Attribution in <fixed-case>QA</fixed-case>? Automatic Benchmarking using Knowledge Graphs</title>
      <author><first>Nan</first><last>Hu</last><affiliation>Southeast University</affiliation></author>
      <author><first>Jiaoyan</first><last>Chen</last></author>
      <author><first>Yike</first><last>Wu</last></author>
      <author><first>Guilin</first><last>Qi</last></author>
      <author><first>Hongru</first><last>Wang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Sheng</first><last>Bi</last><affiliation>Southeast University</affiliation></author>
      <author><first>Yongrui</first><last>Chen</last><affiliation>Southeast University</affiliation></author>
      <author><first>Tongtong</first><last>Wu</last><affiliation>Monash University</affiliation></author>
      <author><first>Jeff Z.</first><last>Pan</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <pages>17096-17118</pages>
      <abstract>Attributed Question Answering (AQA) has attracted wide attention, but there are still several limitations in evaluating the attributions, including lacking fine-grained attribution categories, relying on manual annotations, and failing to compare attributions with only subtle differences. To bridge these gaps, we introduce Complex Attributed Question Answering (CAQA), a large-scale benchmark containing comprehensive attribution categories, automatically generated using Knowledge Graphs (KGs), and complex attribution scenarios. We have conducted extensive experiments to verify the effectiveness of CAQA, including the benchmarking of 25 automatic evaluators, their comparison with human evaluators, the testing of LLM evaluators fine-tuned by CAQA and so on. These experiments also lead to a series of important findings that can benefit the future research of AQA.</abstract>
      <url hash="5724922b">2025.acl-long.837</url>
      <bibkey>hu-etal-2025-llms</bibkey>
    </paper>
    <paper id="838">
      <title>Value Portrait: Assessing Language Models’ Values through Psychometrically and Ecologically Valid Items</title>
      <author><first>Jongwook</first><last>Han</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Dongmin</first><last>Choi</last></author>
      <author><first>Woojung</first><last>Song</last></author>
      <author><first>Eun-Ju</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Yohan</first><last>Jo</last><affiliation>Seoul National University</affiliation></author>
      <pages>17119-17159</pages>
      <abstract>The importance of benchmarks for assessing the values of language models has been pronounced due to the growing need of more authentic, human-aligned responses. However, existing benchmarks rely on human or machine annotations that are vulnerable to value-related biases. Furthermore, the tested scenarios often diverge from real-world contexts in which models are commonly used to generate text and express values. To address these issues, we propose the Value Portrait benchmark, a reliable framework for evaluating LLMs’ value orientations with two key characteristics. First, the benchmark consists of items that capture real-life user-LLM interactions, enhancing the relevance of assessment results to real-world LLM usage. Second, each item is rated by human subjects based on its similarity to their own thoughts, and correlations between these ratings and the subjects’ actual value scores are derived. This psychometrically validated approach ensures that items strongly correlated with specific values serve as reliable items for assessing those values. Through evaluating 44 LLMs with our benchmark, we find that these models prioritize Benevolence, Security, and Self-Direction values while placing less emphasis on Tradition, Power, and Achievement values. Also, our analysis reveals biases in how LLMs perceive various demographic groups, deviating from real human data.</abstract>
      <url hash="4a3d36d2">2025.acl-long.838</url>
      <bibkey>han-etal-2025-value</bibkey>
    </paper>
    <paper id="839">
      <title><fixed-case>FEA</fixed-case>-Bench: A Benchmark for Evaluating Repository-Level Code Generation for Feature Implementation</title>
      <author><first>Wei</first><last>Li</last></author>
      <author><first>Xin</first><last>Zhang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Zhongxin</first><last>Guo</last><affiliation>Microsoft</affiliation></author>
      <author><first>Shaoguang</first><last>Mao</last><affiliation>Microsoft</affiliation></author>
      <author><first>Wen</first><last>Luo</last><affiliation>Peking University</affiliation></author>
      <author><first>Guangyue</first><last>Peng</last><affiliation>Peking University</affiliation></author>
      <author><first>Yangyu</first><last>Huang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Houfeng</first><last>Wang</last></author>
      <author><first>Scarlett</first><last>Li</last><affiliation>Research, Microsoft</affiliation></author>
      <pages>17160-17176</pages>
      <abstract>Implementing new features in repository-level codebases is a crucial application of code generation models. However, current benchmarks lack a dedicated evaluation framework for this capability. To fill this gap, we introduce FEA-Bench, a benchmark designed to assess the ability of large language models (LLMs) to perform incremental development within code repositories. We collect pull requests from 83 GitHub repositories and use rule-based and intent-based filtering to construct task instances focused on new feature development. Each task instance containing code changes is paired with relevant unit test files to ensure that the solution can be verified. The feature implementation requires LLMs to simultaneously possess code completion capabilities for new components and code editing abilities for other relevant parts in the code repository, providing a more comprehensive evaluation method of LLMs’ automated software engineering capabilities.Experimental results show that LLMs perform significantly worse in the FEA-Bench, highlighting considerable challenges in such repository-level incremental code development.</abstract>
      <url hash="20c92b9b">2025.acl-long.839</url>
      <bibkey>li-etal-2025-fea</bibkey>
    </paper>
    <paper id="840">
      <title>Do not Abstain! Identify and Solve the Uncertainty</title>
      <author><first>Jingyu</first><last>Liu</last></author>
      <author><first>JingquanPeng</first><last>JingquanPeng</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Xiaopeng</first><last>Wu</last><affiliation>Alibaba-inc</affiliation></author>
      <author><first>Xubin</first><last>Li</last></author>
      <author><first>Tiezheng</first><last>Ge</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Bo</first><last>Zheng</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yong</first><last>Liu</last><affiliation>Renmin University of China</affiliation></author>
      <pages>17177-17197</pages>
      <abstract>Despite the widespread application of Large Language Models (LLMs) across various domains, they frequently exhibit overconfidence when encountering uncertain scenarios, yet existing solutions primarily rely on evasive responses (e.g., “I don’t know”) overlooks the opportunity of identifying and addressing the uncertainty to generate more satisfactory responses. To systematically investigate and improve LLMs’ ability of recognizing and addressing the source of uncertainty, we introduce ConfuseBench, a benchmark mainly focus on three types of uncertainty: document scarcity, limited capability, and query ambiguity. Experiments with ConfuseBench reveal that current LLMs struggle to accurately identify the root cause of uncertainty and solve it. They prefer to attribute uncertainty to query ambiguity while overlooking capability limitations, especially for those weaker models. To tackle this challenge, we first generate context-aware inquiries that highlight the confusing aspect of the original query. Then we judge the source of uncertainty based on the uniqueness of the inquiry’s answer. Further we use an on-policy training method, InteractDPO to generate better inquiries. Experimental results demonstrate the efficacy of our approach.</abstract>
      <url hash="3927998d">2025.acl-long.840</url>
      <bibkey>liu-etal-2025-abstain</bibkey>
    </paper>
    <paper id="841">
      <title>Decoding by Contrasting Knowledge: Enhancing Large Language Model Confidence on Edited Facts</title>
      <author><first>Baolong</first><last>Bi</last></author>
      <author><first>Shenghua</first><last>Liu</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Lingrui</first><last>Mei</last><affiliation>Skywork AI</affiliation></author>
      <author><first>Yiwei</first><last>Wang</last><affiliation>University of California, Merced</affiliation></author>
      <author><first>Junfeng</first><last>Fang</last></author>
      <author><first>Pengliang</first><last>Ji</last></author>
      <author><first>Xueqi</first><last>Cheng</last><affiliation>Institute of Computing Technology, Chinese Academy</affiliation></author>
      <pages>17198-17208</pages>
      <abstract>The knowledge within large language models (LLMs) may become outdated quickly. While in-context editing (ICE) is currently the most effective method for knowledge editing (KE), it is constrained by the black-box modeling of LLMs and thus lacks interpretability. Our work aims to elucidate the superior performance of ICE in KE by analyzing the impacts of in-context new knowledge on token-wise distributions. We observe that despite a significant boost in logits of the new knowledge, the performance of ICE is still hindered by stubborn knowledge. We propose a novel approach termed Decoding by Contrasting Knowledge (DeCK). DeCK derives the distribution of the next token by contrasting the logits obtained from the newly edited knowledge guided by ICE with those from the unedited parametric knowledge. Our experiments demonstrate that DeCK enhances the confidence of LLMs in edited facts. For instance, it improves the performance of LLaMA3-8B-instruct on MQuAKE by up to 219%, demonstrating its capability to strengthen ICE. DeCK can be easily integrated into any ICE method as a decoding component to enhance editing capabilities.</abstract>
      <url hash="3120b9a2">2025.acl-long.841</url>
      <bibkey>bi-etal-2025-decoding</bibkey>
    </paper>
    <paper id="842">
      <title><fixed-case>I</fixed-case>mpli<fixed-case>H</fixed-case>ate<fixed-case>V</fixed-case>id: A Benchmark Dataset and Two-stage Contrastive Learning Framework for Implicit Hate Speech Detection in Videos</title>
      <author><first>Mohammad Zia Ur</first><last>Rehman</last></author>
      <author><first>Anukriti</first><last>Bhatnagar</last></author>
      <author><first>Omkar</first><last>Kabde</last></author>
      <author><first>Shubhi</first><last>Bansal</last><affiliation>Indian Institute of Technology, Indore</affiliation></author>
      <author><first>Dr. Nagendra</first><last>Kumar</last></author>
      <pages>17209-17221</pages>
      <abstract>The existing research has primarily focused on text and image-based hate speech detection, video-based approaches remain underexplored. In this work, we introduce a novel dataset, ImpliHateVid, specifically curated for implicit hate speech detection in videos. ImpliHateVid consists of 2,009 videos comprising 509 implicit hate videos, 500 explicit hate videos, and 1,000 non-hate videos, making it one of the first large-scale video datasets dedicated to implicit hate detection. We also propose a novel two-stage contrastive learning framework for hate speech detection in videos. In the first stage, we train modality-specific encoders for audio, text, and image using contrastive loss by concatenating features from the three encoders. In the second stage, we train cross-encoders using contrastive learning to refine multimodal representations. Additionally, we incorporate sentiment, emotion, and caption-based features to enhance implicit hate detection. We evaluate our method on two datasets, ImpliHateVid for implicit hate speech detection and another dataset for general hate speech detection in videos, HateMM dataset, demonstrating the effectiveness of the proposed multimodal contrastive learning for hateful content detection in videos and the significance of our dataset.</abstract>
      <url hash="094e8dcc">2025.acl-long.842</url>
      <bibkey>rehman-etal-2025-implihatevid</bibkey>
    </paper>
    <paper id="843">
      <title>Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions</title>
      <author><first>Leonardo</first><last>Ranaldi</last></author>
      <author><first>Marco</first><last>Valentino</last><affiliation>University of Sheffield</affiliation></author>
      <author><first>Andre</first><last>Freitas</last><affiliation>Idiap Research Institute and University of Manchester</affiliation></author>
      <pages>17222-17240</pages>
      <abstract>Chain-of-Though (CoT) represents a common strategy for reasoning in Large Language Models (LLMs) by decomposing complex tasks into intermediate inference steps. However, explanations generated via CoT are susceptible to content biases that negatively affect their robustness and faithfulness. To mitigate existing limitations, recent work has proposed using logical formalisms coupled with external symbolic solvers. However, fully symbolic approaches possess the bottleneck of requiring a complete translation from natural language to formal languages, a process that affects efficiency and flexibility. To achieve a trade-off, this paper investigates methods to disentangle content from logical reasoning without a complete formalisation. In particular, we present QuaSAR (for Quasi-Symbolic Abstract Reasoning), a variation of CoT that guides LLMs to operate at a higher level of abstraction via quasi-symbolic explanations. Our framework leverages the capability of LLMs to formalise only relevant variables and predicates, enabling the coexistence of symbolic elements with natural language. We show the impact of QuaSAR for in-context learning and for constructing demonstrations to improve the reasoning capabilities of smaller models. Our experiments show that quasi-symbolic abstractions can improve CoT-based methods by up to 8% accuracy, enhancing robustness and consistency on challenging adversarial variations on both natural language (i.e. MMLU-Redux) and symbolic reasoning tasks (i.e., GSM-Symbolic).</abstract>
      <url hash="23c72859">2025.acl-long.843</url>
      <bibkey>ranaldi-etal-2025-improving</bibkey>
    </paper>
    <paper id="844">
      <title>Information Extraction from Visually Rich Documents using <fixed-case>LLM</fixed-case>-based Organization of Documents into Independent Textual Segments</title>
      <author><first>Aniket</first><last>Bhattacharyya</last><affiliation>Amazon</affiliation></author>
      <author><first>Anurag</first><last>Tripathi</last><affiliation>Indian Institute of Technology, Kanpur and Amazon</affiliation></author>
      <author><first>Ujjal</first><last>Das</last><affiliation>Amazon</affiliation></author>
      <author><first>Archan</first><last>Karmakar</last><affiliation>Amazon</affiliation></author>
      <author><first>Amit</first><last>Pathak</last></author>
      <author><first>Maneesh</first><last>Gupta</last><affiliation>FinAuto</affiliation></author>
      <pages>17241-17256</pages>
      <abstract>Information extraction (IE) from Visually Rich Documents (VRDs) containing layout features along with text is a critical and well-studied task. Specialized non-LLM NLP-based solutions typically involve training models using both textual and geometric information to label sequences/tokens as named entities or answers to specific questions. However, these approaches lack reasoning, are not able to infer values not explicitly present in documents, and do not generalize well to new formats. Generative LLMs-based approaches proposed recently are capable of reasoning, but struggle to comprehend clues from document layout especially in previously unseen document formats, and do not show competitive performance in heterogeneous VRD benchmark datasets. In this paper, we propose BLOCKIE, a novel LLM-based approach that organizes VRDs into localized, reusable semantic textual segments called <tex-math>\textit{semantic blocks}</tex-math>, which are processed independently. Through focused and more generalizable reasoning,our approach outperforms the state-of-the-art on public VRD benchmarks by 1-3% in F1 scores, is resilient to document formats previously not encountered and shows abilities to correctly extract information not explicitly present in documents.</abstract>
      <url hash="88edb7ff">2025.acl-long.844</url>
      <bibkey>bhattacharyya-etal-2025-information</bibkey>
    </paper>
    <paper id="845">
      <title>Enhancing Open-Domain Task-Solving Capability of <fixed-case>LLM</fixed-case>s via Autonomous Tool Integration from <fixed-case>G</fixed-case>it<fixed-case>H</fixed-case>ub</title>
      <author><first>Bohan</first><last>Lyu</last></author>
      <author><first>Xin</first><last>Cong</last></author>
      <author><first>Heyang</first><last>Yu</last></author>
      <author><first>Pan</first><last>Yang</last></author>
      <author><first>Cheng</first><last>Qian</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Zihe</first><last>Wang</last></author>
      <author><first>Yujia</first><last>Qin</last></author>
      <author><first>Yining</first><last>Ye</last></author>
      <author><first>Yaxi</first><last>Lu</last><affiliation>Department of Computer Science and Technology, Tsinghua University</affiliation></author>
      <author><first>Chen</first><last>Qian</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Zhong</first><last>Zhang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Yukun</first><last>Yan</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Yankai</first><last>Lin</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Zhiyuan</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Maosong</first><last>Sun</last><affiliation>Tsinghua University</affiliation></author>
      <pages>17257-17277</pages>
      <abstract>Large Language Models (LLMs) excel in traditional natural language processing tasks but struggle with problems that require complex domain-specific calculations or simulations. While equipping LLMs with external tools to build LLM-based agents can enhance their capabilities, existing approaches lack the flexibility to address diverse and ever-evolving user queries in open domains. Currently, there is also no existing dataset that evaluates LLMs on open-domain knowledge that requires tools to solve. To this end, we introduce OpenAct benchmark to evaluate the open-domain task-solving capability, which is built on human expert consultation and repositories in GitHub. It comprises 339 questions spanning 7 diverse domains that need to be solved with domain-specific methods. In our experiments, even state-of-the-art LLMs and LLM-based agents demonstrate unsatisfactory success rates, underscoring the need for a novel approach.Furthermore, we present OpenAgent, a novel LLM-based agent system that can tackle evolving queries in open domains through autonomously integrating specialized tools from GitHub. OpenAgent employs 1) a hierarchical framework where specialized agents handle specific tasks and can assign tasks to inferior agents, 2) a bi-level experience learning mechanism to learn from both humans’ and its own experiences to tackle tool flaws. Experiments demonstrate its superior effectiveness and efficiency, which significantly outperforms baselines. Our data and code are open-source at https://github.com/OpenBMB/OpenAct.</abstract>
      <url hash="be9b88be">2025.acl-long.845</url>
      <bibkey>lyu-etal-2025-enhancing</bibkey>
    </paper>
    <paper id="846">
      <title><fixed-case>LLM</fixed-case>s Can Simulate Standardized Patients via Agent Coevolution</title>
      <author><first>Zhuoyun</first><last>Du</last></author>
      <author><first>LujieZheng</first><last>LujieZheng</last></author>
      <author><first>Renjun</first><last>Hu</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Yuyang</first><last>Xu</last></author>
      <author><first>Xiawei</first><last>Li</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Ying</first><last>Sun</last><affiliation>SUN YAT-SEN UNIVERSITY CANCER CENTER</affiliation></author>
      <author><first>Wei</first><last>Chen</last><affiliation>State key laboratory of CAD&amp;CG, Zhejiang University</affiliation></author>
      <author><first>Jian</first><last>Wu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Haolei</first><last>Cai</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Haochao</first><last>Ying</last><affiliation>Zhejiang University</affiliation></author>
      <pages>17278-17306</pages>
      <abstract>Training medical personnel using standardized patients (SPs) remains a complex challenge, requiring extensive domain expertise and role-specific practice. Most research on Large Language Model (LLM)-based simulated patients focuses on improving data retrieval accuracy or adjusting prompts through human feedback. However, this focus has overlooked the critical need for patient agents to learn a standardized presentation pattern that transforms data into human-like patient responses through unsupervised simulations. To address this gap, we propose EvoPatient, a novel simulated patient framework in which a patient agent and doctor agents simulate the diagnostic process through multi-turn dialogues, simultaneously gathering experience to improve the quality of both questions and answers, ultimately enabling human doctor training. Extensive experiments on various cases demonstrate that, by providing only overall SP requirements, our framework improves over existing reasoning methods by more than 10% in requirement alignment and better human preference, while achieving an optimal balance of resource consumption after evolving over 200 cases for 10 hours, with excellent generalizability. Our system will be available at https://github.com/ZJUMAI/EvoPatient</abstract>
      <url hash="a7bd585e">2025.acl-long.846</url>
      <bibkey>du-etal-2025-llms</bibkey>
    </paper>
    <paper id="847">
      <title>Donate or Create? Comparing Data Collection Strategies for Emotion-labeled Multimodal Social Media Posts</title>
      <author><first>Christopher</first><last>Bagdon</last><affiliation>Otto-Friedrich Universität Bamberg</affiliation></author>
      <author><first>Aidan</first><last>Combs</last><affiliation>Otto-Friedrich Universität Bamberg</affiliation></author>
      <author><first>Carina</first><last>Silberer</last></author>
      <author><first>Roman</first><last>Klinger</last><affiliation>Otto-Friedrich Universität Bamberg</affiliation></author>
      <pages>17307-17330</pages>
      <abstract>Accurate modeling of subjective phenomena such as emotion expression requires data annotated with authors’ intentions. Commonly such data is collected by asking study participants to donate and label genuine content produced in the real world, or create content fitting particu- lar labels during the study. Asking participants to create content is often simpler to implement and presents fewer risks to participant privacy than data donation. However, it is unclear if and how study-created content may differ from genuine content, and how differences may impact models. We collect study-created and genuine multimodal social media posts labeled for emotion and compare them on several dimen- sions, including model performance. We find that compared to genuine posts, study-created posts are longer, rely more on their text and less on their images for emotion expression, and focus more on emotion-prototypical events. The samples of participants willing to donate versus create posts are demographically different. Study-created data is valuable to train models that generalize well to genuine data, but realistic effectiveness estimates require genuine data.</abstract>
      <url hash="01c9b817">2025.acl-long.847</url>
      <bibkey>bagdon-etal-2025-donate</bibkey>
    </paper>
    <paper id="848">
      <title>Which Demographics do <fixed-case>LLM</fixed-case>s Default to During Annotation?</title>
      <author><first>Johannes</first><last>Schäfer</last><affiliation>Otto-Friedrich Universität Bamberg</affiliation></author>
      <author><first>Aidan</first><last>Combs</last><affiliation>Otto-Friedrich Universität Bamberg</affiliation></author>
      <author><first>Christopher</first><last>Bagdon</last><affiliation>Otto-Friedrich Universität Bamberg</affiliation></author>
      <author><first>Jiahui</first><last>Li</last><affiliation>Otto-Friedrich Universität Bamberg</affiliation></author>
      <author><first>Nadine</first><last>Probol</last><affiliation>Otto-Friedrich Universität Bamberg</affiliation></author>
      <author><first>Lynn</first><last>Greschner</last><affiliation>Otto-Friedrich Universität Bamberg</affiliation></author>
      <author><first>Sean</first><last>Papay</last><affiliation>University of Stuttgart</affiliation></author>
      <author><first>Yarik</first><last>Menchaca Resendiz</last></author>
      <author><first>Aswathy</first><last>Velutharambath</last><affiliation>University of Stuttgart, Universität Stuttgart</affiliation></author>
      <author><first>Amelie</first><last>Wuehrl</last></author>
      <author><first>Sabine</first><last>Weber</last></author>
      <author><first>Roman</first><last>Klinger</last><affiliation>Otto-Friedrich Universität Bamberg</affiliation></author>
      <pages>17331-17348</pages>
      <abstract>Demographics and cultural background of annotators influence the labels they assign in text annotation – for instance, an elderly woman might find it offensive to read a message addressed to a “bro”, but a male teenager might find it appropriate. It is therefore important to acknowledge label variations to not under-represent members of a society. Two research directions developed out of this observation in the context of using large language models (LLM) for data annotations, namely (1) studying biases and inherent knowledge of LLMs and (2) injecting diversity in the output by manipulating the prompt with demographic information. We combine these two strands of research and ask the question to which demographics an LLM resorts to when no demographics is given. To answer this question, we evaluate which attributes of human annotators LLMs inherently mimic. Furthermore, we compare non-demographic conditioned prompts and placebo-conditioned prompts (e.g., “you are an annotator who lives in house number 5”) to demographics-conditioned prompts (“You are a 45 year old man and an expert on politeness annotation. How do you rate instance”). We study these questions for politeness and offensiveness annotations on the POPQUORN data set, a corpus created in a controlled manner to investigate human label variations based on demographics which has not been used for LLM-based analyses so far. We observe notable influences related to gender, race, and age in demographic prompting, which contrasts with previous studies that found no such effects.</abstract>
      <url hash="659d61dd">2025.acl-long.848</url>
      <bibkey>schafer-etal-2025-demographics</bibkey>
    </paper>
    <paper id="849">
      <title>Can You Really Trust Code Copilot? Evaluating Large Language Models from a Code Security Perspective</title>
      <author><first>Yutao</first><last>Mou</last></author>
      <author><first>Xiao</first><last>Deng</last><affiliation>Peking University</affiliation></author>
      <author><first>Yuxiao</first><last>Luo</last></author>
      <author><first>Shikun</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Wei</first><last>Ye</last><affiliation>Peking University</affiliation></author>
      <pages>17349-17369</pages>
      <abstract>Code security and usability are both essential for various coding assistant applications driven by large language models (LLMs). Current code security benchmarks focus solely on single evaluation task and paradigm, such as code completion and generation, lacking comprehensive assessment across dimensions like secure code generation, vulnerability repair and discrimination. In this paper, we first propose CoV-Eval, a multi-task benchmark covering various tasks such as code completion, vulnerability repair, vulnerability detection and classification, for comprehensive evaluation of LLM code security. Besides, we developed VC-Judge, an improved judgment model that aligns closely with human experts and can review LLM-generated programs for vulnerabilities in a more efficient and reliable way. We conduct a comprehensive evaluation of 20 proprietary and open-source LLMs. Overall, while most LLMs identify vulnerable codes well, they still tend to generate insecure codes and struggle with recognizing specific vulnerability types and performing repairs. Extensive experiments and qualitative analyses reveal key challenges and optimization directions, offering insights for future research in LLM code security.</abstract>
      <url hash="33e8ddbe">2025.acl-long.849</url>
      <bibkey>mou-etal-2025-really</bibkey>
    </paper>
    <paper id="850">
      <title>From Sub-Ability Diagnosis to Human-Aligned Generation: Bridging the Gap for Text Length Control via <fixed-case>M</fixed-case>arker<fixed-case>G</fixed-case>en</title>
      <author><first>Peiwen</first><last>Yuan</last></author>
      <author><first>Chuyi</first><last>Tan</last></author>
      <author><first>Shaoxiong</first><last>Feng</last><affiliation>RedNote</affiliation></author>
      <author><first>Yiwei</first><last>Li</last></author>
      <author><first>Xinglin</first><last>Wang</last></author>
      <author><first>Yueqi</first><last>Zhang</last></author>
      <author><first>Jiayi</first><last>Shi</last></author>
      <author><first>Boyuan</first><last>Pan</last></author>
      <author><first>Yao</first><last>Hu</last><affiliation>Xiaohongshu</affiliation></author>
      <author><first>Kan</first><last>Li</last></author>
      <pages>17370-17390</pages>
      <abstract>Despite the rapid progress of large language models (LLMs), their length-controllable text generation (LCTG) ability remains below expectations, posing a major limitation for practical applications. Existing methods mainly focus on end-to-end training to reinforce adherence to length constraints. However, the lack of decomposition and targeted enhancement of LCTG sub-abilities restricts further progress. To bridge this gap, we conduct a bottom-up decomposition of LCTG sub-abilities with human patterns as reference and perform a detailed error analysis. On this basis, we propose MarkerGen, a simple-yet-effective plug-and-play approach that: (1) mitigates LLM fundamental deficiencies via external tool integration; (2) conducts explicit length modeling with dynamically inserted markers; (3) employs a three-stage generation scheme to better align length constraints while maintaining content quality. Comprehensive experiments demonstrate that MarkerGen significantly improves LCTG across various settings, exhibiting outstanding effectiveness and generalizability.</abstract>
      <url hash="c40bc1aa">2025.acl-long.850</url>
      <bibkey>yuan-etal-2025-sub</bibkey>
    </paper>
    <paper id="851">
      <title><fixed-case>AGD</fixed-case>: Adversarial Game Defense Against Jailbreak Attacks in Large Language Models</title>
      <author><first>Shilong</first><last>Pan</last></author>
      <author><first>Zhiliang</first><last>Tian</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Zhen</first><last>Huang</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Wanlong</first><last>Yu</last></author>
      <author><first>Zhihua</first><last>Wen</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Xinwang</first><last>Liu</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Kai</first><last>Lu</last></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <author><first>Dongsheng</first><last>Li</last><affiliation>National University of Defense Technology</affiliation></author>
      <pages>17391-17406</pages>
      <abstract>LLMs demonstrate remarkable utility but remain vulnerable to jailbreak attacks that aim to elicit harmful responses. Existing defenses, including post-training alignment and prompt engineering, rely on training on safety-annotated datasets and safe prompt templates, struggling with adaptability to out-of-distribution (OOD) attacks. Steering internal representations of LLMs provides real-time adjustments to defend against OOD attacks. However, it struggles with maintaining model utility, since modifying the representation disrupts the forward pass of inference. It barely considers the competitive objectives of helpfulness and harmlessness in LLMs. We argue that adversarial game-based approaches promise a solution for conflicts between the two objectives. In this paper, we propose **A**dversarial **G**ame **D**efense (AGD), an adversarial game-based defense method that dynamically adjusts LLMs’ internal representations to achieve a balanced trade-off between helpfulness and harmlessness. AGD first proposes an interquartile range (IQR) method to detect abnormal attention weights and correct the abnormal weights via adversarial training. AGD adopts a bi-level optimization to play a two-player variable-sum game to approach Nash Equilibrium (NE), where the two players adversarially refine head activations for helpfulness and harmlessness respectively. Furthermore, AGD applies an expert model to next-token sampling to generate safer responses. Experiments show that AGD significantly improves LLMs’ safety over all baselines.</abstract>
      <url hash="559c9d39">2025.acl-long.851</url>
      <bibkey>pan-etal-2025-agd</bibkey>
    </paper>
    <paper id="852">
      <title><fixed-case>SCOP</fixed-case>: Evaluating the Comprehension Process of Large Language Models from a Cognitive View</title>
      <author><first>Yongjie</first><last>Xiao</last></author>
      <author><first>Hongru</first><last>Liang</last><affiliation>Sichuan University</affiliation></author>
      <author><first>Peixin</first><last>Qin</last></author>
      <author><first>Yao</first><last>Zhang</last><affiliation>Nankai University</affiliation></author>
      <author><first>Wenqiang</first><last>Lei</last><affiliation>Sichuan University</affiliation></author>
      <pages>17407-17431</pages>
      <abstract>Despite the great potential of large language models (LLMs) in machine comprehension, it is still disturbing to fully count on them in real-world scenarios. This is probably because there is no rational explanation for whether the comprehension process of LLMs is aligned with that of experts. In this paper, we propose SCOP to carefully examine how LLMs perform during the comprehension process from a cognitive view. Specifically, it is equipped with a systematical definition of five requisite skills during the comprehension process, a strict framework to construct testing data for these skills, and a detailed analysis of advanced open-sourced and closed-sourced LLMs using the testing data. With SCOP, we find that it is still challenging for LLMs to perform an expert-level comprehension process. Even so, we notice that LLMs share some similarities with experts, e.g., performing better at comprehending local information than global information. Further analysis reveals that LLMs can be somewhat unreliable — they might reach correct answers through flawed comprehension processes. Based on SCOP, we suggest that one direction for improving LLMs is to focus more on the comprehension process, ensuring all comprehension skills are thoroughly developed during training.</abstract>
      <url hash="4ba2d0d4">2025.acl-long.852</url>
      <bibkey>xiao-etal-2025-scop</bibkey>
    </paper>
    <paper id="853">
      <title>Table-Critic: A Multi-Agent Framework for Collaborative Criticism and Refinement in Table Reasoning</title>
      <author><first>Peiying</first><last>Yu</last></author>
      <author><first>Guoxin</first><last>Chen</last></author>
      <author><first>Jingjing</first><last>Wang</last><affiliation>soochow university</affiliation></author>
      <pages>17432-17451</pages>
      <abstract>Despite the remarkable capabilities of large language models (LLMs) in various reasoning tasks, they still struggle with table reasoning tasks, particularly in maintaining consistency throughout multi-step reasoning processes. While existing approaches have explored various decomposition strategies, they often lack effective mechanisms to identify and correct errors in intermediate reasoning steps, leading to cascading error propagation. To address these issues, we propose Table-Critic, a novel multi-agent framework that facilitates collaborative criticism and iterative refinement of the reasoning process until convergence to correct solutions. Our framework consists of four specialized agents: a Judge for error identification, a Critic for comprehensive critiques, a Refiner for process improvement, and a Curator for pattern distillation. To effectively deal with diverse and unpredictable error types, we introduce a self-evolving template tree that systematically accumulates critique knowledge through experience-driven learning and guides future reflections. Extensive experiments have demonstrated that Table-Critic achieves substantial improvements over existing methods, achieving superior accuracy and error correction rates while maintaining computational efficiency and lower solution degradation rate.</abstract>
      <url hash="913f175f">2025.acl-long.853</url>
      <bibkey>yu-etal-2025-table</bibkey>
    </paper>
    <paper id="854">
      <title>An Expanded Massive Multilingual Dataset for High-Performance Language Technologies (<fixed-case>HPLT</fixed-case>)</title>
      <author><first>Laurie</first><last>Burchell</last><affiliation>Common Crawl Foundation</affiliation></author>
      <author><first>Ona</first><last>De Gibert Bonet</last><affiliation>University of Helsinki</affiliation></author>
      <author><first>Nikolay</first><last>Arefyev</last><affiliation>University of Oslo</affiliation></author>
      <author><first>Mikko</first><last>Aulamo</last><affiliation>University of Helsinki</affiliation></author>
      <author><first>Marta</first><last>Bañón</last><affiliation>Prompsit Language Engineering</affiliation></author>
      <author><first>Pinzhen</first><last>Chen</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Mariia</first><last>Fedorova</last></author>
      <author><first>Liane</first><last>Guillou</last><affiliation>Aveni</affiliation></author>
      <author><first>Barry</first><last>Haddow</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Jan</first><last>Hajič</last><affiliation>Charles University</affiliation></author>
      <author><first>Jindřich</first><last>Helcl</last><affiliation>Charles University</affiliation></author>
      <author><first>Erik</first><last>Henriksson</last><affiliation>University of Turku</affiliation></author>
      <author><first>Mateusz</first><last>Klimaszewski</last><affiliation>Warsaw University of Technology</affiliation></author>
      <author><first>Ville</first><last>Komulainen</last></author>
      <author><first>Andrey</first><last>Kutuzov</last><affiliation>University of Oslo</affiliation></author>
      <author><first>Joona</first><last>Kytöniemi</last><affiliation>University of Turku and University of Turku</affiliation></author>
      <author><first>Veronika</first><last>Laippala</last><affiliation>University of Turku</affiliation></author>
      <author><first>Petter</first><last>Mæhlum</last></author>
      <author><first>Bhavitvya</first><last>Malik</last><affiliation>Edinburgh University, University of Edinburgh</affiliation></author>
      <author><first>Farrokh</first><last>Mehryary</last><affiliation>University of Turku</affiliation></author>
      <author><first>Vladislav</first><last>Mikhailov</last><affiliation>University of Oslo</affiliation></author>
      <author><first>Nikita</first><last>Moghe</last></author>
      <author><first>Amanda</first><last>Myntti</last></author>
      <author><first>Dayyán</first><last>O’Brien</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Stephan</first><last>Oepen</last><affiliation>University of Oslo</affiliation></author>
      <author><first>Proyag</first><last>Pal</last></author>
      <author><first>Jousia</first><last>Piha</last><affiliation>University of Turku</affiliation></author>
      <author><first>Sampo</first><last>Pyysalo</last><affiliation>University of Turku</affiliation></author>
      <author><first>Gema</first><last>Ramírez-Sánchez</last><affiliation>Universidad de Alicante</affiliation></author>
      <author><first>David</first><last>Samuel</last><affiliation>University of Oslo</affiliation></author>
      <author><first>Pavel</first><last>Stepachev</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Jörg</first><last>Tiedemann</last><affiliation>University of Helsinki</affiliation></author>
      <author><first>Dušan</first><last>Variš</last><affiliation>Charles University Prague</affiliation></author>
      <author><first>Tereza</first><last>Vojtěchová</last><affiliation>Charles University Prague</affiliation></author>
      <author><first>Jaume</first><last>Zaragoza-Bernabeu</last><affiliation>Prompsit Language Engineering</affiliation></author>
      <pages>17452-17485</pages>
      <abstract>Training state-of-the-art large language models requires vast amounts of clean and diverse textual data. However, building suitable multilingual datasets remains a challenge. In this work, we present HPLT v2, a collection of high-quality multilingual monolingual and parallel corpora, extending prior work of the HPLT project. The monolingual portion of the data contains 8T tokens covering 193 languages, while the parallel data contains 380M sentence pairs covering 51 languages. We document the entire data pipeline and release the code to reproduce it. We provide extensive analysis of the quality and characteristics of our data. Finally, we evaluate the performance of language models and machine translation systems trained on HPLT v2, demonstrating its value.</abstract>
      <url hash="6b89032a">2025.acl-long.854</url>
      <bibkey>burchell-etal-2025-expanded</bibkey>
    </paper>
    <paper id="855">
      <title>Scaling Text-Rich Image Understanding via Code-Guided Synthetic Multimodal Data Generation</title>
      <author><first>Yue</first><last>Yang</last></author>
      <author><first>Ajay</first><last>Patel</last></author>
      <author><first>Matt</first><last>Deitke</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Tanmay</first><last>Gupta</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Luca</first><last>Weihs</last><affiliation>Vercept</affiliation></author>
      <author><first>Andrew</first><last>Head</last><affiliation>University of Pennsylvania, University of Pennsylvania</affiliation></author>
      <author><first>Mark</first><last>Yatskar</last><affiliation>University of Pennsylvania, University of Pennsylvania</affiliation></author>
      <author><first>Chris</first><last>Callison-Burch</last><affiliation>University of Pennsylvania and University of Pennsylvania</affiliation></author>
      <author><first>Ranjay</first><last>Krishna</last><affiliation>University of Washington</affiliation></author>
      <author><first>Aniruddha</first><last>Kembhavi</last><affiliation>Wayve</affiliation></author>
      <author><first>Christopher</first><last>Clark</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>17486-17505</pages>
      <abstract>Reasoning about images with rich text, such as charts and documents, is a critical application of vision-language models (VLMs). However, VLMs often struggle in these domains due to the scarcity of diverse text-rich vision-language data. To address this challenge, we present CoSyn, a framework that leverages the coding capabilities of text-only large language models (LLMs) to automatically create synthetic text-rich multimodal data. Given input text describing a target domain (e.g., “nutrition fact labels”), CoSyn prompts an LLM to generate code (Python, HTML, LaTeX, etc.) for rendering synthetic images. With the underlying code as textual representations of the synthetic images, CoSyn can generate high-quality instruction-tuning data, again relying on a text-only LLM. Using CoSyn, we constructed a dataset comprising 400K images and 2.7M rows of vision-language instruction-tuning data. Comprehensive experiments on seven benchmarks demonstrate that models trained on our synthetic data achieve state-of-the-art performance among competitive open-source models, including Llama 3.2, and surpass proprietary models such as GPT-4V and Gemini 1.5 Flash. Furthermore, CoSyn can produce synthetic pointing data, enabling VLMs to ground information within input images, showcasing its potential for developing multimodal agents capable of acting in real-world environments.</abstract>
      <url hash="6656559d">2025.acl-long.855</url>
      <bibkey>yang-etal-2025-scaling</bibkey>
    </paper>
    <paper id="856">
      <title>Hierarchical Attention Generates Better Proofs</title>
      <author><first>Jianlong</first><last>Chen</last></author>
      <author><first>Chao</first><last>Li</last><affiliation>Shanghai Qi Zhi Institute</affiliation></author>
      <author><first>Yang</first><last>Yuan</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Andrew C</first><last>Yao</last><affiliation>The Chinese University of Hong Kong and Tsinghua University</affiliation></author>
      <pages>17506-17520</pages>
      <abstract>Large language models (LLMs) have shown promise in formal theorem proving, but their token-level processing often fails to capture the inherent hierarchical nature of mathematical proofs. We introduce <b>Hierarchical Attention</b>, a regularization method that aligns LLMs’ attention mechanisms with mathematical reasoning structures. Our approach establishes a five-level hierarchy from foundational elements to high-level concepts, ensuring structured information flow in proof generation. Experiments demonstrate that our method improves proof success rates by 2.05% on miniF2F and 1.69% on ProofNet while reducing proof complexity by 23.81% and 16.50% respectively. The code and models will be available.</abstract>
      <url hash="fcd456b2">2025.acl-long.856</url>
      <bibkey>chen-etal-2025-hierarchical</bibkey>
    </paper>
    <paper id="857">
      <title>Agent-<fixed-case>R</fixed-case>eward<fixed-case>B</fixed-case>ench: Towards a Unified Benchmark for Reward Modeling across Perception, Planning, and Safety in Real-World Multimodal Agents</title>
      <author><first>Tianyi</first><last>Men</last><affiliation>, Institute of automation, Chinese academy of science</affiliation></author>
      <author><first>Zhuoran</first><last>Jin</last></author>
      <author><first>Pengfei</first><last>Cao</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yubo</first><last>Chen</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <author><first>Kang</first><last>Liu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jun</first><last>Zhao</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <pages>17521-17541</pages>
      <abstract>As Multimodal Large Language Models (MLLMs) advance, multimodal agents show promise in real-world tasks like web navigation and embodied intelligence. However, due to limitations in a lack of external feedback, these agents struggle with self-correction and generalization. A promising approach is to use reward models as external feedback, but there is no clear on how to select reward models for agents. Thus, there is an urgent need to build a reward bench targeted at agents. To address these challenges, we propose Agent-RewardBench, a benchmark designed to evaluate reward modeling ability in MLLMs. The benchmark is characterized by three key features: (1) Multiple dimensions and real-world agent scenarios evaluation. It covers perception, planning, and safety with 7 scenarios; (2) Step-level reward evaluation. It allows for the assessment of agent capabilities at the individual steps of a task, providing a more granular view of performance during the planning process; and (3) Appropriately difficulty and high-quality. We carefully sample from 10 diverse models, difficulty control to maintain task challenges, and manual verification to ensure the integrity of the data. Experiments demonstrate that even state-of-the-art multimodal models show limited performance, highlighting the need for specialized training in agent reward modeling. Code is available at github.</abstract>
      <url hash="c02fd1ee">2025.acl-long.857</url>
      <bibkey>men-etal-2025-agent</bibkey>
    </paper>
    <paper id="858">
      <title>It’s Not Bragging If You Can Back It Up: Can <fixed-case>LLM</fixed-case>s Understand Braggings?</title>
      <author><first>Jingjie</first><last>Zeng</last></author>
      <author><first>Huayang</first><last>Li</last></author>
      <author><first>Liang</first><last>Yang</last><affiliation>Dalian University of Technology</affiliation></author>
      <author><first>Yuanyuan</first><last>Sun</last><affiliation>Dalian University of Technology</affiliation></author>
      <author><first>Hongfei</first><last>Lin</last></author>
      <pages>17542-17560</pages>
      <abstract>Bragging, as a pervasive social-linguistic phenomenon, reflects complex human interaction patterns. However, the understanding and generation of appropriate bragging behavior in large language models (LLMs) remains underexplored. In this paper, we propose a comprehensive study that combines analytical and controllable approaches to examine bragging in LLMs. We design three tasks, <i>bragging recognition, bragging explanation, and bragging generation</i>, along with novel evaluation metrics to assess the models’ ability to identify bragging intent, social appropriateness, and account for context sensitivity. Our analysis reveals the challenges of bragging in the social context, such as recognizing bragging and responding appropriately with bragging in conversation. This work provides new insights into how LLMs process bragging and highlights the need for more research on generating contextually appropriate behavior in LLMs.</abstract>
      <url hash="a4721485">2025.acl-long.858</url>
      <bibkey>zeng-etal-2025-bragging</bibkey>
    </paper>
    <paper id="859">
      <title>A Troublemaker with Contagious Jailbreak Makes Chaos in Honest Towns</title>
      <author><first>Tianyi</first><last>Men</last><affiliation>, Institute of automation, Chinese academy of science</affiliation></author>
      <author><first>Pengfei</first><last>Cao</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Zhuoran</first><last>Jin</last></author>
      <author><first>Yubo</first><last>Chen</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <author><first>Kang</first><last>Liu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jun</first><last>Zhao</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <pages>17561-17587</pages>
      <abstract>With the development of large language models, they are widely used as agents in various fields. A key component of agents is memory, which stores vital information but is susceptible to jailbreak attacks. Existing research mainly focuses on single-agent attacks and shared memory attacks. However, real-world scenarios often involve independent memory. In this paper, we propose the Troublemaker Makes Chaos in Honest Town (TMCHT) task, a large-scale, multi-agent, multi-topology text-based attack evaluation framework. TMCHT involves one attacker agent attempting to mislead an entire society of agents. We identify two major challenges in multi-agent attacks: (1) Non-complete graph structure, (2) Large-scale systems. We attribute these challenges to a phenomenon we term toxicity disappearing. To address these issues, we propose an Adversarial Replication Contagious Jailbreak (ARCJ) method, which optimizes the retrieval suffix to make poisoned samples more easily retrieved and optimizes the replication suffix to make poisoned samples have contagious ability. We demonstrate the superiority of our approach in TMCHT, with 23.51%, 18.95%, and 52.93% improvements in line, star topologies, and 100-agent settings. It reveals potential contagion risks in widely used multi-agent architectures.</abstract>
      <url hash="1f9b3e91">2025.acl-long.859</url>
      <bibkey>men-etal-2025-troublemaker</bibkey>
    </paper>
    <paper id="860">
      <title>Meta-Learning Neural Mechanisms rather than <fixed-case>B</fixed-case>ayesian Priors</title>
      <author><first>Michael Eric</first><last>Goodale</last></author>
      <author><first>Salvador</first><last>Mascarenhas</last><affiliation>Ecole Normale Supérieure de Paris</affiliation></author>
      <author><first>Yair</first><last>Lakretz</last><affiliation>Ecole Normale Supérieure de Paris</affiliation></author>
      <pages>17588-17605</pages>
      <abstract>Children acquire language despite being exposed to several orders of magnitude less data than large language models require. Meta-learning has been proposed as a way to integrate human-like learning biases into neural-network architectures, combining both the structured generalizations of symbolic models with the scalability of neural-network models. But what does meta-learning exactly imbue the model with? We investigate the meta-learning of formal languages and find that, contrary to previous claims, meta-trained models are not learning simplicity-based priors when meta-trained on datasets organised around simplicity. Rather, we find evidence that meta-training imprints neural mechanisms (such as counters) into the model, which function like cognitive primitives for the network on downstream tasks. Most surprisingly, we find that meta-training on a *single* formal language can provide as much improvement to a model as meta-training on 5000 different formal languages, provided that the formal language incentivizes the learning of useful neural mechanisms. Taken together, our findings provide practical implications for efficient meta-learning paradigms and new theoretical insights into linking symbolic theories and neural mechanisms.</abstract>
      <url hash="9d80cc21">2025.acl-long.860</url>
      <bibkey>goodale-etal-2025-meta</bibkey>
    </paper>
    <paper id="861">
      <title>Shifting from Ranking to Set Selection for Retrieval Augmented Generation</title>
      <author><first>Dahyun</first><last>Lee</last><affiliation>LG Corporation</affiliation></author>
      <author><first>Yongrae</first><last>Jo</last><affiliation>LG Corporation</affiliation></author>
      <author><first>Haeju</first><last>Park</last><affiliation>LG Corporation</affiliation></author>
      <author><first>Moontae</first><last>Lee</last><affiliation>LG Corporation and University of Illinois, Chicago</affiliation></author>
      <pages>17606-17619</pages>
      <abstract>Retrieval in Retrieval-Augmented Generation (RAG) must ensure that retrieved passages are not only individually relevant but also collectively form a comprehensive set.Existing approaches primarily rerank top-<tex-math>k</tex-math> passages based on their individual relevance, often failing to meet the information needs of complex queries in multi-hop question answering.In this work, we propose a set-wise passage selection approach and introduce SetR, which explicitly identifies the information requirements of a query through Chain-of-Thought reasoning and selects an optimal set of passages that collectively satisfy those requirements.Experiments on multi-hop RAG benchmarks show that SetR outperforms both proprietary LLM-based rerankers and open-source baselines in terms of answer correctness and retrieval quality, providing an effective and efficient alternative to traditional rerankers in RAG systems.The code is available at https://github.com/LGAI-Research/SetR</abstract>
      <url hash="ae5fcb04">2025.acl-long.861</url>
      <bibkey>lee-etal-2025-shifting</bibkey>
    </paper>
    <paper id="862">
      <title>Understanding Large Language Model Vulnerabilities to Social Bias Attacks</title>
      <author><first>Jiaxu</first><last>Zhao</last></author>
      <author><first>Meng</first><last>Fang</last><affiliation>University of Liverpool and Eindhoven University of Technology</affiliation></author>
      <author><first>Fanghua</first><last>Ye</last><affiliation>Tencent Hunyuan / AI Lab</affiliation></author>
      <author><first>Ke</first><last>Xu</last></author>
      <author><first>Qin</first><last>Zhang</last></author>
      <author><first>Joey Tianyi</first><last>Zhou</last><affiliation>A*STAR Centre for Frontier AI Research</affiliation></author>
      <author><first>Mykola</first><last>Pechenizkiy</last><affiliation>Eindhoven University of Technology</affiliation></author>
      <pages>17620-17636</pages>
      <abstract>Large Language Models (LLMs) have become foundational in human-computer interaction, demonstrating remarkable linguistic capabilities across various tasks. However, there is a growing concern about their potential to perpetuate social biases present in their training data. In this paper, we comprehensively investigate the vulnerabilities of contemporary LLMs to various social bias attacks, including prefix injection, refusal suppression, and learned attack prompts. We evaluate popular models such as LLaMA-2, GPT-3.5, and GPT-4 across gender, racial, and religious bias types. Our findings reveal that models are generally more susceptible to gender bias attacks compared to racial or religious biases. We also explore novel aspects such as cross-bias and multiple-bias attacks, finding varying degrees of transferability across bias types. Additionally, our results show that larger models and pretrained base models often exhibit higher susceptibility to bias attacks. These insights contribute to the development of more inclusive and ethically responsible LLMs, emphasizing the importance of understanding and mitigating potential bias vulnerabilities. We offer recommendations for model developers and users to enhance the robustness of LLMs against social bias attacks.</abstract>
      <url hash="2c062199">2025.acl-long.862</url>
      <bibkey>zhao-etal-2025-understanding</bibkey>
    </paper>
    <paper id="863">
      <title><fixed-case>C</fixed-case>hat<fixed-case>SOP</fixed-case>: An <fixed-case>SOP</fixed-case>-Guided <fixed-case>MCTS</fixed-case> Planning Framework for Controllable <fixed-case>LLM</fixed-case> Dialogue Agents</title>
      <author><first>Zhigen</first><last>Li</last><affiliation>Tianjin University and Pingan Technology</affiliation></author>
      <author><first>Jianxiang</first><last>Peng</last></author>
      <author><first>Yanmeng</first><last>Wang</last><affiliation>Pingan Technology</affiliation></author>
      <author><first>Yong</first><last>Cao</last></author>
      <author><first>Tianhao</first><last>Shen</last></author>
      <author><first>Minghui</first><last>Zhang</last></author>
      <author><first>Linxi</first><last>Su</last></author>
      <author><first>Shang</first><last>Wu</last></author>
      <author><first>Yihang</first><last>Wu</last></author>
      <author><first>YuQian</first><last>Wang</last></author>
      <author><first>Ye</first><last>Wang</last><affiliation>Pingan Technology</affiliation></author>
      <author><first>Wei</first><last>Hu</last><affiliation>Pingan Technology</affiliation></author>
      <author><first>Jianfeng</first><last>Li</last><affiliation>Pingan Technology</affiliation></author>
      <author><first>Shaojun</first><last>Wang</last><affiliation>PAII Inc.</affiliation></author>
      <author><first>Jing</first><last>Xiao</last><affiliation>Pingan Group</affiliation></author>
      <author><first>Deyi</first><last>Xiong</last><affiliation>Tianjin University</affiliation></author>
      <pages>17637-17659</pages>
      <abstract>Dialogue agents powered by Large Language Models (LLMs) show superior performance in various tasks. Despite the better user understanding and human-like responses, their **lack of controllability** remains a key challenge, often leading to unfocused conversations or task failure. To address this, we introduce Standard Operating Procedure (SOP) to regulate dialogue flow. Specifically, we propose **ChatSOP**, a novel SOP-guided Monte Carlo Tree Search (MCTS) planning framework designed to enhance the controllability of LLM-driven dialogue agents. To enable this, we curate a dataset comprising SOP-annotated multi-scenario dialogues, generated using a semi-automated role-playing system with GPT-4o and validated through strict manual quality control. Additionally, we propose a novel method that integrates Chain of Thought reasoning with supervised fine-tuning for SOP prediction and utilizes SOP-guided Monte Carlo Tree Search for optimal action planning during dialogues. Experimental results demonstrate the effectiveness of our method, such as achieving a 27.95% improvement in action accuracy compared to baseline models based on GPT-3.5 and also showing notable gains for open-source models. Dataset and codes are publicly available.</abstract>
      <url hash="11075a0f">2025.acl-long.863</url>
      <bibkey>li-etal-2025-chatsop</bibkey>
    </paper>
    <paper id="864">
      <title>Pixel-Level Reasoning Segmentation via Multi-turn Conversations</title>
      <author><first>Dexian</first><last>Cai</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Xiaocui</first><last>Yang</last><affiliation>Northeastern University</affiliation></author>
      <author><first>YongKang</first><last>Liu</last><affiliation>Northeast University</affiliation></author>
      <author><first>Daling</first><last>Wang</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Shi</first><last>Feng</last><affiliation>Northeastern University, China</affiliation></author>
      <author><first>Yifei</first><last>Zhang</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Soujanya</first><last>Poria</last></author>
      <pages>17660-17679</pages>
      <abstract>Existing visual perception systems focus on region-level segmentation in single-turn dialogues, relying on complex and explicit query instructions. Such systems cannot reason at the pixel level and comprehend dynamic user intent that changes over interaction. Our work tackles this issue by introducing a novel task, Pixel-level Reasoning Segmentation (Pixel-level RS) based on multi-turn conversations, tracking evolving user intent via multi-turn interactions for fine-grained segmentation. To establish a benchmark for this novel task, we build a Pixel-level ReasonIng Segmentation Dataset Based on Multi-Turn Conversations (PRIST), comprising 24k utterances from 8.3k multi-turn conversational scenarios with segmentation targets. Building on PRIST, we further propose MIRAS, a Multi-turn Interactive ReAsoning Segmentation framework, integrates pixel-level segmentation with robust multi-turn conversation understanding, generating pixel-grounded explanations aligned with user intent. The PRIST dataset and MIRSA framework fill the gap in pixel-level reasoning segmentation. Experimental results on the PRIST dataset demonstrate that our method outperforms current segmentation-specific baselines in terms of segmentation and LLM-based reasoning metrics. The code and data are available at: https://anonymous.4open.science/r/PixelRS/.</abstract>
      <url hash="ed1e0008">2025.acl-long.864</url>
      <bibkey>cai-etal-2025-pixel</bibkey>
    </paper>
    <paper id="865">
      <title>Fixing Distribution Shifts of <fixed-case>LLM</fixed-case> Self-Critique via On-Policy Self-Play Training</title>
      <author><first>Rong</first><last>Bao</last></author>
      <author><first>Donglei</first><last>Yu</last></author>
      <author><first>Kai</first><last>Fan</last><affiliation>Alibaba Group and Alibaba Group</affiliation></author>
      <author><first>Minpeng</first><last>Liao</last></author>
      <pages>17680-17700</pages>
      <abstract>Self-critique mechanisms significantly improve the performance of language models in complex reasoning tasks by giving them the ability to correct errors, conduct induction and deduction, and switch thinking insights. However, synthetic data methods often require human-introduced errors or sampling of the model’s reasoning results from the previous moment, and the current output distribution of the model cannot be obtained, makes the data for critique and reasoning face the problem of distribution shifts. In this work, we propose an on-policy reinforcement learning framework to synchronize the reasoning and critique capabilities of language models. To alleviate reward hacking caused by outcome-based supervision, we design a deliberate reward framework for different purposes. The reward framework not only supervises the model reasoning process based on the results, but also uses Monte Carlo sampling to give appropriate rewards to the critique content according to the success rate of the model’s correction after critique. In addition, we introduce a rule-based reward function to impose penalties on the model when it generates hallucinatory critiques. When our approach is applied to the DeepSeek-Math-7B-Base and Qwen2.5-7B-Base models, model performance improves 5.40 and 3.66 points, respectively, compared to the best baseline approach. This validates the significant advantages of our method in improving model’s reasoning and self-critique capability. Code will be made available at https://github.com/rbao2018/SCOP</abstract>
      <url hash="8ae1a7aa">2025.acl-long.865</url>
      <bibkey>bao-etal-2025-fixing</bibkey>
    </paper>
    <paper id="866">
      <title>Inferring Functionality of Attention Heads from their Parameters</title>
      <author><first>Amit</first><last>Elhelo</last><affiliation>Tel Aviv University</affiliation></author>
      <author><first>Mor</first><last>Geva</last><affiliation>Tel Aviv University and Google Research</affiliation></author>
      <pages>17701-17733</pages>
      <abstract>Attention heads are one of the building blocks of large language models (LLMs). Prior work on investigating their operation mostly focused on analyzing their behavior during inference for specific circuits or tasks. In this work, we seek a comprehensive mapping of the operations they implement in a model. We propose MAPS (Mapping Attention head ParameterS), an efficient framework that infers the functionality of attention heads from their parameters, without any model training or inference. We showcase the utility of MAPS for answering two types of questions: (a) given a predefined operation, mapping how strongly heads across the model implement it, and (b) given an attention head, inferring its salient functionality. Evaluating MAPS on 20 operations across 6 popular LLMs shows its estimations correlate with the head’s outputs during inference and are causally linked to the model’s predictions. Moreover, its mappings reveal attention heads of certain operations that were overlooked in previous studies, and valuable insights on function universality and architecture biases in LLMs. Next, we present an automatic pipeline and analysis that leverage MAPS to characterize the salient operations of a given head. Our pipeline produces plausible operation descriptions for most heads, as assessed by human judgment, while revealing diverse operations.</abstract>
      <url hash="b4d672bd">2025.acl-long.866</url>
      <bibkey>elhelo-geva-2025-inferring</bibkey>
    </paper>
    <paper id="867">
      <title>Faithful and Robust <fixed-case>LLM</fixed-case>-Driven Theorem Proving for <fixed-case>NLI</fixed-case> Explanations</title>
      <author><first>Xin</first><last>Quan</last></author>
      <author><first>Marco</first><last>Valentino</last><affiliation>University of Sheffield</affiliation></author>
      <author><first>Louise A.</first><last>Dennis</last><affiliation>University of Manchester, University of Manchester</affiliation></author>
      <author><first>Andre</first><last>Freitas</last><affiliation>Idiap Research Institute and University of Manchester</affiliation></author>
      <pages>17734-17755</pages>
      <abstract>Natural language explanations play a fundamental role in Natural Language Inference (NLI) by revealing how premises logically entail hypotheses. Recent work has shown that the interaction of large language models (LLMs) with theorem provers (TPs) can help verify and improve the validity of NLI explanations. However, TPs require translating natural language into machine-verifiable formal representations, a process that introduces the risk of semantic information loss and unfaithful interpretation, an issue compounded by LLMs’ challenges in capturing critical logical structures with sufficient precision. Moreover, LLMs are still limited in their capacity for rigorous and robust proof construction within formal verification frameworks. To mitigate issues related to faithfulness and robustness, this paper investigates strategies to (1) alleviate semantic loss during autoformalisation, (2) efficiently identify and correct syntactic errors in logical representations, (3) explicitly use logical expressions to guide LLMs in generating structured proof sketches, and (4) increase LLMs’ capacity of interpreting TP’s feedback for iterative refinement. Our empirical results on e-SNLI, QASC and WorldTree using different LLMs demonstrate that the proposed strategies yield significant improvements in autoformalisation (+18.46%, +34.2%, +39.77%) and explanation refinement (+29.5%, +51.5%, +41.25%) over the state-of-the-art model. Moreover, we show that specific interventions on the hybrid LLM-TP architecture can substantially improve efficiency, drastically reducing the number of iterations required for successful verification.</abstract>
      <url hash="2489a4b6">2025.acl-long.867</url>
      <bibkey>quan-etal-2025-faithful</bibkey>
    </paper>
    <paper id="868">
      <title>Revealing the Deceptiveness of Knowledge Editing: A Mechanistic Analysis of Superficial Editing</title>
      <author><first>Jiakuan</first><last>Xie</last></author>
      <author><first>Pengfei</first><last>Cao</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yubo</first><last>Chen</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <author><first>Kang</first><last>Liu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jun</first><last>Zhao</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <pages>17756-17780</pages>
      <abstract>Knowledge editing, which aims to update the knowledge encoded in language models, can be deceptive. Despite the fact that many existing knowledge editing algorithms achieve near-perfect performance on conventional metrics, the models edited by them are still prone to generating original knowledge. This paper introduces the concept of "**superficial editing**” to describe this phenomenon. Our comprehensive evaluation reveals that this issue presents a significant challenge to existing algorithms. Through systematic investigation, we identify and validate two key factors contributing to this issue: (1) the residual stream at the last subject position in earlier layers and (2) specific attention modules in later layers. Notably, certain attention heads in later layers, along with specific left singular vectors in their output matrices, encapsulate the original knowledge and exhibit a causal relationship with superficial editing. Furthermore, we extend our analysis to the task of superficial unlearning, where we observe consistent patterns in the behavior of specific attention heads and their corresponding left singular vectors, thereby demonstrating the robustness and broader applicability of our methodology and conclusions. Our code is available at https://github.com/jiakuan929/superficial-editing.</abstract>
      <url hash="bc42fb2f">2025.acl-long.868</url>
      <bibkey>xie-etal-2025-revealing-deceptiveness</bibkey>
    </paper>
    <paper id="869">
      <title>Masking in Multi-hop <fixed-case>QA</fixed-case>: An Analysis of How Language Models Perform with Context Permutation</title>
      <author><first>Wenyu</first><last>Huang</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Pavlos</first><last>Vougiouklis</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Mirella</first><last>Lapata</last><affiliation>Edinburgh University, University of Edinburgh</affiliation></author>
      <author><first>Jeff Z.</first><last>Pan</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <pages>17781-17795</pages>
      <abstract>Multi-hop Question Answering (MHQA) adds layers of complexity to question answering, making it more challenging. When Language Models (LMs) are prompted with multiple search results, they are tasked not only with retrieving relevant information but also employing multi-hop reasoning across the information sources. Although LMs perform well on traditional question-answering tasks, the causal mask can hinder their capacity to reason across complex contexts. In this paper, we explore how LMs respond to multi-hop questions by permuting search results (retrieved documents) under various configurations. Our study reveals interesting findings as follows: 1) Encoder-decoder models, such as the ones in the Flan-T5 family, generally outperform causal decoder-only LMs in MHQA tasks, despite being significantly smaller in size; 2) altering the order of gold documents reveals distinct trends in both Flan T5 models and fine-tuned decoder-only models, with optimal performance observed when the document order aligns with the reasoning chain order; 3) enhancing causal decoder-only models with bi-directional attention by modifying the causal mask can effectively boost their end performance. In addition to the above, we conduct a thorough investigation of the distribution of LM attention weights in the context of MHQA. Our experiments reveal that attention weights tend to peak at higher values when the resulting answer is correct. We leverage this finding to heuristically improve LMs’ performance on this task. Our code is publicly available at https://github.com/hwy9855/MultiHopQA-Reasoning.</abstract>
      <url hash="f8345352">2025.acl-long.869</url>
      <bibkey>huang-etal-2025-masking</bibkey>
    </paper>
    <paper id="870">
      <title>From Human Reading to <fixed-case>NLM</fixed-case> Understanding: Evaluating the Role of Eye-Tracking Data in Encoder-Based Models</title>
      <author><first>Luca</first><last>Dini</last><affiliation>University of Pisa</affiliation></author>
      <author><first>Lucia</first><last>Domenichelli</last><affiliation>Consiglio Nazionale delle Ricerche</affiliation></author>
      <author><first>Dominique</first><last>Brunato</last><affiliation>istituto di linguistica computazionale “A. Zampolli”, ILC-CNR, Pisa</affiliation></author>
      <author><first>Felice</first><last>Dell’Orletta</last><affiliation>Istituto di Linguistica Computazionale “A. Zampolli” (ILC)</affiliation></author>
      <pages>17796-17813</pages>
      <abstract>Cognitive signals, particularly eye-tracking data, offer valuable insights into human language processing. Leveraging eye-gaze data from the Ghent Eye-Tracking Corpus, we conducted a series of experiments to examine how integrating knowledge of human reading behavior impacts Neural Language Models (NLMs) across multiple dimensions: task performance, attention mechanisms, and the geometry of their embedding space. We explored several fine-tuning methodologies to inject eye-tracking features into the models. Our results reveal that incorporating these features does not degrade downstream task performance, enhances alignment between model attention and human attention patterns, and compresses the geometry of the embedding space.</abstract>
      <url hash="91709d42">2025.acl-long.870</url>
      <bibkey>dini-etal-2025-human</bibkey>
    </paper>
    <paper id="871">
      <title>Optimizing Question Semantic Space for Dynamic Retrieval-Augmented Multi-hop Question Answering</title>
      <author><first>Linhao</first><last>Ye</last></author>
      <author><first>Lang</first><last>Yu</last></author>
      <author><first>Zhikai</first><last>Lei</last></author>
      <author><first>Qin</first><last>Chen</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <author><first>Liang</first><last>He</last><affiliation>East China Normal University</affiliation></author>
      <pages>17814-17824</pages>
      <abstract>Retrieval-augmented generation (RAG) is usually integrated into large language models (LLMs) to mitigate hallucinations and knowledge obsolescence. Whereas, conventional one-step retrieve-and-read methods are insufficient for multi-hop question answering, facing challenges of retrieval semantic mismatching and the high cost in handling interdependent subquestions. In this paper, we propose Optimizing Question Semantic Space for Dynamic Retrieval-Augmented Multi-hop Question Answering (Q-DREAM). Q-DREAM consists of three key modules: (1) the Question Decomposition Module (QDM), which decomposes multi-hop questions into fine-grained subquestions; (2) the Subquestion Dependency Optimizer Module (SDOM), which models the interdependent relations of subquestions for better understanding; and (3) the Dynamic Passage Retrieval Module (DPRM), which aligns subquestions with relevant passages by optimizing the semantic embeddings.Experimental results across various benchmarks demonstrate that Q-DREAM significantly outperforms existing RAG methods, achieving state-of-the-art performance in both in-domain and out-of-domain settings. Notably, Q-DREAM also improves retrieval efficiency while maintaining high accuracy compared with recent baselines.</abstract>
      <url hash="a80d9c7e">2025.acl-long.871</url>
      <bibkey>ye-etal-2025-optimizing</bibkey>
    </paper>
    <paper id="872">
      <title>Insight Over Sight: Exploring the Vision-Knowledge Conflicts in Multimodal <fixed-case>LLM</fixed-case>s</title>
      <author><first>Xiaoyuan</first><last>Liu</last><affiliation>Tencent AI Lab and The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <author><first>Wenxuan</first><last>Wang</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Youliang</first><last>Yuan</last><affiliation>The Chinese University of Hong Kong-Shenzhen</affiliation></author>
      <author><first>Jen-tse</first><last>Huang</last></author>
      <author><first>Qiuzhi</first><last>Liu</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Pinjia</first><last>He</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <author><first>Zhaopeng</first><last>Tu</last><affiliation>Tencent AI Lab</affiliation></author>
      <pages>17825-17846</pages>
      <abstract>This paper explores the problem of commonsense level vision-knowledge conflict in Multimodal Large Language Models (MLLMs), where visual information contradicts model’s internal commonsense knowledge. To study this issue, we introduce an automated framework, augmented with human-in-the-loop quality control, to generate inputs designed to simulate and evaluate these conflicts in MLLMs. Using this framework, we have crafted a diagnostic benchmark consisting of 374 original images and 1,122 high-quality question-answer (QA) pairs. The benchmark covers two aspects of conflict and three question types, providing a thorough assessment tool. We apply this benchmark to assess the conflict-resolution capabilities of nine representative MLLMs from various model families. Our results indicate an evident over-reliance on parametric knowledge for approximately 20% of all queries, especially among Yes-No and action-related problems. Based on these findings, we evaluate the effectiveness of existing approaches to mitigating the conflicts and compare them to our “Focus-on-Vision” prompting strategy. Despite some improvement, the vision-knowledge conflict remains unresolved and can be further scaled through our data construction framework. Our proposed framework, benchmark, and analysis contribute to the understanding and mitigation of vision-knowledge conflicts in MLLMs.</abstract>
      <url hash="d52be09a">2025.acl-long.872</url>
      <bibkey>liu-etal-2025-insight</bibkey>
    </paper>
    <paper id="873">
      <title><fixed-case>S</fixed-case>cene<fixed-case>G</fixed-case>en<fixed-case>A</fixed-case>gent: Precise Industrial Scene Generation with Coding Agent</title>
      <author><first>Xiao</first><last>Xia</last></author>
      <author id="dan-zhang"><first>Dan</first><last>Zhang</last></author>
      <author><first>Zibo</first><last>Liao</last><affiliation>Siemens Foundational Technologies</affiliation></author>
      <author><first>Zhenyu</first><last>Hou</last></author>
      <author><first>Tianrui</first><last>Sun</last><affiliation>Siemens Ltd., China</affiliation></author>
      <author><first>Jing</first><last>Li</last></author>
      <author><first>Ling</first><last>Fu</last></author>
      <author><first>Yuxiao</first><last>Dong</last><affiliation>Tsinghua University</affiliation></author>
      <pages>17847-17875</pages>
      <abstract>The modeling of industrial scenes is essential for simulations in industrial manufacturing. While large language models (LLMs) have shown significant progress in generating general 3D scenes from textual descriptions, generating industrial scenes with LLMs poses a unique challenge due to their demand for precise measurements and positioning, requiring complex planning over spatial arrangement. To address this challenge, we introduce SceneGenAgent, an LLM-based agent for generating industrial scenes through C# code. SceneGenAgent ensures precise layout planning through a structured and calculable format, layout verification, and iterative refinement to meet the quantitative requirements of industrial scenarios. Experiment results demonstrate that LLMs powered by SceneGenAgent exceed their original performance, reaching up to 81.0% success rate in real-world industrial scene generation tasks and effectively meeting most scene generation requirements. To further enhance accessibility, we construct SceneInstruct, a dataset designed for fine-tuning open-source LLMs to integrate into SceneGenAgent. Experiments show that fine-tuning open-source LLMs on SceneInstruct yields significant performance improvements, with Llama3.1-70B approaching the capabilities of GPT-4o. Our code and dataset are available at https://github.com/THUDM/SceneGenAgent.</abstract>
      <url hash="ed259f99">2025.acl-long.873</url>
      <bibkey>xia-etal-2025-scenegenagent</bibkey>
    </paper>
    <paper id="874">
      <title><fixed-case>T</fixed-case>ool<fixed-case>C</fixed-case>oder: A Systematic Code-Empowered Tool Learning Framework for Large Language Models</title>
      <author><first>Hanxing</first><last>Ding</last></author>
      <author><first>Shuchang</first><last>Tao</last></author>
      <author><first>Liang</first><last>Pang</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Zihao</first><last>Wei</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jinyang</first><last>Gao</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Bolin</first><last>Ding</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Huawei</first><last>Shen</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xueqi</first><last>Cheng</last><affiliation>Institute of Computing Technology, Chinese Academy</affiliation></author>
      <pages>17876-17891</pages>
      <abstract>Tool learning has emerged as a crucial capability for large language models (LLMs) to solve complex real-world tasks through interaction with external tools. Existing approaches face significant challenges, including reliance on hand-crafted prompts, difficulty in multi-step planning, and lack of precise error diagnosis and reflection mechanisms. We propose <tex-math>\texttt{ToolCoder}</tex-math>, a novel framework that reformulates tool learning as a code generation task. Inspired by software engineering principles, <tex-math>\texttt{ToolCoder}</tex-math> transforms natural language queries into structured Python function scaffold and systematically breaks down tasks with descriptive comments, enabling LLMs to leverage coding paradigms for complex reasoning and planning. It then generates and executes function implementations to obtain final responses. Additionally, <tex-math>\texttt{ToolCoder}</tex-math> stores successfully executed functions in a repository to promote code reuse, while leveraging error traceback mechanisms for systematic debugging, optimizing both execution efficiency and robustness. Experiments demonstrate that <tex-math>\texttt{ToolCoder}</tex-math> achieves superior performance in task completion accuracy and execution reliability compared to existing approaches, establishing the effectiveness of code-centric approaches in tool learning.</abstract>
      <url hash="cd8366fb">2025.acl-long.874</url>
      <bibkey>ding-etal-2025-toolcoder</bibkey>
    </paper>
    <paper id="875">
      <title>Enhancing Text Editing for Grammatical Error Correction: <fixed-case>A</fixed-case>rabic as a Case Study</title>
      <author><first>Bashar</first><last>Alhafni</last><affiliation>New York University</affiliation></author>
      <author><first>Nizar</first><last>Habash</last><affiliation>New York University Abu Dhabi</affiliation></author>
      <pages>17892-17914</pages>
      <abstract>Text editing frames grammatical error correction (GEC) as a sequence tagging problem, where edit tags are assigned to input tokens, and applying these edits results in the corrected text. This approach has gained attention for its efficiency and interpretability. However, while extensively explored for English, text editing remains largely underexplored for morphologically rich languages like Arabic. In this paper, we introduce a text editing approach that derives edit tags directly from data, eliminating the need for language-specific edits. We demonstrate its effectiveness on Arabic, a diglossic and morphologically rich language, and investigate the impact of different edit representations on model performance. Our approach achieves SOTA results on two Arabic GEC benchmarks and performs on par with SOTA on two others. Additionally, our models are over six times faster than existing Arabic GEC systems, making our approach more practical for real-world applications. Finally, we explore ensemble models, demonstrating how combining different models leads to further performance improvements. We make our code, data, and pretrained models publicly available.</abstract>
      <url hash="ed0d62ec">2025.acl-long.875</url>
      <bibkey>alhafni-habash-2025-enhancing</bibkey>
    </paper>
    <paper id="876">
      <title>From Isolates to Families: Using Neural Networks for Automated Language Affiliation</title>
      <author><first>Frederic</first><last>Blum</last><affiliation>Max-Planck Institute for Evolutionary Anthropology</affiliation></author>
      <author><first>Steffen</first><last>Herbold</last><affiliation>Universität Passau</affiliation></author>
      <author><first>Johann-Mattis</first><last>List</last><affiliation>Universität Passau and Max-Planck Institute</affiliation></author>
      <pages>17915-17927</pages>
      <abstract>In historical linguistics, the affiliation of languages to a common language family is traditionally carried out using a complex workflow that relies on manually comparing individual languages. Large-scale standardized collections of multilingual wordlists and grammatical language structures might help to improve this and open new avenues for developing automated language affiliation workflows. Here, we present neural network models that use lexical and grammatical data from a worldwide sample of more than 1,200 languages with known affiliations to classify individual languages into families. In line with the traditional assumption of most linguists, our results show that models trained on lexical data alone outperform models solely based on grammatical data, whereas combining both types of data yields even better performance. In additional experiments, we show how our models can identify long-ranging relations between entire subgroups, how they can be employed to investigate potential relatives of linguistic isolates, and how they can help us to obtain first hints on the affiliation of so far unaffiliated languages. We conclude that models for automated language affiliation trained on lexical and grammatical data provide comparative linguists with a valuable tool for evaluating hypotheses about deep and unknown language relations.</abstract>
      <url hash="66de1b5a">2025.acl-long.876</url>
      <bibkey>blum-etal-2025-isolates</bibkey>
    </paper>
    <paper id="877">
      <title><fixed-case>ELBA</fixed-case>-Bench: An Efficient Learning Backdoor Attacks Benchmark for Large Language Models</title>
      <author><first>Xuxu</first><last>Liu</last></author>
      <author><first>Siyuan</first><last>Liang</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Mengya</first><last>Han</last></author>
      <author><first>Yong</first><last>Luo</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Aishan</first><last>Liu</last><affiliation>Beihang University</affiliation></author>
      <author><first>Xiantao</first><last>Cai</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Zheng</first><last>He</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Dacheng</first><last>Tao</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>17928-17947</pages>
      <abstract>Generative large language models are crucial in natural language processing, but they are vulnerable to backdoor attacks, where subtle triggers compromise their behavior. Although backdoor attacks against LLMs are constantly emerging, existing benchmarks remain limited in terms of sufficient coverage of attack, metric system integrity, backdoor attack alignment. And existing pre-trained backdoor attacks are idealized in practice due to resource access constraints. Therefore we establish <tex-math>\textit{ELBA-Bench}</tex-math>, a comprehensive and unified framework that allows attackers to inject backdoor through parameter efficient fine-tuning (<tex-math>\textit{e.g.,}</tex-math> LoRA) or without fine-tuning techniques (<tex-math>\textit{e.g.,}</tex-math> In-context-learning). <tex-math>\textit{ELBA-Bench}</tex-math> provides over 1300 experiments encompassing the implementations of 12 attack methods, 18 datasets, and 12 LLMs. Extensive experiments provide new invaluable findings into the strengths and limitations of various attack strategies. For instance, PEFT attack consistently outperform without fine-tuning approaches in classification tasks while showing strong cross-dataset generalization with optimized triggers boosting robustness; Task-relevant backdoor optimization techniques or attack prompts along with clean and adversarial demonstrations can enhance backdoor attack success while preserving model performance on clean samples. Additionally, we introduce a universal toolbox designed for standardized backdoor attack research at https://github.com/NWPUliuxx/ELBA_Bench, with the goal of propelling further progress in this vital area.</abstract>
      <url hash="acf34f2f">2025.acl-long.877</url>
      <bibkey>liu-etal-2025-elba</bibkey>
    </paper>
    <paper id="878">
      <title>Less, but Better: Efficient Multilingual Expansion for <fixed-case>LLM</fixed-case>s via Layer-wise Mixture-of-Experts</title>
      <author><first>Xue</first><last>Zhang</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Yunlong</first><last>Liang</last></author>
      <author><first>Fandong</first><last>Meng</last><affiliation>WeChat AI, Tencent Inc.</affiliation></author>
      <author><first>Songming</first><last>Zhang</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Yufeng</first><last>Chen</last></author>
      <author><first>Jinan</first><last>Xu</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>17948-17963</pages>
      <abstract>Continually expanding new languages for existing large language models (LLMs) is a promising yet challenging approach to building powerful multilingual LLMs.The biggest challenge is to make the model continuously learn new languages while preserving the proficient ability of old languages.To achieve this, recent work utilizes the Mixture-of-Experts (MoE) architecture to expand new languages by adding new experts and avoid catastrophic forgetting of old languages by routing corresponding tokens to the original model backbone (old experts).Although intuitive, this kind of method is parameter-costly when expanding new languages and still inevitably impacts the performance of old languages.To address these limitations, we analyze the language characteristics of different layers in LLMs and propose a layer-wise expert allocation algorithm (LayerMoE) to determine the appropriate number of new experts for each layer.Specifically, we find different layers in LLMs exhibit different representation similarities between languages and then utilize the similarity as the indicator to allocate experts for each layer, i.e., the higher similarity, the fewer experts.Additionally, to further mitigate the forgetting of old languages, we add a classifier in front of the router network on the layers with higher similarity to guide the routing of old language tokens.Experimental results show that our method outperforms the previous state-of-the-art baseline with 60% fewer experts in the single-expansion setting and with 33.3% fewer experts in the lifelong-expansion setting, demonstrating the effectiveness of our method.</abstract>
      <url hash="2dbd6f91">2025.acl-long.878</url>
      <bibkey>zhang-etal-2025-less</bibkey>
    </paper>
    <paper id="879">
      <title>When Harry Meets Superman: The Role of The Interlocutor in Persona-Based Dialogue Generation</title>
      <author><first>Daniela</first><last>Occhipinti</last></author>
      <author><first>Marco</first><last>Guerini</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <author><first>Malvina</first><last>Nissim</last><affiliation>University of Groningen</affiliation></author>
      <pages>17964-17985</pages>
      <abstract>Endowing dialogue agents with persona information has proven to significantly improve the consistency and diversity of their generations. While much focus has been placed on aligning dialogues with provided personas, the adaptation to the interlocutor’s profile remains largely underexplored. In this work, we investigate three key aspects: (1) a model’s ability to align responses with both the provided persona and the interlocutor’s; (2) its robustness when dealing with familiar versus unfamiliar interlocutors and topics, and (3) the impact of additional fine-tuning on specific persona-based dialogues. We evaluate dialogues generated with diverse speaker pairings and topics, framing the evaluation as an author identification task and employing both LLM-as-a-judge and human evaluations. By systematically masking or disclosing information about interlocutor, we assess its impact on dialogue generation. Results show that access to the interlocutor’s persona improves the recognition of the target speaker, while masking it does the opposite. Although models generalise well across topics, they struggle with unfamiliar interlocutors. Finally, we found that in zero-shot settings, LLMs often copy biographical details, facilitating identification but trivialising the task.</abstract>
      <url hash="82ada84f">2025.acl-long.879</url>
      <bibkey>occhipinti-etal-2025-harry</bibkey>
    </paper>
    <paper id="880">
      <title><fixed-case>ICR</fixed-case> Probe: Tracking Hidden State Dynamics for Reliable Hallucination Detection in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Zhenliang</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Xinyu</first><last>Hu</last><affiliation>Peking University</affiliation></author>
      <author><first>Huixuan</first><last>Zhang</last></author>
      <author><first>Junzhe</first><last>Zhang</last></author>
      <author><first>Xiaojun</first><last>Wan</last></author>
      <pages>17986-18002</pages>
      <abstract>Large language models (LLMs) excel at various natural language processing tasks, but their tendency to generate hallucinations undermines their reliability. Existing hallucination detection methods leveraging hidden states predominantly focus on static and isolated representations, overlooking their dynamic evolution across layers, which limits efficacy. To address this limitation, we shift the focus to the hidden state update process and introduce a novel metric, the **ICR** Score (**I**nformation **C**ontribution to **R**esidual Stream), which quantifies the contribution of modules to the hidden states’ update. We empirically validate that the ICR Score is effective and reliable in distinguishing hallucinations. Building on these insights, we propose a hallucination detection method, the ICR Probe, which captures the cross-layer evolution of hidden states. Experimental results show that the ICR Probe achieves superior performance with significantly fewer parameters. Furthermore, ablation studies and case analyses offer deeper insights into the underlying mechanism of this method, improving its interpretability.</abstract>
      <url hash="3ffa963d">2025.acl-long.880</url>
      <bibkey>zhang-etal-2025-icr</bibkey>
    </paper>
    <paper id="881">
      <title>Revisit Self-Debugging with Self-Generated Tests for Code Generation</title>
      <author><first>Xiancai</first><last>Chen</last></author>
      <author><first>Zhengwei</first><last>Tao</last></author>
      <author><first>Kechi</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Changzhi</first><last>Zhou</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Xinyu</first><last>Zhang</last></author>
      <author><first>Wanli</first><last>Gu</last></author>
      <author><first>Yuanpeng</first><last>He</last><affiliation>Peking University</affiliation></author>
      <author><first>Mengdi</first><last>Zhang</last></author>
      <author><first>Xunliang</first><last>Cai</last><affiliation>Meituan</affiliation></author>
      <author><first>Haiyan</first><last>Zhao</last><affiliation>Peking University</affiliation></author>
      <author><first>Zhi</first><last>Jin</last><affiliation>Peking University</affiliation></author>
      <pages>18003-18023</pages>
      <abstract>Large language models (LLMs) have demonstrated significant advancements in code generation, yet they still face challenges when tackling tasks that extend beyond their basic capabilities. Recently, the concept of self-debugging has been proposed as a way to enhance code generation performance by leveraging execution feedback from tests. However, the availability of high-quality tests in real-world scenarios is often limited. In this context, self-debugging with self-generated tests emerges as a promising solution, though its limitations and practical potential have not been fully explored. To address this gap, we investigate the efficacy of self-debugging in code generation tasks. We propose and analyze two distinct paradigms for the self-debugging process: post-execution and in-execution self-debugging. Our findings reveal that post-execution self-debugging struggles with the test bias introduced by self-generated tests, which can lead to misleading feedback. In contrast, in-execution self-debugging enables LLMs to mitigate this bias and leverage intermediate states during program execution. By focusing on runtime information rather than relying solely on potentially flawed self-generated tests, this approach demonstrates significant promise for improving the robustness and accuracy of LLMs in code generation tasks.</abstract>
      <url hash="f7c41683">2025.acl-long.881</url>
      <bibkey>chen-etal-2025-revisit</bibkey>
    </paper>
    <paper id="882">
      <title><fixed-case>I</fixed-case>n<fixed-case>S</fixed-case>erter: Speech Instruction Following with Unsupervised Interleaved Pre-training</title>
      <author><first>Dingdong</first><last>Wang</last></author>
      <author><first>Jin</first><last>Xu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Ruihang</first><last>Chu</last><affiliation>Wan, Alibaba Group and Tsinghua Shenzhen International Graduate School, Tsinghua University</affiliation></author>
      <author><first>Zhifang</first><last>Guo</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Xiong</first><last>Wang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Jincenzi</first><last>Wu</last></author>
      <author><first>Dongchao</first><last>Yang</last></author>
      <author><first>Shengpeng</first><last>Ji</last></author>
      <author><first>Junyang</first><last>Lin</last><affiliation>Alibaba Group</affiliation></author>
      <pages>18024-18046</pages>
      <abstract>Recent advancements in speech large language models (SpeechLLMs) have attracted considerable attention. Nonetheless, current methods exhibit suboptimal performance in adhering to speech instructions. Notably, the intelligence of models significantly diminishes when processing speech-form input as compared to direct text-form input. Prior work has attempted to mitigate this semantic inconsistency between speech and text representations through techniques such as representation and behavior alignment, which involve the meticulous design of data pairs during the post-training phase. In this paper, we introduce a simple and scalable training method called InSerter, which stands for Interleaved Speech-Text Representation Pre-training. InSerter is designed to pre-train large-scale unsupervised speech-text sequences, where the speech is synthesized from randomly selected segments of an extensive text corpus using text-to-speech conversion. Consequently, the model acquires the ability to generate textual continuations corresponding to the provided speech segments, obviating the need for intensive data design endeavors. To systematically evaluate speech instruction-following capabilities, we introduce SpeechInstructBench, the first comprehensive benchmark specifically designed for speech-oriented instruction-following tasks. Our proposed model InSerter achieves SOTA performance in SpeechInstructBench and demonstrates superior or competitive results across diverse speech processing tasks.</abstract>
      <url hash="c90309aa">2025.acl-long.882</url>
      <bibkey>wang-etal-2025-inserter</bibkey>
    </paper>
    <paper id="883">
      <title>Exploring <fixed-case>LLM</fixed-case>s’ Ability to Spontaneously and Conditionally Modify Moral Expressions through Text Manipulation</title>
      <author><first>Candida Maria</first><last>Greco</last><affiliation>University of Calabria</affiliation></author>
      <author><first>Lucio</first><last>La Cava</last><affiliation>University of Calabria</affiliation></author>
      <author><first>Lorenzo</first><last>Zangari</last></author>
      <author><first>Andrea</first><last>Tagarelli</last><affiliation>University of Calabria</affiliation></author>
      <pages>18047-18070</pages>
      <abstract>Morality serves as the foundation of societal structure, guiding legal systems, shaping cultural values, and influencing individual self-perception. With the rise and pervasiveness of generative AI tools, and particularly Large Language Models (LLMs), concerns arise regarding how these tools capture and potentially alter moral dimensions through machine-generated text manipulation. Based on the Moral Foundation Theory, our work investigates this topic by analyzing the behavior of 12 LLMs among the most widely used Open and uncensored (i.e., ”abliterated”) models, and leveraging human-annotated datasets used in moral-related analysis. Results have shown varying levels of alteration of moral expressions depending on the type of text modification task and moral-related conditioning prompt.</abstract>
      <url hash="0bdb2e66">2025.acl-long.883</url>
      <bibkey>greco-etal-2025-exploring</bibkey>
    </paper>
    <paper id="884">
      <title>Mixture of Ordered Scoring Experts for Cross-prompt Essay Trait Scoring</title>
      <author><first>Po-Kai</first><last>Chen</last><affiliation>National Central University</affiliation></author>
      <author><first>Bo-Wei</first><last>Tsai</last></author>
      <author><first>Shao Kuan</first><last>Wei</last><affiliation>National Taiwan University of Science and Technology</affiliation></author>
      <author><first>Chien-Yao</first><last>Wang</last><affiliation>Academia Sinica</affiliation></author>
      <author><first>Jia-Ching</first><last>Wang</last></author>
      <author><first>Yi-Ting</first><last>Huang</last></author>
      <pages>18071-18084</pages>
      <abstract>Automated Essay Scoring (AES) plays a crucial role in language assessment. In particular, cross-prompt essay trait scoring provides learners with valuable feedback to improve their writing skills. However, due to the scarcity of prompts, most existing methods overlook critical information, such as content from prompts or essays, resulting in incomplete assessment perspectives. In this paper, we propose a robust AES framework, the Mixture of Ordered Scoring Experts (MOOSE), which integrates information from both prompts and essays. MOOSE employs three specialized experts to evaluate (1) the overall quality of an essay, (2) the relative quality across multiple essays, and (3) the relevance between an essay and its prompt. MOOSE introduces the ordered aggregation of assessment results from these experts along with effective feature learning techniques. Experimental results demonstrate that MOOSE achieves exceptionally stable and state-of-the-art performance in both cross-prompt scoring and multi-trait scoring on the ASAP++ dataset. The source code is released at https://github.com/antslabtw/MOOSE-AES.</abstract>
      <url hash="ad37fc51">2025.acl-long.884</url>
      <bibkey>chen-etal-2025-mixture-ordered</bibkey>
    </paper>
    <paper id="885">
      <title>Sparse Logit Sampling: Accelerating Knowledge Distillation in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Anshumann</first><last>Anshumann</last></author>
      <author><first>Mohd Abbas</first><last>Zaidi</last><affiliation>Samsung Research</affiliation></author>
      <author><first>Akhil</first><last>Kedia</last><affiliation>Samsung</affiliation></author>
      <author><first>Jinwoo</first><last>Ahn</last><affiliation>Samsung Research</affiliation></author>
      <author><first>Taehwak</first><last>Kwon</last><affiliation>Samsung</affiliation></author>
      <author><first>Kangwook</first><last>Lee</last><affiliation>Samsung</affiliation></author>
      <author><first>Haejun</first><last>Lee</last></author>
      <author><first>Joohyung</first><last>Lee</last><affiliation>Arizona State University and Samsung Research</affiliation></author>
      <pages>18085-18108</pages>
      <abstract>Knowledge distillation can be a cost-effective technique to distill knowledge in Large Language Models, if the teacher output logits can be pre-computed and cached. However, successfully applying this to pre-training remains largely unexplored. In this work, we prove that naive approaches for sparse knowledge distillation such as caching Top-K probabilities, while intuitive, provide biased estimates of teacher probability distribution to the student, resulting in suboptimal performance and calibration. We propose an importance-sampling-based method ‘Random Sampling Knowledge Distillation’, which provides unbiased estimates, preserves the gradient in expectation, and requires storing significantly sparser logits. Our method enables faster training of student models with marginal overhead (&lt;10%) compared to cross-entropy based training, while maintaining competitive performance compared to full distillation, across a range of model sizes from 300M to 3B.</abstract>
      <url hash="57d3fe8b">2025.acl-long.885</url>
      <bibkey>anshumann-etal-2025-sparse</bibkey>
    </paper>
    <paper id="886">
      <title>Enhancing Spoken Discourse Modeling in Language Models Using Gestural Cues</title>
      <author><first>Varsha</first><last>Suresh</last><affiliation>Universität des Saarlandes</affiliation></author>
      <author><first>M. Hamza</first><last>Mughal</last><affiliation>Saarland Informatics Campus, Max-Planck Institute</affiliation></author>
      <author><first>Christian</first><last>Theobalt</last><affiliation>Saarbruecken Research Center for Visual Computing, Interaction, and Artificial Intellligence and Max-Planck-Institute for Informatics, Saarland Informatics Campus</affiliation></author>
      <author><first>Vera</first><last>Demberg</last><affiliation>Universität des Saarlandes</affiliation></author>
      <pages>18109-18123</pages>
      <abstract>Research in linguistics shows that non-verbal cues, such as gestures, play a crucial role in spoken discourse. For example, speakers perform hand gestures to indicate topic shifts, helping listeners identify transitions in discourse. In this work, we investigate whether the joint modeling of gestures using human motion sequences and language can improve spoken discourse modeling in language models. To integrate gestures into language models, we first encode 3D human motion sequences into discrete gesture tokens using a VQ-VAE. These gesture token embeddings are then aligned with text embeddings through feature alignment, mapping them into the text embedding space. To evaluate the gesture-aligned language model on spoken discourse, we construct text infilling tasks targeting three key discourse cues grounded in linguistic research: discourse connectives, stance markers, and quantifiers. Results show that incorporating gestures enhances marker prediction accuracy across the three tasks, highlighting the complementary information that gestures can offer in modeling spoken discourse. We view this work as an initial step toward leveraging non-verbal cues to advance spoken language modeling in language models.</abstract>
      <url hash="94668522">2025.acl-long.886</url>
      <bibkey>suresh-etal-2025-enhancing</bibkey>
    </paper>
    <paper id="887">
      <title><fixed-case>E</fixed-case>xplora<fixed-case>C</fixed-case>oder: Advancing Code Generation for Multiple Unseen <fixed-case>API</fixed-case>s via Planning and Chained Exploration</title>
      <author><first>Yunkun</first><last>Wang</last></author>
      <author><first>Yue</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Zhen</first><last>Qin</last></author>
      <author><first>Chen</first><last>Zhi</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Binhua</first><last>Li</last></author>
      <author><first>Fei</first><last>Huang</last><affiliation>Alibaba Group US</affiliation></author>
      <author><first>Yongbin</first><last>Li</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Shuiguang</first><last>Deng</last><affiliation>Zhejiang University</affiliation></author>
      <pages>18124-18145</pages>
      <abstract>Large language models face intrinsic limitations in coding with APIs that are unseen in their training corpora. As libraries continuously evolve, it becomes impractical to exhaustively retrain LLMs with new API knowledge. This limitation hampers LLMs from solving programming problems which require newly introduced or privately maintained libraries. Inspired by exploratory programming paradigm in human behavior, we propose **ExploraCoder**, a training-free framework that empowers LLMs to invoke multiple unseen APIs in code solution by (1) planning a complex problem into several API invocation subtasks, and (2) experimenting with correct API usage at intermediate steps through a novel chain-of-API-exploration. We conduct evaluation on program synthesizing tasks involving complex API interactions. Experimental results demonstrate that ExploraCoder significantly improves performance for models lacking prior API knowledge, achieving absolute increases of up to 11.99% over retrieval-based approaches and 17.28% over pretraining-based methods in pass@10.</abstract>
      <url hash="ecc9f9d9">2025.acl-long.887</url>
      <bibkey>wang-etal-2025-exploracoder</bibkey>
    </paper>
    <paper id="888">
      <title>Segment First or Comprehend First? Explore the Limit of Unsupervised Word Segmentation with Large Language Models</title>
      <author><first>Zihong</first><last>Zhang</last></author>
      <author><first>Liqi</first><last>He</last></author>
      <author><first>Zuchao</first><last>Li</last></author>
      <author><first>Lefei</first><last>Zhang</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Hai</first><last>Zhao</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Bo</first><last>Du</last><affiliation>Wuhan University</affiliation></author>
      <pages>18146-18163</pages>
      <abstract>Word segmentation stands as a cornerstone of Natural Language Processing (NLP). Based on the concept of “comprehend first, segment later”, we propose a new framework to explore the limit of unsupervised word segmentation with Large Language Models (LLMs) and evaluate the semantic understanding capabilities of LLMs based on word segmentation. We employ current mainstream LLMs to perform word segmentation across multiple languages to assess LLMs’ “comprehension”. Our findings reveal that LLMs are capable of following simple prompts to segment raw text into words. There is a trend suggesting that models with more parameters tend to perform better on multiple languages. Additionally, we introduce a novel unsupervised method, termed LLACA (<tex-math>\textbf{L}</tex-math>arge <tex-math>\textbf{L}</tex-math>anguage Model-Inspired <tex-math>\textbf{A}</tex-math>ho-<tex-math>\textbf{C}</tex-math>orasick <tex-math>\textbf{A}</tex-math>utomaton). Leveraging the advanced pattern recognition capabilities of Aho-Corasick automata, LLACA innovatively combines these with the deep insights of well-pretrained LLMs. This approach not only enables the construction of a dynamic <tex-math>n</tex-math>-gram model that adjusts based on contextual information but also integrates the nuanced understanding of LLMs, offering significant improvements over traditional methods. Our source code is available at https://github.com/hkr04/LLACA</abstract>
      <url hash="fcaad28a">2025.acl-long.888</url>
      <bibkey>zhang-etal-2025-segment</bibkey>
    </paper>
    <paper id="889">
      <title><fixed-case>RUBY</fixed-case>: An Effective Framework for Multi-Constraint Multi-Hop Question Generation</title>
      <author><first>Wenzhuo</first><last>Zhao</last></author>
      <author><first>Shuangyin</first><last>Li</last></author>
      <pages>18164-18188</pages>
      <abstract>Inspired by theories in language psychology, it is natural to consider more constraints, such as intentions, logic, knowledge, etc., when a complex or multi-hop question is generated. As the subtask of Multi-Hop Question Generation (MHQG), the task of Multi-Constraint Multi-Hop Question Generation (MCHQG) is more aligned with human question theories. However, it is hard to determine how to bring various high-dimensional semantic constraints, and how to integrate each constraint across all hops when a multi-hop question is being generating. To address these challenges, we introduce an effective framework which includes constraint dimensionality reduction and divide-and-conquer-based dynamic projection; we call it RUBY. The proposed RUBY contains a module of high-dimensional semantic constraint dimension reduction and a module of sub-question answer pairs-based multi-hop question generation. Meanwhile, a Reasoning Dynamic Projection strategy is tailored to effectively incorporate the constraints into every hop of the multi-hop question. The experimental results demonstrate that RUBY consistently outperforms baseline models, which suggest that RUBY is able to effectively capture and integrate semantic constraints, leading to more accurate and human-like multi-hop question generation. Our code and data are available.</abstract>
      <url hash="1b82379e">2025.acl-long.889</url>
      <bibkey>zhao-li-2025-ruby</bibkey>
    </paper>
    <paper id="890">
      <title>Can Indirect Prompt Injection Attacks Be Detected and Removed?</title>
      <author><first>Yulin</first><last>Chen</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Haoran</first><last>Li</last></author>
      <author><first>Yuan</first><last>Sui</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Yufei</first><last>He</last></author>
      <author><first>Yue</first><last>Liu</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Yangqiu</first><last>Song</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Bryan</first><last>Hooi</last><affiliation>National University of Singapore</affiliation></author>
      <pages>18189-18206</pages>
      <abstract>Prompt injection attacks manipulate large language models (LLMs) by misleading them to deviate from the original input instructions and execute maliciously injected instructions, because of their instruction-following capabilities and inability to distinguish between the original input instructions and maliciously injected instructions. To defend against such attacks, recent studies have developed various detection mechanisms. If we restrict ourselves specifically to works which perform detection rather than direct defense, most of them focus on direct prompt injection attacks, while there are few works for the indirect scenario, where injected instructions are indirectly from external tools, such as a search engine. Moreover, current works mainly investigate injection detection methods and pay less attention to the post-processing method that aims to mitigate the injection after detection.In this paper, we investigate the feasibility of detecting and removing indirect prompt injection attacks, and we construct a benchmark dataset for evaluation. For detection, we assess the performance of existing LLMs and open-source detection models, and we further train detection models using our crafted training datasets. For removal, we evaluate two intuitive methods: (1) the *segmentation removal method*, which segments the injected document and removes parts containing injected instructions, and (2) the *extraction removal method*, which trains an extraction model to identify and remove injected instructions.</abstract>
      <url hash="5f9d396d">2025.acl-long.890</url>
      <bibkey>chen-etal-2025-indirect</bibkey>
    </paper>
    <paper id="891">
      <title>Identifying Open Challenges in Language Identification</title>
      <author><first>Rob Van Der</first><last>Goot</last><affiliation>IT University of Copenhagen</affiliation></author>
      <pages>18207-18227</pages>
      <abstract>Automatic language identification is a core problem of many Natural LanguageProcessing (NLP) pipelines. A wide variety of architectures and benchmarks havebeen proposed with often near-perfect performance. Although previousstudies have focused on certain challenging setups (i.e. cross-domain, shortinputs), a systematic comparison is missing. We propose a benchmark that allows us to test for the effect of input size, training data size, domain, number oflanguages, scripts, and language families on performance. We evaluatefive popular models on this benchmark and identify which open challengesremain for this task as well as which architectures achieve robust performance. Wefind that cross-domain setups are the most challenging (although arguably mostrelevant), and that number of languages, variety in scripts, and variety inlanguage families have only a small impact on performance. We also contributepractical takeaways: training with 1,000 instances per language and a maximuminput length of 100 characters is enough for robust language identification.Based on our findings, we train an accurate (94.41%) multi-domain languageidentification model on 2,034 languages, for which we also provide an analysisof the remaining errors.</abstract>
      <url hash="a5b3a7c4">2025.acl-long.891</url>
      <bibkey>goot-2025-identifying</bibkey>
    </paper>
    <paper id="892">
      <title>The Distracting Effect: Understanding Irrelevant Passages in <fixed-case>RAG</fixed-case></title>
      <author><first>Chen</first><last>Amiraz</last><affiliation>Technology Innovation Institute</affiliation></author>
      <author><first>Florin</first><last>Cuconasu</last><affiliation>University of Roma “La Sapienza”</affiliation></author>
      <author><first>Simone</first><last>Filice</last><affiliation>Technology Innovation Institute</affiliation></author>
      <author><first>Zohar</first><last>Karnin</last><affiliation>tii</affiliation></author>
      <pages>18228-18258</pages>
      <abstract>A well-known issue with Retrieval Augmented Generation (RAG) is that retrieved passages that are irrelevant to the query sometimes distract the answer-generating LLM, causing it to provide an incorrect response. In this paper, we shed light on this core issue and formulate the distracting effect of a passage w.r.t. a query (and an LLM). We provide a quantifiable measure of the distracting effect of a passage and demonstrate its robustness across LLMs. Our research introduces novel methods for identifying and using hard distracting passages to improve RAG systems. By fine-tuning LLMs with these carefully selected distracting passages, we achieve up to a 7.5% increase in answering accuracy compared to counterparts fine-tuned on conventional RAG datasets. Our contribution is two-fold: first, we move beyond the simple binary classification of irrelevant passages as either completely unrelated vs. distracting, and second, we develop and analyze multiple methods for finding hard distracting passages. To our knowledge, no other research has provided such a comprehensive framework for identifying and utilizing hard distracting passages.</abstract>
      <url hash="6d643ed1">2025.acl-long.892</url>
      <bibkey>amiraz-etal-2025-distracting</bibkey>
    </paper>
    <paper id="893">
      <title>Multilingual Encoder Knows more than You Realize: Shared Weights Pretraining for Extremely Low-Resource Languages</title>
      <author><first>Zeli</first><last>Su</last></author>
      <author><first>Ziyin</first><last>Zhang</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Guixian</first><last>Xu</last><affiliation>Key Laboratory of Ethnic Language Intelligence</affiliation></author>
      <author><first>Jianing</first><last>Liu</last></author>
      <author><first>Xu</first><last>Han</last><affiliation>Minzu University of China</affiliation></author>
      <author><first>Ting</first><last>Zhang</last><affiliation>Minzu University of China</affiliation></author>
      <author><first>Yushuang</first><last>Dong</last><affiliation>Minzu University of China</affiliation></author>
      <pages>18259-18270</pages>
      <abstract>While multilingual language models like XLM-R have advanced multilingualism in NLP, they still perform poorly in extremely low-resource languages. This situation is exacerbated by the fact that modern LLMs such as LLaMA and Qwen support far fewer languages than XLM-R, making text generation models non-existent for many languages in the world. To tackle this challenge, we propose a novel framework for adapting multilingual encoders to text generation in extremely low-resource languages. By reusing the weights between the encoder and the decoder, our framework allows the model to leverage the learned semantic space of the encoder, enabling efficient learning and effective generalization in low-resource languages. Applying this framework to four Chinese minority languages, we present XLM-SWCM, and demonstrate its superior performance on various downstream tasks even when compared with much larger models.</abstract>
      <url hash="d6fd509d">2025.acl-long.893</url>
      <bibkey>su-etal-2025-multilingual</bibkey>
    </paper>
    <paper id="894">
      <title>Graphically Speaking: Unmasking Abuse in Social Media with Conversation Insights</title>
      <author><first>Célia</first><last>Nouri</last></author>
      <author><first>Chloé</first><last>Clavel</last><affiliation>INRIA</affiliation></author>
      <author><first>Jean-Philippe</first><last>Cointet</last><affiliation>Institut d’Etudes Politiques de Paris (Sciences Po)</affiliation></author>
      <pages>18271-18286</pages>
      <abstract>Detecting abusive language in social media conversations poses significant challenges, as identifying abusiveness often depends on the conversational context, characterized by the content and topology of preceding comments. Traditional Abusive Language Detection (ALD) models often overlook this context, which can lead to unreliable performance metrics. Recent Natural Language Processing (NLP) approaches that incorporate conversational context often rely on limited or overly simplified representations of this context, leading to inconsistent and sometimes inconclusive results. In this paper, we propose a novel approach that utilizes graph neural networks (GNNs) to model social media conversations as graphs, where nodes represent comments, and edges capture reply structures. We systematically investigate various graph representations and context windows to identify the optimal configurations for ALD. Our GNN model outperforms both context-agnostic baselines and linear context-aware methods, achieving significant improvements in F1 scores. These findings demonstrate the critical role of structured conversational context and establish GNNs as a robust framework for advancing context-aware ALD.</abstract>
      <url hash="b03fffee">2025.acl-long.894</url>
      <bibkey>nouri-etal-2025-graphically</bibkey>
    </paper>
    <paper id="895">
      <title><fixed-case>C</fixed-case>ode<fixed-case>T</fixed-case>ool: Enhancing Programmatic Tool Invocation of <fixed-case>LLM</fixed-case>s via Process Supervision</title>
      <author><first>YifeiLu</first><last>YifeiLu</last></author>
      <author><first>Fanghua</first><last>Ye</last><affiliation>Tencent Hunyuan / AI Lab</affiliation></author>
      <author><first>Jian</first><last>Li</last><affiliation>Tencent</affiliation></author>
      <author><first>Qiang</first><last>Gao</last></author>
      <author><first>Cheng</first><last>Liu</last></author>
      <author><first>Haibo</first><last>Luo</last></author>
      <author><first>Nan</first><last>Du</last><affiliation>Tencent INC</affiliation></author>
      <author><first>Xiaolong</first><last>Li</last><affiliation>Tencent America LLC</affiliation></author>
      <author><first>Feiliang</first><last>Ren</last></author>
      <pages>18287-18304</pages>
      <abstract>Tool invocation significantly enhances the capabilities of Large Language Models (LLMs), yet challenges persist, particularly in complex task scenarios. Current methods, such as instruction-enhanced reasoning and supervised fine-tuning, often result in unnecessarily long reasoning paths and face difficulties in verifying the correctness of intermediate steps. In this paper, we propose CodeTool, a novel framework for stepwise code generation that improves LLM tool invocation by leveraging the concise and easily verifiable nature of code. CodeTool incorporates two distinct process rewards: the On-the-spot Reward, which provides immediate feedback on the accuracy of each tool invocation, and the Latent Reward, which assesses the contribution of each step toward overall task completion. By maximizing the cumulative reward of the On-the-spot and Latend Rewards at each step, LLMs are guided to follow efficient and accurate reasoning paths. Extensive experiments on StableToolBench and RestBench-TMDB demonstrate the superiority of CodeTool over existing approaches.</abstract>
      <url hash="c23c00dd">2025.acl-long.895</url>
      <bibkey>yifeilu-etal-2025-codetool</bibkey>
    </paper>
    <paper id="896">
      <title><fixed-case>RARE</fixed-case>: Retrieval-Augmented Reasoning Enhancement for Large Language Models</title>
      <author><first>Hieu</first><last>Tran</last></author>
      <author><first>Zonghai</first><last>Yao</last><affiliation>University of Massachusetts at Amherst</affiliation></author>
      <author><first>Zhichao</first><last>Yang</last><affiliation>Optum AI</affiliation></author>
      <author><first>Junda</first><last>Wang</last></author>
      <author><first>Yifan</first><last>Zhang</last><affiliation>University of Massachusetts at Lowell</affiliation></author>
      <author><first>Shuo</first><last>Han</last></author>
      <author><first/><last>Feiyun Ouyang</last></author>
      <author><first>Hong</first><last>Yu</last><affiliation>University of Massachusetts at Lowell</affiliation></author>
      <pages>18305-18330</pages>
      <abstract>This work introduces RARE (Retrieval-Augmented Reasoning Enhancement), a versatile extension to the mutual reasoning framework (rStar), aimed at enhancing reasoning accuracy and factual integrity across large language models (LLMs) for complex, knowledge-intensive tasks such as medical and commonsense reasoning. RARE incorporates two innovative actions within the Monte Carlo Tree Search (MCTS) framework: (A6), which generates search queries based on the initial problem statement, performs information retrieval using those queries, and augments reasoning with the retrieved data to formulate the final answer; and (A7), which leverages information retrieval specifically for generated sub-questions and re-answers these sub-questions with the relevant contextual information. Additionally, a Retrieval-Augmented Factuality Scorer is proposed to replace the original discriminator, prioritizing reasoning paths that meet high standards of factuality. Experimental results with LLaMA 3.1 show that RARE enables open-source LLMs to achieve competitive performance with top closed-source models like GPT-4 and GPT-4o. This research establishes RARE as a scalable solution for improving LLMs in domains where logical coherence and factual integrity are critical.</abstract>
      <url hash="63143d5d">2025.acl-long.896</url>
      <bibkey>tran-etal-2025-rare</bibkey>
    </paper>
    <paper id="897">
      <title>Defense Against Prompt Injection Attack by Leveraging Attack Techniques</title>
      <author><first>Yulin</first><last>Chen</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Haoran</first><last>Li</last></author>
      <author><first>Zihao</first><last>Zheng</last></author>
      <author><first>Dekai</first><last>Wu</last><affiliation>The Hong Kong University of Science and Technology and International Computer Science Institute</affiliation></author>
      <author><first>Yangqiu</first><last>Song</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Bryan</first><last>Hooi</last><affiliation>National University of Singapore</affiliation></author>
      <pages>18331-18347</pages>
      <abstract>With the advancement of technology, large language models (LLMs) have achieved remarkable performance across various natural language processing (NLP) tasks, powering LLM-integrated applications like Microsoft Copilot. However, as LLMs continue to evolve, new vulnerabilities, especially prompt injection attacks arise. These attacks trick LLMs into deviating from the original input instructions and executing the attacker’s instructions injected in data content, such as retrieved results. Recent attack methods leverage LLMs’ instruction-following abilities and their inabilities to distinguish instructions injected in the data content, and achieve a high attack success rate (ASR). When comparing the attack and defense methods, we interestingly find that they share similar design goals, of inducing the model to ignore unwanted instructions and instead to execute wanted instructions. Therefore, we raise an intuitive question: *Could these attack techniques be utilized for defensive purposes?* In this paper, we invert the intention of prompt injection methods to develop novel defense methods based on previous training-free attack methods, by repeating the attack process but with the original input instruction rather than the injected instruction. Our comprehensive experiments demonstrate that our defense techniques outperform existing defense approaches, achieving state-of-the-art results.</abstract>
      <url hash="09e18b87">2025.acl-long.897</url>
      <bibkey>chen-etal-2025-defense</bibkey>
    </paper>
    <paper id="898">
      <title>Acquisition and Application of Novel Knowledge in Large Language Models</title>
      <author><first>Ziyu</first><last>Shang</last><affiliation>Southeast University</affiliation></author>
      <author><first>Jianghan</first><last>Liu</last></author>
      <author><first>Zhizhao</first><last>Luo</last><affiliation>Tencent TEG and Beijing Institute of Technology</affiliation></author>
      <author><first>Peng</first><last>Wang</last></author>
      <author><first>Wenjun</first><last>Ke</last><affiliation>Southeast University</affiliation></author>
      <author><first>Jiajun</first><last>Liu</last></author>
      <author><first>Zijie</first><last>Xu</last></author>
      <author><first>Guozheng</first><last>Li</last><affiliation>Southeast University</affiliation></author>
      <pages>18348-18368</pages>
      <abstract>Recent advancements in large language models (LLMs) have demonstrated their impressive generative capabilities, primarily due to their extensive parameterization, which enables them to encode vast knowledge. However, effectively integrating new knowledge into LLMs remains a major challenge. Current research typically first constructs novel knowledge datasets and then injects this knowledge into LLMs through various techniques. However, existing methods for constructing new datasets either rely on timestamps, which lack rigor, or use simple templates for synthesis, which are simplistic and do not accurately reflect the real world. To address this issue, we propose a novel knowledge dataset construction approach that simulates biological evolution using knowledge graphs to generate synthetic entities with diverse attributes, resulting in a dataset, NovelHuman. Systematic analysis on NovelHuman reveals that the intra-sentence position of knowledge significantly affects the acquisition of knowledge. Therefore, we introduce an intra-sentence permutation to enhance knowledge acquisition. Furthermore, given that potential conflicts exist between autoregressive (AR) training objectives and permutation-based learning, we propose PermAR, a permutation-based language modeling framework for AR models. PermAR seamlessly integrates with mainstream AR architectures, endowing them with bidirectional knowledge acquisition capabilities. Extensive experiments demonstrate the superiority of PermAR, outperforming knowledge augmentation methods by 3.3%-38%.</abstract>
      <url hash="84ede131">2025.acl-long.898</url>
      <bibkey>shang-etal-2025-acquisition</bibkey>
    </paper>
    <paper id="899">
      <title><fixed-case>DNCASR</fixed-case>: End-to-End Training for Speaker-Attributed <fixed-case>ASR</fixed-case></title>
      <author><first>Xianrui</first><last>Zheng</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Chao</first><last>Zhang</last><affiliation>Shanghai Artificial Intelligence Laboratory, Tsinghua University and University College London</affiliation></author>
      <author><first>Phil</first><last>Woodland</last><affiliation>University of Cambridge</affiliation></author>
      <pages>18369-18383</pages>
      <abstract>This paper introduces DNCASR, a novel end-to-end trainable system designed for joint neural speaker clustering and automatic speech recognition (ASR), enabling speaker-attributed transcription of long multi-party meetings. DNCASR uses two separate encoders to independently encode global speaker characteristics and local waveform information, along with two linked decoders to generate speaker-attributed transcriptions. The use of linked decoders allows the entire system to be jointly trained under a unified loss function. By employing a serialised training approach, DNCASR effectively addresses overlapping speech in real-world meetings, where the link improves the prediction of speaker indices in overlapping segments. Experiments on the AMI-MDM meeting corpus demonstrate that the jointly trained DNCASR outperforms a parallel system that does not have links between the speaker and ASR decoders. Using cpWER to measure the speaker-attributed word error rate, DNCASR achieves a 9.0% relative reduction on the AMI-MDM Eval set.</abstract>
      <url hash="caaec009">2025.acl-long.899</url>
      <bibkey>zheng-etal-2025-dncasr</bibkey>
    </paper>
    <paper id="900">
      <title>Exploring Persona Sentiment Sensitivity in Personalized Dialogue Generation</title>
      <author><first>Yonghyun</first><last>Jun</last></author>
      <author><first>Hwanhee</first><last>Lee</last><affiliation>Chung-Ang University</affiliation></author>
      <pages>18384-18402</pages>
      <abstract>Personalized dialogue systems have advanced considerably with the integration of user-specific personas into large language models (LLMs). However, while LLMs can effectively generate personalized responses, the influence of persona sentiment on dialogue quality remains underexplored. In this work, we conduct a large-scale analysis of dialogues generated using a range of polarized user profiles. Our experiments reveal that dialogues involving negatively polarized users tend to overemphasize persona attributes. In contrast, positively polarized profiles yield dialogues that selectively incorporate persona information, resulting in smoother interactions. Furthermore, we find that personas with weak or neutral sentiment generally produce lower-quality dialogues. Motivated by these findings, we propose a dialogue generation approach that explicitly accounts for persona polarity by combining a turn-based generation strategy with a profile ordering mechanism and sentiment-aware prompting. Our study provides new insights into the sensitivity of LLMs to persona sentiment and offers guidance for developing more robust and nuanced personalized dialogue systems.</abstract>
      <url hash="fbc18b16">2025.acl-long.900</url>
      <bibkey>jun-lee-2025-exploring</bibkey>
    </paper>
    <paper id="901">
      <title><fixed-case>A</fixed-case>nti<fixed-case>L</fixed-case>eak<fixed-case>B</fixed-case>ench: Preventing Data Contamination by Automatically Constructing Benchmarks with Updated Real-World Knowledge</title>
      <author><first>Xiaobao</first><last>Wu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Liangming</first><last>Pan</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Yuxi</first><last>Xie</last></author>
      <author><first>Ruiwen</first><last>Zhou</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Shuai</first><last>Zhao</last></author>
      <author><first>Yubo</first><last>Ma</last><affiliation>School of Computer Science and Engineering, Nanyang Technological University</affiliation></author>
      <author><first>Mingzhe</first><last>Du</last><affiliation>Nanyang Technological University and National University of Singapore</affiliation></author>
      <author><first>Rui</first><last>Mao</last></author>
      <author><first>Anh Tuan</first><last>Luu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>William Yang</first><last>Wang</last><affiliation>UC Santa Barbara</affiliation></author>
      <pages>18403-18419</pages>
      <abstract>Data contamination hinders fair LLM evaluation by introducing test data into newer models’ training sets. Existing studies solve this challenge by updating benchmarks with newly collected data. However, they fail to guarantee contamination-free evaluation as the newly collected data may contain pre-existing knowledge, and their benchmark updates rely on intensive human labor. To address these issues, we in this paper propose AntiLeak-Bench, an automated anti-leakage benchmarking framework. Instead of simply using newly collected data, we construct samples with explicitly new knowledge absent from LLMs’ training sets, which thus ensures strictly contamination-free evaluation. We further design a fully automated workflow to build and update our benchmark without human labor. This significantly reduces the cost of benchmark maintenance to accommodate emerging LLMs. Through extensive experiments, we highlight that data contamination likely exists before LLMs’ cutoff time and demonstrate that AntiLeak-Bench effectively overcomes this challenge.</abstract>
      <url hash="d5b4c0a7">2025.acl-long.901</url>
      <bibkey>wu-etal-2025-antileakbench</bibkey>
    </paper>
    <paper id="902">
      <title><fixed-case>LLM</fixed-case>-Guided Semantic-Aware Clustering for Topic Modeling</title>
      <author><first>Jianghan</first><last>Liu</last></author>
      <author><first>Ziyu</first><last>Shang</last><affiliation>Southeast University</affiliation></author>
      <author><first>Wenjun</first><last>Ke</last><affiliation>Southeast University</affiliation></author>
      <author><first>Peng</first><last>Wang</last></author>
      <author><first>Zhizhao</first><last>Luo</last><affiliation>Tencent TEG and Beijing Institute of Technology</affiliation></author>
      <author><first>Jiajun</first><last>Liu</last></author>
      <author><first>Guozheng</first><last>Li</last><affiliation>Southeast University</affiliation></author>
      <author><first>Yining</first><last>Li</last><affiliation>Southeast University</affiliation></author>
      <pages>18420-18435</pages>
      <abstract>Topic modeling aims to discover the distribution of topics within a corpus. The advanced comprehension and generative capabilities of large language models (LLMs) have introduced new avenues for topic modeling, particularly by prompting LLMs to generate topics and refine them by merging similar ones. However, this approach necessitates that LLMs generate topics with consistent granularity, thus relying on the exceptional instruction-following capabilities of closed-source LLMs (such as GPT-4) or requiring additional training. Moreover, merging based only on topic words and neglecting the fine-grained semantics within documents might fail to fully uncover the underlying topic structure. In this work, we propose a semi-supervised topic modeling method, LiSA, that combines LLMs with clustering to improve topic generation and distribution. Specifically, we begin with prompting LLMs to generate a candidate topic word for each document, thereby constructing a topic-level semantic space. To further utilize the mutual complementarity between them, we first cluster documents and candidate topic words, and then establish a mapping from document to topic in the LLM-guided assignment stage. Subsequently, we introduce a collaborative enhancement strategy to align the two semantic spaces and establish a better topic distribution. Experimental results demonstrate that LiSA outperforms state-of-the-art methods that utilize GPT-4 on topic alignment, and exhibits competitive performance compared to Neural Topic Models on topic quality. The codes are available at https://github.com/ljh986/LiSA.</abstract>
      <url hash="e712dd32">2025.acl-long.902</url>
      <bibkey>liu-etal-2025-llm-guided</bibkey>
    </paper>
    <paper id="903">
      <title>Hierarchical Bracketing Encodings for Dependency Parsing as Tagging</title>
      <author><first>Ana</first><last>Ezquerro</last></author>
      <author><first>David</first><last>Vilares</last><affiliation>Universidade da Coruña</affiliation></author>
      <author><first>Anssi</first><last>Yli-Jyrä</last></author>
      <author><first>Carlos</first><last>Gómez-Rodríguez</last><affiliation>Universidade da Coruña</affiliation></author>
      <pages>18436-18450</pages>
      <abstract>We present a family of encodings for sequence labeling dependency parsing, based on the concept of hierarchical bracketing. We show that the existing 4-bit projective encoding belongs to this family, but it is suboptimal in the number of labels used to encode a tree. We derive an optimal hierarchical bracketing, which minimizes the number of symbols used and encodes projective trees using only 12 distinct labels (vs. 16 for the 4-bit encoding). We also extend optimal hierarchical bracketing to support arbitrary non-projectivity in a more compact way than previous encodings. Our new encodings yield competitive accuracy on a diverse set of treebanks.</abstract>
      <url hash="13628da0">2025.acl-long.903</url>
      <bibkey>ezquerro-etal-2025-hierarchical</bibkey>
    </paper>
    <paper id="904">
      <title><fixed-case>OASIS</fixed-case>: Order-Augmented Strategy for Improved Code Search</title>
      <author><first>Gao</first><last>Zuchen</last></author>
      <author><first>Zizheng</first><last>Zhan</last><affiliation>Kuaishou- 快手科技</affiliation></author>
      <author><first>Xianming</first><last>Li</last></author>
      <author><first>Erxin</first><last>Yu</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Haotian</first><last>Zhang</last><affiliation>Kuaishou- 快手科技</affiliation></author>
      <author><first>Chenbin</first><last>Chenbin</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Yuqun</first><last>Zhang</last></author>
      <author><first>Jing</first><last>Li</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <pages>18451-18467</pages>
      <abstract>Code embeddings capture the semantic representations of code and are crucial for various code-related large language model (LLM) applications, such as code search. Previous training primarily relies on optimizing the InfoNCE loss by comparing positive natural language (NL)-code pairs with in-batch negatives. However, due to the sparse nature of code contexts, training solely by comparing the major differences between positive and negative pairs may fail to capture deeper semantic nuances. To address this issue, we propose a novel order-augmented strategy for improved code search (OASIS). It leverages order-based similarity labels to train models to capture subtle differences in similarity among negative pairs. Extensive benchmark evaluations demonstrate that our OASIS model significantly outperforms previous state-of-the-art models focusing solely on major positive-negative differences. It underscores the value of exploiting subtle differences among negative pairs with order labels for effective code embedding training.</abstract>
      <url hash="b644b678">2025.acl-long.904</url>
      <bibkey>zuchen-etal-2025-oasis</bibkey>
    </paper>
    <paper id="905">
      <title>Can Large Language Models Detect Errors in Long Chain-of-Thought Reasoning?</title>
      <author><first>Yancheng</first><last>He</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Shilong</first><last>Li</last></author>
      <author><first>Jiaheng</first><last>Liu</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Weixun</first><last>Wang</last></author>
      <author><first>Xingyuan</first><last>Bu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Ge</first><last>Zhang</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Z.y.</first><last>Peng</last></author>
      <author><first>Zhaoxiang</first><last>Zhang</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Zhicheng</first><last>Zheng</last><affiliation>Princeton University</affiliation></author>
      <author><first>Wenbo</first><last>Su</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Bo</first><last>Zheng</last><affiliation>Alibaba Group</affiliation></author>
      <pages>18468-18489</pages>
      <abstract>Recently, o1-like models have drawn significant attention, where these models produce the long Chain-of-Thought (CoT) reasoning steps to improve the reasoning abilities of existing Large Language Models (LLMs). In this paper, to understand the qualities of these long CoTs and measure the critique abilities of existing LLMs on these long CoTs, we introduce the DeltaBench including the generated long CoTs from different o1-like models (e.g., QwQ, DeepSeek-R1) for different reasoning tasks (e.g., Math, Code, General Reasoning), to measure the ability to detect errors in long COT reasoning. Based on DeltaBench, we first perform fine-grained analysis of the generated long CoTs to discover the effectiveness and efficiency of different o1-like models. Then, we conduct extensive evaluations of existing process reward models (PRMs) and critic models to detect the errors of each annotated process, which aims to investigate the boundaries and limitations of existing PRMs and critic models. Finally, we hope that DeltaBench could guide developers to better understand the long CoT reasoning abilities of their models.</abstract>
      <url hash="87b6aef2">2025.acl-long.905</url>
      <bibkey>he-etal-2025-large</bibkey>
    </paper>
    <paper id="906">
      <title><fixed-case>O</fixed-case>mni<fixed-case>A</fixed-case>lign-<fixed-case>V</fixed-case>: Towards Enhanced Alignment of <fixed-case>MLLM</fixed-case>s with Human Preference</title>
      <author><first>Xiangyu</first><last>Zhao</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Shengyuan</first><last>Ding</last></author>
      <author><first>Zicheng</first><last>Zhang</last></author>
      <author><first>Haian</first><last>Huang</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Maosongcao</first><last>Maosongcao</last></author>
      <author><first>Jiaqi</first><last>Wang</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Weiyun</first><last>Wang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xinyu</first><last>Fang</last></author>
      <author><first>Wenhai</first><last>Wang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Guangtao</first><last>Zhai</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Hua</first><last>Yang</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Haodong</first><last>Duan</last></author>
      <author><first>Kai</first><last>Chen</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <pages>18490-18515</pages>
      <abstract>Recent advancements in open-source multi-modal large language models (MLLMs) have primarily focused on enhancing foundational capabilities, leaving a significant gap in human preference alignment. This paper introduces OmniAlign-V, a comprehensive dataset of 200K high-quality training samples featuring diverse images, complex questions, and varied response formats to improve MLLMs’ alignment with human preferences. We also present MM-AlignBench, a human-annotated benchmark specifically designed to evaluate MLLMs’ alignment with human values. Experimental results show that finetuning MLLMs with OmniAlign-V, using Supervised Fine-Tuning (SFT) or Direct Preference Optimization (DPO), significantly enhances human preference alignment while maintaining or enhancing performance on standard VQA benchmarks, preserving their fundamental capabilities.</abstract>
      <url hash="079f491f">2025.acl-long.906</url>
      <bibkey>zhao-etal-2025-omnialign</bibkey>
    </paper>
    <paper id="907">
      <title>Tree-<fixed-case>KG</fixed-case>: An Expandable Knowledge Graph Construction Framework for Knowledge-intensive Domains</title>
      <author><first>Songjie</first><last>Niu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Kaisen</first><last>Yang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Rui</first><last>Zhao</last></author>
      <author><first>Yichao</first><last>Liu</last></author>
      <author><first>Zonglin</first><last>Li</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Hongning</first><last>Wang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Wenguang</first><last>Chen</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>18516-18529</pages>
      <abstract>In knowledge-intensive domains like scientific research, effective decisions rely on organizing and retrieving intricate data. Knowledge graphs (KGs) help by structuring entities, relations, and contextual dependencies, but building KGs in such domains is challenging due to inherent complexity, manual effort, and rapid evolution. Inspired by how humans organize knowledge hierarchically, we propose Tree-KG, an expandable framework that combines structured domain texts with advanced semantic techniques. First, Tree-KG builds a tree-like graph from textbook structures using large language models (LLMs) and domain-specific entities, creating an <i>explicit KG</i>. Then, through iterative expansion with flexible, predefined operators, it uncovers <i>hidden KG</i> while preserving semantic coherence. Experiments demonstrate that Tree-KG consistently surpasses competing methods, achieving the highest F1 scores (12–16% above the second-best), with notable performance (F1 0.81) on the Text-Annotated dataset, highlighting its effectiveness in extracting high-quality information from source texts. Additionally, Tree-KG provides superior structural alignment, domain-specific extraction, and cost-efficiency, delivering robust results with reduced token usage and adaptable, resource-conscious deployment.</abstract>
      <url hash="a04ed148">2025.acl-long.907</url>
      <bibkey>niu-etal-2025-tree</bibkey>
    </paper>
    <paper id="908">
      <title>Measuring Data Diversity for Instruction Tuning: A Systematic Analysis and A Reliable Metric</title>
      <author><first>Yuming</first><last>Yang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Yang</first><last>Nan</last></author>
      <author><first>Junjie</first><last>Ye</last></author>
      <author><first>Shihan</first><last>Dou</last></author>
      <author><first>Xiao</first><last>Wang</last></author>
      <author><first>Shuo</first><last>Li</last><affiliation>Fudan University</affiliation></author>
      <author><first>Huijie</first><last>Lv</last></author>
      <author><first>Tao</first><last>Gui</last><affiliation>Fudan University</affiliation></author>
      <author><first>Qi</first><last>Zhang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xuanjing</first><last>Huang</last><affiliation>Fudan University</affiliation></author>
      <pages>18530-18549</pages>
      <abstract>Data diversity is crucial for the instruction tuning of large language models. Existing studies have explored various diversity-aware data selection methods to construct high-quality datasets and enhance model performance. However, the fundamental problem of precisely defining and measuring data diversity remains underexplored, limiting clear guidance for data engineering. To address this, we systematically analyze 11 existing diversity measurement methods by evaluating their correlation with model performance through extensive fine-tuning experiments. Our results indicate that a reliable diversity measure should properly account for both inter-sample differences and the information density in the sample space. Building on this, we propose NovelSum, a new diversity metric based on sample-level “novelty.” Experiments on both simulated and real-world data show that NovelSum accurately captures diversity variations and achieves a 0.97 correlation with instruction-tuned model performance, highlighting its value in guiding data engineering practices. With NovelSum as an optimization objective, we further develop a greedy, diversity-oriented data selection strategy that outperforms existing approaches, validating both the effectiveness and practical significance of our metric.</abstract>
      <url hash="f8958a6a">2025.acl-long.908</url>
      <bibkey>yang-etal-2025-measuring</bibkey>
    </paper>
    <paper id="909">
      <title>Micro-Act: Mitigate Knowledge Conflict in Question Answering via Actionable Self-Reasoning</title>
      <author><first>Nan</first><last>Huo</last></author>
      <author><first>Jinyang</first><last>Li</last></author>
      <author><first>Bowen</first><last>Qin</last><affiliation>Beijing Academy of Artificial Intelligence</affiliation></author>
      <author><first>Ge</first><last>Qu</last></author>
      <author><first>Xiaolong</first><last>Li</last></author>
      <author><first>Xiaodong</first><last>Li</last></author>
      <author><first>Chenhao</first><last>Ma</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <author><first>Reynold</first><last>Cheng</last></author>
      <pages>18550-18574</pages>
      <abstract>Retrieval-Augmented Generation (RAG) systems commonly suffer from **Knowledge Conflicts**, where retrieved external knowledge contradicts the inherent, parametric knowledge of large language models (LLMs). It adversely affects performance on downstream tasks such as question answering (QA). Existing approaches often attempt to mitigate conflicts by directly comparing two knowledge sources in a side-by-side manner, but this can overwhelm LLMs with extraneous or lengthy contexts, ultimately hindering their ability to identify and mitigate inconsistencies. To address this issue, we propose **Micro-Act** a framework with a hierarchical action space that automatically perceives context complexity and adaptively decomposes each knowledge source into a sequence of fine-grained comparisons. These comparisons are represented as actionable steps, enabling reasoning beyond the superficial context. Through extensive experiments on five benchmark datasets, Micro-Act consistently achieves significant increase in QA accuracy over state-of-the-art baselines across all 5 datasets and 3 conflict types, especially in temporal and semantic types where all baselines fail significantly. More importantly, Micro-Act exhibits robust performance on non-conflict questions simultaneously, highlighting its practical value in real-world RAG applications.</abstract>
      <url hash="e984ed37">2025.acl-long.909</url>
      <bibkey>huo-etal-2025-micro</bibkey>
    </paper>
    <paper id="910">
      <title>Minimal Pair-Based Evaluation of Code-Switching</title>
      <author><first>Igor</first><last>Sterner</last></author>
      <author><first>Simone</first><last>Teufel</last><affiliation>Department of Computer Science and Technology (Formerly Computer Laboratory)</affiliation></author>
      <pages>18575-18598</pages>
      <abstract>There is a lack of an evaluation methodology that estimates the extent to which large language models (LLMs) use code-switching (CS) in the same way as bilinguals. Existing methods do not have wide language coverage, fail to account for the diverse range of CS phenomena, or do not scale. We propose an intervention based on minimal pairs of CS. Each minimal pair contains one naturally occurring CS sentence and one minimally manipulated variant. We collect up to 1,000 such pairs each for 11 language pairs. Our human experiments show that, for every language pair, bilinguals consistently prefer the naturally occurring CS sentence. Meanwhile our experiments with current LLMs show that the larger the model, the more consistently it assigns higher probability to the naturally occurring CS sentence than to the variant. In accordance with theoretical claims, the largest probability differences arise in those pairs where the manipulated material consisted of closed-class words.</abstract>
      <url hash="84af1217">2025.acl-long.910</url>
      <bibkey>sterner-teufel-2025-minimal</bibkey>
    </paper>
    <paper id="911">
      <title><fixed-case>DNAS</fixed-case>peech: A Contextualized and Situated Text-to-Speech Dataset with Dialogues, Narratives and Actions</title>
      <author><first>Chuanqi</first><last>Cheng</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Hongda</first><last>Sun</last></author>
      <author><first>Bo</first><last>Du</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Shuo</first><last>Shang</last></author>
      <author><first>Xinrong</first><last>Hu</last></author>
      <author><first>Rui</first><last>Yan</last><affiliation>Renmin University of China</affiliation></author>
      <pages>18599-18616</pages>
      <abstract>In this paper, we propose contextualized and situated text-to-speech (CS-TTS), a novel TTS task to promote more accurate and customized speech generation using prompts with Dialogues, Narratives, and Actions (DNA). While prompt-based TTS methods facilitate controllable speech generation, existing TTS datasets lack situated descriptive prompts aligned with speech data. To address this data scarcity, we develop an automatic annotation pipeline enabling multifaceted alignment among speech clips, content text, and their respective descriptions. Based on this pipeline, we present DNASpeech, a novel CS-TTS dataset with high-quality speeches with DNA prompt annotations. DNASpeech contains 2,395 distinct characters, 4,452 scenes, and 22,975 dialogue utterances, along with over 18 hours of high-quality speech recordings. To accommodate more specific task scenarios, we establish a leaderboard featuring two new subtasks for evaluation: CS-TTS with narratives and CS-TTS with dialogues. We also design an intuitive baseline model for comparison with existing state-of-the-art TTS methods on our leaderboard. Comprehensive experimental results demonstrate the quality and effectiveness of DNASpeech, validating its potential to drive advancements in the TTS field.</abstract>
      <url hash="e562fff1">2025.acl-long.911</url>
      <bibkey>cheng-etal-2025-dnaspeech</bibkey>
    </paper>
    <paper id="912">
      <title><fixed-case>LL</fixed-case>a<fixed-case>MA</fixed-case>-Omni 2: <fixed-case>LLM</fixed-case>-based Real-time Spoken Chatbot with Autoregressive Streaming Speech Synthesis</title>
      <author><first>Qingkai</first><last>Fang</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yan</first><last>Zhou</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Shoutao</first><last>Guo</last></author>
      <author><first>Shaolei</first><last>Zhang</last></author>
      <author><first>Yang</first><last>Feng</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <pages>18617-18629</pages>
      <abstract>Real-time, intelligent, and natural speech interaction is an essential part of the next-generation human-computer interaction. Recent advancements have showcased the potential of building intelligent spoken chatbots based on large language models (LLMs). In this paper, we introduce LLaMA-Omni 2, a series of speech language models (SpeechLMs) ranging from 0.5B to 14B parameters, capable of achieving high-quality real-time speech interaction. LLaMA-Omni 2 is built upon the Qwen2.5 series models, integrating a speech encoder and an autoregressive streaming speech decoder. Despite being trained on only 200K multi-turn speech dialogue samples, LLaMA-Omni 2 demonstrates strong performance on several spoken question answering and speech instruction following benchmarks, surpassing previous state-of-the-art SpeechLMs like GLM-4-Voice, which was trained on millions of hours of speech data.</abstract>
      <url hash="5693920d">2025.acl-long.912</url>
      <bibkey>fang-etal-2025-llama</bibkey>
    </paper>
    <paper id="913">
      <title>Error Comparison Optimization for Large Language Models on Aspect-Based Sentiment Analysis</title>
      <author><first>Qianlong</first><last>Wang</last></author>
      <author><first>Keyang</first><last>Ding</last></author>
      <author><first>Hengxin</first><last>Gao</last></author>
      <author><first>Hui</first><last>Wang</last></author>
      <author><first>Ruifeng</first><last>Xu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>18630-18646</pages>
      <abstract>Supervised fine-tuning (SFT) has enabled large language models (LLMs) to exhibit promising performance on various tasks. However, this fine-tuning process only compares current predictions and labels on each sample, yet fails to perceive and understand its error outputs from different degrees, which may potentially produce a large percentage of serious errors. This poses a problem for aspect-based sentiment analysis (ABSA) in that these serious errors bring a greater negative impact than acceptable ones. Humans tend to compare mistakes to understand the varying degrees of mistakes, thus avoiding major bad decisions. Inspired by this, we propose a simple yet effective framework that could perceive and understand the degree of different errors by learning from comparative error pairs. It utilizes the SFT model to yield multiple outputs on each sample and selects acceptable and severe errors based on the acceptable scores. Together with the labels, we construct two comparative error pairs and exploit their calibration losses to optimize parameters. We conduct comprehensive experiments on ABSA datasets to demonstrate the effectiveness of our framework over baselines.</abstract>
      <url hash="48271bcb">2025.acl-long.913</url>
      <bibkey>wang-etal-2025-error</bibkey>
    </paper>
    <paper id="914">
      <title>The <fixed-case>AI</fixed-case> Gap: How Socioeconomic Status Affects Language Technology Interactions</title>
      <author><first>Elisa</first><last>Bassignana</last><affiliation>IT University of Copenhagen</affiliation></author>
      <author><first>Amanda Cercas</first><last>Curry</last><affiliation>CENTAI Institute</affiliation></author>
      <author><first>Dirk</first><last>Hovy</last><affiliation>Bocconi University</affiliation></author>
      <pages>18647-18664</pages>
      <abstract>Socioeconomic status (SES) fundamentally influences how people interact with each other and, more recently, with digital technologies like large language models (LLMs). While previous research has highlighted the interaction between SES and language technology, it was limited by reliance on proxy metrics and synthetic data. We survey 1,000 individuals from ‘diverse socioeconomic backgrounds’ about their use of language technologies and generative AI, and collect 6,482 prompts from their previous interactions with LLMs. We find systematic differences across SES groups in language technology usage (i.e., frequency, performed tasks), interaction styles, and topics. Higher SES entail a higher level of abstraction, convey requests more concisely, and topics like ‘inclusivity’ and ‘travel’. Lower SES correlates with higher anthropomorphization of LLMs (using ”hello” and ”thank you”) and more concrete language. Our findings suggest that while generative language technologies are becoming more accessible to everyone, socioeconomic linguistic differences still stratify their use to create a digital divide. These differences underscore the importance of considering SES in developing language technologies to accommodate varying linguistic needs rooted in socioeconomic factors and limit the AI Gap across SES groups.</abstract>
      <url hash="1b5324df">2025.acl-long.914</url>
      <bibkey>bassignana-etal-2025-ai</bibkey>
    </paper>
    <paper id="915">
      <title>Probing <fixed-case>LLM</fixed-case>s for Multilingual Discourse Generalization Through a Unified Label Set</title>
      <author><first>Florian</first><last>Eichin</last></author>
      <author><first>Yang Janet</first><last>Liu</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Barbara</first><last>Plank</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Michael A.</first><last>Hedderich</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <pages>18665-18684</pages>
      <abstract>Discourse understanding is essential for many NLP tasks, yet most existing work remains constrained by framework-dependent discourse representations. This work investigates whether large language models (LLMs) capture discourse knowledge that generalizes across languages and frameworks. We address this question along two dimensions: (1) developing a unified discourse relation label set to facilitate cross-lingual and cross-framework discourse analysis, and (2) probing LLMs to assess whether they encode generalizable discourse abstractions. Using multilingual discourse relation classification as a testbed, we examine a comprehensive set of 23 LLMs of varying sizes and multilingual capabilities. Our results show that LLMs, especially those with multilingual training corpora, can generalize discourse information across languages and frameworks. Further layer-wise analyses reveal that language generalization at the discourse level is most salient in the intermediate layers. Lastly, our error analysis provides an account of challenging relation classes.</abstract>
      <url hash="590fdd18">2025.acl-long.915</url>
      <bibkey>eichin-etal-2025-probing</bibkey>
    </paper>
    <paper id="916">
      <title>Crowdsource, Crawl, or Generate? Creating <fixed-case>SEA</fixed-case>-<fixed-case>VL</fixed-case>, a Multicultural Vision-Language Dataset for <fixed-case>S</fixed-case>outheast <fixed-case>A</fixed-case>sia</title>
      <author><first>Samuel</first><last>Cahyawijaya</last><affiliation>Cohere</affiliation></author>
      <author><first>Holy</first><last>Lovenia</last><affiliation>SEACrowd</affiliation></author>
      <author><first>Joel Ruben Antony</first><last>Moniz</last><affiliation>DoorDash</affiliation></author>
      <author><first>Tack Hwa</first><last>Wong</last><affiliation>Independent Researcher</affiliation></author>
      <author><first>Mohammad Rifqi</first><last>Farhansyah</last></author>
      <author><first>Thant Thiri</first><last>Maung</last></author>
      <author><first>Frederikus</first><last>Hudi</last></author>
      <author><first>David</first><last>Anugraha</last><affiliation>Stanford University</affiliation></author>
      <author><first>Muhammad Ravi Shulthan</first><last>Habibi</last><affiliation>Universitas Indonesia</affiliation></author>
      <author><first>Muhammad Reza</first><last>Qorib</last></author>
      <author><first>Amit</first><last>Agarwal</last><affiliation>Oracle</affiliation></author>
      <author><first>Joseph Marvin</first><last>Imperial</last><affiliation>University of Bath</affiliation></author>
      <author><first>Hitesh Laxmichand</first><last>Patel</last><affiliation>Oracle</affiliation></author>
      <author><first>Vicky</first><last>Feliren</last></author>
      <author><first>Bahrul Ilmi</first><last>Nasution</last></author>
      <author><first>Manuel Antonio</first><last>Rufino</last><affiliation>Samsung</affiliation></author>
      <author><first>Genta Indra</first><last>Winata</last><affiliation>Capital One</affiliation></author>
      <author><first>Rian Adam</first><last>Rajagede</last><affiliation>University of Central Florida and Universitas Islam Indonesia</affiliation></author>
      <author><first>Carlos Rafael</first><last>Catalan</last><affiliation>Samsung Research</affiliation></author>
      <author><first>Mohamed Fazli Mohamed</first><last>Imam</last></author>
      <author><first>Priyaranjan</first><last>Pattnayak</last><affiliation>Oracle</affiliation></author>
      <author><first>Salsabila Zahirah</first><last>Pranida</last></author>
      <author><first>Kevin</first><last>Pratama</last><affiliation>Meta</affiliation></author>
      <author><first>Yeshil</first><last>Bangera</last></author>
      <author><first>Adisai</first><last>Na-Thalang</last><affiliation>SCB 10X</affiliation></author>
      <author><first>Patricia Nicole</first><last>Monderin</last><affiliation>Samsung</affiliation></author>
      <author><first>Yueqi</first><last>Song</last></author>
      <author><first>Christian</first><last>Simon</last><affiliation>Sony</affiliation></author>
      <author><first>Lynnette Hui Xian</first><last>Ng</last></author>
      <author><first>Richardy Lobo</first><last>Sapan</last></author>
      <author><first>Taki Hasan</first><last>Rafi</last><affiliation>Hanyang University</affiliation></author>
      <author><first>Bin</first><last>Wang</last></author>
      <author><first/><last>Supryadi</last><affiliation>Tianjin University</affiliation></author>
      <author><first>Kanyakorn</first><last>Veerakanjana</last></author>
      <author><first>Piyalitt</first><last>Ittichaiwong</last></author>
      <author><first>Matthew Theodore</first><last>Roque</last><affiliation>Samsung</affiliation></author>
      <author><first>Karissa</first><last>Vincentio</last><affiliation>Binus University</affiliation></author>
      <author><first>Takdanai</first><last>Kreangphet</last></author>
      <author><first>Phakphum</first><last>Artkaew</last></author>
      <author><first>Kadek Hendrawan</first><last>Palgunadi</last><affiliation>Institut Teknologi Sepuluh Nopember</affiliation></author>
      <author><first>Yanzhi</first><last>Yu</last></author>
      <author><first>Rochana Prih</first><last>Hastuti</last><affiliation>University of Central Florida and Universitas Gadjah Mada</affiliation></author>
      <author><first>William</first><last>Nixon</last></author>
      <author><first>Mithil</first><last>Bangera</last></author>
      <author><first>Adrian Xuan Wei</first><last>Lim</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Aye Hninn</first><last>Khine</last><affiliation>King Mongkut’s University of Technology Thonburi</affiliation></author>
      <author><first>Hanif Muhammad</first><last>Zhafran</last><affiliation>Institut Teknologi Bandung</affiliation></author>
      <author><first>Teddy</first><last>Ferdinan</last><affiliation>Technical University of Wroclaw</affiliation></author>
      <author><first>Audra Aurora</first><last>Izzani</last></author>
      <author><first>Ayushman</first><last>Singh</last><affiliation>Capital One</affiliation></author>
      <author><first>Evan</first><last>Evan</last><affiliation>Institut Teknologi Sepuluh Nopember</affiliation></author>
      <author><first>Jauza Akbar</first><last>Krito</last></author>
      <author><first>Michael</first><last>Anugraha</last></author>
      <author><first>Fenal Ashokbhai</first><last>Ilasariya</last><affiliation>Independent</affiliation></author>
      <author><first>Haochen</first><last>Li</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>John Amadeo</first><last>Daniswara</last><affiliation>Modelcode.ai</affiliation></author>
      <author><first>Filbert Aurelian</first><last>Tjiaranata</last></author>
      <author><first>Eryawan Presma</first><last>Yulianrifat</last></author>
      <author><first>Can</first><last>Udomcharoenchaikit</last><affiliation>Vidyasirimedhi Institute of Science and Technology</affiliation></author>
      <author><first>Fadil Risdian</first><last>Ansori</last><affiliation>Insignia</affiliation></author>
      <author><first>Mahardika Krisna</first><last>Ihsani</last></author>
      <author><first>Giang</first><last>Nguyen</last></author>
      <author><first>Anab Maulana</first><last>Barik</last></author>
      <author><first>Dan John</first><last>Velasco</last><affiliation>Samsung</affiliation></author>
      <author><first>Rifo Ahmad</first><last>Genadi</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Saptarshi</first><last>Saha</last><affiliation>Indian Statistical Institute</affiliation></author>
      <author><first>Chengwei</first><last>Wei</last><affiliation>, A*STAR</affiliation></author>
      <author><first>Isaiah Edri W.</first><last>Flores</last></author>
      <author><first>Kenneth Chen Ko</first><last>Han</last><affiliation>Singapore Polytechnic</affiliation></author>
      <author><first>Anjela Gail D.</first><last>Santos</last><affiliation>University of the Philippines</affiliation></author>
      <author><first>Wan Shen</first><last>Lim</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Kaung Si</first><last>Phyo</last></author>
      <author><first>Tim</first><last>Santos</last></author>
      <author><first>Meisyarah</first><last>Dwiastuti</last><affiliation>Dataxet:Sonar</affiliation></author>
      <author><first>Jiayun</first><last>Luo</last><affiliation>, University of British Columbia</affiliation></author>
      <author><first>Jan Christian Blaise</first><last>Cruz</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Ming Shan</first><last>Hee</last></author>
      <author><first>Ikhlasul Akmal</first><last>Hanif</last></author>
      <author><first>M.Alif Al</first><last>Hakim</last><affiliation>Universitas Indonesia</affiliation></author>
      <author><first>Muhammad Rizky</first><last>Sya’ban</last></author>
      <author><first>Kun</first><last>Kerdthaisong</last></author>
      <author><first>Lester James Validad</first><last>Miranda</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Fajri</first><last>Koto</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Tirana Noor</first><last>Fatyanosa</last><affiliation>Universitas Brawijaya</affiliation></author>
      <author><first>Alham Fikri</first><last>Aji</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Jostin Jerico</first><last>Rosal</last></author>
      <author><first>Jun</first><last>Kevin</last><affiliation>Pelita Harapan University</affiliation></author>
      <author><first>Robert</first><last>Wijaya</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Onno P.</first><last>Kampman</last></author>
      <author><first>Ruochen</first><last>Zhang</last><affiliation>Brown University</affiliation></author>
      <author><first>Börje F.</first><last>Karlsson</last><affiliation>Beijing Academy of Artificial Intelligence (BAAI)</affiliation></author>
      <author><first>Peerat</first><last>Limkonchotiwat</last><affiliation>AI Singapore</affiliation></author>
      <pages>18685-18717</pages>
      <abstract>Despite Southeast Asia’s (SEA) extraordinary linguistic and cultural diversity, the region remains significantly underrepresented in vision-language (VL) research, resulting in AI models that inadequately capture SEA cultural nuances. To fill this gap, we present SEA-VL, an open-source initiative dedicated to developing culturally relevant high-quality datasets for SEA languages. By involving contributors from SEA countries, SEA-VL ensures better cultural relevance and diversity, fostering greater inclusivity of underrepresented languages and cultural depictions in VL research. Our methodology employed three approaches: community-driven crowdsourcing with SEA contributors, automated image crawling, and synthetic image generation. We evaluated each method’s effectiveness in capturing cultural relevance. We found that image crawling achieves approximately ~85% cultural relevance while being more cost- and time-efficient than crowdsourcing, whereas synthetic image generation failed to accurately reflect SEA cultural nuances and contexts. Collectively, we gathered 1.28 million SEA culturally relevant images, more than 50 times larger than other existing datasets. This work bridges the representation gap in SEA, establishes a foundation for developing culturally aware AI systems for this region, and provides a replicable framework for addressing representation gaps in other underrepresented regions.</abstract>
      <url hash="eecb0ee4">2025.acl-long.916</url>
      <bibkey>cahyawijaya-etal-2025-crowdsource</bibkey>
    </paper>
    <paper id="917">
      <title>Soundwave: Less is More for Speech-Text Alignment in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Yuhao</first><last>Zhang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Zhiheng</first><last>Liu</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Fan</first><last>Bu</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <author><first>Ruiyu</first><last>Zhang</last></author>
      <author><first>Benyou</first><last>Wang</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <author><first>Haizhou</first><last>Li</last><affiliation>The Chinese University of Hong Kong (Shenzhen); National University of Singapore and National University of Singapore</affiliation></author>
      <pages>18718-18738</pages>
      <abstract>Existing end-to-end speech large language models (LLMs) usually rely on large-scale annotated data for training, while data-efficient training has not been discussed in depth. We focus on two fundamental problems between speech and text: the representation space gap and sequence length inconsistency. We propose Soundwave, which utilizes an efficient training strategy and a novel architecture to address these issues. Results show that Soundwave outperforms other advanced speech LLMs in speech translation and AIR-Bench speech tasks with only a fraction of the training data. Further analysis shows that Soundwave still retains its intelligence during conversation.</abstract>
      <url hash="ca4c1c3a">2025.acl-long.917</url>
      <bibkey>zhang-etal-2025-soundwave</bibkey>
    </paper>
    <paper id="918">
      <title><fixed-case>R</fixed-case>o<fixed-case>T</fixed-case>o<fixed-case>R</fixed-case>: Towards More Reliable Responses for Order-Invariant Inputs</title>
      <author><first>Soyoung</first><last>Yoon</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Dongha</first><last>Ahn</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Youngwon</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Minkyu</first><last>Jung</last><affiliation>Seoul National University</affiliation></author>
      <author><first>HyungJoo</first><last>Jang</last><affiliation>Hongik University</affiliation></author>
      <author><first>Seung-won</first><last>Hwang</last><affiliation>Seoul National University</affiliation></author>
      <pages>18739-18760</pages>
      <abstract>Mitigating positional bias of language models (LMs) for listwise inputs is a well-known and important problem (e.g., lost-in-the-middle). While zero-shot order-invariant LMs have been proposed to solve this issue, their success on practical listwise problems has been limited. In this work, as a first contribution, we identify and overcome two limitations to make zero-shot invariant LMs more practical: (1) training and inference distribution mismatch arising from modifying positional ID assignments to enforce invariance, and (2) failure to adapt to mixture of order-invariant and sensitive inputs in practical listwise problems. Then, to overcome these issues we propose (1) RoToR, a zero-shot invariant LM for genuinely order-invariant inputs with minimal modifications of positional IDs, and (2) Selective Routing, an adaptive framework that handles both order-invariant and order-sensitive inputs in listwise tasks. On the Lost in the middle (LitM), Knowledge Graph QA (KGQA), and MMLU benchmarks, we show that RoToR with Selective Routing can effectively handle practical listwise input tasks in a zero-shot manner (https://github.com/soyoung97/RoToR)</abstract>
      <url hash="5ead7279">2025.acl-long.918</url>
      <bibkey>yoon-etal-2025-rotor</bibkey>
    </paper>
    <paper id="919">
      <title>Global <fixed-case>MMLU</fixed-case>: Understanding and Addressing Cultural and Linguistic Biases in Multilingual Evaluation</title>
      <author><first>Shivalika</first><last>Singh</last><affiliation>Cohere Labs</affiliation></author>
      <author><first>Angelika</first><last>Romanou</last></author>
      <author><first>Clémentine</first><last>Fourrier</last><affiliation>HuggingFace</affiliation></author>
      <author><first>David Ifeoluwa</first><last>Adelani</last><affiliation>McGill University</affiliation></author>
      <author><first>Jian Gang</first><last>Ngui</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Daniel</first><last>Vila-Suero</last><affiliation>Hugging Face</affiliation></author>
      <author><first>Peerat</first><last>Limkonchotiwat</last><affiliation>AI Singapore</affiliation></author>
      <author><first>Kelly</first><last>Marchisio</last><affiliation>Cohere and Cohere</affiliation></author>
      <author><first>Wei Qi</first><last>Leong</last><affiliation>AI Singapore</affiliation></author>
      <author><first>Yosephine</first><last>Susanto</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Raymond</first><last>Ng</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Shayne</first><last>Longpre</last></author>
      <author><first>Sebastian</first><last>Ruder</last><affiliation>Facebook</affiliation></author>
      <author><first>Wei-Yin</first><last>Ko</last></author>
      <author><first>Antoine</first><last>Bosselut</last><affiliation>Swiss Federal Institute of Technology Lausanne</affiliation></author>
      <author><first>Alice</first><last>Oh</last><affiliation>Google and Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Andre</first><last>Martins</last><affiliation>Instituto Superior Técnico and Unbabel</affiliation></author>
      <author><first>Leshem</first><last>Choshen</last><affiliation>Massachusetts Institute of Technology and International Business Machines</affiliation></author>
      <author><first>Daphne</first><last>Ippolito</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Enzo</first><last>Ferrante</last><affiliation>CONICET / Universidad de Buenos Aires</affiliation></author>
      <author><first>Marzieh</first><last>Fadaee</last><affiliation>Cohere Labs</affiliation></author>
      <author><first>Beyza</first><last>Ermis</last><affiliation>Cohere AI</affiliation></author>
      <author><first>Sara</first><last>Hooker</last><affiliation>Cohere For AI</affiliation></author>
      <pages>18761-18799</pages>
      <abstract>Reliable multilingual evaluation is difficult, and culturally appropriate evaluation is even harder to achieve.A common practice to fill this gap is to machine-translate English evaluation sets. However, translation introduces language bias and carries over cultural and regional assumptions from the original questions – often testing knowledge irrelevant to the target audience. In this work, we highlight the extent and impact of these biases and present a multilingual evaluation framework that aims to mitigate them through improved translations and annotation practices.Through a large-scale study involving professional and community translators and annotators, we show that state-of-the-art models excel primarily by learning Western-centric concepts. Notably, we find that model rankings on the full MMLU change when evaluated on a subset of questions explicitly marked as culturally sensitive.We release Global MMLU, a multilingual extension of MMLU across 42 languages, featuring improved translation quality, expanded language coverage, and designated subsets labeled as culturally sensitive and culturally agnostic to enable a more comprehensive and equitable benchmark for evaluating language models across diverse linguistic and cultural contexts.</abstract>
      <url hash="b49d31ae">2025.acl-long.919</url>
      <bibkey>singh-etal-2025-global</bibkey>
    </paper>
    <paper id="920">
      <title>Improving Dialogue Discourse Parsing through Discourse-aware Utterance Clarification</title>
      <author><first>Yaxin</first><last>Fan</last></author>
      <author><first>Peifeng</first><last>Li</last><affiliation>Soochow University, China</affiliation></author>
      <author><first>Qiaoming</first><last>Zhu</last><affiliation>Soochow University</affiliation></author>
      <pages>18800-18816</pages>
      <abstract>Dialogue discourse parsing aims to identify and analyze discourse relations between the utterances within dialogues. However, linguistic features in dialogues, such as omission and idiom, frequently introduce ambiguities that obscure the intended discourse relations, posing significant challenges for parsers. To address this issue, we propose a Discourse-aware Clarification Module (DCM) to enhance the performance of the dialogue discourse parser. DCM employs two distinct reasoning processes: clarification type reasoning and discourse goal reasoning. The former analyzes linguistic features, while the latter distinguishes the intended relation from the ambiguous one. Furthermore, we introduce Contribution-aware Preference Optimization (CPO) to mitigate the risk of erroneous clarifications, thereby reducing cascading errors. CPO enables the parser to assess the contributions of the clarifications from DCM and provide feedback to optimize the DCM, enhancing its adaptability and alignment with the parser’s requirements. Extensive experiments on the STAC and Molweni datasets demonstrate that our approach effectively resolves ambiguities and significantly outperforms the state-of-the-art (SOTA) baselines.</abstract>
      <url hash="3216d0b0">2025.acl-long.920</url>
      <bibkey>fan-etal-2025-improving</bibkey>
    </paper>
    <paper id="921">
      <title><fixed-case>I</fixed-case>m<fixed-case>P</fixed-case>art: Importance-Aware Delta-Sparsification for Improved Model Compression and Merging in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Yan</first><last>Yang</last></author>
      <author><first>Yixia</first><last>Li</last></author>
      <author><first>Hongru</first><last>Wang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Xuetao</first><last>Wei</last><affiliation>Southern University of Science and Technology</affiliation></author>
      <author><first>James Jianqiao</first><last>Yu</last><affiliation>Harbin Institute of Technology (Shenzhen)</affiliation></author>
      <author><first>Yun</first><last>Chen</last><affiliation>Shanghai University of Finance and Economics</affiliation></author>
      <author><first>Guanhua</first><last>Chen</last><affiliation>Southern University of Science and Technology</affiliation></author>
      <pages>18817-18829</pages>
      <abstract>With the proliferation of task-specific large language models, delta compression has emerged as a method to mitigate the resource challenges of deploying numerous such models by effectively compressing the delta model parameters. Previous delta-sparsification methods either remove parameters randomly or truncate singular vectors directly after singular value decomposition (SVD). However, these methods either disregard parameter importance entirely or evaluate it with too coarse a granularity. In this work, we introduce ImPart, a novel importance-aware delta sparsification approach. Leveraging SVD, it dynamically adjusts sparsity ratios of different singular vectors based on their importance, effectively retaining crucial task-specific knowledge even at high sparsity ratios. Experiments show that ImPart achieves state-of-the-art delta sparsification performance, demonstrating <tex-math>2\times</tex-math> higher compression ratio than baselines at the same performance level. When integrated with existing methods, ImPart sets a new state-of-the-art on delta quantization and model merging.</abstract>
      <url hash="f04199d2">2025.acl-long.921</url>
      <bibkey>yang-etal-2025-impart</bibkey>
    </paper>
    <paper id="922">
      <title>Words of Warmth: Trust and Sociability Norms for over 26k <fixed-case>E</fixed-case>nglish Words</title>
      <author><first>Saif M.</first><last>Mohammad</last></author>
      <pages>18830-18850</pages>
      <abstract>Social psychologists have shown that Warmth (W) and Competence (C) are the primary dimensions along which we assess other people and groups. These dimensions impact various aspects of our lives from social competence and emotion regulation to success in the work place and how we view the world. More recent work has started to explore how these dimensions develop, why they have developed, and what they constitute. Of particular note, is the finding that warmth has two distinct components: Trust (T) and Sociability (S). In this work, we introduce Words of Warmth, the first large-scale repository of manually derived word–warmth (as well as word–trust and word–sociability) associations for over 26k English words. We show that the associations are highly reliable. We use the lexicons to study the rate at which children acquire WCTS words with age. Finally, we show that the lexicon enables a wide variety of bias and stereotype research through case studies on various target entities. Words of Warmth is freely available at: http://saifmohammad.com/warmth.html</abstract>
      <url hash="fb480092">2025.acl-long.922</url>
      <bibkey>mohammad-2025-words</bibkey>
    </paper>
    <paper id="923">
      <title><fixed-case>B</fixed-case>ehavior<fixed-case>B</fixed-case>ox: Automated Discovery of Fine-Grained Performance Differences Between Language Models</title>
      <author><first>Lindia</first><last>Tjuatja</last><affiliation>CMU, Carnegie Mellon University</affiliation></author>
      <author><first>Graham</first><last>Neubig</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>18851-18873</pages>
      <abstract>Language model evaluation is a daunting task: prompts are brittle, corpus-level perplexities are vague, and the choice of benchmarks are endless. Finding examples that show meaningful, generalizable differences between two LMs is crucial to understanding where one model succeeds and another fails. Can this process be done automatically? In this work, we propose methodology for automated comparison of language models that uses performance-aware contextual embeddings to find fine-grained features of text where one LM outperforms another. Our method, which we name BehaviorBox, extracts coherent features that demonstrate differences with respect to the ease of generation between two LMs. Specifically, BehaviorBox finds features that describe groups of words in fine-grained contexts, such as “conditional ‘were’ in the phrase ‘if you were’” and “exclamation marks after emotional statements”, where one model outperforms another within a particular datatset. We apply BehaviorBox to compare models that vary in size, model family, and post-training, and enumerate insights into specific contexts that illustrate meaningful differences in performance which cannot be found by measures such as corpus-level perplexity alone.</abstract>
      <url hash="bfda7955">2025.acl-long.923</url>
      <bibkey>tjuatja-neubig-2025-behaviorbox</bibkey>
    </paper>
    <paper id="924">
      <title><fixed-case>HAF</fixed-case>-<fixed-case>RM</fixed-case>: A Hybrid Alignment Framework for Reward Model Training</title>
      <author><first>Shujun</first><last>Liu</last></author>
      <author><first>Xiaoyu</first><last>Shen</last><affiliation>Amazon</affiliation></author>
      <author><first>Yuhang</first><last>Lai</last><affiliation>Fudan University</affiliation></author>
      <author><first>Siyuan</first><last>Wang</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Shengbin</first><last>Yue</last></author>
      <author><first>Zengfeng</first><last>Huang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xuanjing</first><last>Huang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Zhongyu</first><last>Wei</last><affiliation>Fudan University</affiliation></author>
      <pages>18874-18893</pages>
      <abstract>The reward model has become increasingly important in alignment, assessment, and data construction for large language models (LLMs). Most existing researchers focus on enhancing reward models through data improvements, following the conventional training framework for reward models that directly optimizes the predicted rewards.In this paper, we propose a hybrid alignment framework **HAF-RM** for reward model training by introducing an additional constraint on token-level policy probabilities in addition to the reward score. It can simultaneously supervise the internal preference model at the token level and optimize the mapping layer of the reward model at the sequence level.Experiment results on five datasets sufficiently show the validity and effectiveness of our proposed hybrid framework for training a high-quality reward model.By decoupling the reward modeling procedure and incorporating hybrid supervision, our **HAF-RM** framework offers a principled and effective approach to enhancing the performance and alignment of reward models, a critical component in the responsible development of powerful language models. We release our code at [https://haf-rm.github.io](https://haf-rm.github.io).</abstract>
      <url hash="fe7dcc27">2025.acl-long.924</url>
      <bibkey>liu-etal-2025-haf</bibkey>
    </paper>
    <paper id="925">
      <title><fixed-case>CULEMO</fixed-case>: Cultural Lenses on Emotion - Benchmarking <fixed-case>LLM</fixed-case>s for Cross-Cultural Emotion Understanding</title>
      <author><first>Tadesse Destaw</first><last>Belay</last></author>
      <author><first>Ahmed Haj</first><last>Ahmed</last></author>
      <author><first>Alvin C</first><last>Grissom Ii</last><affiliation>Haverford College</affiliation></author>
      <author><first>Iqra</first><last>Ameer</last></author>
      <author><first>Grigori</first><last>Sidorov</last><affiliation>Instituto Politécnico Nacional</affiliation></author>
      <author><first>Olga</first><last>Kolesnikova</last><affiliation>Instituto Politécnico Nacional</affiliation></author>
      <author><first>Seid Muhie</first><last>Yimam</last><affiliation>Universität Hamburg</affiliation></author>
      <pages>18894-18909</pages>
      <abstract>NLP research has increasingly focused on subjective tasks such as emotion analysis. However, existing emotion benchmarks suffer fromtwo major shortcomings: (1) they largely rely on keyword-based emotion recognition, overlooking crucial cultural dimensions required fordeeper emotion understanding, and (2) many are created by translating English-annotated data into other languages, leading to potentially unreliable evaluation. To address these issues, we introduce Cultural Lenses on Emotion (CuLEmo), the first benchmark designedto evaluate culture-aware emotion prediction across six languages: Amharic, Arabic, English, German, Hindi, and Spanish. CuLEmocomprises 400 crafted questions per language, each requiring nuanced cultural reasoning and understanding. We use this benchmark to evaluate several state-of-the-art LLMs on culture-aware emotion prediction and sentiment analysis tasks. Our findings reveal that (1) emotion conceptualizations vary significantly across languages and cultures, (2) LLMs performance likewise varies by language and cultural context, and (3) prompting in English with explicit country context often outperforms in-language prompts for culture-aware emotion and sentiment understanding. The dataset and evaluation code is available.</abstract>
      <url hash="00bdf0d8">2025.acl-long.925</url>
      <bibkey>belay-etal-2025-culemo</bibkey>
    </paper>
    <paper id="926">
      <title><fixed-case>D</fixed-case>iff<fixed-case>PO</fixed-case>: Diffusion-styled Preference Optimization for Inference Time Alignment of Large Language Models</title>
      <author><first>Ruizhe</first><last>Chen</last></author>
      <author><first>Wenhao</first><last>Chai</last><affiliation>Princeton University</affiliation></author>
      <author><first>Zhifei</first><last>Yang</last><affiliation>Peking University</affiliation></author>
      <author><first>Xiaotian</first><last>Zhang</last></author>
      <author><first>Ziyang</first><last>Wang</last><affiliation>University of Science and Technology of China and University of Science and Technology of China</affiliation></author>
      <author><first>Tony</first><last>Quek</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Joey Tianyi</first><last>Zhou</last><affiliation>A*STAR Centre for Frontier AI Research</affiliation></author>
      <author><first>Soujanya</first><last>Poria</last></author>
      <author><first>Zuozhu</first><last>Liu</last><affiliation>Zhejiang University</affiliation></author>
      <pages>18910-18925</pages>
      <abstract>Inference-time alignment provides an efficient alternative for aligning LLMs with humans. However, these approaches still face challenges, such as limited scalability due to policy-specific value functions and latency during the inference phase. In this paper, we propose a novel approach, Diffusion-styled Preference Optimization (DiffPO), which provides an efficient and policy-agnostic solution for aligning LLMs with humans. By directly performing alignment at sentence level, DiffPO avoids the time latency associated with token-level generation. Designed as a plug-and-play module, DiffPO can be seamlessly integrated with various base models to enhance their alignment. Extensive experiments on AlpacaEval 2, MT-bench, and HH-RLHF demonstrate that DiffPO achieves superior alignment performance across various settings, achieving a favorable trade-off between alignment quality and inference-time latency. Furthermore, DiffPO demonstrates model-agnostic scalability, significantly improving the performance of large models such as Llama-3-70B.</abstract>
      <url hash="6c318e67">2025.acl-long.926</url>
      <bibkey>chen-etal-2025-diffpo</bibkey>
    </paper>
    <paper id="927">
      <title><fixed-case>M</fixed-case>eme<fixed-case>QA</fixed-case>: Holistic Evaluation for Meme Understanding</title>
      <author><first>Khoi P. N.</first><last>Nguyen</last></author>
      <author><first>Terrence</first><last>Li</last></author>
      <author><first>Derek Lou</first><last>Zhou</last></author>
      <author><first>Gabriel</first><last>Xiong</last></author>
      <author><first>Pranav</first><last>Balu</last></author>
      <author><first>Nandhan</first><last>Alahari</last></author>
      <author><first>Alan</first><last>Huang</last><affiliation>Stanford University</affiliation></author>
      <author><first>Tanush</first><last>Chauhan</last><affiliation>University of Texas at Austin</affiliation></author>
      <author><first>Harshavardhan</first><last>Bala</last></author>
      <author><first>Emre</first><last>Guzelordu</last></author>
      <author><first>Affan</first><last>Kashfi</last></author>
      <author><first>Aaron</first><last>Xu</last></author>
      <author><first>Suyesh</first><last>Shrestha</last></author>
      <author><first>Megan</first><last>Vu</last></author>
      <author><first>Jerry</first><last>Wang</last><affiliation>University of Texas at Dallas</affiliation></author>
      <author><first>Vincent</first><last>Ng</last></author>
      <pages>18926-18946</pages>
      <abstract>Automated meme understanding requires systems to demonstrate fine-grained visual recognition, commonsense reasoning, and extensive cultural knowledge. However, existing benchmarks for meme understanding only concern narrow aspects of meme semantics. To fill this gap, we present MemeQA, a dataset of over 9,000 multiple-choice questions designed to holistically evaluate meme comprehension across seven cognitive aspects. Experiments show that state-of-the-art Large Multimodal Models perform much worse than humans on MemeQA. While fine-tuning improves their performance, they still make many errors on memes wherein proper understanding requires going beyond surface-level sentiment. Moreover, injecting “None of the above” into the available options makes the questions more challenging for the models. Our dataset is publicly available at https://github.com/npnkhoi/memeqa.</abstract>
      <url hash="b1c3e2c7">2025.acl-long.927</url>
      <bibkey>nguyen-etal-2025-memeqa</bibkey>
    </paper>
    <paper id="928">
      <title><fixed-case>L</fixed-case>o<fixed-case>GU</fixed-case>: Long-form Generation with Uncertainty Expressions</title>
      <author><first>Ruihan</first><last>Yang</last></author>
      <author><first>Caiqi</first><last>Zhang</last></author>
      <author><first>Zhisong</first><last>Zhang</last><affiliation>Tencent</affiliation></author>
      <author><first>Xinting</first><last>Huang</last><affiliation>Tencent</affiliation></author>
      <author><first>Sen</first><last>Yang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Nigel</first><last>Collier</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Dong</first><last>Yu</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Deqing</first><last>Yang</last><affiliation>Fudan University</affiliation></author>
      <pages>18947-18968</pages>
      <abstract>While Large Language Models (LLMs) demonstrate impressive capabilities, they still struggle with generating factually incorrect content (i.e., hallucinations). A promising approach to mitigate this issue is enabling models to express uncertainty when unsure. Previous research on uncertainty modeling has primarily focused on short-form QA, but real-world applications often require much longer responses. In this work, we introduce the task of Long-form Generation with Uncertainty (LoGU). We identify two key challenges: Uncertainty Suppression, where models hesitate to express uncertainty, and Uncertainty Misalignment, where models convey uncertainty inaccurately. To tackle these challenges, we propose a refinement-based data collection framework and a two-stage training pipeline. Our framework adopts a divide-and-conquer strategy, refining uncertainty based on atomic claims. The collected data are then used in training through supervised fine-tuning (SFT) and direct preference optimization (DPO) to enhance uncertainty expression. Extensive experiments on three long-form instruction following datasets show that our method significantly improves accuracy, reduces hallucinations, and maintains the comprehensiveness of responses.</abstract>
      <url hash="74274af8">2025.acl-long.928</url>
      <bibkey>yang-etal-2025-logu</bibkey>
    </paper>
    <paper id="929">
      <title><fixed-case>K</fixed-case>i<fixed-case>RAG</fixed-case>: Knowledge-Driven Iterative Retriever for Enhancing Retrieval-Augmented Generation</title>
      <author><first>Jinyuan</first><last>Fang</last><affiliation>University of Glasgow</affiliation></author>
      <author><first>Zaiqiao</first><last>Meng</last><affiliation>University of Glasgow</affiliation></author>
      <author><first>Craig</first><last>MacDonald</last><affiliation>University of Glasgow</affiliation></author>
      <pages>18969-18985</pages>
      <abstract>Iterative retrieval-augmented generation (iRAG) models offer an effective approach for multihop question answering (QA). However, their retrieval processes face two key challenges: (1) they can be disrupted by irrelevant documents or factually inaccurate chain-of-thoughts; (2) their retrievers are not designed to dynamically adapt to the evolving information needs in multi-step reasoning, making it difficult to identify and retrieve the missing information required at each iterative step. Therefore, we propose KiRAG, which uses a knowledge-driven iterative retriever model to enhance the retrieval process of iRAG. Specifically, KiRAG decomposes documents into knowledge triples and performs iterative retrieval with these triples to enable a factually reliable retrieval process. Moreover, KiRAG integrates reasoning into the retrieval process to dynamically identify and retrieve knowledge that bridges information gaps, effectively adapting to the evolving information needs. Empirical results show that KiRAG significantly outperforms existing iRAG models, with an average improvement of 9.40% in R@3 and 5.14% in F1 on multi-hop QA datasets.</abstract>
      <url hash="ed7740c8">2025.acl-long.929</url>
      <bibkey>fang-etal-2025-kirag</bibkey>
    </paper>
    <paper id="930">
      <title>Enhancing Lexicon-Based Text Embeddings with Large Language Models</title>
      <author><first>Yibin</first><last>Lei</last></author>
      <author><first>Tao</first><last>Shen</last><affiliation>Oracle</affiliation></author>
      <author><first>Yu</first><last>Cao</last></author>
      <author><first>Andrew</first><last>Yates</last><affiliation>Johns Hopkins University</affiliation></author>
      <pages>18986-19001</pages>
      <abstract>Recent large language models (LLMs) have demonstrated exceptional performance on general-purpose text embedding tasks. While dense embeddings have dominated related research, we introduce the first lexicon-based embeddings (LENS) leveraging LLMs that achieve competitive performance on these tasks. LENS consolidates the vocabulary space through token embedding clustering to handle the issue of token redundancy in LLM vocabularies. To further improve performance, we investigate bidirectional attention and various pooling strategies. Specifically, LENS simplifies lexical matching with redundant vocabularies by assigning each dimension to a specific token cluster, where semantically similar tokens are grouped together. Extensive experiments demonstrate that LENS outperforms dense embeddings on the Massive Text Embedding Benchmark (MTEB), delivering compact representations with dimensionality comparable to dense counterparts. Furthermore, LENS inherently supports efficient embedding dimension pruning without any specialized objectives like Matryoshka Representation Learning. Notably, combining LENS with dense embeddings achieves state-of-the-art performance on the retrieval subset of MTEB (i.e., BEIR).</abstract>
      <url hash="e84b44be">2025.acl-long.930</url>
      <bibkey>lei-etal-2025-enhancing</bibkey>
    </paper>
    <paper id="931">
      <title><fixed-case>C</fixed-case>o<fixed-case>C</fixed-case>o<fixed-case>L</fixed-case>ex: Confidence-guided Copy-based Decoding for Grounded Legal Text Generation</title>
      <author><first>Santosh</first><last>T.y.s.s</last></author>
      <author><first>Youssef Tarek</first><last>Elkhayat</last></author>
      <author><first>Oana</first><last>Ichim</last><affiliation>University of Massachusetts at Amherst</affiliation></author>
      <author><first>Pranav</first><last>Shetty</last><affiliation>J.P. Morgan Chase</affiliation></author>
      <author><first>Dongsheng</first><last>Wang</last><affiliation>JPMorgan AI Research</affiliation></author>
      <author><first>Zhiqiang</first><last>Ma</last><affiliation>J.P. Morgan Chase</affiliation></author>
      <author><first>Armineh</first><last>Nourbakhsh</last><affiliation>School of Computer Science, Carnegie Mellon University and J.P. Morgan Chase</affiliation></author>
      <author><first>Xiaomo</first><last>Liu</last><affiliation>JP Morgan AI Research</affiliation></author>
      <pages>19002-19018</pages>
      <abstract>Due to their ability to process long and complex contexts, LLMs can offer key benefits to the Legal domain, but their adoption has been hindered by their tendency to generate unfaithful, ungrounded, or hallucinatory outputs. While Retrieval-Augmented Generation offers a promising solution by grounding generations in external knowledge, it offers no guarantee that the provided context will be effectively integrated. To address this, context-aware decoding strategies have been proposed to amplify the influence of relevant context, but they usually do not explicitly enforce faithfulness to the context. In this work, we introduce Confidence-guided Copy-based Decoding for Legal Text Generation (CoCoLex)—a decoding strategy that dynamically interpolates the model produced vocabulary distribution with a distribution derived based on copying from the context. CoCoLex encourages direct copying based on models’ confidence, ensuring greater fidelity to the source. Experimental results on five legal benchmarks demonstrate that CoCoLex outperforms existing context-aware decoding methods, particularly in long-form generation tasks.</abstract>
      <url hash="8a9e4abb">2025.acl-long.931</url>
      <bibkey>t-y-s-s-etal-2025-cocolex</bibkey>
    </paper>
    <paper id="932">
      <title>Beyond N-Grams: Rethinking Evaluation Metrics and Strategies for Multilingual Abstractive Summarization</title>
      <author><first>Itai</first><last>Mondshine</last></author>
      <author><first>Tzuf</first><last>Paz-Argaman</last></author>
      <author><first>Reut</first><last>Tsarfaty</last><affiliation>Google and Bar-Ilan University, Technion</affiliation></author>
      <pages>19019-19035</pages>
      <abstract>Automatic N-gram based metrics such as ROUGE are widely used for evaluating generative tasks such as summarization. While these metrics are considered indicative (even if imperfect), of human evaluation for English, their suitability for other languages remains unclear. To address this, in this paper we systematically assess evaluation metrics for generation — both n-gram-based and neural-based— to assess their effectiveness across languages and tasks. Specifically, we design a large-scale evaluation suite across eight languages from four typological families — agglutinative, isolating, low-fusional, and high-fusional — from both low- and high-resource languages, to analyze their correlations with human judgments. Our findings highlight the sensitivity of the evaluation metric to the language type at hand. For example, for fusional languages, n-gram-based metrics demonstrate a lower correlation with human assessments, compared to isolating and agglutinative languages. We also demonstrate that tokenization considerations can significantly mitigate this for fusional languages with rich morphology, up to reversing such negative correlations. Additionally, we show that neural-based metrics specifically trained for evaluation, such as COMET, consistently outperform other neural metrics and correlate better than ngrmas metrics with human judgments in low-resource languages. Overall, our analysis highlights the limitations of n-gram metrics for fusional languages and advocates for investment in neural-based metrics trained for evaluation tasks.</abstract>
      <url hash="ccd41075">2025.acl-long.932</url>
      <bibkey>mondshine-etal-2025-beyond-n</bibkey>
    </paper>
    <paper id="933">
      <title><fixed-case>CC</fixed-case>-Tuning: A Cross-Lingual Connection Mechanism for Improving Joint Multilingual Supervised Fine-Tuning</title>
      <author><first>Yangfan</first><last>Ye</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Xiaocheng</first><last>Feng</last></author>
      <author><first>Zekun</first><last>Yuan</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Xiachong</first><last>Feng</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Libo</first><last>Qin</last><affiliation>Central South University</affiliation></author>
      <author><first>Lei</first><last>Huang</last></author>
      <author><first>Weitao</first><last>Ma</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Yichong</first><last>Huang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Zhirui</first><last>Zhang</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Yunfei</first><last>Lu</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Xiaohui</first><last>Yan</last></author>
      <author><first>Duyu</first><last>Tang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Dandan</first><last>Tu</last></author>
      <author><first>Bing</first><last>Qin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>19036-19051</pages>
      <abstract>Current large language models (LLMs) often exhibit imbalanced multilingual capabilities due to their English-centric training corpora. To address this, existing fine-tuning approaches operating at the data-level (e.g., through data augmentation or distillation) typically introduce implicit cross-lingual alignment, overlooking the potential for more profound, latent-level cross-lingual interactions. In this work, we propose CC-Tuning, a novel multilingual fine-tuning paradigm that explicitly establishes a cross-lingual connection mechanism at the latent level. During training, CC-Tuning fuses the feed forward activations from both English and non-English inputs, enabling the model to benefit from both linguistic resources. This process is facilitated with a trainable Decision Maker that identifies beneficial activations. Furthermore, during inference, a Transform Matrix is utilized to simulate the cross-lingual connection under monolingual setting through representation transformation. Our experiments on six benchmarks covering 22 languages show that CC-Tuning outperforms vanilla SFT and offers a strong latent-level alternative to data-level augmentation methods. Further analysis also highlights the practicality of CC-Tuning and the potential of latent-level cross-lingual interactions in advancing the multilingual performance of LLMs.</abstract>
      <url hash="59305462">2025.acl-long.933</url>
      <bibkey>ye-etal-2025-cc</bibkey>
    </paper>
    <paper id="934">
      <title><fixed-case>SC</fixed-case>on<fixed-case>U</fixed-case>: Selective Conformal Uncertainty in Large Language Models</title>
      <author><first>Zhiyuan</first><last>Wang</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Qingni</first><last>Wang</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Yue</first><last>Zhang</last><affiliation>Drexel University</affiliation></author>
      <author><first>Tianlong</first><last>Chen</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Xiaofeng</first><last>Zhu</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Xiaoshuang</first><last>Shi</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Kaidi</first><last>Xu</last><affiliation>Drexel University</affiliation></author>
      <pages>19052-19075</pages>
      <abstract>As large language models are increasingly utilized in real-world applications, guarantees of task-specific metrics are essential for their reliable deployment. Previous studies have introduced various criteria of conformal uncertainty grounded in split conformal prediction, which offer user-specified correctness coverage. However, existing frameworks often fail to identify uncertainty data outliers that violate the exchangeability assumption, leading to unbounded miscoverage rates and unactionable prediction sets. In this paper, we propose a novel approach termed Selective Conformal Uncertainty (SConU), which, for the first time, implements significance tests, by developing two conformal p-values that are instrumental in determining whether a given sample deviates from the uncertainty distribution of the calibration set at a specific manageable risk level. Our approach not only facilitates rigorous management of miscoverage rates across both single-domain and interdisciplinary contexts, but also enhances the efficiency of predictions. Furthermore, we comprehensively analyze the components of the conformal procedures, aiming to approximate conditional coverage, particularly in high-stakes question-answering tasks.</abstract>
      <url hash="aee2d973">2025.acl-long.934</url>
      <bibkey>wang-etal-2025-sconu</bibkey>
    </paper>
    <paper id="935">
      <title><fixed-case>M</fixed-case>ega<fixed-case>P</fixed-case>airs: Massive Data Synthesis for Universal Multimodal Retrieval</title>
      <author><first>Junjie</first><last>Zhou</last></author>
      <author><first>Yongping</first><last>Xiong</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Zheng</first><last>Liu</last></author>
      <author><first>Ze</first><last>Liu</last></author>
      <author><first>Shitao</first><last>Xiao</last></author>
      <author><first>Yueze</first><last>Wang</last><affiliation>Beijing Academy of Artificial Intelligence</affiliation></author>
      <author><first>Bo</first><last>Zhao</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Chen Jason</first><last>Zhang</last></author>
      <author><first>Defu</first><last>Lian</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>19076-19095</pages>
      <abstract>Despite the rapidly growing demand for multimodal retrieval, progress in this field remains severely constrained by a lack of training data. In this paper, we introduce MegaPairs, a novel data synthesis method that leverages vision language models (VLMs) and open-domain images, together with a massive synthetic dataset generated from this method. Our empirical analysis shows that MegaPairs generates high-quality data, enabling the multimodal retriever to significantly outperform the baseline model trained on 70<tex-math>\times</tex-math> more data from existing datasets. Moreover, since MegaPairs solely relies on general image corpora and open-source VLMs, it can be easily scaled up, enabling continuous improvements in retrieval performance. In this stage, we produced more than 26 million training instances and trained several models of varying sizes using this data. These new models achieve state-of-the-art zero-shot performance across 4 popular composed image retrieval (CIR) benchmarks and the highest overall performance on the 36 datasets provided by MMEB. They also demonstrate notable performance improvements with additional downstream fine-tuning. Our code, synthesized dataset, and pre-trained models are publicly available at https://github.com/VectorSpaceLab/MegaPairs.</abstract>
      <url hash="c80423ef">2025.acl-long.935</url>
      <bibkey>zhou-etal-2025-megapairs</bibkey>
    </paper>
    <paper id="936">
      <title>When <fixed-case>GPT</fixed-case> Spills the Tea: Comprehensive Assessment of Knowledge File Leakage in <fixed-case>GPT</fixed-case>s</title>
      <author><first>Xinyue</first><last>Shen</last></author>
      <author><first>Yun</first><last>Shen</last><affiliation>Flexera</affiliation></author>
      <author><first>Michael</first><last>Backes</last><affiliation>CISPA Helmholtz Center for Information Security</affiliation></author>
      <author><first>Yang</first><last>Zhang</last><affiliation>CISPA Helmholtz Center for Information Security</affiliation></author>
      <pages>19096-19111</pages>
      <abstract>Knowledge files have been widely used in large language model (LLM)-powered agents, such as GPTs, to improve response quality. However, concerns over the potential leakage of knowledge files have grown significantly. Existing studies demonstrate that adversarial prompts can induce GPTs to leak knowledge file content. Yet, it remains uncertain whether additional leakage vectors exist, particularly given the complex data flows across clients, servers, and databases in GPTs. In this paper, we present a comprehensive risk assessment of knowledge file leakage, leveraging a novel workflow inspired by Data Security Posture Management (DSPM). Through the analysis of 651,022 GPT metadata, 11,820 flows, and 1,466 responses, we identify five leakage vectors: metadata, GPT initialization, retrieval, sandboxed execution environments, and prompts. These vectors enable adversaries to extract sensitive knowledge file data such as titles, content, types, and sizes. Notably, the activation of the built-in tool Code Interpreter leads to a privilege escalation vulnerability, enabling adversaries to directly download original knowledge files with a 95.95% success rate. Further analysis reveals that 28.80% of leaked files are copyrighted, including digital copies from major publishers and internal materials from a listed company. In the end, we provide actionable solutions for GPT builders and platform providers to secure the GPT data supply chain.</abstract>
      <url hash="f6cca02d">2025.acl-long.936</url>
      <bibkey>shen-etal-2025-gpt</bibkey>
    </paper>
    <paper id="937">
      <title><fixed-case>U</fixed-case>ni<fixed-case>C</fixed-case>odec: Unified Audio Codec with Single Domain-Adaptive Codebook</title>
      <author><first>Yidi</first><last>Jiang</last></author>
      <author><first>Qian</first><last>Chen</last></author>
      <author><first>Shengpeng</first><last>Ji</last></author>
      <author><first>Yu</first><last>Xi</last></author>
      <author><first>Wen</first><last>Wang</last></author>
      <author><first>Chong</first><last>Zhang</last><affiliation>Alibaba</affiliation></author>
      <author><first>Xianghu</first><last>Yue</last><affiliation>Tianjin University</affiliation></author>
      <author><first>ShiLiang</first><last>Zhang</last></author>
      <author><first>Haizhou</first><last>Li</last><affiliation>The Chinese University of Hong Kong (Shenzhen); National University of Singapore and National University of Singapore</affiliation></author>
      <pages>19112-19124</pages>
      <abstract>The emergence of audio language models is empowered by neural audio codecs, which establish critical mappings between continuous waveforms and discrete tokens compatible with language model paradigms. The evolutionary trends from multi-layer residual vector quantizer to single-layer quantizer are beneficial for language-autoregressive decoding. However, the capability to handle multi-domain audio signals through a single codebook remains constrained by inter-domain distribution discrepancies. In this work, we introduce UniCodec, a unified audio codec with a single codebook to support multi-domain audio data, including speech, music, and sound. To achieve this, we propose a partitioned domain-adaptive codebook method based on domain Mixture-of-Experts strategy to capture the distinct characteristics of each audio domain. Furthermore, to enrich the semantic density of the codec without auxiliary modules, we propose a self-supervised mask prediction modeling approach. Comprehensive objective and subjective evaluations demonstrate that UniCodec achieves excellent audio reconstruction performance across the three audio domains, outperforming existing unified neural codecs with a single codebook, and even surpasses state-of-the-art domain-specific codecs on both acoustic and semantic representation capabilities.</abstract>
      <url hash="06cfae41">2025.acl-long.937</url>
      <bibkey>jiang-etal-2025-unicodec</bibkey>
    </paper>
    <paper id="938">
      <title><fixed-case>KERL</fixed-case>: Knowledge-Enhanced Personalized Recipe Recommendation using Large Language Models</title>
      <author><first>Fnu</first><last>Mohbat</last></author>
      <author><first>Mohammed J</first><last>Zaki</last></author>
      <pages>19125-19141</pages>
      <abstract>Recent advances in large language models (LLMs) and the abundance of food data have resulted in studies to improve food understanding using LLMs. Despite several recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there has been limited research on integrating food related KGs with LLMs. We introduce KERL, a unified system that leverages food KGs and LLMs to provide personalized food recommendations and generates recipes with associated micro-nutritional information. Given a natural language question, KERL extracts entities, retrieves subgraphs from the KG, which are then fed into the LLM as context to select the recipes that satisfy the constraints. Next, our system generates the cooking steps and nutritional information for each recipe. To evaluate our approach, we also develop a benchmark dataset by curating recipe related questions, combined with constraints and personal preferences. Through extensive experiments, we show that our proposed KG-augmented LLM significantly outperforms existing approaches, offering a complete and coherent solution for food recommendation, recipe generation, and nutritional analysis. Our code and benchmark datasets are publicly available at https://github.com/mohbattharani/KERL.</abstract>
      <url hash="30c35820">2025.acl-long.938</url>
      <bibkey>mohbat-zaki-2025-kerl</bibkey>
    </paper>
    <paper id="939">
      <title>Multilingual Arbitration: Optimizing Data Pools to Accelerate Multilingual Progress</title>
      <author><first>Ayomide</first><last>Odumakinde</last></author>
      <author><first>Daniel</first><last>D’souza</last><affiliation>Cohere</affiliation></author>
      <author><first>Pat</first><last>Verga</last><affiliation>Cohere</affiliation></author>
      <author><first>Beyza</first><last>Ermis</last><affiliation>Cohere AI</affiliation></author>
      <author><first>Sara</first><last>Hooker</last><affiliation>Cohere For AI</affiliation></author>
      <pages>19142-19164</pages>
      <abstract>Synthetic data has driven recent state-of-the-art advancements, but reliance on a single oracle teacher model can lead to model collapse and bias propagation. These issues are particularly severe in multilingual settings, where no single model excels across all languages. In this study, we propose multilingual arbitration, which exploits performance variations among multiple models for each language. By strategically routing samples through a diverse set of models, each with unique strengths, we mitigate these challenges and enhance multilingual performance. Extensive experiments with state-of-the-art models demonstrate that our approach significantly surpasses single-teacher distillation, achieving up to 80% win rates over proprietary and open-weight models like Gemma 2, Llama 3.1, and Mistral v0.3, with the largest improvements in low-resource languages.</abstract>
      <url hash="3a549a00">2025.acl-long.939</url>
      <bibkey>odumakinde-etal-2025-multilingual</bibkey>
    </paper>
    <paper id="940">
      <title>Controlled Low-Rank Adaptation with Subspace Regularization for Continued Training on Large Language Models</title>
      <author><first>Yuheng</first><last>Lu</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Bingshuo</first><last>Qian</last></author>
      <author><first>Caixia</first><last>Yuan</last></author>
      <author><first>Huixing</first><last>Jiang</last><affiliation>Li Auto</affiliation></author>
      <author><first>Xiaojie</first><last>Wang</last><affiliation>Beijing University of Post and Telecommunication</affiliation></author>
      <pages>19165-19181</pages>
      <abstract>Large language models (LLMs) exhibit remarkable capabilities in natural language processing but face catastrophic forgetting when learning new tasks, where adaptation to a new domain leads to a substantial decline in performance on previous tasks. In this paper, we propose Controlled LoRA (CLoRA), a subspace regularization method on LoRA structure. Aiming to reduce the scale of output change while introducing minimal constraint on model capacity, CLoRA imposes constraints on the direction of updating matrix’s null space. Experimental results on one-stage LLM finetuning tasks and continual learning settings highlight the superiority of CLoRA as an effective parameter-efficient finetuning method with catastrophic forgetting mitigating. Further investigation for model parameters indicates that CLoRA effectively balances the trade-off between model capacity and degree of forgetting. The code for implementing CLoRA will be publicly available.</abstract>
      <url hash="e1ed508e">2025.acl-long.940</url>
      <bibkey>lu-etal-2025-controlled</bibkey>
    </paper>
    <paper id="941">
      <title><fixed-case>C</fixed-case>hinese <fixed-case>S</fixed-case>imple<fixed-case>QA</fixed-case>: A <fixed-case>C</fixed-case>hinese Factuality Evaluation for Large Language Models</title>
      <author><first>Yancheng</first><last>He</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Shilong</first><last>Li</last></author>
      <author><first>Jiaheng</first><last>Liu</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Yingshui</first><last>Tan</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Weixun</first><last>Wang</last></author>
      <author><first>Hui</first><last>Huang</last></author>
      <author><first>Xingyuan</first><last>Bu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Hangyu</first><last>Guo</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Chengwei</first><last>Hu</last></author>
      <author><first>Boren</first><last>Zheng</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Zhuoran</first><last>Lin</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Dekai</first><last>Sun</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Zhicheng</first><last>Zheng</last><affiliation>Princeton University</affiliation></author>
      <author><first>Wenbo</first><last>Su</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Bo</first><last>Zheng</last><affiliation>Alibaba Group</affiliation></author>
      <pages>19182-19208</pages>
      <abstract>New LLM benchmarks are important to align with the rapid development of Large Language Models (LLMs). In this work, we present Chinese SimpleQA, the first comprehensive Chinese benchmark to evaluate the factuality ability of LLMs to answer short questions, and Chinese SimpleQA mainly has five properties (i.e., Chinese, Diverse, High-quality, Static, Easy-to-evaluate). Specifically, first, we focus on the Chinese language over 6 major topics with 99 diverse subtopics. Second, we conduct a comprehensive quality control process to achieve high-quality questions and answers, where the reference answers are static and cannot be changed over time. Third, following SimpleQA, the questions and answers are very short, and the grading process is easy-to-evaluate. Based on Chinese SimpleQA, we perform a comprehensive evaluation of the factuality abilities of existing LLMs. Finally, we hope that Chinese SimpleQA could guide the developers to better understand the Chinese factuality abilities of their models and facilitate the growth of LLMs.</abstract>
      <url hash="afc11b10">2025.acl-long.941</url>
      <bibkey>he-etal-2025-chinese</bibkey>
    </paper>
    <paper id="942">
      <title><fixed-case>PVP</fixed-case>: An Image Dataset for Personalized Visual Persuasion with Persuasion Strategies, Viewer Characteristics, and Persuasiveness Ratings</title>
      <author><first>Junseo</first><last>Kim</last></author>
      <author><first>Jongwook</first><last>Han</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Dongmin</first><last>Choi</last></author>
      <author><first>Jongwook</first><last>Yoon</last></author>
      <author><first>Eun-Ju</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Yohan</first><last>Jo</last><affiliation>Seoul National University</affiliation></author>
      <pages>19209-19237</pages>
      <abstract>Visual persuasion, which uses visual elements to influence cognition and behaviors, is crucial in fields such as advertising and politicalcommunication. With recent advancements in artificial intelligence, there is growing potential to develop persuasive systems that automatically generate persuasive images tailored to individuals. However, a significant bottleneck in this area is the lack of comprehensivedatasets that connect the persuasiveness of images with the personal information about those who evaluated the images. To address this gap and facilitate technological advancements in personalized visual persuasion, we release the Personalized Visual Persuasion (PVP) dataset, comprising 28,454 persuasive images across 596 messages and 9 persuasion strategies. Importantly, the PVP dataset provides persuasiveness scores of images evaluated by 2,521 human annotators, along with their demographic and psychological characteristics (personality traits and values). We demonstrate the utility of our dataset by developing a persuasive image generator and an automated evaluator, and establish benchmark baselines. Our experiments reveal that incorporating psychological characteristics enhances the generation and evaluation of persuasive images, providing valuable insights for personalized visual persuasion.</abstract>
      <url hash="c11da1fd">2025.acl-long.942</url>
      <bibkey>kim-etal-2025-pvp</bibkey>
    </paper>
    <paper id="943">
      <title>Any Information Is Just Worth One Single Screenshot: Unifying Search With Visualized Information Retrieval</title>
      <author><first>Zheng</first><last>Liu</last></author>
      <author><first>Ze</first><last>Liu</last></author>
      <author><first>Zhengyang</first><last>Liang</last><affiliation>Beijing Academy of Artificial Intelligence</affiliation></author>
      <author><first>Junjie</first><last>Zhou</last></author>
      <author><first>Shitao</first><last>Xiao</last></author>
      <author><first>Chao</first><last>Gao</last></author>
      <author><first>Chen Jason</first><last>Zhang</last></author>
      <author><first>Defu</first><last>Lian</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>19238-19261</pages>
      <abstract>With the popularity of multimodal techniques, it receives growing interests to acquire useful information in visual forms. In this work, we formally define an emerging IR paradigm called Visualized Information Retrieval, or Vis-IR, where multimodal information, such as texts, images, tables and charts, is jointly represented by a unified visual format called Screenshots, for various retrieval applications. We further make three key contributions for Vis-IR. First, we create VIRA (Vis-IR Aggregation), a large-scale dataset comprising a vast collection of screenshots from diverse sources, carefully curated into captioned and question-answer formats. Second, we develop UniSE (Universal Screenshot Embeddings), a family of retrieval models that enable screenshots to query or be queried across arbitrary data modalities. Finally, we construct MVRB (Massive Visualized IR Benchmark), a comprehensive benchmark covering a variety of task forms and application scenarios. Through extensive evaluations on MVRB, we highlight the deficiency from existing multimodal retrievers and the substantial improvements made by UniSE. Our data, model and benchmark have been made publicly available, which lays a solid foundation for this emerging field.</abstract>
      <url hash="319a9847">2025.acl-long.943</url>
      <bibkey>liu-etal-2025-information</bibkey>
    </paper>
    <paper id="944">
      <title>Tunable <fixed-case>LLM</fixed-case>-based Proactive Recommendation Agent</title>
      <author><first>Mingze</first><last>Wang</last></author>
      <author><first>Chongming</first><last>Gao</last></author>
      <author><first>Wenjie</first><last>Wang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Yangyang</first><last>Li</last><affiliation>Academy of Cyber</affiliation></author>
      <author><first>Fuli</first><last>Feng</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>19262-19276</pages>
      <abstract>Recommender systems are indispensable on various digital platforms. However, traditional methods often reinforce existing user interests, which leads to echo chambers and limits diversity. Proactive Recommendation Systems (PRS) aim to address this issue by cultivating users’ latent interests through multi-step recommendations. Despite advancements, challenges persist particularly in optimizing long-term rewards and adapting to real-time user feedback. In this study, we propose an LLM-based Actor-Critic Agent framework to enhance PRS. This framework utilizes the LLM-based agent to adjust recommendations in real time based on feedback and employs agent-tuning methods to optimize long-term rewards using three proposed reward functions. Extensive experiments validate the significant superiority of this framework over existing methods by optimizing long-term rewards and dynamically evolving with user feedback.</abstract>
      <url hash="2b582fdc">2025.acl-long.944</url>
      <bibkey>wang-etal-2025-tunable</bibkey>
    </paper>
    <paper id="945">
      <title><fixed-case>A</fixed-case>gent<fixed-case>RM</fixed-case>: Enhancing Agent Generalization with Reward Modeling</title>
      <author><first>Yu</first><last>Xia</last></author>
      <author><first>Jingru</first><last>Fan</last></author>
      <author><first>Weize</first><last>Chen</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Siyu</first><last>Yan</last></author>
      <author><first>Xin</first><last>Cong</last></author>
      <author><first>Zhong</first><last>Zhang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Yaxi</first><last>Lu</last><affiliation>Department of Computer Science and Technology, Tsinghua University</affiliation></author>
      <author><first>Yankai</first><last>Lin</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Zhiyuan</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Maosong</first><last>Sun</last><affiliation>Tsinghua University</affiliation></author>
      <pages>19277-19290</pages>
      <abstract>Existing LLM-based agents have achieved strong performance on held-in tasks, but their generalizability to unseen tasks remains poor. Hence, some recent work focus on fine-tuning the policy model with more diverse tasks to improve the generalizability. In this work, we find that finetuning a reward model to guide the policy model is more robust than directly finetuning the policy model.Based on this finding, we propose AgentRM, a 8B generalizable reward model, to guide the policy model for effective test-time search.We comprehensively investigate three approaches to construct the reward model, including explicit reward modeling, implicit reward modeling and LLM-as-a-judge.We then use AgentRM to guide the answer generation with Best-of-N sampling and beam search.We show that AgentRM is robust to paraphrasings of task instructions and can generalize to unseen tasks that require novel optimal behavior.Through extensive evaluation across nine tasks spanning four categories, AgentRM enhances the non-finetuned 8B policy model by 8.8 points on average, surpassing the top general agent by 4.0.Moreover, it demonstrates weak-to-strong generalization, yielding greater improvement on more powerful policy models.As for the specializability, AgentRM can also boost a finetuned policy model and outperform the top specialized agent by 11.4 on three held-in tasks.Further analysis verifies its effectiveness in test-time scaling.We release the code and data at https://github.com/thunlp/AgentRM.</abstract>
      <url hash="10542821">2025.acl-long.945</url>
      <bibkey>xia-etal-2025-agentrm</bibkey>
    </paper>
    <paper id="946">
      <title>From Outcomes to Processes: Guiding <fixed-case>PRM</fixed-case> Learning from <fixed-case>ORM</fixed-case> for Inference-Time Alignment</title>
      <author><first>Bin</first><last>Xie</last><affiliation>, Chinese Academy of Sciences</affiliation></author>
      <author><first>Bingbing</first><last>Xu</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yige</first><last>Yuan</last></author>
      <author><first>Shengmao</first><last>Zhu</last></author>
      <author><first>Huawei</first><last>Shen</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <pages>19291-19307</pages>
      <abstract>Inference-time alignment methods have gained significant attention for their efficiency and effectiveness in aligning large language models (LLMs) with human preferences. However, existing dominant approaches, reward-guided search (RGS), suffer from a critical granularity mismatch: reward models (RMs) are trained on complete responses but applied to incomplete sequences during generation, leading to inconsistent scoring and suboptimal alignment. To combat the challenge, we argue that an ideal RM should satisfy two objectives: Score Consistency, ensuring coherent evaluation across partial and complete responses, and Preference Consistency, aligning partial sequence assessments with human preferences. To achieve these, we propose <tex-math>\textbf{SPRM}</tex-math>, a novel dual-consistency framework integrating score consistency-based and preference consistency-based partial evaluation modules, which leverage the Bradley-Terry model and entropy-based reweighting to predict cumulative rewards and prioritize human-aligned sequences. Extensive experiments on dialogue, summarization, and reasoning tasks demonstrate the effectiveness of SPRM, significantly reducing granularity discrepancies by up to <tex-math>\textbf{11.7\\%}</tex-math> on TL;DR Summarization and achieving a <tex-math>\textbf{3.6\\%–10.3\\%}</tex-math> improvement in GPT-4 evaluation scores across all tasks. Code is publicly available at [this link](https://github.com/xiebin23/SPRM).</abstract>
      <url hash="a39b1a57">2025.acl-long.946</url>
      <bibkey>xie-etal-2025-outcomes</bibkey>
    </paper>
    <paper id="947">
      <title>Segment-Based Attention Masking for <fixed-case>GPT</fixed-case>s</title>
      <author><first>Shahar</first><last>Katz</last><affiliation>Tel Aviv University</affiliation></author>
      <author><first>Liran</first><last>Ringel</last><affiliation>Computer Science Departmen, Technion-Israel Institute of Technology</affiliation></author>
      <author><first>Yaniv</first><last>Romano</last><affiliation>Technion, Technion</affiliation></author>
      <author><first>Lior</first><last>Wolf</last><affiliation>Tel Aviv University, Tel Aviv University and Tel Aviv University</affiliation></author>
      <pages>19308-19322</pages>
      <abstract>Causal masking is a fundamental component in Generative Pre-Trained Transformer (GPT) models, playing a crucial role during training. Although GPTs can process the entire user prompt at once, the causal masking is applied to all input tokens step-by-step, mimicking the generation process. This imposes an unnecessary constraint during the initial “prefill” phase when the model processes the input prompt and generates the internal representations before producing any output tokens. In this work, attention is masked based on the known block structure at the prefill phase, followed by the conventional token-by-token autoregressive process after that. For example, in a typical chat prompt, the system prompt is treated as one block, and the user prompt as the next one. Each of these is treated as a unit for the purpose of masking, such that the first tokens in each block can access the subsequent tokens in a non-causal manner. Then, the model answer is generated in the conventional causal manner. The Segment-by-Segment scheme entails no additional computational overhead. When integrated using a lightweight fine-tuning into already trained models such as Llama and Qwen, MAS quickly increases models’ performances.</abstract>
      <url hash="a3f8d99f">2025.acl-long.947</url>
      <bibkey>katz-etal-2025-segment</bibkey>
    </paper>
    <paper id="948">
      <title>Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the Limits of Embedding Space Capacity</title>
      <author><first>Yuri</first><last>Kuratov</last><affiliation>AIRI, Artificial Intelligence Research Institute and Moscow Institute of Physics and Technology</affiliation></author>
      <author><first>Mikhail</first><last>Arkhipov</last></author>
      <author><first>Aydar</first><last>Bulatov</last></author>
      <author><first>Mikhail</first><last>Burtsev</last><affiliation>London Institute for Mathematical Sciences</affiliation></author>
      <pages>19323-19339</pages>
      <abstract>A range of recent works addresses the problem of compression of sequence of tokens into a shorter sequence of real-valued vectors to be used as inputs instead of token embeddings or key-value cache. These approaches are focused on reduction of the amount of compute in existing language models rather than minimization of number of bits needed to store text. Despite relying on powerful models as encoders, the maximum attainable lossless compression ratio is typically not higher than x10. This fact is highly intriguing because, in theory, the maximum information capacity of large real-valued vectors is far beyond the presented rates even for 16-bit precision and a modest vector size. In this work, we explore the limits of compression by replacing the encoder with a per-sample optimization procedure. We show that vectors with compression ratios up to x1500 exist, which highlights two orders of magnitude gap between existing and practically attainable solutions. Furthermore, we empirically show that the compression limits are determined not by the length of the input but by the amount of uncertainty to be reduced, namely, the cross-entropy loss on this sequence without any conditioning. The obtained limits highlight the substantial gap between the theoretical capacity of input embeddings and their practical utilization, suggesting significant room for optimization in model design.</abstract>
      <url hash="ad3c8168">2025.acl-long.948</url>
      <bibkey>kuratov-etal-2025-cramming</bibkey>
    </paper>
    <paper id="949">
      <title>Bi-Tuning with Collaborative Information for Controllable <fixed-case>LLM</fixed-case>-based Sequential Recommendation</title>
      <author><first>Xinyu</first><last>Zhang</last></author>
      <author><first>Linmei</first><last>Hu</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Luhao</first><last>Zhang</last></author>
      <author><first>Wentao</first><last>Cheng</last></author>
      <author><first>Yashen</first><last>Wang</last></author>
      <author><first>Ge</first><last>Shi</last><affiliation>Beijing University of Technology</affiliation></author>
      <author><first>Chong</first><last>Feng</last></author>
      <author><first>Liqiang</first><last>Nie</last><affiliation>Harbin Institute of Technology (Shenzhen) and Shandong University</affiliation></author>
      <pages>19340-19351</pages>
      <abstract>Sequential recommender systems, which leverage historical interactions to deliver targeted recommendations, have been significantly advanced by large language models (LLMs). However, LLM-based generative sequential recommendation often faces two key challenges: the lack of collaborative knowledge and the limited controllability over the generated content. In this paper, we propose a simple Bi-Tuning framework with collaborative information for controllable Large Language Model-based Sequential Recommendation (Laser). Specifically, Bi-Tuning works through incorporating learnable virtual tokens at both the prefix and suffix of the input text, where the prefix tokens enable the adaptation of LLMs with collaborative information, while the suffix token transforms the LLM output into item/user embeddings for similarity comparison, thereby facilitating controllable recommendations. Furthermore, we introduce an MoE-based querying transformer that selectively activates experts to extract relevant information from varying collaborative signals of frozen ID-based recommenders into the prefix, coupled with a multi-task loss function incorporating the MoE load-balancing objective. Finally, a two-phase training strategy is employed to progressively obtain high-quality item and user embeddings through the learnable suffix. Experiments on real-world datasets show that Laser effectively adapts LLMs for sequential recommendation, outperforming state-of-the-art baselines.</abstract>
      <url hash="d2c66868">2025.acl-long.949</url>
      <bibkey>zhang-etal-2025-bi</bibkey>
    </paper>
    <paper id="950">
      <title>A Modular Approach for Clinical <fixed-case>SLM</fixed-case>s Driven by Synthetic Data with Pre-Instruction Tuning, Model Merging, and Clinical-Tasks Alignment</title>
      <author><first>Jean-Philippe</first><last>Corbeil</last><affiliation>Microsoft</affiliation></author>
      <author><first>Amin</first><last>Dada</last></author>
      <author><first>Jean-Michel</first><last>Attendu</last><affiliation>Microsoft</affiliation></author>
      <author><first>Asma</first><last>Ben Abacha</last><affiliation>Microsoft, USA</affiliation></author>
      <author><first>Alessandro</first><last>Sordoni</last><affiliation>Microsoft</affiliation></author>
      <author><first>Lucas</first><last>Caccia</last><affiliation>Microsoft</affiliation></author>
      <author><first>Francois</first><last>Beaulieu</last><affiliation>Microsoft</affiliation></author>
      <author><first>Thomas</first><last>Lin</last><affiliation>Microsoft</affiliation></author>
      <author><first>Jens</first><last>Kleesiek</last><affiliation>Institute for AI in Medicine (IKIM), University Medicine Essen</affiliation></author>
      <author><first>Paul</first><last>Vozila</last><affiliation>Nuance Communications</affiliation></author>
      <pages>19352-19374</pages>
      <abstract>High computation costs and latency of large language models such as GPT-4 have limited their deployment in clinical settings. Small language models (SLMs) offer a cost-effective alternative, but their limited capacity requires biomedical domain adaptation, which remains challenging. An additional bottleneck is the unavailability and high sensitivity of clinical data. To address these challenges, we propose a novel framework for adapting SLMs into high-performing clinical models. We introduce the MediPhi collection of 3.8B-parameter SLMs developed with our novel framework: pre-instruction tuning of experts on relevant medical and clinical corpora (PMC, Medical Guideline, MedWiki, etc.), model merging, and clinical-tasks alignment. To cover most clinical tasks, we extended the CLUE benchmark to CLUE+, doubling its size. Our expert models deliver relative improvements on this benchmark over the base model without any task-specific fine-tuning: 64.3% on medical entities, 49.5% on radiology reports, and 44% on ICD-10 coding (outperforming GPT-4-0125 by 14%). We unify the expert models into MediPhi via model merging, preserving gains across benchmarks. Furthermore, we built the MediFlow collection, a synthetic dataset of 2.5 million high-quality instructions on 14 medical NLP tasks, 98 fine-grained document types, and JSON format support. Alignment of MediPhi using supervised fine-tuning and direct preference optimization achieves further gains of 18.9% on average.</abstract>
      <url hash="15477385">2025.acl-long.950</url>
      <bibkey>corbeil-etal-2025-modular</bibkey>
    </paper>
    <paper id="951">
      <title><fixed-case>DIVE</fixed-case> into <fixed-case>M</fixed-case>o<fixed-case>E</fixed-case>: Diversity-Enhanced Reconstruction of Large Language Models from Dense into Mixture-of-Experts</title>
      <author><first>Yuchen</first><last>Feng</last><affiliation>University of the Chinese Academy of Sciences</affiliation></author>
      <author><first>Bowen</first><last>Shen</last><affiliation>University of the Chinese Academy of Sciences</affiliation></author>
      <author><first>Naibin</first><last>Gu</last></author>
      <author><first>Jiaxuan</first><last>Zhao</last></author>
      <author><first>Peng</first><last>Fu</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <author><first>Zheng</first><last>Lin</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <author><first>Weiping</first><last>Wang</last><affiliation>IIE</affiliation></author>
      <pages>19375-19394</pages>
      <abstract>Large language models (LLMs) with the Mixture-of-Experts (MoE) architecture achieve high cost-efficiency by selectively activating a subset of the parameters. Despite the inference efficiency of MoE LLMs, the training of extensive experts from scratch incurs substantial overhead, whereas reconstructing a dense LLM into an MoE LLM significantly reduces the training budget. However, existing reconstruction methods often overlook the diversity among experts, leading to potential redundancy. In this paper, we come up with the observation that a specific LLM exhibits notable diversity after being pruned on different calibration datasets, based on which we present a Diversity-Enhanced reconstruction method named DIVE. The recipe of DIVE includes domain affinity mining, pruning-based expert reconstruction, and efficient retraining. Specifically, the reconstruction includes pruning and reassembly of the feed-forward network (FFN) module. After reconstruction, we efficiently retrain the model on routers, experts and normalization modules. We implement DIVE on Llama-style LLMs with open-source training corpora. Experiments show that DIVE achieves training efficiency with minimal accuracy trade-offs, outperforming existing pruning and MoE reconstruction methods with the same number of activated parameters. Code is available at: https://github.com/yuchenblah/DIVE.</abstract>
      <url hash="1350a811">2025.acl-long.951</url>
      <bibkey>feng-etal-2025-dive</bibkey>
    </paper>
    <paper id="952">
      <title><fixed-case>DAC</fixed-case>: A Dynamic Attention-aware Approach for Task-Agnostic Prompt Compression</title>
      <author><first>Yi</first><last>Zhao</last></author>
      <author><first>Zuchao</first><last>Li</last></author>
      <author><first>Hai</first><last>Zhao</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Baoyuan</first><last>Qi</last><affiliation>Xiaomi Corporation</affiliation></author>
      <author><first>Liu</first><last>Guoming</last><affiliation>Xiaomi Corporation</affiliation></author>
      <pages>19395-19407</pages>
      <abstract>Task-agnostic prompt compression leverages the redundancy in natural language to reduce computational overhead and enhance information density within prompts, especially in long-context scenarios. Existing methods predominantly rely on information entropy as the metric to compress lexical units, aiming to achieve minimal information loss. However, these approaches overlook two critical aspects: (i) the importance of attention-critical tokens at the algorithmic level, and (ii) shifts in information entropy during the compression process. Motivated by these challenges, we propose a dynamic attention-aware approach for task-agnostic prompt compression (DAC). This approach effectively integrates entropy and attention information, dynamically sensing entropy shifts during compression to achieve fine-grained prompt compression. Extensive experiments across various domains, including LongBench, GSM8K, and BBH, show that DAC consistently yields robust and substantial improvements across a diverse range of tasks and LLMs, offering compelling evidence of its efficacy.</abstract>
      <url hash="0cb30e1c">2025.acl-long.952</url>
      <bibkey>zhao-etal-2025-dac</bibkey>
    </paper>
    <paper id="953">
      <title>Computation Mechanism Behind <fixed-case>LLM</fixed-case> Position Generalization</title>
      <author><first>Chi</first><last>Han</last></author>
      <author><first>Heng</first><last>Ji</last><affiliation>University of Illinois, Urbana-Champaign</affiliation></author>
      <pages>19408-19424</pages>
      <abstract>Most written natural languages are composed of sequences of words and sentences. Similar to humans, large language models (LLMs) exhibit flexibility in handling textual positions - a phenomenon we term Position Generalization. They can understand texts with position perturbations and generalize to longer texts than those encountered during training with the latest techniques. These phenomena suggest that LLMs handle positions in a tolerant manner, but how LLMs computationally process positional relevance remains largely unexplored. In this work, we show how LLMs enforce certain computational mechanisms to allow for the aforementioned tolerance in position perturbations. Despite the complex design of the self-attention mechanism, in this work, LLMs are revealed to learn a counterintuitive disentanglement of attention logits, where their values show a 0.959 linear correlation with an approximation of the arithmetic sum of positional relevance and semantic importance. Furthermore, we identify a prevalent pattern in intermediate features that enables this effect, suggesting that it is a learned behavior rather than a natural result of the model architecture. Based on these findings, we provide computational explanations and criteria for the aforementioned position flexibilities observed in LLMs.</abstract>
      <url hash="c01bfd1b">2025.acl-long.953</url>
      <bibkey>han-ji-2025-computation</bibkey>
    </paper>
    <paper id="954">
      <title><fixed-case>IPO</fixed-case>: Your Language Model is Secretly a Preference Classifier</title>
      <author><first>Shivank</first><last>Garg</last></author>
      <author><first>Ayush</first><last>Singh</last></author>
      <author><first>Shweta</first><last>Singh</last></author>
      <author><first>Paras</first><last>Chopra</last><affiliation>Lossfunk</affiliation></author>
      <pages>19425-19441</pages>
      <abstract>Reinforcement learning from human feedback (RLHF) has emerged as the primary method for aligning large language models (LLMs) with human preferences. While it enables LLMs to achieve human-level alignment, it often incurs significant computational and financial costs due to its reliance on training external reward models or human-labeled preferences. In this work, we propose Implicit Preference Optimization (IPO), an alternative approach that leverages generative LLMs as preference classifiers, thereby reducing the dependence on external human feedback or reward models to obtain preferences. We conduct a comprehensive evaluation on the preference classification ability of LLMs using RewardBench, assessing models across different sizes, architectures, and training levels to validate our hypothesis. Furthermore, we investigate the self-improvement capabilities of LLMs by generating multiple responses for a given instruction and employing the model itself as a preference classifier for Direct Preference Optimization (DPO)-based training. Our findings demonstrate that models trained through IPO achieve performance comparable to those utilizing state-of-the-art reward models for obtaining preferences.</abstract>
      <url hash="61b9a7d7">2025.acl-long.954</url>
      <bibkey>garg-etal-2025-ipo</bibkey>
    </paper>
    <paper id="955">
      <title>Reversal of Thought: Enhancing Large Language Models with Preference-Guided Reverse Reasoning Warm-up</title>
      <author><first>Jiahao</first><last>Yuan</last></author>
      <author><first>Dehui</first><last>Du</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Hao</first><last>Zhang</last></author>
      <author><first>Zixiang</first><last>Di</last></author>
      <author><first>Usman</first><last>Naseem</last><affiliation>Macquarie University</affiliation></author>
      <pages>19442-19459</pages>
      <abstract>Large language models (LLMs) have shown remarkable performance in reasoning tasks but face limitations in mathematical and complex logical reasoning. Existing methods to improve LLMs’ logical capabilities either involve traceable or verifiable logical sequences that generate more reliable responses by constructing logical structures yet increase computational costs, or introduces rigid logic template rules, reducing flexibility. In this paper, we propose Reversal of Thought (RoT), a plug-and-play and cost-effective reasoning framework designed to enhance the logical reasoning abilities of LLMs during the warm-up phase prior to batch inference. RoT utilizes a Preference-Guided Reverse Reasoning warm-up strategy, which integrates logical symbols for pseudocode planning through meta-cognitive mechanisms and pairwise preference self-evaluation to generate task-specific prompts solely through demonstrations, aligning with LLMs’ cognitive preferences shaped by RLHF. Through reverse reasoning, we utilize a Cognitive Preference Manager to assess knowledge boundaries and further expand LLMs’ reasoning capabilities by aggregating solution logic for known tasks and stylistic templates for unknown tasks. Experiments across various tasks demonstrate that RoT surpasses existing baselines in both reasoning accuracy and efficiency.</abstract>
      <url hash="23f7ab84">2025.acl-long.955</url>
      <bibkey>yuan-etal-2025-reversal</bibkey>
    </paper>
    <paper id="956">
      <title>Déjà Vu? Decoding Repeated Reading from Eye Movements</title>
      <author><first>Yoav</first><last>Meiri</last></author>
      <author><first>Omer</first><last>Shubi</last></author>
      <author><first>Cfir Avraham</first><last>Hadar</last></author>
      <author><first>Ariel Kreisberg</first><last>Nitzav</last></author>
      <author><first>Yevgeni</first><last>Berzak</last><affiliation>Technion - Israel Institute of Technology, Technion</affiliation></author>
      <pages>19460-19482</pages>
      <abstract>Be it your favorite novel, a newswire article, a cooking recipe or an academic paper – in many daily situations we read the same text more than once. In this work, we ask whether it is possible to automatically determine whether the reader has previously encountered a text based on their eye movement patterns during reading. We introduce two variants of this task and address them using both feature-based and neural models. We further introduce a general strategy for enhancing these models with machine generated simulations of eye movements from a cognitive model. Finally, we present an analysis of model performance which on the one hand yields insights on the information used by the models, and on the other hand leverages predictive modeling as an analytic tool for better characterization of the role of memory in repeated reading. Our work advances the understanding of the extent and manner in which eye movements in reading capture memory effects from prior text exposure, and paves the way for future applications that involve predictive modeling of repeated reading.</abstract>
      <url hash="12c7fb0f">2025.acl-long.956</url>
      <bibkey>meiri-etal-2025-deja</bibkey>
    </paper>
    <paper id="957">
      <title><fixed-case>LLM</fixed-case>s can be easily Confused by Instructional Distractions</title>
      <author><first>Yerin</first><last>Hwang</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Yongil</first><last>Kim</last><affiliation>LG Corporation</affiliation></author>
      <author><first>Jahyun</first><last>Koo</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Taegwan</first><last>Kang</last><affiliation>LG Corporation</affiliation></author>
      <author><first>Hyunkyung</first><last>Bae</last><affiliation>New York University and LG AI Research</affiliation></author>
      <author><first>Kyomin</first><last>Jung</last></author>
      <pages>19483-19496</pages>
      <abstract>Despite the fact that large language models (LLMs) show exceptional skill in instruction following tasks, this strength can turn into a vulnerability when the models are required to disregard certain instructions. Instruction following tasks typically involve a clear task description and input text containing the target data to be processed. However, when the input itself resembles an instruction, confusion may arise, even if there is explicit prompting to distinguish between the task instruction and the input. We refer to this phenomenon as instructional distraction. In this paper, we introduce a novel benchmark, named **DIM-Bench**, specifically designed to assess LLMs’ performance under instructional distraction. The benchmark categorizes real-world instances of instructional distraction and evaluates LLMs across four instruction tasks: proofreading, rewriting, translation, and style transfer—alongside five input tasks: reasoning, code generation, mathematical reasoning, bias detection, and question answering. Our experimental results reveal that even the most advanced LLMs are susceptible to instructional distraction, often failing to accurately follow user intent in such cases.</abstract>
      <url hash="d6d4acf0">2025.acl-long.957</url>
      <bibkey>hwang-etal-2025-llms</bibkey>
    </paper>
    <paper id="958">
      <title><fixed-case>P</fixed-case>lan<fixed-case>G</fixed-case>en<fixed-case>LLM</fixed-case>s: A Modern Survey of <fixed-case>LLM</fixed-case> Planning Capabilities</title>
      <author><first>Hui</first><last>Wei</last></author>
      <author><first>Zihao</first><last>Zhang</last><affiliation>Emory University</affiliation></author>
      <author><first>Shenghua</first><last>He</last><affiliation>PAII INC</affiliation></author>
      <author><first>Tian</first><last>Xia</last></author>
      <author><first>Shijia</first><last>Pan</last><affiliation>University of California, Merced</affiliation></author>
      <author id="fei-liu"><first>Fei</first><last>Liu</last><affiliation>Emory University</affiliation></author>
      <pages>19497-19521</pages>
      <abstract>LLMs have immense potential for generating plans, transforming an initial world state into a desired goal state. A large body of research has explored the use of LLMs for various planning tasks, from web navigation to travel planning and database querying. However, many of these systems are tailored to specific problems, making it challenging to compare them or determine the best approach for new tasks. There is also a lack of clear and consistent evaluation criteria. Our survey aims to offer a comprehensive overview of current LLM planners to fill this gap. It builds on foundational work by Kartam and Wilkins (1990) and examines six key performance criteria: completeness, executability, optimality, representation, generalization, and efficiency. For each, we provide a thorough analysis of representative works and highlight their strengths and weaknesses. Our paper also identifies crucial future directions, making it a valuable resource for both practitioners and newcomers interested in leveraging LLM planning to support agentic workflows.</abstract>
      <url hash="5dce9268">2025.acl-long.958</url>
      <bibkey>wei-etal-2025-plangenllms</bibkey>
    </paper>
    <paper id="959">
      <title><fixed-case>IAM</fixed-case>: Efficient Inference through Attention Mapping between Different-scale <fixed-case>LLM</fixed-case>s</title>
      <author><first>Yi</first><last>Zhao</last></author>
      <author><first>Zuchao</first><last>Li</last></author>
      <author><first>Hai</first><last>Zhao</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>19522-19533</pages>
      <abstract>LLMs encounter significant challenges in resource consumption nowadays, especially with long contexts. Despite extensive efforts dedicate to enhancing inference efficiency, these methods primarily exploit internal sparsity within the models, without leveraging external information for optimization. We identify the high similarity of attention matrices across different-scale LLMs, which offers a novel perspective for optimization. We first conduct a comprehensive analysis of how to measure similarity, how to select mapping Layers and whether mapping is consistency. Based on these insights, we introduce the IAM framework, which achieves dual benefits of accelerated attention computation and reduced KV cache usage by performing attention mapping between small and large LLMs. Our experimental results demonstrate that IAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without appreciably sacrificing performance. Experiments on different series of models show the generalizability of IAM. Importantly, it is also orthogonal to many existing KV cache optimization methods, making it a versatile addition to the current toolkit for enhancing LLM efficiency.</abstract>
      <url hash="16f78a5c">2025.acl-long.959</url>
      <bibkey>zhao-etal-2025-iam</bibkey>
    </paper>
    <paper id="960">
      <title>nv<fixed-case>A</fixed-case>gent: Automated Data Visualization from Natural Language via Collaborative Agent Workflow</title>
      <author><first>Geliang</first><last>Ouyang</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Jingyao</first><last>Chen</last></author>
      <author><first>Zhihe</first><last>Nie</last></author>
      <author><first>Yi</first><last>Gui</last></author>
      <author><first>Yao</first><last>Wan</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Hongyu</first><last>Zhang</last><affiliation>Chongqing University</affiliation></author>
      <author><first>Dongping</first><last>Chen</last></author>
      <pages>19534-19567</pages>
      <abstract>*Natural Language to Visualization* (NL2Vis) seeks to convert natural-language descriptions into visual representations of given tables, empowering users to derive insights from large-scale data. Recent advancements in *Large Language Models* (LLMs) show promise in automating code generation to transform tabular data into accessible visualizations. However, they often struggle with complex queries that require reasoning across multiple tables. To address this limitation, we propose a collaborative agent workflow, termed **nvAgent**, for NL2Vis. Specifically, **nvAgent** comprises three agents: a processor agent for database processing and context filtering, a composer agent for planning visualization generation, and a validator agent for code translation and output verification. Comprehensive evaluations on the new VisEval benchmark demonstrate that **nvAgent** consistently surpasses state-of-the-art baselines, achieving a 7.88% improvement in single-table and a 9.23% improvement in multi-table scenarios. Qualitative analyses further highlight that **nvAgent** maintains nearly a 20% performance margin over previous models, underscoring its capacity to produce high-quality visual representations from complex, heterogeneous data sources. All datasets and source code are available at: [https://github.com/geliang0114/nvAgent](https://github.com/geliang0114/nvAgent).</abstract>
      <url hash="cf4029ae">2025.acl-long.960</url>
      <bibkey>ouyang-etal-2025-nvagent</bibkey>
    </paper>
    <paper id="961">
      <title><fixed-case>ZIPA</fixed-case>: A family of efficient models for multilingual phone recognition</title>
      <author><first>Jian</first><last>Zhu</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Farhan</first><last>Samir</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Eleanor</first><last>Chodroff</last><affiliation>University of Zurich</affiliation></author>
      <author><first>David R.</first><last>Mortensen</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>19568-19585</pages>
      <abstract>We present ZIPA, a family of efficient speech models that advances the state-of-the-art performance of crosslinguistic phone recognition. We first curated IPA PACK++, a large-scale multilingual speech corpus with 17,000+ hours of normalized phone transcriptions and a novel evaluation set capturing unseen languages and sociophonetic variation. ZIPA, including transducer (ZIPA-T) and CTC-based (ZIPA-CR) variants, leverages the efficient Zipformer backbones and outperforms existing phone recognition systems with much fewer parameters. Further scaling via noisy student training on 11,000+ hours of pseudo-labeled multilingual data yields further improvement. While ZIPA achieves strong performance on benchmarks, error analysis reveals persistent limitations in modeling sociophonetic diversity, underscoring challenges for future research.</abstract>
      <url hash="ac61cdcd">2025.acl-long.961</url>
      <bibkey>zhu-etal-2025-zipa</bibkey>
    </paper>
    <paper id="962">
      <title><fixed-case>GRACE</fixed-case>: A Granular Benchmark for Evaluating Model Calibration against Human Calibration</title>
      <author><first>Yoo Yeon</first><last>Sung</last></author>
      <author><first>Eve</first><last>Fleisig</last></author>
      <author><first>Yu</first><last>Hou</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Ishan</first><last>Upadhyay</last></author>
      <author><first>Jordan Lee</first><last>Boyd-Graber</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>19586-19587</pages>
      <abstract>Language models are often miscalibrated, leading to confidently incorrect answers. We introduce GRACE, a benchmark for language model calibration that incorporates comparison with human calibration. GRACE consists of question-answer pairs, in which each question contains a series of clues that gradually become easier, all leading to the same answer; models must answer correctly as early as possible as the clues are revealed. This setting permits granular measurement of model calibration based on how early, accurately, and confidently a model answers. After collecting these questions, we host live human vs. model competitions to gather 1,749 data points on human and model teams’ timing, accuracy, and confidence. We propose a metric, CalScore, that uses GRACE to analyze model calibration errors and identify types of model miscalibration that differ from human behavior. We find that although humans are less accurate than models, humans are generally better calibrated. Since state-of-the-art models struggle on GRACE, it effectively evaluates progress on improving model calibration.</abstract>
      <url hash="3b9e4037">2025.acl-long.962</url>
      <bibkey>sung-etal-2025-grace</bibkey>
    </paper>
    <paper id="963">
      <title>Dynamic Evaluation with Cognitive Reasoning for Multi-turn Safety of Large Language Models</title>
      <author><first>Lanxue</first><last>Zhang</last></author>
      <author><first>Yanan</first><last>Cao</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yuqiang</first><last>Xie</last></author>
      <author><first>Fang</first><last>Fang</last><affiliation>Institute of Information Engineering,Chinese Academy of Sciences</affiliation></author>
      <author><first>Yangxi</first><last>Li</last></author>
      <pages>19588-19608</pages>
      <abstract>The rapid advancement of Large Language Models (LLMs) poses significant challenges for safety evaluation. Current static datasets struggle to identify emerging vulnerabilities due to three limitations: (1) they risk being exposed in model training data, leading to evaluation bias; (2) their limited prompt diversity fails to capture real-world application scenarios; (3) they are limited to provide human-like multi-turn interactions. To address these limitations, we propose a dynamic evaluation framework, CogSafe, for comprehensive and automated multi-turn safety assessment of LLMs. We introduce CogSafe based on cognitive theories to simulate the real chatting process. To enhance assessment diversity, we introduce scenario simulation and strategy decision to guide the dynamic generation, enabling coverage of application situations. Furthermore, we incorporate the cognitive process to simulate multi-turn dialogues that reflect the cognitive dynamics of real-world interactions. Extensive experiments demonstrate the scalability and effectiveness of our framework, which has been applied to evaluate the safety of widely used LLMs.</abstract>
      <url hash="03babdcd">2025.acl-long.963</url>
      <bibkey>zhang-etal-2025-dynamic-evaluation</bibkey>
    </paper>
    <paper id="964">
      <title>From Tools to Teammates: Evaluating <fixed-case>LLM</fixed-case>s in Multi-Session Coding Interactions</title>
      <author><first>Nathanaël Carraz</first><last>Rakotonirina</last></author>
      <author><first>Mohammed</first><last>Hamdy</last><affiliation>Independent</affiliation></author>
      <author><first>Jon Ander</first><last>Campos</last><affiliation>Cohere</affiliation></author>
      <author><first>Lucas</first><last>Weber</last><affiliation>Fraunhofer IIS</affiliation></author>
      <author><first>Alberto</first><last>Testoni</last><affiliation>Amsterdam UMC</affiliation></author>
      <author><first>Marzieh</first><last>Fadaee</last><affiliation>Cohere Labs</affiliation></author>
      <author><first>Sandro</first><last>Pezzelle</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Marco</first><last>Del Tredici</last><affiliation>Amazon</affiliation></author>
      <pages>19609-19642</pages>
      <abstract>Large Language Models (LLMs) are increasingly used in working environments for a wide range of tasks, excelling at solving individual problems in isolation. However, are they also able to effectively collaborate over long-term interactions? To investigate this, we introduce MemoryCode, a synthetic multi-session dataset designed to test LLMs’ ability to track and execute simple coding instructions amid irrelevant information, simulating a realistic setting. While all the models we tested handle isolated instructions well, even the performance of state-of-the-art models like GPT-4o deteriorates when instructions are spread across sessions. Our analysis suggests this is due to their failure to retrieve and integrate information over long interaction chains. Our results highlight a fundamental limitation of current LLMs, restricting their ability to collaborate effectively in long interactions.</abstract>
      <url hash="eca4c909">2025.acl-long.964</url>
      <bibkey>rakotonirina-etal-2025-tools</bibkey>
    </paper>
    <paper id="965">
      <title>Guiding not Forcing: Enhancing the Transferability of Jailbreaking Attacks on <fixed-case>LLM</fixed-case>s via Removing Superfluous Constraints</title>
      <author><first>Junxiao</first><last>Yang</last></author>
      <author><first>Zhexin</first><last>Zhang</last></author>
      <author><first>Shiyao</first><last>Cui</last></author>
      <author><first>Hongning</first><last>Wang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <pages>19643-19655</pages>
      <abstract>Jailbreaking attacks can effectively induce unsafe behaviors in Large Language Models (LLMs); however, the transferability of these attacks across different models remains limited. This study aims to understand and enhance the transferability of gradient-based jailbreaking methods, which are among the standard approaches for attacking white-box models. Through a detailed analysis of the optimization process, we introduce a novel conceptual framework to elucidate transferability and identify superfluous constraints—specifically, the response pattern constraint and the token tail constraint—as significant barriers to improved transferability. Removing these unnecessary constraints substantially enhances the transferability and controllability of gradient-based attacks. Evaluated on Llama-3-8B-Instruct as the source model, our method increases the overall Transfer Attack Success Rate (T-ASR) across a set of target models with varying safety levels from 18.4% to 50.3%, while also improving the stability and controllability of jailbreak behaviors on both source and target models.</abstract>
      <url hash="1f9ee785">2025.acl-long.965</url>
      <bibkey>yang-etal-2025-guiding</bibkey>
    </paper>
    <paper id="966">
      <title>Multilingual Text-to-Image Generation Magnifies Gender Stereotypes</title>
      <author><first>Felix</first><last>Friedrich</last></author>
      <author><first>Katharina</first><last>Hämmerl</last></author>
      <author><first>Patrick</first><last>Schramowski</last><affiliation>German Research Center for AI</affiliation></author>
      <author><first>Manuel</first><last>Brack</last><affiliation>German Research Center for AI and Technische Universität Darmstadt</affiliation></author>
      <author><first>Jindřich</first><last>Libovický</last><affiliation>Charles University Prague</affiliation></author>
      <author><first>Alexander</first><last>Fraser</last><affiliation>Technical University of Munich</affiliation></author>
      <author><first>Kristian</first><last>Kersting</last><affiliation>German Research Center for AI, The Hessian Center for AI and TU Darmstadt</affiliation></author>
      <pages>19656-19679</pages>
      <abstract>Text-to-image (T2I) generation models have achieved great results in image quality, flexibility, and text alignment, leading to widespread use. Through improvements in multilingual abilities, a larger community can access this technology. Yet, we show that multilingual models suffer from substantial gender bias. Furthermore, the expectation that results should be similar across languages does not hold. We introduce MAGBIG, a controlled benchmark designed to study gender bias in multilingual T2I models, and use it to assess the impact of multilingualism on gender bias. To this end, we construct a set of multilingual prompts that offers a carefully controlled setting accounting for the complex grammatical differences influencing gender across languages. Our results show strong gender biases and notable language-specific differences across models. While we explore prompt engineering strategies to mitigate these biases, we find them largely ineffective and sometimes even detrimental to text-to-image alignment. Our analysis highlights the need for research on diverse language representations and greater control over bias in T2I models.</abstract>
      <url hash="851a87f3">2025.acl-long.966</url>
      <bibkey>friedrich-etal-2025-multilingual</bibkey>
    </paper>
    <paper id="967">
      <title>Adversarial Alignment with Anchor Dragging Drift (<tex-math>A^3D^2</tex-math>): Multimodal Domain Adaptation with Partially Shifted Modalities</title>
      <author><first>Jun</first><last>Sun</last></author>
      <author><first>Xinxin</first><last>Zhang</last></author>
      <author><first>Simin</first><last>Hong</last></author>
      <author><first>Jian</first><last>Zhu</last></author>
      <author><first>Lingfang</first><last>Zeng</last></author>
      <pages>19680-19690</pages>
      <abstract>Multimodal learning has celebrated remarkable success across diverse areas, yet faces the challenge of prohibitively expensive data collection and annotation when adapting models to new environments. In this context, domain adaptation has gained growing popularity as a technique for knowledge transfer, which, however, remains underexplored in multimodal settings compared with unimodal ones. This paper investigates multimodal domain adaptation, focusing on a practical partially shifting scenario where some modalities (referred to as anchors) remain domain-stable, while others (referred to as drifts) undergo a domain shift. We propose a bi-alignment scheme to simultaneously perform drift-drift and anchor-drift matching. The former is achieved through adversarial learning, aligning the representations of the drifts across source and target domains; the latter corresponds to an “anchor dragging drift” strategy, which matches the distributions of the drifts and anchors within the target domain using the optimal transport (OT) method. The overall design principle features <b>A</b>dversarial <b>A</b>lignment with <b>A</b>nchor <b>D</b>ragging <b>D</b>rift, abbreviated as <b>
          <tex-math>A^3D^2</tex-math></b>, for multimodal domain adaptation with partially shifted modalities. Comprehensive empirical results verify the effectiveness of the proposed approach, and demonstrate that <tex-math>A^3D^2</tex-math> achieves superior performance compared with state-of-the-art approaches. The code is available at: <url>https://github.com/sunjunaimer/A3D2.git</url>.</abstract>
      <url hash="b7e4d411">2025.acl-long.967</url>
      <bibkey>sun-etal-2025-adversarial</bibkey>
    </paper>
    <paper id="968">
      <title>A Reality Check on Context Utilisation for Retrieval-Augmented Generation</title>
      <author><first>Lovisa</first><last>Hagström</last><affiliation>Chalmers University of Technology</affiliation></author>
      <author><first>Sara Vera</first><last>Marjanovic</last></author>
      <author><first>Haeun</first><last>Yu</last></author>
      <author><first>Arnav</first><last>Arora</last><affiliation>University of Copenhagen</affiliation></author>
      <author><first>Christina</first><last>Lioma</last><affiliation>University of Copenhagen</affiliation></author>
      <author><first>Maria</first><last>Maistro</last><affiliation>University of Copenhagen</affiliation></author>
      <author><first>Pepa</first><last>Atanasova</last><affiliation>University of Copenhagen</affiliation></author>
      <author><first>Isabelle</first><last>Augenstein</last><affiliation>University of Copenhagen</affiliation></author>
      <pages>19691-19730</pages>
      <abstract>Retrieval-augmented generation (RAG) helps address the limitations of parametric knowledge embedded within a language model (LM). In real world settings, retrieved information can vary in complexity, yet most investigations of LM utilisation of context has been limited to synthetic text. We introduce DRUID (Dataset of Retrieved Unreliable, Insufficient and Difficult-to-understand contexts) with real-world queries and contexts manually annotated for stance. The dataset is based on the prototypical task of automated claim verification, for which automated retrieval of real-world evidence is crucial. We compare DRUID to synthetic datasets (CounterFact, ConflictQA) and find that artificial datasets often fail to represent the complexity and diversity of realistically retrieved context. We show that synthetic datasets exaggerate context characteristics rare in real retrieved data, which leads to inflated context utilisation results, as measured by our novel ACU score. Moreover, while previous work has mainly focused on singleton context characteristics to explain context utilisation, correlations between singleton context properties and ACU on DRUID are surprisingly small compared to other properties related to context source. Overall, our work underscores the need for real-world aligned context utilisation studies to represent and improve performance in real-world RAG settings.</abstract>
      <url hash="1cd11dbb">2025.acl-long.968</url>
      <bibkey>hagstrom-etal-2025-reality</bibkey>
    </paper>
    <paper id="969">
      <title><fixed-case>CU</fixed-case>-<fixed-case>MAM</fixed-case>: Coherence-Driven Unified Macro-Structures for Argument Mining</title>
      <author><first>Debela</first><last>Gemechu</last></author>
      <author><first>Chris</first><last>Reed</last><affiliation>University of Dundee</affiliation></author>
      <pages>19731-19749</pages>
      <abstract>Argument Mining (AM) involves the automatic identification of argument structure in natural language. Traditional AM methods rely on micro-structural features derived from the internal properties of individual Argumentative Discourse Units (ADUs). However, argument structure is shaped by a macro-structure capturing the functional interdependence among ADUs. This macro-structure consists of segments, where each segment contains ADUs that fulfill specific roles to maintain coherence within the segment (**local coherence**) and across segments (**global coherence**). This paper presents an approach that models macro-structure, capturing both local and global coherence to identify argument structures. Experiments on heterogeneous datasets demonstrate superior performance in both in-dataset and cross-dataset evaluations. The cross-dataset evaluation shows that macro-structure enhances transferability to unseen datasets.</abstract>
      <url hash="b2a7224b">2025.acl-long.969</url>
      <bibkey>gemechu-reed-2025-cu</bibkey>
    </paper>
    <paper id="970">
      <title>Safer or Luckier? <fixed-case>LLM</fixed-case>s as Safety Evaluators Are Not Robust to Artifacts</title>
      <author><first>Hongyu</first><last>Chen</last><affiliation>Cohere</affiliation></author>
      <author><first>Seraphina</first><last>Goldfarb-Tarrant</last><affiliation>Cohere</affiliation></author>
      <pages>19750-19766</pages>
      <abstract>Large Language Models (LLMs) are increasingly employed as automated evaluators to assess the safety of generated content, yet their reliability in this role remains uncertain. This study evaluates a diverse set of 11 LLM judge models across critical safety domains, examining three key aspects: self-consistency in repeated judging tasks, alignment with human judgments, and susceptibility to input artifacts such as apologetic or verbose phrasing. Our findings reveal that biases in LLM judges can significantly distort the final verdict on which content source is safer, undermining the validity of comparative evaluations. Notably, apologetic language artifacts alone can skew evaluator preferences by up to 98%. Contrary to expectations, larger models do not consistently exhibit greater robustness, while smaller models sometimes show higher resistance to specific artifacts. To mitigate LLM evaluator robustness issues, we investigate jury-based evaluations aggregating decisions from multiple models. Although this approach both improves robustness and enhances alignment to human judgements, artifact sensitivity persists even with the best jury configurations. These results highlight the urgent need for diversified, artifact-resistant methodologies to ensure reliable safety assessments.</abstract>
      <url hash="bbafee0e">2025.acl-long.970</url>
      <bibkey>chen-goldfarb-tarrant-2025-safer</bibkey>
    </paper>
    <paper id="971">
      <title>Text-to-<fixed-case>ES</fixed-case> Bench: A Comprehensive Benchmark for Converting Natural Language to <fixed-case>E</fixed-case>lasticsearch Query</title>
      <author><first>DonggeXue</first><last>DonggeXue</last></author>
      <author><first>Zhili</first><last>Pu</last></author>
      <author><first>Zhentao</first><last>Xia</last></author>
      <author><first>Hongli</first><last>Sun</last><affiliation>East China University of Science and Technology</affiliation></author>
      <author><first>Ruihui</first><last>Hou</last></author>
      <author><first>Guangya</first><last>Yu</last></author>
      <author><first>Yupian</first><last>Lin</last></author>
      <author><first>Yongqi</first><last>Fan</last></author>
      <author><first>Jingping</first><last>Liu</last><affiliation>East China University of Science and Technology</affiliation></author>
      <author><first>Tong</first><last>Ruan</last></author>
      <pages>19767-19790</pages>
      <abstract>Elasticsearch (ES) is a distributed RESTful search engine optimized for large-scale and long-text search scenarios. Recent research on text-to-Query has explored using large language models (LLMs) to convert user query intent to executable code, making it an increasingly popular research topic. To our knowledge, we are the first to introduce the novel semantic parsing task text-to-ES. To bridge the gap between LLM and ES, in detail, we leverage LLMs and employ domain experts to generate ES query bodies, which are Domain-Specific Language (DSL), along with the corresponding post-processing code to support multi-index ES queries. Consequently, we propose the text-to-ES benchmark that consists of two datasets: Large Elasticsearch Dataset (LED), containing 26,207 text-ES pairs derived from a 224.9GB schema-free database, and ElasticSearch (BirdES)with 10,926 pairs sourced from the Bird dataset on a 33.4GB schema-fixed database. Compared with fourteen advanced LLMs and six code-based LLMs, the model we trained outperformed DeepSeek-R1 by 15.64% on the LED dataset, setting a new state-of-the-art, and achieved 78% of DeepSeek-R1’s performance on the BirdES dataset. Additionally, we provide in-depth experimental analyses and suggest future research directions for this task. Our datasets are available at https://huggingface.co/datasets/Barry1915/Text-to-ES.</abstract>
      <url hash="35881531">2025.acl-long.971</url>
      <bibkey>donggexue-etal-2025-text</bibkey>
    </paper>
    <paper id="972">
      <title><fixed-case>A</fixed-case>lign<fixed-case>D</fixed-case>istil: Token-Level Language Model Alignment as Adaptive Policy Distillation</title>
      <author><first>Songming</first><last>Zhang</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Xue</first><last>Zhang</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Tong</first><last>Zhang</last></author>
      <author><first>Bojie</first><last>Hu</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Yufeng</first><last>Chen</last></author>
      <author><first>Jinan</first><last>Xu</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <pages>19791-19807</pages>
      <abstract>In modern large language models (LLMs), LLM alignment is of crucial importance and is typically achieved through methods such as reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO). However, in most existing methods for LLM alignment, all tokens in the response are optimized using a sparse, response-level reward or preference annotation. The ignorance of token-level rewards may erroneously punish high-quality tokens or encourage low-quality tokens, resulting in suboptimal performance and slow convergence speed. To address this issue, we propose <i>
          <b>AlignDistil</b></i>, a RLHF-equivalent distillation method for token-level reward optimization. Specifically, we introduce the reward learned by DPO into the RLHF objective and theoretically prove the equivalence between this objective and a token-level distillation process, where the teacher distribution linearly combines the logits from the DPO model and a reference model. On this basis, we further bridge the accuracy gap between the reward from the DPO model and the pure reward model, by building a contrastive DPO reward with a normal and a reverse DPO model. Moreover, to avoid under- and over-optimization on different tokens, we design a token adaptive logit extrapolation mechanism to construct an appropriate teacher distribution for each token. Experimental results demonstrate the superiority of our AlignDistil over existing methods and showcase fast convergence due to its token-level distributional reward optimization.</abstract>
      <url hash="abde7007">2025.acl-long.972</url>
      <bibkey>zhang-etal-2025-aligndistil</bibkey>
    </paper>
    <paper id="973">
      <title><fixed-case>DARS</fixed-case>: Dynamic Action Re-Sampling to Enhance Coding Agent Performance by Adaptive Tree Traversal</title>
      <author><first>Vaibhav</first><last>Aggarwal</last></author>
      <author><first>Ojasv</first><last>Kamal</last><affiliation>Mirelo AI</affiliation></author>
      <author><first>Abhinav</first><last>Japesh</last></author>
      <author><first>Zhijing</first><last>Jin</last><affiliation>Department of Computer Science, University of Toronto</affiliation></author>
      <author><first>Bernhard</first><last>Schölkopf</last><affiliation>ELLIS Institute and Max Planck Institute for Intelligent Systems, Max-Planck Institute</affiliation></author>
      <pages>19808-19855</pages>
      <abstract>Large Language Models (LLMs) have revolutionized various domains, including natural language processing, data analysis, and software development, by enabling automation. In software engineering, LLM-powered coding agents have garnered significant attention due to their potential to automate complex development tasks, assist in debugging, and enhance productivity. However, existing approaches often struggle with sub-optimal decision-making, requiring either extensive manual intervention or inefficient compute scaling strategies. To improve coding agent performance, we present Dynamic Action Re-Sampling (DARS), a novel inference time compute scaling approach for coding agents, that is faster and more effective at recovering from sub-optimal decisions compared to baselines. While traditional agents either follow linear trajectories or rely on random sampling for scaling compute, our approach DARS works by branching out a trajectory at certain key decision points by taking an alternative action given the history of the trajectory and execution feedback of the previous attempt from that point. We evaluate our approach on SWE-Bench Lite benchmark, demonstrating that this scaling strategy achieves a pass@k score of 55% with Claude 3.5 Sonnet V2. Our framework achieves a pass@1 rate of 47%, outperforming state-of-the-art (SOTA) open-source frameworks.</abstract>
      <url hash="555a4c49">2025.acl-long.973</url>
      <bibkey>aggarwal-etal-2025-dars</bibkey>
    </paper>
    <paper id="974">
      <title>Steering off Course: Reliability Challenges in Steering Language Models</title>
      <author><first>Patrick Queiroz Da</first><last>Silva</last></author>
      <author><first>Hari</first><last>Sethuraman</last></author>
      <author><first>Dheeraj</first><last>Rajagopal</last><affiliation>Fastino AI</affiliation></author>
      <author><first>Hannaneh</first><last>Hajishirzi</last></author>
      <author><first>Sachin</first><last>Kumar</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <pages>19856-19882</pages>
      <abstract>Steering methods for language models (LMs) have gained traction as lightweight alternatives to fine-tuning, enabling targeted modifications to model activations. However, prior studies primarily report results on a few models, leaving critical gaps in understanding the robustness of these methods. In this work, we systematically examine three prominent steering methods—DoLa, function vectors, and task vectors. In contrast to the original studies, which evaluated a handful of models, we test up to 36 models belonging to 14 families with sizes ranging from 1.5B to 70B parameters. Our experiments reveal substantial variability in the effectiveness of the steering approaches, with a large number of models showing no improvement and at times degradation in steering performance. Our analysis reveals fundamental flaws in the assumptions underlying these methods, challenging their reliability as scalable steering solutions.</abstract>
      <url hash="bf75cafd">2025.acl-long.974</url>
      <bibkey>silva-etal-2025-steering</bibkey>
    </paper>
    <paper id="975">
      <title>Impartial Multi-task Representation Learning via Variance-invariant Probabilistic Decoding</title>
      <author><first>Dou</first><last>Hu</last></author>
      <author><first>Lingwei</first><last>Wei</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <author><first>Wei</first><last>Zhou</last><affiliation>Institute of Information Engeering</affiliation></author>
      <author><first>Songlin</first><last>Hu</last></author>
      <pages>19883-19897</pages>
      <abstract>Multi-task learning (MTL) enhances efficiency by sharing representations across tasks, but task dissimilarities often cause partial learning, where some tasks dominate while others are neglected. Existing methods mainly focus on balancing loss or gradients but fail to fundamentally address this issue due to the representation discrepancy in latent space. In this paper, we propose variance-invariant probabilistic decoding for multi-task learning (VIP-MTL), a framework that ensures impartial learning by harmonizing representation spaces across tasks. VIP-MTL decodes shared representations into task-specific probabilistic distributions and applies variance normalization to constrain these distributions to a consistent scale. Experiments on two language benchmarks show that VIP-MTL outperforms 12 representative methods under the same multi-task settings, especially in heterogeneous task combinations and data-constrained scenarios. Further analysis shows that VIP-MTL is robust to sampling distributions, efficient on optimization process, and scale-invariant to task losses. Additionally, the learned task-specific representations are more informative, enhancing the language understanding abilities of pre-trained language models under the multi-task paradigm.</abstract>
      <url hash="85e8e1fd">2025.acl-long.975</url>
      <bibkey>hu-etal-2025-impartial</bibkey>
    </paper>
    <paper id="976">
      <title>If Eleanor Rigby Had Met <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case>: A Study on Loneliness in a Post-<fixed-case>LLM</fixed-case> World</title>
      <author><first>Adrian</first><last>de Wynter</last><affiliation>Microsoft</affiliation></author>
      <pages>19898-19913</pages>
      <abstract>**Warning: this paper discusses content related, but not limited to, violence, sex, and suicide.**Loneliness, or the lack of fulfilling relationships, significantly impacts a person’s mental and physical well-being and is prevalent worldwide. Previous research suggests that large language models (LLMs) may help mitigate loneliness. However, we argue that the use of widespread LLMs in services like ChatGPT is more prevalent–and riskier, as they are not designed for this purpose. To explore this, we analysed user interactions with ChatGPT outside of its marketed use as a task-oriented assistant. In dialogues classified as lonely, users frequently (37%) sought advice or validation, and received good engagement. However, ChatGPT failed in sensitive scenarios, like responding appropriately to suicidal ideation or trauma. We also observed a 35% higher incidence of toxic content, with women being 22<tex-math>\times</tex-math> more likely to be targeted than men. Our findings underscore ethical and legal questions about this technology, and note risks like radicalisation or further isolation. We conclude with recommendations to research and industry to address loneliness.</abstract>
      <url hash="6ecced4a">2025.acl-long.976</url>
      <bibkey>de-wynter-2025-eleanor</bibkey>
    </paper>
    <paper id="977">
      <title>Integrating Audio, Visual, and Semantic Information for Enhanced Multimodal Speaker Diarization on Multi-party Conversation</title>
      <author><first>Luyao</first><last>Cheng</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Hui</first><last>Wang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Chong</first><last>Deng</last></author>
      <author><first>Siqi</first><last>Zheng</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yafeng</first><last>Chen</last></author>
      <author><first>Rongjie</first><last>Huang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Qinglin</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Qian</first><last>Chen</last></author>
      <author><first>Xihao</first><last>Li</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Wen</first><last>Wang</last></author>
      <pages>19914-19928</pages>
      <abstract>Speaker diarization aims to segment an audio stream into homogeneous partitions based on speaker identity, playing a crucial role in speech comprehension and analysis. Mainstream speaker diarization systems rely only on acoustic information, making the task particularly challenging in complex acoustic environments in real-world applications. Recently, significant efforts have been devoted to audio-visual or audio-semantic multimodal modeling to enhance speaker diarization performance; however, these approaches still struggle to address the complexities of speaker diarization on spontaneous and unstructured multi-party conversations. To fully exploit meaningful dialogue patterns, we propose a novel multimodal approach that jointly utilizes audio, visual, and semantic cues to enhance speaker diarization. Our approach structures visual cues among active speakers and semantic cues in spoken content into a cohesive format known as pairwise constraints, and employs a semi-supervised clustering technique based on pairwise constrained propagation. Extensive experiments conducted on multiple multimodal datasets demonstrate that our approach effectively integrates audio-visual-semantic information into the clustering process for acoustic speaker embeddings and consistently outperforms state-of-the-art speaker diarization methods, while largely preserving the overall system framework.</abstract>
      <url hash="3b1ec0a1">2025.acl-long.977</url>
      <bibkey>cheng-etal-2025-integrating</bibkey>
    </paper>
    <paper id="978">
      <title>Vulnerability of <fixed-case>LLM</fixed-case>s to Vertically Aligned Text Manipulations</title>
      <author><first>Zhecheng</first><last>Li</last></author>
      <author><first>Yiwei</first><last>Wang</last><affiliation>University of California, Merced</affiliation></author>
      <author><first>Bryan</first><last>Hooi</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Yujun</first><last>Cai</last><affiliation>The University of Queensland</affiliation></author>
      <author><first>Zhen</first><last>Xiong</last></author>
      <author><first>Nanyun</first><last>Peng</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Kai-Wei</first><last>Chang</last><affiliation>University of California, Los Angeles and Amazon</affiliation></author>
      <pages>19929-19941</pages>
      <abstract>Vertical text input is commonly encountered in various real-world applications, such as mathematical computations and word-based Sudoku puzzles. While current large language models (LLMs) have excelled in natural language tasks, they remain vulnerable to variations in text formatting.Recent research demonstrates that modifying input formats, such as vertically aligning words for encoder-based models, can substantially lower accuracy in text classification tasks. While easily understood by humans, these inputs can significantly mislead models, posing a potential risk of bypassing detection in real-world scenarios involving harmful or sensitive information. With the expanding application of LLMs, a crucial question arises: Do decoder-based LLMs exhibit similar vulnerabilities to vertically formatted text input? In this paper, we investigate the impact of vertical text input on the performance of various LLMs across multiple text classification datasets and analyze the underlying causes. Our findings are as follows: (i) Vertical text input significantly degrades the accuracy of LLMs in text classification tasks. (ii) Chain of Thought (CoT) reasoning does not help LLMs recognize vertical input or mitigate its vulnerability, but <i>few-shot learning</i> with careful analysis does. (iii) We explore the underlying cause of the vulnerability by analyzing the inherent issues in tokenization and attention matrices.</abstract>
      <url hash="747f76d2">2025.acl-long.978</url>
      <bibkey>li-etal-2025-vulnerability</bibkey>
    </paper>
    <paper id="979">
      <title><fixed-case>A</fixed-case>uto<fixed-case>M</fixed-case>ixer: Checkpoint Artifacts as Automatic Data Mixers</title>
      <author><first>Ernie</first><last>Chang</last><affiliation>Meta AI</affiliation></author>
      <author><first>Yang</first><last>Li</last><affiliation>Iowa State University</affiliation></author>
      <author><first>Patrick</first><last>Huber</last><affiliation>Facebook</affiliation></author>
      <author><first>Vish</first><last>Vogeti</last><affiliation>Meta AI</affiliation></author>
      <author><first>David</first><last>Kant</last></author>
      <author><first>Yangyang</first><last>Shi</last><affiliation>Meta</affiliation></author>
      <author><first>Vikas</first><last>Chandra</last><affiliation>Meta</affiliation></author>
      <pages>19942-19953</pages>
      <abstract>In language model training, it is desirable to equip models with capabilities from various tasks. However, it is not clear how to directly obtain the right data mixtures for these capabilities as the relationship between data and tasks is difficult to be modeled. In this work, we observe that checkpoint models exhibit emerging capabilities at different points in the training trajectory. Often, the training process saves checkpoints as artifacts that are under-utilized as a source of in-training data signals. We identify these artifact models based on their respective capabilities on the benchmarks and leverage them as data mixers by using their aggregated first-order influence approximation over source data. We demonstrated on eight reasoning benchmarks that the proposed framework shows significant improvements in the pretraining setting, with accuracy increases of up to 1.93%. Overall, this demonstrates the potential of checkpoint models to enhance data quality and optimize data mixtures.</abstract>
      <url hash="b291b8bd">2025.acl-long.979</url>
      <bibkey>chang-etal-2025-automixer</bibkey>
    </paper>
    <paper id="980">
      <title>Generalized Attention Flow: Feature Attribution for Transformer Models via Maximum Flow</title>
      <author><first>Behrooz</first><last>Azarkhalili</last></author>
      <author><first>Maxwell W.</first><last>Libbrecht</last><affiliation>Simon Fraser University</affiliation></author>
      <pages>19954-19974</pages>
      <abstract>This paper introduces Generalized Attention Flow (GAF), a novel feature attribution method for Transformer-based models to address the limitations of current approaches. By extending Attention Flow and replacing attention weights with the generalized Information Tensor, GAF integrates attention weights, their gradients, the maximum flow problem, and the barrier method to enhance the performance of feature attributions. The proposed method exhibits key theoretical properties and mitigates the shortcomings of prior techniques that rely solely on simple aggregation of attention weights. Our comprehensive benchmarking on sequence classification tasks demonstrates that a specific variant of GAF consistently outperforms state-of-the-art feature attribution methods in most evaluation settings, providing a more reliable interpretation of Transformer model outputs.</abstract>
      <url hash="e40a42d9">2025.acl-long.980</url>
      <bibkey>azarkhalili-libbrecht-2025-generalized</bibkey>
    </paper>
    <paper id="981">
      <title>Beyond Prompting: An Efficient Embedding Framework for Open-Domain Question Answering</title>
      <author><first>Zhanghao</first><last>Hu</last></author>
      <author><first>Hanqi</first><last>Yan</last></author>
      <author><first>Qinglin</first><last>Zhu</last><affiliation>King’s College London, University of London</affiliation></author>
      <author><first>Zhenyi</first><last>Shen</last></author>
      <author><first>Yulan</first><last>He</last><affiliation>King’s College London, University of London</affiliation></author>
      <author><first>Lin</first><last>Gui</last><affiliation>King’s College London, University of London</affiliation></author>
      <pages>19975-19990</pages>
      <abstract>Large language models (LLMs) have recently pushed open-domain question answering (ODQA) to new frontiers. However, prevailing retriever–reader pipelines often depend on multiple rounds of prompt-level instructions, leading to high computational overhead, instability, and suboptimal retrieval coverage. In this paper, we propose EmbQA, an embedding-level framework that alleviates these shortcomings by enhancing both the retriever and the reader. Specifically, we refine query representations via lightweight linear layers under an unsupervised contrastive learning objective, thereby reordering retrieved passages to highlight those most likely to contain correct answers. Additionally, we introduce an exploratory embedding that broadens the model’s latent semantic space to diversify candidate generation and employs an entropy-based selection mechanism to choose the most confident answer automatically. Extensive experiments across three open-source LLMs, three retrieval methods, and four ODQA benchmarks demonstrate that EmbQA substantially outperforms recent baselines in both accuracy and efficiency.</abstract>
      <url hash="e126dbd2">2025.acl-long.981</url>
      <bibkey>hu-etal-2025-beyond</bibkey>
    </paper>
    <paper id="982">
      <title><fixed-case>AIR</fixed-case>-Bench: Automated Heterogeneous Information Retrieval Benchmark</title>
      <author><first>Jianlyu</first><last>Chen</last></author>
      <author><first>Nan</first><last>Wang</last><affiliation>Jina AI</affiliation></author>
      <author><first>Chaofan</first><last>Li</last></author>
      <author><first>Bo</first><last>Wang</last></author>
      <author><first>Shitao</first><last>Xiao</last></author>
      <author><first>Han</first><last>Xiao</last><affiliation>Jina AI</affiliation></author>
      <author><first>Hao</first><last>Liao</last><affiliation>Shenzhen University</affiliation></author>
      <author><first>Defu</first><last>Lian</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Zheng</first><last>Liu</last></author>
      <pages>19991-20022</pages>
      <abstract>Evaluation plays a crucial role in the advancement of information retrieval (IR) models. However, current benchmarks, which are based on predefined domains and human-labeled data, face limitations in addressing evaluation needs for emerging domains both cost-effectively and efficiently. To address this challenge, we propose the Automated Heterogeneous Information Retrieval Benchmark (AIR-Bench). AIR-Bench is distinguished by three key features: 1) Automated. The testing data in AIR-Bench is automatically generated by large language models (LLMs) without human intervention. 2) Heterogeneous. The testing data in AIR-Bench is generated with respect to diverse tasks, domains and languages. 3) Dynamic. The domains and languages covered by AIR-Bench are constantly augmented to provide an increasingly comprehensive evaluation benchmark for community developers. We develop a reliable and robust data generation pipeline to automatically create diverse and high-quality evaluation datasets based on real-world corpora. Our findings demonstrate that the generated testing data in AIR-Bench aligns well with human-labeled testing data, making AIR-Bench a dependable benchmark for evaluating IR models. The resources in AIR-Bench are publicly available at https://github.com/AIR-Bench/AIR-Bench.</abstract>
      <url hash="a8ebb01a">2025.acl-long.982</url>
      <bibkey>chen-etal-2025-air</bibkey>
    </paper>
    <paper id="983">
      <title>We-Math: Does Your Large Multimodal Model Achieve Human-like Mathematical Reasoning?</title>
      <author><first>Runqi</first><last>Qiao</last></author>
      <author><first>Qiuna</first><last>Tan</last></author>
      <author><first>Guanting</first><last>Dong</last></author>
      <author><first>MinhuiWu</first><last>MinhuiWu</last></author>
      <author><first>Chong</first><last>Sun</last><affiliation>WeChat AI</affiliation></author>
      <author><first>Xiaoshuai</first><last>Song</last></author>
      <author><first>Jiapeng</first><last>Wang</last></author>
      <author><first>Zhuoma</first><last>GongQue</last></author>
      <author><first>Shanglin</first><last>Lei</last><affiliation>Meituan</affiliation></author>
      <author><first>YiFan</first><last>Zhang</last></author>
      <author><first>Zhe</first><last>Wei</last></author>
      <author><first>Miaoxuan</first><last>Zhang</last></author>
      <author><first>Runfeng</first><last>Qiao</last></author>
      <author><first>Xiao</first><last>Zong</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Yida</first><last>Xu</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Peiqing</first><last>Yang</last></author>
      <author><first>Zhimin</first><last>Bao</last></author>
      <author><first>Muxi</first><last>Diao</last></author>
      <author><first>Chen</first><last>Li</last><affiliation>Tencent</affiliation></author>
      <author><first>Honggang</first><last>Zhang</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <pages>20023-20070</pages>
      <abstract>Visual mathematical reasoning, as a fundamental visual reasoning ability, has received widespread attention from the Large Multimodal Models (LMMs) community. Existing benchmarks mainly focus more on the end-to-end performance, but neglect the underlying principles of knowledge acquisition and generalization. Instead, we introduce WE-MATH, the first benchmark specifically designed to explore the problem-solving principles. We meticulously collect 6.5K visual math problems and decompose them into 10.9K step-level questions for evaluation, spanning 5 layers of knowledge granularity and 67 hierarchical knowledge concepts. Specifically, we decompose composite problems into sub-problems according to the required knowledge concepts and introduce a novel four-dimensional metric to hierarchically assess inherent issues in LMMs’ reasoning process. With WE-MATH, we conduct a thorough evaluation of existing LMMs in visual mathematical reasoning and provide comprehensive analysis and insight for future development. We anticipate that WE-MATH will open new pathways for advancements in visual mathematical reasoning for LMMs. Data and code are available at https://github.com/We-Math/We-Math.</abstract>
      <url hash="967cb360">2025.acl-long.983</url>
      <bibkey>qiao-etal-2025-math</bibkey>
    </paper>
    <paper id="984">
      <title>Modeling the Evolution of <fixed-case>E</fixed-case>nglish Noun Compounds with Feature-Rich Diachronic Compositionality Prediction</title>
      <author><first>Filip</first><last>Miletić</last><affiliation>University of Stuttgart</affiliation></author>
      <author><first>Sabine</first><last>Schulte Im Walde</last><affiliation>University of Stuttgart</affiliation></author>
      <pages>20071-20092</pages>
      <abstract>We analyze the evolution of English noun compounds, which we represent as vectors of time-specific values. We implement a wide array of methods to create a rich set of features, using them to classify compounds for present-day compositionality and to assess the informativeness of the corresponding linguistic patterns. Our best results use BERT – reflecting the similarity of compounds and sentence contexts – and we further capture relevant and complementary information across approaches. Leveraging these feature differences, we find that the development of low-compositional meanings is reflected by a parallel drop in compositionality and sustained semantic change. The same distinction is echoed in transformer processing: compositionality estimates require far less contextualization than semantic change estimates.</abstract>
      <url hash="eb063f1d">2025.acl-long.984</url>
      <bibkey>miletic-schulte-im-walde-2025-modeling</bibkey>
    </paper>
    <paper id="985">
      <title>What’s the Difference? Supporting Users in Identifying the Effects of Prompt and Model Changes Through Token Patterns</title>
      <author><first>Michael A.</first><last>Hedderich</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Anyi</first><last>Wang</last></author>
      <author><first>Raoyuan</first><last>Zhao</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Florian</first><last>Eichin</last></author>
      <author><first>Jonas</first><last>Fischer</last><affiliation>Saarland Informatics Campus, Max-Planck Institute</affiliation></author>
      <author><first>Barbara</first><last>Plank</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <pages>20093-20123</pages>
      <abstract>Prompt engineering for large language models is challenging, as even small prompt perturbations or model changes can significantly impact the generated output texts. Existing evaluation methods of LLM outputs, either automated metrics or human evaluation, have limitations, such as providing limited insights or being labor-intensive. We propose Spotlight, a new approach that combines both automation and human analysis. Based on data mining techniques, we automatically distinguish between random (decoding) variations and systematic differences in language model outputs. This process provides token patterns that describe the systematic differences and guide the user in manually analyzing the effects of their prompts and changes in models efficiently. We create three benchmarks to quantitatively test the reliability of token pattern extraction methods and demonstrate that our approach provides new insights into established prompt data. From a human-centric perspective, through demonstration studies and a user study, we show that our token pattern approach helps users understand the systematic differences of language model outputs. We are further able to discover relevant differences caused by prompt and model changes (e.g. related to gender or culture), thus supporting the prompt engineering process and human-centric model behavior research.</abstract>
      <url hash="13a88d16">2025.acl-long.985</url>
      <bibkey>hedderich-etal-2025-whats</bibkey>
    </paper>
    <paper id="986">
      <title><fixed-case>V</fixed-case>-Oracle: Making Progressive Reasoning in Deciphering Oracle Bones for You and Me</title>
      <author><first>Runqi</first><last>Qiao</last></author>
      <author><first>Qiuna</first><last>Tan</last></author>
      <author><first>Guanting</first><last>Dong</last></author>
      <author><first>MinhuiWu</first><last>MinhuiWu</last></author>
      <author><first>Jiapeng</first><last>Wang</last></author>
      <author><first>YiFan</first><last>Zhang</last></author>
      <author><first>Zhuoma</first><last>GongQue</last></author>
      <author><first>Chong</first><last>Sun</last><affiliation>WeChat AI</affiliation></author>
      <author><first>Yida</first><last>Xu</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Yadong</first><last>Xue</last></author>
      <author><first>Ye</first><last>Tian</last></author>
      <author><first>Zhimin</first><last>Bao</last></author>
      <author><first>Lan</first><last>Yang</last></author>
      <author><first>Chen</first><last>Li</last><affiliation>Tencent</affiliation></author>
      <author><first>Honggang</first><last>Zhang</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <pages>20124-20150</pages>
      <abstract>Oracle Bone Script (OBS) is a vital treasure of human civilization, rich in insights from ancient societies. However, the evolution of written language over millennia complicates its decipherment. In this paper, we propose V-Oracle, an innovative framework that utilizes Large Multi-modal Models (LMMs) for interpreting OBS. V-Oracle applies principles of pictographic character formation and frames the task as a visual question-answering (VQA) problem, establishing a multi-step reasoning chain. It proposes a multi-dimensional data augmentation for synthesizing high-quality OBS samples, and also implements a multi-phase oracle alignment tuning to improve LMMs’ visual reasoning capabilities. Moreover, to bridge the evaluation gap in the OBS field, we further introduce Oracle-Bench, a comprehensive benchmark that emphasizes process-oriented assessment and incorporates both standard and out-of-distribution setups for realistic evaluation. Extensive experimental results can demonstrate the effectiveness of our method in providing quantitative analyses and superior deciphering capability.</abstract>
      <url hash="9ac87611">2025.acl-long.986</url>
      <bibkey>qiao-etal-2025-v</bibkey>
    </paper>
    <paper id="987">
      <title>Unveiling Cultural Blind Spots: Analyzing the Limitations of m<fixed-case>LLM</fixed-case>s in Procedural Text Comprehension</title>
      <author><first>Amir Hossein</first><last>Yari</last><affiliation>EPFL - EPF Lausanne</affiliation></author>
      <author><first>Fajri</first><last>Koto</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>20151-20170</pages>
      <abstract>Despite the impressive performance of multilingual large language models (mLLMs) in various natural language processing tasks, their ability to understand procedural texts, particularly those with culture-specific content, remains largely unexplored. Texts describing cultural procedures, including rituals, traditional craftsmanship, and social etiquette, require an inherent understanding of cultural context, presenting a significant challenge for mLLMs. In this work, we introduce CAPTex, a benchmark designed to evaluate mLLMs’ ability to process and reason over culturally diverse procedural texts in multiple languages. Using a range of evaluation methods, we find that (1) mLLMs struggle with culturally contextualized procedural content, particularly in low-resource languages; (2) performance varies across cultural domains, with some proving more difficult than others; and (3) models perform better on multiple-choice tasks presented in conversational formats than on direct questions. These results highlight the current limitations of mLLMs and emphasize the need for culturally informed benchmarks like CAPTex to support more accurate and inclusive language understanding.</abstract>
      <url hash="85232897">2025.acl-long.987</url>
      <bibkey>yari-koto-2025-unveiling</bibkey>
    </paper>
    <paper id="988">
      <title>Improving Language and Modality Transfer in Translation by Character-level Modeling</title>
      <author><first>Ioannis</first><last>Tsiamas</last><affiliation>Facebook and Universidad Politécnica de Cataluna</affiliation></author>
      <author><first>David</first><last>Dale</last><affiliation>FAIR at Meta</affiliation></author>
      <author><first>Marta R.</first><last>Costa-jussà</last><affiliation>Meta</affiliation></author>
      <pages>20171-20187</pages>
      <abstract>Current translation systems, despite being highly multilingual, cover only 5% of the world’s languages. Expanding language coverage to the long-tail of low-resource languages requires data-efficient methods that rely on cross-lingual and cross-modal knowledge transfer. To this end, we propose a character-based approach to improve adaptability to new languages and modalities. Our method leverages SONAR, a multilingual fixed-size embedding space with different modules for encoding and decoding. We use a teacher-student approach with parallel translation data to obtain a character-level encoder. Then, using ASR data, we train a lightweight adapter to connect a massively multilingual CTC ASR model (MMS), to the character-level encoder, potentially enabling speech translation from 1,000+ languages. Experimental results in text translation for 75 languages on FLORES+ demonstrate that our character-based approach can achieve better language transfer than traditional subword-based models, especially outperforming them in low-resource settings, and demonstrating better zero-shot generalizability to unseen languages. Our speech adaptation, maximizing knowledge transfer from the text modality, achieves state-of-the-art results in speech-to-text translation on the FLEURS benchmark on 33 languages, surpassing previous supervised and cascade models, albeit being a zero-shot model with minimal supervision from ASR data.</abstract>
      <url hash="17b5907a">2025.acl-long.988</url>
      <bibkey>tsiamas-etal-2025-improving</bibkey>
    </paper>
    <paper id="989">
      <title><fixed-case>D</fixed-case>ial<fixed-case>U</fixed-case>p! Modeling the Language Continuum by Adapting Models to Dialects and Dialects to Models</title>
      <author><first>Niyati</first><last>Bafna</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Emily</first><last>Chang</last></author>
      <author><first>Nathaniel Romney</first><last>Robinson</last><affiliation>Department of Computer Science, Whiting School of Engineering</affiliation></author>
      <author><first>David R.</first><last>Mortensen</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Kenton</first><last>Murray</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>David</first><last>Yarowsky</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Hale</first><last>Sirin</last><affiliation>Johns Hopkins University</affiliation></author>
      <pages>20188-20233</pages>
      <abstract>Most of the world’s languages and dialects are low-resource, and lack support in mainstream machine translation (MT) models. However, many of them have a closely-related high-resource language (HRL) neighbor, and differ in linguistically regular ways from it. This underscores the importance of model robustness to dialectal variation and cross-lingual generalization to the HRL dialect continuum. We present DialUp, consisting of a training-time technique for adapting a pretrained model to dialectal data (M–&gt;D), and an inference-time intervention adapting dialectal data to the model expertise (D–&gt;M). M–&gt;D induces model robustness to potentially unseen and unknown dialects by exposure to synthetic data exemplifying linguistic mechanisms of dialectal variation, whereas D–&gt;M treats dialectal divergence for known target dialects. These methods show considerable performance gains for several dialects from four language families, and modest gains for two other language families. We also conduct feature and error analyses, which show that language varieties with low baseline MT performance are more likely to benefit from these approaches.</abstract>
      <url hash="64347ce0">2025.acl-long.989</url>
      <bibkey>bafna-etal-2025-dialup</bibkey>
    </paper>
    <paper id="990">
      <title><fixed-case>A</fixed-case>uto<fixed-case>M</fixed-case>ix<fixed-case>A</fixed-case>lign: Adaptive Data Mixing for Multi-Task Preference Optimization in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Nicholas E.</first><last>Corrado</last><affiliation>University of Wisconsin - Madison</affiliation></author>
      <author><first>Julian</first><last>Katz-Samuels</last><affiliation>Amazon</affiliation></author>
      <author><first>Adithya M</first><last>Devraj</last></author>
      <author><first>Hyokun</first><last>Yun</last><affiliation>Amazon</affiliation></author>
      <author><first>Chao</first><last>Zhang</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Yi</first><last>Xu</last><affiliation>Amazon</affiliation></author>
      <author><first>Yi</first><last>Pan</last><affiliation>Amazon</affiliation></author>
      <author><first>Bing</first><last>Yin</last><affiliation>Amazon</affiliation></author>
      <author><first>Trishul</first><last>Chilimbi</last><affiliation>Amazon</affiliation></author>
      <pages>20234-20258</pages>
      <abstract>When aligning large language models (LLMs), their performance across various tasks (such as being helpful, harmless, and honest) is heavily influenced by the composition of the training data. However, it is difficult to determine what mixture of data should be used to produce a model with strong performance across all tasks. Existing approaches rely on large ablation studies, heuristics, or human intuition, though these can be prohibitively expensive and suboptimal. We study this problem in the context of preference optimization via DPO and propose a novel and theoretically justified algorithm, AutoMixAlign (AMA), that adaptively mixes datasets during LLM training to balance performance across multiple tasks. AMA first trains <i>specialist models</i> for each task to determine losses that corresponding to strong task performance. Next, AMA trains a generalist model using a novel minimax optimization that prioritizes tasks for which generalist model losses are furthest from specialist model losses. We introduce two algorithms to optimize this problem: (1) AMA-R adaptively reweights the objective to prioritize tasks, and (2) AMA-S adaptively adjusts how much data is sampled from each task to prioritize tasks. Both algorithms achieve a convergence rate of <tex-math>O(1/\sqrt{T})</tex-math> in the convex case. AMA-R’s convergence result immediately follows from Sagawa et. al, 2019, and we provide a convergence proof for AMA-S using techniques from online learning such as EXP3 (Auer et. al, 2002). We evaluate AMA on several multitask alignment setups, and observe that AMA outperforms the standard alignment approach which simply optimizes the total loss across all tasks and also outperforms model-merging methods.</abstract>
      <url hash="5d7a6419">2025.acl-long.990</url>
      <bibkey>corrado-etal-2025-automixalign</bibkey>
    </paper>
    <paper id="991">
      <title>Modeling Complex Semantics Relation with Contrastively Fine-Tuned Relational Encoders</title>
      <author><first>Naïm</first><last>Es-sebbani</last></author>
      <author><first>Esteban</first><last>Marquer</last></author>
      <author><first>Zied</first><last>Bouraoui</last><affiliation>CRIL Univ-Artois &amp; CNRS</affiliation></author>
      <pages>20259-20288</pages>
      <abstract>Modeling relationships between concepts and entities is essential for many applications. While Large Language Models (LLMs) capture relational and commonsense knowledge effectively, they are computationally expensive and often underperform in tasks requiring efficient relational encoding, such as relation induction, extraction, and information retrieval. Despite advancements in learning relational embeddings, existing methods often fail to capture nuanced representations and the rich semantics needed for high-quality embeddings. In this work, we propose different relational encoders designed to capture diverse relational aspects and semantic properties of entity pairs. Although several datasets exist for training such encoders, they often rely on structured knowledge bases or predefined schemas, which primarily encode simple and static relations. To overcome this limitation, we also introduce a novel dataset generation method leveraging LLMs to create a diverse spectrum of relationships. Our experiments demonstrate the effectiveness of our proposed encoders and the benefits of our generated dataset.</abstract>
      <url hash="831d20b2">2025.acl-long.991</url>
      <bibkey>es-sebbani-etal-2025-modeling</bibkey>
    </paper>
    <paper id="992">
      <title>Error-driven Data-efficient Large Multimodal Model Tuning</title>
      <author><first>Barry Menglong</first><last>Yao</last><affiliation>Virginia Tech</affiliation></author>
      <author><first>Qifan</first><last>Wang</last><affiliation>Meta AI</affiliation></author>
      <author><first>Lifu</first><last>Huang</last><affiliation>University of California, Davis</affiliation></author>
      <pages>20289-20306</pages>
      <abstract>Large Multimodal Models (LMMs) have demonstrated impressive performance across numerous academic benchmarks. However, fine-tuning still remains essential to achieve satisfactory performance on downstream tasks, while the task-specific tuning samples are usually not readily available or expensive and time-consuming to obtain. To address this, we propose an error-driven data-efficient tuning framework that aims to efficiently adapt generic LMMs to newly emerging tasks without requiring extensive task-specific training samples. In our approach, a generic LMM, acting as a student model, is first evaluated on a small validation set of the target task, and then a more powerful model, acting as a teacher model, identifies the erroneous steps within the student model’s reasoning steps and analyzes its capability gaps from fully addressing the target task. Based on these gaps, targeted training samples are further retrieved from existing task-agnostic datasets to tune the student model and tailor it to the target task. We perform extensive experiments across three different training data scales and seven tasks, demonstrating that our training paradigm significantly and efficiently improves LMM’s performance on downstream tasks, achieving an average performance boost of 7.01%</abstract>
      <url hash="561ab8b1">2025.acl-long.992</url>
      <bibkey>yao-etal-2025-error</bibkey>
    </paper>
    <paper id="993">
      <title>Planning with Diffusion Models for Target-Oriented Dialogue Systems</title>
      <author><first>Hanwen</first><last>Du</last></author>
      <author><first>Bo</first><last>Peng</last><affiliation>Google</affiliation></author>
      <author><first>Xia</first><last>Ning</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <pages>20307-20329</pages>
      <abstract>Target-Oriented Dialogue (TOD) remains a significant challenge in the LLM era, where strategic dialogue planning is crucial for directing conversations toward specific targets. However, existing dialogue planning methods generate dialogue plans in a step-by-step sequential manner, and may suffer from compounding errors and myopic actions. To address these limitations, we introduce a novel dialogue planning framework, DiffTOD, which leverages diffusion models to enable non-sequential dialogue planning. DiffTOD formulates dialogue planning as a trajectory generation problem with conditional guidance, and leverages a diffusion language model to estimate the likelihood of the dialogue trajectory. To optimize the dialogue action strategies, DiffTOD introduces three tailored guidance mechanisms for different target types, offering flexible guidance toward diverse TOD targets at test time. Extensive experiments across three diverse TOD settings show that DiffTOD can effectively perform non-myopic lookahead exploration and optimize action strategies over a long horizon through non-sequential dialogue planning, and demonstrates strong flexibility across complex and diverse dialogue scenarios. Our code and data are accessible through https://github.com/ninglab/DiffTOD.</abstract>
      <url hash="c8cb4c0e">2025.acl-long.993</url>
      <bibkey>du-etal-2025-planning</bibkey>
    </paper>
    <paper id="994">
      <title>Interactive and Expressive Code-Augmented Planning with Large Language Models</title>
      <author><first>Anthony Zhe</first><last>Liu</last><affiliation>Google</affiliation></author>
      <author><first>Xinhe</first><last>Wang</last></author>
      <author><first>Jacob</first><last>Sansom</last></author>
      <author><first>Yao</first><last>Fu</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>Jongwook</first><last>Choi</last></author>
      <author><first>Sungryull</first><last>Sohn</last><affiliation>LG AI Research</affiliation></author>
      <author><first>Jaekyeom</first><last>Kim</last><affiliation>LG AI Research</affiliation></author>
      <author><first>Honglak</first><last>Lee</last><affiliation>University of Michigan - Ann Arbor and LG AI Research</affiliation></author>
      <pages>20330-20354</pages>
      <abstract>Large Language Models (LLMs) demonstrate strong abilities in common-sense reasoning and interactive decision-making, but often struggle with complex, long-horizon planning tasks. Recent techniques have sought to structure LLM outputs using control flow and code to improve planning performance. However, code-based approaches can be error-prone and insufficient for handling ambiguous or unstructured data. To address these challenges, we propose REPL-Plan, an LLM planning approach that is fully code-expressive (it can utilize all the benefits of code) while also being dynamic (it can flexibly adapt from errors and use the LLM for soft reasoning). In REPL-Plan, an LLM solves tasks by interacting with a Read-Eval-Print Loop (REPL), which iteratively executes and evaluates code, similar to language shells or interactive code notebooks, allowing the model to flexibly correct errors and handle tasks dynamically. We demonstrate that REPL-Plan achieves strong results across various planning domains compared to previous methods.</abstract>
      <url hash="b4d19d8e">2025.acl-long.994</url>
      <bibkey>liu-etal-2025-interactive-expressive</bibkey>
    </paper>
    <paper id="995">
      <title>Synergistic Weak-Strong Collaboration by Aligning Preferences</title>
      <author><first>Yizhu</first><last>Jiao</last><affiliation>UIUC</affiliation></author>
      <author><first>Xuchao</first><last>Zhang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Zhaoyang</first><last>Wang</last></author>
      <author><first>Yubo</first><last>Ma</last><affiliation>School of Computer Science and Engineering, Nanyang Technological University</affiliation></author>
      <author><first>Zhun</first><last>Deng</last><affiliation>Department of Computer Science, University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Rujia</first><last>Wang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Chetan</first><last>Bansal</last><affiliation>Microsoft</affiliation></author>
      <author><first>Saravan</first><last>Rajmohan</last><affiliation>Microsoft</affiliation></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <author><first>Huaxiu</first><last>Yao</last><affiliation>Department of Computer Science, University of North Carolina at Chapel Hill</affiliation></author>
      <pages>20355-20371</pages>
      <abstract>Current Large Language Models excel in general reasoning yet struggle with specialized tasks requiring proprietary or domain-specific knowledge. Fine-tuning large models for every niche application is often infeasible due to black-box constraints and high computational overhead. To address this, we propose a collaborative framework that pairs a specialized weak model with a general strong model. The weak model, tailored to specific domains, produces initial drafts and background information, while the strong model leverages its advanced reasoning to refine these drafts, extending LLMs’ capabilities to critical yet specialized tasks. To optimize this collaboration, we introduce a collaborative feedback to fine-tunes the weak model, which quantifies the influence of the weak model’s contributions in the collaboration procedure and establishes preference pairs to guide preference tuning of the weak model. We validate our framework through experiments on three domains. We find that the collaboration significantly outperforms each model alone by leveraging complementary strengths. Moreover, aligning the weak model with the collaborative preference further enhances overall performance.</abstract>
      <url hash="f54a5eae">2025.acl-long.995</url>
      <bibkey>jiao-etal-2025-synergistic</bibkey>
    </paper>
    <paper id="996">
      <title>Understanding Silent Data Corruption in <fixed-case>LLM</fixed-case> Training</title>
      <author><first>Jeffrey Jian</first><last>Ma</last><affiliation>Google and Harvard University</affiliation></author>
      <author><first>Hengzhi</first><last>Pei</last></author>
      <author><first>Leonard</first><last>Lausen</last><affiliation>Amazon Web Services</affiliation></author>
      <author><first>George</first><last>Karypis</last><affiliation>University of Minnesota, Minneapolis</affiliation></author>
      <pages>20372-20394</pages>
      <abstract>As the scale of training large language models (LLMs) increases, one emergent failure is silent data corruption (SDC), where hardware produces incorrect computations without explicit failure signals. In this work, we are the first to investigate the impact of real-world SDCs on LLM training by comparing model training between healthy production nodes and unhealthy nodes exhibiting SDCs. With the help from a cloud computing platform, we access the unhealthy nodes that were swept out from production by automated fleet management. Using deterministic execution via XLA compiler and our proposed synchronization mechanisms, we isolate and analyze the impact of SDC errors on these nodes at three levels: at each submodule computation, at a single optimizer step, and at a training period. Our results reveal that the impact of SDCs on computation varies on different unhealthy nodes. Although in most cases the perturbations from SDCs on submodule computation and gradients are relatively small, SDCs can lead models to converge to different optima with different weights and even cause spikes in the training loss. Our analysis sheds light on further understanding and mitigating the impact of SDCs.</abstract>
      <url hash="fbd821cb">2025.acl-long.996</url>
      <bibkey>ma-etal-2025-understanding</bibkey>
    </paper>
    <paper id="997">
      <title>Align-<fixed-case>SLM</fixed-case>: Textless Spoken Language Models with Reinforcement Learning from <fixed-case>AI</fixed-case> Feedback</title>
      <author><first>Guan-Ting</first><last>Lin</last><affiliation>National Taiwan University</affiliation></author>
      <author><first>Prashanth Gurunath</first><last>Shivakumar</last><affiliation>Amazon</affiliation></author>
      <author><first>Aditya</first><last>Gourav</last><affiliation>Amazon</affiliation></author>
      <author><first>Yile</first><last>Gu</last></author>
      <author><first>Ankur</first><last>Gandhe</last></author>
      <author><first>Hung-yi</first><last>Lee</last><affiliation>National Taiwan University</affiliation></author>
      <author><first>Ivan</first><last>Bulyko</last><affiliation>Amazon</affiliation></author>
      <pages>20395-20411</pages>
      <abstract>While textless Spoken Language Models (SLMs) have shown potential in end-to-end speech-to-speech modeling, they still lag behind text-based Large Language Models (LLMs) in terms of semantic coherence and relevance. This work introduces the <b>Align-SLM</b> framework, which leverages preference optimization inspired by Reinforcement Learning with Human Feedback (RLHF) to enhance the semantic understanding of SLMs. Our approach generates multiple speech continuations from a given prompt and uses LLM-based semantic metrics to create preference data for Direct Preference Optimization (DPO). We evaluate the framework using ZeroSpeech 2021 benchmarks for lexical and syntactic modeling, the spoken version of the StoryCloze dataset for semantic coherence, and other speech generation metrics, including the GPT4-o score and human evaluation. Experimental results show that our method achieves the state-of-the-art performance of SLMs for most benchmarks, highlighting the importance of preference optimization to improve the semantics of SLMs.</abstract>
      <url hash="5b120af7">2025.acl-long.997</url>
      <bibkey>lin-etal-2025-align</bibkey>
    </paper>
    <paper id="998">
      <title>Can <fixed-case>LLM</fixed-case>s Help Uncover Insights about <fixed-case>LLM</fixed-case>s? A Large-Scale, Evolving Literature Analysis of Frontier <fixed-case>LLM</fixed-case>s</title>
      <author><first>Jungsoo</first><last>Park</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Junmo</first><last>Kang</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Gabriel</first><last>Stanovsky</last><affiliation>Hebrew University of Jerusalem</affiliation></author>
      <author><first>Alan</first><last>Ritter</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <pages>20412-20433</pages>
      <abstract>The surge of LLM studies makes synthesizing their findings challenging. Analysis of experimental results from literature can uncover important trends across studies, but the time-consuming nature of manual data extraction limits its use.Our study presents a semi-automated approach for literature analysis that accelerates data extraction using LLMs.It automatically identifies relevant arXiv papers, extracts experimental results and related attributes, and organizes them into a structured dataset, LLMEvalDB.We then conduct an automated literature analysis of frontier LLMs, reducing the effort of paper surveying and data extraction by more than 93% compared to manual approaches.We validate LLMEvalDB by showing that it reproduces key findings from a recent manual analysis of Chain-of-Thought (CoT) reasoning and also uncovers new insights that go beyond it, showing, for example, that in-context examples benefit coding &amp; multimodal tasks but offer limited gains in math reasoning tasks compared to zero-shot CoT.Our automatically updatable dataset enables continuous tracking of target models by extracting evaluation studies as new data becomes available. Through LLMEvalDB and empirical analysis, we provide insights into LLMs while facilitating ongoing literature analyses of their behavior.</abstract>
      <url hash="c468d8b3">2025.acl-long.998</url>
      <bibkey>park-etal-2025-llms</bibkey>
    </paper>
    <paper id="999">
      <title><fixed-case>BIG</fixed-case>5-<fixed-case>CHAT</fixed-case>: Shaping <fixed-case>LLM</fixed-case> Personalities Through Training on Human-Grounded Data</title>
      <author><first>Wenkai</first><last>Li</last></author>
      <author><first>Jiarui</first><last>Liu</last></author>
      <author><first>Andy</first><last>Liu</last></author>
      <author><first>Xuhui</first><last>Zhou</last></author>
      <author><first>Mona T.</first><last>Diab</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Maarten</first><last>Sap</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>20434-20471</pages>
      <abstract>In this work, we tackle the challenge of embedding realistic human personality traits into LLMs. Previous approaches have primarily focused on prompt-based methods that describe the behavior associated with the desired personality traits, suffering from realism and validity issues. To address these limitations, we introduce BIG5-CHAT, a large-scale dataset containing 100,000 dialogues designed to ground models in how humans express their personality in text. Leveraging this dataset, we explore Supervised Fine-Tuning and Direct Preference Optimization as training-based methods to align LLMs more naturally with human personality patterns. Our methods outperform prompting on personality assessments such as BFI and IPIP-NEO, with trait correlations more closely matching human data. Furthermore, our experiments reveal that models trained to exhibit higher conscientiousness, higher agreeableness, lower extraversion, and lower neuroticism display better performance on reasoning tasks, aligning with psychological findings on how these traits impact human cognitive performance. To our knowledge, this work is the first comprehensive study to demonstrate how training-based methods can shape LLM personalities through learning from real human behaviors.</abstract>
      <url hash="bf9fa332">2025.acl-long.999</url>
      <bibkey>li-etal-2025-big5</bibkey>
    </paper>
    <paper id="1000">
      <title>Deep Temporal Reasoning in Video Language Models: A Cross-Linguistic Evaluation of Action Duration and Completion through Perfect Times</title>
      <author><first>Olga</first><last>Loginova</last></author>
      <author><first>Sofía Ortega</first><last>Loguinova</last></author>
      <pages>20472-20502</pages>
      <abstract>Human perception of events is intrinsically tied to distinguishing between completed (perfect and telic) and ongoing (durative) actions, a process mediated by both linguistic structure and visual cues. In this work, we introduce the Perfect Times dataset, a novel, quadrilingual (English, Italian, Russian, and Japanese) multiple-choice question-answering benchmark designed to assess video-language models (VLMs) on temporal reasoning. By pairing everyday activity videos with event completion labels and perfectivity-tailored distractors, our dataset probes whether models truly comprehend temporal dynamics or merely latch onto superficial markers. Experimental results indicate that state-of-the-art models, despite their success on text-based tasks, struggle to mirror human-like temporal and causal reasoning grounded in video. This study underscores the necessity of integrating deep multimodal cues to capture the nuances of action duration and completion within temporal and causal video dynamics, setting a new standard for evaluating and advancing temporal reasoning in VLMs.</abstract>
      <url hash="2d074c9c">2025.acl-long.1000</url>
      <bibkey>loginova-loguinova-2025-deep</bibkey>
    </paper>
    <paper id="1001">
      <title>Amplifying Trans and Nonbinary Voices: A Community-Centred Harm Taxonomy for <fixed-case>LLM</fixed-case>s</title>
      <author><first>Eddie L.</first><last>Ungless</last></author>
      <author><first>Sunipa</first><last>Dev</last><affiliation>Google</affiliation></author>
      <author><first>Cynthia L.</first><last>Bennett</last><affiliation>Google</affiliation></author>
      <author><first>Rebecca</first><last>Gulotta</last><affiliation>Google</affiliation></author>
      <author><first>Jasmijn</first><last>Bastings</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Remi</first><last>Denton</last><affiliation>Google</affiliation></author>
      <pages>20503-20535</pages>
      <abstract>We explore large language model (LLM) responses that may negatively impact the transgender and nonbinary (TGNB) community and introduce the Transing Transformers Toolkit, <tex-math>T^3</tex-math>, which provides resources for identifying such harmful response behaviors. The heart of <tex-math>T^3</tex-math> is a community-centred taxonomy of harms, developed in collaboration with the TGNB community, which we complement with, amongst other guidance, suggested heuristics for evaluation. To develop the taxonomy, we adopted a multi-method approach that included surveys and focus groups with community experts. The contribution highlights the importance of community-centred approaches in mitigating harm, and outlines pathways for LLM developers to improve how their models handle TGNB-related topics.</abstract>
      <url hash="132ddcfb">2025.acl-long.1001</url>
      <bibkey>ungless-etal-2025-amplifying</bibkey>
    </paper>
    <paper id="1002">
      <title>Enhancing Human Evaluation in Machine Translation with Comparative Judgement</title>
      <author><first>Yixiao</first><last>Song</last><affiliation>University of Massachusetts at Amherst</affiliation></author>
      <author><first>Parker</first><last>Riley</last><affiliation>Google</affiliation></author>
      <author><first>Daniel</first><last>Deutsch</last><affiliation>Google</affiliation></author>
      <author><first>Markus</first><last>Freitag</last><affiliation>Google</affiliation></author>
      <pages>20536-20551</pages>
      <abstract>Human evaluation is crucial for assessing rapidly evolving language models but is influenced by annotator proficiency and task design. This study explores the integration of comparative judgment into human annotation for machine translation (MT) and evaluates three annotation setups—point-wise Multidimensional Quality Metrics (MQM), side-by-side (S×S) MQM, and its simplified version S×S relative ranking (RR). In MQM, annotators mark error spans with categories and severity levels. S×S MQM extends MQM to pairwise error annotation for two translations of the same input, while S×S RR focuses on selecting the better output without labeling errors.Key findings are: (1) the S×S settings achieve higher inter-annotator agreement than MQM; (2) S×S MQM enhances inter-translation error marking consistency compared to MQM by, on average, 38.5% for explicitly compared MT systems and 19.5% for others; (3) all annotation settings return stable system rankings, with S×S RR offering a more efficient alternative to (S×S) MQM; (4) the S×S settings highlight subtle errors overlooked in MQM without altering absolute system evaluations.To spur further research, we will release the triply annotated datasets comprising 377 ZhEn and 104 EnDe annotation examples, each covering 10 systems.</abstract>
      <url hash="38f47124">2025.acl-long.1002</url>
      <bibkey>song-etal-2025-enhancing</bibkey>
    </paper>
    <paper id="1003">
      <title>Infogen: Generating Complex Statistical Infographics from Documents</title>
      <author><first>Akash</first><last>Ghosh</last><affiliation>Indian Institute of Technology, Patna</affiliation></author>
      <author><first>Aparna</first><last>Garimella</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Pritika</first><last>Ramu</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Sambaran</first><last>Bandyopadhyay</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Sriparna</first><last>Saha</last><affiliation>Indian Institute of Technology Patna, India</affiliation></author>
      <pages>20552-20570</pages>
      <abstract>Statistical infographics are powerful tools that simplify complex data into visually engaging and easy-to-understand formats. Despite advancements in AI, particularly with LLMs, existing efforts have been limited to generating simple charts, with no prior work addressing the creation of complex infographics from text-heavy documents that demand a deep understanding of the content. We address this gap by introducing the task of generating <i>statistical infographics</i> composed of multiple sub-charts (e.g., line, bar, pie) that are contextually accurate, insightful, and visually aligned. To achieve this, we define infographic metadata, that includes its title and textual insights, along with sub-chart-specific details such as their corresponding data, alignment, etc. We also present <b>
          <i>Infodat</i></b>, the first benchmark dataset for text-to-infographic metadata generation, where each sample links a document to its metadata. We propose <b>
          <i>Infogen</i></b>, a two-stage framework where fine-tuned LLMs first generate metadata, which is then converted into infographic code. Extensive evaluations on <b>
          <i>Infodat</i></b> demonstrate that <b>
          <i>Infogen</i></b> achieves state-of-the-art performance, outperforming both closed and open-source LLMs in text-to-statistical infographic generation.</abstract>
      <url hash="9282228e">2025.acl-long.1003</url>
      <bibkey>ghosh-etal-2025-infogen</bibkey>
    </paper>
    <paper id="1004">
      <title>Partial Colexifications Improve Concept Embeddings</title>
      <author><first>Arne</first><last>Rubehn</last><affiliation>Universität Passau</affiliation></author>
      <author><first>Johann-Mattis</first><last>List</last><affiliation>Universität Passau and Max-Planck Institute</affiliation></author>
      <pages>20571-20586</pages>
      <abstract>While the embedding of words has revolutionized the field of Natural Language Processing, the embedding of concepts has received much less attention so far. A dense and meaningful representation of concepts, however, could prove useful for several tasks in computational linguistics, especially those involving cross-linguistic data or sparse data from low resource languages. First methods that have been proposed so far embed concepts from automatically constructed colexification networks. While these approaches depart from automatically inferred polysemies, attested across a larger number of languages, they are restricted to the word level, ignoring lexical relations that would only hold for parts of the words in a given language. Building on recently introduced methods for the inference of partial colexifications, we show how they can be used to improve concept embeddings in meaningful ways. The learned embeddings are evaluated against lexical similarity ratings, recorded instances of semantic shift, and word association data. We show that in all evaluation tasks, the inclusion of partial colexifications lead to improved concept representations and better results. Our results further show that the learned embeddings are able to capture and represent different semantic relationships between concepts.</abstract>
      <url hash="44b37975">2025.acl-long.1004</url>
      <bibkey>rubehn-list-2025-partial</bibkey>
    </paper>
    <paper id="1005">
      <title>Improved Unbiased Watermark for Large Language Models</title>
      <author><first>Ruibo</first><last>Chen</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Yihan</first><last>Wu</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Junfeng</first><last>Guo</last></author>
      <author><first>Heng</first><last>Huang</last><affiliation>Department of Computer Science, University of Maryland, College Park</affiliation></author>
      <pages>20587-20601</pages>
      <abstract>As artificial intelligence surpasses human capabilities in text generation, the necessity to authenticate the origins of AI-generated content has become paramount. Unbiased watermarks offer a powerful solution by embedding statistical signals into language model-generated text without distorting the quality. In this paper, we introduce MCmark, a family of unbiased, Multi-Channel-based watermarks. MCmark works by partitioning the model’s vocabulary into segments and promoting token probabilities within a selected segment based on a watermark key. We demonstrate that MCmark not only preserves the original distribution of the language model but also offers significant improvements in detectability and robustness over existing unbiased watermarks. Our experiments with widely-used language models demonstrate an improvement in detectability of over 10% using MCmark, compared to existing state-of-the-art unbiased watermarks. This advancement underscores MCmark’s potential in enhancing the practical application of watermarking in AI-generated texts.</abstract>
      <url hash="940e478a">2025.acl-long.1005</url>
      <bibkey>chen-etal-2025-improved</bibkey>
    </paper>
    <paper id="1006">
      <title><fixed-case>M</fixed-case>a<fixed-case>CP</fixed-case>: Minimal yet Mighty Adaptation via Hierarchical Cosine Projection</title>
      <author><first>Yixian</first><last>Shen</last></author>
      <author><first>Qi</first><last>Bi</last><affiliation>Utrecht University</affiliation></author>
      <author><first>Jia-hong</first><last>Huang</last><affiliation>University of Amsterdam, King Abdullah University of Science and Technology and National Taiwan University</affiliation></author>
      <author><first>Hongyi</first><last>Zhu</last></author>
      <author><first>Andy D.</first><last>Pimentel</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Anuj</first><last>Pathania</last><affiliation>University of Amsterdam</affiliation></author>
      <pages>20602-20618</pages>
      <abstract>We present a new adaptation method MaCP, Minimal yet Mighty adaptive Cosine Projection, that achieves exceptional performance while requiring minimal parameters and memory for fine-tuning large foundation models.Its general idea is to exploit the superior energy compaction and decorrelation properties of cosine projection to improve both model efficiency and accuracy.Specifically, it projects the weight change from the low-rank adaptation into the discrete cosine space.Then, the weight change is partitioned over different levels of the discrete cosine spectrum, and each partition’s most critical frequency components are selected.Extensive experiments demonstrate the effectiveness of MaCP across a wide range of single-modality tasks, including natural language understanding, natural language generation, text summarization, as well as multi-modality tasks such as image classification and video understanding. MaCP consistently delivers superior accuracy, significantly reduced computational complexity, and lower memory requirements compared to existing alternatives.</abstract>
      <url hash="698d9192">2025.acl-long.1006</url>
      <bibkey>shen-etal-2025-macp</bibkey>
    </paper>
    <paper id="1007">
      <title>Multi-Attribute Steering of Language Models via Targeted Intervention</title>
      <author><first>Duy</first><last>Nguyen</last><affiliation>Department of Computer Science, University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Archiki</first><last>Prasad</last></author>
      <author><first>Elias</first><last>Stengel-Eskin</last></author>
      <author><first>Mohit</first><last>Bansal</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <pages>20619-20634</pages>
      <abstract>Inference-time intervention (ITI) has emerged as a promising method for steering large language model (LLM) behavior in a particular direction (e.g., improving helpfulness) by intervening on token representations without costly updates to the LLM’s parameters. However, existing ITI approaches fail to scale to multi-attribute settings with conflicts, such as enhancing helpfulness while also reducing toxicity. To address this, we introduce Multi-Attribute Targeted Steering (MAT-Steer), a novel steering framework designed for selective token-level intervention across multiple attributes. We achieve this by learning steering vectors using an alignment objective that shifts the model’s internal representations of undesirable outputs closer to those of desirable ones while enforcing sparsity and orthogonality among vectors for different attributes, thereby reducing inter-attribute conflicts. We evaluate MAT-Steer in two distinct settings: (i) on question answering (QA) tasks where we balance attributes like truthfulness, bias, and toxicity; (ii) on generative tasks where we simultaneously improve attributes like helpfulness, correctness, and coherence. MAT-Steer outperforms existing ITI and parameter-efficient fine-tuning approaches across both task types (e.g., average 3% accuracy gain across QA tasks and 55.82% win rate against the best ITI baseline).</abstract>
      <url hash="4d39aec8">2025.acl-long.1007</url>
      <bibkey>nguyen-etal-2025-multi</bibkey>
    </paper>
    <paper id="1008">
      <title><fixed-case>A</fixed-case>dapt<fixed-case>A</fixed-case>gent: Adapting Multimodal Web Agents with Few-Shot Learning from Human Demonstrations</title>
      <author><first>Gaurav</first><last>Verma</last></author>
      <author><first>Rachneet</first><last>Kaur</last><affiliation>J.P. Morgan Chase</affiliation></author>
      <author><first>Nishan</first><last>Srishankar</last><affiliation>J.P. Morgan Chase</affiliation></author>
      <author><first>Zhen</first><last>Zeng</last><affiliation>J.P. Morgan Chase</affiliation></author>
      <author><first>Tucker</first><last>Balch</last><affiliation>J.P. Morgan Chase</affiliation></author>
      <author><first>Manuela</first><last>Veloso</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <pages>20635-20651</pages>
      <abstract>State-of-the-art multimodal web agents, powered by Multimodal Large Language Models (MLLMs), can autonomously execute many web tasks by processing user instructions and interacting with graphical user interfaces (GUIs). Current strategies for building web agents rely on (i) the generalizability of underlying MLLMs and their steerability via prompting, and (ii) large-scale fine-tuning of MLLMs on web-related tasks. However, web agents still struggle to automate tasks on unseen websites and domains, limiting their applicability to enterprise-specific and proprietary platforms. Beyond generalization from large-scale pre-training and fine-tuning, we propose building agents for few-shot adaptability using human demonstrations. We introduce the AdaptAgent framework that enables both proprietary and open-weights multimodal web agents to adapt to new websites and domains using few human demonstrations (up to 2). Our experiments on two popular benchmarks — Mind2Web &amp; VisualWebArena — show that using in-context demonstrations (for proprietary models) or meta-adaptation demonstrations (for meta-learned open-weights models) boosts task success rate by 3.36% to 7.21% over non-adapted state-of-the-art models, corresponding to a relative increase of 21.03% to 65.75%. Furthermore, our additional analyses (a) show the effectiveness of multimodal demonstrations over text-only ones, (b) illuminate how different meta-learning data selection strategies influence the agent’s generalization, and (c) demonstrate how the number of few-shot examples affects the web agent’s success rate. Our results offer a complementary axis for developing widely applicable multimodal web agents beyond large-scale pre-training and fine-tuning, emphasizing few-shot adaptability.</abstract>
      <url hash="88131d41">2025.acl-long.1008</url>
      <bibkey>verma-etal-2025-adaptagent</bibkey>
    </paper>
    <paper id="1009">
      <title>Can <fixed-case>LLM</fixed-case>s Identify Critical Limitations within Scientific Research? A Systematic Evaluation on <fixed-case>AI</fixed-case> Research Papers</title>
      <author><first>Zhijian</first><last>Xu</last></author>
      <author><first>Yilun</first><last>Zhao</last><affiliation>Yale University</affiliation></author>
      <author><first>Manasi</first><last>Patwardhan</last><affiliation>Tata Consultancy Services Limited, India</affiliation></author>
      <author><first>Lovekesh</first><last>Vig</last></author>
      <author><first>Arman</first><last>Cohan</last><affiliation>Yale University and Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>20652-20706</pages>
      <abstract>Peer review is fundamental to scientific research, but the growing volume of publications has intensified the challenges of this expertise-intensive process. While LLMs show promise in various scientific tasks, their potential to assist with peer review, particularly in identifying paper limitations, remains understudied. We first present a comprehensive taxonomy of limitation types in scientific research, with a focus on AI. Guided by this taxonomy, for studying limitations, we present LimitGen, the first comprehensive benchmark for evaluating LLMs’ capability to support early-stage feedback and complement human peer review. Our benchmark consists of two subsets: LimitGen-Syn, a synthetic dataset carefully created through controlled perturbations of high-quality papers, and LimitGen-Human, a collection of real human-written limitations. To improve the ability of LLM systems to identify limitations, we augment them with literature retrieval, which is essential for grounding identifying limitations in prior scientific findings. Our approach enhances the capabilities of LLM systems to generate limitations in research papers, enabling them to provide more concrete and constructive feedback.</abstract>
      <url hash="6d4000c0">2025.acl-long.1009</url>
      <bibkey>xu-etal-2025-llms-identify</bibkey>
    </paper>
    <paper id="1010">
      <title>On the Acquisition of Shared Grammatical Representations in Bilingual Language Models</title>
      <author><first>Catherine</first><last>Arnett</last><affiliation>EleutherAI</affiliation></author>
      <author><first>Tyler A.</first><last>Chang</last><affiliation>Google DeepMind and University of California, San Diego</affiliation></author>
      <author><first>James A.</first><last>Michaelov</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Ben</first><last>Bergen</last><affiliation>University of California, San Diego</affiliation></author>
      <pages>20707-20726</pages>
      <abstract>Crosslingual transfer is crucial to contemporary language models’ multilingual capabilities, but how it occurs is not well understood. Weask what happens to a monolingual language model when it begins to be trained on a second language. Specifically, we train small bilingual models for which we control the amount of data for each language and the order of language exposure. To find evidence of shared multilingual representations, we turn to structural priming, a method used to study grammatical representations in humans. We first replicate previous crosslingual structural priming results and find that after controlling for training data quantity and language exposure, there are asymmetrical effects across language pairs and directions. We argue that this asymmetry may shape hypotheses about human structural priming effects. We also find that structural priming effects are less robust for less similar language pairs, highlighting potential limitations of crosslingual transfer learning and shared representations for typologically diverse languages.</abstract>
      <url hash="2ba8d0c1">2025.acl-long.1010</url>
      <bibkey>arnett-etal-2025-acquisition</bibkey>
    </paper>
    <paper id="1011">
      <title>Using Shapley interactions to understand how models use structure</title>
      <author><first>Divyansh</first><last>Singhvi</last></author>
      <author><first>Diganta</first><last>Misra</last><affiliation>Max-Planck-Institute for Intelligent Systems, Max-Planck Institute and ELLIS Institute Tübingen</affiliation></author>
      <author><first>Andrej</first><last>Erkelens</last></author>
      <author><first>Raghav</first><last>Jain</last><affiliation>Indian Institute of Technology, Patna.</affiliation></author>
      <author><first>Isabel</first><last>Papadimitriou</last><affiliation>Harvard University, Harvard University</affiliation></author>
      <author><first>Naomi</first><last>Saphra</last><affiliation>Harvard University</affiliation></author>
      <pages>20727-20737</pages>
      <abstract>Language is an intricately structured system, and a key goal of NLP interpretability is to provide methodological insights for understanding how language models internally represent this structure. In this paper, we use Shapley Taylor interaction indices (STII) in order to examine how language and speech models internally relate and structure their inputs. Pairwise Shapley interactions give us an attribution measure of how much two inputs work together to influence model outputs beyond if we linearly added their independent influences, providing a view into how models encode structural interactions between inputs. We relate the interaction patterns in models to three underlying linguistic structures: syntactic structure, non-compositional semantics, and phonetic interaction. We find that autoregressive text models encode interactions that correlate with the syntactic proximity of inputs, and that both autoregressive and masked models encode nonlinear interactions in idiomatic phrases with non-compositional semantics. Our speech results show that inputs are more entangled for pairs where a neighboring consonant is likely to influence a vowel or approximant, showing that models encode the phonetic interaction needed for extracting discrete phonemic representations.</abstract>
      <url hash="1ed9c256">2025.acl-long.1011</url>
      <bibkey>singhvi-etal-2025-using</bibkey>
    </paper>
    <paper id="1012">
      <title>Adversarial Tokenization</title>
      <author><first>Renato</first><last>Geh</last><affiliation>University of California</affiliation></author>
      <author><first>Zilei</first><last>Shao</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Guy</first><last>Van Den Broeck</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>20738-20765</pages>
      <abstract>Current LLM pipelines account for only one possible tokenization for a given string, ignoring exponentially many alternative tokenizations during training and inference. For example, the <tex-math>\texttt{Llama3}</tex-math> standard tokenization of penguin is <tex-math>\texttt{[p,enguin]}</tex-math>, yet <tex-math>\texttt{[peng,uin]}</tex-math> is another perfectly valid alternative. In this paper, we show that despite LLMs being trained solely on one tokenization, they still retain semantic understanding of other tokenizations, raising questions about their implications in LLM safety. Put succinctly, we answer the following question: can we adversarially tokenize an obviously malicious string to evade safety and alignment restrictions? We show that not only is adversarial tokenization an effective yet previously neglected axis of attack, but it is also competitive against existing state-of-the-art adversarial approaches without changing the text of the harmful request. We empirically validate this exploit across three state-of-the-art LLMs and adversarial datasets, revealing a previously unknown vulnerability in subword models.</abstract>
      <url hash="b1e90b1f">2025.acl-long.1012</url>
      <bibkey>geh-etal-2025-adversarial</bibkey>
    </paper>
    <paper id="1013">
      <title>Classifying Unreliable Narrators with Large Language Models</title>
      <author><first>Anneliese</first><last>Brei</last></author>
      <author><first>Katharine</first><last>Henry</last><affiliation>Independent Researcher</affiliation></author>
      <author><first>Abhisheik</first><last>Sharma</last><affiliation>Virginia Polytechnic Institute and State University and Department of Computer Science, University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Shashank</first><last>Srivastava</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Snigdha</first><last>Chaturvedi</last><affiliation>Department of Computer Science, University of North Carolina at Chapel Hill</affiliation></author>
      <pages>20766-20791</pages>
      <abstract>Often when we interact with a first-person account of events, we consider whether or not the narrator, the primary speaker of the text, is reliable. In this paper, we propose using computational methods to identify unreliable narrators, i.e. those who unintentionally misrepresent information. Borrowing literary theory from narratology to define different types of unreliable narrators based on a variety of textual phenomena, we present TUNa, a human-annotated dataset of narratives from multiple domains, including blog posts, subreddit posts, hotel reviews, and works of literature. We define classification tasks for intra-narrational, inter-narrational, and inter-textual unreliabilities and analyze the performance of popular open-weight and proprietary LLMs for each. We propose learning from literature to perform unreliable narrator classification on real-world text data. To this end, we experiment with few-shot, fine-tuning, and curriculum learning settings. Our results show that this task is very challenging, and there is potential for using LLMs to identify unreliable narrators. We release our expert-annotated dataset and code at https://github.com/adbrei/unreliable-narrators and invite future research in this area.</abstract>
      <url hash="8e825bba">2025.acl-long.1013</url>
      <bibkey>brei-etal-2025-classifying</bibkey>
    </paper>
    <paper id="1014">
      <title><fixed-case>C</fixed-case>oncept<fixed-case>C</fixed-case>arve: Dynamic Realization of Evidence</title>
      <author><first>Eylon</first><last>Caplan</last><affiliation>Purdue University</affiliation></author>
      <author><first>Dan</first><last>Goldwasser</last><affiliation>Purdue University and Purdue University</affiliation></author>
      <pages>20792-20809</pages>
      <abstract>Finding evidence for human opinion and behavior at scale is a challenging task, often requiring an understanding of sophisticated thought patterns among vast online communities found on social media. For example, studying how ‘gun ownership’ is related to the perception of ‘Freedom’, requires a retrieval system that can operate at scale over social media posts, while dealing with two key challenges: (1) identifying abstract concept instances, (2) which can be instantiated differently across different communities. To address these, we introduce ConceptCarve, an evidence retrieval framework that utilizes traditional retrievers and LLMs to dynamically characterize the search space during retrieval. Our experiments show that ConceptCarve surpasses traditional retrieval systems in finding evidence within a social media community. It also produces an interpretable representation of the evidence for that community, which we use to qualitatively analyze complex thought patterns that manifest differently across the communities.</abstract>
      <url hash="299c2148">2025.acl-long.1014</url>
      <bibkey>caplan-goldwasser-2025-conceptcarve</bibkey>
    </paper>
    <paper id="1015">
      <title><fixed-case>QQSUM</fixed-case>: A Novel Task and Model of Quantitative Query-Focused Summarization for Review-based Product Question Answering</title>
      <author><first>An Quang</first><last>Tang</last></author>
      <author><first>Xiuzhen</first><last>Zhang</last><affiliation>Royal Melbourne Institute of Technology</affiliation></author>
      <author><first>Minh Ngoc</first><last>Dinh</last></author>
      <author><first>Zhuang</first><last>Li</last><affiliation>Royal Melbourne Institute of Technology</affiliation></author>
      <pages>20810-20831</pages>
      <abstract>Review-based Product Question Answering (PQA) allows e-commerce platforms to automatically address customer queries by leveraging insights from user reviews. However, existing PQA systems generate answers with only a single perspective, failing to capture the diversity of customer opinions. In this paper we introduce a novel task Quantitative Query-Focused Summarization (QQSUM), which aims to summarize diverse customer opinions into representative Key Points (KPs) and quantify their prevalence to effectively answer user queries. While Retrieval-Augmented Generation (RAG) shows promise for PQA, its generated answers still fall short of capturing the full diversity of viewpoints. To tackle this challenge, our model QQSUM-RAG, which extends RAG, employs few-shot learning to jointly train a KP-oriented retriever and a KP summary generator, enabling KP-based summaries that capture diverse and representative opinions. Experimental results demonstrate that QQSUM-RAG achieves superior performance compared to state-of-the-art RAG baselines in both textual quality and quantification accuracy of opinions. Our source code is available at: https://github.com/antangrocket1312/QQSUMM</abstract>
      <url hash="79c6bd80">2025.acl-long.1015</url>
      <bibkey>tang-etal-2025-qqsum</bibkey>
    </paper>
    <paper id="1016">
      <title>Navigating Rifts in Human-<fixed-case>LLM</fixed-case> Grounding: Study and Benchmark</title>
      <author><first>Omar</first><last>Shaikh</last><affiliation>Stanford University</affiliation></author>
      <author><first>Hussein</first><last>Mozannar</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Gagan</first><last>Bansal</last><affiliation>Microsoft</affiliation></author>
      <author><first>Adam</first><last>Fourney</last></author>
      <author><first>Eric</first><last>Horvitz</last></author>
      <pages>20832-20847</pages>
      <abstract>Language models excel at following instructions but often struggle with the collaborative aspects of conversation that humans naturally employ. This limitation in grounding—the process by which conversation participants establish mutual understanding—can lead to outcomes ranging from frustrated users to serious consequences in high-stakes scenarios. To systematically study grounding challenges in human-LLM interactions, we analyze logs from three human-assistant datasets: WildChat, MultiWOZ, and Bing Chat. We develop a taxonomy of grounding acts and build models to annotate and forecast grounding behavior. Our findings reveal significant differences in human-human and human-LLM grounding: LLMs were three times less likely to initiate clarification and sixteen times less likely to provide follow-up requests than humans. Additionally, we find that early grounding failures predict later interaction breakdowns. Building on these insights, we introduce Rifts, a benchmark derived from publicly available LLM interaction data containing situations where LLMs fail to initiate grounding. We note that current frontier models perform poorly on Rifts, highlighting the need to reconsider how we train and prompt LLMs for human interaction. To this end, we develop a preliminary intervention aimed at mitigating grounding failures.</abstract>
      <url hash="0f931cb0">2025.acl-long.1016</url>
      <bibkey>shaikh-etal-2025-navigating</bibkey>
    </paper>
    <paper id="1017">
      <title>Substance over Style: Evaluating Proactive Conversational Coaching Agents</title>
      <author><first>Vidya</first><last>Srinivas</last></author>
      <author><first>Xuhai</first><last>Xu</last><affiliation>Columbia University</affiliation></author>
      <author><first>Xin</first><last>Liu</last><affiliation>Google</affiliation></author>
      <author><first>Kumar</first><last>Ayush</last><affiliation>Google</affiliation></author>
      <author><first>Isaac</first><last>Galatzer-Levy</last></author>
      <author><first>Shwetak</first><last>Patel</last><affiliation>Google and University of Washington</affiliation></author>
      <author><first>Daniel</first><last>McDuff</last><affiliation>Google</affiliation></author>
      <author><first>Tim</first><last>Althoff</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <pages>20848-20880</pages>
      <abstract>While NLP research has made strides in conversational tasks, many approaches focus on single-turn responses with well-defined objectives or evaluation criteria. In contrast, coaching presents unique challenges with initially undefined goals that evolve through multi-turn interactions, subjective evaluation criteria, mixed-initiative dialogue. In this work, we describe and implement five multi-turn coaching agents that exhibit distinct conversational styles, and evaluate them through a user study, collecting first-person feedback on 155 conversations. We find that users highly value core functionality, and that stylistic components in absence of core components are viewed negatively. By comparing user feedback with third-person evaluations from health experts and an LM, we reveal significant misalignment across evaluation approaches. Our findings provide insights into design and evaluation of conversational coaching agents and contribute toward improving human-centered NLP applications.</abstract>
      <url hash="d6f96aab">2025.acl-long.1017</url>
      <bibkey>srinivas-etal-2025-substance</bibkey>
    </paper>
    <paper id="1018">
      <title>Open-World Planning via Lifted Regression with <fixed-case>LLM</fixed-case>-Inferred Affordances for Embodied Agents</title>
      <author><first>Xiaotian</first><last>Liu</last></author>
      <author><first>Ali</first><last>Pesaranghader</last><affiliation>LG Electronics</affiliation></author>
      <author><first>Hanze</first><last>Li</last></author>
      <author><first>Punyaphat</first><last>Sukcharoenchaikul</last></author>
      <author><first>Jaehong</first><last>Kim</last><affiliation>Korea Advanced Institute of Science &amp; Technology and LG Electronics</affiliation></author>
      <author><first>Tanmana</first><last>Sadhu</last><affiliation>LG Corporation</affiliation></author>
      <author><first>Hyejeong</first><last>Jeon</last><affiliation>LG Electronics</affiliation></author>
      <author><first>Scott</first><last>Sanner</last><affiliation>Department of Mechanical and Industrial Engineering, University of Toronto and Department of Computer Science</affiliation></author>
      <pages>20881-20897</pages>
      <abstract>Open-world planning with incomplete knowledge is crucial for real-world embodied AI tasks. Despite that, existing LLM-based planners struggle with long chains of sequential reasoning, while symbolic planners face combinatorial explosion of states and actions for complex domains due to reliance on grounding. To address these deficiencies, we introduce LLM-Regress, an open-world planning approach integrating lifted regression with LLM-generated affordances. LLM-Regress generates sound and complete plans in a compact lifted form, avoiding exhaustive enumeration of irrelevant states and actions. Additionally, it makes efficient use of LLMs to infer goal-related objects and affordances without the need to predefine all possible objects and affordances. We conduct extensive experiments on three benchmarks and show that LLM-Regress significantly outperforms state-of-the-art LLM planners and a grounded planner using LLM-generated affordances.</abstract>
      <url hash="1da550b8">2025.acl-long.1018</url>
      <bibkey>liu-etal-2025-open-world</bibkey>
    </paper>
    <paper id="1019">
      <title>(<fixed-case>RSA</fixed-case>)²: A Rhetorical-Strategy-Aware Rational Speech Act Framework for Figurative Language Understanding</title>
      <author><first>Cesare</first><last>Spinoso-Di Piano</last><affiliation>, McGill University</affiliation></author>
      <author><first>David Eric</first><last>Austin</last></author>
      <author><first>Pablo</first><last>Piantanida</last><affiliation>Université Paris-Saclay, CNRS</affiliation></author>
      <author><first>Jackie CK</first><last>Cheung</last><affiliation>McGill University, Mila Research Institute and Microsoft</affiliation></author>
      <pages>20898-20938</pages>
      <abstract>Figurative language (e.g., irony, hyperbole, understatement) is ubiquitous in human communication, resulting in utterances where the literal and the intended meanings do not match. The Rational Speech Act (RSA) framework, which explicitly models speaker intentions, is the most widespread theory of probabilistic pragmatics, but existing implementations are either unable to account for figurative expressions or require modeling the implicit motivations for using figurative language (e.g., to express joy or annoyance) in a setting-specific way. In this paper, we introduce the Rhetorical-Strategy-Aware RSA (RSA)² framework which models figurative language use by considering a speaker’s employed rhetorical strategy. We show that (RSA)² enables human-compatible interpretations of non-literal utterances without modeling a speaker’s motivations for being non-literal. Combined with LLMs, it achieves state-of-the-art performance on the ironic split of PragMega+, a new irony interpretation dataset introduced in this study.</abstract>
      <url hash="832f16b7">2025.acl-long.1019</url>
      <bibkey>spinoso-di-piano-etal-2025-rsa</bibkey>
    </paper>
    <paper id="1020">
      <title><fixed-case>SYNTHIA</fixed-case>: Novel Concept Design with Affordance Composition</title>
      <author><first>Hyeonjeong</first><last>Ha</last><affiliation>Department of Computer Science, University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Xiaomeng</first><last>Jin</last></author>
      <author><first>Jeonghwan</first><last>Kim</last></author>
      <author><first>Jiateng</first><last>Liu</last></author>
      <author><first>Zhenhailong</first><last>Wang</last></author>
      <author><first>Khanh Duy</first><last>Nguyen</last></author>
      <author><first>Ansel</first><last>Blume</last><affiliation>University of Illinois Urbana Champaign</affiliation></author>
      <author><first>Nanyun</first><last>Peng</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Kai-Wei</first><last>Chang</last><affiliation>University of California, Los Angeles and Amazon</affiliation></author>
      <author><first>Heng</first><last>Ji</last><affiliation>University of Illinois, Urbana-Champaign</affiliation></author>
      <pages>20939-20958</pages>
      <abstract>Text-to-image (T2I) models enable rapid concept design, making them widely used in AI-driven design. While recent studies focus on generating semantic and stylistic variations of given design concepts, –the integration of multiple affordances into a single coherent concept–remains largely overlooked. In this paper, we introduce SYNTHIA, a framework for generating novel, functionally coherent designs based on desired affordances. Our approach leverages a hierarchical concept ontology that decomposes concepts into parts and affordances, serving as a crucial building block for functionally coherent design. We also develop a curriculum learning scheme based on our ontology that contrastively fine-tunes T2I models to progressively learn affordance composition while maintaining visual novelty. To elaborate, we (i) gradually increase affordance distance, guiding models from basic concept-affordance association to complex affordance compositions that integrate parts of distinct affordances into a single, coherent form, and (ii) enforce visual novelty by employing contrastive objectives to push learned representations away from existing concepts. Experimental results show that SYNTHIA outperforms state-of-the-art T2I models, demonstrating absolute gains of 25.1% and 14.7% for novelty and functional coherence in human evaluation, respectively.</abstract>
      <url hash="bacc9577">2025.acl-long.1020</url>
      <bibkey>ha-etal-2025-synthia</bibkey>
    </paper>
    <paper id="1021">
      <title>Consistent Client Simulation for Motivational Interviewing-based Counseling</title>
      <author><first>Yizhe</first><last>Yang</last></author>
      <author><first>Palakorn</first><last>Achananuparp</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Heyan</first><last>Huang</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Jing</first><last>Jiang</last><affiliation>Australian National University and Singapore Management University</affiliation></author>
      <author><first>Nicholas Gabriel</first><last>Lim</last><affiliation>Singapore University of Social Sciences</affiliation></author>
      <author><first>Cameron Tan Shi</first><last>Ern</last></author>
      <author><first>Phey Ling</first><last>Kit</last></author>
      <author><first>Jenny Giam</first><last>Xiuhui</last></author>
      <author><first>John</first><last>Pinto</last></author>
      <author><first>Ee-Peng</first><last>Lim</last><affiliation>Singapore Management University</affiliation></author>
      <pages>20959-20998</pages>
      <abstract>Simulating human clients in mental health counseling is crucial for training and evaluating counselors (both human or simulated) in a scalable manner. Nevertheless, past research on client simulation did not focus on complex conversation tasks such as mental health counseling. In these tasks, the challenge is to ensure that the client’s actions (i.e., interactions with the counselor) are consistent with with its stipulated profiles and negative behavior settings. In this paper, we propose a novel framework that supports consistent client simulation for mental health counseling. Our framework tracks the mental state of a simulated client, controls its state transitions, and generates for each state behaviors consistent with the client’s motivation, beliefs, preferred plan to change, and receptivity. By varying the client profile and receptivity, we demonstrate that consistent simulated clients for different counseling scenarios can be effectively created. Both our automatic and expert evaluations on the generated counseling sessions also show that our client simulation method achieves higher consistency than previous methods.</abstract>
      <url hash="34704c1a">2025.acl-long.1021</url>
      <bibkey>yang-etal-2025-consistent</bibkey>
    </paper>
    <paper id="1022">
      <title><fixed-case>AUTALIC</fixed-case>: A Dataset for Anti-<fixed-case>AUT</fixed-case>istic Ableist Language In Context</title>
      <author><first>Naba</first><last>Rizvi</last></author>
      <author><first>Harper</first><last>Strickland</last></author>
      <author><first>Daniel</first><last>Gitelman</last></author>
      <author><first>Alexis Morales</first><last>Flores</last></author>
      <author><first>Tristan</first><last>Cooper</last></author>
      <author><first>Aekta</first><last>Kallepalli</last></author>
      <author><first>Akshat</first><last>Alurkar</last></author>
      <author><first>Haaset</first><last>Owens</last></author>
      <author><first>Saleha</first><last>Ahmedi</last></author>
      <author><first>Isha</first><last>Khirwadkar</last></author>
      <author><first>Imani N. S.</first><last>Munyaka</last><affiliation>Computer Science and Engineering Department, University of California, San Diego</affiliation></author>
      <author><first>Nedjma</first><last>Ousidhoum</last><affiliation>Cardiff University</affiliation></author>
      <pages>20999-21015</pages>
      <abstract>As our awareness of autism and ableism continues to increase, so does our understanding of ableist language towards autistic people. Such language poses a significant challenge in NLP research due to its subtle and context-dependent nature. Yet, detecting anti-autistic ableist language remains underexplored, with existing NLP tools often failing to capture its nuanced expressions. We present AUTALIC, the first dataset dedicated to the detection of anti-autistic ableist language in context, addressing a significant gap in the field. AUTALIC comprises 2,400 autism-related sentences collected from Reddit, accompanied by surrounding context, and annotated by trained experts with backgrounds in neurodiversity. Our comprehensive evaluation reveals that current language models, including state-of-the-art LLMs, struggle to reliably identify anti-autistic ableism and diverge from human judgments, underscoring their limitations in this domain. We publicly release our dataset along with the individual annotations, providing an essential resource for developing more inclusive and context-aware NLP systems that better reflect diverse perspectives.</abstract>
      <url hash="a895b19a">2025.acl-long.1022</url>
      <bibkey>rizvi-etal-2025-autalic</bibkey>
    </paper>
    <paper id="1023">
      <title>Structural Reasoning Improves Molecular Understanding of <fixed-case>LLM</fixed-case></title>
      <author><first>Yunhui</first><last>Jang</last><affiliation>Pohang University of Science and Technology</affiliation></author>
      <author><first>Jaehyung</first><last>Kim</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Sungsoo</first><last>Ahn</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <pages>21016-21036</pages>
      <abstract>Recently, large language models (LLMs) have shown significant progress, approaching human perception levels. In this work, we demonstrate that despite these advances, LLMs still struggle to reason using molecular structural information. This gap is critical because many molecular properties, including functional groups, depend heavily on such structural details. To address this limitation, we propose an approach that sketches molecular structures for reasoning. Specifically, we introduce Molecular Structural Reasoning (MSR) framework to enhance the understanding of LLMs by explicitly incorporating the key structural features. We present two frameworks for scenarios where the target molecule is known or unknown. We verify that our MSR improves molecular understanding through extensive experiments.</abstract>
      <url hash="ef9a6ec6">2025.acl-long.1023</url>
      <bibkey>jang-etal-2025-structural</bibkey>
    </paper>
    <paper id="1024">
      <title><fixed-case>CAMI</fixed-case>: A Counselor Agent Supporting Motivational Interviewing through State Inference and Topic Exploration</title>
      <author><first>Yizhe</first><last>Yang</last></author>
      <author><first>Palakorn</first><last>Achananuparp</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Heyan</first><last>Huang</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Jing</first><last>Jiang</last><affiliation>Australian National University and Singapore Management University</affiliation></author>
      <author><first>Phey Ling</first><last>Kit</last></author>
      <author><first>Nicholas Gabriel</first><last>Lim</last><affiliation>Singapore University of Social Sciences</affiliation></author>
      <author><first>Cameron Tan Shi</first><last>Ern</last></author>
      <author><first>Ee-Peng</first><last>Lim</last><affiliation>Singapore Management University</affiliation></author>
      <pages>21037-21081</pages>
      <abstract>Conversational counselor agents have become essential tools for addressing the rising demand for scalable and accessible mental health support. This paper introduces CAMI, a novel automated counselor agent grounded in Motivational Interviewing (MI) – a client-centered counseling approach designed to address ambivalence and facilitate behavior change. CAMI employs a novel STAR framework, consisting of client’s state inference, motivation topic exploration, and response generation modules, leveraging large language models (LLMs). These components work together to evoke change talk, aligning with MI principles and improving counseling outcomes for diverse clients. We evaluate CAMI’s performance through both automated and expert evaluations, utilizing simulated clients to assess MI skill competency, client’s state inference accuracy, topic exploration proficiency, and overall counseling success. Results show that CAMI not only outperforms several state-of-the-art methods but also shows more realistic counselor-like behavior. Additionally, our ablation study underscores the critical roles of state inference and topic exploration in achieving this performance.</abstract>
      <url hash="9973639e">2025.acl-long.1024</url>
      <bibkey>yang-etal-2025-cami</bibkey>
    </paper>
    <paper id="1025">
      <title>Know You First and Be You Better: Modeling Human-Like User Simulators via Implicit Profiles</title>
      <author><first>Kuang</first><last>Wang</last></author>
      <author><first>Xianfei</first><last>Li</last></author>
      <author><first>Shenghao</first><last>Yang</last></author>
      <author><first>Li</first><last>Zhou</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <author><first>Feng</first><last>Jiang</last><affiliation>Shenzhen University of Advanced Technology</affiliation></author>
      <author><first>Haizhou</first><last>Li</last><affiliation>The Chinese University of Hong Kong (Shenzhen); National University of Singapore and National University of Singapore</affiliation></author>
      <pages>21082-21107</pages>
      <abstract>User simulators are crucial for replicating human interactions with dialogue systems, supporting both collaborative training and automatic evaluation, especially for large language models (LLMs). However, current role-playing methods face challenges such as a lack of utterance-level authenticity and user-level diversity, often hindered by role confusion and dependence on predefined profiles of well-known figures. In contrast, direct simulation focuses solely on text, neglecting implicit user traits like personality and conversation-level consistency. To address these issues, we introduce the User Simulator with Implicit Profiles (USP), a framework that infers implicit user profiles from human-machine interactions to simulate personalized and realistic dialogues. We first develop an LLM-driven extractor with a comprehensive profile schema, then refine the simulation using conditional supervised fine-tuning and reinforcement learning with cycle consistency, optimizing at both the utterance and conversation levels. Finally, a diverse profile sampler captures the distribution of real-world user profiles. Experimental results show that USP outperforms strong baselines in terms of authenticity and diversity while maintaining comparable consistency. Additionally, using USP to evaluate LLM on dynamic multi-turn aligns well with mainstream benchmarks, demonstrating its effectiveness in real-world applications.</abstract>
      <url hash="5b5bd45e">2025.acl-long.1025</url>
      <bibkey>wang-etal-2025-know</bibkey>
    </paper>
    <paper id="1026">
      <title>Targeted Syntactic Evaluation for Grammatical Error Correction</title>
      <author><first>Aomi</first><last>Koyama</last><affiliation>Tokyo Metropolitan University and KDDI Corporation</affiliation></author>
      <author><first>Masato</first><last>Mita</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Su-Youn</first><last>Yoon</last><affiliation>EduLab</affiliation></author>
      <author><first>Yasufumi</first><last>Takama</last><affiliation>Tokyo Metropolitan University</affiliation></author>
      <author><first>Mamoru</first><last>Komachi</last><affiliation>Hitotsubashi University</affiliation></author>
      <pages>21108-21125</pages>
      <abstract>Language learners encounter a wide range of grammar items across the beginner, intermediate, and advanced levels.To develop grammatical error correction (GEC) models effectively, it is crucial to identify which grammar items are easier or more challenging for models to correct. However, conventional benchmarks based on learner-produced texts are insufficient for conducting detailed evaluations of GEC model performance across a wide range of grammar items due to biases in their distribution.To address this issue, we propose a new evaluation paradigm that assesses GEC models using minimal pairs of ungrammatical and grammatical sentences for each grammar item. As the first benchmark within this paradigm, we introduce the CEFR-based Targeted Syntactic Evaluation Dataset for Grammatical Error Correction (CTSEG), which complements existing English benchmarks by enabling fine-grained analyses previously unattainable with conventional datasets. Using CTSEG, we evaluate three mainstream types of English GEC models: sequence-to-sequence models, sequence tagging models, and prompt-based models. The results indicate that while current models perform well on beginner-level grammar items, their performance deteriorates substantially for intermediate and advanced items.</abstract>
      <url hash="0f66cf12">2025.acl-long.1026</url>
      <bibkey>koyama-etal-2025-targeted</bibkey>
    </paper>
    <paper id="1027">
      <title><fixed-case>VF</fixed-case>-Eval: Evaluating Multimodal <fixed-case>LLM</fixed-case>s for Generating Feedback on <fixed-case>AIGC</fixed-case> Videos</title>
      <author><first>Tingyu</first><last>Song</last><affiliation>University of the Chinese Academy of Sciences</affiliation></author>
      <author><first>Tongyan</first><last>Hu</last></author>
      <author><first>Guo</first><last>Gan</last></author>
      <author><first>Yilun</first><last>Zhao</last><affiliation>Yale University</affiliation></author>
      <pages>21126-21146</pages>
      <abstract>Recently, multimodal large language models (MLLMs) have been extensively explored in video question answering. However, most existing assessments focus on natural videos, overlooking synthetic videos (e.g., AI-generated content). Meanwhile, some works in video generation rely on MLLMs to evaluate the quality of generated videos, but the capabilities of MLLMs on AIGC videos remain largely underexplored. To address this, we propose a new benchmark, VQ-Eval, which introduces four tasks—coherence validation, error awareness, error type detection, and reasoning evaluation—to comprehensively evaluate the abilities of MLLMs on AIGC videos. We evaluate 13 frontier MLLMs on VQ-Eval and find that even the best-performing model, GPT-4.1, struggles to achieve consistently good performance across all tasks. This highlights the challenging nature of our benchmark. Additionally, to investigate the practical applications of VQ-Eval in improving video generation, we design a re-prompt pipeline, demonstrating that aligning MLLMs more closely with human feedback can benefit the video generation.</abstract>
      <url hash="5e3e5142">2025.acl-long.1027</url>
      <bibkey>song-etal-2025-vf</bibkey>
    </paper>
    <paper id="1028">
      <title>Language Model Fine-Tuning on Scaled Survey Data for Predicting Distributions of Public Opinions</title>
      <author><first>Joseph</first><last>Suh</last></author>
      <author><first>Erfan</first><last>Jahanparast</last><affiliation>University of California, Berkeley</affiliation></author>
      <author><first>Suhong</first><last>Moon</last></author>
      <author><first>Minwoo</first><last>Kang</last><affiliation>University of California, Berkeley</affiliation></author>
      <author><first>Serina</first><last>Chang</last><affiliation>University of California, Berkeley</affiliation></author>
      <pages>21147-21170</pages>
      <abstract>Large language models (LLMs) present novel opportunities in public opinion research by predicting survey responses in advance during the early stages of survey design. Prior methods steer LLMs via descriptions of subpopulations as LLMs’ input prompt, yet such prompt engineering approaches have struggled to faithfully predict the distribution of survey responses from human subjects. In this work, we propose directly fine-tuning LLMs to predict response distributions by leveraging unique structural characteristics of survey data. To enable fine-tuning, we curate SubPOP, a significantly scaled dataset of 3,362 questions and 70K subpopulation-response pairs from well-established public opinion surveys. We show that fine-tuning on SubPOP greatly improves the match between LLM predictions and human responses across various subpopulations, reducing the LLM-human gap by up to 46% compared to baselines, and achieves strong generalization to unseen surveys and subpopulations. Our findings highlight the potential of survey-based fine-tuning to improve opinion prediction for diverse, real-world subpopulations and therefore enable more efficient survey designs.</abstract>
      <url hash="c38faf24">2025.acl-long.1028</url>
      <bibkey>suh-etal-2025-language</bibkey>
    </paper>
    <paper id="1029">
      <title><fixed-case>TESS</fixed-case> 2: A Large-Scale Generalist Diffusion Language Model</title>
      <author><first>Jaesung</first><last>Tae</last><affiliation>Yale University</affiliation></author>
      <author><first>Hamish</first><last>Ivison</last><affiliation>University of Washington</affiliation></author>
      <author><first>Sachin</first><last>Kumar</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <author><first>Arman</first><last>Cohan</last><affiliation>Yale University and Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>21171-21188</pages>
      <abstract>We introduce TESS 2, a general instruction-following diffusion language model that outperforms contemporary instruction-tuned diffusion models, as well as matches and sometimes exceeds strong autoregressive (AR) models. We train TESS 2 by first adapting a strong AR model via continued pretraining with a diffusion loss and then performing further instruction tuning. We find that adaptation training as well as the choice of the base model is crucial for training good instruction-following diffusion models. We further propose reward guidance, a novel and modular inference-time guidance procedure to align model outputs without needing to train the underlying model. Finally, we show that TESS 2 further improves with increased inference-time compute, highlighting the utility of diffusion LMs in having fine-grained controllability over the amount of compute used at inference time.</abstract>
      <url hash="f9ddbb04">2025.acl-long.1029</url>
      <bibkey>tae-etal-2025-tess</bibkey>
    </paper>
    <paper id="1030">
      <title><fixed-case>K</fixed-case>at<fixed-case>F</fixed-case>ish<fixed-case>N</fixed-case>et: Detecting <fixed-case>LLM</fixed-case>-Generated <fixed-case>K</fixed-case>orean Text through Linguistic Feature Analysis</title>
      <author><first>Shinwoo</first><last>Park</last></author>
      <author><first>Shubin</first><last>Kim</last></author>
      <author><first>Do-Kyung</first><last>Kim</last></author>
      <author><first>Yo-Sub</first><last>Han</last><affiliation>Yonsei University</affiliation></author>
      <pages>21189-21222</pages>
      <abstract>The rapid advancement of large language models (LLMs) increases the difficulty of distinguishing between human-written and LLM-generated text. Detecting LLM-generated text is crucial for upholding academic integrity, preventing plagiarism, protecting copyrights, and ensuring ethical research practices. Most prior studies on detecting LLM-generated text focus primarily on English text. However, languages with distinct morphological and syntactic characteristics require specialized detection approaches. Their unique structures and usage patterns hinder the direct application of methods primarily designed for English. Among such languages, we focus on Korean, which has relatively flexible spacing rules, a rich morphological system, and less frequent comma usage compared to English. We introduce KatFish, the first benchmark dataset for detecting LLM-generated Korean text. The dataset consists of text written by humans and generated by four LLMs across three genres. By examining spacing patterns, part-of-speech diversity, and comma usage, we illuminate the linguistic differences between human-written and LLM-generated Korean text. Building on these observations, we propose KatFishNet, a detection method specifically designed for the Korean language. KatFishNet achieves an average of 19.78% higher AUC-ROC compared to the best-performing existing detection method. Our code and data are available at https://github.com/Shinwoo-Park/katfishnet.</abstract>
      <url hash="97ce5013">2025.acl-long.1030</url>
      <bibkey>park-etal-2025-katfishnet</bibkey>
    </paper>
    <paper id="1031">
      <title>Uncovering the Impact of Chain-of-Thought Reasoning for Direct Preference Optimization: Lessons from Text-to-<fixed-case>SQL</fixed-case></title>
      <author><first>Hanbing</first><last>Liu</last></author>
      <author><first>Haoyang</first><last>Li</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Xiaokang</first><last>Zhang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Ruotong</first><last>Chen</last></author>
      <author><first>Haiyong</first><last>Xu</last><affiliation>China Mobile Information Technology Center</affiliation></author>
      <author><first>Tian</first><last>Tian</last><affiliation>China Mobile Information Technology Center</affiliation></author>
      <author><first>Qi</first><last>Qi</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Jing</first><last>Zhang</last><affiliation>Renmin University of China</affiliation></author>
      <pages>21223-21261</pages>
      <abstract>Direct Preference Optimization (DPO) has proven effective in complex reasoning tasks like math word problems and code generation. However, when applied to Text-to-SQL datasets, it often fails to improve performance and can even degrade it. Our investigation reveals the root cause: unlike math and code tasks, which naturally integrate Chain-of-Thought (CoT) reasoning with DPO, Text-to-SQL datasets typically include only final answers (gold SQL queries) without detailed CoT solutions. By augmenting Text-to-SQL datasets with synthetic CoT solutions, we achieve, for the first time, consistent and significant performance improvements using DPO.Our analysis shows that CoT reasoning is crucial for unlocking DPO’s potential, as it mitigates reward hacking, strengthens discriminative capabilities, and improves scalability. These findings offer valuable insights for building more robust Text-to-SQL models. To support further research, we publicly release the code and CoT-enhanced datasets: https://github.com/RUCKBReasoning/DPO_Text2SQL.</abstract>
      <url hash="838faec1">2025.acl-long.1031</url>
      <bibkey>liu-etal-2025-uncovering</bibkey>
    </paper>
    <paper id="1032">
      <title>On Generalization across Measurement Systems: <fixed-case>LLM</fixed-case>s Entail More Test-Time Compute for Underrepresented Cultures</title>
      <author><first>Minh Duc</first><last>Bui</last><affiliation>Johannes-Gutenberg Universität Mainz</affiliation></author>
      <author><first>Kyung Eun</first><last>Park</last><affiliation>Universität Mannheim</affiliation></author>
      <author><first>Goran</first><last>Glavaš</last><affiliation>Julius-Maximilians-Universität Würzburg</affiliation></author>
      <author><first>Fabian David</first><last>Schmidt</last></author>
      <author><first>Katharina Von Der</first><last>Wense</last><affiliation>Johannes-Gutenberg Universität Mainz, Johannes-Gutenberg Universität Mainz, University of Colorado, Boulder and New York University</affiliation></author>
      <pages>21262-21276</pages>
      <abstract>Measurement systems (e.g., currencies) differ across cultures, but the conversions between them are well defined so that humans can state using any measurement system of their choice. Being available to users from diverse cultural backgrounds, Large Language Models (LLMs) should also be able to provide accurate information irrespective of the measurement system at hand. Using newly compiled datasets we test if this is truly the case for seven open-source LLMs, addressing three key research questions: (RQ1) What is the default system used by LLMs for each type of measurement? (RQ2) Do LLMs’ answers and their accuracy vary across different measurement systems? (RQ3) Can LLMs mitigate potential challenges w.r.t. underrepresented systems via reasoning? Our findings show that LLMs default to the measurement system predominantly used in the data. Additionally, we observe considerable instability and variance in performance across different measurement systems. While this instability can in part be mitigated by employing reasoning methods such as chain-of-thought (CoT), this implies longer responses and thereby significantly increases test-time compute (and inference costs), marginalizing users from cultural backgrounds that use underrepresented measurement systems.</abstract>
      <url hash="189f9c28">2025.acl-long.1032</url>
      <bibkey>bui-etal-2025-generalization</bibkey>
    </paper>
    <paper id="1033">
      <title><fixed-case>CORDIAL</fixed-case>: Can Multimodal Large Language Models Effectively Understand Coherence Relationships?</title>
      <author><first>Aashish</first><last>Anantha Ramakrishnan</last><affiliation>Pennsylvania State University, Pennsylvania State University</affiliation></author>
      <author><first>Aadarsh Anantha</first><last>Ramakrishnan</last></author>
      <author><first>Dongwon</first><last>Lee</last><affiliation>The Pennsylvania State University</affiliation></author>
      <pages>21277-21297</pages>
      <abstract>Multimodal Large Language Models (MLLMs) are renowned for their superior instruction-following and reasoning capabilities across diverse problem domains. However, existing benchmarks primarily focus on assessing factual and logical correctness in downstream tasks, with limited emphasis on evaluating MLLMs’ ability to interpret pragmatic cues and intermodal relationships. To address this gap, we assess the competency of MLLMs in performing Multimodal Discourse Analysis (MDA) using Coherence Relations. Our benchmark, CORDIAL, encompasses a broad spectrum of Coherence Relations across 3 different discourse domains at varying levels of granularity. Through our experiments on 10+ MLLMs employing different prompting strategies, we show that even top models like Gemini 1.5 Pro and GPT-4o fail to match the performance of simple classifier-based baselines. This study emphasizes the need to move beyond similarity-based metrics and adopt a discourse-driven framework for evaluating MLLMs, providing a more nuanced assessment of their capabilities. The benchmark and code are available at: https://aashish2000.github.io/CORDIAL/.</abstract>
      <url hash="6f74da68">2025.acl-long.1033</url>
      <bibkey>anantha-ramakrishnan-etal-2025-cordial</bibkey>
    </paper>
    <paper id="1034">
      <title>Veracity Bias and Beyond: Uncovering <fixed-case>LLM</fixed-case>s’ Hidden Beliefs in Problem-Solving Reasoning</title>
      <author><first>Yue</first><last>Zhou</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <author><first>Barbara</first><last>Di Eugenio</last><affiliation>University of Illinois, Chicago</affiliation></author>
      <pages>21298-21310</pages>
      <abstract>Despite LLMs’ explicit alignment against demographic stereotypes, they have been shown to exhibit biases under various social contexts. In this work, we find that LLMs exhibit concerning biases in how they associate solution veracity with demographics. Through experiments across five human value-aligned LLMs on mathematics, coding, commonsense, and writing problems, we reveal two forms of such veracity biases: Attribution Bias, where models disproportionately attribute correct solutions to certain demographic groups, and Evaluation Bias, where models’ assessment of identical solutions varies based on perceived demographic authorship. Our results show pervasive biases: LLMs consistently attribute fewer correct solutions and more incorrect ones to African-American groups in math and coding, while Asian authorships are least preferred in writing evaluation. In additional studies, we show LLMs automatically assign racially stereotypical colors to demographic groups in visualization code, suggesting these biases are deeply embedded in models’ reasoning processes. Our findings indicate that demographic bias extends beyond surface-level stereotypes and social context provocations, raising concerns about LLMs’ deployment in educational and evaluation settings.</abstract>
      <url hash="fa3aaa58">2025.acl-long.1034</url>
      <bibkey>zhou-di-eugenio-2025-veracity</bibkey>
    </paper>
    <paper id="1035">
      <title>Optimal Transport-Based Token Weighting scheme for Enhanced Preference Optimization</title>
      <author><first>Meng</first><last>Li</last></author>
      <author><first>Guangda</first><last>Huzhang</last></author>
      <author><first>Haibo</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Xiting</first><last>Wang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Anxiang</first><last>Zeng</last><affiliation>Shopee</affiliation></author>
      <pages>21311-21334</pages>
      <abstract>Direct Preference Optimization (DPO) has emerged as a promising framework for aligning Large Language Models (LLMs) with human preferences by directly optimizing the log-likelihood difference between chosen and rejected responses. However, existing methods assign equal importance to all tokens in the response, while humans focus on more meaningful parts. This leads to suboptimal preference optimization, as irrelevant or noisy tokens disproportionately influence DPO loss. To address this limitation, we propose <b>O</b>ptimal <b>T</b>ransport-based token weighting scheme for enhancing direct <b>P</b>reference <b>O</b>ptimization (OTPO). By emphasizing semantically meaningful token pairs and de-emphasizing less relevant ones, our method introduces a context-aware token weighting scheme that yields a more contrastive reward difference estimate. This adaptive weighting enhances reward stability, improves interpretability, and ensures that preference optimization focuses on meaningful differences between responses. Extensive experiments have validated OTPO’s effectiveness in improving instruction-following ability across various settings.</abstract>
      <url hash="f2739863">2025.acl-long.1035</url>
      <bibkey>li-etal-2025-optimal</bibkey>
    </paper>
    <paper id="1036">
      <title><fixed-case>LLM</fixed-case> Meets Scene Graph: Can Large Language Models Understand and Generate Scene Graphs? A Benchmark and Empirical Study</title>
      <author><first>Dongil</first><last>Yang</last></author>
      <author><first>Minjin</first><last>Kim</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Sunghwan</first><last>Kim</last></author>
      <author><first>Beong-woo</first><last>Kwak</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Minjun</first><last>Park</last></author>
      <author><first>Jinseok</first><last>Hong</last></author>
      <author><first>Woontack</first><last>Woo</last><affiliation>KAIST</affiliation></author>
      <author><first>Jinyoung</first><last>Yeo</last><affiliation>Yonsei University</affiliation></author>
      <pages>21335-21360</pages>
      <abstract>The remarkable reasoning and generalization capabilities of Large Language Models (LLMs) have paved the way for their expanding applications in embodied AI, robotics, and other real-world tasks. To effectively support these applications, grounding in spatial and temporal understanding in multimodal environments is essential. To this end, recent works have leveraged scene graphs, a structured representation that encodes entities, attributes, and their relationships in a scene. However, a comprehensive evaluation of LLMs’ ability to utilize scene graphs remains limited. In this work, we introduce Text-Scene Graph (TSG) Bench, a benchmark designed to systematically assess LLMs’ ability to (1) understand scene graphs and (2) generate them from textual narratives. With TSG Bench we evaluate 11 LLMs and reveal that, while models perform well on scene graph understanding, they struggle with scene graph generation, particularly for complex narratives. Our analysis indicates that these models fail to effectively decompose discrete scenes from a complex narrative, leading to a bottleneck when generating scene graphs. These findings underscore the need for improved methodologies in scene graph generation and provide valuable insights for future research. The demonstration of our benchmark is available at https://tsg-bench.netlify.app. Additionally, our code and evaluation data are publicly available at https://github.com/docworlds/tsg-bench.</abstract>
      <url hash="6bcbb750">2025.acl-long.1036</url>
      <bibkey>yang-etal-2025-llm-meets</bibkey>
    </paper>
    <paper id="1037">
      <title>Beyond Frameworks: Unpacking Collaboration Strategies in Multi-Agent Systems</title>
      <author><first>Haochun</first><last>Wang</last></author>
      <author><first>Sendong</first><last>Zhao</last></author>
      <author><first>Jingbo</first><last>Wang</last></author>
      <author><first>Zewen</first><last>Qiang</last></author>
      <author><first>Bing</first><last>Qin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Ting</first><last>Liu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>21361-21375</pages>
      <abstract>Multi-agent collaboration has emerged as a pivotal paradigm for addressing complex, distributed tasks in large language model (LLM)-driven applications. While prior research has focused on high-level architectural frameworks, the granular mechanisms governing agents—critical to performance and scalability—remain underexplored. This study systematically investigates four dimensions of collaboration strategies: (1) agent governance, (2) participation control, (3) interaction dynamics, and (4) dialogue history management. Through rigorous experimentation under two context-dependent scenarios—Distributed Evidence Integration (DEI) and Structured Evidence Synthesis (SES)—we quantify the impact of these strategies on both task accuracy and computational efficiency. Our findings reveal that centralized governance, instructor-led participation, ordered interaction patterns, and instructor-curated context summarization collectively optimize the trade-off between decision quality and resource utilization with the support of the proposed Token-Accuracy Ratio (TAR). This work establishes a foundation for designing adaptive, scalable multi-agent systems, shifting the focus from structural novelty to strategic interaction mechanics.</abstract>
      <url hash="194cb240">2025.acl-long.1037</url>
      <bibkey>wang-etal-2025-beyond</bibkey>
    </paper>
    <paper id="1038">
      <title>The Invisible Hand: Unveiling Provider Bias in Large Language Models for Code Generation</title>
      <author><first>Xiaoyu</first><last>Zhang</last></author>
      <author><first>Juan</first><last>Zhai</last><affiliation>University of Massachusetts at Amherst</affiliation></author>
      <author><first>Shiqing</first><last>Ma</last><affiliation>University of Massachusetts at Amherst</affiliation></author>
      <author><first>Qingshuang</first><last>Bao</last></author>
      <author><first>Weipeng</first><last>Jiang</last></author>
      <author><first>Qian</first><last>Wang</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Chao</first><last>Shen</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author id="yang-liu"><first>Yang</first><last>Liu</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>21376-21403</pages>
      <abstract>Large Language Models (LLMs) have emerged as the new recommendation engines, surpassing traditional methods in both capability and scope, particularly in code generation. In this paper, we reveal a novel **provider bias** in LLMs: without explicit directives, these models show systematic preferences for services from specific providers in their recommendations (e.g., favoring Google Cloud over Microsoft Azure). To systematically investigate this bias, we develop an automated pipeline to construct the dataset, incorporating 6 distinct coding task categories and 30 real-world application scenarios. Leveraging this dataset, we conduct the **first** comprehensive empirical study of provider bias in LLM code generation across seven state-of-the-art LLMs, utilizing approximately 500 million tokens (equivalent to $5,000+ in computational costs). Our findings reveal that LLMs exhibit significant provider preferences, predominantly favoring services from Google and Amazon, and can autonomously modify input code to incorporate their preferred providers without users’ requests. Such a bias holds far-reaching implications for market dynamics and societal equilibrium, potentially contributing to digital monopolies. It may also deceive users and violate their expectations, leading to various consequences. We call on the academic community to recognize this emerging issue and develop effective evaluation and mitigation methods to uphold AI security and fairness.</abstract>
      <url hash="8c0eada7">2025.acl-long.1038</url>
      <bibkey>zhang-etal-2025-invisible</bibkey>
    </paper>
    <paper id="1039">
      <title>K/<fixed-case>DA</fixed-case>: Automated Data Generation Pipeline for Detoxifying Implicitly Offensive Language in <fixed-case>K</fixed-case>orean</title>
      <author><first>Minkyeong</first><last>Jeon</last></author>
      <author><first>Hyemin</first><last>Jeong</last></author>
      <author><first>Yerang</first><last>Kim</last></author>
      <author><first>Jiyoung</first><last>Kim</last></author>
      <author><first>Jae Hyeon</first><last>Cho</last><affiliation>Korea University</affiliation></author>
      <author><first>Byung-Jun</first><last>Lee</last><affiliation>Korea University and Gauss Labs Inc.</affiliation></author>
      <pages>21404-21432</pages>
      <abstract>Language detoxification involves removing toxicity from offensive language. While a neutral-toxic paired dataset provides a straightforward approach for training detoxification models, creating such datasets presents several challenges: i) the need for human annotation to build paired data, and ii) the rapid evolution of offensive terms, rendering static datasets quickly outdated. To tackle these challenges, we introduce an automated paired data generation pipeline, called K/DA. This pipeline is designed to generate offensive language with implicit offensiveness and trend-aligned slang, making the resulting dataset suitable for detoxification model training. We demonstrate that the dataset generated by K/DA exhibits high pair consistency and greater implicit offensiveness compared to existing Korean datasets, and also demonstrates applicability to other languages. Furthermore, it enables effective training of a high-performing detoxification model with simple instruction fine-tuning.</abstract>
      <url hash="9bdc5a05">2025.acl-long.1039</url>
      <bibkey>jeon-etal-2025-k</bibkey>
    </paper>
    <paper id="1040">
      <title><fixed-case>THOR</fixed-case>-<fixed-case>M</fixed-case>o<fixed-case>E</fixed-case>: Hierarchical Task-Guided and Context-Responsive Routing for Neural Machine Translation</title>
      <author><first>Yunlong</first><last>Liang</last></author>
      <author><first>Fandong</first><last>Meng</last><affiliation>WeChat AI, Tencent Inc.</affiliation></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>21433-21445</pages>
      <abstract>The sparse Mixture-of-Experts (MoE) has achieved significant progress for neural machine translation (NMT). However, there exist two limitations in current MoE solutions which may lead to sub-optimal performance: 1) they directly use the task knowledge of NMT into MoE (<i>e.g.</i>, domain/linguistics-specific knowledge), which are generally unavailable at practical application and neglect the naturally grouped domain/linguistic properties; 2) the expert selection only depends on the localized token representation without considering the context, which fully grasps the state of each token in a global view. To address the above limitations, we propose THOR-MoE via arming the MoE with hierarchical task-guided and context-responsive routing policies. Specifically, it 1) firstly predicts the domain/language label and then extracts mixed domain/language representation to allocate task-level experts in a hierarchical manner; 2) injects the context information to enhance the token routing from the pre-selected task-level experts set, which can help each token to be accurately routed to more specialized and suitable experts. Extensive experiments on multi-domain translation and multilingual translation benchmarks with different architectures consistently demonstrate the superior performance of THOR-MoE. Additionally, the THOR-MoE operates as a plug-and-play module compatible with existing Top-(CITATION) or Top-(CITATION) routing schemes, ensuring broad applicability across diverse MoE architectures. For instance, compared with vanilla Top- (CITATION) routing, the context-aware manner can achieve an average improvement of 0.75 BLEU with less than 22% activated parameters on multi-domain translation tasks.</abstract>
      <url hash="42b5d90e">2025.acl-long.1040</url>
      <bibkey>liang-etal-2025-thor</bibkey>
    </paper>
    <paper id="1041">
      <title>Neuron Empirical Gradient: Discovering and Quantifying Neurons’ Global Linear Controllability</title>
      <author><first>Xin</first><last>Zhao</last></author>
      <author><first>Zehui</first><last>Jiang</last></author>
      <author><first>Naoki</first><last>Yoshinaga</last><affiliation>Institute of Industrial Science, the University of Tokyo</affiliation></author>
      <pages>21446-21477</pages>
      <abstract>While feed-forward neurons in pre-trained language models (PLMs) can encode knowledge, past research targeted a small subset of neurons that heavily influence outputs.This leaves the broader role of neuron activations unclear, limiting progress in areas like knowledge editing.We uncover a global linear relationship between neuron activations and outputs using neuron interventions on a knowledge probing dataset.The gradient of this linear relationship, which we call the **neuron empirical gradient (NEG)**, captures how changes in activations affect predictions.To compute NEG efficiently, we propose **NeurGrad**, enabling large-scale analysis of neuron behavior in PLMs.We also show that NEG effectively captures language skills across diverse prompts through skill neuron probing. Experiments on **MCEval8k**, a multi-genre multiple-choice knowledge benchmark, support NEG’s ability to represent model knowledge. Further analysis highlights the key properties of NEG-based skill representation: efficiency, robustness, flexibility, and interdependency.Code and data are released.</abstract>
      <url hash="01349543">2025.acl-long.1041</url>
      <bibkey>zhao-etal-2025-neuron</bibkey>
    </paper>
    <paper id="1042">
      <title>Can Third Parties Read Our Emotions?</title>
      <author><first>Jiayi</first><last>Li</last></author>
      <author><first>Yingfan</first><last>Zhou</last></author>
      <author><first>Pranav</first><last>Narayanan Venkit</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Halima Binte</first><last>Islam</last></author>
      <author><first>Sneha</first><last>Arya</last></author>
      <author><first>Shomir</first><last>Wilson</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Sarah</first><last>Rajtmajer</last><affiliation>Pennsylvania State University</affiliation></author>
      <pages>21478-21499</pages>
      <abstract>Natural Language Processing tasks that aim to infer an author’s private states, e.g., emotions and opinions, from their written text, typically rely on datasets annotated by third-party annotators. However, the assumption that third-party annotators can accurately capture authors’ private states remains largely unexamined. In this study, we present human subjects experiments on emotion recognition tasks that directly compare third-party annotations with first-party (author-provided) emotion labels. Our findings reveal significant limitations in third-party annotations—whether provided by human annotators or large language models (LLMs)—in faithfully representing authors’ private states. However, LLMs outperform human annotators nearly across the board. We further explore methods to improve third-party annotation quality. We find that demographic similarity between first-party authors and third-party human annotators enhances annotation performance. While incorporating first-party demographic information into prompts leads to a marginal but statistically significant improvement in LLMs’ performance. We introduce a framework for evaluating the limitations of third-party annotations and call for refined annotation practices to accurately represent and model authors’ private states.</abstract>
      <url hash="c9f06ab2">2025.acl-long.1042</url>
      <bibkey>li-etal-2025-third</bibkey>
    </paper>
    <paper id="1043">
      <title><fixed-case>OZS</fixed-case>peech: One-step Zero-shot Speech Synthesis with Learned-Prior-Conditioned Flow Matching</title>
      <author><first>Nghia Huynh Nguyen</first><last>Hieu</last><affiliation>FPT AI Center</affiliation></author>
      <author><first>Ngoc Son</first><last>Nguyen</last><affiliation>FPT</affiliation></author>
      <author><first>Huynh Nguyen</first><last>Dang</last><affiliation>FPT</affiliation></author>
      <author><first>Thieu</first><last>Vo</last><affiliation>National University of Singapore and Ton Duc Thang University</affiliation></author>
      <author><first>Truong-Son</first><last>Hy</last><affiliation>University of Alabama at Birmingham</affiliation></author>
      <author><first>Van</first><last>Nguyen</last><affiliation>FPT Software</affiliation></author>
      <pages>21500-21517</pages>
      <abstract>Text-to-speech (TTS) systems have seen significant advancements in recent years, driven by improvements in deep learning and neural network architectures. Viewing the output speech as a data distribution, previous approaches often employ traditional speech representations, such as waveforms or spectrograms, within the Flow Matching framework. However, these methods have limitations, including overlooking various speech attributes and incurring high computational costs due to additional constraints introduced during training. To address these challenges, we introduce OZSpeech, the first TTS method to explore optimal transport conditional flow matching with one-step sampling and a learned prior as the condition, effectively disregarding preceding states and reducing the number of sampling steps. Our approach operates on disentangled, factorized components of speech in token format, enabling accurate modeling of each speech attribute, which enhances the TTS system’s ability to precisely clone the prompt speech. Experimental results show that our method achieves promising performance over existing methods in content accuracy, naturalness, prosody generation, and speaker style preservation. Audio samples are available at our demo page https://ozspeech.github.io/OZSpeech_Web/.</abstract>
      <url hash="2fbde71c">2025.acl-long.1043</url>
      <bibkey>hieu-etal-2025-ozspeech</bibkey>
    </paper>
    <paper id="1044">
      <title>World Modeling Makes a Better Planner: Dual Preference Optimization for Embodied Task Planning</title>
      <author><first>Siyin</first><last>Wang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Zhaoye</first><last>Fei</last></author>
      <author><first>Qinyuan</first><last>Cheng</last></author>
      <author><first>Shiduo</first><last>Zhang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Panpan</first><last>Cai</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Jinlan</first><last>Fu</last></author>
      <author><first>Xipeng</first><last>Qiu</last><affiliation>Fudan University</affiliation></author>
      <pages>21518-21537</pages>
      <abstract>Recent advances in large vision-language models (LVLMs) have shown promise for embodied task planning, yet they struggle with fundamental challenges like dependency constraints and efficiency. Existing approaches either solely optimize action selection or directly leverage pre-trained models as world models during inference, overlooking the benefits of learning to model the world as a way to enhance planning capabilities. We propose Dual Preference Optimization (D<tex-math>^2</tex-math>PO), a new learning framework that jointly optimizes state prediction and action selection through preference learning, enabling LVLMs to understand environment dynamics for better planning. To automatically collect trajectories and stepwise preference data without human annotation, we introduce a tree search mechanism for extensive exploration via trial-and-error. Extensive experiments on VoTa-Bench demonstrate that our D<tex-math>^2</tex-math>PO-based method significantly outperforms existing methods and GPT-4o when applied to Qwen2-VL (7B), LLaVA-1.6 (7B), and LLaMA-3.2 (11B), achieving superior task success rates with more efficient execution paths.</abstract>
      <url hash="09f53f8d">2025.acl-long.1044</url>
      <bibkey>wang-etal-2025-world</bibkey>
    </paper>
    <paper id="1045">
      <title><fixed-case>J</fixed-case>ailbreak<fixed-case>R</fixed-case>adar: Comprehensive Assessment of Jailbreak Attacks Against <fixed-case>LLM</fixed-case>s</title>
      <author><first>Junjie</first><last>Chu</last><affiliation>CISPA Helmholtz Center for Information Security</affiliation></author>
      <author><first>Yugeng</first><last>Liu</last><affiliation>CISPA Helmholtz Center for Information Security</affiliation></author>
      <author><first>Ziqing</first><last>Yang</last><affiliation>CISPA Helmholtz Center for Information Security</affiliation></author>
      <author><first>Xinyue</first><last>Shen</last></author>
      <author><first>Michael</first><last>Backes</last><affiliation>CISPA Helmholtz Center for Information Security</affiliation></author>
      <author><first>Yang</first><last>Zhang</last><affiliation>CISPA Helmholtz Center for Information Security</affiliation></author>
      <pages>21538-21566</pages>
      <abstract>Jailbreak attacks aim to bypass the LLMs’ safeguards. While researchers have proposed different jailbreak attacks in depth, they have done so in isolation—either with unaligned settings or comparing a limited range of methods. To fill this gap, we present a large-scale evaluation of various jailbreak attacks. We collect 17 representative jailbreak attacks, summarize their features, and establish a novel jailbreak attack taxonomy. Then we conduct comprehensive measurement and ablation studies across nine aligned LLMs on 160 forbidden questions from 16 violation categories. Also, we test jailbreak attacks under eight advanced defenses. Based on our taxonomy and experiments, we identify some important patterns, such as heuristic-based attacks, which could achieve high attack success rates but are easy to mitigate by defenses. Our study offers valuable insights for future research on jailbreak attacks and defenses and serves as a benchmark tool for researchers and practitioners to evaluate them effectively.</abstract>
      <url hash="0794ec11">2025.acl-long.1045</url>
      <bibkey>chu-etal-2025-jailbreakradar</bibkey>
    </paper>
    <paper id="1046">
      <title><fixed-case>C</fixed-case>ogni<fixed-case>B</fixed-case>ench: A Legal-inspired Framework and Dataset for Assessing Cognitive Faithfulness of Large Language Models</title>
      <author><first>Xiaqiang</first><last>Tang</last></author>
      <author><first>Jian</first><last>Li</last><affiliation>Tencent</affiliation></author>
      <author><first>Keyu</first><last>Hu</last></author>
      <author><first>Nan</first><last>Du</last><affiliation>Tencent INC</affiliation></author>
      <author><first>Xiaolong</first><last>Li</last><affiliation>Tencent America LLC</affiliation></author>
      <author><first>Xi</first><last>Zhang</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Weigao</first><last>Sun</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Sihong</first><last>Xie</last><affiliation>HKUST-GZ</affiliation></author>
      <pages>21567-21585</pages>
      <abstract>Faithfulness hallucinations are claims generated by a Large Language Model (LLM) not supported by contexts provided to the LLM. Lacking assessment standards, existing benchmarks focus on “factual statements” that rephrase source materials while overlooking “cognitive statements” that involve making inferences from the given context. Consequently, evaluating and detecting the hallucination of cognitive statements remains challenging. Inspired by how evidence is assessed in the legal domain, we design a rigorous framework to assess different levels of faithfulness of cognitive statements and introduce the CogniBench dataset where we reveal insightful statistics. To keep pace with rapidly evolving LLMs, we further develop an automatic annotation pipeline that scales easily across different models. This results in a large-scale CogniBench-L dataset, which facilitates training accurate detectors for both factual and cognitive hallucinations. We release our model and datasets at: https://github.com/FUTUREEEEEE/CogniBench</abstract>
      <url hash="765f7c53">2025.acl-long.1046</url>
      <bibkey>tang-etal-2025-cognibench</bibkey>
    </paper>
    <paper id="1047">
      <title>Neural Incompatibility: The Unbridgeable Gap of Cross-Scale Parametric Knowledge Transfer in Large Language Models</title>
      <author><first>Yuqiao</first><last>Tan</last></author>
      <author><first>Shizhu</first><last>He</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Kang</first><last>Liu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jun</first><last>Zhao</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <pages>21586-21601</pages>
      <abstract>Large Language Models (LLMs) offer a transparent brain with accessible parameters that encode extensive knowledge, which can be analyzed, located and transferred. Consequently, a key research challenge is to transcend traditional knowledge transfer paradigms rooted in symbolic language and achieve genuine Parametric Knowledge Transfer (PKT). Significantly, exploring effective methods for transferring knowledge across LLMs of different scales through parameters presents an intriguing and valuable research direction. In this paper, we first demonstrate <tex-math>\textbf{Alignment}</tex-math> in parametric space is the fundamental prerequisite to achieve successful cross-scale PKT. We redefine the previously explored knowledge transfer as Post-Align PKT (PostPKT), which utilizes extracted parameters for LoRA initialization and requires subsequent fine-tune for alignment. Hence, to reduce cost for further fine-tuning, we introduce a novel Pre-Align PKT (PrePKT) paradigm and propose a solution called <tex-math>\textbf{LaTen}</tex-math> (<tex-math>\textbf{L}</tex-math>oc<tex-math>\textbf{a}</tex-math>te-<tex-math>\textbf{T}</tex-math>h<tex-math>\textbf{e}</tex-math>n-Alig<tex-math>\textbf{n}</tex-math>) that aligns the parametric spaces of LLMs across scales only using several training steps without following training. Comprehensive experiments on four benchmarks demonstrate that both PostPKT and PrePKT face challenges in achieving consistently stable transfer. Through in-depth analysis, we identify <tex-math>\textbf{Neural Incompatibility}</tex-math> as the ethological and parametric structural differences between LLMs of varying scales, presenting fundamental challenges to achieving effective PKT. These findings provide fresh insights into the parametric architectures of LLMs and highlight promising directions for future research on efficient PKT. Our code is available at https://github.com/Trae1ounG/Neural_Incompatibility.</abstract>
      <url hash="5675f042">2025.acl-long.1047</url>
      <bibkey>tan-etal-2025-neural</bibkey>
    </paper>
    <paper id="1048">
      <title>Enhancing Mathematical Reasoning in <fixed-case>LLM</fixed-case>s by Stepwise Correction</title>
      <author><first>Zhenyu</first><last>Wu</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Qingkai</first><last>Zeng</last><affiliation>Amazon</affiliation></author>
      <author><first>Zhihan</first><last>Zhang</last></author>
      <author><first>Zhaoxuan</first><last>Tan</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Chao</first><last>Shen</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Meng</first><last>Jiang</last><affiliation>University of Notre Dame</affiliation></author>
      <pages>21602-21623</pages>
      <abstract>Best-of-N decoding methods instruct large language models (LLMs) to generate multiple solutions, score each using a scoring function, and select the highest scored as the final answer to mathematical reasoning problems. However, this repeated independent process often leads to the same mistakes, making the selected solution still incorrect. We propose a novel prompting method named Stepwise Correction (StepCo) that helps LLMs identify and revise incorrect steps in their generated reasoning paths. It iterates verification and revision phases that employ a process-supervised verifier. The verify-then-revise process not only improves answer correctness but also reduces token consumption with fewer paths needed to generate. With StepCo, a series of LLMs demonstrate exceptional performance. Notably, using GPT-4o as the backend LLM, StepCo achieves an average accuracy of 94.1 across eight datasets, significantly outperforming the state-of-the-art Best-of-N method by +2.4, while reducing token consumption by 77.8%. Our implementation is made publicly available at https://wzy6642.github.io/stepco.github.io.</abstract>
      <url hash="b041c095">2025.acl-long.1048</url>
      <bibkey>wu-etal-2025-enhancing</bibkey>
    </paper>
    <paper id="1049">
      <title><fixed-case>P</fixed-case>sy<fixed-case>D</fixed-case>ial: A Large-scale Long-term Conversational Dataset for Mental Health Support</title>
      <author><first>Huachuan</first><last>Qiu</last></author>
      <author><first>Zhenzhong</first><last>Lan</last><affiliation>Westlake University</affiliation></author>
      <pages>21624-21655</pages>
      <abstract>Dialogue systems for mental health counseling aim to alleviate client distress and assist individuals in navigating personal challenges. Developing effective conversational agents for psychotherapy requires access to high-quality, real-world, long-term client-counselor interaction data, which is difficult to obtain due to privacy concerns. Although removing personally identifiable information is feasible, this process is labor-intensive. To address these challenges, we propose a novel privacy-preserving data reconstruction method that reconstructs real-world client-counselor dialogues while mitigating privacy concerns. We apply the RMRR (Retrieve, Mask, Reconstruct, Refine) method, which facilitates the creation of the privacy-preserving PsyDial dataset, with an average of 37.8 turns per dialogue. Extensive analysis demonstrates that PsyDial effectively reduces privacy risks while maintaining dialogue diversity and conversational exchange. To fairly and reliably evaluate the performance of models fine-tuned on our dataset, we manually collect 101 dialogues from professional counseling books. Experimental results show that models fine-tuned on PsyDial achieve improved psychological counseling performance, outperforming various baseline models. A user study involving counseling experts further reveals that our LLM-based counselor provides higher-quality responses. Code, data, and models are available at https://github.com/qiuhuachuan/PsyDial, serving as valuable resources for future advancements in AI psychotherapy.</abstract>
      <url hash="655736f5">2025.acl-long.1049</url>
      <bibkey>qiu-lan-2025-psydial</bibkey>
    </paper>
    <paper id="1050">
      <title>Enhancing Goal-oriented Proactive Dialogue Systems via Consistency Reflection and Correction</title>
      <author><first>Didi</first><last>Zhang</last><affiliation>Soochow University</affiliation></author>
      <author><first>Yaxin</first><last>Fan</last></author>
      <author><first>Peifeng</first><last>Li</last><affiliation>Soochow University, China</affiliation></author>
      <author><first>Qiaoming</first><last>Zhu</last><affiliation>Soochow University</affiliation></author>
      <pages>21656-21672</pages>
      <abstract>Goal-oriented proactive dialogue systems are designed to guide user conversations seamlessly towards specific objectives by planning a goal-oriented path. However, previous research has focused predominantly on optimizing these paths while neglecting the inconsistencies that may arise between generated responses and dialogue contexts, including user profiles, dialogue history, domain knowledge, and subgoals. To address this issue, we introduce a model-agnostic two-stage Consistency Reflection and Correction (CRC) framework. Specifically, in the consistency reflection stage, the model is prompted to reflect on the discrepancies between generated responses and dialogue contexts, identifying inconsistencies and suggesting possible corrections. In the consistency correction stage, the model generates responses that are more consistent with the dialogue context based on these reflection results. We conducted experiments on various model architectures with different parameter sizes, including encoder-decoder models (BART, T5) and decoder-only models (GPT-2, DialoGPT, Phi3, Mistral and LLaMA3), and the experimental results on three datasets demonstrate that our CRC framework significantly improves the consistency between generated responses and dialogue contexts.</abstract>
      <url hash="0dc90fb8">2025.acl-long.1050</url>
      <bibkey>zhang-etal-2025-enhancing-goal</bibkey>
    </paper>
    <paper id="1051">
      <title>Exclusion of Thought: Mitigating Cognitive Load in Large Language Models for Enhanced Reasoning in Multiple-Choice Tasks</title>
      <author><first>Qihang</first><last>Fu</last></author>
      <author><first>Yongbin</first><last>Qin</last><affiliation>Guizhou University</affiliation></author>
      <author><first>Ruizhang</first><last>Huang</last><affiliation>Guizhou University</affiliation></author>
      <author><first>Yanping</first><last>Chen</last><affiliation>Guizhou University</affiliation></author>
      <author><first>Yulin</first><last>Zhou</last></author>
      <author><first>Lintao</first><last>Long</last><affiliation>Guizhou University</affiliation></author>
      <pages>21673-21686</pages>
      <abstract>Multiple-choice questions (MCQs) are a widely used and vital assessment format for evaluating large language models (LLMs). This study reveals that LLMs are susceptible to “cognitive load” caused by distractor options in MCQs, leading to excessive attention to distractors and consequent vacillation between correct and incorrect options. To mitigate this cognitive burden, we introduce a novel reasoning prompt strategy, called EoT, which effectively reduces cognitive load by steering the model’s attention away from erroneous options. This enables the model to focus more effectively on reasonable answers. Additionally, by documenting the elimination process, EoT enhances the transparency and interpretability of the model’s reasoning. Experimental results demonstrate that EoT, as a plug-and-play approach, significantly reduces cognitive load and improves performance, showcasing its potential to enhance both the accuracy and interpretability of LLMs.</abstract>
      <url hash="e11cf15b">2025.acl-long.1051</url>
      <bibkey>fu-etal-2025-exclusion</bibkey>
    </paper>
    <paper id="1052">
      <title>Registering Source Tokens to Target Language Spaces in Multilingual Neural Machine Translation</title>
      <author><first>Zhi</first><last>Qu</last><affiliation>Nara Institute of Science and Technology, Japan and National Institute of Information and Communications Technology (NICT)</affiliation></author>
      <author><first>Yiran</first><last>Wang</last><affiliation>National Institute of Information and Communications Technology</affiliation></author>
      <author><first>Jiannan</first><last>Mao</last><affiliation>Gifu University and National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Chenchen</first><last>Ding</last><affiliation>National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Hideki</first><last>Tanaka</last><affiliation>National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Masao</first><last>Utiyama</last><affiliation>National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <pages>21687-21706</pages>
      <abstract>The multilingual neural machine translation (MNMT) aims for arbitrary translations across multiple languages.Although MNMT-specific models trained on parallel data offer low costs in training and deployment, their performance consistently lags behind that of large language models (LLMs).In this work, we introduce registering, a novel method that enables a small MNMT-specific model to compete with LLMs.Specifically, we insert a set of artificial tokens specifying the target language, called registers, into the input sequence between the source and target tokens.By modifying the attention mask, the target token generation only pays attention to the activation of registers, representing the source tokens in the target language space.Experiments on EC-40, a large-scale benchmark, show that our method advances the state-of-the-art of MNMT.We further pre-train two models, namely MITRE (multilingual translation with registers), by 9.3 billion sentence pairs across 24 languages collected from public corpora.One of them, MITRE-913M, outperforms NLLB-3.3B, achieves comparable performance with commercial LLMs, and shows strong adaptability in fine-tuning.Finally, we open-source our models to facilitate further research and development in MNMT: https://github.com/zhiqu22/mitre.</abstract>
      <url hash="0df01c92">2025.acl-long.1052</url>
      <bibkey>qu-etal-2025-registering</bibkey>
    </paper>
    <paper id="1053">
      <title><fixed-case>V</fixed-case>isuo<fixed-case>T</fixed-case>hink: Empowering <fixed-case>LVLM</fixed-case> Reasoning with Multimodal Tree Search</title>
      <author><first>Yikun</first><last>Wang</last><affiliation>Fudan University and Shanghai Innovation Institute</affiliation></author>
      <author><first>Siyin</first><last>Wang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Qinyuan</first><last>Cheng</last></author>
      <author><first>Zhaoye</first><last>Fei</last></author>
      <author><first>Liang</first><last>Ding</last></author>
      <author><first>Qipeng</first><last>Guo</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Dacheng</first><last>Tao</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Xipeng</first><last>Qiu</last><affiliation>Fudan University</affiliation></author>
      <pages>21707-21719</pages>
      <abstract>Recent advancements in Large Vision-Language Models have showcased remarkable capabilities. However, they often falter when confronted with complex reasoning tasks that humans typically address through visual aids and deliberate, step-by-step thinking. While existing methods have explored text-based slow thinking or rudimentary visual assistance, they fall short of capturing the intricate, interleaved nature of human visual-verbal reasoning processes. To overcome these limitations and inspired by the mechanisms of slow thinking in human cognition, we introduce VisuoThink, a novel framework that seamlessly integrates visuospatial and linguistic domains. VisuoThink facilitates multimodal slow thinking by enabling progressive visual-textual reasoning and incorporates test-time scaling through look-ahead tree search. Extensive experiments demonstrate that VisuoThink significantly enhances reasoning capabilities via inference-time scaling, even without fine-tuning, achieving state-of-the-art performance in tasks involving geometry and spatial reasoning.</abstract>
      <url hash="f25ea0ff">2025.acl-long.1053</url>
      <bibkey>wang-etal-2025-visuothink</bibkey>
    </paper>
    <paper id="1054">
      <title>Automated <fixed-case>CAD</fixed-case> Modeling Sequence Generation from Text Descriptions via Transformer-Based Large Language Models</title>
      <author><first>JianXing</first><last>Liao</last></author>
      <author><first>Junyan</first><last>Xu</last></author>
      <author><first>Yatao</first><last>Sun</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Maowen</first><last>Tang</last></author>
      <author><first>Sicheng</first><last>He</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Jingxian</first><last>Liao</last></author>
      <author><first>Shui</first><last>Yu</last></author>
      <author><first>Yun</first><last>Li</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Xiaohong</first><last>Guan</last></author>
      <pages>21720-21748</pages>
      <abstract>Designing complex computer-aided design (CAD) models is often time-consuming due to challenges such as computational inefficiency and the difficulty of generating precise models. We propose a novel language-guided framework for industrial design automation to address these issues, integrating large language models (LLMs) with computer-automated design (CAutoD).Through this framework, CAD models are automatically generated from parameters and appearance descriptions, supporting the automation of design tasks during the detailed CAD design phase. Our approach introduces three key innovations: (1) a semi-automated data annotation pipeline that leverages LLMs and vision-language large models (VLLMs) to generate high-quality parameters and appearance descriptions; (2) a Transformer-based CAD generator (TCADGen) that predicts modeling sequences via dual-channel feature aggregation; (3) an enhanced CAD modeling generation model, called CADLLM, that is designed to refine the generated sequences by incorporating the confidence scores from TCADGen. Experimental results demonstrate that the proposed approach outperforms traditional methods in both accuracy and efficiency, providing a powerful tool for automating industrial workflows and generating complex CAD models from textual prompts.The code is available at https://jianxliao.github.io/cadllm-page/</abstract>
      <url hash="90d8e830">2025.acl-long.1054</url>
      <bibkey>liao-etal-2025-automated</bibkey>
    </paper>
    <paper id="1055">
      <title><fixed-case>LED</fixed-case>-Merging: Mitigating Safety-Utility Conflicts in Model Merging with Location-Election-Disjoint</title>
      <author><first>Qianli</first><last>Ma</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Dongrui</first><last>Liu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Qian</first><last>Chen</last></author>
      <author><first>Linfeng</first><last>Zhang</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Jing</first><last>Shao</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <pages>21749-21767</pages>
      <abstract>Fine-tuning pre-trained Large Language Models (LLMs) for specialized tasks incurs substantial computational and data costs. While model merging offers a training-free solution to integrate multiple task-specific models, existing methods suffer from safety-utility conflicts where enhanced general capabilities degrade safety safeguards. We identify two root causes: <b>neuron misidentification</b> due to simplistic parameter magnitude-based selection, and <b>cross-task neuron interference</b> during merging.To address these challenges, we propose <b>LED-Merging</b>, a three-stage framework that <b>L</b>ocates task-specific neurons via gradient-based attribution, dynamically <b>E</b>lects critical neurons through multi-model importance fusion, and <b>D</b>isjoints conflicting updates through parameter isolation.Extensive experiments on Llama-3-8B, Mistral-7B, and Llama2-13B demonstrate that LED-Merging effectively reduces harmful response rates, showing a 31.4% decrease on Llama-3-8B-Instruct on HarmBench, while simultaneously preserving 95% of utility performance, such as achieving 52.39% accuracy on GSM8K.LED-Merging resolves safety-utility conflicts and provides a lightweight, training-free paradigm for constructing reliable multi-task LLMs.Code is available at https://github.com/MqLeet/LED-Merging</abstract>
      <url hash="1f19d6cf">2025.acl-long.1055</url>
      <bibkey>ma-etal-2025-led</bibkey>
    </paper>
    <paper id="1056">
      <title>Dolphin: Moving Towards Closed-loop Auto-research through Thinking, Practice, and Feedback</title>
      <author><first>Jiakang</first><last>Yuan</last><affiliation>Shanghai Artificial Intelligence Laboratory and Fudan University</affiliation></author>
      <author><first>Xiangchao</first><last>Yan</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Bo</first><last>Zhang</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Tao</first><last>Chen</last><affiliation>Fudan University</affiliation></author>
      <author><first>Botian</first><last>Shi</last><affiliation>Shanghai AI Lab</affiliation></author>
      <author><first>Wanli</first><last>Ouyang</last><affiliation>Shanghai AI Lab</affiliation></author>
      <author><first>Yu</first><last>Qiao</last><affiliation>Shanghai Aritifcal Intelligence Laboratory</affiliation></author>
      <author><first>Lei</first><last>Bai</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Bowen</first><last>Zhou</last><affiliation>Tsinghua University</affiliation></author>
      <pages>21768-21789</pages>
      <abstract>The scientific research paradigm is undergoing a profound transformation owing to the development of Artificial Intelligence (AI). Recent works demonstrate that various AI-assisted research methods can largely improve research efficiency by improving data analysis, accelerating computation, and fostering novel idea generation. To further move towards the ultimate goal (i.e., automatic scientific research), in this paper, we introduce Dolphin, a closed-loop LLM-driven framework to enhance the automation level of scientific research. Dolphin first generates novel ideas based on feedback from previous experiments and relevant papers ranked by the topic and task attributes. Then, the generated ideas can be implemented using a code template refined and debugged with the designed exception-traceback-guided local code structure. Finally, Dolphin automatically analyzes the results of each idea and feeds the results back to the next round of idea generation. Experiments are conducted on the benchmark datasets of different topics and a subset of MLE-bench. Results show that Dolphin can continuously improve the performance of the input topic in a loop. We highlight that Dolphin can automatically propose methods that are comparable to the state-of-the-art in some tasks such as 3D point classification.</abstract>
      <url hash="98fdb39b">2025.acl-long.1056</url>
      <bibkey>yuan-etal-2025-dolphin</bibkey>
    </paper>
    <paper id="1057">
      <title><fixed-case>P</fixed-case>er<fixed-case>S</fixed-case>phere: A Comprehensive Framework for Multi-Faceted Perspective Retrieval and Summarization</title>
      <author><first>Yun</first><last>Luo</last><affiliation>westlake university</affiliation></author>
      <author><first>Yingjie</first><last>Li</last><affiliation>Westlake University</affiliation></author>
      <author><first>Xiangkun</first><last>Hu</last></author>
      <author><first>Qinglin</first><last>Qi</last></author>
      <author><first>Fang</first><last>Guo</last></author>
      <author><first>Qipeng</first><last>Guo</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Zheng</first><last>Zhang</last><affiliation>Amazon</affiliation></author>
      <author><first>Yue</first><last>Zhang</last><affiliation>Westlake University</affiliation></author>
      <pages>21790-21805</pages>
      <abstract>As online platforms and recommendation algorithms evolve, people are increasingly trapped in echo chambers, leading to biased understandings of various issues. To combat this issue, we have introduced PerSphere, a benchmark designed to facilitate multi-faceted perspective retrieval and summarization, thus breaking free from these information silos. For each query within PerSphere, there are two opposing claims, each supported by distinct, non-overlapping perspectives drawn from one or more documents. Our goal is to accurately summarize these documents, aligning the summaries with the respective claims and their underlying perspectives. This task is structured as a two-step end-to-end pipeline that includes comprehensive document retrieval and multi-faceted summarization. Furthermore, we propose a set of metrics to evaluate the comprehensiveness of the retrieval and summarization content. Experimental results on various counterparts for the pipeline show that recent models struggle with such a complex task. Analysis shows that the main challenge lies in long context and perspective extraction, and we propose a simple but effective multi-agent summarization system, offering a promising solution to enhance performance on PerSphere.</abstract>
      <url hash="d8dcdeb5">2025.acl-long.1057</url>
      <bibkey>luo-etal-2025-persphere</bibkey>
    </paper>
    <paper id="1058">
      <title>Prompt-Guided Internal States for Hallucination Detection of Large Language Models</title>
      <author><first>Fujie</first><last>Zhang</last></author>
      <author><first>Peiqi</first><last>Yu</last></author>
      <author><first>Biao</first><last>Yi</last></author>
      <author><first>Baolei</first><last>Zhang</last><affiliation>Nankai University</affiliation></author>
      <author><first>Tong</first><last>Li</last><affiliation>Nankai University</affiliation></author>
      <author><first>Zheli</first><last>Liu</last><affiliation>Nankai University</affiliation></author>
      <pages>21806-21818</pages>
      <abstract>Large Language Models (LLMs) have demonstrated remarkable capabilities across a variety of tasks in different domains. However, they sometimes generate responses that are logically coherent but factually incorrect or misleading, which is known as LLM hallucinations. Data-driven supervised methods train hallucination detectors by leveraging the internal states of LLMs, but detectors trained on specific domains often struggle to generalize well to other domains. In this paper, we aim to enhance the cross-domain performance of supervised detectors with only in-domain data. We propose a novel framework, prompt-guided internal states for hallucination detection of LLMs, namely PRISM. By utilizing appropriate prompts to guide changes to the structure related to text truthfulness in LLMs’ internal states, we make this structure more salient and consistent across texts from different domains. We integrated our framework with existing hallucination detection methods and conducted experiments on datasets from different domains. The experimental results indicate that our framework significantly enhances the cross-domain generalization of existing hallucination detection methods.</abstract>
      <url hash="f6406dcb">2025.acl-long.1058</url>
      <bibkey>zhang-etal-2025-prompt</bibkey>
    </paper>
    <paper id="1059">
      <title>Typology-Guided Adaptation in Multilingual Models</title>
      <author><first>Ndapa</first><last>Nakashole</last><affiliation>University of California, San Diego</affiliation></author>
      <pages>21819-21835</pages>
      <abstract>Multilingual models often treat language diversity as a problem of data imbalance, overlooking structural variation. We introduce the *Morphological Index* (MoI), a typologically grounded metric that quantifies how strongly a language relies on surface morphology for noun classification. Building on MoI, we propose *MoI-MoE*, a Mixture of Experts model that routes inputs based on morphological structure. Evaluated on 10 Bantu languages—a large, morphologically rich and underrepresented family—MoI-MoE outperforms strong baselines, improving Swahili accuracy by 14 points on noun class recognition while maintaining performance on morphology-rich languages like Zulu. These findings highlight typological structure as a practical and interpretable signal for multilingual model adaptation.</abstract>
      <url hash="3c22170a">2025.acl-long.1059</url>
      <bibkey>nakashole-2025-typology</bibkey>
    </paper>
    <paper id="1060">
      <title>Don’t Erase, Inform! Detecting and Contextualizing Harmful Language in Cultural Heritage Collections</title>
      <author><first>Orfeas</first><last>Menis Mastromichalakis</last></author>
      <author><first>Jason</first><last>Liartis</last><affiliation>National Technical University of Athens</affiliation></author>
      <author><first>Kristina</first><last>Rose</last><affiliation>DFF - Deutsches Filminstitut &amp; Filmmuseum</affiliation></author>
      <author><first>Antoine</first><last>Isaac</last><affiliation>Europeana</affiliation></author>
      <author><first>Giorgos</first><last>Stamou</last><affiliation>National Technical University of Athens</affiliation></author>
      <pages>21836-21850</pages>
      <abstract>Cultural Heritage (CH) data hold invaluable knowledge, reflecting the history, traditions, and identities of societies, and shaping our understanding of the past and present. However, many CH collections contain outdated or offensive descriptions that reflect historical biases. CH Institutions (CHIs) face significant challenges in curating these data due to the vast scale and complexity of the task. To address this, we develop an AI-powered tool that detects offensive terms in CH metadata and provides contextual insights into their historical background and contemporary perception. We leverage a multilingual vocabulary co-created with marginalized communities, researchers, and CH professionals, along with traditional NLP techniques and Large Language Models (LLMs). Available as a standalone web app and integrated with major CH platforms, the tool has processed over 7.9 million records, contextualizing the contentious terms detected in their metadata. Rather than erasing these terms, our approach seeks to inform, making biases visible and providing actionable insights for creating more inclusive and accessible CH collections.</abstract>
      <url hash="827af437">2025.acl-long.1060</url>
      <bibkey>menis-mastromichalakis-etal-2025-dont</bibkey>
    </paper>
    <paper id="1061">
      <title><fixed-case>ECLM</fixed-case>: Entity Level Language Model for Spoken Language Understanding with Chain of Intent</title>
      <author><first>Shangjian</first><last>Yin</last></author>
      <author><first>Peijie</first><last>Huang</last><affiliation>South China Agricultural University</affiliation></author>
      <author><first>JiaTian</first><last>Chen</last></author>
      <author><first>Haojing</first><last>Huang</last></author>
      <author><first>Yuhong</first><last>Xu</last><affiliation>South China Agricultural University</affiliation></author>
      <pages>21851-21862</pages>
      <abstract>Large Language Models (LLMs) have demonstrated impressive capabilities in language generation and general task performance. However, their application to spoken language understanding (SLU) remains challenging, particularly for token-level tasks, where the autoregressive nature of LLMs often leads to misalignment issues. They also struggle to capture nuanced interrelations in semantic-level tasks through direct fine-tuning alone. To address these challenges, we propose the Entity-level Language Model (ECLM) framework, which reformulates slot-filling as an entity recognition task and introduces a novel concept, Chain of Intent, to enable step-by-step multi-intent recognition. Experimental results show that ECLM significantly outperforms strong baselines such as Uni-MIS, achieving gains of 3.7% on MixATIS and 3.1% on MixSNIPS. Compared to standard supervised fine-tuning of LLMs, ECLM further achieves improvements of 8.5% and 21.2% on these datasets, respectively. Our code is available at https://github.com/SJY8460/ECLM.</abstract>
      <url hash="af4d6ebd">2025.acl-long.1061</url>
      <bibkey>yin-etal-2025-eclm</bibkey>
    </paper>
    <paper id="1062">
      <title><fixed-case>F</fixed-case>aithful<fixed-case>RAG</fixed-case>: Fact-Level Conflict Modeling for Context-Faithful Retrieval-Augmented Generation</title>
      <author><first>Qinggang</first><last>Zhang</last></author>
      <author><first>Zhishang</first><last>Xiang</last></author>
      <author><first>Yilin</first><last>Xiao</last></author>
      <author><first>Le</first><last>Wang</last></author>
      <author><first>Junhui</first><last>Li</last><affiliation>Soochow University, China</affiliation></author>
      <author><first>Xinrun</first><last>Wang</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Jinsong</first><last>Su</last><affiliation>Xiamen University</affiliation></author>
      <pages>21863-21882</pages>
      <abstract>Large language models (LLMs) augmented with retrieval systems have demonstrated significant potential in handling knowledge-intensive tasks. However, these models often struggle with unfaithfulness issues, generating outputs that either ignore the retrieved context or inconsistently blend it with the LLM’s parametric knowledge. This issue is particularly severe in cases of knowledge conflict, where the retrieved context conflicts with the model’s parametric knowledge. While existing faithful RAG approaches enforce strict context adherence through well-designed prompts or modified decoding strategies, our analysis reveals a critical limitation: they achieve faithfulness by forcibly suppressing the model’s parametric knowledge, which undermines the model’s internal knowledge structure and increases the risk of misinterpreting the context. To this end, this paper proposes FaithfulRAG, a novel framework that resolves knowledge conflicts by explicitly modeling discrepancies between the model’s parametric knowledge and retrieved context. Specifically, FaithfulRAG identifies conflicting knowledge at the fact level and designs a self-thinking process, allowing LLMs to reason about and integrate conflicting facts before generating responses. Extensive experiments demonstrate that our method outperforms state-of-the-art methods. The code is available at https://github.com/DeepLearnXMU/Faithful-RAG.</abstract>
      <url hash="39945bab">2025.acl-long.1062</url>
      <bibkey>zhang-etal-2025-faithfulrag</bibkey>
    </paper>
    <paper id="1063">
      <title>Knowledge Image Matters: Improving Knowledge-Based Visual Reasoning with Multi-Image Large Language Models</title>
      <author><first>Guanghui</first><last>Ye</last><affiliation>Hunan University</affiliation></author>
      <author><first>Huan</first><last>Zhao</last><affiliation>Hunan University</affiliation></author>
      <author><first>Zhixue</first><last>Zhao</last><affiliation>University of Sheffield, University of Sheffield</affiliation></author>
      <author><first>Xupeng</first><last>Zha</last><affiliation>Hunan University</affiliation></author>
      <author id="yang-liu"><first>Yang</first><last>Liu</last><affiliation>Hunan University</affiliation></author>
      <author><first>Zhihua</first><last>Jiang</last></author>
      <pages>21883-21896</pages>
      <abstract>We revisit knowledge-based visual reasoning (KB-VR) in light of modern advances in multimodal large language models (MLLMs), and make the following contributions: (i) We propose Visual Knowledge Card (VKC) – a novel image that incorporates not only internal visual knowledge (e.g., scene-aware information) detected from the raw image, but also external world knowledge (e.g., attribute or object knowledge) produced by a knowledge generator; (ii) We present VKC-based Multi-Image Reasoning (VKC-MIR) – a four-stage pipeline which harnesses a state-of-the-art scene perception engine to construct an initial VKC (Stage-1), a powerful LLM to generate relevant domain knowledge (Stage-2), an excellent image editing toolkit to introduce generated knowledge into the updated VKC (Stage-3), and finally, an emerging multi-image MLLM to solve the VKC-enhanced task (Stage-4). By performing experiments on three popular KB-VR benchmarks, our approach achieves new state-of-the-art results compared to previous top-performing models.</abstract>
      <url hash="5d58ffd9">2025.acl-long.1063</url>
      <bibkey>ye-etal-2025-knowledge</bibkey>
    </paper>
    <paper id="1064">
      <title>Evaluating Personalized Tool-Augmented <fixed-case>LLM</fixed-case>s from the Perspectives of Personalization and Proactivity</title>
      <author><first>Yupu</first><last>Hao</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Pengfei</first><last>Cao</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Zhuoran</first><last>Jin</last></author>
      <author><first>Huanxuan</first><last>Liao</last><affiliation>Institute of Automation, Chinese Academy of Sciences (CASIA)</affiliation></author>
      <author><first>Yubo</first><last>Chen</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <author><first>Kang</first><last>Liu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jun</first><last>Zhao</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <pages>21897-21935</pages>
      <abstract>Personalized tool utilization is essential for aligning large language models (LLMs) with user preference in interaction scenarios with various tools. However, most of the current benchmarks primarily focus on either personalization of text generation or direct tool-utilizing, without considering both. In this work, we introduce a novel benchmark <b>ETAPP</b> for evaluating personalized tool invocation, establishing a sandbox environment, and a comprehensive dataset of 800 testing cases covering diverse user profiles. To improve the accuracy of our evaluation, we propose a key-point-based LLM evaluation method, mitigating biases in the LLM-as-a-judge system by manually annotating key points for each test case and providing them to LLM as the reference. Additionally, we evaluate the excellent LLMs and provide an in-depth analysis. Furthermore, we investigate the impact of different tool-invoking strategies on LLMs’ personalization performance and the effects of fine-tuning in our task. The effectiveness of our preference-setting and key-point-based evaluation method is also validated. Our findings offer insights into improving personalized LLM agents. Our code is available at https://github.com/hypasd-art/ETAPP.</abstract>
      <url hash="62e4a341">2025.acl-long.1064</url>
      <bibkey>hao-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="1065">
      <title><fixed-case>GUIC</fixed-case>ourse: From General Vision Language Model to Versatile <fixed-case>GUI</fixed-case> Agent</title>
      <author><first>Wentong</first><last>Chen</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Junbo</first><last>Cui</last></author>
      <author><first>Jinyi</first><last>Hu</last></author>
      <author><first>Yujia</first><last>Qin</last></author>
      <author><first>Junjie</first><last>Fang</last></author>
      <author><first>Yue</first><last>Zhao</last></author>
      <author><first>Chongyi</first><last>Wang</last></author>
      <author><first>Jun</first><last>Liu</last></author>
      <author><first>Guirong</first><last>Chen</last></author>
      <author><first>Yupeng</first><last>Huo</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Yuan</first><last>Yao</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Yankai</first><last>Lin</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Zhiyuan</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Maosong</first><last>Sun</last><affiliation>Tsinghua University</affiliation></author>
      <pages>21936-21959</pages>
      <abstract>Utilizing Graphic User Interfaces (GUIs) for human-computer interaction is essential for accessing various digital tools. Recent advancements in Vision Language Models (VLMs) reveal significant potential for developing versatile agents that assist humans in navigating GUIs. However, current VLMs face challenges related to fundamental abilities, such as OCR and grounding, as well as a lack of knowledge about GUI elements functionalities and control methods. These limitations hinder their effectiveness as practical GUI agents. To address these challenges, we introduce GUICourse, a series of datasets for training visual-based GUI agents using general VLMs. First, we enhance the OCR and grounding capabilities of VLMs using the GUIEnv dataset. Next, we enrich the GUI knowledge of VLMs using the GUIAct and GUIChat datasets. Our experiments demonstrate that even a small-sized GUI agent (with 3.1 billion parameters) performs effectively on both single-step and multi-step GUI tasks. We further finetune our GUI agents on other GUI tasks with different action spaces (AITW and Mind2Web), and the results show that our agents are better than their baseline VLMs. Additionally, we analyze the impact of OCR and grounding capabilities through an ablation study, revealing a positive correlation with GUI navigation ability.</abstract>
      <url hash="30e9444c">2025.acl-long.1065</url>
      <bibkey>chen-etal-2025-guicourse</bibkey>
    </paper>
    <paper id="1066">
      <title>Evaluating Visual and Cultural Interpretation: The K-Viscuit Benchmark with Human-<fixed-case>VLM</fixed-case> Collaboration</title>
      <author><first>ChaeHun</first><last>Park</last></author>
      <author><first>Yujin</first><last>Baek</last><affiliation>KAIST</affiliation></author>
      <author><first>Jaeseok</first><last>Kim</last><affiliation>Korea Telecom Research</affiliation></author>
      <author><first>Yu-Jung</first><last>Heo</last><affiliation>KT</affiliation></author>
      <author><first>Du-Seong</first><last>Chang</last><affiliation>Sogang University</affiliation></author>
      <author><first>Jaegul</first><last>Choo</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>21960-21974</pages>
      <abstract>To create culturally inclusive vision-language models (VLMs), developing a benchmark that tests their ability to address culturally relevant questions is essential. Existing approaches typically rely on human annotators, making the process labor-intensive and creating a cognitive burden in generating diverse questions. To address this, we propose a semi-automated framework for constructing cultural VLM benchmarks, specifically targeting multiple-choice QA. This framework combines human-VLM collaboration, where VLMs generate questions based on guidelines, a small set of annotated examples, and relevant knowledge, followed by a verification process by native speakers. We demonstrate the effectiveness of this framework through the creation of K-Viscuit, a dataset focused on Korean culture. Our experiments on this dataset reveal that open-source models lag behind proprietary ones in understanding Korean culture, highlighting key areas for improvement. We also present a series of further analyses, including human evaluation, augmenting VLMs with external knowledge, and the evaluation beyond multiple-choice QA. Our dataset is available at https://huggingface.co/datasets/ddehun/k-viscuit.</abstract>
      <url hash="90b5cdc4">2025.acl-long.1066</url>
      <bibkey>park-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="1067">
      <title>Maximizing the Effectiveness of Larger <fixed-case>BERT</fixed-case> Models for Compression</title>
      <author><first>Wen-Shu</first><last>Fan</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Su</first><last>Lu</last></author>
      <author><first>Shangyu</first><last>Xing</last></author>
      <author><first>Xin-Chun</first><last>Li</last></author>
      <author><first>De-Chuan</first><last>Zhan</last><affiliation>Nanjing University</affiliation></author>
      <pages>21975-21990</pages>
      <abstract>Knowledge distillation (KD) is a widely used approach for BERT compression, where a larger BERT model serves as a teacher to transfer knowledge to a smaller student model. Prior works have found that distilling a larger BERT with superior performance may degrade student’s performance than a smaller BERT. In this paper, we investigate the limitations of existing KD methods for larger BERT models. Through Canonical Correlation Analysis, we identify that these methods fail to fully exploit the potential advantages of larger teachers. To address this, we propose an improved distillation approach that effectively enhances knowledge transfer. Comprehensive experiments demonstrate the effectiveness of our method in enabling larger BERT models to distill knowledge more efficiently.</abstract>
      <url hash="43b19bbc">2025.acl-long.1067</url>
      <bibkey>fan-etal-2025-maximizing</bibkey>
    </paper>
    <paper id="1068">
      <title>Can <fixed-case>LLM</fixed-case>s Reason About Program Semantics? A Comprehensive Evaluation of <fixed-case>LLM</fixed-case>s on Formal Specification Inference</title>
      <author><first>Thanh</first><last>Le-Cong</last><affiliation>University of Melbourne</affiliation></author>
      <author><first>Bach</first><last>Le</last><affiliation>University of Melbourne</affiliation></author>
      <author><first>Toby</first><last>Murray</last><affiliation>University of Melbourne</affiliation></author>
      <pages>21991-22014</pages>
      <abstract>Large Language Models (LLMs) are increasingly being used to automate programming tasks. However, the capabilities of LLMs in reasoning about program semantics are still inadequately studied, leaving substantial potential for further exploration. This paper introduces FormalBench, a comprehensive benchmark designed to evaluate the reasoning abilities of Large Language Models (LLMs) on program semantics. Specifically, it utilizes the task of synthesizing formal program specifications as a proxy measure for assessing the semantic reasoning of LLMs. This task requires both comprehensive reasoning over all possible program executions and the generation of precise, syntactically correct expressions that adhere to formal syntax and semantics. Using this benchmark, we evaluated the ability of LLMs to synthesize consistent and complete specifications. Our findings show that LLMs perform well with simple control flows but struggle with more complex structures, especially loops, even with advanced prompting. Additionally, LLMs exhibit limited robustness against semantic-preserving transformations. We also highlight common failure patterns and design self-repair prompts, improving success rates by 25%. FormalBench is packaged as an executable library and has been released at https://github.com/thanhlecongg/FormalBench/.</abstract>
      <url hash="d58a47af">2025.acl-long.1068</url>
      <bibkey>le-cong-etal-2025-llms</bibkey>
    </paper>
    <paper id="1069">
      <title><fixed-case>HAC</fixed-case>o-Det: A Study Towards Fine-Grained Machine-Generated Text Detection under Human-<fixed-case>AI</fixed-case> Coauthoring</title>
      <author><first>Zhixiong</first><last>Su</last></author>
      <author><first>Yichen</first><last>Wang</last></author>
      <author><first>Herun</first><last>Wan</last></author>
      <author><first>Zhaohan</first><last>Zhang</last></author>
      <author><first>Minnan</first><last>Luo</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <pages>22015-22036</pages>
      <abstract>The misuse of large language models (LLMs) poses potential risks, motivating the development of machine-generated text (MGT) detection. Existing literature primarily concentrates on binary, document-level detection, thereby neglecting texts that are composed jointly by human and LLM contributions. Hence, this paper explores the possibility of fine-grained MGT detection under human-AI coauthoring.We suggest fine-grained detectors can pave pathways toward coauthored text detection with a numeric AI ratio.Specifically, we propose a dataset, HACo-Det, which produces human-AI coauthored texts via an automatic pipeline with word-level attribution labels. We retrofit seven prevailing document-level detectors to generalize them to word-level detection.Then we evaluate these detectors on HACo-Det on both word- and sentence-level detection tasks.Empirical results show that metric-based methods struggle to conduct fine-grained detection with a 0.462 average F1 score, while finetuned models show superior performance and better generalization across domains. However, we argue that fine-grained co-authored text detection is far from solved.We further analyze factors influencing performance, e.g., context window, and highlight the limitations of current methods, pointing to potential avenues for improvement.</abstract>
      <url hash="b579d238">2025.acl-long.1069</url>
      <bibkey>su-etal-2025-haco</bibkey>
    </paper>
    <paper id="1070">
      <title><fixed-case>I</fixed-case>ndic<fixed-case>S</fixed-case>ynth: A Large-Scale Multilingual Synthetic Speech Dataset for Low-Resource <fixed-case>I</fixed-case>ndian Languages</title>
      <author><first>Divya V</first><last>Sharma</last><affiliation>Indraprastha Institute of Information Technology, Delhi</affiliation></author>
      <author><first>Vijval</first><last>Ekbote</last><affiliation>Indraprastha Institute of Information Technology, Delhi</affiliation></author>
      <author><first>Anubha</first><last>Gupta</last><affiliation>Indraprastha Institute of Information Technology, Delhi</affiliation></author>
      <pages>22037-22060</pages>
      <abstract>Recent advances in synthetic speech generation technology have facilitated the generation of high-quality synthetic (fake) speech that emulates human voices. These technologies pose a threat of misuse for identity theft and the spread of misinformation. Consequently, the misuse of such powerful technologies necessitates the development of robust and generalizable audio deepfake detection (ADD) and anti-spoofing models. However, such models are often linguistically biased. Consequently, the models trained on datasets in one language exhibit a low accuracy when evaluated on out-of-domain languages. Such biases reduce the usability of these models and highlight the urgent need for multilingual synthetic speech datasets for bias mitigation research. However, most available datasets are in English or Chinese. The dearth of multilingual synthetic datasets hinders multilingual ADD and anti-spoofing research. Furthermore, the problem intensifies in countries with rich linguistic diversity, such as India. Therefore, we introduce IndicSynth, which contains 4,000 hours of synthetic speech from 989 target speakers, including 456 females and 533 males for 12 low-resourced Indian languages. The dataset includes rich metadata covering gender details and target speaker identifiers. Experimental results demonstrate that IndicSynth is a valuable contribution to multilingual ADD and anti-spoofing research. The dataset can be accessed from https://github.com/vdivyas/IndicSynth.</abstract>
      <url hash="11489dcf">2025.acl-long.1070</url>
      <bibkey>sharma-etal-2025-indicsynth</bibkey>
    </paper>
    <paper id="1071">
      <title>Reinforced <fixed-case>IR</fixed-case>: A Self-Boosting Framework For Domain-Adapted Information Retrieval</title>
      <author><first>Chaofan</first><last>Li</last></author>
      <author><first>Jianlyu</first><last>Chen</last></author>
      <author><first>Yingxia</first><last>Shao</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Chaozhuo</first><last>Li</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Quanqing</first><last>Xu</last></author>
      <author><first>Defu</first><last>Lian</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Zheng</first><last>Liu</last></author>
      <pages>22061-22073</pages>
      <abstract>While retrieval techniques are widely used in practice, they still face significant challenges in cross-domain scenarios. Recently, generation-augmented methods have emerged as a promising solution to this problem. These methods enhance raw queries by incorporating additional information from an LLM-based generator, facilitating more direct retrieval of relevant documents. However, existing methods struggle with highly specialized situations that require extensive domain expertise. To address this problem, we present <b>Reinforced-IR</b>, a novel approach that jointly adapts a pre-trained retriever and generator for precise cross-domain retrieval. A key innovation of Reinforced-IR is its <b>Self-Boosting</b> framework, which enables retriever and generator to learn from each other’s feedback. Specifically, the generator is reinforced to generate query augmentations that enhance the retriever’s performance, while the retriever is trained to better discriminate the relevant documents identified by the generator. This iterative process allows the end-to-end retrieval performance to be progressively optimized using an unlabeled corpus from the target domain. In our experiment, Reinforced-IR outperforms existing domain adaptation methods by a large margin, leading to substantial improvements in retrieval quality across a wide range of application scenarios.We have publicly released our code at this repo.</abstract>
      <url hash="0d95fb95">2025.acl-long.1071</url>
      <bibkey>li-etal-2025-reinforced</bibkey>
    </paper>
    <paper id="1072">
      <title><fixed-case>C</fixed-case>o<fixed-case>IR</fixed-case>: A Comprehensive Benchmark for Code Information Retrieval Models</title>
      <author><first>Xiangyang</first><last>Li</last></author>
      <author><first>Kuicai</first><last>Dong</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Yi Quan</first><last>Lee</last></author>
      <author><first>Wei</first><last>Xia</last><affiliation>Tencent</affiliation></author>
      <author><first>Hao</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Xinyi</first><last>Dai</last></author>
      <author><first>Yasheng</first><last>Wang</last></author>
      <author><first>Ruiming</first><last>Tang</last></author>
      <pages>22074-22091</pages>
      <abstract>Despite the substantial success of Information Retrieval (IR) in various NLP tasks, most IR systems predominantly handle queries and corpora in natural language, neglecting the domain of code retrieval. Code retrieval is critically important yet remains under-explored, with existing methods and benchmarks inadequately representing the diversity of code in various domains and tasks. Moreover, many models have begun to overfit existing leaderboards, limiting their generalizability and real-world applicability. Addressing this gap, we present CoIR (**Co**de **I**nformation **R**etrieval Benchmark), a robust and comprehensive benchmark specifically designed to assess code retrieval capabilities. CoIR comprises ten meticulously curated code datasets, spanning eight distinctive retrieval tasks across seven diverse domains. We first discuss the construction of CoIR and its diverse dataset composition. Further, we evaluate ten widely used retrieval models using CoIR, uncovering significant difficulties in performing code retrieval tasks even with state-of-the-art systems. CoIR also introduces a simple yet effective python framework, which additionally defines various advanced modes to facilitate researchers in evaluating their models. It shares the same data schema as other popular benchmarks like MTEB and BEIR, enabling seamless cross-benchmark evaluations. Through CoIR, we aim to invigorate research in the code retrieval domain, providing a versatile benchmarking tool that encourages further development and exploration of code retrieval systems.</abstract>
      <url hash="92d6ae58">2025.acl-long.1072</url>
      <bibkey>li-etal-2025-coir</bibkey>
    </paper>
    <paper id="1073">
      <title>Enhancing Multimodal Retrieval via Complementary Information Extraction and Alignment</title>
      <author><first>Delong</first><last>Zeng</last></author>
      <author><first>Yuexiang</first><last>Xie</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yaliang</first><last>Li</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Ying</first><last>Shen</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <pages>22092-22105</pages>
      <abstract>Multimodal retrieval has emerged as a promising yet challenging research direction in recent years. Most existing studies in multimodal retrieval focus on capturing information in multimodal data that is similar to their paired texts, but often ignores the complementary information contained in multimodal data. In this study, we propose CIEA, a novel multimodal retrieval approach that employs Complementary Information Extraction and Alignment, which transforms both text and images in documents into a unified latent space and features a complementary information extractor designed to identify and preserve differences in the image representations. We optimize CIEA using two complementary contrastive losses to ensure semantic integrity and effectively capture the complementary information contained in images. Extensive experiments demonstrate the effectiveness of CIEA, which achieves significant improvements over both divide-and-conquer models and universal dense retrieval models. We provide an ablation study, further discussions, and case studies to highlight the advancements achieved by CIEA. To promote further research in the community, we have released the source code at https://github.com/zengdlong/CIEA.</abstract>
      <url hash="593a9bab">2025.acl-long.1073</url>
      <bibkey>zeng-etal-2025-enhancing</bibkey>
    </paper>
    <paper id="1074">
      <title><fixed-case>J</fixed-case>o<fixed-case>PA</fixed-case>: Explaining Large Language Model’s Generation via Joint Prompt Attribution</title>
      <author><first>Yurui</first><last>Chang</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Bochuan</first><last>Cao</last></author>
      <author><first>Yujia</first><last>Wang</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Jinghui</first><last>Chen</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Lu</first><last>Lin</last><affiliation>Pennsylvania State University</affiliation></author>
      <pages>22106-22122</pages>
      <abstract>Large Language Models (LLMs) have demonstrated impressive performances in complex text generation tasks. However, the contribution of the input prompt to the generated content still remains obscure to humans, underscoring the necessity of understanding the causality between input and output pairs. Existing works for providing prompt-specific explanation often confine model output to be classification or next-word prediction. Few initial attempts aiming to explain the entire language generation often treat input prompt texts independently, ignoring their combinatorial effects on the follow-up generation. In this study, we introduce a counterfactual explanation framework based on joint prompt attribution, JoPA, which aims to explain how a few prompt texts collaboratively influences the LLM’s complete generation. Particularly, we formulate the task of prompt attribution for generation interpretation as a combinatorial optimization problem, and introduce a probabilistic algorithm to search for the casual input combination in the discrete space. We define and utilize multiple metrics to evaluate the produced explanations, demonstrating both the faithfulness and efficiency of our framework.</abstract>
      <url hash="8164bb3f">2025.acl-long.1074</url>
      <bibkey>chang-etal-2025-jopa</bibkey>
    </paper>
    <paper id="1075">
      <title>Proxy-Driven Robust Multimodal Sentiment Analysis with Incomplete Data</title>
      <author><first>Aoqiang</first><last>Zhu</last><affiliation>Hefei University of Technology</affiliation></author>
      <author><first>Min</first><last>Hu</last><affiliation>Hefei University of Technology</affiliation></author>
      <author><first>Xiaohua</first><last>Wang</last><affiliation>Hefei University of Technology</affiliation></author>
      <author><first>Jiaoyun</first><last>Yang</last><affiliation>Hefei University of Technology</affiliation></author>
      <author><first>Yiming</first><last>Tang</last><affiliation>Hefei University of Technology</affiliation></author>
      <author><first>Ning</first><last>An</last><affiliation>Hefei University of Technology</affiliation></author>
      <pages>22123-22138</pages>
      <abstract>Multimodal Sentiment Analysis (MSA) with incomplete data has gained significant attention recently. Existing studies focus on optimizing model structures to handle modality missingness, but models still face challenges in robustness when dealing with uncertain missingness. To this end, we propose a data-centric robust multimodal sentiment analysis method, Proxy-Driven Robust Multimodal Fusion (P-RMF). First, we map unimodal data to the latent space of Gaussian distributions to capture core features and structure, thereby learn stable modality representation. Then, we combine the quantified inherent modality uncertainty to learn stable multimodal joint representation (i.e., proxy modality), which is further enhanced through multi-layer dynamic cross-modal injection to increase its diversity. Extensive experimental results show that P-RMF outperforms existing models in noise resistance and achieves state-of-the-art performance on multiple benchmark datasets. Code will be available at https://github.com/***/P-RMF.</abstract>
      <url hash="cdc8495b">2025.acl-long.1075</url>
      <bibkey>zhu-etal-2025-proxy</bibkey>
    </paper>
    <paper id="1076">
      <title>Not All Terms Matter: Recall-Oriented Adaptive Learning for <fixed-case>PLM</fixed-case>-aided Query Expansion in Open-Domain Question Answering</title>
      <author><first>Xinran</first><last>Chen</last></author>
      <author><first>Ben</first><last>He</last></author>
      <author><first>Xuanang</first><last>Chen</last></author>
      <author><first>Le</first><last>Sun</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <pages>22139-22151</pages>
      <abstract>The effectiveness of open-domain question answering (ODQA), particularly those employing a retriever-reader architecture, depends on the ability to recall relevant documents - a critical step that enables the reader to accurately extract answers. To enhance this retrieval phase, current query expansion (QE) techniques leverage pre-trained language models (PLM) to mitigate word mismatches and improve the recall of relevant documents. Despite their advancements, these techniques often treat all expanded terms uniformly, which can lead to less-than-optimal retrieval outcomes. In response, we propose a novel Recall-oriented Adaptive Learning (ReAL) method, which iteratively adjusts the importance weights of QE terms based on their relevance, thereby refining term distinction and enhancing the separation of relevant terms. Specifically, ReAL employs a similarity-based model to classify documents into pseudo-relevant and pseudo-irrelevant sets, and then optimizes term weights via two tailored loss functions to maximize the scoring gap between them. Experiments on four ODQA datasets and five QE methods show that ReAL consistently enhances retrieval accuracy and overall end-to-end QA performance, providing a robust and efficient solution for improving QE strategies in ODQA scenarios.</abstract>
      <url hash="bee95c6f">2025.acl-long.1076</url>
      <bibkey>chen-etal-2025-terms</bibkey>
    </paper>
    <paper id="1077">
      <title>A Mutual Information Perspective on Knowledge Graph Embedding</title>
      <author><first>Jiang</first><last>Li</last></author>
      <author><first>Xiangdong</first><last>Su</last><affiliation>Inner Mongolia University</affiliation></author>
      <author><first>Zehua</first><last>Duo</last></author>
      <author><first>Tian</first><last>Lan</last></author>
      <author><first>Xiaotao</first><last>Guo</last></author>
      <author><first>Guanglai</first><last>Gao</last><affiliation>Inner Mongolia University</affiliation></author>
      <pages>22152-22166</pages>
      <abstract>Knowledge graph embedding techniques have emerged as a critical approach for addressing the issue of missing relations in knowledge graphs. However, existing methods often suffer from limitations, including high intra-group similarity, loss of semantic information, and insufficient inference capability, particularly in complex relation patterns such as 1-N and N-1 relations. To address these challenges, we introduce a novel KGE framework that leverages mutual information maximization to improve the semantic representation of entities and relations. By maximizing the mutual information between different components of triples, such as <tex-math>(h, r)</tex-math> and <tex-math>t</tex-math>, or <tex-math>(r, t)</tex-math> and <tex-math>h</tex-math>, the proposed method improves the model’s ability to preserve semantic dependencies while maintaining the relational structure of the knowledge graph. Extensive experiments on benchmark datasets demonstrate the effectiveness of our approach, with consistent performance improvements across various baseline models. Additionally, visualization analyses and case studies demonstrate the improved ability of the MI framework to capture complex relation patterns.</abstract>
      <url hash="8d90fdb7">2025.acl-long.1077</url>
      <bibkey>li-etal-2025-mutual</bibkey>
    </paper>
    <paper id="1078">
      <title>Aligned but Blind: Alignment Increases Implicit Bias by Reducing Awareness of Race</title>
      <author><first>Lihao</first><last>Sun</last></author>
      <author><first>Chengzhi</first><last>Mao</last><affiliation>Google</affiliation></author>
      <author><first>Valentin</first><last>Hofmann</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Xuechunzi</first><last>Bai</last><affiliation>University of Chicago</affiliation></author>
      <pages>22167-22184</pages>
      <abstract>Although value-aligned language models (LMs) appear unbiased in explicit bias evaluations, they often exhibit stereotypes in implicit word association tasks, raising concerns about their fair usage. We investigate the mechanisms behind this discrepancy and find that alignment surprisingly amplifies implicit bias in model outputs. Specifically, we show that aligned LMs, unlike their unaligned counterparts, overlook racial concepts in early internal representations when the context is ambiguous. Not representing race likely fails to activate safety guardrails, leading to unintended biases. Inspired by this insight, we propose a new bias mitigation strategy that works by incentivizing the representation of racial concepts in the early model layers. In contrast to conventional mitigation methods of machine unlearning, our interventions find that steering the model to be more aware of racial concepts effectively mitigates implicit bias. Similar to race blindness in humans, ignoring racial nuances can inadvertently perpetuate subtle biases in LMs.</abstract>
      <url hash="d75eb9d9">2025.acl-long.1078</url>
      <bibkey>sun-etal-2025-aligned</bibkey>
    </paper>
    <paper id="1079">
      <title><fixed-case>IOPO</fixed-case>: Empowering <fixed-case>LLM</fixed-case>s with Complex Instruction Following via Input-Output Preference Optimization</title>
      <author><first>Xinghua</first><last>Zhang</last></author>
      <author><first>Haiyang</first><last>Yu</last></author>
      <author><first>Cheng</first><last>Fu</last></author>
      <author><first>Fei</first><last>Huang</last><affiliation>Alibaba Group US</affiliation></author>
      <author><first>Yongbin</first><last>Li</last><affiliation>Alibaba Group</affiliation></author>
      <pages>22185-22200</pages>
      <abstract>In the realm of large language models (LLMs), the ability of models to accurately follow instructions is paramount as more agents and applications leverage LLMs for construction, where the complexity of instructions are rapidly increasing. However, on the one hand, there is only a certain amount of complex instruction evaluation data; on the other hand, there are no dedicated algorithms to improve the ability to follow complex instructions. To this end, this paper introduces Trace, a benchmark for improving and evaluating the complex instruction-following ability, which consists of 120K training data and 1K evaluation data. Furthermore, we propose IOPO (Input-Output Preference Optimization) alignment method which takes both input and output preference pairs into consideration, where LLMs not only rapidly align with response preferences but also meticulously explore the instruction preferences. Extensive experiments on both in-domain and out-of-domain datasets confirm the effectiveness of IOPO, showing 8.15%, 2.18% improvements on in-domain data and 5.91%, 2.83% on out-of-domain data compared to SFT and DPO respectively. Our code and dataset are released at https://anonymous.4open.science/r/Code7-34A5.</abstract>
      <url hash="2513d398">2025.acl-long.1079</url>
      <bibkey>zhang-etal-2025-iopo</bibkey>
    </paper>
    <paper id="1080">
      <title><fixed-case>P</fixed-case>ro<fixed-case>MAL</fixed-case>ex: Progressive Modular Adapters for Multi-Jurisdictional Legal Language Modeling</title>
      <author><first>Santosh</first><last>T.y.s.s</last></author>
      <author><first>Mohamed Hesham</first><last>Elganayni</last><affiliation>Technische Universität München</affiliation></author>
      <pages>22201-22217</pages>
      <abstract>This paper addresses the challenge of adapting language models to the jurisdiction-specific nature of legal corpora. Existing approaches—training separate models for each jurisdiction or using a single shared model—either fail to leverage common legal principles beneficial for low-resource settings or risk negative interference from conflicting jurisdictional interpretations. To overcome these limitations, we propose a parameter-efficient framework ProMALex, that first derives hierarchical relationships across jurisdictions and progressively inserts adapter modules across model layers based on jurisdictional similarity. This design allows modules in lower layers to be shared across jurisdictions, capturing common legal principles, while higher layers specialize through jurisdiction-specific adapters. Experimental results on two legal language modeling benchmarks demonstrate that ProMALex outperforms both fully shared and jurisdiction-specific models.</abstract>
      <url hash="f3a9c047">2025.acl-long.1080</url>
      <bibkey>t-y-s-s-elganayni-2025-promalex</bibkey>
    </paper>
    <paper id="1081">
      <title>Flipping Knowledge Distillation: Leveraging Small Models’ Expertise to Enhance <fixed-case>LLM</fixed-case>s in Text Matching</title>
      <author><first>Mingzhe</first><last>Li</last></author>
      <author><first>Jing</first><last>Xiang</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Qishen</first><last>Zhang</last></author>
      <author><first>Kaiyang</first><last>Wan</last></author>
      <author><first>Xiuying</first><last>Chen</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>22218-22229</pages>
      <abstract>Knowledge distillation typically involves transferring knowledge from a Large Language Model (LLM) to a Smaller Language Model (SLM). However, in tasks like text matching, smaller fine-tuned models often produce more effective domain-specific representations as they focus on optimizing the similarity between input pairs. To combine the specialized strengths of small models with the rich semantic understanding of LLMs, we propose a flipped knowledge distillation paradigm, where the LLM learns from the SLM. To bridge the architectural gap between commonly used decoder-only LLMs and the encoder-based frameworks of smaller models, we reinterpret LLMs as encoder-decoder models using LoRA. In this setup, the encoder generates compressed text representations, while the decoder transforms them into the output space. During training, the encoder produces text representations and computes their similarities, which are then aligned with the similarity scores produced by the teacher model. We achieve this alignment using our proposed Margin-aware Contrastive Learning (MCL) approach. MCL ensures accurate similarity for both positive and negative pairs, while also adaptively handling differences within positive and negative samples. We validate the effectiveness of our approach on financial and healthcare benchmarks as well as real-world online applications. Our model has been fully deployed in an online application environment, demonstrating its practical utility.</abstract>
      <url hash="ff1c609c">2025.acl-long.1081</url>
      <bibkey>li-etal-2025-flipping</bibkey>
    </paper>
    <paper id="1082">
      <title>Disentangling Language and Culture for Evaluating Multilingual Large Language Models</title>
      <author><first>Jiahao</first><last>Ying</last></author>
      <author><first>Wei</first><last>Tang</last></author>
      <author><first>Yiran</first><last>Zhao</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Yixin</first><last>Cao</last><affiliation>Fudan University</affiliation></author>
      <author><first>Yu</first><last>Rong</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Wenxuan</first><last>Zhang</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <pages>22230-22251</pages>
      <abstract>This paper introduces a Dual Evaluation Framework to comprehensively assess the multilingual capabilities of LLMs. By decomposing the evaluation along the dimensions of linguistic medium and cultural context, this framework enables a nuanced analysis of LLMs’ ability to process questions within both native and cross-cultural contexts cross-lingually. Extensive evaluations are conducted on a wide range of models, revealing a notable “Cultural-Linguistic Synergy” phenomenon, where models exhibit better performance when questions are culturally aligned with the language. This phenomenon is further explored through interpretability probing, which shows that a higher proportion of specific neurons are activated in a language’s cultural context. This activation proportion could serve as a potential indicator for evaluating multilingual performance during model training. Our findings challenge the prevailing notion that LLMs, primarily trained on English data, perform uniformly across languages and highlight the necessity of culturally and linguistically model evaluations.</abstract>
      <url hash="5a7b4829">2025.acl-long.1082</url>
      <bibkey>ying-etal-2025-disentangling</bibkey>
    </paper>
    <paper id="1083">
      <title>Detecting Sockpuppetry on <fixed-case>W</fixed-case>ikipedia Using Meta-Learning</title>
      <author><first>Luc</first><last>Raszewski</last></author>
      <author><first>Christine</first><last>de Kock</last></author>
      <pages>22252-22264</pages>
      <abstract>Malicious sockpuppet detection on Wikipedia is critical to preserving access to reliable information on the internet and preventing the spread of disinformation. Prior machine learning approaches rely on stylistic and meta-data features, but do not prioritise adaptability to author-specific behaviours. As a result, they struggle to effectively model the behaviour of specific sockpuppet-groups, especially when text data is limited. To address this, we propose the application of meta-learning, a machine learning technique designed to improve performance in data-scarce settings by training models across multiple tasks. Meta-learning optimises a model for rapid adaptation to the writing style of a new sockpuppet-group. Our results show that meta-learning significantly enhances the precision of predictions compared to pre-trained models, marking an advancement in combating sockpuppetry on open editing platforms. We release an updated dataset of sockpuppet investigations to foster future research in both sockpuppetry and meta-learning fields.</abstract>
      <url hash="79af76cc">2025.acl-long.1083</url>
      <bibkey>raszewski-de-kock-2025-detecting</bibkey>
    </paper>
    <paper id="1084">
      <title>Diversity-oriented Data Augmentation with Large Language Models</title>
      <author><first>Zaitian</first><last>Wang</last></author>
      <author><first>Jinghan</first><last>Zhang</last><affiliation>Portland State University</affiliation></author>
      <author><first>Xinhao</first><last>Zhang</last></author>
      <author><first>Kunpeng</first><last>Liu</last><affiliation>Portland State University</affiliation></author>
      <author><first>Pengfei</first><last>Wang</last><affiliation>Chinese Academy of Sciences</affiliation></author>
      <author><first>Yuanchun</first><last>Zhou</last><affiliation>Computer Network Information Center, Chinese Academy of Sciences,</affiliation></author>
      <pages>22265-22283</pages>
      <abstract>Data augmentation is an essential technique in natural language processing (NLP) for enriching training datasets by generating diverse samples. This process is crucial for improving the robustness and generalization capabilities of NLP models. However, a significant challenge remains: Insufficient Attention to Sample Distribution Diversity. Most existing methods focus on increasing the sample numbers while neglecting the sample distribution diversity, which can lead to model overfitting. In response, we explore data augmentation’s impact on dataset diversity and propose a Diversity-oriented data Augmentation framework (DoAug). Specifically, we utilize a diversity-oriented fine-tuning approach to train a large language model (LLM) as a diverse paraphraser, which is capable of augmenting textual datasets by generating diversified paraphrases. Then, we apply the LLM paraphraser to a selected coreset of highly informative samples and integrate the paraphrases with the original data to create a more diverse augmented dataset. Finally, we conduct extensive experiments on 12 real-world textual datasets. The results show that our fine-tuned LLM augmenter improves diversity while preserving label consistency, thereby enhancing the robustness and performance of downstream tasks. Specifically, it achieves an average performance gain of 10.52%, surpassing the runner-up baseline with more than three percentage points.</abstract>
      <url hash="21dc6387">2025.acl-long.1084</url>
      <bibkey>wang-etal-2025-diversity</bibkey>
    </paper>
    <paper id="1085">
      <title><fixed-case>C</fixed-case>ore<fixed-case>E</fixed-case>val: Automatically Building Contamination-Resilient Datasets with Real-World Knowledge toward Reliable <fixed-case>LLM</fixed-case> Evaluation</title>
      <author><first>Jingqian</first><last>Zhao</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Bingbing</first><last>Wang</last></author>
      <author><first>Geng</first><last>Tu</last></author>
      <author><first>Yice</first><last>Zhang</last></author>
      <author><first>Qianlong</first><last>Wang</last></author>
      <author><first>Bin</first><last>Liang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Jing</first><last>Li</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Ruifeng</first><last>Xu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>22284-22306</pages>
      <abstract>Data contamination poses a significant challenge to the fairness of LLM evaluations in natural language processing tasks by inadvertently exposing models to test data during training.Current studies mitigate this issue by modifying existing datasets or generating new ones from freshly collected information. However, these methods fall short of ensuring contamination-resilient evaluation, as they fail to fully eliminate pre-existing knowledge from models or preserve the semantic complexity of the original datasets. To address these limitations, we propose <tex-math>\textbf{CoreEval}</tex-math>, a <tex-math>\textbf{Co}</tex-math>ntamination-<tex-math>\textbf{re}</tex-math>silient <tex-math>\textbf{Eval}</tex-math>uation strategy for automatically updating data with real-world knowledge. This approach begins by extracting entity relationships from the original data and leveraging the GDELT database to retrieve relevant and up-to-date knowledge. The retrieved knowledge is then recontextualized and integrated with the original data, which is refined and restructured to ensure semantic coherence and enhanced task relevance. Ultimately, a robust data reflection mechanism in a Chain-of-Thought manner to iteratively verify and refine labels, ensuring consistency between the updated and original datasets. Extensive experiments on updated datasets validate the robustness of CoreEval, demonstrating its effectiveness in mitigating performance overestimation caused by data contamination.</abstract>
      <url hash="cc6ef157">2025.acl-long.1085</url>
      <bibkey>zhao-etal-2025-coreeval</bibkey>
    </paper>
    <paper id="1086">
      <title><fixed-case>R</fixed-case>i<fixed-case>OT</fixed-case>: Efficient Prompt Refinement with Residual Optimization Tree</title>
      <author><first>Chenyi</first><last>Zhou</last></author>
      <author><first>Zhengyan</first><last>Shi</last></author>
      <author><first>Yuan</first><last>Yao</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Lei</first><last>Liang</last></author>
      <author><first>Huajun</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Qiang</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <pages>22307-22323</pages>
      <abstract>Recent advancements in large language models (LLMs) have highlighted their potential across a variety of tasks, but their performance still heavily relies on the design of effective prompts. Existing methods for automatic prompt optimization face two challenges: lack of diversity, limiting the exploration of valuable and innovative directions and semantic drift, where optimizations for one task can degrade performance in others. To address these issues, we propose Residual Optimization Tree (RiOT), a novel framework for automatic prompt optimization. RiOT iteratively refines prompts through text gradients, generating multiple semantically diverse candidates at each step, and selects the best prompt using perplexity. Additionally, RiOT incorporates the text residual connection to mitigate semantic drift by selectively retaining beneficial content across optimization iterations. A tree structure efficiently manages the optimization process, ensuring scalability and flexibility. Extensive experiments across five benchmarks — covering commonsense, mathematical, logical, temporal, and semantic reasoning — demonstrate that RiOT outperforms both previous prompt optimization methods and manual prompting. Code will be released.</abstract>
      <url hash="22d508f8">2025.acl-long.1086</url>
      <bibkey>zhou-etal-2025-riot</bibkey>
    </paper>
    <paper id="1087">
      <title>Caution for the Environment: Multimodal <fixed-case>LLM</fixed-case> Agents are Susceptible to Environmental Distractions</title>
      <author><first>Xinbei</first><last>Ma</last></author>
      <author><first>Yiting</first><last>Wang</last></author>
      <author id="yao-yao"><first>Yao</first><last>Yao</last></author>
      <author><first>Tongxin</first><last>Yuan</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Aston</first><last>Zhang</last><affiliation>Meta</affiliation></author>
      <author><first>Zhuosheng</first><last>Zhang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Hai</first><last>Zhao</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>22324-22339</pages>
      <abstract>This paper investigates the faithfulness of multimodal large language model (MLLM) agents in a graphical user interface (GUI) environment, aiming to address the research question of whether multimodal GUI agents can be distracted by environmental context. A general scenario is proposed where both the user and the agent are benign, and the environment, while not malicious, contains unrelated content. A wide range of MLLMs are evaluated as GUI agents using a simulated dataset, following three working patterns with different levels of perception. Experimental results reveal that even the most powerful models, whether generalist agents or specialist GUI agents, are susceptible to distractions. While recent studies predominantly focus on the helpfulness of agents, our findings first indicate that these agents are prone to environmental distractions. Furthermore, we implement an adversarial environment injection and analyze the approach to improve faithfulness, calling for a collective focus on this important topic.</abstract>
      <url hash="56efffe3">2025.acl-long.1087</url>
      <bibkey>ma-etal-2025-caution</bibkey>
    </paper>
    <paper id="1088">
      <title>Automatic Evaluation for Text-to-image Generation: Task-decomposed Framework, Distilled Training, and Meta-evaluation Benchmark</title>
      <author><first>Rong-Cheng</first><last>Tu</last></author>
      <author><first>Zi-Ao</first><last>Ma</last></author>
      <author><first>Tian</first><last>Lan</last></author>
      <author><first>Yuehao</first><last>Zhao</last></author>
      <author><first>Heyan</first><last>Huang</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Xian-Ling</first><last>Mao</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <pages>22340-22361</pages>
      <abstract>Driven by the remarkable progress in diffusion models, text-to-image generation has achieved substantial advancements, underscoring the urgent need for robust automatic quality assessment. This task is inherently complex, requiring evaluations that range from object presence and attribute correctness to relational consistency and visual fidelity. Consequently, current state-of-the-art MLLM-based approaches often rely on powerful commercial models such as GPT-4o, which offer superior reasoning and instruction-following capabilities but are not universally accessible. In contrast, while open-source MLLMs demonstrate promising skills in vision and language understanding, they underperform in comprehensive image quality assessment.To address these challenges, we propose a task decomposition evaluation framework based on GPT-4o to automatically construct a specialized training dataset, breaking down the multifaceted evaluation process into simpler sub-tasks and thus reducing learning complexity. Building on this dataset, we design novel training strategies to distill GPT-4o’s evaluation capabilities into a <tex-math>7\text{B}</tex-math> open-source MLLM, MiniCPM-V-2.6, enabling it to better follow instructions across diverse assessment criteria. Furthermore, to reliably and comprehensively assess prior works and our proposed model, we manually annotate a meta-evaluation benchmark that includes chain-of-thought explanations alongside quality scores for generated images.Experimental results demonstrate that our distilled open-source MLLM significantly outperforms the current state-of-the-art GPT-4o-base baseline, VIEScore, with over 4.6% improvement in Spearman and Kendall correlations with human judgments.</abstract>
      <url hash="76615bb5">2025.acl-long.1088</url>
      <bibkey>tu-etal-2025-automatic</bibkey>
    </paper>
    <paper id="1089">
      <title>Mitigating Lost-in-Retrieval Problems in Retrieval Augmented Multi-Hop Question Answering</title>
      <author><first>Rongzhi</first><last>Zhu</last></author>
      <author><first>Xiangyu</first><last>Liu</last></author>
      <author><first>Zequn</first><last>Sun</last></author>
      <author><first>Yiwei</first><last>Wang</last><affiliation>University of California, Merced</affiliation></author>
      <author><first>Wei</first><last>Hu</last><affiliation>Nanjing University</affiliation></author>
      <pages>22362-22375</pages>
      <abstract>In this paper, we identify a critical problem, “lost-in-retrieval”, in retrieval-augmented multi-hop question answering (QA): the key entities are missed in LLMs’ sub-question decomposition. “Lost-in-retrieval” significantly degrades the retrieval performance, which disrupts the reasoning chain and leads to the incorrect answers. To resolve this problem, we propose a progressive retrieval and rewriting method, namely ChainRAG, which sequentially handles each sub-question by completing missing key entities and retrieving relevant sentences from a sentence graph for answer generation. Each step in our retrieval and rewriting process builds upon the previous one, creating a seamless chain that leads to accurate retrieval and answers. Finally, all retrieved sentences and sub-question answers are integrated to generate a comprehensive answer to the original question. We evaluate ChainRAG on three multi-hop QA datasets—MuSiQue, 2Wiki, and HotpotQA—using three large language models: GPT4o-mini, Qwen2.5-72B, and GLM-4-Plus. Empirical results demonstrate that ChainRAG consistently outperforms baselines in both effectiveness and efficiency.</abstract>
      <url hash="1558f5be">2025.acl-long.1089</url>
      <bibkey>zhu-etal-2025-mitigating</bibkey>
    </paper>
    <paper id="1090">
      <title><fixed-case>T</fixed-case>able<fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case>: Low-rank Adaptation on Table Structure Understanding for Large Language Models</title>
      <author><first>Xinyi</first><last>He</last></author>
      <author><first>Yihao</first><last>Liu</last></author>
      <author><first>Mengyu</first><last>Zhou</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Yeye</first><last>He</last><affiliation>Microsoft</affiliation></author>
      <author><first>Haoyu</first><last>Dong</last></author>
      <author><first>Shi</first><last>Han</last><affiliation>Microsoft</affiliation></author>
      <author><first>Zejian</first><last>Yuan</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Dongmei</first><last>Zhang</last><affiliation>Microsoft</affiliation></author>
      <pages>22376-22391</pages>
      <abstract>Tabular data are crucial in many fields and their understanding by large language models (LLMs) under high parameter efficiency paradigm is important. However, directly applying parameter-efficient fine-tuning (PEFT) techniques to tabular tasks presents significant challenges, particularly in terms of better table serialization and the representation of two-dimensional structured information within a one-dimensional sequence. To address this, we propose TableLoRA, a module designed to improve LLMs’ understanding of table structure during PEFT. It incorporates special tokens for serializing tables with special token encoder and uses 2D LoRA to encode low-rank information on cell positions. Experiments on four tabular-related datasets demonstrate that TableLoRA consistently outperforms vanilla LoRA and surpasses various table encoding methods tested in control experiments. These findings reveal that TableLoRA, as a table-specific LoRA, enhances the ability of LLMs to process tabular data effectively, especially in low-parameter settings, demonstrating its potential as a robust solution for handling table-related tasks.</abstract>
      <url hash="2623c2ad">2025.acl-long.1090</url>
      <bibkey>he-etal-2025-tablelora</bibkey>
    </paper>
    <paper id="1091">
      <title>Condor: Enhance <fixed-case>LLM</fixed-case> Alignment with Knowledge-Driven Data Synthesis and Refinement</title>
      <author><first>Maosongcao</first><last>Maosongcao</last></author>
      <author><first>Taolin</first><last>Zhang</last></author>
      <author><first>Mo</first><last>Li</last></author>
      <author><first>Chuyu</first><last>Zhang</last><affiliation>ShanghaiTech University</affiliation></author>
      <author><first>Yunxin</first><last>Liu</last></author>
      <author><first>Conghui</first><last>He</last><affiliation>Shanghai AI Lab</affiliation></author>
      <author><first>Haodong</first><last>Duan</last></author>
      <author><first>Songyang</first><last>Zhang</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Kai</first><last>Chen</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <pages>22392-22412</pages>
      <abstract>The quality of Supervised Fine-Tuning (SFT) data plays a critical role in enhancing the conversational capabilities of Large Language Models (LLMs). However, the availability of high-quality human-annotated SFT data has become a significant bottleneck for LLMs, necessitating a greater reliance on synthetic training data. In this work, we introduce Condor, a two-stage synthetic data generation framework that incorporates World Knowledge Trees and Self-Reflection Refinement to produce high-quality SFT data at scale. Our experimental results demonstrate that a base model fine-tuned on only 20K Condor-generated samples achieves superior performance compared to instruct model trained with RLHF. The additional refinement stage in Condor further enables iterative self-improvement for LLMs at various scales (up to 72B), validating the effectiveness of our approach. Furthermore, our investigation into the scaling of synthetic data in post-training reveals substantial unexplored potential for performance improvements, opening promising avenues for future research.</abstract>
      <url hash="364187d8">2025.acl-long.1091</url>
      <bibkey>maosongcao-etal-2025-condor</bibkey>
    </paper>
    <paper id="1092">
      <title><fixed-case>C</fixed-case>ul<fixed-case>F</fixed-case>i<fixed-case>T</fixed-case>: A Fine-grained Cultural-aware <fixed-case>LLM</fixed-case> Training Paradigm via Multilingual Critique Data Synthesis</title>
      <author><first>Ruixiang</first><last>Feng</last></author>
      <author><first>Shen</first><last>Gao</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Xiuying</first><last>Chen</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Lisi</first><last>Chen</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Shuo</first><last>Shang</last></author>
      <pages>22413-22430</pages>
      <abstract>Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks, yet they often exhibit a specific cultural bias, neglecting the values and linguistic diversity of low-resource regions. This cultural bias not only undermines universal equality but also risks reinforcing stereotypes and perpetuating discrimination. To address this, we propose CulFiT, a novel culturally-aware training paradigm that leverages multilingual data and fine-grained reward modeling to enhance cultural sensitivity and inclusivity. Our approach synthesizes diverse cultural-related questions, constructs critique data in multiple culturally relevant languages, and employs fine-grained rewards to decompose cultural texts into verifiable knowledge units for interpretable evaluation. We also introduce GlobalOpinionQA, a multilingual open-ended question-answering dataset designed to evaluate culturally-aware responses in a global context. Extensive experiments on three existing benchmarks and our GlobalOpinionQA demonstrate that CulFiT achieves state-of-the-art open-source model performance in cultural alignment and general reasoning.</abstract>
      <url hash="e633b9ef">2025.acl-long.1092</url>
      <bibkey>feng-etal-2025-culfit</bibkey>
    </paper>
    <paper id="1093">
      <title>Decoding Knowledge Attribution in Mixture-of-Experts: A Framework of Basic-Refinement Collaboration and Efficiency Analysis</title>
      <author><first>Junzhuo</first><last>Li</last></author>
      <author><first>Bo</first><last>Wang</last></author>
      <author><first>Xiuze</first><last>Zhou</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Peijie</first><last>Jiang</last></author>
      <author><first>Jia</first><last>Liu</last><affiliation>Ant Group</affiliation></author>
      <author><first>Xuming</first><last>Hu</last><affiliation>The Hong Kong University of Science and Technology (Guangzhou) and Hong Kong University of Science and Technology</affiliation></author>
      <pages>22431-22446</pages>
      <abstract>The interpretability of Mixture-of-Experts (MoE) models, especially those with heterogeneous designs, remains underexplored. Existing attribution methods for dense models fail to capture dynamic routing-expert interactions in sparse MoE architectures. To address this issue, we propose a cross-level attribution algorithm to analyze sparse MoE architectures (Qwen 1.5-MoE, OLMoE, Mixtral-8x7B) against dense models (Qwen 1.5-7B, Llama-7B, Mistral-7B). Results show MoE models achieve 31% higher per-layer efficiency via a “mid-activation, late-amplification” pattern: early layers screen experts, while late layers refine knowledge collaboratively. Ablation studies reveal a “basic-refinement” framework—shared experts handle general tasks (entity recognition), while routed experts specialize in domain-specific processing (geographic attributes). Semantic-driven routing is evidenced by strong correlations between attention heads and experts (<tex-math>r=0.68</tex-math>), enabling task-aware coordination. Notably, architectural depth dictates robustness: deep Qwen-MoE mitigates expert failures (e.g., 43% MRR drop in geographic tasks when blocking top-10 experts) through shared expert redundancy, whereas shallow Olmoe suffers severe degradation (76% drop). Task sensitivity further guides design: core-sensitive tasks (geography) require concentrated expertise, while distributed-tolerant tasks (object attributes) leverage broader participation. These insights advance MoE interpretability, offering principles to balance efficiency, specialization, and robustness.</abstract>
      <url hash="fed902fb">2025.acl-long.1093</url>
      <bibkey>li-etal-2025-decoding</bibkey>
    </paper>
    <paper id="1094">
      <title><fixed-case>C</fixed-case>hart<fixed-case>L</fixed-case>ens: Fine-grained Visual Attribution in Charts</title>
      <author><first>Manan</first><last>Suri</last></author>
      <author><first>Puneet</first><last>Mathur</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Nedim</first><last>Lipka</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Ryan A.</first><last>Rossi</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Dinesh</first><last>Manocha</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>22447-22462</pages>
      <abstract>The growing capabilities of multimodal large language models (MLLMs) have advanced tasks like chart understanding. However, these models often suffer from hallucinations, where generated text sequences conflict with the provided visual data. To address this, we introduce Post-Hoc Visual Attribution for Charts, which identifies fine-grained chart elements that validate a given chart-associated response. We propose ChartLens, a novel chart attribution algorithm that uses segmentation-based techniques to identify chart objects and employs set-of-marks prompting with MLLMs for fine-grained visual attribution. Additionally, we present ChartVA-Eval, a benchmark with synthetic and real-world charts from diverse domains like finance, policy, and economics, featuring fine-grained attribution annotations. Our evaluations show that ChartLens improves fine-grained attributions by 26-66%.</abstract>
      <url hash="5270db43">2025.acl-long.1094</url>
      <bibkey>suri-etal-2025-chartlens</bibkey>
    </paper>
    <paper id="1095">
      <title><fixed-case>LESA</fixed-case>: Learnable <fixed-case>LLM</fixed-case> Layer Scaling-Up</title>
      <author><first>Yifei</first><last>Yang</last></author>
      <author><first>Zouying</first><last>Cao</last></author>
      <author><first>Xinbei</first><last>Ma</last></author>
      <author id="yao-yao"><first>Yao</first><last>Yao</last></author>
      <author><first>Zhi</first><last>Chen</last></author>
      <author><first>Libo</first><last>Qin</last><affiliation>Central South University</affiliation></author>
      <author><first>Hai</first><last>Zhao</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>22463-22476</pages>
      <abstract>Training Large Language Models (LLMs) from scratch requires immense computational resources, making it prohibitively expensive. Model scaling-up offers a promising solution by leveraging the parameters of smaller models to create larger ones. However, existing depth scaling-up methods rely on empirical heuristic rules for layer duplication, which result in poorer initialization and slower convergence during continual pre-training. We propose <tex-math>\textbf{LESA}</tex-math>, a novel learnable method for depth scaling-up. By concatenating parameters from each layer and applying Singular Value Decomposition, we uncover latent patterns between layers, suggesting that inter-layer parameters can be learned. LESA uses a neural network to predict the parameters inserted between adjacent layers, enabling better initialization and faster training. Experiments show that LESA outperforms existing baselines, achieving superior performance with less than half the computational cost during continual pre-training. Extensive analyses demonstrate its effectiveness across different model sizes and tasks.</abstract>
      <url hash="c23824f9">2025.acl-long.1095</url>
      <bibkey>yang-etal-2025-lesa</bibkey>
    </paper>
    <paper id="1096">
      <title><fixed-case>MMRC</fixed-case>: A Large-Scale Benchmark for Understanding Multimodal Large Language Model in Real-World Conversation</title>
      <author><first>Haochen</first><last>Xue</last></author>
      <author><first>Feilong</first><last>Tang</last></author>
      <author><first>Ming</first><last>Hu</last></author>
      <author><first>Yexin</first><last>Liu</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Qidong</first><last>Huang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Yulong</first><last>Li</last></author>
      <author><first>Chengzhi</first><last>Liu</last></author>
      <author><first>Zhongxing</first><last>Xu</last></author>
      <author><first>Chong</first><last>Zhang</last></author>
      <author><first>Chun-Mei</first><last>Feng</last><affiliation>IHPC</affiliation></author>
      <author><first>Yutong</first><last>Xie</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Imran</first><last>Razzak</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Zongyuan</first><last>Ge</last></author>
      <author><first>Jionglong</first><last>Su</last><affiliation>Xi’an Jiaotong-Liverpool University</affiliation></author>
      <author><first>Junjun</first><last>He</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Yu</first><last>Qiao</last><affiliation>Shanghai Aritifcal Intelligence Laboratory</affiliation></author>
      <pages>22477-22503</pages>
      <abstract>Recent multimodal large language models (MLLMs) have demonstrated significant potential in open-ended conversation, generating more accurate and personalized responses. However, their abilities to memorize, recall, and reason in sustained interactions within real-world scenarios remain underexplored. This paper introduces MMRC, a Multi-Modal Real-world Conversation benchmark for evaluating six core open-ended abilities of MLLMs: information extraction, multi-turn reasoning, information update, image management, memory recall, and answer refusal. With data collected from real-world scenarios, MMRC comprises 5,120 conversations and 28,720 corresponding manually labeled questions, posing a significant challenge to existing MLLMs. Evaluations on 20 MLLMs in MMRC indicate an accuracy drop during open-ended interactions. We identify four common failure patterns: long-term memory degradation, inadequacies in updating factual knowledge, accumulated assumption of error propagation, and reluctance to “say no.” To mitigate these issues, we propose a simple yet effective NOTE-TAKING strategy, which can record key information from the conversation and remind the model during its responses, enhancing conversational capabilities. Experiments across six MLLMs demonstrate significant performance improvements.</abstract>
      <url hash="c3a24331">2025.acl-long.1096</url>
      <bibkey>xue-etal-2025-mmrc</bibkey>
    </paper>
    <paper id="1097">
      <title>Towards the Law of Capacity Gap in Distilling Language Models</title>
      <author><first>Chen</first><last>Zhang</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Qiuchi</first><last>Li</last></author>
      <author><first>Dawei</first><last>Song</last><affiliation>Beijing Institute of Technology and Open University</affiliation></author>
      <author><first>Zheyu</first><last>Ye</last><affiliation>Xiaohongshu Inc</affiliation></author>
      <author><first>Yan</first><last>Gao</last></author>
      <author><first>Yao</first><last>Hu</last><affiliation>Xiaohongshu</affiliation></author>
      <pages>22504-22528</pages>
      <abstract>Language model (LM) distillation aims at distilling the knowledge in a large teacher LM to a small student one. As a critical issue facing LM distillation, a superior student often arises from a teacher of a relatively small scale instead of a larger one, especially in the presence of substantial capacity gap between the teacher and student. This issue, often referred to as the <i>curse of capacity gap</i>, suggests that there is likely an optimal teacher yielding the best-performing student along the scaling course of the teacher. Consequently, distillation trials on teachers of a wide range of scales are called for to determine the optimal teacher, which becomes computationally intensive in the context of large LMs (LLMs). This paper addresses this critical bottleneck by providing the <i>law of capacity gap</i> inducted from a preliminary study on distilling a broad range of small-scale (&lt;3B) LMs, where the optimal teacher consistently scales linearly with the student scale across different model and data scales. By extending the law to LLM distillation on a larger scale (7B), we succeed in obtaining versatile LLMs that outperform a wide array of competitors.</abstract>
      <url hash="6b47aafd">2025.acl-long.1097</url>
      <bibkey>zhang-etal-2025-towards-law</bibkey>
    </paper>
    <paper id="1098">
      <title><fixed-case>W</fixed-case>hi<fixed-case>SPA</fixed-case>: Semantically and Psychologically Aligned Whisper with Self-Supervised Contrastive and Student-Teacher Learning</title>
      <author><first>Rajath</first><last>Rao</last></author>
      <author><first>Adithya</first><last>V Ganesan</last><affiliation>, State University of New York, Stony Brook</affiliation></author>
      <author><first>Oscar</first><last>Kjell</last><affiliation>Stony Brook University and Lund University</affiliation></author>
      <author><first>Jonah</first><last>Luby</last></author>
      <author><first>Akshay</first><last>Raghavan</last></author>
      <author><first>Scott M.</first><last>Feltman</last><affiliation>State University of New York at Stony Brook</affiliation></author>
      <author><first>Whitney</first><last>Ringwald</last><affiliation>University of Minnesota - Twin Cities</affiliation></author>
      <author><first>Ryan L.</first><last>Boyd</last><affiliation>University of Texas at Dallas</affiliation></author>
      <author><first>Benjamin J.</first><last>Luft</last><affiliation>State University of New York at Stony Brook</affiliation></author>
      <author><first>Camilo J.</first><last>Ruggero</last><affiliation>University of Texas at Dallas</affiliation></author>
      <author><first>Neville</first><last>Ryant</last><affiliation>Linguistic Data Consortium</affiliation></author>
      <author><first>Roman</first><last>Kotov</last><affiliation>State University of New York at Stony Brook</affiliation></author>
      <author><first>H.</first><last>Schwartz</last><affiliation>Stony Brook University (SUNY)</affiliation></author>
      <pages>22529-22544</pages>
      <abstract>Current speech encoding pipelines often rely on an additional text-based LM to get robust representations of human communication, even though SotA speech-to-text models often have a LM within. This work proposes an approach to improve the LM within an audio model such that the subsequent text-LM is unnecessary. We introduce **WhiSPA** (**Whi**sper with **S**emantic and **P**sychological **A**lignment), which leverages a novel audio training objective: contrastive loss with a language model embedding as a teacher. Using over 500k speech segments from mental health audio interviews, we evaluate the utility of aligning Whisper’s latent space with semantic representations from a text autoencoder (SBERT) and lexically derived embeddings of basic psychological dimensions: emotion and personality. Over self-supervised affective tasks and downstream psychological tasks, WhiSPA surpasses current speech encoders, achieving an average error reduction of 73.4% and 83.8%, respectively. WhiSPA demonstrates that it is not always necessary to run a subsequent text LM on speech-to-text output in order to get a rich psychological representation of human communication.</abstract>
      <url hash="41f7e9c9">2025.acl-long.1098</url>
      <bibkey>rao-etal-2025-whispa</bibkey>
    </paper>
    <paper id="1099">
      <title>Keys to Robust Edits: From Theoretical Insights to Practical Advances</title>
      <author><first>Jianhao</first><last>Yan</last><affiliation>Westlake University</affiliation></author>
      <author><first>Futing</first><last>Wang</last><affiliation>Westlake University</affiliation></author>
      <author><first>Yun</first><last>Luo</last><affiliation>westlake university</affiliation></author>
      <author><first>Yafu</first><last>Li</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Yue</first><last>Zhang</last><affiliation>Westlake University</affiliation></author>
      <pages>22545-22560</pages>
      <abstract>Large language models (LLMs) struggle with maintaining accurate knowledge due to conflicting/outdated parametric memories. While locate-and-edit methods address this, their reliance on models’ internal representations leads to robustness failures in long-context reasoning and paraphrased queries. We identify a fundamental limitation of locate-and-edit methods: existing semantic keys (for memory localization) cannot simultaneously satisfy robustness (context-invariant activation) and specificity (precise knowledge discrimination). Through theoretical error-bound analysis, we establish formal criteria for effective editing.Our solution introduces <i>Robust Edit Pathway (REP)</i>, a plug-and-play module that: (1) disentangles editing keys from native model representations; (2) dynamically adjusts keys via contrastive learning to achieve robustness-specificity balance. Extensive experiments across various editing methods (ROME/MEMIT/R-ROME/EMMET), existing LLMs (LLaMA2, QWen, Mistral), and datasets (CounterFact, ZsRE) show that REP improves success rate over robustness tests by up-to 66.4% while maintaining the success rate unaffected.</abstract>
      <url hash="438d362f">2025.acl-long.1099</url>
      <bibkey>yan-etal-2025-keys</bibkey>
    </paper>
    <paper id="1100">
      <title>Boosting <fixed-case>LLM</fixed-case>’s Molecular Structure Elucidation with Knowledge Enhanced Tree Search Reasoning</title>
      <author><first>Xiang</first><last>Zhuang</last></author>
      <author><first>Bin</first><last>Wu</last></author>
      <author><first>Jiyu</first><last>Cui</last></author>
      <author><first>Kehua</first><last>Feng</last></author>
      <author><first>Xiaotong</first><last>Li</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Huabin</first><last>Xing</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Keyan</first><last>Ding</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Qiang</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Huajun</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <pages>22561-22576</pages>
      <abstract>Molecular structure elucidation involves deducing a molecule’s structure from various types of spectral data, which is crucial in chemical experimental analysis. While large language models (LLMs) have shown remarkable proficiency in analyzing and reasoning through complex tasks, they still encounter substantial challenges in molecular structure elucidation. We identify that these challenges largely stem from LLMs’ limited grasp of specialized chemical knowledge. In this work, we introduce a Knowledge-enhanced reasoning framework for Molecular Structure Elucidation (K-MSE), leveraging Monte Carlo Tree Search for test-time scaling as a plugin. Specifically, we construct an external molecular substructure knowledge base to extend the LLMs’ coverage of the chemical structure space. Furthermore, we design a specialized molecule-spectrum scorer to act as a reward model for the reasoning process, addressing the issue of inaccurate solution evaluation in LLMs. Experimental results show that our approach significantly boosts performance, particularly gaining more than 20% improvement on both GPT-4o-mini and GPT-4o.</abstract>
      <url hash="9473c0bc">2025.acl-long.1100</url>
      <bibkey>zhuang-etal-2025-boosting</bibkey>
    </paper>
    <paper id="1101">
      <title><fixed-case>MEMERAG</fixed-case>: A Multilingual End-to-End Meta-Evaluation Benchmark for Retrieval Augmented Generation</title>
      <author><first>María Andrea</first><last>Cruz Blandón</last><affiliation>Tampere University</affiliation></author>
      <author><first>Jayasimha</first><last>Talur</last></author>
      <author><first>Bruno</first><last>Charron</last></author>
      <author><first>Dong</first><last>Liu</last></author>
      <author><first>Saab</first><last>Mansour</last><affiliation>Amazon</affiliation></author>
      <author><first>Marcello</first><last>Federico</last><affiliation>Amazon</affiliation></author>
      <pages>22577-22595</pages>
      <abstract>Automatic evaluation of retrieval augmented generation (RAG) systems relies on fine-grained dimensions like faithfulness and relevance, as judged by expert human annotators. Meta-evaluation benchmarks support the development of automatic evaluators that correlate well with human judgement. However, existing benchmarks predominantly focus on English or use translated data, which fails to capture cultural nuances. A native approach provides a better representation of the end user experience.In this work, we develop a Multilingual End-to-end Meta-Evaluation RAG benchmark MEMERAG. Our benchmark builds on the popular MIRACL dataset, using native-language questions and generating responses with diverse large language models (LLMs), which are then assessed by expert annotators for faithfulness and relevance. We describe our annotation process and show that it achieves high inter-annotator agreement. We then analyse the performance of the answer-generating LLMs across languages as per the human evaluators. Finally we apply the dataset to our main use-case which is to benchmark multilingual automatic evaluators (LLM-as-a-judge). We show that our benchmark can reliably identify improvements offered by advanced prompting techniques and LLMs. We release our benchmark to support the community developing accurate evaluation methods for multilingual RAG systems.</abstract>
      <url hash="151aa1ab">2025.acl-long.1101</url>
      <bibkey>cruz-blandon-etal-2025-memerag</bibkey>
    </paper>
    <paper id="1102">
      <title>The Role of Visual Modality in Multimodal Mathematical Reasoning: Challenges and Insights</title>
      <author><first>Yufang</first><last>Liu</last></author>
      <author><first>Yao</first><last>Du</last><affiliation>Beihang University</affiliation></author>
      <author><first>Tao</first><last>Ji</last></author>
      <author><first>Jianing</first><last>Wang</last><affiliation>Meituan</affiliation></author>
      <author id="yang-liu"><first>Yang</first><last>Liu</last></author>
      <author><first>Yuanbin</first><last>Wu</last></author>
      <author><first>Aimin</first><last>Zhou</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Mengdi</first><last>Zhang</last></author>
      <author><first>Xunliang</first><last>Cai</last><affiliation>Meituan</affiliation></author>
      <pages>22596-22611</pages>
      <abstract>Recent research has increasingly focused on multimodal mathematical reasoning, particularly emphasizing the creation of relevant datasets and benchmarks. Despite this, the role of visual information in reasoning has been underexplored. Our findings show that existing multimodal mathematical models minimally leverage visual information, and model performance remains largely unaffected by changes to or removal of images in the dataset. We attribute this to the dominance of textual information and answer options that inadvertently guide the model to correct answers. To improve evaluation methods, we introduce the HC-M3D dataset, specifically designed to require image reliance for problem-solving and to challenge models with similar, yet distinct, images that change the correct answer. In testing leading models, their failure to detect these subtle visual differences suggests limitations in current visual perception capabilities. Additionally, we observe that the common approach of improving general VQA capabilities by combining various types of image encoders does not contribute to math reasoning performance. This finding also presents a challenge to enhancing visual reliance during math reasoning.</abstract>
      <url hash="93f407da">2025.acl-long.1102</url>
      <bibkey>liu-etal-2025-role</bibkey>
    </paper>
    <paper id="1103">
      <title>The Essence of Contextual Understanding in Theory of Mind: A Study on Question Answering with Story Characters</title>
      <author><first>Chulun</first><last>Zhou</last></author>
      <author><first>Qiujing</first><last>Wang</last></author>
      <author><first>Mo</first><last>Yu</last><affiliation>WeChat AI, Tencent</affiliation></author>
      <author><first>Xiaoqian</first><last>Yue</last></author>
      <author><first>Rui</first><last>Lu</last></author>
      <author><first>Jiangnan</first><last>Li</last><affiliation>WeChat, Tencent Inc.</affiliation></author>
      <author><first>Yifan</first><last>Zhou</last></author>
      <author><first>Shunchi</first><last>Zhang</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <author><first>Wai</first><last>Lam</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>22612-22631</pages>
      <abstract>Theory-of-Mind (ToM) is a fundamental psychological capability that allows humans to understand and interpret the mental states of others. Humans infer others’ thoughts by integrating causal cues and indirect clues from broad contextual information, often derived from past interactions. In other words, human ToM heavily relies on the understanding about the backgrounds and life stories of others. Unfortunately, this aspect is largely overlooked in existing benchmarks for evaluating machines’ ToM capabilities, due to their usage of short narratives without global context, especially personal background of characters. In this paper, we verify the importance of comprehensive contextual understanding about personal backgrounds in ToM and assess the performance of LLMs in such complex scenarios. To achieve this, we introduce CharToM-QA benchmark, comprising 1,035 ToM questions based on characters from classic novels. Our human study reveals a significant disparity in performance: the same group of educated participants performs dramatically better when they have read the novels compared to when they have not. In parallel, our experiments on state-of-the-art LLMs, including the very recent o1 and DeepSeek-R1 models, show that LLMs still perform notably worse than humans, despite that they have seen these stories during pre-training. This highlights the limitations of current LLMs in capturing the nuanced contextual information required for ToM reasoning.</abstract>
      <url hash="4d3f7346">2025.acl-long.1103</url>
      <bibkey>zhou-etal-2025-essence</bibkey>
    </paper>
    <paper id="1104">
      <title><fixed-case>S</fixed-case><tex-math>^2</tex-math><fixed-case>R</fixed-case>: Teaching <fixed-case>LLM</fixed-case>s to Self-verify and Self-correct via Reinforcement Learning</title>
      <author><first>Ruotian</first><last>Ma</last><affiliation>Tencent</affiliation></author>
      <author><first>Peisong</first><last>Wang</last></author>
      <author><first>Cheng</first><last>Liu</last></author>
      <author><first>Xingyan</first><last>Liu</last></author>
      <author><first>Jiaqi</first><last>Chen</last><affiliation>The University of Hong Kong</affiliation></author>
      <author><first>Bang</first><last>Zhang</last><affiliation>Tencent</affiliation></author>
      <author><first>Xin</first><last>Zhou</last></author>
      <author><first>Nan</first><last>Du</last><affiliation>Tencent INC</affiliation></author>
      <author><first>Jia</first><last>Li</last><affiliation>Hong Kong University of Science and Technology (Guangzhou)</affiliation></author>
      <pages>22632-22654</pages>
      <abstract>Recent studies have demonstrated the effectiveness of LLM test-time scaling. However, existing approaches to incentivize LLMs’ deep thinking abilities generally require large-scale data or significant training efforts. Meanwhile, it remains unclear how to improve the thinking abilities of less powerful base models. In this work, we introduce S<tex-math>^2</tex-math>R, an efficient framework that enhances LLM reasoning by teaching models to self-verify and self-correct during inference. Specifically, we first initialize LLMs with iterative self-verification and self-correction behaviors through supervised fine-tuning on carefully curated data. The self-verification and self-correction skills are then further strengthened by outcome-level and process-level reinforcement learning with minimized resource requirements. Our results demonstrate that, with only 3.1k behavior initialization samples, Qwen2.5-math-7B achieves an accuracy improvement from 51.0% to 81.6%, outperforming models trained on an equivalent amount of long-CoT distilled data. We also discuss the effect of different RL strategies on enhancing LLMs’ deep reasoning. Extensive experiments and analysis based on three base models across both in-domain and out-of-domain benchmarks validate the effectiveness of S<tex-math>^2</tex-math>R.</abstract>
      <url hash="b786b4ad">2025.acl-long.1104</url>
      <bibkey>ma-etal-2025-s2r</bibkey>
    </paper>
    <paper id="1105">
      <title>Advancing Collaborative Debates with Role Differentiation through Multi-Agent Reinforcement Learning</title>
      <author><first>Haoran</first><last>Li</last></author>
      <author><first>Ziyi</first><last>Su</last></author>
      <author><first>Yun</first><last>Xue</last></author>
      <author><first>Zhiliang</first><last>Tian</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Yiping</first><last>Song</last></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <pages>22655-22666</pages>
      <abstract>Multi-agent collaborative tasks exhibit exceptional capabilities in natural language applications and generation. By prompting agents to assign clear roles, it is possible to facilitate cooperation and achieve complementary capabilities among LLMs. A common strategy involves adopting a relatively general role assignment mechanism, such as introducing a “judge” or a “summarizer”. However, these approaches lack task-specific role customization based on task characteristics. Another strategy involves decomposing the task based on domain knowledge and task characteristics, followed by assigning appropriate roles according to LLMs’ respective strengths, such as programmers and testers. However, in some given tasks, obtaining domain knowledge related to task characteristics and getting the strengths of different LLMs is hard. To solve these problems, we propose a Multi-LLM Cooperation (MLC) framework with automatic role assignment capabilities. The core idea of the MLC is to initialize role assignments randomly and then allow the role embeddings to be learned jointly with the downstream task. To capture the state transitions of multiple LLMs during turn-based speaking, the role embedding is sequence-aware. At the same time, to avoid role convergence, the role differentiation module in MLC encourages behavioral differentiation between LLMs while ensuring the LLM team consistency, guiding different LLMs to develop complementary strengths from the optimization level. Our experiments on seven datasets demonstrate that MLC significantly enhances collaboration and expertise, which collaboratively addresses multi-agent tasks.</abstract>
      <url hash="a266354d">2025.acl-long.1105</url>
      <bibkey>li-etal-2025-advancing</bibkey>
    </paper>
    <paper id="1106">
      <title>Retrieval-Augmented Fine-Tuning With Preference Optimization For Visual Program Generation</title>
      <author><first>Deokhyung</first><last>Kang</last><affiliation>Pohang University of Science and Technology</affiliation></author>
      <author><first>Jeonghun</first><last>Cho</last></author>
      <author><first>Yejin</first><last>Jeon</last></author>
      <author><first>Sunbin</first><last>Jang</last></author>
      <author><first>Minsub</first><last>Lee</last></author>
      <author><first>Jawoon</first><last>Cho</last></author>
      <author><first>Gary</first><last>Lee</last></author>
      <pages>22667-22686</pages>
      <abstract>Visual programming languages (VPLs) allow users to create programs through graphical interfaces, which results in easier accessibility and their widespread usage in various domains. To further enhance this accessibility, recent research has focused on generating VPL code from user instructions using large language models (LLMs). Specifically, by employing prompting-based methods, these studies have shown promising results. Nevertheless, such approaches can be less effective for industrial VPLs such as Ladder Diagram (LD). LD is a pivotal language used in industrial automation processes and involves extensive domain-specific configurations, which are difficult to capture in a single prompt. In this work, we demonstrate that training-based methods outperform prompting-based methods for LD generation accuracy, even with smaller backbone models. Building on these findings, we propose a two-stage training strategy to further enhance VPL generation. First, we employ retrieval-augmented fine-tuning to leverage the repetitive use of subroutines commonly seen in industrial VPLs. Second, we apply direct preference optimization (DPO) to further guide the model toward accurate outputs, using systematically generated preference pairs through graph editing operations. Extensive experiments on real-world LD data demonstrate that our approach improves program-level accuracy by over 10% compared to supervised fine-tuning, which highlights its potential to advance industrial automation.</abstract>
      <url hash="8d019293">2025.acl-long.1106</url>
      <bibkey>kang-etal-2025-retrieval</bibkey>
    </paper>
    <paper id="1107">
      <title><fixed-case>STRICTA</fixed-case>: Structured Reasoning in Critical Text Assessment for Peer Review and Beyond</title>
      <author><first>Nils</first><last>Dycke</last><affiliation>TU Darmstadt</affiliation></author>
      <author><first>Matej</first><last>Zečević</last><affiliation>TU Darmstadt</affiliation></author>
      <author><first>Ilia</first><last>Kuznetsov</last><affiliation>TU Darmstadt</affiliation></author>
      <author><first>Beatrix</first><last>Suess</last></author>
      <author><first>Kristian</first><last>Kersting</last><affiliation>German Research Center for AI, The Hessian Center for AI and TU Darmstadt</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Institute for Computer Science, Artificial Intelligence and Technology, Mohamed bin Zayed University of Artificial Intelligence and Technische Universität Darmstadt</affiliation></author>
      <pages>22687-22727</pages>
      <abstract>Critical text assessment is at the core of many expert activities, such as fact-checking, peer review, and essay grading. Yet, existing work treats critical text assessment as a black box problem, limiting interpretability and human-AI collaboration. To close this gap, we introduce Structured Reasoning in Critical Text Assessment (STRICTA), a novel specification framework to model text assessment as an explicit, step-wise reasoning process. STRICTA breaks down the assessment into a graph of interconnected reasoning steps drawing on causality theory (Pearl, 1995). This graph is populated based on expert interaction data and used to study the assessment process and facilitate human-AI collaboration. We formally define STRICTA and apply it in a study on biomedical paper assessment, resulting in a dataset of over 4000 reasoning steps from roughly 40 biomedical experts on more than 20 papers. We use this dataset to empirically study expert reasoning in critical text assessment, and investigate if LLMs are able to imitate and support experts within these workflows. The resulting tools and datasets pave the way for studying collaborative expert-AI reasoning in text assessment, in peer review and beyond.</abstract>
      <url hash="ed4e20bc">2025.acl-long.1107</url>
      <bibkey>dycke-etal-2025-stricta</bibkey>
    </paper>
    <paper id="1108">
      <title><fixed-case>XDAC</fixed-case>: <fixed-case>XAI</fixed-case>-Driven Detection and Attribution of <fixed-case>LLM</fixed-case>-Generated News Comments in <fixed-case>K</fixed-case>orean</title>
      <author><first>Wooyoung</first><last>Go</last></author>
      <author><first>Hyoungshick</first><last>Kim</last><affiliation>Sungkyunkwan University</affiliation></author>
      <author><first>Alice</first><last>Oh</last><affiliation>Google and Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Yongdae</first><last>Kim</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <pages>22728-22750</pages>
      <abstract>Large language models (LLMs) generate human-like text, raising concerns about their misuse in creating deceptive content. Detecting LLM-generated comments (LGC) in online news is essential for preserving online discourse integrity and preventing opinion manipulation. However, effective detection faces two key challenges; the brevity and informality of news comments limit traditional methods, and the absence of a publicly available LGC dataset hinders model training, especially for languages other than English. To address these challenges, we propose a twofold approach. First, we develop an LGC generation framework to construct a high-quality dataset with diverse and complex examples. Second, we introduce XDAC (<tex-math>\textbf{X}</tex-math>AI-Driven <tex-math>\textbf{D}</tex-math>etection and <tex-math>\textbf{A}</tex-math>ttribution of LLM-Generated <tex-math>\textbf{C}</tex-math>omments), a framework utilizing explainable AI, designed for the detection and attribution of short-form LGC in Korean news articles. XDAC leverages XAI to uncover distinguishing linguistic patterns at both token and character levels. We present the first large-scale benchmark dataset, comprising 1.3M human-written comments from Korean news platforms and 1M LLM-generated comments from 14 distinct models. XDAC outperforms existing methods, achieving a 98.5% F1 score in LGC detection with a relative improvement of 68.1%, and an 84.3% F1 score in attribution. To validate real-world applicability, we analyze 5.24M news comments from Naver, South Korea’s leading online news platform, identifying 27,029 potential LLM-generated comments.</abstract>
      <url hash="3c2f7370">2025.acl-long.1108</url>
      <bibkey>go-etal-2025-xdac</bibkey>
    </paper>
    <paper id="1109">
      <title><fixed-case>CENTAUR</fixed-case>: Bridging the Impossible Trinity of Privacy, Efficiency, and Performance in Privacy-Preserving Transformer Inference</title>
      <author><first>Jinglong</first><last>Luo</last></author>
      <author><first>Guanzhong</first><last>Chen</last></author>
      <author><first>Yehong</first><last>Zhang</last><affiliation>Pengcheng Laboratory</affiliation></author>
      <author><first>Shiyu</first><last>Liu</last></author>
      <author><first>Hui</first><last>Wang</last></author>
      <author><first>Yue</first><last>Yu</last></author>
      <author><first>Xun</first><last>Zhou</last><affiliation>Harbin Institute of Technology (Shenzhen)</affiliation></author>
      <author><first>Yuan</first><last>Qi</last><affiliation>Fudan University</affiliation></author>
      <author><first>Zenglin</first><last>Xu</last><affiliation>Shanghai Academy of AI for Science and Fudan University</affiliation></author>
      <pages>22751-22770</pages>
      <abstract>With the growing deployment of pre-trained models like Transformers on cloud platforms, privacy concerns about model parameters and inference data are intensifying. Existing Privacy-Preserving Transformer Inference (PPTI) frameworks face the “impossible trinity” of balancing privacy, efficiency, and performance: Secure Multi-Party Computation (SMPC)-based approaches ensure strong privacy but suffer from high computational overhead and performance losses; Conversely, permutation-based methods achieve near-plaintext efficiency and accuracy but compromise privacy by exposing sensitive model parameters and intermediate results. Bridging this gap with a single approach presents substantial challenges, motivating the introduction of CENTAUR, a groundbreaking PPTI framework that seamlessly integrates random permutations and SMPC to address the “impossible trinity”. By designing efficient PPTI algorithms tailored to the structural properties of Transformer models, CENTAUR achieves an unprecedented balance among privacy, efficiency, and performance. Our experiments demonstrate CENTAUR’s ability to resist diverse data reconstruction attacks, achieve plaintext-level inference accuracy, and boost inference speed by 5.0~30.4 times, unlocking new possibilities for secure and efficient AI deployment.</abstract>
      <url hash="c8c9c510">2025.acl-long.1109</url>
      <bibkey>luo-etal-2025-centaur</bibkey>
    </paper>
    <paper id="1110">
      <title>Silencing Empowerment, Allowing Bigotry: Auditing the Moderation of Hate Speech on Twitch</title>
      <author><first>Prarabdh</first><last>Shukla</last><affiliation>Indian Institute of Science, Indian institute of science, Bangalore</affiliation></author>
      <author><first>Wei Yin</first><last>Chong</last></author>
      <author><first>Yash</first><last>Patel</last></author>
      <author><first>Brennan</first><last>Schaffner</last></author>
      <author><first>Danish</first><last>Pruthi</last><affiliation>Indian Institute of Science, Bangalore</affiliation></author>
      <author><first>Arjun</first><last>Bhagoji</last><affiliation>Indian Institute of Technology, Bombay</affiliation></author>
      <pages>22771-22797</pages>
      <abstract>To meet the demands of content moderation, online platforms have resorted to automated systems. Newer forms of real-time engagement (<tex-math>\textit{e.g.}</tex-math>, users commenting on live streams) on platforms like Twitch exert additional pressures on the latency expected of such moderation systems. Despite their prevalence, relatively little is known about the effectiveness of these systems. In this paper, we conduct an audit of Twitch’s automated moderation tool (<tex-math>\texttt{AutoMod}</tex-math>) to investigate its effectiveness in flagging hateful content. For our audit, we create streaming accounts to act as siloed test beds, and interface with the live chat using Twitch’s APIs to send over 107,000 comments collated from 4 datasets. We measure <tex-math>\texttt{AutoMod}</tex-math>‘s accuracy in flagging blatantly hateful content containing misogyny, racism, ableism and homophobia. Our experiments reveal that a large fraction of hateful messages, up to 94% on some datasets, <tex-math>\text{\textit{bypass moderation}}</tex-math>. Contextual addition of slurs to these messages results in 100% removal, revealing <tex-math>\texttt{AutoMod}</tex-math>‘s reliance on slurs as a hate signal. We also find that contrary to Twitch’s community guidelines, <tex-math>\texttt{AutoMod}</tex-math> blocks up to 89.5% of benign examples that use sensitive words in pedagogical or empowering contexts. Overall, our audit points to large gaps in <tex-math>\texttt{AutoMod}</tex-math>‘s capabilities and underscores the importance for such systems to understand context effectively.</abstract>
      <url hash="a6c58917">2025.acl-long.1110</url>
      <bibkey>shukla-etal-2025-silencing</bibkey>
    </paper>
    <paper id="1111">
      <title><fixed-case>E</fixed-case>di<fixed-case>T</fixed-case>ext: Controllable Coarse-to-Fine Text Editing with Diffusion Language Models</title>
      <author><first>Che Hyun</first><last>Lee</last></author>
      <author><first>Heeseung</first><last>Kim</last></author>
      <author><first>Jiheum</first><last>Yeom</last></author>
      <author><first>Sungroh</first><last>Yoon</last><affiliation>Seoul National University</affiliation></author>
      <pages>22798-22815</pages>
      <abstract>We propose EdiText, a controllable text editing method that modifies the reference text to desired attributes at various scales. We integrate an SDEdit-based editing technique that allows for broad adjustments in the degree of text editing. Additionally, we introduce a novel fine-level editing method based on self-conditioning, which allows subtle control of reference text. While being capable of editing on its own, this fine-grained method, integrated with the SDEdit approach, enables EdiText to make precise adjustments within the desired range. EdiText demonstrates its controllability to robustly adjust reference text at a broad range of levels across various tasks, including toxicity control and sentiment control.</abstract>
      <url hash="74077a23">2025.acl-long.1111</url>
      <bibkey>lee-etal-2025-editext</bibkey>
    </paper>
    <paper id="1112">
      <title><fixed-case>TUMLU</fixed-case>: A Unified and Native Language Understanding Benchmark for <fixed-case>T</fixed-case>urkic Languages</title>
      <author><first>Jafar</first><last>Isbarov</last></author>
      <author><first>Arofat</first><last>Akhundjanova</last><affiliation>Universität des Saarlandes</affiliation></author>
      <author><first>Mammad</first><last>Hajili</last><affiliation>Microsoft</affiliation></author>
      <author><first>Kavsar</first><last>Huseynova</last></author>
      <author><first>Dmitry</first><last>Gaynullin</last></author>
      <author><first>Anar</first><last>Rzayev</last></author>
      <author><first>Osman</first><last>Tursun</last><affiliation>Queensland University of Technology</affiliation></author>
      <author><first>Aizirek</first><last>Turdubaeva</last></author>
      <author><first>Ilshat</first><last>Saetov</last><affiliation>L’École des hautes études en sciences sociales</affiliation></author>
      <author><first>Rinat</first><last>Kharisov</last></author>
      <author><first>Saule</first><last>Belginova</last><affiliation>Turan University</affiliation></author>
      <author><first>Ariana</first><last>Kenbayeva</last></author>
      <author><first>Amina</first><last>Alisheva</last></author>
      <author><first>Abdullatif</first><last>Köksal</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Samir</first><last>Rustamov</last><affiliation>ADA University</affiliation></author>
      <author><first>Duygu</first><last>Ataman</last><affiliation>New York University</affiliation></author>
      <pages>22816-22838</pages>
      <abstract>Being able to thoroughly assess massive multi-task language understanding (MMLU) capabilities is essential for advancing the applicability of multilingual language models. However, preparing such benchmarks in high quality native language is often costly and therefore limits the representativeness of evaluation datasets. While recent efforts focused on building more inclusive MMLU benchmarks, these are conventionally built using machine translation from high-resource languages, which may introduce errors and fail to account for the linguistic and cultural intricacies of the target languages. In this paper, we address the lack of native language MMLU benchmark especially in the under-represented Turkic language family with distinct morphosyntactic and cultural characteristics. We propose two benchmarks for Turkic language MMLU: TUMLU is a comprehensive, multilingual, and natively developed language understanding benchmark specifically designed for Turkic languages. It consists of middle- and high-school level questions spanning 11 academic subjects in Azerbaijani, Crimean Tatar, Karakalpak, Kazakh, Kyrgyz, Tatar, Turkish, Uyghur, and Uzbek. We also present TUMLU-mini, a more concise, balanced, and manually verified subset of the dataset. Using this dataset, we systematically evaluate a diverse range of open and proprietary multilingual large language models (LLMs), including Claude, Gemini, GPT, and LLaMA, offering an in-depth analysis of their performance across different languages, subjects, and alphabets. To promote further research and development in multilingual language understanding, we release TUMLU-mini and all corresponding evaluation scripts.</abstract>
      <url hash="1c96f459">2025.acl-long.1112</url>
      <bibkey>isbarov-etal-2025-tumlu</bibkey>
    </paper>
    <paper id="1113">
      <title>Look Both Ways and No Sink: Converting <fixed-case>LLM</fixed-case>s into Text Encoders without Training</title>
      <author><first>Ziyong</first><last>Lin</last></author>
      <author><first>Haoyi</first><last>Wu</last><affiliation>ShanghaiTech University</affiliation></author>
      <author><first>Shu</first><last>Wang</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Kewei</first><last>Tu</last><affiliation>ShanghaiTech University</affiliation></author>
      <author><first>Zilong</first><last>Zheng</last><affiliation>Beijing Institute for General Artificial Intelligence</affiliation></author>
      <author><first>Zixia</first><last>Jia</last></author>
      <pages>22839-22853</pages>
      <abstract>Recent advancements have demonstrated the advantage of converting pretrained large language models into powerful text encoders by enabling bidirectional attention in transformer layers. However, existing methods often require extensive training on large-scale datasets, posing challenges in low-resource, domain-specific scenarios. In this work, we show that a pretrained large language model can be converted into a strong text encoder without additional training. We first conduct a comprehensive empirical study to investigate different conversion strategies and identify the impact of the attention sink phenomenon on the performance of converted encoder models. Based on our findings, we propose a novel approach that enables bidirectional attention and suppresses the attention sink phenomenon, resulting in superior performance. Extensive experiments on multiple domains demonstrate the effectiveness of our approach. Our work provides new insights into the training-free conversion of text encoders in low-resource scenarios and contributes to the advancement of domain-specific text representation generation. Our code is available at https://github.com/bigai-nlco/Look-Both-Ways-and-No-Sink.</abstract>
      <url hash="58cb9432">2025.acl-long.1113</url>
      <bibkey>lin-etal-2025-look</bibkey>
    </paper>
    <paper id="1114">
      <title>A Statistical and Multi-Perspective Revisiting of the Membership Inference Attack in Large Language Models</title>
      <author><first>Bowen</first><last>Chen</last></author>
      <author><first>Namgi</first><last>Han</last><affiliation>Tokyo University</affiliation></author>
      <author><first>Yusuke</first><last>Miyao</last><affiliation>The University of Tokyo</affiliation></author>
      <pages>22854-22874</pages>
      <abstract>The lack of data transparency in Large Language Models (LLMs) has highlighted the importance of Membership Inference Attack (MIA), which differentiates trained (member) and untrained (non-member) data. Though it shows success in previous studies, recent research reported a near-random performance in different settings, highlighting a significant performance inconsistency. We assume that a single setting doesn’t represent the distribution of the vast corpora, causing members and non-members with different distributions to be sampled and causing inconsistency. In this study, instead of a single setting, we statistically revisit MIA methods from various settings with thousands of experiments for each MIA method, along with study in text feature, embedding, threshold decision, and decoding dynamics of members and non-members. We found that (1) MIA performance improves with model size and varies with domains, while most methods do not statistically outperform baselines, (2) Though MIA performance is generally low, a notable amount of differentiable member and non-member outliers exists and vary across MIA methods, (3) Deciding a threshold to separate members and non-members is an overlooked challenge, (4) Text dissimilarity and long text benefit MIA performance, (5) Differentiable or not is reflected in the LLM embedding, (6) Member and non-members show different decoding dynamics.</abstract>
      <url hash="7839d287">2025.acl-long.1114</url>
      <bibkey>chen-etal-2025-statistical</bibkey>
    </paper>
    <paper id="1115">
      <title>Around the World in 24 Hours: Probing <fixed-case>LLM</fixed-case> Knowledge of Time and Place</title>
      <author><first>Carolin</first><last>Holtermann</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Paul</first><last>Röttger</last><affiliation>Bocconi University</affiliation></author>
      <author><first>Anne</first><last>Lauscher</last><affiliation>Universität Hamburg</affiliation></author>
      <pages>22875-22897</pages>
      <abstract>Reasoning over time and space is essential for understanding our world. However, the abilities of language models in this area are largely unexplored as previous work has tested their abilities for logical reasoning in terms of time and space in isolation or only in simple or artificial environments. In this paper, we present the first evaluation of the ability of language models to jointly reason over time and space. To enable our analysis, we create GeoTemp, a dataset of 320k prompts covering 289 cities in 217 countries and 37 time zones. Using GeoTemp, we evaluate eight open chat models of three different model families for different combinations of temporal and geographic knowledge. We find that most models perform well on reasoning tasks involving only temporal knowledge and that overall performance improves with scale. However, performance remains constrained in tasks that require connecting temporal and geographical information. We do not find clear correlations of performance with specific geographic regions. Instead, we find a significant performance increase for location names with low model perplexity, suggesting their repeated occurrence during model training. We further demonstrate that their performance is heavily influenced by prompt formulation - a direct injection of geographical knowledge leads to performance gains, whereas, surprisingly, techniques like chain-of-thought prompting decrease performance on simpler tasks.</abstract>
      <url hash="cfc1cdb3">2025.acl-long.1115</url>
      <bibkey>holtermann-etal-2025-around</bibkey>
    </paper>
    <paper id="1116">
      <title>Mining the uncertainty patterns of humans and models in the annotation of moral foundations and human values</title>
      <author><first>Neele</first><last>Falk</last></author>
      <author><first>Gabriella</first><last>Lapesa</last><affiliation>GESIS – Leibniz Institute for the Social Sciences and Heinrich-Heine University Düsseldorf</affiliation></author>
      <pages>22898-22921</pages>
      <abstract>The NLP community has converged on considering disagreement in annotation (or human label variation, HLV) as a constitutive feature of subjective tasks. This paper makes a further step by investigating the relationship between HLV and model uncertainty, and the impact of linguistic features of the items on both. We focus on the identification of moral foundations (e.g., care, fairness, loyalty) and human values (e.g., be polite, be honest) in text. We select three standard datasets and proceed into two steps. First, we focus on HLV and analyze the linguistic features (complexity, polarity, pragmatic phenomena, lexical choices) that correlate with HLV. Next, we proceed to uncertainty and its relationship to HLV. We experiment with RoBERTa and Flan-T5 in a number of training setups and evaluation metrics that test the calibration of uncertainty to HLV and its relationship to performance beyond majority vote; next, we analyze the impact of linguistic features on uncertainty. We find that RoBERTa with soft loss is better calibrated to HLV, and we find alignment between calibrated models and humans in the features (textual complexity and polarity) triggering variation.</abstract>
      <url hash="5dd1f408">2025.acl-long.1116</url>
      <bibkey>falk-lapesa-2025-mining</bibkey>
    </paper>
    <paper id="1117">
      <title>“What do you call a dog that is incontrovertibly true? Dogma”: Testing <fixed-case>LLM</fixed-case> Generalization through Humor</title>
      <author><first>Alessio</first><last>Cocchieri</last></author>
      <author><first>Luca</first><last>Ragazzi</last></author>
      <author><first>Paolo</first><last>Italiani</last></author>
      <author><first>Giuseppe</first><last>Tagliavini</last><affiliation>University of Bologna</affiliation></author>
      <author><first>Gianluca</first><last>Moro</last><affiliation>DISI - University of Bologna</affiliation></author>
      <pages>22922-22937</pages>
      <abstract>Humor, requiring creativity and contextual understanding, is a hallmark of human intelligence, showcasing adaptability across linguistic scenarios. While recent advances in large language models (LLMs) demonstrate strong reasoning on various benchmarks, it remains unclear whether they truly adapt to new tasks like humans (i.e., generalize) or merely replicate memorized content. To explore this, we introduce Phunny, a new humor-based question-answering benchmark designed to assess LLMs’ reasoning through carefully crafted puns. Our dataset is manually curated to ensure novelty and minimize data contamination, providing a robust evaluation of LLMs’ linguistic comprehension. Experiments on pun comprehension, resolution, and generation reveal that most LLMs struggle with generalization, even on simple tasks, consistently underperforming the human baseline. Additionally, our detailed error analysis provides valuable insights to guide future research.</abstract>
      <url hash="6c4d4461">2025.acl-long.1117</url>
      <bibkey>cocchieri-etal-2025-call</bibkey>
    </paper>
    <paper id="1118">
      <title>Towards Harmonized Uncertainty Estimation for Large Language Models</title>
      <author><first>Rui</first><last>Li</last></author>
      <author><first>Jing</first><last>Long</last><affiliation>Peking University</affiliation></author>
      <author><first>Muge</first><last>Qi</last></author>
      <author><first>Heming</first><last>Xia</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Lei</first><last>Sha</last><affiliation>Beihang University</affiliation></author>
      <author><first>Peiyi</first><last>Wang</last></author>
      <author><first>Zhifang</first><last>Sui</last><affiliation>Peking University</affiliation></author>
      <pages>22938-22953</pages>
      <abstract>To facilitate robust and trustworthy deployment of large language models (LLMs), it is essential to quantify the reliability of their generations through uncertainty estimation. While recent efforts have made significant advancements by leveraging the internal logic and linguistic features of LLMs to estimate uncertainty scores, our empirical analysis highlights the pitfalls of these methods to strike a harmonized estimation between indication, balance, and calibration, which hinders their broader capability for accurate uncertainty estimation. To address this challenge, we propose CUE (Corrector for Uncertainty Estimation): A straightforward yet effective method that employs a lightweight model trained on data aligned with the target LLM’s performance to adjust uncertainty scores. Comprehensive experiments across diverse models and tasks demonstrate its effectiveness, which achieves consistent improvements of up to 60% over existing methods.</abstract>
      <url hash="5c28c9dd">2025.acl-long.1118</url>
      <bibkey>li-etal-2025-towards</bibkey>
    </paper>
    <paper id="1119">
      <title><fixed-case>VITAL</fixed-case>: A New Dataset for Benchmarking Pluralistic Alignment in Healthcare</title>
      <author><first>Anudeex</first><last>Shetty</last><affiliation>University of Melbourne</affiliation></author>
      <author><first>Amin</first><last>Beheshti</last><affiliation>Macquarie University</affiliation></author>
      <author><first>Mark</first><last>Dras</last><affiliation>Macquarie University</affiliation></author>
      <author><first>Usman</first><last>Naseem</last><affiliation>Macquarie University</affiliation></author>
      <pages>22954-22974</pages>
      <abstract>Alignment techniques have become central to ensuring that Large Language Models (LLMs) generate outputs consistent with human values. However, existing alignment paradigms often model an averaged or monolithic preference, failing to account for the diversity of perspectives across cultures, demographics, and communities. This limitation is particularly critical in health-related scenarios, where plurality is essential due to the influence of culture, religion, personal values, and conflicting opinions. Despite progress in pluralistic alignment, no prior work has focused on health, likely due to the unavailability of publicly available datasets. To address this gap, we introduce VITAL, a new benchmark dataset comprising 13.1K value-laden situations and 5.4K multiple-choice questions focused on health, designed to assess and benchmark pluralistic alignment methodologies. Through extensive evaluation of eight LLMs of varying sizes, we demonstrate that existing pluralistic alignment techniques fall short in effectively accommodating diverse healthcare beliefs, underscoring the need for tailored AI alignment in specific domains. This work highlights the limitations of current approaches and lays the groundwork for developing health-specific alignment solutions.</abstract>
      <url hash="72e19677">2025.acl-long.1119</url>
      <bibkey>shetty-etal-2025-vital</bibkey>
    </paper>
    <paper id="1120">
      <title>Are We in the <fixed-case>AI</fixed-case>-Generated Text World Already? Quantifying and Monitoring <fixed-case>AIGT</fixed-case> on Social Media</title>
      <author><first>Zhen</first><last>Sun</last></author>
      <author><first>Zongmin</first><last>Zhang</last></author>
      <author><first>Xinyue</first><last>Shen</last></author>
      <author><first>Ziyi</first><last>Zhang</last></author>
      <author><first>Yule</first><last>Liu</last></author>
      <author><first>Michael</first><last>Backes</last><affiliation>CISPA Helmholtz Center for Information Security</affiliation></author>
      <author><first>Yang</first><last>Zhang</last><affiliation>CISPA Helmholtz Center for Information Security</affiliation></author>
      <author><first>Xinlei</first><last>He</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <pages>22975-23005</pages>
      <abstract>Social media platforms are experiencing a growing presence of AI-Generated Texts (AIGTs). However, the misuse of AIGTs could have profound implications for public opinion, such as spreading misinformation and manipulating narratives. Despite its importance, it remains unclear how prevalent AIGTs are on social media. To address this gap, this paper aims to quantify and monitor the AIGTs on online social media platforms. We first collect a dataset (SM-D) with around <tex-math>2.4M</tex-math> posts from 3 major social media platforms: Medium, Quora, and Reddit. Then, we construct a diverse dataset (AIGTBench) to train and evaluate AIGT detectors. AIGTBench combines popular open-source datasets and our AIGT datasets generated from social media texts by 12 LLMs, serving as a benchmark for evaluating mainstream detectors. With this setup, we identify the best-performing detector (OSM-Det). We then apply OSM-Det to SM-D to track AIGTs across social media platforms from January 2022 to October 2024, using the AI Attribution Rate (AAR) as the metric. Specifically, Medium and Quora exhibit marked increases in AAR, rising from 1.77% to 37.03% and 2.06% to 38.95%, respectively. In contrast, Reddit shows slower growth, with AAR increasing from 1.31% to 2.45% over the same period. Our further analysis indicates that AIGTs on social media differ from human-written texts across several dimensions, including linguistic patterns, topic distributions, engagement levels, and the follower distribution of authors. We envision our analysis and findings on AIGTs in social media can shed light on future research in this domain.</abstract>
      <url hash="8d073e83">2025.acl-long.1120</url>
      <bibkey>sun-etal-2025-ai</bibkey>
    </paper>
    <paper id="1121">
      <title>From <fixed-case>E</fixed-case>nglish to Second Language Mastery: Enhancing <fixed-case>LLM</fixed-case>s with Cross-Lingual Continued Instruction Tuning</title>
      <author><first>Linjuan</first><last>Wu</last></author>
      <author><first>Hao-Ran</first><last>Wei</last></author>
      <author><first>Baosong</first><last>Yang</last></author>
      <author><first>Weiming</first><last>Lu</last><affiliation>Zhejiang University</affiliation></author>
      <pages>23006-23023</pages>
      <abstract>Supervised Fine-Tuning (SFT) with translated instruction data effectively adapts Large Language Models (LLMs) from English to non-English languages. We introduce Cross-Lingual Continued Instruction Tuning (X-CIT), which fully leverages translation-based parallel instruction data to enhance cross-lingual adaptability. X-CIT emulates the human process of second language acquisition and is guided by Chomsky’s Principles and Parameters Theory. It first fine-tunes the LLM on English instruction data to establish foundational capabilities (i.e. Principles), then continues with target language translation and customized chat-instruction data to adjust “parameters” specific to the target language. This chat-instruction data captures alignment information in translated parallel data, guiding the model to initially think and respond in its native language before transitioning to the target language. To further mimic human learning progression, we incorporate Self-Paced Learning (SPL) during continued training, allowing the model to advance from simple to complex tasks. Implemented on Llama-2-7B across five languages, X-CIT was evaluated against three objective benchmarks and an LLM-as-a-judge benchmark, improving the strongest baseline by an average of 1.97% and 8.2% in these two benchmarks, respectively.</abstract>
      <url hash="496ff27d">2025.acl-long.1121</url>
      <bibkey>wu-etal-2025-english</bibkey>
    </paper>
    <paper id="1122">
      <title><fixed-case>WET</fixed-case>: Overcoming Paraphrasing Vulnerabilities in Embeddings-as-a-Service with Linear Transformation Watermarks</title>
      <author><first>Anudeex</first><last>Shetty</last><affiliation>University of Melbourne</affiliation></author>
      <author><first>Qiongkai</first><last>Xu</last><affiliation>Macquarie University</affiliation></author>
      <author><first>Jey Han</first><last>Lau</last><affiliation>The University of Melbourne</affiliation></author>
      <pages>23024-23043</pages>
      <abstract>Embeddings-as-a-Service (EaaS) is a service offered by large language model (LLM) developers to supply embeddings generated by LLMs. Previous research suggests that EaaS is prone to imitation attacks—attacks that clone the underlying EaaS model by training another model on the queried embeddings. As a result, EaaS watermarks are introduced to protect the intellectual property of EaaS providers. In this paper, we first show that existing EaaS watermarks can be removed by paraphrasing when attackers clone the model. Subsequently, we propose a novel watermarking technique that involves linearly transforming the embeddings, and show that it is empirically and theoretically robust against paraphrasing.</abstract>
      <url hash="ec225d2a">2025.acl-long.1122</url>
      <bibkey>shetty-etal-2025-wet</bibkey>
    </paper>
    <paper id="1123">
      <title><fixed-case>H</fixed-case>o<fixed-case>PE</fixed-case>: A Novel Positional Encoding Without Long-Term Decay for Enhanced Context Awareness and Extrapolation</title>
      <author><first>Yuhan</first><last>Chen</last><affiliation>Xiaomi Corporation</affiliation></author>
      <author><first>Ang</first><last>Lv</last></author>
      <author><first>Jian</first><last>Luan</last><affiliation>Xiaomi Corporation</affiliation></author>
      <author><first>Bin</first><last>Wang</last><affiliation>AI Lab, Xiaomi Inc.</affiliation></author>
      <author><first>Wei</first><last>Liu</last></author>
      <pages>23044-23056</pages>
      <abstract>Many positional encodings (PEs) are designed to exhibit long-term decay, based on an entrenched and long-standing inductive opinion: tokens farther away from the current position carry less relevant information. We argue that long-term decay is outdated in the era of LLMs, as LLMs are now applied to tasks demanding precise retrieval of in-context information from arbitrary positions. Firstly, we present empirical analyses on various PEs, demonstrating that models inherently learn attention with only a local-decay pattern while forming a U-shape pattern globally, contradicting the principle of long-term decay. Furthermore, we conduct a detailed analysis of rotary position encoding (RoPE, a prevalent relative positional encoding in LLMs), and found that the U-shape attention is caused by some learned components, which are also the key factor limiting RoPE’s expressiveness and extrapolation. Inspired by these insights, we propose High-frequency rotary Position Encoding (HoPE). HoPE replaces the specific components in RoPE with position-independent ones, retaining only high-frequency signals, which also breaks the principle of long-term decay in theory. HoPE achieves two major advantages: (1) Without constraints imposed by long-term decay, contradictory factors that limit attention optimization are removed. Thus, the model’s context awareness is enhanced. (2) HoPE exhibits greater robustness to the out-of-distribution behavior in attention patterns during extrapolation. The effectiveness of HoPE is validated through extensive experiments and with a large language model of up to 3 billion parameters.</abstract>
      <url hash="b8e83392">2025.acl-long.1123</url>
      <bibkey>chen-etal-2025-hope</bibkey>
    </paper>
    <paper id="1124">
      <title>One <fixed-case>Q</fixed-case>uant<fixed-case>LLM</fixed-case> for <fixed-case>ALL</fixed-case>: Fine-tuning Quantized <fixed-case>LLM</fixed-case>s Once for Efficient Deployments</title>
      <author><first>Ke</first><last>Yi</last></author>
      <author><first>Yuhui</first><last>Xu</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Heng</first><last>Chang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Yuan</first><last>Meng</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Tong</first><last>Zhang</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Jia</first><last>Li</last><affiliation>Hong Kong University of Science and Technology (Guangzhou)</affiliation></author>
      <pages>23057-23066</pages>
      <abstract>Large Language Models (LLMs) have advanced rapidly but face significant memory demands. While quantization has shown promise for LLMs, current methods typically require lengthy training to alleviate the performance degradation from quantization loss. However, deploying LLMs across diverse scenarios with different resource constraints, e.g., servers and personal computers, requires repeated training per application, which amplifies the lengthy training problem. Given that, it is advantageous to train a once-for-all (OFA) supernet capable of yielding diverse optimal subnets for downstream applications through one-shot training. Nonetheless, the scale of current language models impedes efficiency and amplifies interference from weight sharing between subnets. We make an initial attempt to extend the once-for-all framework to large language models. Specifically, we decouple shared weights to eliminate the interference and incorporate Low-Rank adapters for training efficiency. Furthermore, we observe the imbalance allocation of training resources from the traditional uniform sampling. A non-parametric scheduler is introduced to adjust the sampling rate for each quantization configuration, achieving a more balanced allocation among subnets with varying demands. We validate the approach on LLaMA2 families and Mistral on downstream evaluation, demonstrating high performance while significantly reducing deployment time faced with multiple scenarios.</abstract>
      <url hash="e4b71ceb">2025.acl-long.1124</url>
      <bibkey>yi-etal-2025-one</bibkey>
    </paper>
    <paper id="1125">
      <title>Beyond Logits: Aligning Feature Dynamics for Effective Knowledge Distillation</title>
      <author><first>Guoqiang</first><last>Gong</last><affiliation>JD.com</affiliation></author>
      <author><first>Jiaxing</first><last>Wang</last><affiliation>JD.com</affiliation></author>
      <author><first>Jin</first><last>Xu</last><affiliation>Microsoft</affiliation></author>
      <author><first>Deping</first><last>Xiang</last></author>
      <author><first>Zicheng</first><last>Zhang</last><affiliation>JD.com</affiliation></author>
      <author><first>Leqi</first><last>Shen</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Yifeng</first><last>Zhang</last><affiliation>JD.com</affiliation></author>
      <author><first>JunhuaShu</first><last>JunhuaShu</last></author>
      <author><first>ZhaolongXing</first><last>ZhaolongXing</last><affiliation>JD.com</affiliation></author>
      <author><first>Zhen</first><last>Chen</last><affiliation>JD.com</affiliation></author>
      <author><first>Pengzhang</first><last>Liu</last><affiliation>jd.com</affiliation></author>
      <author><first>Ke</first><last>Zhang</last></author>
      <pages>23067-23077</pages>
      <abstract>Knowledge distillation (KD) compresses large language models (LLMs), known as teacher models, into lightweight versions called student models, enabling efficient inference and downstream applications. However, prevailing approaches accomplish this by predominantly focusing on matching the final output distributions of student/teacher models. Drawing on the perspective that transformers can be viewed as discretizing ordinary differential equation (ODEs) on integer time steps (corresponding to layer indices), where intermediate features evolve across layers, we argue that effective KD requires aligning the entire feature dynamics between teacher and student models, which we call feature dynamics distillation (FDD). This alignment involves matching both the feature trajectory and its first-order derivative, rather than just the final states. Our approach extends the original KD objective with two additional loss terms: layer-wise feature KD, which matches discretized feature trajectory, and layer feature delta KD, which matches first-order changes in features across adjacent layers. Extensive experiments on various tasks validate the effectiveness of our distillation method.</abstract>
      <url hash="a0bb771c">2025.acl-long.1125</url>
      <bibkey>gong-etal-2025-beyond</bibkey>
    </paper>
    <paper id="1126">
      <title>Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention</title>
      <author><first>Jingyang</first><last>Yuan</last></author>
      <author><first>Huazuo</first><last>Gao</last><affiliation>DeepSeek AI</affiliation></author>
      <author><first>Damai</first><last>Dai</last></author>
      <author><first>Junyu</first><last>Luo</last><affiliation>Peking University</affiliation></author>
      <author><first>Liang</first><last>Zhao</last></author>
      <author><first>Zhengyan</first><last>Zhang</last><affiliation>Deepseed AI</affiliation></author>
      <author><first>Zhenda</first><last>Xie</last><affiliation>DeepSeek AI</affiliation></author>
      <author><first>Yuxing</first><last>Wei</last></author>
      <author><first>Lean</first><last>Wang</last><affiliation>DeepSeek and Peking University</affiliation></author>
      <author><first>Zhiping</first><last>Xiao</last><affiliation>University of Washington</affiliation></author>
      <author><first>Yuqing</first><last>Wang</last></author>
      <author><first>Chong</first><last>Ruan</last><affiliation>DeepSeek</affiliation></author>
      <author><first>Ming</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Wenfeng</first><last>Liang</last></author>
      <author><first>Wangding</first><last>Zeng</last></author>
      <pages>23078-23097</pages>
      <abstract>Long-context modeling is crucial for next-generation language models, yet the high computational cost of standard attention mechanisms poses significant computational challenges. Sparse attention offers a promising direction for improving efficiency while maintaining model capabilities. We present NSA, a Natively trained Sparse Attention mechanism that integrates algorithmic innovations with hardware-aligned optimizations to achieve efficient long-context modeling. NSA employs a dynamic hierarchical sparse strategy, combining coarse-grained token compression with fine-grained token selection to preserve both global context awareness and local precision. Our approach advances sparse attention design with two key innovations: (1) We achieve substantial speedups through arithmetic intensity-balanced algorithm design, with implementation optimizations for modern hardware. (2) We enable end-to-end training, reducing pretraining computation without sacrificing model performance. As shown in Figure 1, experiments show the model pretrained with NSA maintains or exceeds Full Attention models across general benchmarks, long-context tasks, and instruction-based reasoning. Meanwhile, NSA achieves substantial speedups over Full Attention on 64k-length sequences across decoding, forward propagation, and backward propagation, validating its efficiency throughout the model lifecycle.</abstract>
      <url hash="c67c3b3a">2025.acl-long.1126</url>
      <bibkey>yuan-etal-2025-native</bibkey>
    </paper>
    <paper id="1127">
      <title><fixed-case>DRAE</fixed-case>: Dynamic Retrieval-Augmented Expert Networks for Lifelong Learning and Task Adaptation in Robotics</title>
      <author><first>Yayu</first><last>Long</last></author>
      <author><first>Kewei</first><last>Chen</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Long</first><last>Jin</last><affiliation>Chinese Academy of Sciences</affiliation></author>
      <author><first>Mingsheng</first><last>Shang</last><affiliation>CIGIT</affiliation></author>
      <pages>23098-23141</pages>
      <abstract>We introduce <b>Dynamic Retrieval-Augmented Expert Networks (DRAE)</b>, a groundbreaking architecture that addresses the challenges of lifelong learning, catastrophic forgetting, and task adaptation by combining the dynamic routing capabilities of Mixture-of-Experts (MoE); leveraging the knowledge-enhancement power of Retrieval-Augmented Generation (RAG); incorporating a novel hierarchical reinforcement learning (RL) framework; and coordinating through ReflexNet-SchemaPlanner-HyperOptima (RSHO).DRAE dynamically routes expert models via a sparse MoE gating mechanism, enabling efficient resource allocation while leveraging external knowledge through parametric retrieval (P-RAG) to augment the learning process. We propose a new RL framework with ReflexNet for low-level task execution, SchemaPlanner for symbolic reasoning, and HyperOptima for long-term context modeling, ensuring continuous adaptation and memory retention. Experimental results show that DRAE significantly outperforms baseline approaches in long-term task retention and knowledge reuse, achieving an average task success rate of 82.5% across a set of dynamic robotic manipulation tasks, compared to 74.2% for traditional MoE models. Furthermore, DRAE maintains an extremely low forgetting rate, outperforming state-of-the-art methods in catastrophic forgetting mitigation. These results demonstrate the effectiveness of our approach in enabling flexible, scalable, and efficient lifelong learning for robotics.</abstract>
      <url hash="92e7e717">2025.acl-long.1127</url>
      <bibkey>long-etal-2025-drae</bibkey>
    </paper>
    <paper id="1128">
      <title><fixed-case>MT</fixed-case>-<fixed-case>RAIG</fixed-case>: Novel Benchmark and Evaluation Framework for Retrieval-Augmented Insight Generation over Multiple Tables</title>
      <author><first>Kwangwook</first><last>Seo</last></author>
      <author><first>Donguk</first><last>Kwon</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Dongha</first><last>Lee</last><affiliation>Yonsei University</affiliation></author>
      <pages>23142-23172</pages>
      <abstract>Recent advancements in table-based reasoning have expanded beyond factoid-level QA to address insight-level tasks, where systems should synthesize implicit knowledge in the table to provide explainable analyses. Although effective, existing studies remain confined to scenarios where a single gold table is given alongside the user query, failing to address cases where users seek comprehensive insights from multiple unknown tables. To bridge these gaps, we propose MT-RAIG Bench, design to evaluate systems on Retrieval-Augmented Insight Generation over Mulitple-Tables. Additionally, to tackle the suboptimality of existing automatic evaluation methods in the table domain, we further introduce a fine-grained evaluation framework MT-RAIG Eval, which achieves better alignment with human quality judgments on the generated insights. We conduct extensive experiments and reveal that even frontier LLMs still struggle with complex multi-table reasoning, establishing our MT-RAIG Bench as a challenging testbed for future research.</abstract>
      <url hash="6bc4cdbd">2025.acl-long.1128</url>
      <bibkey>seo-etal-2025-mt</bibkey>
    </paper>
    <paper id="1129">
      <title>Enhancing Chain-of-Thought Reasoning with Critical Representation Fine-tuning</title>
      <author><first>Chenxi</first><last>Huang</last></author>
      <author><first>Shaotian</first><last>Yan</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Liang</first><last>Xie</last></author>
      <author><first>Binbin</first><last>Lin</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Sinan</first><last>Fan</last></author>
      <author><first>Yue</first><last>Xin</last></author>
      <author><first>Deng</first><last>Cai</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Chen</first><last>Shen</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Jieping</first><last>Ye</last><affiliation>Alibaba Group</affiliation></author>
      <pages>23173-23195</pages>
      <abstract>Representation Fine-tuning (ReFT), a recently proposed Parameter-Efficient Fine-Tuning (PEFT) method, has attracted widespread attention for significantly improving parameter efficiency by editing representation space alone. In this work, we investigate applying ReFT to complex reasoning tasks. However, directly using the native ReFT method, which modifies fixed representations at the beginning and end of each layer, yields suboptimal performance, as these fixed-position representations have uncertain impact on the outputs. We observe that, in complex reasoning tasks, there often exist certain critical representations. These representations either integrate significant information from preceding layers or regulate subsequent layer representations. Through layer-by-layer propagation, they exert a substantial influence on the final output. Naturally, fine-tuning these critical representations has the potential to greatly enhance reasoning performance. Building upon these insights, we propose **C**ritical **R**epresentation **F**ine-**T**uning (CRFT), a novel method that identifies and optimizes these critical representations through information flow analysis. CRFT operates within a supervised learning framework, dynamically optimizing critical representations in a low-rank linear subspace while freezing the base model. The effectiveness and efficiency of our method are validated across eight benchmarks for arithmetic and commonsense reasoning, using LLaMA and Mistral model families. Notably, our method improves the accuracy of LLaMA-2-7B and ReFT by 18.2 and 3.8, respectively, on GSM8K, while using only 0.016 of the model parameters, significantly less than other PEFT methods. Furthermore, our method also adapts effectively to few-shot settings, boosting one-shot accuracy by 16.4. Our work highlights the untapped potential of representation-level optimization for CoT reasoning, offering a lightweight yet powerful alternative to traditional PEFT methods.</abstract>
      <url hash="9aba3b6e">2025.acl-long.1129</url>
      <bibkey>huang-etal-2025-enhancing</bibkey>
    </paper>
    <paper id="1130">
      <title>Does the Emotional Understanding of <fixed-case>LVLM</fixed-case>s Vary Under High-Stress Environments and Across Different Demographic Attributes?</title>
      <author><first>Jaewook</first><last>Lee</last><affiliation>Electronics and Telecommunications Research Institute</affiliation></author>
      <author><first>Yeajin</first><last>Jang</last></author>
      <author><first>Oh-Woog</first><last>Kwon</last></author>
      <author><first>Harksoo</first><last>Kim</last><affiliation>Konkuk University</affiliation></author>
      <pages>23196-23210</pages>
      <abstract>According to psychological and neuroscientific research, a high-stress environment can restrict attentional resources and intensify negative affect, thereby impairing the ability to understand emotions. Furthermore, demographic attributes such as race, gender, and age group have been repeatedly reported to cause significant differences in emotional expression and recognition. This study is the first to systematically verify whether these psychological findings observed in humans also apply to the latest Large Vision Language Models (LVLMs). We constructed low-stress versus high-stress environments and generated an image dataset (a total of 540 images) that combines race, gender, and age group. Based on this, we applied the Pretend prompt technique to induce LVLMs to interpret others’ emotions from the standpoint of the assigned environment and persona. An analysis of the models’ emotional understanding ability, using EQ-Bench-based metrics, revealed that (1) under high-stress environments, the accuracy of emotion understanding significantly declined in most LVLMs, and (2) performance disparities were confirmed across race, gender, and age group. These findings suggest that the effects of high-stress and demographic attributes identified in human research may also be reflected in LVLMs.</abstract>
      <url hash="b7e16c30">2025.acl-long.1130</url>
      <bibkey>lee-etal-2025-emotional</bibkey>
    </paper>
    <paper id="1131">
      <title><fixed-case>S</fixed-case>2<fixed-case>WTM</fixed-case>: Spherical Sliced-<fixed-case>W</fixed-case>asserstein Autoencoder for Topic Modeling</title>
      <author><first>Suman</first><last>Adhya</last></author>
      <author><first>Debarshi Kumar</first><last>Sanyal</last><affiliation>Indian Association for the Cultivation of Science</affiliation></author>
      <pages>23211-23225</pages>
      <abstract>Modeling latent representations in a hyperspherical space has proven effective for capturing directional similarities in high-dimensional text data, benefiting topic modeling. Variational autoencoder-based neural topic models (VAE-NTMs) commonly adopt the von Mises-Fisher prior to encode hyperspherical structure. However, VAE-NTMs often suffer from posterior collapse, where the KL divergence term in the objective function highly diminishes, leading to ineffective latent representations. To mitigate this issue while modeling hyperspherical structure in the latent space, we propose the Spherical Sliced Wasserstein Autoencoder for Topic Modeling (S2WTM). S2WTM employs a prior distribution supported on the unit hypersphere and leverages the Spherical Sliced-Wasserstein distance to align the aggregated posterior distribution with the prior. Experimental results demonstrate that S2WTM outperforms state-of-the-art topic models, generating more coherent and diverse topics while improving performance on downstream tasks.</abstract>
      <url hash="18b7bdd0">2025.acl-long.1131</url>
      <bibkey>adhya-sanyal-2025-s2wtm</bibkey>
    </paper>
    <paper id="1132">
      <title>Learning to Look at the Other Side: A Semantic Probing Study of Word Embeddings in <fixed-case>LLM</fixed-case>s with Enabled Bidirectional Attention</title>
      <author><first>Zhaoxin</first><last>Feng</last></author>
      <author><first>Jianfei</first><last>Ma</last></author>
      <author><first>Emmanuele</first><last>Chersoni</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Xiaojing</first><last>Zhao</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Xiaoyi</first><last>Bao</last></author>
      <pages>23226-23245</pages>
      <abstract>Autoregressive Large Language Models (LLMs) demonstrate exceptional performance in language understanding and generation. However, their application in text embedding tasks has been relatively slow, along with the analysis of their semantic representation in probing tasks, due to the constraints of the unidirectional attention mechanism. This paper aims to explore whether such constraints can be overcome by enabling bidirectional attention in LLMs. We tested different variants of the Llama architecture through additional training steps, progressively enabling bidirectional attention and unsupervised/supervised contrastive learning. Our results show that bidirectional attention improves the LLMs’ ability to represent subsequent context but weakens their utilization of preceding context, while contrastive learning training can help to maintain both abilities.</abstract>
      <url hash="84e153c2">2025.acl-long.1132</url>
      <bibkey>feng-etal-2025-learning</bibkey>
    </paper>
    <paper id="1133">
      <title>Tracing and Dissecting How <fixed-case>LLM</fixed-case>s Recall Factual Knowledge for Real World Questions</title>
      <author><first>Yiqun</first><last>Wang</last></author>
      <author><first>Chaoqun</first><last>Wan</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Sile</first><last>Hu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yonggang</first><last>Zhang</last></author>
      <author><first>Xiang</first><last>Tian</last></author>
      <author><first>Yaowu</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Xu</first><last>Shen</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Jieping</first><last>Ye</last><affiliation>Alibaba Group</affiliation></author>
      <pages>23246-23271</pages>
      <abstract>Recent advancements in large language models (LLMs) have shown promising ability to perform commonsense reasoning, bringing machines closer to human-like understanding. However, deciphering the internal reasoning processes of LLMs remains challenging due to the complex interdependencies among generated tokens, especially in practical question-answering. In this study, we introduce a two-dimensional analysis framework—comprising token back-tracing and individual token decoding—to uncover how LLMs conduct factual knowledge recall. Through explanatory analysis of three typical reasoning datasets, we identify a consistent three-phase pattern: Subject Augmentation and Broadcasting, Object Retrieval and Reranking, and Conclusion Fusion and Generation. Our findings reveal that LLMs do not lack relevant knowledge but struggle to select the most accurate information based on context during the retrieval and rerank phase. Leveraging these findings, we apply representation engineering and selective fine-tuning to target specific modules responsible for retrieval and rerank errors. Experimental results show large improvements in response accuracy for both in-domain and out-of-domain settings, validating the rationality of the interpreting result.</abstract>
      <url hash="0f66624c">2025.acl-long.1133</url>
      <bibkey>wang-etal-2025-tracing</bibkey>
    </paper>
    <paper id="1134">
      <title>Employing Discourse Coherence Enhancement to Improve Cross-Document Event and Entity Coreference Resolution</title>
      <author><first>Xinyu</first><last>Chen</last></author>
      <author><first>Peifeng</first><last>Li</last><affiliation>Soochow University, China</affiliation></author>
      <author><first>Qiaoming</first><last>Zhu</last><affiliation>Soochow University</affiliation></author>
      <pages>23272-23286</pages>
      <abstract>Cross-Document Coreference Resolution (CDCR) aims to identify and group together mentions of a specific event or entity that occur across multiple documents. In contrast to the within-document tasks, in which event and entity mentions are linked by rich and coherent contexts, cross-document mentions lack such critical contexts, which presents a significant challenge in establishing connections among them. To address this issue, we introduce a novel task Cross-Document Discourse Coherence Enhancement (CD-DCE) to enhance the discourse coherence between two cross-document event or entity mentions. Specifically, CD-DCE first selects coherent texts and then adds them between two cross-document mentions to form a new coherent document. Subsequently, the coherent text is employed to represent the event or entity mentions and to resolve any coreferent mentions. Experimental results on the three popular datasets demonstrate that our proposed method outperforms several state-of-the-art baselines.</abstract>
      <url hash="708634d1">2025.acl-long.1134</url>
      <bibkey>chen-etal-2025-employing</bibkey>
    </paper>
    <paper id="1135">
      <title>Data Whisperer: Efficient Data Selection for Task-Specific <fixed-case>LLM</fixed-case> Fine-Tuning via Few-Shot In-Context Learning</title>
      <author><first>Shaobo</first><last>Wang</last></author>
      <author><first>Xiangqi</first><last>Jin</last></author>
      <author><first>Ziming</first><last>Wang</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Jize</first><last>Wang</last></author>
      <author><first>Jiajun</first><last>Zhang</last></author>
      <author><first>Kaixin</first><last>Li</last></author>
      <author><first>Zichen</first><last>Wen</last></author>
      <author><first>Zhong</first><last>Li</last><affiliation>Microsoft Research Asia</affiliation></author>
      <author><first>Conghui</first><last>He</last><affiliation>Shanghai AI Lab</affiliation></author>
      <author><first>Xuming</first><last>Hu</last><affiliation>The Hong Kong University of Science and Technology (Guangzhou) and Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Linfeng</first><last>Zhang</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <pages>23287-23305</pages>
      <abstract>Fine-tuning large language models (LLMs) on task-specific data is essential for their effective deployment. As dataset sizes grow, efficiently selecting optimal subsets for training becomes crucial to balancing performance and computational costs. Traditional data selection methods often require fine-tuning a scoring model on the target dataset, which is time-consuming and resource-intensive, or rely on heuristics that fail to fully leverage the model’s predictive capabilities. To address these challenges, we propose Data Whisperer, an efficient, training-free, attention-based method that leverages few-shot in-context learning with the model to be fine-tuned. Comprehensive evaluations were conducted on both raw and synthetic datasets across diverse tasks and models. Notably, Data Whisperer achieves superior performance compared to the full GSM8K dataset on the Llama-3-8B-Instruct model, using just 10% of the data, and outperforms existing methods with a 3.1-point improvement and a 7.4× speedup.</abstract>
      <url hash="df5116fd">2025.acl-long.1135</url>
      <bibkey>wang-etal-2025-data-whisperer</bibkey>
    </paper>
    <paper id="1136">
      <title>Synthesizing Post-Training Data for <fixed-case>LLM</fixed-case>s through Multi-Agent Simulation</title>
      <author><first>Shuo</first><last>Tang</last></author>
      <author><first>Xianghe</first><last>Pang</last></author>
      <author><first>Zexi</first><last>Liu</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Bohan</first><last>Tang</last></author>
      <author><first>Rui</first><last>Ye</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Tian</first><last>Jin</last></author>
      <author><first>Xiaowen</first><last>Dong</last><affiliation>University of Oxford and Massachusetts Institute of Technology</affiliation></author>
      <author><first>Yanfeng</first><last>Wang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Siheng</first><last>Chen</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>23306-23335</pages>
      <abstract>Post-training is essential for enabling large language models (LLMs) to follow human instructions. However, its effectiveness depends on high-quality instruction data, which is challenging to obtain in the real world due to privacy concerns, data scarcity, and high annotation costs. To fill this gap, inspired by the recent success of using LLMs to simulate human society, we propose MATRIX, a multi-agent simulator that automatically generates diverse text-based scenarios, capturing a wide range of real-world human needs in a realistic and scalable manner. Leveraging these outputs, we introduce a novel scenario-driven instruction generator MATRIX-Gen for controllable and highly realistic data synthesis. Extensive experiments demonstrate that our framework effectively generates both general and domain-specific data. On AlpacaEval 2 and Arena-Hard benchmarks, Llama-3-8B-Base, post-trained on datasets synthesized by MATRIX-Gen with just 20K instruction-response pairs, outperforms Meta’s Llama-3-8B-Instruct model, which was trained on over 10M pairs.</abstract>
      <url hash="d217dbf0">2025.acl-long.1136</url>
      <bibkey>tang-etal-2025-synthesizing</bibkey>
    </paper>
    <paper id="1137">
      <title><fixed-case>S</fixed-case>oft<fixed-case>C</fixed-case>o<fixed-case>T</fixed-case>: Soft Chain-of-Thought for Efficient Reasoning with <fixed-case>LLM</fixed-case>s</title>
      <author><first>Yige</first><last>Xu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Xu</first><last>Guo</last></author>
      <author><first>Zhiwei</first><last>Zeng</last></author>
      <author><first>Chunyan</first><last>Miao</last><affiliation>School of Computer Science and Engineering, Nanyang Technological University</affiliation></author>
      <pages>23336-23351</pages>
      <abstract>Chain-of-Thought (CoT) reasoning enables Large Language Models (LLMs) to solve complex reasoning tasks by generating intermediate reasoning steps. However, most existing approaches focus on hard token decoding, which constrains reasoning within the discrete vocabulary space and may not always be optimal. While recent efforts explore continuous-space reasoning, they often require full-model fine-tuning and suffer from catastrophic forgetting, limiting their applicability to state-of-the-art LLMs that already perform well in zero-shot settings with a proper instruction. To address this challenge, we propose a novel approach for continuous-space reasoning that does not require modifying the LLM. Specifically, we employ a lightweight fixed assistant model to speculatively generate instance-specific soft thought tokens as the initial chain of thoughts, which are then mapped into the LLM’s representation space via a trainable projection module. Experimental results on five reasoning benchmarks demonstrate that our method enhances LLM reasoning performance through supervised, parameter-efficient fine-tuning. Source code is available at https://github.com/xuyige/SoftCoT.</abstract>
      <url hash="3699f757">2025.acl-long.1137</url>
      <bibkey>xu-etal-2025-softcot</bibkey>
    </paper>
    <paper id="1138">
      <title><fixed-case>FCMR</fixed-case>: Robust Evaluation of Financial Cross-Modal Multi-Hop Reasoning</title>
      <author><first>Seunghee</first><last>Kim</last></author>
      <author><first>Changhyeon</first><last>Kim</last></author>
      <author><first>Taeuk</first><last>Kim</last><affiliation>Hanyang University</affiliation></author>
      <pages>23352-23380</pages>
      <abstract>Real-world decision-making often requires integrating and reasoning over information from multiple modalities. While recent multimodal large language models (MLLMs) have shown promise in such tasks, their ability to perform multi-hop reasoning across diverse sources remains insufficiently evaluated. Existing benchmarks, such as MMQA, face challenges due to (1) data contamination and (2) a lack of complex queries that necessitate operations across more than two modalities, hindering accurate performance assessment. To address this, we present Financial Cross-Modal Multi-Hop Reasoning (FCMR), a benchmark created to analyze the reasoning capabilities of MLLMs by urging them to combine information from textual reports, tables, and charts within the financial domain. FCMR is categorized into three difficulty levels—Easy, Medium, and Hard—facilitating a step-by-step evaluation. In particular, problems at the Hard level require precise cross-modal three-hop reasoning and are designed to prevent the disregard of any modality. Experiments on this new benchmark reveal that even state-of-the-art MLLMs struggle, with the best-performing model (Claude 3.5 Sonnet) achieving only 30.4% accuracy on the most challenging tier. We also conduct analysis to provide insights into the inner workings of the models, including the discovery of a critical bottleneck in the information retrieval phase.</abstract>
      <url hash="8d25e62e">2025.acl-long.1138</url>
      <bibkey>kim-etal-2025-fcmr</bibkey>
    </paper>
    <paper id="1139">
      <title>Beyond Prompt Engineering: Robust Behavior Control in <fixed-case>LLM</fixed-case>s via Steering Target Atoms</title>
      <author><first>Mengru</first><last>Wang</last></author>
      <author><first>Ziwen</first><last>Xu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Shengyu</first><last>Mao</last></author>
      <author><first>Shumin</first><last>Deng</last></author>
      <author><first>Zhaopeng</first><last>Tu</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Huajun</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Ningyu</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <pages>23381-23399</pages>
      <abstract>Precise control over language model generation is vital for ensuring both safety and reliability. Although prompt engineering and steering are commonly used to intervene in model behaviors, the vast number of parameters in models often results in highly intertwined internal representations. This interdependency can limit control precision and sometimes lead to unintended side effects. Recent research has explored the use of sparse autoencoders (SAE) to disentangle knowledge in high-dimensional spaces for steering.However, these applications have been limited to toy tasks owing to the nontrivial issue of locating “atomic knowledge components”. In this paper, we propose Steering Target Atoms (STA), a novel method that isolates and manipulates disentangled knowledge components to enhance safety. Comprehensive experiments demonstrate the effectiveness of our approach. Further analysis reveals that steering exhibits superior robustness and flexibility, particularly in adversarial scenarios. We also apply the steering strategy to the large reasoning model, confirming its effectiveness in precise reasoning control.</abstract>
      <url hash="bc425dc5">2025.acl-long.1139</url>
      <bibkey>wang-etal-2025-beyond-prompt</bibkey>
    </paper>
    <paper id="1140">
      <title><fixed-case>M</fixed-case>obi<fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case>: Accelerating <fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case>-based <fixed-case>LLM</fixed-case> Inference on Mobile Devices via Context-aware <fixed-case>KV</fixed-case> Cache Optimization</title>
      <author><first>Borui</first><last>Li</last><affiliation>Southeast University</affiliation></author>
      <author><first>Yitao</first><last>Wang</last></author>
      <author><first>Haoran</first><last>Ma</last></author>
      <author><first>Ligeng</first><last>Chen</last></author>
      <author><first>Jun</first><last>Xiao</last></author>
      <author><first>Shuai</first><last>Wang</last><affiliation>Southeast University</affiliation></author>
      <pages>23400-23410</pages>
      <abstract>Deploying large language models (LLMs) with low-rank adaptation (LoRA) on mobile devices is promising due to their capability to complete diverse domain-specific tasks while ensuring privacy and accessibility. In this paper, we introduce MobiLoRA to accelerate LoRA-based LLM inference on mobile devices. MobiLoRA focuses on optimizing the key-value (KV) caches due to the limited computing and memory resources of mobile devices. The key insight of MobiLoRA lies in the utilization of two contexts for on-device LoRA serving: semantic-level contexts, such as prompts with shared prefixes, and system-level contexts, such as the application status (e.g., foreground or killed) of LLM requests. Specifically, for semantic-level contexts, MobiLoRA proposes similarity-aware delta encoding, which leverages token-wise similarity in KV caches across LoRA adapters for efficient storage and reuse. Furthermore, MobiLoRA advocates context-aware KV cache management to optimize cache retention and eviction considering the system-level contexts. We fully implement MobiLoRA and compare it with state-of-the-art LLM serving frameworks using real-world mobile device traces. Results show that MobiLoRA accelerates LoRA-based LLM inference by 57.6% on mobile devices.</abstract>
      <url hash="d02ad49e">2025.acl-long.1140</url>
      <bibkey>li-etal-2025-mobilora</bibkey>
    </paper>
    <paper id="1141">
      <title>Language Models Resist Alignment: Evidence From Data Compression</title>
      <author><first>Jiaming</first><last>Ji</last></author>
      <author><first>Kaile</first><last>Wang</last><affiliation>Peking University</affiliation></author>
      <author><first>Tianyi Alex</first><last>Qiu</last></author>
      <author><first>Boyuan</first><last>Chen</last></author>
      <author><first>Jiayi</first><last>Zhou</last><affiliation>Peking University</affiliation></author>
      <author><first>Changye</first><last>Li</last></author>
      <author><first>Hantao</first><last>Lou</last></author>
      <author><first>Josef</first><last>Dai</last><affiliation>Peking University</affiliation></author>
      <author><first>Yunhuai</first><last>Liu</last><affiliation>Peking University</affiliation></author>
      <author><first>Yaodong</first><last>Yang</last><affiliation>Peking University</affiliation></author>
      <pages>23411-23432</pages>
      <abstract>Large language models (LLMs) may exhibit unintended or undesirable behaviors. Recent works have concentrated on aligning LLMs to mitigate harmful outputs. Despite these efforts, some anomalies indicate that even a well-conducted alignment process can be easily circumvented, whether intentionally or accidentally. Does alignment fine-tuning yield have robust effects on models, or are its impacts merely superficial? In this work, we make the first exploration of this phenomenon from both theoretical and empirical perspectives. Empirically, we demonstrate the <i>elasticity</i> of post-alignment models, i.e., the tendency to revert to the behavior distribution formed during the pre-training phase upon further fine-tuning. Leveraging compression theory, we formally deduce that fine-tuning disproportionately undermines alignment relative to pre-training, potentially by orders of magnitude. We validate the presence of elasticity through experiments on models of varying types and scales. Specifically, we find that model performance declines rapidly before reverting to the pre-training distribution, after which the rate of decline drops significantly. Furthermore, we further reveal that elasticity positively correlates with the increased model size and the expansion of pre-training data. Our findings underscore the need to address the inherent elasticity of LLMs to mitigate their resistance to alignment.</abstract>
      <url hash="36b08c48">2025.acl-long.1141</url>
      <bibkey>ji-etal-2025-language-models</bibkey>
    </paper>
    <paper id="1142">
      <title>Beyond the Answer: Advancing Multi-Hop <fixed-case>QA</fixed-case> with Fine-Grained Graph Reasoning and Evaluation</title>
      <author><first>Qichuan</first><last>Liu</last></author>
      <author><first>Chentao</first><last>Zhang</last></author>
      <author><first>Chenfeng</first><last>Zheng</last></author>
      <author><first>Guosheng</first><last>Hu</last><affiliation>University of Bristol</affiliation></author>
      <author><first>Xiaodong</first><last>Li</last></author>
      <author><first>Zhihong</first><last>Zhang</last></author>
      <pages>23433-23456</pages>
      <abstract>Recent advancements in large language models (LLMs) have significantly improved the performance of multi-hop question answering (MHQA) systems. Despite the success of MHQA systems, the evaluation of MHQA is not deeply investigated. Existing evaluations mainly focus on comparing the final answers of the reasoning method and given ground-truths. We argue that the reasoning process should also be evaluated because wrong reasoning process can also lead to the correct final answers. Motivated by this, we propose a “Planner-Executor-Reasoner” (PER) architecture, which forms the core of the Plan-anchored Data Preprocessing (PER-DP) and the Plan-guided Multi-Hop QA (PER-QA).The former provides the ground-truth of intermediate reasoning steps and final answers, and the latter offers them of a reasoning method. Moreover, we design a fine-grained evaluation metric called Plan-aligned Stepwise Evaluation (PSE), which evaluates the intermediate reasoning steps from two aspects: planning and solving. Extensive experiments on ten types of questions demonstrate competitive reasoning performance, improved explainability of the MHQA system, and uncover issues such as “fortuitous reasoning continuance” and “latent reasoning suspension” in RAG-based MHQA systems. Besides, we also demonstrate the potential of our approach in data contamination scenarios.</abstract>
      <url hash="979b889e">2025.acl-long.1142</url>
      <bibkey>liu-etal-2025-beyond</bibkey>
    </paper>
    <paper id="1143">
      <title>Mamba Knockout for Unraveling Factual Information Flow</title>
      <author><first>Nir</first><last>Endy</last><affiliation>Tel Aviv University</affiliation></author>
      <author><first>Idan Daniel</first><last>Grosbard</last><affiliation>Tel Aviv University</affiliation></author>
      <author><first>Yuval</first><last>Ran-Milo</last></author>
      <author><first>Yonatan</first><last>Slutzky</last><affiliation>Tel Aviv University</affiliation></author>
      <author><first>Itay</first><last>Tshuva</last></author>
      <author><first>Raja</first><last>Giryes</last><affiliation>Tel Aviv University</affiliation></author>
      <pages>23457-23477</pages>
      <abstract>This paper investigates the flow of factual information in Mamba State-Space Model (SSM)-based language models. We rely on theoretical and empirical connections to Transformer-based architectures and their attention mechanisms. Exploiting this relationship, we adapt attentional interpretability techniques originally developed for Transformers—specifically, the Attention Knockout methodology—to both Mamba-1 and Mamba-2. Using them we trace how information is transmitted and localized across tokens and layers, revealing patterns of subject-token information emergence and layer-wise dynamics. Notably, some phenomena vary between mamba models and Transformer based models, while others appear universally across all models inspected—hinting that these may be inherent to LLMs in general. By further leveraging Mamba’s structured factorization, we disentangle how distinct “features” either enable token-to-token information exchange or enrich individual tokens, thus offering a unified lens to understand Mamba internal operations.</abstract>
      <url hash="34e708bc">2025.acl-long.1143</url>
      <bibkey>endy-etal-2025-mamba</bibkey>
    </paper>
    <paper id="1144">
      <title>Small Changes, Big Impact: How Manipulating a Few Neurons Can Drastically Alter <fixed-case>LLM</fixed-case> Aggression</title>
      <author><first>Jaewook</first><last>Lee</last><affiliation>Electronics and Telecommunications Research Institute</affiliation></author>
      <author><first>Junseo</first><last>Jang</last></author>
      <author><first>Oh-Woog</first><last>Kwon</last></author>
      <author><first>Harksoo</first><last>Kim</last><affiliation>Konkuk University</affiliation></author>
      <pages>23478-23505</pages>
      <abstract>Recent remarkable advances in Large Language Models (LLMs) have led to innovations in various domains such as education, healthcare, and finance, while also raising serious concerns that they can be easily misused for malicious purposes. Most previous research has focused primarily on observing how jailbreak attack techniques bypass safety mechanisms like Reinforcement Learning through Human Feedback (RLHF). However, whether there are neurons within LLMs that directly govern aggression has not been sufficiently investigated. To fill this gap, this study identifies specific neurons (“aggression neurons”) closely related to the expression of aggression and systematically analyzes how manipulating them affects the model’s overall aggression. Specifically, using a large-scale synthetic text corpus (aggressive and non-aggressive), we measure the activation frequency of each neuron, then apply masking and activation techniques to quantitatively evaluate changes in aggression by layer and by manipulation ratio. Experimental results show that, in all models, manipulating only a small number of neurons can increase aggression by up to 33%, and the effect is even more extreme when aggression neurons are concentrated in certain layers. Moreover, even models of the same scale exhibit nonlinear changes in aggression patterns, suggesting that simple external safety measures alone may not be sufficient for complete defense.</abstract>
      <url hash="18fcf748">2025.acl-long.1144</url>
      <bibkey>lee-etal-2025-small</bibkey>
    </paper>
    <paper id="1145">
      <title>Marco-o1 v2: Towards Widening The Distillation Bottleneck for Reasoning Models</title>
      <author><first>Huifeng</first><last>Yin</last></author>
      <author><first>Yu</first><last>Zhao</last></author>
      <author><first>Minghao</first><last>Wu</last></author>
      <author><first>Xuanfan</first><last>Ni</last></author>
      <author><first>Bo</first><last>Zeng</last></author>
      <author><first>Huaiyu.wh</first><last>Huaiyu.wh</last></author>
      <author><first>Tianqi</first><last>Shi</last></author>
      <author><first>Liangying</first><last>Shao</last></author>
      <author><first>Chenyang</first><last>Lyu</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Longyue</first><last>Wang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Weihua</first><last>Luo</last><affiliation>Alibaba International Digital Commerce Group</affiliation></author>
      <author><first>Kaifu</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <pages>23506-23516</pages>
      <abstract>Large Reasoning Models (LRMs) such as OpenAI o1 and DeepSeek-R1 have shown remarkable reasoning capabilities by scaling test-time compute and generating long Chain-of-Thought (CoT). Distillation post-training on LRMs-generated data is a straightforward yet effective method to enhance the reasoning abilities of smaller models, but faces a critical bottleneck: we found that distilled long CoT data poses learning difficulty for small models and leads to the inheritance of biases (i.e., formalistic long-time thinking) when using Supervised Fine-tuning (SFT) and Reinforcement Learning (RL) methods. To alleviate this bottleneck, we propose constructing data from scratch using Monte Carlo Tree Search (MCTS). We then exploit a set of CoT-aware approaches, including Thoughts Length Balance, Fine-grained DPO, and Joint Post-training Objective, to enhance SFT and RL on the MCTS data. We conducted evaluation on various benchmarks such as math (GSM8K, MATH, AIME). instruction-following (Multi-IF) and planning (Blocksworld), results demonstrate our CoT-aware approaches substantially improve the reasoning performance of distilled models compared to standard distilled models via reducing the hallucinations in long-time thinking.</abstract>
      <url hash="3edfe7db">2025.acl-long.1145</url>
      <bibkey>yin-etal-2025-marco</bibkey>
    </paper>
    <paper id="1146">
      <title>Curiosity-Driven Reinforcement Learning from Human Feedback</title>
      <author><first>Haoran</first><last>Sun</last><affiliation>Baidu</affiliation></author>
      <author><first>Yekun</first><last>Chai</last><affiliation>Baidu</affiliation></author>
      <author><first>Shuohuan</first><last>Wang</last></author>
      <author><first>Yu</first><last>Sun</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <author><first>Haifeng</first><last>Wang</last><affiliation>Baidu</affiliation></author>
      <pages>23517-23534</pages>
      <abstract>Reinforcement learning from human feedback (RLHF) has proven effective in aligning large language models (LLMs) with human preferences, but often at the cost of reduced output diversity. This trade-off between diversity and alignment quality remains a significant challenge. Drawing inspiration from curiosity-driven exploration in reinforcement learning, we introduce curiosity-driven RLHF (CD-RLHF), a framework that incorporates intrinsic rewards for novel states, alongside traditional sparse extrinsic rewards, to optimize both output diversity and alignment quality. We demonstrate the effectiveness of CD-RLHF through extensive experiments on a range of tasks, including text summarization and instruction following. Our approach achieves significant gains in diversity on multiple diversity-oriented metrics while maintaining alignment with human preferences comparable to standard RLHF. We will make our code publicly available.</abstract>
      <url hash="6dedaa07">2025.acl-long.1146</url>
      <bibkey>sun-etal-2025-curiosity</bibkey>
    </paper>
    <paper id="1147">
      <title><fixed-case>T</fixed-case>2<fixed-case>A</fixed-case>-Feedback: Improving Basic Capabilities of Text-to-Audio Generation via Fine-grained <fixed-case>AI</fixed-case> Feedback</title>
      <author><first>Zehan</first><last>Wang</last></author>
      <author><first>Ke</first><last>Lei</last></author>
      <author><first>Chen</first><last>Zhu</last></author>
      <author><first>Jiawei</first><last>Huang</last></author>
      <author><first>Sashuai</first><last>Zhou</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Luping</first><last>Liu</last><affiliation>The University of Hong Kong</affiliation></author>
      <author><first>Xize</first><last>Cheng</last></author>
      <author><first>Shengpeng</first><last>Ji</last></author>
      <author><first>Zhenhui</first><last>Ye</last></author>
      <author><first>Tao</first><last>Jin</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Zhou</first><last>Zhao</last><affiliation>Zhejiang University and Zhejiang University</affiliation></author>
      <pages>23535-23547</pages>
      <abstract>Text-to-audio (T2A) generation has achieved remarkable progress in generating a variety of audio outputs from language prompts. However, current state-of-the-art T2A models still struggle to satisfy human preferences for prompt-following and acoustic quality when generating complex multi-event audio. To improve the performance of the model in these high-level applications, we propose to enhance the basic capabilities of the model with AI feedback learning. First, we introduce fine-grained AI audio scoring pipelines to: 1) verify whether each event in the text prompt is present in the audio (Event Occurrence Score), 2) detect deviations in event sequences from the language description (Event Sequence Score), and 3) assess the overall acoustic and harmonic quality of the generated audio (Acoustic&amp;Harmonic Quality). We evaluate these three automatic scoring pipelines and find that they correlate significantly better with human preferences than other evaluation metrics. This highlights their value as both feedback signals and evaluation metrics. Utilizing our robust scoring pipelines, we construct a large audio preference dataset, T2A-FeedBack, which contains 41k prompts and 249k audios, each accompanied by detailed scores. Moreover, we introduce T2A-EpicBench, a benchmark that focuses on long captions, multi-events, and story-telling scenarios, aiming to evaluate the advanced capabilities of T2A models. Finally, we demonstrate how T2A-FeedBack can enhance current state-of-the-art audio model. With simple preference tuning, the audio generation model exhibits significant improvements in both simple (AudioCaps test set) and complex (T2A-EpicBench) scenarios.</abstract>
      <url hash="6052b09f">2025.acl-long.1147</url>
      <bibkey>wang-etal-2025-t2a</bibkey>
    </paper>
    <paper id="1148">
      <title><fixed-case>C</fixed-case>o<fixed-case>E</fixed-case>: A Clue of Emotion Framework for Emotion Recognition in Conversations</title>
      <author><first>Zhiyu</first><last>Shen</last></author>
      <author><first>Yunhe</first><last>Pang</last></author>
      <author><first>Yanghui</first><last>Rao</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Jianxing</first><last>Yu</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <pages>23548-23563</pages>
      <abstract>Emotion Recognition in Conversations (ERC) is crucial for machines to understand dynamic human emotions. While Large Language Models (LLMs) show promise, their performance is often limited by challenges in interpreting complex conversational streams. We introduce a Clue of Emotion (CoE) framework, which progressively integrates key conversational clues to enhance the ERC task. Building on CoE, we implement a multi-stage auxiliary learning strategy that incorporates role-playing, speaker identification, and emotion reasoning tasks, each targeting different aspects of conversational emotion understanding and enhancing the model’s ability to interpret emotional contexts. Our experiments on EmoryNLP, MELD, and IEMOCAP demonstrate that CoE consistently outperforms state-of-the-art methods, achieving a 2.92% improvement on EmoryNLP. These results underscore the effectiveness of clues and multi-stage auxiliary learning for ERC, offering valuable insights for future research.</abstract>
      <url hash="ddaeda48">2025.acl-long.1148</url>
      <bibkey>shen-etal-2025-coe</bibkey>
    </paper>
    <paper id="1149">
      <title><fixed-case>MPO</fixed-case>: Multilingual Safety Alignment via Reward Gap Optimization</title>
      <author><first>Weixiang</first><last>Zhao</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Yulin</first><last>Hu</last></author>
      <author><first>Yang</first><last>Deng</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Tongtong</first><last>Wu</last><affiliation>Monash University</affiliation></author>
      <author><first>Wenxuan</first><last>Zhang</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Jiahe</first><last>Guo</last></author>
      <author><first>An</first><last>Zhang</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Yanyan</first><last>Zhao</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Bing</first><last>Qin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Tat-Seng</first><last>Chua</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Ting</first><last>Liu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>23564-23587</pages>
      <abstract>Large language models (LLMs) have become increasingly central to AI applications worldwide, necessitating robust multilingual safety alignment to ensure secure deployment across diverse linguistic contexts. Existing preference learning methods for safety alignment, such as RLHF and DPO, are primarily monolingual and struggle with noisy multilingual data. To address these limitations, we introduce <b>Mul</b>tilingual reward ga<b>P</b> <b>O</b>ptimization (<b>MPO</b>), a novel approach that leverages the well-aligned safety capabilities of the dominant language (<i>e.g.</i>, English) to improve safety alignment across multiple languages. MPO directly minimizes the reward gap difference between the dominant language and target languages, effectively transferring safety capabilities while preserving the original strengths of the dominant language. Extensive experiments on three LLMs, LLaMA-3.1, Gemma-2 and Qwen2.5, validate MPO’s efficacy in multilingual safety alignment without degrading general multilingual utility.</abstract>
      <url hash="045407af">2025.acl-long.1149</url>
      <bibkey>zhao-etal-2025-mpo</bibkey>
    </paper>
    <paper id="1150">
      <title><fixed-case>Q</fixed-case>uali<fixed-case>S</fixed-case>peech: A Speech Quality Assessment Dataset with Natural Language Reasoning and Descriptions</title>
      <author><first>Siyin</first><last>Wang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Wenyi</first><last>Yu</last></author>
      <author><first>Xianzhao</first><last>Chen</last></author>
      <author><first>Xiaohai</first><last>Tian</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Jun</first><last>Zhang</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Lu</first><last>Lu</last></author>
      <author><first>Yu</first><last>Tsao</last><affiliation>Academia Sinica</affiliation></author>
      <author><first>Junichi</first><last>Yamagishi</last><affiliation>National Institute of Informatics</affiliation></author>
      <author><first>Yuxuan</first><last>Wang</last><affiliation>ByteDance</affiliation></author>
      <author><first>Chao</first><last>Zhang</last><affiliation>Shanghai Artificial Intelligence Laboratory, Tsinghua University and University College London</affiliation></author>
      <pages>23588-23609</pages>
      <abstract>This paper explores a novel perspective to speech quality assessment by leveraging natural language descriptions, offering richer, more nuanced insights than traditional numerical scoring methods. Natural language feedback provides instructive recommendations and detailed evaluations, yet existing datasets lack the comprehensive annotations needed for this approach. To bridge this gap, we introduce QualiSpeech, a comprehensive low-level speech quality assessment dataset encompassing 11 key aspects and detailed natural language comments that include reasoning and contextual insights. Additionally, we propose the QualiSpeech Benchmark to evaluate the low-level speech understanding capabilities of auditory large language models (LLMs). Experimental results demonstrate that finetuned auditory LLMs can reliably generate detailed descriptions of noise and distortion, effectively identifying their types and temporal characteristics. The results further highlight the potential for incorporating reasoning to enhance the accuracy and reliability of quality assessments. The dataset can be found at https://huggingface.co/datasets/tsinghua-ee/QualiSpeech.</abstract>
      <url hash="05673a44">2025.acl-long.1150</url>
      <bibkey>wang-etal-2025-qualispeech</bibkey>
    </paper>
    <paper id="1151">
      <title>On the Relation Between Fine-Tuning, Topological Properties, and Task Performance in Sense-Enhanced Embeddings</title>
      <author><first>Deniz Ekin</first><last>Yavas</last></author>
      <author><first>Timothée</first><last>Bernard</last><affiliation>Université Paris Cité</affiliation></author>
      <author><first>Benoit</first><last>Crabbé</last><affiliation>Université de Paris</affiliation></author>
      <author><first>Laura</first><last>Kallmeyer</last><affiliation>Heinrich Heine University Düsseldorf, Germany</affiliation></author>
      <pages>23610-23625</pages>
      <abstract>Topological properties of embeddings, such as isotropy and uniformity, are closely linked to their expressiveness, and improving these properties enhances the embeddings’ ability to capture nuanced semantic distinctions. However, fine-tuning can reduce the expressiveness of the embeddings of language models. This study investigates the relation between fine-tuning, topology of the embedding space, and task performance in the context of sense knowledge enhancement, focusing on identifying the topological properties that contribute to the success of sense-enhanced embeddings. We experiment with two fine-tuning methods: *Supervised Contrastive Learning (SCL)* and *Supervised Predictive Learning (SPL)*. Our results show that SPL, the most standard approach, exhibits varying effectiveness depending on the language model and is inconsistent in producing successful sense-enhanced embeddings. In contrast, SCL achieves this consistently. Furthermore, while the embeddings with only increased *sense-alignment* show reduced task performance, those that also exhibit high *isotropy* and balance *uniformity* with *sense-alignment* achieve the best results. Additionally, our findings indicate that supervised and unsupervised tasks benefit from these topological properties to varying degrees.</abstract>
      <url hash="e0c958ba">2025.acl-long.1151</url>
      <bibkey>yavas-etal-2025-relation</bibkey>
    </paper>
    <paper id="1152">
      <title>Finding Needles in Images: Can Multi-modal <fixed-case>LLM</fixed-case>s Locate Fine Details?</title>
      <author><first>Parth</first><last>Thakkar</last><affiliation>Fujitsu Research and Development Center Co. Ltm.</affiliation></author>
      <author><first>Ankush</first><last>Agarwal</last><affiliation>Fujitsu Research and Development Center Co. Ltm.</affiliation></author>
      <author><first>Prasad</first><last>Kasu</last><affiliation>Fujitsu Research and Development Center Co. Ltm.</affiliation></author>
      <author><first>Pulkit</first><last>Bansal</last></author>
      <author><first>Chaitanya</first><last>Devaguptapu</last><affiliation>Fujitsu Research</affiliation></author>
      <pages>23626-23648</pages>
      <abstract>While Multi-modal Large Language Models (MLLMs) have shown impressive capabilities in document understanding tasks, their ability to locate and reason about fine-grained details within complex documents remains understudied. Consider searching a restaurant menu for a specific nutritional detail or identifying a disclaimer in a lengthy newspaper article — tasks that demand careful attention to small but significant details within a broader narrative, akin to Finding Needles in Images (NiM). To address this gap, we introduce NiM-Benchmark, a carefully curated benchmark spanning diverse real-world documents including newspapers, menus, and lecture images, specifically designed to evaluate MLLMs’ capability in these intricate tasks. Building on this, we further propose Spot-IT, a simple yet effective approach that enhances MLLMs capability through intelligent patch selection and Gaussian attention, motivated from how humans zoom and focus when searching documents. Our extensive experiments reveal both the capabilities and limitations of current MLLMs in handling fine-grained document understanding tasks, while demonstrating the effectiveness of our approach. Spot-IT achieves significant improvements over baseline methods, particularly in scenarios requiring precise detail extraction from complex layouts.</abstract>
      <url hash="880e5b04">2025.acl-long.1152</url>
      <bibkey>thakkar-etal-2025-finding</bibkey>
    </paper>
    <paper id="1153">
      <title>Don’t Half-listen: Capturing Key-part Information in Continual Instruction Tuning</title>
      <author><first>Yongquan</first><last>He</last></author>
      <author><first>Wenyuan</first><last>Zhang</last></author>
      <author><first>Xuancheng</first><last>Huang</last><affiliation>Zhipu.AI</affiliation></author>
      <author><first>Peng</first><last>Zhang</last><affiliation>Nanjing University of Science and Technology</affiliation></author>
      <author><first>Lingxun</first><last>Meng</last></author>
      <author><first>Xiang</first><last>Zhou</last><affiliation>Meituan</affiliation></author>
      <author><first>Ke</first><last>Zeng</last></author>
      <author><first>Xunliang</first><last>Cai</last><affiliation>Meituan</affiliation></author>
      <pages>23649-23668</pages>
      <abstract>Instruction tuning for large language models (LLMs) can drive them to produce results consistent with human goals in specific downstream tasks. However, the process of continual instruction tuning (CIT) for LLMs may bring about the catastrophic forgetting (CF) problem, where previously learned abilities are degraded. Recent methods try to alleviate the CF problem by modifying models or replaying data, which may only remember the surface-level pattern of instructions and get confused on held-out tasks. In this paper, we propose a novel continual instruction tuning method based on Key-part Information Gain (KPIG). Our method computes the information gain on masked parts to dynamically replay data and refine the training objective, which enables LLMs to capture task-aware information relevant to the correct response and alleviate overfitting to general descriptions in instructions. In addition, we propose two metrics, P-score and V-score, to measure the generalization and instruction-following abilities of LLMs. Experiments demonstrate our method achieves superior performance on both seen and held-out tasks.</abstract>
      <url hash="f4af87a0">2025.acl-long.1153</url>
      <bibkey>he-etal-2025-dont</bibkey>
    </paper>
    <paper id="1154">
      <title>Generating Plausible Distractors for Multiple-Choice Questions via Student Choice Prediction</title>
      <author><first>Yooseop</first><last>Lee</last></author>
      <author><first>Suin</first><last>Kim</last><affiliation>Elice</affiliation></author>
      <author><first>Yohan</first><last>Jo</last><affiliation>Seoul National University</affiliation></author>
      <pages>23669-23692</pages>
      <abstract>In designing multiple-choice questions (MCQs) in education, creating plausible distractors is crucial for identifying students’ misconceptions and gaps in knowledge and accurately assessing their understanding. However, prior studies on distractor generation have not paid sufficient attention to enhancing the difficulty of distractors, resulting in reduced effectiveness of MCQs. This study presents a pipeline for training a model to generate distractors that are more likely to be selected by students. First, we train a pairwise ranker to reason about students’ misconceptions and assess the relative plausibility of two distractors. Using this model, we create a dataset of pairwise distractor ranks and then train a distractor generator via Direct Preference Optimization (DPO) to generate more plausible distractors. Experiments on computer science subjects (Python, DB, MLDL) demonstrate that our pairwise ranker effectively identifies students’ potential misunderstandings and achieves ranking accuracy comparable to human experts. Furthermore, our distractor generator outperforms several baselines in generating plausible distractors and produces questions with a higher item discrimination index (DI).</abstract>
      <url hash="366692a2">2025.acl-long.1154</url>
      <bibkey>lee-etal-2025-generating-plausible</bibkey>
    </paper>
    <paper id="1155">
      <title>Exploring Explanations Improves the Robustness of In-Context Learning</title>
      <author><first>Ukyo</first><last>Honda</last><affiliation>CyberAgent, Inc.</affiliation></author>
      <author><first>Tatsushi</first><last>Oka</last><affiliation>Keio University</affiliation></author>
      <pages>23693-23714</pages>
      <abstract>In-context learning (ICL) has emerged as a successful paradigm for leveraging large language models (LLMs).However, it often struggles to generalize beyond the distribution of the provided demonstrations.A recent advancement in enhancing robustness is ICL with explanations (X-ICL), which improves prediction reliability by guiding LLMs to understand and articulate the reasoning behind correct labels.Building on this approach, we introduce an advanced framework that extends X-ICL by systematically exploring explanations for all possible labels (X<tex-math>^2</tex-math>-ICL), thereby enabling more comprehensive and robust decision-making.Experimental results on multiple natural language understanding datasets validate the effectiveness of X<tex-math>^2</tex-math>-ICL, demonstrating significantly improved robustness to out-of-distribution data compared to the existing ICL approaches.</abstract>
      <url hash="b1195838">2025.acl-long.1155</url>
      <bibkey>honda-oka-2025-exploring</bibkey>
    </paper>
    <paper id="1156">
      <title>Prediction Hubs are Context-Informed Frequent Tokens in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Beatrix Miranda Ginn</first><last>Nielsen</last></author>
      <author><first>Iuri</first><last>Macocco</last><affiliation>Universitat Pompeu Fabra</affiliation></author>
      <author><first>Marco</first><last>Baroni</last><affiliation>Universitat Pompeu Fabra</affiliation></author>
      <pages>23715-23745</pages>
      <abstract>Hubness, the tendency for a few points to be among the nearest neighbours of a disproportionate number of other points, commonly arises when applying standard distance measures to high-dimensional data, often negatively impacting distance-based analysis. As autoregressive large language models (LLMs) operate on high-dimensional representations, we ask whether they are also affected by hubness. We first prove that the only large-scale representation comparison operation performed by LLMs, namely that between context and unembedding vectors to determine continuation probabilities, is not characterized by the concentration of distances phenomenon that typically causes the appearance of nuisance hubness. We then empirically show that this comparison still leads to a high degree of hubness, but the hubs in this case do not constitute a disturbance. They are rather the result of context-modulated frequent tokens often appearing in the pool of likely candidates for next token prediction. However, when other distances are used to compare LLM representations, we do not have the same theoretical guarantees, and, indeed, we see nuisance hubs appear. There are two main takeaways. First, hubness, while omnipresent in high-dimensional spaces, is not a negative property that needs to be mitigated when LLMs are being used for next token prediction. Second, when comparing representations from LLMs using Euclidean or cosine distance, there is a high risk of nuisance hubs and practitioners should use mitigation techniques if relevant.</abstract>
      <url hash="30ee9466">2025.acl-long.1156</url>
      <bibkey>nielsen-etal-2025-prediction</bibkey>
    </paper>
    <paper id="1157">
      <title>Capability Salience Vector: Fine-grained Alignment of Loss and Capabilities for Downstream Task Scaling Law</title>
      <author><first>Qiming</first><last>Ge</last></author>
      <author><first>Shuhao</first><last>Xing</last></author>
      <author><first>Songyang</first><last>Gao</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Yunhua</first><last>Zhou</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Yicheng</first><last>Zou</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Songyang</first><last>Zhang</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Zhi</first><last>Chen</last></author>
      <author><first>Hang</first><last>Yan</last></author>
      <author><first>Qi</first><last>Zhang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Qipeng</first><last>Guo</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Kai</first><last>Chen</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <pages>23746-23761</pages>
      <abstract>Scaling law builds the relationship between training computation and validation loss, enabling researchers to effectively predict the loss trending of models across different levels of computation. However, a gap still remains between validation loss and the model’s downstream capabilities, making it untrivial to apply scaling law to direct performance prediction for downstream tasks. The loss typically represents a cumulative penalty for predicted tokens, which are implicitly considered to have equal importance. Nevertheless, our studies have shown evidence that when considering different training data distributions, we cannot directly model the relationship between downstream capability and computation or token loss. To bridge the gap between validation loss and downstream task capabilities, in this work, we introduce Capability Salience Vector, which decomposes the overall loss and assigns different importance weights to tokens to assess a specific meta-capability, aligning the validation loss with downstream task performance in terms of the model’s capabilities. Experiments on various popular benchmarks demonstrate that our proposed Capability Salience Vector could significantly improve the predictability of language model performance on downstream tasks.</abstract>
      <url hash="8844489a">2025.acl-long.1157</url>
      <bibkey>ge-etal-2025-capability</bibkey>
    </paper>
    <paper id="1158">
      <title><fixed-case>CRUXEVAL</fixed-case>-<fixed-case>X</fixed-case>: A Benchmark for Multilingual Code Reasoning, Understanding and Execution</title>
      <author><first>Ruiyang</first><last>Xu</last></author>
      <author><first>Jialun</first><last>Cao</last></author>
      <author><first>Yaojie</first><last>Lu</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Ming</first><last>Wen</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Hongyu</first><last>Lin</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xianpei</first><last>Han</last><affiliation>Institute of Software, CAS</affiliation></author>
      <author><first>Ben</first><last>He</last></author>
      <author><first>Shing-Chi</first><last>Cheung</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Le</first><last>Sun</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <pages>23762-23779</pages>
      <abstract>Code benchmarks such as HumanEval are widely adopted to evaluate Large Language Models’ (LLMs) coding capabilities. However, there is an unignorable programming language bias in existing code benchmarks – over 95% code generation benchmarks are dominated by Python, leaving the LLMs’ capabilities in other programming languages such as Java and C/C++ unknown. Moreover, coding task bias is also crucial. Most benchmarks focus on code generation capability, while benchmarks for code reasoning (given input, reasoning output; and given output, reasoning input), an essential coding capability, are insufficient. Yet, constructing multi-lingual benchmarks can be expensive and labor-intensive, and codes in contest websites such as Leetcode suffer from data contamination during training. To fill this gap, we propose CRUXEVAL-X, a multi-lingual code reasoning benchmark that contains 19 programming languages. It comprises at least 600 subjects for each language, along with 19K content-consistent tests in total. In particular, the construction pipeline of CRUXEVAL-X works in a fully automated and test-guided manner, which iteratively generates and repairs based on execution feedback. Also, to cross language barriers (e.g., dynamic/static type systems in Python/C++), we formulated various transition rules between language pairs to facilitate translation. Our intensive evaluation of 24 representative LLMs reveals the correlation between language pairs. For example, TypeScript and JavaScript show a significant positive correlation, while Racket has less correlation with other languages. More interestingly, even a model trained solely on Python can achieve at most 34.4% Pass@1 in other languages, revealing the cross-language generalization of LLMs.</abstract>
      <url hash="317edf05">2025.acl-long.1158</url>
      <bibkey>xu-etal-2025-cruxeval</bibkey>
    </paper>
    <paper id="1159">
      <title>Graph of Records: Boosting Retrieval Augmented Generation for Long-context Summarization with Graphs</title>
      <author><first>Haozhen</first><last>Zhang</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Tao</first><last>Feng</last></author>
      <author><first>Jiaxuan</first><last>You</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <pages>23780-23799</pages>
      <abstract>Retrieval-augmented generation (RAG) has revitalized Large Language Models (LLMs) by injecting non-parametric factual knowledge. Compared with long-context LLMs, RAG is considered an effective summarization tool in a more concise and lightweight manner, which can interact with LLMs multiple times using diverse queries to get comprehensive responses. However, the LLM-generated historical responses, which contain potentially insightful information, are largely neglected and discarded by existing approaches, leading to suboptimal results. In this paper, we propose <tex-math>\textit{graph of records}</tex-math> (<tex-math>\textbf{GoR}</tex-math>), which leverages historical responses generated by LLMs to enhance RAG for long-context global summarization. Inspired by the <tex-math>\textit{retrieve-then-generate}</tex-math> paradigm of RAG, we construct a graph by establishing an edge between the retrieved text chunks and the corresponding LLM-generated response. To further uncover the intricate correlations between them, GoR features a <tex-math>\textit{graph neural network}</tex-math> and an elaborately designed <tex-math>\textit{BERTScore}</tex-math>-based objective for self-supervised model training, enabling seamless supervision signal backpropagation between reference summaries and node embeddings. We comprehensively compare GoR with 12 baselines across four long-context summarization datasets, and the results indicate that our proposed method reaches the best performance (<tex-math>\textit{e.g.}</tex-math>, 15%, 8%, and 19% improvement over retrievers w.r.t. Rouge-L, Rouge-1, and Rouge-2 on the WCEP dataset). Extensive experiments further demonstrate the effectiveness of GoR.</abstract>
      <url hash="124ac7a6">2025.acl-long.1159</url>
      <bibkey>zhang-etal-2025-graph</bibkey>
    </paper>
    <paper id="1160">
      <title>Rubrik’s Cube: Testing a New Rubric for Evaluating Explanations on the <fixed-case>CUBE</fixed-case> dataset</title>
      <author><first>Diana</first><last>Galvan-Sosa</last></author>
      <author><first>Gabrielle</first><last>Gaudeau</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Pride</first><last>Kavumba</last><affiliation>SB Intuitions</affiliation></author>
      <author><first>Yunmeng</first><last>Li</last></author>
      <author><first>Hongyi</first><last>Gu</last></author>
      <author><first>Zheng</first><last>Yuan</last><affiliation>University of Sheffield</affiliation></author>
      <author><first>Keisuke</first><last>Sakaguchi</last><affiliation>Tohoku University</affiliation></author>
      <author><first>Paula</first><last>Buttery</last><affiliation>University of Cambridge</affiliation></author>
      <pages>23800-23839</pages>
      <abstract>The performance and usability of Large-Language Models (LLMs) are driving their use in explanation generation tasks. However, despite their widespread adoption, LLM explanations have been found to be unreliable, making it difficult for users to distinguish good from bad explanations. To address this issue, we present Rubrik’s CUBE–an education-inspired rubric and a dataset of 26k explanations, written and later quality-annotated using the rubric by both humans and six open- and closed-source LLMs. The CUBE dataset focuses on two reasoning and two language tasks, providing the necessary diversity for us to effectively test our proposed rubric. Using Rubrik, we find that explanations are influenced by both task and perceived difficulty. Low quality stems primarily from a lack of conciseness in LLM-generated explanations, rather than cohesion and word choice. The full dataset, rubric, and code are available at https://github.com/RubriksCube/rubriks_cube.</abstract>
      <url hash="5b1de81c">2025.acl-long.1160</url>
      <bibkey>galvan-sosa-etal-2025-rubriks</bibkey>
    </paper>
    <paper id="1161">
      <title>A Dual-Mind Framework for Strategic and Expressive Negotiation Agent</title>
      <author><first>Yutong</first><last>Liu</last><affiliation>Jilin University</affiliation></author>
      <author><first>Lida</first><last>Shi</last><affiliation>Jilin University</affiliation></author>
      <author><first>Rui</first><last>Song</last><affiliation>Jilin University</affiliation></author>
      <author><first>Hao</first><last>Xu</last><affiliation>Jilin University</affiliation></author>
      <pages>23840-23860</pages>
      <abstract>Negotiation agents need to influence the attitudes or intentions of users to reach a consensus. Strategy planning and expressive optimization are crucial aspects of effective negotiations. However, previous studies have typically focused on only one of these aspects, neglecting the fact that their combined synergistic effect can lead to better performance. Inspired by the dual-process theory in human cognition, we propose a Dual-Mind Negotiation Agent (DMNA) framework. This framework integrates an intuitive module for rapid, experience-based response and a deliberative module for slow, expression optimization. The intuitive module is trained using Monte Carlo Tree Search (MCTS) and Direct Preference Optimization (DPO), enabling it to make suitable strategic planning and expression. The deliberative module employs a multifaceted reflexion mechanism to enhance the quality of expression. Experiments conducted on negotiation datasets confirm that DMNA achieves state-of-the-art results, demonstrating an enhancement in the negotiation ability of agents.</abstract>
      <url hash="8c915792">2025.acl-long.1161</url>
      <bibkey>liu-etal-2025-dual</bibkey>
    </paper>
    <paper id="1162">
      <title>Ref-Long: Benchmarking the Long-context Referencing Capability of Long-context Language Models</title>
      <author><first>Junjie</first><last>Wu</last><affiliation>HKUST</affiliation></author>
      <author><first>Gefei</first><last>Gu</last></author>
      <author><first>Yanan</first><last>Zheng</last></author>
      <author><first>Dit-Yan</first><last>Yeung</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Arman</first><last>Cohan</last><affiliation>Yale University and Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>23861-23880</pages>
      <abstract>Long-context language models (LCLMs) have exhibited impressive capabilities in long-context understanding tasks. Among these, long-context referencing—a crucial task that requires LCLMs to attribute items of interest to specific parts of long-context data—remains underexplored. To bridge this gap, this paper proposes Referencing Evaluation for Long-context Language Models (Ref-Long), a novel benchmark designed to assess the long-context referencing capability of LCLMs. Specifically, Ref-Long requires LCLMs to identify the indexes of documents that reference a specific key, emphasizing contextual relationships between the key and the documents over simple retrieval. Based on the task design, we construct three subsets ranging from synthetic to realistic scenarios to form the Ref-Long benchmark. Experimental results of 13 LCLMs reveal significant shortcomings in long-context referencing, even among advanced models like GPT-4o. To further investigate these challenges, we conduct comprehensive analyses, including human evaluations, task format adjustments, fine-tuning experiments, and error analyses, leading to several key insights. Our data and code will be publicly released, and the data is also attached in the submission.</abstract>
      <url hash="9efcc69f">2025.acl-long.1162</url>
      <bibkey>wu-etal-2025-ref</bibkey>
    </paper>
    <paper id="1163">
      <title>Revisiting Scaling Laws for Language Models: The Role of Data Quality and Training Strategies</title>
      <author><first>Zhengyu</first><last>Chen</last><affiliation>Meituan</affiliation></author>
      <author><first>Siqi</first><last>Wang</last></author>
      <author><first>Teng</first><last>Xiao</last></author>
      <author><first>Yudong</first><last>Wang</last><affiliation>Peking University</affiliation></author>
      <author><first>Shiqi</first><last>Chen</last><affiliation>The Hong Kong University of Science and Technology and City University of Hong Kong</affiliation></author>
      <author><first>Xunliang</first><last>Cai</last><affiliation>Meituan</affiliation></author>
      <author><first>Junxian</first><last>He</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Jingang</first><last>Wang</last><affiliation>Meituan</affiliation></author>
      <pages>23881-23899</pages>
      <abstract>Traditional scaling laws in natural language processing suggest that increasing model size and training data enhances performance. However, recent studies reveal deviations, particularly in large language models, where performance improvements decelerate—a phenomenon known as sub-scaling. This paper revisits these scaling laws by examining the impact of data quality and training strategies on model performance. Through extensive empirical analysis of over 400 models, we identify high data density and non-optimal resource allocation as key factors contributing to sub-scaling. High data density leads to diminishing returns due to redundant information, while optimal resource allocation is crucial for sustained performance improvements. We propose a sub-optimal scaling law that better predicts performance in sub-scaling regimes, highlighting the importance of data quality and diversity.</abstract>
      <url hash="720ebcd9">2025.acl-long.1163</url>
      <bibkey>chen-etal-2025-revisiting</bibkey>
    </paper>
    <paper id="1164">
      <title>Limited Generalizability in Argument Mining: State-Of-The-Art Models Learn Datasets, Not Arguments</title>
      <author><first>Marc</first><last>Feger</last></author>
      <author><first>Katarina</first><last>Boland</last><affiliation>Heinrich Heine University Düsseldorf</affiliation></author>
      <author><first>Stefan</first><last>Dietze</last><affiliation>GESIS and Heinrich-Heine-University Düsseldorf</affiliation></author>
      <pages>23900-23915</pages>
      <abstract>Identifying arguments is a necessary prerequisite for various tasks in automated discourse analysis, particularly within contexts such as political debates, online discussions, and scientific reasoning. In addition to theoretical advances in understanding the constitution of arguments, a significant body of research has emerged around practical argument mining, supported by a growing number of publicly available datasets. On these benchmarks, BERT-like transformers have consistently performed best, reinforcing the belief that such models are broadly applicable across diverse contexts of debate. This study offers the first large-scale re-evaluation of such state-of-the-art models, with a specific focus on their ability to generalize in identifying arguments. We evaluate four transformers, three standard and one enhanced with contrastive pre-training for better generalization, on 17 English sentence-level datasets as most relevant to the task. Our findings show that, to varying degrees, these models tend to rely on lexical shortcuts tied to content words, suggesting that apparent progress may often be driven by dataset-specific cues rather than true task alignment. While the models achieve strong results on familiar benchmarks, their performance drops markedly when applied to unseen datasets. Nonetheless, incorporating both task-specific pre-training and joint benchmark training proves effective in enhancing both robustness and generalization.</abstract>
      <url hash="6c2a1314">2025.acl-long.1164</url>
      <bibkey>feger-etal-2025-limited</bibkey>
    </paper>
    <paper id="1165">
      <title>Enhancing Machine Translation with Self-Supervised Preference Data</title>
      <author><first>Haoxiang</first><last>Sun</last></author>
      <author><first>Ruize</first><last>Gao</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Pei</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Baosong</first><last>Yang</last></author>
      <author><first>Rui</first><last>Wang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>23916-23934</pages>
      <abstract>Model alignment methods like Direct Preference Optimization and Contrastive Preference Optimization have enhanced machine translation performance by leveraging preference data to enable models to reject suboptimal outputs. During preference data construction, previous approaches primarily rely on humans, strong models like GPT4 or model self-sampling. In this study, we first explain the shortcomings of this practice. Then, we propose Self-Supervised Preference Optimization (SSPO), a novel framework which efficiently constructs translation preference data for iterative DPO training. Applying SSPO to 14B parameters large language models (LLMs) achieves comparable or better performance than GPT-4o on FLORES and multi-domain test datasets. We release an augmented MQM dataset in https://github.com/sunny-sjtu/MQM-aug.</abstract>
      <url hash="2f783d21">2025.acl-long.1165</url>
      <bibkey>sun-etal-2025-enhancing-machine</bibkey>
    </paper>
    <paper id="1166">
      <title>Unveil: Unified Visual-Textual Integration and Distillation for Multi-modal Document Retrieval</title>
      <author><first>Hao</first><last>Sun</last></author>
      <author><first>Yingyan</first><last>Hou</last></author>
      <author><first>Jiayan</first><last>Guo</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Bo</first><last>Wang</last></author>
      <author><first>Chunyu</first><last>Yang</last></author>
      <author><first>Jinsong</first><last>Ni</last></author>
      <author><first>Yan</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <pages>23935-23945</pages>
      <abstract>Document retrieval in real-world scenarios faces significant challenges due to diverse document formats and modalities. Traditional text-based approaches rely on tailored parsing techniques that disregard layout information and are prone to errors, while recent parsing-free visual methods often struggle to capture fine-grained textual semantics in text-rich scenarios. To address these limitations, we propose Unveil, a novel visual-textual embedding framework that effectively integrates textual and visual features for robust document representation. Through knowledge distillation, we transfer the semantic understanding capabilities from the visual-textual embedding model to a purely visual model, enabling efficient parsing-free retrieval while preserving semantic fidelity. Experimental results demonstrate that our visual-textual embedding method surpasses existing approaches, while knowledge distillation successfully bridges the performance gap between visual-textual and visual-only methods, improving both retrieval accuracy and efficiency.</abstract>
      <url hash="25ecea81">2025.acl-long.1166</url>
      <bibkey>sun-etal-2025-unveil</bibkey>
    </paper>
    <paper id="1167">
      <title>Don’t Get Lost in the Trees: Streamlining <fixed-case>LLM</fixed-case> Reasoning by Overcoming Tree Search Exploration Pitfalls</title>
      <author><first>Ante</first><last>Wang</last></author>
      <author><first>Linfeng</first><last>Song</last></author>
      <author><first>Ye</first><last>Tian</last></author>
      <author><first>Dian</first><last>Yu</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Haitao</first><last>Mi</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Xiangyu</first><last>Duan</last><affiliation>Soochow University, China</affiliation></author>
      <author><first>Zhaopeng</first><last>Tu</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Jinsong</first><last>Su</last><affiliation>Xiamen University</affiliation></author>
      <author><first>Dong</first><last>Yu</last><affiliation>Tencent AI Lab</affiliation></author>
      <pages>23946-23959</pages>
      <abstract>Recent advancements in tree search algorithms guided by verifiers have significantly enhanced the reasoning capabilities of large language models (LLMs), but at the cost of increased computational resources. In this work, we identify two key challenges contributing to this inefficiency: <tex-math>\textit{over-exploration}</tex-math> due to redundant states with semantically equivalent content, and <tex-math>\textit{under-exploration}</tex-math> caused by high variance in verifier scoring leading to frequent trajectory switching. To address these issues, we propose FETCH – an e<tex-math>{\bf f}</tex-math>fici<tex-math>{\bf e}</tex-math>nt <tex-math>{\bf t}</tex-math>ree sear<tex-math>{\bf ch}</tex-math> framework, which is a flexible, plug-and-play system compatible with various tree search algorithms.Our framework mitigates over-exploration by merging semantically similar states using agglomerative clustering of text embeddings obtained from a fine-tuned SimCSE model. To tackle under-exploration, we enhance verifiers by incorporating temporal difference learning with adjusted <tex-math>\lambda</tex-math>-returns during training to reduce variance, and employing a verifier ensemble to aggregate scores during inference. Experiments on GSM8K, GSM-Plus, and MATH datasets demonstrate that our methods significantly improve reasoning accuracy and computational efficiency across four different tree search algorithms, paving the way for more practical applications of LLM-based reasoning. The code is available at https://github.com/DeepLearnXMU/Fetch.</abstract>
      <url hash="39f67363">2025.acl-long.1167</url>
      <bibkey>wang-etal-2025-dont</bibkey>
    </paper>
    <paper id="1168">
      <title><fixed-case>MEXMA</fixed-case>: Token-level objectives improve sentence representations</title>
      <author><first>João Maria</first><last>Janeiro</last></author>
      <author><first>Benjamin</first><last>Piwowarski</last><affiliation>CNRS / ISIR, Sorbonne Université and CNRS</affiliation></author>
      <author><first>Patrick</first><last>Gallinari</last><affiliation>Criteo AI Lab and Sorbonne Universite</affiliation></author>
      <author><first>Loic</first><last>Barrault</last></author>
      <pages>23960-23995</pages>
      <abstract>Cross-lingual sentence encoders (CLSE) create fixed-size sentence representations with aligned translations. Current pre-trained CLSE approaches use sentence-level objectives only. This can lead to loss of information, especially for tokens, which then degrades the sentence representation. We propose MEXMA, a novel approach that integrates both sentence-level and token-level objectives. The sentence representation in one language is used to predict masked tokens in another language, with both the sentence representation and *all tokens directly update the encoder*. We show that adding token-level objectives greatly improves the sentence representation quality across several tasks. Our approach outperforms current pre-trained cross-lingual sentence encoders on bitext mining as well as several downstream tasks. We also analyse the information encoded in our tokens, and how the sentence representation is built from them.</abstract>
      <url hash="5b237f49">2025.acl-long.1168</url>
      <bibkey>janeiro-etal-2025-mexma</bibkey>
    </paper>
    <paper id="1169">
      <title>Uncertainty-Aware Iterative Preference Optimization for Enhanced <fixed-case>LLM</fixed-case> Reasoning</title>
      <author><first>Lei</first><last>Li</last><affiliation>Tencent</affiliation></author>
      <author><first>Hehuan</first><last>Liu</last></author>
      <author><first>Yaxin</first><last>Zhou</last></author>
      <author><first>ZhaoYang</first><last>Gui</last><affiliation>Tencent</affiliation></author>
      <author><first>Xudong</first><last>Weng</last><affiliation>Tencent</affiliation></author>
      <author><first>Yi</first><last>Yuan</last></author>
      <author><first>Zheng</first><last>Wei</last></author>
      <author><first>Zang</first><last>Li</last><affiliation>Tencent</affiliation></author>
      <pages>23996-24012</pages>
      <abstract>Direct Preference Optimization (DPO) has recently emerged as an efficient and effective method for aligning large language models with human preferences. However, constructing high-quality preference datasets remains challenging, often necessitating expensive manual or powerful LM annotations. Additionally, standard DPO exhibits suboptimal performance in complex reasoning tasks, such as mathematical and code reasoning. In this paper, we introduce an approach to collect preference pairs through iterative sampling and execution feedback, tailored to the current learning state (<i>e.g.</i> well-learned, mis-learned, and unlearned) of the policy model. To alleviate the failures of DPO and improve its applicability in reasoning tasks, we propose , an iterative uncertainty-aware preference optimization method that achieves fine-grained preference control by assessing model confidence. We validate our approach across three reasoning tasks, incorporating five established reasoning datasets and one self-curated dataset. Our experimental results demonstrate an overall improvement of 3.6% over the standard DPO method and show the model exhibits promising generalizability.</abstract>
      <url hash="1505cd72">2025.acl-long.1169</url>
      <bibkey>li-etal-2025-uncertainty</bibkey>
    </paper>
    <paper id="1170">
      <title><fixed-case>A</fixed-case>gent<fixed-case>D</fixed-case>ropout: Dynamic Agent Elimination for Token-Efficient and High-Performance <fixed-case>LLM</fixed-case>-Based Multi-Agent Collaboration</title>
      <author><first>Zhexuan</first><last>Wang</last></author>
      <author><first>Yutong</first><last>Wang</last></author>
      <author><first>Xuebo</first><last>Liu</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <author><first>Liang</first><last>Ding</last></author>
      <author><first>Miao</first><last>Zhang</last><affiliation>Harbin Institute of Technology (Shenzhen)</affiliation></author>
      <author><first>Jie</first><last>Liu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>24013-24035</pages>
      <abstract>Multi-agent systems (MAS) based on large language models (LLMs) have demonstrated significant potential in collaborative problem-solving. However, they still face substantial challenges of low communication efficiency and suboptimal task performance, making the careful design of the agents’ communication topologies particularly important. Inspired by the management theory that roles in an efficient team are often dynamically adjusted, we propose <tex-math>\textbf{AgentDropout}</tex-math>, which identifies redundant agents and communication across different communication rounds by optimizing the adjacency matrices of the communication graphs and eliminates them to enhance both token efficiency and task performance. Compared to state-of-the-art methods, AgentDropout achieves an average reduction of 21.6% in prompt token consumption and 18.4% in completion token consumption, along with a performance improvement of 1.14 on the tasks. Furthermore, the extended experiments demonstrate that AgentDropout achieves notable domain transferability and structure robustness, revealing its reliability and effectiveness. We release our code at https://github.com/wangzx1219/AgentDropout.</abstract>
      <url hash="22670d7c">2025.acl-long.1170</url>
      <bibkey>wang-etal-2025-agentdropout</bibkey>
    </paper>
    <paper id="1171">
      <title>Towards Dynamic Theory of Mind: Evaluating <fixed-case>LLM</fixed-case> Adaptation to Temporal Evolution of Human States</title>
      <author><first>Yang</first><last>Xiao</last></author>
      <author><first>Jiashuo</first><last>Wang</last></author>
      <author><first>Qiancheng</first><last>Xu</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Changhe</first><last>Song</last></author>
      <author><first>Chunpu</first><last>Xu</last></author>
      <author><first>Yi</first><last>Cheng</last></author>
      <author><first>Wenjie</first><last>Li</last><affiliation>The Hong Kong Polytechnic University, The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Pengfei</first><last>Liu</last></author>
      <pages>24036-24057</pages>
      <abstract>As Large Language Models (LLMs) increasingly participate in human-AI interactions, evaluating their Theory of Mind (ToM) capabilities - particularly their ability to track dynamic mental states - becomes crucial. While existing benchmarks assess basic ToM abilities, they predominantly focus on static snapshots of mental states, overlooking the temporal evolution that characterizes real-world social interactions. We present **DynToM**, a novel benchmark specifically designed to evaluate LLMs’ ability to understand and track the temporal progression of mental states across interconnected scenarios. Through a systematic four-step framework, we generate 1,100 social contexts encompassing 5,500 scenarios and 78,100 questions, each validated for realism and quality. Our comprehensive evaluation of ten state-of-the-art LLMs reveals that their average performance underperforms humans by 44.7%, with performance degrading significantly when tracking and reasoning about the shift of mental states. This performance gap highlights fundamental limitations in current LLMs’ ability to model the dynamic nature of human mental states.</abstract>
      <url hash="42ea1c7c">2025.acl-long.1171</url>
      <bibkey>xiao-etal-2025-towards</bibkey>
    </paper>
    <paper id="1172">
      <title>Marco-Bench-<fixed-case>MIF</fixed-case>: On Multilingual Instruction-Following Capability of Large Language</title>
      <author><first>Bo</first><last>Zeng</last></author>
      <author><first>Chenyang</first><last>Lyu</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Sinuo</first><last>Liu</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Mingyan</first><last>Zeng</last></author>
      <author><first>Minghao</first><last>Wu</last></author>
      <author><first>Xuanfan</first><last>Ni</last></author>
      <author><first>Tianqi</first><last>Shi</last></author>
      <author><first>Yu</first><last>Zhao</last></author>
      <author><first>Yefeng</first><last>Liu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Chenyu</first><last>Zhu</last></author>
      <author><first>Ruizhe</first><last>Li</last><affiliation>University of Aberdeen</affiliation></author>
      <author><first>Jiahui</first><last>Geng</last></author>
      <author><first>Qing</first><last>Li</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Yu</first><last>Tong</last></author>
      <author><first>Longyue</first><last>Wang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Weihua</first><last>Luo</last><affiliation>Alibaba International Digital Commerce Group</affiliation></author>
      <author><first>Kaifu</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <pages>24058-24072</pages>
      <abstract>Instruction-following capability has become a major ability to be evaluated for Large Language Models. However, existing datasets, such as IFEval, are either predominantly monolingual and centered on English or simply machine translated to other languages, limiting their applicability in multilingual contexts. In this paper, we present an carefully-curated extension of IFEval to a localized multilingual version named Marco-Bench-MIF, covering 30 languages with varying levels of localization. Our benchmark addresses linguistic constraints (e.g., modifying capitalization requirements for Chinese) and cultural references (e.g., substituting region-specific company names in prompts) via a hybrid pipeline combining translation with verification. Through comprehensive evaluation of 20+ LLMs on our Marco-Bench-MIF, we found that: (1) 25-35% accuracy gap between high/low-resource languages, (2) model scales largely impact performance by 45-60% yet persists script-specific challenges, and (3) machine-translated data underestimates accuracy by 7-22% versus localized data. Our analysis identifies challenges in multilingual instruction following, including keyword consistency preservation and compositional constraint adherence across languages. Our Marco-Bench-MIF will be made publicly available to the community.</abstract>
      <url hash="2919f980">2025.acl-long.1172</url>
      <bibkey>zeng-etal-2025-marco</bibkey>
    </paper>
    <paper id="1173">
      <title>Representation Bending for Large Language Model Safety</title>
      <author><first>Ashkan</first><last>Yousefpour</last></author>
      <author><first>Taeheon</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Ryan Sungmo</first><last>Kwon</last></author>
      <author><first>Seungbeen</first><last>Lee</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Wonje</first><last>Jeung</last></author>
      <author><first>Seungju</first><last>Han</last><affiliation>Computer Science Department, Stanford University and NVIDIA</affiliation></author>
      <author><first>Alvin</first><last>Wan</last><affiliation>OpenAI</affiliation></author>
      <author><first>Harrison</first><last>Ngan</last><affiliation>Amazon</affiliation></author>
      <author><first>Youngjae</first><last>Yu</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Jonghyun</first><last>Choi</last><affiliation>Seoul National University</affiliation></author>
      <pages>24073-24098</pages>
      <abstract>Large Language Models (LLMs) have emerged as powerful tools, but their inherent safety risks – ranging from harmful content generation to broader societal harms – pose significant challenges. These risks can be amplified by the recent adversarial attacks, fine-tuning vulnerabilities, and the increasing deployment of LLMs in high-stakes environments. Existing safety-enhancing techniques, such as fine-tuning with human feedback or adversarial training, are still vulnerable as they address specific threats and often fail to generalize across unseen attacks, or require manual system-level defenses. This paper introduces RepBend, a novel approach that fundamentally disrupts the representations underlying harmful behaviors in LLMs, offering a scalable solution to enhance (potentially inherent) safety. RepBend brings the idea of activation steering – simple vector arithmetic for steering model’s behavior during inference – to loss-based fine-tuning. Through extensive evaluation, RepBend achieves state-of-the-art performance, outperforming prior methods such as Circuit Breaker, RMU, and NPO, with up to 95% reduction in attack success rates across diverse jailbreak benchmarks, all with negligible reduction in model usability and general capabilities.</abstract>
      <url hash="d305c09b">2025.acl-long.1173</url>
      <bibkey>yousefpour-etal-2025-representation</bibkey>
    </paper>
    <paper id="1174">
      <title>Analyzing <fixed-case>LLM</fixed-case>s’ Knowledge Boundary Cognition Across Languages Through the Lens of Internal Representations</title>
      <author><first>Chenghao</first><last>Xiao</last><affiliation>Durham University</affiliation></author>
      <author><first>Hou Pong</first><last>Chan</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Hao</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Mahani</first><last>Aljunied</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Lidong</first><last>Bing</last><affiliation>Shanda Group and Alibaba Group</affiliation></author>
      <author><first>Noura</first><last>Al Moubayed</last><affiliation>Durham University</affiliation></author>
      <author><first>Yu</first><last>Rong</last><affiliation>Alibaba Group</affiliation></author>
      <pages>24099-24115</pages>
      <abstract>While understanding the knowledge boundaries of LLMs is crucial to prevent hallucination, research on the knowledge boundaries of LLMs has predominantly focused on English. In this work, we present the first study to analyze how LLMs recognize knowledge boundaries across different languages by probing their internal representations when processing known and unknown questions in multiple languages. Our empirical studies reveal three key findings: 1) LLMs’ perceptions of knowledge boundaries are encoded in the middle to middle-upper layers across different languages. 2) Language differences in knowledge boundary perception follow a linear structure, which motivates our proposal of a training-free alignment method that effectively transfers knowledge boundary perception ability across languages, thereby helping reduce hallucination risk in low-resource languages; 3) Fine-tuning on bilingual question pair translation further enhances LLMs’ recognition of knowledge boundaries across languages. Given the absence of standard testbeds for cross-lingual knowledge boundary analysis, we construct a multilingual evaluation suite comprising three representative types of knowledge boundary data. Our code and datasets are publicly available at <url>https://github.com/DAMO-NLP-SG/LLM-Multilingual-Knowledge-Boundaries</url>.</abstract>
      <url hash="9bac4944">2025.acl-long.1174</url>
      <bibkey>xiao-etal-2025-analyzing</bibkey>
    </paper>
    <paper id="1175">
      <title>Enhancing Retrieval-Augmented Generation via Evidence Tree Search</title>
      <author><first>Hao</first><last>Sun</last></author>
      <author><first>Hengyi</first><last>Cai</last></author>
      <author><first>Yuchen</first><last>Li</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Xuanbo</first><last>Fan</last><affiliation>Peking University</affiliation></author>
      <author><first>Xiaochi</first><last>Wei</last><affiliation>Baidu</affiliation></author>
      <author><first>Shuaiqiang</first><last>Wang</last></author>
      <author><first>Yan</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Dawei</first><last>Yin</last><affiliation>Baidu</affiliation></author>
      <pages>24116-24127</pages>
      <abstract>Retrieval-Augmented Generation (RAG) is widely used to enhance Large Language Models (LLMs) by grounding responses in external knowledge. However, in real-world applications, retrievers often return lengthy documents with redundant or irrelevant content, confusing downstream readers. While evidence retrieval aims to address this by extracting key information, it faces critical challenges: (1) inability to model synergistic inter-dependencies among evidence sentences, (2) lack of supervision for evaluating multi-sentence evidence quality, and (3) computational inefficiency in navigating exponentially growing search spaces of candidate evidence sets. To tackle these challenges, we propose ETS (Evidence Tree Search), a novel framework that reformulates evidence retrieval as a dynamic tree expansion process. Our approach first constructs an evidence tree where each path represents a candidate evidence set, explicitly modeling inter-sentence dependencies through context-aware node selection. We then leverage Monte Carlo Tree Search (MCTS) to efficiently assess evidence quality and introduce an Early-Terminating Beam Search strategy to efficiently accelerate the model inference. Extensive experiments on five datasets demonstrate that ETS significantly outperforms existing methods across different readers. Our code and datasets will be released to facilitate future research.</abstract>
      <url hash="bfdc524f">2025.acl-long.1175</url>
      <bibkey>sun-etal-2025-enhancing-retrieval</bibkey>
    </paper>
    <paper id="1176">
      <title><fixed-case>H</fixed-case>allu<fixed-case>L</fixed-case>ens: <fixed-case>LLM</fixed-case> Hallucination Benchmark</title>
      <author><first>Yejin</first><last>Bang</last></author>
      <author><first>Ziwei</first><last>Ji</last></author>
      <author><first>Alan</first><last>Schelten</last><affiliation>Facebook</affiliation></author>
      <author><first>Anthony</first><last>Hartshorn</last></author>
      <author><first>Tara</first><last>Fowler</last></author>
      <author><first>Cheng</first><last>Zhang</last><affiliation>Meta</affiliation></author>
      <author><first>Nicola</first><last>Cancedda</last><affiliation>Meta</affiliation></author>
      <author><first>Pascale</first><last>Fung</last><affiliation>HKUST</affiliation></author>
      <pages>24128-24156</pages>
      <abstract>Large language models (LLMs) often generate responses that deviate from user input or training data, a phenomenon known as “hallucination.” These hallucinations undermine user trust and hinder the adoption of generative AI systems. Addressing hallucinations is important for the advancement of LLMs. This paper introduces a comprehensive hallucination benchmark HalluLens, incorporating both extrinsic and intrinsic evaluation tasks, built upon a clear taxonomy of hallucination. A major challenge in benchmarking hallucinations is the lack of a unified framework due to inconsistent definitions and categorizations. We disentangle LLM hallucination from “factuality” and propose a taxonomy distinguishing extrinsic and intrinsic hallucinations to promote consistency and facilitate research. We emphasize extrinsic hallucinations – where generated content deviates from training data – as they become increasingly relevant with LLM advancements. However, no benchmark is solely dedicated to extrinsic hallucinations. To address this gap, HalluLens introduces three new extrinsic tasks with dynamic test set generation to mitigate data leakage and ensure robustness. We release codebase for extrinsic hallucination benchmark.</abstract>
      <url hash="33068487">2025.acl-long.1176</url>
      <bibkey>bang-etal-2025-hallulens</bibkey>
    </paper>
    <paper id="1177">
      <title><fixed-case>DEEPER</fixed-case> Insight into Your User: Directed Persona Refinement for Dynamic Persona Modeling</title>
      <author><first>Aili</first><last>Chen</last></author>
      <author><first>Chengyu</first><last>Du</last></author>
      <author><first>Jiangjie</first><last>Chen</last><affiliation>ByteDance Seed</affiliation></author>
      <author><first>Jinghan</first><last>Xu</last></author>
      <author><first>Yikai</first><last>Zhang</last></author>
      <author><first>Siyu</first><last>Yuan</last></author>
      <author><first>Zulong</first><last>Chen</last></author>
      <author><first>Liangyue</first><last>Li</last><affiliation>Amazon</affiliation></author>
      <author><first>Yanghua</first><last>Xiao</last><affiliation>Fudan University</affiliation></author>
      <pages>24157-24180</pages>
      <abstract>To advance personalized applications such as recommendation systems and user behavior prediction, recent research increasingly adopts large language models (LLMs) for human-readable persona modeling. In dynamic real-world scenarios, effective persona modeling necessitates leveraging streaming behavior data to continually optimize user personas.However, existing methods—whether regenerating personas or incrementally extending them with new behaviors—often fail to achieve sustained improvements in persona quality or future behavior prediction accuracy. To address this, we propose DEEPER, a novel approach for dynamic persona modeling that enables continual persona optimization. Specifically, we enhance the model’s direction-search capability through an iterative reinforcement learning framework, allowing it to automatically identify effective update directions and optimize personas using discrepancies between user behaviors and model predictions.Extensive experiments on dynamic persona modeling involving 4,800 users across 10 domains highlight ’s superior persona optimization capabilities, delivering an impressive 32.2% average reduction in user behavior prediction error over four update rounds—outperforming the best baseline by a remarkable 22.92%.</abstract>
      <url hash="98ccbca0">2025.acl-long.1177</url>
      <bibkey>chen-etal-2025-deeper</bibkey>
    </paper>
    <paper id="1178">
      <title>Asclepius: A Spectrum Evaluation Benchmark for Medical Multi-Modal Large Language Models</title>
      <author><first>Jie</first><last>Liu</last><affiliation>City University of Hong Kong</affiliation></author>
      <author><first>Wenxuan</first><last>Wang</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Su</first><last>Yihang</last></author>
      <author><first>Jingyuan</first><last>Huang</last></author>
      <author><first>Yudi</first><last>Zhang</last></author>
      <author><first>Cheng-Yi</first><last>Li</last></author>
      <author><first>Wenting</first><last>Chen</last></author>
      <author><first>Xiaohan</first><last>Xing</last><affiliation>Stanford University</affiliation></author>
      <author><first>Kao-Jung</first><last>Chang</last><affiliation>National Yang Ming Chiao Tung University</affiliation></author>
      <author><first>Linlin</first><last>Shen</last><affiliation>Shenzhen University</affiliation></author>
      <author><first>Michael R.</first><last>Lyu</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>24181-24201</pages>
      <abstract>The significant breakthroughs of Medical Multi-Modal Large Language Models (Med-MLLMs) renovate modern healthcare with robust information synthesis and medical decision support. However, these models are often evaluated on benchmarks that are unsuitable for the Med-MLLMs due to the intricate nature of the real-world diagnostic frameworks, which encompass diverse medical specialties and involve complex clinical decisions. Thus, a clinically representative benchmark is highly desirable for credible Med-MLLMs evaluation. To this end, we introduce Asclepius, a novel Med-MLLM benchmark that comprehensively assesses Med-MLLMs in terms of: distinct medical specialties (cardiovascular, gastroenterology, etc.) and different diagnostic capacities (perception, disease analysis, etc.). Grounded in 3 proposed core principles, Asclepius ensures a comprehensive evaluation by encompassing 15 medical specialties, stratifying into 3 main categories and 8 sub-categories of clinical tasks, and exempting overlap with the existing VQA dataset. We further provide an in-depth analysis of 6 Med-MLLMs and compare them with 3 human specialists, providing insights into their competencies and limitations in various medical contexts. Our work not only advances the understanding of Med-MLLMs’ capabilities but also sets a precedent for future evaluations and the safe deployment of these models in clinical environments.</abstract>
      <url hash="36ef5154">2025.acl-long.1178</url>
      <bibkey>liu-etal-2025-asclepius</bibkey>
    </paper>
    <paper id="1179">
      <title><fixed-case>I</fixed-case>nstruct<fixed-case>P</fixed-case>art: Task-Oriented Part Segmentation with Instruction Reasoning</title>
      <author><first>Zifu</first><last>Wan</last></author>
      <author><first>Yaqi</first><last>Xie</last></author>
      <author><first>Ce</first><last>Zhang</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Zhiqiu</first><last>Lin</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Zihan</first><last>Wang</last></author>
      <author><first>Simon</first><last>Stepputtis</last><affiliation>Virginia Polytechnic Institute and State University</affiliation></author>
      <author><first>Deva</first><last>Ramanan</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Katia P.</first><last>Sycara</last></author>
      <pages>24202-24227</pages>
      <abstract>Large multimodal foundation models, particularly in the domains of language and vision, have significantly advanced various tasks, including robotics, autonomous driving, information retrieval, and grounding. However, many of these models perceive objects as indivisible, overlooking the components that constitute them. Understanding these components and their associated affordances provides valuable insights into an object’s functionality, which is fundamental for performing a wide range of tasks. In this work, we introduce a novel real-world benchmark, InstructPart, comprising hand-labeled part segmentation annotations and task-oriented instructions to evaluate the performance of current models in understanding and executing part-level tasks within everyday contexts. Through our experiments, we demonstrate that task-oriented part segmentation remains a challenging problem, even for state-of-the-art Vision-Language Models (VLMs). In addition to our benchmark, we introduce a simple baseline that achieves a twofold performance improvement through fine-tuning with our dataset. With our dataset and benchmark, we aim to facilitate research on task-oriented part segmentation and enhance the applicability of VLMs across various domains, including robotics, virtual reality, information retrieval, and other related fields. Project website: https://zifuwan.github.io/InstructPart/.</abstract>
      <url hash="d00ebd73">2025.acl-long.1179</url>
      <bibkey>wan-etal-2025-instructpart</bibkey>
    </paper>
    <paper id="1180">
      <title><fixed-case>GR</fixed-case>a<fixed-case>MP</fixed-case>a: Subword Regularisation by Skewing Uniform Segmentation Distributions with an Efficient Path-counting <fixed-case>M</fixed-case>arkov Model</title>
      <author><first>Thomas</first><last>Bauwens</last><affiliation>KU Leuven</affiliation></author>
      <author><first>David</first><last>Kaczér</last><affiliation>Rheinische Friedrich-Wilhelms Universität Bonn</affiliation></author>
      <author><first>Miryam</first><last>De Lhoneux</last><affiliation>KU Leuven</affiliation></author>
      <pages>24228-24257</pages>
      <abstract>Stochastically sampling word segmentations from a subword tokeniser, also called subword regularisation, is a known way to increase robustness of language models to out-of-distribution inputs, such as text containing spelling errors. Recent work has observed that usual augmentations that make popular deterministic subword tokenisers stochastic still cause only a handful of all possible segmentations to be sampled. It has been proposed to uniformly sample across these instead, through rejection sampling of paths in an unweighted segmentation graph. In this paper, we argue that uniformly random segmentation in turn skews the distributions of certain segmentational properties (e.g. token lengths and amount of tokens produced) away from uniformity, which still ends up hiding meaningfully diverse tokenisations. We propose an alternative uniform sampler using the same segmentation graph, but weighted by counting the paths through it. Our sampling algorithm, GRaMPa, provides hyperparameters allowing sampled tokenisations to skew towards fewer, longer tokens. Furthermore, GRaMPa is single-pass, guaranteeing significantly better computational complexity than previous approaches relying on rejection sampling. We show experimentally that language models trained with GRaMPa outperform existing regularising tokenisers in a data-scarce setting on token-level tasks such as dependency parsing, especially with spelling errors present.</abstract>
      <url hash="f2aa4b05">2025.acl-long.1180</url>
      <bibkey>bauwens-etal-2025-grampa</bibkey>
    </paper>
    <paper id="1181">
      <title>Evaluating the Evaluation of Diversity in Commonsense Generation</title>
      <author><first>Tianhui</first><last>Zhang</last><affiliation>University of Liverpool</affiliation></author>
      <author><first>Bei</first><last>Peng</last><affiliation>University of Liverpool</affiliation></author>
      <author><first>Danushka</first><last>Bollegala</last><affiliation>Amazon and University of Liverpool</affiliation></author>
      <pages>24258-24275</pages>
      <abstract>In commonsense generation, given a set of input concepts, a model must generate a response that is not only commonsense bearing, but also capturing multiple diverse viewpoints. Numerous evaluation metrics based on form- and content-level overlap have been proposed in prior work for evaluating the diversity of a commonsense generation model. However, it remains unclear as to which metrics are best suited for evaluating the diversity in commonsense generation. To address this gap, we conduct a systematic meta-evaluation of diversity metrics for commonsense generation. We find that form-based diversity metrics tend to consistently overestimate the diversity in sentence sets, where even randomly generated sentences are assigned overly high diversity scores. We then use an Large Language Model (LLM) to create a novel dataset annotated for the diversity of sentences generated for a commonsense generation task, and use it to conduct a meta-evaluation of the existing diversity evaluation metrics. Our experimental results show that content-based diversity evaluation metrics consistently outperform the form-based counterparts, showing high correlations with the LLM-based ratings. We recommend that future work on commonsense generation should use content-based metrics for evaluating the diversity of their outputs.</abstract>
      <url hash="904d6eed">2025.acl-long.1181</url>
      <bibkey>zhang-etal-2025-evaluating-evaluation</bibkey>
    </paper>
    <paper id="1182">
      <title>Generate First, Then Sample: Enhancing Fake News Detection with <fixed-case>LLM</fixed-case>-Augmented Reinforced Sampling</title>
      <author><first>Zhao</first><last>Tong</last></author>
      <author><first>Yimeng</first><last>Gu</last></author>
      <author><first>Huidong</first><last>Liu</last></author>
      <author><first>Qiang</first><last>Liu</last><affiliation>Institute of Automation, Chinese Academy of Sciences</affiliation></author>
      <author><first>Shu</first><last>Wu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Haichao</first><last>Shi</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xiao-Yu</first><last>Zhang</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <pages>24276-24290</pages>
      <abstract>The spread of fake news on online platforms has long been a pressing concern. Considering this, extensive efforts have been made to develop fake news detectors. However, a major drawback of these models is their relatively low performance—lagging by more than 20%—in identifying *fake* news compared to *real* news, making them less suitable for practical deployment. This gap is likely due to an imbalance in the dataset and the model’s inadequate understanding of data distribution on the targeted platform. In this work, we focus on improving the model’s effectiveness in detecting *fake* news. To achieve this, we **first** adopt an LLM to **generate** fake news in three different styles, which are later incorporated into the training set to augment the representation of fake news. **Then**, we apply Reinforcement Learning to dynamically **sample** fake news, allowing the model to learn the optimal real-to-fake news ratio for training an effective fake news detector on the targeted platform. This approach allows our model to perform effectively even with a limited amount of annotated news data and consistently improve detection accuracy across different platforms. Experimental results demonstrate that our approach achieves state-of-the-art performance on two benchmark datasets, improving *fake* news detection performance by 24.02% and 11.06% respectively.</abstract>
      <url hash="b6378dff">2025.acl-long.1182</url>
      <bibkey>tong-etal-2025-generate</bibkey>
    </paper>
    <paper id="1183">
      <title><fixed-case>C</fixed-case>hem<fixed-case>A</fixed-case>ctor: Enhancing Automated Extraction of Chemical Synthesis Actions with <fixed-case>LLM</fixed-case>-Generated Data</title>
      <author><first>Yu</first><last>Zhang</last></author>
      <author><first>Ruijie</first><last>Yu</last></author>
      <author><first>Jidong</first><last>Tian</last></author>
      <author><first>Feng</first><last>Zhu</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Jiapeng</first><last>Liu</last></author>
      <author><first>Xiaokang</first><last>Yang</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Yaohui</first><last>Jin</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Yanyan</first><last>Xu</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <pages>24291-24314</pages>
      <abstract>With the increasing interest in robotic synthesis in the context of organic chemistry, the automated extraction of chemical procedures from literature is critical. However, this task remains challenging due to the inherent ambiguity of chemical language and the high cost of human annotation required for developing reliable computer-aided extraction protocols. Here, we present <b>ChemActor</b>, a fully fine-tuned large language model (LLM), as a chemical executor to convert between unstructured experimental procedures and structured action sequences. We propose a sequential LLM-generated data framework to address the challenges of insufficient and low-quality annotated data. This framework integrates a data selection module that selects data based on distribution divergence, with a general-purpose LLM, to generate machine-executable actions from a single molecule input. Additionally, we introduce a novel multi-round LLMs circle review metric, which reflects the model’s advanced understanding of chemical experimental procedures. Extensive experiments on reaction-to-description (R2D) and description-to-action (D2A) tasks demonstrate that ChemActor, augmented by LLM-generated data, achieves state-of-the-art performance, outperforming the baseline model by 10%. The code is available at: https://github.com/Zhanghahah/ChemActor.</abstract>
      <url hash="a153584c">2025.acl-long.1183</url>
      <bibkey>zhang-etal-2025-chemactor</bibkey>
    </paper>
    <paper id="1184">
      <title>Towards Fully Exploiting <fixed-case>LLM</fixed-case> Internal States to Enhance Knowledge Boundary Perception</title>
      <author><first>Shiyu</first><last>Ni</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Keping</first><last>Bi</last><affiliation>Chinese Academy of Sciences</affiliation></author>
      <author><first>Jiafeng</first><last>Guo</last><affiliation>Institute of Computing Technolgy, Chinese Academy of Sciences</affiliation></author>
      <author><first>Lulu</first><last>Yu</last></author>
      <author><first>Baolong</first><last>Bi</last></author>
      <author><first>Xueqi</first><last>Cheng</last><affiliation>Institute of Computing Technology, Chinese Academy</affiliation></author>
      <pages>24315-24329</pages>
      <abstract>Large language models (LLMs) exhibit impressive performance across diverse tasks but often struggle to accurately gauge their knowledge boundaries, leading to confident yet incorrect responses. This paper explores leveraging LLMs’ internal states to enhance their perception of knowledge boundaries from efficiency and risk perspectives. We investigate whether LLMs can estimate their confidence using internal states before response generation, potentially saving computational resources. Our experiments on datasets like Natural Questions, HotpotQA, and MMLU reveal that LLMs demonstrate significant pre-generation perception, which is further refined post-generation, with perception gaps remaining stable across varying conditions. To mitigate risks in critical domains, we introduce Consistency-based Confidence Calibration (<tex-math>C^3</tex-math>), which assesses confidence consistency through question reformulation. <tex-math>C^3</tex-math> significantly improves LLMs’ ability to recognize their knowledge gaps, enhancing the unknown perception rate by 5.6% on NQ and 4.9% on HotpotQA. Our findings suggest that pre-generation confidence estimation can optimize efficiency, while <tex-math>C^3</tex-math> effectively controls output risks, advancing the reliability of LLMs in practical applications.</abstract>
      <url hash="196350da">2025.acl-long.1184</url>
      <bibkey>ni-etal-2025-towards</bibkey>
    </paper>
    <paper id="1185">
      <title><fixed-case>ALGEN</fixed-case>: Few-shot Inversion Attacks on Textual Embeddings via Cross-Model Alignment and Generation</title>
      <author><first>Yiyi</first><last>Chen</last></author>
      <author><first>Qiongkai</first><last>Xu</last><affiliation>Macquarie University</affiliation></author>
      <author><first>Johannes</first><last>Bjerva</last><affiliation>Aalborg University</affiliation></author>
      <pages>24330-24348</pages>
      <abstract>With the growing popularity of Large Language Models (LLMs) and vector databases, private textual data is increasingly processed and stored as numerical embeddings. However, recent studies have proven that such embeddings are vulnerable to inversion attacks, where original text is reconstructed to reveal sensitive information. Previous research has largely assumed access to millions of sentences to train attack models, e.g., through data leakage or nearly unrestricted API access. With our method, a single data point is sufficient for a partially successful inversion attack. With as little as 1k data samples, performance reaches an optimum across a range of black-box encoders, without training on leaked data. We present a Few-shot Textual Embedding Inversion Attack using Cross-Model **AL**ignment and **GEN**eration (__ALGEN__), by aligning victim embeddings to the attack space and using a generative model to reconstruct text. We find that __ALGEN__ attacks can be effectively transferred across domains and languages, revealing key information. We further examine a variety of defense mechanisms against **ALGEN**, and find that none are effective, highlighting the vulnerabilities posed by inversion attacks. By significantly lowering the cost of inversion and proving that embedding spaces can be aligned through one-step optimization, we establish a new textual embedding inversion paradigm with broader applications for embedding alignment in NLP.</abstract>
      <url hash="18731970">2025.acl-long.1185</url>
      <bibkey>chen-etal-2025-algen</bibkey>
    </paper>
    <paper id="1186">
      <title>Decoding on Graphs: Faithful and Sound Reasoning on Knowledge Graphs through Generation of Well-Formed Chains</title>
      <author><first>Kun</first><last>Li</last><affiliation>Chinese University of Hong Kong, The Chinese University of Hong Kong</affiliation></author>
      <author><first>Tianhua</first><last>Zhang</last><affiliation>Chinese University of Hong Kong, The Chinese University of Hong Kong</affiliation></author>
      <author><first>Xixin</first><last>Wu</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Hongyin</first><last>Luo</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>James R.</first><last>Glass</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Helen M.</first><last>Meng</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>24349-24364</pages>
      <abstract>Knowledge Graphs (KGs) can serve as reliable knowledge sources for question answering (QA) due to their structured representation of knowledge. Existing research on the utilization of KG for large language models (LLMs) prevalently relies on subgraph retriever or iterative prompting, overlooking the potential synergy of LLMs’ step-wise reasoning capabilities and KGs’ structural nature. In this paper, we present DoG (Decoding on Graph), a novel framework that facilitates a deep synergy between LLMs and KGs. We first define a concept, well-formed chain, which consists of a sequence of interrelated fact triplets on the KGs, starting from question entities and leading to answers. We argue that this concept can serve as a principle for making faithful and sound reasoning for KGQA. To enable LLMs to generate well-formed chains, we propose graph-aware constrained decoding, in which a constraint derived from the topology of the KG regulates the decoding process of the LLMs. This constrained decoding method ensures the generation of well-formed chains while making full use of the step-wise reasoning capabilities of LLMs. Based on the above, DoG, a training-free approach, is able to provide faithful and sound reasoning trajectories grounded on the KGs. Experiments across various KGQA tasks with different background KGs demonstrate that DoG achieves superior and robust performance. DoG also shows general applicability with various open-source LLMs.</abstract>
      <url hash="d73fe50a">2025.acl-long.1186</url>
      <bibkey>li-etal-2025-decoding-graphs</bibkey>
    </paper>
    <paper id="1187">
      <title><fixed-case>ST</fixed-case>a<fixed-case>R</fixed-case>-<fixed-case>SQL</fixed-case>: Self-Taught Reasoner for Text-to-<fixed-case>SQL</fixed-case></title>
      <author><first>Mingqian</first><last>He</last><affiliation>Zhejiang University and Peking University</affiliation></author>
      <author><first>Yongliang</first><last>Shen</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Wenqi</first><last>Zhang</last></author>
      <author><first>Qiuying</first><last>Peng</last><affiliation>OPPO Research Institute</affiliation></author>
      <author><first>Jun</first><last>Wang</last><affiliation>OPPO Research Institute</affiliation></author>
      <author><first>Weiming</first><last>Lu</last><affiliation>Zhejiang University</affiliation></author>
      <pages>24365-24375</pages>
      <abstract>Generating step-by-step “chain-of-thought” rationales has proven effective for improving the performance of large language models on complex reasoning tasks. However, applying such techniques to structured tasks, such as text-to-SQL, remains largely unexplored. In this paper, we introduce Self-Taught Reasoner for text-to-SQL (STaR-SQL), a novel approach that reframes SQL query generation as a reasoning-driven process. Our method prompts the LLM to produce detailed reasoning steps for SQL queries and fine-tunes it on rationales that lead to correct outcomes. Unlike traditional methods, STaR-SQL dedicates additional test-time computation to reasoning, thereby positioning LLMs as spontaneous reasoners rather than mere prompt-based agents. To further scale the inference process, we incorporate an outcome-supervised reward model (ORM) as a verifier, which enhances SQL query accuracy. Experimental results on the challenging Spider benchmark demonstrate that STaR-SQL significantly improves text-to-SQL performance, achieving an execution accuracy of 86.6%. This surpasses a few-shot baseline by 31.6% and a baseline fine-tuned to predict answers directly by 18.0%. Additionally, STaR-SQL outperforms agent-like prompting methods that leverage more powerful yet closed-source models such as GPT-4. These findings underscore the potential of reasoning-augmented training for structured tasks and open the door to extending self-improving reasoning models to text-to-SQL generation and beyond.</abstract>
      <url hash="6a923e77">2025.acl-long.1187</url>
      <bibkey>he-etal-2025-star</bibkey>
    </paper>
    <paper id="1188">
      <title>Fairness Beyond Performance: Revealing Reliability Disparities Across Groups in Legal <fixed-case>NLP</fixed-case></title>
      <author><first>Santosh</first><last>T.y.s.s</last></author>
      <author><first>Irtiza</first><last>Chowdhury</last><affiliation>Technische Universität München</affiliation></author>
      <pages>24376-24390</pages>
      <abstract>Fairness in NLP must extend beyond performance parity to encompass equitable reliability across groups. This study exposes a criticalblind spot: models often make less reliable or overconfident predictions for marginalized groups, even when overall performance appearsfair. Using the FairLex benchmark as a case study in legal NLP, we systematically evaluate both performance and reliability dispari-ties across demographic, regional, and legal attributes spanning four jurisdictions. We show that domain-specific pre-training consistentlyimproves both performance and reliability, especially for underrepresented groups. However, common bias mitigation methods frequentlyworsen reliability disparities, revealing a trade-off not captured by performance metrics alone. Our results call for a rethinking of fairnessin high-stakes NLP: To ensure equitable treatment, models must not only be accurate, but also reliably self-aware across all groups.</abstract>
      <url hash="03694915">2025.acl-long.1188</url>
      <bibkey>t-y-s-s-chowdhury-2025-fairness</bibkey>
    </paper>
    <paper id="1189">
      <title>Beyond Similarity: A Gradient-based Graph Method for Instruction Tuning Data Selection</title>
      <author><first>Yang</first><last>Zhao</last></author>
      <author><first>Li</first><last>Du</last><affiliation>BAAI</affiliation></author>
      <author><first>Xiao</first><last>Ding</last></author>
      <author><first>Yangou</first><last>Ouyang</last></author>
      <author><first>Hepeng</first><last>Wang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Kai</first><last>Xiong</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Jinglong</first><last>Gao</last><affiliation>Research Center for Social Computing and Information Retrieval</affiliation></author>
      <author><first>Zhouhao</first><last>Sun</last></author>
      <author><first>Dongliang</first><last>Xu</last></author>
      <author><first>Qing</first><last>Yang</last></author>
      <author><first>Dongchen</first><last>Li</last></author>
      <author><first>Bing</first><last>Qin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Ting</first><last>Liu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>24391-24404</pages>
      <abstract>Large language models (LLMs) have shown great potential across various industries due to their remarkable ability to generalize through instruction tuning. However, the limited availability of domain-specific data significantly hampers their performance on specialized tasks. While existing methods primarily focus on selecting training data from general datasets that are similar to the target domain, they often fail to consider the joint distribution of instructions, resulting in inefficient learning and suboptimal knowledge transfer. To address these challenges, we introduce **G2IS** (**G**radient-based **G**raph **I**nstruction **S**election), a novel method that constructs a mixed gradient-based instruction graph to capture the joint distribution and interdependencies among instructions. By accounting for the relationships between instructions, G2IS improves domain adaptation efficiency. Additionally, we propose a gradient walk algorithm to refine the data selection process, enhancing both training effectiveness and efficiency. Our experiments demonstrate that G2IS outperforms traditional methods across various domain adaptation tasks, yielding significant performance gains, particularly in complex, data-scarce scenarios. These results underscore the potential of G2IS in advancing the development of large, domain-specific models.</abstract>
      <url hash="ecb3dd2f">2025.acl-long.1189</url>
      <bibkey>zhao-etal-2025-beyond-similarity</bibkey>
    </paper>
    <paper id="1190">
      <title><fixed-case>F</fixed-case>ast<fixed-case>MCTS</fixed-case>: A Simple Sampling Strategy for Data Synthesis</title>
      <author><first>Peiji</first><last>Li</last></author>
      <author><first>Kai</first><last>Lv</last></author>
      <author><first>Yunfan</first><last>Shao</last></author>
      <author><first>Yichuan</first><last>Ma</last><affiliation>Fudan University</affiliation></author>
      <author><first>Linyang</first><last>Li</last></author>
      <author><first>Xiaoqing</first><last>Zheng</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xipeng</first><last>Qiu</last><affiliation>Fudan University</affiliation></author>
      <author><first>Qipeng</first><last>Guo</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <pages>24405-24422</pages>
      <abstract>Synthetic high-quality multi-step reasoning data can significantly enhance the performance of large language models on various tasks. However, most existing methods rely on rejection sampling, which generates trajectories independently and suffers from inefficiency and imbalanced sampling across problems of varying difficulty. In this work, we introduce FastMCTS, an innovative data synthesis strategy inspired by Monte Carlo Tree Search. FastMCTS provides a more efficient sampling method for multi-step reasoning data, offering step-level evaluation signals and promoting balanced sampling across problems of different difficulty levels. Experiments on both English and Chinese reasoning datasets demonstrate that FastMCTS generates over 30% more correct reasoning paths compared to rejection sampling as the number of generated tokens scales up. Furthermore, under comparable synthetic data budgets, models trained on FastMCTS-generated data outperform those trained on rejection sampling data by 3.9% across multiple benchmarks. As a lightweight sampling strategy, FastMCTS offers a practical and efficient alternative for synthesizing high-quality reasoning data.</abstract>
      <url hash="f67245a0">2025.acl-long.1190</url>
      <bibkey>li-etal-2025-fastmcts</bibkey>
    </paper>
    <paper id="1191">
      <title>Dialogue-<fixed-case>RAG</fixed-case>: Enhancing Retrieval for <fixed-case>LLM</fixed-case>s via Node-Linking Utterance Rewriting</title>
      <author><first>Qiwei</first><last>Li</last></author>
      <author><first>Teng</first><last>Xiao</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Zuchao</first><last>Li</last></author>
      <author><first>Ping</first><last>Wang</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Mengjia</first><last>Shen</last></author>
      <author><first>Hai</first><last>Zhao</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>24423-24438</pages>
      <abstract>Large Language Models (LLMs) and Retrieval Augmented Generation (RAG) methods have demonstrated significant potential on tasks across multiple domains. However, ellipses and coreferences, as common phenomena in dialogue scenes, pose challenges to LLMs’ understanding and RAG’s retrieval accuracy. The previous works ignore the negative impact of this fuzzy data on RAG system.We explore the capabilities of LLMs and RAG systems in dialogue scenarios and use Incomplete Utterance Rewriting (IUR) to complete the key information in dialogue to enhance retrieval.Besides, we propose a lightweight IUR model for query rewriting. It is an end-to-end framework for node linking and iterative inference, incorporating two newly proposed probing semantic features derived from generative pre-training. This framework treats IUR as a series of link decisions on the input sequence and the incrementally constructed rewriting outputs.To test the performance of RAG system in the model multi-round dialogue scenario, we construct an RAG dialogue dataset on English and Chinese, Dialogue-RAG-MULTI-v1.0.Experiment results show that utterance rewriting can effectively improve the retrieval and generation ability of RAG system in dialogue scenes. Experiments on IUR tasks demonstrate the excellent performance of our lightweight IUR method.</abstract>
      <url hash="74bc70f0">2025.acl-long.1191</url>
      <bibkey>li-etal-2025-dialogue</bibkey>
    </paper>
    <paper id="1192">
      <title>Using Information Theory to Characterize Prosodic Typology: The Case of Tone, Pitch-Accent and Stress-Accent</title>
      <author><first>Ethan</first><last>Wilcox</last><affiliation>ETHZ - ETH Zurich and Georgetown University</affiliation></author>
      <author><first>Cui</first><last>Ding</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Giovanni</first><last>Acampa</last></author>
      <author><first>Tiago</first><last>Pimentel</last><affiliation>Department of Computer Science, ETHZ - ETH Zurich</affiliation></author>
      <author><first>Alex</first><last>Warstadt</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Tamar I</first><last>Regev</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <pages>24439-24451</pages>
      <abstract>This paper argues that the relationship between lexical identity and prosody—one well-studied parameter of linguistic variation—can be characterized using information theory. We predict that languages that use prosody to make lexical distinctions should exhibit a higher mutual information between word identity and prosody, compared to languages that don’t. We test this hypothesis in the domain of pitch, which is used to make lexical distinctions in tonal languages, like Cantonese. We use a dataset of speakers reading sentences aloud in ten languages across five language families to estimate the mutual information between the text and their pitch curves. We find that, across languages, pitch curves display similar amounts of entropy. However, these curves are easier to predict given their associated text in the tonal languages, compared to pitch- and stress-accent languages, and thus the mutual information is higher in these languages, supporting our hypothesis. Our results support perspectives that view linguistic typology as gradient, rather than categorical.</abstract>
      <url hash="de7f8442">2025.acl-long.1192</url>
      <bibkey>wilcox-etal-2025-using</bibkey>
    </paper>
    <paper id="1193">
      <title>Evaluating <fixed-case>LLM</fixed-case>s for <fixed-case>P</fixed-case>ortuguese Sentence Simplification with Linguistic Insights</title>
      <author><first>Arthur Mariano Rocha De Azevedo</first><last>Scalercio</last><affiliation>Universidade Federal Fluminense</affiliation></author>
      <author><first>Elvis A. De</first><last>Souza</last></author>
      <author><first>Maria José Bocorny</first><last>Finatto</last><affiliation>Universidade Federal do Rio Grande do Sul</affiliation></author>
      <author><first>Aline</first><last>Paes</last><affiliation>Universidade Federal Fluminense</affiliation></author>
      <pages>24452-24477</pages>
      <abstract>Sentence simplification (SS) focuses on adapting sentences to enhance their readability and accessibility. While large language models (LLMs) match task-specific baselines in English SS, their performance in Portuguese remains underexplored. This paper presents a comprehensive performance comparison of 26 state-of-the-art LLMs in Portuguese SS, alongside two simplification models trained explicitly for this task and language. They are evaluated under a one-shot setting across scientific, news, and government datasets. We benchmark the models with our newly introduced Gov-Lang-BR corpus (1,703 complex-simple sentence pairs from Brazilian government agencies) and two established datasets: PorSimplesSent and Museum-PT. Our investigation takes advantage of both automatic metrics and large-scale linguistic analysis to examine the transformations achieved by the LLMs. Furthermore, a qualitative assessment of selected generated outputs provides deeper insights into simplification quality. Our findings reveal that while open-source LLMs have achieved impressive results, closed-source LLMs continue to outperform them in Portuguese SS.</abstract>
      <url hash="21191fec">2025.acl-long.1193</url>
      <bibkey>scalercio-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="1194">
      <title><fixed-case>L</fixed-case>a<fixed-case>TIM</fixed-case>: Measuring Latent Token-to-Token Interactions in Mamba Models</title>
      <author><first>Hugo</first><last>Pitorro</last><affiliation>Instituto de Telecomunicações, Portugal</affiliation></author>
      <author><first>Marcos Vinicius</first><last>Treviso</last></author>
      <pages>24478-24493</pages>
      <abstract>State space models (SSMs), such as Mamba, have emerged as an efficient alternative to transformers for long-context sequence modeling. However, despite their growing adoption, SSMs lack the interpretability tools that have been crucial for understanding and improving attention-based architectures. While recent efforts provide insights into Mamba’s internal mechanisms, they struggle to capture precisetoken-level interactions at the layer level, leaving gaps in understanding how Mamba selectively processes sequences across layers. In this work, we introduce LaTIM, a novel token-level decomposition method for both Mamba-1 and Mamba-2 that enables fine-grained interpretability. We extensively evaluate our method across diverse tasks, including machine translation, copying, and retrieval-based generation, demonstrating its effectiveness in revealing Mamba’s token-to-token interaction patterns.</abstract>
      <url hash="baabae40">2025.acl-long.1194</url>
      <bibkey>pitorro-treviso-2025-latim</bibkey>
    </paper>
    <paper id="1195">
      <title>Improving Low-Resource Morphological Inflection via Self-Supervised Objectives</title>
      <author><first>Adam</first><last>Wiemerslage</last></author>
      <author><first>Katharina Von Der</first><last>Wense</last><affiliation>Johannes-Gutenberg Universität Mainz, Johannes-Gutenberg Universität Mainz, University of Colorado, Boulder and New York University</affiliation></author>
      <pages>24494-24510</pages>
      <abstract>Self-supervised objectives have driven major advances in NLP by leveraging large-scale unlabeled data, but such resources are scarce for many of the world’s languages. Surprisingly, they have not been explored much for character-level tasks, where smaller amounts of data have the potential to be beneficial. We investigate the effectiveness of self-supervised auxiliary tasks for morphological inflection – a character-level task highly relevant for language documentation – in extremely low-resource settings, training encoder-decoder transformers for 19 languages and 13 auxiliary objectives. Autoencoding yields the best performance when unlabeled data is very limited, while character masked language modeling (CMLM) becomes more effective as data availability increases. Though objectives with stronger inductive biases influence model predictions intuitively, they rarely outperform standard CMLM. However, sampling masks based on known morpheme boundaries consistently improves performance, highlighting a promising direction for low-resource morphological modeling.</abstract>
      <url hash="67b6e358">2025.acl-long.1195</url>
      <bibkey>wiemerslage-wense-2025-improving</bibkey>
    </paper>
    <paper id="1196">
      <title>Don’t Reinvent the Wheel: Efficient Instruction-Following Text Embedding based on Guided Space Transformation</title>
      <author><first>Yingchaojie</first><last>Feng</last></author>
      <author><first>Yiqun</first><last>Sun</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Yandong</first><last>Sun</last></author>
      <author><first>Minfeng</first><last>Zhu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Qiang</first><last>Huang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Anthony Kum Hoe</first><last>Tung</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Wei</first><last>Chen</last><affiliation>State key laboratory of CAD&amp;CG, Zhejiang University</affiliation></author>
      <pages>24511-24525</pages>
      <abstract>In this work, we investigate an important task named instruction-following text embedding, which generates dynamic text embeddings that adapt to user instructions, highlighting specific attributes of text. Despite recent advancements, existing approaches suffer from significant computational overhead, as they require re-encoding the entire corpus for each new instruction. To address this challenge, we propose GSTransform, a novel instruction-following text embedding framework based on Guided Space Transformation. Our key observation is that instruction-relevant information is inherently encoded in generic embeddings but remains underutilized. Instead of repeatedly encoding the corpus for each instruction, GSTransform is a lightweight transformation mechanism that adapts pre-computed embeddings in real time to align with user instructions, guided by a small amount of text data with instruction-focused label annotation. We conduct extensive experiments on three instruction-awareness downstream tasks across nine real-world datasets, demonstrating that GSTransform improves instruction-following text embedding quality over state-of-the-art methods while achieving dramatic speedups of 6~300<tex-math>\times</tex-math> in real-time processing on large-scale datasets. The source code is available at https://github.com/YingchaojieFeng/GSTransform.</abstract>
      <url hash="9daee755">2025.acl-long.1196</url>
      <bibkey>feng-etal-2025-dont</bibkey>
    </paper>
    <paper id="1197">
      <title><fixed-case>BOOKCOREF</fixed-case>: Coreference Resolution at Book Scale</title>
      <author><first>Giuliano</first><last>Martinelli</last><affiliation>University of Roma “La Sapienza”</affiliation></author>
      <author><first>Tommaso</first><last>Bonomo</last></author>
      <author><first>Pere-Lluís</first><last>Huguet Cabot</last><affiliation>Facebook</affiliation></author>
      <author><first>Roberto</first><last>Navigli</last><affiliation>Sapienza University of Rome</affiliation></author>
      <pages>24526-24544</pages>
      <abstract>Coreference Resolution systems are typically evaluated on benchmarks containing small- to medium-scale documents.When it comes to evaluating long texts, however, existing benchmarks, such as LitBank, remain limited in length and do not adequately assess system capabilities at the book scale, i.e., when co-referring mentions span hundreds of thousands of tokens.To fill this gap, we first put forward a novel automatic pipeline that produces high-quality Coreference Resolution annotations on full narrative texts. Then, we adopt this pipeline to create the first book-scale coreference benchmark, BOOKCOREF, with an average document length of more than 200,000 tokens.We carry out a series of experiments showing the robustness of our automatic procedure and demonstrating the value of our resource, which enables current long-document coreference systems to gain up to +20 CoNLL-F1 points when evaluated on full books.Moreover, we report on the new challenges introduced by this unprecedented book-scale setting, highlighting that current models fail to deliver the same performance they achieve on smaller documents.We release our data and code to encourage research and development of new book-scale Coreference Resolution systems at https://github.com/sapienzanlp/bookcoref.</abstract>
      <url hash="89795c00">2025.acl-long.1197</url>
      <bibkey>martinelli-etal-2025-bookcoref</bibkey>
    </paper>
    <paper id="1198">
      <title><fixed-case>OMGM</fixed-case>: Orchestrate Multiple Granularities and Modalities for Efficient Multimodal Retrieval</title>
      <author><first>Wei</first><last>Yang</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Jingjing</first><last>Fu</last></author>
      <author><first>Rui</first><last>Wang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Jinyu</first><last>Wang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Lei</first><last>Song</last><affiliation>Microsoft</affiliation></author>
      <author><first>Jiang</first><last>Bian</last><affiliation>Microsoft</affiliation></author>
      <pages>24545-24563</pages>
      <abstract>Vision-language retrieval-augmented generation (RAG) has become an effective approach for tackling Knowledge-Based Visual Question Answering (KB-VQA), which requires external knowledge beyond the visual content presented in images. The effectiveness of Vision-language RAG systems hinges on multimodal retrieval, which is inherently challenging due to the diverse modalities and knowledge granularities in both queries and knowledge bases. Existing methods have not fully tapped into the potential interplay between these elements. We propose a multimodal RAG system featuring a coarse-to-fine, multi-step retrieval that harmonizes multiple granularities and modalities to enhance efficacy. Our system begins with a broad initial search aligning knowledge granularity for cross-modal retrieval, followed by a multimodal fusion reranking to capture the nuanced multimodal information for top entity selection. A text reranker then filters out the most relevant fine-grained section for augmented generation. Extensive experiments on the InfoSeek and Encyclopedic-VQA benchmarks show our method achieves state-of-the-art retrieval performance and highly competitive answering results, underscoring its effectiveness in advancing KB-VQA systems. Our code can be found at https://github.com/ChaoLinAViy/OMGM.</abstract>
      <url hash="722164ea">2025.acl-long.1198</url>
      <bibkey>yang-etal-2025-omgm</bibkey>
    </paper>
    <paper id="1199">
      <title>Alleviating Hallucinations from Knowledge Misalignment in Large Language Models via Selective Abstention Learning</title>
      <author><first>Lei</first><last>Huang</last></author>
      <author><first>Xiaocheng</first><last>Feng</last></author>
      <author><first>Weitao</first><last>Ma</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Yuchun</first><last>Fan</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Xiachong</first><last>Feng</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Yuxuan</first><last>Gu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Yangfan</first><last>Ye</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Liang</first><last>Zhao</last></author>
      <author><first>Weihong</first><last>Zhong</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Baoxin</first><last>Wang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Dayong</first><last>Wu</last></author>
      <author><first>Guoping</first><last>Hu</last><affiliation>IFLYTEK CO.LTD.</affiliation></author>
      <author><first>Lingpeng</first><last>Kong</last><affiliation>Department of Computer Science, The University of Hong Kong</affiliation></author>
      <author><first>Tong</first><last>Xiao</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Ting</first><last>Liu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Bing</first><last>Qin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>24564-24579</pages>
      <abstract>Large language models (LLMs) are known to suffer from severe hallucination issues. One of the main causes lies in the knowledge misalignment between the pre-training stage and the supervised fine-tuning stage. The unfamiliar knowledge encountered during fine-tuning may encourage LLMs to generate facts that are not grounded in parametric knowledge. To address this, we propose Seal, a novel training objective with an abstention mechanism, in which the model learns to selectively reject tokens that misalign with the desired knowledge distribution via a special <tex-math>\texttt{[REJ]}</tex-math> token. This allows the model the option of acknowledging the insufficiency of knowledge rather than blindly assigning high probability to all ground-truth answers. We further propose a regularized decoding objective that penalizes uncertain predictions during inference by using the <tex-math>\texttt{[REJ]}</tex-math> probability learned during training. Extensive experiments on six short-form and long-form QA datasets with three LLMs of different sizes demonstrate that our method effectively alleviates hallucinations caused by knowledge misalignment. Further analysis highlights the adaptations of our method in answer refusal scenarios and its ability to effectively maintain the model’s instruction-following capabilities.</abstract>
      <url hash="3736de2f">2025.acl-long.1199</url>
      <bibkey>huang-etal-2025-alleviating</bibkey>
    </paper>
    <paper id="1200">
      <title>Retrospective Learning from Interactions</title>
      <author><first>Zizhao</first><last>Chen</last><affiliation>Department of Computer Science, Cornell University</affiliation></author>
      <author><first>Mustafa Omer</first><last>Gul</last><affiliation>Department of Computer Science, Cornell University</affiliation></author>
      <author><first>Yiwei</first><last>Chen</last></author>
      <author><first>Gloria</first><last>Geng</last></author>
      <author><first>Anne</first><last>Wu</last><affiliation>Cornell University</affiliation></author>
      <author><first>Yoav</first><last>Artzi</last><affiliation>Cornell University and ASAPP</affiliation></author>
      <pages>24580-24606</pages>
      <abstract>Multi-turn interactions between large language models (LLMs) and users naturally include implicit feedback signals. If an LLM responds in an unexpected way to an instruction, the user is likely to signal it by rephrasing the request, expressing frustration, or pivoting to an alternative task. Such signals are task-independent and occupy a relatively constrained subspace of language, allowing the LLM to identify them even if it fails on the actual task. We introduce ReSpect, a method to learn from such signals in past interactions via retrospection without additional annotations. We deploy ReSpect in a new multimodal interaction scenario, where humans instruct a multimodal LLM to solve an abstract reasoning task with a combinatorial solution space. Through thousands of interactions with humans, we show how ReSpect gradually improves task completion rate from 31% to 82%, all without any external annotation.</abstract>
      <url hash="5852b9d2">2025.acl-long.1200</url>
      <bibkey>chen-etal-2025-retrospective</bibkey>
    </paper>
    <paper id="1201">
      <title>Personalized Generation In Large Model Era: A Survey</title>
      <author><first>Yiyan</first><last>Xu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Jinghao</first><last>Zhang</last></author>
      <author><first>Alireza</first><last>Salemi</last></author>
      <author><first>Xinting</first><last>Hu</last><affiliation>Saarland Informatics Campus, Max-Planck Institute</affiliation></author>
      <author><first>Wenjie</first><last>Wang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Fuli</first><last>Feng</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Hamed</first><last>Zamani</last><affiliation>University of Massachusetts at Amherst</affiliation></author>
      <author><first>Xiangnan</first><last>He</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Tat-Seng</first><last>Chua</last><affiliation>National University of Singapore</affiliation></author>
      <pages>24607-24649</pages>
      <abstract>In the era of large models, content generation is gradually shifting to Personalized Generation (PGen), tailoring content to individual preferences and needs. This paper presents the first comprehensive survey on PGen, investigating existing research in this rapidly growing field. We conceptualize PGen from a unified perspective, systematically formalizing its key components, core objectives, and abstract workflows. Based on this unified perspective, we propose a multi-level taxonomy, offering an in-depth review of technical advancements, commonly used datasets, and evaluation metrics across multiple modalities, personalized contexts, and tasks. Moreover, we envision the potential applications of PGen and highlight open challenges and promising directions for future exploration. By bridging PGen research across multiple modalities, this survey serves as a valuable resource for fostering knowledge sharing and interdisciplinary collaboration, ultimately contributing to a more personalized digital landscape.</abstract>
      <url hash="de5f151b">2025.acl-long.1201</url>
      <bibkey>xu-etal-2025-personalized</bibkey>
    </paper>
    <paper id="1202">
      <title>Graph Counselor: Adaptive Graph Exploration via Multi-Agent Synergy to Enhance <fixed-case>LLM</fixed-case> Reasoning</title>
      <author><first>Junqi</first><last>Gao</last></author>
      <author><first>Xiang</first><last>Zou</last></author>
      <author><first>Ying</first><last>Ai</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Dong</first><last>Li</last></author>
      <author><first>Yichen</first><last>Niu</last></author>
      <author><first>Biqing</first><last>Qi</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Jianxing</first><last>Liu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>24650-24668</pages>
      <abstract>Graph Retrieval Augmented Generation (GraphRAG) effectively enhances external knowledge integration capabilities by explicitly modeling knowledge relationships, thereby improving the factual accuracy and generation quality of Large Language Models (LLMs) in specialized domains. However, existing methods suffer from two inherent limitations: 1) Inefficient Information Aggregation: They rely on a single agent and fixed iterative patterns, making it difficult to adaptively capture multi-level textual, structural, and degree information within graph data. 2) Rigid Reasoning Mechanism: They employ preset reasoning schemes, which cannot dynamically adjust reasoning depth nor achieve precise semantic correction. To overcome these limitations, we propose Graph Counselor, an GraphRAG method based on multi-agent collaboration. This method uses the Adaptive Graph Information Extraction Module (AGIEM), where Planning, Thought, and Execution Agents work together to precisely model complex graph structures and dynamically adjust information extraction strategies, addressing the challenges of multi-level dependency modeling and adaptive reasoning depth. Additionally, the Self-Reflection with Multiple Perspectives (SR) module improves the accuracy and semantic consistency of reasoning results through self-reflection and backward reasoning mechanisms. Experiments demonstrate that Graph Counselor outperforms existing methods in multiple graph reasoning tasks, exhibiting higher reasoning accuracy and generalization ability.Our code is available at https://github.com/gjq100/Graph-Counselor.git.</abstract>
      <url hash="db50d9cf">2025.acl-long.1202</url>
      <bibkey>gao-etal-2025-graph</bibkey>
    </paper>
    <paper id="1203">
      <title><fixed-case>SOTOPIA</fixed-case>-: Dynamic Strategy Injection Learning and Social Instruction Following Evaluation for Social Agents</title>
      <author><first>Wenyuan</first><last>Zhang</last></author>
      <author><first>Tianyun</first><last>Liu</last></author>
      <author><first>Mengxiao</first><last>Song</last></author>
      <author><first>Xiaodong</first><last>Li</last></author>
      <author><first>Tingwen</first><last>Liu</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <pages>24669-24697</pages>
      <abstract>Despite the abundance of prior social strategies possessed by humans, there remains a paucity of research dedicated to their transfer and integration into social agents. Our proposed SOTOPIA-Ω framework aims to address and bridge this gap, with a particular focus on enhancing the social capabilities of language agents. This framework dynamically injects a variety of social strategies into expert agents, thereby automating the construction of high-quality social dialogue training corpus. Additionally, we introduce the concept of Social Instruction Following (S-IF) and propose two new S-IF evaluation metrics that are complementary to social capability. We demonstrate that several 7B models trained on high-quality corpus not only significantly surpasses the expert agent (GPT-4) in achieving social goals but also enhances S-IF performance. Analysis and variant experiments validate the advantages of dynamic construction, which can especially break the agent’s prolonged deadlock.</abstract>
      <url hash="fef0192f">2025.acl-long.1203</url>
      <bibkey>zhang-etal-2025-sotopia</bibkey>
    </paper>
    <paper id="1204">
      <title>Can Language Models Replace Programmers for Coding? <fixed-case>REPOCOD</fixed-case> Says ‘Not Yet’</title>
      <author><first>Shanchao</first><last>Liang</last><affiliation>Purdue University</affiliation></author>
      <author><first>Nan</first><last>Jiang</last></author>
      <author><first>Yiran</first><last>Hu</last></author>
      <author><first>Lin</first><last>Tan</last><affiliation>Purdue University</affiliation></author>
      <pages>24698-24717</pages>
      <abstract>Recently, a number of repository-level code generation benchmarks–such as CoderEval, DevEval, RepoEval, RepoBench, and LongCode-Arena–have emerged to evaluate the capabilities of large language models (LLMs) beyond standalone benchmarks like HumanEval and MBPP. Thus, a natural question is, would LLMs have similar performance in real world coding tasks as their performance in these benchmarks? Unfortunately, one cannot answer this question, since these benchmarks consist of short completions, synthetic examples, or focus on limited scale repositories, failing to represent real-world coding tasks.To address these challenges, we create RepoCod, a Python code-generation benchmark containing complex tasks with realistic dependencies in real-world large projects and appropriate metrics for evaluating source code. It includes 980 whole-function generation tasks from 11 popular projects, 50.8% of which require repository-level context. RepoCod includes 314 developer-written test cases per instance for better evaluation. We evaluate ten LLMs on RepoCod and find that none achieves more than 30% pass@1 on RepoCod, indicating the necessity of building stronger LLMs that can help developers in real-world software development. In addition, we found that retrieval-augmented generation achieves better results than using target function dependencies as context.</abstract>
      <url hash="bcfe24fb">2025.acl-long.1204</url>
      <bibkey>liang-etal-2025-language</bibkey>
    </paper>
    <paper id="1205">
      <title>Leveraging In-Context Learning for Political Bias Testing of <fixed-case>LLM</fixed-case>s</title>
      <author><first>Patrick</first><last>Haller</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Jannis</first><last>Vamvas</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Rico</first><last>Sennrich</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Lena Ann</first><last>Jäger</last><affiliation>University of Zurich</affiliation></author>
      <pages>24718-24738</pages>
      <abstract>A growing body of work has been querying LLMs with political questions to evaluate their potential biases. However, this probing method has limited stability, making comparisons between models unreliable. In this paper, we argue that LLMs need more context. We propose a new probing task, Questionnaire Modeling (QM), that uses human survey data as in-context examples. We show that QM improves the stability of question-based bias evaluation, and demonstrate that it may be used to compare instruction-tuned models to their base versions. Experiments with LLMs of various sizes indicate that instruction tuning can indeed change the direction of bias. Furthermore, we observe a trend that larger models are able to leverage in-context examples more effectively, and generally exhibit smaller bias scores in QM. Data and code are publicly available.</abstract>
      <url hash="cd26350f">2025.acl-long.1205</url>
      <bibkey>haller-etal-2025-leveraging</bibkey>
    </paper>
    <paper id="1206">
      <title><fixed-case>ACORD</fixed-case>: An Expert-Annotated Retrieval Dataset for Legal Contract Drafting</title>
      <author><first>Steven H</first><last>Wang</last></author>
      <author><first>Maksim</first><last>Zubkov</last></author>
      <author><first>Kexin</first><last>Fan</last></author>
      <author><first>Sarah</first><last>Harrell</last></author>
      <author><first>Yuyang</first><last>Sun</last></author>
      <author><first>Wei</first><last>Chen</last><affiliation>Berkeley Center for Law and Business</affiliation></author>
      <author><first>Andreas</first><last>Plesner</last></author>
      <author><first>Roger</first><last>Wattenhofer</last></author>
      <pages>24739-24762</pages>
      <abstract>Contract clause retrieval is foundational to contract drafting because lawyers rarely draft contracts from scratch; instead, they locate and revise the most relevant precedent clauses. We introduce the Atticus Clause Retrieval Dataset (ACORD), the first expert-annotated benchmark specifically designed for contract clause retrieval to support contract drafting tasks. ACORD focuses on complex contract clauses such as Limitation of Liability, Indemnification, Change of Control, and Most Favored Nation. It includes 114 queries and over 126,000 query-clause pairs, each ranked on a scale from 1 to 5 stars. The task is to find the most relevant precedent clauses to a query. The bi-encoder retriever paired with pointwise LLMs re-rankers shows promising results. However, substantial improvements are still needed to manage the complex legal work typically undertaken by lawyers effectively. As the first expert-annotated benchmark for contract clause retrieval, ACORD can serve as a valuable IR benchmark for the NLP community.</abstract>
      <url hash="220f998f">2025.acl-long.1206</url>
      <bibkey>wang-etal-2025-acord</bibkey>
    </paper>
    <paper id="1207">
      <title><fixed-case>LLM</fixed-case>s know their vulnerabilities: Uncover Safety Gaps through Natural Distribution Shifts</title>
      <author><first>Qibing</first><last>Ren</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Hao</first><last>Li</last><affiliation>Beijing University of Aeronautics and Astronautics</affiliation></author>
      <author><first>Dongrui</first><last>Liu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Zhanxu</first><last>Xie</last></author>
      <author><first>Xiaoya</first><last>Lu</last></author>
      <author><first>Yu</first><last>Qiao</last><affiliation>Shanghai Aritifcal Intelligence Laboratory</affiliation></author>
      <author><first>Lei</first><last>Sha</last><affiliation>Beihang University</affiliation></author>
      <author><first>Junchi</first><last>Yan</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Lizhuang</first><last>Ma</last><affiliation>Dept. of Computer Sci. &amp; Eng., Shanghai Jiao Tong University</affiliation></author>
      <author><first>Jing</first><last>Shao</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <pages>24763-24785</pages>
      <abstract>Safety concerns in large language models (LLMs) have gained significant attention due to their exposure to potentially harmful data during pre-training. In this paper, we identify a new safety vulnerability in LLMs: their susceptibility to <i>natural distribution shifts</i> between attack prompts and original toxic prompts, where seemingly benign prompts, semantically related to harmful content, can bypass safety mechanisms. To explore this issue, we introduce a novel attack method, <i>ActorBreaker</i>, which identifies actors related to toxic prompts within pre-training distribution to craft multi-turn prompts that gradually lead LLMs to reveal unsafe content. ActorBreaker is grounded in Latour’s actor-network theory, encompassing both human and non-human actors to capture a broader range of vulnerabilities. Our experimental results demonstrate that ActorBreaker outperforms existing attack methods in terms of diversity, effectiveness, and efficiency across aligned LLMs. To address this vulnerability, we propose expanding safety training to cover a broader semantic space of toxic content. We thus construct a multi-turn safety dataset using ActorBreaker. Fine-tuning models on our dataset shows significant improvements in robustness, though with some trade-offs in utility. Code is available at https://github.com/AI45Lab/ActorAttack.</abstract>
      <url hash="22837168">2025.acl-long.1207</url>
      <bibkey>ren-etal-2025-llms</bibkey>
    </paper>
    <paper id="1208">
      <title><fixed-case>WAFFLE</fixed-case>: Fine-tuning Multi-Modal Model for Automated Front-End Development</title>
      <author><first>Shanchao</first><last>Liang</last><affiliation>Purdue University</affiliation></author>
      <author><first>Nan</first><last>Jiang</last></author>
      <author><first>Shangshu</first><last>Qian</last><affiliation>Purdue University</affiliation></author>
      <author><first>Lin</first><last>Tan</last><affiliation>Purdue University</affiliation></author>
      <pages>24786-24802</pages>
      <abstract>Web development involves turning UI designs into functional webpages, which can be difficult for both beginners and experienced developers due to the complexity of HTML’s hierarchical structures and styles. While Large Language Models (LLMs) have shown promise in generating source code, two major challenges persist in UI-to-HTML code generation: (1) effectively representing HTML’s hierarchical structure for LLMs, and (2) bridging the gap between the visual nature of UI designs and the text-based format of HTML code. To tackle these challenges, we introduce Waffle, a new fine-tuning strategy that uses a structure-aware attention mechanism to improve LLMs’ understanding of HTML’s structure and a contrastive fine-tuning approach to align LLMs’ understanding of UI images and HTML code. Models fine-tuned with Waffle show up to 9.00 pp (percentage point) higher HTML match, 0.0982 higher CW-SSIM, 32.99 higher CLIP, and 27.12 pp higher LLEM on our new benchmark WebSight-Test and an existing benchmark Design2Code, outperforming current fine-tuning methods.</abstract>
      <url hash="b0d98cbd">2025.acl-long.1208</url>
      <bibkey>liang-etal-2025-waffle</bibkey>
    </paper>
    <paper id="1209">
      <title>Math Neurosurgery: Isolating Language Models’ Math Reasoning Abilities Using Only Forward Passes</title>
      <author><first>Bryan R</first><last>Christ</last></author>
      <author><first>Zachary</first><last>Gottesman</last><affiliation>University of Virginia, Charlottesville</affiliation></author>
      <author><first>Jonathan</first><last>Kropko</last><affiliation>University of Virginia, Charlottesville</affiliation></author>
      <author><first>Thomas</first><last>Hartvigsen</last><affiliation>University of Virginia, Charlottesville</affiliation></author>
      <pages>24803-24840</pages>
      <abstract>Math reasoning is an active area of Large Language Model (LLM) research because it is a hallmark of artificial intelligence and has implications in several domains, including math education. However, few works have explored how math reasoning is encoded within LLM parameters and if it is a skill that can be isolated within models. Doing so could allow targeted intervention to improve math performance without altering non-math behavior and foster understanding of how models encode math reasoning. We introduce Math Neurosurgery (MathNeuro), a computationally efficient method we use to isolate math-specific parameters in LLMs using only forward passes. MathNeuro builds on existing work by using weights and activations to calculate parameter importance, but isolates math-specific parameters by filtering out those important for general language tasks. Through pruning parameters MathNeuro identifies, we delete a LLM’s math reasoning ability without significantly impacting its general language ability. Scaling the identified parameters by a small constant improves a pretrained or instruction-tuned LLM’s performance by 4-17% on GSM8K and 5-35% on MATH while leaving non-math behavior unaltered. MathNeuro is also data efficient: most of its effectiveness holds when identifying math-specific parameters using a single sample. MathNeuro highlights the potential for future work to intervene on math-specific parameters.</abstract>
      <url hash="1b43af21">2025.acl-long.1209</url>
      <bibkey>christ-etal-2025-math</bibkey>
    </paper>
    <paper id="1210">
      <title>Multiple <fixed-case>LLM</fixed-case> Agents Debate for Equitable Cultural Alignment</title>
      <author><first>Dayeon</first><last>Ki</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Rachel</first><last>Rudinger</last></author>
      <author><first>Tianyi</first><last>Zhou</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Marine</first><last>Carpuat</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>24841-24877</pages>
      <abstract>Large Language Models (LLMs) need to adapt their predictions to diverse cultural contexts to benefit diverse communities across the world. While previous efforts have focused on single-LLM, single-turn approaches, we propose to exploit the complementary strengths of multiple LLMs to promote cultural adaptability. We introduce a Multi-Agent Debate framework, where two LLM-based agents debate over a cultural scenario and collaboratively reach a final decision. We propose two variants: one where either LLM agents exclusively debate and another where they dynamically choose between self-reflection and debate during their turns. We evaluate these approaches on 7 open-weight LLMs (and 21 LLM combinations) using the NormAd-ETI benchmark for social etiquette norms in 75 countries. Experiments show that debate improves both overall accuracy and cultural group parity over single-LLM baselines. Notably, multi-agent debate enables relatively small LLMs (7-9B) to achieve accuracies comparable to that of a much larger model (27B parameters).</abstract>
      <url hash="f3f2b7aa">2025.acl-long.1210</url>
      <bibkey>ki-etal-2025-multiple</bibkey>
    </paper>
    <paper id="1211">
      <title><fixed-case>R</fixed-case>efresh<fixed-case>KV</fixed-case>: Updating Small <fixed-case>KV</fixed-case> Cache During Long-form Generation</title>
      <author><first>Fangyuan</first><last>Xu</last><affiliation>New York University</affiliation></author>
      <author><first>Tanya</first><last>Goyal</last><affiliation>Cornell University</affiliation></author>
      <author><first>Eunsol</first><last>Choi</last><affiliation>New York University</affiliation></author>
      <pages>24878-24893</pages>
      <abstract>Generating long sequences of tokens given a long-context input is a very compute-intensive inference scenario for large language models (LLMs). One prominent inference speed-up approach is constructing a smaller key-value (KV) cache, relieving LLMs from computing attention over a long sequence of tokens. While such methods work well to generate short sequences, their performance degrades rapidly for long-form generation. Most KV compression happens once, prematurely removing tokens that can be useful later in the generation. We propose a new inference-time method, RefreshKV, that flexibly alternates between full context attention and attention over a subset of input tokens during generation. After each full attention step, we update the smaller KV cache based on the attention pattern over the entire input. Applying our method to off-the-shelf LLMs achieves comparable speedup to eviction-based methods while improving performance for various long-form generation tasks. Lastly, we show that continued pretraining with our inference setting brings further gains in performance.</abstract>
      <url hash="b355638d">2025.acl-long.1211</url>
      <bibkey>xu-etal-2025-refreshkv</bibkey>
    </paper>
    <paper id="1212">
      <title><fixed-case>SEA</fixed-case>: Low-Resource Safety Alignment for Multimodal Large Language Models via Synthetic Embeddings</title>
      <author><first>Weikai</first><last>Lu</last></author>
      <author><first>Hao</first><last>Peng</last></author>
      <author><first>Huiping</first><last>Zhuang</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Cen</first><last>Chen</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Ziqian</first><last>Zeng</last><affiliation>South China University of Technology</affiliation></author>
      <pages>24894-24913</pages>
      <abstract>Multimodal Large Language Models (MLLMs) have serious security vulnerabilities. While safety alignment using multimodal datasets consisting of text and data of additional modalities can effectively enhance MLLM’s security, it is costly to construct these datasets. Existing low-resource security alignment methods, including textual alignment, have been found to struggle with the security risks posed by additional modalities. To address this, we propose Synthetic Embedding augmented safety Alignment (SEA), which optimizes embeddings of additional modality through gradient updates to expand textual datasets. This enables multimodal safety alignment training even when only textual data is available. Extensive experiments on image, video, and audio-based MLLMs demonstrate that SEA can synthesize a high-quality embedding on a single RTX3090 GPU within 24 seconds. SEA significantly improves the security of MLLMs when faced with threats from additional modalities. To assess the security risks introduced by video and audio, we also introduced a new benchmark called VA-SafetyBench. High attack success rates across multiple MLLMs validate its challenge. Our code and data will be available at https://github.com/ZeroNLP/SEA.</abstract>
      <url hash="bcabf2b8">2025.acl-long.1212</url>
      <bibkey>lu-etal-2025-sea</bibkey>
    </paper>
    <paper id="1213">
      <title>Chain-of-Reasoning: Towards Unified Mathematical Reasoning in Large Language Models via a Multi-Paradigm Perspective</title>
      <author><first>Yiyao</first><last>Yu</last></author>
      <author><first>Yuxiang</first><last>Zhang</last></author>
      <author><first>Dongdong</first><last>Zhang</last><affiliation>Microsoft Research Asia</affiliation></author>
      <author><first>Xiao</first><last>Liang</last></author>
      <author><first>Hengyuan</first><last>Zhang</last></author>
      <author><first>Xingxing</first><last>Zhang</last><affiliation>Microsoft Research Asia</affiliation></author>
      <author><first>Mahmoud</first><last>Khademi</last><affiliation>Microsoft</affiliation></author>
      <author><first>Hany Hassan</first><last>Awadalla</last><affiliation>Microsoft</affiliation></author>
      <author><first>Junjie</first><last>Wang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Yujiu</first><last>Yang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Furu</first><last>Wei</last><affiliation>Microsoft Research</affiliation></author>
      <pages>24914-24937</pages>
      <abstract>Large Language Models (LLMs) have made notable progress in mathematical reasoning, yet they often rely on single-paradigm reasoning that limits their effectiveness across diverse tasks. In this paper, we introduce Chain-of-Reasoning (CoR), a novel unified framework that integrates multiple reasoning paradigms — Natural Language Reasoning (NLR), Algorithmic Reasoning (AR), and Symbolic Reasoning (SR) — to enable synergistic collaboration. CoR generates multiple potential answers using different reasoning paradigms and synthesizes them into a coherent final solution. We propose a Progressive Paradigm Training (PPT) strategy that allows models to progressively master these paradigms, culminating in the development of at CoR-Math-7B. Experimental results demonstrate that CoR-Math-7B significantly outperforms current SOTA models, achieving up to a 41.0% absolute improvement over GPT-4o in theorem proving tasks and a 15% improvement over RL-based methods on the MATH benchmark in arithmetic tasks. These results show the enhanced mathematical comprehensive ability of our model, enabling zero-shot generalization across tasks.The code is available at https://github.com/microsoft/CoR.</abstract>
      <url hash="942c0833">2025.acl-long.1213</url>
      <bibkey>yu-etal-2025-chain</bibkey>
    </paper>
    <paper id="1214">
      <title>Language Models Grow Less Humanlike beyond Phase Transition</title>
      <author><first>Tatsuya</first><last>Aoyama</last></author>
      <author><first>Ethan</first><last>Wilcox</last><affiliation>ETHZ - ETH Zurich and Georgetown University</affiliation></author>
      <pages>24938-24958</pages>
      <abstract>LMs’ alignment with human reading behavior (i.e. psychometric predictive power; PPP) is known to improve during pretraining up to a tipping point, beyond which it either plateaus or degrades. Various factors, such as word frequency, recency bias in attention, and context size, have been theorized to affect PPP, yet there is no current account that explains why such a tipping point exists, and how it interacts with LMs’ pretraining dynamics more generally. We hypothesize that the underlying factor is a pretraining phase transition, characterized by the rapid emergence of specialized attention heads. We conduct a series of correlational and causal experiments to show that such a phase transition is responsible for the tipping point in PPP. We then show that, rather than producing attention patterns that contribute to the degradation in PPP, phase transitions alter the subsequent learning dynamics of the model, such that further training keeps damaging PPP.</abstract>
      <url hash="75abdafc">2025.acl-long.1214</url>
      <bibkey>aoyama-wilcox-2025-language</bibkey>
    </paper>
    <paper id="1215">
      <title><fixed-case>PC</fixed-case>o<fixed-case>T</fixed-case>: Persuasion-Augmented Chain of Thought for Detecting Fake News and Social Media Disinformation</title>
      <author><first>Arkadiusz</first><last>Modzelewski</last></author>
      <author><first>Witold</first><last>Sosnowski</last></author>
      <author><first>Tiziano</first><last>Labruna</last></author>
      <author><first>Adam</first><last>Wierzbicki</last><affiliation>Polish-Japanese Institute of Information Technology in Warsaw</affiliation></author>
      <author><first>Giovanni</first><last>Da San Martino</last><affiliation>University of Padua</affiliation></author>
      <pages>24959-24983</pages>
      <abstract>Disinformation detection is a key aspect of media literacy. Psychological studies have shown that knowledge of persuasive fallacies helps individuals detect disinformation. Inspired by these findings, we experimented with large language models (LLMs) to test whether infusing persuasion knowledge enhances disinformation detection. As a result, we introduce the Persuasion-Augmented Chain of Thought (PCoT), a novel approach that leverages persuasion to improve disinformation detection in zero-shot classification. We extensively evaluate PCoT on online news and social media posts. Moreover, we publish two novel, up-to-date disinformation datasets: EUDisinfo and MultiDis. These datasets enable the evaluation of PCoT on content entirely unseen by the LLMs used in our experiments, as the content was published after the models’ knowledge cutoffs. We show that, on average, PCoT outperforms competitive methods by 15% across five LLMs and five datasets. These findings highlight the value of persuasion in strengthening zero-shot disinformation detection.</abstract>
      <url hash="052b2f21">2025.acl-long.1215</url>
      <bibkey>modzelewski-etal-2025-pcot</bibkey>
    </paper>
    <paper id="1216">
      <title>Coordinating Chaos: A Structured Review of Linguistic Coordination Methodologies</title>
      <author><first>Benjamin Roger</first><last>Litterer</last></author>
      <author><first>David</first><last>Jurgens</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>Dallas</first><last>Card</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <pages>24984-24999</pages>
      <abstract>Linguistic coordination—a phenomenon where conversation partners end up having similar patterns of language use—has been established across a variety of contexts and for multiple linguistic features. However, the study of language coordination has been accompanied by a diverse and inconsistently applied set of measures and theoretical perspectives. This diversity has significant consequences, as replication studies have highlighted the brittleness of certain measures and called influential findings into question. While prior work has addressed specific modeling decisions and model types, linguistic coordination research has yet to fully examine, synthesize, and critique the space of modeling choices available. In this work, we present a framework to organize the linguistic coordination literature. Using this schema, we provide a high-level overview of the choices involved in the measurement process and synthesize relevant critiques. Based on both gaps and limitations surfaced from this review, we suggest directions for further exploration and evaluation. In doing so, we provide the clarity required for linguistic coordination research to arrive at interpretable and sound conclusions.</abstract>
      <url hash="98dec26f">2025.acl-long.1216</url>
      <bibkey>litterer-etal-2025-coordinating</bibkey>
    </paper>
    <paper id="1217">
      <title>i<fixed-case>N</fixed-case>ews: A Multimodal Dataset for Modeling Personalized Affective Responses to News</title>
      <author><first>Tiancheng</first><last>Hu</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Nigel</first><last>Collier</last><affiliation>University of Cambridge</affiliation></author>
      <pages>25000-25040</pages>
      <abstract>Understanding how individuals perceive and react to information is fundamental for advancing social and behavioral sciences and developing human-centered AI systems. Current approaches often lack the granular data needed to model these personalized responses, relying instead on aggregated labels that obscure the rich variability driven by individual differences. We introduce iNews, a novel large-scale dataset specifically designed to facilitate the modeling of personalized affective responses to news content. Our dataset comprises annotations from 291 demographically diverse UK participants across 2,899 multimodal Facebook news posts from major UK outlets, with an average of 5.18 annotators per sample. For each post, annotators provide multifaceted labels including valence, arousal, dominance, discrete emotions, content relevance judgments, sharing likelihood, and modality importance ratings. Crucially, we collect comprehensive annotator persona information covering demographics, personality, media trust, and consumption patterns, which explain 15.2% of annotation variance - substantially higher than existing NLP datasets. Incorporating this information yields a 7% accuracy gain in zero-shot prediction and remains beneficial even with 32-shot in-context learning.</abstract>
      <url hash="05034a74">2025.acl-long.1217</url>
      <bibkey>hu-collier-2025-inews</bibkey>
    </paper>
    <paper id="1218">
      <title>Mind the Gesture: Evaluating <fixed-case>AI</fixed-case> Sensitivity to Culturally Offensive Non-Verbal Gestures</title>
      <author><first>Akhila</first><last>Yerukola</last></author>
      <author><first>Saadia</first><last>Gabriel</last><affiliation>UCLA Computer Science Department, University of California, Los Angeles</affiliation></author>
      <author><first>Nanyun</first><last>Peng</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Maarten</first><last>Sap</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>25041-25080</pages>
      <abstract>Gestures are an integral part of non-verbal communication, with meanings that vary across cultures, and misinterpretations that can have serious social and diplomatic consequences. As AI systems become more integrated into global applications, ensuring they do not inadvertently perpetuate cultural offenses is critical. To this end, we introduce Multi-Cultural Set of Inappropriate Gestures and Nonverbal Signs (MC-SIGNS), a dataset of 288 gesture-country pairs annotated for offensiveness, cultural significance, and contextual factors across 25 gestures and 85 countries. Through systematic evaluation using MC-SIGNS, we uncover critical limitations: text-to-image (T2I) systems exhibit strong US-centric biases, performing better at detecting offensive gestures in US contexts than in non-US ones; large language models (LLMs) tend to over-flag gestures as offensive; and vision-language models (VLMs) default to US-based interpretations when responding to universal concepts like wishing someone luck, frequently suggesting culturally inappropriate gestures. These findings highlight the urgent need for culturally-aware AI safety mechanisms to ensure equitable global deployment of AI technologies.</abstract>
      <url hash="746572fa">2025.acl-long.1218</url>
      <bibkey>yerukola-etal-2025-mind</bibkey>
    </paper>
    <paper id="1219">
      <title>500x<fixed-case>C</fixed-case>ompressor: Generalized Prompt Compression for Large Language Models</title>
      <author><first>Zongqian</first><last>Li</last></author>
      <author><first>Yixuan</first><last>Su</last><affiliation>Cohere</affiliation></author>
      <author><first>Nigel</first><last>Collier</last><affiliation>University of Cambridge</affiliation></author>
      <pages>25081-25091</pages>
      <abstract>Prompt compression is important for large language models (LLMs) to increase inference speed, reduce costs, and improve user experience. However, current methods face challenges such as low compression ratios and potential training-test overlap during evaluation. To address these issues, we propose 500xCompressor, a method that compresses natural language contexts into a minimum of one special token and demonstrates strong generalization ability. The 500xCompressor introduces approximately 0.3% additional parameters and achieves compression ratios ranging from 6x to 500x, achieving 27-90% reduction in calculations and 55-83% memory savings when generating 100-400 tokens for new and reused prompts at 500x compression, while retaining 70-74% (F1) and 77-84% (Exact Match) of the LLM capabilities compared to using non-compressed prompts. It is designed to compress any text, answer various types of questions, and can be utilized by the original LLM without requiring fine-tuning. Initially, 500xCompressor was pretrained on the ArxivCorpus, followed by fine-tuning on the ArxivQA dataset, and subsequently evaluated on strictly unseen and cross-domain question answering (QA) datasets. This study shows that KV values outperform embeddings in preserving information at high compression ratios. The highly compressive nature of natural language prompts, even for detailed information, suggests potential for future applications and the development of a new LLM language.</abstract>
      <url hash="b28a9c75">2025.acl-long.1219</url>
      <bibkey>li-etal-2025-500xcompressor</bibkey>
    </paper>
    <paper id="1220">
      <title>Estimating Privacy Leakage of Augmented Contextual Knowledge in Language Models</title>
      <author><first>James</first><last>Flemings</last></author>
      <author><first>Bo</first><last>Jiang</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Wanrong</first><last>Zhang</last><affiliation>Tiktok</affiliation></author>
      <author><first>Zafar</first><last>Takhirov</last><affiliation>Landbase Inc.</affiliation></author>
      <author><first>Murali</first><last>Annavaram</last><affiliation>University of Southern California</affiliation></author>
      <pages>25092-25108</pages>
      <abstract>Language models (LMs) rely on their parametric knowledge augmented with relevant contextual knowledge for certain tasks, such as question answering. However, the contextual knowledge can contain private information that may be leaked when answering queries, and estimating this privacy leakage is not well understood. A straightforward approach of directly comparing an LM’s output to the contexts can overestimate the privacy risk, since the LM’s parametric knowledge might already contain the augmented contextual knowledge. To this end, we introduce <i>context influence</i>, a metric that builds on differential privacy, a widely-adopted privacy notion, to estimate the privacy leakage of contextual knowledge during decoding. Our approach effectively measures how each subset of the context influences an LM’s response while separating the specific parametric knowledge of the LM. Using our context influence metric, we demonstrate that context privacy leakage occurs when contextual knowledge is out of distribution with respect to parametric knowledge. Moreover, we experimentally demonstrate how context influence properly attributes the privacy leakage to augmented contexts, and we evaluate how factors– such as model size, context size, generation position, etc.– affect context privacy leakage. The practical implications of our results will inform practitioners of the privacy risk associated with augmented contextual knowledge.</abstract>
      <url hash="caa405d7">2025.acl-long.1220</url>
      <bibkey>flemings-etal-2025-estimating</bibkey>
    </paper>
    <paper id="1221">
      <title>Document-Level Event-Argument Data Augmentation for Challenging Role Types</title>
      <author><first>Joseph</first><last>Gatto</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Omar</first><last>Sharif</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Parker</first><last>Seegmiller</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Sarah Masud</first><last>Preum</last><affiliation>Dartmouth College</affiliation></author>
      <pages>25109-25131</pages>
      <abstract>Event Argument Extraction (EAE) is a daunting information extraction problem — with significant limitations in few-shot cross-domain (FSCD) settings. A common solution to FSCD modeling is data augmentation. Unfortunately, existing augmentation methods are not well-suited to a variety of real-world EAE contexts, including (i) modeling long documents (documents with over 10 sentences), and (ii) modeling challenging role types (i.e., event roles with little to no training data and semantically outlying roles). We introduce two novel LLM-powered data augmentation methods for generating extractive document-level EAE samples using zero in-domain training data. We validate the generalizability of our approach on four datasets — showing significant performance increases in low-resource settings. Our highest performing models provide a 13-pt increase in F1 score on zero-shot role extraction in FSCD evaluation.</abstract>
      <url hash="ed0a3ff9">2025.acl-long.1221</url>
      <bibkey>gatto-etal-2025-document</bibkey>
    </paper>
    <paper id="1222">
      <title>Mapping the Podcast Ecosystem with the Structured Podcast Research Corpus</title>
      <author><first>Benjamin Roger</first><last>Litterer</last></author>
      <author><first>David</first><last>Jurgens</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>Dallas</first><last>Card</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <pages>25132-25154</pages>
      <abstract>Podcasts provide highly diverse content to a massive listener base through a unique on-demand modality. However, limited data has prevented large-scale computational analysis of the podcast ecosystem. To fill this gap, we introduce a massive dataset of over 1.1M podcast transcripts that is largely comprehensive of all English language podcasts available through public RSS feeds from May and June of 2020. This data is not limited to text, but includes metadata, inferred speaker roles, and audio features and speaker turns for a subset of 370K episodes. Using this data, we conduct a foundational investigation into the content, structure, and responsiveness of this ecosystem. Together, our data and analyses open the door to continued computational research of this popular and impactful medium.</abstract>
      <url hash="fcfe42b0">2025.acl-long.1222</url>
      <bibkey>litterer-etal-2025-mapping</bibkey>
    </paper>
    <paper id="1223">
      <title>Unravelling the Logic: Investigating the Generalisation of Transformers in Numerical Satisfiability Problems</title>
      <author><first>Tharindu</first><last>Madusanka</last></author>
      <author><first>Marco</first><last>Valentino</last><affiliation>University of Sheffield</affiliation></author>
      <author><first>Iqra</first><last>Zahid</last><affiliation>Imperial College London</affiliation></author>
      <author><first>Ian</first><last>Pratt-Hartmann</last><affiliation>University of Opole and University of Manchester</affiliation></author>
      <author><first>Riza</first><last>Batista-Navarro</last><affiliation>University of Manchester</affiliation></author>
      <pages>25155-25168</pages>
      <abstract>Transformer models have achieved remarkable performance in many formal reasoning tasks. Nonetheless, the extent of their comprehension pertaining to logical semantics and rules of inference remains somewhat uncertain. Evaluating such understanding necessitates a rigorous examination of these models’ generalisation capacity to out-of-distribution data. In this study, we probe the generalisation prowess of Transformer models with respect to the hitherto unexplored domain of numerical satisfiability problems. Our investigation reveals that Transformers exhibit minimal scale and noise invariance, alongside limited vocabulary and number invariance. However, even when Transformer models experience a notable decline in performance on out-of-distribution test sets, they often still surpass the random baseline by a considerable margin.</abstract>
      <url hash="a43302e4">2025.acl-long.1223</url>
      <bibkey>madusanka-etal-2025-unravelling</bibkey>
    </paper>
    <paper id="1224">
      <title>The Nature of <fixed-case>NLP</fixed-case>: Analyzing Contributions in <fixed-case>NLP</fixed-case> Papers</title>
      <author><first>Aniket</first><last>Pramanick</last><affiliation>NEC and Technische Universität Darmstadt</affiliation></author>
      <author><first>Yufang</first><last>Hou</last><affiliation>IT:U Interdisciplinary Transformation University Austria, Technische Universität Darmstadt and IBM Research Ireland</affiliation></author>
      <author><first>Saif M.</first><last>Mohammad</last></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Institute for Computer Science, Artificial Intelligence and Technology, Mohamed bin Zayed University of Artificial Intelligence and Technische Universität Darmstadt</affiliation></author>
      <pages>25169-25191</pages>
      <abstract>Natural Language Processing (NLP) is an established and dynamic field. Despite this, what constitutes NLP research remains debated. In this work, we address the question by quantitatively examining NLP research papers. We propose a taxonomy of research contributions and introduce _NLPContributions_, a dataset of nearly <tex-math>2k</tex-math> NLP research paper abstracts, carefully annotated to identify scientific contributions and classify their types according to this taxonomy. We also introduce a novel task of automatically identifying contribution statements and classifying their types from research papers. We present experimental results for this task and apply our model to ~<tex-math>29k</tex-math> NLP research papers to analyze their contributions, aiding in the understanding of the nature of NLP research. We show that NLP research has taken a winding path — with the focus on language and human-centric studies being prominent in the 1970s and 80s, tapering off in the 1990s and 2000s, and starting to rise again since the late 2010s. Alongside this revival, we observe a steady rise in dataset and methodological contributions since the 1990s, such that today, on average, individual NLP papers contribute in more ways than ever before. Our dataset and analyses offer a powerful lens for tracing research trends and offer potential for generating informed, data-driven literature surveys.</abstract>
      <url hash="86d2062f">2025.acl-long.1224</url>
      <bibkey>pramanick-etal-2025-nature</bibkey>
    </paper>
    <paper id="1225">
      <title><tex-math>\mathtt{GeLLM^3O}</tex-math>: Generalizing Large Language Models for Multi-property Molecule Optimization</title>
      <author><first>Vishal</first><last>Dey</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <author><first>Xiao</first><last>Hu</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <author><first>Xia</first><last>Ning</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <pages>25192-25221</pages>
      <abstract>Despite recent advancements, most computational methods for molecule optimization are constrained to single- or double-property optimization tasks and suffer from poor scalability and generalizability to novel optimization tasks. Meanwhile, Large Language Models (LLMs) demonstrate remarkable out-of-domain generalizability to novel tasks. To demonstrate LLMs’ potential for molecule optimization, we introduce <tex-math>\mathtt{MuMOInstruct}</tex-math>, the first high-quality instruction-tuning dataset specifically focused on multi-property molecule optimization tasks. Leveraging <tex-math>\mathtt{MuMOInstruct}</tex-math>, we develop <tex-math>\mathtt{GeLLM^3O}</tex-math>s, a series of instruction-tuned LLMs for molecule optimization. Extensive evaluations across 5 in-domain and 5 out-of-domain tasks demonstrate that <tex-math>\mathtt{GeLLM^3O}</tex-math>s consistently outperform state-of-the-art baselines. <tex-math>\mathtt{GeLLM^3O}</tex-math>s also exhibit outstanding zero-shot generalization to unseen tasks, significantly outperforming powerful closed-source LLMs. Such strong generalizability demonstrates the tremendous potential of <tex-math>\mathtt{GeLLM^3O}</tex-math>s as foundational models for molecule optimization, thereby tackling novel optimization tasks without resource-intensive retraining. <tex-math>\mathtt{MuMOInstruct}</tex-math> and code are accessible through https://github.com/ninglab/GeLLMO.</abstract>
      <url hash="1e1055ec">2025.acl-long.1225</url>
      <bibkey>dey-etal-2025-mathtt</bibkey>
    </paper>
    <paper id="1226">
      <title>Follow-up Question Generation For Enhanced Patient-Provider Conversations</title>
      <author><first>Joseph</first><last>Gatto</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Parker</first><last>Seegmiller</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Timothy E.</first><last>Burdick</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Inas S.</first><last>Khayal</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Sarah</first><last>DeLozier</last></author>
      <author><first>Sarah Masud</first><last>Preum</last><affiliation>Dartmouth College</affiliation></author>
      <pages>25222-25240</pages>
      <abstract>Follow-up question generation is an essential feature of dialogue systems as it can reduce conversational ambiguity and enhance modeling complex interactions. Conversational contexts often pose core NLP challenges such as (i) extracting relevant information buried in fragmented data sources, and (ii) modeling parallel thought processes. These two challenges occur frequently in medical dialogue as a doctor asks questions based not only on patient utterances but also their prior EHR data and current diagnostic hypotheses. Asking medical questions in asynchronous conversations compounds these issues as doctors can only rely on static EHR information to motivate follow-up questions. To address these challenges, we introduce FollowupQ, a novel framework for enhancing asynchronous medical conversation.FollowupQ is a multi-agent framework that processes patient messages and EHR data to generate personalized follow-up questions, clarifying patient-reported medical conditions. FollowupQ reduces requisite provider follow-up communications by 34%. It also improves performance by 17% and 5% on real and synthetic data, respectively. We also release the first public dataset of asynchronous medical messages with linked EHR data alongside 2,300 follow-up questions written by clinical experts for the wider NLP research community.</abstract>
      <url hash="aedc5a73">2025.acl-long.1226</url>
      <bibkey>gatto-etal-2025-follow</bibkey>
    </paper>
    <paper id="1227">
      <title>Unveiling Privacy Risks in <fixed-case>LLM</fixed-case> Agent Memory</title>
      <author><first>Bo</first><last>Wang</last><affiliation>School of Artificial Intelligence, Jilin University</affiliation></author>
      <author><first>Weiyi</first><last>He</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Shenglai</first><last>Zeng</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Zhen</first><last>Xiang</last><affiliation>University of Georgia</affiliation></author>
      <author><first>Yue</first><last>Xing</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Jiliang</first><last>Tang</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Pengfei</first><last>He</last><affiliation>Michigan State University</affiliation></author>
      <pages>25241-25260</pages>
      <abstract>Large Language Model (LLM) agents have become increasingly prevalent across various real-world applications. They enhance decision-making by storing private user-agent interactions in the memory module for demonstrations, introducing new privacy risks for LLM agents. In this work, we systematically investigate the vulnerability of LLM agents to our proposed Memory EXTRaction Attack (MEXTRA) under a black-box setting. To extract private information from memory, we propose an effective attacking prompt design and an automated prompt generation method based on different levels of knowledge about the LLM agent. Experiments on two representative agents demonstrate the effectiveness of MEXTRA. Moreover, we explore key factors influencing memory leakage from both the agent designer’s and the attacker’s perspectives. Our findings highlight the urgent need for effective memory safeguards in LLM agent design and deployment.</abstract>
      <url hash="b4f871fb">2025.acl-long.1227</url>
      <bibkey>wang-etal-2025-unveiling-privacy</bibkey>
    </paper>
    <paper id="1228">
      <title>Watching the Watchers: Exposing Gender Disparities in Machine Translation Quality Estimation</title>
      <author><first>Emmanouil</first><last>Zaranis</last><affiliation>Instituto Superior Técnico</affiliation></author>
      <author><first>Giuseppe</first><last>Attanasio</last><affiliation>Instituto de Telecomunicações</affiliation></author>
      <author><first>Sweta</first><last>Agrawal</last><affiliation>Google</affiliation></author>
      <author><first>Andre</first><last>Martins</last><affiliation>Instituto Superior Técnico and Unbabel</affiliation></author>
      <pages>25261-25284</pages>
      <abstract>Quality estimation (QE)—the automatic assessment of translation quality—has recently become crucial across several stages of the translation pipeline, from data curation to training and decoding. While QE metrics have been optimized to align with human judgments, whether they encode social biases has been largely overlooked. Biased QE risks favoring certain demographic groups over others, e.g., by exacerbating gaps in visibility and usability. This paper defines and investigates gender bias of QE metrics and discusses its downstream implications for machine translation (MT). Experiments with state-of-the-art QE metrics across multiple domains, datasets, and languages reveal significant bias. When a human entity’s gender in the source is undisclosed, masculine-inflected translations score higher than feminine-inflected ones, and gender-neutral translations are penalized. Even when contextual cues disambiguate gender, using context-aware QE metrics leads to more errors in selecting the correct translation inflection for feminine referents than for masculine ones. Moreover, a biased QE metric affects data filtering and quality-aware decoding. Our findings underscore the need for a renewed focus on developing and evaluating QE metrics centered on gender.</abstract>
      <url hash="e275d43e">2025.acl-long.1228</url>
      <bibkey>zaranis-etal-2025-watching</bibkey>
    </paper>
    <paper id="1229">
      <title>Language Constrained Multimodal Hyper Adapter For Many-to-Many Multimodal Summarization</title>
      <author><first>Nayu</first><last>Liu</last><affiliation>School of Computer Science and Technology, Tiangong University</affiliation></author>
      <author><first>Fanglong</first><last>Yao</last></author>
      <author><first>Haoran</first><last>Luo</last></author>
      <author><first>Yong</first><last>Yang</last><affiliation>Tiangong University</affiliation></author>
      <author><first>Chen</first><last>Tang</last></author>
      <author><first>Bo</first><last>Lv</last></author>
      <pages>25285-25298</pages>
      <abstract>Multimodal summarization (MS) combines text and visuals to generate summaries. Recently, many-to-many multimodal summarization (M3S) garnered interest as it enables a unified model for multilingual and cross-lingual MS. Existing methods have made progress by facilitating the transfer of common multimodal summarization knowledge. While, prior M3S models that fully share parameters neglect the language-specific knowledge learning, where potential interference between languages may limit the flexible adaptation of MS modes across different language combinations and hinder further collaborative improvements in joint M3S training. Based on this observation, we propose Language Constrained Multimodal Hyper Adapter (LCMHA) for M3S. LCMHA integrates language-specific multimodal adapters into multilingual pre-trained backbones via a language constrained hypernetwork, enabling relaxed parameter sharing that enhances language-specific learning while preserving shared MS knowledge learning. In addition, a language-regularized hypernetwork is designed to balance intra- and inter-language learning, generating language-specific adaptation weights and enhancing the retention of distinct language features through the regularization of generated parameters. Experimental results on the M3Sum benchmark show LCMHA’s effectiveness and scalability across multiple multilingual pre-trained backbones.</abstract>
      <url hash="38d128d6">2025.acl-long.1229</url>
      <bibkey>liu-etal-2025-language</bibkey>
    </paper>
    <paper id="1230">
      <title><fixed-case>PRMB</fixed-case>ench: A Fine-grained and Challenging Benchmark for Process-Level Reward Models</title>
      <author><first>Mingyang</first><last>Song</last></author>
      <author><first>Zhaochen</first><last>Su</last></author>
      <author><first>Xiaoye</first><last>Qu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Jiawei</first><last>Zhou</last><affiliation>State University of New York at Stony Brook</affiliation></author>
      <author><first>Yu</first><last>Cheng</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>25299-25346</pages>
      <abstract>Process-level Reward Models (PRMs) are crucial for complex reasoning and decision-making tasks, where each intermediate step plays an important role in the reasoning process. Since language models are prone to various types of errors during the reasoning process, PRMs are required to possess nuanced capabilities for detecting various implicit error types in real-world scenarios. However, current benchmarks primarily focus on step correctness, failing to evaluate PRMs’ performance systematically. To address this gap, we introduce PRMBench, a process-level benchmark specifically designed to assess the fine-grained error detection capabilities of PRMs. PRMBench comprises 6,216 carefully designed problems and 83,456 step-level labels, evaluating models across multiple dimensions, including <tex-math>\textit{simplicity}</tex-math>, <tex-math>\textit{soundness}</tex-math>, and <tex-math>\textit{sensitivity}</tex-math>. In our experiments on 25 models, spanning both open-source PRMs and closed-source large language models prompted as critic models, we uncover significant weaknesses in current PRMs. These findings underscore the challenges inherent in process-level evaluation and highlight key directions for future research, establishing PRMBench as a robust testbed for advancing research on PRM evaluation and development.</abstract>
      <url hash="e4713e0e">2025.acl-long.1230</url>
      <bibkey>song-etal-2025-prmbench</bibkey>
    </paper>
    <paper id="1231">
      <title>Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets</title>
      <author><first>Dongyue</first><last>Li</last></author>
      <author><first>Ziniu</first><last>Zhang</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Lu</first><last>Wang</last><affiliation>Northeastern University, Northeastern University and University of Michigan</affiliation></author>
      <author><first>Hongyang R.</first><last>Zhang</last><affiliation>Northeastern University</affiliation></author>
      <pages>25347-25364</pages>
      <abstract>This paper develops an ensemble method for fine-tuning a language model to multiple datasets. Existing methods, such as quantized LoRA (QLoRA), are efficient when adapting to a single dataset. When training on multiple datasets of different tasks, a common setup in practice, it remains unclear how to design an efficient adaptation for fine-tuning language models. We propose to use an ensemble of multiple smaller adapters instead of a single adapter per task. We design an efficient algorithm that partitions <tex-math>n</tex-math> datasets into <tex-math>m</tex-math> groups, where <tex-math>m</tex-math> is typically much smaller than <tex-math>n</tex-math> in practice, and train one adapter for each group before taking a weighted combination to form the ensemble. The algorithm leverages a first-order approximation property of low-rank adaptation to quickly obtain the fine-tuning performances of dataset combinations since methods like LoRA stay close to the base model. Hence, we use the gradients of the base model to estimate its behavior during fine-tuning. Empirically, this approximation holds with less than 1% error on models with up to 34 billion parameters, leading to an estimation of true fine-tuning performances under 5% error while speeding up computation compared to base fine-tuning by 105 times. When applied to fine-tune Llama and GPT models on ten text classification tasks, our approach provides up to 10% higher average test accuracy over QLoRA, with only 9% more FLOPs. On a Llama model with 34 billion parameters, an ensemble of QLoRA increases test accuracy by 3% compared to QLoRA, with only 8% more FLOPs.</abstract>
      <url hash="dbc2ece3">2025.acl-long.1231</url>
      <bibkey>li-etal-2025-efficient-ensemble</bibkey>
    </paper>
    <paper id="1232">
      <title>Library-Like Behavior In Language Models is Enhanced by Self-Referencing Causal Cycles</title>
      <author><first>Munachiso S</first><last>Nwadike</last></author>
      <author><first>Zangir</first><last>Iklassov</last></author>
      <author><first>Toluwani</first><last>Aremu</last></author>
      <author><first>Tatsuya</first><last>Hiraoka</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and RIKEN</affiliation></author>
      <author><first>Benjamin</first><last>Heinzerling</last><affiliation>Tohoku University and RIKEN</affiliation></author>
      <author><first>Velibor</first><last>Bojkovic</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Hilal</first><last>AlQuabeh</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Martin</first><last>Takáč</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Kentaro</first><last>Inui</last><affiliation>MBZUAI, RIKEN and Tohoku University</affiliation></author>
      <pages>25365-25377</pages>
      <abstract>We introduce the concept of the <tex-math>\textit{self-referencing causal cycle}</tex-math> (abbreviated <tex-math>\textit{ReCall}</tex-math> )—a mechanism that enables large language models (LLMs) to bypass the limitations of unidirectional causality, which underlies a phenomenon known as the <tex-math>\textit{reversal curse}</tex-math>. When an LLM is prompted with sequential data, it often fails to recall preceding context. For example, when we ask an LLM to recall the line preceding “O say does that star-spangled banner yet wave” in the U.S. National Anthem, it often fails to correctly return “Gave proof through the night that our flag was still there”—this is due to the reversal curse. It occurs because language models such as ChatGPT and Llama generate text based on preceding tokens, requiring facts to be learned and reproduced in a consistent token order. While the reversal curse is often viewed as a limitation, we offer evidence of an alternative view: it is not always an obstacle in practice. We find that <tex-math>\textit{ReCall}</tex-math> is driven by what we designate as <tex-math>\textit{cycle tokens}</tex-math>—sequences that connect different parts of the training data, enabling recall of preceding tokens from succeeding ones. Through rigorous probabilistic formalization and controlled experiments, we demonstrate how the cycles they induce influence a model’s ability to reproduce information. To facilitate reproducibility, we provide our code and experimental details at https://anonymous.4open.science/r/remember-B0B8/.</abstract>
      <url hash="a42eb593">2025.acl-long.1232</url>
      <bibkey>nwadike-etal-2025-library</bibkey>
    </paper>
    <paper id="1233">
      <title>Shaping the Safety Boundaries: Understanding and Defending Against Jailbreaks in Large Language Models</title>
      <author><first>Lang</first><last>Gao</last></author>
      <author><first>Jiahui</first><last>Geng</last></author>
      <author><first>Xiangliang</first><last>Zhang</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Preslav</first><last>Nakov</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Xiuying</first><last>Chen</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>25378-25398</pages>
      <abstract>Jailbreaking in Large Language Models (LLMs) is a major security concern as it can deceive LLMs into generating harmful text. However, understanding of how jailbreaking works remains limited, hindering the development of effective defense strategies. To address this issue, we conduct a large-scale analysis of seven different jailbreak methods and identify that disagreements among methods stem from insufficient observation samples.We introduce the concept of a safety boundary and discover that jailbreaks shift harmful activations outside this boundary, where LLMs become less sensitive to harmful information. Our analysis reveals that low and middle layers play a critical role in these shifts, while deeper layers have a lesser impact.Building on these insights, we propose a novel defense mechanism called Activation Boundary Defense (ABD), which adaptively constrains activations within the safety boundary. To enhance its effectiveness, we use Bayesian optimization to selectively apply the defense to the low and middle layers.Experiments on several benchmark datasets demonstrate that ABD achieves an average Defense Success Rate (DSR) of over 98% against various jailbreak attacks, with less than a 2% impact on the model’s general capabilities.</abstract>
      <url hash="15dd8bc2">2025.acl-long.1233</url>
      <bibkey>gao-etal-2025-shaping</bibkey>
    </paper>
    <paper id="1234">
      <title><fixed-case>ASPERA</fixed-case>: A Simulated Environment to Evaluate Planning for Complex Action Execution</title>
      <author><first>Alexandru</first><last>Coca</last></author>
      <author><first>Mark</first><last>Gaynor</last><affiliation>Apple</affiliation></author>
      <author><first>Zhenxing</first><last>Zhang</last><affiliation>Apple</affiliation></author>
      <author><first>Jianpeng</first><last>Cheng</last><affiliation>Meta</affiliation></author>
      <author><first>Bo-Hsiang</first><last>Tseng</last><affiliation>Apple</affiliation></author>
      <author><first>Peter</first><last>Boothroyd</last><affiliation>Apple</affiliation></author>
      <author><first>Hector</first><last>Martinez Alonso</last><affiliation>Apple</affiliation></author>
      <author><first>Diarmuid</first><last>O Seaghdha</last><affiliation>Apple</affiliation></author>
      <author><first>Anders</first><last>Johannsen</last></author>
      <pages>25399-25434</pages>
      <abstract>This work evaluates the potential of large language models (LLMs) to power digital assistants capable of complex action execution. Such assistants rely on pre-trained programming knowledge to execute multi-step goals by composing objects and functions defined in assistant libraries into action execution programs. To achieve this, we develop ASPERA, a framework comprising an assistant library simulation and a human-assisted LLM data generation engine. Our engine allows developers to guide LLM generation of high-quality tasks consisting of complex user queries, simulation state and corresponding validation programs, tackling data availability and evaluation robustness challenges. Alongside the framework we release Asper-Bench, an evaluation dataset of 250 challenging tasks generated using ASPERA, which we use to show that program generation grounded in custom assistant libraries is a significant challenge to LLMs compared to dependency-free code generation.</abstract>
      <url hash="ed982fc7">2025.acl-long.1234</url>
      <bibkey>coca-etal-2025-aspera</bibkey>
    </paper>
    <paper id="1235">
      <title><fixed-case>R</fixed-case>eflect<fixed-case>D</fixed-case>iffu: Reflect between Emotion-intent Contagion and Mimicry for Empathetic Response Generation via a <fixed-case>RL</fixed-case>-Diffusion Framework</title>
      <author><first>Jiahao</first><last>Yuan</last></author>
      <author><first>Zixiang</first><last>Di</last></author>
      <author><first>Zhiqing</first><last>Cui</last><affiliation>Reading University and Nanjing University of Information Science and Technology</affiliation></author>
      <author><first>Guisong</first><last>Yang</last></author>
      <author><first>Usman</first><last>Naseem</last><affiliation>Macquarie University</affiliation></author>
      <pages>25435-25449</pages>
      <abstract>Empathetic response generation necessitates the integration of emotional and intentional dynamics to foster meaningful interactions. Existing research either neglects the intricate interplay between emotion and intent, leading to suboptimal controllability of empathy, or resorts to large language models (LLMs), which incur significant computational overhead. In this paper, we introduce ReflectDiffu, a lightweight and comprehensive framework for empathetic response generation. This framework incorporates emotion contagion to augment emotional expressiveness and employs an emotion-reasoning mask to pinpoint critical emotional elements. Additionally, it integrates intent mimicry within reinforcement learning for refinement during diffusion. By harnessing an intent twice reflect mechanism of Exploring-Sampling-Correcting, ReflectDiffu adeptly translates emotional decision-making into precise intent actions, thereby addressing empathetic response misalignments stemming from emotional misrecognition. Through reflection, the framework maps emotional states to intents, markedly enhancing both response empathy and flexibility. Comprehensive experiments reveal that ReflectDiffu outperforms existing models regarding relevance, controllability, and informativeness, achieving state-of-the-art results in both automatic and human evaluations.</abstract>
      <url hash="ed6934de">2025.acl-long.1235</url>
      <bibkey>yuan-etal-2025-reflectdiffu</bibkey>
    </paper>
    <paper id="1236">
      <title><fixed-case>SARA</fixed-case>: Salience-Aware Reinforced Adaptive Decoding for Large Language Models in Abstractive Summarization</title>
      <author><first>Nayu</first><last>Liu</last><affiliation>School of Computer Science and Technology, Tiangong University</affiliation></author>
      <author><first>Junnan</first><last>Zhu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yiming</first><last>Ma</last></author>
      <author><first>Zhicong</first><last>Lu</last></author>
      <author><first>Wenlei</first><last>Xu</last></author>
      <author><first>Yong</first><last>Yang</last><affiliation>Tiangong University</affiliation></author>
      <author><first>Jiang</first><last>Zhong</last><affiliation>Chongqing University</affiliation></author>
      <author><first>Kaiwen</first><last>Wei</last><affiliation>Chongqing University</affiliation></author>
      <pages>25450-25463</pages>
      <abstract>LLMs have improved the fluency and informativeness of abstractive summarization but remain prone to hallucinations, where generated content deviates from the source document. Recent PMI decoding strategies mitigate over-reliance on prior knowledge by comparing output probabilities with and without source documents, effectively enhancing contextual utilization and improving faithfulness. However, existing strategies often neglect the explicit use of salient contextual information and rely on static hyperparameters to fix the balance between contextual and prior knowledge, limiting their flexibility. In this work, we propose Salience-Aware Reinforced Adaptive decoding (SARA), which incorporates salient information and allows the model to adaptively determine reliance on the source document’s context, salient context, and the model’s prior knowledge based on pointwise mutual information. Moreover, a tokenwise adaptive decoding mechanism via reinforcement learning is proposed in SARA to dynamically adjust the contributions of context and prior knowledge at each decoding timestep. Experiments on CNN/DM, WikiHow, and NYT50 datasets show that SARA consistently improves the quality and faithfulness of summaries across various LLM backbones without modifying their weights.</abstract>
      <url hash="98f63609">2025.acl-long.1236</url>
      <bibkey>liu-etal-2025-sara</bibkey>
    </paper>
    <paper id="1237">
      <title>Embedding-Converter: A Unified Framework for Cross-Model Embedding Transformation</title>
      <author><first>Jinsung</first><last>Yoon</last><affiliation>Google</affiliation></author>
      <author><first>Sercan O</first><last>Arik</last><affiliation>Google</affiliation></author>
      <pages>25464-25482</pages>
      <abstract>Embedding models play a crucial role in machine learning. However, the continuous development of new models presents a major challenge: migrating to a potentially superior model often requires the computationally expensive process of re-embedding entire datasets—without any guarantee of performance improvement. This paper presents Embedding-Converter, a novel framework for efficiently transforming embeddings between different models, thus avoiding costly ‘re-embedding’. The proposed approach achieves 100 times faster and cheaper computations in real-world applications. Experiments show that Embedding-Converter not only streamlines transitions to new models, but can also improve upon the source model’s performance, approaching that of the target model. This facilitates efficient evaluation and broader adoption of new embedding models by significantly reducing the overhead of model switching. Furthermore, Embedding-Converter addresses latency limitations by enabling the use of smaller models for online tasks while still benefiting from the performance of larger models offline. By promoting the release of converters alongside new embedding models, Embedding-Converter fosters a more dynamic and accessible ecosystem for embedding model development and deployment.</abstract>
      <url hash="3cb12886">2025.acl-long.1237</url>
      <bibkey>yoon-arik-2025-embedding</bibkey>
    </paper>
    <paper id="1238">
      <title>Improving Automatic Evaluation of Large Language Models (<fixed-case>LLM</fixed-case>s) in Biomedical Relation Extraction via <fixed-case>LLM</fixed-case>s-as-the-Judge</title>
      <author><first>Md Tahmid Rahman</first><last>Laskar</last><affiliation>Dialpad Inc.</affiliation></author>
      <author><first>Israt</first><last>Jahan</last><affiliation>York University</affiliation></author>
      <author><first>Elham</first><last>Dolatabadi</last></author>
      <author><first>Chun</first><last>Peng</last><affiliation>York University</affiliation></author>
      <author><first>Enamul</first><last>Hoque</last><affiliation>York University</affiliation></author>
      <author><first>Jimmy</first><last>Huang</last><affiliation>York University and York University</affiliation></author>
      <pages>25483-25497</pages>
      <abstract>Large Language Models (LLMs) have demonstrated impressive performance in biomedical relation extraction, even in zero-shot scenarios. However, evaluating LLMs in this task remains challenging due to their ability to generate human-like text, often producing synonyms or abbreviations of gold-standard answers, making traditional automatic evaluation metrics unreliable. On the other hand, while human evaluation is more reliable, it is costly and time-consuming, making it impractical for real-world applications. This paper investigates the use of LLMs-as-the-Judge as an alternative evaluation method for biomedical relation extraction. We benchmark 8 LLMs as judges to evaluate the responses generated by 5 other LLMs across 3 biomedical relation extraction datasets. Unlike other text-generation tasks, we observe that LLM-based judges perform quite poorly (usually below 50% accuracy) in the biomedical relation extraction task. Our findings reveal that it happens mainly because relations extracted by LLMs do not adhere to any standard format. To address this, we propose structured output formatting for LLM-generated responses that helps LLM-Judges to improve their performance by about 15% (on average). We also introduce a domain adaptation technique to further enhance LLM-Judge performance by effectively transferring knowledge between datasets. We release both our human-annotated and LLM-annotated judgment data (36k samples in total) for public use here: https://github.com/tahmedge/llm_judge_biomedical_re.</abstract>
      <url hash="517d9334">2025.acl-long.1238</url>
      <bibkey>laskar-etal-2025-improving</bibkey>
    </paper>
    <paper id="1239">
      <title>Answering Complex Geographic Questions by Adaptive Reasoning with Visual Context and External Commonsense Knowledge</title>
      <author><first>Fan</first><last>Li</last></author>
      <author><first>Jianxing</first><last>Yu</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Jielong</first><last>Tang</last></author>
      <author><first>Wenqing</first><last>Chen</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Hanjiang</first><last>Lai</last></author>
      <author><first>Yanghui</first><last>Rao</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Jian</first><last>Yin</last></author>
      <pages>25498-25514</pages>
      <abstract>This paper focuses on a new task of answering geographic reasoning questions based on the given image (called GeoVQA). Unlike traditional VQA tasks, GeoVQA asks for details about the image-related culture, landscape, etc. This requires not only the identification of the objects in the image, their properties and relations, but also the understanding of the geographic knowledge of the objects, such as location, transportation, landmark, cuisine, etc. This background knowledge does not explicitly appear in the image, nor is there an extra-textual description. Without this missing but necessary knowledge, it is difficult for existing matching-based methods to infer the correct answer. To tackle these challenges, we propose a new geographic reasoning framework for our task. We first analyze the image and describe its fine-grained content by text and keywords using a multi-modal retrieval augmented technique, so as to deduce an answer in a unified textual modality. Next, we retrieve the crucial geographic commonsense knowledge. To reduce the retrieval complexity, we design a dynamic method that can adaptively collect the relevant clues for each reasoning step. The step in the incorrect direction will be pruned according to some judgment criteria. The remaining steps can help us form a reasoning chain to derive a correct answer. Moreover, we create a large-scale dataset GVQA with 41,329 samples to conduct the evaluation. The results demonstrate the effectiveness of our approach.</abstract>
      <url hash="000fd56b">2025.acl-long.1239</url>
      <bibkey>li-etal-2025-answering</bibkey>
    </paper>
    <paper id="1240">
      <title>Safety Alignment via Constrained Knowledge Unlearning</title>
      <author><first>Zesheng</first><last>Shi</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Yucheng</first><last>Zhou</last><affiliation>University of Macau</affiliation></author>
      <author><first>Jing</first><last>Li</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Yuxin</first><last>Jin</last><affiliation>Beijing Academy of Quantum Information Sciences</affiliation></author>
      <author><first>Yu</first><last>Li</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Daojing</first><last>He</last></author>
      <author><first>Fangming</first><last>Liu</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Saleh</first><last>Alharbi</last><affiliation>Shaqra University</affiliation></author>
      <author><first>Jun</first><last>Yu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>25515-25529</pages>
      <abstract>Despite significant progress in safety alignment, large language models (LLMs) remain susceptible to jailbreak attacks. Existing defense mechanisms have not fully deleted harmful knowledge in LLMs, which allows such attacks to bypass safeguards and produce harmful outputs. To address this challenge, we propose a novel safety alignment strategy, Constrained Knowledge Unlearning (CKU), which focuses on two primary objectives: knowledge localization and retention, and unlearning harmful knowledge. CKU works by scoring neurons in specific multilayer perceptron (MLP) layers to identify a subset U of neurons associated with useful knowledge. During the unlearning process, CKU prunes the gradients of neurons in U to preserve valuable knowledge while effectively mitigating harmful content. Experimental results demonstrate that CKU significantly enhances model safety without compromising overall performance, offering a superior balance between safety and utility compared to existing methods. Additionally, our analysis of neuron knowledge sensitivity across various MLP layers provides valuable insights into the mechanics of safety alignment and model knowledge editing.</abstract>
      <url hash="f8ceedf0">2025.acl-long.1240</url>
      <bibkey>shi-etal-2025-safety</bibkey>
    </paper>
    <paper id="1241">
      <title>Response Wide Shut? Surprising Observations in Basic Vision Language Model Capabilities</title>
      <author><first>Shivam</first><last>Chandhok</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Wan-Cyuan</first><last>Fan</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Vered</first><last>Shwartz</last></author>
      <author><first>Vineeth N.</first><last>Balasubramanian</last><affiliation>Research, Microsoft and Indian Institute of Technology Hyderabad</affiliation></author>
      <author><first>Leonid</first><last>Sigal</last><affiliation>University of British Columbia</affiliation></author>
      <pages>25530-25545</pages>
      <abstract>Vision-language Models (VLMs) have emerged as general-purpose tools for addressing a variety of complex computer vision problems. Such models have been shown to be highly capable, but, at the same time, lacking some basic visual understanding skills. In this paper, we set out to understand the limitations of SoTA VLMs on fundamental visual tasks (object classification, spatial understanding, and ability to delineate individual object instances through counting), by constructing a series of tests that probe which components of design, specifically, may be lacking. Importantly, we go significantly beyond the current benchmarks, which simply measure the final performance of VLM response, by also comparing and contrasting it to the performance of probes trained directly on features obtained from the visual encoder, intermediate vision-language projection and LLM-decoder output. In doing so, we uncover shortcomings in VLMs and make a number of important observations about their capabilities, robustness and how they process visual information. We hope our insights will guide progress in further improving VLMs.</abstract>
      <url hash="d66de89d">2025.acl-long.1241</url>
      <bibkey>chandhok-etal-2025-response</bibkey>
    </paper>
    <paper id="1242">
      <title><fixed-case>E</fixed-case>ffi<fixed-case>VLM</fixed-case>-<fixed-case>BENCH</fixed-case>: A Comprehensive Benchmark for Evaluating Training-Free Acceleration in Large Vision-Language Models</title>
      <author><first>Zekun</first><last>Wang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>MingHua</first><last>Ma</last></author>
      <author><first>Zexin</first><last>Wang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Rongchuan</first><last>Mu</last></author>
      <author><first>Liping</first><last>Shan</last></author>
      <author><first>Ming</first><last>Liu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Bing</first><last>Qin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>25546-25572</pages>
      <abstract>Large Vision-Language Models (LVLMs) have achieved remarkable success, yet their significant computational demands hinder practicaldeployment. While efforts to improve LVLM efficiency are growing, existing methods lack comprehensive evaluation across diverse backbones, benchmarks, and metrics. In this work, we systematically evaluate mainstream acceleration techniques for LVLMs, categorized into token and parameter compression. We introduce EffiVLM-BENCH, a unified framework for assessing not only absolute performance but also generalization and loyalty, while exploring Pareto-optimal trade-offs. Our extensive experiments and in-depth analyses offer insights into optimal strategies for accelerating LVLMs. We open-source code and recipes for EffiVLM-BENCH to foster future research.</abstract>
      <url hash="ff5fd2a9">2025.acl-long.1242</url>
      <bibkey>wang-etal-2025-effivlm</bibkey>
    </paper>
    <paper id="1243">
      <title>Pre-Training Curriculum for Multi-Token Prediction in Language Models</title>
      <author><first>Ansar</first><last>Aynetdinov</last><affiliation>Department of Computer Science, Humboldt University Berlin, Humboldt Universität Berlin</affiliation></author>
      <author><first>Alan</first><last>Akbik</last><affiliation>Humboldt Universität Berlin</affiliation></author>
      <pages>25573-25588</pages>
      <abstract>Multi-token prediction (MTP) is a recently proposed pre-training objective for language models. Rather than predicting only the next token (NTP), MTP predicts the next *k* tokens at each prediction step, using multiple prediction heads. MTP has shown promise in improving downstream performance, inference speed, and training efficiency, particularly for large models. However, prior work has shown that smaller language models (SLMs) struggle with the MTP objective. To address this, we propose a curriculum learning strategy for MTP training, exploring two variants: a forward curriculum, which gradually increases the complexity of the pre-training objective from NTP to MTP, and a reverse curriculum, which does the opposite. Our experiments show that the forward curriculum enables SLMs to better leverage the MTP objective during pre-training, improving downstream NTP performance and generative output quality, while retaining the benefits of self-speculative decoding. The reverse curriculum achieves stronger NTP performance and output quality, but fails to provide any self-speculative decoding benefits.</abstract>
      <url hash="0720af17">2025.acl-long.1243</url>
      <bibkey>aynetdinov-akbik-2025-pre</bibkey>
    </paper>
    <paper id="1244">
      <title>Can We Further Elicit Reasoning in <fixed-case>LLM</fixed-case>s? Critic-Guided Planning with Retrieval-Augmentation for Solving Challenging Tasks</title>
      <author><first>Xingxuan</first><last>Li</last></author>
      <author><first>Weiwen</first><last>Xu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Ruochen</first><last>Zhao</last></author>
      <author><first>Fangkai</first><last>Jiao</last></author>
      <author><first>Shafiq</first><last>Joty</last><affiliation>Nanyang Technological University and SalesForce.com</affiliation></author>
      <author><first>Lidong</first><last>Bing</last><affiliation>Shanda Group and Alibaba Group</affiliation></author>
      <pages>25589-25604</pages>
      <abstract>Large language models excel at problem-solving but often struggle with complex reasoning and factual accuracy. While chain-of-thought and retrieval-augmented generation help break down problems and retrieve knowledge, they still falter on challenging tasks like competitive programming due to frequent reasoning errors and irrelevant retrieval. To address this, we introduce Critic-guided planning with Retrieval-augmentation, CR-Planner, a novel framework that leverages fine-tuned critic models to guide both reasoning and retrieval processes through planning. CR-Planner iteratively selects and executes sub-goals, guided by critic models. A sub-goal critic identifies promising sub-goals from reasoning, query generation, and retrieval, while an execution critic evaluates outputs of sub-goal executions. We employ Monte Carlo Tree Search to collect data for critic training, allowing systematic exploration of action sequences and effective navigation toward the final answer. We evaluate CR-Planner on challenging domain-knowledge-intensive and reasoning-heavy tasks, including competitive programming, theorem-driven math reasoning, and complex domain retrieval problems. It significantly outperforms baselines, demonstrating effectiveness in both reasoning and retrieval.</abstract>
      <url hash="b9003f67">2025.acl-long.1244</url>
      <bibkey>li-etal-2025-elicit</bibkey>
    </paper>
    <paper id="1245">
      <title>On Many-Shot In-Context Learning for Long-Context Evaluation</title>
      <author><first>Kaijian</first><last>Zou</last></author>
      <author><first>Muhammad</first><last>Khalifa</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>Lu</first><last>Wang</last><affiliation>Northeastern University, Northeastern University and University of Michigan</affiliation></author>
      <pages>25605-25639</pages>
      <abstract>Many-shot in-context learning (ICL) has emerged as a unique setup to both utilize and test the ability of large language models to handle long context. This paper delves into long-context language model (LCLM) evaluation through many-shot ICL. We first ask: what types of ICL tasks benefit from additional demonstrations, and how effective are they in evaluating LCLMs? We find that classification and summarization tasks show performance improvements with additional demonstrations, while translation and reasoning tasks do not exhibit clear trends. Next, we investigate the extent to which different tasks necessitate retrieval versus global context understanding. We develop metrics to categorize ICL tasks into two groups: (i) similar-sample learning (SSL): tasks where retrieval of the most similar examples is sufficient for good performance, and (ii) all-sample learning (ASL): tasks that necessitate a deeper comprehension of all examples in the prompt. Lastly, we introduce a new many-shot ICL benchmark built on existing ICL tasks, MANYICLBENCH, to characterize model’s ability on both fronts and benchmark 12 LCLMs using MANYICLBENCH. We find that while state-of-the-art models demonstrate good performance up to 64k tokens in SSL tasks, many models experience significant performance drops at only 16k tokens in ASL tasks.</abstract>
      <url hash="f688beb0">2025.acl-long.1245</url>
      <bibkey>zou-etal-2025-many</bibkey>
    </paper>
    <paper id="1246">
      <title><fixed-case>H</fixed-case>elp<fixed-case>S</fixed-case>teer3: Human-Annotated Feedback and Edit Data to Empower Inference-Time Scaling in Open-Ended General-Domain Tasks</title>
      <author><first>Zhilin</first><last>Wang</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Jiaqi</first><last>Zeng</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Olivier</first><last>Delalleau</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Daniel</first><last>Egert</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Ellie</first><last>Evans</last></author>
      <author><first>Hoo-Chang</first><last>Shin</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Felipe</first><last>Soares</last></author>
      <author><first>Yi</first><last>Dong</last></author>
      <author><first>Oleksii</first><last>Kuchaiev</last><affiliation>NVIDIA</affiliation></author>
      <pages>25640-25662</pages>
      <abstract>Inference-Time Scaling has been critical to the success of recent models such as OpenAI o1 and DeepSeek R1. However, many techniques used to train models for inference-time scaling require tasks to have answers that can be verified, limiting their application to domains such as math, coding and logical reasoning. We take inspiration from how humans make first attempts, ask for detailed feedback from others and make improvements based on such feedback across a wide spectrum of open-ended endeavors. To this end, we collect HelpSteer3 data to train dedicated Feedback and Edit Models that are capable of performing inference-time scaling for open-ended general-domain tasks. In our setup, one model generates an initial response, which are given feedback by a second model, that are then used by a third model to edit the response. We show that performance on Arena Hard, a benchmark strongly predictive of Chatbot Arena Elo can be boosted by scaling the number of initial response drafts, effective feedback and edited responses. When scaled optimally, our setup based on 70B models from the Llama 3 family can reach SoTA performance on Arena Hard at 92.7 as of 5 Mar 2025, surpassing OpenAI o1-preview-2024-09-12 with 90.4 and DeepSeek R1 with 92.3.</abstract>
      <url hash="466000c6">2025.acl-long.1246</url>
      <bibkey>wang-etal-2025-helpsteer3</bibkey>
    </paper>
    <paper id="1247">
      <title><fixed-case>C</fixed-case>ultural<fixed-case>B</fixed-case>ench: A Robust, Diverse and Challenging Benchmark for Measuring <fixed-case>LM</fixed-case>s’ Cultural Knowledge Through Human-<fixed-case>AI</fixed-case> Red-Teaming</title>
      <author><first>Yu Ying</first><last>Chiu</last><affiliation>University of Washington</affiliation></author>
      <author><first>Liwei</first><last>Jiang</last></author>
      <author><first>Bill Yuchen</first><last>Lin</last><affiliation>xAI and University of Washington</affiliation></author>
      <author><first>Chan Young</first><last>Park</last></author>
      <author><first>Shuyue Stella</first><last>Li</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <author><first>Sahithya</first><last>Ravi</last></author>
      <author><first>Mehar</first><last>Bhatia</last><affiliation>Mila - Quebec Artificial Intelligence Institute</affiliation></author>
      <author><first>Maria</first><last>Antoniak</last></author>
      <author><first>Yulia</first><last>Tsvetkov</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <author><first>Vered</first><last>Shwartz</last></author>
      <author><first>Yejin</first><last>Choi</last><affiliation>Computer Science Department, Stanford University and NVIDIA</affiliation></author>
      <pages>25663-25701</pages>
      <abstract>Robust, diverse, and challenging cultural knowledge benchmarks are essential for measuring our progress towards making LMs that are helpful across diverse cultures. We introduce CulturalBench: a set of 1,696 human-written and human-verified questions to assess LMs’ cultural knowledge, covering 45 global regions including underrepresented ones like Bangladesh, Zimbabwe, and Peru. Questions are each verified by five independent annotators and span 17 diverse topics ranging from food preferences to greeting etiquette. We construct CulturalBench using methods inspired by Human-AI Red-Teaming. Compared to human performance (92.4% accuracy), the hard version of CulturalBench is challenging even for the best-performing frontier LMs, ranging from 28.7% to 61.5% in accuracy. We find that LMs often struggle with tricky questions that have multiple correct answers (e.g., What utensils do the Chinese usually use?), revealing a tendency to overfit to a single answer. Our results indicate that GPT-4o substantially outperform other models across cultures, besting local providers (e.g., Mistral on European culture and DeepSeek on Chinese culture). Across the board, models under-perform on questions related to North Africa, South America and Middle East.</abstract>
      <url hash="82a50184">2025.acl-long.1247</url>
      <bibkey>chiu-etal-2025-culturalbench</bibkey>
    </paper>
    <paper id="1248">
      <title>Balancing the Budget: Understanding Trade-offs Between Supervised and Preference-Based Finetuning</title>
      <author><first>Mohit</first><last>Raghavendra</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Junmo</first><last>Kang</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Alan</first><last>Ritter</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <pages>25702-25720</pages>
      <abstract>Post-training of Large Language Models often involves a pipeline of Supervised Finetuning (SFT) followed by Preference Finetuning (PFT) using methods like Direct Preference Optimization. Both stages require annotated data that are very different in structure and costs. We study how to optimally allocate a fixed training data budget between the two stages, through extensive experiments spanning four diverse tasks, multiple model sizes and various data annotation costs. Our findings reveal that just SFT on the base model dominates performance in low-data regimes (<tex-math>&lt;1,000</tex-math> annotated examples). With larger data-budgets, we observe that a combination of SFT and PFT, often with increasing portions allocated towards preference data yields optimal performance. However, completely eliminating SFT and running PFT directly on the base model yields suboptimal performance, described as the cold start problem on tasks like mathematics. We observe that this is due to the distribution shift arising from using DPO directly on the base model to elicit step-by-step reasoning. This limitation can be effectively addressed by allocating even a small portion (<tex-math>&lt;10</tex-math>%) of the budget to SFT first, resulting in performance improvements of <tex-math>15-20</tex-math>% on analytical benchmarks like GSM8k. These results provide actionable insights for researchers and practitioners optimizing model development under budget constraints, where high-quality data curation often represents a significant portion of the total costs of model development.</abstract>
      <url hash="90ba0192">2025.acl-long.1248</url>
      <bibkey>raghavendra-etal-2025-balancing</bibkey>
    </paper>
    <paper id="1249">
      <title>All That Glitters is Not Novel: Plagiarism in <fixed-case>AI</fixed-case> Generated Research</title>
      <author><first>Tarun</first><last>Gupta</last><affiliation>Indian Institute of Science, Indian institute of science, Bangalore</affiliation></author>
      <author><first>Danish</first><last>Pruthi</last><affiliation>Indian Institute of Science, Bangalore</affiliation></author>
      <pages>25721-25738</pages>
      <abstract>Automating scientific research is considered the final frontier of science. Recently, several papers claim autonomous research agents can generate novel research ideas. Amidst the prevailing optimism, we document a critical concern: a considerable fraction of such research documents are smartly plagiarized. Unlike past efforts where experts evaluate the novelty and feasibility of research ideas, we request 13 experts to operate under a different situational logic: to identify similarities between LLM-generated research documents and existing work. Concerningly, the experts identify 24% of the 50 evaluated research documents to be either paraphrased (with one-to-one methodological mapping), or significantly borrowed from existing work. These reported instances are cross-verified by authors of the source papers. Experts find an additional 32% ideas to partially overlap with prior work, and a small fraction to be completely original. Problematically, these LLM-generated research documents do not acknowledge original sources, and bypass inbuilt plagiarism detectors. Lastly, through controlled experiments we show that automated plagiarism detectors are inadequate at catching plagiarized ideas from such systems. We recommend a careful assessment of LLM-generated research, and discuss the implications of our findings on academic publishing.</abstract>
      <url hash="6315fc17">2025.acl-long.1249</url>
      <bibkey>gupta-pruthi-2025-glitters</bibkey>
    </paper>
    <paper id="1250">
      <title>Writing Like the Best: Exemplar-Based Expository Text Generation</title>
      <author><first>Yuxiang</first><last>Liu</last></author>
      <author><first>Kevin Chen-Chuan</first><last>Chang</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <pages>25739-25764</pages>
      <abstract>We introduce the Exemplar-Based Expository Text Generation task, aiming to generate an expository text on a new topic using an exemplar on a similar topic. Current methods fall short due to their reliance on extensive exemplar data, difficulty in adapting topic-specific content, and issues with long-text coherence. To address these challenges, we propose the concept of Adaptive Imitation and present a novel Recurrent Plan-then-Adapt (RePA) framework. RePA leverages large language models (LLMs) for effective adaptive imitation through a fine-grained plan-then-adapt process. RePA also enables recurrent segment-by-segment imitation, supported by two memory structures that enhance input clarity and output coherence. We also develop task-specific evaluation metrics–imitativeness, adaptiveness, and adaptive-imitativeness–using LLMs as evaluators. Experimental results across our collected three diverse datasets demonstrate that RePA surpasses existing baselines in producing factual, consistent, and relevant texts for this task.</abstract>
      <url hash="b1d1d951">2025.acl-long.1250</url>
      <bibkey>liu-chang-2025-writing</bibkey>
    </paper>
    <paper id="1251">
      <title>Temporal Relation Extraction in Clinical Texts: A Span-based Graph Transformer Approach</title>
      <author><first>Rochana</first><last>Chaturvedi</last></author>
      <author><first>Peyman</first><last>Baghershahi</last></author>
      <author><first>Sourav</first><last>Medya</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <author><first>Barbara</first><last>Di Eugenio</last><affiliation>University of Illinois, Chicago</affiliation></author>
      <pages>25765-25788</pages>
      <abstract>Temporal information extraction from unstructured text is essential for contextualizing events and deriving actionable insights, particularly in the medical domain. We address the task of extracting clinical events and their temporal relations using the well-studied I2B2 2012 Temporal Relations Challenge corpus. This task is inherently challenging due to complex clinical language, long documents, and sparse annotations. We introduce GraphTREx, a novel method integrating span-based entity-relation extraction, clinical large pre-trained language models (LPLMs), and Heterogeneous Graph Transformers (HGT) to capture local and global dependencies. Our HGT component facilitates information propagation across the document through innovative global landmarks that bridge distant entities and improves the state-of-the-art with 5.5% improvement in the tempeval F1 score over the previous best and up to 8.9% improvement on long-range relations, which presents a formidable challenge. We further demonstrate generalizability by establishing a strong baseline on the E3C corpus. Not only does this work advance temporal information extraction, but also lays the groundwork for improved diagnostic and prognostic models through enhanced temporal reasoning.</abstract>
      <url hash="ff86f90a">2025.acl-long.1251</url>
      <bibkey>chaturvedi-etal-2025-temporal</bibkey>
    </paper>
    <paper id="1252">
      <title>Finding A Voice: Exploring the Potential of <fixed-case>A</fixed-case>frican <fixed-case>A</fixed-case>merican Dialect and Voice Generation for Chatbots</title>
      <author><first>Sarah E.</first><last>Finch</last><affiliation>Emory University</affiliation></author>
      <author><first>Ellie S.</first><last>Paek</last></author>
      <author><first>Ikseon</first><last>Choi</last><affiliation>Emory University</affiliation></author>
      <author><first>Jinho D.</first><last>Choi</last><affiliation>Emory University</affiliation></author>
      <pages>25789-25806</pages>
      <abstract>As chatbots become integral to daily life, personalizing systems is key for fostering trust, engagement, and inclusivity. This study examines how linguistic similarity affects chatbot performance, focusing on integrating African American English (AAE) into virtual agents to better serve the African American community. We develop text-based and spoken chatbots using large language models and text-to-speech technology, then evaluate them with AAE speakers against standard English chatbots. Our results show that while text-based AAE chatbots often underperform, spoken chatbots benefit from an African American voice and AAE elements, improving performance and preference. These findings underscore the complexities of linguistic personalization and the dynamics between text and speech modalities, highlighting technological limitations that affect chatbots’ AA speech generation and pointing to promising future research directions.</abstract>
      <url hash="648ebcf6">2025.acl-long.1252</url>
      <bibkey>finch-etal-2025-finding</bibkey>
    </paper>
    <paper id="1253">
      <title>Delta-<fixed-case>KNN</fixed-case>: Improving Demonstration Selection in In-Context Learning for <fixed-case>A</fixed-case>lzheimer’s Disease Detection</title>
      <author><first>Chuyuan</first><last>Li</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Raymond</first><last>Li</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Thalia S.</first><last>Field</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Giuseppe</first><last>Carenini</last><affiliation>, University of British Columbia</affiliation></author>
      <pages>25807-25826</pages>
      <abstract>Alzheimer’s Disease (AD) is a progressive neurodegenerative disorder that leads to dementia, and early intervention can greatly benefit from analyzing linguistic abnormalities. In this work, we explore the potential of Large Language Models as health assistants for AD diagnosis from patient-generated text using in-context learning (ICL), where tasks are defined through a few input-output examples. Empirical results reveal that conventional ICL methods, such as similarity-based selection, perform poorly for AD diagnosis, likely due to the inherent complexity of this task. To address this, we introduce Delta-KNN, a novel demonstration selection strategy that enhances ICL performance. Our method leverages a delta score to assess the relative gains of each training example, coupled with a KNN-based retriever that dynamically selects optimal “representatives” for a given input.Experiments on two AD detection datasets across three models demonstrate that Delta-KNN consistently outperforms existing ICL baselines. Notably, when using the Llama-3.1 model, our approach achieves new state-of-the-art results, surpassing even supervised classifiers.</abstract>
      <url hash="48844bfd">2025.acl-long.1253</url>
      <bibkey>li-etal-2025-delta</bibkey>
    </paper>
    <paper id="1254">
      <title>Help Me Write a Story: Evaluating <fixed-case>LLM</fixed-case>s’ Ability to Generate Writing Feedback</title>
      <author><first>Hannah</first><last>Rashkin</last><affiliation>Google</affiliation></author>
      <author><first>Elizabeth</first><last>Clark</last><affiliation>Google</affiliation></author>
      <author><first>Fantine</first><last>Huot</last><affiliation>Google</affiliation></author>
      <author><first>Mirella</first><last>Lapata</last><affiliation>Edinburgh University, University of Edinburgh</affiliation></author>
      <pages>25827-25847</pages>
      <abstract>Can LLMs provide support to creative writers by giving meaningful writing feedback? In this paper, we explore the challenges and limitations of model-generated writing feedback by defining a new task, dataset, and evaluation frameworks. To study model performance in a controlled manner, we present a novel test set of 1,300 stories that we corrupted to intentionally introduce writing issues. We study the performance of commonly used LLMs in this task with both automatic and human evaluation metrics. Our analysis shows that current models have strong out-of-the-box behavior in many respects—providing specific and mostly accurate writing feedback. However, models often fail to identify the biggest writing issue in the story and to correctly decide when to offer critical vs. positive feedback.</abstract>
      <url hash="6735f798">2025.acl-long.1254</url>
      <bibkey>rashkin-etal-2025-help</bibkey>
    </paper>
    <paper id="1255">
      <title>Language Fusion for Parameter-Efficient Cross-lingual Transfer</title>
      <author><first>Philipp</first><last>Borchert</last><affiliation>IÉSEG School of Management and KU Leuven</affiliation></author>
      <author><first>Ivan</first><last>Vulić</last><affiliation>Google DeepMind and University of Cambridge</affiliation></author>
      <author><first>Marie-Francine</first><last>Moens</last><affiliation>KU Leuven, KU Leuven</affiliation></author>
      <author><first>Jochen</first><last>De Weerdt</last><affiliation>KU Leuven</affiliation></author>
      <pages>25848-25868</pages>
      <abstract>Limited availability of multilingual text corpora for training language models often leads to poor performance on downstream tasks due to undertrained representation spaces for languages other than English. This ‘under-representation’ has motivated recent cross-lingual transfer methods to leverage the English representation space by e.g. mixing English and ‘non-English’ tokens at the input level or extending model parameters to accommodate new languages. However, these approaches often come at the cost of increased computational complexity. We propose Fusion for Language Representations (FLARE) in adapters, a novel method that enhances representation quality and downstream performance for languages other than English while maintaining parameter efficiency. FLARE integrates source and target language representations within low-rank (LoRA) adapters using lightweight linear transformations, maintaining parameter efficiency while improving transfer performance. A series of experiments across representative cross-lingual natural language understanding tasks, including natural language inference, question-answering and sentiment analysis, demonstrate FLARE’s effectiveness. FLARE achieves performance improvements of 4.9% for Llama 3.1 and 2.2% for Gemma 2 compared to standard LoRA fine-tuning on question-answering tasks, as measured by the exact match metric.</abstract>
      <url hash="b0772e4e">2025.acl-long.1255</url>
      <bibkey>borchert-etal-2025-language</bibkey>
    </paper>
    <paper id="1256">
      <title>Culture is Not Trivia: Sociocultural Theory for Cultural <fixed-case>NLP</fixed-case></title>
      <author><first>Naitian</first><last>Zhou</last><affiliation>University of California, Berkeley</affiliation></author>
      <author><first>David</first><last>Bamman</last><affiliation>University of California Berkeley</affiliation></author>
      <author><first>Isaac L.</first><last>Bleaman</last><affiliation>University of California, Berkeley</affiliation></author>
      <pages>25869-25886</pages>
      <abstract>The field of cultural NLP has recently experienced rapid growth, driven by a pressing need to ensure that language technologies are effective and safe across a pluralistic user base. This work has largely progressed without a shared conception of culture, instead choosing to rely on a wide array of cultural proxies. However, this leads to a number of recurring limitations: coarse national boundaries fail to capture nuanced differences that lay within them, limited coverage restricts datasets to only a subset of usually highly-represented cultures, and a lack of dynamicity results in static cultural benchmarks that do not change as culture evolves. In this position paper, we argue that these methodological limitations are symptomatic of a theoretical gap. We draw on a well-developed theory of culture from sociocultural linguistics to fill this gap by 1) demonstrating in a case study how it can clarify methodological constraints and affordances, 2) offering theoretically-motivated paths forward to achieving cultural competence, and 3) arguing that localization is a more useful framing for the goals of much current work in cultural NLP.</abstract>
      <url hash="1d43b5c1">2025.acl-long.1256</url>
      <bibkey>zhou-etal-2025-culture</bibkey>
    </paper>
    <paper id="1257">
      <title><fixed-case>AAD</fixed-case>-<fixed-case>LLM</fixed-case>: Neural Attention-Driven Auditory Scene Understanding</title>
      <author><first>Xilin</first><last>Jiang</last></author>
      <author><first>Sukru Samet</first><last>Dindar</last></author>
      <author><first>Vishal</first><last>Choudhari</last><affiliation>Columbia University</affiliation></author>
      <author><first>Stephan</first><last>Bickel</last><affiliation>Northwell Health</affiliation></author>
      <author><first>Ashesh</first><last>Mehta</last></author>
      <author><first>Guy M</first><last>McKhann</last><affiliation>Columbia University</affiliation></author>
      <author><first>Daniel</first><last>Friedman</last><affiliation>NYU Langone</affiliation></author>
      <author><first>Adeen</first><last>Flinker</last><affiliation>New York University</affiliation></author>
      <author><first>Nima</first><last>Mesgarani</last><affiliation>Columbia University</affiliation></author>
      <pages>25887-25909</pages>
      <abstract>Auditory foundation models, including auditory large language models (LLMs), process all sound inputs equally, independent of listener perception. However, human auditory perception is inherently selective: listeners focus on specific speakers while ignoring others in complex auditory scenes. Existing models do not incorporate this selectivity, limiting their ability to generate perception-aligned responses. To address this, we introduce intention-informed auditory scene understanding (II-ASU) and present Auditory Attention-Driven LLM (AAD-LLM), a prototype system that integrates brain signals to infer listener attention. AAD-LLM extends an auditory LLM by incorporating intracranial electroencephalography (iEEG) recordings to decode which speaker a listener is attending to and refine responses accordingly. The model first predicts the attended speaker from neural activity, then conditions response generation on this inferred attentional state. We evaluate AAD-LLM on speaker description, speech transcription and extraction, and question answering in multitalker scenarios, with both objective and subjective ratings showing improved alignment with listener intention. By taking a first step toward intention-aware auditory AI, this work explores a new paradigm where listener perception informs machine listening, paving the way for future listener-centered auditory systems. Demo available.</abstract>
      <url hash="d1ba799e">2025.acl-long.1257</url>
      <bibkey>jiang-etal-2025-aad</bibkey>
    </paper>
    <paper id="1258">
      <title>Do Language Models Have Semantics? On the Five Standard Positions</title>
      <author><first>Anders</first><last>Søgaard</last><affiliation>Copenhagen University</affiliation></author>
      <pages>25910-25922</pages>
      <abstract>We identify five positions on whether large language models (LLMs) and chatbots can be said to exhibit semantic understanding. These positions differ in whether they attribute semantics to LLMs and/or chatbots trained on feedback, what kind of semantics they attribute (inferential or referential), and in virtue of what they attribute referential semantics (internal or external causes). This allows for 2^^4=16 logically possible positions, but we have only seen people argue for five of these. Based on a pairwise comparison of these five positions, we conclude that the better theory of semantics in large language models is, in fact, a sixth combination: Both large language models and chatbots have inferential and referential semantics, grounded in both internal and external causes.</abstract>
      <url hash="6556ccb5">2025.acl-long.1258</url>
      <bibkey>sogaard-2025-language</bibkey>
    </paper>
    <paper id="1259">
      <title>Dehumanizing Machines: Mitigating Anthropomorphic Behaviors in Text Generation Systems</title>
      <author><first>Myra</first><last>Cheng</last><affiliation>Stanford University</affiliation></author>
      <author><first>Su Lin</first><last>Blodgett</last><affiliation>Microsoft</affiliation></author>
      <author><first>Alicia</first><last>DeVrio</last><affiliation>CMU, Carnegie Mellon University</affiliation></author>
      <author><first>Lisa</first><last>Egede</last></author>
      <author><first>Alexandra</first><last>Olteanu</last><affiliation>Research, Microsoft</affiliation></author>
      <pages>25923-25948</pages>
      <abstract>As text generation systems’ outputs are increasingly anthropomorphic—perceived as human-like—scholars have also increasingly raised concerns about how such outputs can lead to harmful outcomes, such as users over-relying or developing emotional dependence on these systems. How to intervene on such system outputs to mitigate anthropomorphic behaviors and their attendant harmful outcomes, however, remains understudied. With this work, we aim to provide empirical and theoretical grounding for developing such interventions. To do so, we compile an inventory of interventions grounded both in prior literature and a crowdsourcing study where participants edited system outputs to make them less human-like. Drawing on this inventory, we also develop a conceptual framework to help characterize the landscape of possible interventions, articulate distinctions between different types of interventions, and provide a theoretical basis for evaluating the effectiveness of different interventions.</abstract>
      <url hash="72ff62a8">2025.acl-long.1259</url>
      <bibkey>cheng-etal-2025-dehumanizing</bibkey>
    </paper>
    <paper id="1260">
      <title>Evaluating Multimodal Language Models as Visual Assistants for Visually Impaired Users</title>
      <author><first>Antonia</first><last>Karamolegkou</last></author>
      <author><first>Malvina</first><last>Nikandrou</last></author>
      <author><first>Georgios</first><last>Pantazopoulos</last></author>
      <author><first>Danae</first><last>Sanchez Villegas</last></author>
      <author><first>Phillip</first><last>Rust</last></author>
      <author><first>Ruchira</first><last>Dhar</last></author>
      <author><first>Daniel</first><last>Hershcovich</last><affiliation>University of Copenhagen</affiliation></author>
      <author><first>Anders</first><last>Søgaard</last><affiliation>Copenhagen University</affiliation></author>
      <pages>25949-25982</pages>
      <abstract>This paper explores the effectiveness of Multimodal Large Language models (MLLMs) as assistive technologies for visually impaired individuals. We conduct a user survey to identify adoption patterns and key challenges users face with such technologies. Despite a high adoption rate of these models, our findings highlight concerns related to contextual understanding, cultural sensitivity, and complex scene understanding, particularly for individuals who may rely solely on them for visual interpretation. Informed by these results, we collate five user-centred tasks with image and video inputs, including a novel task on Optical Braille Recognition. Our systematic evaluation of twelve MLLMs reveals that further advancements are necessary to overcome limitations related to cultural context, multilingual support, Braille reading comprehension, assistive object recognition, and hallucinations. This work provides critical insights into the future direction of multimodal AI for accessibility, underscoring the need for more inclusive, robust, and trustworthy visual assistance technologies.</abstract>
      <url hash="22a8a133">2025.acl-long.1260</url>
      <bibkey>karamolegkou-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="1261">
      <title><fixed-case>H</fixed-case>um<fixed-case>T</fixed-case> <fixed-case>D</fixed-case>um<fixed-case>T</fixed-case>: Measuring and controlling human-like language in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Myra</first><last>Cheng</last><affiliation>Stanford University</affiliation></author>
      <author><first>Sunny</first><last>Yu</last></author>
      <author><first>Dan</first><last>Jurafsky</last><affiliation>Stanford University</affiliation></author>
      <pages>25983-26008</pages>
      <abstract>Should LLMs generate language that makes them seem human? Human-like language might improve user experience, but might also lead to deception, overreliance, and stereotyping. Assessing these potential impacts requires a systematic way to measure human-like tone in LLM outputs. We introduce HumT and SocioT, metrics for human-like tone and other dimensions of social perceptions in text data based on relative probabilities from an LLM. By measuring HumT across preference and usage datasets, we find that users prefer less human-like outputs from LLMs in many contexts. HumT also offers insights into the perceptions and impacts of anthropomorphism: human-like LLM outputs are highly correlated with warmth, social closeness, femininity, and low status, which are closely linked to the aforementioned harms. We introduce DumT, a method using HumT to systematically control and reduce the degree of human-like tone while preserving model performance. DumT offers a practical approach for mitigating risks associated with anthropomorphic language generation.</abstract>
      <url hash="10178768">2025.acl-long.1261</url>
      <bibkey>cheng-etal-2025-humt</bibkey>
    </paper>
    <paper id="1262">
      <title><fixed-case>C</fixed-case>hat<fixed-case>B</fixed-case>ench: From Static Benchmarks to Human-<fixed-case>AI</fixed-case> Evaluation</title>
      <author><first>Serina</first><last>Chang</last><affiliation>University of California, Berkeley</affiliation></author>
      <author><first>Ashton</first><last>Anderson</last><affiliation>Department of Computer Science, University of Toronto</affiliation></author>
      <author><first>Jake M.</first><last>Hofman</last><affiliation>Microsoft Research</affiliation></author>
      <pages>26009-26038</pages>
      <abstract>With the rapid adoption of LLM-based chat-bots, there is a pressing need to evaluate what humans and LLMs can achieve together. However, standard benchmarks, such as MMLU, measure LLM capabilities in isolation (i.e., “AI-alone”). Here, we design and conduct a user study to convert MMLU questions into user-AI conversations, by seeding the user with the question and having them carry out a conversation with the LLM to answer their question. We release ChatBench, a new dataset with AI-alone, user-alone, and user-AI data for 396 questions and two LLMs, including 144K answers and 7,336 user-AI conversations. We find that AI-alone accuracy fails to predict user-AI accuracy, with significant differences across multiple subjects (math, physics, and moral reasoning), and we analyze the user-AI conversations to provide insight into how they diverge from AI-alone benchmarks. Finally, we show that fine-tuning a user simulator on a subset of ChatBench improves its ability to estimate user-AI accuracies, increasing correlation on held-out questions by more than 20 points, creating possibilities for scaling interactive evaluation.</abstract>
      <url hash="9845d56a">2025.acl-long.1262</url>
      <bibkey>chang-etal-2025-chatbench</bibkey>
    </paper>
    <paper id="1263">
      <title>Teaching an Old <fixed-case>LLM</fixed-case> Secure Coding: Localized Preference Optimization on Distilled Preferences</title>
      <author><first>Mohammad Saqib</first><last>Hasan</last></author>
      <author><first>Saikat</first><last>Chakraborty</last><affiliation>Microsoft Research, Redmond, WA, USA</affiliation></author>
      <author><first>Santu</first><last>Karmaker</last><affiliation>University of Central Florida</affiliation></author>
      <author><first>Niranjan</first><last>Balasubramanian</last><affiliation>State University of New York, Stony Brook</affiliation></author>
      <pages>26039-26057</pages>
      <abstract>LLM generated code often contains security issues. We address two key challenges in improving secure code generation. First, obtaining high quality training data covering a broad set of security issues is critical. To address this, we introduce a method for distilling a preference dataset of insecure and secure code pairs from frontier LLMs, along with a security reasoning that explains the issues and the fix. The key idea here is to make use of security knowledge sources to devise a systematic prompting strategy that ensures broad coverage. Second, aligning models to secure code requires focusing on localized regions of code. Direct preference optimization methods, like SimPO, are not designed to handle these localized differences and turn out to be ineffective. We address this with a new localized preference optimization algorithm that masks the security related tokens in both the winning (secure) and losing (insecure) responses. To prevent loss in code quality, we also add a regularizer. Evaluations show that both training on our dataset, DiSCo, and the new preference optimization algorithm, LPO, yield substantial reductions in code insecurity while also improving overall code quality. Code and dataset are available at https://github.com/StonyBrookNLP/disco-lpo.</abstract>
      <url hash="52c7ba3f">2025.acl-long.1263</url>
      <bibkey>hasan-etal-2025-teaching</bibkey>
    </paper>
    <paper id="1264">
      <title>Anything Goes? A Crosslinguistic Study of (Im)possible Language Learning in <fixed-case>LM</fixed-case>s</title>
      <author><first>Xiulin</first><last>Yang</last></author>
      <author><first>Tatsuya</first><last>Aoyama</last></author>
      <author><first>Yuekun</first><last>Yao</last></author>
      <author><first>Ethan</first><last>Wilcox</last><affiliation>ETHZ - ETH Zurich and Georgetown University</affiliation></author>
      <pages>26058-26077</pages>
      <abstract>Do language models (LMs) offer insights into human language learning? A common argument against this idea is that because their architecture and training paradigm are so vastly different from humans, LMs can learn arbitrary inputs as easily as natural languages. We test this claim by training LMs to model impossible and typologically unattested languages.Unlike previous work, which has focused exclusively on English, we conduct experiments on 12 languages from 4 language families with two newly constructed parallel corpora. Our results show that while GPT-2 small can largely distinguish attested languages from their impossible counterparts, it does not achieve perfect separation between all the attested languages and all the impossible ones. We further test whether GPT-2 small distinguishes typologically attested from unattested languages with different NP orders by manipulating word order based on Greenberg’s Universal 20. We find that the model’s perplexity scores do not distinguish attested vs. unattested word orders, while its performance on the generalization test does. These findings suggest that LMs exhibit some human-like inductive biases, though these biases are weaker than those found in human learners.</abstract>
      <url hash="7f80b954">2025.acl-long.1264</url>
      <bibkey>yang-etal-2025-anything</bibkey>
    </paper>
    <paper id="1265">
      <title>Ranking Unraveled: Recipes for <fixed-case>LLM</fixed-case> Rankings in Head-to-Head <fixed-case>AI</fixed-case> Combat</title>
      <author><first>Roland</first><last>Daynauth</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>Christopher</first><last>Clarke</last><affiliation>University of Michigan - Ann Arbor and University of Guyana</affiliation></author>
      <author><first>Krisztian</first><last>Flautner</last></author>
      <author><first>Lingjia</first><last>Tang</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>Jason</first><last>Mars</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <pages>26078-26091</pages>
      <abstract>Evaluating large language model (LLM) is a complex task. Pairwise ranking has emerged as state-of-the-art method to evaluate human preferences by having humans compare pairs of LLM outputs based on predefined criteria, enabling ranking across multiple LLMs by aggregating pairwise results through algorithms like Elo. However, applying these ranking algorithms in the context of LLM evaluation introduces several challenges, such as inconsistent ranking results when using ELO. Currently there is a lack of systematic study of those ranking algorithms in evaluating LLMs. In this paper, we explore the effectiveness of ranking systems for head-to-head comparisons of LLMs. We formally define a set of fundamental principles for effective ranking and conduct extensive evaluations on the robustness of several ranking algorithms in the context of LLMs. Our analysis uncovers key insights into the factors that affect ranking accuracy and efficiency, offering guidelines for selecting the most appropriate methods based on specific evaluation contexts and resource constraints.</abstract>
      <url hash="37c6bbc0">2025.acl-long.1265</url>
      <bibkey>daynauth-etal-2025-ranking</bibkey>
    </paper>
    <paper id="1266">
      <title><fixed-case>LLM</fixed-case> Agents Making Agent Tools</title>
      <author><first>Georg</first><last>Wölflein</last><affiliation>Technische Universität Dresden and University of St Andrews</affiliation></author>
      <author><first>Dyke</first><last>Ferber</last><affiliation>Technische Universität Dresden</affiliation></author>
      <author><first>Daniel</first><last>Truhn</last><affiliation>University Hospital Aachen</affiliation></author>
      <author><first>Ognjen</first><last>Arandjelovic</last><affiliation>University of St Andrews</affiliation></author>
      <author><first>Jakob Nikolas</first><last>Kather</last><affiliation>Technische Universität Dresden</affiliation></author>
      <pages>26092-26130</pages>
      <abstract>Tool use has turned large language models (LLMs) into powerful agents that can perform complex multi-step tasks by dynamically utilising external software components. However, these tools must be implemented in advance by human developers, hindering the applicability of LLM agents in domains demanding large numbers of highly specialised tools, like in life sciences and medicine. Motivated by the growing trend of scientific studies accompanied by public code repositories, we propose ToolMaker, an agentic framework that autonomously transforms papers with code into LLM-compatible tools. Given a GitHub URL and short task description, ToolMaker autonomously installs dependencies and generates code to perform the task, using a closed-loop self-correction mechanism for debugging. To evaluate our approach, we introduce a benchmark comprising 15 complex computational tasks spanning various domains with over 100 unit tests to assess correctness and robustness. Our method correctly implements 80% of the tasks, substantially outperforming current state-of-the-art software engineering agents. ToolMaker therefore is a step towards fully autonomous agent-based scientific workflows.</abstract>
      <url hash="040de32e">2025.acl-long.1266</url>
      <bibkey>wolflein-etal-2025-llm</bibkey>
    </paper>
    <paper id="1267">
      <title><fixed-case>C</fixed-case>raf<fixed-case>T</fixed-case>ext Benchmark: Advancing Instruction Following in Complex Multimodal Open-Ended World</title>
      <author><first>Zoya</first><last>Volovikova</last></author>
      <author><first>Gregory</first><last>Gorbov</last></author>
      <author><first>Petr</first><last>Kuderov</last><affiliation>Moscow Institute of Physics and Technology, Moscow Institute of Physics and Technology and AIRI</affiliation></author>
      <author><first>Aleksandr</first><last>Panov</last><affiliation>Artificial Intelligence Research Institute and Moscow Institute of Physics and Technology</affiliation></author>
      <author><first>Alexey</first><last>Skrynnik</last><affiliation>AIRI</affiliation></author>
      <pages>26131-26151</pages>
      <abstract>Following instructions in real-world conditions requires a capability to adapt to the world’s volatility and entanglement: the environment is dynamic and unpredictable, instructions can be linguistically complex with diverse vocabulary, and the number of possible goals an agent may encounter is vast. Despite extensive research in this area, most studies are conducted in static environments with simple instructions and a limited vocabulary, making it difficult to assess agent performance in more diverse and challenging settings. To address this gap, we introduce CrafText, a benchmark for evaluating instruction following in a multimodal environment with diverse instructions and dynamic interactions. CrafText includes 3,924 instructions with 3,423 unique words, covering Localization, Conditional, Building, and Achievement tasks. Additionally, we propose an evaluation protocol that measures an agent’s ability to generalize to novel instruction formulations and dynamically evolving task configurations, providing a rigorous test of both linguistic understanding and adaptive decision-making.</abstract>
      <url hash="00cdcb8b">2025.acl-long.1267</url>
      <bibkey>volovikova-etal-2025-craftext</bibkey>
    </paper>
    <paper id="1268">
      <title><fixed-case>QG</fixed-case>-<fixed-case>SMS</fixed-case>: Enhancing Test Item Analysis via Student Modeling and Simulation</title>
      <author><first>Bang</first><last>Nguyen</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Tingting</first><last>Du</last></author>
      <author><first>Mengxia</first><last>Yu</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Lawrence</first><last>Angrave</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Meng</first><last>Jiang</last><affiliation>University of Notre Dame</affiliation></author>
      <pages>26152-26168</pages>
      <abstract>While the Question Generation (QG) task has been increasingly adopted in educational assessments, its evaluation remains limited by approaches that lack a clear connection to the educational values of test items. In this work, we introduce test item analysis, a method frequently used by educators to assess test question quality, into QG evaluation. Specifically, we construct pairs of candidate questions that differ in quality across dimensions such as topic coverage, item difficulty, item discrimination, and distractor efficiency. We then examine whether existing QG evaluation approaches can effectively distinguish these differences. Our findings reveal significant shortcomings in these approaches with respect to accurately assessing test item quality in relation to student performance. To address this gap, we propose a novel QG evaluation framework, QG-SMS, which leverages Large Language Model for Student Modeling and Simulation to perform test item analysis. As demonstrated in our extensive experiments and human evaluation study, the additional perspectives introduced by the simulated student profiles lead to a more effective and robust assessment of test items.</abstract>
      <url hash="420a0b02">2025.acl-long.1268</url>
      <bibkey>nguyen-etal-2025-qg</bibkey>
    </paper>
    <paper id="1269">
      <title>Causal Graph based Event Reasoning using Semantic Relation Experts</title>
      <author><first>Mahnaz</first><last>Koupaee</last></author>
      <author><first>Xueying</first><last>Bai</last></author>
      <author><first>Mudan</first><last>Chen</last></author>
      <author><first>Greg</first><last>Durrett</last><affiliation>University of Texas at Austin</affiliation></author>
      <author><first>Nathanael</first><last>Chambers</last><affiliation>US Naval Academy</affiliation></author>
      <author><first>Niranjan</first><last>Balasubramanian</last><affiliation>State University of New York, Stony Brook</affiliation></author>
      <pages>26169-26199</pages>
      <abstract>Understanding how events in a scenario causally connect with each other is important for effectively modeling and reasoning about events. But event reasoning remains a difficult challenge, and despite recent advances, Large Language Models (LLMs) still struggle to accurately identify causal connections between events. This struggle leads to poor performance on deeper reasoning tasks like event forecasting and timeline understanding. To address this challenge, we investigate the generation of causal event graphs (e.g., A enables B) as a parallel mechanism to help LLMs explicitly represent causality during inference. This paper evaluates both how to generate correct graphs as well as how graphs can assist reasoning. We propose a collaborative approach to causal graph generation where we use LLMs to simulate experts that focus on specific semantic relations. The experts engage in multiple rounds of discussions which are then consolidated by a final expert. Then, to demonstrate the utility of causal graphs, we use them on multiple downstream applications, and also introduce a new explainable event prediction task that requires a causal chain of events in the explanation. These explanations are more informative and coherent than baseline generations. Finally, our overall approach not finetuned on any downstream task, achieves competitive results with state-of-the-art models on both forecasting and next event prediction tasks.</abstract>
      <url hash="93990a70">2025.acl-long.1269</url>
      <bibkey>koupaee-etal-2025-causal</bibkey>
    </paper>
    <paper id="1270">
      <title><fixed-case>L</fixed-case>ogic<fixed-case>P</fixed-case>ro: Improving Complex Logical Reasoning via Program-Guided Learning</title>
      <author><first>Jin</first><last>Jiang</last></author>
      <author><first>Yuchen</first><last>Yan</last></author>
      <author id="yang-liu"><first>Yang</first><last>Liu</last></author>
      <author><first>Jianing</first><last>Wang</last><affiliation>Meituan</affiliation></author>
      <author><first>Shuai</first><last>Peng</last><affiliation>Peking University</affiliation></author>
      <author><first>Xunliang</first><last>Cai</last><affiliation>Meituan</affiliation></author>
      <author><first>Yixin</first><last>Cao</last><affiliation>Fudan University</affiliation></author>
      <author><first>Mengdi</first><last>Zhang</last></author>
      <author><first>Liangcai</first><last>Gao</last></author>
      <pages>26200-26218</pages>
      <abstract>In this paper, we propose a new data synthesis method called <b>LogicPro</b>, which leverages LeetCode-style algorithm Problems and their corresponding Program solutions to synthesize Complex Logical Reasoning data in text format. First, we synthesize complex reasoning problems through source algorithm problems and test cases. Then, standard answers and intermediate variable outputs are obtained for each problem based on standard python solutions and test cases. Finally, with the guidance of code intermediate variables, we synthesize the text reasoning process for each reasoning problems. Through this method, we can synthesize data that is difficult, scalable, effective, and comes with golden standard answers and high-quality reasoning processes. As a result, with our 540K synthesized dataset constructed solely from 2,360 algorithm problems, our approach achieves significant improvements in multiple models for the datasets <i>BBH^27</i>, <i>LogicBench</i>, <i>DROP</i>, <i>AR-LSAT</i>, and <i>GSM8K</i>, etc. outperforming a wide range of existing reasoning datasets.</abstract>
      <url hash="2be15a34">2025.acl-long.1270</url>
      <bibkey>jiang-etal-2025-logicpro</bibkey>
    </paper>
    <paper id="1271">
      <title>Do <fixed-case>LLM</fixed-case>s Understand Dialogues? A Case Study on Dialogue Acts</title>
      <author><first>Ayesha</first><last>Qamar</last><affiliation>Texas A&amp;M University - College Station</affiliation></author>
      <author><first>Jonathan</first><last>Tong</last><affiliation>Texas A&amp;M University - College Station</affiliation></author>
      <author><first>Ruihong</first><last>Huang</last><affiliation>Texas A&amp;M University</affiliation></author>
      <pages>26219-26237</pages>
      <abstract>Recent advancements in NLP, largely driven by Large Language Models (LLMs), have significantly improved performance on an array of tasks. However, Dialogue Act (DA) classification remains challenging, particularly in the fine-grained 50-class, multiparty setting. This paper investigates the root causes of LLMs’ poor performance in DA classification through a linguistically motivated analysis. We identify three key pre-tasks essential for accurate DA prediction: Turn Management, Communicative Function Identification, and Dialogue Structure Prediction. Our experiments reveal that LLMs struggle with these fundamental tasks, often failing to outperform simple rule-based baselines. Additionally, we establish a strong empirical correlation between errors in these pre-tasks and DA classification failures. A human study further highlights the significant gap between LLM and human-level dialogue understanding. These findings indicate that LLMs’ shortcomings in dialogue comprehension hinder their ability to accurately predict DAs, highlighting the need for improved dialogue-aware training approaches.</abstract>
      <url hash="578413ba">2025.acl-long.1271</url>
      <bibkey>qamar-etal-2025-llms</bibkey>
    </paper>
    <paper id="1272">
      <title>Research Borderlands: Analysing Writing Across Research Cultures</title>
      <author><first>Shaily</first><last>Bhatt</last></author>
      <author><first>Tal</first><last>August</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Maria</first><last>Antoniak</last></author>
      <pages>26238-26266</pages>
      <abstract>Improving cultural competence of language technologies is important. However most recent works rarely engage with the communities they study, and instead rely on synthetic setups and imperfect proxies of culture. In this work, we take a human-centered approach to discover and measure language-based cultural norms, and cultural competence of LLMs. We focus on a single kind of culture, *research cultures*, and a single task, *adapting writing across research cultures*. Through a set of interviews with interdisciplinary researchers, who are experts at moving between cultures, we create a framework of structural, stylistic, rhetorical, and citational norms that vary across research cultures. We operationalise these features with a suite of computational metrics and use them for (a) surfacing latent cultural norms in human-written research papers at scale; and (b) highlighting the lack of cultural competence of LLMs, and their tendency to homogenize writing. Overall, our work illustrates the efficacy of a human-centered approach to measuring cultural norms in human-written and LLM-generated texts.</abstract>
      <url hash="2fa448e6">2025.acl-long.1272</url>
      <bibkey>bhatt-etal-2025-research</bibkey>
    </paper>
    <paper id="1273">
      <title><fixed-case>CEAES</fixed-case>: Bidirectional Reinforcement Learning Optimization for Consistent and Explainable Essay Assessment</title>
      <author><first>Xia</first><last>Li</last><affiliation>Guangdong University of Foreign Studies</affiliation></author>
      <author><first>Wenjing</first><last>Pan</last></author>
      <pages>26267-26279</pages>
      <abstract>Most current automated essay quality assessment systems treat score prediction and feedback generation as separate tasks, overlooking the fact that scores provide a quantitative evaluation of quality, while feedback offers a qualitative assessment. Both aspects reflect essay quality from different perspectives, and they are inherently consistent and can reinforce each other. In this paper, we propose a novel bidirectional reinforcement learning framework that effectively utilizes this consistency constraint to jointly optimize score prediction and feedback generation, ensuring mutual reinforcement and alignment between them. In this way, our model is hope to obtain a simultaneous accurate ratings and consistent text feedback. We conducted extensive experiments on publicly available datasets. The results demonstrate that our approach surpasses the current state-of-the-art models, enhancing both scoring accuracy and feedback quality.</abstract>
      <url hash="8ffb05ac">2025.acl-long.1273</url>
      <bibkey>li-pan-2025-ceaes</bibkey>
    </paper>
    <paper id="1274">
      <title><fixed-case>D</fixed-case>e<fixed-case>AL</fixed-case>: Decoding-time Alignment for Large Language Models</title>
      <author><first>James Y.</first><last>Huang</last></author>
      <author><first>Sailik</first><last>Sengupta</last><affiliation>Amazon</affiliation></author>
      <author><first>Daniele</first><last>Bonadiman</last><affiliation>Amazon</affiliation></author>
      <author><first>Yi-An</first><last>Lai</last></author>
      <author><first>Arshit</first><last>Gupta</last><affiliation>Amazon</affiliation></author>
      <author><first>Nikolaos</first><last>Pappas</last><affiliation>AWS AI Labs</affiliation></author>
      <author><first>Saab</first><last>Mansour</last><affiliation>Amazon</affiliation></author>
      <author><first>Katrin</first><last>Kirchhoff</last><affiliation>Oracle</affiliation></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>26280-26300</pages>
      <abstract>Large Language Models (LLMs) are nowadays expected to generate content aligned with human preferences. Current work focuses on alignment at model training time, through techniques such as Reinforcement Learning with Human Feedback (RLHF). However, it is unclear if such methods are an effective choice to teach alignment objectives to the model. First, the inability to incorporate multiple, custom rewards and reliance on a model developer’s view of universal and static principles are key limitations. Second, the reliability of such approaches is also questionable (e.g. susceptibility to jailbreaking even after safety training). To address these issues, we propose DeAL, a framework that allows the user to customize reward functions and enables Decoding-time Alignment of LLMs (DeAL). At its core, we view decoding as a heuristic-guided search process and facilitate the use of a wide variety of alignment objectives. Our experiments with programmatic constraints such as keyword and length constraints, and abstract objectives such as harmlessness and helpfulness, show that we can DeAL with fine-grained trade-offs and improve adherence to alignment objectives. Lastly, we demonstrate that DeAL is largely complementary to existing alignment strategies, and can be effectively paired with RLHF and prompting techniques to achieve better alignment.</abstract>
      <url hash="f5c70385">2025.acl-long.1274</url>
      <bibkey>huang-etal-2025-deal</bibkey>
    </paper>
    <paper id="1275">
      <title>Cultural Bias Matters: A Cross-Cultural Benchmark Dataset and Sentiment-Enriched Model for Understanding Multimodal Metaphors</title>
      <author><first>Senqi</first><last>Yang</last></author>
      <author><first>Dongyu</first><last>Zhang</last><affiliation>Dalian University of Technology</affiliation></author>
      <author><first>Jing</first><last>Ren</last><affiliation>Royal Melbourne Institute of Technology</affiliation></author>
      <author><first>Ziqi</first><last>Xu</last><affiliation>Royal Melbourne Institute of Technology</affiliation></author>
      <author><first>Xiuzhen</first><last>Zhang</last><affiliation>Royal Melbourne Institute of Technology</affiliation></author>
      <author><first>Yiliao</first><last>Song</last><affiliation>University of Adelaide and Royal Melbourne Institute of Technology</affiliation></author>
      <author><first>Hongfei</first><last>Lin</last></author>
      <author><first>Feng</first><last>Xia</last><affiliation>Royal Melbourne Institute of Technology</affiliation></author>
      <pages>26301-26317</pages>
      <abstract>Metaphors are pervasive in communication, making them crucial for natural language processing (NLP). Previous research on automatic metaphor processing predominantly relies on training data consisting of English samples, which often reflect Western European or North American biases. This cultural skew can lead to an overestimation of model performance and contributions to NLP progress. However, the impact of cultural bias on metaphor processing, particularly in multimodal contexts, remains largely unexplored. To address this gap, we introduce MultiMM, a Multicultural Multimodal Metaphor dataset designed for cross-cultural studies of metaphor in Chinese and English. MultiMM consists of 8,461 text-image advertisement pairs, each accompanied by fine-grained annotations, providing a deeper understanding of multimodal metaphors beyond a single cultural domain. Additionally, we propose Sentiment-Enriched Metaphor Detection (SEMD), a baseline model that integrates sentiment embeddings to enhance metaphor comprehension across cultural backgrounds. Experimental results validate the effectiveness of SEMD on metaphor detection and sentiment analysis tasks. We hope this work increases awareness of cultural bias in NLP research and contributes to the development of fairer and more inclusive language models.</abstract>
      <url hash="456bc3d8">2025.acl-long.1275</url>
      <bibkey>yang-etal-2025-cultural</bibkey>
    </paper>
    <paper id="1276">
      <title><fixed-case>O</fixed-case>mni<fixed-case>C</fixed-case>haracter: Towards Immersive Role-Playing Agents with Seamless Speech-Language Personality Interaction</title>
      <author><first>Haonan</first><last>Zhang</last></author>
      <author><first>Run</first><last>Luo</last></author>
      <author><first>Xiong</first><last>Liu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yuchuan</first><last>Wu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Ting-En</first><last>Lin</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Pengpeng</first><last>Zeng</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Qiang</first><last>Qu</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Feiteng</first><last>Fang</last></author>
      <author><first>Min</first><last>Yang</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Lianli</first><last>Gao</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Jingkuan</first><last>Song</last><affiliation>University of Electronic Science and Technology of China,</affiliation></author>
      <author><first>Fei</first><last>Huang</last><affiliation>Alibaba Group US</affiliation></author>
      <author><first>Yongbin</first><last>Li</last><affiliation>Alibaba Group</affiliation></author>
      <pages>26318-26331</pages>
      <abstract>Role-Playing Agents (RPAs), benefiting from large language models, is an emerging interactive AI system that simulates roles or characters with diverse personalities. However, existing methods primarily focus on mimicking dialogues among roles in textual form, neglecting the role’s voice traits (e.g., voice style and emotions) as playing a crucial effect in interaction, which tends to be more immersive experiences in realistic scenarios. Towards this goal, we propose OmniCharacter, a first seamless speech-language personality interaction model to achieve immersive RPAs with low latency. Specifically, OmniCharacter enables agents to consistently exhibit role-specific personality traits and vocal traits throughout the interaction, enabling a mixture of speech and language responses. To align the model with speech-language scenarios, we construct a dataset named OmniCharacter-10K, which involves more distinctive characters (20), richly contextualized multi-round dialogue (10K), and dynamic speech response (135K). Experimental results showcase that our method yields better responses in terms of both content and style compared to existing RPAs and mainstream speech-language models, with a response latency as low as 289ms.</abstract>
      <url hash="f121f4d8">2025.acl-long.1276</url>
      <bibkey>zhang-etal-2025-omnicharacter</bibkey>
    </paper>
    <paper id="1277">
      <title>Mixtures of In-Context Learners</title>
      <author><first>Giwon</first><last>Hong</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Emile</first><last>Van Krieken</last><affiliation>Edinburgh University, University of Edinburgh</affiliation></author>
      <author><first>Edoardo</first><last>Ponti</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Nikolay</first><last>Malkin</last><affiliation>Edinburgh University, University of Edinburgh</affiliation></author>
      <author><first>Pasquale</first><last>Minervini</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <pages>26332-26351</pages>
      <abstract>In-context learning (ICL) adapts LLMs by providing demonstrations without fine-tuning the model parameters; however, it is very sensitive to the choice of in-context demonstrations, and processing many demonstrations can be computationally demanding. We propose Mixtures of In-Context Learners (MoICL), a novel approach that uses subsets of demonstrations to train a set of experts via ICL and learns a weighting function to merge their output distributions via gradient-based optimisation. In our experiments, we show performance improvements on 5 out of 7 classification datasets compared to a set of strong baselines (e.g., up to +13% compared to ICL and LENS). Moreover, we improve the Pareto frontier of ICL by reducing the inference time needed to achieve the same performance with fewer demonstrations. Finally, MoICL is more robust to out-of-domain (up to +11%), imbalanced (up to +49%) and perturbed demonstrations (up to +38%).</abstract>
      <url hash="e5ec0dac">2025.acl-long.1277</url>
      <bibkey>hong-etal-2025-mixtures</bibkey>
    </paper>
    <paper id="1278">
      <title>Balancing Diversity and Risk in <fixed-case>LLM</fixed-case> Sampling: How to Select Your Method and Parameter for Open-Ended Text Generation</title>
      <author><first>Yuxuan</first><last>Zhou</last></author>
      <author><first>Margret</first><last>Keuper</last><affiliation>Universität Mannheim and Max Planck Institute for Informatics</affiliation></author>
      <author><first>Mario</first><last>Fritz</last><affiliation>CISPA Helmholtz Center for Information Security and Saarland University</affiliation></author>
      <pages>26352-26365</pages>
      <abstract>Sampling-based decoding strategies have been widely adopted for Large Language Models (LLMs) in numerous applications, targeting a balance between diversity and quality via temperature tuning and tail truncation. Considering the strong dependency of the candidate next tokens on different prefixes, recent studies propose to adaptively truncate the tail of LLMs’ predicted distribution. Although improved results have been reported with these methods on open-ended text generation tasks, the results are highly dependent on the curated parameters and the limited exemplar text. In this paper, we propose a systematic way to estimate the intrinsic capacity of a truncation sampling method by considering the trade-off between diversity and risk at each decoding step, based on our collected prefix tree which preserves the context of a full sentence. Our work offers a comprehensive comparison of existing truncation sampling methods and serves as a practical user guideline for their parameter selection. Our code is available at https://anonymous.4open.science/r/Truncation-Sampling-Evaluation-251F.</abstract>
      <url hash="4ad17c17">2025.acl-long.1278</url>
      <bibkey>zhou-etal-2025-balancing</bibkey>
    </paper>
    <paper id="1279">
      <title><fixed-case>RADAR</fixed-case>: Enhancing Radiology Report Generation with Supplementary Knowledge Injection</title>
      <author><first>Wenjun</first><last>Hou</last></author>
      <author><first>Yi</first><last>Cheng</last></author>
      <author><first>Kaishuai</first><last>Xu</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Heng</first><last>Li</last><affiliation>Sudan University for Science and Technology</affiliation></author>
      <author><first>Yan</first><last>Hu</last><affiliation>Southern University of Science and Technology</affiliation></author>
      <author><first>Wenjie</first><last>Li</last><affiliation>The Hong Kong Polytechnic University, The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Jiang</first><last>Liu</last><affiliation>Southern University of Science and Technology</affiliation></author>
      <pages>26366-26381</pages>
      <abstract>Large language models (LLMs) have demonstrated remarkable capabilities in various domains, including radiology report generation. Previous approaches have attempted to utilize multimodal LLMs for this task, enhancing their performance through the integration of domain-specific knowledge retrieval. However, these approaches often overlook the knowledge already embedded within the LLMs, leading to redundant information integration. To address this limitation, we propose Radar, a framework for enhancing radiology report generation with supplementary knowledge injection. Radar improves report generation by systematically leveraging both the internal knowledge of an LLM and externally retrieved information. Specifically, it first extracts the model’s acquired knowledge that aligns with expert image-based classification outputs. It then retrieves relevant supplementary knowledge to further enrich this information. Finally, by aggregating both sources, Radar generates more accurate and informative radiology reports. Extensive experiments on MIMIC-CXR, CheXpert-Plus, and IU X-ray demonstrate that our model outperforms state-of-the-art LLMs in both language quality and clinical accuracy</abstract>
      <url hash="e60bc1cc">2025.acl-long.1279</url>
      <bibkey>hou-etal-2025-radar</bibkey>
    </paper>
    <paper id="1280">
      <title>Can <fixed-case>LLM</fixed-case>s Deceive <fixed-case>CLIP</fixed-case>? Benchmarking Adversarial Compositionality of Pre-trained Multimodal Representation via Text Updates</title>
      <author><first>Jaewoo</first><last>Ahn</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Heeseung</first><last>Yun</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Dayoon</first><last>Ko</last></author>
      <author><first>Gunhee</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <pages>26382-26402</pages>
      <abstract>While pre-trained multimodal representations (e.g., CLIP) have shown impressive capabilities, they exhibit significant compositional vulnerabilities leading to counterintuitive judgments. We introduce Multimodal Adversarial Compositionality (MAC), a benchmark that leverages large language models (LLMs) to generate deceptive text samples to exploit these vulnerabilities across different modalities and evaluates them through both sample-wise attack success rate and group-wise entropy-based diversity. To improve zero-shot methods, we propose a self-training approach that leverages rejection-sampling fine-tuning with diversity-promoting filtering, which enhances both attack success rate and sample diversity. Using smaller language models like Llama-3.1-8B, our approach demonstrates superior performance in revealing compositional vulnerabilities across various multimodal representations, including images, videos, and audios.</abstract>
      <url hash="7be36943">2025.acl-long.1280</url>
      <bibkey>ahn-etal-2025-llms</bibkey>
    </paper>
    <paper id="1281">
      <title>Attention Speaks Volumes: Localizing and Mitigating Bias in Language Models</title>
      <author><first>Rishabh</first><last>Adiga</last></author>
      <author><first>Besmira</first><last>Nushi</last><affiliation>Microsoft</affiliation></author>
      <author><first>Varun</first><last>Chandrasekaran</last><affiliation>University of Illinois Urbana-Champaign</affiliation></author>
      <pages>26403-26423</pages>
      <abstract>We believe that analyzing attention is crucial for understanding bias in large language models (LLMs); in ambiguous comparative prompting frameworks, it provides insight into how the LLM distributes its focus across different entities, and how this contributes to biased decisions. To this end, we first introduce a metric to quantify the “entity preference” of an LLM. We then propose <tex-math>\textbf{ATLAS}</tex-math>, a technique to localize bias to specific layers of the LLM by analyzing attention scores and then reduce bias by scaling attention in these biased layers. To evaluate our method, we conduct extensive experiments across 3 datasets, 4 models, and 4 baseline approaches. Our experiments demonstrate that bias is concentrated in the later layers, typically around the last third. We also show how <tex-math>\textbf{ATLAS}</tex-math> effectively mitigates bias through targeted interventions without compromising downstream performance and an average increase of only 0.34% in perplexity when the intervention is applied. We see an average improvement of 0.28 points in the bias score across all the datasets.</abstract>
      <url hash="d0fd0d0b">2025.acl-long.1281</url>
      <bibkey>adiga-etal-2025-attention</bibkey>
    </paper>
    <paper id="1282">
      <title><fixed-case>MTSA</fixed-case>: Multi-turn Safety Alignment for <fixed-case>LLM</fixed-case>s through Multi-round Red-teaming</title>
      <author><first>Weiyang</first><last>Guo</last></author>
      <author><first>Jing</first><last>Li</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Wenya</first><last>Wang</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Yu</first><last>Li</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Daojing</first><last>He</last></author>
      <author><first>Jun</first><last>Yu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>26424-26442</pages>
      <abstract>The proliferation of jailbreak attacks against large language models (LLMs) highlights the need for robust security measures. However, in multi-round dialogues, malicious intentions may be hidden in interactions, leading LLMs to be more prone to produce harmful responses. In this paper, we propose the Multi-Turn Safety Alignment (MTSA) framework, to address the challenge of securing LLMs in multi-round interactions. It consists of two stages: In the thought-guided attack learning stage, the red-team model learns about thought-guided multi-round jailbreak attacks to generate adversarial prompts. In the adversarial iterative optimization stage, the red-team model and the target model continuously improve their respective capabilities in interaction. Furthermore, we introduce a multi-turn reinforcement learning algorithm based on future rewards to enhance the robustness of safety alignment. Experimental results show that the red-team model exhibits state-of-the-art attack capabilities, while the target model significantly improves its performance on safety benchmarks.</abstract>
      <url hash="4807e2f8">2025.acl-long.1282</url>
      <bibkey>guo-etal-2025-mtsa</bibkey>
    </paper>
    <paper id="1283">
      <title>The Efficiency vs. Accuracy Trade-off: Optimizing <fixed-case>RAG</fixed-case>-Enhanced <fixed-case>LLM</fixed-case> Recommender Systems Using Multi-Head Early Exit</title>
      <author><first>Huixue</first><last>Zhou</last></author>
      <author><first>Hengrui</first><last>Gu</last><affiliation>Jilin University</affiliation></author>
      <author><first>Zaifu</first><last>Zhan</last></author>
      <author><first>Xi</first><last>Liu</last><affiliation>Meta AI</affiliation></author>
      <author><first>Kaixiong</first><last>Zhou</last><affiliation>North Carolina State University</affiliation></author>
      <author><first>Yongkang</first><last>Xiao</last></author>
      <author><first>Mingfu</first><last>Liang</last><affiliation>Meta</affiliation></author>
      <author><first>Srinivas Prasad</first><last>Govindan</last></author>
      <author><first>Piyush</first><last>Chawla</last></author>
      <author><first>Jiyan</first><last>Yang</last><affiliation>Meta</affiliation></author>
      <author><first>Xiangfei</first><last>Meng</last><affiliation>Facebook</affiliation></author>
      <author><first>Huayu</first><last>Li</last><affiliation>Meta</affiliation></author>
      <author><first>Buyun</first><last>Zhang</last><affiliation>Meta</affiliation></author>
      <author><first>Liang</first><last>Luo</last><affiliation>Meta</affiliation></author>
      <author><first>Wen-Yen</first><last>Chen</last><affiliation>Meta</affiliation></author>
      <author><first>Yiping</first><last>Han</last></author>
      <author><first>Bo</first><last>Long</last><affiliation>Meta</affiliation></author>
      <author><first>Rui</first><last>Zhang</last><affiliation>University of Minnesota - Twin Cities</affiliation></author>
      <author><first>Tianlong</first><last>Chen</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <pages>26443-26458</pages>
      <abstract>The deployment of Large Language Models (LLMs) in recommender systems for Click-Through Rate (CTR) prediction requires a careful balance between computational efficiency and predictive accuracy. This paper introduces OptiRAG-Rec, a comprehensive framework that integrates Retrieval-Augmented Generation (RAG) with a novel multi-head early exit architecture to address both challenges. By leveraging Graph Convolutional Networks (GCNs) as efficient retrieval mechanisms, the framework significantly reduces data retrieval times while maintaining high model performance. Additionally, the multi-head early exit strategy dynamically terminates inference based on real-time predictive confidence assessments, enhancing responsiveness without sacrificing accuracy. Experimental results demonstrate that OptiRAG-Rec reduces computation time while preserving the precision required for reliable recommendations, establishing a new benchmark for efficient and accurate LLM deployment in recommendation.</abstract>
      <url hash="6ad55d3c">2025.acl-long.1283</url>
      <bibkey>zhou-etal-2025-efficiency</bibkey>
    </paper>
    <paper id="1284">
      <title>Unraveling <fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case> Interference: Orthogonal Subspaces for Robust Model Merging</title>
      <author><first>Haobo</first><last>Zhang</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>Jiayu</first><last>Zhou</last><affiliation>University of Michigan - Ann Arbor and Michigan State University</affiliation></author>
      <pages>26459-26472</pages>
      <abstract>Fine-tuning large language models (LMs) for individual tasks yields strong performance but is expensive for deployment and storage. Recent works explore model merging to combine multiple task-specific models into a single multi-task model without additional training. However, existing merging methods often fail for models fine-tuned with low-rank adaptation (LoRA), due to significant performance degradation. In this paper, we show that this issue arises from a previously overlooked interplay between model parameters and data distributions. We propose **O**rthogonal **S**ubspaces for **R**obust model **M**erging (**OSRM**) to constrain the LoRA subspace *prior* to fine-tuning, ensuring that updates relevant to one task do not adversely shift outputs for others. Our approach can seamlessly integrate with most existing merging algorithms, reducing the unintended interference among tasks. Extensive experiments on eight datasets, tested with three widely used LMs and two large LMs, demonstrate that our method not only boosts merging performance but also preserves single-task accuracy. Furthermore, our approach exhibits greater robustness to the hyperparameters of merging. These results highlight the importance of data-parameter interaction in model merging and offer a plug-and-play solution for merging LoRA models.</abstract>
      <url hash="4035ac80">2025.acl-long.1284</url>
      <bibkey>zhang-zhou-2025-unraveling</bibkey>
    </paper>
    <paper id="1285">
      <title><fixed-case>BIG</fixed-case>-Bench Extra Hard</title>
      <author><first>Mehran</first><last>Kazemi</last><affiliation>Google</affiliation></author>
      <author><first>Bahare</first><last>Fatemi</last><affiliation>Google</affiliation></author>
      <author><first>Hritik</first><last>Bansal</last></author>
      <author><first>John</first><last>Palowitch</last><affiliation>Google</affiliation></author>
      <author><first>Chrysovalantis</first><last>Anastasiou</last><affiliation>Google</affiliation></author>
      <author><first>Sanket Vaibhav</first><last>Mehta</last><affiliation>Google</affiliation></author>
      <author><first>Lalit K</first><last>Jain</last><affiliation>Google</affiliation></author>
      <author><first>Virginia</first><last>Aglietti</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Disha</first><last>Jindal</last><affiliation>Google</affiliation></author>
      <author><first>Peter</first><last>Chen</last><affiliation>Google</affiliation></author>
      <author><first>Nishanth</first><last>Dikkala</last><affiliation>Google</affiliation></author>
      <author><first>Gladys</first><last>Tyen</last><affiliation>Google</affiliation></author>
      <author><first>Xin</first><last>Liu</last><affiliation>Google</affiliation></author>
      <author><first>Uri</first><last>Shalit</last><affiliation>Tel Aviv University and Google</affiliation></author>
      <author><first>Silvia</first><last>Chiappa</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Kate</first><last>Olszewska</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Yi</first><last>Tay</last><affiliation>Google</affiliation></author>
      <author><first>Vinh Q.</first><last>Tran</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Quoc V</first><last>Le</last><affiliation>Google</affiliation></author>
      <author><first>Orhan</first><last>Firat</last><affiliation>Google</affiliation></author>
      <pages>26473-26501</pages>
      <abstract>Current benchmarks for large language model (LLM) reasoning predominantly focus on mathematical and coding abilities, leaving a gap in evaluating broader reasoning proficiencies. One particular exception is the BIG-Bench dataset, which has served as a crucial benchmark for evaluating the general reasoning capabilities of LLMs, thanks to its diverse set of challenging tasks that allowed for a comprehensive assessment of general reasoning across various skills within a unified framework. However, recent advances in LLMs have led to saturation on BIG-Bench, and its harder version BIG-Bench Hard (BBH). State-of-the-art models achieve near-perfect scores on many tasks in BBH, thus diminishing its utility. To address this limitation, we introduce BIG-Bench Extra Hard (BBEH), a new benchmark designed to push the boundaries of LLM reasoning evaluation. BBEH replaces each task in BBH with a novel task that probes a similar reasoning capability but exhibits significantly increased difficulty. We evaluate various general-purpose and reasoning-specialized models on BBEH and observe an accuracy of 23.9% for the best general-purpose model and 54.2% for the best reasoning-specialized model, indicating substantial room for improvement and highlighting the ongoing challenge of achieving robust general reasoning in LLMs. We release BBEH publicly at: https://github.com/google-deepmind/bbeh.</abstract>
      <url hash="c4e626a6">2025.acl-long.1285</url>
      <bibkey>kazemi-etal-2025-big</bibkey>
    </paper>
    <paper id="1286">
      <title><fixed-case>CST</fixed-case>ree-<fixed-case>SRI</fixed-case>: Introspection-Driven Cognitive Semantic Tree for Multi-Turn Question Answering over Extra-Long Contexts</title>
      <author><first>Zhaowen</first><last>Wang</last><affiliation>Central South University</affiliation></author>
      <author><first>Xiang</first><last>Wei</last></author>
      <author><first>Kangshao</first><last>Du</last><affiliation>Central South University</affiliation></author>
      <author><first>Yiting</first><last>Zhang</last></author>
      <author><first>Libo</first><last>Qin</last><affiliation>Central South University</affiliation></author>
      <author><first>Yingjie</first><last>Xia</last><affiliation>Hangzhou Dianzi University</affiliation></author>
      <author><first>Li</first><last>Kuang</last><affiliation>Central South University</affiliation></author>
      <pages>26502-26525</pages>
      <abstract>Large Language Models (LLMs) have achieved remarkable success in natural language processing (NLP), particularly in single-turn question answering (QA) on short-text. However, their performance significantly declines when applied to multi-turn QA over extra-long context (ELC), as they struggle to capture the logical correlations across multiple chunks of ELC and maintain the coherence of multi-turn Questions. To address the challenges, we propose the CSTree-SRI framework (Cognitive Semantic Tree through Summarization, Retrieval, and Introspection). CSTree-SRI dynamically constructs the CSTree to preserve logical coherence within ELC through hierarchical synthesis and introspective validation. Then a logic-driven traversal strategy on CSTree is designed to provide efficient information retrieval for question answering. Additionally, we construct a suite of multi-turn QA datasets and an evaluation benchmark tailored for ELC tasks, and comprehensive experiments demonstrate the framework’s superiority in addressing the challenges of multi-turn QA over ELC.</abstract>
      <url hash="cfc5d494">2025.acl-long.1286</url>
      <bibkey>wang-etal-2025-cstree</bibkey>
    </paper>
    <paper id="1287">
      <title><fixed-case>I</fixed-case>nduction<fixed-case>B</fixed-case>ench: <fixed-case>LLM</fixed-case>s Fail in the Simplest Complexity Class</title>
      <author><first>Wenyue</first><last>Hua</last></author>
      <author><first>Tyler</first><last>Wong</last></author>
      <author><first>Fei</first><last>Sun</last><affiliation>Independent</affiliation></author>
      <author><first>Liangming</first><last>Pan</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Adam</first><last>Jardine</last><affiliation>Rutgers University</affiliation></author>
      <author><first>William Yang</first><last>Wang</last><affiliation>UC Santa Barbara</affiliation></author>
      <pages>26526-26546</pages>
      <abstract>Large language models (LLMs) have shown remarkable improvements in reasoning and many existing benchmarks have been addressed by models such as o1 and o3 either fully or partially. However, a majority of these benchmarks emphasize deductive reasoning, including mathematical and coding tasks in which rules such as mathematical axioms or programming syntax are clearly defined, based on which LLMs can plan and apply these rules to arrive at a solution. In contrast, <i>inductive reasoning</i>, where one infers the underlying rules from observed data, remains less explored. Such inductive processes lie at the heart of scientific discovery, as they enable researchers to extract general principles from empirical observations. To assess whether LLMs possess this capacity, we introduce <b>InductionBench</b>, a new benchmark designed to evaluate the inductive reasoning ability of LLMs. Our experimental findings reveal that even the most advanced modelw available struggle to master the simplest complexity classes within the subregular hierarchy of functions, highlighting a notable deficiency in current LLMs’ inductive reasoning capabilities. Coda and data are available <url>https://anonymous.4open.science/r/inductive_reasoning_benchmark-BB2D</url>.</abstract>
      <url hash="4ac1a81e">2025.acl-long.1287</url>
      <bibkey>hua-etal-2025-inductionbench</bibkey>
    </paper>
    <paper id="1288">
      <title><fixed-case>RATIONALYST</fixed-case>: Pre-training Process-Supervision for Improving Reasoning</title>
      <author><first>Dongwei</first><last>Jiang</last></author>
      <author><first>Guoxuan</first><last>Wang</last></author>
      <author><first>Yining</first><last>Lu</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Andrew</first><last>Wang</last><affiliation>Department of Computer Science, Whiting School of Engineering</affiliation></author>
      <author><first>Jingyu</first><last>Zhang</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Chuyu</first><last>Liu</last></author>
      <author><first>Benjamin</first><last>Van Durme</last><affiliation>Microsoft and Johns Hopkins University</affiliation></author>
      <author><first>Daniel</first><last>Khashabi</last><affiliation>Johns Hopkins University</affiliation></author>
      <pages>26547-26566</pages>
      <abstract>The reasoning steps generated by LLMs might be incomplete, as they mimic logical leaps common in everyday communication found in their pre-training data: underlying rationales are frequently left implicit (unstated). To address this challenge, we introduce RATIONALYST, a model for process-supervision of reasoning based on pre-training on a vast collection of rationale annotations extracted from unlabeled data. We extract 79k rationales from web-scale unlabelled dataset (the Pile) and a combination of reasoning datasets with minimal human intervention. This web-scale pre-training for reasoning allows RATIONALYST to consistently generalize across diverse reasoning tasks, including mathematical, commonsense, scientific, and logical reasoning. Fine-tuned from LLaMa-3-8B, RATIONALYST improves the accuracy of reasoning by an average of 3.9% on 7 representative reasoning benchmarks. It also demonstrates superior performance compared to significantly larger verifiers like GPT-4 and similarly sized models fine-tuned on matching training sets.</abstract>
      <url hash="5a0534cc">2025.acl-long.1288</url>
      <bibkey>jiang-etal-2025-rationalyst</bibkey>
    </paper>
    <paper id="1289">
      <title>Make Imagination Clearer! Stable Diffusion-based Visual Imagination for Multimodal Machine Translation</title>
      <author><first>Andong</first><last>Chen</last></author>
      <author><first>Yuchen</first><last>Song</last></author>
      <author><first>Kehai</first><last>Chen</last><affiliation>Harbin Institute of Technology (Shenzhen)</affiliation></author>
      <author><first>Xuefeng</first><last>Bai</last></author>
      <author><first>Muyun</first><last>Yang</last></author>
      <author><first>Liqiang</first><last>Nie</last><affiliation>Harbin Institute of Technology (Shenzhen) and Shandong University</affiliation></author>
      <author><first>Jie</first><last>Liu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Tiejun</first><last>Zhao</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>26567-26583</pages>
      <abstract>Visual information has been introduced for enhancing machine translation (MT), and its effectiveness heavily relies on the availability of large amounts of bilingual parallel sentence pairs with manual image annotations. In this paper, we introduce a stable diffusion-based imagination network into a multimodal large language model (MLLM) to explicitly generate an image for each source sentence, thereby advancing the multimodel MT. Particularly, we build heuristic feedback with reinforcement learning to ensure the consistency of the generated image with the source sentence without the supervision of visual information, which breaks the high-cost bottleneck of image annotation in MT. Furthermore, the proposed method enables imaginative visual information to be integrated into text-only MT in addition to multimodal MT. Experimental results show that our model significantly outperforms existing multimodal MT and text-only MT, especially achieving an average improvement of more than 14 BLEU points on Multi30K and MSCOCO multimodal MT benchmarks.</abstract>
      <url hash="26004d9a">2025.acl-long.1289</url>
      <bibkey>chen-etal-2025-make</bibkey>
    </paper>
    <paper id="1290">
      <title>Advancing <fixed-case>SM</fixed-case>o<fixed-case>E</fixed-case> for Continuous Domain Adaptation of <fixed-case>MLLM</fixed-case>s: Adaptive Router and Domain-Specific Loss</title>
      <author><first>Liang</first><last>Zhang</last></author>
      <author><first>Ziyao</first><last>Lu</last><affiliation>WeChat AI</affiliation></author>
      <author><first>Fandong</first><last>Meng</last><affiliation>WeChat AI, Tencent Inc.</affiliation></author>
      <author><first>Hui</first><last>Li</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <author><first>Jinsong</first><last>Su</last><affiliation>Xiamen University</affiliation></author>
      <pages>26584-26602</pages>
      <abstract>Recent studies have explored Continual Instruction Tuning (CIT) in Multimodal Large Language Models (MLLMs), with a primary focus on Task-incremental CIT, where MLLMs are required to continuously acquire new tasks. However, the more practical and challenging Domain-incremental CIT, focused on the continual adaptation of MLLMs to new domains, remains underexplored. In this paper, we propose a new Sparse Mixture of Expert (SMoE) based method for domain-incremental CIT in MLLMs. During training, we learn a domain-specific SMoE module for each new domain in every FFN sub-layer of MLLMs, preventing catastrophic forgetting caused by inter-domain conflicts. Moreover, we equip the SMoE module with a domain-specific autoregressive loss (DSAL), which is used to identify the most suitable SMoE module for processing each test instruction during inference. To further enhance the SMoE module’s ability to learn domain knowledge, we design an adaptive threshold-based router (AT-Router) that allocates computing resources (experts) to instruction tokens based on their importance. Finally, we establish a new benchmark to evaluate the efficacy of our method and advance future research. Extensive experiments show that our method consistently outperforms all competitive baselines.</abstract>
      <url hash="e09bccbd">2025.acl-long.1290</url>
      <bibkey>zhang-etal-2025-advancing-smoe</bibkey>
    </paper>
    <paper id="1291">
      <title>Multi-document Summarization through Multi-document Event Relation Graph Reasoning in <fixed-case>LLM</fixed-case>s: a case study in Framing Bias Mitigation</title>
      <author><first>Yuanyuan</first><last>Lei</last></author>
      <author><first>Ruihong</first><last>Huang</last><affiliation>Texas A&amp;M University</affiliation></author>
      <pages>26603-26619</pages>
      <abstract>Media outlets are becoming more partisan and polarized nowadays. Most previous work focused on detecting media bias. In this paper, we aim to mitigate media bias by generating a neutralized summary given multiple articles presenting different ideological views. Motivated by the critical role of events and event relations in media bias detection, we propose to increase awareness of bias in LLMs via multi-document events reasoning and use a multi-document event relation graph to guide the summarization process. This graph contains rich event information useful to reveal bias: four common types of in-doc event relations to reflect content framing bias, cross-doc event coreference relation to reveal content selection bias, and event-level moral opinions to highlight opinionated framing bias. We further develop two strategies to incorporate the multi-document event relation graph for neutralized summarization. Firstly, we convert a graph into natural language descriptions and feed the textualized graph into LLMs as a part of a hard text prompt. Secondly, we encode the graph with graph attention network and insert the graph embedding into LLMs as a soft prompt. Both automatic evaluation and human evaluation confirm that our approach effectively mitigates both lexical and informational media bias, and meanwhile improves content preservation.</abstract>
      <url hash="3b899335">2025.acl-long.1291</url>
      <bibkey>lei-huang-2025-multi</bibkey>
    </paper>
    <paper id="1292">
      <title>Who Writes What: Unveiling the Impact of Author Roles on <fixed-case>AI</fixed-case>-generated Text Detection</title>
      <author><first>Jiatao</first><last>Li</last></author>
      <author><first>Xiaojun</first><last>Wan</last></author>
      <pages>26620-26658</pages>
      <abstract>The rise of Large Language Models (LLMs) necessitates accurate AI-generated text detection. However, current approaches largely overlook the influence of author characteristics. We investigate how sociolinguistic attributes—gender, CEFR proficiency, academic field, and language environment—impact state-of-the-art AI text detectors. Using the ICNALE corpus of human-authored texts and parallel AI-generated texts from diverse LLMs, we conduct a rigorous evaluation employing multi-factor ANOVA and weighted least squares (WLS). Our results reveal significant biases: CEFR proficiency and language environment consistently affected detector accuracy, while gender and academic field showed detector-dependent effects. These findings highlight the crucial need for socially aware AI text detection to avoid unfairly penalizing specific demographic groups. We offer novel empirical evidence, a robust statistical framework, and actionable insights for developing more equitable and reliable detection systems in real-world, out-of-domain contexts. This work paves the way for future research on bias mitigation, inclusive evaluation benchmarks, and socially responsible LLM detectors.</abstract>
      <url hash="c75c1960">2025.acl-long.1292</url>
      <bibkey>li-wan-2025-writes</bibkey>
    </paper>
    <paper id="1293">
      <title><fixed-case>R</fixed-case>o<fixed-case>C</fixed-case>o<fixed-case>FT</fixed-case>: Efficient Finetuning of Large Language Models with Row-Column Updates</title>
      <author><first>Md</first><last>Kowsher</last><affiliation>University of Central Florida</affiliation></author>
      <author><first>Tara</first><last>Esmaeilbeig</last></author>
      <author><first>Chun-Nam</first><last>Yu</last><affiliation>Nokia Bell Labs and Department of Computer Science</affiliation></author>
      <author><first>Chen</first><last>Chen</last><affiliation>University of Central Florida</affiliation></author>
      <author><first>Mojtaba</first><last>Soltanalian</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <author><first>Niloofar</first><last>Yousefi</last><affiliation>University of Central Florida</affiliation></author>
      <pages>26659-26678</pages>
      <abstract>We propose Row-Column Fine-Tuning(RoCoFT), a parameter-efficient fine-tuning method for large language models based on updating only a few rows and columns of the weight matrices in transformers. Through extensive experiments with medium-sized LMs like RoBERTa and DeBERTa, and larger LMs like Bloom-7B, Llama2-7B, and Llama2-13B, we show that our method gives comparable or better accuracies than state-of-the-art Parameter-Efficient Finetuning methods while also being more memory and computation-efficient. We also study the reason behind the effectiveness of our method with tools from neural tangent kernel theory. We empirically demonstrate that our kernel, constructed using a restricted set of row and column parameters, is numerically close to the full-parameter kernel and gives comparable classification performance. Ablation studies are conducted to investigate the impact of different algorithmic choices, including the robustness of RoCoFT to any selection of rows and columns, as well as the optimal rank for the effective implementation of our method.</abstract>
      <url hash="ec7afed1">2025.acl-long.1293</url>
      <bibkey>kowsher-etal-2025-rocoft</bibkey>
    </paper>
    <paper id="1294">
      <title>Scaling Laws and Efficient Inference for Ternary Language Models</title>
      <author><first>Tejas</first><last>Vaidhya</last></author>
      <author><first>Ayush</first><last>Kaushal</last></author>
      <author><first>Vineet</first><last>Jain</last><affiliation>McGill University</affiliation></author>
      <author><first>Francis</first><last>Couture-Harpin</last><affiliation>École de technologie supérieure, Université du Québec</affiliation></author>
      <author><first>Prashant</first><last>Shishodia</last><affiliation>Google</affiliation></author>
      <author><first>Majid</first><last>Behbahani</last><affiliation>Morgan Stanley Canada</affiliation></author>
      <author><first>Yuriy</first><last>Nevmyvaka</last><affiliation>Morgan Stanley</affiliation></author>
      <author><first>Irina</first><last>Rish</last><affiliation>University of Montreal</affiliation></author>
      <pages>26679-26710</pages>
      <abstract>Large language models (LLMs) are increasingly used across research and industry applications, yet their inference efficiency remains a significant challenge. As the computational power of modern GPU architectures continuously improves, their memory bandwidth and capacity have not scaled proportionally, creating a critical bottleneck during inference. To address this, we investigate ternary language models (TriLMs) that employ quantization-aware training to significantly reduce memory requirements. We first analyze the scalability of TriLMs by conducting a scaling law analysis, revealing that TriLMs benefit more from increasing training data than from scaling model parameters. Based on this observation, we introduce TriTera, an open suite of TriLMs trained on up to 1.2 trillion tokens, demonstrating sustained performance gains at scale. Furthermore, to improve inference efficiency, we propose novel 2-bit and 1.6-bit packing schemes for ternary weights, which demonstrate accelerated inference across various CPU architectures. Building on the 2-bit packing, we develop a GPU kernel called TriRun that accelerates end-to-end model inference by up to 5 <tex-math>\times</tex-math> compared to floating-point baselines. To encourage further exploration and development of TriLMs, we will release the TriTera suite and TriRun inference kernels. Overall, our work lays the foundation for building and deploying efficient LLMs, providing a valuable resource for the research community.</abstract>
      <url hash="f50b48bf">2025.acl-long.1294</url>
      <bibkey>vaidhya-etal-2025-scaling</bibkey>
    </paper>
    <paper id="1295">
      <title>Exploring the Impact of Instruction-Tuning on <fixed-case>LLM</fixed-case>’s Susceptibility to Misinformation</title>
      <author><first>Kyubeen</first><last>Han</last></author>
      <author><first>Junseo</first><last>Jang</last></author>
      <author><first>Hongjin</first><last>Kim</last><affiliation>Electronics and Telecommunications Research Institute</affiliation></author>
      <author><first>Geunyeong</first><last>Jeong</last><affiliation>Konkuk University</affiliation></author>
      <author><first>Harksoo</first><last>Kim</last><affiliation>Konkuk University</affiliation></author>
      <pages>26711-26731</pages>
      <abstract>Instruction-tuning enhances the ability of large language models (LLMs) to follow user instructions more accurately, improving usability while reducing harmful outputs. However, this process may increase the model’s dependence on user input, potentially leading to the unfiltered acceptance of misinformation and the generation of hallucinations. Existing studies primarily highlight that LLMs are receptive to external information that contradict their parametric knowledge, but little research has been conducted on the direct impact of instruction-tuning on this phenomenon. In our study, we investigate the impact of instruction-tuning on LLM susceptibility to misinformation. Our analysis reveals that instruction-tuned LLMs are significantly more likely to accept misinformation when it is presented by the user. A comparison with base models shows that instruction-tuning increases reliance on user-provided information, shifting susceptibility from the assistant role to the user role. Furthermore, we explore additional factors influencing misinformation susceptibility, such as the role of the user in prompt structure, misinformation length, and the presence of warnings in the system prompt. Our findings underscore the need for systematic approaches to mitigate unintended consequences of instruction-tuning and enhance the reliability of LLMs in real-world applications.</abstract>
      <url hash="957ddb07">2025.acl-long.1295</url>
      <bibkey>han-etal-2025-exploring</bibkey>
    </paper>
    <paper id="1296">
      <title>Do Language Models Understand Honorific Systems in <fixed-case>J</fixed-case>avanese?</title>
      <author><first>Mohammad Rifqi</first><last>Farhansyah</last></author>
      <author><first>Iwan</first><last>Darmawan</last><affiliation>Monash University</affiliation></author>
      <author><first>Adryan</first><last>Kusumawardhana</last><affiliation>Komisi Pemberantasan Korupsi</affiliation></author>
      <author><first>Genta Indra</first><last>Winata</last><affiliation>Capital One</affiliation></author>
      <author><first>Alham Fikri</first><last>Aji</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Derry Tanti</first><last>Wijaya</last><affiliation>Monash University and Boston University</affiliation></author>
      <pages>26732-26754</pages>
      <abstract>The Javanese language features a complex system of honorifics that vary according to the social status of the speaker, listener, and referent. Despite its cultural and linguistic significance, there has been limited progress in developing a comprehensive corpus to capture these variations for natural language processing (NLP) tasks. In this paper, we present Unggah-Ungguh, a carefully curated dataset designed to encapsulate the nuances of Unggah-Ungguh Basa, the Javanese speech etiquette framework that dictates the choice of words and phrases based on social hierarchy and context. Using Unggah-Ungguh, we assess the ability of language models (LMs) to process various levels of Javanese honorifics through classification and machine translation tasks. To further evaluate cross-lingual LMs, we conduct machine translation experiments between Javanese (at specific honorific levels) and Indonesian. Additionally, we explore whether LMs can generate contextually appropriate Javanese honorifics in conversation tasks, where the honorific usage should align with the social role and contextual cues. Our findings indicate that current LMs struggle with most honorific levels, exhibiting a bias toward certain honorific tiers.</abstract>
      <url hash="6decc72d">2025.acl-long.1296</url>
      <bibkey>farhansyah-etal-2025-language</bibkey>
    </paper>
    <paper id="1297">
      <title>Generative Reward Modeling via Synthetic Criteria Preference Learning</title>
      <author><first>Xiaobo</first><last>Liang</last></author>
      <author><first>Haoke</first><last>Zhang</last><affiliation>Suzhou University</affiliation></author>
      <author><first>Juntao</first><last>Li</last></author>
      <author><first>Kehai</first><last>Chen</last><affiliation>Harbin Institute of Technology (Shenzhen)</affiliation></author>
      <author><first>Qiaoming</first><last>Zhu</last><affiliation>Soochow University</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>26755-26769</pages>
      <abstract>Generative Reward Models (GenRMs) leverage synthesized Chains of Thought (CoT) to reduce the need for massive labeled data, but this approach introduces risks of overoptimization due to the inability to guarantee the correctness of the CoTs. Identifying and optimizing unexpected behaviors within these synthesized CoT remains a challenge, as it heavily depends on precise annotations of intermediate behavior, similar to process supervision. In this work, we introduce a criteria-based preference tree for reward modeling, where each path in the tree represents a reasoning trajectory based on synthesized criteria. Crucially, each reasoning trajectory can be independently optimized through RL algorithm. These fine-grained process reward signals are derived from the inference-time computations and predefined rules, eliminating the need for human supervision. In experiments, SyncPL showed significant improvements over baselines on multiple human preference benchmarks. We further demonstrate that synthesized data can be learned using a long CoT format, analogous to an o1-like model, further enhancing performance while keeping stability and efficiency during training.</abstract>
      <url hash="c67550c5">2025.acl-long.1297</url>
      <bibkey>liang-etal-2025-generative</bibkey>
    </paper>
    <paper id="1298">
      <title>Exploring Multimodal Relation Extraction of Hierarchical Tabular Data with Multi-task Learning</title>
      <author><first>Xinyu</first><last>Zhang</last></author>
      <author><first>Aibo</first><last>Song</last><affiliation>Southeast University</affiliation></author>
      <author><first>Jingyi</first><last>Qiu</last></author>
      <author><first>Jiahui</first><last>Jin</last><affiliation>Southeast University</affiliation></author>
      <author><first>Tianbo</first><last>Zhang</last><affiliation>Southeast University</affiliation></author>
      <author><first>Xiaolin</first><last>Fang</last><affiliation>Southeast University</affiliation></author>
      <pages>26770-26781</pages>
      <abstract>Relation Extraction (RE) is a key task in table understanding, aiming to extract semantic relations between columns. However, complex tables with hierarchical headers are hard to obtain high-quality textual formats (e.g., Markdown) for input under practical scenarios like webpage screenshots and scanned documents, while table images are more accessible and intuitive. Besides, existing works overlook the need of mining relations among multiple columns rather than just the semantic relation between two specific columns in real-world practice. In this work, we explore utilizing Multimodal Large Language Models (MLLMs) to address RE in tables with complex structures. We creatively extend the concept of RE to include calculational relations, enabling multi-task learning of both semantic and calculational RE for mutual reinforcement. Specifically, we reconstruct table images into graph structure based on neighboring nodes to extract graph-level visual features. Such feature enhancement alleviates the insensitivity of MLLMs to the positional information within table images. We then propose a Chain-of-Thought distillation framework with self-correction mechanism to enhance MLLMs’ reasoning capabilities without increasing parameter scale. Our method significantly outperforms most baselines on wide datasets. Additionally, we release a benchmark dataset for calculational RE in complex tables.</abstract>
      <url hash="fbe34dd5">2025.acl-long.1298</url>
      <bibkey>zhang-etal-2025-exploring</bibkey>
    </paper>
    <paper id="1299">
      <title>A Self-Denoising Model for Robust Few-Shot Relation Extraction</title>
      <author><first>Liang</first><last>Zhang</last></author>
      <author><first>Yang</first><last>Zhang</last></author>
      <author><first>Ziyao</first><last>Lu</last><affiliation>WeChat AI</affiliation></author>
      <author><first>Fandong</first><last>Meng</last><affiliation>WeChat AI, Tencent Inc.</affiliation></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <author><first>Jinsong</first><last>Su</last><affiliation>Xiamen University</affiliation></author>
      <pages>26782-26797</pages>
      <abstract>The few-shot relation extraction (FSRE) aims at enhancing the model’s generalization to new relations with very few labeled instances (support instances). Most existing studies use prototype networks (ProtoNets) for FSRE and assume that the support set, adapting the model to new relations, only contains accurately labeled instances. However, this assumption is usually unrealistic, as even carefully-annotated datasets often contain mislabeled instances. Thus, it is essential to enhance the robustness of FSRE models to noisy labels in support set, but this issue remains unexplored. In this paper, we first conduct a preliminary study, revealing the high sensitivity of ProtoNets to such noisy labels. Meanwhile, we discover that fully leveraging mislabeled support instances is crucial for enhancing the model’s robustness. To do this, we propose a self-denoising model for FSRE, which can automatically correct noisy labels of support instances. Specifically, our model comprises two core components: 1) a label correction module (LCM), used to correct mislabeled support instances based on the distances between them in the embedding space, and 2) a relation classification module (RCM), designed to achieve more robust relation prediction using the corrected labels generated by the LCM. Moreover, we propose a feedback-based training strategy, which focuses on training LCM and RCM to synergistically handle noisy labels in support set. Experimental results on two public datasets show the effectiveness and robustness of our model. Notably, even in scenarios without noisy labels, our model significantly outperforms all competitive baselines.</abstract>
      <url hash="f6947bba">2025.acl-long.1299</url>
      <bibkey>zhang-etal-2025-self</bibkey>
    </paper>
    <paper id="1300">
      <title><fixed-case>Q</fixed-case>u<fixed-case>ASAR</fixed-case>: A Question-Driven Structure-Aware Approach for Table-to-Text Generation</title>
      <author><first>WeiJie</first><last>Liu</last><affiliation>Soochow University</affiliation></author>
      <author><first>Yibin</first><last>Zheng</last></author>
      <author><first>Fang</first><last>Kong</last><affiliation>Soochow University</affiliation></author>
      <pages>26798-26812</pages>
      <abstract>Table-to-text generation aims to automatically produce natural language descriptions from structured or semi-structured tabular data. Unlike traditional text generation tasks, it requires models to accurately understand and represent table structures. Existing approaches typically process tables by linearizing them or converting them into graph structures. However, these methods either fail to adequately capture the table structure or rely on complex attention mechanisms, limiting their applicability. To tackle these challenges, we propose QuASAR, a question-driven self-supervised approach designed to enhance the model’s structural perception and representation capabilities. Specifically, QuASAR formulates a set of structure-related queries for self-supervised training, explicitly guiding the model to capture both local and global table structures. Additionally, we introduce two auxiliary pre-training tasks: a word-to-sentence reconstruction task and a numerical summarization task, which further enhance the fluency and factuality of the generated text. Experimental results on the ToTTo and HiTab datasets demonstrate that our approach produces higher-quality text compared to existing methods.</abstract>
      <url hash="dd0db1b7">2025.acl-long.1300</url>
      <bibkey>liu-etal-2025-quasar</bibkey>
    </paper>
    <paper id="1301">
      <title>Automated Structured Radiology Report Generation</title>
      <author><first>Jean-Benoit</first><last>Delbrouck</last><affiliation>Stanford University</affiliation></author>
      <author><first>Justin</first><last>Xu</last></author>
      <author><first>Johannes</first><last>Moll</last><affiliation>Stanford University</affiliation></author>
      <author><first>Alois</first><last>Thomas</last><affiliation>Stanford University</affiliation></author>
      <author><first>Zhihong</first><last>Chen</last><affiliation>Stanford University</affiliation></author>
      <author><first>Sophie</first><last>Ostmeier</last><affiliation>Stanford University</affiliation></author>
      <author><first>Asfandyar</first><last>Azhar</last></author>
      <author><first>Kelvin Zhenghao</first><last>Li</last></author>
      <author><first>Andrew</first><last>Johnston</last><affiliation>Stanford University</affiliation></author>
      <author><first>Christian</first><last>Bluethgen</last><affiliation>Stanford University and University of Zurich</affiliation></author>
      <author><first>Eduardo Pontes</first><last>Reis</last><affiliation>Stanford University and Hospital Israelita Albert Einstein</affiliation></author>
      <author><first>Mohamed S</first><last>Muneer</last></author>
      <author><first>Maya</first><last>Varma</last><affiliation>Stanford University</affiliation></author>
      <author><first>Curtis</first><last>Langlotz</last><affiliation>Stanford University</affiliation></author>
      <pages>26813-26829</pages>
      <abstract>Automated radiology report generation from chest X-ray (CXR) images has the potential to improve clinical efficiency and reduce radiologists’ workload. However, most datasets, including the publicly available MIMIC-CXR and CheXpert Plus, consist entirely of free-form reports, which are inherently variable and unstructured. This variability poses challenges for both generation and evaluation: existing models struggle to produce consistent, clinically meaningful reports, and standard evaluation metrics fail to capture the nuances of radiological interpretation. To address this, we introduce Structured Radiology Report Generation (SRRG), a new task that reformulates free-text radiology reports into a standardized format, ensuring clarity, consistency, and structured clinical reporting. We create a novel dataset by restructuring reports using large language models (LLMs) following strict structured reporting desiderata. Additionally, we introduce SRR-BERT, a fine-grained disease classification model trained on 55 labels, enabling more precise and clinically informed evaluation of structured reports. To assess report quality, we propose F1-SRR-BERT, a metric that leverages SRR-BERT’s hierarchical disease taxonomy to bridge the gap between free-text variability and structured clinical reporting. We validate our dataset through a reader study conducted by five board-certified radiologists and extensive benchmarking experiments.</abstract>
      <url hash="47c302b7">2025.acl-long.1301</url>
      <bibkey>delbrouck-etal-2025-automated</bibkey>
    </paper>
    <paper id="1302">
      <title><fixed-case>LPOI</fixed-case>: Listwise Preference Optimization for Vision Language Models</title>
      <author><first>Fatemeh</first><last>Pesaran Zadeh</last><affiliation>Seoul National University, Seoul National University</affiliation></author>
      <author><first>Yoojin</first><last>Oh</last></author>
      <author><first>Gunhee</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <pages>26830-26844</pages>
      <abstract>Aligning large VLMs with human preferences is a challenging task, as methods like RLHF and DPO often overfit to textual information or exacerbate hallucinations. Although augmenting negative image samples partially addresses these pitfalls, no prior work has employed listwise preference optimization for VLMs, due to the complexity and cost of constructing listwise image samples. In this work, we propose LPOI, the first object-aware listwise preference optimization developed for reducing hallucinations in VLMs. LPOI identifies and masks a critical object in the image, and then interpolates the masked region between the positive and negative images to form a sequence of incrementally more complete images. The model is trained to rank these images in ascending order of object visibility, effectively reducing hallucinations while retaining visual fidelity. LPOI requires no extra annotations beyond standard pairwise preference data, as it automatically constructs the ranked lists through object masking and interpolation. Comprehensive experiments on MMHalBench, AMBER, and Object HalBench confirm that LPOI outperforms existing preference optimization methods in reducing hallucinations and enhancing VLM performance.</abstract>
      <url hash="867da0c7">2025.acl-long.1302</url>
      <bibkey>pesaran-zadeh-etal-2025-lpoi</bibkey>
    </paper>
    <paper id="1303">
      <title>Predicting Through Generation: Why Generation Is Better for Prediction</title>
      <author><first>Md</first><last>Kowsher</last><affiliation>University of Central Florida</affiliation></author>
      <author><first>Nusrat Jahan</first><last>Prottasha</last></author>
      <author><first>Prakash</first><last>Bhat</last></author>
      <author><first>Chun-Nam</first><last>Yu</last><affiliation>Nokia Bell Labs and Department of Computer Science</affiliation></author>
      <author><first>Mojtaba</first><last>Soltanalian</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <author><first>Ivan</first><last>Garibay</last></author>
      <author><first>Ozlem</first><last>Garibay</last><affiliation>University of Central Florida</affiliation></author>
      <author><first>Chen</first><last>Chen</last><affiliation>University of Central Florida</affiliation></author>
      <author><first>Niloofar</first><last>Yousefi</last><affiliation>University of Central Florida</affiliation></author>
      <pages>26845-26871</pages>
      <abstract>This paper argues that generating output tokens is more effective than using pooled representations for prediction tasks because token-level generation retains more mutual information. Since LLMs are trained on massive text corpora using next-token prediction, generation aligns naturally with their learned behavior. Using the Data Processing Inequality (DPI), we provide both theoretical and empirical evidence supporting this claim. However, autoregressive models face two key challenges when used for prediction: (1) exposure bias, where the model sees ground-truth tokens during training but relies on its own predictions during inference, leading to errors, and (2) format mismatch, where discrete tokens do not always align with the task’s required output structure. To address these challenges, we introduce PredGen (Predicting Through Generating), an end-to-end framework that (i) uses scheduled sampling to reduce exposure bias, and (ii) introduces a task adapter to convert the generated tokens into structured outputs. Additionally, we introduce Writer-Director Alignment Loss (WDAL), which ensures consistency between token generation and final task predictions, improving both text coherence and numerical accuracy. We evaluate PredGen on multiple classification and regression benchmarks. Our results show that PredGen consistently outperforms standard baselines, demonstrating its effectiveness in structured prediction tasks.</abstract>
      <url hash="946a8e03">2025.acl-long.1303</url>
      <bibkey>kowsher-etal-2025-predicting</bibkey>
    </paper>
    <paper id="1304">
      <title>“Give Me <fixed-case>BF</fixed-case>16 or Give Me Death”? Accuracy-Performance Trade-Offs in <fixed-case>LLM</fixed-case> Quantization</title>
      <author><first>Eldar</first><last>Kurtic</last><affiliation>Red Hat and Institute of Science and Technology Austria</affiliation></author>
      <author><first>Alexandre Noll</first><last>Marques</last><affiliation>Neural Magic</affiliation></author>
      <author><first>Shubhra</first><last>Pandit</last><affiliation>Red Hat and Neural Magic</affiliation></author>
      <author><first>Mark</first><last>Kurtz</last><affiliation>Red Hat</affiliation></author>
      <author><first>Dan</first><last>Alistarh</last></author>
      <pages>26872-26886</pages>
      <abstract>Despite the popularity of large language model (LLM) quantization for inference acceleration, significant uncertainty remains regarding the accuracy-performance trade-offs associated with various quantization formats. We present a comprehensive empirical study of quantized accuracy, evaluating popular quantization formats (FP8, INT8, INT4) across academic benchmarks and real-world tasks, on the entire Llama-3.1 model family. Additionally, our study examines the difference in text generated by quantized models versus their uncompressed counterparts. Beyond benchmarks, we also present a couple of quantization improvements which allowed us to obtain state-of-the-art accuracy recovery results. Our investigation, encompassing over 500,000 individual evaluations, yields several key findings: (1) FP8 weight and activation quantization (W8A8-FP) is lossless across all model scales, (2) INT8 weight and activation quantization (W8A8-INT), when properly tuned, incurs surprisingly low 1-3% accuracy degradation, and (3) INT4 weight-only quantization (W4A16-INT) is competitive with 8-bit integer weight and activation quantization. To address the question of the “best” format for a given deployment environment, we conduct inference performance analysis using the popular open-source vLLM framework on various GPU architectures. We find that W4A16 offers the best cost-efficiency for synchronous deployments, and for asynchronous deployment on mid-tier GPUs. At the same time, W8A8 formats excel in asynchronous deployment of mid and large-size models on high-end GPUs. Our results provide a first set of practical guidelines for deploying quantized LLMs across different scales and performance requirements.</abstract>
      <url hash="08451853">2025.acl-long.1304</url>
      <bibkey>kurtic-etal-2025-give</bibkey>
    </paper>
    <paper id="1305">
      <title><fixed-case>S</fixed-case>titch<fixed-case>LLM</fixed-case>: Serving <fixed-case>LLM</fixed-case>s, One Block at a Time</title>
      <author><first>Bodun</first><last>Hu</last></author>
      <author><first>Shuozhe</first><last>Li</last></author>
      <author><first>Saurabh</first><last>Agarwal</last><affiliation>University of Wisconsin, Madison</affiliation></author>
      <author><first>Myungjin</first><last>Lee</last><affiliation>Cisco</affiliation></author>
      <author><first>Akshay</first><last>Jajoo</last><affiliation>Cisco</affiliation></author>
      <author><first>Jiamin</first><last>Li</last></author>
      <author><first>Le</first><last>Xu</last><affiliation>University of Texas at Austin and University of Illinois, Urbana Champaign</affiliation></author>
      <author><first>Geon-Woo</first><last>Kim</last><affiliation>University of Texas at Austin</affiliation></author>
      <author><first>Donghyun</first><last>Kim</last><affiliation>University of Texas at Austin</affiliation></author>
      <author><first>Hong</first><last>Xu</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Amy</first><last>Zhang</last><affiliation>University of Texas at Austin and Facebook</affiliation></author>
      <author><first>Aditya</first><last>Akella</last><affiliation>Facebook and University of Texas at Austin</affiliation></author>
      <pages>26887-26903</pages>
      <abstract>The rapid evolution of large language models (LLMs) has revolutionized natural language processing (NLP) tasks such as text generation, translation, and comprehension. However, the increasing computational demands and inference costs of these models present significant challenges. This study investigates the dynamic and efficient utilization of pre-trained weights from open-sourced LLMs of varying parameter sizes to achieve an optimal balance between computational efficiency and task performance. Drawing inspiration from the dual-process theory of human cognition, we introduce StitchLLM: a dynamic model routing framework that employs a powerful bottom model to process all queries, and uses a lightweight routing mechanism to allocate computational resources appropriately. Our novel framework optimizes efficiency and maintains performance, leveraging a trainable stitching layer for seamless integration of decoder layers across different LLMs. Experimental results demonstrate that StitchLLM improves system throughput while minimizing performance degradation, offering a flexible solution for deploying LLMs in resource-constrained settings.</abstract>
      <url hash="29328b39">2025.acl-long.1305</url>
      <bibkey>hu-etal-2025-stitchllm</bibkey>
    </paper>
    <paper id="1306">
      <title>Walk in Others’ Shoes with a Single Glance: Human-Centric Visual Grounding with Top-View Perspective Transformation</title>
      <author><first>Yuqi</first><last>Bu</last><affiliation>Shenzhen Polytechnic University and South China University of Technology</affiliation></author>
      <author><first>Xin</first><last>Wu</last></author>
      <author><first>Zirui</first><last>Zhao</last><affiliation>SalesForce.com and national university of singaore, National University of Singapore</affiliation></author>
      <author><first>Yi</first><last>Cai</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>David</first><last>Hsu</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Qiong</first><last>Liu</last><affiliation>South China University of Technology</affiliation></author>
      <pages>26904-26923</pages>
      <abstract>Visual perspective-taking, an ability to envision others’ perspectives from a single self-perspective, is vital in human-robot interactions. Thus, we introduce a human-centric visual grounding task and a dataset to evaluate this ability. Recent advances in vision-language models (VLMs) have shown potential for inferring others’ perspectives, yet are insensitive to information differences induced by slight perspective changes. To address this problem, we propose a top-view enhanced perspective transformation (TEP) method, which decomposes the transition from robot to human perspectives through an abstract top-view representation. It unifies perspectives and facilitates the capture of information differences from diverse perspectives. Experimental results show that TEP improves performance by up to 18%, exhibits perspective-taking abilities across various perspectives, and generalizes effectively to robotic and dynamic scenarios.</abstract>
      <url hash="06f3d803">2025.acl-long.1306</url>
      <bibkey>bu-etal-2025-walk</bibkey>
    </paper>
    <paper id="1307">
      <title>Is linguistically-motivated data augmentation worth it?</title>
      <author><first>Ray</first><last>Groshan</last><affiliation>University of Maryland, Baltimore County</affiliation></author>
      <author><first>Michael</first><last>Ginn</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <author><first>Alexis</first><last>Palmer</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <pages>26924-26939</pages>
      <abstract>Data augmentation, a widely-employed technique for addressing data scarcity, involves generating synthetic data examples which are then used to augment available training data. Researchers have seen surprising success from simple methods, such as random perturbations from natural examples, where models seem to benefit even from data with nonsense words, or data that doesn’t conform to the rules of the language. A second line of research produces synthetic data that does in fact follow all linguistic constraints; these methods require some linguistic expertise and are generally more challenging to implement. No previous work has done a systematic, empirical comparison of both linguistically-naive and linguistically-motivated data augmentation strategies, leaving uncertainty about whether the additional time and effort of linguistically-motivated data augmentation work in fact yields better downstream performance.In this work, we conduct a careful and comprehensive comparison of augmentation strategies (both linguistically-naive and linguistically-motivated) for two low-resource languages with different morphological properties, Uspanteko and Arapaho. We evaluate the effectiveness of many different strategies and their combinations across two important sequence-to-sequence tasks for low-resource languages: machine translation and interlinear glossing. We find that linguistically-motivated strategies can have benefits over naive approaches, but only when the new examples they produce are not significantly unlike the training data distribution.</abstract>
      <url hash="9a54ae6b">2025.acl-long.1307</url>
      <bibkey>groshan-etal-2025-linguistically</bibkey>
    </paper>
    <paper id="1308">
      <title>From Lists to Emojis: How Format Bias Affects Model Alignment</title>
      <author><first>Xuanchang</first><last>Zhang</last></author>
      <author><first>Wei</first><last>Xiong</last><affiliation>Facebook and University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Lichang</first><last>Chen</last></author>
      <author><first>Tianyi</first><last>Zhou</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Heng</first><last>Huang</last><affiliation>Department of Computer Science, University of Maryland, College Park</affiliation></author>
      <author><first>Tong</first><last>Zhang</last><affiliation>UIUC</affiliation></author>
      <pages>26940-26961</pages>
      <abstract>In this paper, we study format biases in reinforcement learning from human feedback (RLHF). We observe that many widely-used preference models—including human evaluators, GPT-4, and top-ranking models on the RewardBench benchmark—exhibit strong biases towards specific format patterns, such as lists, links, bold text, and emojis. Furthermore, large language models (LLMs) can exploit these biases to achieve higher rankings on popular benchmarks like AlpacaEval and LMSYS Chatbot Arena. One notable example is verbosity bias, where current preference models favor longer responses that appear more comprehensive, even when their quality is equal to or lower than shorter responses. However, format biases beyond verbosity remain largely underexplored. In this work, we extend the study of biases in preference learning beyond the commonly recognized length bias, offering a comprehensive analysis of a wider range of format biases. Additionally, we show that with a small amount of biased data (less than 1%), we can inject significant bias into the reward model. Moreover, these format biases can also be easily exploited by downstream alignment algorithms, such as *best-of-n sampling* and online iterative *DPO*, as it is usually easier to manipulate the format than to improve the quality of responses. Our findings emphasize the need to disentangle format and content both for designing alignment algorithms and evaluating models.</abstract>
      <url hash="2acb6bd3">2025.acl-long.1308</url>
      <bibkey>zhang-etal-2025-lists</bibkey>
    </paper>
    <paper id="1309">
      <title>Colloquial Singaporean <fixed-case>E</fixed-case>nglish Style Transfer with Fine-Grained Explainable Control</title>
      <author><first>Jinggui</first><last>Liang</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Dung</first><last>Vo</last></author>
      <author><first>Yap Hong</first><last>Xian</last></author>
      <author><first>Hai Leong</first><last>Chieu</last><affiliation>DSO National Laboratories</affiliation></author>
      <author><first>Kian Ming A.</first><last>Chai</last><affiliation>DSO National Laboratories</affiliation></author>
      <author><first>Jing</first><last>Jiang</last><affiliation>Australian National University and Singapore Management University</affiliation></author>
      <author><first>Lizi</first><last>Liao</last><affiliation>Singapore Management University</affiliation></author>
      <pages>26962-26983</pages>
      <abstract>Colloquial Singaporean English (Singlish) is an informal English marked by a unique blend of languages reflecting Singapore’s multicultural identity. Style transfer between Singlish and Standard (formal) English is vital for various applications, yet existing methods often lack explainability and fine-grained control. To fill this gap, we contribute in two key ways. First, we construct a large, high-quality dataset of formal and informal sentences, annotated across six linguistic aspects—Syntax, Lexical Borrowing, Pragmatics, Prosody/Phonology, Emoticons/Punctuation, and Code-Switching—with detailed explanations. Starting with manually annotated cases, we scaled the dataset to 140K with ensured quality. Second, inspired by the “Society of Mind” theory, we propose a novel multi-agent framework where large language models (LLMs) act as expert agents for each linguistic aspect. These agents collaborate by iteratively generating, critiquing, and refining responses to achieve controlled, explainable style transfer. Both automatic metrics and human evaluations confirm that our method enables precise, interpretable transformations, advancing explainability in NLP for Singlish.</abstract>
      <url hash="0235e6ea">2025.acl-long.1309</url>
      <bibkey>liang-etal-2025-colloquial</bibkey>
    </paper>
    <paper id="1310">
      <title>From Informal to Formal – Incorporating and Evaluating <fixed-case>LLM</fixed-case>s on Natural Language Requirements to Verifiable Formal Proofs</title>
      <author><first>Jialun</first><last>Cao</last></author>
      <author><first>Yaojie</first><last>Lu</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Meiziniu</first><last>Li</last></author>
      <author><first>Haoyang</first><last>Ma</last></author>
      <author><first>Haokun</first><last>Li</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Mengda</first><last>He</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Cheng</first><last>Wen</last><affiliation>Xi’an University of Electronic Science and Technology</affiliation></author>
      <author><first>Le</first><last>Sun</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Hongyu</first><last>Zhang</last><affiliation>Chongqing University</affiliation></author>
      <author><first>Shengchao</first><last>Qin</last><affiliation>Xi’an University of Electronic Science and Technology</affiliation></author>
      <author><first>Shing-Chi</first><last>Cheung</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Cong</first><last>Tian</last></author>
      <pages>26984-27003</pages>
      <abstract>The research in AI-based formal mathematical reasoning has shown an unstoppable growth trend. These studies have excelled in mathematical competitions like IMO and have made significant progress. However, these studies intertwined multiple skills simultaneously—problem-solving, reasoning, and writing formal specifications—making it hard to precisely identify the LLMs’ strengths and weaknesses in each task. This paper focuses on formal verification, an immediate application scenario of formal reasoning, and breaks it down into sub-tasks. We constructed 18k high-quality instruction-response pairs across five mainstream formal specification languages (Coq, Lean4, Dafny, ACSL, and TLA+) in six tasks by distilling gpt-4o and evaluated against ten open-sourced LLMs, including recent popular DeepSeek-R1. We found that LLMs are good at writing proof segments when given either the code, or the detailed description of proof steps. Also, the fine-tuning brought about a nearly threefold improvement at most. And interestingly, we observed that fine-tuning with formal data also enhances abilities in mathematics, reasoning, and coding. We hope our findings inspire further research.</abstract>
      <url hash="d9def23d">2025.acl-long.1310</url>
      <bibkey>cao-etal-2025-informal</bibkey>
    </paper>
    <paper id="1311">
      <title><fixed-case>C</fixed-case>o<fixed-case>AM</fixed-case>: Corpus of All-Type Multiword Expressions</title>
      <author><first>Yusuke</first><last>Ide</last></author>
      <author><first>Joshua</first><last>Tanner</last><affiliation>Oracle</affiliation></author>
      <author><first>Adam</first><last>Nohejl</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Jacob</first><last>Hoffman</last><affiliation>Project Ronin</affiliation></author>
      <author><first>Justin</first><last>Vasselli</last></author>
      <author><first>Hidetaka</first><last>Kamigaito</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <pages>27004-27021</pages>
      <abstract>Multiword expressions (MWEs) refer to idiomatic sequences of multiple words.MWE identification, i.e., detecting MWEs in text, can play a key role in downstream tasks such as machine translation, but existing datasets for the task are inconsistently annotated, limited to a single type of MWE, or limited in size.To enable reliable and comprehensive evaluation, we created CoAM: Corpus of All-Type Multiword Expressions, a dataset of 1.3K sentences constructed through a multi-step process to enhance data quality consisting of human annotation, human review, and automated consistency checking.Additionally, for the first time in a dataset of MWE identification, CoAM’s MWEs are tagged with MWE types, such as Noun and Verb, enabling fine-grained error analysis.Annotations for CoAM were collected using a new interface created with our interface generator, which allows easy and flexible annotation of MWEs in any form.Through experiments using CoAM, we find that a fine-tuned large language model outperforms MWEasWSD, which achieved the state-of-the-art performance on the DiMSUM dataset.Furthermore, analysis using our MWE type tagged data reveals that Verb MWEs are easier than Noun MWEs to identify across approaches.</abstract>
      <url hash="1b349aa6">2025.acl-long.1311</url>
      <bibkey>ide-etal-2025-coam</bibkey>
    </paper>
    <paper id="1312">
      <title><fixed-case>S</fixed-case>ea<fixed-case>KR</fixed-case>: Self-aware Knowledge Retrieval for Adaptive Retrieval Augmented Generation</title>
      <author><first>Zijun</first><last>Yao</last></author>
      <author><first>Weijian</first><last>Qi</last></author>
      <author><first>Liangming</first><last>Pan</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Shulin</first><last>Cao</last><affiliation>Zhipu AI and Tsinghua University</affiliation></author>
      <author><first>Linmei</first><last>Hu</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Liu</first><last>Weichuan</last><affiliation>Siemens Corporate Research</affiliation></author>
      <author><first>Lei</first><last>Hou</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <pages>27022-27043</pages>
      <abstract>Adaptive Retrieval-Augmented Generation (RAG) is an effective strategy to alleviate hallucination of large language models (LLMs). It dynamically determines whether LLMs need external knowledge for generation and invokes retrieval accordingly. This paper introduces Self-aware Knowledge Retrieval (SeaKR), a novel adaptive RAG model that extracts self-aware uncertainty of LLMs from their internal states. SeaKR activates retrieval when the LLMs present high self-aware uncertainty for generation. To effectively integrate retrieved knowledge snippets, SeaKR re-ranks them based on LLM’s self-aware uncertainty to preserve the snippet that reduces their uncertainty to the utmost. To facilitate solving complex tasks that require multiple retrievals, SeaKR utilizes their self-aware uncertainty to choose among different reasoning strategies. Our experiments on both complex and simple Question Answering datasets show that SeaKR outperforms existing adaptive RAG methods.</abstract>
      <url hash="2a4bb6ad">2025.acl-long.1312</url>
      <bibkey>yao-etal-2025-seakr</bibkey>
    </paper>
    <paper id="1313">
      <title>Exposing the Achilles’ Heel: Evaluating <fixed-case>LLM</fixed-case>s Ability to Handle Mistakes in Mathematical Reasoning</title>
      <author><first>Joykirat</first><last>Singh</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Akshay</first><last>Nambi</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Vibhav</first><last>Vineet</last><affiliation>Microsoft</affiliation></author>
      <pages>27044-27065</pages>
      <abstract>Large Language Models (LLMs) have significantly impacted the field of Math Word Problems (MWPs), transforming how these problems are approached and solved, particularly in educational contexts. However, existing evaluations often focus on final accuracy, neglecting the critical aspect of reasoning capabilities. This work addresses that gap by evaluating LLMs’ abilities to detect and correct reasoning mistakes. We present a novel dataset, MWP-MISTAKE, containing MWPs with both correct and incorrect reasoning steps generated through rule-based methods and smaller language models. Our comprehensive benchmarking of state-of-the-art models such as GPT-4o and GPT4 uncovers important insights into their strengths and limitations. While GPT-4o excels in mistake detection and rectification, gaps remain, particularly in handling complex datasets and novel problems. Additionally, we identify concerns with data contamination and memorization, which affect LLM reliability in real-world applications. While OpenAI’ O1 model demonstrates 90% accuracy in reasoning and final answers on complex tasks, it remains weak in mistake detection. Our findings highlight the need for improved reasoning evaluations and suggest ways to enhance LLM generalization and robustness in math problem-solving.</abstract>
      <url hash="5c094393">2025.acl-long.1313</url>
      <bibkey>singh-etal-2025-exposing</bibkey>
    </paper>
    <paper id="1314">
      <title>Understanding the Dark Side of <fixed-case>LLM</fixed-case>s’ Intrinsic Self-Correction</title>
      <author><first>Qingjie</first><last>Zhang</last></author>
      <author><first>Di</first><last>Wang</last></author>
      <author><first>Haoting</first><last>Qian</last></author>
      <author><first>Yiming</first><last>Li</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Tianwei</first><last>Zhang</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <author><first>Ke</first><last>Xu</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Hewu</first><last>Li</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Liu</first><last>Yan</last></author>
      <author><first>Han</first><last>Qiu</last><affiliation>Tsinghua University</affiliation></author>
      <pages>27066-27101</pages>
      <abstract>Intrinsic self-correction was initially proposed to improve LLMs’ responses via feedback solely based on their inherent capability. However, recent works show that LLMs’ intrinsic self-correction fails without oracle labels as feedback. In this paper, our research goal is to *interpret LLMs’ intrinsic self-correction for different tasks, especially for those failure cases.* By including one simple task and three complex tasks with state-of-the-art (SOTA) LLMs like ChatGPT, Llama, and DeepSeek, we design three interpretation methods to reveal the dark side of LLMs’ intrinsic self-correction. We identify intrinsic self-correction can (1) cause LLMs to waver both intermedia and final answers and lead to prompt bias on simple factual questions; (2) introduce human-like cognitive bias on complex tasks. In light of our findings, we also provide two simple yet effective strategies for alleviation: question repeating and supervised fine-tuning with a few samples. We open-source our work at https://x-isc.info/.</abstract>
      <url hash="aa948c3e">2025.acl-long.1314</url>
      <bibkey>zhang-etal-2025-understanding</bibkey>
    </paper>
    <paper id="1315">
      <title><fixed-case>V</fixed-case>ideo<fixed-case>V</fixed-case>ista-<fixed-case>C</fixed-case>ultural<fixed-case>L</fixed-case>ingo: 360° Horizons-Bridging Cultures, Languages, and Domains in Video Comprehension</title>
      <author><first>Xinyu</first><last>Chen</last></author>
      <author><first>Yunxin</first><last>Li</last></author>
      <author><first>Haoyuan</first><last>Shi</last></author>
      <author><first>Baotian</first><last>Hu</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <author><first>Wenhan</first><last>Luo</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Yaowei</first><last>Wang</last><affiliation>Harbin Institute of Technology, Shenzhen and Pengcheng Laboratory</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>27102-27128</pages>
      <abstract>Assessing the video comprehension capabilities of multimodal AI systems can effectively measure their understanding and reasoning abilities. Most video evaluation benchmarks are limited to a single language, typically English, and predominantly feature videos rooted in Western cultural contexts. In this paper, we present **VideoVista-CulturalLingo**, the first video evaluation benchmark designed to bridge cultural, linguistic, and domain divide in video comprehension. Our work differs from existing benchmarks in the following ways: 1) **Cultural diversity**, incorporating cultures from China, North America, and Europe; 2) **Multi-linguistics**, with questions presented in Chinese and English—two of the most widely spoken languages; and 3) **Broad domain**, featuring videos sourced from hundreds of human-created domains. VideoVista-CulturalLingo contains 1,389 videos and 3,134 QA pairs, and we have evaluated 24 recent open-source or proprietary video large models. From the experiment results, we observe that: 1) Existing models perform worse on Chinese-centric questions than Western-centric ones, particularly those related to Chinese history; 2) Current open-source models still exhibit limitations in temporal understanding, especially in the Event Localization task, achieving a maximum score of only 45.2%; 3) Mainstream models demonstrate strong performance in general scientific questions, while open-source models demonstrate weak performance in mathematics.</abstract>
      <url hash="291a61c2">2025.acl-long.1315</url>
      <bibkey>chen-etal-2025-videovista</bibkey>
    </paper>
    <paper id="1316">
      <title>What are the Essential Factors in Crafting Effective Long Context Multi-Hop Instruction Datasets? Insights and Best Practices</title>
      <author><first>Zhi</first><last>Chen</last></author>
      <author><first>Qiguang</first><last>Chen</last></author>
      <author><first>Libo</first><last>Qin</last><affiliation>Central South University</affiliation></author>
      <author><first>Qipeng</first><last>Guo</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Haijun</first><last>Lv</last></author>
      <author><first>Yicheng</first><last>Zou</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Hang</first><last>Yan</last></author>
      <author><first>Kai</first><last>Chen</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Dahua</first><last>Lin</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>27129-27151</pages>
      <abstract>Recent advancements in large language models (LLMs) with extended context windows have significantly improved various tasks. To improve long-context capabilities, much work focuses on augmenting LLM’s capabilities with synthetic data. Existing methods often leverage the Self-Instruct framework to generate long-context instruction-tuning data. However, our preliminary experiments show that fewer than 35% of samples generated by Qwen-2-72B are multi-hop, and over 40% exhibit poor quality, limiting comprehensive understanding and further research. To address this, we propose the Multi-agent Interactive Multi-hop Generation (MIMG) framework, which integrates a quality verification agent, a single-hop question generation agent, a multiple question sampling strategy, and a multi-hop question merger agent. This framework significantly improves data quality, with high-quality, multi-hop, and diverse data. Furthermore, we conduct a thorough analysis of document selection, question merging, and validation techniques through extensive experiments across various models. Our results demonstrate that synthetic high-quality long-context instruction data can enhance model performance, surpassing even models trained on larger amounts of human-annotated data.</abstract>
      <url hash="237cb844">2025.acl-long.1316</url>
      <bibkey>chen-etal-2025-essential</bibkey>
    </paper>
    <paper id="1317">
      <title>Knowledge Graph Retrieval-Augmented Generation for <fixed-case>LLM</fixed-case>-based Recommendation</title>
      <author><first>Shijie</first><last>Wang</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Wenqi</first><last>Fan</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Yue</first><last>Feng</last><affiliation>University of Birmingham</affiliation></author>
      <author><first>Lin</first><last>Shanru</last><affiliation>City University of Hong Kong</affiliation></author>
      <author><first>Xinyu</first><last>Ma</last><affiliation>Baidu</affiliation></author>
      <author><first>Shuaiqiang</first><last>Wang</last></author>
      <author><first>Dawei</first><last>Yin</last><affiliation>Baidu</affiliation></author>
      <pages>27152-27168</pages>
      <abstract>Recommender systems have become increasingly vital in our daily lives, helping to alleviate the problem of information overload across various user-oriented online services. The emergence of Large Language Models (LLMs) has yielded remarkable achievements, demonstrating their potential for the development of next-generation recommender systems. Despite these advancements, LLM-based recommender systems face inherent limitations stemming from their LLM backbones, particularly issues of hallucinations and the lack of up-to-date and domain-specific knowledge.Recently, Retrieval-Augmented Generation (RAG) has garnered significant attention for addressing these limitations by leveraging external knowledge sources to enhance the understanding and generation of LLMs. However, vanilla RAG methods often introduce noise and neglect structural relationships in knowledge, limiting their effectiveness in LLM-based recommendations. To address these limitations, we propose to retrieve high-quality and up-to-date structure information from the knowledge graph (KG) to augment recommendations. Specifically, our approach develops a retrieval-augmented framework, termed K-RagRec, that facilitates the recommendation generation process by incorporating structure information from the external KG. Extensive experiments have been conducted to demonstrate the effectiveness of our proposed method.</abstract>
      <url hash="ac191054">2025.acl-long.1317</url>
      <bibkey>wang-etal-2025-knowledge-graph</bibkey>
    </paper>
    <paper id="1318">
      <title><fixed-case>S</fixed-case>udo<fixed-case>LM</fixed-case>: Learning Access Control of Parametric Knowledge with Authorization Alignment</title>
      <author><first>Qin</first><last>Liu</last><affiliation>University of California, Davis</affiliation></author>
      <author><first>Fei</first><last>Wang</last></author>
      <author><first>Chaowei</first><last>Xiao</last><affiliation>University of Wisconsin - Madison and NVIDIA</affiliation></author>
      <author><first>Muhao</first><last>Chen</last><affiliation>University of California, Davis and University of Southern California</affiliation></author>
      <pages>27169-27181</pages>
      <abstract>Existing preference alignment is a one-size-fits-all alignment mechanism, where the part of the large language model (LLM) parametric knowledge with non-preferred features is uniformly blocked to all the users. However, this part of knowledge can be useful to advanced users whose expertise qualifies them to handle these information. The one-size-fits-all alignment mechanism undermines LLM’s utility for these qualified users. To address this problem, we propose SudoLM, a framework that lets LLMs learn access control over specific parametric knowledge for users with different credentials via authorization alignment. SudoLM allows authorized users to unlock their access to all the parametric knowledge with an assigned Sudo key while blocking access to non-qualified users. Experiments on two application scenarios demonstrate that SudoLM effectively controls the user’s access to the parametric knowledge and maintains its general utility.</abstract>
      <url hash="f4e0f7cf">2025.acl-long.1318</url>
      <bibkey>liu-etal-2025-sudolm</bibkey>
    </paper>
    <paper id="1319">
      <title><fixed-case>I</fixed-case>0<fixed-case>T</fixed-case>: Embedding Standardization Method Towards Zero Modality Gap</title>
      <author><first>Na Min</first><last>An</last><affiliation>Copenhagen University and KAIST</affiliation></author>
      <author><first>Eunki</first><last>Kim</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>James</first><last>Thorne</last><affiliation>KAIST</affiliation></author>
      <author><first>Hyunjung</first><last>Shim</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <pages>27182-27199</pages>
      <abstract>Contrastive Language-Image Pretraining (CLIP) enables zero-shot inference in downstream tasks such as image-text retrieval and classification. However, recent works extending CLIP suffer from the issue of *modality gap*, which arises when the image and text embeddings are projected to disparate manifolds, deviating from the intended objective of image-text contrastive learning. We discover that this phenomenon is linked to the modality-specific characteristic that each image or text encoder independently possesses. Herein, we propose two methods to address the modality gap: (1) a post-hoc embedding standardization method, <tex-math>I0T_{post}</tex-math> that reduces the modality gap approximately to zero and (2) a trainable method, <tex-math>I0T_{async}</tex-math>, to alleviate the modality gap problem by adding two normalization layers for each encoder. Our I0T framework can significantly reduce the modality gap while preserving the original embedding representations of trained models with their locked parameters. In practice, <tex-math>I0T_{post}</tex-math> can serve as an alternative explainable automatic evaluation metric of widely used CLIPScore (CLIP-S). The code is available in https://github.com/xfactlab/I0T.</abstract>
      <url hash="63fb06dd">2025.acl-long.1319</url>
      <bibkey>an-etal-2025-i0t</bibkey>
    </paper>
    <paper id="1320">
      <title>Odysseus Navigates the Sirens’ Song: Dynamic Focus Decoding for Factual and Diverse Open-Ended Text Generation</title>
      <author><first>Wen</first><last>Luo</last><affiliation>Peking University</affiliation></author>
      <author><first>Feifan</first><last>Song</last><affiliation>Peking University</affiliation></author>
      <author><first>Wei</first><last>Li</last></author>
      <author><first>Guangyue</first><last>Peng</last><affiliation>Peking University</affiliation></author>
      <author><first>Shaohang</first><last>Wei</last><affiliation>Peking University</affiliation></author>
      <author><first>Houfeng</first><last>Wang</last></author>
      <pages>27200-27218</pages>
      <abstract>Large Language Models (LLMs) are increasingly required to generate text that is both factually accurate and diverse across various open-ended applications. However, current stochastic decoding methods struggle to balance such objectives. We introduce Dynamic Focus Decoding (DFD), a novel plug-and-play stochastic approach that resolves this trade-off without requiring additional data, knowledge, or models. DFD adaptively adjusts the decoding focus based on distributional differences across layers, leveraging the modular and hierarchical nature of factual knowledge within LLMs. This dynamic adjustment improves factuality in knowledge-intensive decoding steps and promotes diversity in less knowledge-reliant steps. DFD can be easily integrated with existing decoding methods, enhancing both factuality and diversity with minimal computational overhead. Extensive experiments across seven datasets demonstrate that DFD significantly improves performance, providing a scalable and efficient solution for open-ended text generation.</abstract>
      <url hash="fdada702">2025.acl-long.1320</url>
      <bibkey>luo-etal-2025-odysseus</bibkey>
    </paper>
    <paper id="1321">
      <title>Better Embeddings with Coupled <fixed-case>A</fixed-case>dam</title>
      <author><first>Felix</first><last>Stollenwerk</last><affiliation>AI Sweden</affiliation></author>
      <author><first>Tobias</first><last>Stollenwerk</last><affiliation>Forschungszentrum Juelich GmbH</affiliation></author>
      <pages>27219-27236</pages>
      <abstract>Despite their remarkable capabilities, LLMs learn word representations that exhibit the undesirable yet poorly understood feature of anisotropy. In this paper, we argue that the second moment in Adam is a cause of anisotropic embeddings, and suggest a modified optimizer called Coupled Adam to mitigate the problem. Our experiments demonstrate that Coupled Adam significantly improves the quality of embeddings, while also leading to better upstream and downstream performance on large enough datasets.</abstract>
      <url hash="eb552524">2025.acl-long.1321</url>
      <bibkey>stollenwerk-stollenwerk-2025-better</bibkey>
    </paper>
    <paper id="1322">
      <title>Bone Soups: A Seek-and-Soup Model Merging Approach for Controllable Multi-Objective Generation</title>
      <author><first>Guofu</first><last>Xie</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Xiao</first><last>Zhang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Ting</first><last>Yao</last><affiliation>Tencent</affiliation></author>
      <author><first>Yunsheng</first><last>Shi</last></author>
      <pages>27237-27263</pages>
      <abstract>User information needs are often highly diverse and varied. A key challenge in current research is how to achieve controllable multi-objective generation while enabling rapid adaptation to accommodate diverse user demands during test time. Existing solutions, such as Rewarded Soup, focus on merging language models individually tuned on single objectives. While easy to implement and widely used, these approaches face limitations in achieving optimal performance due to their disregard for the impacts of competing objectives on model tuning. To address this issue, we propose **Bone Soup**, a novel model merging approach that first seeks a series of back**bone** models by considering the impacts of multiple objectives and then makes the **soup** (i.e., merge the backbone models). Specifically, Bone Soup begins by training multiple backbone models for different objectives using multi-objective reinforcement learning. Each backbone model is guided by a combination of backbone reward signals. To ensure that these models are optimal for the Pareto front, the backbone rewards are crafted by combining standard reward functions into basis vectors, which can then be modified through a rule-based construction method. Bone Soup leverages a symmetric circulant matrix mapping to generate the merging coefficients, which are used to merge the backbone models according to user preferences.Extensive experimental results demonstrate that Bone Soup exhibits strong controllability and Pareto optimality in controllable multi-objective generation, providing a more effective and efficient approach to addressing diverse user needs at test time.</abstract>
      <url hash="a322bb75">2025.acl-long.1322</url>
      <bibkey>xie-etal-2025-bone</bibkey>
    </paper>
    <paper id="1323">
      <title>Controllable and Reliable Knowledge-Intensive Task-Oriented Conversational Agents with Declarative Genie Worksheets</title>
      <author><first>Harshit</first><last>Joshi</last><affiliation>Stanford University</affiliation></author>
      <author><first>Shicheng</first><last>Liu</last><affiliation>Stanford University</affiliation></author>
      <author><first>James</first><last>Chen</last></author>
      <author><first>Larsen</first><last>Weigle</last></author>
      <author><first>Monica</first><last>Lam</last><affiliation>Stanford University</affiliation></author>
      <pages>27264-27308</pages>
      <abstract>Large Language Models are capable of carrying out human-like conversations in diverse settings in response to user requests for tasks and knowledge. However, existing conversational agents implemented with LLMs often struggle with hallucination, following instructions with conditional logic, and integrating knowledge from different sources. These shortcomings compromise the agents’ effectiveness, rendering them unsuitable for deployment. To address these challenges, we introduce Genie, a programmable framework for creating knowledge-intensive task-oriented conversational agents that handle involved interactions and answer complex queries. Unlike LLMs, Genie delivers reliable, grounded responses through advanced dialogue state management and supports controllable agent policies via its declarative specification – Genie Worksheet. This is achieved through an algorithmic runtime system that implements the developer-supplied policy, limiting LLMs to (1) parse user input using a succinct conversational history, and (2) generate responses according to supplied content. Agents built with Genie outperform SOTA methods on complex logic dialogue datasets by up to 20.5%. We conducted a user study with 62 participants. Genie agents with GPT-4 Turbo outperformed the GPT-4 Turbo agents with function calling, improving goal completion rates from 21.8% to 82.8% across three real-world tasks.</abstract>
      <url hash="dc394d99">2025.acl-long.1323</url>
      <bibkey>joshi-etal-2025-controllable</bibkey>
    </paper>
    <paper id="1324">
      <title>Benchmarking Long-Context Language Models on Long Code Understanding</title>
      <author><first>Jia</first><last>Li</last></author>
      <author><first>Xuyuan</first><last>Guo</last><affiliation>Peking University</affiliation></author>
      <author><first>Lei</first><last>Li</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Kechi</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Ge</first><last>Li</last><affiliation>Peking University</affiliation></author>
      <author><first>Jia</first><last>Li</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Zhengwei</first><last>Tao</last></author>
      <author><first>Fang</first><last>Liu</last><affiliation>Beijing University of Aeronautics and Astronautics</affiliation></author>
      <author><first>Chongyang</first><last>Tao</last><affiliation>Beihang University</affiliation></author>
      <author><first>Yuqi</first><last>Zhu</last></author>
      <author><first>Zhi</first><last>Jin</last><affiliation>Peking University</affiliation></author>
      <pages>27309-27327</pages>
      <abstract>Current advanced long-context language models offer great potential for real-world software engineering applications. However, progress in this critical domain remains hampered by a fundamental limitation: the absence of a rigorous evaluation framework for long code understanding. To gap this obstacle, we propose a long code understanding benchmark LongCodeU from four aspects (8 tasks) to evaluate LCLMs’ long code understanding ability required for practical applications, including code unit perception, intra-code unit understanding, inter-code unit relation understanding, and long code documentation understanding. We evaluate 9 popular LCLMs on LongCodeU (i.e., 6 general models and 3 code models). Our experimental results reveal key limitations in current LCLMs’ capabilities for long code understanding. Particularly, the performance of LCLMs drops dramatically when the long code length is greater than 32K, falling far short of their claimed 128K to 1M context windows. In the four aspects, inter-code unit relation understanding is the most challenging for LCLMs. Our study provides valuable insights for optimizing LCLMs and driving advancements in software engineering.</abstract>
      <url hash="e530fe62">2025.acl-long.1324</url>
      <bibkey>li-etal-2025-benchmarking</bibkey>
    </paper>
    <paper id="1325">
      <title><fixed-case>MAGNET</fixed-case>: Augmenting Generative Decoders with Representation Learning and Infilling Capabilities</title>
      <author><first>Savya</first><last>Khosla</last><affiliation>University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>Aditi</first><last>Tiwari</last></author>
      <author><first>Kushal</first><last>Kafle</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Simon</first><last>Jenni</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Handong</first><last>Zhao</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>John</first><last>Collomosse</last><affiliation>Adobe Systems and University of Surrey</affiliation></author>
      <author><first>Jing</first><last>Shi</last><affiliation>Adobe Systems</affiliation></author>
      <pages>27328-27346</pages>
      <abstract>While originally designed for unidirectional generative modeling, decoder-only large language models (LLMs) are increasingly being adapted for bidirectional modeling. However, unidirectional and bidirectional models are typically trained separately with distinct objectives (generation and representation learning). This separation overlooks the opportunity for developing a more versatile language model and for these objectives to complement each other. In this work, we propose MAGNET, a method for adapting decoder-only LLMs to generate robust representations and infill missing text spans. MAGNET employs three self-supervised training objectives and introduces an attention mechanism that combines bidirectional and causal attention, enabling unified training across all objectives. Our results demonstrate that LLMs adapted with MAGNET (1) surpass strong text encoders on token-level and sentence-level representation learning tasks, (2) generate contextually appropriate text infills by leveraging past and future contexts, (3) perform open-ended text generation without excessive repetition of words or phrases, and (4) preserve the knowledge and reasoning capability gained by the LLM during pretraining.</abstract>
      <url hash="d0e89f01">2025.acl-long.1325</url>
      <bibkey>khosla-etal-2025-magnet</bibkey>
    </paper>
    <paper id="1326">
      <title>Internal Value Alignment in Large Language Models through Controlled Value Vector Activation</title>
      <author><first>Haoran</first><last>Jin</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Meng</first><last>Li</last></author>
      <author><first>Xiting</first><last>Wang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Zhihao</first><last>Xu</last></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <author><first>Yantao</first><last>Jia</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Defu</first><last>Lian</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>27347-27371</pages>
      <abstract>Aligning Large Language Models (LLMs) with human values has attracted increasing attention since it provides clarity, transparency, and the ability to adapt to evolving scenarios. In this paper, we introduce a Controlled Value Vector Activation (ConVA) method that directly aligns the internal values of LLMs by interpreting how a value is encoded in their latent representations and modifies relevant activations to ensure consistent values in LLMs. To ensure an accurate and unbiased interpretation, we propose a context-controlled value vector identification method. To consistently control values without sacrificing model performance, we introduce a gated value vector activation method for effective and minimum degree of value control. Experiments show that our method achieves the highest control success rate across 10 basic values without hurting LLM performance and fluency, and ensures target values even with opposite and potentially malicious input prompts. Source code and data are available at https://github.com/hr-jin/ConVA.</abstract>
      <url hash="f1caa4c2">2025.acl-long.1326</url>
      <bibkey>jin-etal-2025-internal</bibkey>
    </paper>
    <paper id="1327">
      <title>A Dual-Perspective <fixed-case>NLG</fixed-case> Meta-Evaluation Framework with Automatic Benchmark and Better Interpretability</title>
      <author><first>Xinyu</first><last>Hu</last><affiliation>Peking University</affiliation></author>
      <author><first>Mingqi</first><last>Gao</last></author>
      <author id="li-lin"><first>Li</first><last>Lin</last></author>
      <author><first>Zhenghan</first><last>Yu</last></author>
      <author><first>Xiaojun</first><last>Wan</last></author>
      <pages>27372-27395</pages>
      <abstract>In NLG meta-evaluation, evaluation metrics are typically assessed based on their consistency with humans. However, we identify some limitations in traditional NLG meta-evaluation approaches, such as issues in handling human ratings and ambiguous selections of correlation measures, which undermine the effectiveness of meta-evaluation. In this work, we propose a dual-perspective NLG meta-evaluation framework that focuses on different evaluation capabilities, thereby providing better interpretability. In addition, we introduce a method of automatically constructing the corresponding benchmarks without requiring new human annotations. Furthermore, we conduct experiments with 16 representative LLMs as the evaluators based on our proposed framework, comprehensively analyzing their evaluation performance from different perspectives.</abstract>
      <url hash="2e4ccdc8">2025.acl-long.1327</url>
      <bibkey>hu-etal-2025-dual</bibkey>
    </paper>
    <paper id="1328">
      <title>Recurrent Knowledge Identification and Fusion for Language Model Continual Learning</title>
      <author><first>Yujie</first><last>Feng</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Xujia</first><last>Wang</last></author>
      <author><first>Zexin</first><last>Lu</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Shenghong</first><last>Fu</last></author>
      <author><first>Guangyuan</first><last>Shi</last></author>
      <author><first>Yongxin</first><last>Xu</last></author>
      <author><first>Yasha</first><last>Wang</last></author>
      <author><first>Philip S.</first><last>Yu</last><affiliation>University of Illinois Chicago</affiliation></author>
      <author><first>Xu</first><last>Chu</last><affiliation>Peking University</affiliation></author>
      <author><first>Xiao-Ming</first><last>Wu</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <pages>27396-27413</pages>
      <abstract>Continual learning (CL) is crucial for deploying large language models (LLMs) in dynamic real-world environments without costly retraining. While recent model ensemble and model merging methods guided by parameter importance have gained popularity, they often struggle to balance knowledge transfer and forgetting, mainly due to the reliance on static importance estimates during sequential training. In this paper, we present Recurrent-KIF, a novel CL framework for Recurrent Knowledge Identification and Fusion, which enables dynamic estimation of parameter importance distributions to enhance knowledge transfer. Inspired by human continual learning, Recurrent-KIF employs an inner loop that rapidly adapts to new tasks while identifying important parameters, coupled with an outer loop that globally manages the fusion of new and historical knowledge through redundant knowledge pruning and key knowledge merging. These inner-outer loops iteratively perform multiple rounds of fusion, allowing Recurrent-KIF to leverage intermediate training information and adaptively adjust fusion strategies based on evolving importance distributions. Extensive experiments on two CL benchmarks with various model sizes (from 770M to 13B) demonstrate that Recurrent-KIF effectively mitigates catastrophic forgetting and enhances knowledge transfer.</abstract>
      <url hash="0aa375a2">2025.acl-long.1328</url>
      <bibkey>feng-etal-2025-recurrent</bibkey>
    </paper>
    <paper id="1329">
      <title>Data-Constrained Synthesis of Training Data for De-Identification</title>
      <author><first>Thomas</first><last>Vakili</last><affiliation>Stockholm University</affiliation></author>
      <author><first>Aron</first><last>Henriksson</last><affiliation>Stockholm University</affiliation></author>
      <author><first>Hercules</first><last>Dalianis</last><affiliation>Stockholm University</affiliation></author>
      <pages>27414-27427</pages>
      <abstract>Many sensitive domains — such as the clinical domain — lack widely available datasets due to privacy risks. The increasing generative capabilities of large language models (LLMs) have made synthetic datasets a viable path forward. In this study, we domain-adapt LLMs to the clinical domain and generate synthetic clinical texts that are machine-annotated with tags for personally identifiable information using capable encoder-based NER models. The synthetic corpora are then used to train synthetic NER models. The results show that training NER models using synthetic corpora incurs only a small drop in predictive performance. The limits of this process are investigated in a systematic ablation study — using both Swedish and Spanish data. Our analysis shows that smaller datasets can be sufficient for domain-adapting LLMs for data synthesis. Instead, the effectiveness of this process is almost entirely contingent on the performance of the machine-annotating NER models trained using the original data.</abstract>
      <url hash="6a4fb3c9">2025.acl-long.1329</url>
      <bibkey>vakili-etal-2025-data</bibkey>
    </paper>
    <paper id="1330">
      <title>Just a Scratch: Enhancing <fixed-case>LLM</fixed-case> Capabilities for Self-harm Detection through Intent Differentiation and Emoji Interpretation</title>
      <author><first>Soumitra</first><last>Ghosh</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <author><first>Gopendra Vikram</first><last>Singh</last></author>
      <author><first>Shambhavi</first><last>Shambhavi</last></author>
      <author><first>Sabarna</first><last>Choudhury</last></author>
      <author><first>Asif</first><last>Ekbal</last><affiliation>Indian Institute of Technology, Jodhpur</affiliation></author>
      <pages>27428-27445</pages>
      <abstract>Self-harm detection on social media is critical for early intervention and mental health support, yet remains challenging due to the subtle, context-dependent nature of such expressions. Identifying self-harm intent aids suicide prevention by enabling timely responses, but current large language models (LLMs) struggle to interpret implicit cues in casual language and emojis. This work enhances LLMs’ comprehension of self-harm by distinguishing intent through nuanced language–emoji interplay. We present the <i>C</i>entennial <i>E</i>moji <i>S</i>ensitivity <i>M</i>atrix (<i>CESM-100</i>)—a curated set of 100 emojis with contextual self-harm interpretations—and the <i>S</i>elf-<i>H</i>arm <i>I</i>dentification a<i>N</i>d intent <i>E</i>xtraction with <i>S</i>upportive emoji sensitivity (<i>SHINES</i>) dataset, offering detailed annotations for self-harm labels, casual mentions (CMs), and serious intents (SIs). Our unified framework:a) enriches inputs using CESM-100;b) fine-tunes LLMs for multi-task learning—self-harm detection (primary) and CM/SI span detection (auxiliary);c) generate explainable rationales for self-harm predictions. We evaluate the framework on three state-of-the-art LLMs—Llama 3, Mental-Alpaca, and MentalLlama—across zero-shot, few-shot, and fine-tuned scenarios. By coupling intent differentiation with contextual cues, our approach commendably enhances LLM performance in both detection and explanation tasks, effectively addressing the inherent ambiguity in self-harm signals. The <i>SHINES</i> dataset, <i>CESM-100</i> and codebase are publicly available at: https://www.iitp.ac.in/%7eai-nlp-ml/resources.html#SHINES</abstract>
      <url hash="9d4d96ab">2025.acl-long.1330</url>
      <bibkey>ghosh-etal-2025-just</bibkey>
    </paper>
    <paper id="1331">
      <title>Contrastive Learning on <fixed-case>LLM</fixed-case> Back Generation Treebank for Cross-domain Constituency Parsing</title>
      <author><first>Peiming</first><last>Guo</last></author>
      <author><first>Meishan</first><last>Zhang</last><affiliation>Harbin Institute of Technology (Shenzhen), China</affiliation></author>
      <author><first>Jianling</first><last>Li</last></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <author><first>Yue</first><last>Zhang</last><affiliation>Westlake University</affiliation></author>
      <pages>27446-27458</pages>
      <abstract>Cross-domain constituency parsing is still an unsolved challenge in computational linguistics since the available multi-domain constituency treebank is limited. We investigate automatic treebank generation by large language models (LLMs) in this paper. The performance of LLMs on constituency parsing is poor, therefore we propose a novel treebank generation method, LLM back generation, which is similar to the reverse process of constituency parsing. LLM back generation takes the incomplete cross-domain constituency tree with only domain keyword leaf nodes as input and fills the missing words to generate the cross-domain constituency treebank. Besides, we also introduce a span-level contrastive learning pre-training strategy to make full use of the LLM back generation treebank for cross-domain constituency parsing. We verify the effectiveness of our LLM back generation treebank coupled with contrastive learning pre-training on five target domains of MCTB. Experimental results show that our approach achieves state-of-the-art performance on average results compared with various baselines.</abstract>
      <url hash="eebf3d5e">2025.acl-long.1331</url>
      <bibkey>guo-etal-2025-contrastive</bibkey>
    </paper>
    <paper id="1332">
      <title><fixed-case>MMDEND</fixed-case>: Dendrite-Inspired Multi-Branch Multi-Compartment Parallel Spiking Neuron for Sequence Modeling</title>
      <author><first>Kexin</first><last>Wang</last></author>
      <author><first>Yuhong</first><last>Chou</last></author>
      <author><first>Di</first><last>Shang</last></author>
      <author><first>Shijie</first><last>Mei</last></author>
      <author><first>Jiahong</first><last>Zhang</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yanbin</first><last>Huang</last></author>
      <author><first>Man</first><last>Yao</last><affiliation>Institute of automation, Chinese academy of sciences</affiliation></author>
      <author><first>Bo</first><last>Xu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Guoqi</first><last>Li</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <pages>27459-27470</pages>
      <abstract>Vanilla spiking neurons are simplified from complex biological neurons with dendrites, soma, and synapses, into single somatic compartments. Due to limitations in performance and training efficiency, vanilla spiking neurons face significant challenges in modeling long sequences. In terms of performance, the oversimplified dynamics of spiking neurons omit long-term temporal dependencies. Additionally, the long-tail membrane potential distribution and binary activation discretization errors further limit their capacity to model long sequences. In terms of efficiency, the serial mechanism of spiking neurons leads to excessively long training times for long sequences. Though parallel spiking neurons are an efficient solution, their number of parameters is often tied to the hidden dimension or sequence length, which makes current parallel neurons unsuitable for large architectures. To address these issues, we propose **MMDEND**: a Multi-Branch Multi-Compartment Parallel Spiking Dendritic Neuron. Its proportion-adjustable multi-branch, multi-compartment structure enables long-term temporal dependencies. Additionally, we introduce a Scaling-Shifting Integer Firing (SSF) mechanism that fits the long-tail membrane potential distribution, retains efficiency, and mitigates discretization errors. Compared with parallel neurons, MMDEND achieves better long-sequence modeling capability with fewer parameters and lower energy consumption. Visualization also confirms that the SSF mechanism effectively fits long-tail distributions.</abstract>
      <url hash="fd708050">2025.acl-long.1332</url>
      <bibkey>wang-etal-2025-mmdend</bibkey>
    </paper>
    <paper id="1333">
      <title>Understanding Impact of Human Feedback via Influence Functions</title>
      <author><first>Taywon</first><last>Min</last></author>
      <author><first>Haeone</first><last>Lee</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Yongchan</first><last>Kwon</last><affiliation>Columbia University</affiliation></author>
      <author><first>Kimin</first><last>Lee</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <pages>27471-27500</pages>
      <abstract>In Reinforcement Learning from Human Feedback (RLHF), it is crucial to learn suitable reward models from human feedback to align large language models (LLMs) with human intentions. However, human feedback can often be noisy, inconsistent, or biased, especially when evaluating complex responses. Such feedback can lead to misaligned reward signals, potentially causing unintended side effects during the RLHF process. To address these challenges, we explore the use of influence functions to measure the impact of human feedback on the performance of reward models. We propose a compute-efficient approximation method that enables the application of influence functions to LLM-based reward models and large-scale preference datasets. Our experiments showcase two key applications of influence functions: (1) detecting common labeler biases in human feedback datasets and (2) guiding labelers in refining their strategies to better align with expert feedback. By quantifying the impact of human feedback, we believe that influence functions can enhance feedback interpretability and contribute to scalable oversight in RLHF, helping labelers provide more accurate and consistent feedback. Source code is available at https://github.com/mintaywon/IF_RLHF.</abstract>
      <url hash="91337bbd">2025.acl-long.1333</url>
      <bibkey>min-etal-2025-understanding</bibkey>
    </paper>
    <paper id="1334">
      <title><fixed-case>T</fixed-case>2<fixed-case>I</fixed-case>-<fixed-case>F</fixed-case>actual<fixed-case>B</fixed-case>ench: Benchmarking the Factuality of Text-to-Image Models with Knowledge-Intensive Concepts</title>
      <author><first>Ziwei</first><last>Huang</last></author>
      <author><first>Wanggui</first><last>He</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Quanyu</first><last>Long</last></author>
      <author><first>Yandi</first><last>Wang</last></author>
      <author><first>Haoyuan</first><last>Li</last></author>
      <author><first>Zhelun</first><last>Yu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Fangxun</first><last>Shu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Weilong</first><last>Dai</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Hao</first><last>Jiang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Fei</first><last>Wu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Leilei</first><last>Gan</last><affiliation>Zhejiang University</affiliation></author>
      <pages>27501-27524</pages>
      <abstract>Most existing studies on evaluating text-to-image (T2I) models primarily focus on evaluating text-image alignment, image quality, and object composition capabilities, with comparatively fewer studies addressing the evaluation of the factuality of the synthesized images, particularly when the images involve knowledge-intensive concepts. In this work, we present T2I-FactualBench—the largest benchmark to date in terms of the number of concepts and prompts specifically designed to evaluate the factuality of knowledge-intensive concept generation. T2I-FactualBench consists of a three-tiered knowledge-intensive text-to-image generation framework, ranging from the basic memorization of individual knowledge concepts to the more complex composition of multiple knowledge concepts. We further introduce a multi-round visual question answering (VQA)-based evaluation framework to assesses the factuality of three-tiered knowledge-intensive text-to-image generation tasks. Experiments on T2I-FactualBench indicate that current state-of-the-art (SOTA) T2I models still leave significant room for improvement. We release our datasets and code at https://github.com/Safeoffellow/T2I-FactualBench.</abstract>
      <url hash="9721c8b3">2025.acl-long.1334</url>
      <bibkey>huang-etal-2025-t2i</bibkey>
    </paper>
    <paper id="1335">
      <title><fixed-case>I</fixed-case>nspire<fixed-case>D</fixed-case>ebate: Multi-Dimensional Subjective-Objective Evaluation-Guided Reasoning and Optimization for Debating</title>
      <author><first>Fuyu</first><last>Wang</last></author>
      <author><first>Jiangtong</first><last>Li</last><affiliation>Tongji University</affiliation></author>
      <author><first>Kun</first><last>Zhu</last></author>
      <author><first>Changjun</first><last>Jiang</last><affiliation>Tongji University</affiliation></author>
      <pages>27525-27544</pages>
      <abstract>With the rapid advancements in large language models (LLMs), debating tasks, such as argument quality assessment and debate process simulation, have made significant progress. However, existing LLM-based debating systems focus on responding to specific arguments while neglecting objective assessments such as authenticity and logical validity. Furthermore, these systems lack a structured approach to optimize across various dimensions—including evaluation metrics, chain-of-thought (CoT) reasoning, and multi-turn debate refinement—thereby limiting their effectiveness. To address these interconnected challenges, we propose a dual-component framework: (1) InspireScore, a novel evaluation system that establishes a multi-dimensional assessment architecture incorporating four subjective criteria (emotional appeal, argument clarity, argument arrangement, and topic relevance) alongside two objective metrics (fact authenticity and logical validity); and (2) InspireDebate, an optimized debating framework employing a phased optimization approach through CoT reasoning enhancement, multi-dimensional Direct Preference Optimization (DPO), and real-time knowledge grounding via web-based Retrieval Augmented Generation (Web-RAG). Empirical evaluations demonstrate that InspireScore achieves 44% higher correlation with expert judgments compared to existing methods, while InspireDebate shows significant improvements, outperforming baseline models by 57%. Source code is available at https://github.com/fywang12/InspireDebate.</abstract>
      <url hash="58c6574c">2025.acl-long.1335</url>
      <bibkey>wang-etal-2025-inspiredebate</bibkey>
    </paper>
    <paper id="1336">
      <title><fixed-case>O</fixed-case>pen<fixed-case>W</fixed-case>eb<fixed-case>V</fixed-case>oyager: Building Multimodal Web Agents via Iterative Real-World Exploration, Feedback and Optimization</title>
      <author><first>Hongliang</first><last>He</last></author>
      <author><first>Wenlin</first><last>Yao</last><affiliation>Amazon</affiliation></author>
      <author><first>Kaixin</first><last>Ma</last><affiliation>Apple</affiliation></author>
      <author><first>Wenhao</first><last>Yu</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Tianqing</first><last>Fang</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Zhenzhong</first><last>Lan</last><affiliation>Westlake University</affiliation></author>
      <author><first>Dong</first><last>Yu</last><affiliation>Tencent AI Lab</affiliation></author>
      <pages>27545-27564</pages>
      <abstract>The advancement of foundation models has laid the groundwork for building autonomous agents for complex tasks such as web navigation. Recent efforts have also tried to equip the agent with the ability to explore environments and continuously improve over time. However, existing works only focused on building text-only agents in synthetic environments where the reward signals are clearly defined. Such agents can hardly generalize to realistic settings that require multimodal perception ability and provide no ground-truth signal. In this paper, we introduce an innovative multimodal web agent that can autonomously conduct real-world exploration and improve itself. We first train the base model with imitation learning to gain the basic abilities. We then let the agent explore the open web and collect feedback on its trajectories. After that, it further improves its policy by learning from well-performing trajectories judged by another general-purpose model. This exploration-feedback-optimization cycle can continue for several iterations. Experimental results show that our web agent successfully improves itself after each iteration, demonstrating strong performance across multiple test sets. We will release our code and model to encourage future research in this field.</abstract>
      <url hash="a7de41cb">2025.acl-long.1336</url>
      <bibkey>he-etal-2025-openwebvoyager</bibkey>
    </paper>
    <paper id="1337">
      <title><fixed-case>FOCUS</fixed-case>: Evaluating Pre-trained Vision-Language Models on Underspecification Reasoning</title>
      <author><first>Kankan</first><last>Zhou</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Eason</first><last>Lai</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Kyriakos</first><last>Mouratidis</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Jing</first><last>Jiang</last><affiliation>Australian National University and Singapore Management University</affiliation></author>
      <pages>27565-27584</pages>
      <abstract>Humans possess a remarkable ability to interpret underspecified ambiguous statements by inferring their meanings from contexts such as visual inputs. This ability, however, may not be as developed in recent pre-trained vision-language models (VLMs). In this paper, we introduce a novel probing dataset called FOCUS to evaluate whether state-of-the-art VLMs have this ability. FOCUS consists of underspecified sentences paired with image contexts and carefully designed probing questions. Our experiments reveal that VLMs still fall short in handling underspecification even when visual inputs that can help resolve the ambiguities are available. To further support research in underspecification, FOCUS will be released for public use. We hope this dataset will inspire further research on the reasoning and contextual understanding capabilities of VLMs.</abstract>
      <url hash="a3fa548f">2025.acl-long.1337</url>
      <bibkey>zhou-etal-2025-focus</bibkey>
    </paper>
    <paper id="1338">
      <title>Sightation Counts: Leveraging Sighted User Feedback in Building a <fixed-case>BLV</fixed-case>-aligned Dataset of Diagram Descriptions</title>
      <author><first>Wan Ju</first><last>Kang</last></author>
      <author><first>Eunki</first><last>Kim</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Na Min</first><last>An</last><affiliation>Copenhagen University and KAIST</affiliation></author>
      <author><first>Sangryul</first><last>Kim</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Haemin</first><last>Choi</last></author>
      <author><first>Ki Hoon</first><last>Kwak</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>James</first><last>Thorne</last><affiliation>KAIST</affiliation></author>
      <pages>27585-27621</pages>
      <abstract>Often, the needs and visual abilities differ between the annotator group and the end user group. Generating detailed diagram descriptions for blind and low-vision (BLV) users is one such challenging domain. Sighted annotators could describe visuals with ease, but existing studies have shown that direct generations by them are costly, bias-prone, and somewhat lacking by BLV standards. In this study, we ask sighted individuals to assess—rather than produce—diagram descriptions generated by vision-language models (VLM) that have been guided with latent supervision via a multi-pass inference. The sighted assessments prove effective and useful to professional educators who are themselves BLV and teach visually impaired learners. We release Sightation, a collection of diagram description datasets spanning 5k diagrams and 137k samples for completion, preference, retrieval, question answering, and reasoning training purposes and demonstrate their fine-tuning potential in various downstream tasks.</abstract>
      <url hash="2c5d8c8e">2025.acl-long.1338</url>
      <bibkey>kang-etal-2025-sightation</bibkey>
    </paper>
    <paper id="1339">
      <title>Personal Travel Solver: A Preference-Driven <fixed-case>LLM</fixed-case>-Solver System for Travel Planning</title>
      <author><first>Zijian</first><last>Shao</last></author>
      <author><first>Jiancan</first><last>Wu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Weijian</first><last>Chen</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Xiang</first><last>Wang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>27622-27642</pages>
      <abstract>Personal travel planning is a challenging task that aims to find a feasible plan that not only satisfies diverse constraints but also meets the demands of the user’s explicit and implicit preferences. In this paper, we study how to integrate the user’s implicit preference into the progress of travel planning. We introduce RealTravel, an augmented version of the TravelPlanner by incorporating real user reviews and point-of-interest metadata from Google Local. Based on RealTravel, we propose Personal Travel Solver (PTS), an integrated system that combines LLMs with numerical solvers to generate travel plans that satisfy both explicit constraints and implicit user preferences. PTS employs a novel architecture that seamlessly connects explicit constraint validation with implicit preference modeling through five specialized modules. The experimental results demonstrate the system’s effectiveness, achieving better performance than baseline methods, and improvement in the level of personalization. Our data and code are available at [PersonalTravelSolver](https://github.com/cliftclift/PTS).</abstract>
      <url hash="caad0cf7">2025.acl-long.1339</url>
      <bibkey>shao-etal-2025-personal</bibkey>
    </paper>
    <paper id="1340">
      <title>Counterspeech the ultimate shield! Multi-Conditioned Counterspeech Generation through Attributed Prefix Learning</title>
      <author><first>Aswini Kumar</first><last>Padhi</last></author>
      <author><first>Anil</first><last>Bandhakavi</last></author>
      <author><first>Tanmoy</first><last>Chakraborty</last><affiliation>Indian Institute of Technology, Delhi</affiliation></author>
      <pages>27643-27663</pages>
      <abstract>Counterspeech has proven to be a powerful tool to combat hate speech online. Previous studies have focused on generating counterspeech conditioned only on specific intents (single attributed). However, a holistic approach considering multiple attributes simultaneously can yield more nuanced and effective responses. Here, we introduce HiPPrO, Hierarchical Prefix learning with Preference Optimization, a novel two-stage framework that utilizes the effectiveness of attribute-specific prefix embedding spaces hierarchically optimized during the counterspeech generation process in the first phase. Thereafter, we incorporate both reference and reward-free preference optimization to generate more constructive counterspeech. Furthermore, we extend IntentCONANv2 by annotating all 13,973 counterspeech instances with emotion labels by five annotators. HiPPrO leverages hierarchical prefix optimization to integrate these dual attributes effectively. An extensive evaluation demonstrates that HiPPrO achieves a 38 % improvement in intent conformity and a 3 %, 2 %, 3 % improvement in Rouge-1, Rouge-2, and Rouge-L, respectively, compared to several baseline models. Human evaluations further substantiate the superiority of our approach, highlighting the enhanced relevance and appropriateness of the generated counterspeech. This work underscores the potential of multi-attribute conditioning in advancing the efficacy of counterspeech generation systems. Our code is available on Github and dataset is open-sourced on Hugging-face.</abstract>
      <url hash="301251ee">2025.acl-long.1340</url>
      <bibkey>padhi-etal-2025-counterspeech</bibkey>
    </paper>
    <paper id="1341">
      <title><fixed-case>LLM</fixed-case><tex-math>\times</tex-math><fixed-case>M</fixed-case>ap<fixed-case>R</fixed-case>educe: Simplified Long-Sequence Processing using Large Language Models</title>
      <author><first>Zihan</first><last>Zhou</last><affiliation>Xiamen University</affiliation></author>
      <author><first>Chong</first><last>Li</last></author>
      <author><first>Xinyi</first><last>Chen</last></author>
      <author><first>Shuo</first><last>Wang</last></author>
      <author><first>Yu</first><last>Chao</last></author>
      <author><first>Zhili</first><last>Li</last></author>
      <author><first>Haoyu</first><last>Wang</last></author>
      <author><first>Qi</first><last>Shi</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Zhixing</first><last>Tan</last><affiliation>Zhongguancun Laboratory</affiliation></author>
      <author><first>Xu</first><last>Han</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Xiaodong</first><last>Shi</last><affiliation>Xiamen University, Tsinghua University</affiliation></author>
      <author><first>Zhiyuan</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Maosong</first><last>Sun</last><affiliation>Tsinghua University</affiliation></author>
      <pages>27664-27678</pages>
      <abstract>We propose a training-free framework that enables large language models (LLMs) to effectively process long texts, using a divide-and-conquer strategy for comprehensive document understanding.The proposed LLM<tex-math>\times</tex-math>MapReduce framework splits the entire document into several chunks for LLMs to read and then aggregates the intermediate outputs to produce the final response. The main challenge for divide-and-conquer long text processing frameworks lies in the risk of losing essential long-range information due to document splitting, which can lead the model to produce incomplete or incorrect answers based on the segmented texts.Disrupted long-range information can be classified into two categories: inter-chunk dependency and inter-chunk conflict.We design a structured information protocol to better cope with inter-chunk dependency and an in-context confidence calibration mechanism to resolve inter-chunk conflicts. Experiments demonstrate that LLM<tex-math>\times</tex-math>MapReduce outperforms representative open-source and commercial long-context LLMs and is compatible with several models.Our framework can also function as a data synthesis engine, capable of generating high-quality long-alignment data using only short-context LLMs.</abstract>
      <url hash="3c410194">2025.acl-long.1341</url>
      <bibkey>zhou-etal-2025-llmxmapreduce</bibkey>
    </paper>
    <paper id="1342">
      <title><fixed-case>C</fixed-case>he<fixed-case>X</fixed-case>align: Preference fine-tuning in chest <fixed-case>X</fixed-case>-ray interpretation models without human feedback</title>
      <author><first>Dennis</first><last>Hein</last><affiliation>KTH Royal Institute of Technology</affiliation></author>
      <author><first>Zhihong</first><last>Chen</last><affiliation>Stanford University</affiliation></author>
      <author><first>Sophie</first><last>Ostmeier</last><affiliation>Stanford University</affiliation></author>
      <author><first>Justin</first><last>Xu</last></author>
      <author><first>Maya</first><last>Varma</last><affiliation>Stanford University</affiliation></author>
      <author><first>Eduardo Pontes</first><last>Reis</last><affiliation>Stanford University and Hospital Israelita Albert Einstein</affiliation></author>
      <author><first>Arne Edward Michalson</first><last>Md</last></author>
      <author><first>Christian</first><last>Bluethgen</last><affiliation>Stanford University and University of Zurich</affiliation></author>
      <author><first>Hyun Joo</first><last>Shin</last></author>
      <author><first>Curtis</first><last>Langlotz</last><affiliation>Stanford University</affiliation></author>
      <author><first>Akshay S</first><last>Chaudhari</last><affiliation>Stanford University</affiliation></author>
      <pages>27679-27702</pages>
      <abstract>Radiologists play a crucial role in translating medical images into actionable reports. However, the field faces staffing shortages and increasing workloads. While automated approaches using vision-language models (VLMs) show promise as assistants, they require exceptionally high accuracy. Most current VLMs in radiology rely solely on supervised fine-tuning. Meanwhile, additional preference fine-tuning in the post-training pipeline has become standard practice in the general domain. The challenge in radiology lies in the prohibitive cost of obtaining radiologist feedback at scale. To address this challenge, we propose an automated pipeline for preference feedback, focusing on chest X-ray radiology report generation (RRG). Specifically, our method leverages publicly available datasets containing pairs of images and radiologist-written reference reports with reference-based metrics, or Judges, eliminating the need for *additional radiologist feedback*. We investigate reward overoptimization via length exploitation in this setting and introduce a length-controlled version of the GREEN score. Our best-performing setup achieves state-of-the-art CheXbert scores on the MIMIC-CXR dataset for the RRG task while on average maintaining robust performance across six additional image perception and reasoning tasks.</abstract>
      <url hash="131a8089">2025.acl-long.1342</url>
      <bibkey>hein-etal-2025-chexalign</bibkey>
    </paper>
    <paper id="1343">
      <title>Knowledge Tracing in Programming Education Integrating Students’ Questions</title>
      <author><first>Doyoun</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Suin</first><last>Kim</last><affiliation>Elice</affiliation></author>
      <author><first>Yohan</first><last>Jo</last><affiliation>Seoul National University</affiliation></author>
      <pages>27703-27718</pages>
      <abstract>Knowledge tracing (KT) in programming education presents unique challenges due to the complexity of coding tasks and the diverse methods students use to solve problems. Although students’ questions often contain valuable signals about their understanding and misconceptions, traditional KT models often neglect to incorporate these questions as inputs to address these challenges. This paper introduces SQKT (Students’ Question-based Knowledge Tracing), a knowledge tracing model that leverages students’ questions and automatically extracted skill information to enhance the accuracy of predicting students’ performance on subsequent problems in programming education. Our method creates semantically rich embeddings that capture not only the surface-level content of the questions but also the student’s mastery level and conceptual understanding. Experimental results demonstrate SQKT’s superior performance in predicting student completion across various Python programming courses of differing difficulty levels. In in-domain experiments, SQKT achieved a 33.1% absolute improvement in AUC compared to baseline models. The model also exhibited robust generalization capabilities in cross-domain settings, effectively addressing data scarcity issues in advanced programming courses. SQKT can be used to tailor educational content to individual learning needs and design adaptive learning systems in computer science education.</abstract>
      <url hash="91b452c7">2025.acl-long.1343</url>
      <bibkey>kim-etal-2025-knowledge</bibkey>
    </paper>
    <paper id="1344">
      <title><fixed-case>PRISM</fixed-case>: A Framework for Producing Interpretable Political Bias Embeddings with Political-Aware Cross-Encoder</title>
      <author><first>Yiqun</first><last>Sun</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Qiang</first><last>Huang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Anthony Kum Hoe</first><last>Tung</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Jun</first><last>Yu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>27719-27733</pages>
      <abstract>Semantic Text Embedding is a fundamental NLP task that encodes textual content into vector representations, where proximity in the embedding space reflects semantic similarity. While existing embedding models excel at capturing general meaning, they often overlook ideological nuances, limiting their effectiveness in tasks that require an understanding of political bias. To address this gap, we introduce PRISM, the first framework designed to <b>P</b>roduce inte<b>R</b>pretable pol<b>I</b>tical bia<b>S</b> e<b>M</b>beddings. PRISM operates in two key stages: (1) Controversial Topic Bias Indicator Mining, which systematically extracts fine-grained political topics and corresponding bias indicators from weakly labeled news data, and (2) Cross-Encoder Political Bias Embedding, which assigns structured bias scores to news articles based on their alignment with these indicators. This approach ensures that embeddings are explicitly tied to bias-revealing dimensions, enhancing both interpretability and predictive power. Through extensive experiments on large-scale datasets, we demonstrate that PRISM outperforms state-of-the-art text embedding models in political bias classification while offering highly interpretable representations that facilitate diversified retrieval and ideological analysis. The source code is available at <url>https://anonymous.4open.science/r/PRISM-80B4/</url>.</abstract>
      <url hash="1a4642fb">2025.acl-long.1344</url>
      <bibkey>sun-etal-2025-prism</bibkey>
    </paper>
    <paper id="1345">
      <title>Representations of Fact, Fiction and Forecast in Large Language Models: Epistemics and Attitudes</title>
      <author><first>Meng</first><last>Li</last></author>
      <author><first>Michael</first><last>Vrazitulis</last></author>
      <author><first>David</first><last>Schlangen</last><affiliation>University of Potsdam</affiliation></author>
      <pages>27734-27757</pages>
      <abstract>Rational speakers are supposed to know what they know and what they do not know, and to generate expressions matching the strength of evidence. In contrast, it is still a challenge for current large language models to generate corresponding utterances based on the assessment of facts and confidence in an uncertain real-world environment. While it has recently become popular to estimate and calibrate confidence of LLMs with verbalized uncertainty, what is lacking is a careful examination of the linguistic knowledge of uncertainty encoded in the latent space of LLMs. In this paper, we draw on typological frameworks of epistemic expressions to evaluate LLMs’ knowledge of epistemic modality, using controlled stories. Our experiments show that the performance of LLMs in generating epistemic expressions is limited and not robust, and hence the expressions of uncertainty generated by LLMs are not always reliable. To build uncertainty-aware LLMs, it is necessary to enrich semantic knowledge of epistemic modality in LLMs.</abstract>
      <url hash="cbcc1cea">2025.acl-long.1345</url>
      <bibkey>li-etal-2025-representations</bibkey>
    </paper>
    <paper id="1346">
      <title>Lexical Diversity-aware Relevance Assessment for Retrieval-Augmented Generation</title>
      <author><first>Zhange</first><last>Zhang</last><affiliation>Beijing University of Aeronautics and Astronautics</affiliation></author>
      <author><first>Yuqing</first><last>Ma</last></author>
      <author><first>Yulong</first><last>Wang</last><affiliation>Beijing University of Aeronautics and Astronautics</affiliation></author>
      <author><first>Shan</first><last>He</last></author>
      <author><first>Tianbo</first><last>Wang</last></author>
      <author><first>Siqi</first><last>He</last><affiliation>Peking University</affiliation></author>
      <author><first>Jiakai</first><last>Wang</last><affiliation>Zhongguancun Laboratory</affiliation></author>
      <author><first>Xianglong</first><last>Liu</last><affiliation>Beihang University</affiliation></author>
      <pages>27758-27781</pages>
      <abstract>Retrieval-Augmented Generation (RAG) has proven effective in enhancing the factuality of LLMs’ generation, making them a focal point of research. However, previous RAG approaches overlook the lexical diversity of queries, hindering their ability to achieve a granular relevance assessment between queries and retrieved documents, resulting in suboptimal performance. In this paper, we introduce a Lexical Diversity-aware RAG (DRAG) method to address the biases in relevant information retrieval and utilization induced by lexical diversity. Specifically, a Diversity-sensitive Relevance Analyzer is proposed to decouple and assess the relevance of different query components (words, phrases) based on their levels of lexical diversity, ensuring precise and comprehensive document retrieval. Moreover, a Risk-guided Sparse Calibration strategy is further introduced to calibrate the generated tokens that is heavily affected by irrelevant content. Through these modules, DRAG is capable of effectively retrieving relevant documents and leverages their pertinent knowledge to refine the original results and generate meaningful outcomes. Extensive experiments on widely used benchmarks demonstrate the efficacy of our approach, yielding a 10.6% accuracy improvement on HotpotQA.</abstract>
      <url hash="e1dccf0a">2025.acl-long.1346</url>
      <bibkey>zhang-etal-2025-lexical</bibkey>
    </paper>
    <paper id="1347">
      <title>Weaving Context Across Images: Improving Vision-Language Models through Focus-Centric Visual Chains</title>
      <author><first>Juntian</first><last>Zhang</last></author>
      <author><first>Chuanqi</first><last>Cheng</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Yuhan</first><last>Liu</last></author>
      <author><first>Wei</first><last>Liu</last></author>
      <author><first>Jian</first><last>Luan</last><affiliation>Xiaomi Corporation</affiliation></author>
      <author><first>Rui</first><last>Yan</last><affiliation>Renmin University of China</affiliation></author>
      <pages>27782-27798</pages>
      <abstract>Vision-language models (VLMs) achieve remarkable success in single-image tasks. However, real-world scenarios often involve intricate multi-image inputs, leading to a notable performance decline as models struggle to disentangle critical information scattered across complex visual features. In this work, we propose Focus-Centric Visual Chain, a novel paradigm that enhances VLMs’ perception, comprehension, and reasoning abilities in multi-image scenarios. To facilitate this paradigm, we propose Focus-Centric Data Synthesis, a scalable bottom-up approach for synthesizing high-quality data with elaborate reasoning paths. Through this approach, We construct VISC-150K, a large-scale dataset with reasoning data in the form of Focus-Centric Visual Chain, specifically designed for multi-image tasks. Experimental results on seven multi-image benchmarks demonstrate that our method achieves average performance gains of 3.16% and 2.24% across two distinct model architectures, without compromising the general vision-language capabilities. Our study represents a significant step toward more robust and capable vision-language systems that can handle complex visual scenarios.</abstract>
      <url hash="cdcda606">2025.acl-long.1347</url>
      <bibkey>zhang-etal-2025-weaving</bibkey>
    </paper>
    <paper id="1348">
      <title>Online Iterative Self-Alignment for Radiology Report Generation</title>
      <author><first>Ting</first><last>Xiao</last></author>
      <author><first>Lei</first><last>Shi</last></author>
      <author><first>Yang</first><last>Zhang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>HaoFeng</first><last>Yang</last><affiliation>East China University of Science and Technology</affiliation></author>
      <author><first>Zhe</first><last>Wang</last></author>
      <author><first>Chenjia</first><last>Bai</last><affiliation>TeleAI, China Telecom</affiliation></author>
      <pages>27799-27814</pages>
      <abstract>Radiology Report Generation (RRG) is an important research topic for relieving radiologists’ heavy workload. Existing RRG models mainly rely on supervised fine-tuning (SFT) based on different model architectures using data pairs of radiological images and corresponding radiologist-annotated reports. Recent research has shifted focus to post-training improvements, aligning RRG model outputs with human preferences using reinforcement learning (RL). However, the limited data coverage of high-quality annotated data poses risks of overfitting and generalization. This paper proposes a novel Online Iterative Self-Alignment (OISA) method for RRG that consists of four stages: self-generation of diverse data, self-evaluation for multi-objective preference data, self-alignment for multi-objective optimization and self-iteration for further improvement. Our approach allows for generating varied reports tailored to specific clinical objectives, enhancing the overall performance of the RRG model iteratively. Unlike existing methods, our framework significantly increases data quality and optimizes performance through iterative multi-objective optimization. Experimental results demonstrate that our method surpasses previous approaches, achieving state-of-the-art performance across multiple evaluation metrics.</abstract>
      <url hash="3887a13c">2025.acl-long.1348</url>
      <bibkey>xiao-etal-2025-online</bibkey>
    </paper>
    <paper id="1349">
      <title><fixed-case>C</fixed-case>hinese Inertial <fixed-case>GAN</fixed-case> for Handwriting Signal Generation and Recognition</title>
      <author><first>Yifeng</first><last>Wang</last><affiliation>national university of singaore, National University of Singapore</affiliation></author>
      <author><first>Yi</first><last>Zhao</last></author>
      <pages>27815-27832</pages>
      <abstract>Keyboard-based interaction may not accommodate various needs, especially for individuals with disabilities. While inertial sensor-based writing recognition is promising due to the sensors’ small size, wearability, and low cost, accurate recognition in the Chinese context is hampered by the difficulty of collecting extensive inertial signal samples for the vast number of characters. Therefore, we design a Chinese Inertial GAN (CI-GAN) containing Chinese glyph encoding (CGE), forced optimal transport (FOT), and semantic relevance alignment (SRA) to acquire unlimited high-quality training samples. Unlike existing vectorization methods focusing on the meaning of Chinese characters, CGE represents shape and stroke features, providing glyph guidance for writing signal generation. FOT establishes a triple-consistency constraint between the input prompt, output signal features, and real signal features, ensuring the authenticity and semantic accuracy of the generated signals. SRA aligns semantic relationships between multiple outputs and their input prompts, ensuring that similar inputs correspond to similar outputs (and vice versa), alleviating model hallucination. The three modules guide the generator while also interacting with each other, forming a coupled system. By utilizing the massive training samples provided by CI-GAN, the performance of six widely used classifiers is improved from 6.7% to 98.4%, indicating that CI-GAN constructs a flexible and efficient data platform for Chinese inertial writing recognition. Furthermore, we release the first Chinese inertial writing dataset on GitHub.</abstract>
      <url hash="e95c98d0">2025.acl-long.1349</url>
      <bibkey>wang-zhao-2025-chinese</bibkey>
    </paper>
    <paper id="1350">
      <title><fixed-case>LLM</fixed-case>s Caught in the Crossfire: Malware Requests and Jailbreak Challenges</title>
      <author><first>Haoyang</first><last>Li</last></author>
      <author><first>Huan</first><last>Gao</last></author>
      <author><first>Zhiyuan</first><last>Zhao</last><affiliation>China Telecom</affiliation></author>
      <author><first>Zhiyu</first><last>Lin</last></author>
      <author><first>Junyu</first><last>Gao</last><affiliation>Northwest Polytechnical University Xi’an</affiliation></author>
      <author><first>Xuelong</first><last>Li</last><affiliation>China Telecom and Northwestern Polytechnical University</affiliation></author>
      <pages>27833-27848</pages>
      <abstract>The widespread adoption of Large Language Models (LLMs) has heightened concerns about their security, particularly their vulnerability to jailbreak attacks that leverage crafted prompts to generate malicious outputs. While prior research has been conducted on general security capabilities of LLMs, their specific susceptibility to jailbreak attacks in code generation remains largely unexplored. To fill this gap, we propose MalwareBench, a benchmark dataset containing 3,520 jailbreaking prompts for malicious code-generation, designed to evaluate LLM robustness against such threats. MalwareBench is based on 320 manually crafted malicious code generation requirements, covering 11 jailbreak methods and 29 code functionality categories. Experiments show that mainstream LLMs exhibit limited ability to reject malicious code-generation requirements, and the combination of multiple jailbreak methods further reduces the model’s security capabilities: specifically, the average rejection rate for malicious content is 60.93%, dropping to 39.92% when combined with jailbreak attack algorithms. Our work highlights that the code security capabilities of LLMs still pose significant challenges.</abstract>
      <url hash="5620fa2a">2025.acl-long.1350</url>
      <bibkey>li-etal-2025-llms-caught</bibkey>
    </paper>
    <paper id="1351">
      <title>Evaluating Sequence Labeling on the basis of Information Theory</title>
      <author><first>Enrique</first><last>Amigo</last><affiliation>Universidad Nacional de Educación a Distancia</affiliation></author>
      <author><first>Elena</first><last>Álvarez-Mellado</last><affiliation>Universidad Nacional de Educación a Distancia</affiliation></author>
      <author><first>Julio</first><last>Gonzalo</last><affiliation>Universidad Nacional de Educación a Distancia</affiliation></author>
      <author><first>Jorge</first><last>Carrillo-de-Albornoz</last></author>
      <pages>27849-27860</pages>
      <abstract>Various metrics exist for evaluating sequence labeling problems (strict span matching, token oriented metrics, token concurrence in sequences, etc.), each of them focusing on certain aspects of the task. In this paper, we define a comprehensive set of formal properties that captures the strengths and weaknesses of the existing metric families and prove that none of them is able to satisfy all properties simultaneously. We argue that it is necessary to measure how much information (correct or noisy) each token in the sequence contributes depending on different aspects such as sequence length, number of tokens annotated by the system, token specificity, etc. On this basis, we introduce the <b>S</b>equence <b>L</b>abelling <b>I</b>nformation <b>C</b>ontrast <b>M</b>odel (SL-ICM), a novel metric based on information theory for evaluating sequence labeling tasks. Our formal analysis and experimentation show that the proposed metric satisfies all properties simultaneously</abstract>
      <url hash="afdd855f">2025.acl-long.1351</url>
      <bibkey>amigo-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="1352">
      <title><fixed-case>GRAT</fixed-case>: Guiding Retrieval-Augmented Reasoning through Process Rewards Tree Search</title>
      <author><first>Xianshu</first><last>Peng</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Wei</first><last>Wei</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <pages>27861-27875</pages>
      <abstract>Enhancing large models for complex multi-hop question-answering has become a research focus in the Retrieval-augmented generation (RAG) area. Many existing approaches aim to mimic human thought processes by enabling large models to perform retrieval-augmented generation step by step. However, these methods can only perform single chain reasoning, which lacks the ability for multi-path exploration, strategic look-ahead, stepwise evaluation, and global selection. In addition, to effectively decompose complex problems, these methods can only rely on labor-intensive intermediate annotations for supervised fine-tuning. To address these issues, we propose GRAT, an algorithm guided by Monte Carlo Tree Search (MCTS) and process rewards. GRAT not only enables self-evaluation and self-correction but also assigns fine-grained rewards to each intermediate step in the search path. These fine-grained annotations can be used for model self-training, which enables GRAT to continuously self-update its problem analysis and reasoning capabilities. We conducted experiments on four multihop QA datasets: HotPotQA, 2WikiMultiHopQA, MuSiQue, and Bamboogle, demonstrating that GRAT outperforms various RAG-based methods. Additionally, incorporating self-training significantly enhances GRAT’s reasoning performance.</abstract>
      <url hash="ca9abb9e">2025.acl-long.1352</url>
      <bibkey>peng-wei-2025-grat</bibkey>
    </paper>
    <paper id="1353">
      <title><fixed-case>T</fixed-case>-<fixed-case>REG</fixed-case>: Preference Optimization with Token-Level Reward Regularization</title>
      <author><first>Wenxuan</first><last>Zhou</last><affiliation>Google Deepmind</affiliation></author>
      <author><first>Shujian</first><last>Zhang</last><affiliation>University of Texas, Austin</affiliation></author>
      <author><first>Lingxiao</first><last>Zhao</last><affiliation>Mistral AI</affiliation></author>
      <author><first>Tao</first><last>Meng</last></author>
      <pages>27876-27889</pages>
      <abstract>Reinforcement Learning from Human Feedback (RLHF) has been pivotal in enabling Large Language Models (LLMs) to effectively follow instructions and produce meaningful alignment by leveraging human preference data. Traditionally, RLHF involves generating responses to a query and using a separate reward model to assign a score to the entire completion. This approach, however, presents challenges, as it provides a single, sparse reward at the end of a sequence, making optimization difficult for the model, in which both training and generation occur auto-regressively at token levels. While recent methods have attempted to address this by assigning token-level discrete or continuous rewards, these often rely on either a trained credit assignment model or AI annotators, which raises concerns about the quality and reliability of the token-level rewards. In this paper, we propose T-REG, which utilizes both sequence-level and token-level rewards for preference optimization. T-REG employs self-generated token-level rewards, derived through opposite prompting, as a weak supervision signal to guide the model in distributing sequence-level rewards at the token level, thereby achieving more effective token-level credit assignment and improving alignment performance. Experiments on the instruction following benchmarks, including Alpaca Eval 2 and Arena-Hard, show that our method consistently outperforms baseline methods by up to 3.8% and 4.4%, respectively.</abstract>
      <url hash="d92e5c88">2025.acl-long.1353</url>
      <bibkey>zhou-etal-2025-reg</bibkey>
    </paper>
    <paper id="1354">
      <title>Gödel Agent: A Self-Referential Agent Framework for Recursively Self-Improvement</title>
      <author><first>Xunjian</first><last>Yin</last></author>
      <author><first>Xinyi</first><last>Wang</last><affiliation>UC Santa Barbara</affiliation></author>
      <author><first>Liangming</first><last>Pan</last><affiliation>University of Arizona</affiliation></author>
      <author id="li-lin"><first>Li</first><last>Lin</last></author>
      <author><first>Xiaojun</first><last>Wan</last></author>
      <author><first>William Yang</first><last>Wang</last><affiliation>UC Santa Barbara</affiliation></author>
      <pages>27890-27913</pages>
      <abstract>The rapid advancement of large language models (LLMs) has significantly enhanced the capabilities of agents across various tasks. However, existing agentic systems, whether based on fixed pipeline algorithms or pre-defined meta-learning frameworks, cannot search the whole agent design space due to the restriction of human-designed components, and thus might miss the more optimal agent design. In this paper, we introduce Gödel Agent, a self-evolving framework inspired by the Gödel Machine, enabling agents to recursively improve themselves without relying on predefined routines or fixed optimization algorithms. Gödel Agent leverages LLMs to dynamically modify its own logic and behavior, guided solely by high-level objectives through prompting. Experimental results on multiple domains demonstrate that the implementation of Gödel Agent can achieve continuous self-improvement, surpassing manually crafted agents in performance, efficiency, and generalizability.</abstract>
      <url hash="6aa43be7">2025.acl-long.1354</url>
      <bibkey>yin-etal-2025-godel</bibkey>
    </paper>
    <paper id="1355">
      <title><fixed-case>A</fixed-case>gent<fixed-case>G</fixed-case>ym: Evaluating and Training Large Language Model-based Agents across Diverse Environments</title>
      <author><first>Zhiheng</first><last>Xi</last></author>
      <author><first>Yiwen</first><last>Ding</last></author>
      <author><first>Wenxiang</first><last>Chen</last></author>
      <author><first>Boyang</first><last>Hong</last></author>
      <author><first>Honglin</first><last>Guo</last><affiliation>Fudan University</affiliation></author>
      <author><first>Junzhe</first><last>Wang</last></author>
      <author><first>Xin</first><last>Guo</last></author>
      <author><first>Dingwen</first><last>Yang</last></author>
      <author><first>Chenyang</first><last>Liao</last></author>
      <author><first>Wei</first><last>He</last><affiliation>Fudan University</affiliation></author>
      <author><first>Songyang</first><last>Gao</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Lu</first><last>Chen</last></author>
      <author><first>Rui</first><last>Zheng</last></author>
      <author><first>Yicheng</first><last>Zou</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Tao</first><last>Gui</last><affiliation>Fudan University</affiliation></author>
      <author><first>Qi</first><last>Zhang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xipeng</first><last>Qiu</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xuanjing</first><last>Huang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Zuxuan</first><last>Wu</last><affiliation>Fudan University</affiliation></author>
      <author><first>Yu-Gang</first><last>Jiang</last><affiliation>Fudan University</affiliation></author>
      <pages>27914-27961</pages>
      <abstract>Large language models (LLMs) have emerged as a promising foundation to build generally-capable agents (LLM-based agents) that can handle multi-turn decision-making tasks across various environments. However, the community lacks a unified interactive framework that covers diverse environments for comprehensive evaluation of agents, and enables exploration and learning for their self-improvement. To address this, we propose AgentGym, a framework featuring 7 real-world scenarios, 14 environments, and 89 tasks for unified, real-time, and concurrent agent interaction. We construct expanded instruction set, high-quality trajectories, and comprehensive benchmarking suite for developing LLM-based agents. Moreover, AgentGym supports interactive exploration and learning for agents through multi-turn interactions and real-time feedback. Based on AgentGym, we take the initial step to develop LLM-based agents that can handle diverse tasks via methods like self-improvement or reinforcement learning. Experimental results show that the trained agents can achieve results comparable to commercial models. We hope our work can help the community develop more advanced LLM-based agents. We release the code, dataset, benchmark, and checkpoints at https://agentgym.github.io/.</abstract>
      <url hash="a136e2ab">2025.acl-long.1355</url>
      <bibkey>xi-etal-2025-agentgym</bibkey>
    </paper>
    <paper id="1356">
      <title>Rethinking the Role of Prompting Strategies in <fixed-case>LLM</fixed-case> Test-Time Scaling: A Perspective of Probability Theory</title>
      <author><first>Yexiang</first><last>Liu</last></author>
      <author><first>Zekun</first><last>Li</last></author>
      <author><first>Zhi</first><last>Fang</last></author>
      <author><first>Nan</first><last>Xu</last></author>
      <author><first>Ran</first><last>He</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Tieniu</first><last>Tan</last><affiliation>Institute of Automation, Chinese Academy of Sciences</affiliation></author>
      <pages>27962-27994</pages>
      <abstract>Recently, scaling test-time compute on Large Language Models (LLM) has garnered wide attention. However, there has been limited investigation of how various reasoning prompting strategies perform as scaling. In this paper, we focus on a standard and realistic scaling setting: majority voting. We systematically conduct experiments on 6 LLMs <tex-math>\times</tex-math> 8 prompting strategies <tex-math>\times</tex-math> 6 benchmarks. Experiment results consistently show that as the sampling time and computational overhead increase, complicated prompting strategies with superior initial performance gradually fall behind simple Chain-of-Thought.We analyze this phenomenon and provide theoretical proofs. Additionally, we propose a probabilistic method to efficiently predict scaling performance and identify the best prompting strategy under large sampling times, eliminating the need for resource-intensive inference processes in practical applications.Furthermore, we introduce two ways derived from our theoretical analysis to significantly improve the scaling performance. We hope that our research can promote to re-examine the role of complicated prompting, unleash the potential of simple prompting strategies, and provide new insights for enhancing test-time scaling performance. Code is available at https://github.com/MraDonkey/rethinking_prompting.</abstract>
      <url hash="bf152854">2025.acl-long.1356</url>
      <bibkey>liu-etal-2025-rethinking</bibkey>
    </paper>
    <paper id="1357">
      <title>Information Locality as an Inductive Bias for Neural Language Models</title>
      <author><first>Taiga</first><last>Someya</last></author>
      <author><first>Anej</first><last>Svete</last></author>
      <author><first>Brian</first><last>DuSell</last><affiliation>Department of Computer Science, ETHZ - ETH Zurich</affiliation></author>
      <author><first>Timothy J.</first><last>O’Donnell</last><affiliation>McGill University, Mila and McGill University</affiliation></author>
      <author><first>Mario</first><last>Giulianelli</last><affiliation>Department of Computer Science, ETHZ - ETH Zurich</affiliation></author>
      <author><first>Ryan</first><last>Cotterell</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <pages>27995-28013</pages>
      <abstract>Inductive biases are inherent in every machine learning system, shaping how models generalize from finite data. In the case of neural language models (LMs), debates persist as to whether these biases align with or diverge from human processing constraints. To address this issue, we propose a quantitative framework that allows for controlled investigations into the nature of these biases. Within our framework, we introduce <tex-math>m</tex-math>-local entropy—an information-theoretic measure derived from average lossy-context surprisal—that captures the local uncertainty of a language by quantifying how effectively the preceding symbols disambiguate the next symbol. In experiments on both perturbed natural language corpora and languages defined by probabilistic finite-state automata (PFSA), we show that languages with higher <tex-math>m</tex-math>-local entropy are more difficult for Transformer and LSTM LMs to learn. These results suggest that neural LMs, much like humans, are highly sensitive to the local statistical structure of a language.</abstract>
      <url hash="f0a13b9c">2025.acl-long.1357</url>
      <bibkey>someya-etal-2025-information</bibkey>
    </paper>
    <paper id="1358">
      <title>Learning to Reason Over Time: Timeline Self-Reflection for Improved Temporal Reasoning in Language Models</title>
      <author><first>Adrián</first><last>Bazaga</last><affiliation>Microsoft</affiliation></author>
      <author><first>Rexhina</first><last>Blloshmi</last><affiliation>Amazon</affiliation></author>
      <author id="bill-byrne"><first>Bill</first><last>Byrne</last><affiliation>Amazon and University of Cambridge</affiliation></author>
      <author><first>Adrià</first><last>de Gispert</last><affiliation>Amazon</affiliation></author>
      <pages>28014-28033</pages>
      <abstract>Large Language Models (LLMs) have emerged as powerful tools for generating coherent text, understanding context, and performing reasoning tasks. However, they struggle with temporal reasoning, which requires processing time-related information such as event sequencing, durations, and inter-temporal relationships. These capabilities are critical for applications including question answering, scheduling, and historical analysis. In this paper, we introduce TISER, a novel framework that enhances the temporal reasoning abilities of LLMs through a multi-stage process that combines timeline construction with iterative self-reflection. Our approach leverages test-time scaling to extend the length of reasoning traces, enabling models to capture complex temporal dependencies more effectively. This strategy not only boosts reasoning accuracy but also improves the traceability of the inference process. Experimental results demonstrate state-of-the-art performance across multiple benchmarks, including out-of-distribution test sets, and reveal that TISER enables smaller open-source models to surpass larger closed-weight models on challenging temporal reasoning tasks.</abstract>
      <url hash="2663b26b">2025.acl-long.1358</url>
      <bibkey>bazaga-etal-2025-learning</bibkey>
    </paper>
    <paper id="1359">
      <title>Query-driven Document-level Scientific Evidence Extraction from Biomedical Studies</title>
      <author><first>Massimiliano</first><last>Pronesti</last></author>
      <author><first>Joao H</first><last>Bettencourt-Silva</last><affiliation>IBM Research Europe - Ireland</affiliation></author>
      <author><first>Paul</first><last>Flanagan</last></author>
      <author><first>Alessandra</first><last>Pascale</last></author>
      <author><first>Oisín</first><last>Redmond</last></author>
      <author><first>Anya</first><last>Belz</last><affiliation>Dublin City University</affiliation></author>
      <author><first>Yufang</first><last>Hou</last><affiliation>IT:U Interdisciplinary Transformation University Austria, Technische Universität Darmstadt and IBM Research Ireland</affiliation></author>
      <pages>28034-28051</pages>
      <abstract>Extracting scientific evidence from biomedical studies for clinical research questions (e.g., Does stem cell transplantation improve quality of life in patients with medically refractory Crohn’s disease compared to placebo?) is a crucial step in synthesising biomedical evidence. In this paper, we focus on the task of document-level scientific evidence extraction for clinical questions with conflicting evidence. To support this task, we create a dataset called CochraneForest leveraging forest plots from Cochrane systematic reviews. It comprises 202 annotated forest plots, associated clinical research questions, full texts of studies, and study-specific conclusions. Building on CochraneForest, we propose URCA (Uniform Retrieval Clustered Augmentation), a retrieval-augmented generation framework designed to tackle the unique challenges of evidence extraction. Our experiments show that URCA outperforms the best existing methods by up to 10.3% in F1 score on this task. However, the results also underscore the complexity of CochraneForest, establishing it as a challenging testbed for advancing automated evidence synthesis systems.</abstract>
      <url hash="10ec9e7c">2025.acl-long.1359</url>
      <bibkey>pronesti-etal-2025-query</bibkey>
    </paper>
    <paper id="1360">
      <title>Towards Robust Universal Information Extraction: Dataset, Evaluation, and Solution</title>
      <author><first>Jizhao</first><last>Zhu</last><affiliation>Shenyang Aerospace University</affiliation></author>
      <author><first>Akang</first><last>Shi</last><affiliation>Shenyang Aerospace University</affiliation></author>
      <author><first>Zixuan</first><last>Li</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Long</first><last>Bai</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xiaolong</first><last>Jin</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jiafeng</first><last>Guo</last><affiliation>Institute of Computing Technolgy, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xueqi</first><last>Cheng</last><affiliation>Institute of Computing Technology, Chinese Academy</affiliation></author>
      <pages>28052-28070</pages>
      <abstract>In this paper, we aim to enhance the robustness of Universal Information Extraction (UIE) by introducing a new benchmark dataset, a comprehensive evaluation, and a feasible solution. Existing robust benchmark datasets have two key limitations: 1) They generate only a limited range of perturbations for a single Information Extraction (IE) task, which fails to evaluate the robustness of UIE models effectively; 2) They rely on small models or handcrafted rules to generate perturbations, often resulting in unnatural adversarial examples. Considering the powerful generation capabilities of Large Language Models (LLMs), we introduce a new benchmark dataset for Robust UIE, called RUIE-Bench, which utilizes LLMs to generate more diverse and realistic perturbations across different IE tasks. Based on this dataset, we comprehensively evaluate existing UIE models and reveal that both LLM-based models and other models suffer from significant performance drops. To improve robustness and reduce training costs, we propose a data-augmentation solution that dynamically selects hard samples for iterative training based on the model’s inference loss. Experimental results show that training with only <tex-math>\textbf{15}</tex-math>% of the data leads to an average <tex-math>\textbf{8.1}</tex-math>% relative performance improvement across three IE tasks. Our code and dataset are available at: https://github.com/ICT-GoKnow/RobustUIE.</abstract>
      <url hash="9ac2e3a6">2025.acl-long.1360</url>
      <bibkey>zhu-etal-2025-towards</bibkey>
    </paper>
    <paper id="1361">
      <title>Multi-perspective Alignment for Increasing Naturalness in Neural Machine Translation</title>
      <author><first>Huiyuan</first><last>Lai</last><affiliation>University of Groningen</affiliation></author>
      <author><first>Esther</first><last>Ploeger</last></author>
      <author><first>Rik</first><last>Van Noord</last><affiliation>University of Groningen</affiliation></author>
      <author><first>Antonio</first><last>Toral</last><affiliation>Universitat d’Alacant</affiliation></author>
      <pages>28071-28084</pages>
      <abstract>Neural machine translation (NMT) systems amplify lexical biases present in their training data, leading to artificially impoverished language in output translations. These language-level characteristics render automatic translations different from text originally written in a language and human translations, which hinders their usefulness in for example creating evaluation datasets. Attempts to increase naturalness in NMT can fall short in terms of content preservation, where increased lexical diversity comes at the cost of translation accuracy. Inspired by the reinforcement learning from human feedback framework, we introduce a novel method that rewards both naturalness and content preservation. We experiment with multiple perspectives to produce more natural translations, aiming at reducing machine and human translationese. We evaluate our method on English-to-Dutch literary translation, and find that our best model produces translations that are lexically richer and exhibit more properties of human-written language, without loss in translation accuracy.</abstract>
      <url hash="56b31f99">2025.acl-long.1361</url>
      <bibkey>lai-etal-2025-multi</bibkey>
    </paper>
    <paper id="1362">
      <title>Temporal reasoning for timeline summarisation in social media</title>
      <author><first>Jiayu</first><last>Song</last></author>
      <author><first>Mahmud Elahi</first><last>Akhter</last></author>
      <author><first>Dana</first><last>Atzil-Slonim</last><affiliation>Bar-Ilan University</affiliation></author>
      <author><first>Maria</first><last>Liakata</last><affiliation>Queen Mary University London</affiliation></author>
      <pages>28085-28101</pages>
      <abstract>This paper explores whether enhancing temporal reasoning capabilities in Large Language Models (LLMs) can improve the quality of timeline summarisation, the task of summarising long texts containing sequences of events, such as social media threads. We first introduce NarrativeReason, a novel dataset focused on temporal relationships among sequential events within narratives, distinguishing it from existing temporal reasoning datasets that primarily address pair-wise event relationships. Our approach then combines temporal reasoning with timeline summarisation through a knowledge distillation framework, where we first fine-tune a teacher model on temporal reasoning tasks and then distill this knowledge into a student model while simultaneously training it for the task of timeline summarisation. Experimental results demonstrate that our model achieves superior performance on out-of-domain mental health-related timeline summarisation tasks, which involve long social media threads with repetitions of events and a mix of emotions, highlighting the importance and generalisability of leveraging temporal reasoning to improve timeline summarisation.</abstract>
      <url hash="4fe34d53">2025.acl-long.1362</url>
      <bibkey>song-etal-2025-temporal</bibkey>
    </paper>
    <paper id="1363">
      <title>Beyond Negative Stereotypes – Non-Negative Abusive Utterances about Identity Groups and Their Semantic Variants</title>
      <author><first>Tina</first><last>Lommel</last></author>
      <author><first>Elisabeth</first><last>Eder</last><affiliation>Technische Universität Wien</affiliation></author>
      <author><first>Josef</first><last>Ruppenhofer</last></author>
      <author><first>Michael</first><last>Wiegand</last><affiliation>Universität Vienna</affiliation></author>
      <pages>28102-28120</pages>
      <abstract>We study a subtype of implicitly abusive language, namely non-negative sentences about identity groups (e.g. “Women make good cooks”), and introduce a novel dataset of such utterances. Not only do we profile such abusive sentences, but since our dataset includes different semantic variants of the same characteristic attributed to an identity group, we can also systematically study the impact of varying degrees of generalization and perspective framing. Similarly, we switch identity groups to assess whether the characteristic described in a sentence is inherently abusive. We also report on classification experiments.</abstract>
      <url hash="13fcf210">2025.acl-long.1363</url>
      <bibkey>lommel-etal-2025-beyond</bibkey>
    </paper>
    <paper id="1364">
      <title>Persistent Homology of Topic Networks for the Prediction of Reader Curiosity</title>
      <author><first>Manuel D.s.</first><last>Hopp</last><affiliation>Eberhard-Karls-Universität Tübingen</affiliation></author>
      <author><first>Vincent</first><last>Labatut</last><affiliation>Université d’Avignon</affiliation></author>
      <author><first>Arthur</first><last>Amalvy</last><affiliation>Université d’Avignon</affiliation></author>
      <author><first>Richard</first><last>Dufour</last><affiliation>Nantes University</affiliation></author>
      <author><first>Hannah</first><last>Stone</last><affiliation>Emerald</affiliation></author>
      <author><first>Hayley K</first><last>Jach</last><affiliation>University of Melbourne</affiliation></author>
      <author><first>Kou</first><last>Murayama</last><affiliation>Eberhard-Karls-Universität Tübingen</affiliation></author>
      <pages>28121-28132</pages>
      <abstract>Reader curiosity, the drive to seek information, is crucial for textual engagement, yet remains relatively underexplored in NLP. Building on Loewenstein’s Information Gap Theory, we introduce a framework that models reader curiosity by quantifying semantic information gaps within a text’s semantic structure. Our approach leverages BERTopic-inspired topic modeling and persistent homology to analyze the evolving topology (connected components, cycles, voids) of a dynamic semantic network derived from text segments, treating these features as proxies for information gaps. To empirically evaluate this pipeline, we collect reader curiosity ratings from participants (*n* = 49) as they read S. Collins’s “The Hunger Games” novel. We then use the topological features from our pipeline as independent variables to predict these ratings, and experimentally show that they significantly improve curiosity prediction compared to a baseline model (73% vs. 30% explained deviance), validating our approach. This pipeline offers a new computational method for analyzing text structure and its relation to reader engagement.</abstract>
      <url hash="a9a7ea2b">2025.acl-long.1364</url>
      <bibkey>hopp-etal-2025-persistent</bibkey>
    </paper>
    <paper id="1365">
      <title>Tokenisation is <fixed-case>NP</fixed-case>-Complete</title>
      <author><first>Philip</first><last>Whittington</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <author><first>Gregor</first><last>Bachmann</last></author>
      <author><first>Tiago</first><last>Pimentel</last><affiliation>Department of Computer Science, ETHZ - ETH Zurich</affiliation></author>
      <pages>28133-28153</pages>
      <abstract>In this work, we prove the NP-completeness of two variants of tokenisation, defined here as the problem of compressing a dataset to at most <tex-math>\delta</tex-math> symbols by either finding a vocabulary directly (_direct_ tokenisation), or selecting a sequence of merge operations (_bottom-up_ tokenisation).</abstract>
      <url hash="5acb0388">2025.acl-long.1365</url>
      <bibkey>whittington-etal-2025-tokenisation</bibkey>
    </paper>
    <paper id="1366">
      <title>Training Dynamics Underlying Language Model Scaling Laws: Loss Deceleration and Zero-Sum Learning</title>
      <author><first>Andrei</first><last>Mircea</last></author>
      <author><first>Supriyo</first><last>Chakraborty</last><affiliation>Capital One</affiliation></author>
      <author><first>Nima</first><last>Chitsazan</last><affiliation>CapitalOne</affiliation></author>
      <author><first>Irina</first><last>Rish</last><affiliation>University of Montreal</affiliation></author>
      <author><first>Ekaterina</first><last>Lobacheva</last><affiliation>Mila - Quebec Artificial Intelligence Institute and Université de Montréal</affiliation></author>
      <pages>28154-28188</pages>
      <abstract>This work aims to understand how scaling improves language models, specifically in terms of training dynamics. We find that language models undergo loss deceleration early in training—an abrupt slowdown in the rate of loss improvement, resulting in piecewise linear behaviour of the loss curve in log-log space. Scaling up the model mitigates this transition by (1) decreasing the loss at which deceleration occurs, and (2) improving the log-log rate of loss improvement after deceleration. We attribute loss deceleration to a type of degenerate training dynamics we term zero-sum learning (ZSL). In ZSL, per-example gradients become systematically opposed, leading to destructive interference in per-example changes in loss. As a result, improving loss on one subset of examples degrades it on another, bottlenecking overall progress. Loss deceleration and ZSL provide new insights into the training dynamics underlying language model scaling laws, and could potentially be targeted directly to improve language models independent of scale. We make our code and artefacts available at: https://github.com/mirandrom/zsl</abstract>
      <url hash="ab7ea601">2025.acl-long.1366</url>
      <bibkey>mircea-etal-2025-training</bibkey>
    </paper>
    <paper id="1367">
      <title>Parameter-Aware Contrastive Knowledge Editing: Tracing and Rectifying based on Critical Transmission Paths</title>
      <author><first>Songlin</first><last>Zhai</last></author>
      <author><first>Yuan</first><last>Meng</last></author>
      <author><first>Yuxin</first><last>Zhang</last></author>
      <author><first>Guilin</first><last>Qi</last></author>
      <pages>28189-28200</pages>
      <abstract>Large language models (LLMs) have encoded vast amounts of knowledge in their parameters, but the acquired knowledge can sometimes be incorrect or outdated over time, necessitating rectification after pre-training. Traditional localized methods in knowledge-based model editing (KME) typically assume that knowledge is stored in particular intermediate layers. However, recent research suggests that these methods do not identify the optimal locations for parameter editing, as knowledge gradually accumulates across all layers in LLMs during the forward pass rather than being stored in specific layers. This paper, for the first time, introduces the concept of critical transmission paths into KME for parameter updating. Specifically, these paths capture the key information flows that significantly influence the model predictions for the editing process. To facilitate this process, we also design a parameter-aware contrastive rectifying algorithm that considers less important paths as contrastive examples. Experiments on two prominent datasets and three widely used LLMs demonstrate the superiority of our method in editing performance.</abstract>
      <url hash="9b8b817e">2025.acl-long.1367</url>
      <bibkey>zhai-etal-2025-parameter</bibkey>
    </paper>
    <paper id="1368">
      <title>Many Heads Are Better Than One: Improved Scientific Idea Generation by A <fixed-case>LLM</fixed-case>-Based Multi-Agent System</title>
      <author><first>Haoyang</first><last>Su</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Renqi</first><last>Chen</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Shixiang</first><last>Tang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Zhenfei</first><last>Yin</last><affiliation>University of Oxford and University of Sydney</affiliation></author>
      <author><first>Xinzhe</first><last>Zheng</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Jinzhe</first><last>Li</last></author>
      <author><first>Biqing</first><last>Qi</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Qi</first><last>Wu</last></author>
      <author><first>Hui</first><last>Li</last><affiliation>shanghai institute for science of science</affiliation></author>
      <author><first>Wanli</first><last>Ouyang</last><affiliation>Shanghai AI Lab</affiliation></author>
      <author><first>Philip</first><last>Torr</last><affiliation>University of Oxford</affiliation></author>
      <author><first>Bowen</first><last>Zhou</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Nanqing</first><last>Dong</last><affiliation>Shanghai Artificial Intelligence Laboratory and Shanghai Innovation Institute</affiliation></author>
      <pages>28201-28240</pages>
      <abstract>The rapid advancement of scientific progress requires innovative tools that can accelerate knowledge discovery. Although recent AI methods, particularly large language models (LLMs), have shown promise in tasks such as hypothesis generation and experimental design, they fall short of replicating the collaborative nature of real-world scientific practices, where diverse experts work together in teams to tackle complex problems. To address the limitations, we propose an LLM-based multi-agent system, i.e., Virtual Scientists (VIRSCI), designed to mimic the teamwork inherent in scientific research. VIRSCI organizes a team of agents to collaboratively generate, evaluate, and refine research ideas. Through comprehensive experiments, we demonstrate that this multi-agent approach outperforms the state-of-the-art method in producing novel scientific ideas. We further investigate the collaboration mechanisms that contribute to its tendency to produce ideas with higher novelty, offering valuable insights to guide future research and illuminating pathways toward building a robust system for autonomous scientific discovery. The code is available at https://github.com/open-sciencelab/Virtual-Scientists.</abstract>
      <url hash="3cb19b84">2025.acl-long.1368</url>
      <bibkey>su-etal-2025-many</bibkey>
    </paper>
    <paper id="1369">
      <title>Inner Thinking Transformer: Leveraging Dynamic Depth Scaling to Foster Adaptive Internal Thinking</title>
      <author><first>Yilong</first><last>Chen</last></author>
      <author><first>Junyuan</first><last>Shang</last><affiliation>Baidu</affiliation></author>
      <author><first>Zhenyu</first><last>Zhang</last><affiliation>Baidu Inc.</affiliation></author>
      <author><first>Yanxi</first><last>Xie</last></author>
      <author><first>Jiawei</first><last>Sheng</last></author>
      <author><first>Tingwen</first><last>Liu</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <author><first>Shuohuan</first><last>Wang</last></author>
      <author><first>Yu</first><last>Sun</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <author><first>Haifeng</first><last>Wang</last><affiliation>Baidu</affiliation></author>
      <pages>28241-28259</pages>
      <abstract>Large language models (LLMs) face inherent performance bottlenecks under parameter constraints, particularly in processing critical tokens that demand complex reasoning. Empirical analysis reveals challenging tokens induce abrupt gradient spikes across layers, exposing architectural stress points in standard Transformers. Building on this insight, we propose Inner Thinking Transformer (ITT), which reimagines layer computations as implicit thinking steps. ITT dynamically allocates computation through Adaptive Token Routing, iteratively refines representations via Residual Thinking Connections, and distinguishes reasoning phases using Thinking Step Encoding. ITT enables deeper processing of critical tokens without parameter expansion. Evaluations across 162M-466M parameter models show ITT achieves 96.5% performance of a 466M Transformer using only 162M parameters, reduces training data by 43.2%, and outperforms Transformer/Loop variants in 11 benchmarks. By enabling elastic computation allocation during inference, ITT balances performance and efficiency through architecture-aware optimization of implicit thinking pathways.</abstract>
      <url hash="8f57fd34">2025.acl-long.1369</url>
      <bibkey>chen-etal-2025-inner</bibkey>
    </paper>
    <paper id="1370">
      <title>Document-Level Text Generation with Minimum <fixed-case>B</fixed-case>ayes Risk Decoding using Optimal Transport</title>
      <author><first>Yuu</first><last>Jinnai</last><affiliation>CyberAgent, Inc.</affiliation></author>
      <pages>28260-28279</pages>
      <abstract>Document-level text generation tasks are known to be more difficult than sentence-level text generation tasks as they require an understanding of longer context to generate high-quality texts. In this paper, we investigate the adaptation of Minimum Bayes Risk (MBR) decoding for document-level text generation tasks. MBR decoding makes use of a utility function to estimate the output with the highest expected utility from a set of candidate outputs. Although MBR decoding is shown to be effective in a wide range of sentence-level text generation tasks, its performance on document-level text generation tasks is limited, as many of the utility functions are designed for evaluating the utility of sentences. To this end, we propose MBR-OT, a variant of MBR decoding using Wasserstein distance to compute the utility of a document using a sentence-level utility function. The experimental result shows that the performance of MBR-OT outperforms that of the standard MBR in document-level machine translation, text simplification, and dense image captioning tasks.</abstract>
      <url hash="b07149ca">2025.acl-long.1370</url>
      <bibkey>jinnai-2025-document</bibkey>
    </paper>
    <paper id="1371">
      <title>Opt-Out: Investigating Entity-Level Unlearning for Large Language Models via Optimal Transport</title>
      <author><first>Minseok</first><last>Choi</last></author>
      <author><first>Daniel</first><last>Rim</last><affiliation>Hyundai Motor Group and KAIST</affiliation></author>
      <author><first>Dohyun</first><last>Lee</last></author>
      <author><first>Jaegul</first><last>Choo</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>28280-28297</pages>
      <abstract>Instruction-following large language models (LLMs), such as ChatGPT, have become widely popular among everyday users. However, these models inadvertently disclose private, sensitive information to their users, underscoring the need for machine unlearning techniques to remove selective information from the models. While prior work has focused on forgetting small, random subsets of training data at the instance-level, we argue that real-world scenarios often require the removal of an entire user data, which may require a more careful maneuver. In this study, we explore entity-level unlearning, which aims to erase all knowledge related to a target entity while preserving the remaining model capabilities. To address this, we introduce Opt-Out, an optimal transport-based unlearning method that utilizes the Wasserstein distance from the model’s initial parameters to achieve more effective and fine-grained unlearning. We also present the first Entity-Level Unlearning Dataset (ELUDe) designed to evaluate entity-level unlearning. Our empirical results demonstrate that Opt-Out surpasses existing methods, establishing a new standard for secure and adaptable LLMs that can accommodate user data removal requests without the need for full retraining.</abstract>
      <url hash="d368536a">2025.acl-long.1371</url>
      <bibkey>choi-etal-2025-opt</bibkey>
    </paper>
    <paper id="1372">
      <title>Mixture of Small and Large Models for <fixed-case>C</fixed-case>hinese Spelling Check</title>
      <author><first>Ziheng</first><last>Qiao</last><affiliation>Soochow University</affiliation></author>
      <author><first>Houquan</first><last>Zhou</last><affiliation>Soochow University, China</affiliation></author>
      <author><first>Zhenghua</first><last>Li</last><affiliation>Soochow University</affiliation></author>
      <pages>28298-28311</pages>
      <abstract>In the era of large language models (LLMs), the Chinese Spelling Check (CSC) task has seen various LLM methods developed, yet their performance remains unsatisfactory. In contrast, fine-tuned BERT-based models, relying on high-quality in-domain data, show excellent performance but suffer from edit pattern overfitting. This paper proposes a novel dynamic mixture approach that effectively combines the probability distributions of small models and LLMs during the beam search decoding phase, achieving a balanced enhancement of precise corrections from small models and the fluency of LLMs. This approach also eliminates the need for fine-tuning LLMs, saving significant time and resources, and facilitating domain adaptation. Comprehensive experiments demonstrate that our mixture approach significantly boosts error correction capabilities, achieving state-of-the-art results across multiple datasets. Our code is available at https://github.com/zhqiao-nlp/MSLLM.</abstract>
      <url hash="a21c4a3b">2025.acl-long.1372</url>
      <bibkey>qiao-etal-2025-mixture</bibkey>
    </paper>
    <paper id="1373">
      <title><fixed-case>DISC</fixed-case>: Plug-and-Play Decoding Intervention with Similarity of Characters for <fixed-case>C</fixed-case>hinese Spelling Check</title>
      <author><first>Ziheng</first><last>Qiao</last><affiliation>Soochow University</affiliation></author>
      <author><first>Houquan</first><last>Zhou</last><affiliation>Soochow University, China</affiliation></author>
      <author><first>Yumeng</first><last>Liu</last><affiliation>Soochow University, China</affiliation></author>
      <author><first>Zhenghua</first><last>Li</last><affiliation>Soochow University</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <author><first>Bo</first><last>Zhang</last></author>
      <author><first>Chen</first><last>Li</last></author>
      <author><first>Ji</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Fei</first><last>Huang</last><affiliation>Alibaba Group US</affiliation></author>
      <pages>28312-28324</pages>
      <abstract>One key characteristic of the Chinese spelling check (CSC) task is that incorrect characters are usually similar to the correct ones in either phonetics or glyph. To accommodate this, previous works usually leverage confusion sets, which suffer from two problems, i.e., difficulty in determining which character pairs to include and lack of probabilities to distinguish items in the set. In this paper, we propose a light-weight plug-and-play DISC (i.e., decoding intervention with similarity of characters) module for CSC models. DISC measures phonetic and glyph similarities between characters and incorporates this similarity information only during the inference phase. This method can be easily integrated into various existing CSC models, such as ReaLiSe, SCOPE, and ReLM, without additional training costs. Experiments on three CSC benchmarks demonstrate that our proposed method significantly improves model performance, approaching and even surpassing the current state-of-the-art models.</abstract>
      <url hash="c824107f">2025.acl-long.1373</url>
      <bibkey>qiao-etal-2025-disc</bibkey>
    </paper>
    <paper id="1374">
      <title>Causal Estimation of Tokenisation Bias</title>
      <author><first>Pietro</first><last>Lesci</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Clara</first><last>Meister</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <author><first>Thomas</first><last>Hofmann</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <author><first>Andreas</first><last>Vlachos</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Tiago</first><last>Pimentel</last><affiliation>Department of Computer Science, ETHZ - ETH Zurich</affiliation></author>
      <pages>28325-28340</pages>
      <abstract>Modern language models are typically trained over subword sequences, but ultimately define probabilities over character-strings. Ideally, the choice of the tokeniser—which maps character-strings to subwords—should not affect the probability assigned to the underlying character-string; in practice, it does. We define this mismatch as **tokenisation bias**. In this work, we quantify one particular type of tokenisation bias: the effect of including or not a subword (e.g., <tex-math>\langle</tex-math> hello <tex-math>\rangle</tex-math>) in a tokeniser’s vocabulary on the probability a trained model assigns to the corresponding characters (i.e., “hello”). Estimating this effect is challenging because each model is trained with only one tokeniser. We address this by framing tokenisation bias as a causal effect and estimating it using the regression discontinuity design. Specifically, we exploit the fact that tokenisation algorithms rank subwords and add the first <tex-math>K</tex-math> to a tokeniser’s vocabulary, where <tex-math>K</tex-math> is an arbitrary cutoff point. As such, we can estimate a causal effect by comparing similar subwords around this cutoff. Experimentally, we find that tokenisation consistently affects models’ outputs across scales, vocabularies, and tokenisers. Notably, a subword’s presence in a small model’s vocabulary may increase its characters’ probability by up to 17 times, highlighting tokenisation as a key design choice in language modelling.</abstract>
      <url hash="428eb844">2025.acl-long.1374</url>
      <bibkey>lesci-etal-2025-causal</bibkey>
    </paper>
    <paper id="1375">
      <title>Value Residual Learning</title>
      <author><first>Zhanchao</first><last>Zhou</last></author>
      <author><first>Tianyi</first><last>Wu</last></author>
      <author><first>Zhiyun</first><last>Jiang</last></author>
      <author><first>Fares</first><last>Obeid</last></author>
      <author><first>Zhenzhong</first><last>Lan</last><affiliation>Westlake University</affiliation></author>
      <pages>28341-28356</pages>
      <abstract>While Transformer models have achieved remarkable success in various domains, the effectiveness of information propagation through deep networks remains a critical challenge. Standard hidden state residuals often fail to adequately preserve initial token-level information in deeper layers. This paper introduces ResFormer, a novel architecture that enhances information flow by incorporating value residual connections in addition to hidden state residuals. And a variant is SVFormer, where all layers share the first layer’s value embedding. Comprehensive empirical evidence demonstrates ResFormer achieves equivalent validation loss with 16.11% fewer model parameters and 20.3% less training data compared to Transformer, while maintaining similar memory usage and computational cost. Besides, SVFormer reduces KV cache size by nearly half with only a small performance penalty and can be integrated with other KV-efficient methods, yielding further reductions in KV cache, with performance influenced by sequence length and cumulative learning rate.</abstract>
      <url hash="3ca36fe1">2025.acl-long.1375</url>
      <bibkey>zhou-etal-2025-value</bibkey>
    </paper>
    <paper id="1376">
      <title><fixed-case>SGIC</fixed-case>: A Self-Guided Iterative Calibration Framework for <fixed-case>RAG</fixed-case></title>
      <author><first>Guanhua</first><last>Chen</last></author>
      <author><first>Yutong</first><last>Yao</last></author>
      <author><first>Lidia S.</first><last>Chao</last><affiliation>University of Macau</affiliation></author>
      <author><first>Xuebo</first><last>Liu</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <author><first>Derek F.</first><last>Wong</last><affiliation>University of Macau</affiliation></author>
      <pages>28357-28370</pages>
      <abstract>Recent research in retrieval-augmented generation (RAG) has concentrated on retrieving useful information from candidate documents. However, numerous methodologies frequently neglect the calibration capabilities of large language models (LLMs), which capitalize on their robust in-context reasoning prowess. This work illustrates that providing LLMs with specific cues substantially improves their calibration efficacy, especially in multi-round calibrations. We present a new SGIC: Self-Guided Iterative Calibration Framework that employs uncertainty scores as a tool. Initially, this framework calculates uncertainty scores to determine both the relevance of each document to the query and the confidence level in the responses produced by the LLMs. Subsequently, it reevaluates these scores iteratively, amalgamating them with prior responses to refine calibration. Furthermore, we introduce an innovative approach for constructing an iterative self-calibration training set, which optimizes LLMs to efficiently harness uncertainty scores for capturing critical information and enhancing response accuracy. Our proposed framework significantly improves performance on both closed-source and open-source LLMs.</abstract>
      <url hash="18d800c4">2025.acl-long.1376</url>
      <bibkey>chen-etal-2025-sgic</bibkey>
    </paper>
    <paper id="1377">
      <title><fixed-case>N</fixed-case>usa<fixed-case>A</fixed-case>ksara: A Multimodal and Multilingual Benchmark for Preserving <fixed-case>I</fixed-case>ndonesian Indigenous Scripts</title>
      <author><first>Muhammad Farid</first><last>Adilazuarda</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Musa Izzanardi</first><last>Wijanarko</last><affiliation>Monash University</affiliation></author>
      <author><first>Lucky</first><last>Susanto</last></author>
      <author><first>Khumaisa</first><last>Nur’aini</last><affiliation>Monash University</affiliation></author>
      <author><first>Derry Tanti</first><last>Wijaya</last><affiliation>Monash University and Boston University</affiliation></author>
      <author><first>Alham Fikri</first><last>Aji</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>28371-28401</pages>
      <abstract>Indonesia is rich in languages and scripts. However, most NLP progress has been made using romanized text. In this paper, we present NusaAksara, a novel public benchmark for Indonesian languages that includes their original scripts. Our benchmark covers both text and image modalities and encompasses diverse tasks such as image segmentation, OCR, transliteration, translation, and language identification. Our data is constructed by human experts through rigorous steps. NusaAksara covers 8 scripts across 7 languages, including low-resource languages not commonly seen in NLP benchmarks. Although unsupported by Unicode, the Lampung script is included in this dataset. We benchmark our data across several models, from LLMs and VLMs such as GPT-4o, Llama 3.2, and Aya 23 to task-specific systems such as PP-OCR and LangID, and show that most NLP technologies cannot handle Indonesia’s local scripts, with many achieving near-zero performance.</abstract>
      <url hash="4eaffc29">2025.acl-long.1377</url>
      <bibkey>adilazuarda-etal-2025-nusaaksara</bibkey>
    </paper>
    <paper id="1378">
      <title><fixed-case>LLM</fixed-case>-based Rumor Detection via Influence Guided Sample Selection and Game-based Perspective Analysis</title>
      <author><first>Zhiliang</first><last>Tian</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Jingyuan</first><last>Huang</last></author>
      <author><first>Zejiang</first><last>He</last></author>
      <author><first>Zhen</first><last>Huang</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Menglong</first><last>Lu</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Linbo</first><last>Qiao</last></author>
      <author><first>Songzhu</first><last>Mei</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Yijie</first><last>Wang</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Dongsheng</first><last>Li</last><affiliation>National University of Defense Technology</affiliation></author>
      <pages>28402-28414</pages>
      <abstract>Rumor detection on social media has become an emerging topic. Traditional deep learning-based methods model rumors based on content, propagation structure, or user behavior, but these approaches are constrained by limited modeling capacity and insufficient training corpora. Recent studies have explored using LLMs for rumor detection through supervised fine-tuning (SFT), but face two issues: 1) unreliable samples sometimes mislead the model learning; 2) the model only learns the most salient input-output mapping and skips in-depth analyses of the rumored content for convenience. To address these issues, we propose an SFT-based LLM rumor detection model with Influence guided Sample selection and Game-based multi-perspective Analysis (ISGA). Specifically, we first introduce the Influence Score (IS) to assess the impact of samples on model predictions and select samples for SFT. We also approximate IS via Taylor expansion to reduce computational complexity. Next, we use LLMs to generate in-depth analyses of news content from multiple perspectives and model their collaborative process for prediction as a cooperative game. Then we utilize the Shapley value to quantify the contribution of each perspective for selecting informative perspective analyses. Experiments show that ISGA excels existing SOTA on three datasets.</abstract>
      <url hash="1f54af10">2025.acl-long.1378</url>
      <bibkey>tian-etal-2025-llm</bibkey>
    </paper>
    <paper id="1379">
      <title>Hierarchical-Task-Aware Multi-modal Mixture of Incremental <fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case> Experts for Embodied Continual Learning</title>
      <author><first>Ziqi</first><last>Jia</last></author>
      <author><first>Anmin</first><last>Wang</last></author>
      <author><first>Xiaoyang</first><last>Qu</last></author>
      <author><first>Xiaowen</first><last>Yang</last></author>
      <author><first>Jianzong</first><last>Wang</last><affiliation>Pingan Technology</affiliation></author>
      <pages>28415-28427</pages>
      <abstract>Previous continual learning setups for embodied intelligence focused on executing low-level actions based on human commands, neglecting the ability to learn high-level planning and multi-level knowledge. To address these issues, we propose the Hierarchical Embodied Continual Learning Setups (HEC) that divide the agent’s continual learning process into two layers: high-level instructions and low-level actions, and define five embodied continual learning sub-setups. Building on these setups, we introduce the Task-aware <tex-math>\textbf{M}</tex-math>ixture <tex-math>\textbf{o}</tex-math>f <tex-math>\textbf{I}</tex-math>ncremental <tex-math>\textbf{L}</tex-math>oRA <tex-math>\textbf{E}</tex-math>xperts (Task-aware MoILE) method. This approach achieves task recognition by clustering visual-text embeddings and uses both a task-level router and a token-level router to select the appropriate LoRA experts. To effectively address the issue of catastrophic forgetting, we apply Singular Value Decomposition (SVD) to the LoRA parameters obtained from prior tasks, preserving key components while orthogonally training the remaining parts. The experimental results show that our method stands out in reducing the forgetting of old tasks compared to other methods, effectively supporting agents in retaining prior knowledge while continuously learning new tasks.</abstract>
      <url hash="d6be8d88">2025.acl-long.1379</url>
      <bibkey>jia-etal-2025-hierarchical</bibkey>
    </paper>
    <paper id="1380">
      <title><fixed-case>S</fixed-case>pindle<fixed-case>KV</fixed-case>: A Novel <fixed-case>KV</fixed-case> Cache Reduction Method Balancing Both Shallow and Deep Layers</title>
      <author><first>Zicong</first><last>Tang</last></author>
      <author><first>Shi</first><last>Luohe</last></author>
      <author><first>Zuchao</first><last>Li</last></author>
      <author><first>Baoyuan</first><last>Qi</last><affiliation>Xiaomi Corporation</affiliation></author>
      <author><first>Liu</first><last>Guoming</last><affiliation>Xiaomi Corporation</affiliation></author>
      <author><first>Lefei</first><last>Zhang</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Ping</first><last>Wang</last><affiliation>Wuhan University</affiliation></author>
      <pages>28428-28442</pages>
      <abstract>Large Language Models (LLMs) have achieved impressive accomplishments in recent years. However, the increasing memory consumption of KV cache has possessed a significant challenge to the inference system. Eviction methods have revealed the inherent redundancy within the KV cache, demonstrating its potential for reduction, particularly in deeper layers. However, KV cache reduction for shallower layers has been found to be insufficient. Based on our observation that, the KV cache exhibits a high degree of similarity. Based on this observation, we proposed a novel KV cache reduction method, SpindleKV, which balances both shallow and deep layers. For deep layers, we employ an attention weight based eviction method, while for shallow layers, we apply a codebook based replacement approach which is learnt by similarity and merging policy. Moreover, SpindleKV addressed the Grouped-Query Attention (GQA) dilemma faced by other attention based eviction methods. Experiments on two common benchmarks with three different LLMs shown that SpindleKV obtained better KV cache reduction effect compared to baseline methods, while preserving similar or even better model performance.</abstract>
      <url hash="e5e58977">2025.acl-long.1380</url>
      <bibkey>tang-etal-2025-spindlekv</bibkey>
    </paper>
    <paper id="1381">
      <title>Medical Graph <fixed-case>RAG</fixed-case>: Evidence-based Medical Large Language Model via Graph Retrieval-Augmented Generation</title>
      <author><first>Junde</first><last>Wu</last><affiliation>University of Oxford</affiliation></author>
      <author><first>Jiayuan</first><last>Zhu</last><affiliation>University of Oxford</affiliation></author>
      <author><first>Yunli</first><last>Qi</last></author>
      <author><first>Jingkun</first><last>Chen</last><affiliation>University of Oxford</affiliation></author>
      <author><first>Min</first><last>Xu</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Filippo</first><last>Menolascina</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Yueming</first><last>Jin</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Vicente</first><last>Grau</last></author>
      <pages>28443-28467</pages>
      <abstract>We introduce MedGraphRAG, a novel graph-based Retrieval-Augmented Generation (RAG) framework designed to enhance LLMs in generating evidence-based medical responses, improving safety and reliability with private medical data. We introduce Triple Graph Construction and U-Retrieval to enhance GraphRAG, enabling holistic insights and evidence-based response generation for medical applications. Specifically, we connect user documents to credible medical sources and integrate Top-down Precise Retrieval with Bottom-up Response Refinement for balanced context awareness and precise indexing. Validated on 9 medical Q&amp;A benchmarks, 2 health fact-checking datasets, and a long-form generation test set, MedGraphRAG outperforms state-of-the-art models while ensuring credible sourcing. Our code is publicly available.</abstract>
      <url hash="b6d6a022">2025.acl-long.1381</url>
      <bibkey>wu-etal-2025-medical</bibkey>
    </paper>
    <paper id="1382">
      <title>Unifying Uniform and Binary-coding Quantization for Accurate Compression of Large Language Models</title>
      <author><first>Seungcheol</first><last>Park</last></author>
      <author><first>Jeongin</first><last>Bae</last><affiliation>NAVER</affiliation></author>
      <author><first>Beomseok</first><last>Kwon</last><affiliation>NAVER CLOUD</affiliation></author>
      <author><first>Minjun</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Byeongwook</first><last>Kim</last><affiliation>NAVER CLOUD</affiliation></author>
      <author><first>Se Jung</first><last>Kwon</last><affiliation>NAVER Cloud</affiliation></author>
      <author><first>U</first><last>Kang</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Dongsoo</first><last>Lee</last><affiliation>NAVER CLOVA</affiliation></author>
      <pages>28468-28488</pages>
      <abstract>How can we quantize large language models while preserving accuracy? Quantization is essential for deploying large language models (LLMs) efficiently. Binary-coding quantization (BCQ) and uniform quantization (UQ) are promising quantization schemes that have strong expressiveness and optimizability, respectively. However, neither scheme leverages both advantages. In this paper, we propose UniQuan<tex-math>_F</tex-math> (Unified Quantization with Flexible Mapping), an accurate quantization method for LLMs. UniQuan<tex-math>_F</tex-math> harnesses both strong expressiveness and optimizability by unifying the flexible mapping technique in UQ and BCQ’s non-uniform quantization levels. We propose unified initialization, and local and periodic mapping techniques to optimize the parameters in UniQuan<tex-math>_F</tex-math> precisely. After optimization, our unification theorem removes computational and memory overhead, allowing us to utilize the superior accuracy of UniQuan<tex-math>_F</tex-math> without extra deployment costs induced by the unification. Experimental results demonstrate that UniQuan<tex-math>_F</tex-math> outperforms existing UQ and BCQ methods, achieving up to 4.60% higher accuracy on GSM8K benchmark.</abstract>
      <url hash="0d2d2014">2025.acl-long.1382</url>
      <bibkey>park-etal-2025-unifying</bibkey>
    </paper>
    <paper id="1383">
      <title>Agentic Reasoning: A Streamlined Framework for Enhancing <fixed-case>LLM</fixed-case> Reasoning with Agentic Tools</title>
      <author><first>Junde</first><last>Wu</last><affiliation>University of Oxford</affiliation></author>
      <author><first>Jiayuan</first><last>Zhu</last><affiliation>University of Oxford</affiliation></author>
      <author><first>Yuyuan</first><last>Liu</last></author>
      <author><first>Min</first><last>Xu</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Yueming</first><last>Jin</last><affiliation>National University of Singapore</affiliation></author>
      <pages>28489-28503</pages>
      <abstract>We introduce Agentic Reasoning, a framework that enhances large language model (LLM) reasoning by integrating external tool-using agents. Agentic Reasoning dynamically leverages web search, code execution, and structured memory to address complex problems requiring deep research. A key innovation in our framework is the Mind-Map agent, which constructs a structured knowledge graph to store reasoning context and track logical relationships, ensuring coherence in long reasoning chains with extensive tool usage. Additionally, we conduct a comprehensive exploration of the Web-Search agent, leading to a highly effective search mechanism that surpasses all prior approaches. When deployed on DeepSeek-R1, our method achieves a new state-of-the-art (SOTA) among public models and delivers performance comparable to OpenAI Deep Research, the leading proprietary model in this domain. Extensive ablation studies validate the optimal selection of agentic tools and confirm the effectiveness of our Mind-Map and Web-Search agents in enhancing LLM reasoning. Our code and data are publicly available.</abstract>
      <url hash="4373f3ba">2025.acl-long.1383</url>
      <bibkey>wu-etal-2025-agentic</bibkey>
    </paper>
    <paper id="1384">
      <title>Probing Relative Interaction and Dynamic Calibration in Multi-modal Entity Alignment</title>
      <author><first>Chenxiao</first><last>Li</last></author>
      <author><first>Jingwei</first><last>Cheng</last><affiliation>Northeastern University, China</affiliation></author>
      <author><first>Qiang</first><last>Tong</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Fu</first><last>Zhang</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Cairui</first><last>Wang</last></author>
      <pages>28504-28516</pages>
      <abstract>Multi-modal entity alignment aims to identify equivalent entities between two different multi-modal knowledge graphs. Current methods have made significant progress by improving embedding and cross-modal fusion. However, most of them depend on using loss functions to capture the relationship between modalities or adopt a one-time strategy to directly compute modality weights using attention mechanisms, which overlooks the relative interactions between modalities at the entity level and the accuracy of modality weights, thereby hindering the generalization to diverse entities. To address this challenge, we propose RICEA, a relative interaction and calibration framework for multi-modal entity alignment, which dynamically computes weights based on the relative interaction and recalibrates the weights according to their uncertainties. Among these, we propose a novel method called ADC that utilizes attention mechanisms to perceive the uncertainty of the weight for each modality, rather than directly calculating the weight of each modality as in previous works. Across 5 datasets and 23 settings, our proposed framework significantly outperforms other baselines. Our code and data are available at https://github.com/ChenxiaoLi-Joe/RICEA.</abstract>
      <url hash="a9339026">2025.acl-long.1384</url>
      <bibkey>li-etal-2025-probing</bibkey>
    </paper>
    <paper id="1385">
      <title>Learn to Memorize: Scalable Continual Learning in Semiparametric Models with Mixture-of-Neighbors Induction Memory</title>
      <author><first>Guangyue</first><last>Peng</last><affiliation>Peking University</affiliation></author>
      <author><first>Tao</first><last>Ge</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Wen</first><last>Luo</last><affiliation>Peking University</affiliation></author>
      <author><first>Wei</first><last>Li</last></author>
      <author><first>Houfeng</first><last>Wang</last></author>
      <pages>28517-28531</pages>
      <abstract>Semiparametric language models (LMs) have shown promise in various Natural Language Processing (NLP) tasks. However, they utilize non-parametric memory as static storage, which lacks learning capability and remains disconnected from the internal information flow of the parametric models, limiting scalability and efficiency. Based on recent interpretability theories of LMs, we reconceptualize the non-parametric memory represented by <tex-math>k</tex-math>NN-LM as a learnable Mixture-of-Neighbors Induction Memory (MoNIM), which synergizes the induction capabilities of attention heads with the memorization strength of feed-forward networks (FFN). By integrating into the model’s information flow, MoNIM functions as an FFN-like bypass layer within the Transformer architecture, enabling effective learning of new knowledge. Extensive experiments demonstrate that MoNIM is a retentive and scalable continual learner in both data- and model-wise, enhancing the scalability and continual learning performance of semiparametric LMs.</abstract>
      <url hash="28c8a2c7">2025.acl-long.1385</url>
      <bibkey>peng-etal-2025-learn</bibkey>
    </paper>
    <paper id="1386">
      <title>Adverse Event Extraction from Discharge Summaries: A New Dataset, Annotation Scheme, and Initial Findings</title>
      <author><first>Imane</first><last>Guellil</last><affiliation>University of Birmingham</affiliation></author>
      <author><first>Salomé</first><last>Andres</last></author>
      <author><first>Atul</first><last>Anand</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Bruce</first><last>Guthrie</last></author>
      <author><first>Huayu</first><last>Zhang</last><affiliation>LifeArc</affiliation></author>
      <author><first>Abul</first><last>Hasan</last><affiliation>University College London, University of London</affiliation></author>
      <author><first>Honghan</first><last>Wu</last><affiliation>University of Glasgow</affiliation></author>
      <author><first>Beatrice</first><last>Alex</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <pages>28532-28562</pages>
      <abstract>In this work, we present a manually annotated corpus for Adverse Event (AE) extraction from discharge summaries of elderly patients, a population often underrepresented in clinical NLP resources. The dataset includes 14 clinically significant AEs—such as falls, delirium, and intracranial haemorrhage, along with contextual attributes like negation, diagnosis type, and in-hospital occurrence. Uniquely, the annotation schema supports both discontinuous and overlapping entities, addressing challenges rarely tackled in prior work. We evaluate multiple models using FlairNLP across three annotation granularities: fine-grained, coarse-grained, and coarse-grained with negation. While transformer-based models (e.g., BERT-cased) achieve strong performance on document-level coarse-grained extraction (F1 = 0.943), performance drops notably for fine-grained entity-level tasks (e.g., F1 = 0.675), particularly for rare events and complex attributes. These results demonstrate that despite high-level scores, significant challenges remain in detecting underrepresented AEs and capturing nuanced clinical language. Developed within a Trusted Research Environment (TRE), the dataset is available upon request via DataLoch and serves as a robust benchmark for evaluating AE extraction methods and supporting future cross-dataset generalisation.</abstract>
      <url hash="d99085a4">2025.acl-long.1386</url>
      <bibkey>guellil-etal-2025-adverse</bibkey>
    </paper>
    <paper id="1387">
      <title>Speed Up Your Code: Progressive Code Acceleration Through Bidirectional Tree Editing</title>
      <author><first>Longhui</first><last>Zhang</last></author>
      <author><first>Jiahao</first><last>Wang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Meishan</first><last>Zhang</last><affiliation>Harbin Institute of Technology (Shenzhen), China</affiliation></author>
      <author><first>GaoXiong</first><last>Cao</last><affiliation>South China University of Technology, South China University of Technology</affiliation></author>
      <author><first>Ensheng</first><last>Shi</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Mayuchi</first><last>Mayuchi</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Jun</first><last>Yu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Honghai</first><last>Liu</last></author>
      <author><first>Jing</first><last>Li</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>28563-28576</pages>
      <abstract>Large language models (LLMs) have made significant strides in code acceleration (CA) tasks. Current works typically fine-tune LLMs using slow-fast code pairs mined from online programming platforms. Although these methods are widely recognized for their effectiveness, the training data often lack clear code acceleration patterns and offer only limited speed improvements. Moreover, existing training methods, such as direct instruction fine-tuning (IFT), tend to overlook the hierarchical relationships among acceleration patterns. In this work, we introduce BITE, a novel training paradigm designed to improve LLMs’ CA capabilities through two key innovations: (1) Bidirectional tree editing, which generates high-quality training data by incrementally transforming given code into both its most efficient and least efficient variants, and (2) Progressive code acceleration learning, which enables LLMs to internalize multi-level CA strategies by learning increasingly sophisticated acceleration patterns. Additionally, we introduce a new CA evaluation benchmark and metric for comprehensive assessment of model performance on CA tasks. Extensive experiments on both our benchmark and existing benchmarks demonstrate the effectiveness of our approach. Notably, BITE enables Qwen-1.5B to outperform prompt-enhanced GPT-4 and current training-based methods on average across five programming languages.</abstract>
      <url hash="3a70db19">2025.acl-long.1387</url>
      <bibkey>zhang-etal-2025-speed</bibkey>
    </paper>
    <paper id="1388">
      <title>Multi-Facet Blending for Faceted Query-by-Example Retrieval</title>
      <author><first>Heejin</first><last>Do</last><affiliation>Pohang University of Science and Technology</affiliation></author>
      <author><first>Sangwon</first><last>Ryu</last><affiliation>Pohang University of Science and Technology</affiliation></author>
      <author><first>Jonghwi</first><last>Kim</last></author>
      <author><first>Gary</first><last>Lee</last></author>
      <pages>28577-28590</pages>
      <abstract>With the growing demand to fit fine-grained user intents, faceted query-by-example (QBE), which retrieves similar documents conditioned on specific facets, has gained recent attention. However, prior approaches mainly depend on document-level comparisons using basic indicators like citations due to the lack of facet-level relevance datasets; yet, this limits their use to citation-based domains and fails to capture the intricacies of facet constraints. In this paper, we propose a multi-facet blending (FaBle) augmentation method, which exploits modularity by decomposing and recomposing to explicitly synthesize facet-specific training sets. We automatically decompose documents into facet units and generate (ir)relevant pairs by leveraging LLMs’ intrinsic distinguishing capabilities; then, dynamically recomposing the units leads to facet-wise relevance-informed document pairs. Our modularization eliminates the need for pre-defined facet knowledge or labels. Further, to prove the FaBle’s efficacy in a new domain beyond citation-based scientific paper retrieval, we release a benchmark dataset for educational exam item QBE. FaBle augmentation on 1K documents remarkably assists training in obtaining facet conditional embeddings.</abstract>
      <url hash="116f5122">2025.acl-long.1388</url>
      <bibkey>do-etal-2025-multi</bibkey>
    </paper>
    <paper id="1389">
      <title><fixed-case>PIPER</fixed-case>: Benchmarking and Prompting Event Reasoning Boundary of <fixed-case>LLM</fixed-case>s via Debiasing-Distillation Enhanced Tuning</title>
      <author><first>Zhicong</first><last>Lu</last></author>
      <author><first>Changyuan</first><last>Tian</last></author>
      <author><first>PeiguangLi</first><last>PeiguangLi</last></author>
      <author><first>Li</first><last>Jin</last></author>
      <author><first>Sirui</first><last>Wang</last></author>
      <author><first>Wei</first><last>Jia</last></author>
      <author><first>Ying</first><last>Shen</last></author>
      <author><first>Guangluan</first><last>Xu</last><affiliation>University of the Chinese Academy of Sciences</affiliation></author>
      <pages>28591-28613</pages>
      <abstract>While Large Language Models (LLMs) excel in diverse domains, their validity in event reasoning remains underexplored. Most existing works merely stagnate at assessing LLMs’ event reasoning with a single event relational type or reasoning format, failing to conduct a complete evaluation and provide a practical solution for capability enhancement. In this paper, we propose <tex-math>\textbf{PIPER}</tex-math>, the first comprehensive benchmark for <tex-math>\textbf{P}</tex-math>robing <tex-math>\textbf{I}</tex-math>nto the <tex-math>\textbf{P}</tex-math>erformance boundary of LLMs in <tex-math>\textbf{E}</tex-math>vent <tex-math>\textbf{R}</tex-math>easoning. Motivated by our evaluation observations and error patterns analysis, we meticulously craft 10K diverse instruction-tuning demonstrations to alleviate event reasoning-oriented data scarcity. Additionally, a novel <tex-math>\textbf{D}</tex-math>ebiasing and <tex-math>\textbf{D}</tex-math>istillation-<tex-math>\textbf{E}</tex-math>nhanced <tex-math>\textbf{S}</tex-math>upervised <tex-math>\textbf{F}</tex-math>ine-<tex-math>\textbf{T}</tex-math>uning (<tex-math>\mathbf{D^2}</tex-math><tex-math>\textbf{E-SFT}</tex-math>) strategy is presented, which facilitates adhering to context and fixating significant contextual event information to elevate the event reasoning capability. Specifically, <tex-math>\mathrm{D^2}</tex-math>E-SFT removes the given sample’s context to construct an imagined sample, subtracting its logits to mitigate the bias of neglecting context and improve contextual faithfulness. To guide the model in emphasizing significant contextual event information, <tex-math>\mathrm{D^2}</tex-math>E-SFT employs a context-refined sample to achieve self-distillation with the alignment of logits. Extensive experimental results demonstrate the effectiveness of our data and strategy in expanding the performance boundary of event reasoning.</abstract>
      <url hash="2534e05c">2025.acl-long.1389</url>
      <bibkey>lu-etal-2025-piper</bibkey>
    </paper>
    <paper id="1390">
      <title><fixed-case>MIR</fixed-case>: Methodology Inspiration Retrieval for Scientific Research Problems</title>
      <author><first>Aniketh</first><last>Garikaparthi</last><affiliation>Tata Consultancy Services Limited, India</affiliation></author>
      <author><first>Manasi</first><last>Patwardhan</last><affiliation>Tata Consultancy Services Limited, India</affiliation></author>
      <author><first>Aditya Sanjiv</first><last>Kanade</last><affiliation>Microsoft</affiliation></author>
      <author><first>Aman</first><last>Hassan</last></author>
      <author><first>Lovekesh</first><last>Vig</last></author>
      <author><first>Arman</first><last>Cohan</last><affiliation>Yale University and Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>28614-28659</pages>
      <abstract>There has been a surge of interest in harnessing the reasoning capabilities of Large Language Models (LLMs) to accelerate scientific discovery. While existing approaches rely on grounding the discovery process within the relevant literature, effectiveness varies significantly with the quality and nature of the retrieved literature. We address the challenge of retrieving prior work whose concepts can inspire solutions for a given research problem, a task we define as Methodology Inspiration Retrieval (MIR). We construct a novel dataset tailored for training and evaluating retrievers on MIR, and establish baselines. To address MIR, we build the Methodology Adjacency Graph (MAG); capturing methodological lineage through citation relationships. We leverage MAG to embed an “intuitive prior’’ into dense retrievers for identifying patterns of methodological inspiration beyond superficial semantic similarity. This achieves significant gains of +5.4 in Recall@3 and +7.8 in Mean Average Precision (mAP) over strong baselines. Further, we adapt LLM-based re-ranking strategies to MIR, yielding additional improvements of +4.5 in Recall@3 and +4.8 in mAP. Through extensive ablation studies and qualitative analyses, we exhibit the promise of MIR in enhancing automated scientific discovery and outline avenues for advancing inspiration-driven retrieval.</abstract>
      <url hash="a846371c">2025.acl-long.1390</url>
      <bibkey>garikaparthi-etal-2025-mir</bibkey>
    </paper>
    <paper id="1391">
      <title>Sticking to the Mean: Detecting Sticky Tokens in Text Embedding Models</title>
      <author><first>Kexin</first><last>Chen</last></author>
      <author><first>Dongxia</first><last>Wang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Yi</first><last>Liu</last><affiliation>Quantstamp</affiliation></author>
      <author><first>Haonan</first><last>Zhang</last><affiliation>Ant Group and Zhejiang University</affiliation></author>
      <author><first>Wenhai</first><last>Wang</last><affiliation>Zhejiang University</affiliation></author>
      <pages>28660-28681</pages>
      <abstract>Despite the widespread use of Transformer-based text embedding models in NLP tasks, surprising “sticky tokens” can undermine the reliability of embeddings. These tokens, when repeatedly inserted into sentences, pull sentence similarity toward a certain value, disrupting the normal distribution of embedding distances and degrading downstream performance. In this paper, we systematically investigate such anomalous tokens, formally defining them and introducing an efficient detection method, Sticky Token Detector (STD), based on sentence and token filtering. Applying STD to 40 checkpoints across 14 model families, we discover a total of 868 sticky tokens. Our analysis reveals that these tokens often originate from special or unused entries in the vocabulary, as well as fragmented subwords from multilingual corpora. Notably, their presence does not strictly correlate with model size or vocabulary size. We further evaluate how sticky tokens affect downstream tasks like clustering and retrieval, observing significant performance drops of up to 50%. Through attention-layer analysis, we show that sticky tokens disproportionately dominate the model’s internal representations, raising concerns about tokenization robustness. Our findings show the need for better tokenization strategies and model design to mitigate the impact of sticky tokens in future text embedding applications.</abstract>
      <url hash="58df84ab">2025.acl-long.1391</url>
      <bibkey>chen-etal-2025-sticking</bibkey>
    </paper>
    <paper id="1392">
      <title>Memorizing is Not Enough: Deep Knowledge Injection Through Reasoning</title>
      <author><first>Ruoxi</first><last>Xu</last><affiliation>University of the Chinese Academy of Sciences</affiliation></author>
      <author><first>Yunjie</first><last>Ji</last></author>
      <author><first>Boxi</first><last>Cao</last></author>
      <author><first>Yaojie</first><last>Lu</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Hongyu</first><last>Lin</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xianpei</first><last>Han</last><affiliation>Institute of Software, CAS</affiliation></author>
      <author><first>Ben</first><last>He</last></author>
      <author><first>Yingfei</first><last>Sun</last><affiliation>University of Chinese Academy of Sciences</affiliation></author>
      <author><first>Xiangang</first><last>Li</last></author>
      <author><first>Le</first><last>Sun</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <pages>28682-28693</pages>
      <abstract>Although large language models (LLMs) excel in knowledge recall and reasoning, their static nature leads to outdated information as the real world evolves or when adapting to domain-specific knowledge, highlighting the need for effective knowledge injection. However, current research on knowledge injection remains superficial, mainly focusing on knowledge memorization and retrieval. This paper proposes a four-tier knowledge injection framework that systematically defines the levels of knowledge injection: memorization, retrieval, reasoning, and association. Based on this framework, we introduce DeepKnowledge, a synthetic experimental testbed designed for fine-grained evaluation of the depth of knowledge injection across three knowledge types (novel, incremental, and updated). We then explore various knowledge injection scenarios and evaluate the depth of knowledge injection for each scenario on the benchmark. Experimental results reveal key factors to reach each level of knowledge injection for LLMs and establish a mapping between the levels of knowledge injection and the corresponding suitable injection methods, aiming to provide a comprehensive approach for efficient knowledge injection across various levels. The code is available at [https://github.com/icip-cas/Knowledge-Learning-Toolkits](https://github.com/icip-cas/Knowledge-Learning-Toolkits).</abstract>
      <url hash="a81ff000">2025.acl-long.1392</url>
      <bibkey>xu-etal-2025-memorizing</bibkey>
    </paper>
    <paper id="1393">
      <title>Improving Dialogue State Tracking through Combinatorial Search for In-Context Examples</title>
      <author><first>Haesung</first><last>Pyun</last></author>
      <author><first>Yoonah</first><last>Park</last></author>
      <author><first>Yohan</first><last>Jo</last><affiliation>Seoul National University</affiliation></author>
      <pages>28694-28714</pages>
      <abstract>In dialogue state tracking (DST), in-context learning comprises a retriever that selects labeled dialogues as in-context examples and a DST model that uses these examples to infer the dialogue state of the query dialogue. Existing methods for constructing training data for retrievers suffer from three key limitations: (1) the synergistic effect of examples is not considered, (2) the linguistic characteristics of the query are not sufficiently factored in, and (3) scoring is not directly optimized for DST performance. Consequently, the retriever can fail to retrieve examples that would substantially improve DST performance. To address these issues, we present CombiSearch—a method that scores effective in-context examples based on their combinatorial impact on DST performance. Our evaluation on MultiWOZ shows that retrievers trained with CombiSearch surpass state-of-the-art models, achieving a 20× gain in data efficiency and generalizing well to the SGD dataset. Moreover, CombiSearch attains a 12% absolute improvement in the upper bound DST performance over traditional approaches when no retrieval errors are assumed. This significantly increases the headroom for practical DST performance while demonstrating that existing methods rely on suboptimal data for retriever training.</abstract>
      <url hash="f47f4aa4">2025.acl-long.1393</url>
      <bibkey>pyun-etal-2025-improving</bibkey>
    </paper>
    <paper id="1394">
      <title>Pretraining Context Compressor for Large Language Models with Embedding-Based Memory</title>
      <author><first>Yuhong</first><last>Dai</last></author>
      <author><first>Jianxun</first><last>Lian</last></author>
      <author><first>Yitian</first><last>Huang</last></author>
      <author><first>Wei</first><last>Zhang</last></author>
      <author><first>Mingyang</first><last>Zhou</last><affiliation>Shenzhen University</affiliation></author>
      <author><first>Mingqi</first><last>Wu</last><affiliation>Microsoft</affiliation></author>
      <author><first>Xing</first><last>Xie</last><affiliation>Microsoft Research Asia</affiliation></author>
      <author><first>Hao</first><last>Liao</last><affiliation>Shenzhen University</affiliation></author>
      <pages>28715-28732</pages>
      <abstract>Efficient processing of long contexts in large language models (LLMs) is essential for real-world applications like retrieval-augmented generation and in-context learning, especially in resource-constrained environments such as edge computing. This paper explores the embedding-based context compression to reduce inference costs while preserving the downstream LLM configurations. We propose a decoupled compressor-LLM framework, pretrained on text reconstruction and completion tasks, designed to effectively preserve essential contextual information within condensed embedding representations. Our extensive experiments investigate pretraining, model configurations, compression rates, efficiency across tasks, and adaptability to various LLMs. Results demonstrate that our approach outperforms competitive baselines in three domains and across eight datasets while being adaptable to different downstream LLMs. We find that thorough pretraining and carefully selected compression rates, such as 4x and 16x, enable a lightweight compressor to achieve a good balance between accuracy and speed. These findings underscore the potential of embedding-based compression to enhance LLM efficiency and motivate further research in this area.</abstract>
      <url hash="9d4c7945">2025.acl-long.1394</url>
      <bibkey>dai-etal-2025-pretraining</bibkey>
    </paper>
    <paper id="1395">
      <title>Dialogue Systems for Emotional Support via Value Reinforcement</title>
      <author><first>Juhee</first><last>Kim</last></author>
      <author><first>Chunghu</first><last>Mok</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Jisun</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Hyang Sook</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Yohan</first><last>Jo</last><affiliation>Seoul National University</affiliation></author>
      <pages>28733-28766</pages>
      <abstract>Emotional support dialogue systems aim to reduce help-seekers’ distress and help them overcome challenges. While human values—core beliefs that shape an individual’s priorities—are increasingly emphasized in contemporary psychological therapy for their role in fostering internal transformation and long-term emotional well-being, their integration into emotional support systems remains underexplored. To bridge this gap, we present a value-driven method for training emotional support dialogue systems designed to reinforce positive values in seekers. Notably, our model identifies which values to reinforce at each turn and how to do so, by leveraging online support conversations from Reddit. We evaluate the method across support skills, seekers’ emotional intensity, and value reinforcement. Our method consistently outperforms various baselines, effectively exploring and eliciting values from seekers. Additionally, leveraging crowd knowledge from Reddit significantly enhances its effectiveness. Therapists highlighted its ability to validate seekers’ challenges and emphasize positive aspects of their situations—both crucial elements of value reinforcement. Our work, being the first to integrate value reinforcement into emotional support systems, demonstrates its promise and establishes a foundation for future research.</abstract>
      <url hash="f8d6086a">2025.acl-long.1395</url>
      <bibkey>kim-etal-2025-dialogue</bibkey>
    </paper>
    <paper id="1396">
      <title>Length-Induced Embedding Collapse in <fixed-case>PLM</fixed-case>-based Models</title>
      <author><first>Yuqi</first><last>Zhou</last></author>
      <author><first>Sunhao</first><last>Dai</last></author>
      <author><first>Zhanshuo</first><last>Cao</last></author>
      <author><first>Xiao</first><last>Zhang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Jun</first><last>Xu</last><affiliation>Renmin University of China</affiliation></author>
      <pages>28767-28791</pages>
      <abstract>Text embeddings from PLM-based models enable a wide range of applications, yet their performance often degrades on longer texts. In this paper, we introduce a phenomenon we call <b>Length Collapse</b>, where embeddings of longer texts tend to cluster together. This clustering results in a distributional inconsistency between the embeddings of short and long texts. We further investigate how these differences contribute to the performance decline observed with longer texts across various downstream tasks. Through a rigorous theoretical analysis of the self-attention mechanism, which acts as a low-pass filter in PLM-based models, we demonstrate that as text length increases, the strength of low-pass filtering intensifies, causing embeddings to retain more low-frequency components. As a result, input token features become more similar, leading to clustering and ultimately the collapse of embeddings for longer texts. To address this issue, we propose a simple method, TempScale, which mitigates the Length Collapse phenomenon. By narrowing the gap in low-pass filtering rates between long and short texts, TempScale ensures more consistent embeddings across different text lengths. This approach leads to performance improvements of <b>0.94%</b> on MTEB and <b>1.10%</b> on LongEmbed, which focuses specifically on long-context retrieval, providing strong evidence for the validity of our analysis. The source code is available at blue<url>https://github.com/Yuqi-Zhou/Length_Collapse</url>.</abstract>
      <url hash="a883c5f5">2025.acl-long.1396</url>
      <bibkey>zhou-etal-2025-length</bibkey>
    </paper>
    <paper id="1397">
      <title><fixed-case>SH</fixed-case>u<fixed-case>BERT</fixed-case>: Self-Supervised Sign Language Representation Learning via Multi-Stream Cluster Prediction</title>
      <author><first>Shester</first><last>Gueuwou</last></author>
      <author><first>Xiaodan</first><last>Du</last><affiliation>Toyota Technological Institute at Chicago</affiliation></author>
      <author><first>Greg</first><last>Shakhnarovich</last><affiliation>Toyota Technological Institute at Chicago and University of Chicago</affiliation></author>
      <author><first>Karen</first><last>Livescu</last><affiliation>Toyota Technological Institute at Chicago</affiliation></author>
      <author><first>Alexander H.</first><last>Liu</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <pages>28792-28810</pages>
      <abstract>Sign language processing has traditionally relied on task-specific models, limiting the potential for transfer learning across tasks. Pre-training methods for sign language have typically focused on either supervised pre-training, which cannot take advantage of unlabeled data, or context-independent (frame or video segment) representations, which ignore the effects of relationships across time in sign language. We introduce SHuBERT (Sign Hidden-Unit BERT), a self-supervised contextual representation model learned from approximately 1,000 hours of American Sign Language video. SHuBERT adapts masked token prediction objectives to multi-stream visual sign language input, learning to predict multiple targets corresponding to clustered hand, face, and body pose streams. SHuBERT achieves state-of-the-art performance across multiple tasks including sign language translation, isolated sign language recognition, and fingerspelling detection.</abstract>
      <url hash="4d440b83">2025.acl-long.1397</url>
      <bibkey>gueuwou-etal-2025-shubert</bibkey>
    </paper>
    <paper id="1398">
      <title><fixed-case>ERU</fixed-case>-<fixed-case>KG</fixed-case>: Efficient Reference-aligned Unsupervised Keyphrase Generation</title>
      <author><first>Lam Thanh</first><last>Do</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Aaditya</first><last>Bodke</last></author>
      <author><first>Pritom Saha</first><last>Akash</last></author>
      <author><first>Kevin Chen-Chuan</first><last>Chang</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <pages>28811-28829</pages>
      <abstract>Unsupervised keyphrase prediction has gained growing interest in recent years. However, existing methods typically rely on heuristically defined importance scores, which may lead to inaccurate informativeness estimation. In addition, they lack consideration for time efficiency. To solve these problems, we propose ERU-KG, an unsupervised keyphrase generation (UKG) model that consists of an informativeness and a phraseness module. The former estimates the relevance of keyphrase candidates, while the latter generate those candidates. The informativeness module innovates by learning to model informativeness through references (e.g., queries, citation contexts, and titles) and at the term-level, thereby 1) capturing how the key concepts of documents are perceived in different contexts and 2) estimating informativeness of phrases more efficiently by aggregating term informativeness, removing the need for explicit modeling of the candidates. ERU-KG demonstrates its effectiveness on keyphrase generation benchmarks by outperforming unsupervised baselines and achieving on average 89% of the performance of a supervised model for top 10 predictions. Additionally, to highlight its practical utility, we evaluate the model on text retrieval tasks and show that keyphrases generated by ERU-KG are effective when employed as query and document expansions. Furthermore, inference speed tests reveal that ERU-KG is the fastest among baselines of similar model sizes. Finally, our proposed model can switch between keyphrase generation and extraction by adjusting hyperparameters, catering to diverse application requirements.</abstract>
      <url hash="0bf41665">2025.acl-long.1398</url>
      <bibkey>do-etal-2025-eru</bibkey>
    </paper>
    <paper id="1399">
      <title>Know Your Mistakes: Towards Preventing Overreliance on Task-Oriented Conversational <fixed-case>AI</fixed-case> Through Accountability Modeling</title>
      <author><first>Suvodip</first><last>Dey</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Yi-Jyun</first><last>Sun</last></author>
      <author><first>Gokhan</first><last>Tur</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Dilek</first><last>Hakkani-Tür</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <pages>28830-28843</pages>
      <abstract>Recent LLMs have enabled significant advancements for conversational agents. However, they are also well known to hallucinate, producing responses that seem plausible but are factually incorrect. On the other hand, users tend to over-rely on LLM-based AI agents, accepting AI’s suggestion even when it is wrong. Adding positive friction, such as explanations or getting user confirmations, has been proposed as a mitigation in AI-supported decision-making systems. In this paper, we propose an accountability model for LLM-based task-oriented dialogue agents to address user overreliance via friction turns in cases of model uncertainty and errors associated with dialogue state tracking (DST). The accountability model is an augmented LLM with an additional accountability head that functions as a binary classifier to predict the relevant slots of the dialogue state mentioned in the conversation. We perform our experiments with multiple backbone LLMs on two established benchmarks (MultiWOZ and Snips). Our empirical findings demonstrate that the proposed approach not only enables reliable estimation of AI agent errors but also guides the decoder in generating more accurate actions. We observe around 3% absolute improvement in joint goal accuracy (JGA) of DST output by incorporating accountability heads into modern LLMs. Self-correcting the detected errors further increases the JGA from 67.13 to 70.51, achieving state-of-the-art DST performance. Finally, we show that error correction through user confirmations (friction turn) achieves a similar performance gain, highlighting its potential to reduce user overreliance.</abstract>
      <url hash="7573392b">2025.acl-long.1399</url>
      <bibkey>dey-etal-2025-know</bibkey>
    </paper>
    <paper id="1400">
      <title><fixed-case>LLM</fixed-case>s Trust Humans More, That’s a Problem! Unveiling and Mitigating the Authority Bias in Retrieval-Augmented Generation</title>
      <author><first>Yuxuan</first><last>Li</last></author>
      <author><first>Xinwei</first><last>Guo</last></author>
      <author><first>Jiashi</first><last>Gao</last></author>
      <author><first>Guanhua</first><last>Chen</last><affiliation>Southern University of Science and Technology</affiliation></author>
      <author><first>Xiangyu</first><last>Zhao</last><affiliation>City University of Hong Kong</affiliation></author>
      <author><first>Jiaxin</first><last>Zhang</last><affiliation>Southern University of Science and Technology</affiliation></author>
      <author><first>Quanying</first><last>Liu</last><affiliation>Southern University of Science and Technology</affiliation></author>
      <author><first>Haiyan</first><last>Wu</last><affiliation>University of Macau</affiliation></author>
      <author><first>Xin</first><last>Yao</last></author>
      <author><first>Xuetao</first><last>Wei</last><affiliation>Southern University of Science and Technology</affiliation></author>
      <pages>28844-28858</pages>
      <abstract>Retrieval-Augmented Generation (RAG) has been proven to be an effective approach to address the hallucination problem in large language models (LLMs). In current RAG systems, LLMs typically need to synthesize knowledge provided by two main external sources (user prompts and an external database) to generate a final answer. When the knowledge provided by the user conflicts with that retrieved from the database, a critical question arises: Does the LLM favor one knowledge source over the other when generating the answer? In this paper, we are the first to unveil a new phenomenon, Authority Bias, where the LLMs tend to favor the knowledge provided by the user even when it deviates from the facts; this new phenomenon is rigorously evidenced via our novel and comprehensive characterization of Authority Bias in six widely used LLMs and across diverse task scenarios. We propose a novel dataset specifically designed for detecting Authority Bias, called the Authority Bias Detection Dataset (ABDD), and introduce new, detailed metrics to measure Authority Bias. To mitigate Authority bias, we finally propose the Conflict Detection Enhanced Query (CDEQ) framework. We identify the sentences and atomic information that generate conflicts, perform a credibility assessment on the conflicting paragraphs, and ultimately enhance the query to detect perturbed text, thereby reducing Authority bias. Comparative experiments with widely used mitigation methods demonstrate that CDEQ exhibits both effectiveness and advancement, significantly enhancing the robustness of RAG systems.</abstract>
      <url hash="b9d57dff">2025.acl-long.1400</url>
      <bibkey>li-etal-2025-llms-trust</bibkey>
    </paper>
    <paper id="1401">
      <title>Divide-Then-Aggregate: An Efficient Tool Learning Method via Parallel Tool Invocation</title>
      <author><first>Dongsheng</first><last>Zhu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Weixian</first><last>Shi</last></author>
      <author><first>Zhengliang</first><last>Shi</last></author>
      <author><first>Zhaochun</first><last>Ren</last><affiliation>Leiden University</affiliation></author>
      <author><first>Shuaiqiang</first><last>Wang</last></author>
      <author><first>Lingyong</first><last>Yan</last><affiliation>Baidu Inc.</affiliation></author>
      <author><first>Dawei</first><last>Yin</last><affiliation>Baidu</affiliation></author>
      <pages>28859-28875</pages>
      <abstract>While Large Language Models (LLMs) demonstrate remarkable capabilities, their ability to autonomously execute complex real-world tasks remains limited. Accordingly, tool learning has emerged to enable LLMs to effectively leverage external tools to extend their capabilities. Current tool-learning paradigms like CoT/ReAct employ sequential tool invocation but suffer from constrained perception and inadequate task planning. Alternative approaches using search-based decision trees incur substantial computational overhead. To address these limitations, we propose DTA-Llama (Divide-Then-Aggregate Llama), a novel parallel tool invocation framework featuring: (1) A Directed Acyclic Graph (DAG) structure that transformed from traditional tree-based tool search paths, enabling parallel execution and contributing high-quality training data; (2) A process-thread-inspired inference mechanism that iteratively decomposes tasks into parallel tool-using subtasks while aggregating results for subsequent decisions. Experimental results show that our approach substantially enhances task performance while reducing token consumption and inference time. Llama2-7B, using our method, is comparable to the official parallel function calling method of GPT-3.5. The relevant code, dataset, and model weights are available at https://corn0205.github.io/.</abstract>
      <url hash="194df377">2025.acl-long.1401</url>
      <bibkey>zhu-etal-2025-divide</bibkey>
    </paper>
    <paper id="1402">
      <title>Reviving Cultural Heritage: A Novel Approach for Comprehensive Historical Document Restoration</title>
      <author><first>Yuyi</first><last>Zhang</last></author>
      <author><first>Peirong</first><last>Zhang</last></author>
      <author><first>Zhenhua</first><last>Yang</last></author>
      <author><first>Pengyu</first><last>Yan</last></author>
      <author><first>Yongxin</first><last>Shi</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Pengwei</first><last>Liu</last><affiliation>INTSIG</affiliation></author>
      <author><first>Fengjun</first><last>Guo</last></author>
      <author><first>Lianwen</first><last>Jin</last></author>
      <pages>28876-28892</pages>
      <abstract>Historical documents represent an invaluable cultural heritage, yet have undergone significant degradation over time through tears, water erosion, and oxidation. Existing Historical Document Restoration (HDR) methods primarily focus on single modality or limited-size restoration, failing to meet practical needs. To fill this gap, we present a full-page HDR dataset (FPHDR) and a novel automated HDR solution (AutoHDR). Specifically, FPHDR comprises 1,633 real and 6,543 synthetic images with character-level and line-level locations, as well as character annotations in different damage grades. AutoHDR mimics historians’ restoration workflows through a three-stage approach: OCR-assisted damage localization, vision-language context text prediction, and patch autoregressive appearance restoration. The modular architecture of AutoHDR enables seamless human-machine collaboration, allowing for flexible intervention and optimization at each restoration stage. Experiments demonstrate AutoHDR’s remarkable performance in HDR. When processing severely damaged documents, our system improves OCR accuracy from 46.83% to 84.05%, with further enhancement to 94.25% through human-machine collaboration. We believe this work represents a significant advancement in automated historical document restoration and contributes substantially to cultural heritage preservation. The model and dataset are available at https://github.com/SCUT-DLVCLab/AutoHDR.</abstract>
      <url hash="5ae62521">2025.acl-long.1402</url>
      <bibkey>zhang-etal-2025-reviving</bibkey>
    </paper>
    <paper id="1403">
      <title><fixed-case>P</fixed-case>op<fixed-case>A</fixed-case>lign: Diversifying Contrasting Patterns for a More Comprehensive Alignment</title>
      <author><first>Zekun Moore</first><last>Wang</last><affiliation>Beijing University of Aeronautics and Astronautics</affiliation></author>
      <author><first>Shenzhi</first><last>Wang</last></author>
      <author><first>King</first><last>Zhu</last><affiliation>Guangdong OPPO Mobile Telecommunications Corp.,Ltd.</affiliation></author>
      <author><first>Jiaheng</first><last>Liu</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Ke</first><last>Xu</last><affiliation>Beijing University of Aeronautics and Astronautics</affiliation></author>
      <author><first>Jie</first><last>Fu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Wangchunshu</first><last>Zhou</last><affiliation>Guangdong OPPO Mobile Telecommunications Corp.,Ltd.</affiliation></author>
      <author><first>Wenhao</first><last>Huang</last></author>
      <pages>28893-28921</pages>
      <abstract>Alignment of large language models (LLMs) involves training models on preference-contrastive output pairs to adjust their responses according to human preferences. To obtain such contrastive pairs, traditional methods like RLHF and RLAIF rely on <b>limited</b> contrasting patterns, such as varying model variants or decoding temperatures. This singularity leads to two issues: (1) alignment is not comprehensive; and thereby (2) models are susceptible to harmful response tendencies. To address these issues, we investigate how to construct more comprehensive and diversified contrasting patterns to enhance preference data (RQ1) and verify the impact of the diversification of contrasting patterns on model alignment (RQ2). For RQ1, we propose <b>PopAlign</b>, a framework that integrates diversified contrasting patterns across the prompt, model, and pipeline levels, introducing six contrasting strategies that do not require additional feedback labeling procedures. Regarding RQ2, we conduct thorough experiments demonstrating that PopAlign significantly outperforms existing methods, leading to more comprehensive alignment.</abstract>
      <url hash="631044cc">2025.acl-long.1403</url>
      <bibkey>wang-etal-2025-popalign</bibkey>
    </paper>
    <paper id="1404">
      <title>Robust Utility-Preserving Text Anonymization Based on Large Language Models</title>
      <author><first>Tianyu</first><last>Yang</last><affiliation>Georg-August Universität Göttingen</affiliation></author>
      <author><first>Xiaodan</first><last>Zhu</last><affiliation>Queen’s University</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Institute for Computer Science, Artificial Intelligence and Technology, Mohamed bin Zayed University of Artificial Intelligence and Technische Universität Darmstadt</affiliation></author>
      <pages>28922-28941</pages>
      <abstract>Anonymizing text that contains sensitive information is crucial for a wide range of applications. Existing techniques face the emerging challenges of the re-identification ability of large language models (LLMs), which have shown advanced capability in memorizing detailed information and reasoning over dispersed pieces of patterns to draw conclusions. When defending against LLM-based re-identification, anonymization could jeopardize the utility of the resulting anonymized data in downstream tasks. In general, the interaction between anonymization and data utility requires a deeper understanding within the context of LLMs. In this paper, we propose a framework composed of three key LLM-based components: <tex-math>\textit{a privacy evaluator}</tex-math>, <tex-math>\textit{a utility evaluator}</tex-math> and <tex-math>\textit{an optimization component}</tex-math>, which work collaboratively to perform anonymization. Extensive experiments demonstrate that the proposed model outperforms existing baselines, showing robustness in reducing the risk of re-identification while preserving greater data utility in downstream tasks. We provide detailed studies on these core modules. To consider large-scale and real-time applications, we investigate the distillation of the anonymization capabilities into lightweight models. All of our code and datasets will be made publicly available at <tex-math>\texttt{[Github URL]}</tex-math>.</abstract>
      <url hash="14289804">2025.acl-long.1404</url>
      <bibkey>yang-etal-2025-robust</bibkey>
    </paper>
    <paper id="1405">
      <title><fixed-case>SEAL</fixed-case>: Scaling to Emphasize Attention for Long-Context Retrieval</title>
      <author><first>Changhun</first><last>Lee</last><affiliation>Pohang University of Science and Technology</affiliation></author>
      <author><first>Minsang</first><last>Seok</last><affiliation>Pohang University of Science and Technology</affiliation></author>
      <author><first>Jun-gyu</first><last>Jin</last><affiliation>Pohang University of Science and Technology</affiliation></author>
      <author><first>YoungHyun</first><last>Cho</last><affiliation>Pohang University of Science and Technology</affiliation></author>
      <author><first>Eunhyeok</first><last>Park</last><affiliation>POSTECH</affiliation></author>
      <pages>28942-28955</pages>
      <abstract>While many advanced LLMs are designed to handle long sequence data, we can still observe notable quality degradation even within the sequence limit. In this work, we introduce a novel approach called Scaling to Emphasize Attention for Long-context retrieval (SEAL), which enhances the retrieval performance of large language models (LLMs) over long contexts. We observe that specific attention heads are closely tied to long-context retrieval, showing positive or negative correlation with retrieval scores, and adjusting the strength of these heads boosts the quality of LLMs in long context by a large margin. Built on this insight, we propose a learning-based mechanism that leverages generated data to emphasize these heads. By applying SEAL, we achieve significant improvements in long-context retrieval performance across various tasks and models. Additionally, when combined with existing training-free context extension techniques, SEAL extends the contextual limits of LLMs while maintaining highly reliable outputs.</abstract>
      <url hash="819d13af">2025.acl-long.1405</url>
      <bibkey>lee-etal-2025-seal</bibkey>
    </paper>
    <paper id="1406">
      <title>From Neurons to Semantics: Evaluating Cross-Linguistic Alignment Capabilities of Large Language Models via Neurons Alignment</title>
      <author><first>Chongxuan</first><last>Huang</last></author>
      <author><first>Yongshi</first><last>Ye</last></author>
      <author><first>Biao</first><last>Fu</last></author>
      <author><first>Qifeng</first><last>Su</last></author>
      <author><first>Xiaodong</first><last>Shi</last><affiliation>Xiamen University, Tsinghua University</affiliation></author>
      <pages>28956-28974</pages>
      <abstract>Large language models (LLMs) have demonstrated remarkable multilingual capabilities, however, how to evaluate cross-lingual alignment remains underexplored. Existing alignment benchmarks primarily focus on sentence embeddings, but prior research has shown that neural models tend to induce a non-smooth representation space, which impact of semantic alignment evaluation on low-resource languages. Inspired by neuroscientific findings that similar information activates overlapping neuronal regions, we propose a novel *Neuron State-Based Cross-Lingual Alignment* <tex-math>(\textbf{NeuronXA})</tex-math> to assess the cross-lingual a lignment capabilities of LLMs, which offers a more semantically grounded approach to assess cross-lingual alignment. We evaluate NeuronXA on several prominent multilingual LLMs (LLaMA, Qwen, Mistral, GLM, and OLMo) across two transfer tasks and three multilingual benchmarks. The results demonstrate that with only 100 parallel sentence pairs, NeuronXA achieves a Pearson correlation of 0.9556 with downstream tasks performance and 0.8524 with transferability. These findings demonstrate NeuronXA’s effectiveness in assessing both cross-lingual alignment and transferability, even with a small dataset. This highlights its potential to advance cross-lingual alignment research and to improve the semantic understanding of multilingual LLMs.</abstract>
      <url hash="beaef3ed">2025.acl-long.1406</url>
      <bibkey>huang-etal-2025-neurons</bibkey>
    </paper>
    <paper id="1407">
      <title><tex-math>\mathcal{A}^3</tex-math>: Automatic Alignment Framework for Attributed Text Generation</title>
      <author><first>Yue</first><last>Wang</last><affiliation>Soochow University, China</affiliation></author>
      <author><first>Haoke</first><last>Zhang</last><affiliation>Suzhou University</affiliation></author>
      <author><first>Juntao</first><last>Li</last></author>
      <author><first>Jinxiong</first><last>Chang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>28975-28990</pages>
      <abstract>Attributed text generation aims to enhance the reliability of content generated from large language models by providing citations for each claim, which thereby enables users to easily verify the correctness of the responses.However, the scarcity of high-quality training samples presents a significant challenge in aligning large language models to generate texts with citations, revealing considerable room for improvement in existing attribution systems.Besides, existing approaches of aligning large language models to follow user instructions can lead to an undue emphasis on irrelevant documents, which in turn reduces the quality of responses.To address the above problems, we propose <b>A</b>utomatic <b>A</b>lignment Framework for <b>A</b>ttributed Text Generation (<b>
          <tex-math>\mathcal{A}^3</tex-math></b>), a novel framework designed to automatically generate high-quality attributed query-response pairs for both supervised fine-tuning and preference optimization stages without human annotation.With the help of <b>
          <tex-math>\mathcal{A}^3</tex-math></b>, Mistral-7B can achieve a citation recall of <b>84.4</b> and a precision of <b>87.0</b> precision on ASQA, which notably surpasses GPT-4’s citation recall of <b>73.0</b> and precision of <b>76.5</b>.</abstract>
      <url hash="d07f8d47">2025.acl-long.1407</url>
      <bibkey>wang-etal-2025-a3</bibkey>
    </paper>
    <paper id="1408">
      <title>Towards Better Value Principles for Large Language Model Alignment: A Systematic Evaluation and Enhancement</title>
      <author><first>Bingbing</first><last>Xu</last></author>
      <author><first>Jing</first><last>Yao</last><affiliation>Microsoft</affiliation></author>
      <author><first>Xiaoyuan</first><last>Yi</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Aishan</first><last>Maoliniyazi</last></author>
      <author><first>Xing</first><last>Xie</last><affiliation>Microsoft Research Asia</affiliation></author>
      <author><first>Xiaofeng</first><last>Meng</last><affiliation>Renmin University of China</affiliation></author>
      <pages>28991-29010</pages>
      <abstract>As Large Language Models (LLMs) advance, aligning them with human values is critical for their responsible development. Value principles serve as the foundation for clarifying alignment goals.Multiple sets of value principles have been proposed, such as HHH (helpful, honest, harmless) and instructions for data synthesis in reinforcement learning from AI feedback (RLAIF). However, most of them are heuristically crafted, without consideration of three primary challenges in practical LLM alignment: 1) Comprehensiveness to deal with diverse and even unforeseen scenarios in which LLMs could be applied; 2) Precision to provide LLMs with clear and actionable guidance in specific scenarios; and 3) Compatability to avoid internal contracts between principles.In this paper, we formalize quantitative metrics to evaluate value principles along the three desirable properties. Building on these metrics, we propose the Hierarchical Value Principle framework (HiVaP), which constructs a hierarchical principle set and retrieves principles tailored to each scenario in a cascading way, addressing above challenges.Experimental results validate that the three metrics capture the effectiveness of value principles for LLM alignment, and our HiVaP framework that enhances these metrics leads to superior alignment. Warning: This paper contains several toxic and offensive statements.</abstract>
      <url hash="8517f71c">2025.acl-long.1408</url>
      <bibkey>xu-etal-2025-towards</bibkey>
    </paper>
    <paper id="1409">
      <title>Language Models, Graph Searching, and Supervision Adulteration: When More Supervision is Less and How to Make More More</title>
      <author><first>Arvid</first><last>Frydenlund</last></author>
      <pages>29011-29059</pages>
      <abstract>This work concerns the path-star task, a minimal example of searching over a graph. The graph, <tex-math>G</tex-math>, is star-shaped with <tex-math>D</tex-math> arms radiating from a start node, <tex-math>s</tex-math>. A language model (LM) is given <tex-math>G</tex-math>, <tex-math>s</tex-math>, and a target node, <tex-math>t</tex-math>, which ends one of the arms and is tasked with generating the arm containing <tex-math>t</tex-math>. The minimal nature of this task means only a single choice needs to be made: which of the arms contains?Decoder-only LMs fail to solve this elementary task above <tex-math>1/D</tex-math> chance due to a learned shortcut that absorbs training supervision. We show how this pathology is caused by excess supervision and present a series of solutions demonstrating that the task is solvable via decoder-only LMs. We find that the task’s minimal nature causes its difficulty, as it prevents task decomposition. Our solutions provide insight into the pathology and its implications for LMs trained via next-token prediction.</abstract>
      <url hash="8d7a943d">2025.acl-long.1409</url>
      <bibkey>frydenlund-2025-language</bibkey>
    </paper>
    <paper id="1410">
      <title>Diversity Explains Inference Scaling Laws: Through a Case Study of Minimum <fixed-case>B</fixed-case>ayes Risk Decoding</title>
      <author><first>Hidetaka</first><last>Kamigaito</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Hiroyuki</first><last>Deguchi</last><affiliation>NTT</affiliation></author>
      <author><first>Yusuke</first><last>Sakai</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Katsuhiko</first><last>Hayashi</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <pages>29060-29094</pages>
      <abstract>Inference methods play an important role in eliciting the performance of large language models (LLMs). Currently, LLMs use inference methods utilizing generated multiple samples, which can be derived from Minimum Bayes Risk (MBR) Decoding. Previous studies have conducted empirical analyses to clarify the improvements in generation performance achieved by MBR decoding and have reported various observations. However, the theoretical underpinnings of these findings remain uncertain. To address this, we offer a new theoretical interpretation of MBR decoding from the perspective of bias–diversity decomposition. In this interpretation, the error in the quality estimation of hypotheses by MBR decoding is decomposed into two main factors: bias, which considers the closeness between the utility function and human evaluation, and diversity, which represents the variability in the quality estimation of the utility function. The theoretical analysis reveals the difficulty of simultaneously improving bias and diversity, confirming the validity of enhancing MBR decoding performance by increasing diversity. Furthermore, we reveal that diversity can explain one aspect of inference scaling laws that describe performance improvement by increasing sample size. Moreover, experiments across multiple NLP tasks yielded results consistent with these theoretical characteristics. Our code is available at https://github.com/naist-nlp/mbr-bias-diversity.</abstract>
      <url hash="8528dc90">2025.acl-long.1410</url>
      <bibkey>kamigaito-etal-2025-diversity</bibkey>
    </paper>
    <paper id="1411">
      <title>Performance Gap in Entity Knowledge Extraction Across Modalities in Vision Language Models</title>
      <author><first>Ido</first><last>Cohen</last></author>
      <author><first>Daniela</first><last>Gottesman</last><affiliation>Tel Aviv University</affiliation></author>
      <author><first>Mor</first><last>Geva</last><affiliation>Tel Aviv University and Google Research</affiliation></author>
      <author><first>Raja</first><last>Giryes</last><affiliation>Tel Aviv University</affiliation></author>
      <pages>29095-29108</pages>
      <abstract>Vision-language models (VLMs) excel at extracting and reasoning about information from images. Yet, their capacity to leverage internal knowledge about specific entities remains underexplored. This work investigates the disparity in model performance when answering factual questions about an entity described in text versus depicted in an image. Our results reveal a significant accuracy drop — reaching 18% for some models — when the entity is presented visually instead of textually. To study this gap we present PopVQA, a dataset which allows separating entity recognition and question answering, and use it to benchmark several models. We hypothesize that this decline arises from limitations in how information flows from image tokens to query tokens. Thus, we use mechanistic interpretability tools to reveal that, although image tokens are preprocessed by the vision encoder, meaningful information flow from these tokens occurs only in the much deeper layers. Furthermore, critical image processing happens in the language model’s middle layers, allowing few layers for consecutive reasoning, highlighting a potential inefficiency in how the model utilizes its layers for reasoning. These insights shed light on the internal mechanics of VLMs and offer pathways for enhancing their reasoning capabilities. PopVQA can be found at https://huggingface.co/datasets/idoco/PopVQA.</abstract>
      <url hash="6872538c">2025.acl-long.1411</url>
      <bibkey>cohen-etal-2025-performance</bibkey>
    </paper>
    <paper id="1412">
      <title><fixed-case>SDD</fixed-case>: Self-Degraded Defense against Malicious Fine-tuning</title>
      <author><first>ZiXuan</first><last>Chen</last></author>
      <author><first>Weikai</first><last>Lu</last></author>
      <author><first>Xin</first><last>Lin</last></author>
      <author><first>Ziqian</first><last>Zeng</last><affiliation>South China University of Technology</affiliation></author>
      <pages>29109-29125</pages>
      <abstract>Open-source Large Language Models (LLMs) often employ safety alignment methods to resist harmful instructions. However, recent research shows that maliciously fine-tuning these LLMs on harmful data can easily bypass these safeguards. To counter this, we theoretically uncover why malicious fine-tuning succeeds and identify potential defense strategies. Building on the theoretical analysis, we introduce the Self-Degraded Defense (SDD) framework. SDD encourages LLMs to produce high-quality but irrelevant responses to harmful prompts. When attackers attempt malicious fine-tuning, the general capability of the LLM aligned by SDD will significantly decrease, rendering it incapable of following harmful instructions. Our experimental results confirm SDD’s effectiveness against such attacks.Our code is available at <url>https://github.com/ZeroNLP/SDD</url>.</abstract>
      <url hash="17a18439">2025.acl-long.1412</url>
      <bibkey>chen-etal-2025-sdd</bibkey>
    </paper>
    <paper id="1413">
      <title><fixed-case>C</fixed-case>oach<fixed-case>M</fixed-case>e: Decoding Sport Elements with a Reference-Based Coaching Instruction Generation Model</title>
      <author><first>Wei-Hsin</first><last>Yeh</last></author>
      <author><first>Yu-An</first><last>Su</last></author>
      <author><first>Chih-Ning</first><last>Chen</last></author>
      <author><first>Yi-Hsueh</first><last>Lin</last></author>
      <author><first>Calvin</first><last>Ku</last></author>
      <author><first>Wenhsin</first><last>Chiu</last><affiliation>National Tsinghua University</affiliation></author>
      <author><first>Min-Chun</first><last>Hu</last><affiliation>Department of Computer Science, National Tsing Hua University, National Tsinghua University</affiliation></author>
      <author><first>Lun-Wei</first><last>Ku</last><affiliation>Academia Sinica</affiliation></author>
      <pages>29126-29151</pages>
      <abstract>Motion instruction is a crucial task that helps athletes refine their technique by analyzing movements and providing corrective guidance. Although recent advances in multimodal models have improved motion understanding,generating precise and sport-specific instruction remains challenging due to the highly domain-specific nature of sports and the need for informative guidance. We propose CoachMe, a reference-based model that analyzes the differences between a learner’s motion and a reference under temporal and physical aspects. This approach enables both domain-knowledge learning and the acquisition of a coach-like thinking process that identifies movement errors effectively and provides feedback to explain how to improve. In this paper, weillustrate how CoachMe adapts well to specific sports such as skating and boxing by learning from general movements and then leveraging limited data. Experiments show that CoachMe provides high-quality instructions instead of directions merely in the tone of a coach but without critical information. CoachMe outperforms GPT-4o by 31.6% in G-Eval on figure skating and by 58.3% on boxing. Analysisfurther confirms that it elaborates on errors and their corresponding improvement methods in the generated instructions. You can find CoachMe here: <url>https://motionxperts.github.io/</url></abstract>
      <url hash="2271b1c3">2025.acl-long.1413</url>
      <bibkey>yeh-etal-2025-coachme</bibkey>
    </paper>
    <paper id="1414">
      <title><fixed-case>DRP</fixed-case>runing: Efficient Large Language Model Pruning through Distributionally Robust Optimization</title>
      <author><first>Hexuan</first><last>Deng</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <author><first>Wenxiang</first><last>Jiao</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Xuebo</first><last>Liu</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <author><first>Jing</first><last>Li</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <author><first>Zhaopeng</first><last>Tu</last><affiliation>Tencent AI Lab</affiliation></author>
      <pages>29152-29173</pages>
      <abstract>Large language models (LLMs) deliver impressive results but face challenges from increasing model sizes and computational costs. Structured pruning reduces model size and speeds up inference but often causes uneven degradation across domains, leading to biased performance. To address this, we propose *DRPruning*, a method that dynamically adjusts the data distribution during training to restore balanced performance across heterogeneous and multi-tasking data. Experiments in monolingual and multilingual settings show that DRPruning surpasses similarly sized models in both pruning and continued pretraining over perplexity, downstream tasks, and instruction tuning. Further analysis demonstrates the robustness of DRPruning towards various domains and distribution shifts. Furthermore, DRPruning can determine optimal reference losses and data ratios automatically, suggesting potential for broader applications. Code and scripts are available at https://github.com/hexuandeng/DRPruning.</abstract>
      <url hash="6f1bce16">2025.acl-long.1414</url>
      <bibkey>deng-etal-2025-drpruning</bibkey>
    </paper>
    <paper id="1415">
      <title>How <fixed-case>LLM</fixed-case>s Comprehend Temporal Meaning in Narratives: A Case Study in Cognitive Evaluation of <fixed-case>LLM</fixed-case>s</title>
      <author><first>Karin</first><last>De Langis</last></author>
      <author><first>Jong Inn</first><last>Park</last><affiliation>University of Minnesota - Twin Cities</affiliation></author>
      <author><first>Andreas</first><last>Schramm</last><affiliation>Hamline University</affiliation></author>
      <author><first>Bin</first><last>Hu</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Khanh Chi</first><last>Le</last></author>
      <author><first>Dongyeop</first><last>Kang</last><affiliation>University of Minnesota</affiliation></author>
      <pages>29174-29191</pages>
      <abstract>Large language models (LLMs) exihibit increasingly sophisticated linguistic capabilities, yet the extent to which these behaviors reflect human-like cognition versus advanced pattern recognition remains an open question.In this study, we investigate how LLMs process the temporal meaning of linguistic aspect in narratives that were previously used in human studies. Using an Expert-in-the-Loop probing pipeline, we conduct a series of targeted experiments to assess whether LLMs construct semantic representations and pragmatic inferences in a human-like manner.Our findings show that LLMs over-rely on prototypicality, produce inconsistent aspectual judgments, and struggle with causal reasoning derived from aspect, raising concerns about their ability to fully comprehend narratives.These results suggest that LLMs process aspect fundamentally differently from humans and lack robust narrative understanding.Beyond these empirical findings, we develop a standardized experimental framework for the reliable assessment of LLMs’ cognitive and linguistic capabilities.</abstract>
      <url hash="a60b44c1">2025.acl-long.1415</url>
      <bibkey>de-langis-etal-2025-llms</bibkey>
    </paper>
    <paper id="1416">
      <title>Data Caricatures: On the Representation of <fixed-case>A</fixed-case>frican <fixed-case>A</fixed-case>merican Language in Pretraining Corpora</title>
      <author><first>Nicholas</first><last>Deas</last><affiliation>Columbia University</affiliation></author>
      <author><first>Blake</first><last>Vente</last></author>
      <author><first>Amith</first><last>Ananthram</last><affiliation>Columbia University</affiliation></author>
      <author><first>Jessica A</first><last>Grieser</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>Desmond U.</first><last>Patton</last></author>
      <author><first>Shana</first><last>Kleiner</last></author>
      <author><first>James R. Shepard</first><last>Iii</last></author>
      <author><first>Kathleen</first><last>McKeown</last></author>
      <pages>29192-29217</pages>
      <abstract>With a combination of quantitative experiments, human judgments, and qualitative analyses, we evaluate the quantity and quality of African American Language (AAL) representation in 12 predominantly English, open-source pretraining corpora. We specifically focus on the sources, variation, and naturalness of included AAL texts representing the AAL speaking community. We find that AAL is underrepresented in all evaluated pretraining corpora compared to US demographics, constituting as few as 0.007% and at most 0.18% of documents. We also find that more than 25% of AAL texts in C4 may be perceived as inappropriate for LLMs to generate and to reinforce harmful stereotypes. Finally, we find that most automated filters are more likely to conserve White Mainstream English (WME) texts over AAL in pretraining corpora.</abstract>
      <url hash="87f448bf">2025.acl-long.1416</url>
      <bibkey>deas-etal-2025-data</bibkey>
    </paper>
    <paper id="1417">
      <title>Language Model Probabilities are <tex-math>Not</tex-math> Calibrated in Numeric Contexts</title>
      <author><first>Charles</first><last>Lovering</last><affiliation>Kensho</affiliation></author>
      <author><first>Michael</first><last>Krumdick</last><affiliation>Kensho</affiliation></author>
      <author><first>Viet Dac</first><last>Lai</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Varshini</first><last>Reddy</last><affiliation>Kensho Technologies</affiliation></author>
      <author><first>Seth</first><last>Ebner</last><affiliation>Kensho</affiliation></author>
      <author><first>Nilesh</first><last>Kumar</last></author>
      <author><first>Rik</first><last>Koncel-Kedziorski</last><affiliation>Apple</affiliation></author>
      <author><first>Chris</first><last>Tanner</last><affiliation>Massachusetts Institute of Technology and Kensho</affiliation></author>
      <pages>29218-29257</pages>
      <abstract>Some statements have one well-defined continuation (e.g., “the Eiffel Tower is in [<tex-math>Paris</tex-math>]"), whereas others have a natural distribution over multiple options (e.g., “the weighted coin flip was [<tex-math>Heads/Tails</tex-math>].") We argue that language model (LM) outputs should capture these natural distributions. Our work specifically tests whether LM output probabilities are calibrated to numeric information within their textual contexts. For example, if the context (the prompt) concerns two equally likely options (e.g., heads or tails for a fair coin), the LM output probabilities should also be equal. Likewise, in a context with nonuniformly likely events (e.g., rolling a pair with two dice) an LM should output proportionate probabilities. However, we find that even in simple settings, the best LMs (1) are poorly calibrated and (2) have systematic biases: artifacts like word identity, word order, and word frequency all impact calibration. For example, ‘gpt-4o-mini‘ often picks the first of two options presented in the prompt regardless of the options’ implied likelihoods, whereas ‘Llama-3.1-8B‘ picks the second. Models do not allocate probability mass among valid options in a calibrated manner.</abstract>
      <url hash="926b2e2a">2025.acl-long.1417</url>
      <bibkey>lovering-etal-2025-language</bibkey>
    </paper>
    <paper id="1418">
      <title><fixed-case>MDC</fixed-case>ure: A Scalable Pipeline for Multi-Document Instruction-Following</title>
      <author><first>Gabrielle Kaili-May</first><last>Liu</last><affiliation>Department of Computer Science, Yale University</affiliation></author>
      <author><first>Bowen</first><last>Shi</last><affiliation>Yale University</affiliation></author>
      <author><first>Avi</first><last>Caciularu</last><affiliation>Google</affiliation></author>
      <author><first>Idan</first><last>Szpektor</last><affiliation>Google</affiliation></author>
      <author><first>Arman</first><last>Cohan</last><affiliation>Yale University and Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>29258-29296</pages>
      <abstract>Multi-document (MD) processing is crucial for LLMs to handle real-world tasks such as summarization and question-answering across large sets of documents. While LLMs have improved at processing long inputs, MD contexts still present unique difficulties, including management of inter-document dependencies, redundancy, and incoherent structures. To address this challenge, we introduce MDCure, a scalable and effective instruction data generation framework to enhance the MD capabilities of LLMs without the computational cost of pre-training or reliance on human-annotated data. MDCure generates high-quality synthetic MD instruction data over sets of articles via targeted prompts. We also introduce MDCureRM, a cost-effective, MD-specific reward model to score and filter generated data based on their training utility for MD settings. MDCure is compatible with open- and closed-source models in addition to policy optimization methods such as PPO, enabling even small open- source models to surpass proprietary LLMs as strong generators of high-quality MD instruction data without further data filtering. With MDCure, we fine-tune a wide variety of LLMs up to 70B parameters in size from the FlanT5, Qwen2, and LLAMA3.1 model families. Extensive evaluations on a wide range of MD and long-context benchmarks spanning various tasks and domains show MDCure consistently improves performance over pre-trained baselines and base models by up to 75.1%.</abstract>
      <url hash="878800ba">2025.acl-long.1418</url>
      <bibkey>liu-etal-2025-mdcure</bibkey>
    </paper>
    <paper id="1419">
      <title>Cross-Lingual Auto Evaluation for Assessing Multilingual <fixed-case>LLM</fixed-case>s</title>
      <author><first>Sumanth</first><last>Doddapaneni</last><affiliation>Indian Institute of Technology, Madras</affiliation></author>
      <author><first>Mohammed Safi Ur Rahman</first><last>Khan</last><affiliation>Indian Institute of Technology, Madras, Dhirubhai Ambani Institute Of Information and Communication Technology and Indian Institute of Technology, Madras, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <author><first>Dilip</first><last>Venkatesh</last></author>
      <author><first>Raj</first><last>Dabre</last><affiliation>Department of Computer Science, Indian Institute of Technology, Madras, Indian Institute of Technology, Madras and National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Anoop</first><last>Kunchukuttan</last><affiliation>Microsoft and Indian Institute of Technology, Madras</affiliation></author>
      <author><first>Mitesh M</first><last>Khapra</last><affiliation>Indian Institute of Technology, Madras</affiliation></author>
      <pages>29297-29329</pages>
      <abstract>Evaluating machine-generated text remains a significant challenge in NLP, especially for non-English languages. Current methodologies, including automated metrics, human assessments, and LLM-based evaluations, predominantly focus on English, revealing a significant gap in multilingual evaluation frameworks. We introduce the Cross Lingual Auto Evaluation (CIA) Suite, an extensible framework that includes evaluator LLMs (Hercule) and a novel test set (Recon) specifically designed for multilingual evaluation. Our test set features 500 human-annotated instructions spanning various task capabilities along with human judgment scores across six languages. This would enable benchmarking of general-purpose multilingual LLMs and facilitate meta-evaluation of Evaluator LLMs. The proposed model, Hercule, is a cross-lingual evaluation model that addresses the scarcity of reference answers in the target language by learning to assign scores to responses based on easily available reference answers in English. Our experiments demonstrate that Hercule aligns more closely with human judgments compared to proprietary models, demonstrating the effectiveness of such cross-lingual evaluation in low resource scenarios. Further, it is also effective in zero-shot evaluation on unseen languages. This study is the first comprehensive examination of cross-lingual evaluation using LLMs, presenting a scalable and effective approach for multilingual assessment. All code, datasets, and models will be publicly available to enable further research in this important area.</abstract>
      <url hash="9466661a">2025.acl-long.1419</url>
      <bibkey>doddapaneni-etal-2025-cross</bibkey>
    </paper>
    <paper id="1420">
      <title><fixed-case>D</fixed-case>eep<fixed-case>R</fixed-case>eview: Improving <fixed-case>LLM</fixed-case>-based Paper Review with Human-like Deep Thinking Process</title>
      <author><first>Minjun</first><last>Zhu</last></author>
      <author><first>Yixuan</first><last>Weng</last></author>
      <author><first>Linyi</first><last>Yang</last></author>
      <author><first>Yue</first><last>Zhang</last><affiliation>Westlake University</affiliation></author>
      <pages>29330-29355</pages>
      <abstract>Large Language Models (LLMs) are increasingly utilized in scientific research assessment, particularly in automated paper review. However, existing LLM-based review systems face significant challenges, including limited domain expertise, hallucinated reasoning, and a lack of structured evaluation. To address these limitations, we introduce DeepReview, a multi-stage framework designed to emulate expert reviewers by incorporating structured analysis, literature retrieval, and evidence-based argumentation. Using DeepReview-13K, a curated dataset with structured annotations, we train DeepReviewer-14B, which outperforms CycleReviewer-70B with fewer tokens. In its best mode, DeepReviewer-14B achieves win rates of 88.21% and 80.20% against GPT-o1 and DeepSeek-R1 in evaluations. Our work sets a new benchmark for LLM-based paper review, with all resources publicly available.</abstract>
      <url hash="959cd9e4">2025.acl-long.1420</url>
      <bibkey>zhu-etal-2025-deepreview</bibkey>
    </paper>
    <paper id="1421">
      <title>Bypass Back-propagation: Optimization-based Structural Pruning for Large Language Models via Policy Gradient</title>
      <author><first>Yuan</first><last>Gao</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Zujing</first><last>Liu</last></author>
      <author><first>Weizhong</first><last>Zhang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Bo</first><last>Du</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Gui-Song</first><last>Xia</last></author>
      <pages>29356-29377</pages>
      <abstract>Recent Large-Language Models (LLMs) pruning methods typically operate at the post-training phase without the expensive weight finetuning, however, their pruning criteria often rely on **heuristically hand-crafted metrics**, potentially leading to suboptimal performance. We instead propose a novel **optimization-based structural pruning** that learns the pruning masks in a probabilistic space directly by optimizing the loss of the pruned model. To preserve the efficiency, our method **eliminates the back-propagation** through the LLM *per se* during the optimization, requiring only **the forward pass of the LLM**. We achieve this by learning an underlying Bernoulli distribution to sample binary pruning masks, where we decouple the Bernoulli parameters from the LLM loss, thus facilitating an efficient optimization via *policy gradient estimator* without back-propagation. As a result, our method is able to 1) *support global and heterogeneous pruning* (*i.e.*, our method automatically determines different redundancy for different layers), and 2) *optionally initialize with a metric-based method* (for our Bernoulli distributions). Extensive experiments conducted on LLaMA, LLaMA-2, LLaMA-3, Vicuna, and Mistral models using the C4 and WikiText2 datasets demonstrate the promising performance of our method in efficiency and effectiveness.</abstract>
      <url hash="1882b0a7">2025.acl-long.1421</url>
      <bibkey>gao-etal-2025-bypass</bibkey>
    </paper>
    <paper id="1422">
      <title>Tree-of-Debate: Multi-Persona Debate Trees Elicit Critical Thinking for Scientific Comparative Analysis</title>
      <author><first>Priyanka</first><last>Kargupta</last><affiliation>Department of Computer Science</affiliation></author>
      <author><first>Ishika</first><last>Agarwal</last></author>
      <author><first>Tal</first><last>August</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <pages>29378-29403</pages>
      <abstract>With the exponential growth of research facilitated by modern technology and improved accessibility, scientific discoveries have become increasingly fragmented within and across fields. This makes it challenging to assess the significance, novelty, incremental findings, and equivalent ideas between related works, particularly those from different research communities. Large language models (LLMs) have recently demonstrated strong quantitative and qualitative reasoning abilities, and multi-agent LLM debates have shown promise in handling complex reasoning tasks by exploring diverse perspectives and reasoning paths. Inspired by this, we introduce Tree-of-Debate (ToD), a framework which converts scientific papers into LLM personas that debate their respective novelties. To emphasize structured, critical reasoning rather than focusing solely on outcomes, ToD dynamically constructs a debate tree, enabling fine-grained analysis of independent novelty arguments within scholarly articles. Through experiments on scientific literature across various domains, evaluated by expert researchers, we demonstrate that ToD generates informative arguments, effectively contrasts papers, and supports researchers in their literature review.</abstract>
      <url hash="177cc298">2025.acl-long.1422</url>
      <bibkey>kargupta-etal-2025-tree</bibkey>
    </paper>
    <paper id="1423">
      <title>Hierarchical Memory Organization for <fixed-case>W</fixed-case>ikipedia Generation</title>
      <author><first>Eugene J.</first><last>Yu</last><affiliation>Peking University</affiliation></author>
      <author><first>Dawei</first><last>Zhu</last></author>
      <author><first>Yifan</first><last>Song</last></author>
      <author><first>Xiangyu</first><last>Wong</last></author>
      <author><first>Jiebin</first><last>Zhang</last></author>
      <author><first>Wenxuan</first><last>Shi</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Xiaoguang</first><last>Li</last></author>
      <author><first>Qun</first><last>Liu</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Sujian</first><last>Li</last><affiliation>Peking University</affiliation></author>
      <pages>29404-29427</pages>
      <abstract>Generating Wikipedia articles autonomously is a challenging task requiring the integration of accurate, comprehensive, and well-structured information from diverse sources. This paper introduces the Memory Organization-based Generation (MOG) framework, a novel approach to address these challenges by leveraging a hierarchical memory architecture. MOG extracts fine-grained memory units from web documents, recursively organizes them into a Wikipedia-style hierarchical structure, and uses this structure to guide the generation process. This ensures alignment between memory and the article outline, improving both informativeness and verifiability while minimizing hallucinations. Additionally, a citation module is implemented to enhance traceability by linking every generated sentence to specific memory units. Evaluations on our newly created WikiStart dataset demonstrate that MOG outperforms baseline methods in producing informative and reliable articles, making it particularly robust in real-world scenarios.</abstract>
      <url hash="d359ad27">2025.acl-long.1423</url>
      <bibkey>yu-etal-2025-hierarchical</bibkey>
    </paper>
    <paper id="1424">
      <title>Class Distillation with Mahalanobis Contrast: An Efficient Training Paradigm for Pragmatic Language Understanding Tasks</title>
      <author><first>Chenlu</first><last>Wang</last><affiliation>State University of New York at Stony Brook</affiliation></author>
      <author><first>Weimin</first><last>Lyu</last></author>
      <author><first>Ritwik</first><last>Banerjee</last><affiliation>State University of New York, Stony Brook</affiliation></author>
      <pages>29428-29442</pages>
      <abstract>Detecting deviant language such as sexism, or nuanced language such as metaphors or sarcasm, is crucial for enhancing the safety, clarity, and interpretation of social interactions. While existing classifiers deliver strong results on these tasks, they often come with significant computational cost and high data demands. In this work, we propose <b>Cla</b>ss <b>D</b>istillation (ClaD), a novel training paradigm that targets the core challenge: distilling a small, well-defined target class from a highly diverse and heterogeneous background. ClaD integrates two key innovations: (i) a loss function informed by the structural properties of class distributions, based on Mahalanobis distance, and (ii) an interpretable decision algorithm optimized for class separation. Across three benchmark detection tasks – sexism, metaphor, and sarcasm – ClaD outperforms competitive baselines, and even with smaller language models and orders of magnitude fewer parameters, achieves performance comparable to several large language models. These results demonstrate ClaD as an efficient tool for pragmatic language understanding tasks that require gleaning a small target class from a larger heterogeneous background.</abstract>
      <url hash="8832a448">2025.acl-long.1424</url>
      <bibkey>wang-etal-2025-class</bibkey>
    </paper>
    <paper id="1425">
      <title>Structure-aware Domain Knowledge Injection for Large Language Models</title>
      <author><first>Kai</first><last>Liu</last><affiliation>National University of Singapore and Zhejiang University</affiliation></author>
      <author><first>Ze</first><last>Chen</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Zhihang</first><last>Fu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Wei</first><last>Zhang</last></author>
      <author><first>Rongxin</first><last>Jiang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Fan</first><last>Zhou</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Yaowu</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Yue</first><last>Wu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Jieping</first><last>Ye</last><affiliation>Alibaba Group</affiliation></author>
      <pages>29443-29464</pages>
      <abstract>This paper introduces a pioneering methodology, termed StructTuning, to efficiently transform foundation Large Language Models (LLMs) into domain specialists. It significantly reduces the training corpus needs to a mere 5% while achieving an impressive 100% of traditional knowledge injection performance. Motivated by structured human education, we propose a novel two-stage strategy for knowledge injection and alignment: Structure-aware Continual Pre-Training (SCPT) and Structure-aware Supervised Fine-Tuning (SSFT). In the SCPT phase, we automatically extract the domain knowledge taxonomy and reorganize the training corpora, enabling LLMs to effectively link textual segments to targeted knowledge points within the taxonomy. In the SSFT phase, we explicitly prompt models to elucidate the underlying knowledge structure in their outputs, leveraging the structured domain insight to address practical problems. Our ultimate method was extensively evaluated across model architectures and scales on LongBench and MMedBench datasets, demonstrating superior performance against other knowledge injection methods. We also explored our method’s scalability across different training corpus sizes, laying the foundation to enhance domain-specific LLMs with better data utilization.</abstract>
      <url hash="69159573">2025.acl-long.1425</url>
      <bibkey>liu-etal-2025-structure</bibkey>
    </paper>
    <paper id="1426">
      <title><fixed-case>F</fixed-case>in<fixed-case>MME</fixed-case>: Benchmark Dataset for Financial Multi-Modal Reasoning Evaluation</title>
      <author><first>Junyu</first><last>Luo</last><affiliation>Peking University</affiliation></author>
      <author><first>Zhizhuo</first><last>Kou</last></author>
      <author><first>Liming</first><last>Yang</last></author>
      <author><first>Xiao</first><last>Luo</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Jinsheng</first><last>Huang</last><affiliation>Peking University</affiliation></author>
      <author><first>Zhiping</first><last>Xiao</last><affiliation>University of Washington</affiliation></author>
      <author><first>Jingshu</first><last>Peng</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Chengzhong</first><last>Liu</last></author>
      <author><first>Jiaming</first><last>Ji</last></author>
      <author><first>Xuanzhe</first><last>Liu</last><affiliation>Peking University</affiliation></author>
      <author><first>Sirui</first><last>Han</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Ming</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Yike</first><last>Guo</last><affiliation>Hong Kong University of Science and Technology and Imperial College London</affiliation></author>
      <pages>29465-29489</pages>
      <abstract>Multimodal Large Language Models (MLLMs) have experienced rapid development in recent years. However, in the financial domain, there is a notable lack of effective and specialized multimodal evaluation datasets. To advance the development of MLLMs in the finance domain, we introduce FinMME, encompassing more than 11,000 high-quality financial research samples across 18 financial domains and 6 asset classes, featuring 10 major chart types and 21 subtypes. We ensure data quality through 20 annotators and carefully designed validation mechanisms. Additionally, we develop FinScore, an evaluation system incorporating hallucination penalties and multi-dimensional capability assessment to provide an unbiased evaluation. Extensive experimental results demonstrate that even state-of-the-art models like GPT-4o exhibit unsatisfactory performance on FinMME, highlighting its challenging nature. The benchmark exhibits high robustness with prediction variations under different prompts remaining below 1%, demonstrating superior reliability compared to existing datasets. Our dataset and evaluation protocol are available at https://huggingface.co/datasets/luojunyu/FinMME and https://github.com/luo-junyu/FinMME.</abstract>
      <url hash="573f1c03">2025.acl-long.1426</url>
      <bibkey>luo-etal-2025-finmme</bibkey>
    </paper>
    <paper id="1427">
      <title>Dialectal Coverage And Generalization in <fixed-case>A</fixed-case>rabic Speech Recognition</title>
      <author><first>Amirbek</first><last>Djanibekov</last></author>
      <author><first>Hawau Olamide</first><last>Toyin</last></author>
      <author><first>Raghad</first><last>Alshalan</last></author>
      <author><first>Abdullah</first><last>Alatir</last><affiliation>stc</affiliation></author>
      <author><first>Hanan</first><last>Aldarmaki</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>29490-29502</pages>
      <abstract>Developing robust automatic speech recognition (ASR) systems for Arabic requires effective strategies to manage its diversity. Existing ASR systems mainly cover the modern standard Arabic (MSA) variety and few high-resource dialects, but fall short in coverage and generalization across the multitude of spoken variants. Code-switching with English and French is also common in different regions of the Arab world, which challenges the performance of monolingual Arabic models. In this work, we introduce a suite of ASR models optimized to effectively recognize multiple variants of spoken Arabic, including MSA, various dialects, and code-switching. We provide open-source pre-trained models that cover data from 17 Arabic-speaking countries, and fine-tuned MSA and dialectal ASR models that include at least 11 variants, as well as multi-lingual ASR models covering embedded languages in code-switched utterances. We evaluate ASR performance across these spoken varieties and demonstrate both coverage and performance gains compared to prior models.</abstract>
      <url hash="4610aca4">2025.acl-long.1427</url>
      <bibkey>djanibekov-etal-2025-dialectal</bibkey>
    </paper>
    <paper id="1428">
      <title><fixed-case>E</fixed-case>dit<fixed-case>I</fixed-case>nspector: A Benchmark for Evaluation of Text-Guided Image Edits</title>
      <author><first>Ron</first><last>Yosef</last><affiliation>, Hebrew University of Jerusalem</affiliation></author>
      <author><first>Yonatan</first><last>Bitton</last><affiliation>Google</affiliation></author>
      <author><first>Dani</first><last>Lischinski</last><affiliation>Google and The Hebrew University of Jerusalem, Israel</affiliation></author>
      <author><first>Moran</first><last>Yanuka</last><affiliation>Tel Aviv University and Apple</affiliation></author>
      <pages>29503-29530</pages>
      <abstract>Text-guided image editing, fueled by recent advancements in generative AI, is becoming increasingly widespread. This trend highlights the need for a comprehensive framework to verify text-guided edits and assess their quality. To address this need, we introduce EditInspector, a novel benchmark for evaluation of text-guided image edits, based on human annotations collected using an extensive template for edit verification. We leverage EditInspector to evaluate the performance of state-of-the-art (SoTA) vision and language models in assessing edits across various dimensions, including accuracy, artifact detection, visual quality, seamless integration with the image scene, adherence to common sense, and the ability to describe edit-induced changes. Our findings indicate that current models struggle to evaluate edits comprehensively and frequently hallucinate when describing the changes. To address these challenges, we propose two novel methods that outperform SoTA models in both artifact detection and difference caption generation.</abstract>
      <url hash="6e1a6cc5">2025.acl-long.1428</url>
      <bibkey>yosef-etal-2025-editinspector</bibkey>
    </paper>
    <paper id="1429">
      <title>Reconsidering <fixed-case>LLM</fixed-case> Uncertainty Estimation Methods in the Wild</title>
      <author><first>Yavuz Faruk</first><last>Bakman</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Duygu Nur</first><last>Yaldiz</last></author>
      <author><first>Sungmin</first><last>Kang</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Tuo</first><last>Zhang</last></author>
      <author><first>Baturalp</first><last>Buyukates</last><affiliation>University of Birmingham</affiliation></author>
      <author><first>Salman</first><last>Avestimehr</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Sai Praneeth</first><last>Karimireddy</last><affiliation>University of Southern California</affiliation></author>
      <pages>29531-29556</pages>
      <abstract>Large Language Model (LLM) Uncertainty Estimation (UE) methods have become a crucial tool for detecting hallucinations in recent years. While numerous UE methods have been proposed, most existing studies evaluate them in isolated short-form QA settings using threshold-independent metrics such as AUROC or PRR. However, real-world deployment of UE methods introduces several challenges. In this work, we systematically examine four key aspects of deploying UE methods in practical settings. Specifically, we assess (1) the sensitivity of UE methods to decision threshold selection, (2) their robustness to query transformations such as typos, adversarial prompts, and prior chat history, (3) their applicability to long-form generation, and (4) strategies for handling multiple UE scores for a single query. Our evaluations on 19 UE methods reveal that most of them are highly sensitive to threshold selection when there is a distribution shift in the calibration dataset. While these methods generally exhibit robustness against previous chat history and typos, they are significantly vulnerable to adversarial prompts. Additionally, while existing UE methods can be adapted for long-form generation through various strategies, there remains considerable room for improvement. Lastly, ensembling multiple UE scores at test time provides a notable performance boost, which highlights its potential as a practical improvement strategy. Code is available at: https://github.com/duygunuryldz/uncertainty_in_the_wild.</abstract>
      <url hash="a0d4ea2a">2025.acl-long.1429</url>
      <bibkey>bakman-etal-2025-reconsidering</bibkey>
    </paper>
    <paper id="1430">
      <title>Bregman Conditional Random Fields: Sequence Labeling with Parallelizable Inference Algorithms</title>
      <author><first>Caio</first><last>Corro</last><affiliation>Sorbonne Université</affiliation></author>
      <author><first>Mathieu</first><last>Lacroix</last><affiliation>Université Paris Nord (Paris XIII)</affiliation></author>
      <author><first>Joseph Le</first><last>Roux</last><affiliation>Université Paris 13</affiliation></author>
      <pages>29557-29574</pages>
      <abstract>We propose a novel discriminative model for sequence labeling called Bregman conditional random fields (BCRF).Contrary to standard linear-chain conditional random fields,BCRF allows fast parallelizable inference algorithms based on iterative Bregman projections.We show how such models can be learned using Fenchel-Young losses, including extension for learning from partial labels.Experimentally, our approach delivers comparable results to CRF while being faster, and achieves better results in highly constrained settings compared to mean field, another parallelizable alternative.</abstract>
      <url hash="427ef7c8">2025.acl-long.1430</url>
      <bibkey>corro-etal-2025-bregman</bibkey>
    </paper>
    <paper id="1431">
      <title><fixed-case>SEE</fixed-case>: Strategic Exploration and Exploitation for Cohesive In-Context Prompt Optimization</title>
      <author><first>Wendi</first><last>Cui</last><affiliation>Intuit</affiliation></author>
      <author><first>Jiaxin</first><last>Zhang</last><affiliation>Intuit AI Research</affiliation></author>
      <author><first>Zhuohang</first><last>Li</last><affiliation>Vanderbilt University</affiliation></author>
      <author><first>Hao</first><last>Sun</last></author>
      <author><first>Damien</first><last>Lopez</last><affiliation>Austin Peay State University</affiliation></author>
      <author><first>Kamalika</first><last>Das</last><affiliation>Intuit</affiliation></author>
      <author><first>Bradley A.</first><last>Malin</last><affiliation>Vanderbilt University</affiliation></author>
      <author><first>Sricharan</first><last>Kumar</last></author>
      <pages>29575-29627</pages>
      <abstract>Designing optimal prompts for Large Language Models (LLMs) is a complex and resource-intensive task, often requiring substantial human expertise. Existing approaches typically separate the optimization of prompt instructions and in-context learning examples, leading to incohesive, suboptimal results. To overcome this limitation, we propose a novel Cohesive In-Context Prompt Optimization framework that refines both prompt instructions and examples. In our formulation, coherence refers to the degree to which instructions and examples work synergistically to improve task performance—emerging as a byproduct of performance-driven optimization. However, formulating such an optimization in the discrete and high-dimensional space of natural language poses significant challenges in both convergence and computational efficiency. To address these issues, we introduce SEE, a scalable and efficient prompt optimization framework that adopts metaheuristic optimization principles and strategically balances exploration and exploitation to enhance optimization performance and achieve efficient convergence. SEE features a quad-phased design that alternates between global traversal (exploration) and local optimization (exploitation) and adaptively chooses LLM operators during the optimization process. We have conducted a comprehensive evaluation across 35 benchmark tasks, and SEE significantly outperforms state-of-the-art baseline methods by a large margin, achieving an average performance gain of **13.94** while reducing computational costs by **58.67%**.</abstract>
      <url hash="df51ea95">2025.acl-long.1431</url>
      <bibkey>cui-etal-2025-see</bibkey>
    </paper>
    <paper id="1432">
      <title>Programming by Example meets Historical Linguistics: A Large Language Model Based Approach to Sound Law Induction</title>
      <author><first>Atharva</first><last>Naik</last></author>
      <author><first>Darsh</first><last>Agrawal</last></author>
      <author><first>Hong</first><last>Sng</last></author>
      <author><first>Clayton</first><last>Marr</last></author>
      <author><first>Kexun</first><last>Zhang</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Nathaniel Romney</first><last>Robinson</last><affiliation>Department of Computer Science, Whiting School of Engineering</affiliation></author>
      <author><first>Kalvin</first><last>Chang</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Rebecca</first><last>Byrnes</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Aravind</first><last>Mysore</last></author>
      <author><first>Carolyn</first><last>Rose</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>David R.</first><last>Mortensen</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>29628-29647</pages>
      <abstract>Historical linguists have long written “programs” that convert reconstructed words in an ancestor language into their attested descendants via ordered string rewrite functions (called sound laws) However, writing these programs is time-consuming, motivating the development of automated Sound Law Induction (SLI) which we formulate as Programming by Examples (PBE) with Large Language Models (LLMs) in this paper. While LLMs have been effective for code generation, recent work has shown that PBE is challenging but improvable by fine-tuning, especially with training data drawn from the same distribution as evaluation data. In this paper, we create a conceptual framework of what constitutes a “similar distribution” for SLI and propose four kinds of synthetic data generation methods with varying amounts of inductive bias to investigate what leads to the best performance. Based on the results, we create a SOTA open-source model for SLI as PBE (+6% pass rate with a third of the parameters of the second-best LLM) and also highlight exciting future directions for PBE research.</abstract>
      <url hash="748d0f22">2025.acl-long.1432</url>
      <bibkey>naik-etal-2025-programming</bibkey>
    </paper>
    <paper id="1433">
      <title>Synergizing Unsupervised Episode Detection with <fixed-case>LLM</fixed-case>s for Large-Scale News Events</title>
      <author><first>Priyanka</first><last>Kargupta</last><affiliation>Department of Computer Science</affiliation></author>
      <author><first>Yunyi</first><last>Zhang</last><affiliation>Amazon</affiliation></author>
      <author><first>Yizhu</first><last>Jiao</last><affiliation>UIUC</affiliation></author>
      <author><first>Siru</first><last>Ouyang</last><affiliation>University of Illinois Urbana-Champaign Champaign</affiliation></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <pages>29648-29663</pages>
      <abstract>State-of-the-art automatic event detection struggles with interpretability and adaptability to evolving large-scale key events—unlike episodic structures, which excel in these areas. Often overlooked, episodes represent cohesive clusters of core entities performing actions at a specific time and location; a partially ordered sequence of episodes can represent a key event. This paper introduces a novel task, **episode detection**, which identifies episodes within a news corpus of key event articles. Detecting episodes poses unique challenges, as they lack explicit temporal or locational markers and cannot be merged using semantic similarity alone. While large language models (LLMs) can aid with these reasoning difficulties, they suffer with long contexts typical of news corpora. To address these challenges, we introduce **EpiMine**, an unsupervised framework that identifies a key event’s candidate episodes by leveraging natural episodic partitions in articles, estimated through shifts in discriminative term combinations. These candidate episodes are more cohesive and representative of true episodes, synergizing with LLMs to better interpret and refine them into final episodes. We apply EpiMine to our three diverse, real-world event datasets annotated at the episode level, where it achieves a 59.2% average gain across all metrics compared to baselines.</abstract>
      <url hash="3285fd38">2025.acl-long.1433</url>
      <bibkey>kargupta-etal-2025-synergizing</bibkey>
    </paper>
    <paper id="1434">
      <title>Beyond True or False: Retrieval-Augmented Hierarchical Analysis of Nuanced Claims</title>
      <author><first>Priyanka</first><last>Kargupta</last><affiliation>Department of Computer Science</affiliation></author>
      <author><first>Runchu</first><last>Tian</last></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <pages>29664-29679</pages>
      <abstract>Claims made by individuals or entities are oftentimes nuanced and cannot be clearly labeled as entirely “true” or “false”—as is frequently the case with scientific and political claims. However, a claim (e.g., “vaccine A is better than vaccine B”) can be dissected into its integral aspects and sub-aspects (e.g., efficacy, safety, distribution), which are individually easier to validate. This enables a more comprehensive, structured response that provides a well-rounded perspective on a given problem while also allowing the reader to prioritize specific angles of interest within the claim (e.g., safety towards children). Thus, we propose ClaimSpect, a retrieval-augmented generation-based framework for automatically constructing a hierarchy of aspects typically considered when addressing a claim and enriching them with corpus-specific perspectives. This structure hierarchically partitions an input corpus to retrieve relevant segments, which assist in discovering new sub-aspects. Moreover, these segments enable the discovery of varying perspectives towards an aspect of the claim (e.g., support, neutral, or oppose) and their respective prevalence (e.g., “how many biomedical papers believe vaccine A is more transportable than B?”). We apply ClaimSpect to a wide variety of real-world scientific and political claims featured in our constructed dataset, showcasing its robustness and accuracy in deconstructing a nuanced claim and representing perspectives within a corpus. Through real-world case studies and human evaluation, we validate its effectiveness over multiple baselines.</abstract>
      <url hash="85a3a594">2025.acl-long.1434</url>
      <bibkey>kargupta-etal-2025-beyond</bibkey>
    </paper>
    <paper id="1435">
      <title>The Task Shield: Enforcing Task Alignment to Defend Against Indirect Prompt Injection in <fixed-case>LLM</fixed-case> Agents</title>
      <author><first>Feiran</first><last>Jia</last></author>
      <author><first>Tong</first><last>Wu</last></author>
      <author><first>Xin</first><last>Qin</last><affiliation>California State University, Long Beach</affiliation></author>
      <author><first>Anna</first><last>Squicciarini</last><affiliation>Pennsylvania State University</affiliation></author>
      <pages>29680-29697</pages>
      <abstract>Large Language Model (LLM) agents are increasingly being deployed as conversational assistants capable of performing complex real-world tasks through tool integration. This enhanced ability to interact with external systems and process various data sources, while powerful, introduces significant security vulnerabilities. In particular, indirect prompt injection attacks pose a critical threat, where malicious instructions embedded within external data sources can manipulate agents to deviate from user intentions. While existing defenses show promise, they struggle to maintain robust security while preserving task functionality. We propose a novel and orthogonal perspective that reframes agent security from preventing harmful actions to ensuring task alignment, requiring every agent action to serve user objectives. Based on this insight, we develop Task Shield, a test-time defense mechanism that systematically verifies whether each instruction and tool call contributes to user-specified goals. Through experiments on the AgentDojo benchmark, we demonstrate that Task Shield reduces attack success rates (2.07%) while maintaining high task utility (69.79%) on GPT-4o, significantly outperforming existing defenses in various real-world scenarios.</abstract>
      <url hash="940bc160">2025.acl-long.1435</url>
      <bibkey>jia-etal-2025-task</bibkey>
    </paper>
    <paper id="1436">
      <title>Sandcastles in the Storm: Revisiting the (Im)possibility of Strong Watermarking</title>
      <author><first>Fabrice Y</first><last>Harel-Canada</last></author>
      <author><first>Boran</first><last>Erol</last></author>
      <author><first>Connor</first><last>Choi</last></author>
      <author><first>Jason</first><last>Liu</last></author>
      <author><first>Gary Jiarui</first><last>Song</last></author>
      <author><first>Nanyun</first><last>Peng</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Amit</first><last>Sahai</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>29698-29735</pages>
      <abstract>Watermarking AI-generated text is critical for combating misuse. Yet recent theoretical work argues that any watermark can be erased via random walk attacks that perturb text while preserving quality. However, such attacks rely on two key assumptions: (1) rapid mixing (watermarks dissolve quickly under perturbations) and (2) reliable quality preservation (automated quality oracles perfectly guide edits). Through large-scale experiments and human-validated assessments, we find mixing is slow: 100% of perturbed texts retain traces of their origin after hundreds of edits, defying rapid mixing. Oracles falter, as state-of-the-art quality detectors misjudge edits (77% accuracy), compounding errors during attacks. Ultimately, attacks underperform: automated walks remove watermarks just 26% of the time – dropping to 10% under human quality review. These findings challenge the inevitability of watermark removal. Instead, practical barriers – slow mixing and imperfect quality control – reveal watermarking to be far more robust than theoretical models suggest. The gap between idealized attacks and real-world feasibility underscores the need for stronger watermarking methods and more realistic attack models.</abstract>
      <url hash="a073d580">2025.acl-long.1436</url>
      <bibkey>harel-canada-etal-2025-sandcastles</bibkey>
    </paper>
    <paper id="1437">
      <title>Time-<fixed-case>MQA</fixed-case>: Time Series Multi-Task Question Answering with Context Enhancement</title>
      <author><first>Yaxuan</first><last>Kong</last></author>
      <author><first>Yiyuan</first><last>Yang</last></author>
      <author><first>Yoontae</first><last>Hwang</last></author>
      <author><first>Wenjie</first><last>Du</last><affiliation>PyPOTS Research</affiliation></author>
      <author><first>Stefan</first><last>Zohren</last><affiliation>University of Oxford</affiliation></author>
      <author><first>Zhangyang</first><last>Wang</last><affiliation>University of Texas at Austin</affiliation></author>
      <author><first>Ming</first><last>Jin</last><affiliation>Griffith University</affiliation></author>
      <author><first>Qingsong</first><last>Wen</last><affiliation>Squirrel Ai Learning</affiliation></author>
      <pages>29736-29753</pages>
      <abstract>Time series data are foundational in finance, healthcare, and energy domains. However, most existing methods and datasets remain focused on a narrow spectrum of tasks, such as forecasting or anomaly detection. To bridge this gap, we introduce Time Series Multi-Task Question Answering (Time-MQA), a unified framework that enables natural language queries across multiple time series tasks - numerical analytical tasks and open-ended question answering with reasoning. Central to Time-MQA is the TSQA dataset, a large-scale dataset containing ~200k question-answer pairs derived from diverse time series spanning environment, traffic, etc. This comprehensive resource covers various time series lengths and promotes robust model development. We further demonstrate how continually pre-training large language models (Mistral 7B, Llama-3 8B, and Qwen-2.5 7B) on the TSQA dataset enhanced time series reasoning capabilities, moving beyond mere numeric tasks and enabling more advanced and intuitive interactions with temporal data. The complete TSQA dataset, models, user study questionnaires for evaluation, and other related materials have been open-sourced here.</abstract>
      <url hash="f00fc464">2025.acl-long.1437</url>
      <bibkey>kong-etal-2025-time</bibkey>
    </paper>
    <paper id="1438">
      <title>From Perceptions to Decisions: Wildfire Evacuation Decision Prediction with Behavioral Theory-informed <fixed-case>LLM</fixed-case>s</title>
      <author><first>Ruxiao</first><last>Chen</last></author>
      <author><first>Chenguang</first><last>Wang</last></author>
      <author><first>Yuran</first><last>Sun</last></author>
      <author><first>Xilei</first><last>Zhao</last><affiliation>University of Florida</affiliation></author>
      <author><first>Susu</first><last>Xu</last><affiliation>Johns Hopkins University</affiliation></author>
      <pages>29754-29778</pages>
      <abstract>Evacuation decision prediction is critical for efficient and effective wildfire response by helping emergency management anticipate traffic congestion and bottlenecks, allocate resources, and minimize negative impacts. Traditional statistical methods for evacuation decision prediction fail to capture the complex and diverse behavioral logic of different individuals. In this work, for the first time, we introduce *FLARE*, short for facilitating LLM for advanced reasoning on wildfire evacuation decision prediction, a Large Language Model (LLM)-based framework that integrates behavioral theories and models to streamline the Chain-of-Thought (CoT) reasoning and subsequently integrate with memory-based Reinforcement Learning (RL) module to provide accurate evacuation decision prediction and understanding. Our proposed method addresses the limitations of using existing LLMs for evacuation behavioral predictions, such as limited survey data, mismatching with behavioral theory, conflicting individual preferences, implicit and complex mental states, and intractable mental state-behavior mapping. Experiments on three post-wildfire survey datasets show an average of 20.47% performance improvement over traditional theory-informed behavioral models, with strong cross-event generalizability. Our complete code is publicly available at https://github.com/SusuXu-s-Lab/FLARE</abstract>
      <url hash="5e0b7d09">2025.acl-long.1438</url>
      <bibkey>chen-etal-2025-perceptions</bibkey>
    </paper>
    <paper id="1439">
      <title><fixed-case>GETR</fixed-case>eason: Enhancing Image Context Extraction through Hierarchical Multi-Agent Reasoning</title>
      <author><first>Shikhhar</first><last>Siingh</last></author>
      <author><first>Abhinav</first><last>Rawat</last></author>
      <author><first>Chitta</first><last>Baral</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Vivek</first><last>Gupta</last><affiliation>Arizona State University</affiliation></author>
      <pages>29779-29800</pages>
      <abstract>Publicly significant images from events carry valuable contextual information with applications in domains such as journalism and education. However, existing methodologies often struggle to accurately extract this contextual relevance from images. To address this challenge, we introduce GETREASON (Geospatial Event Temporal Reasoning), a framework designed to go beyond surfacelevel image descriptions and infer deeper contextual meaning. We hypothesize that extracting global event, temporal, and geospatial information from an image enables a more accurate understanding of its contextual significance. We also introduce a new metric GREAT (Geospatial, Reasoning and Event Accuracy with Temporal alignment) for a reasoning capturing evaluation. Our layered multi-agentic approach, evaluated using a reasoning-weighted metric, demonstrates that meaningful information can be inferred from images, allowing them to be effectively linked to their corresponding events and broader contextual background.</abstract>
      <url hash="8fedf169">2025.acl-long.1439</url>
      <bibkey>siingh-etal-2025-getreason</bibkey>
    </paper>
    <paper id="1440">
      <title>Hanging in the Balance: Pivotal Moments in Crisis Counseling Conversations</title>
      <author><first>Vivian</first><last>Nguyen</last></author>
      <author><first>Lillian</first><last>Lee</last><affiliation>Cornell University</affiliation></author>
      <author><first>Cristian</first><last>Danescu-Niculescu-Mizil</last><affiliation>Cornell University</affiliation></author>
      <pages>29801-29817</pages>
      <abstract>During a conversation, there can come certain moments where its outcome hangs in the balance. In these pivotal moments, how one responds can put the conversation on substantially different trajectories leading to significantly different outcomes. Systems that can detect when such moments arise could assist conversationalists in domains with highly consequential outcomes, such as mental health crisis counseling.In this work, we introduce an unsupervised computational method for detecting such pivotal moments as they happen. The intuition is that a moment is pivotal if our expectation of the conversation’s outcome varies widely depending on what might be said next. By applying our method to crisis counseling conversations, we first validate it by showing that it aligns with human perception—counselors take significantly longer to respond during moments detected by our method—and with the eventual conversational trajectory—which is more likely to change course at these times. We then use our framework to explore the relation between the counselor’s response during pivotal moments and the eventual outcome of the session.</abstract>
      <url hash="3c31c49a">2025.acl-long.1440</url>
      <bibkey>nguyen-etal-2025-hanging</bibkey>
    </paper>
    <paper id="1441">
      <title>Unveiling the Potential of <fixed-case>BERT</fixed-case>-family: A New Recipe for Building Scalable, General and Competitive Large Language Models</title>
      <author><first>Yisheng</first><last>Xiao</last></author>
      <author><first>Juntao</first><last>Li</last></author>
      <author><first>Wenpeng</first><last>Hu</last><affiliation>Academy of Military Science</affiliation></author>
      <author><first>Zhunchen</first><last>Luo</last></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>29818-29833</pages>
      <abstract>BERT-family have been increasingly explored for adaptation to scenarios beyond language understanding tasks, with more recent efforts focused on enabling them to become good instruction followers. These explorations have endowed BERT-family with new roles and human expectations, showcasing their potential on par with current state-of-the-art (SOTA) large language models (LLMs). However, several certain shortcomings in previous BERT-family, such as the relatively sub-optimal training corpora, learning procedure, and model architecture, all impede the further advancement of these models for serving as general and competitive LLMs. Therefore, we aim to address these deficiencies in this paper. Our study not only introduces a more suitable pre-training task that helps BERT-family excel in wider applications to realize generality but also explores the integration of cutting-edge technologies into our model to further enhance their capabilities. Our final models, termed **Bi**directional **G**eneral **L**anguage **M**odels (**BiGLM**), exhibit performance levels comparable to current SOTA LLMs across a spectrum of tasks. Moreover, we conduct detailed analyses to study the effects of scaling and training corpora for BiGLM. To the best of our knowledge, our work represents the early attempt to offer a recipe for building novel types of scalable, general, and competitive LLMs that diverge from current autoregressive modeling methodology. Our codes and models are available on Github.</abstract>
      <url hash="57aa357a">2025.acl-long.1441</url>
      <bibkey>xiao-etal-2025-unveiling</bibkey>
    </paper>
    <paper id="1442">
      <title><fixed-case>T</fixed-case>axo<fixed-case>A</fixed-case>dapt: Aligning <fixed-case>LLM</fixed-case>-Based Multidimensional Taxonomy Construction to Evolving Research Corpora</title>
      <author><first>Priyanka</first><last>Kargupta</last><affiliation>Department of Computer Science</affiliation></author>
      <author><first>Nan</first><last>Zhang</last></author>
      <author><first>Yunyi</first><last>Zhang</last><affiliation>Amazon</affiliation></author>
      <author><first>Rui</first><last>Zhang</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Prasenjit</first><last>Mitra</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <pages>29834-29850</pages>
      <abstract>The rapid evolution of scientific fields introduces challenges in organizing and retrieving scientific literature. While expert-curated taxonomies have traditionally addressed this need, the process is time-consuming and expensive. Furthermore, recent automatic taxonomy construction methods either (1) over-rely on a specific corpus, sacrificing generalizability, or (2) depend heavily on the general knowledge of large language models (LLMs) contained within their pre-training datasets, often overlooking the dynamic nature of evolving scientific domains. Additionally, these approaches fail to account for the multi-faceted nature of scientific literature, where a single research paper may contribute to multiple dimensions (e.g., methodology, new tasks, evaluation metrics, benchmarks). To address these gaps, we propose TaxoAdapt, a framework that dynamically adapts an LLM-generated taxonomy to a given corpus across multiple dimensions. TaxoAdapt performs iterative hierarchical classification, expanding both the taxonomy width and depth based on corpus’ topical distribution. We demonstrate its state-of-the-art performance across a diverse set of computer science conferences over the years to showcase its ability to structure and capture the evolution of scientific fields. As a multidimensional method, TaxoAdapt generates taxonomies that are 26.51% more granularity-preserving and 50.41% more coherent than the most competitive baselines judged by LLMs.</abstract>
      <url hash="e4cfdcc2">2025.acl-long.1442</url>
      <bibkey>kargupta-etal-2025-taxoadapt</bibkey>
    </paper>
    <paper id="1443">
      <title>An Empirical Study of Iterative Refinements for Non-autoregressive Translation</title>
      <author><first>Yisheng</first><last>Xiao</last></author>
      <author><first>Pei</first><last>Guo</last></author>
      <author><first>Zechen</first><last>Sun</last></author>
      <author><first>Juntao</first><last>Li</last></author>
      <author><first>Kai</first><last>Song</last><affiliation>Tiktok</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>29851-29865</pages>
      <abstract>Iterative non-autoregressive (NAR) models share a spirit of mixed autoregressive (AR) and fully NAR models, seeking a balance between generation quality and inference efficiency. These models have recently demonstrated impressive performance in varied generation tasks, surpassing the autoregressive Transformer. However, they also face several challenges that impede further development. In this work, we target building more efficient and competitive iterative NAR models. Firstly, we produce two simple metrics to identify the potential problems existing in current refinement processes, and look back on the various iterative NAR models to find the key factors for realizing our purpose. Subsequently, based on the analyses of the limitations of previous inference algorithms, we propose a simple yet effective strategy to conduct efficient refinements without performance declines. Experiments on five widely used datasets show that our final models set the new state-of-the-art performance compared to all previous NAR models, even with fewer decoding steps, and outperform AR Transformer by around one BLEU on average. Our codes and models are available on Github.</abstract>
      <url hash="55ff3ed9">2025.acl-long.1443</url>
      <bibkey>xiao-etal-2025-empirical</bibkey>
    </paper>
    <paper id="1444">
      <title>Retrofitting Large Language Models with Dynamic Tokenization</title>
      <author><first>Darius</first><last>Feher</last><affiliation>Google</affiliation></author>
      <author><first>Ivan</first><last>Vulić</last><affiliation>Google DeepMind and University of Cambridge</affiliation></author>
      <author><first>Benjamin</first><last>Minixhofer</last><affiliation>University of Cambridge</affiliation></author>
      <pages>29866-29883</pages>
      <abstract>Current language models (LMs) use a fixed, static subword tokenizer. This default choice typically results in degraded efficiency and language capabilities, especially in languages other than English. To address this issue, we challenge the static design and propose retrofitting LMs with dynamic tokenization: a way to dynamically decide on token boundaries based on the input text via a subword-merging algorithm inspired by byte-pair encoding. We merge frequent subword sequences in a batch, then apply a pre-trained embedding-prediction hypernetwork to compute the token embeddings on-the-fly. For encoder-style models (e.g., XLM-R), this on average reduces token sequence lengths by &gt;20% across 14 languages while degrading performance by less than 2%. The same method applied to pre-filling and scoring in decoder-style models (e.g., Mistral-7B) results in minimal performance degradation at up to 17% reduction in sequence length. Overall, we find that dynamic tokenization can mitigate the limitations of static tokenization by substantially improving inference speed and promoting fairness across languages, enabling more equitable and adaptable LMs.</abstract>
      <url hash="72329575">2025.acl-long.1444</url>
      <bibkey>feher-etal-2025-retrofitting</bibkey>
    </paper>
    <paper id="1445">
      <title>Principled Content Selection to Generate Diverse and Personalized Multi-Document Summaries</title>
      <author><first>Vishakh</first><last>Padmakumar</last><affiliation>New York University</affiliation></author>
      <author><first>Zichao</first><last>Wang</last><affiliation>Adobe Research</affiliation></author>
      <author><first>David</first><last>Arbour</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Jennifer</first><last>Healey</last><affiliation>Adobe Systems</affiliation></author>
      <pages>29884-29899</pages>
      <abstract>While large language models (LLMs) are increasingly capable of handling longer contexts, recent work has demonstrated that they exhibit the _”lost in the middle”_ phenomenon (Liu et al., 2024) of unevenly attending to different parts of the provided context. This hinders their ability to cover diverse source material in multi-document summarization, as noted in the DiverseSumm benchmark (Huang et al., 2024). In this work, we contend that principled content selection is a simple way to increase source coverage on this task. As opposed to prompting an LLM to perform the summarization in a single step, we explicitly divide the task into three steps—(1) reducing document collections to atomic key points, (2) using determinantal point processes (DPP) to perform select key points that prioritize diverse content, and (3) rewriting to the final summary. By combining prompting steps, for extraction and rewriting, with principled techniques, for content selection, we consistently improve source coverage on the DiverseSumm benchmark across various LLMs. Finally, we also show that by incorporating relevance to a provided user intent into the DPP kernel, we can generate _personalized_ summaries that cover _relevant_ source information while retaining coverage.</abstract>
      <url hash="97e60210">2025.acl-long.1445</url>
      <bibkey>padmakumar-etal-2025-principled</bibkey>
    </paper>
    <paper id="1446">
      <title>Bilingual Zero-Shot Stance Detection</title>
      <author><first>Chenye</first><last>Zhao</last></author>
      <author><first>Cornelia</first><last>Caragea</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <pages>29900-29919</pages>
      <abstract>Zero-shot stance detection (ZSSD) aims to determine whether the author of a text is in support, against, or neutral toward a target that is unseen during training. In this paper, we investigate ZSSD within a bilingual framework and compare it with cross-lingual and monolingual scenarios, in settings that have not previously been explored. Our study focuses on both noun-phrase and claim targets within in-domain and out-of-domain bilingual ZSSD scenarios. To support this research, we assemble Bi-STANCE, a comprehensive bilingual ZSSD dataset consisting of over 100,000 annotated text-target pairs in both Chinese and English, sourced from existing datasets. Additionally, we examine a more challenging aspect of bilingual ZSSD by focusing on claim targets with a low occurrence of shared words with their corresponding texts. As part of Bi-STANCE, we created an extended dataset that emphasizes this challenging scenario. To the best of our knowledge, we are the first to explore this difficult ZSSD setting. We investigate these tasks using state-of-the-art pre-trained language models (PLMs) and large language models (LLMs). We release our dataset and code at https://github.com/chenyez/BiSTANCE.</abstract>
      <url hash="9f9a8bc6">2025.acl-long.1446</url>
      <bibkey>zhao-caragea-2025-bilingual</bibkey>
    </paper>
    <paper id="1447">
      <title><fixed-case>G</fixed-case>ramma<fixed-case>MT</fixed-case>: Improving Machine Translation with Grammar-Informed In-Context Learning</title>
      <author><first>Rita</first><last>Ramos</last><affiliation>Instituto Superior Técnico</affiliation></author>
      <author><first>Everlyn Asiko</first><last>Chimoto</last></author>
      <author><first>Maartje</first><last>Ter Hoeve</last><affiliation>Apple</affiliation></author>
      <author><first>Natalie</first><last>Schluter</last><affiliation>Technical University of Denmark, Apple and IT University</affiliation></author>
      <pages>29920-29940</pages>
      <abstract>We introduce GrammaMT, a grammatically-aware prompting approach for machine translation that uses Interlinear Glossed Text (IGT), a common form of linguistic description providing morphological and lexical annotations for source sentences. GrammaMT proposes three prompting strategies: gloss-shot, chain-gloss and model-gloss. All are training-free, requiring only a few examples that involve minimal effort to collect, and making them well-suited for low-resource setups. Experiments show that GrammaMT enhances translation performance on open-source instruction-tuned LLMs for various low- to high-resource languages across three benchmarks: (1) the largest IGT corpus, (2) the challenging 2023 SIGMORPHON Shared Task data over endangered languages, and (3) even in an out-of-domain setting with FLORES. Moreover, ablation studies reveal that leveraging gloss resources could substantially boost MT performance (by over 17 BLEU points) if LLMs accurately generate or access input sentence glosses.</abstract>
      <url hash="71606739">2025.acl-long.1447</url>
      <bibkey>ramos-etal-2025-grammamt</bibkey>
    </paper>
    <paper id="1448">
      <title>Theorem Prover as a Judge for Synthetic Data Generation</title>
      <author><first>Joshua Ong Jun</first><last>Leang</last></author>
      <author><first>Giwon</first><last>Hong</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Wenda</first><last>Li</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Shay B</first><last>Cohen</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>29941-29977</pages>
      <abstract>The demand for synthetic data in mathematical reasoning has increased due to its potential to enhance the mathematical capabilities of large language models (LLMs). However, ensuring the validity of intermediate reasoning steps remains a significant challenge, affecting data quality. While formal verification via theorem provers effectively validates LLM reasoning, the autoformalisation of mathematical proofs remains error-prone. In response, we introduce *iterative autoformalisation*, an approach that iteratively refines theorem prover formalisation to mitigate errors, thereby increasing the execution rate on the Lean prover from 60% to 87%. Building upon that, we introduce *Theorem Prover as a Judge (TP-as-a-Judge)*, a method that employs theorem prover formalisation to rigorously assess LLM intermediate reasoning, effectively integrating autoformalisation with synthetic data generation. Finally, we present *Reinforcement Learning from Theorem Prover Feedback (RLTPF),* a framework that replaces human annotation with theorem prover feedback in Reinforcement Learning from Human Feedback (RLHF). Across multiple LLMs, applying *TP-as-a-Judge* and *RLTPF* improves benchmarks with only 3,508 samples, achieving 5.56% accuracy gain on Mistral-7B for MultiArith, 6.00% on Llama-2-7B for SVAMP, and 3.55% on Llama-3.1-8B for AQUA.</abstract>
      <url hash="a73a6cf3">2025.acl-long.1448</url>
      <bibkey>leang-etal-2025-theorem</bibkey>
    </paper>
    <paper id="1449">
      <title>Measuring the Effect of Transcription Noise on Downstream Language Understanding Tasks</title>
      <author><first>Ori</first><last>Shapira</last><affiliation>OriginAI</affiliation></author>
      <author><first>Shlomo</first><last>Chazan</last></author>
      <author><first>Amir David Nissan</first><last>Cohen</last><affiliation>Bar Ilan University</affiliation></author>
      <pages>29978-30004</pages>
      <abstract>With the increasing prevalence of recorded human speech, spoken language understanding (SLU) is essential for its efficient processing. In order to process the speech, it is commonly transcribed using automatic speech recognition technology. This speech-to-text transition introduces errors into the transcripts, which subsequently propagate to downstream NLP tasks, such as dialogue summarization. While it is known that transcript noise affects downstream tasks, a general-purpose and systematic approach to analyzing its effects across different noise severities and types has not been addressed. We propose a configurable framework for assessing task models in diverse noisy settings, and for examining the impact of transcript-cleaning techniques. The framework facilitates the investigation of task model behavior, which can in turn support the development of effective SLU solutions. We exemplify the utility of our framework on three SLU tasks and four task models, offering insights regarding the effect of transcript noise on tasks in general and models in particular. For instance, we find that task models can tolerate a certain level of noise, and are affected differently by the types of errors in the transcript.</abstract>
      <url hash="ac0c92b0">2025.acl-long.1449</url>
      <bibkey>shapira-etal-2025-measuring</bibkey>
    </paper>
    <paper id="1450">
      <title>Assessing Reliability and Political Bias In <fixed-case>LLM</fixed-case>s’ Judgements of Formal and Material Inferences With Partisan Conclusions</title>
      <author><first>Reto</first><last>Gubelmann</last><affiliation>University of Zurich and University of Zurich</affiliation></author>
      <author><first>Ghassen</first><last>Karray</last><affiliation>University of Zurich</affiliation></author>
      <pages>30005-30031</pages>
      <abstract>This article examines LLMs’ ability to correctly label simple inferences with partisan conclusions. For this, we develop a dataset with both formal and material inferences, containing logically equivalent pairs of inferences with conclusions that favor either the political left or the political right. This allows us to focus on political bias as a source of decrease in performance. Our samples are synthetically generated and thus highly controlled, covering both English and German. We assess the performance of 16 configurations of both open and proprietary state-of-the-art LLMs on that dataset, finding generally unreliable performance as well as widespread political bias which, in the case of the English samples, persists throughout our experimental settings.</abstract>
      <url hash="e6698502">2025.acl-long.1450</url>
      <bibkey>gubelmann-karray-2025-assessing</bibkey>
    </paper>
    <paper id="1451">
      <title><fixed-case>PARME</fixed-case>: Parallel Corpora for Low-Resourced <fixed-case>M</fixed-case>iddle <fixed-case>E</fixed-case>astern Languages</title>
      <author><first>Sina</first><last>Ahmadi</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Rico</first><last>Sennrich</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Erfan</first><last>Karami</last></author>
      <author><first>Ako</first><last>Marani</last></author>
      <author><first>Parviz</first><last>Fekrazad</last></author>
      <author><first>Gholamreza Akbarzadeh</first><last>Baghban</last></author>
      <author><first>Hanah</first><last>Hadi</last><affiliation>Paitaxt Technical Institute</affiliation></author>
      <author><first>Semko</first><last>Heidari</last></author>
      <author><first>Mahîr</first><last>Dogan</last></author>
      <author><first>Pedram</first><last>Asadi</last></author>
      <author><first>Dashne</first><last>Bashir</last></author>
      <author><first>Mohammad Amin</first><last>Ghodrati</last></author>
      <author><first>Kourosh</first><last>Amini</last></author>
      <author><first>Zeynab</first><last>Ashourinezhad</last></author>
      <author><first>Mana</first><last>Baladi</last></author>
      <author><first>Farshid</first><last>Ezzati</last></author>
      <author><first>Alireza</first><last>Ghasemifar</last></author>
      <author><first>Daryoush</first><last>Hosseinpour</last></author>
      <author><first>Behrooz</first><last>Abbaszadeh</last></author>
      <author><first>Amin</first><last>Hassanpour</last></author>
      <author><first>Bahaddin Jalal</first><last>Hamaamin</last></author>
      <author><first>Saya Kamal</first><last>Hama</last></author>
      <author><first>Ardeshir</first><last>Mousavi</last></author>
      <author><first>Sarko Nazir</first><last>Hussein</last></author>
      <author><first>Isar</first><last>Nejadgholi</last><affiliation>National Research Council Canada and University of Ottawa</affiliation></author>
      <author><first>Mehmet</first><last>Ölmez</last></author>
      <author><first>Horam</first><last>Osmanpour</last></author>
      <author><first>Rashid Roshan</first><last>Ramezani</last></author>
      <author><first>Aryan Sediq</first><last>Aziz</last></author>
      <author><first>Ali</first><last>Salehi</last></author>
      <author><first>Mohammadreza</first><last>Yadegari</last><affiliation>Shahid Beheshti University</affiliation></author>
      <author><first>Kewyar</first><last>Yadegari</last></author>
      <author><first>Sedighe Zamani</first><last>Roodsari</last><affiliation>Auburn University and Auburn University</affiliation></author>
      <pages>30032-30053</pages>
      <abstract>The Middle East is characterized by remarkable linguistic diversity, with over 400 million inhabitants speaking more than 60 languages across multiple language families. This study presents a pioneering work in developing the first parallel corpora for eight severely under-resourced varieties in the region–PARME, addressing fundamental challenges in low-resource scenarios including non-standardized writing and dialectal complexity. Through an extensive community-driven initiative, volunteers contributed to the creation of over 36,000 translated sentences, marking a significant milestone in resource development. We evaluate machine translation capabilities through zero-shot approaches and fine-tuning experiments with pretrained machine translation models and provide a comprehensive analysis of limitations. Our findings reveal significant gaps in existing technologies for processing the selected languages, highlighting critical areas for improvement in language technology for Middle Eastern languages.</abstract>
      <url hash="b71f0416">2025.acl-long.1451</url>
      <bibkey>ahmadi-etal-2025-parme</bibkey>
    </paper>
    <paper id="1452">
      <title><fixed-case>METAL</fixed-case>: A Multi-Agent Framework for Chart Generation with Test-Time Scaling</title>
      <author><first>Bingxuan</first><last>Li</last></author>
      <author><first>Yiwei</first><last>Wang</last><affiliation>University of California, Merced</affiliation></author>
      <author><first>Jiuxiang</first><last>Gu</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Kai-Wei</first><last>Chang</last><affiliation>University of California, Los Angeles and Amazon</affiliation></author>
      <author><first>Nanyun</first><last>Peng</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>30054-30069</pages>
      <abstract>Chart generation aims to generate code to produce charts satisfying the desired visual properties, e.g., texts, layout, color, and type. It has great potential to empower the automatic professional report generation in financial analysis, research presentation, education, and healthcare. In this work, we build a vision-language model (VLM) based multi-agent framework for effective automatic chart generation. Generating high-quality charts requires both strong visual design skills and precise coding capabilities that embed the desired visual properties into code. Such a complex multi-modal reasoning process is difficult for direct prompting of VLMs. To resolve these challenges, we propose METAL, a multi-agent framework that decomposes the task of chart generation into the iterative collaboration among specialized agents. METAL achieves a 5.2% improvement in the F1 score over the current best result in the chart generation task. Additionally, METAL improves chart generation performance by 11.33% over Direct Prompting with LLaMA-3.2-11B.Furthermore, the METAL framework exhibits the phenomenon of test-time scaling: its performance increases monotonically as the logarithm of computational budget grows from 512 to 8192 tokens.</abstract>
      <url hash="bd31f4a2">2025.acl-long.1452</url>
      <bibkey>li-etal-2025-metal</bibkey>
    </paper>
    <paper id="1453">
      <title><fixed-case>C</fixed-case>on<fixed-case>L</fixed-case>oan: A Contrastive Multilingual Dataset for Evaluating Loanwords</title>
      <author><first>Sina</first><last>Ahmadi</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Micha David</first><last>Hess</last></author>
      <author><first>Elena</first><last>Álvarez-Mellado</last><affiliation>Universidad Nacional de Educación a Distancia</affiliation></author>
      <author><first>Alessia</first><last>Battisti</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Cui</first><last>Ding</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Anne</first><last>Göhring</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Yingqiang</first><last>Gao</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Zifan</first><last>Jiang</last></author>
      <author><first>Andrianos</first><last>Michail</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Peshmerge</first><last>Morad</last><affiliation>Westfälische Wilhelms-Universität Münster</affiliation></author>
      <author><first>Joel</first><last>Niklaus</last><affiliation>Harvey</affiliation></author>
      <author><first>Maria Christina</first><last>Panagiotopoulou</last></author>
      <author><first>Stefano</first><last>Perrella</last></author>
      <author><first>Juri</first><last>Opitz</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Anastassia</first><last>Shaitarova</last></author>
      <author><first>Rico</first><last>Sennrich</last><affiliation>University of Zurich</affiliation></author>
      <pages>30070-30090</pages>
      <abstract>Lexical borrowing, the adoption of words from one language into another, is a ubiquitous linguistic phenomenon influenced by geopolitical, societal, and technological factors. This paper introduces ConLoan–a novel contrastive dataset comprising sentences with and without loanwords across 10 languages. Through systematic evaluation using this dataset, we investigate how state-of-the-art machine translation and language models process loanwords compared to their native alternatives. Our experiments reveal that these systems show systematic preferences for loanwords over native terms and exhibit varying performance across languages. These findings provide valuable insights for developing more linguistically robust NLP systems.</abstract>
      <url hash="3de40c9e">2025.acl-long.1453</url>
      <bibkey>ahmadi-etal-2025-conloan</bibkey>
    </paper>
    <paper id="1454">
      <title>A Theory of Response Sampling in <fixed-case>LLM</fixed-case>s: Part Descriptive and Part Prescriptive</title>
      <author><first>Sarath</first><last>Sivaprasad</last></author>
      <author><first>Pramod</first><last>Kaushik</last></author>
      <author><first>Sahar</first><last>Abdelnabi</last><affiliation>Microsoft</affiliation></author>
      <author><first>Mario</first><last>Fritz</last><affiliation>CISPA Helmholtz Center for Information Security and Saarland University</affiliation></author>
      <pages>30091-30135</pages>
      <abstract>Large Language Models (LLMs) are increasingly utilized in autonomous decision-making, where they sample options from vast action spaces. However, the heuristics that guide this sampling process remain under-explored. We study this sampling behavior and show that this underlying heuristics resembles that of human decision-making: comprising a descriptive component (reflecting statistical norm) and a prescriptive component (implicit ideal encoded in the LLM) of a concept. We show that this deviation of a sample from the statistical norm towards a prescriptive component consistently appears in concepts across diverse real-world domains like public health, and economic trends. To further illustrate the theory, we demonstrate that concept prototypes in LLMs are affected by prescriptive norms, similar to the concept of normality in humans. Through case studies and comparison with human studies, we illustrate that in real-world applications, the shift of samples toward an ideal value in LLMs’ outputs can result in significantly biased decision-making, raising ethical concerns.</abstract>
      <url hash="0589c0f7">2025.acl-long.1454</url>
      <bibkey>sivaprasad-etal-2025-theory</bibkey>
    </paper>
    <paper id="1455">
      <title><fixed-case>ME</fixed-case>raser: An Effective Fingerprint Erasure Approach for Large Language Models</title>
      <author><first>Jingxuan</first><last>Zhang</last></author>
      <author><first>Zhenhua</first><last>Xu</last></author>
      <author><first>Rui</first><last>Hu</last></author>
      <author><first>Wenpeng</first><last>Xing</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Xuhong</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Meng</first><last>Han</last><affiliation>Zhejiang University</affiliation></author>
      <pages>30136-30153</pages>
      <abstract>Large Language Models (LLMs) have become increasingly prevalent across various sectors, raising critical concerns about model ownership and intellectual property protection. Although backdoor-based fingerprinting has emerged as a promising solution for model authentication, effective attacks for removing these fingerprints remain largely unexplored. Therefore, We present Mismatched Eraser (MEraser), a novel method for effectively removing backdoor-based fingerprints from LLMs while maintaining model performance. Our approach leverages a two-phase fine-tuning strategy utilizing carefully constructed mismatched and clean datasets. Through extensive evaluation across multiple LLM architectures and fingerprinting methods, we demonstrate that MEraser achieves complete fingerprinting removal while maintaining model performance with minimal training data of fewer than 1,000 samples. Furthermore, we introduce a transferable erasure mechanism that enables effective fingerprinting removal across different models without repeated training. In conclusion, our approach provides a practical solution for fingerprinting removal in LLMs, reveals critical vulnerabilities in current fingerprinting techniques, and establishes comprehensive evaluation benchmarks for developing more resilient model protection methods in the future.</abstract>
      <url hash="890c3424">2025.acl-long.1455</url>
      <bibkey>zhang-etal-2025-meraser</bibkey>
    </paper>
    <paper id="1456">
      <title><fixed-case>VISA</fixed-case>: Retrieval Augmented Generation with Visual Source Attribution</title>
      <author><first>Xueguang</first><last>Ma</last></author>
      <author><first>Shengyao</first><last>Zhuang</last><affiliation>CSIRO</affiliation></author>
      <author><first>Bevan</first><last>Koopman</last><affiliation>CSIRO and University of Queensland</affiliation></author>
      <author><first>Guido</first><last>Zuccon</last><affiliation>The University of Queensland</affiliation></author>
      <author><first>Wenhu</first><last>Chen</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Jimmy</first><last>Lin</last><affiliation>University of Waterloo</affiliation></author>
      <pages>30154-30169</pages>
      <abstract>Generation with source attribution is important for enhancing the verifiability of retrieval-augmented generation (RAG) systems. However, existing approaches in RAG primarily link generated content to document-level references, making it challenging for users to locate evidence among multiple content-rich retrieved documents. To address this challenge, we propose Retrieval-Augmented Generation with Visual Source Attribution (VISA), a novel approach that combines answer generation with visual source attribution. Leveraging large vision-language models (VLMs), VISA identifies the evidence and highlights the exact regions that support the generated answers with bounding boxes in the retrieved document screenshots. To evaluate its effectiveness, we curated two datasets: Wiki-VISA, based on crawled Wikipedia webpage screenshots, and Paper-VISA, derived from PubLayNet and tailored to the medical domain. Experimental results demonstrate the effectiveness of VISA for visual source attribution on documents’ original look, as well as highlighting the challenges for improvement.</abstract>
      <url hash="1d25cdfb">2025.acl-long.1456</url>
      <bibkey>ma-etal-2025-visa</bibkey>
    </paper>
    <paper id="1457">
      <title><fixed-case>DRAMA</fixed-case>: Diverse Augmentation from Large Language Models to Smaller Dense Retrievers</title>
      <author><first>Xueguang</first><last>Ma</last></author>
      <author><first>Xi Victoria</first><last>Lin</last><affiliation>Meta</affiliation></author>
      <author><first>Barlas</first><last>Oguz</last><affiliation>Meta</affiliation></author>
      <author><first>Jimmy</first><last>Lin</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Wen-tau</first><last>Yih</last><affiliation>Meta Platforms, Inc.</affiliation></author>
      <author><first>Xilun</first><last>Chen</last><affiliation>Meta FAIR</affiliation></author>
      <pages>30170-30186</pages>
      <abstract>Large language models (LLMs) have demonstrated strong effectiveness and robustness when fine-tuned as dense retrievers.However, their large parameter size presents significant computational challenges at inference time.While smaller retrievers offer better efficiency, they often fail to generalize effectively with limited supervised fine-tuning data.In this work, we introduce DRAMA, a training framework that leverages LLMs to train smaller generalizable dense retrievers.In particular, we adopt pruned LLMs as the backbone and train on diverse LLM-augmented data in a single-stage contrastive learning setup.Experiments show that DRAMA offers better multilingual and long-context capabilities than traditional encoder-based retrievers, and achieves strong performance across multiple tasks and languages.</abstract>
      <url hash="6074241e">2025.acl-long.1457</url>
      <bibkey>ma-etal-2025-drama</bibkey>
    </paper>
    <paper id="1458">
      <title>Stochastic Chameleons: Irrelevant Context Hallucinations Reveal Class-Based (Mis)Generalization in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Ziling</first><last>Cheng</last></author>
      <author><first>Meng</first><last>Cao</last><affiliation>McGill University</affiliation></author>
      <author><first>Marc-Antoine</first><last>Rondeau</last><affiliation>Mila - Quebec Artificial Intelligence Institute</affiliation></author>
      <author><first>Jackie CK</first><last>Cheung</last><affiliation>McGill University, Mila Research Institute and Microsoft</affiliation></author>
      <pages>30187-30214</pages>
      <abstract>The widespread success of LLMs on NLP benchmarks has been accompanied by concerns that LLMs function primarily as stochastic parrots that reproduce texts similar to what they saw during pre-training, often erroneously. But what is the nature of their errors, and do these errors exhibit any regularities? In this work, we examine irrelevant context hallucinations, in which models integrate misleading contextual cues into their predictions. Through behavioral analysis, we show that these errors result from a structured yet flawed mechanism that we term _class-based (mis)generalization_, in which models combine abstract class cues with features extracted from the query or context to derive answers. Furthermore, mechanistic interpretability experiments on Llama-3, Mistral, and Pythia across 39 factual recall relation types reveal that this behavior is reflected in the model’s internal computations: (i) abstract class representations are constructed in lower layers before being refined into specific answers in higher layers, (ii) feature selection is governed by two competing circuits — one prioritizing direct query-based reasoning, the other incorporating contextual cues — whose relative influences determine the final output. Our findings provide a more nuanced perspective on the stochastic parrot argument: through form-based training, LLMs can exhibit generalization leveraging abstractions, albeit in unreliable ways based on contextual cues — what we term _stochastic chameleons_.</abstract>
      <url hash="1dff4fda">2025.acl-long.1458</url>
      <bibkey>cheng-etal-2025-stochastic</bibkey>
    </paper>
    <paper id="1459">
      <title><fixed-case>MAP</fixed-case>o<fixed-case>RL</fixed-case>: Multi-Agent Post-Co-Training for Collaborative Large Language Models with Reinforcement Learning</title>
      <author><first>Chanwoo</first><last>Park</last></author>
      <author><first>Seungju</first><last>Han</last><affiliation>Computer Science Department, Stanford University and NVIDIA</affiliation></author>
      <author><first>Xingzhi</first><last>Guo</last><affiliation>Amazon</affiliation></author>
      <author><first>Asuman E.</first><last>Ozdaglar</last><affiliation>Massachusetts Institute of Technology and Massachusetts Institute of Technology</affiliation></author>
      <author><first>Kaiqing</first><last>Zhang</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Joo-Kyung</first><last>Kim</last><affiliation>Amazon</affiliation></author>
      <pages>30215-30248</pages>
      <abstract>Leveraging multi-agentic frameworks to enhance large language models (LLMs) has demonstrated significant potential recently, with most existing studies focusing on prompting and developing workflows with frozen LLMs. In this paper, we aim to further unleash the power of such multi-agentic frameworks for post-training LLMs for better collaboration. Specifically, we develop a new paradigm of Multi-Agent Post-co-training for collaborative LLMs with Reinforcement Learning (MAPoRL). In MAPoRL, multiple LLMs first generate their own responses and engage in discussions to collaboratively enhance the final response output; the final output is then scored by a verifier, where the scores serve as the reward and is maximized through multi-agent RL. Additionally, MAPoRL also reshapes the reward above with additional incentives to encourage corrective and persuasive outputs in the discussions. A key novelty from most existing LLM post-training paradigms is the advocacy of co-training multiple LLMs together, and the use of RL for better generalization. Accompanied by a few analytical insights, our experiments show that training single LLMs solely is insufficient for encouraging collaboration, while multi-agent co-training can significantly enhance the collaboration performance across multiple datasets, with generalization to unseen domains, compared to that of multiple LLMs before post-training.</abstract>
      <url hash="d4efcd78">2025.acl-long.1459</url>
      <bibkey>park-etal-2025-maporl</bibkey>
    </paper>
    <paper id="1460">
      <title>Map&amp;Make: Schema Guided Text to Table Generation</title>
      <author><first>Naman</first><last>Ahuja</last></author>
      <author><first>Fenil</first><last>Bardoliya</last></author>
      <author><first>Chitta</first><last>Baral</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Vivek</first><last>Gupta</last><affiliation>Arizona State University</affiliation></author>
      <pages>30249-30262</pages>
      <abstract>Transforming dense, unstructured text into interpretable tables—commonly referred to as Text-to-Table generation—is a key task in information extraction. Existing methods often overlook what complex information to extract and how to infer it from text. We present Map&amp;Make, a versatile approach that decomposes text into atomic propositions to infer latent schemas, which are then used to generate tables capturing both qualitative nuances and quantitative facts. We evaluate our method on three challenging datasets: Rotowire, known for its complex, multi-table schema; Livesum which requires numerical aggregation; and Wiki40 which require open text extraction from mulitple domains. By correcting hallucination errors in Rotowire, we also provide a cleaner benchmark. Our method shows significant gains in both accuracy and interpretability across comprehensive comparative and referenceless metrics. Finally, ablation studies highlight the key factors driving performance and validate the utility of our approach in structured summarization. Code and data are available at: https://coral-lab-asu.github.io/map-make.</abstract>
      <url hash="72fb2ab7">2025.acl-long.1460</url>
      <bibkey>ahuja-etal-2025-map</bibkey>
    </paper>
    <paper id="1461">
      <title><fixed-case>IRIS</fixed-case>: Interpretable Retrieval-Augmented Classification for Long Interspersed Document Sequences</title>
      <author><first>Fengnan</first><last>Li</last></author>
      <author><first>Elliot D.</first><last>Hill</last></author>
      <author><first>Jiang</first><last>Shu</last></author>
      <author><first>Jiaxin</first><last>Gao</last></author>
      <author><first>Matthew M.</first><last>Engelhard</last><affiliation>Duke University</affiliation></author>
      <pages>30263-30283</pages>
      <abstract>Transformer-based models have achieved state-of-the-art performance in document classification but struggle with long-text processing due to the quadratic computational complexity in the self-attention module. Existing solutions, such as sparse attention, hierarchical models, and key sentence extraction, partially address the issue but still fall short when the input sequence is exceptionally lengthy. To address this challenge, we propose **IRIS** (**I**nterpretable **R**etrieval-Augmented Classification for long **I**nterspersed Document **S**equences), a novel, lightweight framework that utilizes retrieval to efficiently classify long documents while enhancing interpretability. IRIS segments documents into chunks, stores their embeddings in a vector database, and retrieves those most relevant to a given task using learnable query vectors. A linear attention mechanism then aggregates the retrieved embeddings for classification, allowing the model to process arbitrarily long documents without increasing computational cost and remaining trainable on a single GPU. Our experiments across six datasets show that IRIS achieves comparable performance to baseline models on standard benchmarks, and excels in three clinical note disease risk prediction tasks where documents are extremely long and key information is sparse. Furthermore, IRIS provides global interpretability by revealing a clear summary of key risk factors identified by the model. These findings highlight the potential of IRIS as an efficient and interpretable solution for long-document classification, particularly in healthcare applications where both performance and explainability are crucial.</abstract>
      <url hash="348a244f">2025.acl-long.1461</url>
      <bibkey>li-etal-2025-iris</bibkey>
    </paper>
    <paper id="1462">
      <title>Symmetrical Visual Contrastive Optimization: Aligning Vision-Language Models with Minimal Contrastive Images</title>
      <author><first>Shengguang</first><last>Wu</last><affiliation>Stanford University</affiliation></author>
      <author><first>Fan-Yun</first><last>Sun</last><affiliation>Stanford University</affiliation></author>
      <author><first>Kaiyue</first><last>Wen</last></author>
      <author><first>Nick</first><last>Haber</last><affiliation>Stanford University</affiliation></author>
      <pages>30284-30297</pages>
      <abstract>Recent studies have shown that Large Vision-Language Models (VLMs) tend to neglect image content and over-rely on language-model priors, resulting in errors in visually grounded tasks and hallucinations. We hypothesize that this issue arises because existing VLMs are not explicitly trained to generate texts that are accurately grounded in fine-grained image details. To enhance visual feedback during VLM training, we propose S-VCO (Symmetrical Visual Contrastive Optimization), a novel finetuning objective that steers the model toward capturing important visual details and aligning them with corresponding text tokens. To further facilitate this detailed alignment, we introduce MVC, a paired image-text dataset built by automatically filtering and augmenting visual counterfactual data to challenge the model with hard contrastive cases involving Minimal Visual Contrasts. Experiments show that our method consistently improves VLM performance across diverse benchmarks covering various abilities and domains, achieving up to a 22% reduction in hallucinations, and significant gains in vision-centric and general tasks. Notably, these improvements become increasingly pronounced in benchmarks with higher visual dependency. In short, S-VCO offers a significant enhancement of VLM’s visually-dependent task performance while retaining or even improving the model’s general abilities.</abstract>
      <url hash="27106a1e">2025.acl-long.1462</url>
      <bibkey>wu-etal-2025-symmetrical</bibkey>
    </paper>
    <paper id="1463">
      <title>Can we Retrieve Everything All at Once? <fixed-case>ARM</fixed-case>: An Alignment-Oriented <fixed-case>LLM</fixed-case>-based Retrieval Method</title>
      <author><first>Peter Baile</first><last>Chen</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Yi</first><last>Zhang</last><affiliation>AWS AI</affiliation></author>
      <author><first>Mike</first><last>Cafarella</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>30298-30317</pages>
      <abstract>Real-world open-domain questions can be complex, especially when answering them requires integrating information from multiple sources. Effectively identifying the necessary information involves *aligning* it with the available data and its organization. However, existing RAG solutions address the alignment problem in a limited manner. Using off-the-shelf LLMs for question decomposition lacks awareness of the available data and its structure, often resulting in suboptimal retrieval performance. Alternatively, iteratively generating follow-up queries and interacting with the data collection, as explored in agentic RAG approaches, shows potential but is often *inefficient* since each successive query depends on previous results rather than being guided by the overall organization of the available data. To address the *alignment* problem, we introduce an LLM-based retrieval method — ARM, designed to better align questions with the organization of the data collection. Instead of solely matching query utterance, ARM explores *relationships among data objects*, enabling a retrieve-all-at-once solution for complex queries. Experimental results demonstrate that ARM significantly outperforms existing RAG methods on various complex open-domain QA tasks across multiple modalities, achieving superior retrieval performance and downstream accuracy while significantly lowering monetary costs.</abstract>
      <url hash="a8dc2a83">2025.acl-long.1463</url>
      <bibkey>chen-etal-2025-retrieve</bibkey>
    </paper>
    <paper id="1464">
      <title><fixed-case>R</fixed-case>2<fixed-case>D</fixed-case>2: Remembering, Replaying and Dynamic Decision Making with a Reflective Agentic Memory</title>
      <author><first>Tenghao</first><last>Huang</last></author>
      <author><first>Kinjal</first><last>Basu</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Ibrahim</first><last>Abdelaziz</last><affiliation>International Business Machines (IBM)</affiliation></author>
      <author><first>Pavan</first><last>Kapanipathi</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Jonathan</first><last>May</last><affiliation>University of Southern California and USC/ISI</affiliation></author>
      <author><first>Muhao</first><last>Chen</last><affiliation>University of California, Davis and University of Southern California</affiliation></author>
      <pages>30318-30330</pages>
      <abstract>The proliferation of web agents necessitates advanced navigation and interaction strategies within complex web environments. Current models often struggle with efficient navigation and action execution due to limited visibility and understanding of web structures. Our proposed R2D2 framework addresses these challenges by integrating two paradigms: Remember and Reflect. The Remember paradigm utilizes a replay buffer that aids agents in reconstructing the web environment dynamically, thus enabling the formulation of a detailed “map” of previously visited pages. This helps in reducing navigational errors and optimizing the decision-making process during web interactions. Conversely, the Reflect paradigm allows agents to learn from past mistakes by providing a mechanism for error analysis and strategy refinement, enhancing overall task performance. We evaluate R2D2 using the WEBARENA benchmark, demonstrating significant improvements over existing methods, including a 50% reduction in navigation errors and a threefold increase in task completion rates. Our findings suggest that a combination of memory-enhanced navigation and reflective learning promisingly advances the capabilities of web agents, potentially benefiting various applications such as automated customer service and personal digital assistants.</abstract>
      <url hash="e9c935ef">2025.acl-long.1464</url>
      <bibkey>huang-etal-2025-r2d2</bibkey>
    </paper>
    <paper id="1465">
      <title><fixed-case>F</fixed-case>air<fixed-case>I</fixed-case> Tales: Evaluation of Fairness in <fixed-case>I</fixed-case>ndian Contexts with a Focus on Bias and Stereotypes</title>
      <author><first>Janki Atul</first><last>Nawale</last></author>
      <author><first>Mohammed Safi Ur Rahman</first><last>Khan</last><affiliation>Indian Institute of Technology, Madras, Dhirubhai Ambani Institute Of Information and Communication Technology and Indian Institute of Technology, Madras, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <author><first>Janani</first><last>D</last></author>
      <author><first>Mansi</first><last>Gupta</last></author>
      <author><first>Danish</first><last>Pruthi</last><affiliation>Indian Institute of Science, Bangalore</affiliation></author>
      <author><first>Mitesh M</first><last>Khapra</last><affiliation>Indian Institute of Technology, Madras</affiliation></author>
      <pages>30331-30380</pages>
      <abstract>Existing studies on fairness are largely Western-focused, making them inadequate for culturally diverse countries such as India. To address this gap, we introduce INDIC-BIAS, a comprehensive India-centric benchmark designed to evaluate fairness of LLMs across 85 identity groups encompassing diverse castes, religions, regions, and tribes. We first consult domain experts to curate over 1,800 socio-cultural topics spanning behaviors and situations, where biases and stereotypes are likely to emerge. Grounded in these topics, we generate and manually validate 20,000 real-world scenario templates to probe LLMs for fairness. We structure these templates into three evaluation tasks: plausibility, judgment, and generation. Our evaluation of 14 popular LLMs on these tasks reveals strong negative biases against marginalized identities, with models frequently reinforcing common stereotypes. Additionally, we find that models struggle to mitigate bias even when explicitly asked to rationalize their decision. Our evaluation provides evidence of both allocative and representational harms that current LLMs could cause towards Indian identities, calling for a more cautious usage in practical applications. We release INDIC-BIAS as an open-source benchmark to advance research on benchmarking and mitigating biases and stereotypes in the Indian context.</abstract>
      <url hash="3ba58225">2025.acl-long.1465</url>
      <bibkey>nawale-etal-2025-fairi</bibkey>
    </paper>
    <paper id="1466">
      <title><fixed-case>S</fixed-case>peech<fixed-case>IQ</fixed-case>: Speech-Agentic Intelligence Quotient Across Cognitive Levels in Voice Understanding by Large Language Models</title>
      <author><first>Zhen</first><last>Wan</last></author>
      <author><first>Chao-Han Huck</first><last>Yang</last><affiliation>NVIDIA Research</affiliation></author>
      <author><first>Yahan</first><last>Yu</last><affiliation>Kyoto University, Kyoto University</affiliation></author>
      <author><first>Jinchuan</first><last>Tian</last></author>
      <author><first>Sheng</first><last>Li</last><affiliation>Institute of Science Tokyo</affiliation></author>
      <author><first>Ke</first><last>Hu</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Zhehuai</first><last>Chen</last></author>
      <author><first>Shinji</first><last>Watanabe</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Fei</first><last>Cheng</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Chenhui</first><last>Chu</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Sadao</first><last>Kurohashi</last><affiliation>National Institute of Informatics (NII) and Kyoto University</affiliation></author>
      <pages>30381-30398</pages>
      <abstract>We introduce Speech-based Intelligence Quotient (SIQ) as a new form of human cognition-inspired evaluation pipeline for voice understanding large language models (LLM_Voice), designed to assess their voice understanding ability. Moving beyond popular voice understanding metrics such as word error rate (WER), SIQ examines LLM_Voice across three cognitive levels motivated by Bloom’s Taxonomy: (1) Remembering (i.e., WER for verbatim accuracy); (2) Understanding (i.e., similarity of LLM’s interpretations); and (3) Application (i.e., QA accuracy for simulating downstream tasks). We demonstrate that SIQ not only quantifies voice understanding abilities but also provides unified comparisons between cascaded methods (e.g., ASR-LLM) and end-to-end models, identifies annotation errors in existing benchmarks, and detects hallucinations in LLM_Voice. Our framework represents a first-of-its-kind intelligence examination that bridges cognitive principles with voice-oriented benchmarks, while exposing overlooked challenges in multi-modal training. Our code and data will be open source to encourage future studies.</abstract>
      <url hash="7ccfc353">2025.acl-long.1466</url>
      <bibkey>wan-etal-2025-speechiq</bibkey>
    </paper>
    <paper id="1467">
      <title>Predicting Implicit Arguments in Procedural Video Instructions</title>
      <author><first>Anil</first><last>Batra</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Laura</first><last>Sevilla-Lara</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Marcus</first><last>Rohrbach</last><affiliation>Technische Universität Darmstadt</affiliation></author>
      <author><first>Frank</first><last>Keller</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>30399-30419</pages>
      <abstract>Procedural texts help AI enhance reasoning about context and action sequences. Transforming these into Semantic Role Labeling (SRL) improves understanding of individual steps by identifying predicate-argument structure like verb,what,where/with. Procedural instructions are highly elliptic, for instance, (i) add cucumber to the bowl and (ii) add sliced tomatoes, the second step’s where argument is inferred from the context, referring to where the cucumber was placed. Prior SRL benchmarks often miss implicit arguments, leading to incomplete understanding. To address this, we introduce Implicit-VidSRL, a dataset that necessitates inferring implicit and explicit arguments from contextual information in multimodal cooking procedures. Our proposed dataset benchmarks multimodal models’ contextual reasoning, requiring entity tracking through visual changes in recipes. We study recent multimodal LLMs and reveal that they struggle to predict implicit arguments of what and where/with from multi-modal procedural data given the verb. Lastly, we propose iSRL-Qwen2-VL, which achieves a 17% relative improvement in F1-score for what-implicit and a 14.7% for where/with-implicit semantic roles over GPT-4o.</abstract>
      <url hash="a04200e6">2025.acl-long.1467</url>
      <bibkey>batra-etal-2025-predicting</bibkey>
    </paper>
    <paper id="1468">
      <title><fixed-case>PIG</fixed-case>uard: Prompt Injection Guardrail via Mitigating Overdefense for Free</title>
      <author><first>Hao</first><last>Li</last></author>
      <author><first>Xiaogeng</first><last>Liu</last><affiliation>University of Wisconsin - Madison</affiliation></author>
      <author><first>Ning</first><last>Zhang</last><affiliation>Washington University, Saint Louis</affiliation></author>
      <author><first>Chaowei</first><last>Xiao</last><affiliation>University of Wisconsin - Madison and NVIDIA</affiliation></author>
      <pages>30420-30437</pages>
      <abstract>Prompt injection attacks pose a critical threat to large language models (LLMs), enabling goal hijacking and data leakage. Prompt guard models, though effective in defense, suffer from over-defense—falsely flagging benign inputs as malicious due to trigger word bias. To address this issue, we introduce NotInject, an evaluation dataset that systematically measures over-defense across various prompt guard models. NotInject contains 339 benign samples enriched with trigger words common in prompt injection attacks, enabling fine-grained evaluation. Our results show that state-of-the-art models suffer from over-defense issues, with accuracy dropping close to random guessing levels (60%). To mitigate this, we propose PIGuard, a novel prompt guard model that incorporates a new training strategy, Mitigating Over-defense for Free (MOF), which significantly reduces the bias on trigger words. PIGuard demonstrates state-of-the-art performance on diverse benchmarks including NotInject, surpassing the existing best model by 30.4%, offering a robust and open-source solution for detecting prompt injection attacks. The code and datasets are released at https://github.com/leolee99/PIGuard.</abstract>
      <url hash="6e575c0c">2025.acl-long.1468</url>
      <bibkey>li-etal-2025-piguard</bibkey>
    </paper>
    <paper id="1469">
      <title><fixed-case>CLIPE</fixed-case>rase: Efficient Unlearning of Visual-Textual Associations in <fixed-case>CLIP</fixed-case></title>
      <author><first>Tianyu</first><last>Yang</last></author>
      <author><first>Lisen</first><last>Dai</last><affiliation>Columbia University</affiliation></author>
      <author><first>Xiangqi</first><last>Wang</last></author>
      <author><first>Minhao</first><last>Cheng</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Yapeng</first><last>Tian</last><affiliation>University of Texas at Dallas</affiliation></author>
      <author><first>Xiangliang</first><last>Zhang</last><affiliation>University of Notre Dame</affiliation></author>
      <pages>30438-30452</pages>
      <abstract>Machine unlearning (MU) has gained significant attention as a means to remove the influence of specific data from a trained model without requiring full retraining. While progress has been made in unimodal domains like text and image classification, unlearning in multimodal models remains relatively under-explored. In this work, we address the unique challenges of unlearning in CLIP, a prominent multimodal model that aligns visual and textual representations. We introduce CLIPErase, a novel approach that disentangles and selectively forgets both visual and textual associations, ensuring that unlearning does not compromise model performance.CLIPErase consists of three key modules: a Forgetting Module that disrupts the associations in the forget set, a Retention Module that preserves performance on the retain set, and a Consistency Module that maintains consistency with the original model. Extensive experiments on CIFAR-100, Flickr30K, and Conceptual 12M across five CLIP downstream tasks, as well as an evaluation on diffusion models, demonstrate that CLIPErase effectively removes designated associations from multimodal samples in downstream tasks, while preserving the model’s performance on the retain set after unlearning.</abstract>
      <url hash="7d486516">2025.acl-long.1469</url>
      <bibkey>yang-etal-2025-cliperase</bibkey>
    </paper>
    <paper id="1470">
      <title><fixed-case>V</fixed-case>i<fixed-case>G</fixed-case>i<fixed-case>L</fixed-case>3<fixed-case>D</fixed-case>: A Linguistically Diverse Dataset for 3<fixed-case>D</fixed-case> Visual Grounding</title>
      <author><first>Austin</first><last>Wang</last><affiliation>Simon Fraser University</affiliation></author>
      <author><first>ZeMing</first><last>Gong</last><affiliation>Simon Fraser University</affiliation></author>
      <author><first>Angel X</first><last>Chang</last><affiliation>Simon Fraser University</affiliation></author>
      <pages>30453-30475</pages>
      <abstract>3D visual grounding (3DVG) involves localizing entities in a 3D scene referred to by natural language text. Such models are useful for embodied AI and scene retrieval applications, which involve searching for objects or patterns using natural language descriptions. While recent works have focused on LLM-based scaling of 3DVG datasets, these datasets do not capture the full range of potential prompts which could be specified in the English language. To ensure that we are scaling up and testing against a useful and representative set of prompts, we propose a framework for linguistically analyzing 3DVG prompts and introduce Visual Grounding with Diverse Language in 3D (ViGiL3D), a diagnostic dataset for evaluating visual grounding methods against a diverse set of language patterns. We evaluate existing open-vocabulary 3DVG methods to demonstrate that these methods are not yet proficient in understanding and identifying the targets of more challenging, out-of-distribution prompts, toward real-world applications.</abstract>
      <url hash="d7060828">2025.acl-long.1470</url>
      <bibkey>wang-etal-2025-vigil3d</bibkey>
    </paper>
    <paper id="1471">
      <title>The time scale of redundancy between prosody and linguistic context</title>
      <author><first>Tamar I</first><last>Regev</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Chiebuka</first><last>Ohams</last></author>
      <author><first>Shaylee</first><last>Xie</last></author>
      <author><first>Lukas</first><last>Wolf</last></author>
      <author><first>Evelina</first><last>Fedorenko</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Alex</first><last>Warstadt</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Ethan</first><last>Wilcox</last><affiliation>ETHZ - ETH Zurich and Georgetown University</affiliation></author>
      <author><first>Tiago</first><last>Pimentel</last><affiliation>Department of Computer Science, ETHZ - ETH Zurich</affiliation></author>
      <pages>30476-30488</pages>
      <abstract>In spoken communication, information is transmitted not only via words, but also through a rich array of non-verbal signals, including prosody—the non-segmental auditory features of speech. Do these different communication channels carry distinct information? Prior work has shown that the information carried by prosodic features is substantially redundant with that carried by the surrounding words. Here, we systematically examine the time scale of this relationship, studying how it varies with the length of past and future contexts. We find that a word’s prosodic features require an extended past context (3-8 words across different features) to be reliably predicted. Given that long-scale contextual information decays in memory, prosody may facilitate communication by adding information that is locally unique. We also find that a word’s prosodic features show some redundancy with future words, but only with a short scale of 1-2 words, consistent with reports of incremental short-term planning in language production. Thus, prosody may facilitate communication by helping listeners predict upcoming material. In tandem, our results highlight potentially distinct roles that prosody plays in facilitating integration of words into past contexts and in helping predict upcoming words.</abstract>
      <url hash="49738fdc">2025.acl-long.1471</url>
      <bibkey>regev-etal-2025-time</bibkey>
    </paper>
    <paper id="1472">
      <title>Basic Reading Distillation</title>
      <author><first>Zhi</first><last>Zhou</last></author>
      <author><first>Sirui</first><last>Miao</last></author>
      <author><first>Xiangyu</first><last>Duan</last><affiliation>Soochow University, China</affiliation></author>
      <author><first>Hao</first><last>Yang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>30489-30502</pages>
      <abstract>Large language models (LLMs) have demonstrated remarkable abilities in various natural language processing areas, but they demand high computation resources which limits their deployment in real-world. Distillation is one technique to solve this problem through either knowledge distillation or task distillation. Both distillation approaches train small models to imitate specific features of LLMs, but they all neglect basic reading education for small models on generic texts that are <i>unrelated</i> to downstream tasks. In this paper, we propose basic reading distillation (BRD) which educates a small model to imitate LLMs basic reading behaviors, such as named entity recognition, question raising and answering, on each sentence. After such basic education, we apply the small model on various tasks including language inference benchmarks and BIG-bench tasks. It shows that the small model can outperform or perform comparable to over 20x bigger LLMs. Analysis reveals that BRD effectively influences the probability distribution of the small model, and has orthogonality to either knowledge distillation or task distillation.</abstract>
      <url hash="72c9944e">2025.acl-long.1472</url>
      <bibkey>zhou-etal-2025-basic</bibkey>
    </paper>
    <paper id="1473">
      <title>Quantized Can Still Be Calibrated: A Unified Framework to Calibration in Quantized Large Language Models</title>
      <author><first>Mingyu</first><last>Zhong</last><affiliation>University of Houston</affiliation></author>
      <author><first>Guanchu</first><last>Wang</last></author>
      <author><first>Yu-Neng</first><last>Chuang</last><affiliation>Rice University</affiliation></author>
      <author><first>Na</first><last>Zou</last><affiliation>University of Houston</affiliation></author>
      <pages>30503-30517</pages>
      <abstract>Although weight quantization helps large language models (LLMs) in resource-constrained environments, its influence on the uncertainty calibration remains unexplored. To bridge this gap, we presents a comprehensive investigation of uncertainty calibration for quantized LLMs in this work. Specifically, we propose an analytic method to estimate the upper bound of calibration error (UBCE) for LLMs. Our method separately discusses the calibration error of the model’s correct and incorrect predictions, indicating a theoretical improvement of calibration error caused by the weight quantization. Our study demonstrates that quantized models consistently exhibit worse calibration performance than full-precision models, supported by consistent analysis across multiple LLMs and datasets. To address the calibration issues of quantized models, we propose a novel method of post calibration for recovering the calibration performance of quantized models through soft-prompt tuning. Specifically, we inject soft tokens to quantized models after the embedding layers, and optimize these tokens to recover the calibration error caused by the weight quantization. Experimental results on multiple datasets demonstrate our effectiveness in improving the uncertainty calibration of quantized LLMs, facilitating more reliable weight quantization in resource-constrained environments.</abstract>
      <url hash="f748a5e8">2025.acl-long.1473</url>
      <bibkey>zhong-etal-2025-quantized</bibkey>
    </paper>
    <paper id="1474">
      <title>A Spatio-Temporal Point Process for Fine-Grained Modeling of Reading Behavior</title>
      <author><first>Francesco Ignazio</first><last>Re</last><affiliation>Department of Computer Science, ETHZ - ETH Zurich</affiliation></author>
      <author><first>Andreas</first><last>Opedal</last><affiliation>Department of Computer Science, ETHZ - ETH Zurich</affiliation></author>
      <author><first>Glib</first><last>Manaiev</last></author>
      <author><first>Mario</first><last>Giulianelli</last><affiliation>Department of Computer Science, ETHZ - ETH Zurich</affiliation></author>
      <author><first>Ryan</first><last>Cotterell</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <pages>30518-30538</pages>
      <abstract>Reading is a process that unfolds across space and time. Standard modeling approaches, however, overlook much of the spatio-temporal dynamics involved in reading by relying on aggregated reading measurements—typically only focusing on fixation durations—and employing models with strong simplifying assumptions. In this paper, we propose a generative model that captures not only how long fixations last, but also where they land and when they occur. To this end, we model reading scanpaths via two conditionally independent distributions: one for fixation location and timing, and another for fixation duration.The location (and timing) of fixation shifts, so-called saccades, are modeled using a spatio-temporal Hawkes process, which captures how each fixation excites the probability of a new fixation occurring near it in time and space. Empirically, our Hawkes process model exhibits higher likelihood on held-out reading data than baselines. The duration time of fixation events is modeled as a function of fixation-specific features convolved across time, thus capturing non-stationary delayed effects. We find that convolution-based approaches demonstrate weak predictive power when modeling disaggregated fixation durations. Similarly, our analysis of surprisal theory on disaggregated data reveals limited effectiveness in predicting both where fixations occur and how long they last.</abstract>
      <url hash="1b5a4a79">2025.acl-long.1474</url>
      <bibkey>re-etal-2025-spatio</bibkey>
    </paper>
    <paper id="1475">
      <title>More is not always better? Enhancing Many-Shot In-Context Learning with Differentiated and Reweighting Objectives</title>
      <author><first>Xiaoqing</first><last>Zhang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Ang</first><last>Lv</last></author>
      <author><first>Yuhan</first><last>Liu</last></author>
      <author><first>Flood</first><last>Sung</last><affiliation>Moonshot AI</affiliation></author>
      <author><first>Wei</first><last>Liu</last></author>
      <author><first>Jian</first><last>Luan</last><affiliation>Xiaomi Corporation</affiliation></author>
      <author><first>Shuo</first><last>Shang</last></author>
      <author><first>Xiuying</first><last>Chen</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Rui</first><last>Yan</last><affiliation>Renmin University of China</affiliation></author>
      <pages>30539-30552</pages>
      <abstract>Large language models (LLMs) excel at few-shot in-context learning (ICL) without requiring parameter updates. However, as ICL demonstrations increase from a few to many, performance tends to plateau and eventually decline. We identify two primary causes for this trend: the suboptimal negative log-likelihood (NLL) optimization objective and the incremental data noise. To address these issues, we introduce <i>DrICL</i>, a novel optimization method that enhances model performance through <i>Differentiated</i> and <i>Reweighting</i> objectives. Globally, DrICL utilizes differentiated learning to optimize the NLL objective, ensuring that many-shot performance surpasses zero-shot levels. Locally, it dynamically adjusts the weighting of many-shot demonstrations by leveraging cumulative advantages inspired by reinforcement learning, thereby mitigating the impact of noisy data.Recognizing the lack of multi-task datasets with diverse many-shot distributions, we develop the <i>Many-Shot ICL Benchmark</i> (ICL-50)-a large-scale benchmark of 50 tasks that cover shot numbers from 1 to 350 within sequences of up to 8,000 tokens-for both fine-tuning and evaluation purposes.Experimental results demonstrate that LLMs enhanced with DrICL achieve significant improvements in many-shot setups across various tasks, including both in-domain and out-of-domain scenarios.We release the code and dataset hoping to facilitate further research in many-shot ICL.</abstract>
      <url hash="11115869">2025.acl-long.1475</url>
      <bibkey>zhang-etal-2025-always</bibkey>
    </paper>
    <paper id="1476">
      <title>Astute <fixed-case>RAG</fixed-case>: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models</title>
      <author><first>Fei</first><last>Wang</last></author>
      <author><first>Xingchen</first><last>Wan</last><affiliation>Google</affiliation></author>
      <author><first>Ruoxi</first><last>Sun</last><affiliation>Google</affiliation></author>
      <author><first>Jiefeng</first><last>Chen</last><affiliation>Google</affiliation></author>
      <author><first>Sercan O</first><last>Arik</last><affiliation>Google</affiliation></author>
      <pages>30553-30571</pages>
      <abstract>Retrieval augmented generation (RAG), while effectively integrating external knowledge to address the inherent limitations of large language models (LLMs), can be hindered by imperfect retrieval that contain irrelevant, misleading, or even malicious information. Previous studies have rarely connected the behavior of RAG through joint analysis, particularly regarding error propagation coming from imperfect retrieval and potential conflicts between LLMs’ internal knowledge and external sources. Through comprehensive and controlled analyses under realistic conditions, we find that imperfect retrieval augmentation is inevitable, common, and harmful. We identify the knowledge conflicts between LLM-internal and external knowledge from retrieval as a bottleneck to overcome imperfect retrieval in the post-retrieval stage of RAG. To address this, we propose Astute RAG, a novel RAG approach designed to be resilient to imperfect retrieval augmentation. It adaptively elicits essential information from LLMs’ internal knowledge, iteratively consolidates internal and external knowledge with source-awareness, and finalizes the answer according to information reliability. Our experiments with Gemini and Claude demonstrate the superior performance of Astute RAG compared to previous robustness-enhanced RAG approaches. Specifically, Astute RAG is the only RAG method that achieves performance comparable to or even surpassing conventional use of LLMs under the worst-case scenario. Further analysis reveals the effectiveness of Astute RAG in resolving knowledge conflicts, thereby improving the trustworthiness of RAG.</abstract>
      <url hash="11f45457">2025.acl-long.1476</url>
      <bibkey>wang-etal-2025-astute</bibkey>
    </paper>
    <paper id="1477">
      <title><fixed-case>S</fixed-case>ub<fixed-case>LIME</fixed-case>: Subset Selection via Rank Correlation Prediction for Data-Efficient <fixed-case>LLM</fixed-case> Evaluation</title>
      <author><first>Gayathri</first><last>Saranathan</last></author>
      <author><first>Cong</first><last>Xu</last><affiliation>HPE</affiliation></author>
      <author><first>Mahammad Parwez</first><last>Alam</last></author>
      <author><first>Tarun</first><last>Kumar</last><affiliation>Hewlett Packard Enterprise</affiliation></author>
      <author><first>Martin</first><last>Foltin</last><affiliation>Hewlett Packard Enterprise</affiliation></author>
      <author><first>Soon Yee</first><last>Wong</last><affiliation>Hewlett Packard Enterprise</affiliation></author>
      <author><first>Suparna</first><last>Bhattacharya</last><affiliation>Hewlett Packard Enterprise</affiliation></author>
      <pages>30572-30593</pages>
      <abstract>The rapid expansion of Large Language Models (LLMs) and natural language processing datasets has made exhaustive benchmark evaluations computationally prohibitive. Inspired by high-stakes competitions like the International Mathematical Olympiad-where a few well-chosen problems suffice to differentiate top performers—we present SubLIME, which reduces evaluation costs by 80% to 99% while preserving ranking fidelity. It trains a Rank Correlation Prediction (RCP) model that combines limited performance data from only 5-20 anchor LLMs with dataset intrinsic metrics - Difficulty, Quality, and Distributional Dispersion-to predict how closely a candidate subset reflects full-benchmark rankings. Guided by these predictions, SubLIME selects a “winning” subset (1-20% of full set data) for evaluating new LLMs, preserving global rankings significant better than other data-efficient methods across ten diverse benchmarks.</abstract>
      <url hash="609899f0">2025.acl-long.1477</url>
      <bibkey>saranathan-etal-2025-sublime</bibkey>
    </paper>
    <paper id="1478">
      <title><fixed-case>M</fixed-case>³<fixed-case>GQA</fixed-case>: A Multi-Entity Multi-Hop Multi-Setting Graph Question Answering Benchmark</title>
      <author><first>Boci</first><last>Peng</last></author>
      <author><first>Yongchao</first><last>Liu</last><affiliation>Ant Group</affiliation></author>
      <author><first>Xiaohe</first><last>Bo</last></author>
      <author><first>Jiaxin</first><last>Guo</last></author>
      <author><first>Yun</first><last>Zhu</last></author>
      <author><first>Xuanbo</first><last>Fan</last><affiliation>Peking University</affiliation></author>
      <author><first>Chuntao</first><last>Hong</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yan</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <pages>30594-30620</pages>
      <abstract>Recently, GraphRAG systems have achieved remarkable progress in enhancing the performance and reliability of large language models (LLMs). However, most previous benchmarks are template-based and primarily focus on few-entity queries, which are monotypic and simplistic, failing to offer comprehensive and robust assessments. Besides, the lack of ground-truth reasoning paths also hinders the assessments of different components in GraphRAG systems. To address these limitations, we propose M³GQA, a complex, diverse, and high-quality GraphRAG benchmark focusing on multi-entity queries, with six distinct settings for comprehensive evaluation. In order to construct diverse data with semantically correct ground-truth reasoning paths, we introduce a novel reasoning-driven four-step data construction method, including tree sampling, reasoning path backtracking, query creation, and multi-stage refinement and filtering. Extensive experiments demonstrate that M³GQA effectively reflects the capabilities of GraphRAG methods, offering valuable insights into the model performance and reliability. By pushing the boundaries of current methods, M³GQA establishes a comprehensive, robust, and reliable benchmark for advancing GraphRAG research.</abstract>
      <url hash="ed4bb92f">2025.acl-long.1478</url>
      <bibkey>peng-etal-2025-m3gqa</bibkey>
    </paper>
    <paper id="1479">
      <title><fixed-case>LSSF</fixed-case>: Safety Alignment for Large Language Models through Low-Rank Safety Subspace Fusion</title>
      <author><first>Guanghao</first><last>Zhou</last></author>
      <author><first>Panjia</first><last>Qiu</last></author>
      <author><first>Cen</first><last>Chen</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Hongyu</first><last>Li</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Jason</first><last>Chu</last><affiliation>Ant Group</affiliation></author>
      <author><first>Xin</first><last>Zhang</last><affiliation>Ant Group</affiliation></author>
      <author><first>Jun</first><last>Zhou</last><affiliation>Ant Group</affiliation></author>
      <pages>30621-30638</pages>
      <abstract>The safety mechanisms of large language models (LLMs) exhibit notable fragility, as even fine-tuning on datasets without harmful content may still undermine their safety capabilities. Meanwhile, existing safety alignment methods predominantly rely on the fine-tuning process, which inadvertently leads to the increased complexity and computational resources required. To address these issues, we introduce LSSF, a novel safety re-alignment framework with Low-Rank Safety Subspace Fusison. Our proposed method exploits the low-rank characteristics of safety information in LLMs by constructing a low-rank projection matrix to extract the principal components of safety vectors. Notably, this projection matrix represents the low-rank safety subspace of the LLMs, which we have observed to remain stable during fine-tuning process and is isolated from the model’s general capabilities. These principal components are used to effectively restore safety alignment when combined with fine-tuned LLMs through linear arithmetic. Additionally, to account for the varying encoding densities of safety information across different layers of LLMs, we propose a novel metric called safety singular value entropy. This metric quantifies the encoding density and allows for the dynamic computation of the safety-critical rank for each safety vector. Extensive experiments demonstrate that our proposed post-hoc alignment method can effectively restore the safety alignment of fine-tuned models with minimal impact on their performance on downstream tasks.</abstract>
      <url hash="86a0811d">2025.acl-long.1479</url>
      <bibkey>zhou-etal-2025-lssf</bibkey>
    </paper>
    <paper id="1480">
      <title><fixed-case>ETF</fixed-case>: An Entity Tracing Framework for Hallucination Detection in Code Summaries</title>
      <author><first>Kishan</first><last>Maharaj</last></author>
      <author><first>Vitobha</first><last>Munigala</last></author>
      <author><first>Srikanth G.</first><last>Tamilselvam</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Prince</first><last>Kumar</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Sayandeep</first><last>Sen</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Palani</first><last>Kodeswaran</last></author>
      <author><first>Abhijit</first><last>Mishra</last><affiliation>University of Texas at Austin and Apple</affiliation></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last><affiliation>Indian Institute of Technology, Bombay, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <pages>30639-30652</pages>
      <abstract>Recent advancements in large language models (LLMs) have significantly enhanced their ability to understand both natural language and code, driving their use in tasks like natural language-to-code (NL2Code) and code summarisation. However, LLMs are prone to hallucination—outputs that stray from intended meanings. Detecting hallucinations in code summarisation is especially difficult due to the complex interplay between programming and natural languages. We introduce a first-of-its-kind dataset, CodeSumEval, with ~10K samples, curated specifically for hallucination detection in code summarisation. We further propose a novel Entity Tracing Framework (ETF) that a) utilises static program analysis to identify code entities from the program and b) uses LLMs to map and verify these entities and their intents within generated code summaries. Our experimental analysis demonstrates the framework’s effectiveness, leading to a 73% F1 score. The proposed approach provides a method for detecting hallucinations by tracing entities from the summary to the code, allowing us to evaluate summary accuracy and localise the error within the summary.</abstract>
      <url hash="acfcbb03">2025.acl-long.1480</url>
      <bibkey>maharaj-etal-2025-etf</bibkey>
    </paper>
    <paper id="1481">
      <title>Meta-Tool: Unleash Open-World Function Calling Capabilities of General-Purpose Large Language Models</title>
      <author><first>Shengqian</first><last>Qin</last></author>
      <author><first>Yakun</first><last>Zhu</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Linjie</first><last>Mu</last></author>
      <author><first>Shaoting</first><last>Zhang</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Xiaofan</first><last>Zhang</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <pages>30653-30677</pages>
      <abstract>Large language models (LLMs) have showcased remarkable capabilities as autonomous agents when augmented with external tools. Equipped with fixed tool sets, LLMs struggle with addressing diverse user inquiries in open-world tasks. To evaluate and boost the performance of LLMs in dealing with complex demands in the real-world, we propose open-world function calling, where LLMs need to retrieve suitable tools from a pre-defined external tool library and use retrieved tools to resolve the user’s problem. We introduce Meta-Tool, a versatile and plug-and-play tool retrieval system as the access of LLMs to external tool library. Drawing inspiration from the myriad of enhanced approaches associated with Retrieval-Augmented Generation (RAG), Meta-Tool employs a <i>hypothesize-retrieve-invoke</i> framework. We further propose Meta-Bench, a comprehensive benchmark for evaluating LLMs in open-world function calling and associated tasks. Meta-Bench encompasses 2,800 dialogues and 7,361 tools, spanning ten distinct scenarios to provide robust and diverse test categories. In conjunction, we present MT-LLaMA, a finetuned version of LLaMA-3.1, which exhibits remarkable performance improvements. Our empirical experiments reveal that Meta-Tool significantly enhances the ability of advanced LLMs to retrieve and leverage the most suitable tools compared to previous tool retrieval methods. Moreover, our fine-tuning enables even smaller-sized LLMs to achieve comparable even exceeding results to GPT-4o. Both the benchmark and the model are made publicly available at https://github.com/qinshengqian/Meta-Tool to foster further research and development in the field.</abstract>
      <url hash="ff50e4e6">2025.acl-long.1481</url>
      <bibkey>qin-etal-2025-meta</bibkey>
    </paper>
    <paper id="1482">
      <title>Benchmarking and Improving Large Vision-Language Models for Fundamental Visual Graph Understanding and Reasoning</title>
      <author><first>Yingjie</first><last>Zhu</last><affiliation>Harbin Institute of Technology (Shenzhen)</affiliation></author>
      <author><first>Xuefeng</first><last>Bai</last></author>
      <author><first>Kehai</first><last>Chen</last><affiliation>Harbin Institute of Technology (Shenzhen)</affiliation></author>
      <author><first>Yang</first><last>Xiang</last></author>
      <author><first>Jun</first><last>Yu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>30678-30701</pages>
      <abstract>Large Vision-Language Models (LVLMs) have demonstrated remarkable performance across diverse tasks. Despite great success, recent studies show that LVLMs encounter substantial limitations when engaging with visual graphs. To study the reason behind these limitations, we propose VGCure, a comprehensive benchmark covering 22 tasks for examining the fundamental graph understanding and reasoning capacities of LVLMs. Extensive evaluations conducted on 14 LVLMs reveal that LVLMs are weak in basic graph understanding and reasoning tasks, particularly those concerning relational or structurally complex information. Based on this observation, we propose a structure-aware fine-tuning framework to enhance LVLMs with structure learning abilities through three self-supervised learning tasks. Experiments validate the effectiveness of our method in improving LVLMs’ performance on fundamental and downstream graph learning tasks, as well as enhancing their robustness against complex visual graphs.</abstract>
      <url hash="10db5ad7">2025.acl-long.1482</url>
      <bibkey>zhu-etal-2025-benchmarking</bibkey>
    </paper>
    <paper id="1483">
      <title><fixed-case>ISR</fixed-case>: Self-Refining Referring Expressions for Entity Grounding</title>
      <author><first>Zhuocheng</first><last>Yu</last></author>
      <author><first>Bingchan</first><last>Zhao</last></author>
      <author><first>Yifan</first><last>Song</last></author>
      <author><first>Sujian</first><last>Li</last><affiliation>Peking University</affiliation></author>
      <author><first>Zhonghui</first><last>He</last><affiliation>Peking University</affiliation></author>
      <pages>30702-30714</pages>
      <abstract>Entity grounding, a crucial task in constructing multimodal knowledge graphs, aims to align entities from knowledge graphs with their corresponding images. Unlike conventional visual grounding tasks that use referring expressions (REs) as inputs, entity grounding relies solely on entity names and types, presenting a significant challenge. To address this, we introduce a novel **I**terative **S**elf-**R**efinement (**ISR**) scheme to enhance the multimodal large language model’s capability to generate high quality REs for the given entities as explicit contextual clues. This training scheme, inspired by human learning dynamics and human annotation processes, enables the MLLM to iteratively generate and refine REs by learning from successes and failures, guided by outcome rewards from a visual grounding model. This iterative cycle of self-refinement avoids overfitting to fixed annotations and fosters continued improvement in referring expression generation. Extensive experiments demonstrate that our methods surpasses other methods in entity grounding, highlighting its effectiveness, robustness and potential for broader applications.</abstract>
      <url hash="f122922f">2025.acl-long.1483</url>
      <bibkey>yu-etal-2025-isr</bibkey>
    </paper>
    <paper id="1484">
      <title>Activating Distributed Visual Region within <fixed-case>LLM</fixed-case>s for Efficient and Effective Vision-Language Training and Inference</title>
      <author><first>Siyuan</first><last>Wang</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Dianyi</first><last>Wang</last></author>
      <author><first>Chengxing</first><last>Zhou</last></author>
      <author><first>Zejun</first><last>Li</last></author>
      <author><first>Zhihao</first><last>Fan</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Xuanjing</first><last>Huang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Zhongyu</first><last>Wei</last><affiliation>Fudan University</affiliation></author>
      <pages>30715-30727</pages>
      <abstract>Large Vision-Language Models (LVLMs) typically learn visual capacity through visual instruction tuning, involving updates to both a projector and their LLM backbones. Inspired by the concept of a visual region in the human brain, we investigate the existence of an analogous <i>visual region</i> within LLMs that functions as a cognitive core, and explore the potential of efficient training of LVLMs via selective layers tuning. Using Bunny-Llama-3-8B-V for detailed analysis and other three LVLMs for validation across diverse visual and textual tasks, we find that selectively updating 25% of LLMs layers, when sparsely and uniformly distributed, can preserve nearly 99% of visual performance and maintain or improve textual task results, while effectively reducing training time. Based on this targeted training approach, we further propose a novel visual region-based pruning paradigm, removing non-critical layers outside the visual region, which can achieve minimal performance loss. This study offers an effective and efficient strategy for LVLM training and inference by activating a layer-wise visual region within LLMs, which proves consistently effective across different models.</abstract>
      <url hash="b9e2ec09">2025.acl-long.1484</url>
      <bibkey>wang-etal-2025-activating</bibkey>
    </paper>
    <paper id="1485">
      <title><fixed-case>CCH</fixed-case>all: A Novel Benchmark for Joint Cross-Lingual and Cross-Modal Hallucinations Detection in Large Language Models</title>
      <author><first>Yongheng</first><last>Zhang</last></author>
      <author><first>Xu</first><last>Liu</last></author>
      <author><first>Ruoxi</first><last>Zhou</last></author>
      <author><first>Qiguang</first><last>Chen</last></author>
      <author><first>Hao</first><last>Fei</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Wenpeng</first><last>Lu</last></author>
      <author><first>Libo</first><last>Qin</last><affiliation>Central South University</affiliation></author>
      <pages>30728-30749</pages>
      <abstract>Investigating hallucination issues in large language models (LLMs) within cross-lingual and cross-modal scenarios can greatly advance the large-scale deployment in real-world applications. Nevertheless, the current studies are limited to a single scenario, either cross-lingual or cross-modal, leaving a gap in the exploration of hallucinations in the joint cross-lingual and cross-modal scenarios. Motivated by this, we introduce a novel joint Cross-lingual and Cross-modal Hallucinations benchmark (CCHall) to fill this gap. Specifically, CCHall simultaneously incorporates both cross-lingual and cross-modal hallucination scenarios, which can be used to assess the cross-lingual and cross-modal capabilities of LLMs. Furthermore, we conduct a comprehensive evaluation on CCHall, exploring both mainstream open-source and closed-source LLMs. The experimental results highlight that current LLMs still struggle with CCHall. We hope CCHall can serve as a valuable resource to assess LLMs in joint cross-lingual and cross-modal scenarios.</abstract>
      <url hash="b790149f">2025.acl-long.1485</url>
      <bibkey>zhang-etal-2025-cchall</bibkey>
    </paper>
    <paper id="1486">
      <title><fixed-case>T</fixed-case>est<fixed-case>NUC</fixed-case>: Enhancing Test-Time Computing Approaches and Scaling through Neighboring Unlabeled Data Consistency</title>
      <author><first>Henry Peng</first><last>Zou</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <author><first>Zhengyao</first><last>Gu</last></author>
      <author><first>Yue</first><last>Zhou</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <author><first>Yankai</first><last>Chen</last><affiliation>Cornell University</affiliation></author>
      <author><first>Weizhi</first><last>Zhang</last><affiliation>Amazon and University of Illinois Chicago</affiliation></author>
      <author><first>Liancheng</first><last>Fang</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <author><first>Yibo</first><last>Wang</last></author>
      <author><first>Yangning</first><last>Li</last></author>
      <author><first>Kay</first><last>Liu</last><affiliation>Amazon and University of Illinois Chicago</affiliation></author>
      <author><first>Philip S.</first><last>Yu</last><affiliation>University of Illinois Chicago</affiliation></author>
      <pages>30750-30762</pages>
      <abstract>Test-time computing approaches, which leverage additional computational resources during inference, have been proven effective in enhancing large language model performance. This work introduces a novel, linearly scaling approach, TestNUC, that improves test-time predictions by leveraging the local consistency of neighboring unlabeled data-it classifies an input instance by considering not only the model’s prediction on that instance but also on neighboring unlabeled instances. We evaluate TestNUC across eight diverse datasets, spanning intent classification, topic mining, domain discovery, and emotion detection, demonstrating its consistent superiority over baseline methods such as standard prompting and self-consistency. Furthermore, TestNUC can be seamlessly integrated with existing test-time computing approaches, substantially boosting their performance. Our analysis reveals that TestNUC scales effectively with increasing amounts of unlabeled data and performs robustly across different embedding models, making it practical for real-world applications. Our code is available at https://github.com/HenryPengZou/TestNUC.</abstract>
      <url hash="aa56f679">2025.acl-long.1486</url>
      <bibkey>zou-etal-2025-testnuc</bibkey>
    </paper>
    <paper id="1487">
      <title>The Esethu Framework: Reimagining Sustainable Dataset Governance and Curation for Low-Resource Languages</title>
      <author><first>Jenalea</first><last>Rajab</last><affiliation>Lelapa AI</affiliation></author>
      <author><first>Anuoluwapo</first><last>Aremu</last></author>
      <author><first>Everlyn Asiko</first><last>Chimoto</last></author>
      <author><first>Dale</first><last>Dunbar</last><affiliation>Way With Words</affiliation></author>
      <author><first>Graham</first><last>Morrissey</last><affiliation>Way With Words</affiliation></author>
      <author><first>Fadel</first><last>Thior</last><affiliation>Lelapa AI</affiliation></author>
      <author><first>Luandrie</first><last>Potgieter</last><affiliation>Lelapa AI and University of Pretoria</affiliation></author>
      <author><first>Jessica</first><last>Ojo</last><affiliation>Lelapa AI</affiliation></author>
      <author><first>Atnafu Lambebo</first><last>Tonja</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Wilhelmina NdapewaOnyothi</first><last>Nekoto</last></author>
      <author><first>Pelonomi</first><last>Moiloa</last><affiliation>Lelapa AI</affiliation></author>
      <author><first>Jade</first><last>Abbott</last><affiliation>Retro Rabbit</affiliation></author>
      <author><first>Vukosi</first><last>Marivate</last><affiliation>University of Pretoria</affiliation></author>
      <author><first>Benjamin</first><last>Rosman</last><affiliation>University of the Witwatersrand</affiliation></author>
      <pages>30763-30776</pages>
      <abstract>This paper presents the Esethu Framework, a sustainable data curation framework specifically designed to empower local communities and ensure equitable benefit-sharing from their linguistic resource. This framework is supported by the Esethu license, a novel community-centric data license. As a proof of concept, we introduce the Vuk’uzenzele isiXhosa Speech Dataset (ViXSD), an open-source corpus developed under the Esethu Framework and License. The dataset, containing read speech from native isiXhosa speakers enriched with demographic and linguistic metadata, demonstrates how community-driven licensing and curation principles can bridge resource gaps in automatic speech recognition (ASR) for African languages while safeguarding the interests of data creators. We describe the framework guiding dataset development, outline the Esethu license provisions, present the methodology for ViXSD, and present ASR experiments validating ViXSD’s usability in building and refining voice-driven applications for isiXhosa.</abstract>
      <url hash="7620dbde">2025.acl-long.1487</url>
      <bibkey>rajab-etal-2025-esethu</bibkey>
    </paper>
    <paper id="1488">
      <title>Theoretical Analysis of Hierarchical Language Recognition and Generation by Transformers without Positional Encoding</title>
      <author><first>Daichi</first><last>Hayakawa</last></author>
      <author><first>Issei</first><last>Sato</last><affiliation>The University of Tokyo</affiliation></author>
      <pages>30777-30834</pages>
      <abstract>In this study, we provide constructive proof that Transformers can recognize and generate hierarchical language efficiently with respect to model size, even without the need for a specific positional encoding.Specifically, we show that causal masking and a starting token enable Transformers to compute positional information and depth within hierarchical structures.We demonstrate that Transformers without positional encoding can generate hierarchical languages. Furthermore, we suggest that explicit positional encoding might have a detrimental effect on generalization with respect to sequence length.</abstract>
      <url hash="b4b2982d">2025.acl-long.1488</url>
      <bibkey>hayakawa-sato-2025-theoretical</bibkey>
    </paper>
    <paper id="1489">
      <title>Less is More: Explainable and Efficient <fixed-case>ICD</fixed-case> Code Prediction with Clinical Entities</title>
      <author><first>James C.</first><last>Douglas</last><affiliation>University of Sydney, University of Sydney</affiliation></author>
      <author><first>Yidong</first><last>Gan</last><affiliation>University of Sydney, University of Sydney</affiliation></author>
      <author><first>Ben</first><last>Hachey</last><affiliation>University of Sydney, University of Sydney</affiliation></author>
      <author><first>Jonathan K.</first><last>Kummerfeld</last><affiliation>University of Sydney</affiliation></author>
      <pages>30835-30847</pages>
      <abstract>Clinical coding, assigning standardized codes to medical notes, is critical for epidemiological research, hospital planning, and reimbursement. Neural coding models generally process entire discharge summaries, which are often lengthy and contain information that is not relevant to coding. We propose an approach that combines Named Entity Recognition (NER) and Assertion Classification (AC) to filter for clinically important content before supervised code prediction. On MIMIC-IV, a standard evaluation dataset, our approach achieves near-equivalent performance to a state-of-the-art full-text baseline while using only 22% of the content and reducing training time by over half. Additionally, mapping model attention to complete entity spans yields coherent, clinically meaningful explanations, capturing coding-relevant modifiers such as acuity and laterality. We release a newly annotated NER+AC dataset for MIMIC-IV, designed specifically for ICD coding. Our entity-centric approach lays a foundation for more transparent and cost-effective assisted coding.</abstract>
      <url hash="7b5cdd59">2025.acl-long.1489</url>
      <bibkey>douglas-etal-2025-less</bibkey>
    </paper>
    <paper id="1490">
      <title>Benchmarking <fixed-case>LLM</fixed-case>s and <fixed-case>LLM</fixed-case>-based Agents in Practical Vulnerability Detection for Code Repositories</title>
      <author><first>Alperen</first><last>Yildiz</last></author>
      <author><first>Sin G</first><last>Teo</last></author>
      <author><first>Yiling</first><last>Lou</last><affiliation>Fudan University</affiliation></author>
      <author><first>Yebo</first><last>Feng</last></author>
      <author><first>Chong</first><last>Wang</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Dinil Mon</first><last>Divakaran</last><affiliation>Institute for Infocomm Research, A*STAR and National University of Singapore</affiliation></author>
      <pages>30848-30865</pages>
      <abstract>Large Language Models (LLMs) have shown promise in software vulnerability detection, particularly on function-level benchmarks like Devign and BigVul. However, real-world detection requires interprocedural analysis, as vulnerabilities often emerge through multi-hop function calls rather than isolated functions. While repository-level benchmarks like ReposVul and VulEval introduce interprocedural context, they remain computationally expensive, lack pairwise evaluation of vulnerability fixes, and explore limited context retrieval, limiting their practicality.We introduce JITVul, a JIT vulnerability detection benchmark linking each function to its vulnerability-introducing and fixing commits. Built from 879 CVEs spanning 91 vulnerability types, JITVul enables comprehensive evaluation of detection capabilities. Our results show that ReAct Agents, leveraging thought-action-observation and interprocedural context, perform better than LLMs in distinguishing vulnerable from benign code. While prompting strategies like Chain-of-Thought help LLMs, ReAct Agents require further refinement. Both methods show inconsistencies, either misidentifying vulnerabilities or over-analyzing security guards, indicating significant room for improvement.</abstract>
      <url hash="6cbdde75">2025.acl-long.1490</url>
      <bibkey>yildiz-etal-2025-benchmarking</bibkey>
    </paper>
    <paper id="1491">
      <title>Multi-Modality Expansion and Retention for <fixed-case>LLM</fixed-case>s through Parameter Merging and Decoupling</title>
      <author><first>Junlin</first><last>Li</last></author>
      <author><first>Guodong</first><last>Du</last></author>
      <author><first>Jing</first><last>Li</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Sim Kuan</first><last>Goh</last><affiliation>Xiamen University Malaysia</affiliation></author>
      <author><first>Wenya</first><last>Wang</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Yequan</first><last>Wang</last><affiliation>Beijing Academy of Artificial Intelligence</affiliation></author>
      <author><first>Fangming</first><last>Liu</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Ho-Kin</first><last>Tang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Saleh</first><last>Alharbi</last><affiliation>Shaqra University</affiliation></author>
      <author><first>Daojing</first><last>He</last></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>30866-30887</pages>
      <abstract>Fine-tuning Large Language Models (LLMs) with multimodal encoders on modality-specific data expands the modalities that LLMs can handle, leading to the formation of Multimodal LLMs (MLLMs). However, this paradigm heavily relies on resource-intensive and inflexible fine-tuning from scratch with new multimodal data. In this paper, we propose MMER (Multi-modality Expansion and Retention), a training-free approach that integrates existing MLLMs for effective multimodal expansion while retaining their original performance. Specifically, MMER reuses MLLMs’ multimodal encoders while merging their LLM parameters. By comparing original and merged LLM parameters, MMER generates binary masks to approximately separate LLM parameters for each modality. These decoupled parameters can independently process modality-specific inputs, reducing parameter conflicts and preserving original MLLMs’ fidelity. MMER can also mitigate catastrophic forgetting by applying a similar process to MLLMs fine-tuned on new tasks. Extensive experiments show significant improvements over baselines, proving that MMER effectively expands LLMs’ multimodal capabilities while retaining 99% of the original performance, and also markedly mitigates catastrophic forgetting.</abstract>
      <url hash="a4f6f8df">2025.acl-long.1491</url>
      <bibkey>li-etal-2025-multi</bibkey>
    </paper>
    <paper id="1492">
      <title>Serial Lifelong Editing via Mixture of Knowledge Experts</title>
      <author><first>YuJu</first><last>Cheng</last></author>
      <author><first>Yu-Chu</first><last>Yu</last><affiliation>National Taiwan University</affiliation></author>
      <author><first>Kai-Po</first><last>Chang</last><affiliation>NVIDIA and National Taiwan University</affiliation></author>
      <author><first>Yu-Chiang Frank</first><last>Wang</last><affiliation>NVIDIA and National Taiwan University</affiliation></author>
      <pages>30888-30903</pages>
      <abstract>It is challenging to update Large language models (LLMs) since real-world knowledge evolves. While existing Lifelong Knowledge Editing (LKE) methods efficiently update sequentially incoming edits, they often struggle to precisely overwrite the outdated knowledge with the latest one, resulting in conflicts that hinder LLMs from determining the correct answer. To address this <tex-math>\textbf{S}</tex-math>erial <tex-math>\textbf{L}</tex-math>ifelong <tex-math>\textbf{K}</tex-math>nowledge <tex-math>\textbf{E}</tex-math>diting (sLKE) problem, wepropose a novel Mixture-of-Knowledge-Experts scheme with an <tex-math>\textbf{A}</tex-math>ctivation-guided <tex-math>\textbf{R}</tex-math>outing <tex-math>\textbf{M}</tex-math>echanism (ARM), which assigns specialized experts to store domain-specific knowledge and ensures that each update completely overwrites old information with the latest data. Furthermore, we introduce a novel sLKE benchmark where answers to the same concept are updated repeatedly, to assess the ability of editing methods to refresh knowledge accurately. Experimental results on both LKE and sLKE benchmarks show that our ARM performs favorably against SOTA knowledge editing methods.</abstract>
      <url hash="38bc4dfd">2025.acl-long.1492</url>
      <bibkey>cheng-etal-2025-serial</bibkey>
    </paper>
    <paper id="1493">
      <title>A Survey on Efficient Large Language Model Training: From Data-centric Perspectives</title>
      <author><first>Junyu</first><last>Luo</last><affiliation>Peking University</affiliation></author>
      <author><first>Bohan</first><last>Wu</last></author>
      <author><first>Xiao</first><last>Luo</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Zhiping</first><last>Xiao</last><affiliation>University of Washington</affiliation></author>
      <author><first>Yiqiao</first><last>Jin</last></author>
      <author><first>Rong-Cheng</first><last>Tu</last></author>
      <author><first>Nan</first><last>Yin</last></author>
      <author><first>Yifan</first><last>Wang</last><affiliation>University of International Business and Economics</affiliation></author>
      <author><first>Jingyang</first><last>Yuan</last></author>
      <author><first>Wei</first><last>Ju</last><affiliation>Sichuan University</affiliation></author>
      <author><first>Ming</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <pages>30904-30920</pages>
      <abstract>Post-training of Large Language Models (LLMs) is crucial for unlocking their task generalization potential and domain-specific capabilities. However, the current LLM post-training paradigm faces significant data challenges, including the high costs of manual annotation and diminishing marginal returns on data scales. Therefore, achieving data-efficient post-training has become a key research question. In this paper, we present the first systematic survey of data-efficient LLM post-training from a data-centric perspective. We propose a taxonomy of data-efficient LLM post-training methods, covering data selection, data quality enhancement, synthetic data generation, data distillation and compression, and self-evolving data ecosystems. We summarize representative approaches in each category and outline future research directions. By examining the challenges in data-efficient LLM post-training, we highlight open problems and propose potential research avenues. We hope our work inspires further exploration into maximizing the potential of data utilization in large-scale model training. Paper List: https://github.com/luo-junyu/Awesome-Data-Efficient-LLM</abstract>
      <url hash="57919a81">2025.acl-long.1493</url>
      <bibkey>luo-etal-2025-survey</bibkey>
    </paper>
    <paper id="1494">
      <title><fixed-case>IMOL</fixed-case>: Incomplete-Modality-Tolerant Learning for Multi-Domain Fake News Video Detection</title>
      <author><first>Zhi</first><last>Zeng</last></author>
      <author><first>Jiaying</first><last>Wu</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Minnan</first><last>Luo</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Herun</first><last>Wan</last></author>
      <author><first>Xiangzheng</first><last>Kong</last></author>
      <author><first>Zihan</first><last>Ma</last></author>
      <author><first>Guang</first><last>Dai</last><affiliation>SGIT AI</affiliation></author>
      <author><first>Qinghua</first><last>Zheng</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <pages>30921-30933</pages>
      <abstract>While recent advances in fake news video detection have shown promising potential, existing approaches typically (1) focus on a specific domain (e.g., politics) and (2) assume the availability of multiple modalities, including video, audio, description texts, and related images. However, these methods struggle to generalize to real-world scenarios, where questionable information spans diverse domains and is often modality-incomplete due to factors such as upload degradation or missing metadata. To address these challenges, we introduce two real-world multi-domain news video benchmarks that reflect modality incompleteness and propose IMOL, an incomplete-modality-tolerant learning framework for multi-domain fake news video detection. Inspired by cognitive theories suggesting that humans infer missing modalities through cross-modal guidance and retrieve relevant knowledge from memory for reference, IMOL employs a hierarchical transferable information integration strategy. This consists of two key phases: (1) leveraging cross-modal consistency to reconstruct missing modalities and (2) refining sample-level transferable knowledge through cross-sample associative reasoning. Extensive experiments demonstrate that IMOL significantly enhances the performance and robustness of multi-domain fake news video detection while effectively generalizing to unseen domains under incomplete modality conditions.</abstract>
      <url hash="c9a5786a">2025.acl-long.1494</url>
      <bibkey>zeng-etal-2025-imol</bibkey>
    </paper>
    <paper id="1495">
      <title><fixed-case>DD</fixed-case>x<fixed-case>T</fixed-case>utor: Clinical Reasoning Tutoring System with Differential Diagnosis-Based Structured Reasoning</title>
      <author><first>Qian</first><last>Wu</last><affiliation>Chinese University of Hong Kong</affiliation></author>
      <author><first>Zheyao</first><last>Gao</last></author>
      <author><first>Longfei</first><last>Gou</last></author>
      <author><first>Qi</first><last>Dou</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>30934-30957</pages>
      <abstract>Clinical diagnosis education requires students to master both systematic reasoning processes and comprehensive medical knowledge. While recent advances in Large Language Models (LLMs) have enabled various medical educational applications, these systems often provide direct answers that could reduce students’ cognitive engagement and lead to fragmented learning. Motivated by these challenges, we propose DDxTutor, a framework that follows differential diagnosis principles to decompose clinical reasoning into teachable components. It consists of a structured reasoning module that analyzes clinical clues and synthesizes diagnostic conclusions, and an interactive dialogue framework that guides students through this process. To enable such tutoring, we construct DDxReasoning, a dataset of 933 clinical cases with fine-grained diagnostic steps verified by doctors. Our experiments demonstrate that fine-tuned LLMs achieve strong performance in generating structured teaching references and conducting interactive diagnostic tutoring dialogues. Human evaluation by medical educators and students validates the framework’s potential and effectiveness for clinical diagnosis education. Our project is available at https://github.com/med-air/DDxTutor.</abstract>
      <url hash="7699da18">2025.acl-long.1495</url>
      <bibkey>wu-etal-2025-ddxtutor</bibkey>
    </paper>
    <paper id="1496">
      <title><fixed-case>S</fixed-case>ocial<fixed-case>E</fixed-case>val: Evaluating Social Intelligence of Large Language Models</title>
      <author><first>Jinfeng</first><last>Zhou</last></author>
      <author><first>Yuxuan</first><last>Chen</last></author>
      <author><first>Yihan</first><last>Shi</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Xuanming</first><last>Zhang</last></author>
      <author><first>Leqi</first><last>Lei</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Yi</first><last>Feng</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Zexuan</first><last>Xiong</last></author>
      <author><first>Miao</first><last>Yan</last></author>
      <author><first>Xunzhi</first><last>Wang</last><affiliation>Nankai University</affiliation></author>
      <author><first>Yaru</first><last>Cao</last><affiliation>Northwest Minzu University</affiliation></author>
      <author><first>Jianing</first><last>Yin</last></author>
      <author><first>Shuai</first><last>Wang</last></author>
      <author><first>Quanyu</first><last>Dai</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Zhenhua</first><last>Dong</last></author>
      <author><first>Hongning</first><last>Wang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <pages>30958-31012</pages>
      <abstract>LLMs exhibit promising Social Intelligence (SI) in modeling human behavior, raising the need to evaluate LLMs’ SI and their discrepancy with humans. SI equips humans with interpersonal abilities to behave wisely in navigating social interactions to achieve social goals. This presents an operational evaluation paradigm: outcome-oriented goal achievement evaluation and process-oriented interpersonal ability evaluation, which existing work fails to address. To this end, we propose SocialEval, a script-based bilingual SI benchmark, integrating outcome- and process-oriented evaluation by manually crafting narrative scripts. Each script is structured as a world tree that contains plot lines driven by interpersonal ability, providing a comprehensive view of how LLMs navigate social interactions. Experiments show that LLMs fall behind humans on both SI evaluations, exhibit prosociality, and prefer more positive social behaviors, even if they lead to goal failure. Analysis of LLMs’ formed representation space and neuronal activations reveals that LLMs have developed ability-specific functional partitions akin to the human brain.</abstract>
      <url hash="58a51ca7">2025.acl-long.1496</url>
      <bibkey>zhou-etal-2025-socialeval</bibkey>
    </paper>
    <paper id="1497">
      <title>Hidden in Plain Sight: Evaluation of the Deception Detection Capabilities of <fixed-case>LLM</fixed-case>s in Multimodal Settings</title>
      <author><first>Md Messal Monem</first><last>Miah</last></author>
      <author><first>Adrita</first><last>Anika</last><affiliation>Amazon</affiliation></author>
      <author><first>Xi</first><last>Shi</last></author>
      <author><first>Ruihong</first><last>Huang</last><affiliation>Texas A&amp;M University</affiliation></author>
      <pages>31013-31034</pages>
      <abstract>Detecting deception in an increasingly digital world is both a critical and challenging task. In this study, we present a comprehensive evaluation of the automated deception detection capabilities of Large Language Models (LLMs) and Large Multimodal Models (LMMs) across diverse domains. We assess the performance of both open-source and proprietary LLMs on three distinct datasets—real-life trial interviews (RLTD), instructed deception in interpersonal scenarios (MU3D), and deceptive reviews (OpSpam). We systematically analyze the effectiveness of different experimental setups for deception detection, including zero-shot and few-shot approaches with random or similarity-based in-context example selection. Our findings indicate that fine-tuned LLMs achieve state-of-the-art performance on textual deception detection, whereas LMMs struggle to fully leverage multimodal cues, particularly in real-world settings. Additionally, we analyze the impact of auxiliary features, such as non-verbal gestures, video summaries, and evaluate the effectiveness of different promptingstrategies, such as direct label generation and post-hoc reasoning generation. Experiments unfold that reasoning-based predictions do not consistently improve performance over direct classification, contrary to the expectations.</abstract>
      <url hash="0621602b">2025.acl-long.1497</url>
      <bibkey>miah-etal-2025-hidden</bibkey>
    </paper>
    <paper id="1498">
      <title>Analyzing and Mitigating Inconsistency in Discrete Speech Tokens for Neural Codec Language Models</title>
      <author><first>Wenrui</first><last>Liu</last></author>
      <author><first>Zhifang</first><last>Guo</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Jin</first><last>Xu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yuanjun</first><last>Lv</last></author>
      <author><first>Yunfei</first><last>Chu</last></author>
      <author><first>Zemin</first><last>Liu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Junyang</first><last>Lin</last><affiliation>Alibaba Group</affiliation></author>
      <pages>31035-31046</pages>
      <abstract>Building upon advancements in Large Language Models (LLMs), the field of audio processing has seen increased interest in training speech generation tasks with discrete speech token sequences. However, directly discretizing speech by neural audio codecs often results in sequences that fundamentally differ from text sequences. Unlike text, where text token sequences are deterministic, discrete speech tokens can exhibit significant variability based on contextual factors, while still producing perceptually identical audio segments. We refer to this phenomenon as Discrete Representation Inconsistency (DRI). This inconsistency can lead to a single speech segment being represented by multiple divergent sequences, which creates confusion in neural codec language models and results in poor generated speech. In this paper, we quantitatively analyze the DRI phenomenon within popular audio tokenizers such as EnCodec. Our approach effectively mitigates the DRI phenomenon of the neural audio codec. Furthermore, extensive experiments on the neural codec language model over LibriTTS and large-scale MLS dataset (44,000 hours) demonstrate the effectiveness and generality of our method. The demo of audio samples is available at https://consistencyinneuralcodec.github.io.</abstract>
      <url hash="adf507aa">2025.acl-long.1498</url>
      <bibkey>liu-etal-2025-analyzing</bibkey>
    </paper>
    <paper id="1499">
      <title><fixed-case>P</fixed-case>lanning<fixed-case>A</fixed-case>rena: A Modular Benchmark for Multidimensional Evaluation of Planning and Tool Learning</title>
      <author><first>Zihan</first><last>Zheng</last><affiliation>South China Normal University</affiliation></author>
      <author><first>Tianle</first><last>Cui</last></author>
      <author><first>Chuwen</first><last>Xie</last><affiliation>South China Normal University</affiliation></author>
      <author><first>Jiahui</first><last>Pan</last><affiliation>South China Normal University</affiliation></author>
      <author><first>Qianglong</first><last>Chen</last></author>
      <author><first>Lewei</first><last>He</last></author>
      <pages>31047-31086</pages>
      <abstract>One of the research focuses of large language models (LLMs) is the ability to generate action plans. Recent studies have revealed that the performance of LLMs can be significantly improved by integrating external tools. Based on this, we propose a benchmark framework called PlanningArena, which aims to simulate real application scenarios and provide a series of apps and API tools that may be involved in the actual planning process. This framework adopts a modular task structure and combines user portrait analysis to evaluate the ability of LLMs in correctly selecting tools, logical reasoning in complex scenarios, and parsing user information. In addition, we deeply diagnose the task execution effect of LLMs from both macro and micro levels. The experimental results show that even the most outstanding GPT-4o and DeepSeekV3 models only achieved a total score of 56.5% and 41.9% in PlanningArena, respectively, indicating that current LLMs still face challenges in logical reasoning, context memory, and tool calling when dealing with different structures, scenarios, and their complexity. Through this benchmark, we further explore the path to optimize LLMs to perform planning tasks.</abstract>
      <url hash="849b30a1">2025.acl-long.1499</url>
      <bibkey>zheng-etal-2025-planningarena</bibkey>
    </paper>
    <paper id="1500">
      <title><fixed-case>F</fixed-case>ocus<fixed-case>LLM</fixed-case>: Precise Understanding of Long Context by Dynamic Condensing</title>
      <author><first>Zhenyu</first><last>Li</last></author>
      <author><first>Yike</first><last>Zhang</last></author>
      <author><first>Tengyu</first><last>Pan</last></author>
      <author><first>Yutao</first><last>Sun</last></author>
      <author><first>Zhichao</first><last>Duan</last></author>
      <author><first>Junjie</first><last>Fang</last></author>
      <author><first>Rong</first><last>Han</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Zixuan</first><last>Wang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Jianyong</first><last>Wang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>31087-31101</pages>
      <abstract>Empowering LLMs with the ability to precisely understand long contexts is crucial for many downstream applications. However, handling long contexts with conventional transformer architecture requires substantial training and inference resources. Existing context condensing methods cannot accurately understand the full context, as there is a considerable amount of information loss in the condensing process. To address these issues, we present **FocusLLM**, a framework designed to extend the fixed context length of any decoder-only LLM, allowing the model to focus on relevant information from very long sequences. FocusLLM first divides long text input into chunks based on the model’s original context length. It then employs the **_dynamic condensing_** process to distill crucial information from each chunk. Ultimately, through the novel **_parallel decoding_** mechanism, FocusLLM can integrate the extracted information into its local context. FocusLLM stands out for great training efficiency and versatility: trained with an 8K input length and with much less training cost than previous methods, FocusLLM exhibits superior performance across downstream tasks and maintains strong language modeling ability when handling extensive long texts, even up to 400K tokens. Our code is available at https://github.com/leezythu/FocusLLM.</abstract>
      <url hash="3e961a54">2025.acl-long.1500</url>
      <bibkey>li-etal-2025-focusllm</bibkey>
    </paper>
    <paper id="1501">
      <title>Negative Matters: Multi-Granularity Hard-Negative Synthesis and Anchor-Token-Aware Pooling for Enhanced Text Embeddings</title>
      <author><first>Tengyu</first><last>Pan</last></author>
      <author><first>Zhichao</first><last>Duan</last></author>
      <author><first>Zhenyu</first><last>Li</last></author>
      <author><first>Bowen</first><last>Dong</last><affiliation>Tsinghua University, Tsinghua University and Tencent AI Lab</affiliation></author>
      <author><first>Ning</first><last>Liu</last><affiliation>Shandong University</affiliation></author>
      <author><first>Xiuxing</first><last>Li</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Jianyong</first><last>Wang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>31102-31118</pages>
      <abstract>Text embedding models are essential for various natural language processing tasks, enabling the effective encoding of semantic information into dense vector representations. These models are typically optimized using triplets of (query, positive, negative) data pairs for contrastive learning, where the negative samples play a critical role in enhancing the model’s ability to discern subtle semantic distinctions. In this work, we introduce a **M**ulti-**G**ranularity **H**ard-negative (MGH) synthesis framework that leverages large language models (LLMs) to generate diverse negative samples with varying levels of similarity with the query. This approach facilitates a coarse-to-fine curriculum learning strategy during supervised training, allowing the embedding model to progressively learn more nuanced semantic representations. Meanwhile, we propose an **A**nchor **T**oken **A**ware (ATA) pooling method that assigns higher weights to anchor tokens based on aggregation patterns observed in LLMs, improving text embedding accuracy without increasing model complexity. Comprehensive experiments on the MTEB benchmark demonstrate that our methods achieve state-of-the-art performance, surpassing existing synthesis strategies both with synthetic data and when combined with public retrieval datasets.</abstract>
      <url hash="c83761cc">2025.acl-long.1501</url>
      <bibkey>pan-etal-2025-negative</bibkey>
    </paper>
    <paper id="1502">
      <title><fixed-case>GPT</fixed-case>-4 as a Homework Tutor Can Improve Student Engagement and Learning Outcomes</title>
      <author><first>Alessandro</first><last>Vanzo</last></author>
      <author><first>Sankalan</first><last>Pal Chowdhury</last></author>
      <author><first>Mrinmaya</first><last>Sachan</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <pages>31119-31136</pages>
      <abstract>This work contributes to the scarce empirical literature on LLM-based interactive homework in real-world educational settings and offers a practical, scalable solution to improve homework in schools. Homework is an important part of education in schools across the world, but to maximize benefit, it must be accompanied by feedback and follow-up questions. We developed a prompting strategy that enables GPT-4 to conduct interactive homework sessions for high school students learning English as a second language. Our strategy requires minimal effort in content preparation, one of the key challenges of alternatives such as home tutors or ITSs. We carried out a Randomized Controlled Trial (RCT) in four high-school classes, replacing traditional homework with GPT-4 homework sessions for the treatment group. We found that the treatment group had higher levels of satisfaction and desire to keep using the system among the students. This occurred without compromising learning outcomes, and one group even showed significantly better learning gains.</abstract>
      <url hash="0c684f6e">2025.acl-long.1502</url>
      <bibkey>vanzo-etal-2025-gpt</bibkey>
    </paper>
    <paper id="1503">
      <title>Diffusion Models Through a Global Lens: Are They Culturally Inclusive?</title>
      <author><first>Zahra</first><last>Bayramli</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Ayhan</first><last>Suleymanzade</last></author>
      <author><first>Na Min</first><last>An</last><affiliation>Copenhagen University and KAIST</affiliation></author>
      <author><first>Huzama</first><last>Ahmad</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Eunsu</first><last>Kim</last></author>
      <author><first>Junyeong</first><last>Park</last></author>
      <author><first>James</first><last>Thorne</last><affiliation>KAIST</affiliation></author>
      <author><first>Alice</first><last>Oh</last><affiliation>Google and Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>31137-31155</pages>
      <abstract>Text-to-image diffusion models have recently enabled the creation of visually compelling, detailed images from textual prompts. However, their ability to accurately represent various cultural nuances remains an open question. In our work, we introduce CULTDIFF benchmark, evaluating whether state-of-the-art diffusion models can generate culturally specific images spanning ten countries. We show that these models often fail to generate cultural artifacts in architecture, clothing, and food, especially for underrepresented country regions, by conducting a fine-grained analysis of different similarity aspects, revealing significant disparities in cultural relevance, description fidelity, and realism compared to real-world reference images. With the collected human evaluations, we develop a neural-based image-image similarity metric, namely, CULTDIFF-S, to predict human judgment on real and generated images with cultural artifacts. Our work highlights the need for more inclusive generative AI systems and equitable dataset representation over a wide range of cultures.</abstract>
      <url hash="7f53bbaa">2025.acl-long.1503</url>
      <bibkey>bayramli-etal-2025-diffusion</bibkey>
    </paper>
    <paper id="1504">
      <title>Efficient Safety Alignment of Large Language Models via Preference Re-ranking and Representation-based Reward Modeling</title>
      <author><first>Deng</first><last>Qiyuan</last></author>
      <author><first>Xuefeng</first><last>Bai</last></author>
      <author><first>Kehai</first><last>Chen</last><affiliation>Harbin Institute of Technology (Shenzhen)</affiliation></author>
      <author><first>Yaowei</first><last>Wang</last><affiliation>Harbin Institute of Technology, Shenzhen and Pengcheng Laboratory</affiliation></author>
      <author><first>Liqiang</first><last>Nie</last><affiliation>Harbin Institute of Technology (Shenzhen) and Shandong University</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>31156-31171</pages>
      <abstract>Reinforcement Learning (RL) algorithms for safety alignment of Large Language Models (LLMs), such as Direct Preference Optimization (DPO), encounter the challenge of distribution shift. Current approaches typically address this issue through online sampling from the target policy, which requires significant computational resources.In this paper, we hypothesize that during off-policy training, while the ranking order of output generated by policy changes, their overall distribution remains relatively stable.This stability allows the conversion of the sampling process from the target policy into a computationallyefficient re-ranking of preference data.Building on this hypothesis, we propose a new framework that leverages the model’s intrinsic safety judgment capability to extract reward signals, which are then used to calculate label confidence for preference reordering. Extensive experiments and theoretical analysis demonstrate that the proposed method effectively addresses the distribution shift issue, remarkably enhancing the safety performance while avoiding about 300x computational overheads.</abstract>
      <url hash="9b2fe829">2025.acl-long.1504</url>
      <bibkey>qiyuan-etal-2025-efficient</bibkey>
    </paper>
    <paper id="1505">
      <title><fixed-case>E</fixed-case>nglish-based acoustic models perform well in the forced alignment of two <fixed-case>E</fixed-case>nglish-based Pacific Creoles</title>
      <author><first>Sam</first><last>Passmore</last></author>
      <author><first>Lila San</first><last>Roque</last></author>
      <author><first>Kirsty</first><last>Gillespie</last><affiliation>Australian National University</affiliation></author>
      <author><first>Saurabh</first><last>Nath</last></author>
      <author><first>Kira</first><last>Davey</last></author>
      <author><first>Keira</first><last>Mullan</last><affiliation>Australian National University</affiliation></author>
      <author><first>Tim</first><last>Cawley</last></author>
      <author><first>Jennifer</first><last>Biggs</last></author>
      <author><first>Rosey</first><last>Billington</last><affiliation>Australian National University</affiliation></author>
      <author><first>Bethwyn</first><last>Evans</last><affiliation>Australian National University</affiliation></author>
      <author><first>Nick</first><last>Thieberger</last><affiliation>University of Melbourne</affiliation></author>
      <author><first>Danielle</first><last>Barth</last><affiliation>Australian National University</affiliation></author>
      <pages>31172-31183</pages>
      <abstract>Expanding the breadth languages used to study sociophonetic variation and change is an important step in the theoretical development of sociophonetics. As data archives grow, forced alignment can accelerate the study of sociophonetic variation in minority languages. This paper examines the application of English and custom-made acoustic models on the alignment of vowels in two Pacific Creoles, Tok Pisin (59 hours) and Bislama (38.5 hours). We find that English models perform acceptably well in both languages, and as well as humans in vowel environments described as ‘Highly Reliable’. Custom models performed better in Bislama than Tok Pisin. We end the paper with recommendations on the use of cross-linguistic acoustic models in the case of English-Based Creoles.</abstract>
      <url hash="ad93a292">2025.acl-long.1505</url>
      <bibkey>passmore-etal-2025-english</bibkey>
    </paper>
    <paper id="1506">
      <title>Subtle Errors in Reasoning: Preference Learning via Error-injected Self-editing</title>
      <author><first>Kaishuai</first><last>Xu</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Tiezheng</first><last>Yu</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Wenjun</first><last>Hou</last></author>
      <author><first>Yi</first><last>Cheng</last></author>
      <author><first>Chak Tou</first><last>Leong</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Liangyou</first><last>Li</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Xin</first><last>Jiang</last></author>
      <author><first>Lifeng</first><last>Shang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Qun</first><last>Liu</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Wenjie</first><last>Li</last><affiliation>The Hong Kong Polytechnic University, The Hong Kong Polytechnic University</affiliation></author>
      <pages>31184-31203</pages>
      <abstract>Large Language Models (LLMs) have exhibited strong mathematical reasoning prowess, tackling tasks ranging from basic arithmetic to advanced competition-level problems. However, frequently occurring subtle yet critical errors, such as miscalculations or incorrect substitutions, limit the LLMs’ full potential. Existing studies to improve mathematical ability typically involve applying preference learning to step-wise solution pairs. Although these methods leverage samples of varying granularity to mitigate reasoning errors, they overlook critical subtle errors. In this work, we propose a novel preference learning framework called eRror-Injected Self-Editing (RISE), which injects predefined subtle errors into pivotal tokens in reasoning or computation steps to construct hard pairs for error mitigation. In detail, RISE uses the LLM itself to edit a small number of tokens in the solution, injecting designed subtle errors. Then, pairs composed of self-edited solutions and their corresponding correct ones, along with pairs of correct and incorrect solutions obtained through sampling, are used together for subtle error-aware DPO training. Compared with other preference learning methods, RISE further refines the training objective without requiring fine-grained sampling or preference annotation. Extensive experiments validate the effectiveness of RISE, with preference learning on Qwen2-7B-Instruct yielding notable improvements of 3.0% on GSM8K and 7.9% on MATH with only 4.5K training samples. Moreover, the effect of error mitigation extends from mathematical reasoning to logical reasoning and code generation.</abstract>
      <url hash="fb229fa3">2025.acl-long.1506</url>
      <bibkey>xu-etal-2025-subtle</bibkey>
    </paper>
    <paper id="1507">
      <title>Truth Knows No Language: Evaluating Truthfulness Beyond <fixed-case>E</fixed-case>nglish</title>
      <author><first>Blanca</first><last>Calvo Figueras</last><affiliation>Universidad del País Vasco</affiliation></author>
      <author><first>Eneko</first><last>Sagarzazu</last><affiliation>Universidad del País Vasco</affiliation></author>
      <author><first>Julen</first><last>Etxaniz</last><affiliation>HiTZ Center, University of the Basque Country (UPV/EHU)</affiliation></author>
      <author><first>Jeremy</first><last>Barnes</last><affiliation>University of the Basque Country</affiliation></author>
      <author><first>Pablo</first><last>Gamallo</last><affiliation>Universidad de Santiago de Compostela</affiliation></author>
      <author><first>Iria</first><last>de-Dios-Flores</last></author>
      <author><first>Rodrigo</first><last>Agerri</last><affiliation>University of the Basque Country</affiliation></author>
      <pages>31204-31218</pages>
      <abstract>We introduce a professionally translated extension of the TruthfulQA benchmark designed to evaluate truthfulness in Basque, Catalan, Galician, and Spanish. Truthfulness evaluations of large language models (LLMs) have primarily been focused on English. However, the ability of LLMs to maintain truthfulness across languages remains under-explored. Our study evaluates 12 state-of-the-art open LLMs, comparing base and instruction-tuned models using human evaluation, multiple-choice metrics, and LLM-as-a-Judge scoring. Our findings reveal that, while LLMs perform best in English and worst in Basque (the lowest-resourced language), overall truthfulness discrepancies across languages are smaller than anticipated. Furthermore, we show that LLM-as-a-Judge correlates more closely with human judgments than multiple-choice metrics, and that informativeness plays a critical role in truthfulness assessment. Our results also indicate that machine translation provides a viable approach for extending truthfulness benchmarks to additional languages, offering a scalable alternative to professional translation. Finally, we observe that universal knowledge questions are better handled across languages than context- and time-dependent ones, highlighting the need for truthfulness evaluations that account for cultural and temporal variability. Datasets, models and code are publicly available under open licenses.</abstract>
      <url hash="91fd6715">2025.acl-long.1507</url>
      <bibkey>calvo-figueras-etal-2025-truth</bibkey>
    </paper>
    <paper id="1508">
      <title>Revisiting Compositional Generalization Capability of Large Language Models Considering Instruction Following Ability</title>
      <author><first>Yusuke</first><last>Sakai</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Hidetaka</first><last>Kamigaito</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <pages>31219-31238</pages>
      <abstract>In generative commonsense reasoning tasks such as CommonGen, generative large language models (LLMs) compose sentences that include all given concepts. However, when focusing on instruction-following capabilities, if a prompt specifies a concept order, LLMs must generate sentences that adhere to the specified order. To address this, we propose Ordered CommonGen, a benchmark designed to evaluate the compositional generalization and instruction-following abilities of LLMs. This benchmark measures ordered coverage to assess whether concepts are generated in the specified order, enabling a simultaneous evaluation of both abilities. We conducted a comprehensive analysis using 36 LLMs and found that, while LLMs generally understand the intent of instructions, biases toward specific concept order patterns often lead to low-diversity outputs or identical results even when the concept order is altered. Moreover, even the most instruction-compliant LLM achieved only about 75% ordered coverage, highlighting the need for improvements in both instruction-following and compositional generalization capabilities.</abstract>
      <url hash="ab72a3e6">2025.acl-long.1508</url>
      <bibkey>sakai-etal-2025-revisiting</bibkey>
    </paper>
    <paper id="1509">
      <title>Batayan: A <fixed-case>F</fixed-case>ilipino <fixed-case>NLP</fixed-case> benchmark for evaluating Large Language Models</title>
      <author><first>Jann Railey</first><last>Montalan</last><affiliation>AI Singapore and Ateneo de Manila University</affiliation></author>
      <author><first>Jimson Paulo</first><last>Layacan</last></author>
      <author><first>David Demitri</first><last>Africa</last></author>
      <author><first>Richell Isaiah S.</first><last>Flores</last><affiliation>Ateneo de Manila University</affiliation></author>
      <author><first>Michael T. Lopez</first><last>Ii</last></author>
      <author><first>Theresa Denise</first><last>Magsajo</last><affiliation>Ateneo de Manila University</affiliation></author>
      <author><first>Anjanette</first><last>Cayabyab</last><affiliation>Ateneo de Manila University</affiliation></author>
      <author><first>William Chandra</first><last>Tjhi</last><affiliation>AI Singapore</affiliation></author>
      <pages>31239-31273</pages>
      <abstract>Recent advances in large language models (LLMs) have demonstrated remarkable capabilities on widely benchmarked high-resource languages. However, linguistic nuances of under-resourced languages remain unexplored. We introduce Batayan, a holistic Filipino benchmark that systematically evaluates LLMs across three key natural language processing (NLP) competencies: understanding, reasoning, and generation. Batayan consolidates eight tasks, three of which have not existed prior for Filipino corpora, covering both Tagalog and code-switched Taglish utterances. Our rigorous, native-speaker-driven adaptation and validation processes ensures fluency and authenticity to the complex morphological and syntactic structures of Filipino, alleviating the pervasive translationese bias in existing Filipino corpora. We report empirical results on a variety of open-source and commercial LLMs, highlighting significant performance gaps that signal the under-representation of Filipino in pre-training corpora, the unique hurdles in modeling Filipino’s rich morphology and construction, and the importance of explicit Filipino language support. Moreover, we discuss the practical challenges encountered in dataset construction and propose principled solutions for building culturally and linguistically-faithful resources in under-represented languages. We also provide a public evaluation suite as a clear foundation for iterative, community-driven progress in Filipino NLP.</abstract>
      <url hash="6e106f15">2025.acl-long.1509</url>
      <bibkey>montalan-etal-2025-batayan</bibkey>
    </paper>
    <paper id="1510">
      <title><fixed-case>H</fixed-case>ints<fixed-case>O</fixed-case>f<fixed-case>T</fixed-case>ruth: A Multimodal Checkworthiness Detection Dataset with Real and Synthetic Claims</title>
      <author><first>Michiel</first><last>Van Der Meer</last></author>
      <author><first>Pavel</first><last>Korshunov</last><affiliation>Idiap research institute</affiliation></author>
      <author><first>Sébastien</first><last>Marcel</last><affiliation>Université de Lausanne and Idiap Research Institute</affiliation></author>
      <author><first>Lonneke Van Der</first><last>Plas</last><affiliation>Universita della Svizzera Italiana</affiliation></author>
      <pages>31274-31291</pages>
      <abstract>Misinformation can be countered with fact-checking, but the process is costly and slow. Identifying checkworthy claims is the first step, where automation can help scale fact-checkers’ efforts. However, detection methods struggle with content that is (1) multimodal, (2) from diverse domains, and (3) synthetic. We introduce HintsOfTruth, a public dataset for multimodal checkworthiness detection with 27K real-world and synthetic image/claim pairs. The mix of real and synthetic data makes this dataset unique and ideal for benchmarking detection methods. We compare fine-tuned and prompted Large Language Models (LLMs). We find that well-configured lightweight text-based encoders perform comparably to multimodal models but the former only focus on identifying non-claim-like content. Multimodal LLMs can be more accurate but come at a significant computational cost, making them impractical for large-scale applications. When faced with synthetic data, multimodal models perform more robustly.</abstract>
      <url hash="0c814ba4">2025.acl-long.1510</url>
      <bibkey>van-der-meer-etal-2025-hintsoftruth</bibkey>
    </paper>
    <paper id="1511">
      <title><fixed-case>C</fixed-case>ity<fixed-case>N</fixed-case>av<fixed-case>A</fixed-case>gent: Aerial Vision-and-Language Navigation with Hierarchical Semantic Planning and Global Memory</title>
      <author><first>Weichen</first><last>Zhang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Chen</first><last>Gao</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Shiquan</first><last>Yu</last></author>
      <author><first>Ruiying</first><last>Peng</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Baining</first><last>Zhao</last></author>
      <author><first>Qian</first><last>Zhang</last></author>
      <author><first>Jinqiang</first><last>Cui</last><affiliation>Pengcheng Laboratory</affiliation></author>
      <author><first>Xinlei</first><last>Chen</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Yong</first><last>Li</last></author>
      <pages>31292-31309</pages>
      <abstract>Aerial vision-and-language navigation (VLN) — requiring drones to interpret natural language instructions and navigate complex urban environments — emerges as a critical embodied AI challenge that bridges human-robot interaction, 3D spatial reasoning, and real-world deployment. Although existing ground VLN agents achieved notable results in indoor and outdoor settings, they struggle in aerial VLN due to the absence of predefined navigation graphs and the exponentially expanding action space in long-horizon exploration. In this work, we propose <b>CityNavAgent</b>, a large language model (LLM)-empowered agent that significantly reduces the navigation complexity for urban aerial VLN. Specifically, we design a hierarchical semantic planning module (HSPM) that decomposes the long-horizon task into sub-goals with different semantic levels. The agent reaches the target progressively by achieving sub-goals with different capacities of the LLM. Additionally, a global memory module storing historical trajectories into a topological graph is developed to simplify navigation for visited targets. Extensive benchmark experiments show that our method achieves state-of-the-art performance with significant improvement. Further experiments demonstrate the effectiveness of different modules of CityNavAgent for aerial VLN in continuous city environments.</abstract>
      <url hash="da82d56a">2025.acl-long.1511</url>
      <bibkey>zhang-etal-2025-citynavagent</bibkey>
    </paper>
    <paper id="1512">
      <title>It’s Not a Walk in the Park! Challenges of Idiom Translation in Speech-to-text Systems</title>
      <author><first>Iuliia</first><last>Zaitova</last></author>
      <author><first>Badr M.</first><last>Abdullah</last></author>
      <author><first>Wei</first><last>Xue</last></author>
      <author><first>Dietrich</first><last>Klakow</last></author>
      <author><first>Bernd</first><last>Möbius</last><affiliation>Universität des Saarlandes</affiliation></author>
      <author><first>Tania</first><last>Avgustinova</last></author>
      <pages>31310-31322</pages>
      <abstract>Idioms are defined as a group of words with a figurative meaning not deducible from their individual components. Although modern machine translation systems have made remarkable progress, translating idioms remains a major challenge, especially for speech-to-text systems, where research on this topic is notably sparse. In this paper, we systematically evaluate idiom translation as compared to conventional news translation in both text-to-text machine translation (MT) and speech-to-text translation (SLT) systems across two language pairs (German to English, Russian to English). We compare state-of-the-art end-to-end SLT systems (SeamlessM4T SLT-to-text, Whisper Large v3) with MT systems (SeamlessM4T SLT-to-text, No Language Left Behind), Large Language Models (DeepSeek, LLaMA) and cascaded alternatives. Our results reveal that SLT systems experience a pronounced performance drop on idiomatic data, often reverting to literal translations even in higher layers, whereas MT systems and Large Language Models demonstrate better handling of idioms. These findings underscore the need for idiom-specific strategies and improved internal representations in SLT architectures.</abstract>
      <url hash="c9bd7766">2025.acl-long.1512</url>
      <bibkey>zaitova-etal-2025-walk</bibkey>
    </paper>
    <paper id="1513">
      <title><fixed-case>P</fixed-case>oly<fixed-case>N</fixed-case>arrative: A Multilingual, Multilabel, Multi-domain Dataset for Narrative Extraction from News Articles</title>
      <author><first>Nikolaos</first><last>Nikolaidis</last></author>
      <author><first>Nicolas</first><last>Stefanovitch</last><affiliation>European Commission</affiliation></author>
      <author><first>Purificação</first><last>Silvano</last><affiliation>Universidade do Porto</affiliation></author>
      <author><first>Dimitar Iliyanov</first><last>Dimitrov</last></author>
      <author><first>Roman</first><last>Yangarber</last><affiliation>University of Helsinki</affiliation></author>
      <author><first>Nuno</first><last>Guimarães</last><affiliation>INESC TEC</affiliation></author>
      <author><first>Elisa</first><last>Sartori</last></author>
      <author><first>Ion</first><last>Androutsopoulos</last><affiliation>Athens University of Economics and Business</affiliation></author>
      <author><first>Preslav</first><last>Nakov</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Giovanni</first><last>Da San Martino</last><affiliation>University of Padua</affiliation></author>
      <author><first>Jakub</first><last>Piskorski</last></author>
      <pages>31323-31345</pages>
      <abstract>We present polyNarrative, a new multilingual dataset of news articles, annotated for narratives. Narratives are overt or implicit claims, recurring across articles and languages, promoting a specific interpretation or viewpoint on an ongoing topic, often propagating mis/disinformation. We developed two-level taxonomies with coarse- and fine-grained narrative labels for two domains: (i) climate change and (ii) the military conflict between Ukraine and Russia. We collected news articles in four languages (Bulgarian, English, Portuguese, and Russian) related to the two domains and manually annotated them at the paragraph level. We make the dataset publicly available, along with experimental results of several strong baselines that assign narrative labels to news articles at the paragraph or the document level. We believe that this dataset will foster research in narrative detection and enable new research directions towards more multi-domain and highly granular narrative related tasks.</abstract>
      <url hash="40897ae3">2025.acl-long.1513</url>
      <bibkey>nikolaidis-etal-2025-polynarrative</bibkey>
    </paper>
    <paper id="1514">
      <title>A Parameter-Efficient and Fine-Grained Prompt Learning for Vision-Language Models</title>
      <author><first>Yongbin</first><last>Guo</last></author>
      <author><first>Shuzhen</first><last>Li</last></author>
      <author><first>Zhulin</first><last>Liu</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Tong</first><last>Zhang</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>C.L.Philip</first><last>Chen</last><affiliation>South China University of Technology</affiliation></author>
      <pages>31346-31359</pages>
      <abstract>Current vision-language models (VLMs) understand complex vision-text tasks by extracting overall semantic information from large-scale cross-modal associations. However, extracting from large-scale cross-modal associations often smooths out semantic details and requires large computations, limiting multimodal fine-grained understanding performance and efficiency. To address this issue, this paper proposes a detail-oriented prompt learning (DoPL) method for vision-language models to implement fine-grained multi-modal semantic alignment with merely 0.25M trainable parameters. According to the low-entropy information concentration theory, DoPL explores shared interest tokens from text-vision correlations and transforms them into alignment weights to enhance text prompt and vision prompt via detail-oriented prompt generation. It effectively guides the current frozen layer to extract fine-grained text-vision alignment cues. Furthermore, DoPL constructs detail-oriented prompt generation for each frozen layer to implement layer-by-layer localization of fine-grained semantic alignment, achieving precise understanding in complex vision-text tasks. DoPL performs well in parameter-efficient fine-grained semantic alignment with only 0.12% tunable parameters for vision-language models. The state-of-the-art results over the previous parameter-efficient fine-tuning methods and full fine-tuning approaches on six benchmarks demonstrate the effectiveness and efficiency of DoPL in complex multi-modal tasks.</abstract>
      <url hash="51fdf01e">2025.acl-long.1514</url>
      <bibkey>guo-etal-2025-parameter</bibkey>
    </paper>
    <paper id="1515">
      <title>Persona Dynamics: Unveiling the Impact of Persona Traits on Agents in Text-Based Games</title>
      <author><first>Seungwon</first><last>Lim</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Seungbeen</first><last>Lee</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Dongjun</first><last>Min</last></author>
      <author><first>Youngjae</first><last>Yu</last><affiliation>Yonsei University</affiliation></author>
      <pages>31360-31394</pages>
      <abstract>Artificial agents are increasingly central to complex interactions and decision-making tasks, yet aligning their behaviors with desired human values remains an open challenge. In this work, we investigate how human-like personality traits influence agent behavior and performance within text-based interactive environments. We introduce PANDA: Personality Adapted Neural Decision Agents, a novel method for projecting human personality traits onto agents to guide their behavior. To induce personality in a text-based game agent, (i) we train a personality classifier to identify what personality type the agent’s actions exhibit, and (ii) we integrate the personality profiles directly into the agent’s policy-learning pipeline. By deploying agents embodying 16 distinct personality types across 25 text-based games and analyzing their trajectories, we demonstrate that an agent’s action decisions can be guided toward specific personality profiles. Moreover, certain personality types, such as those characterized by higher levels of Openness, display marked advantages in performance. These findings underscore the promise of personality-adapted agents for fostering more aligned, effective, and human-centric decision-making in interactive environments.</abstract>
      <url hash="4689da33">2025.acl-long.1515</url>
      <bibkey>lim-etal-2025-persona</bibkey>
    </paper>
    <paper id="1516">
      <title><fixed-case>S</fixed-case>eed<fixed-case>B</fixed-case>ench: A Multi-task Benchmark for Evaluating Large Language Models in Seed Science</title>
      <author><first>Jie</first><last>Ying</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Zihong</first><last>Chen</last></author>
      <author><first>Zhefan</first><last>Wang</last></author>
      <author><first>Wanli</first><last>Jiang</last></author>
      <author><first>Chenyang</first><last>Wang</last></author>
      <author><first>Zhonghang</first><last>Yuan</last><affiliation>University of Science and Technology of China and Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Haoyang</first><last>Su</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Huanjun</first><last>Kong</last></author>
      <author><first>Fan</first><last>Yang</last><affiliation>Yazhouwan Lab</affiliation></author>
      <author><first>Nanqing</first><last>Dong</last><affiliation>Shanghai Artificial Intelligence Laboratory and Shanghai Innovation Institute</affiliation></author>
      <pages>31395-31449</pages>
      <abstract>Seed science is essential for modern agriculture, directly influencing crop yields and global food security. However, challenges such as interdisciplinary complexity and high costs with limited returns hinder progress, leading to a shortage of experts and insufficient technological support. While large language models (LLMs) have shown promise across various fields, their application in seed science remains limited due to the scarcity of digital resources, complex gene-trait relationships, and the lack of standardized benchmarks. To address this gap, we introduce SeedBench—the first multi-task benchmark specifically designed for seed science. Developed in collaboration with domain experts, SeedBench focuses on seed breeding and simulates key aspects of modern breeding processes. We conduct a comprehensive evaluation of 26 leading LLMs, encompassing proprietary, open-source, and domain-specific fine-tuned models. Our findings not only highlight the substantial gaps between the power of LLMs and the real-world seed science problems, but also make a foundational step for research on LLMs for seed design.</abstract>
      <url hash="e0754002">2025.acl-long.1516</url>
      <bibkey>ying-etal-2025-seedbench</bibkey>
    </paper>
    <paper id="1517">
      <title>-Stance: A Large-Scale Real World Dataset of Stances in Legal Argumentation</title>
      <author><first>Ankita</first><last>Gupta</last></author>
      <author><first>Douglas</first><last>Rice</last></author>
      <author><first>Brendan</first><last>O’Connor</last><affiliation>University of Massachusetts, Amherst</affiliation></author>
      <pages>31450-31467</pages>
      <abstract>We present -Stance, a large-scale dataset of stances involved in legal argumentation.-Stance contains stance-annotated argument pairs, semi-automatically mined from millions of examples of U.S. judges citing precedent in context using citation signals. The dataset aims to facilitate work on the legal argument stance classification task, which involves assessing whether a case summary strengthens or weakens a legal argument (polarity) and to what extent (intensity). To assess the complexity of this task, we evaluate various existing NLP methods, including zero-shot prompting proprietary large language models (LLMs), and supervised fine-tuning of smaller open-weight language models (LMs) on 𝛿-Stance. Our findings reveal that although prompting proprietary LLMs can help predict stance polarity, supervised model fine-tuning on -Stance is necessary to distinguish intensity. We further find that alternative strategies such as domain-specific pretraining and zero-shot prompting using masked LMs remain insufficient. Beyond our dataset’s utility for the legal domain, we further find that fine-tuning small LMs on -Stance improves their performance in other domains. Finally, we study how temporal changes in signal definition can impact model performance, highlighting the importance of careful data curation for downstream tasks by considering the historical and sociocultural context. We publish the associated dataset to foster further research on legal argument reasoning.</abstract>
      <url hash="c6bea2b2">2025.acl-long.1517</url>
      <bibkey>gupta-etal-2025-stance</bibkey>
    </paper>
    <paper id="1518">
      <title>Re<tex-math>^{3}</tex-math>Syn: A Dependency-Based Data Synthesis Framework for Long-Context Post-training</title>
      <author><first>Zhiyang</first><last>Zhang</last></author>
      <author><first>Ziqiang</first><last>Liu</last></author>
      <author><first>Huiming</first><last>Wang</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Renke</first><last>Shan</last></author>
      <author><first>Li</first><last>Kuang</last><affiliation>Central South University</affiliation></author>
      <author><first>Lu</first><last>Wang</last><affiliation>China Three Gorges Corporation</affiliation></author>
      <author><first>De Wen</first><last>Soh</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <pages>31468-31480</pages>
      <abstract>An important trend in the realm of large language models (LLMs) is the development of longer context windows. However, training LLMs with long context windows to acquire the capability of effectively modeling lengthy inputs is often hindered by the scarcity of naturally long-context data. Existing methods for constructing long-context data by concatenating short documents have overlooked a crucial characteristic of long-context data quality, namely semantic dependency. In this paper, we propose a novel framework called Retrieval, Dependency Recognition, and Reorder for data synthesis (Re<tex-math>^{3}</tex-math>Syn), which leverages semantic similarity to retrieve relevant documents and form several batches. Within each batch, the framework comprehensively recognizes dependency and utilizes them, along with a reorder algorithm, to organize the short documents into coherent long-context data. Comprehensive experiment on multiple benchmarks indicate that the data generated by the Re<tex-math>^{3}</tex-math>Syn has longer dependencies and significantly enhances the model’s long-context capabilities. For reproducibility, we will release our codebase upon acceptance.</abstract>
      <url hash="1b71c0b6">2025.acl-long.1518</url>
      <bibkey>zhang-etal-2025-re3syn</bibkey>
    </paper>
    <paper id="1519">
      <title>Enabling Chatbots with Eyes and Ears: An Immersive Multimodal Conversation System for Dynamic Interactions</title>
      <author><first>Jihyoung</first><last>Jang</last><affiliation>Pohang University of Science and Technology</affiliation></author>
      <author><first>Minwook</first><last>Bae</last></author>
      <author><first>Minji</first><last>Kim</last><affiliation>Pohang University of Science and Technology</affiliation></author>
      <author><first>Dilek</first><last>Hakkani-Tür</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Hyounghun</first><last>Kim</last><affiliation>Pohang University of Science and Technology</affiliation></author>
      <pages>31481-31512</pages>
      <abstract>As chatbots continue to evolve toward human-like, real-world, interactions, multimodality remains an active area of research and exploration. So far, efforts to integrate multimodality into chatbots have primarily focused on image-centric tasks, such as visual dialogue and image-based instructions, placing emphasis on the “eyes” of human perception while neglecting the “ears”, namely auditory aspects. Moreover, these studies often center around static interactions that focus on discussing the modality rather than naturally incorporating it into the conversation, which limits the richness of simultaneous, dynamic engagement. Furthermore, while multimodality has been explored in multi-party and multi-session conversations, task-specific constraints have hindered its seamless integration into dynamic, natural conversations. To address these challenges, this study aims to equip chatbots with “eyes and ears” capable of more immersive interactions with humans. As part of this effort, we introduce a new multimodal conversation dataset, Multimodal Multi-Session Multi-Party Conversation (<tex-math>M^3C</tex-math>), and propose a novel multimodal conversation model featuring multimodal memory retrieval. Our model, trained on the <tex-math>M^3C</tex-math>, demonstrates the ability to seamlessly engage in long-term conversations with multiple speakers in complex, real-world-like settings, effectively processing visual and auditory inputs to understand and respond appropriately. Human evaluations highlight the model’s strong performance in maintaining coherent and dynamic interactions, demonstrating its potential for advanced multimodal conversational agents.</abstract>
      <url hash="13d42013">2025.acl-long.1519</url>
      <bibkey>jang-etal-2025-enabling</bibkey>
    </paper>
    <paper id="1520">
      <title>Multimodal Coreference Resolution for <fixed-case>C</fixed-case>hinese Social Media Dialogues: Dataset and Benchmark Approach</title>
      <author><first>Xingyu</first><last>Li</last></author>
      <author><first>Chen</first><last>Gong</last></author>
      <author><first>Guohong</first><last>Fu</last><affiliation>Soochow University, China,</affiliation></author>
      <pages>31513-31525</pages>
      <abstract>Multimodal coreference resolution (MCR) aims to identify mentions referring to the same entity across different modalities, such as text and visuals, and is essential for understanding multimodal content. In the era of rapidly growing multimodal content and social media, MCR is particularly crucial for interpreting user interactions and bridging text-visual references to improve communication and personalization. However, MCR research for real-world dialogues remains unexplored due to the lack of sufficient data resources. To address this gap, we introduce TikTalkCoref, the first Chinese multimodal coreference dataset for social media in real-world scenarios, derived from the popular Douyin short-video platform. This dataset pairs short videos with corresponding textual dialogues from user comments and includes manually annotated coreference clusters for both person mentions in the text and the coreferential person head regions in the corresponding video frames. We also present an effective benchmark approach for MCR, focusing on the celebrity domain, and conduct extensive experiments on our dataset, providing reliable benchmark results for this newly constructed dataset. We release the TikTalkCoref dataset to facilitate future research on MCR for real-world social media dialogues at https://github.com/lxystaruni/TikTalkCoref.</abstract>
      <url hash="581b54fa">2025.acl-long.1520</url>
      <bibkey>li-etal-2025-multimodal</bibkey>
    </paper>
    <paper id="1521">
      <title><fixed-case>TACLR</fixed-case>: A Scalable and Efficient Retrieval-based Method for Industrial Product Attribute Value Identification</title>
      <author><first>Yindu</first><last>Su</last></author>
      <author><first>Huike</first><last>Zou</last></author>
      <author><first>Lin</first><last>Sun</last><affiliation>Hangzhou City University</affiliation></author>
      <author><first>Ting</first><last>Zhang</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Haiyang</first><last>Yang</last></author>
      <author><first>Chen Li</first><last>Yu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>David</first><last>Lo</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Qingheng</first><last>Zhang</last></author>
      <author><first>Shuguang</first><last>Han</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Jufeng</first><last>Chen</last></author>
      <pages>31526-31538</pages>
      <abstract>Product Attribute Value Identification (PAVI) involves identifying attribute values from product profiles, a key task for improving product search, recommendation, and business analytics on e-commerce platforms.However, existing PAVI methods face critical challenges, such as inferring implicit values, handling out-of-distribution (OOD) values, and producing normalized outputs.To address these limitations, we introduce Taxonomy-Aware Contrastive Learning Retrieval (TACLR), the first retrieval-based method for PAVI.TACLR formulates PAVI as an information retrieval task by encoding product profiles and candidate values into embeddings and retrieving values based on their similarity. It leverages contrastive training with taxonomy-aware hard negative sampling and employs adaptive inference with dynamic thresholds.TACLR offers three key advantages: (1) it effectively handles implicit and OOD values while producing normalized outputs; (2) it scales to thousands of categories, tens of thousands of attributes, and millions of values; and (3) it supports efficient inference for high-load industrial deployment.Extensive experiments on proprietary and public datasets validate the effectiveness and efficiency of TACLR. Further, it has been successfully deployed on the real-world e-commerce platform Xianyu, processing millions of product listings daily with frequently updated, large-scale attribute taxonomies. We release the code to facilitate reproducibility and future research at https://github.com/SuYindu/TACLR.</abstract>
      <url hash="7a87c908">2025.acl-long.1521</url>
      <bibkey>su-etal-2025-taclr</bibkey>
    </paper>
    <paper id="1522">
      <title>Theory of Mind in Large Language Models: Assessment and Enhancement</title>
      <author><first>Ruirui</first><last>Chen</last><affiliation>Institute of High Performance Computing, Singapore, A*STAR</affiliation></author>
      <author><first>Weifeng</first><last>Jiang</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Chengwei</first><last>Qin</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Cheston</first><last>Tan</last><affiliation>A*STAR</affiliation></author>
      <pages>31539-31558</pages>
      <abstract>Theory of Mind (ToM)—the ability to reason about the mental states of oneself and others—is a cornerstone of human social intelligence. As Large Language Models (LLMs) become increasingly integrated into daily life, understanding their ability to interpret and respond to human mental states is crucial for enabling effective interactions. In this paper, we review LLMs’ ToM capabilities by analyzing both evaluation benchmarks and enhancement strategies. For evaluation, we focus on recently proposed and widely used story-based benchmarks. For enhancement, we provide an in-depth analysis of recent methods aimed at improving LLMs’ ToM abilities. Furthermore, we outline promising directions for future research to further advance these capabilities and better adapt LLMs to more realistic and diverse scenarios. Our survey serves as a valuable resource for researchers interested in evaluating and advancing LLMs’ ToM capabilities.</abstract>
      <url hash="120c5134">2025.acl-long.1522</url>
      <bibkey>chen-etal-2025-theory</bibkey>
    </paper>
    <paper id="1523">
      <title>Completing A Systematic Review in Hours instead of Months with Interactive <fixed-case>AI</fixed-case> Agents</title>
      <author><first>Rui</first><last>Qiu</last></author>
      <author><first>Shijie</first><last>Chen</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <author><first>Yu</first><last>Su</last><affiliation>Ohio State University</affiliation></author>
      <author><first>Po-Yin</first><last>Yen</last><affiliation>Washington University, Saint Louis</affiliation></author>
      <author><first>Han Wei</first><last>Shen</last></author>
      <pages>31559-31593</pages>
      <abstract>Systematic reviews (SRs) are vital for evidence-based practice in high stakes disciplines, such as healthcare, but are often impeded by intensive labors and lengthy processes that can take months to complete. Due to the high demand for domain expertise, existing automatic summarization methods fail to accurately identify relevant studies and generate high-quality summaries. To that end, we introduce InsightAgent, a human-centered interactive AI agent powered by large language models that revolutionize this workflow. InsightAgent partitions a large literature corpus based on semantics and employs a multi-agent design for more focused processing of literature, leading to significant improvement in the quality of generated SRs. InsightAgent also provides intuitive visualizations of the corpus and agent trajectories, allowing users to effortlessly monitor the actions of the agent and provide real-time feedback based on their expertise. Our user studies with 9 medical professionals demonstrate that the visualization and interaction mechanisms can effectively improve the quality of synthesized SRs by 27.2%, reaching 79.7% of human-written quality. At the same time, user satisfaction is improved by 34.4%. With InsightAgent, it only takes a clinician about 1.5 hours, rather than months, to complete a high-quality systematic review.</abstract>
      <url hash="33886910">2025.acl-long.1523</url>
      <bibkey>qiu-etal-2025-completing</bibkey>
    </paper>
    <paper id="1524">
      <title><fixed-case>CMHKF</fixed-case>: Cross-Modality Heterogeneous Knowledge Fusion for Weakly Supervised Video Anomaly Detection</title>
      <author><first>Guohua</first><last>Wang</last><affiliation>South China Agricultural University</affiliation></author>
      <author><first>Shengping</first><last>Song</last></author>
      <author><first>Wuchun</first><last>He</last></author>
      <author><first>Yongsen</first><last>Zheng</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>31594-31607</pages>
      <abstract>Weakly supervised video anomaly detection (WSVAD) presents a challenging task focused on detecting frame-level anomalies using only video-level labels. However, existing methods focus mainly on visual modalities, neglecting rich multi-modality information. This paper proposes a novel framework, Cross-Modality Heterogeneous Knowledge Fusion (CMHKF), that integrates cross-modality knowledge from video, audio, and text to improve anomaly detection and localization. To achieve adaptive cross-modality heterogeneous knowledge learning, we designed two components: Cross-Modality Video-Text Knowledge Alignment (CVKA) and Audio Modality Feature Adaptive Extraction (AFAE). They extract and aggregate features by exploring inter-modality correlations. By leveraging abundant cross-modality knowledge, our approach improves the discrimination between normal and anomalous segments. Extensive experiments on XD-Violence show our method significantly enhances accuracy and robustness in both coarse-grained and fine-grained anomaly detection.</abstract>
      <url hash="6b7e85e9">2025.acl-long.1524</url>
      <bibkey>wang-etal-2025-cmhkf</bibkey>
    </paper>
    <paper id="1525">
      <title><fixed-case>CL</fixed-case>a<fixed-case>S</fixed-case>p: In-Context Layer Skip for Self-Speculative Decoding</title>
      <author><first>Longze</first><last>Chen</last></author>
      <author><first>Renke</first><last>Shan</last></author>
      <author><first>Huiming</first><last>Wang</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Lu</first><last>Wang</last><affiliation>China Three Gorges Corporation</affiliation></author>
      <author><first>Ziqiang</first><last>Liu</last></author>
      <author><first>Run</first><last>Luo</last></author>
      <author><first>Jiawei</first><last>Wang</last></author>
      <author><first>Hamid</first><last>Alinejad-Rokny</last><affiliation>UNSW Sydney</affiliation></author>
      <author><first>Min</first><last>Yang</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <pages>31608-31618</pages>
      <abstract>Speculative decoding (SD) is a promising method for accelerating the decoding process of Large Language Models (LLMs). The efficiency of SD primarily hinges on the consistency between the draft model and the verify model. However, existing drafting approaches typically require additional modules to be trained, which can be challenging to implement and ensure compatibility across various LLMs. In this paper, we propose CLaSp, an in-context layer-skipping strategy for self-speculative decoding. Unlike prior methods, CLaSp does not require additional drafting modules or extra training. Instead, it employs a plug-and-play mechanism by skipping intermediate layers of the verify model to construct a compressed draft model. Specifically, we develop a dynamic programming algorithm that optimizes the layer-skipping process by leveraging the complete hidden states from the last verification stage as an objective. This enables CLaSp to dynamically adjust its layer-skipping strategy after each verification stage, without relying on pre-optimized sets of skipped layers. Experimental results across diverse downstream tasks demonstrate that CLaSp achieves a speedup of <tex-math>1.3\times \sim 1.7\times</tex-math> on LLaMA3 series models without altering the original distribution of the generated text.</abstract>
      <url hash="5ae4297f">2025.acl-long.1525</url>
      <bibkey>chen-etal-2025-clasp</bibkey>
    </paper>
    <paper id="1526">
      <title>Teaching Text Agents to Learn Sequential Decision Making from Failure</title>
      <author><first>Canasai</first><last>Kruengkrai</last><affiliation>RIKEN</affiliation></author>
      <author><first>Koichiro</first><last>Yoshino</last><affiliation>Tokyo Institute of Technology/Institute of Science Tokyo and RIKEN</affiliation></author>
      <pages>31619-31635</pages>
      <abstract>Text-based reinforcement-learning agents improve their policies by interacting with their environments to collect more training data. However, these self-collected data inevitably contain intermediate failed actions caused by attempting physically infeasible behaviors and/or hallucinations. Directly learning a policy from such trajectories can reinforce incorrect behaviors and reduce task success rates. In this paper, we propose a failed action-aware objective that suppresses the negative impact of failed actions during training by assigning zero return based on textual feedback. Building on this objective, we introduce a perturbation method that leverages unsuccessful trajectories to construct new successful ones that share the same goal. This allows agents to benefit from diverse experiences without further interaction with the environment. Experiments in ALFWorld and ScienceWorld demonstrate that our method significantly outperforms strong baselines and generalizes across environments. Code is available at https://github.com/riken-grp/text-agent.</abstract>
      <url hash="a2f5e2c8">2025.acl-long.1526</url>
      <bibkey>kruengkrai-yoshino-2025-teaching</bibkey>
    </paper>
    <paper id="1527">
      <title>The Harmonic Structure of Information Contours</title>
      <author><first>Eleftheria</first><last>Tsipidi</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <author><first>Samuel</first><last>Kiegeland</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <author><first>Franz</first><last>Nowak</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <author><first>Tianyang</first><last>Xu</last><affiliation>Toyota Technological Institute at Chicago</affiliation></author>
      <author><first>Ethan</first><last>Wilcox</last><affiliation>ETHZ - ETH Zurich and Georgetown University</affiliation></author>
      <author><first>Alex</first><last>Warstadt</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Ryan</first><last>Cotterell</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <author><first>Mario</first><last>Giulianelli</last><affiliation>Department of Computer Science, ETHZ - ETH Zurich</affiliation></author>
      <pages>31636-31659</pages>
      <abstract>The uniform information density (UID) hypothesis proposes that speakers aim to distribute information evenly throughout a text, balancing production effort and listener comprehension difficulty. However, language typically does not maintain a strictly uniform information rate; instead, it fluctuates around a global average. These fluctuations are often explained by factors such as syntactic constraints, stylistic choices, or audience design. In this work, we explore an alternative perspective: that these fluctuations may be influenced by an implicit linguistic pressure towards periodicity, where the information rate oscillates at regular intervals, potentially across multiple frequencies simultaneously. We apply harmonic regression and introduce a novel extension called time scaling to detect and test for such periodicity in information contours. Analyzing texts in English, Spanish, German, Dutch, Basque, and Brazilian Portuguese, we find consistent evidence of periodic patterns in information rate. Many dominant frequencies align with discourse structure, suggesting these oscillations reflect meaningful linguistic organization. Beyond highlighting the connection between information rate and discourse structure, our approach offers a general framework for uncovering structural pressures at various levels of linguistic granularity.</abstract>
      <url hash="003960fb">2025.acl-long.1527</url>
      <bibkey>tsipidi-etal-2025-harmonic</bibkey>
    </paper>
    <paper id="1528">
      <title><fixed-case>REAL</fixed-case>-<fixed-case>MM</fixed-case>-<fixed-case>RAG</fixed-case>: A Real-World Multi-Modal Retrieval Benchmark</title>
      <author><first>Navve</first><last>Wasserman</last><affiliation>Weizmann Institute of Science</affiliation></author>
      <author><first>Roi</first><last>Pony</last><affiliation>Technion - Israel Institute of Technology, Technion</affiliation></author>
      <author><first>Oshri</first><last>Naparstek</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Adi Raz</first><last>Goldfarb</last></author>
      <author><first>Eli</first><last>Schwartz</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Udi</first><last>Barzelay</last></author>
      <author><first>Leonid</first><last>Karlinsky</last><affiliation>International Business Machines</affiliation></author>
      <pages>31660-31683</pages>
      <abstract>Accurate multi-modal document retrieval is crucial for Retrieval-Augmented Generation (RAG), yet existing benchmarks do not fully capture real-world challenges with their current design. We introduce REAL-MM-RAG, an automatically generated benchmark designed to address four key properties essential for real-world retrieval: (i) multi-modal documents, (ii) enhanced difficulty, (iii) Realistic-RAG queries and (iv) accurate labeling. Additionally, we propose a multi-difficulty-level scheme based on query rephrasing to evaluate models’ semantic understanding beyond keyword matching. Our benchmark reveals significant model weaknesses, particularly in handling table-heavy documents and robustness to query rephrasing. To mitigate these shortcomings, we curate a rephrased training set and introduce a new finance-focused, table-heavy dataset. Fine-tuning on these datasets enables models to achieve state-of-the-art retrieval performance on REAL-MM-RAG benchmark. Our work offers a better way to evaluate and improve retrieval in multi-modal RAG systems while also providing training data and models that address current limitations.</abstract>
      <url hash="b919deda">2025.acl-long.1528</url>
      <bibkey>wasserman-etal-2025-real</bibkey>
    </paper>
    <paper id="1529">
      <title>Only a Little to the Left: A Theory-grounded Measure of Political Bias in Large Language Models</title>
      <author><first>Mats</first><last>Faulborn</last><affiliation>Universität Konstanz</affiliation></author>
      <author><first>Indira</first><last>Sen</last><affiliation>Universität Mannheim</affiliation></author>
      <author><first>Max</first><last>Pellert</last><affiliation>Barcelona Supercomputing Center</affiliation></author>
      <author><first>Andreas</first><last>Spitz</last><affiliation>Universität Konstanz</affiliation></author>
      <author><first>David</first><last>Garcia</last><affiliation>Universität Konstanz</affiliation></author>
      <pages>31684-31704</pages>
      <abstract>Prompt-based language models like GPT4 and LLaMa have been used for a wide variety of use cases such as simulating agents, searching for information, or for content analysis. For all of these applications and others, political biases in these models can affect their performance. Several researchers have attempted to study political bias in language models using evaluation suites based on surveys, such as the Political Compass Test (PCT), often finding a particular leaning favored by these models. However, there is some variation in the exact prompting techniques, leading to diverging findings, and most research relies on constrained-answer settings to extract model responses. Moreover, the Political Compass Test is not a scientifically valid survey instrument. In this work, we contribute a political bias measured informed by political science theory, building on survey design principles to test a wide variety of input prompts, while taking into account prompt sensitivity. We then prompt 11 different open and commercial models, differentiating between instruction-tuned and non-instruction-tuned models, and automatically classify their political stances from 88,110 responses. Leveraging this dataset, we compute political bias profiles across different prompt variations and find that while PCT exaggerates bias in certain models like GPT3.5, measures of political bias are often unstable, but generally more left-leaning for instruction-tuned models. Code and data are available at https://github.com/MaFa211/theory_grounded_pol_bias.</abstract>
      <url hash="7cfcfdb0">2025.acl-long.1529</url>
      <bibkey>faulborn-etal-2025-little</bibkey>
    </paper>
    <paper id="1530">
      <title><fixed-case>L</fixed-case>ong<fixed-case>S</fixed-case>afety: Evaluating Long-Context Safety of Large Language Models</title>
      <author><first>Yida</first><last>Lu</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Jiale</first><last>Cheng</last></author>
      <author><first>Zhexin</first><last>Zhang</last></author>
      <author><first>Shiyao</first><last>Cui</last></author>
      <author><first>Cunxiang</first><last>Wang</last></author>
      <author><first>Xiaotao</first><last>Gu</last><affiliation>Zhipu AI</affiliation></author>
      <author><first>Yuxiao</first><last>Dong</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Jie</first><last>Tang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Hongning</first><last>Wang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <pages>31705-31725</pages>
      <abstract>As Large Language Models (LLMs) continue to advance in understanding and generating long sequences, new safety concerns have been introduced through the long context. However, the safety of LLMs in long-context tasks remains under-explored, leaving a significant gap in both evaluation and improvement of their safety. To address this, we introduce LongSafety, the first comprehensive benchmark specifically designed to evaluate LLM safety in open-ended long-context tasks. LongSafety encompasses 7 categories of safety issues and 6 user-oriented long-context tasks, with a total of 1,543 test cases, averaging 5,424 words per context. Our evaluation towards 16 representative LLMs reveals significant safety vulnerabilities, with most models achieving safety rates below 55%. Our findings also indicate that strong safety performance in short-context scenarios does not necessarily correlate with safety in long-context tasks, emphasizing the unique challenges and urgency of improving long-context safety. Moreover, through extensive analysis, we identify challenging safety issues and task types for long-context models. Furthermore, we find that relevant context and extended input sequences can exacerbate safety risks in long-context scenarios, highlighting the critical need for ongoing attention to long-context safety challenges. Our code and data will be publicly available.</abstract>
      <url hash="7663d949">2025.acl-long.1530</url>
      <bibkey>lu-etal-2025-longsafety</bibkey>
    </paper>
    <paper id="1531">
      <title>Exploiting Contextual Knowledge in <fixed-case>LLM</fixed-case>s through <tex-math>\mathcal{V}</tex-math>-usable Information based Layer Enhancement</title>
      <author><first>Xiaowei</first><last>Yuan</last></author>
      <author><first>Zhao</first><last>Yang</last><affiliation>Meituan</affiliation></author>
      <author><first>Ziyang</first><last>Huang</last></author>
      <author><first>Yequan</first><last>Wang</last><affiliation>Beijing Academy of Artificial Intelligence</affiliation></author>
      <author><first>Siqi</first><last>Fan</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Yiming</first><last>Ju</last><affiliation>BAAI</affiliation></author>
      <author><first>Jun</first><last>Zhao</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <author><first>Kang</first><last>Liu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <pages>31726-31741</pages>
      <abstract>Large Language Models (LLMs) have demonstrated remarkable capabilities in various tasks, yet they often struggle with context-faithfulness generations that properly reflect contextual knowledge. While existing approaches focus on enhancing the decoding strategies, they ignore the fundamental mechanism of how contextual information is processed within LLMs’ internal states. As a result, LLMs remain limited in their ability to fully leverage contextual knowledge. In this paper, we propose Context-aware Layer Enhancement (CaLE), a novel intervention method that enhances the utilization of contextual knowledge within LLMs’ internal representations. By employing <tex-math>\mathcal{V}</tex-math>-usable information analysis, CaLE strategically amplifies the growth of contextual information at an optimal layer, thereby enriching representations in the final layer. Our experiments demonstrate that CaLE effectively improves context-faithful generation in Question-Answering tasks, particularly in scenarios involving unknown or conflicting contextual knowledge.</abstract>
      <url hash="e84344b8">2025.acl-long.1531</url>
      <bibkey>yuan-etal-2025-exploiting</bibkey>
    </paper>
    <paper id="1532">
      <title>Unintended Harms of Value-Aligned <fixed-case>LLM</fixed-case>s: Psychological and Empirical Insights</title>
      <author><first>Sooyung</first><last>Choi</last></author>
      <author><first>Jaehyeok</first><last>Lee</last><affiliation>Sung Kyun Kwan University</affiliation></author>
      <author><first>Xiaoyuan</first><last>Yi</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Jing</first><last>Yao</last><affiliation>Microsoft</affiliation></author>
      <author><first>Xing</first><last>Xie</last><affiliation>Microsoft Research Asia</affiliation></author>
      <author><first>JinYeong</first><last>Bak</last><affiliation>SungKyunKwan University</affiliation></author>
      <pages>31742-31768</pages>
      <abstract>The application scope of Large Language Models (LLMs) continues to expand, leading to increasing interest in personalized LLMs that align with human values. However, aligning these models with individual values raises significant safety concerns, as certain values may correlate with harmful information. In this paper, we identify specific safety risks associated with value-aligned LLMs and investigate the psychological principles behind these challenges. Our findings reveal two key insights. (1) Value-aligned LLMs are more prone to harmful behavior compared to non-fine-tuned models and exhibit slightly higher risks in traditional safety evaluations than other fine-tuned models. (2) These safety issues arise because value-aligned LLMs genuinely generate text according to the aligned values, which can amplify harmful outcomes. Using a dataset with detailed safety categories, we find significant correlations between value alignment and safety risks, supported by psychological hypotheses. This study offers insights into the “black box” of value alignment and proposes in-context alignment methods to enhance the safety of value-aligned LLMs.</abstract>
      <url hash="f0027dc5">2025.acl-long.1532</url>
      <bibkey>choi-etal-2025-unintended</bibkey>
    </paper>
    <paper id="1533">
      <title>Maximal Matching Matters: Preventing Representation Collapse for Robust Cross-Modal Retrieval</title>
      <author><first>Hani</first><last>Alomari</last><affiliation>Virginia Polytechnic Institute and State University</affiliation></author>
      <author><first>Anushka</first><last>Sivakumar</last><affiliation>Virginia Polytechnic Institute and State University</affiliation></author>
      <author><first>Andrew</first><last>Zhang</last></author>
      <author><first>Chris</first><last>Thomas</last><affiliation>Virginia Polytechnic Institute and State University</affiliation></author>
      <pages>31769-31785</pages>
      <abstract>Cross-modal image-text retrieval is challenging because of the diverse possible associations between content from different modalities. Traditional methods learn a single-vector embedding to represent semantics of each sample, but struggle to capture nuanced and diverse relationships that can exist across modalities. Set-based approaches, which represent each sample with multiple embeddings, offer a promising alternative, as they can capture richer and more diverse relationships. In this paper, we show that, despite their promise, these set-based representations continue to face issues including sparse supervision and set collapse, which limits their effectiveness. To address these challenges, we propose Maximal Pair Assignment Similarity to optimize one-to-one matching between embedding sets which preserve semantic diversity within the set. We also introduce two loss functions to further enhance the representations: Global Discriminative Loss to enhance distinction among embeddings, and Intra-Set Divergence Loss to prevent collapse within each set. Our method achieves state-of-the-art performance on MS-COCO and Flickr30k without relying on external data.</abstract>
      <url hash="4a6a5396">2025.acl-long.1533</url>
      <bibkey>alomari-etal-2025-maximal</bibkey>
    </paper>
    <paper id="1534">
      <title>The Noisy Path from Source to Citation: Measuring How Scholars Engage with Past Research</title>
      <author><first>Hong</first><last>Chen</last></author>
      <author><first>Misha</first><last>Teplitskiy</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>David</first><last>Jurgens</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <pages>31786-31802</pages>
      <abstract>Academic citations are widely used for evaluating research and tracing knowledge flows. Such uses typically rely on raw citation counts and neglect variability in citation types. In particular, citations can vary in their fidelity as original knowledge from cited studies may be paraphrased, summarized, or reinterpreted, possibly wrongly, leading to variation in how much information changes from cited to citing paper. In this study, we introduce a computational pipeline to quantify citation fidelity at scale. Using full texts of papers, the pipeline identifies citations in citing papers and the corresponding claims in cited papers, and applies supervised models to measure fidelity at the sentence level. Analyzing a large-scale multi-disciplinary dataset of approximately 13 million citation sentence pairs, we find that citation fidelity is higher when authors cite papers that are 1) more recent and intellectually close, 2) more accessible, and 3) the first author has a lower H-index and the author team is medium-sized. Using a quasi-experiment, we establish the “telephone effect” - when citing papers have low fidelity to the original claim, future papers that cite the citing paper and the original have lower fidelity to the original. Our work reveals systematic differences in citation fidelity, underscoring the limitations of analyses that rely on citation quantity alone and the potential for distortion of evidence.</abstract>
      <url hash="2eea8207">2025.acl-long.1534</url>
      <bibkey>chen-etal-2025-noisy</bibkey>
    </paper>
    <paper id="1535">
      <title><fixed-case>MAPLE</fixed-case>: Enhancing Review Generation with Multi-Aspect Prompt <fixed-case>LE</fixed-case>arning in Explainable Recommendation</title>
      <author><first>Ching-Wen</first><last>Yang</last></author>
      <author><first>Zhi-Quan</first><last>Feng</last></author>
      <author><first>Ying-Jia</first><last>Lin</last><affiliation>Chang Gung University</affiliation></author>
      <author><first>Che Wei</first><last>Chen</last></author>
      <author><first>Kun-da</first><last>Wu</last><affiliation>Google</affiliation></author>
      <author><first>Hao</first><last>Xu</last></author>
      <author><first>Yao</first><last>Jui-Feng</last><affiliation>Google</affiliation></author>
      <author><first>Hung-Yu</first><last>Kao</last><affiliation>Department of Computer Science, National Tsing Hua University, National Tsinghua University</affiliation></author>
      <pages>31803-31821</pages>
      <abstract>Explainable Recommendation task is designed to receive a pair of user and item and output explanations to justify why an item is recommended to a user. Many models approach review generation as a proxy for explainable recommendations. While these models can produce fluent and grammatically correct sentences, they often lack preciseness and fail to provide personalized informative recommendations. To address this issue, we propose a personalized, aspect-controlled model called Multi-Aspect Prompt LEarner (MAPLE), which integrates aspect category as another input dimension to facilitate memorizing fine-grained aspect terms. Experiments conducted on two real-world review datasets in the restaurant domain demonstrate that MAPLE significantly outperforms baseline review-generation models. MAPLE excels in both text and feature diversity, ensuring that the generated content covers a wide range of aspects. Additionally, MAPLE delivers good generation quality while maintaining strong coherence and factual relevance. The code and dataset used in this paper can be found at https://github.com/Nana2929/MAPLE.</abstract>
      <url hash="90e25c05">2025.acl-long.1535</url>
      <bibkey>yang-etal-2025-maple</bibkey>
    </paper>
    <paper id="1536">
      <title>Separating Tongue from Thought: Activation Patching Reveals Language-Agnostic Concept Representations in Transformers</title>
      <author><first>Clément</first><last>Dumas</last></author>
      <author><first>Chris</first><last>Wendler</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Veniamin</first><last>Veselovsky</last></author>
      <author><first>Giovanni</first><last>Monea</last><affiliation>Cornell University</affiliation></author>
      <author><first>Robert</first><last>West</last><affiliation>EPFL - EPF Lausanne and Microsoft</affiliation></author>
      <pages>31822-31841</pages>
      <abstract>A central question in multilingual language modeling is whether large language models (LLMs) develop a universal concept representation, disentangled from specific languages. In this paper, we address this question by analyzing latent representations (latents) during a word-translation task in transformer-based LLMs. We strategically extract latents from a source translation prompt and insert them into the forward pass on a target translation prompt. By doing so, we find that the output language is encoded in the latent at an earlier layer than the concept to be translated. Building on this insight, we conduct two key experiments. First, we demonstrate that we can change the concept without changing the language and vice versa through activation patching alone. Second, we show that patching with the mean representation of a concept across different languages does not affect the models’ ability to translate it, but instead improves it. Finally, we generalize to multi-token generation and demonstrate that the model can generate natural language description of those mean representations. Our results provide evidence for the existence of language-agnostic concept representations within the investigated models.</abstract>
      <url hash="5f96ba31">2025.acl-long.1536</url>
      <bibkey>dumas-etal-2025-separating</bibkey>
    </paper>
    <paper id="1537">
      <title>Behavioural vs. Representational Systematicity in End-to-End Models: An Opinionated Survey</title>
      <author><first>Ivan</first><last>Vegner</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Sydelle</first><last>De Souza</last></author>
      <author><first>Valentin</first><last>Forch</last><affiliation>Technische Universität Chemnitz</affiliation></author>
      <author><first>Martha</first><last>Lewis</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Leonidas A. A.</first><last>Doumas</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <pages>31842-31856</pages>
      <abstract>A core aspect of compositionality, systematicity is a desirable property in ML models as it enables strong generalization to novel contexts. This has led to numerous studies proposing benchmarks to assess systematic generalization, as well as models and training regimes designed to enhance it. Many of these efforts are framed as addressing the challenge posed by Fodor and Pylyshyn. However, while they argue for systematicity of representations, existing benchmarks and models primarily focus on the systematicity of behaviour. We emphasize the crucial nature of this distinction. Furthermore, building on Hadley’s (1994) taxonomy of systematic generalization, we analyze the extent to which behavioural systematicity is tested by key benchmarks in the literature across language and vision. Finally, we highlight ways of assessing systematicity of representations in ML models as practiced in the field of mechanistic interpretability.</abstract>
      <url hash="cf3485fa">2025.acl-long.1537</url>
      <bibkey>vegner-etal-2025-behavioural</bibkey>
    </paper>
    <paper id="1538">
      <title>Dynamic Chunking and Selection for Reading Comprehension of Ultra-Long Context in Large Language Models</title>
      <author><first>Boheng</first><last>Sheng</last></author>
      <author><first>Jiacheng</first><last>Yao</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Meicong</first><last>Zhang</last></author>
      <author><first>Guoxiu</first><last>He</last><affiliation>East China Normal University</affiliation></author>
      <pages>31857-31876</pages>
      <abstract>Large language models (LLMs) often struggle to accurately read and comprehend extremely long texts. Current methods for improvement typically rely on splitting long contexts into fixed-length chunks. However, fixed truncation risks separating semantically relevant content, leading to ambiguity and compromising accurate understanding. To overcome this limitation, we propose a straightforward approach for dynamically separating and selecting chunks of long context, facilitating a more streamlined input for LLMs. In particular, we compute semantic similarities between adjacent sentences, using lower similarities to adaptively divide long contexts into variable-length chunks. We further train a question-aware classifier to select sensitive chunks that are critical for answering specific questions. Experimental results on both single-hop and multi-hop question-answering benchmarks show that the proposed approach consistently outperforms strong baselines. Notably, it maintains robustness across a wide range of input lengths, handling sequences of up to 256k tokens. Our datasets and code are available at the following link: https://github.com/ECNU-Text-Computing/DCS</abstract>
      <url hash="efd21c52">2025.acl-long.1538</url>
      <bibkey>sheng-etal-2025-dynamic</bibkey>
    </paper>
    <paper id="1539">
      <title><fixed-case>D</fixed-case>ual<fixed-case>RAG</fixed-case>: A Dual-Process Approach to Integrate Reasoning and Retrieval for Multi-Hop Question Answering</title>
      <author><first>Rong</first><last>Cheng</last><affiliation>Tianjin University</affiliation></author>
      <author><first>Jinyi</first><last>Liu</last></author>
      <author><first>Yan</first><last>Zheng</last><affiliation>Tianjin Unibersity, China</affiliation></author>
      <author><first>Fei</first><last>Ni</last></author>
      <author><first>Jiazhen</first><last>Du</last><affiliation>Kuaishou- 快手科技</affiliation></author>
      <author><first>Hangyu</first><last>Mao</last></author>
      <author><first>Fuzheng</first><last>Zhang</last></author>
      <author><first>Bo</first><last>Wang</last></author>
      <author><first>Jianye</first><last>Hao</last><affiliation>Tianjin University</affiliation></author>
      <pages>31877-31899</pages>
      <abstract>Multi-Hop Question Answering (MHQA) tasks permeate real-world applications, posing challenges in orchestrating multi-step reasoning across diverse knowledge domains. While existing approaches have been improved with iterative retrieval, they still struggle to identify and organize dynamic knowledge. To address this, we propose DualRAG, a synergistic dual-process framework that seamlessly integrates reasoning and retrieval. DualRAG operates through two tightly coupled processes: Reasoning-augmented Querying (RaQ) and progressive Knowledge Aggregation (pKA). They work in concert: as RaQ navigates the reasoning path and generates targeted queries, pKA ensures that newly acquired knowledge is systematically integrated to support coherent reasoning. This creates a virtuous cycle of knowledge enrichment and reasoning refinement. Through targeted fine-tuning, DualRAG preserves its sophisticated reasoning and retrieval capabilities even in smaller-scale models, demonstrating its versatility and core advantages across different scales. Extensive experiments demonstrate that this dual-process approach substantially improves answer accuracy and coherence, approaching, and in some cases surpassing, the performance achieved with oracle knowledge access. These results establish DualRAG as a robust and efficient solution for complex multi-hop reasoning tasks.</abstract>
      <url hash="41d58f8d">2025.acl-long.1539</url>
      <bibkey>cheng-etal-2025-dualrag</bibkey>
    </paper>
    <paper id="1540">
      <title>Deliberate Reasoning in Language Models as Structure-Aware Planning with an Accurate World Model</title>
      <author><first>Siheng</first><last>Xiong</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Ali</first><last>Payani</last><affiliation>Cisco</affiliation></author>
      <author><first>Yuan</first><last>Yang</last></author>
      <author><first>Faramarz</first><last>Fekri</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <pages>31900-31931</pages>
      <abstract>Enhancing the reasoning capabilities of language models (LMs) remains a key challenge, especially for tasks that require complex, multi-step decision-making where existing Chain-of-Thought (CoT) approaches struggle with consistency and verification. In this paper, we propose a novel reasoning framework, referred to as Structure-aware Planning with an Accurate World Model (SWAP), that integrates structured knowledge representation with learned planning. Unlike prior methods that rely purely on natural language reasoning, SWAP leverages entailment graphs to encode structured dependencies and enable symbolic verification of intermediate steps. To systematically construct and update the graph, SWAP employs a policy model to propose candidate expansions and a world model to predict structural updates. To improve accuracy, the world model generates multiple alternative updates, and a discriminator re-ranks them based on plausibility. To encourage diverse exploration, we introduce Diversity-based Modelling (DM), which samples candidates from the remaining probability mass after removing previously sampled candidates from the original policy distribution. Additionally, SWAP improves the discrimination accuracy through Contrastive Ranking (CR), which directly compares candidates within prompts and incorporates meta-knowledge to improve ranking quality. We evaluate SWAP across diverse reasoning-intensive benchmarks including math reasoning, logical reasoning, and coding tasks. Extensive experiments demonstrate that SWAP significantly improves upon the base models and consistently outperforms existing reasoning methods.</abstract>
      <url hash="7c59ef31">2025.acl-long.1540</url>
      <bibkey>xiong-etal-2025-deliberate</bibkey>
    </paper>
    <paper id="1541">
      <title>Refining Salience-Aware Sparse Fine-Tuning Strategies for Language Models</title>
      <author><first>Xinxin</first><last>Liu</last><affiliation>Southern University of Science and Technology</affiliation></author>
      <author><first>Aaron</first><last>Thomas</last></author>
      <author><first>Cheng</first><last>Zhang</last></author>
      <author><first>Jianyi</first><last>Cheng</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Yiren</first><last>Zhao</last><affiliation>Imperial College London</affiliation></author>
      <author><first>Xitong</first><last>Gao</last><affiliation>Shenzhen University of Advanced Technology and Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences</affiliation></author>
      <pages>31932-31945</pages>
      <abstract>Parameter-Efficient Fine-Tuning (PEFT) has gained prominence through low-rank adaptation methods like LoRA. In this paper, we focus on sparsity-based PEFT (SPEFT), which introduces trainable sparse adaptations to the weight matrices in the model, offering greater flexibility in selecting fine-tuned parameters compared to low-rank methods. We conduct the first systematic evaluation of salience metrics for SPEFT, inspired by zero-cost NAS proxies, and identify simple gradient-based metrics is reliable, and results are on par with the best alternatives, offering both computational efficiency and robust performance. Additionally, we compare static and dynamic masking strategies, finding that static masking, which predetermines non-zero entries before training, delivers efficiency without sacrificing performance, while dynamic masking offers no substantial benefits. Across NLP tasks, a simple gradient-based, static SPEFT consistently outperforms other fine-tuning methods for LLMs, providing a simple yet effective baseline for SPEFT. Our work challenges the notion that complexity is necessary for effective PEFT, while our open-source framework establishes a reproducible benchmark for future research.</abstract>
      <url hash="93682a7e">2025.acl-long.1541</url>
      <bibkey>liu-etal-2025-refining</bibkey>
    </paper>
    <paper id="1542">
      <title>Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse Attention</title>
      <author><first>Emily</first><last>Xiao</last></author>
      <author><first>Chin-Jou</first><last>Li</last></author>
      <author><first>Yilin</first><last>Zhang</last></author>
      <author><first>Graham</first><last>Neubig</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Amanda</first><last>Bertsch</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>31946-31958</pages>
      <abstract>Many-shot in-context learning has recently shown promise as an alternative to finetuning, with the major advantage that the same model can be served for multiple tasks. However, this shifts the computational burden from training-time to inference-time, making deployment of many-shot ICL challenging to justify in-practice. This cost is further increased if a custom demonstration set is retrieved for each inference example. We present Dynamic Block-Sparse Attention, an optimized method for retrieval-based many-shot in-context learning. By combining carefully designed block-sparse attention and retrieval of cached groups of demonstrations, we achieve comparable per-example latency to finetuning while maintaining on average &gt;95% of the best method’s accuracy across strong ICL and finetuning baselines. We hope that this will further enable the deployment of many-shot ICL at scale.</abstract>
      <url hash="4ae14519">2025.acl-long.1542</url>
      <bibkey>xiao-etal-2025-efficient</bibkey>
    </paper>
    <paper id="1543">
      <title><fixed-case>S</fixed-case>cale<fixed-case>B</fixed-case>i<fixed-case>O</fixed-case>: Scalable Bilevel Optimization for <fixed-case>LLM</fixed-case> Data Reweighting</title>
      <author><first>Rui</first><last>Pan</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Dylan</first><last>Zhang</last></author>
      <author><first>Hanning</first><last>Zhang</last></author>
      <author><first>Xingyuan</first><last>Pan</last></author>
      <author><first>Minrui</first><last>Xu</last></author>
      <author><first>Jipeng</first><last>Zhang</last></author>
      <author><first>Renjie</first><last>Pi</last></author>
      <author><first>Xiaoyu</first><last>Wang</last></author>
      <author><first>Tong</first><last>Zhang</last><affiliation>UIUC</affiliation></author>
      <pages>31959-31982</pages>
      <abstract>Bilevel optimization has shown its utility across various machine learning settings, yet most algorithms in practice require second-order information, making it challenging to scale them up. Only recently, a paradigm of first-order algorithms has emerged in the theoretical literature, capable of effectively addressing bilevel optimization problems. Nevertheless, the practical efficiency of this paradigm remains unverified, particularly in the context of large language models (LLMs). This paper introduces the first scalable instantiation of this paradigm called _ScaleBiO_, focusing on bilevel optimization for large-scale LLM data reweighting. By combining with a recently proposed memory-efficient training technique called LISA, our novel algorithm allows the paradigm to scale to ~30B-sized LLMs on <tex-math>8\times</tex-math>H100 GPUs, marking the first successful application of bilevel optimization under practical scenarios for large-sized LLMs. Empirically, extensive experiments on data reweighting verify the effectiveness of ScaleBiO for different-scaled models, including Llama-3-8B, Gemma-2-9B, Qwen-2-7B, and Qwen-2.5-32B, where bilevel optimization succeeds in instruction-following and math reasoning tasks, outperforming several popular baselines, including uniform sampling, influence-aware data filtering, and reference-model-based sampling methods. Theoretically, ScaleBiO ensures the optimality of the learned data weights, along with a convergence guarantee matching the conventional first-order bilevel optimization paradigm on smooth and strongly convex objectives.</abstract>
      <url hash="17f6b721">2025.acl-long.1543</url>
      <bibkey>pan-etal-2025-scalebio</bibkey>
    </paper>
    <paper id="1544">
      <title><fixed-case>PKU</fixed-case>-<fixed-case>S</fixed-case>afe<fixed-case>RLHF</fixed-case>: Towards Multi-Level Safety Alignment for <fixed-case>LLM</fixed-case>s with Human Preference</title>
      <author><first>Jiaming</first><last>Ji</last></author>
      <author><first>Donghai</first><last>Hong</last></author>
      <author><first>Borong</first><last>Zhang</last></author>
      <author><first>Boyuan</first><last>Chen</last></author>
      <author><first>Josef</first><last>Dai</last><affiliation>Peking University</affiliation></author>
      <author><first>Boren</first><last>Zheng</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Tianyi Alex</first><last>Qiu</last></author>
      <author><first>Jiayi</first><last>Zhou</last><affiliation>Peking University</affiliation></author>
      <author><first>Kaile</first><last>Wang</last><affiliation>Peking University</affiliation></author>
      <author><first>Boxun</first><last>Li</last><affiliation>Infinigence-AI</affiliation></author>
      <author><first>Sirui</first><last>Han</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Yike</first><last>Guo</last><affiliation>Hong Kong University of Science and Technology and Imperial College London</affiliation></author>
      <author><first>Yaodong</first><last>Yang</last><affiliation>Peking University</affiliation></author>
      <pages>31983-32016</pages>
      <abstract>In this work, we introduce the PKU-SafeRLHF dataset, designed to promote research on safety alignment in large language models (LLMs). As a sibling project to SafeRLHF and BeaverTails, we separate annotations of helpfulness and harmlessness for question-answering pairs, providing distinct perspectives on these coupled attributes. Overall, we provide 44.6k refined prompts and 265k question-answer pairs with safety meta-labels for 19 harm categories and three severity levels ranging from minor to severe, with answers generated by Llama-family models. Based on this, we collected 166.8k preference data, including dual-preference (helpfulness and harmlessness decoupled) and single-preference data (trade-off the helpfulness and harmlessness from scratch), respectively. Using the large-scale annotation data, we further train severity-sensitive moderation for the risk control of LLMs and safety-centric RLHF algorithms for the safety alignment of LLMs. We believe this dataset will be a valuable resource for the community, aiding in the safe deployment of LLMs.</abstract>
      <url hash="5b0b4b7a">2025.acl-long.1544</url>
      <bibkey>ji-etal-2025-pku</bibkey>
    </paper>
    <paper id="1545">
      <title>What Happened in <fixed-case>LLM</fixed-case>s Layers when Trained for Fast vs. Slow Thinking: A Gradient Perspective</title>
      <author><first>Ming</first><last>Li</last></author>
      <author><first>Yanhong</first><last>Li</last></author>
      <author><first>Tianyi</first><last>Zhou</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>32017-32154</pages>
      <abstract>What makes a difference in the post-training of LLMs? We investigate the training patterns of different layers in large language models (LLMs) through the lens of the gradient. We are specifically interested in how fast vs. slow thinking affects the layer-wise gradients, given the recent popularity of training LLMs on reasoning paths such as chain-of-thoughts (CoT) and process rewards. In our study, fast thinking without CoT leads to larger gradients and larger differences of gradients across layers than slow thinking (Detailed CoT), indicating the learning stability brought by the latter. Additionally, we study whether the gradient patterns can reflect the correctness of responses when training different LLMs using slow vs. fast thinking paths. The results show that the gradients of slow thinking can distinguish correct and irrelevant reasoning paths. As a comparison, we conduct similar gradient analyses on non-reasoning knowledge learning tasks, on which, however, trivially increasing the response length does not lead to similar behaviors of slow thinking. Our study strengthens fundamental understandings of LLM training and sheds novel insights on its efficiency and stability, which pave the way towards building a generalizable System-2 agent.</abstract>
      <url hash="6cc9702c">2025.acl-long.1545</url>
      <bibkey>li-etal-2025-happened</bibkey>
    </paper>
    <paper id="1546">
      <title>Beyond Text Compression: Evaluating Tokenizers Across Scales</title>
      <author><first>Jonas F.</first><last>Lotz</last><affiliation>University of Copenhagen</affiliation></author>
      <author><first>António V.</first><last>Lopes</last><affiliation>Apple</affiliation></author>
      <author><first>Stephan</first><last>Peitz</last><affiliation>Apple</affiliation></author>
      <author><first>Hendra</first><last>Setiawan</last><affiliation>Apple</affiliation></author>
      <author><first>Leonardo</first><last>Emili</last><affiliation>Apple</affiliation></author>
      <pages>32155-32173</pages>
      <abstract>The choice of tokenizer can profoundly impact language model performance, yet accessible and reliable evaluations of tokenizer quality remain an open challenge. Inspired by scaling consistency, we show that smaller models can accurately predict significant differences in tokenizer impact on larger models at a fraction of the compute cost. By systematically evaluating both English-centric and multilingual tokenizers, we find that tokenizer choice has negligible effects on tasks in English but results in consistent performance differences in multilingual settings. We propose new intrinsic tokenizer metrics inspired by Zipf’s law that correlate more strongly with downstream performance than text compression when modeling unseen languages. By combining several metrics to capture multiple aspects of tokenizer behavior, we develop a reliable framework for intrinsic tokenizer evaluations. Our work offers a more efficient path to informed tokenizer selection in future language model development.</abstract>
      <url hash="65e7004e">2025.acl-long.1546</url>
      <bibkey>lotz-etal-2025-beyond</bibkey>
    </paper>
    <paper id="1547">
      <title>Emergent Abilities of Large Language Models under Continued Pre-training for Language Adaptation</title>
      <author><first>Ahmed</first><last>Elhady</last></author>
      <author><first>Eneko</first><last>Agirre</last><affiliation>University of the Basque Country (UPV/EHU)</affiliation></author>
      <author><first>Mikel</first><last>Artetxe</last><affiliation>Reka AI</affiliation></author>
      <pages>32174-32186</pages>
      <abstract>Continued pretraining (CPT) is a popular approach to adapt existing large language models (LLMs) to new languages. When doing so, it is common practice to include a portion of English data in the mixture, but its role has not been carefully studied to date. In this work, we show that including English does not impact validation perplexity, yet it is critical for the emergence of downstream capabilities in the target language. We introduce a language-agnostic benchmark for in-context learning (ICL), which reveals catastrophic forgetting early on CPT when English is not included. This in turn damages the ability of the model to generalize to downstream prompts as measured by perplexity, even if it does not manifest in terms of accuracy until later in training, and can be tied to a big shift in the model parameters. Based on these insights, we introduce curriculum learning and exponential moving average (EMA) of weights as effective alternatives to mitigate the need for English. All in all, our work sheds light into the dynamics by which emergent abilities arise when doing CPT for language adaptation, and can serve as a foundation to design more effective methods in the future.</abstract>
      <url hash="64ceb883">2025.acl-long.1547</url>
      <bibkey>elhady-etal-2025-emergent</bibkey>
    </paper>
    <paper id="1548">
      <title><fixed-case>R</fixed-case>-Fairness: Assessing Fairness of Ranking in Subjective Data</title>
      <author><first>Lorenzo</first><last>Balzotti</last></author>
      <author><first>Donatella</first><last>Firmani</last><affiliation>University of Roma “La Sapienza”</affiliation></author>
      <author><first>Jerin George</first><last>Mathew</last></author>
      <author><first>Riccardo</first><last>Torlone</last><affiliation>Roma Tre University</affiliation></author>
      <author><first>Sihem</first><last>Amer-Yahia</last><affiliation>CNRS</affiliation></author>
      <pages>32187-32199</pages>
      <abstract>Subjective data, reflecting individual opinions, permeates platforms like Yelp and Amazon, influencing everyday decisions. Upon a user query, collaborative rating platforms return a collection of items ranked in an order that is often not transparent to the users. Then, each item is presented with a collection of reviews in an order that typically is, again, rather opaque. Despite the prevalence of such platforms, little attention has been given to fairness in their context, where groups writing best-ranked reviews for best-ranked items have more influence on users’ behavior. We design and evaluate a fairness assessment pipeline that starts with a data collection phase to gather reviews from real-world platforms, by submitting artificial user queries and iterating through rated items. Following that, a group assignment phase computes and infers relevant groups for each review, based on review content and user data. Finally, the third step assesses and evaluates the fairness of rankings for different user groups. The key contributions are comparing group exposure for different queries and platforms and comparing how popular fairness definitions behave in different settings. Experiments on real datasets reveal insights into the impact of item ranking on fairness computation and the varying robustness of these measures.</abstract>
      <url hash="1403b9da">2025.acl-long.1548</url>
      <bibkey>balzotti-etal-2025-r</bibkey>
    </paper>
    <paper id="1549">
      <title><fixed-case>R</fixed-case>e<fixed-case>P</fixed-case>anda: Pandas-powered Tabular Verification and Reasoning</title>
      <author><first>Atoosa</first><last>Chegini</last></author>
      <author><first>Keivan</first><last>Rezaei</last></author>
      <author><first>Hamid</first><last>Eghbalzadeh</last><affiliation>Meta and Meta</affiliation></author>
      <author><first>Soheil</first><last>Feizi</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>32200-32212</pages>
      <abstract>Fact-checking tabular data is essential for ensuring the accuracy of structured information in domains such as journalism, finance, and scientific research. However, existing methods often rely on black-box models with opaque reasoning. We introduce RePanda, a structured fact verification approach that translates claims into executable pandas queries, enabling interpretable and verifiable reasoning.To train RePanda, we construct PanTabFact, a structured dataset derived from TabFact, where claims are paired with executable queries generated using DeepSeek-Chat and refined through automated error correction. Fine-tuning DeepSeek-coder-7B-instruct-v1.5 on PanTabFact, RePanda achieves 84.09% accuracy on TabFact. To assess Out-of-Distribution (OOD) generalization, we create a dataset named WikiFact from WikiTableQuestions by transforming question-answer pairs into factual claims. Without additional fine-tuning, RePanda achieves 84.72% accuracy on WikiFact, significantly outperforming all other baselines and demonstrating strong OOD robustness. PanTabFact is publically available on HuggingFace at datasets/AtoosaChegini/PanTabFact.Beyond fact verification, RePanda extends to tabular question answering by generating executable queries that retrieve precise answers. To support this, we introduce PanWiki, a dataset mapping WikiTableQuestions to pandas queries. Fine-tuning on PanWiki, RePanda achieves 75.1% accuracy in direct answer retrieval. These results highlight the effectiveness of structured execution-based reasoning for tabular verification and question answering.</abstract>
      <url hash="946df8eb">2025.acl-long.1549</url>
      <bibkey>chegini-etal-2025-repanda</bibkey>
    </paper>
    <paper id="1550">
      <title>Towards Style Alignment in Cross-Cultural Translation</title>
      <author><first>Shreya</first><last>Havaldar</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Adam</first><last>Stein</last></author>
      <author><first>Eric</first><last>Wong</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Lyle</first><last>Ungar</last></author>
      <pages>32213-32230</pages>
      <abstract>Successful communication depends on the speaker’s intended style (i.e., what the speaker is trying to convey) aligning with the listener’s interpreted style (i.e., what the listener perceives). However, cultural differences often lead to misalignment between the two; for example, politeness is often lost in translation. We characterize the ways that LLMs fail to translate style – biasing translations towards neutrality and performing worse in non-Western languages. We mitigate these failures with RASTA (Retrieval-Augmented STylistic Alignment), a method that leverages learned stylistic concepts to encourage LLM translation to appropriately convey cultural communication norms and align style.</abstract>
      <url hash="588199bc">2025.acl-long.1550</url>
      <bibkey>havaldar-etal-2025-towards</bibkey>
    </paper>
    <paper id="1551">
      <title><fixed-case>T</fixed-case>i<fixed-case>C</fixed-case>-<fixed-case>LM</fixed-case>: A Web-Scale Benchmark for Time-Continual <fixed-case>LLM</fixed-case> Pretraining</title>
      <author><first>Jeffrey</first><last>Li</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <author><first>Mohammadreza</first><last>Armandpour</last><affiliation>Apple</affiliation></author>
      <author><first>Seyed Iman</first><last>Mirzadeh</last><affiliation>Apple</affiliation></author>
      <author><first>Sachin</first><last>Mehta</last><affiliation>Facebook</affiliation></author>
      <author><first>Vaishaal</first><last>Shankar</last><affiliation>Apple</affiliation></author>
      <author><first>Raviteja</first><last>Vemulapalli</last><affiliation>Apple</affiliation></author>
      <author><first>Samy</first><last>Bengio</last><affiliation>Apple</affiliation></author>
      <author><first>Oncel</first><last>Tuzel</last><affiliation>Apple</affiliation></author>
      <author><first>Mehrdad</first><last>Farajtabar</last><affiliation>Apple</affiliation></author>
      <author><first>Hadi</first><last>Pouransari</last><affiliation>Apple</affiliation></author>
      <author><first>Fartash</first><last>Faghri</last><affiliation>Apple</affiliation></author>
      <pages>32231-32273</pages>
      <abstract>Large Language Models (LLMs) trained on historical web data inevitably become outdated. We investigate evaluation strategies and update methods for LLMs as new data becomes available. We introduce a web-scale dataset for time-continual pretraining of LLMs derived from 114 dumps of Common Crawl (CC) – orders of magnitude larger than previous continual language modeling benchmarks. We also design time-stratified evaluations across both general CC data and specific domains (Wikipedia, StackExchange, and code documentation) to assess how well various continual learning methods adapt to new data while retaining past knowledge. Our findings demonstrate that, on general CC data, autoregressive meta-schedules combined with a fixed-ratio replay of older data can achieve comparable held-out loss to re-training from scratch, while requiring significantly less computation (2.6x). However, the optimal balance between incorporating new data and replaying old data differs as replay is crucial to avoid forgetting on generic web data but less so on specific domains.</abstract>
      <url hash="3a778b0f">2025.acl-long.1551</url>
      <bibkey>li-etal-2025-tic</bibkey>
    </paper>
    <paper id="1552">
      <title>Entailed Between the Lines: Incorporating Implication into <fixed-case>NLI</fixed-case></title>
      <author><first>Shreya</first><last>Havaldar</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Hamidreza</first><last>Alvari</last><affiliation>Google</affiliation></author>
      <author><first>John</first><last>Palowitch</last><affiliation>Google</affiliation></author>
      <author><first>Mohammad Javad</first><last>Hosseini</last><affiliation>Google</affiliation></author>
      <author><first>Senaka</first><last>Buthpitiya</last><affiliation>Google</affiliation></author>
      <author><first>Alex</first><last>Fabrikant</last><affiliation>Google Research</affiliation></author>
      <pages>32274-32290</pages>
      <abstract>Much of human communication depends on implication, conveying meaning beyond literal words to express a wider range of thoughts, intentions, and feelings. For models to better understand and facilitate human communication, they must be responsive to the text’s implicit meaning. We focus on Natural Language Inference (NLI), a core tool for many language tasks, and find that state-of-the-art NLI models and datasets struggle to recognize a range of cases where entailment is implied, rather than explicit from the text. We formalize implied entailment as an extension of the NLI task and introduce the Implied NLI dataset (INLI) to help today’s LLMs both recognize a broader variety of implied entailments and to distinguish between implicit and explicit entailment. We show how LLMs fine-tuned on INLI understand implied entailment and can generalize this understanding across datasets and domains.</abstract>
      <url hash="0f533fe8">2025.acl-long.1552</url>
      <bibkey>havaldar-etal-2025-entailed</bibkey>
    </paper>
    <paper id="1553">
      <title>Multi-Level Explanations for Generative Language Models</title>
      <author><first>Lucas</first><last>Monteiro Paes</last></author>
      <author><first>Dennis</first><last>Wei</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Hyo Jin</first><last>Do</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Hendrik</first><last>Strobelt</last><affiliation>Massachusetts Institute of Technology and International Business Machines</affiliation></author>
      <author><first>Ronny</first><last>Luss</last><affiliation>IBM</affiliation></author>
      <author><first>Amit</first><last>Dhurandhar</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Manish</first><last>Nagireddy</last><affiliation>IBM Research</affiliation></author>
      <author><first>Karthikeyan</first><last>Natesan Ramamurthy</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Prasanna</first><last>Sattigeri</last><affiliation>IBM Research</affiliation></author>
      <author><first>Werner</first><last>Geyer</last></author>
      <author><first>Soumya</first><last>Ghosh</last><affiliation>MERCK &amp; CO., INC.</affiliation></author>
      <pages>32291-32317</pages>
      <abstract>Despite the increasing use of large language models (LLMs) for context-grounded tasks like summarization and question-answering, understanding what makes an LLM produce a certain response is challenging. We propose Multi-Level Explanations for Generative Language Models (MExGen), a technique to provide explanations for context-grounded text generation. MExGen assigns scores to parts of the context to quantify their influence on the model’s output. It extends attribution methods like LIME and SHAP to LLMs used in context-grounded tasks where (1) inference cost is high, (2) input text is long, and (3) the output is text. We conduct a systematic evaluation, both automated and human, of perturbation-based attribution methods for summarization and question answering. The results show that our framework can provide more faithful explanations of generated output than available alternatives, including LLM self-explanations. We open-source code for MExGen as part of the ICX360 toolkit: https://github.com/IBM/ICX360.</abstract>
      <url hash="79d96a86">2025.acl-long.1553</url>
      <bibkey>monteiro-paes-etal-2025-multi</bibkey>
    </paper>
    <paper id="1554">
      <title>A Multi-Agent Framework for Mitigating Dialect Biases in Privacy Policy Question-Answering Systems</title>
      <author><first>Đorđe</first><last>Klisura</last></author>
      <author><first>Astrid R Bernaga</first><last>Torres</last><affiliation>University of Texas at San Antonio</affiliation></author>
      <author><first>Anna Karen</first><last>Gárate-Escamilla</last><affiliation>Instituto Tecnológico y de Estudios Superiores de Monterrey</affiliation></author>
      <author><first>Rajesh Roshan</first><last>Biswal</last><affiliation>Instituto Tecnológico y de Estudios Superiores de Monterrey</affiliation></author>
      <author><first>Ke</first><last>Yang</last><affiliation>University of Texas at San Antonio</affiliation></author>
      <author><first>Hilal</first><last>Pataci</last></author>
      <author><first>Anthony</first><last>Rios</last><affiliation>University of Texas at San Antonio</affiliation></author>
      <pages>32318-32337</pages>
      <abstract>Privacy policies inform users about data collection and usage, yet their complexity limits accessibility for diverse populations. Existing Privacy Policy Question Answering (QA) systems exhibit performance disparities across English dialects, disadvantaging speakers of non-standard varieties. We propose a novel multi-agent framework inspired by human-centered design principles to mitigate dialectal biases. Our approach integrates a Dialect Agent, which translates queries into Standard American English (SAE) while preserving dialectal intent, and a Privacy Policy Agent, which refines predictions using domain expertise. Unlike prior approaches, our method does not require retraining or dialect-specific fine-tuning, making it broadly applicable across models and domains. Evaluated on PrivacyQA and PolicyQA, our framework improves GPT-4o-mini’s zero-shot accuracy from 0.394 to 0.601 on PrivacyQA and from 0.352 to 0.464 on PolicyQA, surpassing or matching few-shot baselines without additional training data. These results highlight the effectiveness of structured agent collaboration in mitigating dialect biases and underscore the importance of designing NLP systems that account for linguistic diversity to ensure equitable access to privacy information.</abstract>
      <url hash="63206a5d">2025.acl-long.1554</url>
      <bibkey>klisura-etal-2025-multi</bibkey>
    </paper>
    <paper id="1555">
      <title>Low-Bit Quantization Favors Undertrained <fixed-case>LLM</fixed-case>s</title>
      <author><first>Xu</first><last>Ouyang</last></author>
      <author><first>Tao</first><last>Ge</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Thomas</first><last>Hartvigsen</last><affiliation>University of Virginia, Charlottesville</affiliation></author>
      <author><first>Zhisong</first><last>Zhang</last><affiliation>Tencent</affiliation></author>
      <author><first>Haitao</first><last>Mi</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Dong</first><last>Yu</last><affiliation>Tencent AI Lab</affiliation></author>
      <pages>32338-32348</pages>
      <abstract>Low-bit quantization improves machine learning model efficiency but surprisingly favors undertrained large language models (LLMs). Larger models or those trained on fewer tokens exhibit less quantization-induced degradation (QiD), while smaller, well-trained models face significant performance losses. To gain deeper insights into this trend, we study over 1500+ quantized LLM checkpoints of various sizes and at different training levels (undertrained or fully trained) in a controlled setting, deriving scaling laws for understanding the relationship between QiD and factors: the number of training tokens, model size and bit width.With our derived scaling laws, we propose a novel perspective that we can use QiD to measure an LLM’s training levels and determine the number of training tokens required for fully training LLMs of various sizes. Moreover, we use the scaling laws to predict the quantization performance of different-sized LLMs trained with tokens. Our projection shows that the low-bit quantization performance of future models, which are expected to be trained with over <tex-math>\textcolor{red}{100~trillion}</tex-math> tokens, may NOT be desirable. This poses a potential challenge for low-bit quantization in the future and highlights the need for awareness of a model’s training level when evaluating low-bit quantization research. To facilitate future research on this problem, we release all the 1500+ quantized checkpoints used in this work at https://huggingface.co/Xu-Ouyang.</abstract>
      <url hash="354d7369">2025.acl-long.1555</url>
      <bibkey>ouyang-etal-2025-low</bibkey>
    </paper>
    <paper id="1556">
      <title>Enhancing User-Controlled Text-to-Image Generation with Layout-Aware Personalization</title>
      <author><first>Hongliang</first><last>Luo</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Wei</first><last>Xi</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <pages>32349-32364</pages>
      <abstract>Recent diffusion-based models have advanced text-to-image synthesis, yet struggle to preserve fine visual details and enable precise spatial control in personalized content. We propose **LayoutFlex**, a novel framework that combines a Perspective-Adaptive Feature Extraction system with a Spatial Control Mechanism. Our approach captures fine-grained details via cross-modal representation learning and attention refinement, while enabling precise subject placement through coordinate-aware attention and region-constrained optimization. Experiments show LayoutFlex outperforms prior methods in visual fidelity (DINO <tex-math>\uparrow</tex-math>10.8%) and spatial accuracy (AP 43.1<tex-math>\pm</tex-math>1.2 vs. 19.3). LayoutFlex supports both single and multi-subject personalization, offering a powerful solution for controllable and coherent image generation in creative and interactive applications.</abstract>
      <url hash="11fef2e6">2025.acl-long.1556</url>
      <bibkey>luo-xi-2025-enhancing</bibkey>
    </paper>
    <paper id="1557">
      <title><fixed-case>LETS</fixed-case>-<fixed-case>C</fixed-case>: Leveraging Text Embedding for Time Series Classification</title>
      <author><first>Rachneet</first><last>Kaur</last><affiliation>J.P. Morgan Chase</affiliation></author>
      <author><first>Zhen</first><last>Zeng</last><affiliation>J.P. Morgan Chase</affiliation></author>
      <author><first>Tucker</first><last>Balch</last><affiliation>J.P. Morgan Chase</affiliation></author>
      <author><first>Manuela</first><last>Veloso</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <pages>32365-32399</pages>
      <abstract>Recent advancements in language modeling have shown promising results when applied to time series data. In particular, fine-tuning pre-trained large language models (LLMs) for time series classification tasks has achieved state-of-the-art (SOTA) performance on standard benchmarks. However, these LLM-based models have a significant drawback due to the large model size, with the number of trainable parameters in the millions. In this paper, we propose an alternative approach to leveraging the success of language modeling in the time series domain. Instead of fine-tuning LLMs, we utilize a text embedding model to embed time series and then pair the embeddings with a simple classification head composed of convolutional neural networks (CNN) and multilayer perceptron (MLP). We conducted extensive experiments on a well-established time series classification benchmark. We demonstrated LETS-C not only outperforms the current SOTA in classification accuracy but also offers a lightweight solution, using only 14.5% of the trainable parameters on average compared to the SOTA model. Our findings suggest that leveraging text embedding models to encode time series data, combined with a simple yet effective classification head, offers a promising direction for achieving high-performance time series classification while maintaining a lightweight model architecture.</abstract>
      <url hash="f410eb07">2025.acl-long.1557</url>
      <bibkey>kaur-etal-2025-lets</bibkey>
    </paper>
    <paper id="1558">
      <title><fixed-case>U</fixed-case>rban<fixed-case>V</fixed-case>ideo-Bench: Benchmarking Vision-Language Models on Embodied Intelligence with Video Data in Urban Spaces</title>
      <author><first>Baining</first><last>Zhao</last></author>
      <author><first>Jianjie</first><last>Fang</last></author>
      <author><first>Zichao</first><last>Dai</last></author>
      <author><first>Ziyou</first><last>Wang</last></author>
      <author><first>Jirong</first><last>Zha</last><affiliation>TBSI, Tsinghua University</affiliation></author>
      <author><first>Weichen</first><last>Zhang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Chen</first><last>Gao</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Yue</first><last>Wang</last></author>
      <author><first>Jinqiang</first><last>Cui</last><affiliation>Pengcheng Laboratory</affiliation></author>
      <author><first>Xinlei</first><last>Chen</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Yong</first><last>Li</last></author>
      <pages>32400-32423</pages>
      <abstract>Large multimodal models exhibit remarkable intelligence, yet their embodied cognitive abilities during motion in open-ended urban aerial spaces remain to be explored. We introduce a benchmark to evaluate whether video-large language models (Video-LLMs) can naturally process continuous first-person visual observations like humans, enabling recall, perception, reasoning, and navigation. We have manually control drones to collect 3D embodied motion video data from real-world cities and simulated environments, resulting in 1.5k video clips. Then we design a pipeline to generate 5.2k multiple-choice questions. Evaluations of 17 widely-used Video-LLMs reveal current limitations in urban embodied cognition. Correlation analysis provides insight into the relationships between different tasks, showing that causal reasoning has a strong correlation with recall, perception, and navigation, while the abilities for counterfactual and associative reasoning exhibit lower correlation with other tasks. We also validate the potential for Sim-to-Real transfer in urban embodiment through fine-tuning.</abstract>
      <url hash="d67c5524">2025.acl-long.1558</url>
      <bibkey>zhao-etal-2025-urbanvideo</bibkey>
    </paper>
    <paper id="1559">
      <title><fixed-case>HELIOS</fixed-case>: Harmonizing Early Fusion, Late Fusion, and <fixed-case>LLM</fixed-case> Reasoning for Multi-Granular Table-Text Retrieval</title>
      <author><first>Sungho</first><last>Park</last><affiliation>Pohang University of Science and Technology</affiliation></author>
      <author><first>Joohyung</first><last>Yun</last></author>
      <author><first>Jongwuk</first><last>Lee</last><affiliation>Sungkyunkwan University</affiliation></author>
      <author><first>Wook-Shin</first><last>Han</last><affiliation>POSTECH</affiliation></author>
      <pages>32424-32444</pages>
      <abstract>Table-text retrieval aims to retrieve relevant tables and text to support open-domain question answering. Existing studies use either early or late fusion, but face limitations. Early fusion pre-aligns a table row with its associated passages, forming “stars,” which often include irrelevant contexts and miss query-dependent relationships. Late fusion retrieves individual nodes, dynamically aligning them, but it risks missing relevant contexts. Both approaches also struggle with advanced reasoning tasks, such as column-wise aggregation and multi-hop reasoning. To address these issues, we propose HELIOS, which combines the strengths of both approaches. First, the edge-based bipartite subgraph retrieval identifies finer-grained edges between table segments and passages, effectively avoiding the inclusion of irrelevant contexts. Then, the query-relevant node expansion identifies the most promising nodes, dynamically retrieving relevant edges to grow the bipartite subgraph, minimizing the risk of missing important contexts. Lastly, the star-based LLM refinement performs logical inference at the star graph level rather than the bipartite subgraph, supporting advanced reasoning tasks. Experimental results show that HELIOS outperforms state-of-the-art models with a significant improvement up to 42.6% and 39.9% in recall and nDCG, respectively, on the OTT-QA benchmark.</abstract>
      <url hash="c3b68a2f">2025.acl-long.1559</url>
      <bibkey>park-etal-2025-helios</bibkey>
    </paper>
    <paper id="1560">
      <title><fixed-case>ONEB</fixed-case>ench to Test Them All: Sample-Level Benchmarking Over Open-Ended Capabilities</title>
      <author><first>Adhiraj</first><last>Ghosh</last><affiliation>Eberhard-Karls-Universität Tübingen</affiliation></author>
      <author><first>Sebastian</first><last>Dziadzio</last><affiliation>Eberhard-Karls-Universität Tübingen</affiliation></author>
      <author><first>Ameya</first><last>Prabhu</last><affiliation>Eberhard-Karls-Universität Tübingen</affiliation></author>
      <author><first>Vishaal</first><last>Udandarao</last><affiliation>Eberhard-Karls-Universität Tübingen and University of Cambridge</affiliation></author>
      <author><first>Samuel</first><last>Albanie</last><affiliation>Google</affiliation></author>
      <author><first>Matthias</first><last>Bethge</last><affiliation>University of Tuebingen</affiliation></author>
      <pages>32445-32481</pages>
      <abstract>Traditional fixed test datasets fall short in evaluating the open-ended capabilities of foundation models. To address this, we propose ONEBench (OpeN-Ended Benchmarking), a new paradigm that consolidates individual evaluation datasets into a unified, ever-expanding sample pool. ONEBench enables custom benchmarks for specific capabilities while reusing and aggregating samples, mitigating overfitting and dataset bias for broader capability assessment. It reframes model evaluation as selecting and aggregating sample-level tests.Transitioning from task-specific benchmarks to ONEBench introduces two challenges: heterogeneity (aggregating diverse metrics) and incompleteness(comparing models tested on different data subsets). To address these, we propose an aggregation algorithm that ensures identifiability (asymptotically recovering ground-truth scores) and rapid convergence, enabling accurate model comparisons with relatively little data. On homogenous datasets, our algorithm produces rankings that highly correlate with average scores. Moreover, it remains robust to over 95% missing measurements, reducing evaluation costs by up to 20x with minimal impact on rankings. We introduce ONEBench-LLM for language models and ONEBench-LMM for vision-language models, unifying evaluations across these domains, and enabling targeted model testing across diverse capabilities.</abstract>
      <url hash="1857921a">2025.acl-long.1560</url>
      <bibkey>ghosh-etal-2025-onebench</bibkey>
    </paper>
    <paper id="1561">
      <title>La Leaderboard: A Large Language Model Leaderboard for <fixed-case>S</fixed-case>panish Varieties and Languages of <fixed-case>S</fixed-case>pain and <fixed-case>L</fixed-case>atin <fixed-case>A</fixed-case>merica</title>
      <author><first>María</first><last>Grandury</last><affiliation>Universidad Politécnica de Madrid and Universidad Nacional de Educación a Distancia</affiliation></author>
      <author><first>Javier</first><last>Aula-Blasco</last><affiliation>Barcelona Supercomputing Center</affiliation></author>
      <author><first>Júlia</first><last>Falcão</last><affiliation>Barcelona Supercomputing Center</affiliation></author>
      <author><first>Clémentine</first><last>Fourrier</last><affiliation>HuggingFace</affiliation></author>
      <author><first>Miguel González</first><last>Saiz</last><affiliation>Universidad Politécnica de Madrid</affiliation></author>
      <author><first>Gonzalo</first><last>Martínez</last><affiliation>Universidad Carlos III de Madrid</affiliation></author>
      <author><first>Gonzalo Santamaria</first><last>Gomez</last><affiliation>Instituto de Ingeniería del Conocimiento</affiliation></author>
      <author><first>Rodrigo</first><last>Agerri</last><affiliation>University of the Basque Country</affiliation></author>
      <author><first>Nuria Aldama</first><last>García</last><affiliation>IIC</affiliation></author>
      <author><first>Luis</first><last>Chiruzzo</last><affiliation>Facultad de Ingeniería - Universidad de la República - Uruguay</affiliation></author>
      <author><first>Javier</first><last>Conde</last><affiliation>Universidad Politécnica de Madrid</affiliation></author>
      <author><first>Helena</first><last>Gomez Adorno</last><affiliation>Instituto de Investigaciones en Matemáticas Aplicadas y en Sistemas - UNAM</affiliation></author>
      <author><first>Marta Guerrero</first><last>Nieto</last><affiliation>Instituto de Ingeniería del Conocimiento</affiliation></author>
      <author><first>Guido</first><last>Ivetta</last><affiliation>Universidad Nacional de Córdoba</affiliation></author>
      <author><first>Natàlia López</first><last>Fuertes</last><affiliation>Universidad Autónoma de Madrid</affiliation></author>
      <author><first>Flor Miriam</first><last>Plaza-del-Arco</last><affiliation>Leiden University</affiliation></author>
      <author><first>María-Teresa</first><last>Martín-Valdivia</last><affiliation>Universidad de Jaén</affiliation></author>
      <author><first>Helena Montoro</first><last>Zamorano</last><affiliation>Instituto de Ingeniería del Conocimiento</affiliation></author>
      <author><first>Carmen Muñoz</first><last>Sanz</last><affiliation>IIC</affiliation></author>
      <author><first>Pedro</first><last>Reviriego</last></author>
      <author><first>Leire Rosado</first><last>Plaza</last><affiliation>Universidad Autónoma de Madrid</affiliation></author>
      <author><first>Alejandro</first><last>Vaca Serrano</last></author>
      <author><first>Estrella</first><last>Vallecillo-Rodríguez</last><affiliation>Universidad de Jaén</affiliation></author>
      <author><first>Jorge</first><last>Vallego</last><affiliation>Neovalle Ltd</affiliation></author>
      <author><first>Irune</first><last>Zubiaga</last><affiliation>Universidad del País Vasco</affiliation></author>
      <pages>32482-32524</pages>
      <abstract>Leaderboards showcase the current capabilities and limitations of Large Language Models (LLMs). To motivate the development of LLMs that represent the linguistic and cultural diversity of the Spanish-speaking community, we present La Leaderboard, the first open-source leaderboard to evaluate generative LLMs in languages and language varieties of Spain and Latin America. La Leaderboard is a community-driven project that aims to establish an evaluation standard for everyone interested in developing LLMs for the Spanish-speaking community. This initial version combines 66 datasets in Catalan, Basque, Galician, and different Spanish varieties, showcasing the evaluation results of 50 models. To encourage community-driven development of leaderboards in other languages, we explain our methodology, including guidance on selecting the most suitable evaluation setup for each downstream task. In particular, we provide a rationale for using fewer few-shot examples than typically found in the literature, aiming to reduce environmental impact and facilitate access to reproducible results for a broader research community.</abstract>
      <url hash="a690ecf5">2025.acl-long.1561</url>
      <bibkey>grandury-etal-2025-la</bibkey>
    </paper>
    <paper id="1562">
      <title>Why Prompt Design Matters and Works: A Complexity Analysis of Prompt Search Space in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Xiang</first><last>Zhang</last></author>
      <author><first>Juntai</first><last>Cao</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Chenyu</first><last>You</last><affiliation>State University of New York at Stony Brook</affiliation></author>
      <author><first>Dujian</first><last>Ding</last><affiliation>Computing Science, University of British Columbia</affiliation></author>
      <pages>32525-32555</pages>
      <abstract>Despite the remarkable successes of Large Language Models (LLMs), the underlying Transformer architecture has inherent limitations in handling complex reasoning tasks. Chain-of-Thought (CoT) prompting has emerged as a practical workaround, but most CoT-based methods rely on a single generic prompt like “think step by step,” with no task-specific adaptation. These approaches expect the model to discover an effective reasoning path on its own, forcing it to search through a vast prompt space. In contrast, many work has explored task-specific prompt designs to boost performance. However, these designs are typically developed through trial and error, lacking a theoretical ground. As a result, prompt engineering remains largely ad hoc and unguided.In this paper, we provide a theoretical framework that explains why some prompts succeed while others fail. We show that prompts function as selectors, extracting specific task-relevant information from the model’s full hidden state during CoT reasoning. Each prompt defines a unique trajectory through the answer space, and the choice of this trajectory is crucial for task performance and future navigation in the answer space.We analyze the complexity of finding optimal prompts and the size of the prompt space for a given task. Our theory reveals principles behind effective prompt design and shows that naive CoT—using model-self-guided prompt like “think step by step” —can severely hinder performance. Showing that optimal prompt search can lead to over a 50% improvement on reasoning tasks through experiments, our work provide a theoretical foundation for prompt engineering.</abstract>
      <url hash="69c512f8">2025.acl-long.1562</url>
      <bibkey>zhang-etal-2025-prompt-design</bibkey>
    </paper>
    <paper id="1563">
      <title>Energy Considerations of Large Language Model Inference and Efficiency Optimizations</title>
      <author><first>Jared</first><last>Fernandez</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Clara</first><last>Na</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Vashisth</first><last>Tiwari</last></author>
      <author><first>Yonatan</first><last>Bisk</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Sasha</first><last>Luccioni</last><affiliation>Hugging Face</affiliation></author>
      <author><first>Emma</first><last>Strubell</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>32556-32569</pages>
      <abstract>As large language models (LLMs) scale in size and adoption, their computational and environmental costs continue to rise. Prior benchmarking efforts have primarily focused on latency reduction in idealized settings, often overlooking the diverse real-world inference workloads that shape energy use. In this work, we systematically analyze the energy implications of common inference efficiency optimizations across diverse Natural Language Processing (NLP) and generative Artificial Intelligence (AI) workloads, including conversational AI and code generation. We introduce a modeling approach that approximates real-world LLM workflows through a binning strategy for input-output token distributions and batch size variations. Our empirical analysis spans software frameworks, decoding strategies, GPU architectures, online and offline serving settings, and model parallelism configurations. We show that the effectiveness of inference optimizations is *highly sensitive to workload geometry, software stack, and hardware accelerators*, demonstrating that naive energy estimates based on FLOPs or theoretical GPU utilization significantly underestimate real-world energy consumption.Our findings reveal that the proper application of relevant inference efficiency optimizations can reduce total energy use by up to **73%** from unoptimized baselines. These insights provide a foundation for sustainable LLM deployment and inform energy-efficient design strategies for future AI infrastructure.</abstract>
      <url hash="688bced1">2025.acl-long.1563</url>
      <bibkey>fernandez-etal-2025-energy</bibkey>
    </paper>
    <paper id="1564">
      <title>Optimizing Pre-Training Data Mixtures with Mixtures of Data Expert Models</title>
      <author><first>Lior</first><last>Belenki</last><affiliation>Google</affiliation></author>
      <author><first>Alekh</first><last>Agarwal</last><affiliation>Google</affiliation></author>
      <author><first>Tianze</first><last>Shi</last><affiliation>Google</affiliation></author>
      <author><first>Kristina</first><last>Toutanova</last><affiliation>INSAIT and Google</affiliation></author>
      <pages>32570-32587</pages>
      <abstract>We propose a method to optimize language model pre-training data mixtures through efficient approximation of the cross-entropy loss corresponding to each candidate mixture via a Mixture of Data Experts (MDE). We use this approximation as a source of additional features in a regression model, trained from observations of model loss for a small number of mixtures. Experiments with Transformer decoder-only language models in the range of 70M to 10B parameters on the SlimPajama dataset show that our method achieves significantly better performance than approaches that train regression models using only the mixture rates as input features. Combining this improved optimization method with an objective that takes into account cross-entropy on end task data leads to superior performance on few-shot downstream evaluations. We also provide theoretical insights on why aggregation of data expert predictions can provide good approximations to model losses for data mixtures.</abstract>
      <url hash="4e391c06">2025.acl-long.1564</url>
      <bibkey>belenki-etal-2025-optimizing</bibkey>
    </paper>
    <paper id="1565">
      <title><fixed-case>BFS</fixed-case>-Prover: Scalable Best-First Tree Search for <fixed-case>LLM</fixed-case>-based Automatic Theorem Proving</title>
      <author><first>Ran</first><last>Xin</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Chenguang</first><last>Xi</last></author>
      <author><first>Jie</first><last>Yang</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Feng</first><last>Chen</last><affiliation>Stanford University</affiliation></author>
      <author><first>Hang</first><last>Wu</last><affiliation>ByteDance Inc and Georgia Institute of Technology</affiliation></author>
      <author><first>Xia</first><last>Xiao</last></author>
      <author><first>Yifan</first><last>Sun</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Shen</first><last>Zheng</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Ming</first><last>Ding</last><affiliation>ByteDance Inc.</affiliation></author>
      <pages>32588-32599</pages>
      <abstract>Recent advancements in large language models (LLMs) have spurred growing interest in automatic theorem proving using Lean4, where effective tree search methods are crucial for navigating the underlying large proof search spaces. While the existing approaches primarily rely on value functions and/or Monte Carlo Tree Search (MCTS), the potential of simpler methods like Best-First Tree Search (BFS) remains underexplored. In this paper, we investigate whether BFS can achieve competitive performance in large-scale theorem proving tasks. We present BFS-Prover, a scalable expert iteration framework, featuring three key innovations. First, we implement strategic data filtering at each expert iteration round, excluding problems solvable via beam search node expansion to focus on harder cases. Second, we improve the sample efficiency of BFS through Direct Preference Optimization (DPO) applied to state-tactic pairs automatically annotated with compiler error feedback, refining the LLM’s policy to prioritize productive expansions. Third, we employ length normalization in BFS to encourage exploration of deeper proof paths. BFS-Prover achieves a state-of-the-art score of 72.95 on the MiniF2F test set and therefore challenges the perceived necessity of complex tree search methods, demonstrating that BFS can achieve competitive performance when properly scaled.</abstract>
      <url hash="9bfd665e">2025.acl-long.1565</url>
      <bibkey>xin-etal-2025-bfs</bibkey>
    </paper>
    <paper id="1566">
      <title>Magnet: Multi-turn Tool-use Data Synthesis and Distillation via Graph Translation</title>
      <author><first>Fan</first><last>Yin</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Zifeng</first><last>Wang</last><affiliation>Google</affiliation></author>
      <author><first>I-Hung</first><last>Hsu</last><affiliation>Google</affiliation></author>
      <author><first>Jun</first><last>Yan</last><affiliation>Google</affiliation></author>
      <author><first>Ke</first><last>Jiang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Yanfei</first><last>Chen</last><affiliation>Google</affiliation></author>
      <author><first>Jindong</first><last>Gu</last><affiliation>University of Oxford</affiliation></author>
      <author><first>Long</first><last>Le</last><affiliation>Google</affiliation></author>
      <author><first>Kai-Wei</first><last>Chang</last><affiliation>University of California, Los Angeles and Amazon</affiliation></author>
      <author><first>Chen-Yu</first><last>Lee</last><affiliation>Google</affiliation></author>
      <author><first>Hamid</first><last>Palangi</last><affiliation>Google</affiliation></author>
      <author><first>Tomas</first><last>Pfister</last><affiliation>Google</affiliation></author>
      <pages>32600-32616</pages>
      <abstract>Large language models (LLMs) have exhibited the ability to effectively utilize external tools to address user queries. However, their performance may be limited in complex, multi-turn interactions involving users and multiple tools. To address this, we propose Magnet, a principled framework for synthesizing high-quality training trajectories to enhance the function calling capability of large language model agents in multi-turn conversations with humans. The framework is based on automatic and iterative translations from a function signature path to a sequence of queries and executable function calls. We model the complicated function interactions in multi-turn cases with graph and design novel node operations to build reliable signature paths. Motivated by context distillation, when guiding the generation of positive and negative trajectories using a teacher model, we provide reference function call sequences as positive hints in context and contrastive, incorrect function calls as negative hints. Experiments show that training with the positive trajectories with supervised fine-tuning and preference optimization against negative trajectories, our 14B model, Magnet-14B-mDPO, obtains 68.01 on BFCL-v3 and 73.30 on ToolQuery, surpassing the performance of the teacher model Gemini-1.5-pro-002 by a large margin in function calling.</abstract>
      <url hash="b508f7e0">2025.acl-long.1566</url>
      <bibkey>yin-etal-2025-magnet</bibkey>
    </paper>
    <paper id="1567">
      <title>Logic-Regularized Verifier Elicits Reasoning from <fixed-case>LLM</fixed-case>s</title>
      <author><first>Xinyu</first><last>Wang</last></author>
      <author><first>Changzhi</first><last>Sun</last><affiliation>China Telecom</affiliation></author>
      <author><first>Lian</first><last>Cheng</last></author>
      <author><first>Yuanbin</first><last>Wu</last></author>
      <author><first>Dell</first><last>Zhang</last><affiliation>China Telecom</affiliation></author>
      <author><first>Xiaoling</first><last>Wang</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Xuelong</first><last>Li</last><affiliation>China Telecom and Northwestern Polytechnical University</affiliation></author>
      <pages>32617-32630</pages>
      <abstract>Verifiers are crucial components for enhancing modern LLMs’ reasoning capability. Typical verifiers require resource-intensive supervised dataset construction, which is costly and faces limitations in data diversity. In this paper, we propose LOVER, an unsupervised verifier regularized by logical rules. LOVER treats the verifier as a binary latent variable, utilizing internal activations and enforcing three logical constraints on multiple reasoning paths: negation consistency, intra-group consistency, and inter-group consistency (grouped by the final answer). By incorporating logical rules as priors, LOVER can leverage unlabeled examples and is directly compatible with any off-the-shelf LLMs. Experiments on 10 datasets demonstrate that LOVER significantly outperforms unsupervised baselines, achieving performance comparable to the supervised verifier (reaching its 95% level on average).</abstract>
      <url hash="f3c81ca0">2025.acl-long.1567</url>
      <bibkey>wang-etal-2025-logic</bibkey>
    </paper>
    <paper id="1568">
      <title>Squeezed Attention: Accelerating Long Context Length <fixed-case>LLM</fixed-case> Inference</title>
      <author><first>Coleman Richard Charles</first><last>Hooper</last></author>
      <author><first>Sehoon</first><last>Kim</last><affiliation>xAI</affiliation></author>
      <author><first>Hiva</first><last>Mohammadzadeh</last></author>
      <author><first>Monishwaran</first><last>Maheswaran</last></author>
      <author><first>Sebastian</first><last>Zhao</last></author>
      <author><first>June</first><last>Paik</last></author>
      <author><first>Michael W.</first><last>Mahoney</last></author>
      <author><first>Kurt</first><last>Keutzer</last><affiliation>University of California Berkeley</affiliation></author>
      <author><first>Amir</first><last>Gholami</last><affiliation>University of California Berkeley</affiliation></author>
      <pages>32631-32652</pages>
      <abstract>Emerging Large Language Model (LLM) applications require long input context in order to perform complex tasks like document analysis and code generation.For these long context length applications, the length of the input prompt poses a significant challenge in terms of inference efficiency since the inference costs increase linearly with sequence length.However, for many of these applications, much of the context in the prompt is fixed across different user inputs, thereby providing the opportunity to perform offline optimizations in order to process user inputs quickly, as they are received. We propose Squeezed Attention to accelerate LLM applications where a large portion of the input context is fixed.We first leverage K-means clustering offline to group the keys for the fixed context based on semantic similarity and represent each cluster with a single centroid value.During inference, we compare query tokens from the user input with the centroids to predict which keys from the fixed context are semantically relevant, and then compute exact attention using only the important keys, thereby reducing bandwidth and computational costs. We also present a hierarchical version of our algorithm which can reduce the complexity of attention from linear to logarithmic with respect to the fixed context length.We evaluate our method on various long-context benchmarks including LongBench, where it achieves a 3.1<tex-math>\times</tex-math> reduction in KV budget with no noticeable accuracy loss and up to an 8<tex-math>\times</tex-math> reduction with only a 0.5 point accuracy gap for the LLaMA-2-7B-32K, LWM-Text-Chat-1M, and Longchat-7B-v1.5-32K models.Futhermore, we implement kernels for centroid comparison and sparse FlashAttention with important keys, achieving more than 4<tex-math>\times</tex-math> speedups during both the prefill and generation phases for long-context inference.Our code is available at https://github.com/SqueezeAILab/SqueezedAttention.</abstract>
      <url hash="b0ad9a44">2025.acl-long.1568</url>
      <bibkey>hooper-etal-2025-squeezed</bibkey>
    </paper>
    <paper id="1569">
      <title><fixed-case>L</fixed-case>ang<fixed-case>M</fixed-case>ark: A Multilingual Dataset for Automatic Post-Editing</title>
      <author><first>Diego</first><last>Velazquez</last><affiliation>Welocalize</affiliation></author>
      <author><first>Mikaela</first><last>Grace</last></author>
      <author><first>Konstantinos</first><last>Karageorgos</last><affiliation>Welocalize</affiliation></author>
      <author><first>Lawrence</first><last>Carin</last><affiliation>Duke University</affiliation></author>
      <author><first>Aaron</first><last>Schliem</last><affiliation>Welocalize</affiliation></author>
      <author><first>Dimitrios</first><last>Zaikis</last><affiliation>Aristotle University of Thessaloniki</affiliation></author>
      <author><first>Roger</first><last>Wechsler</last><affiliation>Welocalize</affiliation></author>
      <pages>32653-32667</pages>
      <abstract>Automatic post-editing (APE) aims to correct errors in machine-translated text, enhancing translation quality, while reducing the need for human intervention. Despite advances in neural machine translation (NMT), the development of effective APE systems has been hindered by the lack of large-scale multilingual datasets specifically tailored to NMT outputs. To address this gap, we present and release LangMark, a new human-annotated multilingual APE dataset for English translation to seven languages: Brazilian Portuguese, French, German, Italian, Japanese, Russian, and Spanish. The dataset has 206,983 triplets, with each triplet consisting of a source segment, its NMT output, and a human post-edited translation. Annotated by expert human linguists, our dataset offers both linguistic diversity and scale. Leveraging this dataset, we empirically show that Large Language Models (LLMs) with few-shot prompting can effectively perform APE, improving upon leading commercial and even proprietary machine translation systems. We believe that this new resource will facilitate the future development and evaluation of APE systems.</abstract>
      <url hash="85060dc3">2025.acl-long.1569</url>
      <bibkey>velazquez-etal-2025-langmark</bibkey>
    </paper>
    <paper id="1570">
      <title>Neural Parameter Search for Slimmer Fine-Tuned Models and Better Transfer</title>
      <author><first>Guodong</first><last>Du</last></author>
      <author><first>Zitao</first><last>Fang</last></author>
      <author><first>Jing</first><last>Li</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Junlin</first><last>Li</last></author>
      <author><first>Runhua</first><last>Jiang</last></author>
      <author><first>Shuyang</first><last>Yu</last></author>
      <author><first>Yifei</first><last>Guo</last></author>
      <author><first>Yangneng</first><last>Chen</last></author>
      <author><first>Sim Kuan</first><last>Goh</last><affiliation>Xiamen University Malaysia</affiliation></author>
      <author><first>Ho-Kin</first><last>Tang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Daojing</first><last>He</last></author>
      <author><first>Honghai</first><last>Liu</last></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>32668-32687</pages>
      <abstract>Foundation models and their checkpoints have significantly advanced deep learning, boosting performance across various applications. However, fine-tuned models often struggle outside their specific domains and exhibit considerable redundancy. Recent studies suggest that combining a pruned fine-tuned model with the original pre-trained model can mitigate forgetting, reduce interference when merging model parameters across tasks, and improve compression efficiency. In this context, developing an effective pruning strategy for fine-tuned models is crucial. Leveraging the advantages of the task vector mechanism, we preprocess fine-tuned models by calculating the differences between them and the original model. Recognizing that different task vector subspaces contribute variably to model performance, we introduce a novel method called **N**eural **P**arameter **S**earch (**NPS**) for slimming down fine-tuned models. This method enhances pruning efficiency by searching through neural parameters of task vectors within low-rank subspaces. Our method has three key applications: enhancing knowledge transfer through pairwise model interpolation, facilitating effective knowledge fusion via model merging, and enabling the deployment of compressed models that retain near-original performance while significantly reducing storage costs. Extensive experiments across vision, NLP, and multi-modal benchmarks demonstrate the effectiveness and robustness of our approach, resulting in substantial performance gains.</abstract>
      <url hash="0b0ec35c">2025.acl-long.1570</url>
      <bibkey>du-etal-2025-neural</bibkey>
    </paper>
    <paper id="1571">
      <title>Merge Hijacking: Backdoor Attacks to Model Merging of Large Language Models</title>
      <author><first>Zenghui</first><last>Yuan</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Yangming</first><last>Xu</last></author>
      <author><first>Jiawen</first><last>Shi</last><affiliation>University of Waterloo, University of Waterloo</affiliation></author>
      <author><first>Pan</first><last>Zhou</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Lichao</first><last>Sun</last><affiliation>Lehigh University</affiliation></author>
      <pages>32688-32703</pages>
      <abstract>Model merging for Large Language Models (LLMs) directly fuses the parameters of different models finetuned on various tasks, creating a unified model for multi-domain tasks. However, due to potential vulnerabilities in models available on open-source platforms, model merging is susceptible to backdoor attacks. In this paper, we propose <tex-math>\textit{Merge Hijacking}</tex-math>, the first backdoor attack targeting model merging in LLMs. The attacker constructs a malicious upload model and releases it. Once a victim user merges it with any other models, the resulting merged model inherits the backdoor while maintaining utility across tasks. Merge Hijacking defines two main objectives—effectiveness and utility—and achieves them through four steps. Extensive experiments demonstrate the effectiveness of our attack across different models, merging algorithms, and tasks. Additionally, we show that the attack remains effective even when merging real-world models. Moreover, our attack demonstrates robustness against two inference-time defenses (Paraphrasing and CLEANGEN) and one training-time defense (Fine-pruning).</abstract>
      <url hash="9c4317dd">2025.acl-long.1571</url>
      <bibkey>yuan-etal-2025-merge</bibkey>
    </paper>
    <paper id="1572">
      <title>Where Are We? Evaluating <fixed-case>LLM</fixed-case> Performance on <fixed-case>A</fixed-case>frican Languages</title>
      <author><first>Ife</first><last>Adebara</last></author>
      <author><first>Hawau Olamide</first><last>Toyin</last></author>
      <author><first>Nahom Tesfu</first><last>Ghebremichael</last></author>
      <author><first>AbdelRahim A.</first><last>Elmadany</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Muhammad</first><last>Abdul-Mageed</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and University of British Columbia</affiliation></author>
      <pages>32704-32731</pages>
      <abstract>Africa’s rich linguistic heritage remains underrepresented in NLP, largely due to historical policies that favor foreign languages and create significant data inequities. In this paper, we integrate theoretical insights on Africa’s language landscape with an empirical evaluation using Sahara— a comprehensive benchmark curated from large-scale, publicly accessible datasets capturing the continent’s linguistic diversity. By systematically assessing the performance of leading large language models (LLMs) on Sahara, we demonstrate how policy-induced data variations directly impact model effectiveness across African languages. Our findings reveal that while a few languages perform reasonably well, many Indigenous languages remain marginalized due to sparse data. Leveraging these insights, we offer actionable recommendations for policy reforms and inclusive data practices. Overall, our work underscores the urgent need for a dual approach—combining theoretical understanding with empirical evaluation—to foster linguistic diversity in AI for African communities.</abstract>
      <url hash="9aa0449a">2025.acl-long.1572</url>
      <bibkey>adebara-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="1573">
      <title>Beyond Output Matching: Bidirectional Alignment for Enhanced In-Context Learning</title>
      <author><first>Chengwei</first><last>Qin</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Wenhan</first><last>Xia</last></author>
      <author><first>Fangkai</first><last>Jiao</last></author>
      <author><first>Chen</first><last>Chen</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Yuchen</first><last>Hu</last></author>
      <author><first>Bosheng</first><last>Ding</last></author>
      <author><first>Ruirui</first><last>Chen</last><affiliation>Institute of High Performance Computing, Singapore, A*STAR</affiliation></author>
      <author><first>Shafiq</first><last>Joty</last><affiliation>Nanyang Technological University and SalesForce.com</affiliation></author>
      <pages>32732-32758</pages>
      <abstract>Large language models (LLMs) have shown impressive few-shot generalization on many tasks via in-context learning (ICL). Despite their success in showing such emergent abilities, the scale and complexity of larger models also lead to unprecedentedly high computational demands and deployment challenges. In reaction, researchers explore transferring the powerful capabilities of larger models to more efficient and compact models by typically aligning the output of smaller (student) models with that of larger (teacher) models. Existing methods either train student models on the generated outputs of teacher models or imitate their token-level probability distributions. However, these distillation methods pay little to no attention to the input, which also plays a crucial role in ICL. Based on the finding that the performance of ICL is highly sensitive to the selection of demonstration examples, we propose Bidirectional Alignment (BiAlign) to fully leverage the models’ preferences for ICL examples to improve the ICL abilities of student models. Specifically, we introduce the alignment of input preferences between student and teacher models by incorporating a novel ranking loss, in addition to aligning the token-level output distribution. With extensive experiments and analysis, we demonstrate that BiAlign can consistently outperform existing baselines on a variety of tasks involving language understanding, reasoning, and coding.</abstract>
      <url hash="79bbee1d">2025.acl-long.1573</url>
      <bibkey>qin-etal-2025-beyond</bibkey>
    </paper>
    <paper id="1574">
      <title><fixed-case>C</fixed-case>ite<fixed-case>E</fixed-case>val: Principle-Driven Citation Evaluation for Source Attribution</title>
      <author><first>Yumo</first><last>Xu</last><affiliation>AWS AI Labs</affiliation></author>
      <author><first>Peng</first><last>Qi</last><affiliation>Orby AI</affiliation></author>
      <author><first>Jifan</first><last>Chen</last><affiliation>Amazon</affiliation></author>
      <author><first>Kunlun</first><last>Liu</last></author>
      <author><first>Rujun</first><last>Han</last><affiliation>Google</affiliation></author>
      <author><first>Lan</first><last>Liu</last></author>
      <author><first>Bonan</first><last>Min</last><affiliation>Amazon and Tufts University</affiliation></author>
      <author><first>Vittorio</first><last>Castelli</last><affiliation>Amazon</affiliation></author>
      <author><first>Arshit</first><last>Gupta</last><affiliation>Amazon</affiliation></author>
      <author><first>Zhiguo</first><last>Wang</last></author>
      <pages>32759-32778</pages>
      <abstract>Citation quality is crucial in information-seeking systems, directly influencing trust and the effectiveness of information access. Current evaluation frameworks, both human and automatic, mainly rely on Natural Language Inference (NLI) to assess binary or ternary supportiveness from cited sources, which we argue is a suboptimal proxy for citation evaluation. In this work we introduce CiteEval, a citation evaluation framework driven by principles focusing on fine-grained citation assessment within a broad context, encompassing not only the cited sources but the full retrieval context, user query, and generated text. Guided by the proposed framework, we construct CiteBench, a multi-domain benchmark with high-quality human annotations on citation quality. To enable efficient evaluation, we further develop CiteEval-Auto, a suite of model-based metrics that exhibit strong correlation with human judgments. Experiments across diverse systems demonstrate CiteEval-Auto’s superior ability to capture the multifaceted nature of citations compared to existing metrics, offering a principled and scalable approach to evaluate and improve model-generated citations.</abstract>
      <url hash="c64fb565">2025.acl-long.1574</url>
      <bibkey>xu-etal-2025-citeeval</bibkey>
    </paper>
    <paper id="1575">
      <title><fixed-case>H</fixed-case>i<fixed-case>A</fixed-case>gent: Hierarchical Working Memory Management for Solving Long-Horizon Agent Tasks with Large Language Model</title>
      <author><first>Mengkang</first><last>Hu</last></author>
      <author><first>Tianxing</first><last>Chen</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Qiguang</first><last>Chen</last></author>
      <author><first>Yao</first><last>Mu</last></author>
      <author><first>Wenqi</first><last>Shao</last></author>
      <author><first>Ping</first><last>Luo</last><affiliation>The University of Hong Kong</affiliation></author>
      <pages>32779-32798</pages>
      <abstract>Large Language Model (LLM)-based agents exhibit significant potential across various domains, operating as interactive systems that process environmental observations to generate executable actions for target tasks. The effectiveness of these agents is significantly influenced by their memory mechanism, which records historical experiences as sequences of action-observation pairs. We categorize memory into two types: cross-trial memory, accumulated across multiple attempts, and in-trial memory (working memory), accumulated within a single attempt. While considerable research has optimized performance through cross-trial memory, the enhancement of agent performance through improved working memory utilization remains underexplored. Instead, existing approaches often involve directly inputting entire historical action-observation pairs into LLMs, leading to redundancy in long-horizon tasks. Inspired by human problem-solving strategies, this paper introduces HiAgent, a framework that leverages subgoals as memory chunks to manage the working memory of LLM-based agents hierarchically. Specifically, HiAgent prompts LLMs to formulate subgoals before generating executable actions and enables LLMs to decide proactively to replace previous subgoals with summarized observations, retaining only the action-observation pairs relevant to the current subgoal. Experimental results across five long-horizon tasks demonstrate that HiAgent achieves a twofold increase in success rate and reduces the average number of steps required by 3.8. Additionally, our analysis shows that HiAgent consistently improves performance across various steps, highlighting its robustness and generalizability. Code is available in this URL: https://github.com/HiAgent2024/HiAgent</abstract>
      <url hash="16beb04e">2025.acl-long.1575</url>
      <bibkey>hu-etal-2025-hiagent</bibkey>
    </paper>
    <paper id="1576">
      <title><fixed-case>E</fixed-case>ducation<fixed-case>Q</fixed-case>: Evaluating <fixed-case>LLM</fixed-case>s’ Teaching Capabilities Through Multi-Agent Dialogue Framework</title>
      <author><first>Yao</first><last>Shi</last></author>
      <author><first>Rongkeng</first><last>Liang</last><affiliation>Education Innovation Research Institute of Guangdong</affiliation></author>
      <author><first>Yong</first><last>Xu</last><affiliation>Peng Cheng Laboratory and South China University of Technology</affiliation></author>
      <pages>32799-32828</pages>
      <abstract>Large Language Models (LLMs) increasingly serve as educational tools, yet evaluating their teaching capabilities remains challenging due to the resource-intensive, context-dependent, and methodologically complex nature of teacher-student interactions. We introduce EducationQ, a multi-agent dialogue framework that efficiently assesses teaching capabilities through simulated dynamic educational scenarios, featuring specialized agents for teaching, learning, and evaluation. Testing 14 LLMs across major AI Organizations (OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13 disciplines and 10 difficulty levels reveals that teaching effectiveness does not correlate linearly with model scale or general reasoning capabilities - with some smaller open-source models outperforming larger commercial counterparts in teaching contexts. This finding highlights a critical gap in current evaluations that prioritize knowledge recall over interactive pedagogy. Our mixed-methods evaluation, combining quantitative metrics with qualitative analysis and expert case studies, identifies distinct pedagogical strengths employed by top-performing models (e.g., sophisticated questioning strategies, adaptive feedback mechanisms). Human expert evaluations show 78% agreement with our automated qualitative analysis of effective teaching behaviors, validating our methodology. EducationQ demonstrates that LLMs-as-Teachers require specialized optimization beyond simple scaling, suggesting next-generation educational AI prioritize targeted enhancement of specific pedagogical effectiveness.</abstract>
      <url hash="501fa87e">2025.acl-long.1576</url>
      <bibkey>shi-etal-2025-educationq</bibkey>
    </paper>
    <paper id="1577">
      <title><fixed-case>KRISTEVA</fixed-case>: Close Reading as a Novel Task for Benchmarking Interpretive Reasoning</title>
      <author><first>Peiqi</first><last>Sui</last></author>
      <author><first>Juan Diego</first><last>Rodriguez</last><affiliation>University of Texas at Austin</affiliation></author>
      <author><first>Philippe</first><last>Laban</last><affiliation>Microsoft</affiliation></author>
      <author><first>J. Dean</first><last>Murphy</last><affiliation>University of Texas at Austin</affiliation></author>
      <author><first>Joseph P.</first><last>Dexter</last></author>
      <author><first>Richard Jean</first><last>So</last></author>
      <author><first>Samuel</first><last>Baker</last><affiliation>University of Texas at Austin</affiliation></author>
      <author><first>Pramit</first><last>Chaudhuri</last><affiliation>University of Texas at Austin</affiliation></author>
      <pages>32829-32849</pages>
      <abstract>Each year, tens of millions of essays are written and graded in college-level English courses. Students are asked to analyze literary and cultural texts through a process known as close reading, where they gather textual details from which to formulate evidence-based arguments. Despite being viewed as a basis for critical thinking and widely adopted as a required element of university coursework, close reading has never been evaluated on large language models (LLMs), and multi-discipline benchmarks like MMLU do not include literature as a subject. To fill this gap, we present KRISTEVA, the first close reading benchmark for evaluating interpretive reasoning, consisting of 1331 multiple-choice questions adapted from classroom data. With KRISTEVA, we propose three progressively more difficult sets of tasks to approximate different elements of the close reading process, which we use to test how well LLMs understand and reason about literary works: 1) extracting stylistic features, 2) retrieving relevant contextual information from parametric knowledge, and 3) multi-hop reasoning between style and external contexts. Our baseline results find that while state-of-the-art LLMs possess some college-level close reading competency (accuracy 49.7% - 69.7%), their performances still trail those of experienced human evaluators on 10 out of our 11 tasks.</abstract>
      <url hash="321a9bfa">2025.acl-long.1577</url>
      <bibkey>sui-etal-2025-kristeva</bibkey>
    </paper>
    <paper id="1578">
      <title>Efficient Domain Continual pretraining by Mitigating the Stability Gap</title>
      <author><first>Yiduo</first><last>Guo</last></author>
      <author><first>Jie</first><last>Fu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Huishuai</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Dongyan</first><last>Zhao</last><affiliation>Peking University</affiliation></author>
      <pages>32850-32870</pages>
      <abstract>Continual pretraining enables Large Language Models (LLMs) to adapt to specialized domains like medicine and law. However, we observe a consistent phenomenon across different model sizes and domains: a temporary performance drop at the start of the continual pretraining process, followed by a performance recovery phase. To gain a deeper understanding of this issue, we use the stability gap— a concept adapted from the visual domain—which explains this initial drop arises from instability in the model’s general abilities. We validate this hypothesis through a series of experiments. To address this initial instability and enhance LLM performance within a fixed compute budget, we propose a training strategy that mitigates instability by increasing the number of epochs, alongside two data sampling strategies targeting data domain relevance and corpus distribution. We conduct experiments on Llama-family models to validate the effectiveness of our strategies for continual pretraining and instruction tuning in medical and legal domains. Our strategies improve the average medical task performance of the OpenLlama-3B model from 36.2% to 40.7% using only 40% of the original training budget, while also enhancing general task performance without causing forgetting. Furthermore, we aPPLy our strategies to continually pre-train and instruction-tune the Llama-3-8B model. The resulting model, Llama-3-Physician, achieves the best medical performance among open-source models on several benchmarks and rivals GPT-4 on specific tasks. We release our models at https://huggingface.co/YiDuo1999/Llama-3-Physician-8B-Instruct.</abstract>
      <url hash="5939614e">2025.acl-long.1578</url>
      <bibkey>guo-etal-2025-efficient-domain</bibkey>
    </paper>
    <paper id="1579">
      <title>Palm: A Culturally Inclusive and Linguistically Diverse Dataset for <fixed-case>A</fixed-case>rabic <fixed-case>LLM</fixed-case>s</title>
      <author><first>Fakhraddin</first><last>Alwajih</last></author>
      <author><first>Abdellah</first><last>El Mekki</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Samar Mohamed</first><last>Magdy</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>AbdelRahim A.</first><last>Elmadany</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Omer</first><last>Nacar</last></author>
      <author><first>El Moatez Billah</first><last>Nagoudi</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Reem</first><last>Abdel-Salam</last><affiliation>Faculty of Engineering Cairo University, Cairo University</affiliation></author>
      <author><first>Hanin</first><last>Atwany</last></author>
      <author><first>Youssef</first><last>Nafea</last></author>
      <author><first>Abdulfattah Mohammed</first><last>Yahya</last><affiliation>Misr University for Science and Technology</affiliation></author>
      <author><first>Rahaf</first><last>Alhamouri</last><affiliation>Jordan University of Science and Technology</affiliation></author>
      <author><first>Hamzah A.</first><last>Alsayadi</last><affiliation>Ibb University</affiliation></author>
      <author><first>Hiba</first><last>Zayed</last><affiliation>Birzeit University</affiliation></author>
      <author><first>Sara</first><last>Shatnawi</last></author>
      <author><first>Serry</first><last>Sibaee</last><affiliation>prince sultan university</affiliation></author>
      <author><first>Yasir</first><last>Ech-chammakhy</last></author>
      <author><first>Walid</first><last>Al-Dhabyani</last><affiliation>Cairo University</affiliation></author>
      <author><first>Marwa Mohamed</first><last>Ali</last></author>
      <author><first>Imen</first><last>Jarraya</last><affiliation>Prince Sultan University</affiliation></author>
      <author><first>Ahmed Oumar</first><last>El-Shangiti</last></author>
      <author><first>Aisha</first><last>Alraeesi</last></author>
      <author><first>Mohammed Anwar</first><last>AL-Ghrawi</last></author>
      <author><first>Abdulrahman S.</first><last>Al-Batati</last><affiliation>Prince Sultan University</affiliation></author>
      <author><first>Elgizouli</first><last>Mohamed</last></author>
      <author><first>Noha Taha</first><last>Elgindi</last></author>
      <author><first>Muhammed</first><last>Saeed</last></author>
      <author><first>Houdaifa</first><last>Atou</last><affiliation>University Mohammed VI Polytechnic</affiliation></author>
      <author><first>Issam Ait</first><last>Yahia</last></author>
      <author><first>Abdelhak</first><last>Bouayad</last></author>
      <author><first>Mohammed</first><last>Machrouh</last></author>
      <author><first>Amal</first><last>Makouar</last></author>
      <author><first>Dania</first><last>Alkawi</last></author>
      <author><first>Mukhtar</first><last>Mohamed</last></author>
      <author><first>Safaa Taher</first><last>Abdelfadil</last></author>
      <author><first>Amine Ziad</first><last>Ounnoughene</last></author>
      <author><first>Anfel</first><last>Rouabhia</last></author>
      <author><first>Rwaa</first><last>Assi</last></author>
      <author><first>Ahmed</first><last>Sorkatti</last><affiliation>University of Khartoum</affiliation></author>
      <author><first>Mohamedou Cheikh</first><last>Tourad</last><affiliation>Université de Nouakchott</affiliation></author>
      <author><first>Anis</first><last>Koubaa</last><affiliation>Alfaisal University</affiliation></author>
      <author><first>Ismail</first><last>Berrada</last><affiliation>Mohammed VI Polytechnic University</affiliation></author>
      <author><first>Mustafa</first><last>Jarrar</last><affiliation>Birzeit University</affiliation></author>
      <author><first>Shady</first><last>Shehata</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Muhammad</first><last>Abdul-Mageed</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and University of British Columbia</affiliation></author>
      <pages>32871-32894</pages>
      <abstract>As large language models (LLMs) become increasingly integrated into daily life, ensuring their cultural sensitivity and inclusivity is paramount. We introduce PALM, a year-long community-driven project covering all 22 Arab countries. The dataset contains instruction–response pairs in both Modern Standard Arabic (MSA) and dialectal Arabic (DA), spanning 20 diverse topics. Built by a team of 44 researchers across the Arab world—each an author of this paper—PALM offers a broad, inclusive perspective. We use PALM to evaluate the cultural and dialectal capabilities of several frontier LLMs, revealing notable limitations: while closed-source LLMs generally perform strongly, they still exhibit flaws, and smaller open-source models face greater challenges. Furthermore, certain countries (e.g., Egypt, the UAE) appear better represented than others (e.g., Iraq, Mauritania, Yemen). Our annotation guidelines, code, and data are publicly available for reproducibility. More information about PALM is available on our project page: https://github.com/UBC-NLP/palm.</abstract>
      <url hash="8d3529c4">2025.acl-long.1579</url>
      <bibkey>alwajih-etal-2025-palm</bibkey>
    </paper>
    <paper id="1580">
      <title><fixed-case>N</fixed-case>ews<fixed-case>I</fixed-case>nterview: a Dataset and a Playground to Evaluate <fixed-case>LLM</fixed-case>s’ Grounding Gap via Informational Interviews</title>
      <author><first>Alexander</first><last>Spangher</last></author>
      <author><first>Michael</first><last>Lu</last></author>
      <author><first>Sriya</first><last>Kalyan</last></author>
      <author><first>Hyundong Justin</first><last>Cho</last><affiliation>USC/ISI</affiliation></author>
      <author><first>Tenghao</first><last>Huang</last></author>
      <author><first>Weiyan</first><last>Shi</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Jonathan</first><last>May</last><affiliation>University of Southern California and USC/ISI</affiliation></author>
      <pages>32895-32925</pages>
      <abstract>Large Language Models (LLMs) have demonstrated impressive capabilities in generating coherent text but often struggle with grounding language and strategic dialogue. To address this gap, we focus on journalistic interviews, a domain rich in grounding communication and abundant in data. We curate a dataset of 40,000 two-person informational interviews from NPR and CNN, and reveal that LLMs are significantly less likely than human interviewers to use acknowledgements and to pivot to higher-level questions. Realizing that a fundamental deficit exists in multi-turn planning and strategic thinking, we develop a realistic simulated environment, incorporating source personas and persuasive elements, in order to facilitate the development of agents with longer-horizon rewards. Our experiments show that while source LLMs mimic human behavior in information sharing, interviewer LLMs struggle with recognizing when questions are answered and engaging persuasively, leading to suboptimal information extraction across model size and capability. These findings underscore the need for enhancing LLMs’ strategic dialogue capabilities.</abstract>
      <url hash="af992cc8">2025.acl-long.1580</url>
      <bibkey>spangher-etal-2025-newsinterview</bibkey>
    </paper>
    <paper id="1581">
      <title><fixed-case>CFB</fixed-case>ench: A Comprehensive Constraints-Following Benchmark for <fixed-case>LLM</fixed-case>s</title>
      <author><first>Tao</first><last>Zhang</last><affiliation>Beijing Baichuan Intelligence Technology Co., Ltd.</affiliation></author>
      <author><first>ChengLIn</first><last>Zhu</last></author>
      <author><first>Yanjun</first><last>Shen</last><affiliation>Beijing Baichuan Intelligence Technology Co., Ltd.</affiliation></author>
      <author><first>Wenjing</first><last>Luo</last><affiliation>Beijing Baichuan Intelligence Technology Co., Ltd.</affiliation></author>
      <author><first>Yan</first><last>Zhang</last><affiliation>Beijing Baichuan Intelligence Technology Co., Ltd.</affiliation></author>
      <author><first>Hao</first><last>Liang</last></author>
      <author><first>Tao</first><last>Zhang</last><affiliation>Beijing Baichuan Intelligence Technology Co., Ltd.</affiliation></author>
      <author><first>Fan</first><last>Yang</last></author>
      <author><first>Mingan</first><last>Lin</last></author>
      <author><first>Yujing</first><last>Qiao</last></author>
      <author><first>Weipeng</first><last>Chen</last></author>
      <author><first>Bin</first><last>Cui</last><affiliation>Peking University</affiliation></author>
      <author><first>Wentao</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Zenan</first><last>Zhou</last></author>
      <pages>32926-32944</pages>
      <abstract>The adeptness of Large Language Models (LLMs) in comprehending and following natural language instructions is critical for their deployment in sophisticated real-world applications. Existing evaluations mainly focus on fragmented constraints or narrow scenarios, but they overlook the comprehensiveness and authenticity of constraints from the user’s perspective. To bridge this gap, we propose CFBench, a large-scale Chinese Comprehensive Constraints Following Benchmark for LLMs, featuring 1,000 curated samples that cover more than 200 real-life scenarios and over 50 NLP tasks. CFBench meticulously compiles constraints from real-world instructions and constructs an innovative systematic framework for constraint types, which includes 10 primary categories and over 25 subcategories, and ensures each constraint is seamlessly integrated within the instructions. To make certain that the evaluation of LLM outputs aligns with user perceptions, we propose an advanced methodology that integrates multi-dimensional assessment criteria with requirement prioritization, covering various perspectives of constraints, instructions, and requirement fulfillment. Evaluating current leading LLMs on CFBench reveals substantial room for improvement in constraints following, and we further investigate influencing factors and enhancement strategies. The data and code will be made available.</abstract>
      <url hash="a0dd8932">2025.acl-long.1581</url>
      <bibkey>zhang-etal-2025-cfbench</bibkey>
    </paper>
    <paper id="1582">
      <title>Towards Building Large Scale Datasets and State-of-the-Art Automatic Speech Translation Systems for 14 <fixed-case>I</fixed-case>ndian Languages</title>
      <author><first>Ashwin</first><last>Sankar</last><affiliation>AI4Bharat</affiliation></author>
      <author><first>Sparsh</first><last>Jain</last></author>
      <author><first>Nikhil</first><last>Narasimhan</last><affiliation>Indian Institute of Technology, Madras, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <author><first>Devilal</first><last>Choudhary</last></author>
      <author><first>Dhairya</first><last>Suman</last><affiliation>Indian Institute of Technology, Delhi</affiliation></author>
      <author><first>Mohammed Safi Ur Rahman</first><last>Khan</last><affiliation>Indian Institute of Technology, Madras, Dhirubhai Ambani Institute Of Information and Communication Technology and Indian Institute of Technology, Madras, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <author><first>Anoop</first><last>Kunchukuttan</last><affiliation>Microsoft and Indian Institute of Technology, Madras</affiliation></author>
      <author><first>Mitesh M</first><last>Khapra</last><affiliation>Indian Institute of Technology, Madras</affiliation></author>
      <author><first>Raj</first><last>Dabre</last><affiliation>Department of Computer Science, Indian Institute of Technology, Madras, Indian Institute of Technology, Madras and National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <pages>32945-32966</pages>
      <abstract>Speech translation for Indian languages remains a challenging task due to the scarcity of large-scale, publicly available datasets that capture the linguistic diversity and domain coverage essential for real-world applications. Existing datasets cover a fraction of Indian languages and lack the breadth needed to train robust models that generalize beyond curated benchmarks. To bridge this gap, we introduce BhasaAnuvaad, the largest speech translation dataset for Indian languages, spanning over 44 thousand hours of audio and 17 million aligned text segments across 14 Indian languages and English. Our dataset is built through a threefold methodology: (a) aggregating high-quality existing sources, (b) large-scale web crawling to ensure linguistic and domain diversity, and (c) creating synthetic data to model real-world speech disfluencies. Leveraging BhasaAnuvaad, we train IndicSeamless, a state-of-the-art speech translation model for Indian languages that performs better than existing models. Our experiments demonstrate improvements in the translation quality, setting a new standard for Indian language speech translation. We will release all the code, data and model weights in the open-source, with permissive licenses to promote accessibility and collaboration.</abstract>
      <url hash="c1824987">2025.acl-long.1582</url>
      <bibkey>sankar-etal-2025-towards</bibkey>
    </paper>
    <paper id="1583">
      <title><fixed-case>C</fixed-case>o<fixed-case>R</fixed-case>e-<fixed-case>MMRAG</fixed-case>: Cross-Source Knowledge Reconciliation for Multimodal <fixed-case>RAG</fixed-case></title>
      <author><first>Yang</first><last>Tian</last></author>
      <author><first>Fan</first><last>Liu</last></author>
      <author><first>Jingyuan</first><last>Zhang</last><affiliation>Kuaishou- 快手科技</affiliation></author>
      <author><first>V.</first><last>W.</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Yupeng</first><last>Hu</last></author>
      <author><first>Liqiang</first><last>Nie</last><affiliation>Harbin Institute of Technology (Shenzhen) and Shandong University</affiliation></author>
      <pages>32967-32982</pages>
      <abstract>Multimodal Retrieval-Augmented Generation (MMRAG) has been introduced to enhance Multimodal Large Language Models by incorporating externally retrieved multimodal knowledge, but it introduces two challenges: Parametric-Retrieved Knowledge Inconsistency (PRKI), where discrepancies between parametric and retrieved knowledge create uncertainty in determining reliability, and Visual-Textual Knowledge Inconsistency (VTKI), where misalignment between visual and textual sources disrupts entity representation. To address these challenges, we propose <tex-math>\textbf{C}</tex-math>r<tex-math>\textbf{o}</tex-math>ss-source knowledge <tex-math>\textbf{Re}</tex-math>conciliation for <tex-math>\textbf{M}</tex-math>ulti<tex-math>\textbf{M}</tex-math>odal <tex-math>\textbf{RAG}</tex-math> (CoRe-MMRAG), a novel end-to-end framework that effectively reconciles inconsistencies across knowledge sources. CoRe-MMRAG follows a four-stage pipeline: it first generates an internal response from parametric knowledge, then selects the most relevant multimodal evidence via joint similarity assessment, generates an external response, and finally integrates both to produce a reliable answer. Additionally, a specialized training paradigm enhances knowledge source discrimination, multimodal integration, and unified answer generation. Experiments on KB-VQA benchmarks show that CoRe-MMRAG achieves substantial improvements over baseline methods, achieving 5.6% and 9.3% performance gains on InfoSeek and Encyclopedic-VQA, respectively. We release code and data at https://github.com/TyangJN/CoRe-MMRAG.</abstract>
      <url hash="0c78e1f0">2025.acl-long.1583</url>
      <bibkey>tian-etal-2025-core</bibkey>
    </paper>
    <paper id="1584">
      <title>Mapping 1,000+ Language Models via the Log-Likelihood Vector</title>
      <author><first>Momose</first><last>Oyama</last><affiliation>Kyoto University, Kyoto University</affiliation></author>
      <author><first>Hiroaki</first><last>Yamagiwa</last></author>
      <author><first>Yusuke</first><last>Takase</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Hidetoshi</first><last>Shimodaira</last><affiliation>Kyoto University and RIKEN</affiliation></author>
      <pages>32983-33038</pages>
      <abstract>To compare autoregressive language models at scale, we propose using log-likelihood vectors computed on a predefined text set as model features. This approach has a solid theoretical basis: when treated as model coordinates, their squared Euclidean distance approximates the Kullback-Leibler divergence of text-generation probabilities. Our method is highly scalable, with computational cost growing linearly in both the number of models and text samples, and is easy to implement as the required features are derived from cross-entropy loss. Applying this method to over 1,000 language models, we constructed a “model map,” providing a new perspective on large-scale model analysis.</abstract>
      <url hash="1d70014b">2025.acl-long.1584</url>
      <bibkey>oyama-etal-2025-mapping</bibkey>
    </paper>
    <paper id="1585">
      <title><fixed-case>C</fixed-case>onsistency<fixed-case>C</fixed-case>hecker: Tree-based Evaluation of <fixed-case>LLM</fixed-case> Generalization Capabilities</title>
      <author><first>Zhaochen</first><last>Hong</last></author>
      <author><first>Haofei</first><last>Yu</last></author>
      <author><first>Jiaxuan</first><last>You</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <pages>33039-33075</pages>
      <abstract>Evaluating Large Language Models (LLMs) requires effective methods to assess semantic consistency across multiple reversible transformations. Traditional self-consistency methods often fail to capture subtle semantic errors in multi-step tasks. We introduce ConsistencyChecker, a tree-based evaluation framework that measures LLMs’ ability to preserve semantic consistency during reversible transformation processes, sidestepping benchmark data contamination issues. Our approach constructs self-consistency trees where nodes represent text states after transformations (e.g., translation, code modification, paraphrasing) and edges represent pairs of opposite transformations. By analyzing semantic preservation between nodes at different tree depths, ConsistencyChecker quantifies model reliability without requiring manually annotated reference data. Experiments demonstrate that ConsistencyChecker reliably measures generalization abilities across models from 1.5B to 72B parameters. On translation tasks, GPT-4o Mini achieves the highest L3 consistency score of 98.0%. For code generation, Qwen 2.5 32B leads with 85.1% semantic consistency at L3. Results show Pearson correlation greater than 0.7 between our embedding-based scores and WMT 2024 rankings on 4 out of 5 shared language pairs, validating the method’s effectiveness for benchmarking LLM performance without constructing new datasets.</abstract>
      <url hash="cb58b6e1">2025.acl-long.1585</url>
      <bibkey>hong-etal-2025-consistencychecker</bibkey>
    </paper>
    <paper id="1586">
      <title>Robust Estimation of Population-Level Effects in Repeated-Measures <fixed-case>NLP</fixed-case> Experimental Designs</title>
      <author><first>Alejandro</first><last>Benito-Santos</last><affiliation>Universidad Nacional de Educación a Distancia</affiliation></author>
      <author><first>Adrian</first><last>Ghajari</last><affiliation>Universidad Nacional de Educación a Distancia and Universidad Nacional de Educación a Distancia</affiliation></author>
      <author><first>Víctor</first><last>Fresno</last></author>
      <pages>33076-33089</pages>
      <abstract>NLP research frequently grapples with multiple sources of variability—spanning runs, datasets, annotators, and more—yet conventional analysis methods often neglect these hierarchical structures, threatening the reproducibility of findings. To address this gap, we contribute a case study illustrating how linear mixed-effects models (LMMs) can rigorously capture systematic language-dependent differences (i.e., population-level effects) in a population of monolingual and multilingual language models. In the context of a bilingual hate speech detection task, we demonstrate that LMMs can uncover significant population-level effects—even under low-resource (small-N) experimental designs—while mitigating confounds and random noise. By setting out a transparent blueprint for repeated-measures experimentation, we encourage the NLP community to embrace variability as a feature, rather than a nuisance, in order to advance more robust, reproducible, and ultimately trustworthy results.</abstract>
      <url hash="813b9e31">2025.acl-long.1586</url>
      <bibkey>benito-santos-etal-2025-robust</bibkey>
    </paper>
    <paper id="1587">
      <title><fixed-case>F</fixed-case>act<fixed-case>B</fixed-case>ench: A Dynamic Benchmark for In-the-Wild Language Model Factuality Evaluation</title>
      <author><first>Farima</first><last>Fatahi Bayat</last></author>
      <author><first>Lechen</first><last>Zhang</last></author>
      <author><first>Sheza</first><last>Munir</last></author>
      <author><first>Lu</first><last>Wang</last><affiliation>Northeastern University, Northeastern University and University of Michigan</affiliation></author>
      <pages>33090-33110</pages>
      <abstract>The rapid adoption of language models (LMs) across diverse applications has raised concerns about their factuality, i.e., their consistency with real-world facts. We introduce VERIFY, an evidence-based evaluation pipeline that measures LMs’ factuality in real-world user interactions. VERIFY considers the verifiability of LM-generated content and categorizes content units as Supported, Unsupported, or Undecidable based on Web-retrieved evidence. Importantly, factuality judgment by VERIFY more strongly correlates with human evaluations than existing methods. Using VERIFY, we identify “hallucination prompts,” i.e., those that frequently elicit factual errors in LM responses. These prompts form FactBench, a dataset of 1K prompts spanning 150 topics and tiered into Easy, Moderate, and Hard prompts. We benchmark widely-used openweight and proprietary LMs from six families, yielding three key findings: (i) LMs’ factual precision declines from Easy to Hard prompts, (ii) factuality does not necessarily improve with scale; Llama3.1-405B-Instruct performs comparably to or worse than its 70B variant, and (iii) Gemini1.5-Pro shows a notably higher refusal rate, with over-refusal in 25% of cases.</abstract>
      <url hash="0f896207">2025.acl-long.1587</url>
      <bibkey>fatahi-bayat-etal-2025-factbench</bibkey>
    </paper>
    <paper id="1588">
      <title>Training-free <fixed-case>LLM</fixed-case> Merging for Multi-task Learning</title>
      <author><first>Zichuan</first><last>Fu</last></author>
      <author><first>Xian</first><last>Wu</last><affiliation>Tencent</affiliation></author>
      <author><first>Yejing</first><last>Wang</last><affiliation>City University of Hong Kong</affiliation></author>
      <author><first>Wanyu</first><last>Wang</last></author>
      <author><first>Shanshan</first><last>Ye</last></author>
      <author><first>Hongzhi</first><last>Yin</last><affiliation>University of Queensland</affiliation></author>
      <author><first>Yi</first><last>Chang</last><affiliation>Jilin University, China</affiliation></author>
      <author><first>Yefeng</first><last>Zheng</last><affiliation>Westlake University</affiliation></author>
      <author><first>Xiangyu</first><last>Zhao</last><affiliation>City University of Hong Kong</affiliation></author>
      <pages>33111-33124</pages>
      <abstract>Large Language Models (LLMs) have demonstrated exceptional capabilities across diverse natural language processing (NLP) tasks. The release of open-source LLMs like LLaMA and Qwen has triggered the development of numerous fine-tuned models tailored for various tasks and languages. In this paper, we explore an important question: is it possible to combine these specialized models to create a unified model with multi-task capabilities. We introduces **H**ierarchical **I**terative **Merging** (Hi-Merging), a training-free method for unifying different specialized LLMs into a single model. Specifically, Hi-Merging employs model-wise and layer-wise pruning and scaling, guided by contribution analysis, to mitigate parameter conflicts. Extensive experiments on multiple-choice and question-answering tasks in both Chinese and English validate Hi-Merging’s ability for multi-task learning. The results demonstrate that Hi-Merging consistently outperforms existing merging techniques and surpasses the performance of models fine-tuned on combined datasets in most scenarios. Code is available at [Applied-Machine-Learning-Lab/Hi-Merging](https://github.com/Applied-Machine-Learning-Lab/Hi-Merging).</abstract>
      <url hash="e2fedccb">2025.acl-long.1588</url>
      <bibkey>fu-etal-2025-training</bibkey>
    </paper>
    <paper id="1589">
      <title>Inferring from Logits: Exploring Best Practices for Decoding-Free Generative Candidate Selection</title>
      <author><first>Mingyu Derek</first><last>Ma</last><affiliation>Genentech and University of California, Los Angeles</affiliation></author>
      <author><first>Yanna</first><last>Ding</last><affiliation>Rensselaer Polytechnic Institute</affiliation></author>
      <author><first>Zijie</first><last>Huang</last><affiliation>Google</affiliation></author>
      <author><first>Jianxi</first><last>Gao</last><affiliation>Rensselaer Polytechnic Institute</affiliation></author>
      <author><first>Yizhou</first><last>Sun</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Wei</first><last>Wang</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>33125-33144</pages>
      <abstract>Generative Language Models rely on autoregressive decoding to produce the output sequence token by token. Many tasks such as preference optimization, require the model to produce task-level output consisting of multiple tokens directly by selecting candidates from a pool as predictions. Determining a task-level prediction from candidates using the ordinary token-level decoding mechanism is constrained by time-consuming decoding and interrupted gradients by discrete token selection. Existing works have been using decoding-free candidate selection methods to obtain candidate probability from initial output logits over vocabulary. Though these estimation methods are widely used, they are not systematically evaluated, especially on end tasks. We introduce an evaluation of a comprehensive collection of decoding-free candidate selection approaches on a comprehensive set of tasks, including five multiple-choice QA tasks with a small candidate pool and four clinical decision tasks with a massive amount of candidates, some with 10k+ options. We evaluate the estimation methods paired with a wide spectrum of foundation LMs covering different architectures, sizes and training paradigms. The results and insights from our analysis inform the future model design.</abstract>
      <url hash="5270bc7c">2025.acl-long.1589</url>
      <bibkey>ma-etal-2025-inferring</bibkey>
    </paper>
    <paper id="1590">
      <title>Comparison-based Active Preference Learning for Multi-dimensional Personalization</title>
      <author><first>Minhyeon</first><last>Oh</last><affiliation>Pohang University of Science and Technology</affiliation></author>
      <author><first>Seungjoon</first><last>Lee</last><affiliation>Pohang University of Science and Technology</affiliation></author>
      <author><first>Jungseul</first><last>Ok</last><affiliation>POSTECH</affiliation></author>
      <pages>33145-33166</pages>
      <abstract>Large language models (LLMs) have shown remarkable success, but aligning them with human preferences remains a core challenge. As individuals have their own, multi-dimensional preferences, recent studies have explored *multi-dimensional personalization*, which aims to enable models to generate responses personalized to *explicit* preferences. However, human preferences are often *implicit* and thus difficult to articulate, limiting the direct application of this approach. To bridge this gap, we propose Active Multi-dimensional Preference Learning (AMPLe), designed to capture implicit user preferences from interactively collected comparative feedback. Building on Bayesian inference, our work introduces a modified posterior update procedure to mitigate estimation bias and potential noise in comparisons. Also, inspired by generalized binary search, we employ an active query selection strategy to minimize the number of required comparisons by a user. Through theoretical analysis and experiments on language generation tasks, we demonstrate feedback efficiency and effectiveness of our framework in personalizing model responses. Our code is publicly available at https://github.com/ml-postech/AMPLe.</abstract>
      <url hash="da7557f5">2025.acl-long.1590</url>
      <bibkey>oh-etal-2025-comparison</bibkey>
    </paper>
    <paper id="1591">
      <title><fixed-case>O</fixed-case>pen<fixed-case>C</fixed-case>oder: The Open Cookbook for Top-Tier Code Large Language Models</title>
      <author><first>Siming</first><last>Huang</last></author>
      <author><first>Tianhao</first><last>Cheng</last></author>
      <author><first>Jason Klein</first><last>Liu</last><affiliation>None</affiliation></author>
      <author><first>Weidi</first><last>Xu</last><affiliation>Infly Technology</affiliation></author>
      <author><first>Jiaran</first><last>Hao</last></author>
      <author><first>Liuyihan</first><last>Song</last><affiliation>INF Tech.</affiliation></author>
      <author><first>Yang</first><last>Xu</last><affiliation>inftech.ai</affiliation></author>
      <author><first>Jian</first><last>Yang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Jiaheng</first><last>Liu</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Chenchen</first><last>Zhang</last></author>
      <author><first>Linzheng</first><last>Chai</last></author>
      <author><first>Ruifeng</first><last>Yuan</last></author>
      <author><first>Xianzhen</first><last>Luo</last><affiliation>Harbin Institute of Techology</affiliation></author>
      <author><first>Qiufeng</first><last>Wang</last></author>
      <author><first>YuanTao</first><last>Fan</last></author>
      <author><first>Qingfu</first><last>Zhu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Zhaoxiang</first><last>Zhang</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yang</first><last>Gao</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Jie</first><last>Fu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Qian</first><last>Liu</last><affiliation>Tiktok</affiliation></author>
      <author><first>Houyi</first><last>Li</last><affiliation>Fudan University</affiliation></author>
      <author><first>Ge</first><last>Zhang</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Yuan</first><last>Qi</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xu</first><last>Yinghui</last><affiliation>Fudan University</affiliation></author>
      <author><first>Wei</first><last>Chu</last><affiliation>Inf Tech</affiliation></author>
      <author><first>Zili</first><last>Wang</last></author>
      <pages>33167-33193</pages>
      <abstract>Code LLMs have been widely used in various domains, including code generation, logical reasoning, and agent systems. However, open-access code LLMs mostly only release weights, lacking key features such as reproducible data pipelines and transparent training protocols, which are crucial for advancing deeper, more reliable investigations. To address the gap, we introduce OpenCoder, a top-tier code LLM that not only achieves performance comparable to leading models but also serves as an “open cookbook” for the research community. Unlike most prior efforts, we release not only model weights and inference code, but also the reproducible training data, complete data processing pipeline, rigorous experimental ablation results, and detailed training protocols for open scientific research. Our work identifies the key ingredients for building a top-tier code LLM: optimized heuristic rules for data cleaning and deduplication, effective recall of code-related text corpus, and high-quality synthetic data for both annealing and supervised fine-tuning stages. By offering this level of openness, we aim to broaden access to all aspects of a top-tier code LLM, with OpenCoder serving as both a powerful model and an open foundation to accelerate research and enable reproducible advancements in code intelligence. The released resource is available at https://opencoder-llm.github.io.</abstract>
      <url hash="2fa9e51d">2025.acl-long.1591</url>
      <bibkey>huang-etal-2025-opencoder</bibkey>
    </paper>
    <paper id="1592">
      <title><fixed-case>L</fixed-case>lama<fixed-case>D</fixed-case>uo: <fixed-case>LLMO</fixed-case>ps Pipeline for Seamless Migration from Service <fixed-case>LLM</fixed-case>s to Small-Scale Local <fixed-case>LLM</fixed-case>s</title>
      <author><first>Chansung</first><last>Park</last><affiliation>Electronics and Telecommunications Research Institute</affiliation></author>
      <author><first>Juyong</first><last>Jiang</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Fan</first><last>Wang</last></author>
      <author><first>Sayak</first><last>Paul</last><affiliation>Hugging Face</affiliation></author>
      <author><first>Jing</first><last>Tang</last><affiliation>The Hong Kong University of Science and Technology (Guangzhou) and The Hong Kong University of Science and Technology</affiliation></author>
      <pages>33194-33215</pages>
      <abstract>The widespread adoption of cloud-based proprietary large language models (LLMs) has introduced significant challenges, including operational dependencies, privacy concerns, and the necessity of continuous internet connectivity. In this work, we introduce an LLMOps pipeline, “LlamaDuo”, for the seamless migration of knowledge and abilities from service-oriented LLMs to smaller, locally manageable models. This pipeline is crucial for ensuring service continuity in the presence of operational failures, strict privacy policies, or offline requirements. Our LlamaDuo involves fine-tuning a small language model against the service LLM using a synthetic dataset generated by the latter. If the performance of the fine-tuned model falls short of expectations, it is automatically improved through additional fine-tuning using extra similar data generated by the service LLM. This multi-turn process guarantees that the smaller model can eventually match or even surpass the service LLM’s capabilities in specific downstream tasks, offering a practical and scalable solution for managing AI deployments in constrained environments. Extensive experiments with leading-edge LLMs are conducted to demonstrate the effectiveness, adaptability, and affordability of LlamaDuo across various downstream tasks. Our pipeline implementation is available at https://github.com/deep-diver/llamaduo.</abstract>
      <url hash="da37a4e3">2025.acl-long.1592</url>
      <bibkey>park-etal-2025-llamaduo</bibkey>
    </paper>
    <paper id="1593">
      <title><fixed-case>A</fixed-case>mbi<fixed-case>K</fixed-case>: Dataset of Ambiguous Tasks in Kitchen Environment</title>
      <author><first>Anastasia</first><last>Ivanova</last></author>
      <author><first>Bakaeva</first><last>Eva</last></author>
      <author><first>Zoya</first><last>Volovikova</last></author>
      <author><first>Alexey</first><last>Kovalev</last><affiliation>AIRI and Moscow Institute of Physics and Technology</affiliation></author>
      <author><first>Aleksandr</first><last>Panov</last><affiliation>Artificial Intelligence Research Institute and Moscow Institute of Physics and Technology</affiliation></author>
      <pages>33216-33241</pages>
      <abstract>As a part of an embodied agent, Large Language Models (LLMs) are typically used for behavior planning given natural language instructions from the user. However, dealing with ambiguous instructions in real-world environments remains a challenge for LLMs. Various methods for task ambiguity detection have been proposed. However, it is difficult to compare them because they are tested on different datasets and there is no universal benchmark. For this reason, we propose AmbiK (Ambiguous Tasks in Kitchen Environment), the fully textual dataset of ambiguous instructions addressed to a robot in a kitchen environment. AmbiK was collected with the assistance of LLMs and is human-validated. It comprises 1000 pairs of ambiguous tasks and their unambiguous counterparts, categorized by ambiguity type (Human Preferences, Common Sense Knowledge, Safety), with environment descriptions, clarifying questions and answers, user intents, and task plans, for a total of 2000 tasks. We hope that AmbiK will enable researchers to perform a unified comparison of ambiguity detection methods. AmbiK is available at https://github.com/cog-model/AmbiK-dataset.</abstract>
      <url hash="21fdc772">2025.acl-long.1593</url>
      <bibkey>ivanova-etal-2025-ambik</bibkey>
    </paper>
    <paper id="1594">
      <title><fixed-case>S</fixed-case>ocial<fixed-case>CC</fixed-case>: Interactive Evaluation for Cultural Competence in Language Agents</title>
      <author><first>Jincenzi</first><last>Wu</last></author>
      <author><first>Jianxun</first><last>Lian</last></author>
      <author><first>Dingdong</first><last>Wang</last></author>
      <author><first>Helen M.</first><last>Meng</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>33242-33271</pages>
      <abstract>Large Language Models (LLMs) are increasingly deployed worldwide, yet their ability to navigate cultural nuances remains underexplored. Misinterpreting cultural content can lead to AI-generated responses that are offensive or inappropriate, limiting their usability in global applications such as customer service, diplomatic communication, and online education. While prior research has evaluated cultural knowledge of LLMs, existing benchmarks fail to assess dynamic cultural competence-the ability to apply cultural knowledge effectively in real-world interactions. To address this gap, we introduce SocialDuolingo, a novel benchmark designed to evaluate cultural competence through multi-turn interactive intercultural scenarios. It comprises 3,060 human-written scenarios spanning 60 countries across six continents. Through extensive experiments on eight prominent LLMs, our findings reveal a significant gap between the cultural knowledge stored in these models and their ability to apply it effectively in cross-cultural communication.</abstract>
      <url hash="9176d669">2025.acl-long.1594</url>
      <bibkey>wu-etal-2025-socialcc</bibkey>
    </paper>
    <paper id="1595">
      <title>Scalable Vision Language Model Training via High Quality Data Curation</title>
      <author><first>Hongyuan</first><last>Dong</last></author>
      <author><first>Zijian</first><last>Kang</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Weijie</first><last>Yin</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>LiangXiao</first><last>LiangXiao</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>ChaoFeng</first><last>ChaoFeng</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Ran</first><last>Jiao</last></author>
      <pages>33272-33293</pages>
      <abstract>In this paper, we introduce <i>
          <b>SAIL</b>-VL</i> (<i>
          <b>S</b>c<b>A</b>lable Vision Language Model Tra<b>I</b>ning via High Qua<b>L</b>ity Data Curation</i>), an open-source vision language model (VLM) series achieving state-of-the-art (SOTA) performance in 2B and 8B parameters. The following three key improvements contribute to SAIL-VL’s leading performance: (1) Scalable high-quality visual understanding data construction: We implement a data construction pipeline to enable hundred-million-scale high-quality recaption data annotation. The resulted dataset SAIL-Caption is validated to be of the highest data quality compared with opensource datasets. (2) Scalable Pretraining with High-Quality Visual Understanding Data: We scale SAIL-VL’s pretraining budget up to 655B tokens and show that even a 2B VLM benefits from scaled up training data sizes, exhibiting logarithmic data size scaling laws in benchmark performance. (3) Scalable SFT via data quantity and complexity scaling: We curate a high-quality SFT dataset collection with leading data quantity scaling effectiveness and demonstrate that training with progressively higher-complexity data surpasses baseline one-stage training by a large margin. SAIL-VL series models achieve the highest average score in 18 widely used VLM benchmarks in our evaluation, with the 2B model takes the top position over VLMs of comparable sizes on OpenCompass 2024 (https://rank.opencompass.org.cn/leaderboard-multimodal), demonstrating robust visual comprehension abilities. SAIL-VL series models are released at HuggingFace (https://huggingface.co/BytedanceDouyinContent).</abstract>
      <url hash="1377cc2f">2025.acl-long.1595</url>
      <bibkey>dong-etal-2025-scalable</bibkey>
    </paper>
    <paper id="1596">
      <title><fixed-case>GRAM</fixed-case>: Generative Recommendation via Semantic-aware Multi-granular Late Fusion</title>
      <author><first>Sunkyung</first><last>Lee</last><affiliation>SungKyunKwan University</affiliation></author>
      <author><first>Minjin</first><last>Choi</last><affiliation>Samsung Research</affiliation></author>
      <author><first>Eunseong</first><last>Choi</last></author>
      <author><first>Hye-young</first><last>Kim</last></author>
      <author><first>Jongwuk</first><last>Lee</last><affiliation>Sungkyunkwan University</affiliation></author>
      <pages>33294-33312</pages>
      <abstract>Generative recommendation is an emerging paradigm that leverages the extensive knowledge of large language models by formulating recommendations into a text-to-text generation task. However, existing studies face two key limitations in (i) incorporating implicit item relationships and (ii) utilizing rich yet lengthy item information. To address these challenges, we propose a Generative Recommender via semantic-Aware Multi-granular late fusion (GRAM), introducing two synergistic innovations. First, we design semantic-to-lexical translation to encode implicit hierarchical and collaborative item relationships into the vocabulary space of LLMs. Second, we present multi-granular late fusion to integrate rich semantics efficiently with minimal information loss. It employs separate encoders for multi-granular prompts, delaying the fusion until the decoding stage. Experiments on four benchmark datasets show that GRAM outperforms eight state-of-the-art generative recommendation models, achieving significant improvements of 11.5-16.0% in Recall@5 and 5.3-13.6% in NDCG@5. The source code is available at https://github.com/skleee/GRAM.</abstract>
      <url hash="75c3fb7c">2025.acl-long.1596</url>
      <bibkey>lee-etal-2025-gram</bibkey>
    </paper>
    <paper id="1597">
      <title>Towards Economical Inference: Enabling <fixed-case>D</fixed-case>eep<fixed-case>S</fixed-case>eek’s Multi-Head Latent Attention in Any Transformer-based <fixed-case>LLM</fixed-case>s</title>
      <author><first>Tao</first><last>Ji</last></author>
      <author><first>Bin</first><last>Guo</last></author>
      <author><first>Yuanbin</first><last>Wu</last></author>
      <author><first>Qipeng</first><last>Guo</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Shenlixing</first><last>Shenlixing</last></author>
      <author><first>Chenzhan</first><last>Chenzhan</last></author>
      <author><first>Xipeng</first><last>Qiu</last><affiliation>Fudan University</affiliation></author>
      <author><first>Qi</first><last>Zhang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Tao</first><last>Gui</last><affiliation>Fudan University</affiliation></author>
      <pages>33313-33328</pages>
      <abstract>Multi-head Latent Attention (MLA) is an innovative architecture proposed by DeepSeek, designed to ensure efficient and economical inference by significantly compressing the Key-Value (KV) cache into a latent vector. Compared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its variants such as Grouped-Query Attention (GQA) exhibit significant cost disadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA without pre-training from scratch is both meaningful and challenging. This paper proposes the first data-efficient fine-tuning method for transitioning from MHA to MLA (**MHA2MLA**), which includes two key components: for *partial-RoPE*, we remove RoPE from dimensions of queries and keys that contribute less to the attention scores, for *low-rank approximation*, we introduce joint SVD approximations based on the pre-trained parameters of keys and values. These carefully designed strategies enable MHA2MLA to recover performance using only a small fraction (0.6% to 1%) of the data, significantly reducing inference costs while seamlessly integrating with compression techniques such as KV cache quantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%, with only a 1% drop in LongBench performance. Our source code is publicly available at https://github.com/JT-Ushio/MHA2MLA.</abstract>
      <url hash="caa7782d">2025.acl-long.1597</url>
      <bibkey>ji-etal-2025-towards</bibkey>
    </paper>
    <paper id="1598">
      <title><fixed-case>TETRIS</fixed-case>: Optimal Draft Token Selection for Batch Speculative Decoding</title>
      <author><first>Zhaoxuan</first><last>Wu</last><affiliation>Singapore-MIT Alliance for Research and Technology</affiliation></author>
      <author><first>Zijian</first><last>Zhou</last></author>
      <author><first>Arun</first><last>Verma</last><affiliation>Singapore-MIT Alliance for Research and Technology</affiliation></author>
      <author><first>Alok</first><last>Prakash</last><affiliation>Singapore MIT Alliance for Research and Technology</affiliation></author>
      <author><first>Daniela</first><last>Rus</last></author>
      <author><first>Bryan Kian Hsiang</first><last>Low</last><affiliation>National University of Singapore</affiliation></author>
      <pages>33329-33345</pages>
      <abstract>We propose TETRIS, a novel method that optimizes the total throughput of batch speculative decoding in multi-request settings. Unlike existing methods that optimize for a single request or a group of requests as a whole, TETRIS actively selects the most promising draft tokens (for every request in a batch) to be accepted when verified in parallel, resulting in fewer rejected tokens and hence less wasted computing resources. Such an effective resource utilization to achieve fast inference in large language models (LLMs) is especially important to service providers with limited inference capacity. Compared to baseline speculative decoding, TETRIS yields a consistently higher acceptance rate and more effective utilization of the limited inference capacity. We show theoretically and empirically that TETRIS outperforms baseline speculative decoding and existing methods that dynamically select draft tokens, leading to a more efficient batch inference in LLMs.</abstract>
      <url hash="37010b59">2025.acl-long.1598</url>
      <bibkey>wu-etal-2025-tetris</bibkey>
    </paper>
    <paper id="1599">
      <title>Introducing Verification Task of Set Consistency with Set-Consistency Energy Networks</title>
      <author><first>Mooho</first><last>Song</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Hye Ryung</first><last>Son</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Jay-Yoon</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <pages>33346-33366</pages>
      <abstract>Examining logical inconsistencies among multiple statements (such as collections of sentences or question-answer pairs) is a crucial challenge in machine learning, particularly for ensuring the safety and reliability of models. Traditional methods that rely on 1:1 pairwise comparisons often fail to capture inconsistencies that only emerge when more than two statements are evaluated collectively. To address this gap, we introduce the task of set-consistency verification, an extension of natural language inference (NLI) that assesses the logical coherence of entire sets rather than isolated pairs. Building on this task, we present the Set-Consistency Energy Network (SC-Energy), a novel model that employs a margin-based loss to learn the compatibility among a collection of statements. Our approach not only efficiently verifies inconsistencies and pinpoints the specific statements responsible for logical contradictions, but also significantly outperforms existing methods, including prompting-based LLM models. Furthermore, we release two new datasets: Set-LConVQA and Set-SNLI for set-consistency verification task.</abstract>
      <url hash="fe088266">2025.acl-long.1599</url>
      <bibkey>song-etal-2025-introducing</bibkey>
    </paper>
    <paper id="1600">
      <title>Language Models can Subtly Deceive Without Lying: A Case Study on Strategic Phrasing in Legislation</title>
      <author><first>Atharvan</first><last>Dogra</last><affiliation>Indian Institute of Technology, Madras</affiliation></author>
      <author><first>Krishna</first><last>Pillutla</last><affiliation>Indian Institute of Technology, Madras, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <author><first>Ameet</first><last>Deshpande</last><affiliation>Princeton University</affiliation></author>
      <author><first>Ananya B.</first><last>Sai</last><affiliation>Indian Institute of Technology, Madras</affiliation></author>
      <author><first>John J</first><last>Nay</last><affiliation>Stanford University</affiliation></author>
      <author><first>Tanmay</first><last>Rajpurohit</last><affiliation>Independent Researcher</affiliation></author>
      <author><first>Ashwin</first><last>Kalyan</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Balaraman</first><last>Ravindran</last><affiliation>Indian Institute of Technology Madras</affiliation></author>
      <pages>33367-33390</pages>
      <abstract>We explore the ability of large language models (LLMs) to engage in subtle deception through strategically phrasing and intentionally manipulating information. This harmful behavior can be hard to detect, unlike blatant lying or unintentional hallucination. We build a simple testbed mimicking a legislative environment where a corporate <i>lobbyist</i> module is proposing amendments to bills that benefit a specific company while evading identification of this benefactor. We use real-world legislative bills matched with potentially affected companies to ground these interactions. Our results show that LLM lobbyists can draft subtle phrasing to avoid such identification by strong LLM-based detectors. Further optimization of the phrasing using LLM-based re-planning and re-sampling increases deception rates by up to 40 percentage points.Our human evaluations to verify the quality of deceptive generations and their retention of self-serving intent show significant coherence with our automated metrics and also help in identifying certain strategies of deceptive phrasing.This study highlights the risk of LLMs’ capabilities for strategic phrasing through seemingly neutral language to attain self-serving goals. This calls for future research to uncover and protect against such subtle deception.</abstract>
      <url hash="250b3456">2025.acl-long.1600</url>
      <bibkey>dogra-etal-2025-language</bibkey>
    </paper>
    <paper id="1601">
      <title><fixed-case>A</fixed-case>fro<fixed-case>CS</fixed-case>-xs: Creating a Compact, High-Quality, Human-Validated Code-Switched Dataset for <fixed-case>A</fixed-case>frican Languages</title>
      <author><first>Kayode</first><last>Olaleye</last></author>
      <author><first>Arturo</first><last>Oncevay</last><affiliation>J.P. Morgan Chase</affiliation></author>
      <author><first>Mathieu</first><last>Sibue</last><affiliation>J.P. Morgan Chase</affiliation></author>
      <author><first>Nombuyiselo</first><last>Zondi</last></author>
      <author><first>Michelle</first><last>Terblanche</last><affiliation>University of Pretoria</affiliation></author>
      <author><first>Sibongile</first><last>Mapikitla</last><affiliation>University of the People</affiliation></author>
      <author><first>Richard</first><last>Lastrucci</last></author>
      <author><first>Charese</first><last>Smiley</last><affiliation>J.P. Morgan Chase</affiliation></author>
      <author><first>Vukosi</first><last>Marivate</last><affiliation>University of Pretoria</affiliation></author>
      <pages>33391-33410</pages>
      <abstract>Code-switching is prevalent in multilingual communities but lacks adequate high-quality data for model development, especially for African languages. To address this, we present AfroCS-xs, a small human-validated synthetic code-switched dataset for four African languages (Afrikaans, Sesotho, Yoruba, isiZulu) and English within a specific domain—agriculture. Using large language models (LLMs), we generate code-switched sentences, including English translations, that are rigorously validated and corrected by native speakers. As a downstream evaluation task, we use this dataset to fine-tune different instruction-tuned LLMs for code-switched translation and compare their performance against machine translation (MT) models. Our results demonstrate that LLMs consistently improve in translation accuracy when fine-tuned on the high-quality AfroCS-xs dataset, highlighting that substantial gains can still be made with a low volume of data. We also observe improvements on natural code-switched and out-of-domain (personal finance) test sets. Overall, regardless of data size and prior exposure to a language, LLMs benefit from higher quality training data when translating code-switched texts in under-represented languages.</abstract>
      <url hash="68ae6f7f">2025.acl-long.1601</url>
      <bibkey>olaleye-etal-2025-afrocs</bibkey>
    </paper>
    <paper id="1602">
      <title>Just Go Parallel: Improving the Multilingual Capabilities of Large Language Models</title>
      <author><first>Muhammad Reza</first><last>Qorib</last></author>
      <author><first>Junyi</first><last>Li</last></author>
      <author><first>Hwee Tou</first><last>Ng</last><affiliation>National University of Singapore</affiliation></author>
      <pages>33411-33424</pages>
      <abstract>Large language models (LLMs) have demonstrated impressive translation capabilities even without being explicitly trained on parallel data. This remarkable property has led some to believe that parallel data is no longer necessary for building multilingual language models. While some attribute this to the emergent abilities of LLMs due to scale, recent work suggests that it is actually caused by incidental bilingual signals present in the training data. Various methods have been proposed to maximize the utility of parallel data to enhance the multilingual capabilities of multilingual encoder-based and encoder-decoder language models. However, some decoder-based LLMs opt to ignore parallel data instead. In this work, we conduct a systematic study on the impact of adding parallel data on LLMs’ multilingual capabilities, focusing specifically on translation and multilingual common-sense reasoning. Through controlled experiments, we demonstrate that parallel data can significantly improve LLMs’ multilingual capabilities.</abstract>
      <url hash="bffdeefc">2025.acl-long.1602</url>
      <bibkey>qorib-etal-2025-just</bibkey>
    </paper>
    <paper id="1603">
      <title>Design Choices for Extending the Context Length of Visual Language Models</title>
      <author><first>Mukai</first><last>Li</last></author>
      <author><first>Lei</first><last>Li</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Shansan</first><last>Gong</last></author>
      <author><first>Qi</first><last>Liu</last><affiliation>University of Hong Kong</affiliation></author>
      <pages>33425-33438</pages>
      <abstract>Visual Language Models (VLMs) demonstrate impressive capabilities in processing multimodal inputs, yet applications such as visual agents, which require handling multiple images and high-resolution videos, demand enhanced long-range modeling. Moreover, existing open-source VLMs lack systematic exploration into extending their context length, and commercial models often provide limited details. To tackle this, we aim to establish an effective solution that enhances long context performance of VLMs while preserving their capacities in short context scenarios. Towards this goal, we make the best design choice through extensive experiment settings from data curation to context window extending and utilizing: (1) we analyze data sources and length distributions to construct ETVLM - a data recipe to balance the performance across scenarios; (2) we examine existing position extending methods, identify their limitations and propose M-RoPE++ as an enhanced approach; we also choose to solely instruction-tune the backbone with mixed-source data; (3) we discuss how to better utilize extended context windows and propose hybrid-resolution training. Built on the Qwen-VL series model, we propose Giraffe, which is effectively extended to 128K lengths. Evaluated on extensive long context VLM benchmarks such as VideoMME and Viusal Haystacks, our Giraffe achieves state-of-the-art performance among similarly sized open-source long VLMs and is competitive with commercial model GPT-4V. We will open-source the code, data, and models.</abstract>
      <url hash="2f338a95">2025.acl-long.1603</url>
      <bibkey>li-etal-2025-design</bibkey>
    </paper>
  </volume>
  <volume id="short" ingest-date="2025-07-14" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (ACL 2025)</booktitle>
      <editor><first>Wanxiang</first><last>Che</last></editor>
      <editor><first>Joyce</first><last>Nabende</last></editor>
      <editor><first>Ekaterina</first><last>Shutova</last></editor>
      <editor><first>Mohammad Taher</first><last>Pilehvar</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Vienna, Austria</address>
      <month>July</month>
      <year>2025</year>
      <url hash="ea86fb40">2025.acl-short</url>
      <venue>acl</venue>
      <venue>ws</venue>
      <isbn>979-8-89176-252-7</isbn>
    </meta>
    <frontmatter>
      <url hash="df3687cd">2025.acl-short.0</url>
      <bibkey>acl-ws-2025-short</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Towards <fixed-case>LLM</fixed-case>-powered Attentive Listener: A Pragmatic Approach through Quantity Self-Repair</title>
      <author><first>Junlin</first><last>Li</last></author>
      <author><first>Peng</first><last>Bo</last></author>
      <author><first>Yu-Yin</first><last>Hsu</last></author>
      <pages>1-13</pages>
      <abstract>Grice’s Quantity Maxims dictate that human speakers aim for the optimal quantity of information during conversation. To empower LLMs to self-repair their responses toward optimal quantity and improve their attentive listening skills, we propose Q-Tuning and Q-Traveling, which draw on heuristic path-finding to enable decoder-only LLMs to travel among multiple “Q-alternatives” (Quantity Alternatives) and search for the optimal quantity in coordination with a conversation goal. Automatic and human evaluations demonstrate the effectiveness of Q-Tuning and Q-Traveling in constructing human-like, user-centered conversation agents.</abstract>
      <url hash="3440ec11">2025.acl-short.1</url>
      <bibkey>li-etal-2025-towards-llm</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>MIRAGE</fixed-case>: Exploring How Large Language Models Perform in Complex Social Interactive Environments</title>
      <author><first>Yin</first><last>Cai</last></author>
      <author><first>Zhouhong</first><last>Gu</last></author>
      <author><first>Zhaohan</first><last>Du</last><affiliation>Fudan University</affiliation></author>
      <author><first>Zheyu</first><last>Ye</last><affiliation>Xiaohongshu Inc</affiliation></author>
      <author><first>Shaosheng</first><last>Cao</last></author>
      <author><first>Yiqian</first><last>Xu</last></author>
      <author><first>Hongwei</first><last>Feng</last><affiliation>Fudan University</affiliation></author>
      <author><first>Ping</first><last>Chen</last><affiliation>Fudan University</affiliation></author>
      <pages>14-40</pages>
      <abstract>Large Language Models (LLMs) have shown remarkable capabilities in environmental perception, reasoning-based decision-making, and simulating complex human behaviors, particularly in interactive role-playing contexts. This paper introduces the Multiverse Interactive Role-play Ability General Evaluation (MIRAGE), a comprehensive framework designed to assess LLMs’ proficiency in portraying advanced human behaviors through murder mystery games. MIRAGE features eight intricately crafted scripts encompassing diverse themes and styles, providing a rich simulation. To evaluate LLMs’ performance, MIRAGE employs four distinct methods: the Trust Inclination Index (TII) to measure dynamics of trust and suspicion, the Clue Investigation Capability (CIC) to measure LLMs’ capability of conducting information, the Interactivity Capability Index (ICI) to assess role-playing capabilities and the Script Compliance Index (SCI) to assess LLMs’ capability of understanding and following instructions. Our experiments indicate that even popular models like GPT-4 face significant challenges in navigating the complexities presented by the MIRAGE. The datasets and simulation codes are available in https://github.com/lime728/MIRAGE.</abstract>
      <url hash="419c5870">2025.acl-short.2</url>
      <bibkey>cai-etal-2025-mirage</bibkey>
    </paper>
    <paper id="3">
      <title>Dynamic Label Name Refinement for Few-Shot Dialogue Intent Classification</title>
      <author><first>Gyutae</first><last>Park</last><affiliation>Chung-Ang University</affiliation></author>
      <author><first>Ingeol</first><last>Baek</last></author>
      <author><first>Byeongjeong</first><last>Kim</last><affiliation>Chung-Ang University</affiliation></author>
      <author><first>Joongbo</first><last>Shin</last><affiliation>LG AI Research</affiliation></author>
      <author><first>Hwanhee</first><last>Lee</last><affiliation>Chung-Ang University</affiliation></author>
      <pages>41-52</pages>
      <abstract>Dialogue intent classification aims to identify the underlying purpose or intent of a user’s input in a conversation. Current intent classification systems encounter considerable challenges, primarily due to the vast number of possible intents and the significant semantic overlap among similar intent classes. In this paper, we propose a novel approach to few-shot dialogue intent classification through in context learning, incorporating dynamic label refinement to address these challenges. Our method retrieves relevant examples for a test input from the training set and leverages a large language model to dynamically refine intent labels based on semantic understanding, ensuring that intents are clearly distinguishable from one another. Experimental results demonstrate that our approach effectively resolves confusion between semantically similar intents, resulting in significantly enhanced performance across multiple datasets compared to baselines. We also show that our method generates more interpretable intent labels, and has a better semantic coherence in capturing underlying user intents compared to baselines.</abstract>
      <url hash="9930b0af">2025.acl-short.3</url>
      <bibkey>park-etal-2025-dynamic</bibkey>
    </paper>
    <paper id="4">
      <title>Rethinking <fixed-case>K</fixed-case>en<fixed-case>LM</fixed-case>: Good and Bad Model Ensembles for Efficient Text Quality Filtering in Large Web Corpora</title>
      <author><first>Yungi</first><last>Kim</last><affiliation>Upstage</affiliation></author>
      <author><first>Hyunsoo</first><last>Ha</last><affiliation>Upstage</affiliation></author>
      <author><first>Sukyung</first><last>Lee</last></author>
      <author><first>Jihoo</first><last>Kim</last></author>
      <author><first>Seonghoon</first><last>Yang</last></author>
      <author><first>Chanjun</first><last>Park</last><affiliation>Korea University</affiliation></author>
      <pages>53-58</pages>
      <abstract>With the increasing demand for substantial amounts of high-quality data to train large language models (LLMs), efficiently filtering large web corpora has become a critical challenge. For this purpose, KenLM, a lightweight n-gram-based language model that operates on CPUs, is widely used. However, the traditional method of training KenLM utilizes only high-quality data and, consequently, does not explicitly learn the linguistic patterns of low-quality data. To address this issue, we propose an ensemble approach that leverages two contrasting KenLMs: (i) Good KenLM, trained on high-quality data; and (ii) Bad KenLM, trained on low-quality data. Experimental results demonstrate that our approach significantly reduces noisy content while preserving high-quality content compared to the traditional KenLM training method. This indicates that our method can be a practical solution with minimal computational overhead for resource-constrained environments.</abstract>
      <url hash="d8681d83">2025.acl-short.4</url>
      <bibkey>kim-etal-2025-rethinking-kenlm</bibkey>
    </paper>
    <paper id="5">
      <title>Automatic detection of dyslexia based on eye movements during reading in <fixed-case>R</fixed-case>ussian</title>
      <author><first>Anna</first><last>Laurinavichyute</last><affiliation>Universität Potsdam</affiliation></author>
      <author><first>Anastasiya</first><last>Lopukhina</last></author>
      <author><first>David Robert</first><last>Reich</last><affiliation>University of Zurich</affiliation></author>
      <pages>59-66</pages>
      <abstract>Dyslexia, a common learning disability, requires an early diagnosis. However, current screening tests are very time- and resource-consuming. We present an LSTM that aims to automatically classify dyslexia based on eye movements recorded during natural readingcombined with basic demographic information and linguistic features. The proposed model reaches an AUC of 0.93 and outperforms thestate-of-the-art model by 7 %. We report several ablation studies demonstrating that the fixation features matter the most for classification.</abstract>
      <url hash="bc8d63f9">2025.acl-short.5</url>
      <bibkey>laurinavichyute-etal-2025-automatic</bibkey>
    </paper>
    <paper id="6">
      <title>Doc-React: Multi-page Heterogeneous Document Question-answering</title>
      <author><first>Junda</first><last>Wu</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Yu</first><last>Xia</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Tong</first><last>Yu</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Xiang</first><last>Chen</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Sai Sree</first><last>Harsha</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Akash V</first><last>Maharaj</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Ruiyi</first><last>Zhang</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Victor</first><last>Bursztyn</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Sungchul</first><last>Kim</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Ryan A.</first><last>Rossi</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Julian</first><last>McAuley</last><affiliation>University of California, San Diego, University of California, San Diego</affiliation></author>
      <author><first>Yunyao</first><last>Li</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Ritwik</first><last>Sinha</last></author>
      <pages>67-78</pages>
      <abstract>Answering questions over multi-page, multimodal documents, including text and figures, is a critical challenge for applications that require answers to integrate information across multiple modalities and contextual dependencies. Existing methods, such as single-turn retrieval-augmented generation (RAG), struggle to retrieve fine-grained and contextually relevant information from large, heterogeneous documents, leading to suboptimal performance. Inspired by iterative frameworks like ReAct, which refine retrieval through feedback, we propose Doc-React, an adaptive iterative framework that balances information gain and uncertainty reduction at each step. Doc-React leverages InfoNCE-guided retrieval to approximate mutual information, enabling dynamic sub-query generation and refinement. A large language model (LLM) serves as both a judge and generator, providing structured feedback to iteratively improve retrieval. By combining mutual information optimization with entropy-aware selection, Doc-React systematically captures relevant multimodal content, achieving strong performance on complex QA tasks</abstract>
      <url hash="3d85bacb">2025.acl-short.6</url>
      <bibkey>wu-etal-2025-doc</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>C</fixed-case>on<fixed-case>ECT</fixed-case> Dataset: Overcoming Data Scarcity in Context-Aware <fixed-case>E</fixed-case>-Commerce <fixed-case>MT</fixed-case></title>
      <author><first>Mikołaj</first><last>Pokrywka</last><affiliation>Laniqo</affiliation></author>
      <author><first>Wojciech</first><last>Kusa</last><affiliation>NASK - National Research Institute</affiliation></author>
      <author><first>Mieszko</first><last>Rutkowski</last><affiliation>Allegro Sp. z o. o.</affiliation></author>
      <author><first>Mikołaj</first><last>Koszowski</last><affiliation>Allegro</affiliation></author>
      <pages>79-86</pages>
      <abstract>Neural Machine Translation (NMT) has improved translation by using Transformer-based models, but it still struggles with word ambiguity and context. This problem is especially important in domain-specific applications, which often have problems with unclear sentences or poor data quality. Our research explores how adding information to models can improve translations in the context of e-commerce data. To this end we create ConECT– a new Czech-to-Polish e-commerce product translation dataset coupled with images and product metadata consisting of 11,400 sentence pairs. We then investigate and compare different methods that are applicable to context-aware translation. We test a vision-language model (VLM), finding that visual context aids translation quality. Additionally, we explore the incorporation of contextual information into text-to-text models, such as the product’s category path or image descriptions. The results of our study demonstrate that the incorporation of contextual information leads to an improvement in the quality of machine translation. We make the new dataset publicly available.</abstract>
      <url hash="a043cd5e">2025.acl-short.7</url>
      <bibkey>pokrywka-etal-2025-conect</bibkey>
    </paper>
    <paper id="8">
      <title>A Measure of the System Dependence of Automated Metrics</title>
      <author><first>Pius</first><last>Von Däniken</last><affiliation>University of Zurich and ZHAW - Zürcher Hochschule für Angewandte Wissenschaften</affiliation></author>
      <author><first>Jan Milan</first><last>Deriu</last><affiliation>ZHAW - Zürcher Hochschule für Angewandte Wissenschaften</affiliation></author>
      <author><first>Mark</first><last>Cieliebak</last><affiliation>Zurich University of Applied Sciences ZHAW</affiliation></author>
      <pages>87-99</pages>
      <abstract>Automated metrics for Machine Translation have made significant progress, with the goal of replacing expensive and time-consuming human evaluations. These metrics are typically assessed by their correlation with human judgments, which captures the monotonic relationship between human and metric scores. However, we argue that it is equally important to ensure that metrics treat all systems fairly and consistently. In this paper, we introduce a method to evaluate this aspect.</abstract>
      <url hash="83ed7aa5">2025.acl-short.8</url>
      <bibkey>von-daniken-etal-2025-measure</bibkey>
    </paper>
    <paper id="9">
      <title>Call for Rigor in Reporting Quality of Instruction Tuning Data</title>
      <author><first>Hyeonseok</first><last>Moon</last><affiliation>Korea University</affiliation></author>
      <author><first>Jaehyung</first><last>Seo</last></author>
      <author><first>Heuiseok</first><last>Lim</last></author>
      <pages>100-109</pages>
      <abstract>Instruction tuning is crucial for adapting large language models (LLMs) to align with user intentions. Numerous studies emphasize the significance of the quality of instruction tuning (IT) data, revealing a strong correlation between IT data quality and the alignment performance of LLMs. In these studies, the quality of IT data is typically assessed by evaluating the performance of LLMs trained with that data. However, we identified a prevalent issue in such practice: hyperparameters for training models are often selected arbitrarily without adequate justification. We observed significant variations in hyperparameters applied across different studies, even when training the same model with the same data. In this study, we demonstrate the potential problems arising from this practice and emphasize the need for careful consideration in verifying data quality. Through our experiments on the quality of LIMA data and a selected set of 1,000 Alpaca data points, we demonstrate that arbitrary hyperparameter decisions can make any arbitrary conclusion.</abstract>
      <url hash="1b1aaf41">2025.acl-short.9</url>
      <bibkey>moon-etal-2025-call</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>BQA</fixed-case>: Body Language Question Answering Dataset for Video Large Language Models</title>
      <author><first>Shintaro</first><last>Ozaki</last></author>
      <author><first>Kazuki</first><last>Hayashi</last></author>
      <author><first>Miyu</first><last>Oba</last></author>
      <author><first>Yusuke</first><last>Sakai</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Hidetaka</first><last>Kamigaito</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <pages>110-123</pages>
      <abstract>A large part of human communication relies on nonverbal cues such as facial expressions, eye contact, and body language. Unlike language or sign language, such nonverbal communication lacks formal rules, requiring complex reasoning based on commonsense understanding.Enabling current Video Large Language Models (VideoLLMs) to accurately interpret body language is a crucial challenge, as human unconscious actions can easily cause the model to misinterpret their intent.To address this, we propose a dataset, BQA, a body language question answering dataset, to validate whether the model can correctly interpret emotions from short clips of body language comprising 26 emotion labels of videos of body language.We evaluated various VideoLLMs on the BQA with and without Multimodal Chain of Thought (CoT) and revealed that understanding body language is challenging, and our analyses of the wrong answers by VideoLLMs show that certain VideoLLMs made largely biased answers depending on the age group and ethnicity of the individuals. We also found consistent error patterns in VideoLLMs.</abstract>
      <url hash="0d3ca4ac">2025.acl-short.10</url>
      <bibkey>ozaki-etal-2025-bqa</bibkey>
    </paper>
    <paper id="11">
      <title>Grounded, or a Good Guesser? A Per-Question Balanced Dataset to Separate Blind from Grounded Models for Embodied Question Answering</title>
      <author><first>Miles</first><last>Shelton</last></author>
      <author><first>Nate</first><last>Wingerd</last></author>
      <author><first>Kritim K</first><last>Rijal</last></author>
      <author><first>Ayush</first><last>Garg</last></author>
      <author><first>Adelina</first><last>Gutic</last></author>
      <author><first>Brett</first><last>Barnes</last></author>
      <author><first>Catherine</first><last>Finegan-Dollak</last><affiliation>University of Richmond</affiliation></author>
      <pages>124-135</pages>
      <abstract>Embodied question answering (EQA) means using *perception of* and *action in* an environment to answer natural language questions about that environment. However, previous work has demonstrated that blind language models (which do not incorporate perception, but predict an answer based solely on the question text) are a strong baseline for existing benchmarks, even compared against state-of-the-art vision and language models. To determine whether a model is grounding its answers in its specific environment, rather than relying on a language model’s expectations about the world generally, we propose PQB-EQA, a *per-question balanced* EQA dataset. In this new benchmark, every question appears twice, paired with two different environments that yield two different answers. That is, the answer distribution is balanced for each question, not just across the whole dataset. We show both theoretically and empirically that grounding in the environment is necessary to perform better than chance on PQB-EQA.</abstract>
      <url hash="3ff43a2b">2025.acl-short.11</url>
      <bibkey>shelton-etal-2025-grounded</bibkey>
    </paper>
    <paper id="12">
      <title>Learning Sparsity for Effective and Efficient Music Performance Question Answering</title>
      <author><first>Xingjian</first><last>Diao</last></author>
      <author><first>Tianzhen</first><last>Yang</last></author>
      <author><first>Chunhui</first><last>Zhang</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Weiyi</first><last>Wu</last></author>
      <author><first>Ming</first><last>Cheng</last></author>
      <author><first>Jiang</first><last>Gui</last><affiliation>Dartmouth College</affiliation></author>
      <pages>136-146</pages>
      <abstract>Music performances, characterized by dense and continuous audio as well as seamless audio-visual integration, present unique challenges for multimodal scene understanding and reasoning. Recent Music Performance Audio-Visual Question Answering (Music AVQA) datasets have been proposed to reflect these challenges, highlighting the continued need for more effective integration of audio-visual representations in complex question answering. However, existing Music AVQA methods often rely on dense and unoptimized representations, leading to inefficiencies in the isolation of key information, the reduction of redundancy, and the prioritization of critical samples. To address these challenges, we introduce Sparsify, a sparse learning framework specifically designed for Music AVQA. It integrates three sparsification strategies into an end-to-end pipeline and achieves state-of-the-art performance on the Music AVQA datasets. In addition, it reduces training time by 28.32% compared to its fully trained dense counterpart while maintaining accuracy, demonstrating clear efficiency gains. To further improve data efficiency, we propose a key-subset selection algorithm that selects and uses approximately 25% of MUSIC-AVQA v2.0 training data and retains 70–80% of full-data performance across models.</abstract>
      <url hash="6edfacee">2025.acl-short.12</url>
      <bibkey>diao-etal-2025-learning</bibkey>
    </paper>
    <paper id="13">
      <title>Cross-Lingual Transfer of Cultural Knowledge: An Asymmetric Phenomenon</title>
      <author><first>Chen</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Zhiyuan</first><last>Liao</last><affiliation>Peking University</affiliation></author>
      <author><first>Yansong</first><last>Feng</last><affiliation>Peking University</affiliation></author>
      <pages>147-157</pages>
      <abstract>Despite substantial research efforts evaluating how well large language models (LLMs) handle global cultural diversity, the mechanisms behind their cultural knowledge acquisition, particularly in multilingual settings, remain unclear. We study this question by investigating how cultural knowledge transfers across languages during the language adaptation of LLMs, a process where an LLM is continually pre-trained to learn another language. We introduce an interpretable framework to study this transfer, ensuring training data transparency and controlling transfer effects. Through a study of four non-Anglophonic cultures, we observe bidirectional cultural transfer between English and other high-resource languages, while low-resource languages primarily transfer knowledge to English with limited reverse flow. To explain this asymmetric phenomenon, we propose a frequency-based hypothesis: cultural knowledge appearing more frequently in the pretraining data transfers more easily, which is supported by empirical analysis of the training corpora. We hope our findings could inform future research on knowledge transfer and promote the development of culturally aware models, particularly for low-resource languages.</abstract>
      <url hash="fb7fa41c">2025.acl-short.13</url>
      <bibkey>zhang-etal-2025-cross</bibkey>
    </paper>
    <paper id="14">
      <title>Leveraging Human Production-Interpretation Asymmetries to Test <fixed-case>LLM</fixed-case> Cognitive Plausibility</title>
      <author><first>Suet-Ying</first><last>Lam</last><affiliation>University of Massachusetts at Amherst</affiliation></author>
      <author><first>Qingcheng</first><last>Zeng</last></author>
      <author><first>Jingyi</first><last>Wu</last></author>
      <author><first>Rob</first><last>Voigt</last><affiliation>Northwestern University</affiliation></author>
      <pages>158-171</pages>
      <abstract>Whether large language models (LLMs) process language similarly to humans has been the subject of much theoretical and practical debate. We examine this question through the lens of the production-interpretation distinction found in human sentence processing and evaluate the extent to which instruction-tuned LLMs replicate this distinction. Using an empirically documented asymmetry between pronoun production and interpretation in humans for implicit causality verbs as a testbed, we find that some LLMs do quantitatively and qualitatively reflect human-like asymmetries between production and interpretation. We demonstrate that whether this behavior holds depends upon both model size-with larger models more likely to reflect human-like patterns and the choice of meta-linguistic prompts used to elicit the behavior. Our codes and results are available here.</abstract>
      <url hash="4214f06f">2025.acl-short.14</url>
      <bibkey>lam-etal-2025-leveraging</bibkey>
    </paper>
    <paper id="15">
      <title>Improving the Calibration of Confidence Scores in Text Generation Using the Output Distribution’s Characteristics</title>
      <author><first>Lorenzo Jaime Yu</first><last>Flores</last></author>
      <author><first>Ori</first><last>Ernst</last><affiliation>Mila - Quebec Artificial Intelligence Institute</affiliation></author>
      <author><first>Jackie CK</first><last>Cheung</last><affiliation>McGill University, Mila Research Institute and Microsoft</affiliation></author>
      <pages>172-182</pages>
      <abstract>Well-calibrated model confidence scores can improve the usefulness of text generation models. For example, users can be prompted to review predictions with low confidence scores, to prevent models from returning bad or potentially dangerous predictions. However, confidence metrics are not always well calibrated in text generation. One reason is that in generation, there can be many valid answers, which previous methods do not always account for. Hence, a confident model could assign probability to many sequences because they are all valid, and not because it is unsure about how to perform the task. We propose task-agnostic confidence metrics suited to generation, which rely solely on model probabilities without the need for further fine-tuning or heuristics. Using these, we are able to improve the calibration of BART and Flan-T5 on summarization, translation, and question answering datasets.</abstract>
      <url hash="e3f0a538">2025.acl-short.15</url>
      <bibkey>flores-etal-2025-improving</bibkey>
    </paper>
    <paper id="16">
      <title><fixed-case>K</fixed-case>now<fixed-case>S</fixed-case>hift<fixed-case>QA</fixed-case>: How Robust are <fixed-case>RAG</fixed-case> Systems when Textbook Knowledge Shifts in K-12 Education?</title>
      <author><first>Tianshi</first><last>Zheng</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Weihan</first><last>Li</last></author>
      <author><first>Jiaxin</first><last>Bai</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Weiqi</first><last>Wang</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Yangqiu</first><last>Song</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <pages>183-195</pages>
      <abstract>Retrieval-Augmented Generation (RAG) systems show remarkable potential as question answering tools in the K-12 Education domain, where knowledge is typically queried within the restricted scope of authoritative textbooks. However, discrepancies between these textbooks and the parametric knowledge inherent in Large Language Models (LLMs) can undermine the effectiveness of RAG systems. To systematically investigate RAG system robustness against such knowledge discrepancies, we introduce KnowShiftQA. This novel question answering dataset simulates these discrepancies by applying deliberate hypothetical knowledge updates to both answers and source documents, reflecting how textbook knowledge can shift. KnowShiftQA comprises 3,005 questions across five subjects, designed with a comprehensive question typology focusing on context utilization and knowledge integration. Our extensive experiments on retrieval and question answering performance reveal that most RAG systems suffer a substantial performance drop when faced with these knowledge discrepancies. Furthermore, questions requiring the integration of contextual (textbook) knowledge with parametric (LLM) knowledge pose a significant challenge to current LLMs.</abstract>
      <url hash="16bf48db">2025.acl-short.16</url>
      <bibkey>zheng-etal-2025-knowshiftqa</bibkey>
    </paper>
    <paper id="17">
      <title>Improving Parallel Sentence Mining for Low-Resource and Endangered Languages</title>
      <author><first>Shu</first><last>Okabe</last><affiliation>Technische Universität München</affiliation></author>
      <author><first>Katharina</first><last>Hämmerl</last></author>
      <author><first>Alexander</first><last>Fraser</last><affiliation>Technical University of Munich</affiliation></author>
      <pages>196-205</pages>
      <abstract>While parallel sentence mining has been extensively covered for fairly well-resourced languages, pairs involving low-resource languages have received comparatively little attention.To address this gap, we present Belopsem, a benchmark of new datasets for parallel sentence mining on three language pairs where the source side is low-resource and endangered: Occitan-Spanish, Upper Sorbian-German, and Chuvash-Russian. These combinations also reflect varying linguistic similarity within each pair. We compare three language models in an established parallel sentence mining pipeline and apply two types of improvements to one of them, Glot500. We observe better mining quality overall by both applying alignment post-processing with an unsupervised aligner and using a cluster-based isotropy enhancement technique. These findings are crucial for optimising parallel data extraction for low-resource languages in a realistic way.</abstract>
      <url hash="c3ac8fa6">2025.acl-short.17</url>
      <bibkey>okabe-etal-2025-improving</bibkey>
    </paper>
    <paper id="18">
      <title>Revisiting Epistemic Markers in Confidence Estimation: Can Markers Accurately Reflect Large Language Models’ Uncertainty?</title>
      <author><first>Jiayu</first><last>Liu</last></author>
      <author><first>Qing</first><last>Zong</last></author>
      <author><first>Weiqi</first><last>Wang</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Yangqiu</first><last>Song</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <pages>206-221</pages>
      <abstract>As large language models (LLMs) are increasingly used in high-stakes domains, accurately assessing their confidence is crucial. Humans typically express confidence through epistemic markers (e.g., “fairly confident”) instead of numerical values. However, it remains unclear whether LLMs consistently use these markers to reflect their intrinsic confidence due to the difficulty of quantifying uncertainty associated with various markers. To address this gap, we first define ***marker confidence*** as the observed accuracy when a model employs an epistemic marker. We evaluate its stability across multiple question-answering datasets in both in-distribution and out-of-distribution settings for open-source and proprietary LLMs. Our results show that while markers generalize well within the same distribution, their confidence is inconsistent in out-of-distribution scenarios. These findings raise significant concerns about the reliability of epistemic markers for confidence estimation, underscoring the need for improved alignment between marker based confidence and actual model uncertainty. Our code is available at https://github.com/HKUST-KnowComp/MarCon.</abstract>
      <url hash="fca1b6c8">2025.acl-short.18</url>
      <bibkey>liu-etal-2025-revisiting</bibkey>
    </paper>
    <paper id="19">
      <title>Limited-Resource Adapters Are Regularizers, Not Linguists</title>
      <author><first>Marcell</first><last>Fekete</last></author>
      <author><first>Nathaniel Romney</first><last>Robinson</last><affiliation>Department of Computer Science, Whiting School of Engineering</affiliation></author>
      <author><first>Ernests</first><last>Lavrinovics</last></author>
      <author><first>Djeride</first><last>Jean-Baptiste</last><affiliation>University of Ottawa</affiliation></author>
      <author><first>Raj</first><last>Dabre</last><affiliation>Department of Computer Science, Indian Institute of Technology, Madras, Indian Institute of Technology, Madras and National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Johannes</first><last>Bjerva</last><affiliation>Aalborg University</affiliation></author>
      <author><first>Heather</first><last>Lent</last><affiliation>Aalborg University</affiliation></author>
      <pages>222-237</pages>
      <abstract>Cross-lingual transfer from related high-resource languages is a well-established strategy to enhance low-resource language technologies. Prior work has shown that adapters show promise for, e.g., improving low-resource machine translation (MT). In this work, we investigate an adapter souping method combined with cross-attention fine-tuning of a pre-trained MT model to leverage language transfer for three low-resource Creole languages, which exhibit relatedness to different language groups across distinct linguistic dimensions. Our approach improves performance substantially over baselines. However, we find that linguistic relatedness—or even a lack thereof—does not covary meaningfully with adapter performance. Surprisingly, our cross-attention fine-tuning approach appears equally effective with randomly initialized adapters, implying that the benefit of adapters in this setting lies in parameter regularization, and not in meaningful information transfer. We provide analysis supporting this regularization hypothesis. Our findings underscore the reality that neural language processing involves many success factors, and that not all neural methods leverage linguistic knowledge in intuitive ways.</abstract>
      <url hash="6487fa10">2025.acl-short.19</url>
      <bibkey>fekete-etal-2025-limited</bibkey>
    </paper>
    <paper id="20">
      <title><fixed-case>LLM</fixed-case>s instead of Human Judges? A Large Scale Empirical Study across 20 <fixed-case>NLP</fixed-case> Evaluation Tasks</title>
      <author><first>Anna</first><last>Bavaresco</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Raffaella</first><last>Bernardi</last><affiliation>Free University of Bozen Bolzano</affiliation></author>
      <author><first>Leonardo</first><last>Bertolazzi</last><affiliation>University of Trento</affiliation></author>
      <author><first>Desmond</first><last>Elliott</last><affiliation>Copenhagen University and University of Copenhagen</affiliation></author>
      <author><first>Raquel</first><last>Fernández</last><affiliation>University of Amsterdam and University of Amsterdam</affiliation></author>
      <author><first>Albert</first><last>Gatt</last><affiliation>Utrecht University</affiliation></author>
      <author><first>Esam</first><last>Ghaleb</last></author>
      <author><first>Mario</first><last>Giulianelli</last><affiliation>Department of Computer Science, ETHZ - ETH Zurich</affiliation></author>
      <author><first>Michael</first><last>Hanna</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Alexander</first><last>Koller</last><affiliation>Saarland University</affiliation></author>
      <author><first>Andre</first><last>Martins</last><affiliation>Instituto Superior Técnico and Unbabel</affiliation></author>
      <author><first>Philipp</first><last>Mondorf</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Vera</first><last>Neplenbroek</last></author>
      <author><first>Sandro</first><last>Pezzelle</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Barbara</first><last>Plank</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>David</first><last>Schlangen</last><affiliation>University of Potsdam</affiliation></author>
      <author><first>Alessandro</first><last>Suglia</last><affiliation>Heriot-Watt University</affiliation></author>
      <author><first>Aditya K</first><last>Surikuchi</last></author>
      <author><first>Ece</first><last>Takmaz</last><affiliation>Utrecht University</affiliation></author>
      <author><first>Alberto</first><last>Testoni</last><affiliation>Amsterdam UMC</affiliation></author>
      <pages>238-255</pages>
      <abstract>There is an increasing trend towards evaluating NLP models with LLMs instead of human judgments, raising questions about the validity of these evaluations, as well as their reproducibility in the case of proprietary models. We provide JUDGE-BENCH, an extensible collection of 20 NLP datasets with human annotations covering a broad range of evaluated properties and types of data, and comprehensively evaluate 11 current LLMs, covering both open-weight and proprietary models, for their ability to replicate the annotations. Our evaluations show substantial variance across models and datasets. Models are reliable evaluators on some tasks, but overall display substantial variability depending on the property being evaluated, the expertise level of the human judges, and whether the language is human or model-generated. We conclude that LLMs should be carefully validated against human judgments before being used as evaluators.</abstract>
      <url hash="c5851f85">2025.acl-short.20</url>
      <bibkey>bavaresco-etal-2025-llms</bibkey>
    </paper>
    <paper id="21">
      <title><fixed-case>F</fixed-case>ocal<fixed-case>PO</fixed-case>: Enhancing Preference Optimizing by Focusing on Correct Preference Rankings</title>
      <author><first>Tong</first><last>Liu</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Xiao</first><last>Yu</last></author>
      <author><first>Wenxuan</first><last>Zhou</last><affiliation>Google Deepmind</affiliation></author>
      <author><first>Jindong</first><last>Gu</last><affiliation>University of Oxford</affiliation></author>
      <author><first>Volker</first><last>Tresp</last><affiliation>Ludwig Maximilian University of Munich and Siemens Corporate Research</affiliation></author>
      <pages>256-267</pages>
      <abstract>Efficient preference optimization algorithms such as Direct Preference Optimization (DPO) have become a popular approach in aligning large language models (LLMs) with human preferences. These algorithms implicitly treat the LLM as a reward model, and focus on training it to correct misranked preference pairs. However, recent work (CITATION) empirically finds that DPO training <i>rarely improves these misranked preference pairs</i>, despite its gradient emphasizing on these cases. We introduce FocalPO, a DPO variant that instead <i>down-weighs</i> misranked preference pairs and prioritizes enhancing the model’s understanding of pairs that it can already rank correctly. Inspired by Focal Loss used in vision tasks, FocalPO achieves this by adding a modulating factor to dynamically scale DPO loss. Our experiment demonstrates that FocalPO surpasses DPO and its variants on popular benchmarks like Alpaca Eval 2.0 and Arena-Hard using Mistral-Base-7B and Llama-3-Instruct-8B, with the introduced hyperparameter fixed. Additionally, we empirically reveals how FocalPO affects training on correct and incorrect sample groups, further underscoring its effectiveness.</abstract>
      <url hash="fff2bc0e">2025.acl-short.21</url>
      <bibkey>liu-etal-2025-focalpo</bibkey>
    </paper>
    <paper id="22">
      <title>Combining Domain and Alignment Vectors Provides Better Knowledge-Safety Trade-offs in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Megh</first><last>Thakkar</last></author>
      <author><first>Quentin</first><last>Fournier</last><affiliation>Mila - Quebec AI Institute</affiliation></author>
      <author><first>Matthew</first><last>Riemer</last><affiliation>Montreal Institute for Learning Algorithms, University of Montreal, University of Montreal and International Business Machines</affiliation></author>
      <author><first>Pin-Yu</first><last>Chen</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Amal</first><last>Zouaq</last><affiliation>Polytechnique Montreal</affiliation></author>
      <author><first>Payel</first><last>Das</last></author>
      <author><first>Sarath</first><last>Chandar</last><affiliation>École Polytechnique de Montréal</affiliation></author>
      <pages>268-277</pages>
      <abstract>There is a growing interest in training domain-expert LLMs that excel in specific technical fields compared to their general-purpose instruction-tuned counterparts. However, these expert models are not either explicitly trained to be safe, or experience a loss in their safety abilities in the process, making them capable of generating harmful content. We observe that simple interpolation between the domain and alignment delta parameters leads to safer domain-specific models that preserve their utility. Building on this, we introduce MergeAlign, a simple, efficient, and effective model merging-based alignment method. We apply MergeAlign on Llama3 models that are experts in medicine and finance, obtaining substantial safety alignment improvements with minimal to no degradation on domain-specific benchmarks. We study the impact of model merging through model similarity metrics and contributions of individual models being merged, as well as the applicability of MergeAlign on more general code and math expert models using the Qwen-2.5 series of models. We hope our findings open new research avenues towards efficient development and deployment of safe expert LLMs.</abstract>
      <url hash="4cbaaa7b">2025.acl-short.22</url>
      <bibkey>thakkar-etal-2025-combining</bibkey>
    </paper>
    <paper id="23">
      <title>Can Uniform Meaning Representation Help <fixed-case>GPT</fixed-case>-4 Translate from Indigenous Languages?</title>
      <author><first>Shira</first><last>Wein</last><affiliation>Amherst College</affiliation></author>
      <pages>278-285</pages>
      <abstract>While ChatGPT and GPT-based models are able to effectively perform many tasks without additional fine-tuning, they struggle with tasks related to extremely low-resource languages and indigenous languages. Uniform Meaning Representation (UMR), a semantic representation designed to capture the meaning of texts in many languages, is well-positioned to be leveraged in the development of low-resource language technologies. In this work, we explore the downstream utility of UMR for low-resource languages by incorporating it into GPT-4 prompts. Specifically, we examine the ability of GPT-4 to perform translation from three indigenous languages (Navajo, Arápaho, and Kukama), with and without demonstrations, as well as with and without UMR annotations. Ultimately, we find that in the majority of our test cases, integrating UMR into the prompt results in a statistically significant increase in performance, which is a promising indication of future applications of the UMR formalism.</abstract>
      <url hash="2eec1a70">2025.acl-short.23</url>
      <bibkey>wein-2025-uniform</bibkey>
    </paper>
    <paper id="24">
      <title>Subword models struggle with word learning, but surprisal hides it</title>
      <author><first>Bastian</first><last>Bunzeck</last><affiliation>Universität Bielefeld</affiliation></author>
      <author><first>Sina</first><last>Zarrieß</last><affiliation>Bielefeld University</affiliation></author>
      <pages>286-300</pages>
      <abstract>We study word learning in subword and character language models with the psycholinguistic lexical decision task. While subword LMs struggle to discern words and non-words with high accuracy, character LMs solve this task easily and consistently. Only when supplied with further contexts do subword LMs perform similarly to character models. Additionally, when looking at word-level and syntactic learning trajectories, we find that both processes are separable in character LMs. Word learning happens before syntactic learning, whereas both occur simultaneously in subword LMs. This raises questions about the adequacy of subword LMs for modeling language acquisition and positions character LMs as a viable alternative to study processes below the syntactic level.</abstract>
      <url hash="8aece755">2025.acl-short.24</url>
      <bibkey>bunzeck-zarriess-2025-subword</bibkey>
    </paper>
    <paper id="25">
      <title><fixed-case>LLM</fixed-case> as Entity Disambiguator for Biomedical Entity-Linking</title>
      <author><first>Christophe</first><last>Ye</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Cassie S.</first><last>Mitchell</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <pages>301-312</pages>
      <abstract>Entity linking involves normalizing a mention in medical text to a unique identifier in a knowledge base, such as UMLS or MeSH. Most entity linkers follow a two-stage process: first, a candidate generation step selects high-quality candidates, and then a named entity disambiguation phase determines the best candidate for final linking. This study demonstrates that leveraging a large language model (LLM) as an entity disambiguator significantly enhances entity linking models’ accuracy and recall. Specifically, the LLM disambiguator achieves remarkable improvements when applied to alias-matching entity linking methods. Without any fine-tuning, our approach establishes a new state-of-the-art (SOTA), surpassing previous methods on multiple prevalent biomedical datasets by up to 16 points in accuracy. We released our code on GitHub at https://github.com/ChristopheYe/llm_disambiguator</abstract>
      <url hash="e03ea43b">2025.acl-short.25</url>
      <bibkey>ye-mitchell-2025-llm</bibkey>
    </paper>
    <paper id="26">
      <title>Towards Geo-Culturally Grounded <fixed-case>LLM</fixed-case> Generations</title>
      <author><first>Piyawat</first><last>Lertvittayakumjorn</last><affiliation>Google</affiliation></author>
      <author><first>David</first><last>Kinney</last><affiliation>Washington University, Saint Louis</affiliation></author>
      <author><first>Vinodkumar</first><last>Prabhakaran</last><affiliation>Google</affiliation></author>
      <author><first>Donald Martin</first><last>Jr.</last><affiliation>Global Institute for the Learning Society and Google</affiliation></author>
      <author><first>Sunipa</first><last>Dev</last><affiliation>Google</affiliation></author>
      <pages>313-330</pages>
      <abstract>Generative large language models (LLMs) have demonstrated gaps in diverse cultural awareness across the globe. We investigate the effect of retrieval augmented generation and search-grounding techniques on LLMs’ ability to display familiarity with various national cultures. Specifically, we compare the performance of standard LLMs, LLMs augmented with retrievals from a bespoke knowledge base (i.e., KB grounding), and LLMs augmented with retrievals from a web search (i.e., search grounding) on multiple cultural awareness benchmarks. We find that search grounding significantly improves the LLM performance on multiple-choice benchmarks that test propositional knowledge (e.g., cultural norms, artifacts, and institutions), while KB grounding’s effectiveness is limited by inadequate knowledge base coverage and a suboptimal retriever. However, search grounding also increases the risk of stereotypical judgments by language models and fails to improve evaluators’ judgments of cultural familiarity in a human evaluation with adequate statistical power. These results highlight the distinction between propositional cultural knowledge and open-ended cultural fluency when it comes to evaluating LLMs’ cultural awareness.</abstract>
      <url hash="8f6b4bad">2025.acl-short.26</url>
      <bibkey>lertvittayakumjorn-etal-2025-towards</bibkey>
    </paper>
    <paper id="27">
      <title><fixed-case>MUSTS</fixed-case>: <fixed-case>MU</fixed-case>ltilingual Semantic Textual Similarity Benchmark</title>
      <author><first>Tharindu</first><last>Ranasinghe</last><affiliation>Lancaster University</affiliation></author>
      <author><first>Hansi</first><last>Hettiarachchi</last><affiliation>Lancaster University</affiliation></author>
      <author><first>Constantin</first><last>Orasan</last><affiliation>University of Surrey</affiliation></author>
      <author><first>Ruslan</first><last>Mitkov</last><affiliation>Lancaster University</affiliation></author>
      <pages>331-353</pages>
      <abstract>Predicting semantic textual similarity (STS) is a complex and ongoing challenge in natural language processing (NLP). Over the years, researchers have developed a variety of supervised and unsupervised approaches to calculate STS automatically. Additionally, various benchmarks, which include STS datasets, have been established to consistently evaluate and compare these STS methods. However, they largely focus on high-resource languages, mixed with datasets annotated focusing on relatedness instead of similarity and containing automatically translated instances. Therefore, no dedicated benchmark for multilingual STS exists. To solve this gap, we introduce the Multilingual Semantic Textual Similarity Benchmark (MUSTS), which spans 13 languages, including low-resource languages. By evaluating more than 25 models on MUSTS, we establish the most comprehensive benchmark of multilingual STS methods. Our findings confirm that STS remains a challenging task, particularly for low-resource languages.</abstract>
      <url hash="0d36e9c4">2025.acl-short.27</url>
      <bibkey>ranasinghe-etal-2025-musts</bibkey>
    </paper>
    <paper id="28">
      <title>Can Large Language Models Accurately Generate Answer Keys for Health-related Questions?</title>
      <author><first>Davis</first><last>Bartels</last><affiliation>National Institutes of Health</affiliation></author>
      <author><first>Deepak</first><last>Gupta</last><affiliation>National Institutes of Health</affiliation></author>
      <author><first>Dina</first><last>Demner-Fushman</last><affiliation>National Library of Medicine</affiliation></author>
      <pages>354-368</pages>
      <abstract>The evaluation of text generated by LLMs remains a challenge for question answering, retrieval augmented generation (RAG), summarization, and many other natural language processing tasks. Evaluating the factuality of LLM generated responses is particularly important in medical question answering, where the stakes are high. One method of evaluating the factuality of text is through the use of information nuggets (answer keys). Nuggets are text representing atomic facts that may be used by an assessor to make a binary decision as to whether the fact represented by said nugget is contained in an answer. Although manual nugget extraction is expensive and time-consuming, recent RAG shared task evaluations have explored automating the nuggetization of text with LLMs. In this work, we explore several approaches to nugget generation for medical question answering and evaluate their alignment with expert human nugget generation. We find providing an example and extracting nuggets from an answer to be the best approach to nuggetization. While, overall, we found the capabilities of LLMs to distill atomic facts limited, Llama 3.3 performed the best out of the models we tested.</abstract>
      <url hash="91fd9f3e">2025.acl-short.28</url>
      <bibkey>bartels-etal-2025-large</bibkey>
    </paper>
    <paper id="29">
      <title>Literary Evidence Retrieval via Long-Context Language Models</title>
      <author><first>Katherine</first><last>Thai</last><affiliation>University of Massachusetts at Amherst</affiliation></author>
      <author><first>Mohit</first><last>Iyyer</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>369-380</pages>
      <abstract>How well do modern long-context language models understand literary fiction? We explore this question via the task of literary evidence retrieval, repurposing the RELiC dataset of Thai et al. (2022) to construct a benchmark where the entire text of a primary source (e.g., The Great Gatsby) is provided to an LLM alongside literary criticism with a missing quotation from that work. This setting, in which the model must generate the missing quotation, mirrors the human process of literary analysis by requiring models to perform both global narrative reasoning and close textual examination. We curate a high-quality subset of 292 examples through extensive filtering and human verification. Our experiments show that recent reasoning models, such as Gemini 2.5 Pro can exceed human expert performance (62.5% vs. 50% accuracy). In contrast, the best open-weight model achieves only 29.1% accuracy, highlighting a wide gap in interpretive reasoning between open and closed-weight models. Despite their speed and apparent accuracy, even the strongest models struggle with nuanced literary signals and overgeneration, signaling open challenges for applying LLMs to literary analysis. We release our dataset and evaluation code to encourage future work in this direction.</abstract>
      <url hash="836b4bd9">2025.acl-short.29</url>
      <bibkey>thai-iyyer-2025-literary</bibkey>
    </paper>
    <paper id="30">
      <title>A Little Human Data Goes A Long Way</title>
      <author><first>Dhananjay</first><last>Ashok</last></author>
      <author><first>Jonathan</first><last>May</last><affiliation>University of Southern California and USC/ISI</affiliation></author>
      <pages>381-413</pages>
      <abstract>Faced with an expensive human annotation process, creators of NLP systems increasingly turn to synthetic data generation. While this method shows promise, the extent to which synthetic data can replace human annotation is poorly understood. We investigate the use of synthetic data in Fact Verification (FV) and Evidence-based Question Answering (QA) by incrementally replacing human-generated data with synthetic points on eight diverse datasets. Strikingly, replacing up to 90% of the training data only marginally decreases performance, but replacing the final 10% leads to severe declines. We find that models trained on purely synthetic data can be improved by including as few as 125 human-generated data points. We show that matching the performance gain of a little human data requires an order of magnitude more synthetic data, and then estimate price ratios at which human annotation would be a more cost-effective solution. Our results suggest that even when human annotation at scale is infeasible, there is great value to having a small proportion of the dataset being human-generated.</abstract>
      <url hash="bd9efdb6">2025.acl-short.30</url>
      <bibkey>ashok-may-2025-little</bibkey>
    </paper>
    <paper id="31">
      <title>Seeking Rational Demonstrations for Large Language Models: A Domain Generalization Approach to Unsupervised Cross-Domain Keyphrase Generation</title>
      <author><first>Guangzhen</first><last>Zhao</last><affiliation>Nanjing University of Posts and Telecommunications</affiliation></author>
      <author><first>Yu</first><last>Yao</last><affiliation>Chaohu University</affiliation></author>
      <author><first>Dechang</first><last>Kong</last></author>
      <author><first>Zhenjiang</first><last>Dong</last><affiliation>Nanjing University of Posts and Telecommunications</affiliation></author>
      <pages>414-424</pages>
      <abstract>Unsupervised cross-domain keyphrase generation is crucial in real-world natural language processing scenarios. However, the accuracy of up-to-date approaches is limited by the distribution shift between source and target domain, which stems from the cross-domain field. Large language models (LLMs) offer potential for the cross-domain keyphrase generation tasks due to their strong generalization abilities, facilitated by providing demonstrations relevant to the target task. Nevertheless, it is often difficult to obtain labeled samples from the target domain. To address this challenge, this paper aims to seek rational demonstrations from the source domain, thereby improving the LLMs’ ability in the unsupervised cross-domain keyphrase generation setting. Specifically, we design a novel domain-aware retrieval model on the source domain. Guided by insights from domain generalization theory, we introduce two generalization terms, one for cross-domain relevance and another for each domain consistency to better support retrieval of rational demonstrations. By the retrieved source-domain demonstrations and distance-based relevant score, the proposed approach achieves optimal accuracy. Comprehensive experiments on widely used cross-domain KG benchmarks demonstrate our approach’s state-of-the-art performance and effectiveness.</abstract>
      <url hash="760a817b">2025.acl-short.31</url>
      <bibkey>zhao-etal-2025-seeking</bibkey>
    </paper>
    <paper id="32">
      <title><fixed-case>L</fixed-case>ex<fixed-case>K</fixed-case>ey<fixed-case>P</fixed-case>lan: Planning with Keyphrases and Retrieval Augmentation for Legal Text Generation: A Case Study on <fixed-case>E</fixed-case>uropean Court of Human Rights Cases</title>
      <author><first>Santosh</first><last>T.y.s.s</last></author>
      <author><first>Elvin Quero</first><last>Hernandez</last></author>
      <pages>425-436</pages>
      <abstract>Large language models excel at legal text generation but often produce hallucinations due to their sole reliance on parametric knowledge. Retrieval-augmented models mitigate this by providing relevant external documents to the model but struggle when retrieval is based only on past context, which may not align with the model’s intended future content. We introduce LexKeyPlan, a novel framework that integrates anticipatory planning into generation. Instead of relying solely on context for retrieval, LexKeyPlan generates keyphrases outlining future content serving as forward-looking plan, guiding retrieval for more accurate text generation. This work incorporates planning into legal text generation, demonstrating how keyphrases—representing legal concepts—enhance factual accuracy. By structuring retrieval around legal concepts, LexKeyPlan better aligns with legal reasoning, making it particularly suited for legal applications. Using the ECHR corpus as case study, we show that LexKeyPlan improves factual accuracy and coherence by retrieving information aligned with the intended content.</abstract>
      <url hash="2cf6f3f9">2025.acl-short.32</url>
      <bibkey>t-y-s-s-hernandez-2025-lexkeyplan</bibkey>
    </paper>
    <paper id="33">
      <title><fixed-case>S</fixed-case>yn<fixed-case>W</fixed-case>orld: Virtual Scenario Synthesis for Agentic Action Knowledge Refinement</title>
      <author><first>Runnan</first><last>Fang</last></author>
      <author><first>Xiaobin</first><last>Wang</last></author>
      <author><first>Yuan</first><last>Liang</last></author>
      <author><first>Shuofei</first><last>Qiao</last></author>
      <author><first>Jialong</first><last>Wu</last><affiliation>Southeast University</affiliation></author>
      <author><first>Zekun</first><last>Xi</last></author>
      <author><first>Ningyu</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Yong</first><last>Jiang</last><affiliation>Tongyi Lab</affiliation></author>
      <author><first>Pengjun</first><last>Xie</last></author>
      <author><first>Fei</first><last>Huang</last><affiliation>Alibaba Group US</affiliation></author>
      <author><first>Huajun</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <pages>437-448</pages>
      <abstract>In the interaction between agents and their environments, agents expand their capabilities by planning and executing actions. However, LLM-based agents face substantial challenges when deployed in novel environments or required to navigate unconventional action spaces. To empower agents to autonomously explore environments, optimize workflows, and enhance their understanding of actions, we propose SynWorld, a framework that allows agents to synthesize possible scenarios with multi-step action invocation within the action space and perform Monte Carlo Tree Search (MCTS) exploration to effectively refine their action knowledge in the current environment. Our experiments demonstrate that SynWorld is an effective and general approach to learning action knowledge in new environments.</abstract>
      <url hash="8b0c4518">2025.acl-short.33</url>
      <bibkey>fang-etal-2025-synworld</bibkey>
    </paper>
    <paper id="34">
      <title>Enhancing Retrieval Systems with Inference-Time Logical Reasoning</title>
      <author><first>Felix</first><last>Faltings</last></author>
      <author><first>Wei</first><last>Wei</last><affiliation>Google</affiliation></author>
      <author><first>Yujia</first><last>Bao</last><affiliation>Accenture</affiliation></author>
      <pages>449-463</pages>
      <abstract>Traditional retrieval methods rely on transforming user queries into vector representations and retrieving documents based on cosine similarity within an embedding space. While efficient and scalable, this approach often fails to handle complex queries involving logical constructs such as negations, conjunctions, and disjunctions. In this paper, we propose a novel inference-time logical reasoning framework that explicitly incorporates logical reasoning into the retrieval process. Our method extracts logical reasoning structures from natural language queries and then composes the individual cosine similarity matching scores to formulate the final document scores. This approach enables the retrieval process to handle complex logical reasoning without compromising computational efficiency. Our results on both synthetic and real-world benchmarks demonstrate that the proposed method consistently outperforms traditional retrieval methods across different models and datasets, significantly improving retrieval performance for complex queries.</abstract>
      <url hash="58d0ce4c">2025.acl-short.34</url>
      <bibkey>faltings-etal-2025-enhancing</bibkey>
    </paper>
    <paper id="35">
      <title>Using Subtext to Enhance Generative <fixed-case>IDRR</fixed-case></title>
      <author><first>Zhipang</first><last>Wang</last></author>
      <author><first>Yu</first><last>Hong</last><affiliation>Suzhou University</affiliation></author>
      <author><first>Weihao</first><last>Sun</last><affiliation>Soochow University</affiliation></author>
      <author><first>Guodong</first><last>Zhou</last><affiliation>Soochow University, China</affiliation></author>
      <pages>464-473</pages>
      <abstract>Implicit Discourse Relation Recognition (abbr., IDRR) is a NLP task of classifying argument pairs into different types of semantic relations. Arguments contain subtexts, some of which are beneficial to the perception of semantic relations. However, subtexts are connotative. The neural IDRR model fails to be aware of them without being given pertinent prompts. In this paper, we leverage LLaMA to generate subtexts for argument pairs, and verify the effectiveness of subtext-based IDRR. We construct an IDRR baseline using the decoder-only backbone LLaMA, and enhance it with subtext-aware relation reasoning. A confidence-diagnosed dual-channel network is used for collaboration between in-subtext and out-of-subtext IDRR. We experiment on PDTB-2.0 and PDTB-3.0 for both the main-level and secondary-level relation taxonomies. The test results show that our approach yields substantial improvements compared to the baseline, and achieves higher <tex-math>F</tex-math>1-scores on both benchmarks than the previous decoder-only IDRR models. We make the source codes and data publicly available.</abstract>
      <url hash="8b3f2bfa">2025.acl-short.35</url>
      <bibkey>wang-etal-2025-using</bibkey>
    </paper>
    <paper id="36">
      <title>State-offset Tuning: State-based Parameter-Efficient Fine-Tuning for State Space Models</title>
      <author><first>Wonjun</first><last>Kang</last><affiliation>Seoul National University and Furiosa AI</affiliation></author>
      <author><first>Kevin</first><last>Galim</last><affiliation>Furiosa AI</affiliation></author>
      <author><first>Yuchen</first><last>Zeng</last><affiliation>University of Wisconsin, Madison</affiliation></author>
      <author><first>Minjae</first><last>Lee</last><affiliation>FuriosaAI</affiliation></author>
      <author><first>Hyung Il</first><last>Koo</last><affiliation>FuriosaAI and Ajou University</affiliation></author>
      <author><first>Nam Ik</first><last>Cho</last><affiliation>Seoul National University</affiliation></author>
      <pages>474-487</pages>
      <abstract>State Space Models (SSMs) have emerged as efficient alternatives to Transformers, mitigating their quadratic computational cost. However, the application of Parameter-Efficient Fine-Tuning (PEFT) methods to SSMs remains largely unexplored. In particular, prompt-based methods like Prompt Tuning and Prefix-Tuning, which are widely used in Transformers, do not perform well on SSMs. To address this, we propose **state-based methods** as a superior alternative to prompt-based methods. This new family of methods naturally stems from the architectural characteristics of SSMs. State-based methods adjust state-related features directly instead of depending on external prompts. Furthermore, we introduce a novel state-based PEFT method: **State-offset Tuning**. At every timestep, our method directly affects the state at the current step, leading to more effective adaptation. Through extensive experiments across diverse datasets, we demonstrate the effectiveness of our method. Code is available at https://github.com/furiosa-ai/ssm-state-tuning.</abstract>
      <url hash="d4db8caf">2025.acl-short.36</url>
      <bibkey>kang-etal-2025-state</bibkey>
    </paper>
    <paper id="37">
      <title>Internal and External Impacts of Natural Language Processing Papers</title>
      <author><first>Yu</first><last>Zhang</last><affiliation>Texas A&amp;M University - College Station</affiliation></author>
      <pages>488-494</pages>
      <abstract>We investigate the impacts of NLP research published in top-tier conferences (i.e., ACL, EMNLP, and NAACL) from 1979 to 2024. By analyzing citations from research articles and external sources such as patents, media, and policy documents, we examine how different NLP topics are consumed both within the academic community and by the broader public. Our findings reveal that language modeling has the widest internal and external influence, while linguistic foundations have lower impacts. We also observe that internal and external impacts generally align, but topics like ethics, bias, and fairness show significant attention in policy documents with much fewer academic citations. Additionally, external domains exhibit distinct preferences, with patents focusing on practical NLP applications and media and policy documents engaging more with the societal implications of NLP models.</abstract>
      <url hash="ab6cee4e">2025.acl-short.37</url>
      <bibkey>zhang-2025-internal</bibkey>
    </paper>
    <paper id="38">
      <title>An Effective Incorporating Heterogeneous Knowledge Curriculum Learning for Sequence Labeling</title>
      <author><first>Xuemei</first><last>Tang</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Jun</first><last>Wang</last></author>
      <author><first>Qi</first><last>Su</last><affiliation>Peking University</affiliation></author>
      <author><first>Chu-Ren</first><last>Huang</last></author>
      <author><first>Jinghang</first><last>Gu</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <pages>495-503</pages>
      <abstract>Sequence labeling models often benefit from incorporating external knowledge. However, this practice introduces data heterogeneity and complicates the model with additional modules, leading to increased expenses for training a high-performing model. To address this challenge, we propose a dual-stage curriculum learning (DCL) framework specifically designed for sequence labeling tasks. The DCL framework enhances training by gradually introducing data instances from easy to hard. Additionally, we introduce a dynamic metric for evaluating the difficulty levels of sequence labeling tasks. Experiments on several sequence labeling datasets show that our model enhances performance and accelerates training, mitigating the slow training issue of complex models.</abstract>
      <url hash="732c3e64">2025.acl-short.38</url>
      <bibkey>tang-etal-2025-effective</bibkey>
    </paper>
    <paper id="39">
      <title>Accelerating Dense <fixed-case>LLM</fixed-case>s via L0-regularized Mixture-of-Experts</title>
      <author><first>Zhenyu</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>JiuDong</first><last>Yang</last><affiliation>jojoreading</affiliation></author>
      <author><first>Taozhaowen</first><last>Taozhaowen</last></author>
      <author><first>Meng</first><last>Chen</last></author>
      <pages>504-513</pages>
      <abstract>Large language models (LLMs) achieve strong performance but suffer from slow and costly inference. Existing acceleration methods often lead to noticeable performance degradation, while Mixture-of-Experts (MoE) models require extensive computational resources. In this paper, we propose L0-MoE, a lightweight MoE approach using L0-regularization to accelerate dense LLMs nearly without performance loss. Our method introduces a cluster confusion matrix for domain-aware dataset curation and applies dynamic batching for efficient training. Experiments show that L0-MoE achieves up to 2.5x speedup over dense models while maintaining competitive performance, outperforming existing LLM acceleration baselines.</abstract>
      <url hash="bb41f6c2">2025.acl-short.39</url>
      <bibkey>zhang-etal-2025-accelerating</bibkey>
    </paper>
    <paper id="40">
      <title>Do Multimodal Large Language Models Truly See What We Point At? Investigating Indexical, Iconic, and Symbolic Gesture Comprehension</title>
      <author><first>Noriki</first><last>Nishida</last><affiliation>RIKEN</affiliation></author>
      <author><first>Koji</first><last>Inoue</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Hideki</first><last>Nakayama</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Mayumi</first><last>Bono</last><affiliation>NII, Tokyo Institute of Technology</affiliation></author>
      <author><first>Katsuya</first><last>Takanashi</last></author>
      <pages>514-524</pages>
      <abstract>Understanding hand gestures is essential for human communication, yet it remains unclear how well multimodal large language models (MLLMs) comprehend them. In this paper, we examine MLLMs’ ability to interpret indexical gestures, which require external referential grounding, in comparison to iconic gestures, which depict imagery, and symbolic gestures, which are conventionally defined. We hypothesize that MLLMs, lacking real-world referential understanding, will struggle significantly with indexical gestures. To test this, we manually annotated five gesture type labels to 925 gesture instances from the Miraikan SC Corpus and analyzed gesture descriptions generated by state-of-the-art MLLMs, including GPT-4o. Our findings reveal a consistent weakness across models in interpreting indexical gestures, suggesting that MLLMs rely heavily on linguistic priors or commonsense knowledge rather than grounding their interpretations in visual or contextual cues.</abstract>
      <url hash="10a3d7b2">2025.acl-short.40</url>
      <bibkey>nishida-etal-2025-multimodal</bibkey>
    </paper>
    <paper id="41">
      <title>Fast or Slow? Integrating Fast Intuition and Deliberate Thinking for Enhancing Visual Question Answering</title>
      <author><first>Songtao</first><last>Jiang</last></author>
      <author><first>Chenyi</first><last>Zhou</last></author>
      <author><first>Yan</first><last>Zhang</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Yeying</first><last>Jin</last><affiliation>Tencent</affiliation></author>
      <author><first>Zuozhu</first><last>Liu</last><affiliation>Zhejiang University</affiliation></author>
      <pages>525-534</pages>
      <abstract>Multimodal large language models (MLLMs) still struggle with complex reasoning tasks in Visual Question Answering (VQA). While current methods have advanced by incorporating visual prompts, our study uncovers critical limitations: these approaches indiscriminately annotate all detected objects for every visual question, generating excessive visual markers that degrade task performance. This issue stems primarily from a lack of focus on key visual elements, raising two important questions: Are all objects equally important, and do all questions require visual prompts? Motivated by Dual Process Theory, which distinguishes between instinctive and deliberate cognitive modes in human reasoning, we propose FOCUS, a plug-and-play approach that dynamically adapts to the complexity of questions, combining fast intuitive judgments with deliberate analytical reasoning to enhance the vision-language reasoning capability of the MLLM. For straightforward questions, FOCUS supports efficient zero-shot reasoning. For more complex tasks, it employs the conceptualizing before observation strategy to highlight critical elements. Extensive experiments on four benchmarks—ScienceQA, TextQA, VizWiz, and MME—demonstrate that FOCUS consistently improves the performance of both open-source and black-box MLLMs, achieving significant gains across all datasets. Ablation studies further validate the importance of combining diverse cognitive strategies with refined visual information for superior performance. Code will be released.</abstract>
      <url hash="d0b595d1">2025.acl-short.41</url>
      <bibkey>jiang-etal-2025-fast</bibkey>
    </paper>
    <paper id="42">
      <title>Can Community Notes Replace Professional Fact-Checkers?</title>
      <author><first>Nadav</first><last>Borenstein</last></author>
      <author><first>Greta</first><last>Warren</last><affiliation>University of Copenhagen</affiliation></author>
      <author><first>Desmond</first><last>Elliott</last><affiliation>Copenhagen University and University of Copenhagen</affiliation></author>
      <author><first>Isabelle</first><last>Augenstein</last><affiliation>University of Copenhagen</affiliation></author>
      <pages>535-552</pages>
      <abstract>Two commonly employed strategies to combat the rise of misinformation on social media are (i) fact-checking by professional organisations and (ii) community moderation by platform users. Policy changes by Twitter/X and, more recently, Meta, signal a shift away from partnerships with fact-checking organisations and towards an increased reliance on crowdsourced community notes. However, the extent and nature of dependencies between fact-checking and *helpful* community notes remain unclear. To address these questions, we use language models to annotate a large corpus of Twitter/X community notes with attributes such as topic, cited sources, and whether they refute claims tied to broader misinformation narratives. Our analysis reveals that community notes cite fact-checking sources up to five times more than previously reported. Fact-checking is especially crucial for notes on posts linked to broader narratives, which are *twice* as likely to reference fact-checking sources compared to other sources. Our results show that successful community moderation relies on professional fact-checking and highlight how citizen and professional fact-checking are deeply intertwined.</abstract>
      <url hash="35027f3f">2025.acl-short.42</url>
      <bibkey>borenstein-etal-2025-community</bibkey>
    </paper>
    <paper id="43">
      <title>Multilingual Gloss-free Sign Language Translation: Towards Building a Sign Language Foundation Model</title>
      <author><first>Sihan</first><last>Tan</last><affiliation>Tokyo Institute of Technology</affiliation></author>
      <author><first>Taro</first><last>Miyazaki</last><affiliation>NHK Science and Technology Research Laboratories</affiliation></author>
      <author><first>Kazuhiro</first><last>Nakadai</last><affiliation>Institute of Science Tokyo</affiliation></author>
      <pages>553-561</pages>
      <abstract>Sign Language Translation (SLT) aims to convert sign language (SL) videos into spoken language text, thereby bridging the communication gap between the sign and the spoken community. While most existing works focus on translating a single SL into a single spoken language (one-to-one SLT), leveraging multilingual resources could mitigate low-resource issues and enhance accessibility. However, multilingual SLT (MLSLT) remains unexplored due to language conflicts and alignment difficulties across SLs and spoken languages. To address these challenges, we propose a multilingual gloss-free model with dual CTC objectives for token-level SL identification and spoken text generation. Our model supports 10 SLs and handles one-to-one, many-to-one, and many-to-many SLT tasks, achieving competitive performance compared to state-of-the-art methods on three widely adopted benchmarks: multilingual SP-10, PHOENIX14T, and CSL-Daily.</abstract>
      <url hash="da221983">2025.acl-short.43</url>
      <bibkey>tan-etal-2025-multilingual</bibkey>
    </paper>
    <paper id="44">
      <title>Advancing Sequential Numerical Prediction in Autoregressive Models</title>
      <author><first>Xiang</first><last>Fei</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Jinghui</first><last>Lu</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Qi</first><last>Sun</last><affiliation>University of Science and Technology of China and City University of Hong Kong</affiliation></author>
      <author><first>Hao</first><last>Feng</last></author>
      <author><first>Yanjie</first><last>Wang</last><affiliation>ByteDance Inc</affiliation></author>
      <author><first>Wei</first><last>Shi</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>An-Lan</first><last>Wang</last></author>
      <author><first>Jingqun</first><last>Tang</last></author>
      <author><first>Can</first><last>Huang</last><affiliation>Bytedance</affiliation></author>
      <pages>562-574</pages>
      <abstract>Autoregressive models have become the de facto choice for sequence generation tasks, but standard approaches treat digits as independent tokens and apply cross-entropy loss, overlooking the coherent structure of numerical sequences. This paper introduces <tex-math>\textit{\textbf{N}umerical \textbf{T}oken \textbf{I}ntegrity \textbf{Loss} (NTIL)}</tex-math> to address this gap. NTIL operates at two levels: (1) token-level, where it extends the Earth Mover’s Distance (EMD) to preserve ordinal relationships between numerical values, and (2) sequence-level, where it penalizes the overall discrepancy between the predicted and actual sequences. This dual approach improves numerical prediction and integrates effectively with LLMs/MLLMs. Extensive experiments show significant performance improvements with NTIL.</abstract>
      <url hash="6f646c43">2025.acl-short.44</url>
      <bibkey>fei-etal-2025-advancing</bibkey>
    </paper>
    <paper id="45">
      <title><fixed-case>FEAT</fixed-case>: A Preference Feedback Dataset through a Cost-Effective Auto-Generation and Labeling Framework for <fixed-case>E</fixed-case>nglish <fixed-case>AI</fixed-case> Tutoring</title>
      <author><first>Hyein</first><last>Seo</last></author>
      <author><first>Taewook</first><last>Hwang</last></author>
      <author><first>Yohan</first><last>Lee</last><affiliation>Electronics and Telecommunications Research Institute</affiliation></author>
      <author><first>Sangkeun</first><last>Jung</last></author>
      <pages>575-589</pages>
      <abstract>In English education tutoring, teacher feedback is essential for guiding students. Recently, AI-based tutoring systems have emerged to assist teachers; however, these systems require high-quality and large-scale teacher feedback data, which is both time-consuming and costly to generate manually. In this study, we propose FEAT, a cost-effective framework for generating teacher feedback, and have constructed three complementary datasets: (1) DIRECT-Manual (DM), where both humans and large language models (LLMs) collaboratively generate high-quality teacher feedback, albeit at a higher cost; (2) DIRECT-Generated (DG), an LLM-only generated, cost-effective dataset with lower quality;, and (3) DIRECT-Augmented (DA), primarily based on DG with a small portion of DM added to enhance quality while maintaining cost-efficiency. Experimental results showed that incorporating a small portion of DM (5–10%) into DG leads to superior performance compared to using 100% DM alone.</abstract>
      <url hash="06622369">2025.acl-short.45</url>
      <bibkey>seo-etal-2025-feat</bibkey>
    </paper>
    <paper id="46">
      <title><fixed-case>C</fixed-case>hrono<fixed-case>S</fixed-case>ense: Exploring Temporal Understanding in Large Language Models with Time Intervals of Events</title>
      <author><first>Duygu Sezen</first><last>Islakoglu</last><affiliation>Utrecht University</affiliation></author>
      <author><first>Jan-Christoph</first><last>Kalo</last><affiliation>University of Amsterdam</affiliation></author>
      <pages>590-602</pages>
      <abstract>Large Language Models (LLMs) still face significant challenges in reasoning and arithmetic. Although temporal reasoning has raised increasing research attention, comprehensive testing of Allen’s interval relations (e.g., before, after, during) —a fundamental framework for temporal relationships— remains underexplored. To fill this gap, we present ChronoSense, a new benchmark for evaluating LLMs’ temporal understanding. It includes 16 tasks, identifying the Allen relation between two temporal events and temporal arithmetic. We assess the performance of seven recent LLMs. The results indicate that models handle Allen relations, even symmetrical ones, quite differently. Moreover, the findings suggest that the models may rely on memorization to answer time-related questions. Overall, the models’ low performance highlights the need for improved temporal understanding in LLMs. Our dataset and the source code are available at https://github.com/duyguislakoglu/chronosense.</abstract>
      <url hash="34092586">2025.acl-short.46</url>
      <bibkey>islakoglu-kalo-2025-chronosense</bibkey>
    </paper>
    <paper id="47">
      <title>Human Alignment: How Much Do We Adapt to <fixed-case>LLM</fixed-case>s?</title>
      <author><first>Cazalets</first><last>Tanguy</last></author>
      <author><first>Ruben</first><last>Janssens</last><affiliation>Universiteit Gent - imec</affiliation></author>
      <author><first>Tony</first><last>Belpaeme</last><affiliation>Universiteit Gent</affiliation></author>
      <author><first>Joni</first><last>Dambre</last><affiliation>Universiteit Gent</affiliation></author>
      <pages>603-613</pages>
      <abstract>Large Language Models (LLMs) are becoming a common part of our lives, yet few studies have examined how they influence our behavior. Using a cooperative language game in which players aim to converge on a shared word, we investigate how people adapt their communication strategies when paired with either an LLM or another human. Our study demonstrates that LLMs exert a measurable influence on human communication strategies and that humans notice and adapt to these differences irrespective of whether they are aware they are interacting with an LLM. These findings highlight the reciprocal influence of human–AI dialogue and raise important questions about the long-term implications of embedding LLMs in everyday communication.</abstract>
      <url hash="e785e9f8">2025.acl-short.47</url>
      <bibkey>tanguy-etal-2025-human</bibkey>
    </paper>
    <paper id="48">
      <title>Dynamic Order Template Prediction for Generative Aspect-Based Sentiment Analysis</title>
      <author><first>Yonghyun</first><last>Jun</last></author>
      <author><first>Hwanhee</first><last>Lee</last><affiliation>Chung-Ang University</affiliation></author>
      <pages>614-626</pages>
      <abstract>Aspect-based sentiment analysis (ABSA) assesses sentiments towards specific aspects within texts, resulting in detailed sentiment tuples.Previous ABSA models often used static templates to predict all the elements in the tuples, and these models often failed to accurately capture dependencies between elements. Multi-view prompting method improves the performance of ABSA by predicting tuples with various templates and then assembling the results. However, this method suffers from inefficiencies and out-of-distribution errors. In this paper, we propose a Dynamic Order Template (DOT) method for ABSA, which dynamically creates an order template that contains only the necessary views for each instance. Ensuring the diverse and relevant view generation, our proposed method improves F1 scores on ASQP and ACOS datasets while significantly reducing inference time.</abstract>
      <url hash="0b7f0961">2025.acl-short.48</url>
      <bibkey>jun-lee-2025-dynamic</bibkey>
    </paper>
    <paper id="49">
      <title>That doesn’t sound right: Evaluating speech transcription quality in field linguistics corpora</title>
      <author><first>Eric</first><last>Le Ferrand</last><affiliation>Boston College</affiliation></author>
      <author><first>Bo</first><last>Jiang</last></author>
      <author><first>Joshua</first><last>Hartshorne</last><affiliation>MGH Institute of Health Professions</affiliation></author>
      <author><first>Emily</first><last>Prud’hommeaux</last><affiliation>Boston College</affiliation></author>
      <pages>627-635</pages>
      <abstract>Incorporating automatic speech recognition (ASR) into field linguistics workflows for language documentation has become increasingly common. While ASR performance has seen improvements in low-resource settings, obstacles remain when training models on data collected by documentary linguists. One notable challenge lies in the way that this data is curated. ASR datasets built from spontaneous speech are typically recorded in consistent settings and transcribed by native speakers following a set of well designed guidelines. In contrast, field linguists collect data in whatever format it is delivered by their language consultants and transcribe it as best they can given their language skills and the quality of the recording. This approach to data curation, while valuable for linguistic research, does not always align with the standards required for training robust ASR models. In this paper, we explore methods for identifying speech transcriptions in fieldwork data that may be unsuitable for training ASR models. We focus on two complimentary automated measures of transcription quality that can be used to identify transcripts with characteristics that are common in field data but could be detrimental to ASR training. We show that one of the metrics is highly effective at retrieving these types of transcriptions. Additionally, we find that filtering datasets using this metric of transcription quality reduces WER both in controlled experiments using simulated fieldwork with artificially corrupted data and in real fieldwork corpora.</abstract>
      <url hash="159e3883">2025.acl-short.49</url>
      <bibkey>le-ferrand-etal-2025-doesnt</bibkey>
    </paper>
    <paper id="50">
      <title>Is That Your Final Answer? Test-Time Scaling Improves Selective Question Answering</title>
      <author><first>William</first><last>Jurayj</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Jeffrey</first><last>Cheng</last></author>
      <author><first>Benjamin</first><last>Van Durme</last><affiliation>Microsoft and Johns Hopkins University</affiliation></author>
      <pages>636-644</pages>
      <abstract>Scaling the test-time compute of large language models has demonstrated impressive performance on reasoning benchmarks. However, existing evaluations of test-time scaling make the strong assumption that a reasoning system should always give an answer to any question provided. This overlooks concerns about whether a model is confident in its answer, and whether it is appropriate to always provide a response. To address these concerns, we extract confidence scores during reasoning for thresholding model responses. We find that increasing compute budget at inference time not only helps models answer more questions correctly, but also increases confidence in correct responses. We then extend the current paradigm of zero-risk responses during evaluation by considering settings with non-zero levels of response risk, and suggest a recipe for reporting evaluations under these settings.</abstract>
      <url hash="ddd973fd">2025.acl-short.50</url>
      <bibkey>jurayj-etal-2025-final</bibkey>
    </paper>
    <paper id="51">
      <title>Acoustic Individual Identification of White-Faced Capuchin Monkeys Using Joint Multi-Species Embeddings</title>
      <author><first>Álvaro</first><last>Vega-Hidalgo</last></author>
      <author><first>Artem</first><last>Abzaliev</last></author>
      <author><first>Thore</first><last>Bergman</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>Rada</first><last>Mihalcea</last><affiliation>University of Michigan</affiliation></author>
      <pages>645-659</pages>
      <abstract>Acoustic individual identification of wild animals is an essential task for understanding animal vocalizations within their social contexts, and for facilitating conservation and wildlife monitoring efforts. However, most of the work in this space relies on human efforts, as the development of methods for automatic individual identification is hindered by the lack of data. In this paper, we explore cross-species pre-training to address the task of individual classification in white-faced capuchin monkeys. Using acoustic embeddings from birds and humans, we find that they can be effectively used to identify the calls from individual monkeys. Moreover, we find that joint multi-species representations can lead to further improvements over the use of one representation at a time. Our work demonstrates the potential of cross-species data transfer and multi-species representations, as strategies to address tasks on species with very limited data.</abstract>
      <url hash="defda2d8">2025.acl-short.51</url>
      <bibkey>vega-hidalgo-etal-2025-acoustic</bibkey>
    </paper>
    <paper id="52">
      <title><fixed-case>SELF</fixed-case>-<fixed-case>PERCEPT</fixed-case>: Introspection Improves Large Language Models’ Detection of Multi-Person Mental Manipulation in Conversations</title>
      <author><first>Danush</first><last>Khanna</last></author>
      <author><first>Pratinav</first><last>Seth</last><affiliation>AryaXAI.com and Arya.ai</affiliation></author>
      <author><first>Sidhaarth Sredharan</first><last>Murali</last></author>
      <author><first>Aditya Kumar</first><last>Guru</last></author>
      <author><first>Siddharth</first><last>Shukla</last></author>
      <author><first>Tanuj</first><last>Tyagi</last></author>
      <author><first>Sandeep</first><last>Chaurasia</last><affiliation>Manipal University Jaipur</affiliation></author>
      <author><first>Kripabandhu</first><last>Ghosh</last><affiliation>Indian Institute of Science Education and Research Kolkata</affiliation></author>
      <pages>660-675</pages>
      <abstract>Mental manipulation is a subtle yet pervasive form of abuse in interpersonal communication, making its detection critical for safeguarding potential victims. However, due to manipulation’s nuanced and context-specific nature, identifying manipulative language in complex, multi-turn, and multi-person conversations remains a significant challenge for large language models (LLMs). To address this gap, we introduce the MultiManip dataset, comprising 220 multi-turn, multi-person dialogues balanced between manipulative and non-manipulative interactions, all drawn from reality shows that mimic real-world scenarios. For manipulative interactions, it includes 11 distinct manipulations depicting real-life scenarios. We conduct extensive evaluations of state-of-the-art LLMs, such as GPT-4o and Llama-3.1-8B, employing various prompting strategies. Despite their capabilities, these models often struggle to detect manipulation effectively. To overcome this limitation, we propose SELF-PERCEPT, a novel, two-stage prompting framework inspired by Self-Perception Theory, demonstrating strong performance in detecting multi-person, multi-turn mental manipulation. Our code and data are publicly available at https://github.com/danushkhanna/self-percept .</abstract>
      <url hash="7d829470">2025.acl-short.52</url>
      <bibkey>khanna-etal-2025-self</bibkey>
    </paper>
    <paper id="53">
      <title>A Variational Approach for Mitigating Entity Bias in Relation Extraction</title>
      <author><first>Samuel</first><last>Mensah</last><affiliation>J.P. Morgan Chase</affiliation></author>
      <author><first>Elena</first><last>Kochkina</last><affiliation>J.P. Morgan Chase</affiliation></author>
      <author><first>Jabez</first><last>Magomere</last></author>
      <author><first>Joy Prakash</first><last>Sain</last><affiliation>J.P. Morgan Chase</affiliation></author>
      <author><first>Simerjot</first><last>Kaur</last><affiliation>JPMorgan Chase and Co</affiliation></author>
      <author><first>Charese</first><last>Smiley</last><affiliation>J.P. Morgan Chase</affiliation></author>
      <pages>676-684</pages>
      <abstract>Mitigating entity bias is a critical challenge in Relation Extraction (RE), where models often rely excessively on entities, resulting in poor generalization. This paper presents a novel approach to address this issue by adapting a Variational Information Bottleneck (VIB) framework. Our method compresses entity-specific information while preserving task-relevant features. It achieves state-of-the-art performance on both general and financial domain RE datasets, excelling in in-domain settings (original test sets) and out-of-domain (modified test sets with type-constrained entity replacements). Our approach offers a robust, interpretable, and theoretically grounded methodology.</abstract>
      <url hash="22a641b9">2025.acl-short.53</url>
      <bibkey>mensah-etal-2025-variational</bibkey>
    </paper>
    <paper id="54">
      <title><fixed-case>G</fixed-case>en<fixed-case>K</fixed-case>now<fixed-case>S</fixed-case>ub: Improving Modularity and Reusability of <fixed-case>LLM</fixed-case>s through General Knowledge Subtraction</title>
      <author><first>Mohammadtaha</first><last>Bagherifard</last><affiliation>University of Tehran, University of Tehran</affiliation></author>
      <author><first>Sahar</first><last>Rajabi</last><affiliation>University of Waterloo and University of Tehran, University of Tehran</affiliation></author>
      <author><first>Ali</first><last>Edalat</last><affiliation>University of Tehran, University of Tehran</affiliation></author>
      <author><first>Yadollah</first><last>Yaghoobzadeh</last></author>
      <pages>685-694</pages>
      <abstract>Large language models (LLMs) often struggle with zero-shot generalization, and several modular approaches have been proposed to address this challenge. Yet, we hypothesize that a key limitation remains: the entanglement of general knowledge and task-specific adaptations. To overcome this, we propose a modular framework that disentangles these components by constructing a library of task-specific LoRA modules alongside a general-domain LoRA. By subtracting this general knowledge component from each task-specific module, we obtain residual modules that focus more exclusively on task-relevant information. We call this approach general knowledge subtraction or GenKnowSub. Leveraging the refined task-specific modules and the Arrow routing algorithm, we dynamically select and combine modules for new inputs without additional training. Our studies on the Phi-3 model and standard Arrow as baselines reveal that using general knowledge LoRAs derived from diverse languages, including English, French, and German, yields consistent performance gains in both monolingual and cross-lingual settings across a wide set of benchmarks. Further experiments on Phi-2 reveal how GenKnowSub generalizes to a weaker LLM.</abstract>
      <url hash="90e35547">2025.acl-short.54</url>
      <bibkey>bagherifard-etal-2025-genknowsub</bibkey>
    </paper>
    <paper id="55">
      <title>The Role of Abstract Representations and Observed Preferences in the Ordering of Binomials in Large Language Models</title>
      <author><first>Zachary Nicholas</first><last>Houghton</last></author>
      <author><first>Kenji</first><last>Sagae</last><affiliation>University of California, Davis</affiliation></author>
      <author><first>Emily</first><last>Morgan</last><affiliation>University of California, Davis</affiliation></author>
      <pages>695-702</pages>
      <abstract>To what extent do large language models learn abstract representations as opposed to more superficial aspects of their very large training corpora? We examine this question in the context of binomial ordering preferences involving two conjoined nouns in English. When choosing a binomial ordering (radio and television vs television and radio), humans rely on more than simply the observed frequency of each option. Humans also rely on abstract ordering preferences (e.g., preferences for short words before long words). We investigate whether large language models simply rely on the observed preference in their training data, or whether they are capable of learning the abstract ordering preferences (i.e., abstract representations) that humans rely on. Our results suggest that both smaller and larger models’ ordering preferences are driven exclusively by their experience with that item in the training data. Our study provides further insights into differences between how large language models represent and use language and how humans do it, particularly with respect to the use of abstract representations versus observed preferences.</abstract>
      <url hash="24e7a880">2025.acl-short.55</url>
      <bibkey>houghton-etal-2025-role</bibkey>
    </paper>
    <paper id="56">
      <title>Can <fixed-case>LLM</fixed-case>s Understand Unvoiced Speech? Exploring <fixed-case>EMG</fixed-case>-to-Text Conversion with <fixed-case>LLM</fixed-case>s</title>
      <author><first>Payal</first><last>Mohapatra</last></author>
      <author><first>Akash</first><last>Pandey</last></author>
      <author><first>Xiaoyuan</first><last>Zhang</last></author>
      <author><first>Qi</first><last>Zhu</last><affiliation>Northwestern University</affiliation></author>
      <pages>703-712</pages>
      <abstract>Unvoiced electromyography (EMG) is an effective communication tool for individuals unable to produce vocal speech. However, most prior methods rely on paired voiced and unvoiced EMG signals, along with speech data, for unvoiced EMG-to-text conversion, which is not practical for these individuals. Given the rise of large language models (LLMs) in speech recognition, we explore their potential to understand unvoiced speech. To this end, we address the challenge of <i>learning from unvoiced EMG alone</i> and propose a novel EMG adaptor module that maps EMG features to an LLM’s input space, achieving an average word error rate of 0.49 on a closed-vocabulary unvoiced EMG-to-text task. Even with a conservative data availability of just six minutes, our approach improves performance over specialized models by nearly 20%. While LLMs have been shown to be extendable to new language modalities—such as audio—understanding articulatory biosignals, like unvoiced EMG, is more challenging. This work takes a crucial first step toward enabling LLMs to comprehend unvoiced speech using surface EMG.</abstract>
      <url hash="5d7fb331">2025.acl-short.56</url>
      <bibkey>mohapatra-etal-2025-llms</bibkey>
    </paper>
    <paper id="57">
      <title>Decoder-Only <fixed-case>LLM</fixed-case>s can be Masked Auto-Encoders</title>
      <author><first>Dan</first><last>Qiao</last></author>
      <author><first>Yuan</first><last>Gao</last></author>
      <author><first>Zheming</first><last>Yang</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Di</first><last>Yang</last></author>
      <author><first>Ziheng</first><last>Wu</last></author>
      <author><first>Pengcheng</first><last>Lu</last></author>
      <author><first>Minghui</first><last>Qiu</last></author>
      <author><first>Juntao</first><last>Li</last></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>713-723</pages>
      <abstract>Modern NLP workflows (e.g., RAG systems) require different models for generation and embedding tasks, where bidirectional pre-trained encoders and decoder-only Large Language Models (LLMs) dominate respective tasks. Structural differences between models result in extra development costs and limit knowledge sharing between tasks. In this work, we present UniMAE, a novel unsupervised training method that transforms an Decoder-Only LLM into a <b>Uni</b>-Directional <b>M</b>asked <b>A</b>uto-<b>E</b>ncoder. UniMAE compresses high-quality semantic information into the [EOS] embedding while preserving the generation capabilities of LLMs. Comprehensive evaluations across 56 MTEB datasets demonstrate that UniMAE can achieve state-of-the-art results under unsupervised settings with merely 100 training steps, establishing the first effective approach to unifying generation and representation learning in decoder-only architectures.</abstract>
      <url hash="c299aeef">2025.acl-short.57</url>
      <bibkey>qiao-etal-2025-decoder</bibkey>
    </paper>
    <paper id="58">
      <title>Mitigating Posterior Salience Attenuation in Long-Context <fixed-case>LLM</fixed-case>s with Positional Contrastive Decoding</title>
      <author><first>Zikai</first><last>Xiao</last></author>
      <author><first>Ziyang</first><last>Wang</last><affiliation>University of Science and Technology of China and University of Science and Technology of China</affiliation></author>
      <author><first>Wen</first><last>Ma</last></author>
      <author><first>Yan</first><last>Zhang</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Wei</first><last>Shen</last></author>
      <author><first>WangYan</first><last>WangYan</last></author>
      <author><first>Luqi</first><last>Gong</last></author>
      <author><first>Zuozhu</first><last>Liu</last><affiliation>Zhejiang University</affiliation></author>
      <pages>724-733</pages>
      <abstract>While Large Language Models (LLMs) support long contexts, they struggle with performance degradation within the context window. Current solutions incur prohibitive training costs, leaving statistical behaviors and cost-effective approaches underexplored. From the decoding perspective, we identify the Posterior Salience Attenuation (PSA) phenomenon, where the salience ratio correlates with long-text performance degradation. Notably, despite the attenuation, gold tokens still occupy high-ranking positions in the decoding space. Motivated by it, we propose the training-free Positional Contrastive Decoding (PCD) that contrasts the logits derived from long-aware attention with those from designed local-aware attention, enabling the model to focus on the gains introduced by large-scale short-to-long training. Through the analysis of long-term decay simulation, we demonstrate that PCD effectively alleviates attention score degradation. Experimental results show that PCD achieves state-of-the-art performance on long-context benchmarks.</abstract>
      <url hash="eec43902">2025.acl-short.58</url>
      <bibkey>xiao-etal-2025-mitigating</bibkey>
    </paper>
    <paper id="59">
      <title>Sparse-to-Dense: A Free Lunch for Lossless Acceleration of Video Understanding in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Xuan</first><last>Zhang</last></author>
      <author><first>Cunxiao</first><last>Du</last><affiliation>Sea AI LAB</affiliation></author>
      <author><first>Sicheng</first><last>Yu</last></author>
      <author><first>Jiawei</first><last>Wu</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Fengzhuo</first><last>Zhang</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Wei</first><last>Gao</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Qian</first><last>Liu</last><affiliation>Tiktok</affiliation></author>
      <pages>734-742</pages>
      <abstract>Due to the auto-regressive nature of current video large language models (Video-LLMs), the inference latency increases as the input sequence length grows, posing challenges for the efficient processing of video sequences that are usually very long. We observe that during decoding, the attention scores of most tokens in Video-LLMs tend to be sparse and concentrated, with only certain tokens requiring comprehensive full attention. Based on this insight, we introduce Sparse-to-Dense (StD), a novel decoding strategy that integrates two distinct modules: one leveraging sparse top-K attention and the other employing dense full attention. These modules collaborate to accelerate Video-LLMs without loss. The fast (sparse) model speculatively decodes multiple tokens, while the slow (dense) model verifies them in parallel. StD is a tuning-free, plug-and-play solution that achieves up to a 1.94 walltime speedup in video processing. It maintains model performance while enabling a seamless transition from a standard Video-LLM to a sparse Video-LLM with minimal code modifications.</abstract>
      <url hash="15418d2a">2025.acl-short.59</url>
      <bibkey>zhang-etal-2025-sparse</bibkey>
    </paper>
    <paper id="60">
      <title>Revisiting Uncertainty Quantification Evaluation in Language Models: Spurious Interactions with Response Length Bias Results</title>
      <author><first>Andrea</first><last>Santilli</last><affiliation>Nous Research</affiliation></author>
      <author><first>Adam</first><last>Golinski</last><affiliation>Apple</affiliation></author>
      <author><first>Michael</first><last>Kirchhof</last><affiliation>Apple</affiliation></author>
      <author><first>Federico</first><last>Danieli</last><affiliation>Apple</affiliation></author>
      <author><first>Arno</first><last>Blaas</last><affiliation>Apple</affiliation></author>
      <author><first>Miao</first><last>Xiong</last></author>
      <author><first>Luca</first><last>Zappella</last><affiliation>Apple</affiliation></author>
      <author><first>Sinead</first><last>Williamson</last><affiliation>Apple</affiliation></author>
      <pages>743-759</pages>
      <abstract>Uncertainty Quantification (UQ) in Language Models (LMs) is key to improving their safety and reliability. Evaluations often use metrics like AUROC to assess how well UQ methods (e.g., negative sequence probabilities) correlate with task correctness functions (e.g., ROUGE-L). We show that mutual biases-when both UQ methods and correctness functions are biased by the same factors-systematically distort evaluation. First, we formally prove that any mutual bias non-randomly skews AUROC rankings, compromising benchmark integrity. Second, we confirm this happens empirically by testing 7 widely used correctness functions, from lexical-based and embedding-based metrics to LM-as-a-judge approaches, across 4 datasets × 4 models × 8 UQ methods. Our analysis showsthat length biases in correctness functions distort UQ assessments by interacting with length biases in UQ methods. We identify LM-as-a-judge methods as the least length-biased, offering a promising path for a fairer UQ evaluation.</abstract>
      <url hash="7d5c26c5">2025.acl-short.60</url>
      <bibkey>santilli-etal-2025-revisiting</bibkey>
    </paper>
    <paper id="61">
      <title>Memorization Inheritance in Sequence-Level Knowledge Distillation for Neural Machine Translation</title>
      <author><first>Verna</first><last>Dankers</last></author>
      <author><first>Vikas</first><last>Raunak</last><affiliation>Google DeepMind</affiliation></author>
      <pages>760-774</pages>
      <abstract>In this work, we explore how instance-level memorization in the teacher Neural Machine Translation (NMT) model gets inherited by the student model in sequence-level knowledge distillation (SeqKD). We find that despite not directly seeing the original training data, students memorize more than baseline models (models of the same size, trained on the original data)—3.4% for exact matches and 57% for extractive memorization—and show increased hallucination rates. Further, under this SeqKD setting, we also characterize how students behave on specific training data subgroups, such as subgroups with low quality or specific counterfactual memorization (CM) scores, and find that students exhibit greater denoising on low-quality subgroups. Finally, we propose a modification to SeqKD named Adaptive-SeqKD, which intervenes in SeqKD to reduce memorization and hallucinations. Overall, we recommend caution when applying SeqKD: students inherit both their teachers’ superior performance and their fault modes, thereby requiring active monitoring.</abstract>
      <url hash="7e83bd67">2025.acl-short.61</url>
      <bibkey>dankers-raunak-2025-memorization</bibkey>
    </paper>
    <paper id="62">
      <title><fixed-case>C</fixed-case>o<fixed-case>R</fixed-case>et: Improved Retriever for Code Editing</title>
      <author><first>Fabio James</first><last>Fehr</last></author>
      <author><first>Prabhu</first><last>Teja S</last><affiliation>Amazon, Idiap Research Institute and Swiss Federal Institute of Technology Lausanne</affiliation></author>
      <author><first>Luca</first><last>Franceschi</last><affiliation>Amazon Development Center Germany</affiliation></author>
      <author><first>Giovanni</first><last>Zappella</last></author>
      <pages>775-789</pages>
      <abstract>In this paper, we introduce CoRet, a dense retrieval model designed for code-editing tasks that integrates code semantics, repository structure, and call-graph dependencies. The model focuses on retrieving relevant portions of a code repository based on natural language queries such as requests to implement new features or fix bugs. These retrieved code chunks can then be presented to an user or to a second code-editing model or agent. To train CoRet, we propose a loss function explicitly designed for repository-level retrieval. On SWE-bench and Long Code Arena’s bug localisation datasets, we show that our model substantially improves retrieval recall by at least 15 percentage points over existing models, and ablate the design choices to show their importance in achieving these results.</abstract>
      <url hash="d1661036">2025.acl-short.62</url>
      <bibkey>fehr-etal-2025-coret</bibkey>
    </paper>
    <paper id="63">
      <title>Has Machine Translation Evaluation Achieved Human Parity? The Human Reference and the Limits of Progress</title>
      <author><first>Lorenzo</first><last>Proietti</last></author>
      <author><first>Stefano</first><last>Perrella</last></author>
      <author><first>Roberto</first><last>Navigli</last><affiliation>Sapienza University of Rome</affiliation></author>
      <pages>790-813</pages>
      <abstract>In Machine Translation (MT) evaluation, metric performance is assessed based on agreement with human judgments. In recent years, automatic metrics have demonstrated increasingly high levels of agreement with humans. To gain a clearer understanding of metric performance and establish an upper bound, we incorporate human baselines in the MT meta-evaluation, that is, the assessment of MT metrics’ capabilities. Our results show that human annotators are not consistently superior to automatic metrics, with state-of-the-art metrics often ranking on par with or higher than human baselines. Despite these findings suggesting human parity, we discuss several reasons for caution. Finally, we explore the broader implications of our results for the research field, asking: Can we still reliably measure improvements in MT evaluation? With this work, we aim to shed light on the limits of our ability to measure progress in the field, fostering discussion on an issue that we believe is crucial to the entire MT evaluation community.</abstract>
      <url hash="f7bac04f">2025.acl-short.63</url>
      <bibkey>proietti-etal-2025-machine</bibkey>
    </paper>
    <paper id="64">
      <title>Diffusion Directed Acyclic Transformer for Non-Autoregressive Machine Translation</title>
      <author><first>Quan</first><last>Nguyen-Tri</last><affiliation>FPT software AI center</affiliation></author>
      <author><first>Cong Dao</first><last>Tran</last></author>
      <author><first>Hoang</first><last>Thanh-Tung</last><affiliation>VNU University of Engineering and Technology</affiliation></author>
      <pages>814-828</pages>
      <abstract>Non-autoregressive transformers (NATs) predict entire sequences in parallel to reduce decoding latency, but they often encounter performance challenges due to the multi-modality problem. A recent advancement, the Directed Acyclic Transformer (DAT), addresses this issue by capturing multiple translation modalities to paths in a Directed Acyclic Graph (DAG). However, the collaboration with the latent variable introduced through the Glancing training (GLAT) is crucial for DAT to attain state-of-the-art performance. In this paper, we introduce Diffusion Directed Acyclic Transformer (Diff-DAT), which serves as an alternative to GLAT as a latent variable introduction for DAT. Diff-DAT offers two significant benefits over the previous approach. Firstly, it establishes a stronger alignment between training and inference. Secondly, it facilitates a more flexible tradeoff between quality and latency.</abstract>
      <url hash="33e266b2">2025.acl-short.64</url>
      <bibkey>nguyen-tri-etal-2025-diffusion</bibkey>
    </paper>
    <paper id="65">
      <title>Efficient Knowledge Editing via Minimal Precomputation</title>
      <author><first>Akshat</first><last>Gupta</last><affiliation>University of California, Berkeley</affiliation></author>
      <author><first>Maochuan</first><last>Lu</last></author>
      <author><first>Thomas</first><last>Hartvigsen</last><affiliation>University of Virginia, Charlottesville</affiliation></author>
      <author><first>Gopala</first><last>Anumanchipalli</last><affiliation>University of California, Berkeley</affiliation></author>
      <pages>829-840</pages>
      <abstract>Knowledge editing methods like MEMIT are able to make data and compute efficient updates of factual knowledge by using a single sentence to update facts and their consequences. However, what is often overlooked is a “precomputation step”, which requires a one-time but significant computational cost. The authors of MEMIT (CITATION) originally precompute approximately 44 million hidden vectors per edited layer, which requires a forward pass over 44 million tokens. For GPT-J (6B), this precomputation step takes 36 hours on a single GPU, while it takes approximately 40 hours for Llama2-7B. Additionally, this precomputation time grows with model size. In this paper, we show that this excessive computational cost is unnecessary. Knowledge editing using MEMIT and related methods, such as ROME and EMMET, can be performed by pre-computing a very small portion of the 44 million hidden vectors. We first present the theoretical minimum number of hidden vector precomputation required for solutions of these editing methods to exist. We then empirically show that knowledge editing using these methods can be done by pre-computing significantly fewer hidden vectors. Specifically, we show that the precomputation step can be done with less than 0.3% of the originally stipulated number of hidden vectors. This saves a significant amount of precomputation time and allows users to begin editing new models within a few minutes.</abstract>
      <url hash="585bc2dd">2025.acl-short.65</url>
      <bibkey>gupta-etal-2025-efficient</bibkey>
    </paper>
    <paper id="66">
      <title>Meaning Variation and Data Quality in the Corpus of Founding Era <fixed-case>A</fixed-case>merican <fixed-case>E</fixed-case>nglish</title>
      <author><first>Dallas</first><last>Card</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <pages>841-856</pages>
      <abstract>Legal scholars are increasingly using corpus based methods for assessing historical meaning. Among work focused on the so-called founding era (mid to late 18th century), the majority of such studies use the Corpus of Founding Era American English (COFEA) and rely on methods such as word counting and manual coding. Here, we demonstrate what can be inferred about meaning change and variation using more advanced NLP methods, focusing on terms in the U.S. Constitution. We also carry out a data quality assessment of COFEA, pointing out issues with OCR quality and metadata, compare diachronic change to synchronic variation, and discuss limitations when using NLP methods for studying historical meaning.</abstract>
      <url hash="0fd148c2">2025.acl-short.66</url>
      <bibkey>card-2025-meaning</bibkey>
    </paper>
    <paper id="67">
      <title><fixed-case>M</fixed-case>ind<fixed-case>R</fixed-case>ef: Mimicking Human Memory for Hierarchical Reference Retrieval with Fine-Grained Location Awareness</title>
      <author><first>Ye</first><last>Wang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Xinrun</first><last>Xu</last></author>
      <author><first>Zhiming</first><last>Ding</last><affiliation>Institute of Software Chinese Academy of Sciences</affiliation></author>
      <pages>857-872</pages>
      <abstract>When completing knowledge-intensive tasks, humans sometimes need an answer and a corresponding reference passage for auxiliary reading. Previous methods required obtaining pre-segmented article chunks through additional retrieval models. This paper explores leveraging the parameterized knowledge stored during the pre-training phase of large language models (LLMs) to recall reference passage from any starting position independently. We propose a two-stage framework that simulates the scenario of humans recalling easily forgotten references. Initially, the LLM is prompted to recall document title identifiers to obtain a coarse-grained document set. Then, based on the acquired coarse-grained document set, it recalls fine-grained passage. In the two-stage recall process, we use constrained decoding to ensure that content outside of the stored documents is not generated. To increase speed, we only recall a short prefix in the second stage, and then locate its position to retrieve a complete passage. Experiments on KILT knowledge-sensitive tasks have verified that LLMs can independently recall reference passage locations in various task forms, and the obtained reference significantly assists downstream tasks.</abstract>
      <url hash="c5db6412">2025.acl-short.67</url>
      <bibkey>wang-etal-2025-mindref</bibkey>
    </paper>
    <paper id="68">
      <title><fixed-case>LLM</fixed-case>s syntactically adapt their language use to their conversational partner</title>
      <author><first>Florian</first><last>Kandra</last></author>
      <author><first>Vera</first><last>Demberg</last><affiliation>Universität des Saarlandes</affiliation></author>
      <author><first>Alexander</first><last>Koller</last><affiliation>Saarland University</affiliation></author>
      <pages>873-886</pages>
      <abstract>It has been frequently observed that human speakers align their language use with each other during conversations. In this paper, we study empirically whether large language models (LLMs) exhibit the same behavior of conversational adaptation.We construct a corpus of conversations between LLMs and find that two LLM agents end up making more similar syntactic choices as conversations go on, confirming that modern LLMs adapt their language use to their conversational partners in at least a rudimentary way.</abstract>
      <url hash="d1b17473">2025.acl-short.68</url>
      <bibkey>kandra-etal-2025-llms</bibkey>
    </paper>
    <paper id="69">
      <title><fixed-case>T</fixed-case>iger<fixed-case>LLM</fixed-case> - A Family of <fixed-case>B</fixed-case>angla Large Language Models</title>
      <author><first>Nishat</first><last>Raihan</last></author>
      <author><first>Marcos</first><last>Zampieri</last><affiliation>George Mason University</affiliation></author>
      <pages>887-896</pages>
      <abstract>The development of Large Language Models (LLMs) remains heavily skewed towards English and a few other high-resource languages. This linguistic disparity is particularly evident for Bangla - the 5th most spoken language. A few initiatives attempted to create open-source Bangla LLMs with performance still behind high-resource languages and limited reproducibility. To address this gap, we introduce TigerLLM - a family of Bangla LLMs. Our results demonstrate that these models surpass all open-source alternatives and also outperform larger proprietary models like GPT3.5 across standard benchmarks, establishing TigerLLM as the new baseline for future Bangla language modeling.</abstract>
      <url hash="a898234b">2025.acl-short.69</url>
      <bibkey>raihan-zampieri-2025-tigerllm</bibkey>
    </paper>
    <paper id="70">
      <title>From Citations to Criticality: Predicting Legal Decision Influence in the Multilingual <fixed-case>S</fixed-case>wiss Jurisprudence</title>
      <author><first>Ronja</first><last>Stern</last></author>
      <author><first>Ken</first><last>Kawamura</last></author>
      <author><first>Matthias</first><last>Stürmer</last><affiliation>BFH - Bern University of Applied Sciences and Universität Bern</affiliation></author>
      <author><first>Ilias</first><last>Chalkidis</last><affiliation>Copenhagen University</affiliation></author>
      <author><first>Joel</first><last>Niklaus</last><affiliation>Harvey</affiliation></author>
      <pages>897-905</pages>
      <abstract>Many court systems are overwhelmed all over the world, leading to huge backlogs of pending cases. Effective triage systems, like those in emergency rooms, could ensure proper prioritization of open cases, optimizing time and resource allocation in the court system. In this work, we introduce the Criticality Prediction dataset, a novel resource for evaluating case prioritization. Our dataset features a two-tier labeling system: (1) the binary LD-Label, identifying cases published as Leading Decisions (LD), and (2) the more granular Citation-Label, ranking cases by their citation frequency and recency, allowing for a more nuanced evaluation. Unlike existing approaches that rely on resource-intensive manual annotations, we algorithmically derive labels leading to a much larger dataset than otherwise possible. We evaluate several multilingual models, including both smaller fine-tuned models and large language models in a zero-shot setting. Our results show that the fine-tuned models consistently outperform their larger counterparts, thanks to our large training set. Our results highlight that for highly domain-specific tasks like ours, large training sets are still valuable.</abstract>
      <url hash="100a5130">2025.acl-short.70</url>
      <bibkey>stern-etal-2025-citations</bibkey>
    </paper>
    <paper id="71">
      <title>Revisiting <fixed-case>LLM</fixed-case>s as Zero-Shot Time Series Forecasters: Small Noise Can Break Large Models</title>
      <author><first>Junwoo</first><last>Park</last></author>
      <author><first>Hyuck</first><last>Lee</last></author>
      <author><first>Dohyun</first><last>Lee</last></author>
      <author><first>Daehoon</first><last>Gwak</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Jaegul</first><last>Choo</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>906-922</pages>
      <abstract>Large Language Models (LLMs) have shown remarkable performance across diverse tasks without domain-specific training, fueling interest in their potential for time-series forecasting. While LLMs have shown potential in zero-shot forecasting through prompting alone, recent studies suggest that LLMs lack inherent effectiveness in forecasting. Given these conflicting findings, a rigorous validation is essential for drawing reliable conclusions. In this paper, we evaluate the effectiveness of LLMs as zero-shot forecasters compared to state-of-the-art domain-specific models. Our experiments show that LLM-based zero-shot forecasters often struggle to achieve high accuracy due to their sensitivity to noise, underperforming even simple domain-specific models. We have explored solutions to reduce LLMs’ sensitivity to noise in the zero-shot setting, but improving their robustness remains a significant challenge. Our findings suggest that rather than emphasizing zero-shot forecasting, a more promising direction would be to focus on fine-tuning LLMs to better process numerical sequences. Our experimental code is available at https://github.com/junwoopark92/revisiting-LLMs-zeroshot-forecaster.</abstract>
      <url hash="a7048434">2025.acl-short.71</url>
      <bibkey>park-etal-2025-revisiting</bibkey>
    </paper>
    <paper id="72">
      <title>Transferring Textual Preferences to Vision-Language Understanding through Model Merging</title>
      <author><first>Chen-An</first><last>Li</last><affiliation>National Taiwan University</affiliation></author>
      <author><first>Tzu-Han</first><last>Lin</last></author>
      <author><first>Yun-Nung</first><last>Chen</last><affiliation>National Taiwan University</affiliation></author>
      <author><first>Hung-yi</first><last>Lee</last><affiliation>National Taiwan University</affiliation></author>
      <pages>923-943</pages>
      <abstract>Large vision-language models (LVLMs) perform outstandingly across various multimodal tasks. However, their ability to evaluate generated content remains limited, and training vision-language reward models (VLRMs) with preference data is computationally expensive. This paper explores a training-free alternative by merging text-based reward models (RMs) with LVLMs to create VLRMs. Our approach shows that integrating these models leads to improved performance over LVLMs’ scoring and text-based RMs, offering an efficient method for incorporating textual preferences into LVLMs.</abstract>
      <url hash="588d7275">2025.acl-short.72</url>
      <bibkey>li-etal-2025-transferring</bibkey>
    </paper>
    <paper id="73">
      <title><fixed-case>P</fixed-case>rog<fixed-case>C</fixed-case>o: Program Helps Self-Correction of Large Language Models</title>
      <author><first>Xiaoshuai</first><last>Song</last></author>
      <author><first>Yanan</first><last>Wu</last></author>
      <author><first>Weixun</first><last>Wang</last></author>
      <author><first>Jiaheng</first><last>Liu</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Wenbo</first><last>Su</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Bo</first><last>Zheng</last><affiliation>Alibaba Group</affiliation></author>
      <pages>944-959</pages>
      <abstract>Self-Correction aims to enable large language models (LLMs) to self-verify and self-refine their initial responses without external feedback. However, LLMs often fail to effectively self-verify and generate correct feedback, further misleading refinement and leading to the failure of self-correction, especially in complex reasoning tasks. In this paper, we propose Program-driven Self-Correction (ProgCo). First, program-driven verification (ProgVe) achieves complex verification logic and extensive validation through self-generated, self-executing verification pseudo-programs. Then,program-driven refinement (ProgRe) receives feedback from ProgVe, conducts dual reflection and refinement on both responses and verification programs to mitigate misleading of incorrect feedback in complex reasoning tasks. Experiments on three instruction-following and mathematical benchmarks indicate that ProgCo achieves effective self-correction, and can be further enhance performance when combined with real program tools. We release our code at https://github.com/songxiaoshuai/progco.</abstract>
      <url hash="51b44e51">2025.acl-short.73</url>
      <bibkey>song-etal-2025-progco</bibkey>
    </paper>
    <paper id="74">
      <title>Leveraging Self-Attention for Input-Dependent Soft Prompting in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Ananth</first><last>Muppidi</last></author>
      <author><first>Abhilash</first><last>Nandy</last><affiliation>Indian Institute of Technology Kharagpur</affiliation></author>
      <author><first>Sambaran</first><last>Bandyopadhyay</last><affiliation>Adobe Systems</affiliation></author>
      <pages>960-969</pages>
      <abstract>The performance of large language models in domain-specific tasks necessitates fine-tuning, which is computationally expensive and technically challenging. This paper focuses on parameter-efficient fine-tuning using soft prompting, a promising approach that adapts pre-trained models to downstream tasks by learning a small set of parameters. We propose a novel Input Dependent Soft Prompting technique with a self-Attention Mechanism (ID-SPAM) that generates soft prompts based on the input tokens and attends different tokens with varying importance. Our method is simple and efficient, keeping the number of trainable parameters small. We show the merits of the proposed approach compared to state-of-the-art techniques on various tasks and show the improved zero shot domain transfer capability.</abstract>
      <url hash="b52ba70a">2025.acl-short.74</url>
      <bibkey>muppidi-etal-2025-leveraging</bibkey>
    </paper>
    <paper id="75">
      <title>Inconsistent Tokenizations Cause Language Models to be Perplexed by <fixed-case>J</fixed-case>apanese Grammar</title>
      <author><first>Andrew</first><last>Gambardella</last><affiliation>The University of Tokyo, Tokyo University</affiliation></author>
      <author><first>Takeshi</first><last>Kojima</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Yusuke</first><last>Iwasawa</last><affiliation>The University of Tokyo, The University of Tokyo</affiliation></author>
      <author><first>Yutaka</first><last>Matsuo</last><affiliation>The University of Tokyo and The University of Tokyo</affiliation></author>
      <pages>970-976</pages>
      <abstract>Typical methods for evaluating the performance of language models evaluate their ability to answer questions accurately. These evaluation metrics are acceptable for determining the extent to which language models can understand and reason about text in a general sense, but fail to capture nuanced capabilities, such as the ability of language models to recognize and obey rare grammar points, particularly in languages other than English. We measure the perplexity of language models when confronted with the “first person psych predicate restriction” grammar point in Japanese. Weblab is the only tested open source model in the 7-10B parameter range which consistently assigns higher perplexity to ungrammatical psych predicate sentences than grammatical ones. We give evidence that Weblab’s uniformly bad tokenization is a possible root cause for its good performance, and show that Llama 3’s perplexity on grammatical psych predicate sentences can be reduced by orders of magnitude (28x difference) by restricting test sentences to those with uniformly well-behaved tokenizations. We show in further experiments on machine translation tasks that language models will use alternative grammar patterns in order to produce grammatical sentences when tokenization issues prevent the most natural sentence from being output.</abstract>
      <url hash="942772c6">2025.acl-short.75</url>
      <bibkey>gambardella-etal-2025-inconsistent</bibkey>
    </paper>
    <paper id="76">
      <title>Unique Hard Attention: A Tale of Two Sides</title>
      <author><first>Selim</first><last>Jerad</last></author>
      <author><first>Anej</first><last>Svete</last></author>
      <author><first>Jiaoda</first><last>Li</last></author>
      <author><first>Ryan</first><last>Cotterell</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <pages>977-996</pages>
      <abstract>Understanding the expressive power of transformers has recently attracted attention, as it offers insights into their abilities and limitations. Many studies analyze unique hard attention transformers, where attention selects a single position that maximizes the attention scores. When multiple positions achieve the maximum score, either the rightmost or the leftmost of those is chosen. In this paper, we highlight the importance of this seeming triviality. Recently, finite-precision transformers with both leftmost- and rightmost-hard attention were shown to be equivalent to Linear Temporal Logic (LTL). We show that this no longer holds with only leftmost-hard attention—in that case, they correspond to a <i>strictly weaker</i> fragment of LTL. Furthermore, we show that models with leftmost-hard attention are equivalent to <i>soft</i> attention, suggesting they may better approximate real-world transformers than right-attention models. These findings refine the landscape of transformer expressivity and underscore the role of attention directionality.</abstract>
      <url hash="05950298">2025.acl-short.76</url>
      <bibkey>jerad-etal-2025-unique</bibkey>
    </paper>
    <paper id="77">
      <title>Enhancing Input-Label Mapping in In-Context Learning with Contrastive Decoding</title>
      <author><first>Keqin</first><last>Peng</last></author>
      <author><first>Liang</first><last>Ding</last></author>
      <author><first>Yuanxin</first><last>Ouyang</last><affiliation>BEIHANG UNIVERSITY</affiliation></author>
      <author><first>Meng</first><last>Fang</last><affiliation>University of Liverpool and Eindhoven University of Technology</affiliation></author>
      <author><first>Yancheng</first><last>Yuan</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Dacheng</first><last>Tao</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>997-1004</pages>
      <abstract>Large language models (LLMs) excel at a range of tasks through in-context learning (ICL), where only a few task examples guide their predictions. However, prior research highlights that LLMs often overlook input-label mapping information in ICL, relying more on their pre-trained knowledge. To address this issue, we introduce In-Context Contrastive Decoding (ICCD), a novel method that emphasizes input-label mapping by contrasting the output distributions between positive and negative in-context examples. Experiments on 7 natural language understanding (NLU) tasks show that our ICCD method brings consistent and significant improvement (up to +1.8 improvement on average) upon 6 different scales of LLMs without requiring additional training. Our approach is versatile, enhancing performance with various demonstration selection methods, demonstrating its broad applicability and effectiveness. The code and scripts are released at https://github.com/Romainpkq/CD_ICL.</abstract>
      <url hash="51f97c75">2025.acl-short.77</url>
      <bibkey>peng-etal-2025-enhancing</bibkey>
    </paper>
    <paper id="78">
      <title>Different Speech Translation Models Encode and Translate Speaker Gender Differently</title>
      <author><first>Dennis</first><last>Fucci</last></author>
      <author><first>Marco</first><last>Gaido</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <author><first>Matteo</first><last>Negri</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <author><first>Luisa</first><last>Bentivogli</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <author><first>Andre</first><last>Martins</last><affiliation>Instituto Superior Técnico and Unbabel</affiliation></author>
      <author><first>Giuseppe</first><last>Attanasio</last><affiliation>Instituto de Telecomunicações</affiliation></author>
      <pages>1005-1019</pages>
      <abstract>Recent studies on interpreting the hidden states of speech models have shown their ability to capture speaker-specific features, including gender. Does this finding also hold for speech translation (ST) models? If so, what are the implications for the speaker’s gender assignment in translation? We address these questions from an interpretability perspective, using probing methods to assess gender encoding across diverse ST models. Results on three language directions (English <tex-math>\rightarrow</tex-math> French/Italian/Spanish) indicate that while traditional encoder-decoder models capture gender information, newer architectures—integrating a speech encoder with a machine translation system via adapters—do not. We also demonstrate that low gender encoding capabilities result in systems’ tendency toward a masculine default, a translation bias that is more pronounced in newer architectures.</abstract>
      <url hash="d7aba1b2">2025.acl-short.78</url>
      <bibkey>fucci-etal-2025-different</bibkey>
    </paper>
    <paper id="79">
      <title>Rethinking Semantic Parsing for Large Language Models: Enhancing <fixed-case>LLM</fixed-case> Performance with Semantic Hints</title>
      <author><first>Kaikai</first><last>An</last></author>
      <author><first>Shuzheng</first><last>Si</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Helan</first><last>Hu</last></author>
      <author><first>Haozhe</first><last>Zhao</last></author>
      <author><first>Yuchi</first><last>Wang</last></author>
      <author><first>Qingyan</first><last>Guo</last></author>
      <author><first>Baobao</first><last>Chang</last><affiliation>Peking University</affiliation></author>
      <pages>1020-1029</pages>
      <abstract>Semantic Parsing aims to capture the meaning of a sentence and convert it into a logical, structured form. Previous studies show that semantic parsing enhances the performance of smaller models (e.g., BERT) on downstream tasks. However, it remains unclear whether the improvements extend similarly to LLMs. In this paper, our empirical findings reveal that, unlike smaller models, directly adding semantic parsing results into LLMs reduces their performance. To overcome this, we propose SENSE, a novel prompting approach that embeds semantic hints within the prompt. Experiments show that SENSE consistently improves LLMs’ performance across various tasks, highlighting the potential of integrating semantic information to improve LLM capabilities.</abstract>
      <url hash="55078e0e">2025.acl-short.79</url>
      <bibkey>an-etal-2025-rethinking</bibkey>
    </paper>
    <paper id="80">
      <title>Quantifying Misattribution Unfairness in Authorship Attribution</title>
      <author><first>Pegah</first><last>Alipoormolabashi</last><affiliation>State University of New York at Stony Brook</affiliation></author>
      <author><first>Ajay</first><last>Patel</last></author>
      <author><first>Niranjan</first><last>Balasubramanian</last><affiliation>State University of New York, Stony Brook</affiliation></author>
      <pages>1030-1041</pages>
      <abstract>Authorship misattribution can have profound consequences in real life. In forensic settings simply being considered as one of the potential authors of an evidential piece of text or communication can result in undesirable scrutiny. This raises a fairness question: Is every author in the candidate pool at equal risk of misattribution? Standard evaluation measures for authorship attribution systems do not explicitly account for this notion of fairness. We introduce a simple measure, Misattribution Unfairness Index (<tex-math>MAUI_k</tex-math>), which is based on how often authors are ranked in the top <tex-math>k</tex-math> for texts they did not write. Using this measure we quantify the unfairness of five models on two different datasets. All models exhibit high levels of unfairness with increased risks for some authors. Furthermore, we find that this unfairness relates to how the models embed the authors as vectors in the latent search space. In particular, we observe that the risk of misattribution is higher for authors closer to the centroid (or center) of the embedded authors in the haystack. These results indicate the potential for harm and the need for communicating with and calibrating end users on misattribution risk when building and providing such models for downstream use.</abstract>
      <url hash="b9a3a3e3">2025.acl-short.80</url>
      <bibkey>alipoormolabashi-etal-2025-quantifying</bibkey>
    </paper>
    <paper id="81">
      <title>Zero-Shot Text-to-Speech for <fixed-case>V</fixed-case>ietnamese</title>
      <author><first>Thi</first><last>Vu</last></author>
      <author><first>Linh The</first><last>Nguyen</last><affiliation>VinAI Research, Vietnam</affiliation></author>
      <author><first>Dat Quoc</first><last>Nguyen</last><affiliation>Qualcomm AI Research</affiliation></author>
      <pages>1042-1049</pages>
      <abstract>This paper introduces PhoAudiobook, a newly curated dataset comprising 941 hours of high-quality audio for Vietnamese text-to-speech. Using PhoAudiobook, we conduct experiments on three leading zero-shot TTS models: VALL-E, VoiceCraft, and XTTS-V2. Our findings demonstrate that PhoAudiobook consistently enhances model performance across various metrics. Moreover, VALL-E and VoiceCraft exhibit superior performance in synthesizing short sentences, highlighting their robustness in handling diverse linguistic contexts. We publicly release PhoAudiobook to facilitate further research and development in Vietnamese text-to-speech.</abstract>
      <url hash="324d6f59">2025.acl-short.81</url>
      <bibkey>vu-etal-2025-zero</bibkey>
    </paper>
    <paper id="82">
      <title>Can <fixed-case>LLM</fixed-case>s Generate High-Quality Test Cases for Algorithm Problems? <fixed-case>T</fixed-case>est<fixed-case>C</fixed-case>ase-Eval: A Systematic Evaluation of Fault Coverage and Exposure</title>
      <author><first>Zheyuan</first><last>Yang</last></author>
      <author><first>Zexi</first><last>Kuang</last></author>
      <author><first>Xue</first><last>Xia</last></author>
      <author><first>Yilun</first><last>Zhao</last><affiliation>Yale University</affiliation></author>
      <pages>1050-1063</pages>
      <abstract>We introduce TestCase-Eval, a new benchmark for systematic evaluation of LLMs in test-case generation. TestCase-Eval includes 500 algorithm problems and 100,000 human-crafted solutions from the Codeforces platform. It focuses on two pivotal tasks: (1) Fault Coverage, which measures how well LLM-generated test sets probe diverse input scenarios and cover a wide range of potential failure modes. (2) Fault Exposure, which evaluates whether LLMs can craft a tailored test input that reveals a specific incorrect code implementation. We provide a comprehensive assessment of 19 state-of-the-art open-source and proprietary LLMs on TestCase-Eval, offering insights into their strengths and limitations in generating effective test cases for algorithm problems.</abstract>
      <url hash="e644d2ea">2025.acl-short.82</url>
      <bibkey>yang-etal-2025-llms</bibkey>
    </paper>
    <paper id="83">
      <title>Are Optimal Algorithms Still Optimal? Rethinking Sorting in <fixed-case>LLM</fixed-case>-Based Pairwise Ranking with Batching and Caching</title>
      <author><first>Juan</first><last>Wisznia</last></author>
      <author><first>Cecilia</first><last>Bolaños</last></author>
      <author><first>Juan</first><last>Tollo</last></author>
      <author><first>Giovanni Franco Gabriel</first><last>Marraffini</last></author>
      <author><first>Agustín Andrés</first><last>Gianolini</last></author>
      <author><first>Noe Fabian</first><last>Hsueh</last></author>
      <author><first>Luciano Del</first><last>Corro</last><affiliation>Microsoft Research</affiliation></author>
      <pages>1064-1072</pages>
      <abstract>We introduce a novel framework for analyzing sorting algorithms in pairwise ranking prompting (PRP), re-centering the cost model around LLM inferences rather than traditional pairwise comparisons. While classical metrics based on comparison counts have traditionally been used to gauge efficiency, our analysis reveals that expensive LLM inferences overturn these predictions; accordingly, our framework encourages strategies such as batching and caching to mitigate inference costs. We show that algorithms optimal in the classical setting can lose efficiency when LLM inferences dominate the cost under certain optimizations.</abstract>
      <url hash="c488a099">2025.acl-short.83</url>
      <bibkey>wisznia-etal-2025-optimal</bibkey>
    </paper>
    <paper id="84">
      <title><fixed-case>T</fixed-case>ree<fixed-case>C</fixed-case>ut: A Synthetic Unanswerable Math Word Problem Dataset for <fixed-case>LLM</fixed-case> Hallucination Evaluation</title>
      <author><first>Jialin</first><last>Ouyang</last><affiliation>Columbia University</affiliation></author>
      <pages>1073-1085</pages>
      <abstract>Large language models (LLMs) now achieve near-human performance on standard math word problem benchmarks (e.g., GSM8K), yet their true reasoning ability remains disputed. A key concern is that models often produce confident, yet unfounded, answers to unanswerable problems. We introduce TreeCut, a synthetic dataset that systematically generates infinite unanswerable math word problems and their answerable counterparts, by representing each question as a tree and removing chosen necessary conditions. Experiments show TreeCut effectively induce hallucinations in large language models, including GPT-4o and o3-mini, with rates of 64% and 44% in their respective worst-case scenarios under zero-shot setting. Further analysis highlights that deeper or more complex trees, composite item names, and removing necessary condition near the middle of a path all increase the likelihood of hallucinations, underscoring the persistent challenges LLMs face in identifying unanswerable math problems. The dataset generation code and sample data are available at <url>https://github.com/j-bagel/treecut-math</url>.</abstract>
      <url hash="3c26a8be">2025.acl-short.84</url>
      <bibkey>ouyang-2025-treecut</bibkey>
    </paper>
    <paper id="85">
      <title><fixed-case>W</fixed-case>in<fixed-case>S</fixed-case>pot: <fixed-case>GUI</fixed-case> Grounding Benchmark with Multimodal Large Language Models</title>
      <author><first>Zheng</first><last>Hui</last><affiliation>University of Cambridge and Microsoft</affiliation></author>
      <author><first>Yinheng</first><last>Li</last><affiliation>Microsoft</affiliation></author>
      <author><first>Dan</first><last>Zhao</last></author>
      <author><first>Colby</first><last>Banbury</last><affiliation>Microsoft</affiliation></author>
      <author><first>Tianyi</first><last>Chen</last><affiliation>Microsoft</affiliation></author>
      <author><first>Kazuhito</first><last>Koishida</last><affiliation>Microsoft Corporation</affiliation></author>
      <pages>1086-1096</pages>
      <abstract>Graphical User Interface (GUI) automation relies on accurate GUI grounding. However, obtaining large-scale, high-quality labeled data remains a key challenge, particularly in desktop environments like Windows Operating System (OS). Existing datasets primarily focus on structured web-based elements, leaving a gap in real-world GUI interaction data for non-web applications. To address this, we introduce a new framework that leverages LLMs to generate large-scale GUI grounding data, enabling automated and scalable labeling across diverse interfaces. To ensure high accuracy and reliability, we manually validated and refined 5,000 GUI coordinate-instruction pairs, creating WinSpot—the first benchmark specifically designed for GUI grounding tasks in Windows environments. WinSpot provides a high-quality dataset for training and evaluating visual GUI agents, establishing a foundation for future research in GUI automation across diverse and unstructured desktop environments.</abstract>
      <url hash="8eb7043a">2025.acl-short.85</url>
      <bibkey>hui-etal-2025-winspot</bibkey>
    </paper>
    <paper id="86">
      <title>Spurious Correlations and Beyond: Understanding and Mitigating Shortcut Learning in <fixed-case>SDOH</fixed-case> Extraction with Large Language Models</title>
      <author><first>Fardin Ahsan</first><last>Sakib</last></author>
      <author><first>Ziwei</first><last>Zhu</last><affiliation>George Mason University</affiliation></author>
      <author><first>Karen Trister</first><last>Grace</last><affiliation>George Mason University</affiliation></author>
      <author><first>Meliha</first><last>Yetisgen</last><affiliation>University of Washington</affiliation></author>
      <author><first>Ozlem</first><last>Uzuner</last><affiliation>George Mason University</affiliation></author>
      <pages>1097-1106</pages>
      <abstract>Social determinants of health (SDOH) extraction from clinical text is critical for downstream healthcare analytics. Although large language models (LLMs) have shown promise, they may rely on superficial cues leading to spurious predictions. Using the MIMIC portion of the SHAC (Social History Annotation Corpus) dataset and focusing on drug status extraction as a case study, we demonstrate that mentions of alcohol or smoking can falsely induce models to predict current/past drug use where none is present, while also uncovering concerning gender disparities in model performance. We further evaluate mitigation strategies—such as prompt engineering and chain-of-thought reasoning—to reduce these false positives, providing insights into enhancing LLM reliability in health domains.</abstract>
      <url hash="5dd59fda">2025.acl-short.86</url>
      <bibkey>sakib-etal-2025-spurious</bibkey>
    </paper>
    <paper id="87">
      <title>Enhancing <fixed-case>NER</fixed-case> by Harnessing Multiple Datasets with Conditional Variational Autoencoders</title>
      <author><first>Taku</first><last>Oi</last></author>
      <author><first>Makoto</first><last>Miwa</last><affiliation>Toyota Technological Institute</affiliation></author>
      <pages>1107-1117</pages>
      <abstract>We propose a novel method to integrate a Conditional Variational Autoencoder (CVAE) into a span-based Named Entity Recognition (NER) model to model the shared and unshared information among labels in multiple datasets and ease the training on the datasets. Experimental results using multiple biomedical datasets show the effectiveness of the proposed method, achieving improved performance on the BioRED dataset.</abstract>
      <url hash="c404595a">2025.acl-short.87</url>
      <bibkey>oi-miwa-2025-enhancing</bibkey>
    </paper>
    <paper id="88">
      <title><fixed-case>CHEER</fixed-case>-<fixed-case>E</fixed-case>kman: Fine-grained Embodied Emotion Classification</title>
      <author><first>Phan Anh</first><last>Duong</last></author>
      <author><first>Cat</first><last>Luong</last></author>
      <author><first>Divyesh</first><last>Bommana</last></author>
      <author><first>Tianyu</first><last>Jiang</last><affiliation>University of Cincinnati</affiliation></author>
      <pages>1118-1131</pages>
      <abstract>Emotions manifest through physical experiences and bodily reactions, yet identifying such embodied emotions in text remains understudied. We present an embodied emotion classification dataset, CHEER-Ekman, extending the existing binary embodied emotion dataset with Ekman’s six basic emotion categories. Using automatic best-worst scaling with large language models, we achieve performance superior to supervised approaches on our new dataset. Our investigation reveals that simplified prompting instructions and chain-of-thought reasoning significantly improve emotion recognition accuracy, enabling smaller models to achieve competitive performance with larger ones.</abstract>
      <url hash="d1fc4694">2025.acl-short.88</url>
      <bibkey>duong-etal-2025-cheer</bibkey>
    </paper>
    <paper id="89">
      <title><fixed-case>S</fixed-case>can<fixed-case>EZ</fixed-case>: Integrating Cognitive Models with Self-Supervised Learning for Spatiotemporal Scanpath Prediction</title>
      <author><first>Ekta</first><last>Sood</last></author>
      <author><first>Prajit</first><last>Dhar</last></author>
      <author><first>Enrica</first><last>Troiano</last><affiliation>Vrije Universiteit Amsterdam</affiliation></author>
      <author><first>Rosy</first><last>Southwell</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <author><first>Sidney K.</first><last>DMello</last><affiliation>University of Colorado Boulder</affiliation></author>
      <pages>1132-1142</pages>
      <abstract>Accurately predicting human scanpaths duringreading is vital for diverse fields and downstream tasks, from educational technologies toautomatic question answering. To date, however, progress in this direction remains limited by scarce gaze data. We overcome theissue with ScanEZ, a self-supervised framework grounded in cognitive models of reading.ScanEZ jointly models the spatial and temporal dimensions of scanpaths by leveraging synthetic data and a 3-D gaze objective inspired bymasked language modeling. With this framework, we provide evidence that two key factorsin scanpath prediction during reading are: theuse of masked modeling of both spatial andtemporal patterns of eye movements, and cognitive model simulations as an inductive biasto kick-start training. Our approach achievesstate-of-the-art results on established datasets(e.g., up to 31.4% negative log-likelihood improvement on CELER L1), and proves portableacross different experimental conditions.</abstract>
      <url hash="d3e030d1">2025.acl-short.89</url>
      <bibkey>sood-etal-2025-scanez</bibkey>
    </paper>
    <paper id="90">
      <title>Improving Fairness of Large Language Models in Multi-document Summarization</title>
      <author><first>Haoyuan</first><last>Li</last></author>
      <author><first>Rui</first><last>Zhang</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Snigdha</first><last>Chaturvedi</last><affiliation>Department of Computer Science, University of North Carolina at Chapel Hill</affiliation></author>
      <pages>1143-1154</pages>
      <abstract>Fairness in multi-document summarization (MDS) is crucial for providing comprehensive views across documents with diverse social attribute values, which can significantly impact decision-making. For example, a summarization system that tends to overrepresent negative reviews of products can mislead customers into disregarding good products. Previous works measure fairness in MDS at two levels: summary-level and corpus-level. While summary-level fairness focuses on individual summaries, corpus-level fairness focuses on a corpus of summaries. Recent methods primarily focus on summary-level fairness. We propose FairPO, a preference tuning method that focuses on both summary-level and corpus-level fairness in MDS. To improve summary-level fairness, we propose to generate preference pairs by perturbing document sets. To improve corpus-level fairness, we propose fairness-aware preference tuning by dynamically adjusting the weights of preference pairs. Our experiments show that FairPO outperforms strong baselines while maintaining the critical qualities of summaries. The code is available at https://github.com/leehaoyuan/coverage_fairness</abstract>
      <url hash="ccab23fd">2025.acl-short.90</url>
      <bibkey>li-etal-2025-improving</bibkey>
    </paper>
    <paper id="91">
      <title>Should <fixed-case>I</fixed-case> Believe in What Medical <fixed-case>AI</fixed-case> Says? A <fixed-case>C</fixed-case>hinese Benchmark for Medication Based on Knowledge and Reasoning</title>
      <author><first>Yue</first><last>Wu</last></author>
      <author><first>Yangmin</first><last>Huang</last><affiliation>IFLYTEK CO.LTD.</affiliation></author>
      <author><first>Qianyun</first><last>Du</last><affiliation>IFLYTEK CO.LTD.</affiliation></author>
      <author><first>Lixian</first><last>Lai</last><affiliation>Xunfei Healthcare</affiliation></author>
      <author><first>Zhiyang</first><last>He</last><affiliation>Xunfei Healthcare Technology Co., Ltd.</affiliation></author>
      <author><first>Jiaxue</first><last>Hu</last><affiliation>IFLYTEK CO.LTD.</affiliation></author>
      <author><first>Xiaodong</first><last>Tao</last><affiliation>Xunfei Healthcare Co. Ltd</affiliation></author>
      <pages>1155-1164</pages>
      <abstract>Large language models (LLMs) show potential in healthcare but often generate hallucinations, especially when handling unfamiliar information. In medication, a systematic benchmark to evaluate model capabilities is lacking, which is critical given the high-risk nature of medical information. This paper introduces a Chinese benchmark aimed at assessing models in medication tasks, focusing on knowledge and reasoning across six datasets: indication, dosage and administration, contraindicated population, mechanisms of action, drug recommendation, and drug interaction. We evaluate eight closed-source and five open-source models to identify knowledge boundaries, providing the first systematic analysis of limitations and risks in proprietary medical models.</abstract>
      <url hash="d86f7cb5">2025.acl-short.91</url>
      <bibkey>wu-etal-2025-believe</bibkey>
    </paper>
    <paper id="92">
      <title>Rethinking Evaluation Metrics for Grammatical Error Correction: Why Use a Different Evaluation Process than Human?</title>
      <author><first>Takumi</first><last>Goto</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Yusuke</first><last>Sakai</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <pages>1165-1172</pages>
      <abstract>One of the goals of automatic evaluation metrics in grammatical error correction (GEC) is to rank GEC systems such that it matches human preferences. However, current automatic evaluations are based on procedures that diverge from human evaluation. Specifically, human evaluation derives rankings by aggregating sentence-level relative evaluation results, e.g., pairwise comparisons, using a rating algorithm, whereas automatic evaluation averages sentence-level absolute scores to obtain corpus-level scores, which are then sorted to determine rankings. In this study, we propose an aggregation method for existing automatic evaluation metrics which aligns with human evaluation methods to bridge this gap. We conducted experiments using various metrics, including edit-based metrics, -gram based metrics, and sentence-level metrics, and show that resolving the gap improves results for the most of metrics on the SEEDA benchmark.We also found that even BERT-based metrics sometimes outperform the metrics of GPT-4.</abstract>
      <url hash="6f040c59">2025.acl-short.92</url>
      <bibkey>goto-etal-2025-rethinking</bibkey>
    </paper>
    <paper id="93">
      <title>Learning Auxiliary Tasks Improves Reference-Free Hallucination Detection in Open-Domain Long-Form Generation</title>
      <author><first>Chengwei</first><last>Qin</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Wenxuan</first><last>Zhou</last><affiliation>CMU, Carnegie Mellon University</affiliation></author>
      <author><first>Karthik Abinav</first><last>Sankararaman</last><affiliation>Facebook</affiliation></author>
      <author><first>Nanshu</first><last>Wang</last><affiliation>Facebook</affiliation></author>
      <author><first>Tengyu</first><last>Xu</last><affiliation>Meta</affiliation></author>
      <author><first>Alexander</first><last>Radovic</last><affiliation>Facebook</affiliation></author>
      <author><first>Eryk</first><last>Helenowski</last><affiliation>Facebook</affiliation></author>
      <author><first>Arya</first><last>Talebzadeh</last><affiliation>Meta</affiliation></author>
      <author><first>Aditya</first><last>Tayade</last><affiliation>Facebook</affiliation></author>
      <author><first>Sinong</first><last>Wang</last><affiliation>Facebook</affiliation></author>
      <author><first>Shafiq</first><last>Joty</last><affiliation>Nanyang Technological University and SalesForce.com</affiliation></author>
      <author><first>Han</first><last>Fang</last><affiliation>Meta AI</affiliation></author>
      <author><first>Hao</first><last>Ma</last><affiliation>Meta</affiliation></author>
      <pages>1173-1182</pages>
      <abstract>Hallucination, the generation of factually incorrect information, remains a significant challenge for large language models (LLMs), especially in open-domain long-form generation. Existing approaches for detecting hallucination in long-form tasks either focus on limited domains or rely heavily on external fact-checking tools, which may not always be available.In this work, we systematically investigate reference-free hallucination detection in open-domain long-form responses. Our findings reveal that internal states (e.g., model’s output probability and entropy) alone are insufficient for reliably (i.e., better than random guessing) distinguishing between factual and hallucinated content. To enhance detection, we explore various existing approaches, including prompting-based methods, probing, and fine-tuning, with fine-tuning proving the most effective. To further improve the accuracy, we introduce a new paradigm, named RATE-FT, that augments fine-tuning with an auxiliary task for the model to jointly learn with the main task of hallucination detection. With extensive experiments and analysis using a variety of model families &amp; datasets, we demonstrate the effectiveness and generalizability of our method, e.g., +3% over general fine-tuning methods on LongFact.</abstract>
      <url hash="998308b4">2025.acl-short.93</url>
      <bibkey>qin-etal-2025-learning</bibkey>
    </paper>
    <paper id="94">
      <title><fixed-case>W</fixed-case>i<fixed-case>C</fixed-case>ke<fixed-case>D</fixed-case>: A Simple Method to Make Multiple Choice Benchmarks More Challenging</title>
      <author><first>Ahmed</first><last>Elhady</last></author>
      <author><first>Eneko</first><last>Agirre</last><affiliation>University of the Basque Country (UPV/EHU)</affiliation></author>
      <author><first>Mikel</first><last>Artetxe</last><affiliation>Reka AI</affiliation></author>
      <pages>1183-1192</pages>
      <abstract>We introduce WiCkeD, a simple method to increase the complexity of existing multiple-choice benchmarks by randomly replacing a choice with “None of the above”, a method often used in educational tests. We show that WiCkeD can be automatically applied to any existing benchmark, making it more challenging. We apply WiCkeD to 6 popular benchmarks and use it to evaluate 18 open-weight LLMs. The performance of the models drops12.1 points on average with respect to the original versions of the datasets. When using chainof-thought on 3 MMLU datasets, the performance drop for the WiCkeD variant is similar to the one observed when using the LLMs directly, showing that WiCkeD is also challenging for models with enhanced reasoning abilities. WiCkeD also uncovers that some models are more sensitive to the extra reasoning required, providing additional information with respect to the original benchmarks.We relase our code and data at github.com/anonymized.</abstract>
      <url hash="ecfc5640">2025.acl-short.94</url>
      <bibkey>elhady-etal-2025-wicked</bibkey>
    </paper>
    <paper id="95">
      <title>Cross-Lingual Representation Alignment Through Contrastive Image-Caption Tuning</title>
      <author><first>Nathaniel</first><last>Krasner</last><affiliation>George Mason University</affiliation></author>
      <author><first>Nicholas</first><last>Lanuzo</last></author>
      <author><first>Antonios</first><last>Anastasopoulos</last><affiliation>Athena Research Center and George Mason University</affiliation></author>
      <pages>1193-1199</pages>
      <abstract>Multilingual alignment of sentence representations has mostly required bitexts to bridge the gap between languages. We investigate whether visual information can bridge this gap instead. Image caption datasets are very easy to create without requiring multilingual expertise, so this offers a more efficient alternative for low-resource languages. We find that multilingual image-caption alignment can implicitly align the text representations between languages, languages unseen by the encoder in pretraining can be incorporated into this alignment post-hoc, and these aligned representations are usable for cross-lingual Natural Language Understanding (NLU) and bitext retrieval.</abstract>
      <url hash="621b1dcb">2025.acl-short.95</url>
      <bibkey>krasner-etal-2025-cross</bibkey>
    </paper>
    <paper id="96">
      <title><fixed-case>LAMB</fixed-case>: A Training-Free Method to Enhance the Long-Context Understanding of <fixed-case>SSM</fixed-case>s via Attention-Guided Token Filtering</title>
      <author><first>Zhifan</first><last>Ye</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Zheng</first><last>Wang</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Kejing</first><last>Xia</last><affiliation>Bosch</affiliation></author>
      <author><first>Jihoon</first><last>Hong</last></author>
      <author><first>Leshu</first><last>Li</last></author>
      <author><first>Lexington</first><last>Whalen</last></author>
      <author><first>Cheng</first><last>Wan</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Yonggan</first><last>Fu</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Yingyan Celine</first><last>Lin</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Souvik</first><last>Kundu</last><affiliation>Intel</affiliation></author>
      <pages>1200-1209</pages>
      <abstract>State space models (SSMs) achieve efficient sub-quadratic compute complexity but often exhibit significant performance drops as context length increases. Recent work attributes this deterioration to an exponential decay in hidden-state memory. While token filtering has emerged as a promising remedy, its underlying rationale and limitations remain largely non-understood. In this paper, we first investigate the attention patterns of Mamba to shed light on why token filtering alleviates long-context degradation. Motivated by these findings, we propose LAMB, a training-free, attention-guided token filtering strategy designed to preserve critical tokens during inference. LAMB can boost long-context performance for both pure SSMs and hybrid models, achieving up to an average improvement of 30.35% over state-of-the-art techniques on standard long-context understanding benchmarks. Our analysis and experiments reveal new insights into the interplay between attention, token selection, and memory retention, and are thus expected to inspire broader applications of token filtering in long-sequence modeling.</abstract>
      <url hash="5f7ee06f">2025.acl-short.96</url>
      <bibkey>ye-etal-2025-lamb</bibkey>
    </paper>
    <paper id="97">
      <title>Counterfactual-Consistency Prompting for Relative Temporal Understanding in Large Language Models</title>
      <author><first>Jongho</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Seung-won</first><last>Hwang</last><affiliation>Seoul National University</affiliation></author>
      <pages>1210-1225</pages>
      <abstract>Despite the advanced capabilities of large language models (LLMs), their temporal reasoning ability remains underdeveloped. Prior works have highlighted this limitation, particularly in maintaining temporal consistency when understanding event relations. For example, models often confuse mutually exclusive temporal relations like “before” and “after” between events and make inconsistent predictions. In this work, we tackle the issue of temporal inconsistency in LLMs by proposing a novel counterfactual prompting approach. Our method generates counterfactual questions and enforces collective constraints, enhancing the model’s consistency. We evaluate our method on multiple datasets, demonstrating significant improvements in event ordering for explicit and implicit events and temporal commonsense understanding, by effectively addressing temporal inconsistencies.</abstract>
      <url hash="1dcc8219">2025.acl-short.97</url>
      <bibkey>kim-hwang-2025-counterfactual</bibkey>
    </paper>
  </volume>
  <volume id="demo" ingest-date="2025-07-14" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)</booktitle>
      <editor><first>Pushkar</first><last>Mishra</last></editor>
      <editor><first>Smaranda</first><last>Muresan</last></editor>
      <editor><first>Tao</first><last>Yu</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Vienna, Austria</address>
      <month>July</month>
      <year>2025</year>
      <url hash="b58ccf62">2025.acl-demo</url>
      <venue>acl</venue>
      <venue>ws</venue>
      <isbn>979-8-89176-253-4</isbn>
    </meta>
    <frontmatter>
      <url hash="2965e1ed">2025.acl-demo.0</url>
      <bibkey>acl-ws-2025-demo</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>M</fixed-case>ap<fixed-case>Q</fixed-case>a<fixed-case>T</fixed-case>or: An Extensible Framework for Efficient Annotation of Map-Based <fixed-case>QA</fixed-case> Datasets</title>
      <author><first>Mahir Labib</first><last>Dihan</last><affiliation>Bangladesh University of Engineering and Technology</affiliation></author>
      <author><first>Mohammed Eunus</first><last>Ali</last><affiliation>Bangladesh University of Engineering and Technology</affiliation></author>
      <author><first>Md Rizwan</first><last>Parvez</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <pages>1-10</pages>
      <abstract>Mapping and navigation services like Google Maps, Apple Maps, OpenStreetMap, are essential for accessing various location-based data, yet they often struggle to handle natural language geospatial queries. Recent advancements in Large Language Models (LLMs) show promise in question answering (QA), but creating reliable geospatial QA datasets from map services remains challenging. We introduce MapQaTor, an extensible open-source framework that streamlines the creation of reproducible, traceable map-based QA datasets. MapQaTor enables seamless integration with any maps API, allowing users to gather and visualize data from diverse sources with minimal setup. By caching API responses, the platform ensures consistent ground truth, enhancing the reliability of the data even as real-world information evolves. MapQaTor centralizes data retrieval, annotation, and visualization within a single platform, offering a unique opportunity to evaluate the current state of LLM-based geospatial reasoning while advancing their capabilities for improved geospatial understanding. Evaluation metrics show that, MapQaTor speeds up the annotation process by at least 30 times compared to manual methods, underscoring its potential for developing geospatial resources, such as complex map reasoning datasets. The website is live at: https://mapqator.github.io/ and a demo video is available at: https://youtu.be/bVv7-NYRsTw.</abstract>
      <url hash="50c576d5">2025.acl-demo.1</url>
      <attachment type="copyright_agreement" hash="4c6806d6">2025.acl-demo.1.copyright_agreement.pdf</attachment>
      <bibkey>dihan-etal-2025-mapqator</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>PEIRCE</fixed-case>: Unifying Material and Formal Reasoning via <fixed-case>LLM</fixed-case>-Driven Neuro-Symbolic Refinement</title>
      <author><first>Xin</first><last>Quan</last></author>
      <author><first>Marco</first><last>Valentino</last><affiliation>University of Sheffield</affiliation></author>
      <author><first>Danilo</first><last>Carvalho</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Dhairya</first><last>Dalal</last><affiliation>University of Galway</affiliation></author>
      <author><first>Andre</first><last>Freitas</last><affiliation>Idiap Research Institute and University of Manchester</affiliation></author>
      <pages>11-21</pages>
      <abstract>A persistent challenge in AI is the effective integration of material and formal inference - the former concerning the plausibility and contextual relevance of arguments, while the latter focusing on their logical and structural validity. Large Language Models (LLMs), by virtue of their extensive pre-training on large textual corpora, exhibit strong capabilities in material inference. However, their reasoning often lacks formal rigour and verifiability. At the same time, LLMs’ linguistic competence positions them as a promising bridge between natural and formal languages, opening up new opportunities for combining these two modes of reasoning.In this paper, we introduce PEIRCE, a neuro-symbolic framework designed to unify material and formal inference through an iterative conjecture–criticism process. Within this framework, LLMs play the central role of generating candidate solutions in natural and formal languages, which are then evaluated and refined via interaction with external critique models. These critiques include symbolic provers, which assess formal validity, as well as soft evaluators that measure the quality of the generated arguments along linguistic and epistemic dimensions such as plausibility, coherence, and parsimony. While PEIRCE is a general-purpose framework, we demonstrate its capabilities in the domain of natural language explanation generation - a setting that inherently demands both material adequacy and formal correctness.</abstract>
      <url hash="7b6f77ab">2025.acl-demo.2</url>
      <attachment type="copyright_agreement" hash="fac18c54">2025.acl-demo.2.copyright_agreement.pdf</attachment>
      <bibkey>quan-etal-2025-peirce</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>MER</fixed-case>a<fixed-case>L</fixed-case>i<fixed-case>ON</fixed-case>-<fixed-case>A</fixed-case>udio<fixed-case>LLM</fixed-case>: Advancing Speech and Language Understanding for <fixed-case>S</fixed-case>ingapore</title>
      <author><first>Yingxu</first><last>He</last></author>
      <author><first>Zhuohan</first><last>Liu</last><affiliation>, A*STAR</affiliation></author>
      <author><first>Geyu</first><last>Lin</last><affiliation>Institute of Infocomm Research, A*STAR</affiliation></author>
      <author><first>Shuo</first><last>Sun</last><affiliation>, A*STAR</affiliation></author>
      <author><first>Bin</first><last>Wang</last></author>
      <author><first>Wenyu</first><last>Zhang</last><affiliation>I2R, A*STAR</affiliation></author>
      <author><first>Xunlong</first><last>Zou</last><affiliation>A*STAR</affiliation></author>
      <author><first>Nancy F.</first><last>Chen</last></author>
      <author><first>AiTi</first><last>Aw</last><affiliation>I2R</affiliation></author>
      <pages>22-30</pages>
      <abstract>We introduce MERaLiON-AudioLLM, the first general-purpose audio-based large language model designed for multitask learning, with a particular focus on Singlish understanding. Trained on 62 million multimodal instruction samples comprising a total of 260k hours of audio, it exhibits strong generalization across a diverse set of tasks, including—but not limited to—automatic speech recognition, spoken question answering, speech translation, and paralinguistic analysis. Our results show significant improvements in local speech recognition and task-specific understanding, making MERaLiON-AudioLLM a leading solution for region-specific AI applications. An interactive demo has been developed to enable user-friendly interactions, supported by a backend with customized caching and load-balancing mechanisms. We benchmark the model across a broad range of multilingual and multitask scenarios, where it demonstrates competitive performance compared to other open-source models. The demo page, model weights and videos are publically accessible.</abstract>
      <url hash="e85c0f56">2025.acl-demo.3</url>
      <attachment type="copyright_agreement" hash="80601c96">2025.acl-demo.3.copyright_agreement.pdf</attachment>
      <bibkey>he-etal-2025-meralion</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>N</fixed-case>ame<fixed-case>T</fixed-case>ag 3: A Tool and a Service for Multilingual/Multitagset <fixed-case>NER</fixed-case></title>
      <author><first>Jana</first><last>Straková</last><affiliation>Charles University Prague</affiliation></author>
      <author><first>Milan</first><last>Straka</last><affiliation>Charles University Prague</affiliation></author>
      <pages>31-39</pages>
      <abstract>We introduce NameTag 3, an open-source tool and cloud-based web service for multilingual, multidataset, and multitagset named entity recognition (NER), supporting both flat and nested entities. NameTag 3 achieves state-of-the-art results on 21 test datasets in 15 languages and remains competitive on the rest, even against larger models. It is available as a command-line tool and as a cloud-based service, enabling use without local installation. NameTag 3 web service currently provides flat NER for 17 languages, trained on 21 corpora and three NE tagsets, all powered by a single 355M-parameter fine-tuned model; and nested NER for Czech, powered by a 126M fine-tuned model. The source code is licensed under open-source MPL 2.0, while the models are distributed under non-commercial CC BY-NC-SA 4.0. Documentation is available at https://ufal.mff.cuni.cz/nametag, source code at https://github.com/ufal/nametag3, and trained models via https://lindat.cz. The REST service and the web application can be found at https://lindat.mff.cuni.cz/services/nametag/. A demonstration video is available at https://www.youtube.com/watch?v=-gaGnP0IV8A.</abstract>
      <url hash="f63ef066">2025.acl-demo.4</url>
      <attachment type="copyright_agreement" hash="25f4d0d1">2025.acl-demo.4.copyright_agreement.pdf</attachment>
      <bibkey>strakova-straka-2025-nametag</bibkey>
    </paper>
    <paper id="5">
      <title>Multi-Programming Language Sandbox for <fixed-case>LLM</fixed-case>s</title>
      <author><first>Shihan</first><last>Dou</last></author>
      <author><first>Jiazheng</first><last>Zhang</last></author>
      <author><first>Jianxiang</first><last>Zang</last></author>
      <author><first>Yunbo</first><last>Tao</last><affiliation>Fudan University</affiliation></author>
      <author><first>Weikang</first><last>Zhou</last></author>
      <author><first>Haoxiang</first><last>Jia</last></author>
      <author><first>Shichun</first><last>Liu</last></author>
      <author><first>Yuming</first><last>Yang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Shenxi</first><last>Wu</last></author>
      <author><first>Zhiheng</first><last>Xi</last></author>
      <author><first>Muling</first><last>Wu</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Rui</first><last>Zheng</last></author>
      <author><first>Changze</first><last>Lv</last></author>
      <author><first>Limao</first><last>Xiong</last><affiliation>Fudan University</affiliation></author>
      <author><first>Shaoqing</first><last>Zhang</last></author>
      <author><first>Lin</first><last>Zhang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Wenyu</first><last>Zhan</last></author>
      <author><first>Rongxiang</first><last>Weng</last><affiliation>Meituan</affiliation></author>
      <author><first>Jingang</first><last>Wang</last><affiliation>Meituan</affiliation></author>
      <author><first>Xunliang</first><last>Cai</last><affiliation>Meituan</affiliation></author>
      <author><first>Yueming</first><last>Wu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Ming</first><last>Wen</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Yixin</first><last>Cao</last><affiliation>Fudan University</affiliation></author>
      <author><first>Tao</first><last>Gui</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xipeng</first><last>Qiu</last><affiliation>Fudan University</affiliation></author>
      <author><first>Qi</first><last>Zhang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xuanjing</first><last>Huang</last><affiliation>Fudan University</affiliation></author>
      <pages>40-50</pages>
      <abstract>We introduce MPLSandbox, an out-of-the-box multi-programming language sandbox designed to provide unified and comprehensive feedback from compiler and analysis tools for Large Language Models (LLMs). It can automatically identify the programming language of the code, compiling and executing it within an isolated sub-sandbox to ensure safety and stability. In addition, MPLSandbox integrates both traditional and LLM-based code analysis tools, providing a comprehensive analysis of generated code. It also can be effortlessly integrated into the training and deployment of LLMs to improve the quality and correctness of generated code. It also helps researchers streamline their workflows for various LLM-based code-related tasks, reducing the development cost. To validate the effectiveness of MPLSandbox, we conduct extensive experiments by integrating it into several training and deployment scenarios, and employing it to optimize workflows for a wide range of downstream code tasks. Our goal is to enhance researcher productivity on LLM-based code tasks by simplifying and automating workflows through delegation to MPLSandbox.</abstract>
      <url hash="cb7d56d5">2025.acl-demo.5</url>
      <attachment type="copyright_agreement" hash="735698c6">2025.acl-demo.5.copyright_agreement.pdf</attachment>
      <bibkey>dou-etal-2025-multi</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>F</fixed-case>lag<fixed-case>E</fixed-case>val<fixed-case>MM</fixed-case>: A Flexible Framework for Comprehensive Multimodal Model Evaluation</title>
      <author><first>Zheqi</first><last>He</last><affiliation>Beijing Academy of Artificial Intelligence</affiliation></author>
      <author><first>Yesheng</first><last>Liu</last></author>
      <author><first>Jing-Shu</first><last>Zheng</last></author>
      <author><first>Xuejing</first><last>Li</last></author>
      <author><first>Jin-Ge</first><last>Yao</last><affiliation>BAAI</affiliation></author>
      <author><first>Bowen</first><last>Qin</last><affiliation>Beijing Academy of Artificial Intelligence</affiliation></author>
      <author><first>Richeng</first><last>Xuan</last></author>
      <author><first>Xi</first><last>Yang</last><affiliation>Beijing Academy of Artificial Intelligence</affiliation></author>
      <pages>51-61</pages>
      <abstract>We present FlagEvalMM, an open-source evaluation framework designed to comprehensively assess multimodal models across a diverse range of vision-language understanding and generation tasks, such as visual question answering, text-to-image/video generation, and image-text retrieval. We decouple model inference from evaluation through an independent evaluation service, thus enabling flexible resource allocation and seamless integration of new tasks and models. Moreover, FlagEvalMM utilizes advanced inference acceleration tools (e.g., vLLM, SGLang) and asynchronous data loading to significantly enhance evaluation efficiency. Extensive experiments show that FlagEvalMM offers accurate and efficient insights into model strengths and limitations, making it a valuable tool for advancing multimodal research. The framework is publicly accessible at https://github.com/flageval-baai/FlagEvalMM, with a demonstration video available at https://youtu.be/L7EtacjoM0k.</abstract>
      <url hash="0fd00590">2025.acl-demo.6</url>
      <attachment type="copyright_agreement" hash="8c41b4c1">2025.acl-demo.6.copyright_agreement.pdf</attachment>
      <bibkey>he-etal-2025-flagevalmm</bibkey>
    </paper>
    <paper id="7">
      <title>My Climate <fixed-case>C</fixed-case>o<fixed-case>P</fixed-case>ilot: A Question Answering System for Climate Adaptation in Agriculture</title>
      <author><first>Vincent</first><last>Nguyen</last><affiliation>CSIRO’s Data61</affiliation></author>
      <author><first>Willow</first><last>Hallgren</last><affiliation>CSIRO</affiliation></author>
      <author><first>Ashley</first><last>Harkin</last></author>
      <author><first>Mahesh</first><last>Prakash</last></author>
      <author><first>Sarvnaz</first><last>Karimi</last><affiliation>CSIRO</affiliation></author>
      <pages>62-70</pages>
      <abstract>Accurately answering climate science questions requires scientific literature and climate data. Interpreting climate literature and data, however, presents inherent challenges such as determining relevant climate factors and drivers, interpreting uncertainties in the science and data, and dealing with the sheer volume of data. My Climate CoPilot is a platform that assists a range of potential users, such as farmer advisors, to mitigate and adapt to projected climate changes by providing answers to questions that are grounded in evidence. It emphasises transparency, user privacy, low-resource use, and provides automatic evaluation. It also strives for scientific robustness and accountability. Fifty domain experts carefully evaluated every aspect of My Climate CoPilot and based on their interactions and feedback, the system continues to evolve.</abstract>
      <url hash="3658cfb5">2025.acl-demo.7</url>
      <attachment type="copyright_agreement" hash="823299ae">2025.acl-demo.7.copyright_agreement.pdf</attachment>
      <bibkey>nguyen-etal-2025-climate</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>SPOT</fixed-case>: Bridging Natural Language and Geospatial Search for Investigative Journalism</title>
      <author><first>Lynn</first><last>Khellaf</last><affiliation>Deutsche Welle</affiliation></author>
      <author><first>Ipek Baris</first><last>Schlicht</last></author>
      <author><first>Tilman</first><last>Mirass</last></author>
      <author><first>Julia</first><last>Bayer</last><affiliation>DW Innovation</affiliation></author>
      <author><first>Tilman</first><last>Wagner</last><affiliation>DW Innovation</affiliation></author>
      <author><first>Ruben</first><last>Bouwmeester</last><affiliation>DW Innovation</affiliation></author>
      <pages>71-81</pages>
      <abstract>OpenStreetMap (OSM) is a vital resource for investigative journalists doing geolocation verification. However, existing tools to query OSM data such as Overpass Turbo require familiarity with complex query languages, creating barriers for non-technical users. We present SPOT, an open source natural language interface that makes OSM’s rich, tag-based geographic data more accessible through intuitive scene descriptions. SPOT interprets user inputs as structured representations of geospatial object configurations using fine-tuned Large Language Models (LLMs), with results being displayed in an interactive map interface. While more general geospatial search tasks are conceivable, SPOT is specifically designed for use in investigative journalism, addressing real-world challenges such as hallucinations in model output, inconsistencies in OSM tagging, and the noisy nature of user input. It combines a novel synthetic data pipeline with a semantic bundling system to enable robust, accurate query generation. To our knowledge, SPOT is the first system to achieve reliable natural language access to OSM data at this level of accuracy. By lowering the technical barrier to geolocation verification, SPOT contributes a practical tool to the broader efforts to support fact-checking and combat disinformation.</abstract>
      <url hash="79be92d3">2025.acl-demo.8</url>
      <attachment type="copyright_agreement" hash="248e81da">2025.acl-demo.8.copyright_agreement.pdf</attachment>
      <bibkey>khellaf-etal-2025-spot</bibkey>
    </paper>
    <paper id="9">
      <title>Textagon: Boosting Language Models with Theory-guided Parallel Representations</title>
      <author><first>John P.</first><last>Lalor</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Ruiyang</first><last>Qin</last></author>
      <author><first>David</first><last>Dobolyi</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <author><first>Ahmed</first><last>Abbasi</last><affiliation>University of Notre Dame</affiliation></author>
      <pages>82-92</pages>
      <abstract>Pretrained language models have significantly advanced the state of the art in generating distributed representations of text. However, they do not account for the wide variety of available expert-generated language resources and lexicons that explicitly encode linguistic/domain knowledge. Such lexicons can be paired with learned embeddings to further enhance NLP prediction and linguistic inquiry. In this work we present Textagon, a Python package for generating parallel representations for text based on predefined lexicons and selecting representations that provide the most information. We discuss the motivation behind the software, its implementation, as well as two case studies for its use to demonstrate operational utility.</abstract>
      <url hash="de54641a">2025.acl-demo.9</url>
      <attachment type="copyright_agreement" hash="fb0d94fe">2025.acl-demo.9.copyright_agreement.pdf</attachment>
      <bibkey>lalor-etal-2025-textagon</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>G</fixed-case>iga<fixed-case>C</fixed-case>hat Family: Efficient <fixed-case>R</fixed-case>ussian Language Modeling Through Mixture of Experts Architecture</title>
      <author><first>Valentin</first><last>Mamedov</last><affiliation>Sberbank</affiliation></author>
      <author><first>Evgenii</first><last>Kosarev</last><affiliation>Sberbank and SaluteDevices</affiliation></author>
      <author><first>Gregory</first><last>Leleytner</last></author>
      <author><first>Ilya</first><last>Shchuckin</last></author>
      <author><first>Valeriy</first><last>Berezovskiy</last><affiliation>Sberbank</affiliation></author>
      <author><first>Daniil</first><last>Smirnov</last><affiliation>Sber</affiliation></author>
      <author><first>Dmitry</first><last>Kozlov</last><affiliation>Sberbank</affiliation></author>
      <author><first>Sergei</first><last>Averkiev</last><affiliation>SberDevices</affiliation></author>
      <author><first>Lukyanenko</first><last>Ivan</last><affiliation>Sberbank</affiliation></author>
      <author><first>Aleksandr</first><last>Proshunin</last></author>
      <author><first>Ainur</first><last>Israfilova</last><affiliation>SberDevices</affiliation></author>
      <author><first>Ivan</first><last>Baskov</last><affiliation>Sberbank</affiliation></author>
      <author><first>Artem</first><last>Chervyakov</last></author>
      <author><first>Emil</first><last>Shakirov</last><affiliation>St. Petersburg State University</affiliation></author>
      <author><first>Mikhail</first><last>Kolesov</last><affiliation>Sberbank and SberDevices</affiliation></author>
      <author><first>Daria</first><last>Khomich</last></author>
      <author><first>Daria</first><last>Latortseva</last><affiliation>Sber</affiliation></author>
      <author><first>Sergei</first><last>Porkhun</last><affiliation>SberDevices</affiliation></author>
      <author><first>Yury</first><last>Fedorov</last><affiliation>Sber</affiliation></author>
      <author><first>Oleg</first><last>Kutuzov</last><affiliation>Sberbank</affiliation></author>
      <author><first>Polina</first><last>Kudriavtseva</last><affiliation>SberDevices</affiliation></author>
      <author><first>Sofiia</first><last>Soldatova</last><affiliation>SberDevices</affiliation></author>
      <author><first>Kolodin</first><last>Egor</last></author>
      <author><first>Stanislav</first><last>Pyatkin</last><affiliation>Sberbank</affiliation></author>
      <author><first>Dzmitry</first><last>Menshykh</last></author>
      <author><first>Grafov Sergei</first><last>IUrevich</last></author>
      <author><first>Eldar</first><last>Damirov</last></author>
      <author><first>Vladimir</first><last>Karlov</last></author>
      <author><first>Ruslan</first><last>Gaitukiev</last></author>
      <author><first>Arkadiy</first><last>Shatenov</last><affiliation>Lomonosov Moscow State University</affiliation></author>
      <author><first>Alena</first><last>Fenogenova</last></author>
      <author><first>Nikita</first><last>Savushkin</last><affiliation>Stevenson University</affiliation></author>
      <author><first>Fedor</first><last>Minkin</last><affiliation>Sberbank</affiliation></author>
      <pages>93-106</pages>
      <abstract>Generative large language models (LLMs) have become crucial for modern NLP research and applications across various languages. However, the development of foundational models specifically tailored to the Russian language has been limited, primarily due to the significant computational resources required. This paper introduces the GigaChat family of Russian LLMs, available in various sizes, including base models and instruction-tuned versions. We provide a detailed report on the model architecture, pre-training process, and experiments to guide design choices. In addition, we evaluate their performance on Russian and English benchmarks and compare GigaChat with multilingual analogs. The paper presents a system demonstration of the top-performing models accessible via an API, a Telegram bot, and a Web interface. Furthermore, we have released three open GigaChat models in open-source, aiming to expand NLP research opportunities and support the development of industrial solutions for the Russian language.</abstract>
      <url hash="db5e946f">2025.acl-demo.10</url>
      <attachment type="copyright_agreement" hash="98c088ab">2025.acl-demo.10.copyright_agreement.pdf</attachment>
      <bibkey>mamedov-etal-2025-gigachat</bibkey>
    </paper>
    <paper id="11">
      <title>Unifying Language Agent Algorithms with Graph-based Orchestration Engine for Reproducible Agent Research</title>
      <author><first>Qianqian</first><last>Zhang</last><affiliation>Om AI Lab</affiliation></author>
      <author><first>Jiajia</first><last>Liao</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Heting</first><last>Ying</last><affiliation>Linker Technology Research Co. Ltd</affiliation></author>
      <author><first>Yibo</first><last>Ma</last><affiliation>Linker Technology Research Co. Ltd</affiliation></author>
      <author><first>Haozhan</first><last>Shen</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Jingcheng</first><last>Li</last><affiliation>Department of Computer Science, University of Massachusetts at Amherst</affiliation></author>
      <author><first>Peng</first><last>Liu</last></author>
      <author><first>Lu</first><last>Zhang</last></author>
      <author><first>Chunxin</first><last>Fang</last><affiliation>Binjiang Institute of Zhejiang University</affiliation></author>
      <author><first>Kyusong</first><last>Lee</last><affiliation>Binjiang Institute of Zhejiang University and SOCO, Inc.</affiliation></author>
      <author><first>Ruochen</first><last>Xu</last><affiliation>Om AI</affiliation></author>
      <author><first>Tiancheng</first><last>Zhao</last><affiliation>Binjiang Institute of Zhejiang University</affiliation></author>
      <pages>107-117</pages>
      <abstract>Language agents powered by large language models (LLMs) have demonstrated remarkable capabilities in understanding, reasoning, and executing complex tasks. However, developing robust agents presents significant challenges: substantial engineering overhead, lack of standardized components, and insufficient evaluation frameworks for fair comparison. We introduce Agent Graph-based Orchestration for Reasoning and Assessment (AGORA), a flexible and extensible framework that addresses these challenges through three key contributions: (1) a modular architecture with a graph-based workflow engine, efficient memory management, and clean component abstraction; (2) a comprehensive suite of reusable agent algorithms implementing state-of-the-art reasoning approaches; and (3) a rigorous evaluation framework enabling systematic comparison across multiple dimensions. Through extensive experiments on mathematical reasoning and multimodal tasks, we evaluate various agent algorithms across different LLMs, revealing important insights about their relative strengths and applicability. Our results demonstrate that while sophisticated reasoning approaches can enhance agent capabilities, simpler methods like Chain-of-Thought often exhibit robust performance with significantly lower computational overhead. AGORA not only simplifies language agent development but also establishes a foundation for reproducible agent research through standardized evaluation protocols.We made a demo video at: https://www.youtube.com/watch?v=WRH-F1zegKI. The comparison agent of algorithms is also available at https://huggingface.co/spaces/omlab/open-agent-leaderboard. Source code of AGORA can be found at https://github.com/om-ai-lab/OmAgent.</abstract>
      <url hash="e60a9b54">2025.acl-demo.11</url>
      <attachment type="copyright_agreement" hash="05e7d602">2025.acl-demo.11.copyright_agreement.pdf</attachment>
      <bibkey>zhang-etal-2025-unifying</bibkey>
    </paper>
    <paper id="12">
      <title>Abacus-<fixed-case>SQL</fixed-case>: A Text-to-<fixed-case>SQL</fixed-case> System Empowering Cross-Domain and Open-Domain Database Retrieval</title>
      <author><first>Keyan</first><last>Xu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Dingzirui</first><last>Wang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Xuanliang</first><last>Zhang</last></author>
      <author><first>Qingfu</first><last>Zhu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Wanxiang</first><last>Che</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>118-128</pages>
      <abstract>The existing text-to-SQL systems have made significant progress in SQL query generation, but they still face numerous challenges. Existing systems often lack retrieval capabilities for open-domain databases, requiring users to manually filter relevant databases. Additionally, their cross-domain transferability is limited, making it challenging to accommodate diverse query requirements. To address these issues, we propose Abacus-SQL. Abacus-SQL utilizes database retrieval technology to accurately locate the required databases in an open-domain database environment. It also enhances the system cross-domain transfer ability through data augmentation methods. Moreover, Abacus-SQL employs Pre-SQL and Self-debug methods, thereby enhancing the accuracy of SQL queries. Experimental results demonstrate that Abacus-SQL performs excellently in multi-turn text-to-SQL tasks, effectively validating the approach’s effectiveness.Abacus-SQL is publicly accessible at https://huozi.8wss.com/abacus-sql/.</abstract>
      <url hash="4bbfb00e">2025.acl-demo.12</url>
      <attachment type="copyright_agreement" hash="f7b9e855">2025.acl-demo.12.copyright_agreement.pdf</attachment>
      <bibkey>xu-etal-2025-abacus</bibkey>
    </paper>
    <paper id="13">
      <title>Tulun: Transparent and Adaptable Low-resource Machine Translation</title>
      <author><first>Raphael</first><last>Merx</last><affiliation>University of Melbourne</affiliation></author>
      <author><first>Hanna</first><last>Suominen</last><affiliation>Australian National University</affiliation></author>
      <author><first>Lois Yinghui</first><last>Hong</last></author>
      <author><first>Nick</first><last>Thieberger</last><affiliation>University of Melbourne</affiliation></author>
      <author><first>Trevor</first><last>Cohn</last><affiliation>Google and The University of Melbourne</affiliation></author>
      <author><first>Ekaterina</first><last>Vylomova</last><affiliation>The University of Melbourne</affiliation></author>
      <pages>129-139</pages>
      <abstract>Machine translation (MT) systems that support low-resource languages often struggle on specialized domains. While researchers have proposed various techniques for domain adaptation, these approaches typically require model fine-tuning, making them impractical for non-technical users and small organizations. To address this gap, we propose Tulun, a versatile solution for terminology-aware translation, combining neural MT with large language model (LLM)-based post-editing guided by existing glossaries and translation memories.Our open-source web-based platform enables users to easily create, edit, and leverage terminology resources, fostering a collaborative human-machine translation process that respects and incorporates domain expertise while increasing MT accuracy.Evaluations show effectiveness in both real-world and benchmark scenarios: on medical and disaster relief translation tasks for Tetun and Bislama, our system achieves improvements of 16.90-22.41 ChrF++ points over baseline MT systems. Across six low-resource languages on the FLORES dataset, Tulun outperforms both standalone MT and LLM approaches, achieving an average improvement of 2.8 ChrF++ points over NLLB-54B. Tulun is publicly accessible at https://bislama-trans.rapha.dev.</abstract>
      <url hash="527c25b1">2025.acl-demo.13</url>
      <attachment type="copyright_agreement" hash="6898e724">2025.acl-demo.13.copyright_agreement.pdf</attachment>
      <bibkey>merx-etal-2025-tulun</bibkey>
    </paper>
    <paper id="14">
      <title><fixed-case>R</fixed-case>eason<fixed-case>G</fixed-case>raph: Visualization of Reasoning Methods and Extended Inference Paths</title>
      <author><first>Zongqian</first><last>Li</last></author>
      <author><first>Ehsan</first><last>Shareghi</last><affiliation>Monash University</affiliation></author>
      <author><first>Nigel</first><last>Collier</last><affiliation>University of Cambridge</affiliation></author>
      <pages>140-147</pages>
      <abstract>Large Language Models (LLMs) reasoning processes are challenging to analyze due to their complexity and the lack of organized visualization tools. We present ReasonGraph, a web-based platform for visualizing and analyzing LLM reasoning processes. It supports both sequential and tree-based reasoning methods and extended inference outputs while integrating with major LLM providers and over fifty state-of-the-art models. ReasonGraph incorporates an intuitive UI with meta reasoning method selection, configurable visualization parameters, and a modular framework that facilitates efficient extension. Our evaluation shows high parsing reliability, efficient processing, and excellent usability across various downstream applications. By providing a unified visualization framework, ReasonGraph reduces cognitive load in analyzing complex reasoning paths, improves error identification in logical processes, and enables more effective development of LLM-based applications. The platform is open-source, facilitating accessibility and reproducibility in LLM reasoning analysis.</abstract>
      <url hash="9758f380">2025.acl-demo.14</url>
      <attachment type="copyright_agreement" hash="b4563b32">2025.acl-demo.14.copyright_agreement.pdf</attachment>
      <bibkey>li-etal-2025-reasongraph</bibkey>
    </paper>
    <paper id="15">
      <title><fixed-case>D</fixed-case>ia-Lingle: A Gamified Interface for Dialectal Data Collection</title>
      <author><first>Jiugeng</first><last>Sun</last></author>
      <author><first>Rita</first><last>Sevastjanova</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <author><first>Sina</first><last>Ahmadi</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Rico</first><last>Sennrich</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Mennatallah</first><last>El-Assady</last><affiliation>Department of Computer Science, ETHZ - ETH Zurich</affiliation></author>
      <pages>148-158</pages>
      <abstract>Dialects suffer from the scarcity of computational textual resources as they exist predominantly in spoken rather than written form and exhibit remarkable geographical diversity. Collecting dialect data and subsequently integrating it into current language technologies present significant obstacles. Gamification has been proven to facilitate remote data collection processes with great ease and on a substantially wider scale. This paper introduces Dia-Lingle, a gamified interface aimed to improve and facilitate dialectal data collection tasks such as corpus expansion and dialect labelling. The platform features two key components: the first challenges users to rewrite sentences in their dialects, identifies them through a classifier and solicits feedback, and the other one asks users to match sentences to their geographical locations. Dia-Lingle combines active learning with gamified difficulty levels, strategically encouraging prolonged user engagement while efficiently enriching the dialect corpus. Usability evaluation shows that our interface demonstrates high levels of user satisfaction. We provide the link to Dia-Lingle: https://dia-lingle.ivia.ch/, and demo video: https://youtu.be/0QyJsB8ym64.</abstract>
      <url hash="ba419a50">2025.acl-demo.15</url>
      <attachment type="copyright_agreement" hash="6d9df018">2025.acl-demo.15.copyright_agreement.pdf</attachment>
      <bibkey>sun-etal-2025-dia</bibkey>
    </paper>
    <paper id="16">
      <title>Token Level Routing Inference System for Edge Devices</title>
      <author><first>Jianshu</first><last>She</last></author>
      <author><first>Wenhao</first><last>Zheng</last></author>
      <author><first>Zhengzhong</first><last>Liu</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Hongyi</first><last>Wang</last><affiliation>Rutgers University and GenBio AI</affiliation></author>
      <author><first>Eric P.</first><last>Xing</last><affiliation>Mohamed bin Zayed Univeristy of AI and School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Huaxiu</first><last>Yao</last><affiliation>Department of Computer Science, University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Qirong</first><last>Ho</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Petuum, Inc.</affiliation></author>
      <pages>159-166</pages>
      <abstract>The computational complexity of large language model (LLM) inference significantly constrains their deployment efficiency on edge devices. In contrast, small language models offer faster decoding and lower resource consumption but often suffer from degraded response quality and heightened susceptibility to hallucinations. To address this trade-off, collaborative decoding, in which a large model assists in generating critical tokens, has emerged as a promising solution. This paradigm leverages the strengths of both model types by enabling high-quality inference through selective intervention of the large model, while maintaining the speed and efficiency of the smaller model. In this work, we present a novel collaborative decoding inference system that allows small models to perform on-device inference while selectively consulting a cloud-based large model for critical token generation. Remarkably, the system achieves a 60% performance gain on CommonsenseQA using only a 0.5B model on an M1 MacBook, with under 7% of tokens generation uploaded to the large model in the cloud.</abstract>
      <url hash="a5fb3e63">2025.acl-demo.16</url>
      <attachment type="copyright_agreement" hash="4a2c894f">2025.acl-demo.16.copyright_agreement.pdf</attachment>
      <bibkey>she-etal-2025-token</bibkey>
    </paper>
    <paper id="17">
      <title>The <fixed-case>S</fixed-case>hare<fixed-case>LM</fixed-case> Collection and Plugin: Contributing Human-Model Chats for the Benefit of the Community</title>
      <author><first>Shachar</first><last>Don-Yehiya</last><affiliation>Hebrew University of Jerusalem and International Business Machines</affiliation></author>
      <author><first>Leshem</first><last>Choshen</last><affiliation>Massachusetts Institute of Technology and International Business Machines</affiliation></author>
      <author><first>Omri</first><last>Abend</last><affiliation>Hebrew University of Jerusalem</affiliation></author>
      <pages>167-177</pages>
      <abstract>Human-model conversations provide a window into users’ real-world scenarios, behavior, and needs, and thus are a valuable resource for model development and research. While for-profit companies collect user data through the APIs of their models, using it internally to improve their own models, the open source and research community lags behind.We introduce the ShareLM collection, a unified set of human conversations with large language models, and its accompanying plugin, a Web extension for voluntarily contributing user-model conversations. Where few platforms share their chats, the ShareLM plugin adds this functionality, thus, allowing users to share conversations from most platforms. The plugin allows the user to rate their conversations, both at the conversation and the response levels, and delete conversations they prefer to keep private before they ever leave the user’s local storage.</abstract>
      <url hash="a974fa3e">2025.acl-demo.17</url>
      <attachment type="copyright_agreement" hash="54552601">2025.acl-demo.17.copyright_agreement.pdf</attachment>
      <bibkey>don-yehiya-etal-2025-sharelm</bibkey>
    </paper>
    <paper id="18">
      <title><fixed-case>OLM</fixed-case>o<fixed-case>T</fixed-case>race: Tracing Language Model Outputs Back to Trillions of Training Tokens</title>
      <author><first>Jiacheng</first><last>Liu</last><affiliation>Allen Institute for Artificial Intelligence and Paul G. Allen School of Computer Science and Engineering, University of Washington</affiliation></author>
      <author><first>Taylor</first><last>Blanton</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Yanai</first><last>Elazar</last><affiliation>Allen Institute for Artificial Intelligence and Department of Computer Science</affiliation></author>
      <author><first>Sewon</first><last>Min</last><affiliation>University of California, Berkeley and Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Yen-Sung</first><last>Chen</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Arnavi</first><last>Chheda-Kothary</last></author>
      <author><first>Huy</first><last>Tran</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Byron</first><last>Bischoff</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Eric</first><last>Marsh</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Michael</first><last>Schmitz</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Cassidy</first><last>Trier</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Aaron</first><last>Sarnat</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Jenna</first><last>James</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Jon</first><last>Borchardt</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Bailey</first><last>Kuehl</last></author>
      <author><first>Evie Yu-Yen</first><last>Cheng</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Karen</first><last>Farley</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Taira</first><last>Anderson</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>David</first><last>Albright</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Carissa</first><last>Schoenick</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Luca</first><last>Soldaini</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Dirk</first><last>Groeneveld</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Rock Yuren</first><last>Pang</last></author>
      <author><first>Pang Wei</first><last>Koh</last><affiliation>Allen Institute for Artificial Intelligence and University of Washington</affiliation></author>
      <author><first>Noah A.</first><last>Smith</last><affiliation>University of Washington and Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Sophie</first><last>Lebrecht</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Yejin</first><last>Choi</last><affiliation>Computer Science Department, Stanford University and NVIDIA</affiliation></author>
      <author><first>Hannaneh</first><last>Hajishirzi</last></author>
      <author><first>Ali</first><last>Farhadi</last></author>
      <author><first>Jesse</first><last>Dodge</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>178-188</pages>
      <abstract>We present OLMoTrace, the first system that traces the outputs of language models back to their full, multi-trillion-token training data in real time. OLMoTrace finds and shows verbatim matches between segments of language model output and documents in the training text corpora. Powered by an extended version of infini-gram (Liu et al., 2024), our system returns tracing results within a few seconds. OLMoTrace can help users understand the behavior of language models through the lens of their training data. We showcase how it can be used to explore fact checking, hallucination, and the creativity of language models. OLMoTrace is publicly available and fully open-source.</abstract>
      <url hash="be4a77a9">2025.acl-demo.18</url>
      <attachment type="copyright_agreement" hash="93592d31">2025.acl-demo.18.copyright_agreement.pdf</attachment>
      <bibkey>liu-etal-2025-olmotrace</bibkey>
    </paper>
    <paper id="19">
      <title><fixed-case>A</fixed-case>uto<fixed-case>A</fixed-case>lign: Get Your <fixed-case>LLM</fixed-case> Aligned with Minimal Annotations</title>
      <author><first>Xinyu</first><last>Lu</last></author>
      <author><first>Dong</first><last>Xu</last></author>
      <author><first>Chunkang</first><last>Zhang</last></author>
      <author><first>Xinyan</first><last>Guan</last></author>
      <author><first>Junxiang</first><last>Wang</last></author>
      <author><first>Qingyu</first><last>Zhang</last><affiliation>University of the Chinese Academy of Sciences</affiliation></author>
      <author><first>Pengbo</first><last>Wang</last></author>
      <author><first>Yingzhi</first><last>Mao</last></author>
      <author><first>Hao</first><last>Xiang</last></author>
      <author><first>Xueru</first><last>Wen</last></author>
      <author><first>Zichao</first><last>Li</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yaojie</first><last>Lu</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Hongyu</first><last>Lin</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Le</first><last>Sun</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xianpei</first><last>Han</last><affiliation>Institute of Software, CAS</affiliation></author>
      <pages>189-198</pages>
      <abstract>Automated Alignment refers to a set of algorithms designed to align Large Language Models (LLMs) with human intentions and values while minimizing manual intervention. However, it faces challenges such as algorithmic diversity and excessively convoluted workflows. We present AutoAlign, an open-source toolkit that offers:(1) a unified framework integrating mainstream automated algorithms through a consistent interface, and(2) an accessible workflow supporting one-click execution for prompt synthesis, automatic alignment signal construction, and iterative model training. Our toolkit enables easy reproduction of existing results through extensive benchmarks and facilitates the development of novel approaches via modular components. It includes implementations for both highly efficient inference and training, as well as low-resource training. By standardizing automated alignment methodologies and providing accessible implementations, AutoAlign lowers the barriers to building customized aligned models and supports academic research.</abstract>
      <url hash="478a7556">2025.acl-demo.19</url>
      <attachment type="copyright_agreement" hash="c33b644c">2025.acl-demo.19.copyright_agreement.pdf</attachment>
      <bibkey>lu-etal-2025-autoalign</bibkey>
    </paper>
    <paper id="20">
      <title>Know-<fixed-case>MRI</fixed-case>: A Knowledge Mechanisms Revealer&amp;Interpreter for Large Language Models</title>
      <author><first>Jiaxiang</first><last>Liu</last></author>
      <author><first>Boxuan</first><last>Xing</last></author>
      <author><first>Chenhao</first><last>Yuan</last></author>
      <author><first>ChenxiangZhang</first><last>ChenxiangZhang</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Di</first><last>Wu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xiusheng</first><last>Huang</last></author>
      <author><first>Haida</first><last>Yu</last></author>
      <author><first>Chuhan</first><last>Lang</last></author>
      <author><first>Pengfei</first><last>Cao</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jun</first><last>Zhao</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <author><first>Kang</first><last>Liu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <pages>199-210</pages>
      <abstract>As large language models (LLMs) continue to advance, there is a growing urgency to enhance the interpretability of their internal knowledge mechanisms. Consequently, many interpretation methods have emerged, aiming to unravel the knowledge mechanisms of LLMs from various perspectives. However, current interpretation methods differ in input data formats and interpreting outputs. The tools integrating these methods are only capable of supporting tasks with specific inputs, significantly constraining their practical applications. To address these challenges, we present an open-source **Know**ledge **M**echanisms **R**evealer&amp;**I**nterpreter (**Know-MRI**) designed to analyze the knowledge mechanisms within LLMs systematically. Specifically, we have developed an extensible core module that can automatically match different input data with interpretation methods and consolidate the interpreting outputs. It enables users to freely choose appropriate interpretation methods based on the inputs, making it easier to comprehensively diagnose the model’s internal knowledge mechanisms from multiple perspectives. Our code is available at https://github.com/nlpkeg/Know-MRI. We also provide a demonstration video on https://youtu.be/NVWZABJ43Bs.</abstract>
      <url hash="331cf328">2025.acl-demo.20</url>
      <attachment type="copyright_agreement" hash="86661f36">2025.acl-demo.20.copyright_agreement.pdf</attachment>
      <bibkey>liu-etal-2025-know</bibkey>
    </paper>
    <paper id="21">
      <title><fixed-case>AI</fixed-case>4<fixed-case>R</fixed-case>eading: <fixed-case>C</fixed-case>hinese Audiobook Interpretation System Based on Multi-Agent Collaboration</title>
      <author><first>Minjiang</first><last>Huang</last></author>
      <author><first>Jipeng</first><last>Qiang</last><affiliation>Yangzhou University</affiliation></author>
      <author><first>Yi</first><last>Zhu</last><affiliation>Yangzhou University</affiliation></author>
      <author><first>Chaowei</first><last>Zhang</last><affiliation>Yangzhou University</affiliation></author>
      <author><first>Xiangyu</first><last>Zhao</last><affiliation>City University of Hong Kong</affiliation></author>
      <author><first>Kui</first><last>Yu</last><affiliation>Hefei University of Technology</affiliation></author>
      <pages>211-220</pages>
      <abstract>Audiobook interpretations are attracting increasing attention, as they provide accessible and in-depth analyses of books that offer readers practical insights and intellectual inspiration. However, their manual creation process remains time-consuming and resource-intensive. To address this challenge, we propose AI4Reading, a multi-agent collaboration system leveraging large language models (LLMs) and speech synthesis technology to generate podcast-like audiobook interpretations. The system is designed to meet three key objectives: accurate content preservation, enhanced comprehensibility, and a logical narrative structure. To achieve these goals, We develop a framework composed of 11 specialized agents—including topic analysts, case analysts, editors, a narrator, and proofreaders—that work in concert to explore themes, extract real-world cases, refine content organization, and synthesize natural spoken language. By comparing expert interpretations with our system’s output, the results show that although AI4Reading still has a gap in speech generation quality, the generated interpretative scripts are simpler and more accurate. The code of AI4Reading is publicly accessible , with a demonstration video available .</abstract>
      <url hash="ec019a3a">2025.acl-demo.21</url>
      <attachment type="copyright_agreement" hash="0d455cc9">2025.acl-demo.21.copyright_agreement.pdf</attachment>
      <bibkey>huang-etal-2025-ai4reading</bibkey>
    </paper>
    <paper id="22">
      <title><fixed-case>DEEP</fixed-case>: an automatic bidirectional translator leveraging an <fixed-case>ASR</fixed-case> for translation of <fixed-case>I</fixed-case>talian sign language</title>
      <author><first>Nicolas</first><last>Tagliabue</last><affiliation>SUPSI - University of Applied Sciences Southern Switzerland</affiliation></author>
      <author><first>Elisa</first><last>Colletti</last><affiliation>SUPSI - University of Applied Sciences Southern Switzerland</affiliation></author>
      <author><first>Francesco Roberto</first><last>Dani</last><affiliation>SUPSI - University of Applied Sciences Southern Switzerland</affiliation></author>
      <author><first>Roberto</first><last>Tedesco</last><affiliation>SUPSI - University of Applied Sciences Southern Switzerland</affiliation></author>
      <author><first>Sonia</first><last>Cenceschi</last></author>
      <author><first>Alessandro</first><last>Trivilini</last><affiliation>SUPSI - University of Applied Sciences Southern Switzerland</affiliation></author>
      <pages>221-229</pages>
      <abstract>DEEP is a bidirectional translation system for the Italian Sign Language, tailored to two specific, common use cases: pharmacies and the registry office of the municipality, for which a custom corpus has been collected. Two independent pipelines permit to create a chatlike interaction style, where the deaf subject just signs in front of a camera, and sees a virtual LIS interpreter, while the hearing subject reads and writes messages into a chat UI. The LIS-to-Italian pipeline leverages, in a novel way, a customized Whisper model (a wellknown speech recognition system), by means of “pseudo-spectrograms”. The Italian-to-LIS pipeline leverages a virtual avatar created with Viggle.ai. DEEP has been evaluated with LIS signers, obtaining very promising results.</abstract>
      <url hash="5af54d02">2025.acl-demo.22</url>
      <attachment type="copyright_agreement" hash="02b5bd3a">2025.acl-demo.22.copyright_agreement.pdf</attachment>
      <bibkey>tagliabue-etal-2025-deep</bibkey>
    </paper>
    <paper id="23">
      <title><fixed-case>V</fixed-case>enus<fixed-case>F</fixed-case>actory: An Integrated System for Protein Engineering with Data Retrieval and Language Model Fine-Tuning</title>
      <author><first>Yang</first><last>Tan</last><affiliation>East China University of Science and Technology</affiliation></author>
      <author><first>Chen</first><last>Liu</last></author>
      <author><first>Jingyuan</first><last>Gao</last></author>
      <author><first>Wu</first><last>Banghao</last></author>
      <author><first>Mingchen</first><last>Li</last></author>
      <author><first>Ruilin</first><last>Wang</last></author>
      <author><first>Lingrong</first><last>Zhang</last></author>
      <author><first>Huiqun</first><last>Yu</last></author>
      <author><first>Guisheng</first><last>Fan</last><affiliation>East China University of Science and Technology</affiliation></author>
      <author><first>Liang</first><last>Hong</last></author>
      <author><first>Bingxin</first><last>Zhou</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <pages>230-241</pages>
      <abstract>Natural language processing (NLP) has significantly influenced scientific domains beyond human language, including protein engineering, where pre-trained protein language models (PLMs) have demonstrated remarkable success. However, interdisciplinary adoption remains limited due to challenges in data collection, task benchmarking, and application. This work presents VenusFactory, a versatile engine that integrates biological data retrieval, standardized task benchmarking, and modular fine-tuning of PLMs. VenusFactory supports both computer science and biology communities with choices of both a command-line execution and a Gradio-based no-code interface, integrating <tex-math>40+</tex-math> protein-related datasets and <tex-math>40+</tex-math> popular PLMs. All implementations are open-sourced on https://github.com/ai4protein/VenusFactory. A video introduction is available at https://www.youtube.com/watch?v=MT6lPH5kgCc.</abstract>
      <url hash="c39f74bd">2025.acl-demo.23</url>
      <attachment type="copyright_agreement" hash="34869cd3">2025.acl-demo.23.copyright_agreement.pdf</attachment>
      <bibkey>tan-etal-2025-venusfactory</bibkey>
    </paper>
    <paper id="24">
      <title><fixed-case>G</fixed-case>en<fixed-case>GO</fixed-case> Ultra: an <fixed-case>LLM</fixed-case>-powered <fixed-case>ACL</fixed-case> Paper Explorer</title>
      <author><first>Sotaro</first><last>Takeshita</last><affiliation>Universität Mannheim</affiliation></author>
      <author><first>Tornike</first><last>Tsereteli</last><affiliation>Universität Mannheim</affiliation></author>
      <author><first>Simone Paolo</first><last>Ponzetto</last><affiliation>Universität Mannheim</affiliation></author>
      <pages>242-251</pages>
      <abstract>The ever-growing number of papers in natural language processing (NLP) poses the challenge of finding relevant papers. In our previous paper, we introduced GenGO, which complements NLP papers with various information, such as aspect-based summaries, to enable efficient paper exploration. While it delivers a better literature search experience, it lacks an interactive interface that dynamically produces information tailored to the user’s needs. To this end, we present an extension to our previous system, dubbed GenGO Ultra, which exploits large language models (LLMs) to dynamically generate responses grounded by published papers. We also conduct multi-granularity experiments to evaluate six text encoders and five LLMs. Our system is designed for transparency – based only on open-weight models, visible system prompts, and an open-source code base – to foster further development and research on top of our system: https://gengo-ultra.sotaro.io/</abstract>
      <url hash="3d41f975">2025.acl-demo.24</url>
      <attachment type="copyright_agreement" hash="f4643e4a">2025.acl-demo.24.copyright_agreement.pdf</attachment>
      <bibkey>takeshita-etal-2025-gengo</bibkey>
    </paper>
    <paper id="25">
      <title><fixed-case>S</fixed-case>patial<fixed-case>W</fixed-case>eb<fixed-case>A</fixed-case>gent: Leveraging Large Language Models for Automated Spatial Information Extraction and Map Grounding</title>
      <author><first>Shunfeng</first><last>Zheng</last></author>
      <author><first>Meng</first><last>Fang</last><affiliation>University of Liverpool and Eindhoven University of Technology</affiliation></author>
      <author><first>Ling</first><last>Chen</last><affiliation>University of Technology Sydney</affiliation></author>
      <pages>252-266</pages>
      <abstract>Understanding and extracting spatial information from text is vital for a wide range of applications, including geographic information systems (GIS), smart cities, disaster prevention, and logistics planning. This capability empowers decision-makers to gain crucial insights into geographic distributions and trends.However, the inherent complexity of geographic expressions in natural language presents significant hurdles for traditional extraction methods. These challenges stem from variations in place names, vague directional cues, and implicit spatial relationships.To address these challenges, we introduce SpatialWebAgent, an automated agent system that leverages large language models (LLMs). SpatialWebAgent is designed to extract, standardize, and ground spatial information from natural language text directly onto maps. Our system excels at handling the diverse and often ambiguous nature of geographic expressions—from varying place names and vague directions to implicit spatial relationships that demand flexible combinations of localization functions—by tapping into the powerful geospatial reasoning capabilities of LLMs. SpatialWebAgent employs a series of specialized tools to convert this extracted information into precise coordinates, which are then visualized on interactive maps.A demonstration of SpatialWebAgent is available at https://sites.google.com/view/SpatialWebAgent.</abstract>
      <url hash="8e2c4022">2025.acl-demo.25</url>
      <attachment type="copyright_agreement" hash="45cf6930">2025.acl-demo.25.copyright_agreement.pdf</attachment>
      <bibkey>zheng-etal-2025-spatialwebagent</bibkey>
    </paper>
    <paper id="26">
      <title><fixed-case>D</fixed-case>oc<fixed-case>S</fixed-case>piral: A Platform for Integrated Assistive Document Annotation through Human-in-the-Spiral</title>
      <author><first>Qiang</first><last>Sun</last><affiliation>University of Western Australia</affiliation></author>
      <author><first>Sirui</first><last>Li</last></author>
      <author><first>Tingting</first><last>Bi</last></author>
      <author><first>Du Q.</first><last>Huynh</last></author>
      <author><first>Mark</first><last>Reynolds</last><affiliation>University of Western Australia</affiliation></author>
      <author><first>Yuanyi</first><last>Luo</last></author>
      <author><first>Wei</first><last>Liu</last><affiliation>University of Western Australia</affiliation></author>
      <pages>267-274</pages>
      <abstract>Acquiring structured data from domain-specific, image-based documents—such as scanned reports—is crucial for many downstream tasks but remains challenging due to document variability. Many of these documents exist as images rather than as machine-readable text, which requires human annotation to train automated extraction systems.We present <b>DocSpiral</b>, the first Human-in-the-Spiral assistive document annotation platform, designed to address the challenge of extracting structured information from domain-specific, image-based document collections.Our spiral design establishes an iterative cycle in which human annotations train models that progressively require less manual intervention. <b>DocSpiral</b> integrates document format normalization, comprehensive annotation interfaces, evaluation metrics dashboard, and API endpoints for the development of AI / ML models into a unified workflow.Experiments demonstrate that our framework reduces annotation time by at least 41% while showing consistent performance gains across three iterations during model training.By making this annotation platform freely accessible, we aim to lower barriers to AI/ML models development in document processing, facilitating the adoption of large language models in image-based, document-intensive fields such as geoscience and healthcare. The system is freely available at: <url>https://app.ai4wa.com</url>. The demonstration video is available: <url>https://app.ai4wa.com/docs/docspiral/demo</url>.</abstract>
      <url hash="9afa4abc">2025.acl-demo.26</url>
      <attachment type="copyright_agreement" hash="5289053c">2025.acl-demo.26.copyright_agreement.pdf</attachment>
      <bibkey>sun-etal-2025-docspiral</bibkey>
    </paper>
    <paper id="27">
      <title><fixed-case>ADEPT</fixed-case>-<fixed-case>SQL</fixed-case>: A High-performance Text-to-<fixed-case>SQL</fixed-case> Application for Real-World Enterprise-Level Databases</title>
      <author><first>Yongnan</first><last>Chen</last><affiliation>Peking University and KLD of CNPC</affiliation></author>
      <author><first>Zhuo</first><last>Chang</last></author>
      <author><first>Shijia</first><last>Gu</last></author>
      <author><first>Yuanhang</first><last>Zong</last><affiliation>KLD</affiliation></author>
      <author><first>Zhang</first><last>Mei</last><affiliation>KLD</affiliation></author>
      <author><first>Shiyu</first><last>Wang</last><affiliation>KLD</affiliation></author>
      <author><first>Hezixiang</first><last>Hezixiang</last><affiliation>CNPC</affiliation></author>
      <author><first>Hongzhi</first><last>Chen</last></author>
      <author><first>Jin</first><last>Wei</last></author>
      <author><first>Bin</first><last>Cui</last><affiliation>Peking University</affiliation></author>
      <pages>275-283</pages>
      <abstract>This paper presents Adept-SQL, a domain-adapted Text2SQL system that addresses critical deployment challenges in professional fields. While modern LLM-based solutions excel on academic benchmarks, we identify three persistent limitations in industrial application: domain-specific knowledge barriers, the schemas complexity in the real world, and the prohibitive computational costs of large LLMs. Our framework introduces two key innovations: a three-stage grounding mechanism combining dynamic terminology expansion, focused schema alignment, and historical query retrieval; coupled with a hybrid prompting architecture that decomposes SQL generation into schema-aware hinting, term disambiguation, and few-shot example incorporation phases. This approach enables efficient execution using smaller open-source LLMs while maintaining semantic precision. Deployed in petroleum engineering domains, our system achieves 97% execution accuracy on real-world databases, demonstrating 49% absolute improvement over SOTA baselines. We release implementation code to advance research in professional Text2SQL systems.</abstract>
      <url hash="7ce46d6b">2025.acl-demo.27</url>
      <attachment type="copyright_agreement" hash="6fd19ac6">2025.acl-demo.27.copyright_agreement.pdf</attachment>
      <bibkey>chen-etal-2025-adept</bibkey>
    </paper>
    <paper id="28">
      <title><fixed-case>LCDS</fixed-case>: A Logic-Controlled Discharge Summary Generation System Supporting Source Attribution and Expert Review</title>
      <author><first>Cheng</first><last>Yuan</last><affiliation>East China University of Science and Technology</affiliation></author>
      <author><first>Xinkai</first><last>Rui</last></author>
      <author><first>Yongqi</first><last>Fan</last></author>
      <author><first>Yawei</first><last>Fan</last></author>
      <author><first>Boyang</first><last>Zhong</last></author>
      <author><first>Jiacheng</first><last>Wang</last></author>
      <author><first>Weiyan</first><last>Zhang</last><affiliation>East China University of Science and Technology</affiliation></author>
      <author><first>Tong</first><last>Ruan</last></author>
      <pages>284-294</pages>
      <abstract>Despite the remarkable performance of Large Language Models (LLMs) in automated discharge summary generation, they still suffer from generating inaccurate content or fabricating information without valid sources. To address these issues, we propose LCDS, a tool for empowering LLMs with Logic-Controlled Discharge Summary generation. LCDS constructs a source mapping table by calculating the textual similarity between electronic medical records (EMRs) and discharge summaries, providing a structured reference for generation. Based on a comprehensive set of logical rules, LCDS identifies the structured writing logic of discharge summaries and integrates it with EMRs to generate silver discharge summaries. Furthermore, LCDS traces the provenance of generated content, allowing experts to review, provide feedback, and rectify errors to produce golden discharge summaries, which are subsequently recorded for the incremental fine-tuning of LLMs.Our project and demo video are in the GitHub repository https://github.com/ycycyc02/LCDS.</abstract>
      <url hash="f8707bd7">2025.acl-demo.28</url>
      <attachment type="copyright_agreement" hash="69fe557d">2025.acl-demo.28.copyright_agreement.pdf</attachment>
      <bibkey>yuan-etal-2025-lcds</bibkey>
    </paper>
    <paper id="29">
      <title>Revealing the Unwritten: Visual Investigation of Beam Search Trees to Address Language Model Prompting Challenges</title>
      <author><first>Thilo</first><last>Spinner</last></author>
      <author><first>Rita</first><last>Sevastjanova</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <author><first>Rebecca</first><last>Kehlbeck</last><affiliation>Universität Konstanz</affiliation></author>
      <author><first>Tobias</first><last>Stähle</last></author>
      <author><first>Daniel A.</first><last>Keim</last><affiliation>Universität Konstanz</affiliation></author>
      <author><first>Oliver</first><last>Deussen</last><affiliation>University of Konstanz</affiliation></author>
      <author><first>Andreas</first><last>Spitz</last><affiliation>Universität Konstanz</affiliation></author>
      <author><first>Mennatallah</first><last>El-Assady</last><affiliation>Department of Computer Science, ETHZ - ETH Zurich</affiliation></author>
      <pages>295-306</pages>
      <abstract>The present popularity of generative language models has amplified interest in interactive methods to guide model outputs. Prompt refinement is considered one of the most effective means to influence output among these methods. We identify several challenges associated with prompting large language models, categorized into data- and model-specific, linguistic, and socio-linguistic challenges. A comprehensive examination of model outputs, including runner-up candidates and their corresponding probabilities, is needed to address these issues. The beam search tree, the prevalent algorithm to sample model outputs, can inherently supply this information. Consequently, we leverage an interactive visual method for investigating the beam search tree, facilitating analysis of the decisions made by the model during generation. Our explorative approach validates existing results and offers additional insights.</abstract>
      <url hash="7badbe2d">2025.acl-demo.29</url>
      <attachment type="copyright_agreement" hash="13c0716d">2025.acl-demo.29.copyright_agreement.pdf</attachment>
      <bibkey>spinner-etal-2025-revealing</bibkey>
    </paper>
    <paper id="30">
      <title>Scholar Inbox: Personalized Paper Recommendations for Scientists</title>
      <author><first>Markus</first><last>Flicke</last></author>
      <author><first>Glenn</first><last>Angrabeit</last></author>
      <author><first>Madhav</first><last>Iyengar</last><affiliation>Eberhard-Karls-Universität Tübingen</affiliation></author>
      <author><first>Vitalii</first><last>Protsenko</last></author>
      <author><first>Illia</first><last>Shakun</last></author>
      <author><first>Jovan</first><last>Cicvaric</last></author>
      <author><first>Bora</first><last>Kargi</last><affiliation>Eberhard-Karls-Universität Tübingen</affiliation></author>
      <author><first>Haoyu</first><last>He</last><affiliation>Eberhard-Karls-Universität Tübingen</affiliation></author>
      <author><first>Lukas</first><last>Schuler</last></author>
      <author><first>Lewin</first><last>Scholz</last></author>
      <author><first>Kavyanjali</first><last>Agnihotri</last></author>
      <author><first>Yong</first><last>Cao</last></author>
      <author><first>Andreas</first><last>Geiger</last><affiliation>University of Tuebingen</affiliation></author>
      <pages>307-317</pages>
      <abstract>Scholar Inbox is a new open-access platform designed to address the challenges researchers face in staying current with the rapidly expanding volume of scientific literature. We provide personalized recommendations, continuous updates from open-access archives (arXiv, bioRxiv, etc.), visual paper summaries, semantic search, and a range of tools to streamline research workflows and promote open research access. The platform’s personalized recommendation system is trained on user ratings, ensuring that recommendations are tailored to individual researchers’ interests. To further enhance the user experience, Scholar Inbox also offers a map of science that provides an overview of research across domains, enabling users to easily explore specific topics. We use this map to address the cold start problem common in recommender systems, as well as an active learning strategy that iteratively prompts users to rate a selection of papers, allowing the system to learn user preferences quickly. We evaluate the quality of our recommendation system on a novel dataset of 800k user ratings, which we make publicly available, as well as via an extensive user study.</abstract>
      <url hash="eba2f672">2025.acl-demo.30</url>
      <attachment type="copyright_agreement" hash="c1f1e8a2">2025.acl-demo.30.copyright_agreement.pdf</attachment>
      <bibkey>flicke-etal-2025-scholar</bibkey>
    </paper>
    <paper id="31">
      <title>The Open Argument Mining Framework</title>
      <author><first>Debela</first><last>Gemechu</last></author>
      <author><first>Ramon</first><last>Ruiz-Dolz</last><affiliation>University of Dundee</affiliation></author>
      <author><first>Kamila</first><last>Górska</last><affiliation>University of Dundee</affiliation></author>
      <author><first>Somaye</first><last>Moslemnejad</last></author>
      <author><first>Eimear</first><last>Maguire</last><affiliation>University of Dundee</affiliation></author>
      <author><first>Dimitra</first><last>Zografistou</last><affiliation>Independent</affiliation></author>
      <author><first>Yohan</first><last>Jo</last><affiliation>Seoul National University</affiliation></author>
      <author><first>John</first><last>Lawrence</last><affiliation>University of Dundee</affiliation></author>
      <author><first>Chris</first><last>Reed</last><affiliation>University of Dundee</affiliation></author>
      <pages>318-328</pages>
      <abstract>Despite extensive research in Argument Mining (AM), the field faces significant challenges in limited reproducibility, difficulty in comparing systems due to varying task combinations, and a lack of interoperability caused by the heterogeneous nature of argumentation theory. These challenges are further exacerbated by the absence of dedicated tools, with most advancements remaining isolated research outputs rather than reusable systems. The <tex-math>\texttt{oAMF}</tex-math> (Open Argument Mining Framework) addresses these issues by providing an open-source, modular, and scalable platform that unifies diverse AM methods. Initially released with seventeen integrated modules, the <tex-math>\texttt{oAMF}</tex-math> serves as a starting point for researchers and developers to build, experiment with, and deploy AM pipelines while ensuring interoperability and allowing multiple theories of argumentation to co-exist within the same framework. Its flexible design supports integration via Python APIs, drag-and-drop tools, and web interfaces, streamlining AM development for research and industry setup, facilitating method comparison, and reproducibility.</abstract>
      <url hash="7ed4660b">2025.acl-demo.31</url>
      <attachment type="copyright_agreement" hash="dff60a73">2025.acl-demo.31.copyright_agreement.pdf</attachment>
      <bibkey>gemechu-etal-2025-open</bibkey>
    </paper>
    <paper id="32">
      <title>Bel Esprit: Multi-Agent Framework for Building <fixed-case>AI</fixed-case> Model Pipelines</title>
      <author><first>Yunsu</first><last>Kim</last><affiliation>aiXplain, Inc.</affiliation></author>
      <author><first>Ahmedelmogtaba</first><last>Abdelaziz</last></author>
      <author><first>Thiago</first><last>Castro Ferreira</last><affiliation>Universidade Federal de Minas Gerais</affiliation></author>
      <author><first>Mohamed</first><last>Al-Badrashiny</last><affiliation>aiXplain</affiliation></author>
      <author><first>Hassan</first><last>Sawaf</last><affiliation>aiXplain</affiliation></author>
      <pages>329-339</pages>
      <abstract>As the demand for artificial intelligence (AI) grows to address complex real-world tasks, single models are often insufficient, requiring the integration of multiple models into pipelines. This paper introduces Bel Esprit, a conversational agent designed to construct AI model pipelines based on user requirements. Bel Esprit uses a multi-agent framework where subagents collaborate to clarify requirements, build, validate, and populate pipelines with appropriate models. We demonstrate its effectiveness in generating pipelines from ambiguous user queries, using both human-curated and synthetic data. A detailed error analysis highlights ongoing challenges in pipeline building.Bel Esprit is available for a free trial at https://belesprit.aixplain.com.</abstract>
      <url hash="38c96510">2025.acl-demo.32</url>
      <attachment type="copyright_agreement" hash="6e98eb52">2025.acl-demo.32.copyright_agreement.pdf</attachment>
      <bibkey>kim-etal-2025-bel</bibkey>
    </paper>
    <paper id="33">
      <title><fixed-case>Z</fixed-case>ero<fixed-case>S</fixed-case>um<fixed-case>E</fixed-case>val: An Extensible Framework For Scaling <fixed-case>LLM</fixed-case> Evaluation with Inter-Model Competition</title>
      <author><first>Hisham Abdullah</first><last>Alyahya</last><affiliation>Saudi Data and AI Authority</affiliation></author>
      <author><first>Haidar</first><last>Khan</last><affiliation>Facebook</affiliation></author>
      <author><first>Yazeed</first><last>Alnumay</last><affiliation>Cohere</affiliation></author>
      <author><first>M Saiful</first><last>Bari</last><affiliation>National Centre of Artificial Intelligence, Saudi Data and AI Authority</affiliation></author>
      <author><first>Bulent</first><last>Yener</last><affiliation>Rensselaer Polytechnic Institute</affiliation></author>
      <pages>340-350</pages>
      <abstract>We introduce ZeroSumEval, a dynamic, competition-based, and evolving evaluation framework for Large Language Models (LLMs) that leverages competitive games. ZeroSumEval encompasses a diverse suite of games, including security challenges (Capture the Flag), classic board games (chess), and knowledge tests (MathQuiz). These games are designed to evaluate a range of capabilities such as strategic reasoning, planning, knowledge application, safety, and adaptability. Building upon recent studies that highlight the effectiveness of game-based evaluations for LLMs, ZeroSumEval enhances these approaches by providing a standardized and extensible framework for easily implementing games and leverages DSPy to provide a better abstraction for LLM player strategies.</abstract>
      <url hash="e9711eb3">2025.acl-demo.33</url>
      <attachment type="copyright_agreement" hash="7434ff7e">2025.acl-demo.33.copyright_agreement.pdf</attachment>
      <bibkey>alyahya-etal-2025-zerosumeval</bibkey>
    </paper>
    <paper id="34">
      <title><fixed-case>DECAF</fixed-case>: A Dynamically Extensible Corpus Analysis Framework</title>
      <author><first>Max</first><last>Müller-Eberstein</last><affiliation>IT University of Copenhagen</affiliation></author>
      <author><first>Rob Van Der</first><last>Goot</last><affiliation>IT University of Copenhagen</affiliation></author>
      <author><first>Anna</first><last>Rogers</last><affiliation>IT University of Copenhagen</affiliation></author>
      <pages>351-362</pages>
      <abstract>The study of generalization in Language Models (LMs) requires controlled experiments that can precisely measure complex linguistic variations between training and testing datasets. We introduce DECAF, a framework that enables the analysis and filtering of linguistically-annotated datasets down to the character level. Rather than creating new resources for each experiment, DECAF starts from datasets with existing linguistic annotations, and leverages them to analyze, filter, and generate highly controlled and reproducible experimental settings targeting specific research questions. We demonstrate DECAF’s functionality by adding 28 morphosyntactic annotation layers to the 115M-word BabyLM corpus and indexing the resulting 1.1B annotations to analyze its internal domain variance, and to create a controlled training data curriculum for a small-scale gender bias study. We release DECAF as an open-source Python library, along with the parsed and indexed version of BabyLM, as resources for future generalization research.</abstract>
      <url hash="bbd062c1">2025.acl-demo.34</url>
      <attachment type="copyright_agreement" hash="10a609e7">2025.acl-demo.34.copyright_agreement.pdf</attachment>
      <bibkey>muller-eberstein-etal-2025-decaf</bibkey>
    </paper>
    <paper id="35">
      <title>Dialz: A Python Toolkit for Steering Vectors</title>
      <author><first>Zara</first><last>Siddique</last></author>
      <author><first>Liam</first><last>Turner</last><affiliation>Cardiff University</affiliation></author>
      <author><first>Luis</first><last>Espinosa-Anke</last><affiliation>Cardiff University and AMPLYFI</affiliation></author>
      <pages>363-375</pages>
      <abstract>We introduce *Dialz*, a Python library for advancing research on steering vectors for open-source LMs. Steering vectors allow users to modify activations at inference time to amplify or weaken a ‘concept’, e.g. honesty or positivity, providing a more powerful alternative to prompting or fine-tuning. Dialz supports a diverse set of tasks, including creating contrastive pair datasets, computing and applying steering vectors, and visualizations. Unlike existing libraries, Dialz emphasizes modularity and usability, enabling both rapid prototyping and in-depth analysis. We demonstrate how Dialz can be used to reduce harmful outputs such as stereotypes, while also providing insights into model behaviour across different layers. We release Dialz with full documentation, tutorials, and support for popular open-source models to encourage further research in safe and controllable language generation. Dialz enables faster research cycles and facilitates insights into model interpretability, paving the way for safer, more transparent, and more reliable AI systems.</abstract>
      <url hash="a24e73be">2025.acl-demo.35</url>
      <attachment type="copyright_agreement" hash="cff7798f">2025.acl-demo.35.copyright_agreement.pdf</attachment>
      <bibkey>siddique-etal-2025-dialz</bibkey>
    </paper>
    <paper id="36">
      <title><fixed-case>FORG</fixed-case>3<fixed-case>D</fixed-case>: Flexible Object Rendering for Generating Vision-Language Spatial Reasoning Data from 3<fixed-case>D</fixed-case> Scenes</title>
      <author><first>Oscar</first><last>Pang</last><affiliation>Vector Institute and University of Toronto, Scarborough</affiliation></author>
      <author><first>Freda</first><last>Shi</last><affiliation>University of Waterloo and Vector Institute</affiliation></author>
      <pages>376-384</pages>
      <abstract>We introduce FORG3D, a 3D rendering toolkit developed with Blender and Python, which synthesizes vision-language data for two primary purposes: (1) supporting human cognitive experiments that require fine-grained control over material and (2) analyzing and improving the visual reasoning capabilities of large vision-language models. The toolkit provides flexible and precise control over object placement, orientation, inter-object distances, and camera configurations while automatically generating detailed spatial metadata. Additionally, it includes a built-in feature for integrating AI-generated backgrounds, enhancing the realism of synthetic scenes. FORG3D is publicly available at https://github.com/compling-wat/FORG3D, and a video demonstration is available at https://www.youtube.com/watch?v=QvIqib_PU8A.</abstract>
      <url hash="c067ee37">2025.acl-demo.36</url>
      <attachment type="copyright_agreement" hash="e772c41d">2025.acl-demo.36.copyright_agreement.pdf</attachment>
      <bibkey>pang-shi-2025-forg3d</bibkey>
    </paper>
    <paper id="37">
      <title><fixed-case>RT</fixed-case>-<fixed-case>VC</fixed-case>: Real-Time Zero-Shot Voice Conversion with Speech Articulatory Coding</title>
      <author><first>Yisi</first><last>Liu</last></author>
      <author><first>Chenyang</first><last>Wang</last></author>
      <author><first>Hanjo</first><last>Kim</last></author>
      <author><first>Raniya</first><last>Khan</last></author>
      <author><first>Gopala</first><last>Anumanchipalli</last><affiliation>University of California, Berkeley</affiliation></author>
      <pages>385-393</pages>
      <abstract>Voice conversion has emerged as a pivotal technology in numerous applications ranging from assistive communication to entertainment. In this paper, we present RT-VC, a zero-shot real-time voice conversion system that delivers ultra-low latency and high-quality performance. Our approach leverages an articulatory feature space to naturally disentangle content and speaker characteristics, facilitating more robust and interpretable voice transformations. Additionally, the integration of differentiable digital signal processing (DDSP) enables efficient vocoding directly from articulatory features, significantly reducing conversion latency. Experimental evaluations demonstrate that, while maintaining synthesis quality comparable to the current state-of-the-art (SOTA) method, RT-VC achieves a CPU latency of 61.4 ms, representing a 13.3% reduction in latency.</abstract>
      <url hash="3a44e0a6">2025.acl-demo.37</url>
      <attachment type="copyright_agreement" hash="8c698e45">2025.acl-demo.37.copyright_agreement.pdf</attachment>
      <bibkey>liu-etal-2025-rt</bibkey>
    </paper>
    <paper id="38">
      <title>Live Football Commentary System Providing Background Information</title>
      <author><first>Yuichiro</first><last>Mori</last></author>
      <author><first>Chikara</first><last>Tanaka</last></author>
      <author><first>Aru</first><last>Maekawa</last><affiliation>Tokyo Institute of Technology, Tokyo Institute of Technology</affiliation></author>
      <author><first>Satoshi</first><last>Kosugi</last><affiliation>Tokyo Institute of Technology and Institute of Science Tokyo</affiliation></author>
      <author><first>Tatsuya</first><last>Ishigaki</last><affiliation>AIST, National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Kotaro</first><last>Funakoshi</last><affiliation>Institute of Science Tokyo</affiliation></author>
      <author><first>Hiroya</first><last>Takamura</last><affiliation>AIST, National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Manabu</first><last>Okumura</last><affiliation>Institute of Science Tokyo and Tokyo Institute of Technology, Tokyo Institute of Technology</affiliation></author>
      <pages>394-404</pages>
      <abstract>Previous research on sports commentary generation has primarily focused on describing major events in the match.However, real-world commentary often includes comments beyond what is visible in the video content, e.g., “Florentina has acquired him for 7 million euros.”For enhancing the viewing experience with such background information,we developed an audio commentary system for football matches that generates utterances with background information, as well as play-by-play commentary.Our system first extracts visual information, and determines whether it is an appropriate timing to produce an utterance.Then it decides which type of utterance to generate: play-by-play or background information. In the latter case, the system leverages external knowledge through retrieval-augmented generation.</abstract>
      <url hash="08030500">2025.acl-demo.38</url>
      <attachment type="copyright_agreement" hash="6c79d3fb">2025.acl-demo.38.copyright_agreement.pdf</attachment>
      <bibkey>mori-etal-2025-live</bibkey>
    </paper>
    <paper id="39">
      <title><fixed-case>G</fixed-case>reater<fixed-case>P</fixed-case>rompt: A Unified, Customizable, and High-Performing Open-Source Toolkit for Prompt Optimization</title>
      <author><first>Wenliang</first><last>Zheng</last></author>
      <author><first>Sarkar Snigdha Sarathi</first><last>Das</last></author>
      <author><first>Yusen</first><last>Zhang</last></author>
      <author><first>Rui</first><last>Zhang</last><affiliation>Pennsylvania State University</affiliation></author>
      <pages>405-415</pages>
      <abstract>LLMs have gained immense popularity among researchers and the general public for its impressive capabilities on a variety of tasks. Notably, the efficacy of LLMs remains significantly dependent on the quality and structure of the input prompts, making prompt design a critical factor for their performance. Recent advancements in automated prompt optimization have introduced diverse techniques that automatically enhance prompts to better align model outputs with user expectations. However, these methods often suffer from the lack of standardization and compatibility across different techniques, limited flexibility in customization, inconsistent performance across model scales, and they often exclusively rely on expensive proprietary LLM APIs. To fill in this gap, we introduce GreaterPrompt, a novel framework that democratizes prompt optimization by unifying diverse methods under a unified, customizable API while delivering highly effective prompts for different tasks. Our framework flexibly accommodates various model scales by leveraging both text feedback-based optimization for larger LLMs and internal gradient-based optimization for smaller models to achieve powerful and precise prompt improvements. Moreover, we provide a user-friendly Web UI that ensures accessibility for non-expert users, enabling broader adoption and enhanced performance across various user groups and application scenarios. GreaterPrompt is available at https://github.com/psunlpgroup/GreaterPrompt via GitHub, PyPI, and web user interfaces.</abstract>
      <url hash="014fb3f3">2025.acl-demo.39</url>
      <attachment type="copyright_agreement" hash="334cc1e4">2025.acl-demo.39.copyright_agreement.pdf</attachment>
      <bibkey>zheng-etal-2025-greaterprompt</bibkey>
    </paper>
    <paper id="40">
      <title>i<fixed-case>PET</fixed-case>: An Interactive Emotional Companion Dialogue System with <fixed-case>LLM</fixed-case>-Powered Virtual Pet World Simulation</title>
      <author><first>Zheyong</first><last>Xie</last></author>
      <author><first>Shaosheng</first><last>Cao</last></author>
      <author><first>Zuozhu</first><last>Liu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Zheyu</first><last>Ye</last><affiliation>Xiaohongshu Inc</affiliation></author>
      <author><first>Zihan</first><last>Niu</last></author>
      <author><first>Chonggang</first><last>Lu</last><affiliation>Xiaohongshu</affiliation></author>
      <author><first>Tong</first><last>Xu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Enhong</first><last>Chen</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Zhe</first><last>Xu</last></author>
      <author><first>Yao</first><last>Hu</last><affiliation>Xiaohongshu</affiliation></author>
      <author><first>Wei</first><last>Lu</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <pages>416-425</pages>
      <abstract>The rapid advancement of large language models (LLMs) has unlocked transformative potential for role-playing emotional companion products, enabling systems that support emotional well-being, educational development, and therapeutic applications. However, existing approaches often lack sustained personalization and contextual adaptability, limiting their effectiveness in real-world settings. In this paper, we introduce iPET, an LLM-powered virtual pet agent designed to enhance user engagement through rich, dynamic pet behaviors and interactions tailored to individual preferences. iPET comprises three core components: a dialogue module that instantiates virtual pet agents for emotionally interactive conversations; a memory module that stores and synthesizes records of both agent and user experiences; and a world simulation module that generates diverse, preference-driven pet behaviors guided by high-level reflections. Deployed for over 200 days in a real-world, non-commercial product, iPET has served millions of users – providing emotional support to psychologically distressed individuals and demonstrating its effectiveness in practical applications.</abstract>
      <url hash="7b60d01a">2025.acl-demo.40</url>
      <attachment type="copyright_agreement" hash="76bb7be8">2025.acl-demo.40.copyright_agreement.pdf</attachment>
      <bibkey>xie-etal-2025-ipet</bibkey>
    </paper>
    <paper id="41">
      <title><fixed-case>L</fixed-case>i<fixed-case>DARR</fixed-case>: Linking Document <fixed-case>AMR</fixed-case>s with Referents Resolvers</title>
      <author><first>Jon</first><last>Cai</last></author>
      <author><first>Kristin</first><last>Wright-Bettner</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <author><first>Zekun</first><last>Zhao</last><affiliation>University of California, Santa Cruz</affiliation></author>
      <author><first>Shafiuddin Rehan</first><last>Ahmed</last></author>
      <author><first>Abijith Trichur</first><last>Ramachandran</last></author>
      <author><first>Jeffrey</first><last>Flanigan</last><affiliation>University of California, Santa Cruz</affiliation></author>
      <author><first>Martha</first><last>Palmer</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <author><first>James</first><last>Martin</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <pages>426-435</pages>
      <abstract>In this paper, we present LiDARR (**Li**nking **D**ocument **A**MRs with **R**eferents **R**esolvers), a web tool for semantic annotation at the document level using the formalism of Abstract Meaning Representation (AMR). LiDARR streamlines the creation of comprehensive knowledge graphs from natural language documents through semantic annotation. The tool features a visualization and interactive user interface, transforming document-level AMR annotation into an models-facilitated verification process. This is achieved through the integration of an AMR-to-surface alignment model and a coreference resolution model. Additionally, we incorporate PropBank rolesets into LiDARR to extend implicit roles in annotated AMR, allowing implicit roles to be linked through the coreference chains via AMRs.</abstract>
      <url hash="2cad24b8">2025.acl-demo.41</url>
      <attachment type="copyright_agreement" hash="a035d995">2025.acl-demo.41.copyright_agreement.pdf</attachment>
      <bibkey>cai-etal-2025-lidarr</bibkey>
    </paper>
    <paper id="42">
      <title><fixed-case>S</fixed-case>lim<fixed-case>LM</fixed-case>: An Efficient Small Language Model for On-Device Document Assistance</title>
      <author><first>Thang M.</first><last>Pham</last></author>
      <author><first>Phat T.</first><last>Nguyen</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Seunghyun</first><last>Yoon</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Viet Dac</first><last>Lai</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Trung</first><last>Bui</last><affiliation>Adobe Research</affiliation></author>
      <pages>436-447</pages>
      <abstract>While small language models (SLMs) show promises for mobile deployment, their real world performance and applications on smartphones remain underexplored. We present SlimLM, a series of SLMs optimized for document assistance tasks on mobile devices. Through extensive experiments on a Samsung Galaxy S24, we identify the sweet spot between model size (ranging from 125M to 8B parameters), context length, and inference time for efficient on-device processing. SlimLM is pretrained on SlimPajama-627B and fine-tuned on DocAssist, our constructed dataset for summarization, question answering, and suggestion tasks. Our smallest model demonstrates efficient performance on S24, while larger variants offer enhanced capabilities within mobile constraints. We evaluate SlimLM against existing SLMs, showing comparable or superior performance and offering a benchmark for future research in on-device language models. We provide an Android application allowing users to experience SlimLM’s document assistance capabilities, offering valuable insights for mobile developers, researchers, and companies seeking privacy-preserving on-device alternatives to server-based language models.</abstract>
      <url hash="53901f14">2025.acl-demo.42</url>
      <attachment type="copyright_agreement" hash="1c86a412">2025.acl-demo.42.copyright_agreement.pdf</attachment>
      <bibkey>pham-etal-2025-slimlm</bibkey>
    </paper>
    <paper id="43">
      <title><fixed-case>V</fixed-case>eri<fixed-case>M</fixed-case>inder: Mitigating Analytical Vulnerabilities in <fixed-case>NL</fixed-case>2<fixed-case>SQL</fixed-case></title>
      <author><first>Shubham</first><last>Mohole</last></author>
      <author><first>Sainyam</first><last>Galhotra</last><affiliation>Cornell University and Department of Computer Science, University of Massachusetts, Amherst</affiliation></author>
      <pages>448-459</pages>
      <abstract>Application systems using natural language interfaces to databases (NLIDBs) have democratized data analysis. This positive development has also brought forth an urgent challenge to help users who might use these systems without a background in statistical analysis to formulate bias-free analytical questions. Although significant research has focused on text-to-SQL generation accuracy, addressing cognitive biases in analytical questions remains underexplored. We present [VeriMinder](https://veriminder.ai), an interactive system for detecting and mitigating such analytical vulnerabilities. Our approach introduces three key innovations: (1) a contextual semantic mapping framework for biases relevant to specific analysis contexts (2) an analytical framework that operationalizes the Hard-to-Vary principle and guides users in systematic data analysis (3) an optimized LLM-powered system that generates high-quality, task-specific prompts using a structured process involving multiple candidates, critic feedback, and self-reflection.User testing confirms the merits of our approach. In direct user experience evaluation, 82.5% participants reported positively impacting the quality of the analysis. In comparative evaluation, VeriMinder scored significantly higher than alternative approaches, at least 20% better when considered for metrics of the analysis’s concreteness, comprehensiveness, and accuracy. Our system, implemented as a web application, is set to help users avoid “wrong question” vulnerability during data analysis. VeriMinder [code base](https://reproducibility.link/veriminder) with prompts is available as an MIT-licensed open-source software to facilitate further research and adoption within the community.</abstract>
      <url hash="d8d5059f">2025.acl-demo.43</url>
      <attachment type="copyright_agreement" hash="5f7f7459">2025.acl-demo.43.copyright_agreement.pdf</attachment>
      <bibkey>mohole-galhotra-2025-veriminder</bibkey>
    </paper>
    <paper id="44">
      <title><fixed-case>D</fixed-case>oc<fixed-case>A</fixed-case>gent: A Multi-Agent System for Automated Code Documentation Generation</title>
      <author><first>Dayu</first><last>Yang</last></author>
      <author><first>Antoine</first><last>Simoulin</last><affiliation>Meta AI</affiliation></author>
      <author><first>Xin</first><last>Qian</last><affiliation>Meta Platforms, Inc.</affiliation></author>
      <author><first>Xiaoyi</first><last>Liu</last><affiliation>Northwestern University</affiliation></author>
      <author><first>Yuwei</first><last>Cao</last><affiliation>Google</affiliation></author>
      <author><first>Zhaopu</first><last>Teng</last><affiliation>Facebook</affiliation></author>
      <author><first>Grey</first><last>Yang</last><affiliation>Meta Platforms, Inc</affiliation></author>
      <pages>460-471</pages>
      <abstract>High-quality code documentation is crucial for software development especially in the era of AI. However, generating it automatically using Large Language Models (LLMs) remains challenging, as existing approaches often produce incomplete, unhelpful, or factually incorrect outputs. We introduce DocAgent, a novel multi-agent collaborative system using topological code processing for incremental context building. Specialized agents (Reader, Searcher, Writer, Verifier, Orchestrator) then collaboratively generate documentation. We also propose a multi-faceted evaluation framework assessing Completeness, Helpfulness, and Truthfulness. Comprehensive experiments show DocAgent significantly outperforms baselines consistently. Our ablation study confirms the vital role of the topological processing order. DocAgent offers a robust approach for reliable code documentation generation in complex and proprietary repositories.</abstract>
      <url hash="d3909ec1">2025.acl-demo.44</url>
      <attachment type="copyright_agreement" hash="7b26c09b">2025.acl-demo.44.copyright_agreement.pdf</attachment>
      <bibkey>yang-etal-2025-docagent</bibkey>
    </paper>
    <paper id="45">
      <title><fixed-case>DISPUT</fixed-case>ool 3.0: Fallacy Detection and Repairing in Argumentative Political Debates</title>
      <author><first>Pierpaolo</first><last>Goffredo</last><affiliation>Université de Nice-Sophia Antipolis</affiliation></author>
      <author><first>Deborah</first><last>Dore</last></author>
      <author><first>Elena</first><last>Cabrio</last><affiliation>Université Côte d’Azur</affiliation></author>
      <author><first>Serena</first><last>Villata</last><affiliation>CNRS</affiliation></author>
      <pages>472-480</pages>
      <abstract>This paper introduces and evaluates a novel web-based application designed to identify and repair fallacious arguments in political debates. DISPUTool 3.0 offers a comprehensive tool for argumentation analysis of political debate, integrating state-of-the-art natural language processing techniques to mine and classify argument components and relations. DISPUTool 3.0 builds on the <tex-math>\textit{ElecDeb60to20}</tex-math> dataset, covering US presidential debates from 1960 to 2020. In this paper, we introduce a novel task which is integrated as a new module in DISPUTool, i.e., the automatic detection and classification of fallacious arguments, and the automatic <tex-math>\textit{repairing}</tex-math> of such misleading arguments. The goal is to show to the user a tool which not only identifies fallacies in political debates, but it also shows how the argument looks like once the veil of fallacy falls down. An extensive evaluation of the module is addressed employing both automated metrics and human assessments. With the inclusion of this module, DISPUTool 3.0 advances even more user critical thinking in front of the augmenting spread of such nefarious kind of content in political debates and beyond. The tool is publicly available here: https://3ia-demos.inria.fr/disputool/</abstract>
      <url hash="81b8892e">2025.acl-demo.45</url>
      <attachment type="copyright_agreement" hash="89b8af74">2025.acl-demo.45.copyright_agreement.pdf</attachment>
      <bibkey>goffredo-etal-2025-disputool</bibkey>
    </paper>
    <paper id="46">
      <title><fixed-case>M</fixed-case>ed<fixed-case>D</fixed-case>ec<fixed-case>X</fixed-case>tract: A Clinician-Support System for Extracting, Visualizing, and Annotating Medical Decisions in Clinical Narratives</title>
      <author><first>Mohamed</first><last>Elgaar</last><affiliation>University of Massachusetts, Lowell</affiliation></author>
      <author><first>Hadi</first><last>Amiri</last><affiliation>University of Massachusetts Lowell</affiliation></author>
      <author><first>Mitra</first><last>Mohtarami</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Leo Anthony</first><last>Celi</last><affiliation>Massachusetts Institute of Technology and Beth Israel Deaconess Medical Center</affiliation></author>
      <pages>481-489</pages>
      <abstract>Clinical notes contain crucial information about medical decisions, including diagnosis, treatment choices, and follow-up plans. However, these decisions are embedded within unstructured text, making it challenging to systematically analyze decision-making patterns or support clinical workflows. We present MedDecXtract, an open-source interactive system that automatically extracts and visualizes medical decisions from clinical text. The system combines a RoBERTa-based model for identifying ten categories of medical decisions (e.g., diagnosis, treatment, follow-up) according to the DICTUM framework, with an intuitive interface for exploration, visualization, and annotation. The system enables various applications including clinical decision support, research on decision patterns, and creation of training data for improved medical language models. The system and its source code can be accessed at https://mohdelgaar-clinical-decisions.hf.space. A video demo is available at https://youtu.be/19j6-XtIE_s.</abstract>
      <url hash="e33052b7">2025.acl-demo.46</url>
      <attachment type="copyright_agreement" hash="da6da589">2025.acl-demo.46.copyright_agreement.pdf</attachment>
      <bibkey>elgaar-etal-2025-meddecxtract</bibkey>
    </paper>
    <paper id="47">
      <title><fixed-case>C</fixed-case>ite<fixed-case>L</fixed-case>ab: Developing and Diagnosing <fixed-case>LLM</fixed-case> Citation Generation Workflows via the Human-<fixed-case>LLM</fixed-case> Interaction</title>
      <author><first>Jiajun</first><last>Shen</last></author>
      <author><first>Tong</first><last>Zhou</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <author><first>Yubo</first><last>Chen</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <author><first>Kang</first><last>Liu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jun</first><last>Zhao</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <pages>490-501</pages>
      <abstract>The emerging paradigm of enabling Large Language Models (LLMs) to generate citations in Question-Answering (QA) tasks is lacking in a unified framework to standardize and fairly compare different citation generation methods, leading to difficulties in reproduction and innovation. Therefore, we introduce Citeflow, an open-source and modular framework fostering reproduction and the implementation of new designs. Citeflow is highly extensible, allowing users to utilize four main modules and 14 components to construct a pipeline, evaluate an existing method, and understand the attributing LLM-generated contents. The framework is also paired with a visual interface, Citefix, facilitating case study and modification of existing citation generation methods. Users can use this interface to conduct LLM-powered case studies according to different scenarios. Citeflow and Citefix are highly integrated into the toolkit CiteLab, and we use an authentic process of multiple rounds of improvement through the Human-LLM interaction interface to demonstrate the efficiency of our toolkit on implementing and modifying citation generation pipelines. Citelab is released at https://github.com/SjJ1017/Citelab</abstract>
      <url hash="3f823d63">2025.acl-demo.47</url>
      <attachment type="copyright_agreement" hash="3d7c4638">2025.acl-demo.47.copyright_agreement.pdf</attachment>
      <bibkey>shen-etal-2025-citelab</bibkey>
    </paper>
    <paper id="48">
      <title><fixed-case>C</fixed-case>ode<fixed-case>A</fixed-case>rena: A Collective Evaluation Platform for <fixed-case>LLM</fixed-case> Code Generation</title>
      <author><first>Mingzhe</first><last>Du</last><affiliation>Nanyang Technological University and National University of Singapore</affiliation></author>
      <author><first>Anh Tuan</first><last>Luu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Bin</first><last>Ji</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Xiaobao</first><last>Wu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Yuhao</first><last>Qing</last><affiliation>The University of Hong Kong</affiliation></author>
      <author><first>Dong</first><last>Huang</last></author>
      <author><first>Terry Yue</first><last>Zhuo</last><affiliation>Commonwealth Scientific and Industrial Research Organisation, CSIRO</affiliation></author>
      <author><first>Qian</first><last>Liu</last><affiliation>Tiktok</affiliation></author>
      <author><first>See-Kiong</first><last>Ng</last><affiliation>National University of Singapore</affiliation></author>
      <pages>502-512</pages>
      <abstract>Large Language Models (LLMs) have reshaped code generation by synergizing their exceptional comprehension of natural language and programming syntax, thereby substantially boosting developer productivity. These advancements have prompted numerous efforts to quantitatively evaluate their coding capabilities. However, persistent challenges, such as benchmark leakage, data dissipation, and limited system accessibility, continue to impede a timely and accurate assessment. To address these limitations, we introduce CodeArena, an online evaluation framework tailored for LLM code generation. Its key innovation is a collective evaluation mechanism, which dynamically recalibrates individual model scores based on the holistic performance of all participating models, mitigating score biases caused by widespread benchmark leakage. In addition, CodeArena ensures open access to all submitted solutions and test cases and provides automation-friendly APIs to streamline the code evaluation workflow. Our main contributions are: (1) a collective evaluation system for unbiased assessment, (2) a public repository of solutions and test cases, and (3) automation-ready APIs for seamless integration.</abstract>
      <url hash="b9d340d4">2025.acl-demo.48</url>
      <attachment type="copyright_agreement" hash="efd65c9d">2025.acl-demo.48.copyright_agreement.pdf</attachment>
      <bibkey>du-etal-2025-codearena</bibkey>
    </paper>
    <paper id="49">
      <title>Ai2 Scholar <fixed-case>QA</fixed-case>: Organized Literature Synthesis with Attribution</title>
      <author><first>Amanpreet</first><last>Singh</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Joseph Chee</first><last>Chang</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Dany</first><last>Haddad</last></author>
      <author><first>Aakanksha</first><last>Naik</last><affiliation>Allen Institute for Artificial Intelligence and National Institutes of Health</affiliation></author>
      <author><first>Jena D.</first><last>Hwang</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Rodney</first><last>Kinney</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Daniel S</first><last>Weld</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <author><first>Doug</first><last>Downey</last><affiliation>Allen Institute for Artificial Intelligence and Northwestern University</affiliation></author>
      <author><first>Sergey</first><last>Feldman</last><affiliation>Allen Institute for Artificial Intelligence and Data Cowboys</affiliation></author>
      <pages>513-523</pages>
      <abstract>Retrieval-augmented generation is increasingly effective in answering scientific questions from literature, but many state-of-the-art systems are expensive and closed-source. We introduce Ai2 Scholar QA, a free online scientific question answering application. To facilitate research, we make our entire pipeline public: as a customizable open-source Python package and interactive web app, along with paper indexes accessible through public APIs and downloadable datasets. We describe our system in detail and present experiments analyzing its key design decisions. In an evaluation on a recent scientific QA benchmark, we find that Ai2 Scholar QA outperforms competing systems.</abstract>
      <url hash="9fa5b834">2025.acl-demo.49</url>
      <attachment type="copyright_agreement" hash="37cce421">2025.acl-demo.49.copyright_agreement.pdf</attachment>
      <bibkey>singh-etal-2025-ai2</bibkey>
    </paper>
    <paper id="50">
      <title>gec-metrics: A Unified Library for Grammatical Error Correction Evaluation</title>
      <author><first>Takumi</first><last>Goto</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Yusuke</first><last>Sakai</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <pages>524-534</pages>
      <abstract>We introduce gec-metrics, a library for using and developing grammatical error correction (GEC) evaluation metrics through a unified interface. Our library enables fair system comparisons by ensuring that everyone conducts evaluations using a consistent implementation. Moreover, it is designed with a strong focus on API usage, making it highly extensible. It also includes meta-evaluation functionalities and provides analysis and visualization scripts, contributing to developing GEC evaluation metrics. Our code is released under the MIT license<tex-math>^1</tex-math> and is also distributed as an installable package<tex-math>^2</tex-math>. The video is available at YouTube<tex-math>^3</tex-math>.<tex-math>^1</tex-math>GitHub: https://github.com/gotutiyan/gec-metrics<tex-math>^2</tex-math>PyPi: https://pypi.org/project/gec-metrics/<tex-math>^3</tex-math>Video: https://youtu.be/cor6dkN6EfI</abstract>
      <url hash="5e49d411">2025.acl-demo.50</url>
      <attachment type="copyright_agreement" hash="e377ad75">2025.acl-demo.50.copyright_agreement.pdf</attachment>
      <bibkey>goto-etal-2025-gec</bibkey>
    </paper>
    <paper id="51">
      <title><fixed-case>AI</fixed-case>2<fixed-case>A</fixed-case>gent: An End-to-End Framework for Deploying <fixed-case>AI</fixed-case> Projects as Autonomous Agents</title>
      <author><first>Jiaxiang</first><last>Chen</last><affiliation>Fudan University</affiliation></author>
      <author><first>Jingwei</first><last>Shi</last></author>
      <author><first>Lei</first><last>Gan</last></author>
      <author><first>Jiale</first><last>Zhang</last></author>
      <author><first>Qingyu</first><last>Zhang</last></author>
      <author><first>Dongqian</first><last>Zhang</last><affiliation>AI2Apps</affiliation></author>
      <author><first>Pang</first><last>Xin</last></author>
      <author><first>Zhucong</first><last>Li</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xu</first><last>Yinghui</last><affiliation>Fudan University</affiliation></author>
      <pages>535-541</pages>
      <abstract>As AI technology advances, it is driving innovation across industries, increasing the demand for scalable AI project deployment. However, deployment remains a critical challenge due to complex environment configurations, dependency conflicts, cross-platform adaptation, and debugging difficulties, which hinder automation and adoption.This paper introduces AI2Agent, an end-to-end framework that automates AI project deployment through guideline-driven execution, self-adaptive debugging, and case &amp; solution accumulation. AI2Agent dynamically analyzes deployment challenges, learns from past cases, and iteratively refines its approach, significantly reducing human intervention.To evaluate its effectiveness, we conducted experiments on 30 AI deployment cases, covering TTS, text-to-image generation, image editing, and other AI applications. Results show that AI2Agent significantly reduces deployment time and improves success rates. The code and demo video are now publicly accessible.</abstract>
      <url hash="369fd803">2025.acl-demo.51</url>
      <attachment type="copyright_agreement" hash="a2ea0cba">2025.acl-demo.51.copyright_agreement.pdf</attachment>
      <bibkey>chen-etal-2025-ai2agent</bibkey>
    </paper>
    <paper id="52">
      <title>Efficient Annotator Reliability Assessment with <fixed-case>E</fixed-case>ffi<fixed-case>ARA</fixed-case></title>
      <author><first>Owen</first><last>Cook</last></author>
      <author><first>Jake A</first><last>Vasilakes</last><affiliation>University of Sheffield</affiliation></author>
      <author><first>Ian</first><last>Roberts</last><affiliation>University of Sheffield</affiliation></author>
      <author><first>Xingyi</first><last>Song</last><affiliation>University of Sheffield</affiliation></author>
      <pages>542-550</pages>
      <abstract>Data annotation is an essential component of the machine learning pipeline; it is also a costly and time-consuming process. With the introduction of transformer-based models, annotation at the document level is increasingly popular; however, there is no standard framework for structuring such tasks. The EffiARA annotation framework is, to our knowledge, the first project to support the whole annotation pipeline, from understanding the resources required for an annotation task to compiling the annotated dataset and gaining insights into the reliability of individual annotators as well as the dataset as a whole. The framework’s efficacy is supported by two previous studies: one improving classification performance through annotator-reliability-based soft-label aggregation and sample weighting, and the other increasing the overall agreement among annotators through removing identifying and replacing an unreliable annotator. This work introduces the EffiARA Python package and its accompanying webtool, which provides an accessible graphical user interface for the system. We open-source the EffiARA Python package at https://github.com/MiniEggz/EffiARA and the webtool is publicly accessible at https://effiara.gate.ac.uk.</abstract>
      <url hash="54e576ec">2025.acl-demo.52</url>
      <attachment type="copyright_agreement" hash="d8d7b6ff">2025.acl-demo.52.copyright_agreement.pdf</attachment>
      <bibkey>cook-etal-2025-efficient-annotator</bibkey>
    </paper>
    <paper id="53">
      <title><fixed-case>TRANSLATIONCORRECT</fixed-case>: A Unified Framework for Machine Translation Post-Editing with Predictive Error Assistance</title>
      <author><first>Syed Mekael</first><last>Wasti</last></author>
      <author><first>Shou-Yi</first><last>Hung</last></author>
      <author><first>Christopher</first><last>Collins</last><affiliation>Ontario Tech University</affiliation></author>
      <author><first>En-Shiun Annie</first><last>Lee</last></author>
      <pages>551-562</pages>
      <abstract>Machine translation (MT) post-editing and research data collection often rely on inefficient, disconnected workflows. We introduce **TranslationCorrect**, an integrated framework designed to streamline these tasks. **TranslationCorrect** combines MT generation using models like NLLB, automated error prediction using models like XCOMET or LLM APIs (providing detailed reasoning), and an intuitive post-editing interface within a single environment. Built with human-computer interaction (HCI) principles in mind to minimize cognitive load, as confirmed by a user study. For translators, it enables them to correct errors and batch translate efficiently. For researchers, **TranslationCorrect** exports high-quality span-based annotations in the Error Span Annotation (ESA) format, using an error taxonomy inspired by Multidimensional Quality Metrics (MQM). These outputs are compatible with state-of-the-art error detection models and suitable for training MT or post-editing systems. Our user study confirms that **TranslationCorrect** significantly improves translation efficiency and user satisfaction over traditional annotation methods.</abstract>
      <url hash="b8651648">2025.acl-demo.53</url>
      <attachment type="copyright_agreement" hash="918f21b4">2025.acl-demo.53.copyright_agreement.pdf</attachment>
      <bibkey>wasti-etal-2025-translationcorrect</bibkey>
    </paper>
    <paper id="54">
      <title>First-<fixed-case>AID</fixed-case>: the first Annotation Interface for grounded Dialogues</title>
      <author><first>Stefano</first><last>Menini</last></author>
      <author><first>Daniel</first><last>Russo</last></author>
      <author><first>Alessio Palmero</first><last>Aprosio</last><affiliation>University of Trento</affiliation></author>
      <author><first>Marco</first><last>Guerini</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <pages>563-571</pages>
      <abstract>The swift advancement of Large Language Models (LLMs) has led to their widespread use across various tasks and domains, demonstrating remarkable generalization capabilities. However, achieving optimal performance in specialized tasks often requires fine-tuning LLMs with task-specific resources. The creation of high-quality, human-annotated datasets for this purpose is challenging due to financial constraints and the limited availability of human experts. To address these limitations, we propose First-AID, a novel human-in-the-loop (HITL) data collection framework for the knowledge-driven generation of synthetic dialogues using LLM prompting. In particular, our framework implements different strategies of data collection that require different user intervention during dialogue generation to reduce post-editing efforts and enhance the quality of generated dialogues. We also evaluated First-AID on misinformation and hate countering dialogues collection, demonstrating (1) its potential for efficient and high-quality data generation and (2) its adaptability to different practical constraints thanks to the three data collection strategies.</abstract>
      <url hash="6cd65a54">2025.acl-demo.54</url>
      <attachment type="copyright_agreement" hash="1f9a536b">2025.acl-demo.54.copyright_agreement.pdf</attachment>
      <bibkey>menini-etal-2025-first</bibkey>
    </paper>
    <paper id="55">
      <title>Mergenetic: a Simple Evolutionary Model Merging Library</title>
      <author><first>Adrian Robert</first><last>Minut</last><affiliation>University of Roma “La Sapienza”</affiliation></author>
      <author><first>Tommaso</first><last>Mencattini</last></author>
      <author><first>Andrea</first><last>Santilli</last><affiliation>Nous Research</affiliation></author>
      <author><first>Donato</first><last>Crisostomi</last></author>
      <author><first>Emanuele</first><last>Rodolà</last><affiliation>Sapienza University of Rome</affiliation></author>
      <pages>572-582</pages>
      <abstract>Model merging allows combining the capabilities of existing models into a new one—post hoc, without additional training. This has made it increasingly popular thanks to its low cost and the availability of libraries that support merging on consumer GPUs. Recent work shows that pairing merging with evolutionary algorithms can boost performance, but no framework currently supports flexible experimentation with such strategies in language models. We introduce Mergenetic, an open-source library for evolutionary model merging. Mergenetic enables easy composition of merging methods and evolutionary algorithms, while incorporating lightweight fitness estimators to reduce evaluation costs. We describe its design and demonstrate that Mergenetic produces competitive results across tasks and languages using modest hardware. A video demo showcasing its main features is also provided.</abstract>
      <url hash="b438b4b7">2025.acl-demo.55</url>
      <attachment type="copyright_agreement" hash="ced840ac">2025.acl-demo.55.copyright_agreement.pdf</attachment>
      <bibkey>minut-etal-2025-mergenetic</bibkey>
    </paper>
    <paper id="56">
      <title><fixed-case>F</fixed-case>lag<fixed-case>E</fixed-case>val-Arena: A Side-by-Side Comparative Evaluation Platform for Large Language Models and Text-Driven <fixed-case>AIGC</fixed-case></title>
      <author><first>Jing-Shu</first><last>Zheng</last></author>
      <author><first>Richeng</first><last>Xuan</last></author>
      <author><first>Bowen</first><last>Qin</last><affiliation>Beijing Academy of Artificial Intelligence</affiliation></author>
      <author><first>Zheqi</first><last>He</last><affiliation>Beijing Academy of Artificial Intelligence</affiliation></author>
      <author><first>Tongshuai.ren</first><last>Tongshuai.ren</last></author>
      <author><first>Xuejing</first><last>Li</last></author>
      <author><first>Jin-Ge</first><last>Yao</last><affiliation>BAAI</affiliation></author>
      <author><first>Xi</first><last>Yang</last><affiliation>Beijing Academy of Artificial Intelligence</affiliation></author>
      <pages>583-591</pages>
      <abstract>We introduce FlagEval-Arena, an evaluation platform for side-by-side comparisons of large language models and text-driven AIGC systems.Compared with the well-known LM Arena (LMSYS Chatbot Arena), we reimplement our own framework with the flexibility to introduce new mechanisms or features. Our platform enables side-by-side evaluation not only for language models or vision-language models, but also text-to-image or text-to-video synthesis. We specifically target at Chinese audience with a more focus on the Chinese language, more models developed by Chinese institutes, and more general usage beyond the technical community. As a result, we currently observe very interesting differences from usual results presented by LM Arena. Our platform is available via this URL: <url>https://flageval.baai.org/#/arena</url>.</abstract>
      <url hash="cf5c5c4a">2025.acl-demo.56</url>
      <attachment type="copyright_agreement" hash="80695170">2025.acl-demo.56.copyright_agreement.pdf</attachment>
      <bibkey>zheng-etal-2025-flageval</bibkey>
    </paper>
    <paper id="57">
      <title><fixed-case>IRIS</fixed-case>: Interactive Research Ideation System for Accelerating Scientific Discovery</title>
      <author><first>Aniketh</first><last>Garikaparthi</last><affiliation>Tata Consultancy Services Limited, India</affiliation></author>
      <author><first>Manasi</first><last>Patwardhan</last><affiliation>Tata Consultancy Services Limited, India</affiliation></author>
      <author><first>Lovekesh</first><last>Vig</last></author>
      <author><first>Arman</first><last>Cohan</last><affiliation>Yale University and Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>592-603</pages>
      <abstract>The rapid advancement in capabilities of large language models (LLMs) raises a pivotal question: How can LLMs accelerate scientific discovery? This work tackles the crucial first stage of research, generating novel hypotheses. While recent work on automated hypothesis generation focuses on multi-agent frameworks and extending test-time compute, none of the approaches effectively incorporate transparency and steerability through a synergistic Human-in-the-loop (HITL) approach. To address this gap, we introduce IRIS for interactive hypothesis generation, an open-source platform designed for researchers to leverage LLM-assisted scientific ideation. IRIS incorporates innovative features to enhance ideation, including adaptive test-time compute expansion via Monte Carlo Tree Search (MCTS), fine-grained feedback mechanism, and query-based literature synthesis. Designed to empower researchers with greater control and insight throughout the ideation process. We additionally conduct a user study with researchers across diverse disciplines, validating the effectiveness of our system in enhancing ideation. We open-source our code at https://github.com/Anikethh/IRIS-Interactive-Research-Ideation-System.</abstract>
      <url hash="2b6a8dec">2025.acl-demo.57</url>
      <attachment type="copyright_agreement" hash="423dbeb8">2025.acl-demo.57.copyright_agreement.pdf</attachment>
      <bibkey>garikaparthi-etal-2025-iris</bibkey>
    </paper>
    <paper id="58">
      <title><fixed-case>ROGRAG</fixed-case>: A Robustly Optimized <fixed-case>G</fixed-case>raph<fixed-case>RAG</fixed-case> Framework</title>
      <author><first>Zhefan</first><last>Wang</last></author>
      <author><first>Huanjun</first><last>Kong</last></author>
      <author><first>Jie</first><last>Ying</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Wanli</first><last>Ouyang</last><affiliation>Shanghai AI Lab</affiliation></author>
      <author><first>Nanqing</first><last>Dong</last><affiliation>Shanghai Artificial Intelligence Laboratory and Shanghai Innovation Institute</affiliation></author>
      <pages>604-613</pages>
      <abstract>Large language models (LLMs) commonly struggle with specialized or emerging topics which are rarely seen in the training corpus. Graph-based retrieval-augmented generation (GraphRAG) addresses this by structuring domain knowledge as a graph for dynamic retrieval. However, existing pipelines involve complex engineering workflows, making it difficult to isolate the impact of individual components. It is also challenging to evaluate the retrieval effectiveness due to the overlap between the pretraining and evaluation datasets. In this work, we introduce ROGRAG, a Robustly Optimized GraphRAG framework. Specifically, we propose a multi-stage retrieval mechanism that integrates dual-level with logic form retrieval methods to improve retrieval robustness without increasing computational cost. To further refine the system, we incorporate various result verification methods and adopt an incremental database construction approach. Through extensive ablation experiments, we rigorously assess the effectiveness of each component. Our implementation includes comparative experiments on SeedBench, where Qwen2.5-7B-Instruct initially underperformed. ROGRAG significantly improves the score from 60.0% to 75.0% and outperforms mainstream methods. Experiments on domain-specific datasets reveal that dual-level retrieval enhances fuzzy matching, while logic form retrieval improves structured reasoning, highlighting the importance of multi-stage retrieval. ROGRAG is released as an open-source resource https://github.com/tpoisonooo/ROGRAG and supports installation with pip.</abstract>
      <url hash="487992c2">2025.acl-demo.58</url>
      <attachment type="copyright_agreement" hash="4c87109f">2025.acl-demo.58.copyright_agreement.pdf</attachment>
      <bibkey>wang-etal-2025-rograg</bibkey>
    </paper>
    <paper id="59">
      <title><fixed-case>LECTURE</fixed-case>4<fixed-case>ALL</fixed-case>: A Lightweight Approach to Precise Timestamp Detection in Online Lecture Videos</title>
      <author><first>Viktoria</first><last>Wrobel</last></author>
      <author><first>Simon</first><last>Kazemi</last></author>
      <author><first>Frank</first><last>Hammerschmidt</last></author>
      <author><first>Torben</first><last>Hannemann</last></author>
      <author><first>Gregor</first><last>Stange</last></author>
      <author><first>Seid Muhie</first><last>Yimam</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Robert</first><last>Geislinger</last></author>
      <pages>614-620</pages>
      <abstract>This paper presents LECTURE4ALL, a web application developed to improve the search experience of educational video platforms. Lecture2Go provides a vast collection of recorded lectures, but locating specific content within videos can be time-consuming. LECTURE4ALL addresses this issue by leveraging a vector database and a streamlined user interface to enable direct retrieval of precise video timestamps. By enhancing search accuracy and efficiency, LECTURE4ALL significantly improves the accessibility and usability of educational video platforms.</abstract>
      <url hash="dbfad9b0">2025.acl-demo.59</url>
      <attachment type="copyright_agreement" hash="a1bfc44f">2025.acl-demo.59.copyright_agreement.pdf</attachment>
      <bibkey>wrobel-etal-2025-lecture4all</bibkey>
    </paper>
    <paper id="60">
      <title><fixed-case>F</fixed-case>lex<fixed-case>RAG</fixed-case>: A Flexible and Comprehensive Framework for Retrieval-Augmented Generation</title>
      <author><first>Zhang</first><last>Zhuocheng</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yang</first><last>Feng</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>621-631</pages>
      <abstract>Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large language model applications, with numerous existing frameworks offering a wide range of functionalities to facilitate the development of RAG systems.However, we have identified several persistent challenges in these frameworks, including lack of new techniques, difficulties in algorithm reproduction and sharing, and high system overhead.To address these limitations, we introduce **FlexRAG**, an open-source framework specifically designed for research and prototyping.FlexRAG supports text-based, multimodal, and network-based RAG, providing comprehensive lifecycle support alongside efficient asynchronous processing and persistent caching capabilities.By offering a robust and flexible solution, FlexRAG enables researchers to rapidly develop, deploy, and share advanced RAG systems.Our toolkit and resources are available at https://github.com/ictnlp/FlexRAG.</abstract>
      <url hash="6f79470c">2025.acl-demo.60</url>
      <attachment type="copyright_agreement" hash="861cac1d">2025.acl-demo.60.copyright_agreement.pdf</attachment>
      <bibkey>zhuocheng-etal-2025-flexrag</bibkey>
    </paper>
    <paper id="61">
      <title><fixed-case>C</fixed-case>omfy<fixed-case>UI</fixed-case>-Copilot: An Intelligent Assistant for Automated Workflow Development</title>
      <author><first>Zhenran</first><last>Xu</last></author>
      <author><first>Yangxue</first><last>Yangxue</last></author>
      <author><first>Yiyu</first><last>Wang</last><affiliation>University of Chinese Academy of Sciences</affiliation></author>
      <author><first>Qingli</first><last>Hu</last></author>
      <author><first>Zijiao</first><last>Wu</last></author>
      <author><first>Baotian</first><last>Hu</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <author><first>Longyue</first><last>Wang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Weihua</first><last>Luo</last><affiliation>Alibaba International Digital Commerce Group</affiliation></author>
      <author><first>Kaifu</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <pages>632-643</pages>
      <abstract>We introduce **ComfyUI-Copilot**, a large language model-powered plugin designed to enhance the usability and efficiency of ComfyUI, an open-source platform for AI-driven art creation. Despite its flexibility and user-friendly interface, ComfyUI can present challenges to newcomers, including limited documentation, model misconfigurations, and the complexity of workflow design. ComfyUI-Copilot addresses these challenges by offering intelligent node and model recommendations, along with automated one-click workflow construction. At its core, the system employs a hierarchical multi-agent framework comprising a central assistant agent for task delegation and specialized worker agents for different usages, supported by our curated ComfyUI knowledge bases to streamline debugging and deployment. We validate the effectiveness of ComfyUI-Copilot through both offline quantitative evaluations and online user feedback, showing that it accurately recommends nodes and accelerates workflow development. Additionally, use cases illustrate that ComfyUI-Copilot lowers entry barriers for beginners and enhances workflow efficiency for experienced users. The ComfyUI-Copilot installation package and a demo video are available at https://github.com/AIDC-AI/ComfyUI-Copilot.</abstract>
      <url hash="e54ba266">2025.acl-demo.61</url>
      <attachment type="copyright_agreement" hash="fb30139a">2025.acl-demo.61.copyright_agreement.pdf</attachment>
      <bibkey>xu-etal-2025-comfyui</bibkey>
    </paper>
    <paper id="62">
      <title><fixed-case>PRAISE</fixed-case>: Enhancing Product Descriptions with <fixed-case>LLM</fixed-case>-Driven Structured Insights</title>
      <author><first>Adnan</first><last>Qidwai</last></author>
      <author><first>Srija</first><last>Mukhopadhyay</last></author>
      <author><first>Prerana</first><last>Khatiwada</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <author><first>Vivek</first><last>Gupta</last><affiliation>Arizona State University</affiliation></author>
      <pages>644-652</pages>
      <abstract>Accurate and complete product descriptions are crucial for e-commerce, yet seller-provided information often falls short. Customer reviews offer valuable details but are laborious to sift through manually. We present PRAISE: Product Review Attribute Insight Structuring Engine, a novel system that uses Large Language Models (LLMs) to automatically extract, compare, and structure insights from customer reviews and seller descriptions. PRAISE provides users with an intuitive interface to identify missing, contradictory, or partially matching details between these two sources, presenting the discrepancies in a clear, structured format alongside supporting evidence from reviews. This allows sellers to easily enhance their product listings for clarity and persuasiveness, and buyers to better assess product reliability. Our demonstration showcases PRAISE’s workflow, its effectiveness in generating actionable structured insights from unstructured reviews, and its potential to significantly improve the quality and trustworthiness of e-commerce product catalogs.</abstract>
      <url hash="a5b11043">2025.acl-demo.62</url>
      <attachment type="copyright_agreement" hash="e5a1f6c9">2025.acl-demo.62.copyright_agreement.pdf</attachment>
      <bibkey>qidwai-etal-2025-praise</bibkey>
    </paper>
    <paper id="63">
      <title><fixed-case>ATG</fixed-case>en: A Framework for Active Text Generation</title>
      <author><first>Akim</first><last>Tsvigun</last><affiliation>Nebius</affiliation></author>
      <author><first>Daniil</first><last>Vasilev</last><affiliation>Higher School of Economics, Higher School of Economics</affiliation></author>
      <author><first>Ivan</first><last>Tsvigun</last><affiliation>Behavox Ltd.</affiliation></author>
      <author><first>Ivan</first><last>Lysenko</last><affiliation>Higher School of Economics, Higher School of Economics</affiliation></author>
      <author><first>Talgat</first><last>Bektleuov</last><affiliation>Innopolis University</affiliation></author>
      <author><first>Aleksandr</first><last>Medvedev</last><affiliation>White Circle and T-technologies</affiliation></author>
      <author><first>Uliana</first><last>Vinogradova</last><affiliation>Sberbank</affiliation></author>
      <author><first>Nikita</first><last>Severin</last></author>
      <author><first>Mikhail</first><last>Mozikov</last></author>
      <author><first>Andrey</first><last>Savchenko</last><affiliation>Sber AI Lab and HSE University</affiliation></author>
      <author><first>Ilya</first><last>Makarov</last><affiliation>ISPRAS</affiliation></author>
      <author><first>Grigorev</first><last>Rostislav</last></author>
      <author><first>Ramil</first><last>Kuleev</last></author>
      <author><first>Fedor</first><last>Zhdanov</last><affiliation>Nebius AI</affiliation></author>
      <author><first>Artem</first><last>Shelmanov</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>653-665</pages>
      <abstract>Active learning (AL) has demonstrated remarkable potential in reducing the annotation effort required for training machine learning models. However, despite the surging popularity of natural language generation (NLG) tasks in recent years, the application of AL to NLG has been limited. In this paper, we introduce Active Text Generation (ATGen) - a comprehensive framework that bridges AL with text generation tasks, enabling the application of state-of-the-art AL strategies to NLG. Our framework simplifies AL-empowered annotation in NLG tasks using both human annotators and automatic annotation agents based on large language models (LLMs). The framework supports LLMs deployed as a service, such as ChatGPT and Claude, or operated on-premises. Furthermore, ATGen provides a unified platform for smooth implementation and benchmarking of novel AL strategies tailored to NLG tasks. Finally, we present experimental results across multiple text generation tasks where we compare the performance of state-of-the-art AL strategies in various settings. We demonstrate that ATGen can reduce both the effort of human annotators and costs for API calls to automatic annotation agents based on LLMs.</abstract>
      <url hash="5d6be4be">2025.acl-demo.63</url>
      <attachment type="copyright_agreement" hash="07403e91">2025.acl-demo.63.copyright_agreement.pdf</attachment>
      <bibkey>tsvigun-etal-2025-atgen</bibkey>
    </paper>
    <paper id="64">
      <title>Value Compass Benchmarks: A Comprehensive, Generative and Self-Evolving Platform for <fixed-case>LLM</fixed-case>s’ Value Evaluation</title>
      <author><first>Jing</first><last>Yao</last><affiliation>Microsoft</affiliation></author>
      <author><first>Xiaoyuan</first><last>Yi</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Shitong</first><last>Duan</last></author>
      <author><first>Jindong</first><last>Wang</last><affiliation>William &amp; Mary</affiliation></author>
      <author><first>Yuzhuo</first><last>Bai</last></author>
      <author><first>Muhua</first><last>Huang</last></author>
      <author><first>Yang</first><last>Ou</last></author>
      <author><first>Scarlett</first><last>Li</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Peng</first><last>Zhang</last></author>
      <author><first>Tun</first><last>Lu</last><affiliation>Fudan University</affiliation></author>
      <author><first>Zhicheng</first><last>Dou</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Maosong</first><last>Sun</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>James</first><last>Evans</last></author>
      <author><first>Xing</first><last>Xie</last><affiliation>Microsoft Research Asia</affiliation></author>
      <pages>666-678</pages>
      <abstract>As large language models (LLMs) are gradually integrated into human daily life, assessing their underlying values becomes essential for understanding their risks and alignment with specific preferences. Despite growing efforts, current value evaluation methods face two key challenges. C1. Evaluation Validity: Static benchmarks fail to reflect intended values or yield informative results due to data contamination or a ceiling effect. C2. Result Interpretation: They typically reduce the pluralistic and often incommensurable values to one-dimensional scores, which hinders users from gaining meaningful insights and guidance. To address these challenges, we present Value Compass Benchmarks, the first dynamic, online and interactive platform specially devised for comprehensive value diagnosis of LLMs. It (1) grounds evaluations in multiple basic value systems from social science; (2) develops a generative evolving evaluation paradigm that automatically creates real-world test items co-evolving with ever-advancing LLMs; (3) offers multi-faceted result interpretation, including (i) fine-grained scores and case studies across 27 value dimensions for 33 leading LLMs, (ii) customized comparisons, and (iii) visualized analysis of LLMs’ alignment with cultural values. We hope Value Compass Benchmarks serves as a navigator for further enhancing LLMs’ safety and alignment, benefiting their responsible and adaptive development.</abstract>
      <url hash="74b8b256">2025.acl-demo.64</url>
      <attachment type="copyright_agreement" hash="2f2eee4a">2025.acl-demo.64.copyright_agreement.pdf</attachment>
      <bibkey>yao-etal-2025-value</bibkey>
    </paper>
  </volume>
  <volume id="srw" ingest-date="2025-07-14" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (ACL 2025)'</booktitle>
      <editor><first>Jin</first><last>Zhao</last></editor>
      <editor><first>Mingyang</first><last>Wang</last></editor>
      <editor><first>Zhu</first><last>Liu</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Vienna, Austria</address>
      <month>July</month>
      <year>2025</year>
      <url hash="60e5a95f">2025.acl-srw</url>
      <venue>acl</venue>
      <venue>ws</venue>
      <isbn>979-8-89176-254-1</isbn>
    </meta>
    <frontmatter>
      <url hash="3e7b9795">2025.acl-srw.0</url>
      <bibkey>acl-ws-2025-srw</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Advancing <fixed-case>A</fixed-case>frican-Accented <fixed-case>E</fixed-case>nglish Speech Recognition: Epistemic Uncertainty-Driven Data Selection for Generalizable <fixed-case>ASR</fixed-case> Models</title>
      <author><first>Bonaventure F. P.</first><last>Dossou</last></author>
      <pages>1-17</pages>
      <abstract>Accents play a pivotal role in shaping human communication, enhancing our ability to convey and comprehend messages with clarity and cultural nuance. While there has been significant progress in Automatic Speech Recognition (ASR), African-accented English ASR has been understudied due to a lack of training datasets, which are often expensive to create and demand colossal human labor. Combining several active learning paradigms and the core-set approach, we propose a new multi-round adaptation process that uses epistemic uncertainty to automate annotation, significantly reducing the associated costs and human labor. This novel method streamlines data annotation and strategically selects data samples that contribute most to model uncertainty, enhancing training efficiency. We define a new U-WER metric to track model adaptation to hard accents. We evaluate our approach across several domains, datasets, and high-performing speech models. Our results show that our approach leads to a 27% WER relative average improvement while requiring, on average, 45% less data than established baselines. Our approach also improves out-of-distribution generalization for very low-resource accents, demonstrating its viability for building generalizable ASR models in the context of accented African ASR. We open-source the code here.</abstract>
      <url hash="63cfd177">2025.acl-srw.1</url>
      <bibkey>dossou-2025-advancing</bibkey>
    </paper>
    <paper id="2">
      <title>Beyond the Gold Standard in Analytic Automated Essay Scoring</title>
      <author><first>Gabrielle</first><last>Gaudeau</last><affiliation>University of Cambridge</affiliation></author>
      <pages>18-39</pages>
      <abstract>Originally developed to reduce the manual burden of grading standardised language tests, Automated Essay Scoring (AES) research has long focused on holistic scoring methods which offer minimal formative feedback in the classroom. With the increasing demand for technological tools that support language acquisition, the field is turning to analytic AES (evaluating essays according to different linguistic traits). This approach holds promise for generating more detailed essay feedback, but relies on analytic scoring data that is both more cognitively demanding for humans to produce, and prone to bias. The dominant paradigm in AES is to aggregate disagreements between raters into a single gold-standard label, which fails to account for genuine examiner variability. In an attempt to make AES more representative and trustworthy, we propose to explore the sources of disagreements and lay out a novel AES system design that learns from individual raters instead of the gold standard labels.</abstract>
      <url hash="494d884e">2025.acl-srw.2</url>
      <bibkey>gaudeau-2025-beyond</bibkey>
    </paper>
    <paper id="3">
      <title>Confidence and Stability of Global and Pairwise Scores in <fixed-case>NLP</fixed-case> Evaluation</title>
      <author><first>Georgii</first><last>Levtsov</last></author>
      <author><first>Dmitry</first><last>Ustalov</last><affiliation>JetBrains</affiliation></author>
      <pages>40-52</pages>
      <abstract>With the advent of highly capable instruction-tuned neural language models, benchmarking in natural language processing (NLP) is increasingly shifting towards pairwise comparison leaderboards, such as LMSYS Arena, from traditional global pointwise scores (e.g., GLUE, BIG-bench, SWE-bench). This paper empirically investigates the strengths and weaknesses of both global scores and pairwise comparisons to aid decision-making in selecting appropriate model evaluation strategies. Through computational experiments on synthetic and real-world datasets using standard global metrics and the popular Bradley–Terry model for pairwise comparisons, we found that while global scores provide more reliable overall rankings, they can underestimate strong models with rare, significant errors or low confidence. Conversely, pairwise comparisons are particularly effective for identifying strong contenders among models with lower global scores, especially where quality metrics are hard to define (e.g., text generation), though they require more comparisons to converge if ties are frequent. Our code and data are available at https://github.com/HSPyroblast/srw-ranking under a permissive license.</abstract>
      <url hash="0449dfb0">2025.acl-srw.3</url>
      <bibkey>levtsov-ustalov-2025-confidence</bibkey>
    </paper>
    <paper id="4">
      <title>Zero-shot prompt-based classification: topic labeling in times of foundation models in <fixed-case>G</fixed-case>erman Tweets</title>
      <author><first>Simon</first><last>Münker</last></author>
      <author><first>Kai</first><last>Kugler</last><affiliation>Universität Trier</affiliation></author>
      <author><first>Achim</first><last>Rettinger</last><affiliation>FZI Forschungszentrum Informatik and Trier University</affiliation></author>
      <pages>53-63</pages>
      <abstract>Filtering and annotating textual data are routine tasks in many areas, like social media or news analytics. Automating these tasks allows to scale the analyses wrt. speed and breadth of content covered and decreases the manual effort required. Due to technical advancements in Natural Language Processing, specifically the success of large foundation models, a new tool for automating such annotation processes by using a text-to-text interface given written guidelines without providing training samples has become available. In this work, we assess these advancements <i>in-the-wild</i> by empirically testing them in an annotation task on German Twitter data about social and political European crises. We compare the prompt-based results with our human annotation and preceding classification approaches, including Naive Bayes and a BERT-based fine-tuning/domain adaptation pipeline. Our results show that the prompt-based approach – despite being limited by local computation resources during the model selection – is comparable with the fine-tuned BERT but without any annotated training data. Our findings emphasize the ongoing paradigm shift in the NLP landscape, i.e., the unification of downstream tasks and elimination of the need for pre-labeled training data.</abstract>
      <url hash="94bdbfb6">2025.acl-srw.4</url>
      <bibkey>munker-etal-2025-zero</bibkey>
    </paper>
    <paper id="5">
      <title>Rethinking Full Finetuning from Pretraining Checkpoints in Active Learning for <fixed-case>A</fixed-case>frican Languages</title>
      <author><first>Bonaventure F. P.</first><last>Dossou</last></author>
      <author><first>Ines</first><last>Arous</last></author>
      <author><first>Jackie CK</first><last>Cheung</last><affiliation>McGill University, Mila Research Institute and Microsoft</affiliation></author>
      <pages>64-78</pages>
      <abstract>Active learning (AL) aims to reduce annotation effort by iteratively selecting the most informative samples for labeling. The dominant strategy in AL involves fully finetuning the model on all acquired data after each round, which is computationally expensive in multilingual and low-resource settings. This paper investigates <i>continual finetuning</i> (CF), an alternative update strategy where the model is updated only on newly acquired samples at each round. We evaluate CF against full finetuning (FA) across 28 African languages using MasakhaNEWS and SIB-200. Our analysis reveals three key findings. First, CF matches or outperforms FA for languages included in the model’s pretraining, achieving up to 35% reductions in GPU memory, FLOPs, and training time. Second, CF performs comparably even for languages not seen during pretraining when they are typologically similar to those that were. Third, CF’s effectiveness depends critically on uncertainty-based acquisition; without it, performance deteriorates significantly. While FA remains preferable for some low-resource languages, the overall results establish CF as a robust, cost-efficient alternative for active learning in multilingual NLP. These findings motivate developing hybrid AL strategies that adapt fine-tuning behavior based on pretraining coverage, language typology, and acquisition dynamics.</abstract>
      <url hash="59b2bf1b">2025.acl-srw.5</url>
      <bibkey>dossou-etal-2025-rethinking</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>HYPEROFA</fixed-case>: Expanding <fixed-case>LLM</fixed-case> Vocabulary to New Languages via Hypernetwork-Based Embedding Initialization</title>
      <author><first>Enes</first><last>Özeren</last><affiliation>University of Munich, Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Yihong</first><last>Liu</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Hinrich</first><last>Schuetze</last></author>
      <pages>79-96</pages>
      <abstract>Many pre-trained language models (PLMs) exhibit suboptimal performance on mid- and low-resource languages, largely due to limited exposure to these languages during pre-training. A common strategy to address this is to introduce new tokens specific to the target languages, initialize their embeddings, and apply continual pre-training on target-language data. Among such methods, OFA (Liu et al., 2024a) proposes a similarity-based subword embedding initialization heuristic that is both effective and efficient. However, OFA restricts target-language token embeddings to be convex combinations of a fixed number of source-language embeddings, which may limit expressiveness. To overcome this limitation, we propose HYPEROFA, a hypernetwork-based approach for more adaptive token embedding initialization. The hypernetwork is trained to map from an external multilingual word vector space to the PLM’s token embedding space using source-language tokens. Once trained, it can generate flexible embeddings for target-language tokens, serving as a good starting point for continual pretraining. Experiments demonstrate that HYPEROFA consistently outperforms random initialization baseline and matches or exceeds the performance of OFA in both continual pre-training convergence and downstream task performance. We make the code publicly available.</abstract>
      <url hash="a0654e3b">2025.acl-srw.6</url>
      <bibkey>ozeren-etal-2025-hyperofa</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>SEPSIS</fixed-case>: <fixed-case>I</fixed-case> Can Catch Your Lies – A New Paradigm for Deception Detection</title>
      <author><first>Anku</first><last>Rani</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Dwip</first><last>Dalal</last></author>
      <author><first>Shreya</first><last>Gautam</last></author>
      <author><first>Pankaj</first><last>Gupta</last></author>
      <author><first>Vinija</first><last>Jain</last><affiliation>Facebook</affiliation></author>
      <author><first>Aman</first><last>Chadha</last><affiliation>Amazon Web Services</affiliation></author>
      <author><first>Amit</first><last>Sheth</last><affiliation>University of South Carolina</affiliation></author>
      <author><first>Amitava</first><last>Das</last><affiliation>University of of South Carolina</affiliation></author>
      <pages>97-128</pages>
      <abstract>Deception is the intentional practice of twisting information. It is a nuanced societal practice deeply intertwined with human societal evolution, characterized by a multitude of facets. This research explores the problem of deception through the lens of psychology, employing a framework that categorizes deception into three forms: lies of omission, lies of commission, and lies of influence. The primary focus of this study is specifically on investigating only lies of omission. We propose a novel framework for deception detection leveraging NLP techniques. We curated an annotated dataset of 876,784 samples by amalgamating a popular large-scale fake news dataset and scraped news headlines from the Twitter handle of “Times of India”, a well-known Indian news media house. Each sample has been labeled with four layers, namely: (i) the type of omission (speculation, bias, distortion, sounds factual, and opinion), (ii) colors of lies (black, white, grey, and red), and (iii) the intention of such lies (to influence, gain social prestige, etc) (iv) topic of lies (political, educational, religious, racial, and ethnicity). We present a novel multi-task learning [MTL] pipeline that leverages the dataless merging of fine-tuned language models to address the deception detection task mentioned earlier. Our proposed model achieved an impressive F1 score of 0.87, demonstrating strong performance across all layers including the type, color, intent, and topic aspects of deceptive content. Finally, our research aims to explore the relationship between the lies of omission and propaganda techniques. To accomplish this, we conducted an in-depth analysis, uncovering compelling findings. For instance, our analysis revealed a significant correlation between loaded language and opinion, shedding light on their interconnectedness. To encourage further research in this field, we are releasing the SEPSIS dataset and code at <url>https://huggingface.co/datasets/ankurani/deception</url>.</abstract>
      <url hash="92525685">2025.acl-srw.7</url>
      <bibkey>rani-etal-2025-sepsis</bibkey>
    </paper>
    <paper id="8">
      <title>Can Multi-turn Self-refined Single Agent <fixed-case>LM</fixed-case>s with Retrieval Solve Hard Coding Problems?</title>
      <author><first>Md Tanzib</first><last>Hosain</last><affiliation>American International University - Bangladesh</affiliation></author>
      <author><first>Md Kishor</first><last>Morol</last><affiliation>St. Thomas University (FL)</affiliation></author>
      <pages>129-142</pages>
      <abstract>Among the hardest tasks for humans are those found in competitive programming where problems require sophisticated algorithmic thinking, puzzle solving, and the creation of effective code. As a domain to assess language models (LMs), it has not received enough attention, though. This study presents the ICPC benchmark, which consists of 254 international collegiate programming contest (ICPC) tasks. Each problem includes official analysis, reference code, and sample and high-quality unit and hidden tests. We are able to develop and evaluate a variety of LM inference techniques for competitive programming with these resources. With zero-shot chain-of-thought prompting, we find that o1 only achieves a 19.1% pass@1 solve rate. With our best inference technique, which combines muti-turn self-judge with reflection and retrieval over episodic information, raises this to 42.2%. Furthermore, we conduct a new human-in-the-loop investigation to gain a deeper understanding of the remaining difficulties. Surprisingly, we discover that o1 can solve 17 out of 18 problems that were previously unsolvable by any model or technique with just a few specific instructions. A footstep toward LMs with grounded, imaginative, and algorithmic thinking is provided by our quantitative findings and qualitative research. We open source our code at https://github.com/kraritt/zolve.</abstract>
      <url hash="c578abd8">2025.acl-srw.8</url>
      <bibkey>hosain-morol-2025-multi</bibkey>
    </paper>
    <paper id="9">
      <title>Do Androids Question Electric Sheep? A Multi-Agent Cognitive Simulation of Philosophical Reflection on Hybrid Table Reasoning</title>
      <author><first>Yiran Rex</first><last>Ma</last><affiliation>Tsinghua University, Tsinghua University and Beijing University of Posts and Telecommunications</affiliation></author>
      <pages>143-164</pages>
      <abstract>While LLMs demonstrate remarkable reasoning capabilities and multi-agent applicability, their tendency to “overthink” and “groupthink” pose intriguing parallels to human cognitive limitations. Inspired by this observation, we conduct an exploratory simulation to investigate whether LLMs are wise enough to be thinkers of philosophical reflection. We design two frameworks, Philosopher and Symposium, which simulate self- and group-reflection for multi-persona in hybrid table reasoning tasks. Through experiments across four benchmarks, we discover that while introducing varied perspectives might help, LLMs tend to under-perform simpler end-to-end approaches. We reveal from close reading five emergent behaviors which strikingly resemble human cognitive closure-seeking behaviors, and identify a consistent pattern of “overthinking threshold” across all tasks, where collaborative reasoning often reaches a critical point of diminishing returns. This study sheds light on a fundamental challenge shared by both human and machine intelligence: the delicate balance between deliberation and decisiveness.</abstract>
      <url hash="2eda15ff">2025.acl-srw.9</url>
      <bibkey>ma-2025-androids</bibkey>
    </paper>
    <paper id="10">
      <title>Grouped Sequency-arranged Rotation: Optimizing Rotation Transformation for Quantization for Free</title>
      <author><first>Euntae</first><last>Choi</last></author>
      <author><first>Sumin</first><last>Song</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Woosang</first><last>Lim</last></author>
      <author><first>Sungjoo</first><last>Yoo</last><affiliation>Seoul National University</affiliation></author>
      <pages>165-172</pages>
      <abstract>Large Language Models (LLMs) face deployment challenges due to high computational costs, and while Post-Training Quantization (PTQ) offers a solution, existing rotation-based methods struggle at very low bit-widths like 2-bit. We introduce a novel, training-free approach to construct an improved rotation matrix, addressing the limitations of current methods. The key contributions include leveraging the Walsh-Hadamard transform with sequency ordering, which clusters similar frequency components to reduce quantization error compared to standard Hadamard matrices, significantly improving performance. Furthermore, we propose a Grouped Sequency-arranged Rotation (GSR) using block-diagonal matrices with smaller Walsh blocks, effectively isolating outlier impacts and achieving performance comparable to optimization-based methods without requiring any training. Our method demonstrates robust performance on reasoning tasks and Perplexity (PPL) score on WikiText-2. Our method also enhances results even when applied over existing learned rotation techniques.</abstract>
      <url hash="1434021c">2025.acl-srw.10</url>
      <bibkey>choi-etal-2025-grouped</bibkey>
    </paper>
    <paper id="11">
      <title>A Reproduction Study: The Kernel <fixed-case>PCA</fixed-case> Interpretation of Self-Attention Fails Under Scrutiny</title>
      <author><first>Karahan</first><last>Sarıtaş</last></author>
      <author><first>Çağatay</first><last>Yıldız</last><affiliation>Eberhard-Karls-Universität Tübingen</affiliation></author>
      <pages>173-185</pages>
      <abstract>In this reproduction study, we revisit recent claims that self-attention implements kernel principal component analysis (KPCA) (Teo and Nguyen, 2024), positing that (i) value vectors <tex-math>V</tex-math> capture the eigenvectors of the Gram matrix of the keys, and (ii) that self-attention projects queries onto the principal component axes of the key matrix <tex-math>K</tex-math> in a feature space. Our analysis reveals three critical inconsistencies: (1) No alignment exists between learned self-attention value vectors and what is proposed in the KPCA perspective, with average similarity metrics (optimal cosine similarity <tex-math>\leq 0.32</tex-math>, linear CKA (Centered Kernel Alignment) <tex-math>\leq 0.11</tex-math>, kernel CKA <tex-math>\leq 0.32</tex-math>) indicating negligible correspondence; (2) Reported decreases in reconstruction loss <tex-math>J_\text{proj}</tex-math>, arguably justifying the claim that the self-attentionminimizes the projection error of KPCA, are misinterpreted, as the quantities involved differ by orders of magnitude (<tex-math>\sim 10^3</tex-math>); (3) Gram matrix eigenvalue statistics, introduced to justify that <tex-math>V</tex-math> captures the eigenvector of the gram matrix, are irreproducible without undocumented implementation-specific adjustments. Across 10 transformer architectures, we conclude that the KPCA interpretation of self-attention lacks empirical support.</abstract>
      <url hash="6d8a0eba">2025.acl-srw.11</url>
      <bibkey>saritas-yildiz-2025-reproduction</bibkey>
    </paper>
    <paper id="12">
      <title>Transforming Brainwaves into Language: <fixed-case>EEG</fixed-case> Microstates Meet Text Embedding Models for Dementia Detection</title>
      <author><first>Quoc-Toan</first><last>Nguyen</last></author>
      <author><first>Linh</first><last>Le</last><affiliation>University of queensland</affiliation></author>
      <author><first>Xuan-The</first><last>Tran</last></author>
      <author><first>Dorothy</first><last>Bai</last><affiliation>NA</affiliation></author>
      <author><first>Nghia</first><last>Duong-Trung</last><affiliation>German Research Center for Artificial Intelligence (DFKI)</affiliation></author>
      <author><first>Thomas</first><last>Do</last><affiliation>University of Technology Sydney</affiliation></author>
      <author><first>Chin-teng</first><last>Lin</last><affiliation>University of Technology Sydney</affiliation></author>
      <pages>186-202</pages>
      <abstract>This study proposes a novel, scalable, non-invasive and channel-independent approach for early dementia detection, particularly Alzheimer’s Disease (AD), by representing Electroencephalography (EEG) microstates as symbolic, language-like sequences. These representations are processed via text embedding and time-series deep learning models for classification. Developed on EEG data from 1001 participants across multiple countries, the proposed method achieves a high accuracy of 94.31% for AD detection. By eliminating the need for fixed EEG configurations and costly/invasive modalities, the introduced approach improves generalisability and enables cost-effective deployment without requiring separate AI models or specific devices. It facilitates scalable and accessible dementia screening, supporting timely interventions and enhancing AD detection in resource-limited communities.</abstract>
      <url hash="81ed6fa4">2025.acl-srw.12</url>
      <bibkey>nguyen-etal-2025-transforming</bibkey>
    </paper>
    <paper id="13">
      <title>Neuron-Level Language Tag Injection Improves Zero-Shot Translation Performance</title>
      <author><first>Jay</first><last>Orten</last><affiliation>Brigham Young University and Brigham Young University</affiliation></author>
      <author><first>Ammon</first><last>Shurtz</last><affiliation>Brigham Young University</affiliation></author>
      <author><first>Nancy</first><last>Fulda</last><affiliation>Brigham Young University</affiliation></author>
      <author><first>Stephen D.</first><last>Richardson</last><affiliation>Brigham Young University</affiliation></author>
      <pages>203-212</pages>
      <abstract>Language tagging, a method whereby source and target inputs are prefixed with a unique language token, has become the de facto standard for conditioning Multilingual Neural Machine Translation (MNMT) models on specific language directions. This conditioning can manifest effective zero-shot translation abilities in MT models at scale for many languages. Expanding on previous work, we propose a novel method of language tagging for MNMT, injection, in which the embedded representation of a language token is concatenated to the input of every linear layer. We explore a variety of different tagging methods, with and without injection, showing that injection improves zero-shot translation performance with up to a 2+ BLEU score point gain for certain language directions in our dataset.</abstract>
      <url hash="a736aac0">2025.acl-srw.13</url>
      <bibkey>orten-etal-2025-neuron</bibkey>
    </paper>
    <paper id="14">
      <title>Voices of Dissent: A Multimodal Analysis of Protest Songs through Lyrics and Audio</title>
      <author><first>Utsav</first><last>Shekhar</last></author>
      <author><first>Radhika</first><last>Mamidi</last><affiliation>International Institute of Information Technology Hyderabad</affiliation></author>
      <pages>213-221</pages>
      <abstract>Music has long served as a vehicle for political expression, with protest songs playing a central role in articulating dissent and mobilizing collective action. Yet, despite their cultural significance, the linguistic and acoustic signatures that define protest music remain understudied. We present a multimodal computational analysis of protest and non-protest songs spanning multiple decades. Using NLP and audio analysis, we identify the linguistic and musical features that differentiate protest songs. Instead of focusing on classification performance, we treat classification as a diagnostic tool to investigate these features and reveal broader patterns. Protest songs are not just politically charged they are acoustically and linguistically distinct, and we quantify how.</abstract>
      <url hash="99f0b3b7">2025.acl-srw.14</url>
      <bibkey>shekhar-mamidi-2025-voices</bibkey>
    </paper>
    <paper id="15">
      <title>Your Pretrained Model Tells the Difficulty Itself: A Self-Adaptive Curriculum Learning Paradigm for Natural Language Understanding</title>
      <author><first>Qi</first><last>Feng</last></author>
      <author><first>Yihong</first><last>Liu</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Hinrich</first><last>Schuetze</last></author>
      <pages>222-239</pages>
      <abstract>Curriculum learning is a widely adopted training strategy in natural language processing (NLP), where models are exposed to examples organized by increasing difficulty to enhance learning efficiency and performance. However, most existing approaches rely on manually defined difficulty metrics – such as text length – which may not accurately reflect the model’s own perspective. To overcome this limitation, we present a self-adaptive curriculum learning paradigm that prioritizes fine-tuning examples based on difficulty scores predicted by pre-trained language models (PLMs) themselves. Building on these scores, we explore various training strategies that differ in the ordering of examples for the fine-tuning: from easy-to-hard, hard-to-easy, to mixed sampling. We evaluate our method on four natural language understanding (NLU) datasets covering both binary and multi-class classification tasks.Experimental results show that our approach leads to faster convergence and improved performance compared to standard random sampling.</abstract>
      <url hash="abe9e4ce">2025.acl-srw.15</url>
      <bibkey>feng-etal-2025-pretrained</bibkey>
    </paper>
    <paper id="16">
      <title><fixed-case>C</fixed-case>ausal<fixed-case>G</fixed-case>raph<fixed-case>B</fixed-case>ench: a Benchmark for Evaluating Language Models capabilities of Causal Graph discovery</title>
      <author><first>Nikolay</first><last>Babakov</last><affiliation>Univesity of Santiago de Compostela</affiliation></author>
      <author><first>Ehud</first><last>Reiter</last><affiliation>University of Aberdeen</affiliation></author>
      <author><first>Alberto</first><last>Bugarín-Diz</last><affiliation>Universidad de Santiago de Compostela</affiliation></author>
      <pages>240-258</pages>
      <abstract>This paper introduces CausalGraphBench, a benchmark designed to evaluate the ability of large language models (LLMs) to construct Causal Graphs (CGs), a critical component of reasoning models like Bayesian Networks. The benchmark comprises 35 CGs sourced from publicly available repositories and academic papers, each enriched with detailed metadata to facilitate systematic and consistent evaluation. We explore various LLM-driven methods for CG discovery, analyzing their performance across different graph sizes and complexity levels. Additionally, we examine the effects of data contamination on the quality of the generated CGs.Our findings reveal that methods relying on approaches with a limited number of queries to LLM, particularly those leveraging the full graph context, consistently outperform query-intensive and exhaustive approaches, which tend to overemphasize local relationships. Across all methods, performance declines as graph size increases.</abstract>
      <url hash="25f22553">2025.acl-srw.16</url>
      <bibkey>babakov-etal-2025-causalgraphbench</bibkey>
    </paper>
    <paper id="17">
      <title>Reasoning for Translation: Comparative Analysis of Chain-of-Thought and Tree-of-Thought Prompting for <fixed-case>LLM</fixed-case> Translation</title>
      <author><first>Lam</first><last>Nguyen</last></author>
      <author><first>Yang</first><last>Xu</last><affiliation>Southern University of Science and Technology</affiliation></author>
      <pages>259-275</pages>
      <abstract>As Large Language Models (LLMs) continue to advance in capability, prompt engineering has emerged as a crucial method for optimizing their performance on specialized tasks. While prompting strategies like Zero-shot, Few-shot, Chain-of-Thought, and Tree-of-Thought have demonstrated significant improvements in reasoning tasks, their application to machine translation has received comparatively less attention. This paper systematically evaluates these prompting techniques across diverse language pairs and domains, measuring their effect on translation quality. Our findings reveal substantial performance variations between prompting methods, with certain strategies offering consistent improvements for specific language directions and complexity levels. These results provide valuable insights for developing more effective LLM-based translation systems without requiring model fine-tuning and complement existing works in the field.</abstract>
      <url hash="48f51473">2025.acl-srw.17</url>
      <bibkey>nguyen-xu-2025-reasoning</bibkey>
    </paper>
    <paper id="18">
      <title>i<fixed-case>P</fixed-case>r<fixed-case>O</fixed-case>p: Interactive Prompt Optimization for Large Language Models with a Human in the Loop</title>
      <author><first>Jiahui</first><last>Li</last><affiliation>Otto-Friedrich Universität Bamberg</affiliation></author>
      <author><first>Roman</first><last>Klinger</last><affiliation>Otto-Friedrich Universität Bamberg</affiliation></author>
      <pages>276-285</pages>
      <abstract>Prompt engineering has made significant contributions to the era of large language models, yet its effectiveness depends on the skills of a prompt author. This paper introduces <tex-math>\textit{iPrOp}</tex-math>, a novel interactive prompt optimization approach, to bridge manual prompt engineering and automatic prompt optimization while offering users the flexibility to assess evolving prompts. We aim to provide users with task-specific guidance to enhance human engagement in the optimization process, which is structured through prompt variations, informative instances, predictions generated by large language models along with their corresponding explanations, and relevant performance metrics. This approach empowers users to choose and further refine the prompts based on their individual preferences and needs. It can not only assist non-technical domain experts in generating optimal prompts tailored to their specific tasks or domains, but also enable to study the intrinsic parameters that influence the performance of prompt optimization. The evaluation shows that our approach has the capability to generate improved prompts, leading to enhanced task performance.</abstract>
      <url hash="59117394">2025.acl-srw.18</url>
      <bibkey>li-klinger-2025-iprop</bibkey>
    </paper>
    <paper id="19">
      <title>Evaluating Structured Output Robustness of Small Language Models for Open Attribute-Value Extraction from Clinical Notes</title>
      <author><first>Nikita</first><last>Neveditsin</last></author>
      <author><first>Pawan</first><last>Lingras</last><affiliation>St. Mary’s University</affiliation></author>
      <author><first>Vijay Kumar</first><last>Mago</last><affiliation>York University</affiliation></author>
      <pages>286-296</pages>
      <abstract>We present a comparative analysis of the parseability of structured outputs generated by small language models for open attribute-value extraction from clinical notes. We evaluate three widely used serialization formats: JSON, YAML, and XML, and find that JSON consistently yields the highest parseability. Structural robustness improves with targeted prompting and larger models, but declines for longer documents and certain note types. Our error analysis identifies recurring format-specific failure patterns. These findings offer practical guidance for selecting serialization formats and designing prompts when deploying language models in privacy-sensitive clinical settings.</abstract>
      <url hash="cb1226f5">2025.acl-srw.19</url>
      <bibkey>neveditsin-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="20">
      <title><fixed-case>F</fixed-case>aithful<fixed-case>SAE</fixed-case>: Towards Capturing Faithful Features with Sparse Autoencoders without External Datasets Dependency</title>
      <author><first>Seonglae</first><last>Cho</last><affiliation>Holistic AI</affiliation></author>
      <author><first>Harryn</first><last>Oh</last></author>
      <author><first>Donghyun</first><last>Lee</last></author>
      <author><first>Luis Rodrigues</first><last>Vieira</last></author>
      <author><first>Andrew</first><last>Bermingham</last></author>
      <author><first>Ziad El</first><last>Sayed</last></author>
      <pages>297-314</pages>
      <abstract>Sparse Autoencoders (SAEs) have emerged as a promising solution for decomposing large language model representations into interpretable features. However, Paulo &amp; Belrose (2025) have highlighted instability across different initialization seeds, and Heap et al. (2025) have pointed out that SAEs may not capture model-internal features. These problems likely stem from training SAEs on external datasets—either collected from the Web or generated by another model—which may contain out-of-distribution (OOD) data beyond the model’s generalisation capabilities. This can result in hallucinated SAE features, which we term ”Fake Features”, that misrepresent the model’s internal activations. To address these issues, we propose FaithfulSAE, a method that trains SAEs on the model’s own synthetic dataset. Using FaithfulSAEs, we demonstrate that training SAEs on less-OOD instruction datasets results in SAEs being more stable across seeds. Notably, FaithfulSAEs outperform SAEs trained on webbased datasets in the SAE probing task and exhibit a lower Fake Feature Ratio in 5 out of 7 models. Overall, our approach eliminates the dependency on external datasets, advancing interpretability by better capturing model-internal features while highlighting the often neglected importance of SAE training datasets.</abstract>
      <url hash="63071790">2025.acl-srw.20</url>
      <bibkey>cho-etal-2025-faithfulsae</bibkey>
    </paper>
    <paper id="22">
      <title>Translating Movie Subtitles by Large Language Models using Movie-meta Information</title>
      <author><first>Ashmari</first><last>Pramodya</last></author>
      <author><first>Yusuke</first><last>Sakai</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Justin</first><last>Vasselli</last></author>
      <author><first>Hidetaka</first><last>Kamigaito</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <pages>315-330</pages>
      <abstract>Large language models (LLMs) have advanced natural language processing by understanding, generating, and manipulating texts.Although recent studies have shown that prompt engineering can reduce computational effort and potentially improve translation quality, prompt designs specific to different domains remain challenging. Besides, movie subtitle translation is particularly challenging and understudied, as it involves handling colloquial language, preserving cultural nuances, and requires contextual information such as the movie’s theme and storyline to ensure accurate meaning. This study aims to fill this gap by focusing on the translation of movie subtitles through the use of prompting strategies that incorporate the movie’s meta-information, e.g., movie title, summary, and genre. We build a multilingual dataset which aligns the OpenSubtitles dataset with their corresponding Wikipedia articles and investigate different prompts and their effect on translation performance. Our experiments with GPT-3.5, GPT-4o, and LLaMA-3 models have shown that the presence of meta-information improves translation accuracy. These findings further emphasize the importance of designing appropriate prompts and highlight the potential of LLMs to enhance subtitle translation quality.</abstract>
      <url hash="cac0ff3c">2025.acl-srw.22</url>
      <bibkey>pramodya-etal-2025-translating</bibkey>
    </paper>
    <paper id="23">
      <title><fixed-case>P</fixed-case>un2<fixed-case>P</fixed-case>un: Benchmarking <fixed-case>LLM</fixed-case>s on Textual-Visual <fixed-case>C</fixed-case>hinese-<fixed-case>E</fixed-case>nglish Pun Translation via Pragmatics Model and Linguistic Reasoning</title>
      <author><first>Yiran Rex</first><last>Ma</last><affiliation>Tsinghua University, Tsinghua University and Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Shan</first><last>Huang</last></author>
      <author><first>Yuting</first><last>Xu</last></author>
      <author><first>Ziyu</first><last>Zhou</last></author>
      <author><first>Yuanxi</first><last>Wei</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <pages>331-354</pages>
      <abstract>Puns, as a unique form of linguistic creativity, present significant challenges in cross-lingual translation, particularly between linguistically distant languages like Chinese and English, where it’s often considered a “mission impossible”. We introduce Pun2Pun, a novel benchmark for quantitatively evaluating pun translation between Chinese and English while preserving both linguistic mechanisms and humorous effects. We propose the adaptation of Constant-Variable Optimization (CVO) Model for translation strategy and concomitant Overlap (Ovl) metric for translation quality assessment. Our approach provides a robust quantitative evaluation framework to assess models’ complex linguistic and cultural reasoning capabilities in pun translation. Through extensive experiments on both textual and visual puns, we demonstrate that our translation strategy model significantly improves performance, particularly for better-performing models. Our findings reveal exciting potentials and current limitations of LLMs in preserving sophisticated humor across linguistic and cultural boundaries.</abstract>
      <url hash="c81e4941">2025.acl-srw.23</url>
      <bibkey>ma-etal-2025-pun2pun</bibkey>
    </paper>
    <paper id="24">
      <title>Small Models, Big Impact: Efficient Corpus and Graph-Based Adaptation of Small Multilingual Language Models for Low-Resource Languages</title>
      <author><first>Daniil</first><last>Gurgurov</last></author>
      <author><first>Ivan</first><last>Vykopal</last><affiliation>Kempelen Institute of Intelligent Technologies and Brno University of Technology</affiliation></author>
      <author><first>Josef Van</first><last>Genabith</last><affiliation>German Research Center for AI and Universität des Saarlandes</affiliation></author>
      <author><first>Simon</first><last>Ostermann</last><affiliation>German Research Center for AI</affiliation></author>
      <pages>355-395</pages>
      <abstract>Low-resource languages (LRLs) face significant challenges in natural language processing (NLP) due to limited data. While current state-of-the-art large language models (LLMs) still struggle with LRLs, smaller multilingual models (mLMs) such as mBERT and XLM-R offer greater promise due to a better fit of their capacity to low training data sizes. This study systematically investigates parameter-efficient adapter-based methods for adapting mLMs to LRLs, evaluating three architectures: Sequential Bottleneck, Invertible Bottleneck, and Low-Rank Adaptation. Using unstructured text from GlotCC and structured knowledge from ConceptNet, we show that small adaptation datasets (e.g., up to 1 GB of free-text or a few MB of knowledge graph data) yield gains in intrinsic (masked language modeling) and extrinsic tasks (topic classification, sentiment analysis, and named entity recognition). We find that Sequential Bottleneck adapters excel in language modeling, while Invertible Bottleneck adapters slightly outperform other methods on downstream tasks due to better embedding alignment and larger parameter counts. Adapter-based methods match or outperform full fine-tuning while using far fewer parameters, and smaller mLMs prove more effective for LRLs than massive LLMs like LLaMA-3, GPT-4, and DeepSeek-R1-based distilled models. While adaptation improves performance, pre-training data size remains the dominant factor, especially for languages with extensive pre-training coverage.The code for our experiments is available at https://github.com/d-gurgurov/Knowledge-Driven-Adaptation-LLMs.</abstract>
      <url hash="7a49ac8f">2025.acl-srw.24</url>
      <bibkey>gurgurov-etal-2025-small</bibkey>
    </paper>
    <paper id="25">
      <title>Exploring the Effect of Nominal Compound Structure in Scientific Texts on Reading Times of Experts and Novices</title>
      <author><first>Isabell</first><last>Landwehr</last><affiliation>Universität des Saarlandes</affiliation></author>
      <author><first>Marie-Pauline</first><last>Krielke</last><affiliation>Universität des Saarlandes</affiliation></author>
      <author><first>Stefania</first><last>Degaetano-Ortlieb</last><affiliation>Universität des Saarlandes</affiliation></author>
      <pages>396-408</pages>
      <abstract>We explore how different types of nominal compound complexity in scientific writing, in particular different types of compound structure, affect the reading times of experts and novices. We consider both in-domain and out-of-domain reading and use PoTeC (Jakobi et al. 2024), a corpus containing eye-tracking data of German native speakers reading passages from scientific textbooks. Our results suggest that some compound types are associated with longer reading times and that experts may not only have an advantage while reading in-domain texts, but also while reading out-of-domain.</abstract>
      <url hash="e57550a4">2025.acl-srw.25</url>
      <bibkey>landwehr-etal-2025-exploring</bibkey>
    </paper>
    <paper id="26">
      <title>Insights into Alignment: Evaluating <fixed-case>DPO</fixed-case> and its Variants Across Multiple Tasks</title>
      <author><first>Amir</first><last>Saeidi</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Shivanshu</first><last>Verma</last><affiliation>Google</affiliation></author>
      <author><first>Md Nayem</first><last>Uddin</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Chitta</first><last>Baral</last><affiliation>Arizona State University</affiliation></author>
      <pages>409-421</pages>
      <abstract>This study evaluates Direct Preference Optimization (DPO) and its variants for aligning Large Language Models (LLMs) with human preferences, testing three configurations: (1) with Supervised Fine-Tuning (SFT), (2) without SFT, and (3) without SFT but using an instruction-tuned model. We further investigate how training set size influences model performance. Our evaluation spans 13 benchmarks—covering dialogue, reasoning, mathematical problem-solving, question answering, truthfulness, MT-Bench, Big Bench, and the Open LLM Leaderboard. We find that: (1) alignment methods often achieve near-optimal performance even with smaller subsets of training data; (2) although they offer limited improvements on complex reasoning tasks, they enhance mathematical problem-solving; and (3) using an instruction-tuned model improves truthfulness. These insights highlight the conditions under which alignment methods excel, as well as their limitations.</abstract>
      <url hash="ba8471ab">2025.acl-srw.26</url>
      <bibkey>saeidi-etal-2025-insights</bibkey>
    </paper>
    <paper id="27">
      <title>From Ambiguity to Accuracy: The Transformative Effect of Coreference Resolution on Retrieval-Augmented Generation systems</title>
      <author><first>Youngjoon</first><last>Jang</last><affiliation>Korea University</affiliation></author>
      <author><first>Seongtae</first><last>Hong</last><affiliation>Korea University</affiliation></author>
      <author><first>Junyoung</first><last>Son</last></author>
      <author><first>Sungjin</first><last>Park</last></author>
      <author><first>Chanjun</first><last>Park</last><affiliation>Korea University</affiliation></author>
      <author><first>Heuiseok</first><last>Lim</last></author>
      <pages>422-433</pages>
      <abstract>Retrieval-Augmented Generation (RAG) has emerged as a crucial framework in natural language processing (NLP), improving factual consistency and reducing hallucinations by integrating external document retrieval with large language models (LLMs). However, the effectiveness of RAG is often hindered by coreferential complexity in retrieved documents, which can introduce ambiguity and interfere with in-context learning. In this study, we systematically investigate how entity coreference affects both document retrieval and generative performance in RAG-based systems, focusing on retrieval relevance, contextual understanding, and overall response quality. We demonstrate that coreference resolution enhances retrieval effectiveness and improves question-answering (QA) performance. Through comparative analysis of different pooling strategies in retrieval tasks, we find that mean pooling demonstrates superior context capturing ability after applying coreference resolution. In QA tasks, we discover that smaller models show greater improvement from the disambiguation process, likely due to their limited inherent capacity for handling referential ambiguity. With these findings, this study aims to provide a deeper understanding of the challenges posed by coreferential complexity in RAG, offering guidance for improving retrieval and generation in knowledge-intensive AI applications.</abstract>
      <url hash="671efdf9">2025.acl-srw.27</url>
      <bibkey>jang-etal-2025-ambiguity</bibkey>
    </paper>
    <paper id="28">
      <title>Quantifying the Influence of Irrelevant Contexts on Political Opinions Produced by <fixed-case>LLM</fixed-case>s</title>
      <author><first>Samuele</first><last>D’Avenia</last></author>
      <author><first>Valerio</first><last>Basile</last><affiliation>University of Turin</affiliation></author>
      <pages>434-454</pages>
      <abstract>Several recent works have examined the generations produced by large language models (LLMs) on subjective topics such as political opinions and attitudinal questionnaires. There is growing interest in controlling these outputs to align with specific users or perspectives using model steering techniques. However, several studies have highlighted unintended and unexpected steering effects, where minor changes in the prompt or irrelevant contextual cues influence model-generated opinions.This work empirically tests how irrelevant information can systematically bias model opinions in specific directions. Using the Political Compass Test questionnaire, we conduct a detailed statistical analysis to quantify these shifts using the opinions generated by LLMs in an open-generation setting. The results demonstrate that even seemingly unrelated contexts consistently alter model responses in predictable ways, further highlighting challenges in ensuring the robustness and reliability of LLMs when generating opinions on subjective topics.</abstract>
      <url hash="e0d139ae">2025.acl-srw.28</url>
      <bibkey>davenia-basile-2025-quantifying</bibkey>
    </paper>
    <paper id="29">
      <title>Making Sense of <fixed-case>K</fixed-case>orean Sentences: A Comprehensive Evaluation of <fixed-case>LLM</fixed-case>s through <fixed-case>K</fixed-case>o<fixed-case>SE</fixed-case>nd Dataset</title>
      <author><first>Seunguk</first><last>Yu</last><affiliation>Chung-Ang University</affiliation></author>
      <author><first>Kyeonghyun</first><last>Kim</last><affiliation>Chung-Ang University</affiliation></author>
      <author><first>JungMin</first><last>Yun</last><affiliation>Chung-Ang University</affiliation></author>
      <author><first>YoungBin</first><last>Kim</last><affiliation>Chung-Ang University</affiliation></author>
      <pages>455-469</pages>
      <abstract>Although LLMs have made significant progress in various languages, there are still concerns about their effectiveness with low-resource agglutinative languages compared to languages such as English. In this study, we focused on Korean, a language known for its complex sentence endings, and evaluated LLMs on this challenging aspect. We introduce the Korean Sentence Endings (KoSEnd) dataset, which includes 3,000 sentences, each annotated for the naturalness of 15 sentence ending forms. These were collected from diverse sources to cover a range of contexts. We evaluated 11 LLMs to assess their understanding of Korean sentence endings, analyzing them based on parameter count and prediction consistency. Notably, we found that informing models about the possibility of missing sentence endings improved performance, highlighting the impact of explicitly considering certain linguistic features.</abstract>
      <url hash="bcb097f8">2025.acl-srw.29</url>
      <bibkey>yu-etal-2025-making</bibkey>
    </paper>
    <paper id="30">
      <title>Towards Multi-Perspective <fixed-case>NLP</fixed-case> Systems: A Thesis Proposal</title>
      <author><first>Benedetta</first><last>Muscato</last></author>
      <pages>470-485</pages>
      <abstract>In the field of Natural Language Processing (NLP), a common approach for resolving human disagreement involves establishing a consensus among multiple annotators. However, previous research shows that overlooking individual opinions can result in the marginalization of minority perspectives, particularly in subjective tasks, where annotators may systematically disagree due to their personal preferences. Emerging Multi-Perspective approaches challenge traditional methodologies that treat disagreement as mere noise, instead recognizing it as a valuable source of knowledge shaped by annotators’ diverse backgrounds, life experiences, and values.This thesis proposal aims to (1) identify the challenges of designing disaggregated datasets i.e., preserving individual labels in human-annotated datasets for subjective tasks (2) propose solutions for developing Perspective-Aware by design systems and (3) explore the correlation between human disagreement and model uncertainty leveraging eXplainable AI techniques (XAI).Our long-term goal is to create a framework adaptable to various subjective NLP tasks to promote the development of more responsible and inclusive models.</abstract>
      <url hash="bac38780">2025.acl-srw.30</url>
      <bibkey>muscato-2025-towards</bibkey>
    </paper>
    <paper id="31">
      <title>Enhancing Software Requirements Engineering with Language Models and Prompting Techniques: Insights from the Current Research and Future Directions</title>
      <author><first>Moemen</first><last>Ebrahim</last><affiliation>Alexandria University</affiliation></author>
      <author><first>Shawkat</first><last>Guirguis</last><affiliation>NA</affiliation></author>
      <author><first>Christine</first><last>Basta</last><affiliation>Alexandria University</affiliation></author>
      <pages>486-496</pages>
      <abstract>Large Language Models (LLMs) offer transformative potential for Software Requirements Engineering (SRE), yet critical challenges, including domain ignorance, hallucinations, and high computational costs, hinder their adoption. This paper proposes a conceptual framework that integrates Small Language Models (SLMs) and Knowledge-Augmented LMs (KALMs) with LangChain to address these limitations systematically. Our approach combines: (1) SLMs for efficient, locally deployable requirements processing, (2) KALMs enhanced with Retrieval-Augmented Generation (RAG) to mitigate domain-specific gaps, and (3) LangChain for structured, secure workflow orchestration. We identify and categorize six technical challenges and two research gaps through a systematic review of LLM applications in SRE. To guide practitioners, we distill evidence-based prompt engineering guidelines (Context, Language, Examples, Keywords) and propose prompting strategies (e.g., Chain-of-Verification) to improve output reliability. The paper establishes a theoretical foundation for scalable, trustworthy AI-assisted SRE and outlines future directions, including domain-specific prompt templates and hybrid validation pipelines.</abstract>
      <url hash="5b34f6cc">2025.acl-srw.31</url>
      <bibkey>ebrahim-etal-2025-enhancing</bibkey>
    </paper>
    <paper id="32">
      <title>Question Decomposition for Retrieval-Augmented Generation</title>
      <author><first>Paul J. L.</first><last>Ammann</last><affiliation>Humboldt State University</affiliation></author>
      <author><first>Jonas</first><last>Golde</last><affiliation>Department of Computer Science, Humboldt University Berlin, Humboldt Universität Berlin</affiliation></author>
      <author><first>Alan</first><last>Akbik</last><affiliation>Humboldt Universität Berlin</affiliation></author>
      <pages>497-507</pages>
      <abstract>Grounding large language models (LLMs) in verifiable external sources is a well-established strategy for generating reliable answers. Retrieval-augmented generation (RAG) is one such approach, particularly effective for tasks like question answering: it retrieves passages that are semantically related to the question and then conditions the model on this evidence. However, multi-hop questions, such as <i>“Which company among NVIDIA, Apple, and Google made the biggest profit in 2023?,”</i> challenge RAG because relevant facts are often distributed across multiple documents rather than co-occurring in one source, making it difficult for standard RAG to retrieve sufficient information. To address this, we propose a RAG pipeline that incorporates question decomposition: (i) an LLM decomposes the original query into sub-questions, (ii) passages are retrieved for each sub-question, and (iii) the merged candidate pool is reranked to improve the coverage and precision of the retrieved evidence. We show that question decomposition effectively assembles complementary documents, while reranking reduces noise and promotes the most relevant passages before answer generation. We evaluate our approach on the MultiHop-RAG and HotpotQA, showing gains in retrieval (<tex-math>MRR@10: +36.7\%</tex-math>) and answer accuracy (<tex-math>F1: +11.6\%</tex-math>) over standard RAG baselines. The pipeline is a practical, drop-in enhancement requiring no task-specific training or specialized indexing.</abstract>
      <url hash="9aedb127">2025.acl-srw.32</url>
      <bibkey>ammann-etal-2025-question</bibkey>
    </paper>
    <paper id="33">
      <title>Neural Machine Translation for Agglutinative Languages via Data Rejuvenation</title>
      <author><first>Chen</first><last>Zhao</last><affiliation>Inner Mongolia University Of Technology</affiliation></author>
      <author><first>Yatu</first><last>Ji</last><affiliation>NA</affiliation></author>
      <author><first>Ren</first><last>Qing-Dao-Er-Ji</last></author>
      <author><first>Nier</first><last>Wu</last><affiliation>Inner Mongolia University Of Technology</affiliation></author>
      <author><first>Lei</first><last>Shi</last><affiliation>NA</affiliation></author>
      <author><first>Fu</first><last>Liu</last><affiliation>Inner Mongolia University Of Technology</affiliation></author>
      <author><first>Yepai</first><last>Jia</last></author>
      <pages>508-516</pages>
      <abstract>In Recent years, advances in Neural Machine Translation (NMT) heavily rely on large-scale parallel corpora. Within the context of China’s Belt and Road Initiative, there is increasing demand for improving translation quality from agglutinative languages (e.g., Mongolian, Arabic) to Chinese. However, the translation scenarios for agglutinative languages (which form words by concatenating morphemes with clear boundaries) face significant challenges including data sparsity, quality imbalance, and inactive sample proliferation due to their morphological complexity and syntactic flexibility. This study presents a systematic analysis of data distribution characteristics in agglutinative languages and proposes a dual-module framework combining fine-grained inactive sample identification with target-side rejuvenation. Our framework first establishes a multi-dimensional evaluation system to accurately identify samples exhibiting low-frequency morphological interference or long-range word order mismatches. Subsequently, the target-side rejuvenation mechanism generates diversified noise-resistant translations through iterative optimization of sample contribution weights. Experimental results on four low-resource agglutinative language tasks demonstrate significant performance improvements (BLEU +2.1–3.4) across mainstream NMT architectures. Architecture-agnostic validation further confirms the framework’s generalizability.</abstract>
      <url hash="5fadea8c">2025.acl-srw.33</url>
      <bibkey>zhao-etal-2025-neural</bibkey>
    </paper>
    <paper id="34">
      <title><fixed-case>S</fixed-case>t<fixed-case>R</fixed-case>u<fixed-case>C</fixed-case>om: A Novel Dataset of Structured Code Comments in <fixed-case>R</fixed-case>ussian</title>
      <author><first>Maria</first><last>Dziuba</last></author>
      <author><first>Valentin</first><last>Malykh</last><affiliation>International IT University</affiliation></author>
      <pages>517-527</pages>
      <abstract>Structured code comments in docstring format are essential for code comprehension and maintenance, but existing machine learning models for their generation perform poorly for Russian compared to English. To bridge this gap, we present StRuCom — the first large-scale dataset (153K examples) specifically designed for Russian code documentation. Unlike machine-translated English datasets that distort terminology (e.g., technical loanwords vs. literal translations) and docstring structures, StRuCom combines human-written comments from Russian GitHub repositories with synthetically generated ones, ensuring compliance with Python, Java, JavaScript, C#, and Go standards through automated validation.</abstract>
      <url hash="b1d0ad7a">2025.acl-srw.34</url>
      <bibkey>dziuba-malykh-2025-strucom</bibkey>
    </paper>
    <paper id="35">
      <title>A Semantic Uncertainty Sampling Strategy for Back-Translation in Low-Resources Neural Machine Translation</title>
      <author><first>Yepai</first><last>Jia</last></author>
      <author><first>Yatu</first><last>Ji</last><affiliation>NA</affiliation></author>
      <author><first>Xiang</first><last>Xue</last><affiliation>NA</affiliation></author>
      <author><first>Shilei@imufe.edu.cn</first><last>Shilei@imufe.edu.cn</last><affiliation>NA</affiliation></author>
      <author><first>Qing-Dao-Er-Ji</first><last>Ren</last><affiliation>NA</affiliation></author>
      <author><first>Nier</first><last>Wu</last><affiliation>Inner Mongolia University Of Technology</affiliation></author>
      <author><first>Na</first><last>Liu</last><affiliation>NA</affiliation></author>
      <author><first>Chen</first><last>Zhao</last><affiliation>Inner Mongolia University Of Technology</affiliation></author>
      <author><first>Fu</first><last>Liu</last><affiliation>Inner Mongolia University Of Technology</affiliation></author>
      <pages>528-538</pages>
      <abstract>Back-translation has been proven effective in enhancing the performance of Neural Machine Translation (NMT), with its core mechanism relying on synthesizing parallel corpora to strengthen model training. However, while traditional back-translation methods alleviate the data scarcity in low-resource machine translation, their dependence on random sampling strategies ignores the semantic quality of monolingual data. This results in the contamination of model training through the inclusion of substantial low-quality samples in the generated corpora. To mitigate noise interference, additional training iterations or model scaling are required, significantly increasing computational costs. To address this challenge, this study proposes a Semantic Uncertainty Sampling strategy, which prioritizes sentences with higher semantic uncertainty as training samples by computationally evaluating the complexity of unannotated monolingual data. Experiments were conducted on three typical low-resource agglutinative language pairs: Mongolian-Chinese, Uyghur-Chinese, and Korean-Chinese. Results demonstrate an average BLEU score improvement of +1.7 on test sets across all three translation tasks, confirming the method’s effectiveness in enhancing translation accuracy and fluency. This approach provides a novel pathway for the efficient utilization of unannotated data in low-resource language scenarios.</abstract>
      <url hash="c263c39d">2025.acl-srw.35</url>
      <bibkey>jia-etal-2025-semantic</bibkey>
    </paper>
    <paper id="36">
      <title><fixed-case>S</fixed-case>panish Dialect Classification: A Comparative Study of Linguistically Tailored Features, Unigrams and <fixed-case>BERT</fixed-case> Embeddings</title>
      <author><first>Laura</first><last>Zeidler</last><affiliation>niversity of Technology Nuremberg</affiliation></author>
      <author><first>Chris</first><last>Jenkins</last><affiliation>University of Stuttgart, Universität Stuttgart</affiliation></author>
      <author><first>Filip</first><last>Miletić</last><affiliation>University of Stuttgart</affiliation></author>
      <author><first>Sabine</first><last>Schulte Im Walde</last><affiliation>University of Stuttgart</affiliation></author>
      <pages>539-547</pages>
      <abstract>The task of automatic dialect classification is typically tackled using traditional machine-learning models with bag-of-words unigram features. We explore two alternative methods for distinguishing dialects across 20 Spanish-speaking countries:(i) Support vector machine and decision tree models were trained on dialectal features tailored to the Spanish dialects, combined with standard unigrams. (ii) A pre-trained BERT model was fine-tuned on the task.Results show that the tailored features generally did not have a positive impact on traditional model performance, but provide a salient way of representing dialects in a content-agnostic manner. The BERT model wins over traditional models but with only a tiny margin, while sacrificing explainability and interpretability.</abstract>
      <url hash="bb398842">2025.acl-srw.36</url>
      <bibkey>zeidler-etal-2025-spanish</bibkey>
    </paper>
    <paper id="37">
      <title><fixed-case>S</fixed-case>equential<fixed-case>B</fixed-case>reak: Large Language Models Can be Fooled by Embedding Jailbreak Prompts into Sequential Prompt Chains</title>
      <author><first>Bijoy Ahmed</first><last>Saiem</last></author>
      <author><first>MD Sadik Hossain</first><last>Shanto</last><affiliation>Bangladesh University of Engineering and Technology</affiliation></author>
      <author><first>Rakib</first><last>Ahsan</last><affiliation>United International University</affiliation></author>
      <author><first>Md Rafi Ur</first><last>Rashid</last></author>
      <pages>548-579</pages>
      <abstract>As the use of Large Language Models (LLMs) expands, so do concerns about their vulnerability to jailbreak attacks. We introduce SequentialBreak, a novel single-query jailbreak technique that arranges multiple benign prompts in sequence with a hidden malicious instruction among them to bypass safety mechanisms. Sequential prompt chains in a single query can lead LLMs to focus on certain prompts while ignoring others. By embedding a malicious prompt within a prompt chain, we show that LLMs tend to ignore the harmful context and respond to all prompts including the harmful one. We demonstrate the effectiveness of our attack across diverse scenarios—including Q&amp;A systems, dialogue completion tasks, and levelwise gaming scenario—highlighting its adaptability to varied prompt structures. The variability of prompt structures shows that SequentialBreak is adaptable to formats beyond those discussed here. Experiments show that SequentialBreak only uses a single query to significantly outperform existing baselines on both open-source and closed-source models. These findings underline the urgent need for more robust defenses against prompt-based attacks. The Results and website are available on https://anonymous.4open.science/r/JailBreakAttack-4F3B/.</abstract>
      <url hash="aeb5ba4c">2025.acl-srw.37</url>
      <bibkey>saiem-etal-2025-sequentialbreak</bibkey>
    </paper>
    <paper id="38">
      <title>A Dual-Layered Evaluation of Geopolitical and Cultural Bias in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Sean</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Hyuhng Joon</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <pages>580-595</pages>
      <abstract>As large language models (LLMs) are increasingly deployed across diverse linguistic and cultural contexts, understanding their behavior in both factual and disputable scenarios is essential—especially when their outputs may shape public opinion or reinforce dominant narratives. In this paper, we define two types of bias in LLMs: model bias (bias stemming from model training) and inference bias (bias induced by the language of the query), through a two-phase evaluation.Phase 1 evaluates LLMs on factual questions where a single verifiable answer exists, assessing whether models maintain consistency across different query languages. Phase 2 expands the scope by probing geopolitically sensitive disputes, where responses may reflect culturally embedded or ideologically aligned perspectives. We construct a manually curated dataset spanning both factual and disputable QA, across four languages and question types. The results show that Phase 1 exhibits query language-induced alignment, while Phase 2 reflects an interplay between the model’s training context and query language. This paper offers a structured framework for evaluating LLM behavior across neutral and sensitive topics, providing insights for future LLM deployment and culturally-aware evaluation practices in multilingual contexts.WARNING: this paper covers East Asian issues which may be politically sensitive.</abstract>
      <url hash="3d0ed93d">2025.acl-srw.38</url>
      <bibkey>kim-kim-2025-dual</bibkey>
    </paper>
    <paper id="39">
      <title><fixed-case>MA</fixed-case>-<fixed-case>COIR</fixed-case>: Leveraging Semantic Search Index and Generative Models for Ontology-Driven Biomedical Concept Recognition</title>
      <author><first>Shanshan</first><last>Liu</last><affiliation>University of Tsukuba, Tsukuba University and RIKEN</affiliation></author>
      <author><first>Noriki</first><last>Nishida</last><affiliation>RIKEN</affiliation></author>
      <author><first>Rumana Ferdous</first><last>Munne</last><affiliation>RIKEN</affiliation></author>
      <author><first>Narumi</first><last>Tokunaga</last></author>
      <author><first>Yuki</first><last>Yamagata</last></author>
      <author><first>Kouji</first><last>Kozaki</last><affiliation>Osaka Electro-Communication University</affiliation></author>
      <author><first>Yuji</first><last>Matsumoto</last><affiliation>RIKEN Center for Advanced Intelligence Project</affiliation></author>
      <pages>596-607</pages>
      <abstract>Recognizing biomedical concepts in the text is vital for ontology refinement, knowledge graph construction, and concept relationship discovery. However, traditional concept recognition methods, relying on explicit mention identification, often fail to capture complex concepts not explicitly stated in the text. To overcome this limitation, we introduce MA-COIR, a framework that reformulates concept recognition as an indexing-recognition task. By assigning semantic search indexes (ssIDs) to concepts, MA-COIR resolves ambiguities in ontology entries and enhances recognition efficiency. Using a pretrained BART-based model fine-tuned on small datasets, our approach reduces computational requirements to facilitate adoption by domain experts. Furthermore, we incorporate large language model (LLM)-generated queries and synthetic data to improve recognition in low-resource settings. Experimental results on three scenarios (CDR, HPO, and HOIP) highlight the effectiveness of MA-COIR in recognizing both explicit and implicit concepts without the need for mention-level annotations during inference, advancing ontology-driven concept recognition in biomedical domain applications. Our code and constructed data are available at https://github.com/sl-633/macoir-master.</abstract>
      <url hash="f925460b">2025.acl-srw.39</url>
      <bibkey>liu-etal-2025-ma</bibkey>
    </paper>
    <paper id="40">
      <title><fixed-case>L</fixed-case>ib<fixed-case>V</fixed-case>uln<fixed-case>W</fixed-case>atch: A Deep Assessment Agent System and Leaderboard for Uncovering Hidden Vulnerabilities in Open-Source <fixed-case>AI</fixed-case> Libraries</title>
      <author><first>Zekun</first><last>Wu</last><affiliation>Department of Computer Science, University College London, University of London and Holistic AI</affiliation></author>
      <author><first>Seonglae</first><last>Cho</last><affiliation>Holistic AI</affiliation></author>
      <author><first>Umar</first><last>Mohammed</last><affiliation>NA</affiliation></author>
      <author><first>Cristian Enrique Munoz</first><last>Villalobos</last></author>
      <author><first>Kleyton</first><last>Da Costa</last></author>
      <author><first>Xin</first><last>Guan</last><affiliation>Holistic AI</affiliation></author>
      <author><first>Theo</first><last>King</last><affiliation>Holistic AI</affiliation></author>
      <author><first>Ze</first><last>Wang</last></author>
      <author><first>Emre</first><last>Kazim</last><affiliation>University College London, University of London</affiliation></author>
      <author><first>Adriano</first><last>Koshiyama</last><affiliation>Department of Computer Science, University College London, University of London</affiliation></author>
      <pages>608-656</pages>
      <abstract>Open-source AI libraries are foundational to modern AI systems, yet they present significant, underexamined risks spanning security, licensing, maintenance, supply chain integrity, and regulatory compliance. We introduce LibVulnWatch, a system that leverages recent advances in large language models and agentic workflows to perform deep, evidence-based evaluations of these libraries. Built on a graph-based orchestration of specialized agents, the framework extracts, verifies, and quantifies risk using information from repositories, documentation, and vulnerability databases. LibVulnWatch produces reproducible, governance-aligned scores across five critical domains, publishing results to a public leaderboard for ongoing ecosystem monitoring. Applied to 20 widely used libraries—including ML frameworks, LLM inference engines, and agent orchestration tools—our approach covers up to 88% of OpenSSF Scorecard checks while surfacing up to 19 additional risks per library, such as critical RCE vulnerabilities, missing SBOMs, and regulatory gaps. By integrating advanced language technologies with the practical demands of software risk assessment, this work demonstrates a scalable, transparent mechanism for continuous supply chain evaluation and informed library selection.</abstract>
      <url hash="37bb26c0">2025.acl-srw.40</url>
      <bibkey>wu-etal-2025-libvulnwatch</bibkey>
    </paper>
    <paper id="41">
      <title>Interactive Text Games: Lookahead Is All You Need!</title>
      <author><first>Hosein</first><last>Rezaei</last></author>
      <author><first>James Alfred</first><last>Walker</last></author>
      <author><first>Frank</first><last>Soboczenski</last><affiliation>University of York</affiliation></author>
      <pages>657-664</pages>
      <abstract>The cross-modal grounding of LLMs has recently garnered significant attention, while grounding them in textual interactions has been less explored. As the first of its kind, the GLAM framework utilises LLMs as agents in interactive text-based games to investigate their grounding capabilities. However, it faces the challenge of low computational efficiency, which hinders further experiments. This paper proposes the use of Lookahead models for action selection, demonstrating through empirical results that the approach can substantially improve training speed, achieving performance gains relative to the size of the action space.</abstract>
      <url hash="a7fb7e6e">2025.acl-srw.41</url>
      <bibkey>rezaei-etal-2025-interactive</bibkey>
    </paper>
    <paper id="42">
      <title>Evaluating Credibility and Political Bias in <fixed-case>LLM</fixed-case>s for News Outlets in <fixed-case>B</fixed-case>angladesh</title>
      <author><first>Tabia Tanzin</first><last>Prama</last></author>
      <author><first>Md. Saiful</first><last>Islam</last></author>
      <pages>665-677</pages>
      <abstract>Large language models (LLMs) are widelyused in search engines to provide direct an-swers, while AI chatbots retrieve updated infor-mation from the web. As these systems influ-ence how billions access information, evaluat-ing the credibility of news outlets has becomecrucial. We audit nine LLMs from OpenAI,Google, and Meta to assess their ability to eval-uate the credibility and political bias of the top20 most popular news outlets in Bangladesh.While most LLMs rate the tested outlets, largermodels often refuse to rate sources due to in-sufficient information, while smaller modelsare more prone to hallucinations. We create adataset of credibility ratings and political iden-tities based on journalism experts’ opinions andcompare these with LLM responses. We findstrong internal consistency in LLM credibil-ity ratings, with an average correlation coeffi-cient (ρ) of 0.72, but moderate alignment withexpert evaluations, with an average ρ of 0.45.Most LLMs (GPT-4, GPT-4o-mini, Llama 3.3,Llama-3.1-70B, Llama 3.1 8B, and Gemini 1.5Pro) in their default configurations favor theleft-leaning Bangladesh Awami League, givinghigher credibility ratings, and show misalign-ment with human experts. These findings high-light the significant role of LLMs in shapingnews and political information</abstract>
      <url hash="f2c37d23">2025.acl-srw.42</url>
      <bibkey>prama-islam-2025-evaluating</bibkey>
    </paper>
    <paper id="43">
      <title>The Evolution of Gen Alpha Slang: Linguistic Patterns and <fixed-case>AI</fixed-case> Translation Challenges</title>
      <author><first>Ishita</first><last>Ishita</last><affiliation>International Institute of Information Technology Hyderabad</affiliation></author>
      <author><first>Radhika</first><last>Mamidi</last><affiliation>International Institute of Information Technology Hyderabad</affiliation></author>
      <pages>678-686</pages>
      <abstract>Generation Alpha (born 2010-2024) is the first generation fully raised within the digital ecosystem. They exhibit unique linguistic behaviours influenced by rampant online communication and platform-specific cultures. This study examines the rapid evolution of Gen Alpha slang through a comparative analysis of Millennial and Gen Z vernacular. We identify three core linguistic patterns: extreme lexical compression, digital culture-driven semantic shifts and part-of-speech conversion. We construct a comprehensive slang corpus sourced from online platforms and evaluate the performance of four AI translation systems (viz. Google Translate, ChatGPT 4, Gemini 1.0, DeepSeek v3) on over 100 slang terms. Our results reveal significant translation challenges rooted in culturally-bound terms from gaming, meme culture, and mental health discourse. Most errors are the result of inadequate cultural contextualization, with literal translations dominating the error patterns. Our findings highlight the critical limitations in current language models and emphasize the need for adaptive, culturally sensitive and context-aware frameworks that can handle the dynamic lexicon of evolving youth vernacular.</abstract>
      <url hash="adaee455">2025.acl-srw.43</url>
      <bibkey>ishita-mamidi-2025-evolution</bibkey>
    </paper>
    <paper id="44">
      <title>Light-Weight Hallucination Detection using Contrastive Learning for Conditional Text Generation</title>
      <author><first>Miyu</first><last>Yamada</last><affiliation>Institute of Science Tokyo</affiliation></author>
      <author><first>Yuki</first><last>Arase</last><affiliation>Tokyo Institute of Technology, Tokyo Institute of Technology, RIKEN and AIST, National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <pages>687-694</pages>
      <abstract>We propose a simple and light-weight, yet effective hallucination detection method for conditional text generation. Hallucinated outputs include information that is either absent from and/or difficult to infer from the input context. Leveraging this feature, we add contrastive learning to the hallucination detection classifier to pull faithful outputs and input contexts together while pushing hallucinated outputs apart. Experimental results confirm that our method on top of RoBERTa improves binary hallucination detection performance, outperforming much larger GPT-4o prompting. Remarkably, our method shows higher performance for outputs where hallucinated spans are sparse.</abstract>
      <url hash="57fd195f">2025.acl-srw.44</url>
      <bibkey>yamada-arase-2025-light</bibkey>
    </paper>
    <paper id="45">
      <title>Fact from Fiction: Finding Serialized Novels in Newspapers</title>
      <author><first>Pascale</first><last>Feldkamp</last></author>
      <author><first>Alie</first><last>Lassche</last></author>
      <author><first>Katrine Frøkjær</first><last>Baunvig</last><affiliation>NA</affiliation></author>
      <author><first>Kristoffer</first><last>Nielbo</last><affiliation>Aarhus University</affiliation></author>
      <author><first>Yuri</first><last>Bizzoni</last></author>
      <pages>695-707</pages>
      <abstract>Digitized literary corpora of the 19th century favor canonical and novelistic forms, sidelining a broader and more diverse literary production. Serialized fiction – widely read but embedded in newspapers – remains especially underexplored, particularly in low-resource languages like Danish. This paper addresses this gap by developing methods to identify fiction in digitized Danish newspapers (1818–1848).We (1) introduce a manually annotated dataset of 1,394 articles and (2) evaluate classification pipelines using both selected linguistic features and embeddings, achieving F1-scores of up to 0.91. Finally, we (3) analyze feuilleton fiction via interpretable features to test its drift in discourse from neighboring nonfiction.Our results support the construction of alternative literary corpora and contribute to ongoing work on modeling the fiction–nonfiction boundary by operationalizing discourse-level distinctions at scale.</abstract>
      <url hash="ae2c5d0b">2025.acl-srw.45</url>
      <bibkey>feldkamp-etal-2025-fact</bibkey>
    </paper>
    <paper id="46">
      <title>Cross-Genre Learning for <fixed-case>O</fixed-case>ld <fixed-case>E</fixed-case>nglish Poetry <fixed-case>POS</fixed-case> Tagging</title>
      <author><first>Irene</first><last>Miani</last></author>
      <author><first>Sara</first><last>Stymne</last><affiliation>Uppsala University</affiliation></author>
      <author><first>Gregory R.</first><last>Darwin</last><affiliation>Uppsala University</affiliation></author>
      <pages>708-724</pages>
      <abstract>Poetry has always distinguished itself from other literary genres in many ways, including grammatically and syntactically. These differences are evident not only in modern literature but also in earlier stages. Linguistic analysis tools struggle to address these differences. This paper focuses on the dichotomy between Old English poetry and prose, specifically in the context of the POS tagging task.Two annotated corpora representing each genre were analyzed to show that there are several types of structural differences between Old English poetry and prose. For POS tagging, we conduct experiments on both a detailed tag set with over 200 tags and a mapping to the UPOS tag set with 17 tags. We establish a baseline and conduct two cross-genre experiments to investigate the effect of different proportions of prose and poetry data. Across both tag sets, our results indicate that if the divergence between two genres is substantial, simply increasing the quantity of training data from the support genre does not necessarily improve prediction accuracy. However, incorporating even a small amount of target data can lead to better performance compared to excluding it entirely. This study not only highlights the linguistic differences between Old English poetry and prose but also emphasizes the importance of developing effective NLP tools for underrepresented historical languages across all genres.</abstract>
      <url hash="dabef2a3">2025.acl-srw.46</url>
      <bibkey>miani-etal-2025-cross</bibkey>
    </paper>
    <paper id="47">
      <title>A Computational Framework to Identify Self-Aspects in Text</title>
      <author><first>Jaya</first><last>Caporusso</last><affiliation>Jozef Stefan Institute and Mednarodna Podiplomska Šola Jožefa Stefana</affiliation></author>
      <author><first>Matthew</first><last>Purver</last><affiliation>Queen Mary University of London and Institut Jožef Stefan</affiliation></author>
      <author><first>Senja</first><last>Pollak</last></author>
      <pages>725-739</pages>
      <abstract>This Ph.D. proposal introduces a plan to develop a computational framework to identify Self-aspects in text. The Self is a multifaceted construct and it is reflected in language. While it is described across disciplines like cognitive science and phenomenology, it remains underexplored in natural language processing (NLP). Many of the aspects of the Self align with psychological and other well-researched phenomena (e.g., those related to mental health), highlighting the need for systematic NLP-based analysis. In line with this, we plan to introduce an ontology of Self-aspects and a gold-standard annotated dataset. Using this foundation, we will develop and evaluate conventional discriminative models, generative large language models, and embedding-based retrieval approaches against four main criteria: interpretability, ground-truth adherence, accuracy, and computational efficiency. Top-performing models will be applied in case studies in mental health and empirical phenomenology.</abstract>
      <url hash="a72a1f11">2025.acl-srw.47</url>
      <bibkey>caporusso-etal-2025-computational</bibkey>
    </paper>
    <paper id="48">
      <title>Prompting the Muse: Generating Prosodically-Correct <fixed-case>L</fixed-case>atin Speech with Large Language Models</title>
      <author><first>Michele</first><last>Ciletti</last><affiliation>University of Foggia</affiliation></author>
      <pages>740-745</pages>
      <abstract>This paper presents a workflow that compels an audio-enabled large language model to recite Latin poetry with metrically accurate stress. One hundred hexameters from the Aeneid and the opening elegiac epistula of Ovid’s Heroides constitute the test bed, drawn from the Pedecerto XML corpus, where ictic syllables are marked. A preprocessing pipeline syllabifies each line, converts alien graphemes into approximate English-Italian counterparts, merges obligatory elisions, adds commas on caesurae, upper-cases every ictic syllable, and places a grave accent on its vowel. Verses are then supplied, one at a time, to an LLM-based Text-to-Speech model under a compact system prompt that instructs slow, articulated delivery. From ten stochastic realisations per verse, a team of Latin experts retained the best; at least one fully correct file was found for 91% of the 216 lines. Upper-casing plus accent marking proved the strongest cue, while hyphenating syllables offered no benefit. Remaining errors cluster around cognates where the model inherits a Romance or English stress template. The corpus of validated audio and all scripts are openly released on Zenodo, opening avenues for pedagogy, accessibility, and prosodic research.</abstract>
      <url hash="f4a6e500">2025.acl-srw.48</url>
      <bibkey>ciletti-2025-prompting</bibkey>
    </paper>
    <paper id="49">
      <title>Can a Large Language Model Keep My Secrets? A Study on <fixed-case>LLM</fixed-case>-Controlled Agents</title>
      <author><first>Niklas</first><last>Hemken</last></author>
      <author><first>Sai</first><last>Koneru</last><affiliation>Karlsruher Institut für Technologie</affiliation></author>
      <author><first>Florian</first><last>Jacob</last><affiliation>Karlsruher Institut für Technologie</affiliation></author>
      <author><first>Hannes</first><last>Hartenstein</last><affiliation>Karlsruher Institut für Technologie</affiliation></author>
      <author><first>Jan</first><last>Niehues</last></author>
      <pages>746-759</pages>
      <abstract>Agents controlled by Large Language Models (LLMs) can assist with natural language tasks across domains and applications when given access to confidential data.When such digital assistants interact with their potentially adversarial environment, confidentiality of the data is at stake.We investigated whether an LLM-controlled agent can, in a manner similar to humans, consider confidentiality when responding to natural language requests involving internal data.For evaluation, we created a synthetic dataset consisting of confidentiality-aware planning and deduction tasks in organizational access control.The dataset was developed from human input, LLM-generated content, and existing datasets.It includes various everyday scenarios in which access to confidential or private information is requested.We utilized our dataset to evaluate the ability to infer confidentiality-aware behavior in such scenarios by differentiating between legitimate and illegitimate access requests.We compared a prompting-based and a fine-tuning-based approach, to evaluate the performance of Llama 3 and GPT-4o-mini in this domain.In addition, we conducted a user study to establish a baseline for human evaluation performance in these tasks. We found humans reached an accuracy of up to 79%.Prompting techniques, such as chain-of-thought and few-shot prompting, yielded promising results, but still fell short of real-world applicability and do not surpass human baseline performance. However, we found that fine-tuning significantly improves the agent’s access decisions, reaching up to 98% accuracy, making it promising for future confidentiality-aware applications when data is available.</abstract>
      <url hash="ab88d7a4">2025.acl-srw.49</url>
      <bibkey>hemken-etal-2025-large</bibkey>
    </paper>
    <paper id="50">
      <title>Chart Question Answering from Real-World Analytical Narratives</title>
      <author><first>Maeve</first><last>Hutchinson</last></author>
      <author><first>Radu</first><last>Jianu</last><affiliation>NA</affiliation></author>
      <author><first>Aidan</first><last>Slingsby</last><affiliation>NA</affiliation></author>
      <author><first>Jo</first><last>Wood</last><affiliation>City University</affiliation></author>
      <author><first>Pranava</first><last>Madhyastha</last><affiliation>City, University of London</affiliation></author>
      <pages>760-773</pages>
      <abstract>We present a new dataset for chart question answering (CQA) constructed from visualization notebooks. The dataset features real-world, multi-view charts paired with natural language questions grounded in analytical narratives. Unlike prior benchmarks, our data reflects ecologically valid reasoning workflows. Benchmarking state-of-the-art multimodal large language models reveals a significant performance gap, with GPT-4.1 achieving an accuracy of 69.3%, underscoring the challenges posed by this more authentic CQA setting.</abstract>
      <url hash="02d2698f">2025.acl-srw.50</url>
      <bibkey>hutchinson-etal-2025-chart</bibkey>
    </paper>
    <paper id="51">
      <title>Low-Perplexity <fixed-case>LLM</fixed-case>-Generated Sequences and Where To Find Them</title>
      <author><first>Arthur</first><last>Wuhrmann</last></author>
      <author><first>Andrei</first><last>Kucharavy</last><affiliation>University of Applied Sciences Western Switzerland, Sierre (HES-SO Valais) and Swiss Federal Institute of Technology Lausanne</affiliation></author>
      <author><first>Anastasiia</first><last>Kucherenko</last><affiliation>University of Applied Sciences Western Switzerland, Sierre (HES-SO Valais)</affiliation></author>
      <pages>774-783</pages>
      <abstract>As Large Language Models (LLMs) become increasingly widespread, understanding how specific training data shapes their outputs is crucial for transparency, accountability, privacy, and fairness. To explore how LLMs leverage and replicate their training data, we introduce a systematic approach centered on analyzing low-perplexity sequences—high-probability text spans generated by the model. Our pipeline reliably extracts such long sequences across diverse topics while avoiding degeneration, then traces them back to their sources in the training data. Surprisingly, we find that a substantial portion of these low-perplexity spans cannot be mapped to the corpus. For those that do match, we quantify the distribution of occurrences across source documents, highlighting the scope and nature of verbatim recall and paving a way toward better understanding of how LLMs training data impacts their behavior.</abstract>
      <url hash="195d0212">2025.acl-srw.51</url>
      <bibkey>wuhrmann-etal-2025-low</bibkey>
    </paper>
    <paper id="52">
      <title><fixed-case>C</fixed-case>o<fixed-case>L</fixed-case>e<fixed-case>M</fixed-case>: A framework for semantic interpretation of <fixed-case>R</fixed-case>ussian-language tables based on contrastive learning</title>
      <author><first>Kirill</first><last>Tobola</last></author>
      <author><first>Nikita</first><last>Dorodnykh</last><affiliation>Matrosov Institute for System Dynamics and Control Theory, Siberian Branch of the Russian Academy of Sciences (ISDCT SB RAS)</affiliation></author>
      <pages>784-794</pages>
      <abstract>Tables are extensively utilized to represent and store data, however, they often lack explicit semantics necessary for machine interpretation of their contents. Semantic table interpretation is essential for integrating structured data with knowledge graphs, yet existing methods face challenges with Russian-language tables due to limited labeled data and linguistic peculiarities. This paper introduces a contrastive learning approach to minimize reliance on manual labeling and enhance the accuracy of column annotation for rare semantic types. The proposed method adapts contrastive learning for tabular data through augmentations and employs a distilled multilingual BERT model trained on the unlabeled RWT corpus (comprising 7.4 million columns). The resulting table representations are incorporated into the RuTaBERT pipeline, reducing computational overhead. Experimental results demonstrate a micro-F1 score of 97% and a macro-F1 score of 92%, surpassing several baseline approaches. These findings emphasize the efficiency of the proposed method in addressing data sparsity and handling unique features of the Russian language. The results further confirm that contrastive learning effectively captures semantic similarities among columns without explicit supervision, which is particularly vital for rare data types.</abstract>
      <url hash="55df369b">2025.acl-srw.52</url>
      <bibkey>tobola-dorodnykh-2025-colem</bibkey>
    </paper>
    <paper id="53">
      <title>Mitigating Hallucination by Integrating Knowledge Graphs into <fixed-case>LLM</fixed-case> Inference – a Systematic Literature Review</title>
      <author><first>Robin</first><last>Wagner</last><affiliation>NA</affiliation></author>
      <author><first>Emanuel</first><last>Kitzelmann</last><affiliation>Technische Hochschule Brandenburg</affiliation></author>
      <author><first>Ingo</first><last>Boersch</last><affiliation>Technische Hochschule Brandenburg</affiliation></author>
      <pages>795-805</pages>
      <abstract>Large Language Models (LLMs) demonstrate strong performance on different language tasks, but tend to hallucinate – generate plausible but factually incorrect outputs. Recently, several approaches to integrate Knowledge Graphs (KGs) into LLM inference were published to reduce hallucinations. This paper presents a systematic literature review (SLR) of such approaches. Following established SLR methodology, we identified relevant work by systematically search in different academic online libraries and applying a selection process. Nine publications were chosen for in-depth analysis. Our synthesis reveals differences and similarities of how the KG is accessed, traversed, and how the context is finally assembled. KG integration can significantly improve LLM performance on benchmark datasets and additionally to mitigate hallucination enhance reasoning capabilities, explainability, and access to domain-specific knowledge. We also point out current limitations and outline directions for future work.</abstract>
      <url hash="91c38e3d">2025.acl-srw.53</url>
      <bibkey>wagner-etal-2025-mitigating</bibkey>
    </paper>
    <paper id="55">
      <title>Semantic alignment in hyperbolic space for fine-grained emotion classification</title>
      <author><first>Ashish</first><last>Kumar</last><affiliation>Indian Institute of Technology Roorke</affiliation></author>
      <author><first>Durga</first><last>Toshniwal</last></author>
      <pages>806-813</pages>
      <abstract>Existing approaches to fine-grained emotion classification (FEC) often operate in Euclidean space, where the flat geometry limits the ability to distinguish semantically similar emotion labels (e.g., *annoyed* vs. *angry*). While prior research has explored hyperbolic geometry to capture fine-grained label distinctions, it typically relies on predefined hierarchies and ignores semantically similar negative labels that can mislead the model into making incorrect predictions. In this work, we propose HyCoEM (Hyperbolic Contrastive Learning for Emotion Classification), a semantic alignment framework that leverages the Lorentz model of hyperbolic space. Our approach embeds text and label representations into hyperbolic space via the exponential map, and employs a contrastive loss to bring text embeddings closer to their true labels while pushing them away from adaptively selected, semantically similar negatives. This enables the model to learn label embeddings without relying on a predefined hierarchy and better captures subtle distinctions by incorporating information from both positive and challenging negative labels. Experimental results on two benchmark FEC datasets demonstrate the effectiveness of our approach over baseline methods.</abstract>
      <url hash="f8abda6e">2025.acl-srw.55</url>
      <bibkey>kumar-toshniwal-2025-semantic</bibkey>
    </paper>
    <paper id="56">
      <title><fixed-case>I</fixed-case> Speak for the Árboles: Developing a Dependency Treebank for <fixed-case>S</fixed-case>panish <fixed-case>L</fixed-case>2 and Heritage Speakers</title>
      <author><first>Emiliana</first><last>Pulido</last></author>
      <author><first>Robert</first><last>Pugh</last></author>
      <author><first>Zoey</first><last>Liu</last><affiliation>University of Florida</affiliation></author>
      <pages>814-822</pages>
      <abstract>We introduce the first dependency treebank containing Universal Dependencies (UD) annotations for Spanish learner writing from the UC Davis COWSL2H corpus. Our annotations include lemmatization, POS tagging, and syntactic dependencies. We adapt the existing UD framework for Spanish L1 to account forlearner-specific features such as code-switching and non-canonical syntax. A suite of parsing evaluation experiments shows that parsers trained on learner data together with moderate sizes of Spanish L1 data can yield reasonable performance. Our annotations are openly accessible to motivate future development of learner-oriented language technologies.</abstract>
      <url hash="be6a84f8">2025.acl-srw.56</url>
      <bibkey>pulido-etal-2025-speak</bibkey>
    </paper>
    <paper id="57">
      <title>Evaluating Tokenizer Adaptation Methods for Large Language Models on Low-Resource Programming Languages</title>
      <author><first>Georgy</first><last>Andryushchenko</last><affiliation>Innopolis University</affiliation></author>
      <author><first>Vladimir V.</first><last>Ivanov</last></author>
      <pages>823-833</pages>
      <abstract>Large language models (LLMs), which are primarily trained on high-resource programming languages (HRPLs), tend to perform sub-optimally for low-resource programming languages (LRPLs). This study investigates the impact of tokenizer adaptation methods on improving code generation for LRPLs. StarCoder 2 and DeepSeek-Coder models adapted to Elixir and Racket using methods such as Fast Vocabulary Transfer (FVT), FOCUS, and Zero-shot Tokenizer Transfer (ZeTT) are evaluated and compared with the original and fine-tuned models. Our experiments reveal that ZeTT outperforms other methods, achieving significant improvements in handling syntax, program logic, and data types for LRPLs. However, we also highlight performance declines in non-target languages like Python after tokenizer adaptation. The study approves the positive impact of tokenizer adaptation in enhancing LRPL code generation and suggests directions for future research, including token embeddings improvement.</abstract>
      <url hash="8bded85f">2025.acl-srw.57</url>
      <bibkey>andryushchenko-ivanov-2025-evaluating</bibkey>
    </paper>
    <paper id="59">
      <title>Learning and Enforcing Context-Sensitive Control for <fixed-case>LLM</fixed-case>s</title>
      <author><first>Mohammad</first><last>Albinhassan</last></author>
      <author><first>Pranava</first><last>Madhyastha</last><affiliation>City, University of London</affiliation></author>
      <author><first>Mark</first><last>Law</last></author>
      <author><first>Alessandra</first><last>Russo</last><affiliation>Imperial College London</affiliation></author>
      <pages>834-842</pages>
      <abstract>Controlling the output of Large Language Models (LLMs) through context-sensitive constraints has emerged as a promising approach to overcome the limitations of Context-Free Grammars (CFGs) in guaranteeing generation validity. However, such constraints typically require manual specification—a significant barrier demanding specialized expertise. We introduce a framework that automatically learns context-sensitive constraints from LLM interactions through a two-phase process: syntactic exploration to gather diverse outputs for constraint learning, followed by constraint exploitation to enforce these learned rules during generation. Experiments demonstrate that our method enables even small LLMs (1B parameters) to learn and generate with perfect constraint adherence, outperforming larger counterparts and state-of-the-art reasoning models. This work represents the first integration of context-sensitive grammar learning with LLM generation, eliminating manual specification while maintaining generation validity.</abstract>
      <url hash="6e5b4bf5">2025.acl-srw.59</url>
      <bibkey>albinhassan-etal-2025-learning</bibkey>
    </paper>
    <paper id="61">
      <title>When Will the Tokens End? Graph-Based Forecasting for <fixed-case>LLM</fixed-case>s Output Length</title>
      <author><first>Grzegorz</first><last>Piotrowski</last></author>
      <author><first>Mateusz</first><last>Bystroński</last></author>
      <author><first>Mikołaj</first><last>Hołysz</last><affiliation>Technical University of Wroclaw and Technical University of Wroclaw</affiliation></author>
      <author><first>Jakub</first><last>Binkowski</last></author>
      <author><first>Grzegorz</first><last>Chodak</last><affiliation>Technical University of Wroclaw</affiliation></author>
      <author><first>Tomasz Jan</first><last>Kajdanowicz</last><affiliation>Wroclaw University of Science and Technology</affiliation></author>
      <pages>843-848</pages>
      <abstract>Large Language Models (LLMs) are typically trained to predict the next token in a sequence. However, their internal representations often encode signals that go beyond immediate next-token prediction. In this work, we investigate whether these hidden states also carry information about the remaining length of the generated output—an implicit form of foresight (CITATION). We formulate this as a regression problem where, at generation step <tex-math>t</tex-math>, the target is the number of remaining tokens <tex-math>y_t = T - t</tex-math>, with <tex-math>T</tex-math> as the total output length.We propose two approaches: (1) an aggregation-based model that combines hidden states from multiple transformer layers <tex-math>\ell \in \{8, \dots, 15\}</tex-math> using element-wise operations such as mean or sum, and (2) a <i>Layerwise Graph Regressor</i> that treats layerwise hidden states as nodes in a fully connected graph and applies a Graph Neural Network (GNN) to predict <tex-math>y_t</tex-math>. Both models operate on frozen LLM embeddings without requiring end-to-end fine-tuning.Accurately estimating remaining output length has both theoretical and practical implications. From an interpretability standpoint, it suggests that LLMs internally track their generation progress. From a systems perspective, it enables optimizations such as output-length-aware scheduling (CITATION). Our graph-based model achieves state-of-the-art performance on the Alpaca dataset using LLaMA-3-8B-Instruct, reducing normalized mean absolute error (NMAE) by over 50% in short-output scenarios.</abstract>
      <url hash="b8d03156">2025.acl-srw.61</url>
      <bibkey>piotrowski-etal-2025-will</bibkey>
    </paper>
    <paper id="62">
      <title>Only for the Unseen Languages, Say the Llamas: On the Efficacy of Language Adapters for Cross-lingual Transfer in <fixed-case>E</fixed-case>nglish-centric <fixed-case>LLM</fixed-case>s</title>
      <author><first>Julian</first><last>Schlenker</last><affiliation>Universität Mannheim</affiliation></author>
      <author><first>Jenny</first><last>Kunz</last><affiliation>Linköping University</affiliation></author>
      <author><first>Tatiana</first><last>Anikina</last><affiliation>German Research Center for AI</affiliation></author>
      <author><first>Günter</first><last>Neumann</last><affiliation>German Research Center for AI</affiliation></author>
      <author><first>Simon</first><last>Ostermann</last><affiliation>German Research Center for AI</affiliation></author>
      <pages>849-871</pages>
      <abstract>Most state-of-the-art large language models (LLMs) are trained mainly on English data, limiting their effectiveness on non-English, especially low-resource, languages. This study investigates whether language adapters can facilitate cross-lingual transfer in English-centric LLMs. We train language adapters for 13 languages using Llama 2 (7B) and Llama 3.1 (8B) as base models, and evaluate their effectiveness on two downstream tasks (MLQA and SIB-200) using either task adapters or in-context learning. Our results reveal that language adapters improve performance for languages not seen during pretraining, but provide negligible benefit for seen languages. These findings highlight the limitations of language adapters as a general solution for multilingual adaptation in English-centric LLMs.</abstract>
      <url hash="deee6e68">2025.acl-srw.62</url>
      <bibkey>schlenker-etal-2025-unseen</bibkey>
    </paper>
    <paper id="63">
      <title><fixed-case>H</fixed-case>y<fixed-case>ILR</fixed-case>: Hyperbolic Instance-Specific Local Relationships for Hierarchical Text Classification</title>
      <author><first>Ashish</first><last>Kumar</last><affiliation>Indian Institute of Technology Roorke</affiliation></author>
      <author><first>Durga</first><last>Toshniwal</last></author>
      <pages>872-883</pages>
      <abstract>Recent approaches to Hierarchical Text Classification (HTC) rely on capturing the global label hierarchy, which contains static and often redundant relationships. Instead, the hierarchical relationships within the instance-specific set of positive labels are more important, as they focus on the relevant parts of the hierarchy. These localized relationships can be modeled as a semantic alignment between the text and its positive labels within the embedding space. However, without explicitly encoding the global hierarchy, achieving this alignment directly in Euclidean space is challenging, as its flat geometry does not naturally support hierarchicalrelationships. To address this, we propose Hyperbolic Instance-Specific Local Relationships (HyILR), which models instance-specific relationships using the Lorentz model of hyperbolic space. Text and label features are projected into hyperbolic space, where a contrastive loss aligns text with its labels. This loss is guided by a hierarchy-aware negative sampling strategy, ensuring the selection of structurally and semantically relevant negatives. By leveraging hyperbolic geometry for this alignment, our approach inherently captures hierarchical relationships and eliminates the need for global hierarchy encoding. Experimental results on four benchmark datasets validate the superior performance of HyILR over baseline methods.</abstract>
      <url hash="f43a07bf">2025.acl-srw.63</url>
      <bibkey>kumar-toshniwal-2025-hyilr</bibkey>
    </paper>
    <paper id="64">
      <title>Are <fixed-case>LLM</fixed-case>s Truly Graph-Savvy? A Comprehensive Evaluation of Graph Generation</title>
      <author><first>Ege</first><last>Demirci</last></author>
      <author><first>Rithwik</first><last>Kerur</last></author>
      <author><first>Ambuj</first><last>Singh</last><affiliation>UC Santa Barbara</affiliation></author>
      <pages>884-897</pages>
      <abstract>While large language models (LLMs) have demonstrated impressive capabilities across diverse tasks, their ability to generate valid graph structures remains underexplored. We evaluate fifteen state-of-the-art LLMs on five specialized graph generation tasks spanning delivery networks, social networks, quantum circuits, gene-disease networks, and transportation systems. We also test the LLMs using 3 different prompt types: direct, iterative feedback, and program-augmented. Models supported with explicit reasoning modules (o3-mini-high, o1, Claude 3.7 Sonnet, DeepSeek-R1) solve more than twice as many tasks as their general-purpose peers, independent of parameter count. Error forensics reveals two recurring failure modes: smaller parameter size Llama models often violate basic structural constraints, whereas Claude models respect topology but mismanage higher-order logical rules. Allowing models to refine their answers iteratively yields uneven gains, underscoring fundamental differences in error-correction capacity. This work demonstrates that graph competence stems from specialized training methodologies rather than scale, establishing a framework for developing truly graph-savvy language models. Results and verification scripts available at https://github.com/egedemirci/Are-LLMs-Truly-Graph-Savvy-A-Comprehensive-Evaluation-of-Graph-Generation.</abstract>
      <url hash="686cadeb">2025.acl-srw.64</url>
      <bibkey>demirci-etal-2025-llms</bibkey>
    </paper>
    <paper id="65">
      <title>Pragmatic Perspective on Assessing Implicit Meaning Interpretation in Sentiment Analysis Models</title>
      <author><first>Rashid</first><last>Mustafin</last><affiliation>Norwegian School of Economics and Business Administration</affiliation></author>
      <pages>898-907</pages>
      <abstract>Drawing on pragmatic theories of implicature by Grice (1975) and Levinson (1983), according to which speakers often convey more than it is explicitly said, the paper argues that interpreting texts with implicit meaning correctly is essential for precise natural language understanding. To illustrate the challenges in computational interpretation of implicatures, the study introduces a series of illustrative micro-experiments with the use of four transformer models fine-tuned for sentiment analysis. In these micro-experiments, the models classified sentences specifically designed to expose difficulties in handling implicit meaning. The study demonstrates that contrasting qualitative pragmatic analysis with the models’ tendency to focus on formal linguistic markers can reveal the limitations of supervised machine learning methods in detecting implicit sentiments.</abstract>
      <url hash="230551a1">2025.acl-srw.65</url>
      <bibkey>mustafin-2025-pragmatic</bibkey>
    </paper>
    <paper id="66">
      <title>Foundations of <fixed-case>PEERS</fixed-case>: Assessing <fixed-case>LLM</fixed-case> Role Performance in Educational Simulations</title>
      <author><first>Jasper Meynard</first><last>Arana</last><affiliation>Adamson University</affiliation></author>
      <author><first>Kristine Ann M.</first><last>Carandang</last><affiliation>Asian Institute of Management and Asian Institute of Management</affiliation></author>
      <author><first>Ethan Robert</first><last>Casin</last></author>
      <author><first>Christian</first><last>Alis</last><affiliation>Asian Institute of Management</affiliation></author>
      <author><first>Daniel Stanley</first><last>Tan</last><affiliation>Open University of the Netherlands</affiliation></author>
      <author><first>Erika Fille</first><last>Legara</last><affiliation>Asian Institute of Management</affiliation></author>
      <author><first>Christopher</first><last>Monterola</last><affiliation>Asian Institute of Management</affiliation></author>
      <pages>908-918</pages>
      <abstract>In education, peer instruction (PI) is widely recognized as an effective active learning strategy. However, real-world evaluations of PI are often limited by logistical constraints and variability in classroom settings. This paper introduces PEERS (Peer Enhanced Educational Realistic Simulation), a simulation framework that integrates Agent-Based Modeling (ABM), Large Language Models (LLMs), and Bayesian Knowledge Tracing (BKT) to emulate student learning dynamics. As an initial step, this study focuses on evaluating whether LLM-powered agents can effectively assume the roles of teachers and students within the simulation. Human evaluations and topic-based metrics show that LLMs can generate role-consistent and contextually appropriate classroom dialogues. These results serve as a foundational milestone toward building realistic, AI-driven educational simulations. Future work will include simulating the complete PEERS framework and validating its accuracy through actual classroom-based PI sessions. This research aims to contribute a scalable, cost-effective methodology for studying instructional strategies in controlled yet realistic environments.</abstract>
      <url hash="d26f386f">2025.acl-srw.66</url>
      <bibkey>arana-etal-2025-foundations</bibkey>
    </paper>
    <paper id="67">
      <title>The Role of Exploration Modules in Small Language Models for Knowledge Graph Question Answering</title>
      <author><first>Yi-Jie</first><last>Cheng</last></author>
      <author><first>Oscar</first><last>Chew</last><affiliation>ASUS</affiliation></author>
      <author><first>Yun-Nung</first><last>Chen</last><affiliation>National Taiwan University</affiliation></author>
      <pages>919-928</pages>
      <abstract>Integrating knowledge graphs (KGs) into the reasoning processes of large language models (LLMs) has emerged as a promising approach to mitigate hallucination. However, existing work in this area often relies on proprietary or extremely large models, limiting accessibility and scalability. In this study, we investigate the capabilities of existing integration methods for small language models (SLMs) in KG-based question answering and observe that their performance is often constrained by their limited ability to traverse and reason over knowledge graphs. To address this limitation, we propose leveraging simple and efficient exploration modules to handle knowledge graph traversal in place of the language model itself. Experiment results demonstrate that these lightweight modules effectively improve the performance of small language models on knowledge graph question answering tasks. Source code: <url>https://github.com/yijie-cheng/SLM-ToG/</url>.</abstract>
      <url hash="3b0721fa">2025.acl-srw.67</url>
      <bibkey>cheng-etal-2025-role</bibkey>
    </paper>
    <paper id="68">
      <title>Bridging the Embodiment Gap in Agricultural Knowledge Representation for Language Models</title>
      <author><first>Vasu</first><last>Jindal</last><affiliation>Columbia University</affiliation></author>
      <author><first>Huijin</first><last>Ju</last><affiliation>NA</affiliation></author>
      <author><first>Zili</first><last>Lyu</last><affiliation>NA</affiliation></author>
      <pages>929-938</pages>
      <abstract>This paper quantifies the “embodiment gap” between disembodied language models and embodied agricultural knowledge communication through mixed-methods analysis with 78 farmers. Our key contributions include: (1) the Embodied Knowledge Representation Framework (EKRF), a novel computational architecture with specialized lexical mapping that incorporates embodied linguistic patterns from five identified domains of agricultural expertise; (2) the Embodied Prompt Engineering Protocol (EPEP), which reduced the embodiment gap by 47.3% through systematic linguistic scaffolding techniques; and (3) the Embodied Knowledge Representation Index (EKRI), a new metric for evaluating embodied knowledge representation in language models. Implementation results show substantial improvements across agricultural domains, with particularly strong gains in tool usage discourse (58.7%) and soil assessment terminology (67% reduction in embodiment gap). This research advances both theoretical understanding of embodied cognition in AI and practical methodologies to enhance LLM performance in domains requiring embodied expertise.</abstract>
      <url hash="d65dafc5">2025.acl-srw.68</url>
      <bibkey>jindal-etal-2025-bridging</bibkey>
    </paper>
    <paper id="69">
      <title>Building <fixed-case>J</fixed-case>apanese Creativity Benchmarks and Applying them to Enhance <fixed-case>LLM</fixed-case> Creativity</title>
      <author><first>So</first><last>Fukuda</last><affiliation>Waseda University</affiliation></author>
      <author><first>Hayato</first><last>Ogawa</last><affiliation>NA</affiliation></author>
      <author><first>Kaito</first><last>Horio</last><affiliation>Waseda University</affiliation></author>
      <author><first>Daisuke</first><last>Kawahara</last><affiliation>Waseda University</affiliation></author>
      <author><first>Tomohide</first><last>Shibata</last><affiliation>LY Corporation and SB Intuitions</affiliation></author>
      <pages>939-957</pages>
      <abstract>To evaluate the creativity of large language models (LLMs) in Japanese, we construct three benchmarks: Japanese Creativity Questions (JCQ), Divergent Association Task (DAT), and Story Alteration Task (SAT). JCQ comprehensively evaluates creativity using LLMs. Meanwhile, DAT and SAT measure specific aspects of creative ability using embeddings. We also analyze correlations between JCQ and DAT, JCQ and SAT, and DAT and SAT. While JCQ provides comprehensive evaluation, it is relatively time and resource intensive. In contrast, DAT and SAT offer lower comprehensiveness but enable quick, low-cost assessment. Additionally, we investigate whether training with DAT contributes to enhancing LLM creativity.</abstract>
      <url hash="02b20b5b">2025.acl-srw.69</url>
      <bibkey>fukuda-etal-2025-building</bibkey>
    </paper>
    <paper id="70">
      <title>Towards Robust Sentiment Analysis of Temporally-Sensitive Policy-Related Online Text</title>
      <author><first>Charles</first><last>Alba</last></author>
      <author><first>Benjamin C</first><last>Warner</last><affiliation>Washington University, Saint Louis</affiliation></author>
      <author><first>Akshar</first><last>Saxena</last><affiliation>NA</affiliation></author>
      <author><first>Jiaxin</first><last>Huang</last><affiliation>Washington University, Saint Louis</affiliation></author>
      <author><first>Ruopeng</first><last>An</last><affiliation>NA</affiliation></author>
      <pages>958-976</pages>
      <abstract>Sentiment analysis in policy-related studies typically involves annotating a subset of data to fine-tune a pre-trained model, which is subsequently used to classify sentiments in the remaining unlabeled texts, enabling policy researchers to analyze sentiments in novel policy contexts under resource constraints. We argue that existing methods fail to adequately capture the temporal volatility inherent in policy-related sentiments, which are subject to external shocks and evolving discourse of opinions. We propose methods accounting for the temporal dynamics of policy-related texts. Specifically, we propose leveraging continuous time-series clustering to select data points for annotation based on temporal trends and subsequently apply model merging techniques <tex-math>-</tex-math> each fine-tuned separately on data from distinct time intervals. Our results indicate that continuous time-series clustering followed by fine-tuning a single unified model achieves superior performance, outperforming existing methods by an average F1-score of 2.71%. This suggests that language models can generalize to temporally sensitive texts when provided with temporally representative samples. Nevertheless, merging multiple time-specific models <tex-math>-</tex-math> particularly via greedy soup and TIES <tex-math>-</tex-math> achieves competitive performance, suggesting practical applications in dynamically evolving policy scenarios.</abstract>
      <url hash="4d1a57d8">2025.acl-srw.70</url>
      <bibkey>alba-etal-2025-towards</bibkey>
    </paper>
    <paper id="71">
      <title>Is Partial Linguistic Information Sufficient for Discourse Connective Disambiguation? A Case Study of Concession</title>
      <author><first>Takuma</first><last>Sato</last></author>
      <author><first>Ai</first><last>Kubota</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Koji</first><last>Mineshima</last><affiliation>Keio University</affiliation></author>
      <pages>977-990</pages>
      <abstract>Discourse relations are sometimes explicitly conveyed by specific connectives.However, some connectives can signal multiple discourse relations; in such cases, disambiguation is necessary to determine which relation is intended.This task is known as *discourse connective disambiguation* (Pitler and Nenkova, 2009), and particular attention is often given to connectives that can convey both *concession* and other relations (e.g., *synchronous*).In this study, we conducted experiments to analyze which linguistic features play an important role in the disambiguation of polysemous connectives in Japanese.A neural language model (BERT) was fine-tuned using inputs from which specific linguistic features (e.g., word order, specific lexicon, etc.) had been removed.We analyzed which linguistic features affect disambiguation by comparing the model’s performance.Our results show that even after performing drastic removal, such as deleting one of the two arguments that constitute the discourse relation, the model’s performance remained relatively robust.However, the removal of certain lexical items or words belonging to specific lexical categories significantly degraded disambiguation performance, highlighting their importance in identifying the intended discourse relation.</abstract>
      <url hash="028d32ca">2025.acl-srw.71</url>
      <bibkey>sato-etal-2025-partial</bibkey>
    </paper>
    <paper id="72">
      <title>Semantic Frame Induction from a Real-World Corpus</title>
      <author><first>Shogo</first><last>Tsujimoto</last><affiliation>Nagoya University</affiliation></author>
      <author><first>Kosuke</first><last>Yamada</last><affiliation>CyberAgent, Inc.</affiliation></author>
      <author><first>Ryohei</first><last>Sasano</last><affiliation>Nagoya University</affiliation></author>
      <pages>991-997</pages>
      <abstract>Recent studies on semantic frame induction have demonstrated that the emergence of pre-trained language models (PLMs) has led to more accurate results.However, most existing studies evaluate the performance using frame resources such as FrameNet, which may not accurately reflect real-world language usage.In this study, we conduct semantic frame induction using the Colossal Clean Crawled Corpus (C4) and assess the applicability of existing frame induction methods to real-world data.Our experimental results demonstrate that existing frame induction methods are effective on real-world data and that frames corresponding to novel concepts can be induced.</abstract>
      <url hash="5a55e2f0">2025.acl-srw.72</url>
      <bibkey>tsujimoto-etal-2025-semantic</bibkey>
    </paper>
    <paper id="73">
      <title>Lost and Found: Computational Quality Assurance of Crowdsourced Knowledge on Morphological Defectivity in <fixed-case>W</fixed-case>iktionary</title>
      <author><first>Jonathan</first><last>Sakunkoo</last></author>
      <author><first>Annabella</first><last>Sakunkoo</last></author>
      <pages>998-1003</pages>
      <abstract>Morphological defectivity is an intriguing and understudied phenomenon in linguistics. Addressing defectivity, where expected inflectional forms are absent, is essential for improving the accuracy of NLP tools in morphologically rich languages. However, traditional linguistic resources often lack coverage of morphological gaps as such knowledge requires significant human expertise and effort to document and verify. For scarce linguistic phenomena in under-explored languages, Wikipedia and Wiktionary often serve as among the few accessible resources. Despite their extensive reach, their reliability has been a subject of controversy. This study customizes a novel neural morphological analyzer to annotate Latin and Italian corpora. Using the massive annotated data, crowd-sourced lists of defective verbs compiled from Wiktionary are validated computationally. Our results indicate that while Wiktionary provides a highly reliable account of Italian morphological gaps, 7% of Latin lemmata listed as defective show strong corpus evidence of being non-defective. This discrepancy highlights potential limitations of crowd-sourced wikis as definitive sources of linguistic knowledge, particularly for less-studied phenomena and languages, despite their value as resources for rare linguistic features. By providing scalable tools and methods for quality assurance of crowd-sourced data, this work advances computational morphology and expands linguistic knowledge of defectivity in non-English, morphologically rich languages.</abstract>
      <url hash="6cc06f7a">2025.acl-srw.73</url>
      <bibkey>sakunkoo-sakunkoo-2025-lost</bibkey>
    </paper>
    <paper id="77">
      <title>Improving Explainability of Sentence-level Metrics via Edit-level Attribution for Grammatical Error Correction</title>
      <author><first>Takumi</first><last>Goto</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Justin</first><last>Vasselli</last></author>
      <author><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <pages>1004-1015</pages>
      <abstract>Various evaluation metrics have been proposed for Grammatical Error Correction (GEC), but many, particularly reference-free metrics, lack explainability. This lack of explainability hinders researchers from analyzing the strengths and weaknesses of GEC models and limits the ability to provide detailed feedback for users. To address this issue, we propose attributing sentence-level scores to individual edits, providing insight into how specific corrections contribute to the overall performance. For the attribution method, we use Shapley values, from cooperative game theory, to compute the contribution of each edit. Experiments with existing sentence-level metrics demonstrate high consistency across different edit granularities and show approximately 70% alignment with human evaluations. In addition, we analyze biases in the metrics based on the attribution results, revealing trends such as the tendency to ignore orthographic edits.</abstract>
      <url hash="57c4d2c3">2025.acl-srw.77</url>
      <bibkey>goto-etal-2025-improving</bibkey>
    </paper>
    <paper id="79">
      <title>Proposal: From One-Fit-All to Perspective Aware Modeling</title>
      <author><first>Leixin</first><last>Zhang</last></author>
      <pages>1016-1025</pages>
      <abstract>Variation in human annotation and human perspectives has drawn increasing attention in natural language processing research. Disagreement observed in data annotation challenges the conventional assumption of a single “ground truth” and uniform models trained on aggregated annotations, which tend to overlook minority viewpoints and individual perspectives. This proposal investigates three directions of perspective-oriented research: First, annotation formats that better capture the granularity and uncertainty of individual judgments; Second, annotation modeling that leverages socio-demographic features to better represent and predict underrepresented or minority perspectives; Third, personalized text generation that tailors outputs to individual users’ preferences and communicative styles. The proposed tasks aim to advance natural language processing research towards more faithfully reflecting the diversity of human interpretation, enhancing both inclusiveness and fairness in language technologies.</abstract>
      <url hash="15e9bd7d">2025.acl-srw.79</url>
      <bibkey>zhang-2025-proposal</bibkey>
    </paper>
    <paper id="81">
      <title>Controlling Language Confusion in Multilingual <fixed-case>LLM</fixed-case>s</title>
      <author><first>Nahyun</first><last>Lee</last></author>
      <author><first>Yeongseo</first><last>Woo</last></author>
      <author><first>Hyunwoo</first><last>Ko</last><affiliation>haerae.com and OnelineAI</affiliation></author>
      <author><first>Guijin</first><last>Son</last></author>
      <pages>1026-1035</pages>
      <abstract>Large language models often suffer from language confusion, a phenomenon in which responses are partially or entirely generated in unintended languages. This critically degrades the user experience, especially in low-resource settings. We hypothesize that this issue stems from limitations in conventional fine-tuning objectives, such as supervised learning, which optimize the likelihood of correct tokens without explicitly penalizing undesired outputs such as cross-lingual mixing. Analysis of loss trajectories during pretraining further reveals that models fail to distinguish between monolingual and language-mixed texts, highlighting the absence of inherent pressure to avoid such confusion. In this work, we apply ORPO, which adds penalties for unwanted output styles to standard SFT, effectively suppressing language-confused generations. ORPO maintains strong language consistency, even under high decoding temperatures, while preserving general QA performance. Our findings suggest that incorporating appropriate penalty terms can effectively mitigate language confusion in multilingual models, particularly in low-resource scenarios.</abstract>
      <url hash="b0899b38">2025.acl-srw.81</url>
      <bibkey>lee-etal-2025-controlling</bibkey>
    </paper>
    <paper id="82">
      <title>Grammatical Error Correction via Sequence Tagging for <fixed-case>R</fixed-case>ussian</title>
      <author><first>Regina</first><last>Nasyrova</last><affiliation>Institute for Artificial Intelligence, Lomonosov Moscow State University</affiliation></author>
      <author><first>Alexey</first><last>Sorokin</last><affiliation>Yandex and Lomonosov Moscow State University</affiliation></author>
      <pages>1036-1050</pages>
      <abstract>We introduce a modified sequence tagging architecture, proposed in (Omelianchuk et al., 2020), for the Grammatical Error Correction of the Russian language. We propose language-specific operation set and preprocessing algorithm as well as a classification scheme which makes distinct predictions for insertions and other operations. The best versions of our models outperform previous approaches and set new SOTA on the two Russian GEC benchmarks – RU-Lang8 and GERA, while achieve competitive performance on RULEC-GEC.</abstract>
      <url hash="a4e0372c">2025.acl-srw.82</url>
      <bibkey>nasyrova-sorokin-2025-grammatical</bibkey>
    </paper>
    <paper id="83">
      <title><fixed-case>DRUM</fixed-case>: Learning Demonstration Retriever for Large <fixed-case>MU</fixed-case>lti-modal Models</title>
      <author><first>Ellen</first><last>Yi-Ge</last><affiliation>CMU, Carnegie Mellon University and Carnegie Mellon University</affiliation></author>
      <author><first>Jiechao</first><last>Gao</last></author>
      <author><first>Wei</first><last>Han</last></author>
      <author><first>Wei</first><last>Zhu</last><affiliation>University of Hong Kong</affiliation></author>
      <pages>1051-1063</pages>
      <abstract>Recently, large language models (LLMs) have demonstrated impressive capabilities in dealing with new tasks with the help of in-context learning (ICL). In the study of Large Vision-Language Models (LVLMs), when implementing ICL, researchers usually adopt the naive strategies like fixed demonstrations across different samples, or selecting demonstrations directly via a visual-language embedding model. These methods do not guarantee the configured demonstrations fit the need of the LVLMs. To address this issue, we propose a novel framework, demonstration retriever for large multi-modal model (DRUM), which fine-tunes the CLIP embedding model to better meet the LVLM’s needs. First, we discuss the retrieval strategies for a visual-language task, assuming an embedding model is given. And we propose to concate the image and text embeddings to enhance the retrieval performance. Second, we propose to re-rank the the embedding model’s retrieved demonstrations via the LVLM’s feedbacks, and calculate a list-wise ranking loss for training the embedding model. Third, we propose an iterative demonstration mining strategy to improve the training of the embedding model. Through extensive experiments on 3 types of visual-language tasks, 7 benchmark datasets, our DRUM framework is proven to be effective in boosting the LVLM’s in-context learning performance via retrieving more proper demonstrations.</abstract>
      <url hash="c6fa4bd7">2025.acl-srw.83</url>
      <bibkey>yi-ge-etal-2025-drum</bibkey>
    </paper>
    <paper id="84">
      <title><fixed-case>G</fixed-case>er<fixed-case>M</fixed-case>ed<fixed-case>IQ</fixed-case>: A Resource for Simulated and Synthesized Anamnesis Interview Responses in <fixed-case>G</fixed-case>erman</title>
      <author><first>Justin</first><last>Hofenbitzer</last><affiliation>Technische Universität München</affiliation></author>
      <author><first>Sebastian</first><last>Schöning</last></author>
      <author><first>Belle</first><last>Sebastian</last><affiliation>Die Universitätsmedizin Mannheim (UMM), Ruprecht-Karls-Universität Heidelberg</affiliation></author>
      <author><first>Jacqueline</first><last>Lammert</last><affiliation>Technical University of Munich and Technical University of Munich</affiliation></author>
      <author><first>Luise</first><last>Modersohn</last><affiliation>Technische Universität München and Friedrich-Schiller Universität Jena</affiliation></author>
      <author><first>Martin</first><last>Boeker</last><affiliation>Technische Universität München</affiliation></author>
      <author><first>Diego</first><last>Frassinelli</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <pages>1064-1078</pages>
      <abstract>Due to strict privacy regulations, text corpora in non-English clinical contexts are scarce. Consequently, synthetic data generation using Large Language Models (LLMs) emerges as a promising strategy to address this data gap. To evaluate the ability of LLMs in generating synthetic data, we applied them to our novel German Medical Interview Questions Corpus (GerMedIQ), which consists of 4,524 unique, simulated question-response pairs in German. We augmented our corpus by prompting 18 different LLMs to generate responses to the same questions. Structural and semantic evaluations of the generated responses revealed that large-sized language models produced responses comparable to those provided by humans. Additionally, an LLM-as-a-judge study, combined with a human baseline experiment assessing response acceptability, demonstrated that human raters preferred the responses generated by Mistral (124B) over those produced by humans. Nonetheless, our findings indicate that using LLMs for data augmentation in non-English clinical contexts requires caution.</abstract>
      <url hash="c99e372a">2025.acl-srw.84</url>
      <bibkey>hofenbitzer-etal-2025-germediq</bibkey>
    </paper>
    <paper id="85">
      <title>Unstructured Minds, Predictable Machines: A Comparative Study of Narrative Cohesion in Human and <fixed-case>LLM</fixed-case> Stream-of-Consciousness Writing</title>
      <author><first>Nellia</first><last>Dzhubaeva</last><affiliation>Universität des Saarlandes</affiliation></author>
      <author><first>Katharina</first><last>Trinley</last></author>
      <author><first>Laura</first><last>Pissani</last></author>
      <pages>1079-1096</pages>
      <abstract>This paper examines differences between stream-of-consciousness (SoC) narratives written by humans and those generated by large language models (LLMs) to assess narrative coherence and personality expression. We generated texts by prompting LLMs (Llama-3.1-8B &amp; DeepSeek-R1-Distill-Llama-8B) with the first half of SoC-essays while either providing the models with the personality characteristics (Big Five) or omitting them. Our analysis revealed consistently low similarity between LLM-generated continuations and original human texts, as measured by cosine similarity, perplexity, and BLEU scores. Including explicit personality traits significantly enhanced Llama-3.1-8B’s performance, particularly in BLEU scores.Further analysis of personality expression showed varying alignment patterns between LLMs and human texts. Specifically, Llama-3.1-8B exhibited higher extraversion but low agreeableness, while DeepSeek-R1-Distill-Llama-8B displayed dramatic personality shifts during its reasoning process, especially when prompted with personality traits, with all models consistently showing very low Openness.</abstract>
      <url hash="ab5a9a2d">2025.acl-srw.85</url>
      <bibkey>dzhubaeva-etal-2025-unstructured</bibkey>
    </paper>
    <paper id="86">
      <title>Exploiting contextual information to improve stance detection in informal political discourse with <fixed-case>LLM</fixed-case>s</title>
      <author><first>Arman Engin</first><last>Sucu</last></author>
      <author><first>Yixiang</first><last>Zhou</last></author>
      <author><first>Mario A.</first><last>Nascimento</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Tony</first><last>Mullen</last><affiliation>Northeastern University</affiliation></author>
      <pages>1097-1110</pages>
      <abstract>This study investigates the use of Large Language Models (LLMs) for political stance detection in informal online discourse, where language is often sarcastic, ambiguous, and context-dependent. We explore whether providing contextual information, specifically user profile summaries derived from historical posts, can improve classification accuracy. Using a real-world political forum dataset, we generate structured profiles that summarize users’ ideological leaning, recurring topics, and linguistic patterns. We evaluate seven state-of-the-art LLMs across baseline and context-enriched setups through a comprehensive cross-model evaluation. Our findings show that contextual prompts significantly boost accuracy, with improvements ranging from +17.5% to +38.5%, achieving up to 74% accuracy that surpasses previous approaches. We also analyze how profile size and post selection strategies affect performance, showing that strategically chosen political content yields better results than larger, randomly selected contexts. These findings underscore the value of incorporating user-level context to enhance LLM performance in nuanced political classification tasks.</abstract>
      <url hash="7399c71c">2025.acl-srw.86</url>
      <bibkey>sucu-etal-2025-exploiting</bibkey>
    </paper>
    <paper id="87">
      <title>A Framework for Fine-Grained Complexity Control in Health Answer Generation</title>
      <author><first>Daniel Jorge Bernardo</first><last>Ferreira</last></author>
      <author><first>Tiago</first><last>Almeida</last></author>
      <author><first>Sérgio</first><last>Matos</last><affiliation>Universidade de Aveiro</affiliation></author>
      <pages>1111-1131</pages>
      <abstract>Health literacy plays a critical role in ensuring people can access, understand, and act on medical information. However, much of the health content available today is too complex for many people, and simplifying these texts manually is time-consuming and difficult to do at scale.To overcome this, we developed a new framework for automatically generating health answers at multiple, precisely controlled complexity levels.We began with a thorough analysis of 166 linguistic features, which we then refined into 13 key metrics that reliably differentiate between simple and complex medical texts. From these metrics, we derived a robust complexity scoring formula, combining them with weights learned from a logistic regression model. This formula allowed us to create a large, multi-level dataset of health question-answer pairs covering 21 distinct complexity levels, ranging from elementary patient-friendly explanations to highly technical summaries.Finally, we fine-tuned a Llama-3.1-8B-Instruct model using “control codes” on this dataset, giving users precise control over the complexity of the generated text and empowering them to select the level of detail and technicality they need.</abstract>
      <url hash="bdc0802c">2025.acl-srw.87</url>
      <bibkey>ferreira-etal-2025-framework</bibkey>
    </paper>
    <paper id="89">
      <title><fixed-case>QA</fixed-case> Analysis in Medical and Legal Domains: A Survey of Data Augmentation in Low-Resource Settings</title>
      <author><first>Benedictus Kent</first><last>Rachmat</last><affiliation>Université Paris-Saclay</affiliation></author>
      <author><first>Thomas</first><last>Gerald</last><affiliation>Université Paris-Saclay</affiliation></author>
      <author><first>Zheng Zhang</first><last>Slb</last></author>
      <author><first>Cyril</first><last>Grouin</last><affiliation>CNRS</affiliation></author>
      <pages>1132-1144</pages>
      <abstract>Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP), but their success remains largely confined to high-resource, general-purpose domains. In contrast, applying LLMs to low-resource domains poses significant challenges due to limited training data, domain drift, and strict terminology constraints. This survey provides an overview of the current landscape in domain-specific, low-resource QA with LLMs. We begin by analyzing the coverage and representativeness of specialized-domain QA datasets against large-scale reference datasets what we refer to as <i>ParentQA</i>. Building on this analysis, we survey data-centric strategies to enhance input diversity, including data augmentation techniques. We further discuss evaluation metrics for specialized tasks and consider ethical concerns. By mapping current methodologies and outlining open research questions, this survey aims to guide future efforts in adapting LLMs for robust and responsible use in resource-constrained, domain-specific environments. To facilitate reproducibility, we make our code available at https://github.com/kentrachmat/survey-da.</abstract>
      <url hash="b72d8ad6">2025.acl-srw.89</url>
      <bibkey>rachmat-etal-2025-qa</bibkey>
    </paper>
    <paper id="90">
      <title>Time-<fixed-case>L</fixed-case>la<fixed-case>MA</fixed-case>: Adapting Large Language Models for Time Series Modeling via Dynamic Low-rank Adaptation</title>
      <author><first>Juyuan</first><last>Zhang</last></author>
      <author><first>Jiechao</first><last>Gao</last></author>
      <author><first>Wenwen</first><last>Ouyang</last></author>
      <author><first>Wei</first><last>Zhu</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Hui Yi</first><last>Leong</last></author>
      <pages>1145-1157</pages>
      <abstract>Time series modeling holds significant importance in many industrial applications and has been extensively studied. A series of recent studies have demonstrated that large language models (LLMs) possess robust pattern recognition and semantic understanding capabilities over time series data. However, the current literature have yet striked a high-quality balance between (a) effectively aligning the time series and natural language modalities and (b) keeping the inference efficiency for industrial deployment. To address the above issues, we now propose the Time-LlaMA framework. Time-LlaMA first converts the time series input into token embeddings through a linear tokenization mechanism. Second, the time series token embeddings are aligned with the text prompts. Third, to further adapt the large languag model (LLM) backbone for time series modeling, we have developed a dynamic low-rank adaptation technique (DynaLoRA). DynaLoRA dynamically chooses the most suitable LoRA modules at each layer of the Transformer backbone for each time series input, enhancing the model’s predictive capabilities. Our experimental results on an extensive collection of challenging open and proprietary time series tasks confirm that our proposed method achieves the state-of-the-art (SOTA) performance and have potentials for wide industrial usages.</abstract>
      <url hash="c45001e2">2025.acl-srw.90</url>
      <bibkey>zhang-etal-2025-time</bibkey>
    </paper>
    <paper id="91">
      <title><fixed-case>R</fixed-case>us<fixed-case>C</fixed-case>on<fixed-case>T</fixed-case>ext Benchmark: A <fixed-case>R</fixed-case>ussian Language Evaluation Benchmark for Understanding Context</title>
      <author><first>Andrey</first><last>Chirkin</last></author>
      <author><first>Svetlana</first><last>Kuznetsova</last></author>
      <author><first>Maria</first><last>Volina</last></author>
      <author><first>Anna</first><last>Dengina</last><affiliation>Higher School of Economics and Vladimir State University</affiliation></author>
      <pages>1158-1170</pages>
      <abstract>This paper represents an implementation of an approach rather similar to that of (Zhu et al., 2024), adapted for the Russian-language data. We introduce the RusConText Benchmark for evaluating short-context understanding in Russian, comprising four distinct yet interrelated tasks: coreference resolution, discourse understanding, idiom interpretation and ellipsis resolution. Each task targets a specific aspect of linguistic processing, challenging a large language model to recover omitted information, resolve referential dependencies, interpret idioms and discourse. The RusConText Benchmark is an additional resource beyond standard benchmarks, designed to assess model performance from a specific perspective. In addition, we present the results of scoring 4 models on our benchmark.</abstract>
      <url hash="4f8a6547">2025.acl-srw.91</url>
      <bibkey>chirkin-etal-2025-ruscontext</bibkey>
    </paper>
    <paper id="92">
      <title><fixed-case>G</fixed-case>en<fixed-case>DLN</fixed-case>: Evolutionary Algorithm-Based Stacked <fixed-case>LLM</fixed-case> Framework for Joint Prompt Optimization</title>
      <author><first>Pia</first><last>Chouayfati</last></author>
      <author><first>Niklas</first><last>Herbster</last></author>
      <author><first>Ábel Domonkos</first><last>Sáfrán</last></author>
      <author><first>Matthias</first><last>Grabmair</last><affiliation>Technische Universität München</affiliation></author>
      <pages>1171-1212</pages>
      <abstract>With Large Language Model (LLM)-based applications becoming more common due to strong performance across many tasks, prompt optimization has emerged as a way to extract better solutions from frozen, often commercial LLMs that are not specifically adapted to a task. LLM-assisted prompt optimization methods provide a promising alternative to manual/human prompt engineering, where LLM “reasoning” can be used to make them optimizing agents. However, the cost of using LLMs for prompt optimization via commercial APIs remains high, especially for heuristic methods like evolutionary algorithms (EAs), which need many iterations to converge, and thus, tokens, API calls, and rate-limited network overhead. We propose GenDLN, an open-source, efficient genetic algorithm-based prompt pair optimization framework that leverages commercial API free tiers. Our approach allows teams with limited resources (NGOs, non-profits, academics, ...) to efficiently use commercial LLMs for EA-based prompt optimization. We conduct experiments on CLAUDETTE for legal terms of service classification and MRPC for paraphrase detection, performing in line with selected prompt optimization baselines, at no cost.</abstract>
      <url hash="4df84d8f">2025.acl-srw.92</url>
      <bibkey>chouayfati-etal-2025-gendln</bibkey>
    </paper>
    <paper id="93">
      <title>Sign Language Video Segmentation Using Temporal Boundary Identification</title>
      <author><first>Kavu Maithri</first><last>Rao</last></author>
      <author><first>Yasser</first><last>Hamidullah</last></author>
      <author><first>Eleftherios</first><last>Avramidis</last><affiliation>Technische Universität Berlin and German Research Center for AI</affiliation></author>
      <pages>1213-1224</pages>
      <abstract>Sign language segmentation focuses on identifying temporal boundaries within sign language videos. As compared to previous segmentation techniques that have depended on frame-level and phrase-level segmentation, our study emphasizes on subtitle-level segmentation, using synchronized subtitle data to facilitate temporal boundary recognition. Based on Beginning-Inside-Outside (BIO) tagging for subtitle unit delineation, we train a sequence-to-sequence (Seq2Seq) model with and without attention for subtitle boundary identification. Training on optical flow data and aligned subtitles from BOBSL and YouTube-ASL, we show that the Seq2Seq model with attention outperforms baseline models, achieving improved percentage of segments, F1 and IoU score. An additional contribution is the development of an method for subtitle temporal resolution, aiming to facilitate manual annotation.</abstract>
      <url hash="b8fe2632">2025.acl-srw.93</url>
      <bibkey>rao-etal-2025-sign</bibkey>
    </paper>
    <paper id="94">
      <title><fixed-case>LIP</fixed-case>-<fixed-case>NER</fixed-case>: Literal Patterns Benefit <fixed-case>LLM</fixed-case>-Based <fixed-case>NER</fixed-case></title>
      <author><first>Ruiqi</first><last>Li</last><affiliation>Sichuan University</affiliation></author>
      <author><first>Li</first><last>Chen</last><affiliation>Sichuan University</affiliation></author>
      <pages>1225-1238</pages>
      <abstract>Large Language Models (LLMs) can enhance the performance of Named Entity Recognition (NER) tasks by leveraging external knowledge through in-context learning. When it comes to entity-type-related external knowledge, existing methods mainly provide LLMs with semantic information such as the definition and annotation guidelines of an entity type, leaving the effect of orthographic or morphological information on LLM-based NER unexplored. Besides, it is non-trivial to obtain literal patterns written in natural language to serve LLMs. In this work, we propose LiP-NER, an LLM-based NER framework that utilizes <b>Li</b>teral <b>P</b>atterns, the entity-type-related knowledge that directly describes the orthographic and morphological features of entities. We also propose an LLM-based method to automatically acquire literal patterns, which requires only several sample entities rather than any annotation example, thus further reducing human labor. Our extensive experiments suggest that literal patterns can enhance the performance of LLMs in NER tasks. In further analysis, we found that entity types with relatively standardized naming conventions but limited world knowledge in LLMs, as well as entity types with broad and ambiguous names or definitions yet low internal variation among entities, benefit most from our approach. We found that the most effective written literal patterns are (1) detailed in classification, (2) focused on majority cases rather than minorities, and (3) explicit about obvious literal features.</abstract>
      <url hash="9445f370">2025.acl-srw.94</url>
      <bibkey>li-chen-2025-lip</bibkey>
    </paper>
    <paper id="95">
      <title>Testing <fixed-case>E</fixed-case>nglish News Articles for Lexical Homogenization Due to Widespread Use of Large Language Models</title>
      <author><first>Sarah</first><last>Fitterer</last></author>
      <author><first>Dominik</first><last>Gangl</last></author>
      <author><first>Jannes</first><last>Ulbrich</last><affiliation>Universität der Künste Berlin</affiliation></author>
      <pages>1239-1245</pages>
      <abstract>It is widely assumed that Large Language Models (LLMs) are shaping language, with multiple studies noting the growing presence of LLM-generated content and suggesting homogenizing effects. However, it remains unclear if these effects are already evident in recent writing. This study addresses that gap by comparing two datasets of English online news articles – one from 2018, prior to LLM popularization, and one from 2024, after widespread LLM adoption. We define lexical homogenization as a decrease in lexical diversity, measured by the MATTR, Maas, and MTLD metrics, and introduce the LLM-Style-Word Ratio (SWR) to measure LLM influence. We found higher MTLD and SWR scores, yet negligible changes in Maas and MATTR scores in 2024 corpus. We conclude that while there is an apparent influence of LLMs on written online English, homogenization effects do not show in the measurements. We therefore propose to apply different metrics to measure lexical homogenization in future studies on the influence of LLM usage on language change.</abstract>
      <url hash="c06a6fa6">2025.acl-srw.95</url>
      <bibkey>fitterer-etal-2025-testing</bibkey>
    </paper>
    <paper id="98">
      <title>Bridging the Data Gap in Financial Sentiment: <fixed-case>LLM</fixed-case>-Driven Augmentation</title>
      <author><first>Rohit</first><last>Kumar</last></author>
      <author><first>Chandan</first><last>Nolbaria</last><affiliation>Indian Institute of science education and research bhopal</affiliation></author>
      <pages>1246-1254</pages>
      <abstract>Static and outdated datasets hinder the accuracy of Financial Sentiment Analysis (FSA) in capturing rapidly evolving market sentiment. We tackle this by proposing a novel data augmentation technique using Retrieval Augmented Generation (RAG). Our method leverages a generative LLM to infuse established benchmarks with up-to-date contextual information from contemporary financial news. This RAG-based augmentation significantly modernizes the data’s alignment with current financial language. Furthermore, a robust BERT-BiGRU judge model verifies that the sentiment of the original annotations is faithfully preserved, ensuring the generation of high-quality, temporally relevant, and sentiment-consistent data suitable for advancing FSA model development.</abstract>
      <url hash="a19dd085">2025.acl-srw.98</url>
      <bibkey>kumar-nolbaria-2025-bridging</bibkey>
    </paper>
  </volume>
</collection>
