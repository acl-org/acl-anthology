<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.sustainlp">
  <volume id="1" ingest-date="2020-11-06">
    <meta>
      <booktitle>Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing</booktitle>
      <editor><first>Nafise Sadat</first><last>Moosavi</last></editor>
      <editor><first>Angela</first><last>Fan</last></editor>
      <editor><first>Vered</first><last>Shwartz</last></editor>
      <editor><first>Goran</first><last>Glavaš</last></editor>
      <editor><first>Shafiq</first><last>Joty</last></editor>
      <editor><first>Alex</first><last>Wang</last></editor>
      <editor><first>Thomas</first><last>Wolf</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>November</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="29071254">2020.sustainlp-1.0</url>
    </frontmatter>
    <paper id="1">
      <title>Knowing Right from Wrong: Should We Use More Complex Models for Automatic Short-Answer Scoring in <fixed-case>B</fixed-case>ahasa <fixed-case>I</fixed-case>ndonesia?</title>
      <author><first>Ali Akbar</first><last>Septiandri</last></author>
      <author><first>Yosef Ardhito</first><last>Winatmoko</last></author>
      <author><first>Ilham Firdausi</first><last>Putra</last></author>
      <pages>1–7</pages>
      <abstract>We compare three solutions to UKARA 1.0 challenge on automated short-answer scoring: single classical, ensemble classical, and deep learning. The task is to classify given answers to two questions, whether they are right or wrong. While recent development shows increasing model complexity to push the benchmark performances, they tend to be resource-demanding with mundane improvement. For the UKARA task, we found that bag-of-words and classical machine learning approaches can compete with ensemble models and Bi-LSTM model with pre-trained word2vec embedding from 200 million words. In this case, the single classical machine learning achieved less than 2% difference in F1 compared to the deep learning approach with 1/18 time for model training.</abstract>
      <url hash="737436b0">2020.sustainlp-1.1</url>
      <doi>10.18653/v1/2020.sustainlp-1.1</doi>
    </paper>
    <paper id="2">
      <title>Rank and run-time aware compression of <fixed-case>NLP</fixed-case> Applications</title>
      <author><first>Urmish</first><last>Thakker</last></author>
      <author><first>Jesse</first><last>Beu</last></author>
      <author><first>Dibakar</first><last>Gope</last></author>
      <author><first>Ganesh</first><last>Dasika</last></author>
      <author><first>Matthew</first><last>Mattina</last></author>
      <pages>8–18</pages>
      <abstract>Sequence model based NLP applications canbe large. Yet, many applications that benefit from them run on small devices with very limited compute and storage capabilities, while still having run-time constraints.As a result, there is a need for a compression technique that can achieve significant compression without negatively impacting inference run-time and task accuracy. This paper proposes a new compression technique called Hybrid Matrix Factorization (HMF) that achieves this dual objective. HMF improves low-rank matrix factorization (LMF) techniques by doubling the rank of the matrix using an intelligent hybrid-structure leading to better accuracy than LMF. Further, by preserving dense matrices, it leads to faster inference run-timethan pruning or structure matrix based compression technique. We evaluate the impact of this technique on 5 NLP benchmarks across multiple tasks (Translation, Intent Detection,Language Modeling) and show that for similar accuracy values and compression factors, HMF can achieve more than 2.32x faster inference run-time than pruning and 16.77% better accuracy than LMF.</abstract>
      <url hash="7b1879e5">2020.sustainlp-1.2</url>
      <doi>10.18653/v1/2020.sustainlp-1.2</doi>
    </paper>
    <paper id="3">
      <title>Learning Informative Representations of Biomedical Relations with Latent Variable Models</title>
      <author><first>Harshil</first><last>Shah</last></author>
      <author><first>Julien</first><last>Fauqueur</last></author>
      <pages>19–28</pages>
      <abstract>Extracting biomedical relations from large corpora of scientific documents is a challenging natural language processing task. Existing approaches usually focus on identifying a relation either in a single sentence (mention-level) or across an entire corpus (pair-level). In both cases, recent methods have achieved strong results by learning a point estimate to represent the relation; this is then used as the input to a relation classifier. However, the relation expressed in text between a pair of biomedical entities is often more complex than can be captured by a point estimate. To address this issue, we propose a latent variable model with an arbitrarily flexible distribution to represent the relation between an entity pair. Additionally, our model provides a unified architecture for both mention-level and pair-level relation extraction. We demonstrate that our model achieves results competitive with strong baselines for both tasks while having fewer parameters and being significantly faster to train. We make our code publicly available.</abstract>
      <url hash="cbaf98a3">2020.sustainlp-1.3</url>
      <doi>10.18653/v1/2020.sustainlp-1.3</doi>
    </paper>
    <paper id="4">
      <title>End to End Binarized Neural Networks for Text Classification</title>
      <author><first>Kumar</first><last>Shridhar</last></author>
      <author><first>Harshil</first><last>Jain</last></author>
      <author><first>Akshat</first><last>Agarwal</last></author>
      <author><first>Denis</first><last>Kleyko</last></author>
      <pages>29–34</pages>
      <abstract>Deep neural networks have demonstrated their superior performance in almost every Natural Language Processing task, however, their increasing complexity raises concerns. A particular concern is that these networks pose high requirements for computing hardware and training budgets. The state-of-the-art transformer models are a vivid example. Simplifying the computations performed by a network is one way of addressing the issue of the increasing complexity. In this paper, we propose an end to end binarized neural network for the task of intent and text classification. In order to fully utilize the potential of end to end binarization, both the input representations (vector embeddings of tokens statistics) and the classifier are binarized. We demonstrate the efficiency of such a network on the intent classification of short texts over three datasets and text classification with a larger dataset. On the considered datasets, the proposed network achieves comparable to the state-of-the-art results while utilizing 20-40% lesser memory and training time compared to the benchmarks.</abstract>
      <url hash="ab07164c">2020.sustainlp-1.4</url>
      <attachment type="OptionalSupplementaryMaterial" hash="1f601b55">2020.sustainlp-1.4.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.sustainlp-1.4</doi>
    </paper>
    <paper id="5">
      <title>Exploring the Boundaries of Low-Resource <fixed-case>BERT</fixed-case> Distillation</title>
      <author><first>Moshe</first><last>Wasserblat</last></author>
      <author><first>Oren</first><last>Pereg</last></author>
      <author><first>Peter</first><last>Izsak</last></author>
      <pages>35–40</pages>
      <abstract>In recent years, large pre-trained models have demonstrated state-of-the-art performance in many of NLP tasks. However, the deployment of these models on devices with limited resources is challenging due to the models’ large computational consumption and memory requirements. Moreover, the need for a considerable amount of labeled training data also hinders real-world deployment scenarios. Model distillation has shown promising results for reducing model size, computational load and data efficiency. In this paper we test the boundaries of BERT model distillation in terms of model compression, inference efficiency and data scarcity. We show that classification tasks that require the capturing of general lexical semantics can be successfully distilled by very simple and efficient models and require relatively small amount of labeled training data. We also show that the distillation of large pre-trained models is more effective in real-life scenarios where limited amounts of labeled training are available.</abstract>
      <url hash="fd7284b7">2020.sustainlp-1.5</url>
      <doi>10.18653/v1/2020.sustainlp-1.5</doi>
    </paper>
    <paper id="6">
      <title>Efficient Estimation of Influence of a Training Instance</title>
      <author><first>Sosuke</first><last>Kobayashi</last></author>
      <author><first>Sho</first><last>Yokoi</last></author>
      <author><first>Jun</first><last>Suzuki</last></author>
      <author><first>Kentaro</first><last>Inui</last></author>
      <pages>41–47</pages>
      <abstract>Understanding the influence of a training instance on a neural network model leads to improving interpretability. However, it is difficult and inefficient to evaluate the influence, which shows how a model’s prediction would be changed if a training instance were not used. In this paper, we propose an efficient method for estimating the influence. Our method is inspired by dropout, which zero-masks a sub-network and prevents the sub-network from learning each training instance. By switching between dropout masks, we can use sub-networks that learned or did not learn each training instance and estimate its influence. Through experiments with BERT and VGGNet on classification datasets, we demonstrate that the proposed method can capture training influences, enhance the interpretability of error predictions, and cleanse the training dataset for improving generalization.</abstract>
      <url hash="472a0c08">2020.sustainlp-1.6</url>
      <attachment type="OptionalSupplementaryMaterial" hash="2924916b">2020.sustainlp-1.6.OptionalSupplementaryMaterial.pdf</attachment>
      <doi>10.18653/v1/2020.sustainlp-1.6</doi>
    </paper>
    <paper id="7">
      <title>Efficient Inference For Neural Machine Translation</title>
      <author><first>Yi-Te</first><last>Hsu</last></author>
      <author><first>Sarthak</first><last>Garg</last></author>
      <author><first>Yi-Hsiu</first><last>Liao</last></author>
      <author><first>Ilya</first><last>Chatsviorkin</last></author>
      <pages>48–53</pages>
      <abstract>Large Transformer models have achieved state-of-the-art results in neural machine translation and have become standard in the field. In this work, we look for the optimal combination of known techniques to optimize inference speed without sacrificing translation quality. We conduct an empirical study that stacks various approaches and demonstrates that combination of replacing decoder self-attention with simplified recurrent units, adopting a deep encoder and a shallow decoder architecture and multi-head attention pruning can achieve up to 109% and 84% speedup on CPU and GPU respectively and reduce the number of parameters by 25% while maintaining the same translation quality in terms of BLEU.</abstract>
      <url hash="7ad2b4df">2020.sustainlp-1.7</url>
      <doi>10.18653/v1/2020.sustainlp-1.7</doi>
    </paper>
    <paper id="8">
      <title>Sparse Optimization for Unsupervised Extractive Summarization of Long Documents with the Frank-Wolfe Algorithm</title>
      <author><first>Alicia</first><last>Tsai</last></author>
      <author><first>Laurent</first><last>El Ghaoui</last></author>
      <pages>54–62</pages>
      <abstract>We address the problem of unsupervised extractive document summarization, especially for long documents. We model the unsupervised problem as a sparse auto-regression one and approximate the resulting combinatorial problem via a convex, norm-constrained problem. We solve it using a dedicated Frank-Wolfe algorithm. To generate a summary with k sentences, the algorithm only needs to execute approximately k iterations, making it very efficient for a long document. We evaluate our approach against two other unsupervised methods using both lexical (standard) ROUGE scores, as well as semantic (embedding-based) ones. Our method achieves better results with both datasets and works especially well when combined with embeddings for highly paraphrased summaries.</abstract>
      <url hash="b22dc89c">2020.sustainlp-1.8</url>
      <attachment type="OptionalSupplementaryMaterial" hash="957e4570">2020.sustainlp-1.8.OptionalSupplementaryMaterial.pdf</attachment>
      <doi>10.18653/v1/2020.sustainlp-1.8</doi>
    </paper>
    <paper id="9">
      <title>Don’t Read Too Much Into It: Adaptive Computation for Open-Domain Question Answering</title>
      <author><first>Yuxiang</first><last>Wu</last></author>
      <author><first>Pasquale</first><last>Minervini</last></author>
      <author><first>Pontus</first><last>Stenetorp</last></author>
      <author><first>Sebastian</first><last>Riedel</last></author>
      <pages>63–72</pages>
      <abstract>Most approaches to Open-Domain Question Answering consist of a light-weight retriever that selects a set of candidate passages, and a computationally expensive reader that examines the passages to identify the correct answer. Previous works have shown that as the number of retrieved passages increases, so does the performance of the reader. However, they assume all retrieved passages are of equal importance and allocate the same amount of computation to them, leading to a substantial increase in computational cost. To reduce this cost, we propose the use of adaptive computation to control the computational budget allocated for the passages to be read. We first introduce a technique operating on individual passages in isolation which relies on anytime prediction and a per-layer estimation of an early exit probability. We then introduce SKYLINEBUILDER, an approach for dynamically deciding on which passage to allocate computation at each step, based on a resource allocation policy trained via reinforcement learning. Our results on SQuAD-Open show that adaptive computation with global prioritisation improves over several strong static and adaptive methods, leading to a 4.3x reduction in computation while retaining 95% performance of the full model.</abstract>
      <url hash="1d14ae6a">2020.sustainlp-1.9</url>
      <attachment type="OptionalSupplementaryMaterial" hash="255de707">2020.sustainlp-1.9.OptionalSupplementaryMaterial.pdf</attachment>
      <doi>10.18653/v1/2020.sustainlp-1.9</doi>
    </paper>
    <paper id="10">
      <title>A Two-stage Model for Slot Filling in Low-resource Settings: Domain-agnostic Non-slot Reduction and Pretrained Contextual Embeddings</title>
      <author><first>Cennet</first><last>Oguz</last></author>
      <author><first>Ngoc Thang</first><last>Vu</last></author>
      <pages>73–82</pages>
      <abstract>Learning-based slot filling - a key component of spoken language understanding systems - typically requires a large amount of in-domain hand-labeled data for training. In this paper, we propose a novel two-stage model architecture that can be trained with only a few in-domain hand-labeled examples. The first step is designed to remove non-slot tokens (i.e., O labeled tokens), as they introduce noise in the input of slot filling models. This step is domain-agnostic and therefore, can be trained by exploiting out-of-domain data. The second step identifies slot names only for slot tokens by using state-of-the-art pretrained contextual embeddings such as ELMO and BERT. We show that our approach outperforms other state-of-art systems on the SNIPS benchmark dataset.</abstract>
      <url hash="ab915359">2020.sustainlp-1.10</url>
      <doi>10.18653/v1/2020.sustainlp-1.10</doi>
    </paper>
    <paper id="11">
      <title>Early Exiting <fixed-case>BERT</fixed-case> for Efficient Document Ranking</title>
      <author><first>Ji</first><last>Xin</last></author>
      <author><first>Rodrigo</first><last>Nogueira</last></author>
      <author><first>Yaoliang</first><last>Yu</last></author>
      <author><first>Jimmy</first><last>Lin</last></author>
      <pages>83–88</pages>
      <abstract>Pre-trained language models such as BERT have shown their effectiveness in various tasks. Despite their power, they are known to be computationally intensive, which hinders real-world applications. In this paper, we introduce early exiting BERT for document ranking. With a slight modification, BERT becomes a model with multiple output paths, and each inference sample can exit early from these paths. In this way, computation can be effectively allocated among samples, and overall system latency is significantly reduced while the original quality is maintained. Our experiments on two document ranking datasets demonstrate up to 2.5x inference speedup with minimal quality degradation. The source code of our implementation can be found at https://github.com/castorini/earlyexiting-monobert.</abstract>
      <url hash="58f3e0f0">2020.sustainlp-1.11</url>
      <doi>10.18653/v1/2020.sustainlp-1.11</doi>
    </paper>
    <paper id="12">
      <title>Keyphrase Generation with <fixed-case>GAN</fixed-case>s in Low-Resources Scenarios</title>
      <author><first>Giuseppe</first><last>Lancioni</last></author>
      <author><first>Saida</first><last>S.Mohamed</last></author>
      <author><first>Beatrice</first><last>Portelli</last></author>
      <author><first>Giuseppe</first><last>Serra</last></author>
      <author><first>Carlo</first><last>Tasso</last></author>
      <pages>89–96</pages>
      <abstract>Keyphrase Generation is the task of predicting Keyphrases (KPs), short phrases that summarize the semantic meaning of a given document. Several past studies provided diverse approaches to generate Keyphrases for an input document. However, all of these approaches still need to be trained on very large datasets. In this paper, we introduce BeGanKP, a new conditional GAN model to address the problem of Keyphrase Generation in a low-resource scenario. Our main contribution relies in the Discriminator’s architecture: a new BERT-based module which is able to distinguish between the generated and humancurated KPs reliably. Its characteristics allow us to use it in a low-resource scenario, where only a small amount of training data are available, obtaining an efficient Generator. The resulting architecture achieves, on five public datasets, competitive results with respect to the state-of-the-art approaches, using less than 1% of the training data.</abstract>
      <url hash="d7f6a21e">2020.sustainlp-1.12</url>
      <doi>10.18653/v1/2020.sustainlp-1.12</doi>
    </paper>
    <paper id="13">
      <title>Quasi-Multitask Learning: an Efficient Surrogate for Obtaining Model Ensembles</title>
      <author><first>Norbert</first><last>Kis-Szabó</last></author>
      <author><first>Gábor</first><last>Berend</last></author>
      <pages>97–106</pages>
      <abstract>We propose the technique of quasi-multitask learning (Q-MTL), a simple and easy to implement modification of standard multitask learning, in which the tasks to be modeled are identical. With this easy modification of a standard neural classifier we can get benefits similar to an ensemble of classifiers with a fraction of the resources required.We illustrate it through a series of sequence labeling experiments over a diverse set of languages, that applying Q-MTL consistently increases the generalization ability of the applied models. The proposed architecture can be regarded as a new regularization technique that encourages the model to develop an internal representation of the problem at hand which is beneficial to multiple output units of the classifier at the same time. Our experiments corroborate that by relying on the proposed algorithm, we can approximate the quality of an ensemble of classifiers at a fraction of computational resources required. Additionally, our results suggest that Q-MTL handles the presence of noisy training labels better than ensembles.</abstract>
      <url hash="b522c363">2020.sustainlp-1.13</url>
      <doi>10.18653/v1/2020.sustainlp-1.13</doi>
    </paper>
    <paper id="14">
      <title>A Little Bit Is Worse Than None: Ranking with Limited Training Data</title>
      <author><first>Xinyu</first><last>Zhang</last></author>
      <author><first>Andrew</first><last>Yates</last></author>
      <author><first>Jimmy</first><last>Lin</last></author>
      <pages>107–112</pages>
      <abstract>Researchers have proposed simple yet effective techniques for the retrieval problem based on using BERT as a relevance classifier to rerank initial candidates from keyword search. In this work, we tackle the challenge of fine-tuning these models for specific domains in a data and computationally efficient manner. Typically, researchers fine-tune models using corpus-specific labeled data from sources such as TREC. We first answer the question: How much data of this type do we need? Recognizing that the most computationally efficient training is no training, we explore zero-shot ranking using BERT models that have already been fine-tuned with the large MS MARCO passage retrieval dataset. We arrive at the surprising and novel finding that “some” labeled in-domain data can be worse than none at all.</abstract>
      <url hash="8f1f5096">2020.sustainlp-1.14</url>
      <doi>10.18653/v1/2020.sustainlp-1.14</doi>
    </paper>
    <paper id="15">
      <title>Predictive Model Selection for Transfer Learning in Sequence Labeling Tasks</title>
      <author><first>Parul</first><last>Awasthy</last></author>
      <author><first>Bishwaranjan</first><last>Bhattacharjee</last></author>
      <author><first>John</first><last>Kender</last></author>
      <author><first>Radu</first><last>Florian</last></author>
      <pages>113–118</pages>
      <abstract>Transfer learning is a popular technique to learn a task using less training data and fewer compute resources. However, selecting the correct source model for transfer learning is a challenging task. We demonstrate a novel predictive method that determines which existing source model would minimize error for transfer learning to a given target. This technique does not require learning for prediction, and avoids computational costs of trail-and-error. We have evaluated this technique on nine datasets across diverse domains, including newswire, user forums, air flight booking, cybersecurity news, etc. We show that it per-forms better than existing techniques such as fine-tuning over vanilla BERT, or curriculum learning over the largest dataset on top of BERT, resulting in average F1 score gains in excess of 3%. Moreover, our technique consistently selects the best model using fewer tries.</abstract>
      <url hash="7d051c86">2020.sustainlp-1.15</url>
      <doi>10.18653/v1/2020.sustainlp-1.15</doi>
    </paper>
    <paper id="16">
      <title>Load What You Need: Smaller Versions of Mutililingual <fixed-case>BERT</fixed-case></title>
      <author><first>Amine</first><last>Abdaoui</last></author>
      <author><first>Camille</first><last>Pradel</last></author>
      <author><first>Grégoire</first><last>Sigel</last></author>
      <pages>119–123</pages>
      <abstract>Pre-trained Transformer-based models are achieving state-of-the-art results on a variety of Natural Language Processing data sets. However, the size of these models is often a drawback for their deployment in real production applications. In the case of multilingual models, most of the parameters are located in the embeddings layer. Therefore, reducing the vocabulary size should have an important impact on the total number of parameters. In this paper, we propose to extract smaller models that handle fewer number of languages according to the targeted corpora. We present an evaluation of smaller versions of multilingual BERT on the XNLI data set, but we believe that this method may be applied to other multilingual transformers. The obtained results confirm that we can generate smaller models that keep comparable results, while reducing up to 45% of the total number of parameters. We compared our models with DistilmBERT (a distilled version of multilingual BERT) and showed that unlike language reduction, distillation induced a 1.7% to 6% drop in the overall accuracy on the XNLI data set. The presented models and code are publicly available.</abstract>
      <url hash="f74dd40a">2020.sustainlp-1.16</url>
      <doi>10.18653/v1/2020.sustainlp-1.16</doi>
    </paper>
    <paper id="17">
      <title><fixed-case>S</fixed-case>queeze<fixed-case>BERT</fixed-case>: What can computer vision teach <fixed-case>NLP</fixed-case> about efficient neural networks?</title>
      <author><first>Forrest</first><last>Iandola</last></author>
      <author><first>Albert</first><last>Shaw</last></author>
      <author><first>Ravi</first><last>Krishna</last></author>
      <author><first>Kurt</first><last>Keutzer</last></author>
      <pages>124–135</pages>
      <abstract>Humans read and write hundreds of billions of messages every day. Further, due to the availability of large datasets, large computing systems, and better neural network models, natural language processing (NLP) technology has made significant strides in understanding, proofreading, and organizing these messages. Thus, there is a significant opportunity to deploy NLP in myriad applications to help web users, social networks, and businesses. Toward this end, we consider smartphones and other mobile devices as crucial platforms for deploying NLP models at scale. However, today’s highly-accurate NLP neural network models such as BERT and RoBERTa are extremely computationally expensive, with BERT-base taking 1.7 seconds to classify a text snippet on a Pixel 3 smartphone. To begin to address this problem, we draw inspiration from the computer vision community, where work such as MobileNet has demonstrated that grouped convolutions (e.g. depthwise convolutions) can enable speedups without sacrificing accuracy. We demonstrate how to replace several operations in self-attention layers with grouped convolutions, and we use this technique in a novel network architecture called SqueezeBERT, which runs 4.3x faster than BERT-base on the Pixel 3 while achieving competitive accuracy on the GLUE test set. A PyTorch-based implementation of SqueezeBERT is available as part of the Hugging Face Transformers library: https://huggingface.co/squeezebert</abstract>
      <url hash="5d3de3a3">2020.sustainlp-1.17</url>
      <attachment type="OptionalSupplementaryMaterial" hash="5fe60fd4">2020.sustainlp-1.17.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.sustainlp-1.17</doi>
    </paper>
    <paper id="18">
      <title>Analysis of Resource-efficient Predictive Models for Natural Language Processing</title>
      <author><first>Raj</first><last>Pranesh</last></author>
      <author><first>Ambesh</first><last>Shekhar</last></author>
      <pages>136–140</pages>
      <abstract>In this paper, we presented an analyses of the resource efficient predictive models, namely Bonsai, Binary Neighbor Compression(BNC), ProtoNN, Random Forest, Naive Bayes and Support vector machine(SVM), in the machine learning field for resource constraint devices. These models try to minimize resource requirements like RAM and storage without hurting the accuracy much. We utilized these models on multiple benchmark natural language processing tasks, which were sentimental analysis, spam message detection, emotion analysis and fake news classification. The experiment results shows that the tree-based algorithm, Bonsai, surpassed the rest of the machine learning algorithms by achieve higher accuracy scores while having significantly lower memory usage.</abstract>
      <url hash="da83c35d">2020.sustainlp-1.18</url>
      <doi>10.18653/v1/2020.sustainlp-1.18</doi>
    </paper>
    <paper id="19">
      <title>Towards Accurate and Reliable Energy Measurement of <fixed-case>NLP</fixed-case> Models</title>
      <author><first>Qingqing</first><last>Cao</last></author>
      <author><first>Aruna</first><last>Balasubramanian</last></author>
      <author><first>Niranjan</first><last>Balasubramanian</last></author>
      <pages>141–148</pages>
      <abstract>Accurate and reliable measurement of energy consumption is critical for making well-informed design choices when choosing and training large scale NLP models. In this work, we show that existing software-based energy estimations are not accurate because they do not take into account hardware differences and how resource utilization affects energy consumption. We conduct energy measurement experiments with four different models for a question answering task. We quantify the error of existing software-based energy estimations by using a hardware power meter that provides highly accurate energy measurements. Our key takeaway is the need for a more accurate energy estimation model that takes into account hardware variabilities and the non-linear relationship between resource utilization and energy consumption. We release the code and data at https://github.com/csarron/sustainlp2020-energy.</abstract>
      <url hash="8fd7694b">2020.sustainlp-1.19</url>
      <doi>10.18653/v1/2020.sustainlp-1.19</doi>
    </paper>
    <paper id="20">
      <title><fixed-case>F</fixed-case>ast<fixed-case>F</fixed-case>ormers: Highly Efficient Transformer Models for Natural Language Understanding</title>
      <author><first>Young Jin</first><last>Kim</last></author>
      <author><first>Hany</first><last>Hassan</last></author>
      <pages>149–158</pages>
      <abstract>Transformer-based models are the state-of-the-art for Natural Language Understanding (NLU) applications. Models are getting bigger and better on various tasks. However, Transformer models remain computationally challenging since they are not efficient at inference-time compared to traditional approaches. In this paper, we present FastFormers, a set of recipes to achieve efficient inference-time performance for Transformer-based models on various NLU tasks. We show how carefully utilizing knowledge distillation, structured pruning and numerical optimization can lead to drastic improvements on inference efficiency. We provide effective recipes that can guide practitioners to choose the best settings for various NLU tasks and pretrained models. Applying the proposed recipes to the SuperGLUE benchmark, we achieve from 9.8x up to 233.9x speed-up compared to out-of-the-box models on CPU. On GPU, we also achieve up to 12.4x speed-up with the presented methods. We show that FastFormers can drastically reduce cost of serving 100 million requests from 4,223 USD to just 18 USD on an Azure F16s_v2 instance. This translates to a sustainable runtime by reducing energy consumption 6.9x - 125.8x according to the metrics used in the SustaiNLP 2020 shared task.</abstract>
      <url hash="958ba189">2020.sustainlp-1.20</url>
      <doi>10.18653/v1/2020.sustainlp-1.20</doi>
    </paper>
    <paper id="21">
      <title>A comparison between <fixed-case>CNN</fixed-case>s and <fixed-case>WFA</fixed-case>s for Sequence Classification</title>
      <author><first>Ariadna</first><last>Quattoni</last></author>
      <author><first>Xavier</first><last>Carreras</last></author>
      <pages>159–163</pages>
      <abstract>We compare a classical CNN architecture for sequence classification involving several convolutional and max-pooling layers against a simple model based on weighted finite state automata (WFA). Each model has its advantages and disadvantages and it is possible that they could be combined. However, we believe that the first research goal should be to investigate and understand how do these two apparently dissimilar models compare in the context of specific natural language processing tasks. This paper is the first step towards that goal. Our experiments with five sequence classification datasets suggest that, despite the apparent simplicity of WFA models and training algorithms, the performance of WFAs is comparable to that of the CNNs.</abstract>
      <url hash="ba257c75">2020.sustainlp-1.21</url>
      <doi>10.18653/v1/2020.sustainlp-1.21</doi>
    </paper>
    <paper id="22">
      <title>Label-Efficient Training for Next Response Selection</title>
      <author><first>Seungtaek</first><last>Choi</last></author>
      <author><first>Myeongho</first><last>Jeong</last></author>
      <author><first>Jinyoung</first><last>Yeo</last></author>
      <author><first>Seung-won</first><last>Hwang</last></author>
      <pages>164–168</pages>
      <abstract>This paper studies label augmentation for training dialogue response selection. The existing model is trained by “observational” annotation, where one observed response is annotated as gold. In this paper, we propose “counterfactual augmentation” of pseudo-positive labels. We validate that the effectiveness of augmented labels are comparable to positives, such that ours outperform state-of-the-arts without augmentation.</abstract>
      <url hash="3e0aff8a">2020.sustainlp-1.22</url>
      <doi>10.18653/v1/2020.sustainlp-1.22</doi>
    </paper>
    <paper id="23">
      <title>Do We Need to Create Big Datasets to Learn a Task?</title>
      <author><first>Swaroop</first><last>Mishra</last></author>
      <author><first>Bhavdeep Singh</first><last>Sachdeva</last></author>
      <pages>169–173</pages>
      <abstract>Deep Learning research has been largely accelerated by the development of huge datasets such as Imagenet. The general trend has been to create big datasets to make a deep neural network learn. A huge amount of resources is being spent in creating these big datasets, developing models, training them, and iterating this process to dominate leaderboards. We argue that the trend of creating bigger datasets needs to be revised by better leveraging the power of pre-trained language models. Since the language models have already been pre-trained with huge amount of data and have basic linguistic knowledge, there is no need to create big datasets to learn a task. Instead, we need to create a dataset that is sufficient for the model to learn various task-specific terminologies, such as ‘Entailment’, ‘Neutral’, and ‘Contradiction’ for NLI. As evidence, we show that RoBERTA is able to achieve near-equal performance on 2% data of SNLI. We also observe competitive zero-shot generalization on several OOD datasets. In this paper, we propose a baseline algorithm to find the optimal dataset for learning a task.</abstract>
      <url hash="13c9c62c">2020.sustainlp-1.23</url>
      <doi>10.18653/v1/2020.sustainlp-1.23</doi>
    </paper>
    <paper id="24">
      <title>Overview of the <fixed-case>S</fixed-case>ustai<fixed-case>NLP</fixed-case> 2020 Shared Task</title>
      <author><first>Alex</first><last>Wang</last></author>
      <author><first>Thomas</first><last>Wolf</last></author>
      <pages>174–178</pages>
      <abstract>We describe the SustaiNLP 2020 shared task: efficient inference on the SuperGLUE benchmark (Wang et al., 2019). Participants are evaluated based on performance on the benchmark as well as energy consumed in making predictions on the test sets. We describe the task, its organization, and the submitted systems. Across the six submissions to the shared task, participants achieved efficiency gains of 20× over a standard BERT (Devlin et al., 2019) baseline, while losing less than an absolute point in performance.</abstract>
      <url hash="79668c0d">2020.sustainlp-1.24</url>
      <doi>10.18653/v1/2020.sustainlp-1.24</doi>
    </paper>
  </volume>
</collection>
