<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.ecnlp">
  <volume id="1" ingest-date="2022-05-15" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Fifth Workshop on e-Commerce and NLP (ECNLP 5)</booktitle>
      <editor><first>Shervin</first><last>Malmasi</last></editor>
      <editor><first>Oleg</first><last>Rokhlenko</last></editor>
      <editor><first>Nicola</first><last>Ueffing</last></editor>
      <editor><first>Ido</first><last>Guy</last></editor>
      <editor><first>Eugene</first><last>Agichtein</last></editor>
      <editor><first>Surya</first><last>Kallumadi</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Dublin, Ireland</address>
      <month>May</month>
      <year>2022</year>
      <url hash="c77daea7">2022.ecnlp-1</url>
      <venue>ecnlp</venue>
    </meta>
    <frontmatter>
      <url hash="e6bd47bd">2022.ecnlp-1.0</url>
      <bibkey>ecnlp-2022-e</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>DEFT</fixed-case>ri: A Few-Shot Label Fused Contextual Representation Learning For Product Defect Triage in e-Commerce</title>
      <author><first>Ipsita</first><last>Mohanty</last></author>
      <pages>1-7</pages>
      <abstract>Defect Triage is a time-sensitive and critical process in a large-scale agile software development lifecycle for e-commerce. Inefficiencies arising from human and process dependencies in this domain have motivated research in automated approaches using machine learning to accurately assign defects to qualified teams. This work proposes a novel framework for automated defect triage (DEFTri) using fine-tuned state-of-the-art pre-trained BERT on labels fused text embeddings to improve contextual representations from human-generated product defects. For our multi-label text classification defect triage task, we also introduce a Walmart proprietary dataset of product defects using weak supervision and adversarial learning, in a few-shot setting.</abstract>
      <url hash="c1b502de">2022.ecnlp-1.1</url>
      <bibkey>mohanty-2022-deftri</bibkey>
      <doi>10.18653/v1/2022.ecnlp-1.1</doi>
      <video href="2022.ecnlp-1.1.mp4"/>
    </paper>
    <paper id="2">
      <title>Interactive Latent Knowledge Selection for <fixed-case>E</fixed-case>-Commerce Product Copywriting Generation</title>
      <author><first>Zeming</first><last>Wang</last></author>
      <author><first>Yanyan</first><last>Zou</last></author>
      <author><first>Yuejian</first><last>Fang</last></author>
      <author><first>Hongshen</first><last>Chen</last></author>
      <author><first>Mian</first><last>Ma</last></author>
      <author><first>Zhuoye</first><last>Ding</last></author>
      <author><first>Bo</first><last>Long</last></author>
      <pages>8-19</pages>
      <abstract>As the multi-modal e-commerce is thriving, high-quality advertising product copywriting has gain more attentions, which plays a crucial role in the e-commerce recommender, advertising and even search platforms. The advertising product copywriting is able to enhance the user experience by highlighting the product’s characteristics with textual descriptions and thus to improve the likelihood of user click and purchase. Automatically generating product copywriting has attracted noticeable interests from both academic and industrial communities, where existing solutions merely make use of a product’s title and attribute information to generate its corresponding description. However, in addition to the product title and attributes, we observe that there are various auxiliary descriptions created by the shoppers or marketers in the e-commerce platforms (namely human knowledge), which contains valuable information for product copywriting generation, yet always accompanying lots of noises. In this work, we propose a novel solution to automatically generating product copywriting that involves all the title, attributes and denoised auxiliary knowledge. To be specific, we design an end-to-end generation framework equipped with two variational autoencoders that works interactively to select informative human knowledge and generate diverse copywriting.</abstract>
      <url hash="7856da03">2022.ecnlp-1.2</url>
      <bibkey>wang-etal-2022-interactive</bibkey>
      <doi>10.18653/v1/2022.ecnlp-1.2</doi>
      <video href="2022.ecnlp-1.2.mp4"/>
    </paper>
    <paper id="3">
      <title>Leveraging Seq2seq Language Generation for Multi-level Product Issue Identification</title>
      <author id="yang-liu-icsi"><first>Yang</first><last>Liu</last></author>
      <author><first>Varnith</first><last>Chordia</last></author>
      <author><first>Hua</first><last>Li</last></author>
      <author><first>Siavash</first><last>Fazeli Dehkordy</last></author>
      <author><first>Yifei</first><last>Sun</last></author>
      <author><first>Vincent</first><last>Gao</last></author>
      <author><first>Na</first><last>Zhang</last></author>
      <pages>20-28</pages>
      <abstract>In a leading e-commerce business, we receive hundreds of millions of customer feedback from different text communication channels such as product reviews. The feedback can contain rich information regarding customers’ dissatisfaction in the quality of goods and services. To harness such information to better serve customers, in this paper, we created a machine learning approach to automatically identify product issues and uncover root causes from the customer feedback text. We identify issues at two levels: coarse grained (L-Coarse) and fine grained (L-Granular). We formulate this multi-level product issue identification problem as a seq2seq language generation problem. Specifically, we utilize transformer-based seq2seq models due to their versatility and strong transfer-learning capability. We demonstrate that our approach is label efficient and outperforms the traditional approach such as multi-class multi-label classification formulation. Based on human evaluation, our fine-tuned model achieves 82.1% and 95.4% human-level performance for L-Coarse and L-Granular issue identification, respectively. Furthermore, our experiments illustrate that the model can generalize to identify unseen L-Granular issues.</abstract>
      <url hash="f2a30476">2022.ecnlp-1.3</url>
      <bibkey>liu-etal-2022-leveraging</bibkey>
      <doi>10.18653/v1/2022.ecnlp-1.3</doi>
      <video href="2022.ecnlp-1.3.mp4"/>
    </paper>
    <paper id="4">
      <title>Data Quality Estimation Framework for Faster Tax Code Classification</title>
      <author><first>Ravi</first><last>Kondadadi</last></author>
      <author><first>Allen</first><last>Williams</last></author>
      <author><first>Nicolas</first><last>Nicolov</last></author>
      <pages>29-34</pages>
      <abstract>This paper describes a novel framework to estimate the data quality of a collection of product descriptions to identify required relevant information for accurate product listing classification for tax-code assignment. Our Data Quality Estimation (DQE) framework consists of a Question Answering (QA) based attribute value extraction model to identify missing attributes and a classification model to identify bad quality records. We show that our framework can accurately predict the quality of product descriptions. In addition to identifying low-quality product listings, our framework can also generate a detailed report at a category level showing missing product information resulting in a better customer experience.</abstract>
      <url hash="3256ac65">2022.ecnlp-1.4</url>
      <bibkey>kondadadi-etal-2022-data</bibkey>
      <doi>10.18653/v1/2022.ecnlp-1.4</doi>
      <video href="2022.ecnlp-1.4.mp4"/>
    </paper>
    <paper id="5">
      <title><fixed-case>CML</fixed-case>: A Contrastive Meta Learning Method to Estimate Human Label Confidence Scores and Reduce Data Collection Cost</title>
      <author><first>Bo</first><last>Dong</last></author>
      <author><first>Yiyi</first><last>Wang</last></author>
      <author><first>Hanbo</first><last>Sun</last></author>
      <author><first>Yunji</first><last>Wang</last></author>
      <author><first>Alireza</first><last>Hashemi</last></author>
      <author><first>Zheng</first><last>Du</last></author>
      <pages>35-43</pages>
      <abstract>Deep neural network models are especially susceptible to noise in annotated labels. In the real world, annotated data typically contains noise caused by a variety of factors such as task difficulty, annotator experience, and annotator bias. Label quality is critical for label validation tasks; however, correcting for noise by collecting more data is often costly. In this paper, we propose a contrastive meta-learning framework (CML) to address the challenges introduced by noisy annotated data, specifically in the context of natural language processing. CML combines contrastive and meta learning to improve the quality of text feature representations. Meta-learning is also used to generate confidence scores to assess label quality. We demonstrate that a model built on CML-filtered data outperforms a model built on clean data. Furthermore, we perform experiments on deidentified commercial voice assistant datasets and demonstrate that our model outperforms several SOTA approaches.</abstract>
      <url hash="7f4af822">2022.ecnlp-1.5</url>
      <bibkey>dong-etal-2022-cml</bibkey>
      <doi>10.18653/v1/2022.ecnlp-1.5</doi>
      <video href="2022.ecnlp-1.5.mp4"/>
    </paper>
    <paper id="6">
      <title>Improving Relevance Quality in Product Search using High-Precision Query-Product Semantic Similarity</title>
      <author><first>Alireza</first><last>Bagheri Garakani</last></author>
      <author><first>Fan</first><last>Yang</last></author>
      <author><first>Wen-Yu</first><last>Hua</last></author>
      <author><first>Yetian</first><last>Chen</last></author>
      <author><first>Michinari</first><last>Momma</last></author>
      <author><first>Jingyuan</first><last>Deng</last></author>
      <author><first>Yan</first><last>Gao</last></author>
      <author><first>Yi</first><last>Sun</last></author>
      <pages>44-48</pages>
      <abstract>Ensuring relevance quality in product search is a critical task as it impacts the customer’s ability to find intended products in the short-term as well as the general perception and trust of the e-commerce system in the long term. In this work we leverage a high-precision cross-encoder BERT model for semantic similarity between customer query and products and survey its effectiveness for three ranking applications where offline-generated scores could be used: (1) as an offline metric for estimating relevance quality impact, (2) as a re-ranking feature covering head/torso queries, and (3) as a training objective for optimization. We present results on effectiveness of this strategy for the large e-commerce setting, which has general applicability for choice of other high-precision models and tasks in ranking.</abstract>
      <url hash="522e9521">2022.ecnlp-1.6</url>
      <bibkey>bagheri-garakani-etal-2022-improving</bibkey>
      <doi>10.18653/v1/2022.ecnlp-1.6</doi>
      <video href="2022.ecnlp-1.6.mp4"/>
    </paper>
    <paper id="7">
      <title>Comparative Snippet Generation</title>
      <author><first>Saurabh</first><last>Jain</last></author>
      <author><first>Yisong</first><last>Miao</last></author>
      <author><first>Min-Yen</first><last>Kan</last></author>
      <pages>49-57</pages>
      <abstract>We model products’ reviews to generate comparative responses consisting of positive and negative experiences regarding the product. Specifically, we generate a single-sentence, comparative response from a given positive and a negative opinion. We contribute the first dataset for this task of Comparative Snippet Generation from contrasting opinions regarding a product, and an analysis of performance of a pre-trained BERT model to generate such snippets.</abstract>
      <url hash="84ae4f33">2022.ecnlp-1.7</url>
      <bibkey>jain-etal-2022-comparative</bibkey>
      <doi>10.18653/v1/2022.ecnlp-1.7</doi>
      <video href="2022.ecnlp-1.7.mp4"/>
      <pwccode url="https://github.com/wing-nus/comparative-snippet-generation-dataset" additional="false">wing-nus/comparative-snippet-generation-dataset</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/spot">SPOT</pwcdataset>
    </paper>
    <paper id="8">
      <title>Textual Content Moderation in <fixed-case>C</fixed-case>2<fixed-case>C</fixed-case> Marketplace</title>
      <author><first>Yusuke</first><last>Shido</last></author>
      <author><first>Hsien-Chi</first><last>Liu</last></author>
      <author><first>Keisuke</first><last>Umezawa</last></author>
      <pages>58-62</pages>
      <abstract>Automatic monitoring systems for inappropriate user-generated messages have been found to be effective in reducing human operation costs in Consumer to Consumer (C2C) marketplace services, in which customers send messages directly to other customers. We propose a lightweight neural network that takes a conversation as input, which we deployed to a production service. Our results show that the system reduced the human operation costs to less than one-sixth compared to the conventional rule-based monitoring at Mercari.</abstract>
      <url hash="c210ec11">2022.ecnlp-1.8</url>
      <bibkey>shido-etal-2022-textual</bibkey>
      <doi>10.18653/v1/2022.ecnlp-1.8</doi>
      <video href="2022.ecnlp-1.8.mp4"/>
    </paper>
    <paper id="9">
      <title>Spelling Correction using Phonetics in <fixed-case>E</fixed-case>-commerce Search</title>
      <author><first>Fan</first><last>Yang</last></author>
      <author><first>Alireza</first><last>Bagheri Garakani</last></author>
      <author><first>Yifei</first><last>Teng</last></author>
      <author><first>Yan</first><last>Gao</last></author>
      <author><first>Jia</first><last>Liu</last></author>
      <author><first>Jingyuan</first><last>Deng</last></author>
      <author><first>Yi</first><last>Sun</last></author>
      <pages>63-67</pages>
      <abstract>In E-commerce search, spelling correction plays an important role to find desired products for customers in processing user-typed search queries. However, resolving phonetic errors is a critical but much overlooked area. The query with phonetic spelling errors tends to appear correct based on pronunciation but is nonetheless inaccurate in spelling (e.g., “bluetooth sound system” vs. “blutut sant sistam”) with numerous noisy forms and sparse occurrences. In this work, we propose a generalized spelling correction system integrating phonetics to address phonetic errors in E-commerce search without additional latency cost. Using India (IN) E-commerce market for illustration, the experiment shows that our proposed phonetic solution significantly improves the F1 score by 9%+ and recall of phonetic errors by 8%+. This phonetic spelling correction system has been deployed to production, currently serving hundreds of millions of customers.</abstract>
      <url hash="8cedf7ed">2022.ecnlp-1.9</url>
      <bibkey>yang-etal-2022-spelling</bibkey>
      <doi>10.18653/v1/2022.ecnlp-1.9</doi>
      <video href="2022.ecnlp-1.9.mp4"/>
    </paper>
    <paper id="10">
      <title>Logical Reasoning for Task Oriented Dialogue Systems</title>
      <author><first>Sajjad</first><last>Beygi</last></author>
      <author><first>Maryam</first><last>Fazel-Zarandi</last></author>
      <author><first>Alessandra</first><last>Cervone</last></author>
      <author><first>Prakash</first><last>Krishnan</last></author>
      <author><first>Siddhartha</first><last>Jonnalagadda</last></author>
      <pages>68-79</pages>
      <abstract>In recent years, large pretrained models have been used in dialogue systems to improve successful task completion rates. However, lack of reasoning capabilities of dialogue platforms make it difficult to provide relevant and fluent responses, unless the designers of a conversational experience spend a considerable amount of time implementing these capabilities in external rule based modules. In this work, we propose a novel method to fine-tune pretrained transformer models such as Roberta and T5, to reason over a set of facts in a given dialogue context. Our method includes a synthetic data generation mechanism which helps the model learn logical relations, such as comparison between list of numerical values, inverse relations (and negation), inclusion and exclusion for categorical attributes, and application of a combination of attributes over both numerical and categorical values, and spoken form for numerical values, without need for additional training data. We show that the transformer based model can perform logical reasoning to answer questions when the dialogue context contains all the required information, otherwise it is able to extract appropriate constraints to pass to downstream components (e.g. a knowledge base) when partial information is available. We observe that transformer based models such as UnifiedQA-T5 can be fine-tuned to perform logical reasoning (such as numerical and categorical attributes’ comparison) over attributes seen at training time (e.g., accuracy of 90%+ for comparison of smaller than kmax=5 values over heldout test dataset).</abstract>
      <url hash="9ea923ff">2022.ecnlp-1.10</url>
      <bibkey>beygi-etal-2022-logical</bibkey>
      <doi>10.18653/v1/2022.ecnlp-1.10</doi>
      <video href="2022.ecnlp-1.10.mp4"/>
    </paper>
    <paper id="11">
      <title><fixed-case>C</fixed-case>o<fixed-case>VA</fixed-case>: Context-aware Visual Attention for Webpage Information Extraction</title>
      <author><first>Anurendra</first><last>Kumar</last></author>
      <author><first>Keval</first><last>Morabia</last></author>
      <author><first>William</first><last>Wang</last></author>
      <author><first>Kevin</first><last>Chang</last></author>
      <author><first>Alex</first><last>Schwing</last></author>
      <pages>80-90</pages>
      <abstract>Webpage information extraction (WIE) is an important step to create knowledge bases. For this, classical WIE methods leverage the Document Object Model (DOM) tree of a website. However, use of the DOM tree poses significant challenges as context and appearance are encoded in an abstract manner. To address this challenge we propose to reformulate WIE as a context-aware Webpage Object Detection task. Specifically, we develop a Context-aware Visual Attention-based (CoVA) detection pipeline which combines appearance features with syntactical structure from the DOM tree. To study the approach we collect a new large-scale datase of e-commerce websites for which we manually annotate every web element with four labels: product price, product title, product image and others. On this dataset we show that the proposed CoVA approach is a new challenging baseline which improves upon prior state-of-the-art methods.</abstract>
      <url hash="3199703c">2022.ecnlp-1.11</url>
      <bibkey>kumar-etal-2022-cova</bibkey>
      <doi>10.18653/v1/2022.ecnlp-1.11</doi>
      <video href="2022.ecnlp-1.11.mp4"/>
      <pwccode url="https://github.com/kevalmorabia97/cova-web-object-detection" additional="false">kevalmorabia97/cova-web-object-detection</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cova">CoVA</pwcdataset>
    </paper>
    <paper id="12">
      <title>Product Titles-to-Attributes As a Text-to-Text Task</title>
      <author><first>Gilad</first><last>Fuchs</last></author>
      <author><first>Yoni</first><last>Acriche</last></author>
      <pages>91-98</pages>
      <abstract>Online marketplaces use attribute-value pairs, such as brand, size, size type, color, etc. to help define important and relevant facts about a listing. These help buyers to curate their search results using attribute filtering and overall create a richer experience. Although their critical importance for listings’ discoverability, getting sellers to input tens of different attribute-value pairs per listing is costly and often results in missing information. This can later translate to the unnecessary removal of relevant listings from the search results when buyers are filtering by attribute values. In this paper we demonstrate using a Text-to-Text hierarchical multi-label ranking model framework to predict the most relevant attributes per listing, along with their expected values, using historic user behavioral data. This solution helps sellers by allowing them to focus on verifying information on attributes that are likely to be used by buyers, and thus, increase the expected recall for their listings. Specifically for eBay’s case we show that using this model can improve the relevancy of the attribute extraction process by 33.2% compared to the current highly-optimized production system. Apart from the empirical contribution, the highly generalized nature of the framework presented in this paper makes it relevant for many high-volume search-driven websites.</abstract>
      <url hash="d1ccbdcd">2022.ecnlp-1.12</url>
      <bibkey>fuchs-acriche-2022-product</bibkey>
      <doi>10.18653/v1/2022.ecnlp-1.12</doi>
      <video href="2022.ecnlp-1.12.mp4"/>
    </paper>
    <paper id="13">
      <title>Product Answer Generation from Heterogeneous Sources: A New Benchmark and Best Practices</title>
      <author><first>Xiaoyu</first><last>Shen</last></author>
      <author><first>Gianni</first><last>Barlacchi</last></author>
      <author><first>Marco</first><last>Del Tredici</last></author>
      <author><first>Weiwei</first><last>Cheng</last></author>
      <author id="bill-byrne"><first>Bill</first><last>Byrne</last></author>
      <author><first>Adrià</first><last>Gispert</last></author>
      <pages>99-110</pages>
      <abstract>It is of great value to answer product questions based on heterogeneous information sources available on web product pages, e.g., semi-structured attributes, text descriptions, user-provided contents, etc. However, these sources have different structures and writing styles, which poses challenges for (1) evidence ranking, (2) source selection, and (3) answer generation. In this paper, we build a benchmark with annotations for both evidence selection and answer generation covering 6 information sources. Based on this benchmark, we conduct a comprehensive study and present a set of best practices. We show that all sources are important and contribute to answering questions. Handling all sources within one single model can produce comparable confidence scores across sources and combining multiple sources for training always helps, even for sources with totally different structures. We further propose a novel data augmentation method to iteratively create training samples for answer generation, which achieves close-to-human performance with only a few thousandannotations. Finally, we perform an in-depth error analysis of model predictions and highlight the challenges for future research.</abstract>
      <url hash="b611f2ee">2022.ecnlp-1.13</url>
      <bibkey>shen-etal-2022-product</bibkey>
      <doi>10.18653/v1/2022.ecnlp-1.13</doi>
      <video href="2022.ecnlp-1.13.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/amazonqa">AmazonQA</pwcdataset>
    </paper>
    <paper id="14">
      <title>semi<fixed-case>PQA</fixed-case>: A Study on Product Question Answering over Semi-structured Data</title>
      <author><first>Xiaoyu</first><last>Shen</last></author>
      <author><first>Gianni</first><last>Barlacchi</last></author>
      <author><first>Marco</first><last>Del Tredici</last></author>
      <author><first>Weiwei</first><last>Cheng</last></author>
      <author><first>Adrià</first><last>Gispert</last></author>
      <pages>111-120</pages>
      <abstract>Product question answering (PQA) aims to automatically address customer questions to improve their online shopping experience. Current research mainly focuses on finding answers from either unstructured text, like product descriptions and user reviews, or structured knowledge bases with pre-defined schemas. Apart from the above two sources, a lot of product information is represented in a semi-structured way, e.g., key-value pairs, lists, tables, json and xml files, etc. These semi-structured data can be a valuable answer source since they are better organized than free text, while being easier to construct than structured knowledge bases. However, little attention has been paid to them. To fill in this blank, here we study how to effectively incorporate semi-structured answer sources for PQA and focus on presenting answers in a natural, fluent sentence. To this end, we present semiPQA: a dataset to benchmark PQA over semi-structured data. It contains 11,243 written questions about json-formatted data covering 320 unique attribute types. Each data point is paired with manually-annotated text that describes its contents, so that we can train a neural answer presenter to present the data in a natural way. We provide baseline results and a deep analysis on the successes and challenges of leveraging semi-structured data for PQA. In general, state-of-the-art neural models can perform remarkably well when dealing with seen attribute types. For unseen attribute types, however, a noticeable drop is observed for both answer presentation and attribute ranking.</abstract>
      <url hash="829bab91">2022.ecnlp-1.14</url>
      <bibkey>shen-etal-2022-semipqa</bibkey>
      <doi>10.18653/v1/2022.ecnlp-1.14</doi>
      <video href="2022.ecnlp-1.14.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/amazonqa">AmazonQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsqa">NewsQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="15">
      <title>Improving Specificity in Review Response Generation with Data-Driven Data Filtering</title>
      <author><first>Tannon</first><last>Kew</last></author>
      <author><first>Martin</first><last>Volk</last></author>
      <pages>121-133</pages>
      <abstract>Responding to online customer reviews has become an essential part of successfully managing and growing a business both in e-commerce and the hospitality and tourism sectors. Recently, neural text generation methods intended to assist authors in composing responses have been shown to deliver highly fluent and natural looking texts. However, they also tend to learn a strong, undesirable bias towards generating overly generic, one-size-fits-all outputs to a wide range of inputs. While this often results in ‘safe’, high-probability responses, there are many practical settings in which greater specificity is preferable. In this work we examine the task of generating more specific responses for online reviews in the hospitality domain by identifying generic responses in the training data, filtering them and fine-tuning the generation model. We experiment with a range of data-driven filtering methods and show through automatic and human evaluation that, despite a 60% reduction in the amount of training data, filtering helps to derive models that are capable of generating more specific, useful responses.</abstract>
      <url hash="be3239c6">2022.ecnlp-1.15</url>
      <bibkey>kew-volk-2022-improving</bibkey>
      <doi>10.18653/v1/2022.ecnlp-1.15</doi>
      <video href="2022.ecnlp-1.15.mp4"/>
      <pwccode url="https://github.com/zurichnlp/specific_hospo_respo" additional="false">zurichnlp/specific_hospo_respo</pwccode>
    </paper>
    <paper id="16">
      <title>Extreme Multi-Label Classification with Label Masking for Product Attribute Value Extraction</title>
      <author><first>Wei-Te</first><last>Chen</last></author>
      <author><first>Yandi</first><last>Xia</last></author>
      <author><first>Keiji</first><last>Shinzato</last></author>
      <pages>134-140</pages>
      <abstract>Although most studies have treated attribute value extraction (AVE) as named entity recognition, these approaches are not practical in real-world e-commerce platforms because they perform poorly, and require canonicalization of extracted values. Furthermore, since values needed for actual services is static in many attributes, extraction of new values is not always necessary. Given the above, we formalize AVE as extreme multi-label classification (XMC). A major problem in solving AVE as XMC is that the distribution between positive and negative labels for products is heavily imbalanced. To mitigate the negative impact derived from such biased distribution, we propose label masking, a simple and effective method to reduce the number of negative labels in training. We exploit attribute taxonomy designed for e-commerce platforms to determine which labels are negative for products. Experimental results using a dataset collected from a Japanese e-commerce platform demonstrate that the label masking improves micro and macro F<tex-math>_1</tex-math> scores by 3.38 and 23.20 points, respectively.</abstract>
      <url hash="691c7994">2022.ecnlp-1.16</url>
      <bibkey>chen-etal-2022-extreme</bibkey>
      <doi>10.18653/v1/2022.ecnlp-1.16</doi>
      <video href="2022.ecnlp-1.16.mp4"/>
    </paper>
    <paper id="17">
      <title>Enhanced Representation with Contrastive Loss for Long-Tail Query Classification in e-commerce</title>
      <author><first>Lvxing</first><last>Zhu</last></author>
      <author><first>Hao</first><last>Chen</last></author>
      <author><first>Chao</first><last>Wei</last></author>
      <author><first>Weiru</first><last>Zhang</last></author>
      <pages>141-150</pages>
      <abstract>Query classification is a fundamental task in an e-commerce search engine, which assigns one or multiple predefined product categories in response to each search query. Taking click-through logs as training data in deep learning methods is a common and effective approach for query classification. However, the frequency distribution of queries typically has long-tail property, which means that there are few logs for most of the queries. The lack of reliable user feedback information results in worse performance of long-tail queries compared with frequent queries. To solve the above problem, we propose a novel method that leverages an auxiliary module to enhance the representations of long-tail queries by taking advantage of reliable supervised information of variant frequent queries. The long-tail queries are guided by the contrastive loss to obtain category-aligned representations in the auxiliary module, where the variant frequent queries serve as anchors in the representation space. We train our model with real-world click data from AliExpress and conduct evaluation on both offline labeled data and online AB test. The results and further analysis demonstrate the effectiveness of our proposed method.</abstract>
      <url hash="b06c823b">2022.ecnlp-1.17</url>
      <bibkey>zhu-etal-2022-enhanced</bibkey>
      <doi>10.18653/v1/2022.ecnlp-1.17</doi>
      <video href="2022.ecnlp-1.17.mp4"/>
    </paper>
    <paper id="18">
      <title>Domain-specific knowledge distillation yields smaller and better models for conversational commerce</title>
      <author><first>Kristen</first><last>Howell</last></author>
      <author><first>Jian</first><last>Wang</last></author>
      <author><first>Akshay</first><last>Hazare</last></author>
      <author><first>Joseph</first><last>Bradley</last></author>
      <author><first>Chris</first><last>Brew</last></author>
      <author><first>Xi</first><last>Chen</last></author>
      <author><first>Matthew</first><last>Dunn</last></author>
      <author><first>Beth</first><last>Hockey</last></author>
      <author><first>Andrew</first><last>Maurer</last></author>
      <author><first>Dominic</first><last>Widdows</last></author>
      <pages>151-160</pages>
      <abstract>We demonstrate that knowledge distillation can be used not only to reduce model size, but to simultaneously adapt a contextual language model to a specific domain. We use Multilingual BERT (mBERT; Devlin et al., 2019) as a starting point and follow the knowledge distillation approach of (Sahn et al., 2019) to train a smaller multilingual BERT model that is adapted to the domain at hand. We show that for in-domain tasks, the domain-specific model shows on average 2.3% improvement in F1 score, relative to a model distilled on domain-general data. Whereas much previous work with BERT has fine-tuned the encoder weights during task training, we show that the model improvements from distillation on in-domain data persist even when the encoder weights are frozen during task training, allowing a single encoder to support classifiers for multiple tasks and languages.</abstract>
      <url hash="6bfdf962">2022.ecnlp-1.18</url>
      <bibkey>howell-etal-2022-domain</bibkey>
      <doi>10.18653/v1/2022.ecnlp-1.18</doi>
      <video href="2022.ecnlp-1.18.mp4"/>
    </paper>
    <paper id="19">
      <title><fixed-case>O</fixed-case>pen<fixed-case>B</fixed-case>rand: Open Brand Value Extraction from Product Descriptions</title>
      <author><first>Kassem</first><last>Sabeh</last></author>
      <author><first>Mouna</first><last>Kacimi</last></author>
      <author><first>Johann</first><last>Gamper</last></author>
      <pages>161-170</pages>
      <abstract>Extracting attribute-value information from unstructured product descriptions continue to be of a vital importance in e-commerce applications. One of the most important product attributes is the brand which highly influences costumers’ purchasing behaviour. Thus, it is crucial to accurately extract brand information dealing with the main challenge of discovering new brand names. Under the open world assumption, several approaches have adopted deep learning models to extract attribute-values using sequence tagging paradigm. However, they did not employ finer grained data representations such as character level embeddings which improve generalizability. In this paper, we introduce OpenBrand, a novel approach for discovering brand names. OpenBrand is a BiLSTM-CRF-Attention model with embeddings at different granularities. Such embeddings are learned using CNN and LSTM architectures to provide more accurate representations. We further propose a new dataset for brand value extraction, with a very challenging task on zero-shot extraction. We have tested our approach, through extensive experiments, and shown that it outperforms state-of-the-art models in brand name discovery.</abstract>
      <url hash="29d9d1a7">2022.ecnlp-1.19</url>
      <bibkey>sabeh-etal-2022-openbrand</bibkey>
      <doi>10.18653/v1/2022.ecnlp-1.19</doi>
      <video href="2022.ecnlp-1.19.mp4"/>
      <pwccode url="https://github.com/kassemsabeh/open-brand" additional="false">kassemsabeh/open-brand</pwccode>
    </paper>
    <paper id="20">
      <title>Robust Product Classification with Instance-Dependent Noise</title>
      <author id="huy-nguyen-pgh"><first>Huy</first><last>Nguyen</last></author>
      <author><first>Devashish</first><last>Khatwani</last></author>
      <pages>171-180</pages>
      <abstract>Noisy labels in large E-commerce product data (i.e., product items are placed into incorrect categories) is a critical issue for product categorization task because they are unavoidable, non-trivial to remove and degrade prediction performance significantly. Training a product title classification model which is robust to noisy labels in the data is very important to make product classification applications more practical. In this paper, we study the impact of instance-dependent noise to performance of product title classification by comparing our data denoising algorithm and different noise-resistance training algorithms which were designed to prevent a classifier model from over-fitting to noise. We develop a simple yet effective Deep Neural Network for product title classification to use as a base classifier. Along with recent methods of stimulating instance-dependent noise, we propose a novel noise stimulation algorithm based on product title similarity. Our experiments cover multiple datasets, various noise methods and different training solutions. Results uncover the limit of classification task when noise rate is not negligible and data distribution is highly skewed.</abstract>
      <url hash="f65fe769">2022.ecnlp-1.20</url>
      <bibkey>nguyen-khatwani-2022-robust</bibkey>
      <doi>10.18653/v1/2022.ecnlp-1.20</doi>
      <video href="2022.ecnlp-1.20.mp4"/>
    </paper>
    <paper id="21">
      <title>Structured Extraction of Terms and Conditions from <fixed-case>G</fixed-case>erman and <fixed-case>E</fixed-case>nglish Online Shops</title>
      <author><first>Tobias</first><last>Schamel</last></author>
      <author><first>Daniel</first><last>Braun</last></author>
      <author><first>Florian</first><last>Matthes</last></author>
      <pages>181-190</pages>
      <abstract>The automated analysis of Terms and Conditions has gained attention in recent years, mainly due to its relevance to consumer protection. Well-structured data sets are the base for every analysis. While content extraction, in general, is a well-researched field and many open source libraries are available, our evaluation shows, that existing solutions cannot extract Terms and Conditions in sufficient quality, mainly because of their special structure. In this paper, we present an approach to extract the content and hierarchy of Terms and Conditions from German and English online shops. Our evaluation shows, that the approach outperforms the current state of the art. A python implementation of the approach is made available under an open license.</abstract>
      <url hash="4ff85b37">2022.ecnlp-1.21</url>
      <bibkey>schamel-etal-2022-structured</bibkey>
      <doi>10.18653/v1/2022.ecnlp-1.21</doi>
      <video href="2022.ecnlp-1.21.mp4"/>
      <pwccode url="https://github.com/sebischair/lowestcommonancestorextractor" additional="false">sebischair/lowestcommonancestorextractor</pwccode>
    </paper>
    <paper id="22">
      <title>“Does it come in black?” <fixed-case>CLIP</fixed-case>-like models are zero-shot recommenders</title>
      <author><first>Patrick John</first><last>Chia</last></author>
      <author><first>Jacopo</first><last>Tagliabue</last></author>
      <author><first>Federico</first><last>Bianchi</last></author>
      <author><first>Ciro</first><last>Greco</last></author>
      <author><first>Diogo</first><last>Goncalves</last></author>
      <pages>191-198</pages>
      <abstract>Product discovery is a crucial component for online shopping. However, item-to-item recommendations today do not allow users to explore changes along selected dimensions: given a query item, can a model suggest something similar but in a different color? We consider item recommendations of the comparative nature (e.g. “something darker”) and show how CLIP-based models can support this use case in a zero-shot manner. Leveraging a large model built for fashion, we introduce GradREC and its industry potential, and offer a first rounded assessment of its strength and weaknesses.</abstract>
      <url hash="515d5c5c">2022.ecnlp-1.22</url>
      <bibkey>chia-etal-2022-come</bibkey>
      <doi>10.18653/v1/2022.ecnlp-1.22</doi>
      <video href="2022.ecnlp-1.22.mp4"/>
    </paper>
    <paper id="23">
      <title>Clause Topic Classification in <fixed-case>G</fixed-case>erman and <fixed-case>E</fixed-case>nglish Standard Form Contracts</title>
      <author><first>Daniel</first><last>Braun</last></author>
      <author><first>Florian</first><last>Matthes</last></author>
      <pages>199-209</pages>
      <abstract>So-called standard form contracts, i.e. contracts that are drafted unilaterally by one party, like terms and conditions of online shops or terms of services of social networks, are cornerstones of our modern economy. Their processing is, therefore, of significant practical value. Often, the sheer size of these contracts allows the drafting party to hide unfavourable terms from the other party. In this paper, we compare different approaches for automatically classifying the topics of clauses in standard form contracts, based on a data-set of more than 6,000 clauses from more than 170 contracts, which we collected from German and English online shops and annotated based on a taxonomy of clause topics, that we developed together with legal experts. We will show that, in our comparison of seven approaches, from simple keyword matching to transformer language models, BERT performed best with an F1-score of up to 0.91, however much simpler and computationally cheaper models like logistic regression also achieved similarly good results of up to 0.87.</abstract>
      <url hash="095f35ec">2022.ecnlp-1.23</url>
      <bibkey>braun-matthes-2022-clause</bibkey>
      <doi>10.18653/v1/2022.ecnlp-1.23</doi>
      <video href="2022.ecnlp-1.23.mp4"/>
    </paper>
    <paper id="24">
      <title>Investigating the Generative Approach for Question Answering in <fixed-case>E</fixed-case>-Commerce</title>
      <author><first>Kalyani</first><last>Roy</last></author>
      <author><first>Vineeth</first><last>Balapanuru</last></author>
      <author><first>Tapas</first><last>Nayak</last></author>
      <author><first>Pawan</first><last>Goyal</last></author>
      <pages>210-216</pages>
      <abstract>Many e-commerce websites provide Product-related Question Answering (PQA) platform where potential customers can ask questions related to a product, and other consumers can post an answer to that question based on their experience. Recently, there has been a growing interest in providing automated responses to product questions. In this paper, we investigate the suitability of the generative approach for PQA. We use state-of-the-art generative models proposed by Deng et al.(2020) and Lu et al.(2020) for this purpose. On closer examination, we find several drawbacks in this approach: (1) input reviews are not always utilized significantly for answer generation, (2) the performance of the models is abysmal while answering the numerical questions, (3) many of the generated answers contain phrases like “I do not know” which are taken from the reference answer in training data, and these answers do not convey any information to the customer. Although these approaches achieve a high ROUGE score, it does not reflect upon these shortcomings of the generated answers. We hope that our analysis will lead to more rigorous PQA approaches, and future research will focus on addressing these shortcomings in PQA.</abstract>
      <url hash="b4ed5ec5">2022.ecnlp-1.24</url>
      <bibkey>roy-etal-2022-investigating</bibkey>
      <doi>10.18653/v1/2022.ecnlp-1.24</doi>
      <video href="2022.ecnlp-1.24.mp4"/>
    </paper>
    <paper id="25">
      <title>Utilizing Cross-Modal Contrastive Learning to Improve Item Categorization <fixed-case>BERT</fixed-case> Model</title>
      <author><first>Lei</first><last>Chen</last></author>
      <author><first>Hou Wei</first><last>Chou</last></author>
      <pages>217-223</pages>
      <abstract>Item categorization (IC) is a core natural language processing (NLP) task in e-commerce. As a special text classification task, fine-tuning pre-trained models, e.g., BERT, has become a mainstream solution. To improve IC performance further, other product metadata, e.g., product images, have been used. Although multimodal IC (MIC) systems show higher performance, expanding from processing text to more resource-demanding images brings large engineering impacts and hinders the deployment of such dual-input MIC systems. In this paper, we proposed a new way of using product images to improve text-only IC model: leveraging cross-modal signals between products’ titles and associated images to adapt BERT models in a self-supervised learning (SSL) way. Our experiments on the three genres in the public Amazon product dataset show that the proposed method generates improved prediction accuracy and macro-F1 values than simply using the original BERT. Moreover, the proposed method is able to keep using existing text-only IC inference implementation and shows a resource advantage than the deployment of a dual-input MIC system.</abstract>
      <url hash="eab00f23">2022.ecnlp-1.25</url>
      <bibkey>chen-chou-2022-utilizing</bibkey>
      <doi>10.18653/v1/2022.ecnlp-1.25</doi>
      <video href="2022.ecnlp-1.25.mp4"/>
    </paper>
    <paper id="26">
      <title>Towards Generalizeable Semantic Product Search by Text Similarity Pre-training on Search Click Logs</title>
      <author><first>Zheng</first><last>Liu</last></author>
      <author><first>Wei</first><last>Zhang</last></author>
      <author><first>Yan</first><last>Chen</last></author>
      <author><first>Weiyi</first><last>Sun</last></author>
      <author><first>Tianchuan</first><last>Du</last></author>
      <author><first>Benjamin</first><last>Schroeder</last></author>
      <pages>224-233</pages>
      <abstract>Recently, semantic search has been successfully applied to E-commerce product search and the learned semantic space for query and product encoding are expected to generalize well to unseen queries or products. Yet, whether generalization can conveniently emerge has not been thoroughly studied in the domain thus far. In this paper, we examine several general-domain and domain-specific pre-trained Roberta variants and discover that general-domain fine-tuning does not really help generalization which aligns with the discovery of prior art, yet proper domain-specific fine-tuning with clickstream data can lead to better model generalization, based on a bucketed analysis of a manually annotated query-product relevance data.</abstract>
      <url hash="759f3658">2022.ecnlp-1.26</url>
      <bibkey>liu-etal-2022-towards</bibkey>
      <doi>10.18653/v1/2022.ecnlp-1.26</doi>
      <video href="2022.ecnlp-1.26.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/wands">WANDS</pwcdataset>
    </paper>
    <paper id="27">
      <title>Can Pretrained Language Models Generate Persuasive, Faithful, and Informative Ad Text for Product Descriptions?</title>
      <author><first>Fajri</first><last>Koto</last></author>
      <author><first>Jey Han</first><last>Lau</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <pages>234-243</pages>
      <abstract>For any e-commerce service, persuasive, faithful, and informative product descriptions can attract shoppers and improve sales. While not all sellers are capable of providing such interesting descriptions, a language generation system can be a source of such descriptions at scale, and potentially assist sellers to improve their product descriptions. Most previous work has addressed this task based on statistical approaches (Wang et al., 2017), limited attributes such as titles (Chen et al., 2019; Chan et al., 2020), and focused on only one product type (Wang et al., 2017; Munigala et al., 2018; Hong et al., 2021). In this paper, we jointly train image features and 10 text attributes across 23 diverse product types, with two different target text types with different writing styles: bullet points and paragraph descriptions. Our findings suggest that multimodal training with modern pretrained language models can generate fluent and persuasive advertisements, but are less faithful and informative, especially out of domain.</abstract>
      <url hash="9bb72f8c">2022.ecnlp-1.27</url>
      <bibkey>koto-etal-2022-pretrained</bibkey>
      <doi>10.18653/v1/2022.ecnlp-1.27</doi>
      <video href="2022.ecnlp-1.27.mp4"/>
    </paper>
    <paper id="28">
      <title>A Simple Baseline for Domain Adaptation in End to End <fixed-case>ASR</fixed-case> Systems Using Synthetic Data</title>
      <author><first>Raviraj</first><last>Joshi</last></author>
      <author><first>Anupam</first><last>Singh</last></author>
      <pages>244-249</pages>
      <abstract>Automatic Speech Recognition(ASR) has been dominated by deep learning-based end-to-end speech recognition models. These approaches require large amounts of labeled data in the form of audio-text pairs. Moreover, these models are more susceptible to domain shift as compared to traditional models. It is common practice to train generic ASR models and then adapt them to target domains using comparatively smaller data sets. We consider a more extreme case of domain adaptation where text-only corpus is available. In this work, we propose a simple baseline technique for domain adaptation in end-to-end speech recognition models. We convert the text-only corpus to audio data using single speaker Text to Speech (TTS) engine. The parallel data in the target domain is then used to fine-tune the final dense layer of generic ASR models. We show that single speaker synthetic TTS data coupled with final dense layer only fine-tuning provides reasonable improvements in word error rates. We use text data from address and e-commerce search domains to show the effectiveness of our low-cost baseline approach on CTC and attention-based models.</abstract>
      <url hash="b6b30a29">2022.ecnlp-1.28</url>
      <bibkey>joshi-singh-2022-simple</bibkey>
      <doi>10.18653/v1/2022.ecnlp-1.28</doi>
      <video href="2022.ecnlp-1.28.mp4"/>
    </paper>
    <paper id="29">
      <title>Lot or Not: Identifying Multi-Quantity Offerings in <fixed-case>E</fixed-case>-Commerce</title>
      <author><first>Gal</first><last>Lavee</last></author>
      <author><first>Ido</first><last>Guy</last></author>
      <pages>250-262</pages>
      <abstract>The term <i>lot</i> in is defined to mean an offering that contains a collection of multiple identical items for sale. In a large online marketplace, lot offerings play an important role, allowing buyers and sellers to set price levels to optimally balance supply and demand needs. In spite of their central role, platforms often struggle to identify lot offerings, since explicit lot status identification is frequently not provided by sellers. The ability to identify lot offerings plays a key role in many fundamental tasks, from matching offerings to catalog products, through ranking search results, to providing effective pricing guidance. In this work, we seek to determine the lot status (and lot size) of each offering, in order to facilitate an improved buyer experience, while reducing the friction for sellers posting new offerings. We demonstrate experimentally the ability to accurately classify offerings as lots and predict their lot size using only the offer title, by adapting state-of-the-art natural language techniques to the lot identification problem. </abstract>
      <url hash="37294547">2022.ecnlp-1.29</url>
      <bibkey>lavee-guy-2022-lot</bibkey>
      <doi>10.18653/v1/2022.ecnlp-1.29</doi>
      <video href="2022.ecnlp-1.29.mp4"/>
    </paper>
  </volume>
</collection>
