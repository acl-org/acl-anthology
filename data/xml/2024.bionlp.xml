<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.bionlp">
  <volume id="1" ingest-date="2024-07-20" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 23rd Workshop on Biomedical Natural Language Processing</booktitle>
      <editor><first>Dina</first><last>Demner-Fushman</last></editor>
      <editor><first>Sophia</first><last>Ananiadou</last></editor>
      <editor><first>Makoto</first><last>Miwa</last></editor>
      <editor><first>Kirk</first><last>Roberts</last></editor>
      <editor><first>Junichi</first><last>Tsujii</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Bangkok, Thailand</address>
      <month>August</month>
      <year>2024</year>
      <url hash="6d02cd6b">2024.bionlp-1</url>
      <venue>bionlp</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="1dafa236">2024.bionlp-1.0</url>
      <bibkey>bionlp-2024-biomedical</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Improving Self-training with Prototypical Learning for Source-Free Domain Adaptation on Clinical Text</title>
      <author><first>Seiji</first><last>Shimizu</last></author>
      <author><first>Shuntaro</first><last>Yada</last></author>
      <author><first>Lisa</first><last>Raithel</last></author>
      <author><first>Eiji</first><last>Aramaki</last></author>
      <pages>1–13</pages>
      <abstract>Domain adaptation is crucial in the clinical domain since the performance of a model trained on one domain (source) degrades seriously when applied to another domain (target). However, conventional domain adaptation methods often cannot be applied due to data sharing restrictions on source data. Source-Free Domain Adaptation (SFDA) addresses this issue by only utilizing a source model and unlabeled target data to adapt to the target domain. In SFDA, self-training is the most widely applied method involving retraining models with target data using predictions from the source model as pseudo-labels. Nevertheless, this approach is prone to contain substantial numbers of errors in pseudo-labeling and might limit model performance in the target domain. In this paper, we propose a Source-Free Prototype-based Self-training (SFPS) aiming to improve the performance of self-training. SFPS generates prototypes without accessing source data and utilizes them for prototypical learning, namely prototype-based pseudo-labeling and contrastive learning. Also, we compare entropy-based, centroid-based, and class-weights-based prototype generation methods to identify the most effective formulation of the proposed method. Experimental results across various datasets demonstrate the effectiveness of the proposed method, consistently outperforming vanilla self-training. The comparison of various prototype-generation methods identifies the most reliable generation method that improves the source model persistently. Additionally, our analysis illustrates SFPS can successfully alleviate errors in pseudo-labeling.</abstract>
      <url hash="b6d5ee47">2024.bionlp-1.1</url>
      <bibkey>shimizu-etal-2024-improving</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.1</doi>
    </paper>
    <paper id="2">
      <title>Generation and Evaluation of Synthetic Endoscopy Free-Text Reports with Differential Privacy</title>
      <author><first>Agathe</first><last>Zecevic</last></author>
      <author><first>Xinyue</first><last>Zhang</last></author>
      <author><first>Sebastian</first><last>Zeki</last></author>
      <author><first>Angus</first><last>Roberts</last></author>
      <pages>14–24</pages>
      <abstract>The development of NLP models in the healthcare sector faces important challenges due to the limited availability of patient data, mainly driven by privacy concerns. This study proposes the generation of synthetic free-text medical reports, specifically focusing on the gastroenterology domain, to address the scarcity of specialised datasets, while preserving patient privacy. We fine-tune BioGPT on over 90 000 endoscopy reports and integrate Differential Privacy (DP) into the training process. 10 000 DP-private synthetic reports are generated by this model. The generated synthetic data is evaluated through multiple dimensions: similarity to real datasets, language quality, and utility in both supervised and semi-supervised NLP tasks. Results suggest that while DP integration impacts text quality, it offers a promising balance between data utility and privacy, improving the performance of a real-world downstream task. Our study underscores the potential of synthetic data to facilitate model development in the healthcare domain without compromising patient privacy.</abstract>
      <url hash="29178582">2024.bionlp-1.2</url>
      <bibkey>zecevic-etal-2024-generation</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.2</doi>
    </paper>
    <paper id="3">
      <title>Evaluating the Robustness of Adverse Drug Event Classification Models using Templates</title>
      <author><first>Dorothea</first><last>MacPhail</last></author>
      <author><first>David</first><last>Harbecke</last></author>
      <author><first>Lisa</first><last>Raithel</last></author>
      <author><first>Sebastian</first><last>Möller</last></author>
      <pages>25–38</pages>
      <abstract>An adverse drug effect (ADE) is any harmful event resulting from medical drug treatment. Despite their importance, ADEs are often under-reported in official channels. Some research has therefore turned to detecting discussions of ADEs in social media. Impressive results have been achieved in various attempts to detect ADEs. In a high-stakes domain such as medicine, however, an in-depth evaluation of a model’s abilities is crucial. We address the issue of thorough performance evaluation in detecting ADEs with hand-crafted templates for four capabilities, temporal order, negation, sentiment and beneficial effect. We find that models with similar performance on held-out test sets have varying results on these capabilities.</abstract>
      <url hash="cbd07592">2024.bionlp-1.3</url>
      <bibkey>macphail-etal-2024-evaluating</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.3</doi>
    </paper>
    <paper id="4">
      <title>Advancing Healthcare Automation: Multi-Agent System for Medical Necessity Justification</title>
      <author><first>Himanshu Gautam</first><last>Pandey</last></author>
      <author><first>Akhil</first><last>Amod</last></author>
      <author><first>Shivang</first><last>Kumar</last></author>
      <pages>39–49</pages>
      <abstract>Prior Authorization delivers safe, appropriate, and cost-effective care that is medically justified with evidence-based guidelines. However, the process often requires labor-intensive manual comparisons between patient medical records and clinical guidelines, that is both repetitive and time-consuming. Recent developments in Large Language Models (LLMs) have shown potential in addressing complex medical NLP tasks with minimal supervision. This paper explores the application of Multi-Agent System (MAS) that utilize specialized LLM agents to automate Prior Authorization task by breaking them down into simpler and manageable sub-tasks. Our study systematically investigates the effects of various prompting strategies on these agents and benchmarks the performance of different LLMs. We demonstrate that GPT-4 achieves an accuracy of 86.2% in predicting checklist item-level judgments with evidence, and 95.6% in determining overall checklist judgment. Additionally, we explore how these agents can contribute to explainability of steps taken in the process, thereby enhancing trust and transparency in the system.</abstract>
      <url hash="651d42ef">2024.bionlp-1.4</url>
      <bibkey>pandey-etal-2024-advancing</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.4</doi>
    </paper>
    <paper id="5">
      <title>Open (Clinical) <fixed-case>LLM</fixed-case>s are Sensitive to Instruction Phrasings</title>
      <author><first>Alberto Mario</first><last>Ceballos-Arroyo</last></author>
      <author><first>Monica</first><last>Munnangi</last></author>
      <author><first>Jiuding</first><last>Sun</last></author>
      <author><first>Karen</first><last>Zhang</last></author>
      <author><first>Jered</first><last>McInerney</last></author>
      <author><first>Byron C.</first><last>Wallace</last></author>
      <author><first>Silvio</first><last>Amir</last></author>
      <pages>50–71</pages>
      <abstract>Instruction-tuned Large Language Models (LLMs) can perform a wide range of tasks given natural language instructions to do so, but they are sensitive to how such instructions are phrased. This issue is especially concerning in healthcare, as clinicians are unlikely to be experienced prompt engineers and the potential consequences of inaccurate outputs are heightened in this domain. This raises a practical question: How robust are instruction-tuned LLMs to natural variations in the instructions provided for clinical NLP tasks? We collect prompts from medical doctors across a range of tasks and quantify the sensitivity of seven LLMs—some general, others specialized—to natural (i.e., non-adversarial) instruction phrasings. We find that performance varies substantially across all models, and that—perhaps surprisingly—domain-specific models explicitly trained on clinical data are especially brittle, compared to their general domain counterparts. Further, arbitrary phrasing differences can affect fairness, e.g., valid but distinct instructions for mortality prediction yield a range both in overall performance, and in terms of differences between demographic groups.</abstract>
      <url hash="59ad9eff">2024.bionlp-1.5</url>
      <bibkey>ceballos-arroyo-etal-2024-open</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.5</doi>
    </paper>
    <paper id="6">
      <title>Analysing zero-shot temporal relation extraction on clinical notes using temporal consistency</title>
      <author><first>Vasiliki</first><last>Kougia</last></author>
      <author><first>Anastasiia</first><last>Sedova</last></author>
      <author><first>Andreas Joseph</first><last>Stephan</last></author>
      <author><first>Klim</first><last>Zaporojets</last></author>
      <author><first>Benjamin</first><last>Roth</last></author>
      <pages>72–84</pages>
      <abstract>This paper presents the first study for temporal relation extraction in a zero-shot setting focusing on biomedical text. We employ two types of prompts and five Large Language Models (LLMs; GPT-3.5, Mixtral, Llama 2, Gemma, and PMC-LLaMA) to obtain responses about the temporal relations between two events. Our experiments demonstrate that LLMs struggle in the zero-shot setting, performing worse than fine-tuned specialized models in terms of F1 score. This highlights the challenging nature of this task and underscores the need for further research to enhance the performance of LLMs in this context. We further contribute a novel comprehensive temporal analysis by calculating consistency scores for each LLM. Our findings reveal that LLMs face challenges in providing responses consistent with the temporal properties of uniqueness and transitivity. Moreover, we study the relation between the temporal consistency of an LLM and its accuracy, and whether the latter can be improved by solving temporal inconsistencies. Our analysis shows that even when temporal consistency is achieved, the predictions can remain inaccurate.</abstract>
      <url hash="8dc48884">2024.bionlp-1.6</url>
      <bibkey>kougia-etal-2024-analysing</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.6</doi>
    </paper>
    <paper id="7">
      <title>Overview of the First Shared Task on Clinical Text Generation: <fixed-case>RRG</fixed-case>24 and “Discharge Me!”</title>
      <author><first>Justin</first><last>Xu</last></author>
      <author><first>Zhihong</first><last>Chen</last></author>
      <author><first>Andrew</first><last>Johnston</last></author>
      <author><first>Louis</first><last>Blankemeier</last></author>
      <author><first>Maya</first><last>Varma</last></author>
      <author><first>Jason</first><last>Hom</last></author>
      <author><first>William J.</first><last>Collins</last></author>
      <author><first>Ankit</first><last>Modi</last></author>
      <author><first>Robert</first><last>Lloyd</last></author>
      <author><first>Benjamin</first><last>Hopkins</last></author>
      <author><first>Curtis</first><last>Langlotz</last></author>
      <author><first>Jean-Benoit</first><last>Delbrouck</last></author>
      <pages>85–98</pages>
      <abstract>Recent developments in natural language generation have tremendous implications for healthcare. For instance, state-of-the-art systems could automate the generation of sections in clinical reports to alleviate physician workload and streamline hospital documentation. To explore these applications, we present a shared task consisting of two subtasks: (1) Radiology Report Generation (RRG24) and (2) Discharge Summary Generation (“Discharge Me!”). RRG24 involves generating the ‘Findings’ and ‘Impression’ sections of radiology reports given chest X-rays. “Discharge Me!” involves generating the ‘Brief Hospital Course’ and '‘Discharge Instructions’ sections of discharge summaries for patients admitted through the emergency department. “Discharge Me!” submissions were subsequently reviewed by a team of clinicians. Both tasks emphasize the goal of reducing clinician burnout and repetitive workloads by generating documentation. We received 201 submissions from across 8 teams for RRG24, and 211 submissions from across 16 teams for “Discharge Me!”.</abstract>
      <url hash="f69ee7dd">2024.bionlp-1.7</url>
      <bibkey>xu-etal-2024-overview</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.7</doi>
    </paper>
    <paper id="8">
      <title>e-Health <fixed-case>CSIRO</fixed-case> at <fixed-case>RRG</fixed-case>24: Entropy-Augmented Self-Critical Sequence Training for Radiology Report Generation</title>
      <author><first>Aaron</first><last>Nicolson</last></author>
      <author><first>Jinghui</first><last>Liu</last></author>
      <author><first>Jason</first><last>Dowling</last></author>
      <author><first>Anthony</first><last>Nguyen</last></author>
      <author><first>Bevan</first><last>Koopman</last></author>
      <pages>99–104</pages>
      <abstract>The core novelty of our approach lies in the addition of entropy regularisation to self-critical sequence training. This helps maintain a higher entropy in the token distribution, preventing overfitting to common phrases and ensuring a broader exploration of the vocabulary during training, which is essential for handling the diversity of the radiology reports in the RRG24 datasets. We apply this to a multimodal language model with RadGraph as the reward. Additionally, our model incorporates several other aspects. We use token type embeddings to differentiate between findings and impression section tokens, as well as image embeddings. To handle missing sections, we employ special tokens. We also utilise an attention mask with non-causal masking for the image embeddings and a causal mask for the report token embeddings.</abstract>
      <url hash="002ccd4f">2024.bionlp-1.8</url>
      <bibkey>nicolson-etal-2024-e</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.8</doi>
    </paper>
    <paper id="9">
      <title><fixed-case>W</fixed-case>is<fixed-case>P</fixed-case>er<fixed-case>M</fixed-case>ed at “Discharge Me!”: Advancing Text Generation in Healthcare with Large Language Models, Dynamic Expert Selection, and Priming Techniques on <fixed-case>MIMIC</fixed-case>-<fixed-case>IV</fixed-case></title>
      <author><first>Hendrik</first><last>Damm</last></author>
      <author><first>Tabea Margareta Grace</first><last>Pakull</last></author>
      <author><first>Bahadır</first><last>Eryılmaz</last></author>
      <author><first>Helmut</first><last>Becker</last></author>
      <author><first>Ahmad</first><last>Idrissi-Yaghir</last></author>
      <author><first>Henning</first><last>Schäfer</last></author>
      <author><first>Sergej</first><last>Schultenkämper</last></author>
      <author><first>Christoph M.</first><last>Friedrich</last></author>
      <pages>105–121</pages>
      <abstract>This study aims to leverage state of the art language models to automate generating the “Brief Hospital Course” and “Discharge Instructions” sections of Discharge Summaries from the MIMIC-IV dataset, reducing clinicians’ administrative workload. We investigate how automation can improve documentation accuracy, alleviate clinician burnout, and enhance operational efficacy in healthcare facilities. This research was conducted within our participation in the Shared Task Discharge Me! at BioNLP @ ACL 2024. Various strategies were employed, including Few-Shot learning, instruction tuning, and Dynamic Expert Selection (DES), to develop models capable of generating the required text sections. Utilizing an additional clinical domain-specific dataset demonstrated substantial potential to enhance clinical language processing. The DES method, which optimizes the selection of text outputs from multiple predictions, proved to be especially effective. It achieved the highest overall score of 0.332 in the competition, surpassing single-model outputs. This finding suggests that advanced deep learning methods in combination with DES can effectively automate parts of electronic health record documentation. These advancements could enhance patient care by freeing clinician time for patient interactions. The integration of text selection strategies represents a promising avenue for further research.</abstract>
      <url hash="a9667d54">2024.bionlp-1.9</url>
      <bibkey>damm-etal-2024-wispermed</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.9</doi>
    </paper>
    <paper id="10">
      <title>Overview of the <fixed-case>B</fixed-case>io<fixed-case>L</fixed-case>ay<fixed-case>S</fixed-case>umm 2024 Shared Task on the Lay Summarization of Biomedical Research Articles</title>
      <author><first>Tomas</first><last>Goldsack</last></author>
      <author><first>Carolina</first><last>Scarton</last></author>
      <author><first>Matthew</first><last>Shardlow</last></author>
      <author><first>Chenghua</first><last>Lin</last></author>
      <pages>122–131</pages>
      <abstract>This paper presents the setup and results of the second edition of the BioLaySumm shared task on the Lay Summarisation of Biomedical Research Articles, hosted at the BioNLP Workshop at ACL 2024. In this task edition, we aim to build on the first edition’s success by further increasing research interest in this important task and encouraging participants to explore novel approaches that will help advance the state-of-the-art. Encouragingly, we found research interest in the task to be high, with this edition of the task attracting a total of 53 participating teams, a significant increase in engagement from the previous edition. Overall, our results show that a broad range of innovative approaches were adopted by task participants, with a predictable shift towards the use of Large Language Models (LLMs).</abstract>
      <url hash="b4f3aefa">2024.bionlp-1.10</url>
      <bibkey>goldsack-etal-2024-overview</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.10</doi>
    </paper>
    <paper id="11">
      <title><fixed-case>UIUC</fixed-case>_<fixed-case>B</fixed-case>io<fixed-case>NLP</fixed-case> at <fixed-case>B</fixed-case>io<fixed-case>L</fixed-case>ay<fixed-case>S</fixed-case>umm: An Extract-then-Summarize Approach Augmented with <fixed-case>W</fixed-case>ikipedia Knowledge for Biomedical Lay Summarization</title>
      <author><first>Zhiwen</first><last>You</last></author>
      <author><first>Shruthan</first><last>Radhakrishna</last></author>
      <author><first>Shufan</first><last>Ming</last></author>
      <author><first>Halil</first><last>Kilicoglu</last></author>
      <pages>132–143</pages>
      <abstract>As the number of scientific publications is growing at a rapid pace, it is difficult for laypeople to keep track of and understand the latest scientific advances, especially in the biomedical domain. While the summarization of scientific publications has been widely studied, research on summarization targeting laypeople has remained scarce. In this study, considering the lengthy input of biomedical articles, we have developed a lay summarization system through an extract-then-summarize framework with large language models (LLMs) to summarize biomedical articles for laypeople. Using a fine-tuned GPT-3.5 model, our approach achieves the highest overall ranking and demonstrates the best relevance performance in the BioLaySumm 2024 shared task.</abstract>
      <url hash="deb7ae49">2024.bionlp-1.11</url>
      <bibkey>you-etal-2024-uiuc</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.11</doi>
    </paper>
    <paper id="12">
      <title>End-to-End Relation Extraction of Pharmacokinetic Estimates from the Scientific Literature</title>
      <author><first>Ferran</first><last>Gonzalez Hernandez</last></author>
      <author><first>Victoria</first><last>Smith</last></author>
      <author><first>Quang</first><last>Nguyen</last></author>
      <author><first>Palang</first><last>Chotsiri</last></author>
      <author><first>Thanaporn</first><last>Wattanakul</last></author>
      <author><first>José</first><last>Antonio Cordero</last></author>
      <author><first>Maria Rosa</first><last>Ballester</last></author>
      <author><first>Albert</first><last>Sole</last></author>
      <author><first>Gill</first><last>Mundin</last></author>
      <author><first>Watjana</first><last>Lilaonitkul</last></author>
      <author><first>Joseph F.</first><last>Standing</last></author>
      <author><first>Frank</first><last>Kloprogge</last></author>
      <pages>144–154</pages>
      <abstract>The lack of comprehensive and standardised databases containing Pharmacokinetic (PK) parameters presents a challenge in the drug development pipeline. Efficiently managing the increasing volume of published PK Parameters requires automated approaches that centralise information from diverse studies. In this work, we present the Pharmacokinetic Relation Extraction Dataset (PRED), a novel, manually curated corpus developed by pharmacometricians and NLP specialists, covering multiple types of PK parameters and numerical expressions reported in open-access scientific articles. PRED covers annotations for various entities and relations involved in PK parameter measurements from 3,600 sentences. We also introduce an end-to-end relation extraction model based on BioBERT, which is trained with joint named entity recognition (NER) and relation extraction objectives. The optimal pipeline achieved a micro-average F1-score of 94% for NER and over 85% F1-score across all relation types. This work represents the first resource for training and evaluating models for PK end-to-end extraction across multiple parameters and study types. We make our corpus and model openly available to accelerate the construction of large PK databases and to support similar endeavours in other scientific disciplines.</abstract>
      <url hash="ae647373">2024.bionlp-1.12</url>
      <bibkey>gonzalez-hernandez-etal-2024-end</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.12</doi>
    </paper>
    <paper id="13">
      <title><fixed-case>KG</fixed-case>-Rank: Enhancing Large Language Models for Medical <fixed-case>QA</fixed-case> with Knowledge Graphs and Ranking Techniques</title>
      <author><first>Rui</first><last>Yang</last></author>
      <author><first>Haoran</first><last>Liu</last></author>
      <author><first>Edison</first><last>Marrese-Taylor</last></author>
      <author><first>Qingcheng</first><last>Zeng</last></author>
      <author><first>Yuhe</first><last>Ke</last></author>
      <author><first>Wanxin</first><last>Li</last></author>
      <author><first>Lechao</first><last>Cheng</last></author>
      <author><first>Qingyu</first><last>Chen</last></author>
      <author><first>James</first><last>Caverlee</last></author>
      <author><first>Yutaka</first><last>Matsuo</last></author>
      <author><first>Irene</first><last>Li</last></author>
      <pages>155–166</pages>
      <abstract>Large Language Models (LLMs) have significantly advanced healthcare innovation on generation capabilities. However, their application in real clinical settings is challenging due to potential deviations from medical facts and inherent biases. In this work, we develop an augmented LLM framework, KG-Rank, which leverages a medical knowledge graph (KG) with ranking and re-ranking techniques, aiming to improve free-text question-answering (QA) in the medical domain. Specifically, upon receiving a question, we initially retrieve triplets from a medical KG to gather factual information. Subsequently, we innovatively apply ranking methods to refine the ordering of these triplets, aiming to yield more precise answers. To the best of our knowledge, KG-Rank is the first application of ranking models combined with KG in medical QA specifically for generating long answers. Evaluation of four selected medical QA datasets shows that KG-Rank achieves an improvement of over 18% in the ROUGE-L score. Moreover, we extend KG-Rank to open domains, where it realizes a 14% improvement in ROUGE-L, showing the effectiveness and potential of KG-Rank.</abstract>
      <url hash="a88d3aa3">2024.bionlp-1.13</url>
      <bibkey>yang-etal-2024-kg</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.13</doi>
    </paper>
    <paper id="14">
      <title><fixed-case>M</fixed-case>ed<fixed-case>E</fixed-case>x<fixed-case>QA</fixed-case>: Medical Question Answering Benchmark with Multiple Explanations</title>
      <author><first>Yunsoo</first><last>Kim</last></author>
      <author><first>Jinge</first><last>Wu</last></author>
      <author><first>Yusuf</first><last>Abdulle</last></author>
      <author><first>Honghan</first><last>Wu</last></author>
      <pages>167–181</pages>
      <abstract>This paper introduces MedExQA, a novel benchmark in medical question-answering, to evaluate large language models’ (LLMs) understanding of medical knowledge through explanations. By constructing datasets across five distinct medical specialties that are underrepresented in current datasets and further incorporating multiple explanations for each question-answer pair, we address a major gap in current medical QA benchmarks which is the absence of comprehensive assessments of LLMs’ ability to generate nuanced medical explanations. Our work highlights the importance of explainability in medical LLMs, proposes an effective methodology for evaluating models beyond classification accuracy, and sheds light on one specific domain, speech language pathology, where current LLMs including GPT4 lack good understanding. Our results show generation evaluation with multiple explanations aligns better with human assessment, highlighting an opportunity for a more robust automated comprehension assessment for LLMs. To diversify open-source medical LLMs (currently mostly based on Llama2), this work also proposes a new medical model, MedPhi-2, based on Phi-2 (2.7B). The model outperformed medical LLMs based on Llama2-70B in generating explanations, showing its effectiveness in the resource-constrained medical domain. We will share our benchmark datasets and the trained model.</abstract>
      <url hash="2895c704">2024.bionlp-1.14</url>
      <bibkey>kim-etal-2024-medexqa</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.14</doi>
    </paper>
    <paper id="15">
      <title>Do Clinicians Know How to Prompt? The Need for Automatic Prompt Optimization Help in Clinical Note Generation</title>
      <author><first>Zonghai</first><last>Yao</last></author>
      <author><first>Ahmed</first><last>Jaafar</last></author>
      <author><first>Beining</first><last>Wang</last></author>
      <author><first>Zhichao</first><last>Yang</last></author>
      <author><first>Hong</first><last>Yu</last></author>
      <pages>182–201</pages>
      <abstract>This study examines the effect of prompt engineering on the performance of Large Language Models (LLMs) in clinical note generation. We introduce an Automatic Prompt Optimization (APO) framework to refine initial prompts and compare the outputs of medical experts, non-medical experts, and APO-enhanced GPT3.5 and GPT4. Results highlight GPT4-APO’s superior performance in standardizing prompt quality across clinical note sections. A human-in-the-loop approach shows that experts maintain content quality post-APO, with a preference for their own modifications, suggesting the value of expert customization. We recommend a two-phase optimization process, leveraging APO-GPT4 for consistency and expert input for personalization.</abstract>
      <url hash="71c25c0e">2024.bionlp-1.15</url>
      <bibkey>yao-etal-2024-clinicians</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.15</doi>
    </paper>
    <paper id="16">
      <title>Domain-specific or Uncertainty-aware models: Does it really make a difference for biomedical text classification?</title>
      <author><first>Aman</first><last>Sinha</last></author>
      <author><first>Timothee</first><last>Mickus</last></author>
      <author><first>Marianne</first><last>Clausel</last></author>
      <author><first>Mathieu</first><last>Constant</last></author>
      <author><first>Xavier</first><last>Coubez</last></author>
      <pages>202–211</pages>
      <abstract>The success of pretrained language models (PLMs) across a spate of use-cases has led to significant investment from the NLP community towards building domain-specific foundational models. On the other hand, in mission critical settings such as biomedical applications, other aspects also factor in—chief of which is a model’s ability to produce reasonable estimates of its own uncertainty. In the present study, we discuss these two desiderata through the lens of how they shape the entropy of a model’s output probability distribution. We find that domain specificity and uncertainty awareness can often be successfully combined, but the exact task at hand weighs in much more strongly.</abstract>
      <url hash="e84a0638">2024.bionlp-1.16</url>
      <bibkey>sinha-etal-2024-domain</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.16</doi>
    </paper>
    <paper id="17">
      <title>Can Rule-Based Insights Enhance <fixed-case>LLM</fixed-case>s for Radiology Report Classification? Introducing the <fixed-case>R</fixed-case>ad<fixed-case>P</fixed-case>rompt Methodology.</title>
      <author><first>Panagiotis</first><last>Fytas</last></author>
      <author><first>Anna</first><last>Breger</last></author>
      <author><first>Ian</first><last>Selby</last></author>
      <author><first>Simon</first><last>Baker</last></author>
      <author><first>Shahab</first><last>Shahipasand</last></author>
      <author><first>Anna</first><last>Korhonen</last></author>
      <pages>212–235</pages>
      <abstract>Developing imaging models capable of detecting pathologies from chest X-rays can be cost and time-prohibitive for large datasets as it requires supervision to attain state-of-the-art performance. Instead, labels extracted from radiology reports may serve as distant supervision since these are routinely generated as part of clinical practice. Despite their widespread use, current rule-based methods for label extraction rely on extensive rule sets that are limited in their robustness to syntactic variability. To alleviate these limitations, we introduce RadPert, a rule-based system that integrates an uncertainty-aware information schema with a streamlined set of rules, enhancing performance. Additionally, we have developed RadPrompt, a multi-turn prompting strategy that leverages RadPert to bolster the zero-shot predictive capabilities of large language models, achieving a statistically significant improvement in weighted average F1 score over GPT-4 Turbo. Most notably, RadPrompt surpasses both its underlying models, showcasing the synergistic potential of LLMs with rule-based models. We have evaluated our methods on two English Corpora: the MIMIC-CXR gold-standard test set and a gold-standard dataset collected from the Cambridge University Hospitals.</abstract>
      <url hash="e76e5747">2024.bionlp-1.17</url>
      <bibkey>fytas-etal-2024-rule</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.17</doi>
    </paper>
    <paper id="18">
      <title>Using Large Language Models to Evaluate Biomedical Query-Focused Summarisation</title>
      <author><first>Hashem</first><last>Hijazi</last></author>
      <author><first>Diego</first><last>Molla</last></author>
      <author><first>Vincent</first><last>Nguyen</last></author>
      <author><first>Sarvnaz</first><last>Karimi</last></author>
      <pages>236–242</pages>
      <abstract>Biomedical question-answering systems remain popular for biomedical experts interacting with the literature to answer their medical questions. However, these systems are difficult to evaluate in the absence of costly human experts. Therefore, automatic evaluation metrics are often used in this space. Traditional automatic metrics such as ROUGE or BLEU, which rely on token overlap, have shown a low correlation with humans. We present a study that uses large language models (LLMs) to automatically evaluate systems from an international challenge on biomedical semantic indexing and question answering, called BioASQ. We measure the agreement of LLM-produced scores against human judgements. We show that LLMs correlate similarly to lexical methods when using basic prompting techniques. However, by aggregating evaluators with LLMs or by fine-tuning, we find that our methods outperform the baselines by a large margin, achieving a Spearman correlation of 0.501 and 0.511, respectively.</abstract>
      <url hash="816c21fd">2024.bionlp-1.18</url>
      <bibkey>hijazi-etal-2024-using</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.18</doi>
    </paper>
    <paper id="19">
      <title>Continuous Predictive Modeling of Clinical Notes and <fixed-case>ICD</fixed-case> Codes in Patient Health Records</title>
      <author><first>Mireia Hernandez</first><last>Caralt</last></author>
      <author><first>Clarence Boon Liang</first><last>Ng</last></author>
      <author><first>Marek</first><last>Rei</last></author>
      <pages>243–255</pages>
      <abstract>Electronic Health Records (EHR) serve as a valuable source of patient information, offering insights into medical histories, treatments, and outcomes. Previous research has developed systems for detecting applicable ICD codes that should be assigned while writing a given EHR document, mainly focusing on discharge summaries written at the end of a hospital stay. In this work, we investigate the potential of predicting these codes for the whole patient stay at different time points during their stay, even before they are officially assigned by clinicians. The development of methods to predict diagnoses and treatments earlier in advance could open opportunities for predictive medicine, such as identifying disease risks sooner, suggesting treatments, and optimizing resource allocation. Our experiments show that predictions regarding final ICD codes can be made already two days after admission and we propose a custom model that improves performance on this early prediction task.</abstract>
      <url hash="854da042">2024.bionlp-1.19</url>
      <bibkey>caralt-etal-2024-continuous</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.19</doi>
    </paper>
    <paper id="20">
      <title>Can <fixed-case>GPT</fixed-case> Redefine Medical Understanding? Evaluating <fixed-case>GPT</fixed-case> on Biomedical Machine Reading Comprehension</title>
      <author><first>Shubham</first><last>Vatsal</last></author>
      <author><first>Ayush</first><last>Singh</last></author>
      <pages>256–265</pages>
      <abstract>Large language models (LLMs) have shown remarkable performance on many tasks in different domains. However, their performance in contextual biomedical machine reading comprehension (MRC) has not been evaluated in depth. In this work, we evaluate GPT on four contextual biomedical MRC benchmarks. We experiment with different conventional prompting techniques as well as introduce our own novel prompting method. To solve some of the retrieval problems inherent to LLMs, we propose a prompting strategy named Implicit Retrieval Augmented Generation (RAG) that alleviates the need for using vector databases to retrieve important chunks in traditional RAG setups. Moreover, we report qualitative assessments on the natural language generation outputs from our approach. The results show that our new prompting technique is able to get the best performance in two out of four datasets and ranks second in rest of them. Experiments show that modern-day LLMs like GPT even in a zero-shot setting can outperform supervised models, leading to new state-of-the-art (SoTA) results on two of the benchmarks.</abstract>
      <url hash="a22be40d">2024.bionlp-1.20</url>
      <bibkey>vatsal-singh-2024-gpt</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.20</doi>
    </paper>
    <paper id="21">
      <title>Get the Best out of 1<fixed-case>B</fixed-case> <fixed-case>LLM</fixed-case>s: Insights from Information Extraction on Clinical Documents</title>
      <author><first>Saeed</first><last>Farzi</last></author>
      <author><first>Soumitra</first><last>Ghosh</last></author>
      <author><first>Alberto</first><last>Lavelli</last></author>
      <author><first>Bernardo</first><last>Magnini</last></author>
      <pages>266–276</pages>
      <abstract>While the popularity of large, versatile language models like ChatGPT continues to rise, the landscape shifts when considering open-source models tailored to specific domains. Moreover, many areas, such as clinical documents, suffer from a scarcity of training data, often amounting to only a few hundred instances. Additionally, in certain settings, such as hospitals, cloud-based solutions pose privacy concerns, necessitating the deployment of language models on traditional hardware, such as single GPUs or powerful CPUs. To address these complexities, we conduct extensive experiments on both clinical entity detection and relation extraction in clinical documents using 1B parameter models. Our study delves into traditional fine-tuning, continuous pre-training in the medical domain, and instruction-tuning methods, providing valuable insights into their effectiveness in a multilingual setting. Our results underscore the importance of domain-specific models and pre-training for clinical natural language processing tasks. Furthermore, data augmentation using cross-lingual information improves performance in most cases, highlighting the potential for multilingual enhancements.</abstract>
      <url hash="27998f64">2024.bionlp-1.21</url>
      <bibkey>farzi-etal-2024-get</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.21</doi>
    </paper>
    <paper id="22">
      <title>K-<fixed-case>QA</fixed-case>: A Real-World Medical <fixed-case>Q</fixed-case>&amp;<fixed-case>A</fixed-case> Benchmark</title>
      <author><first>Itay</first><last>Manes</last></author>
      <author><first>Naama</first><last>Ronn</last></author>
      <author><first>David</first><last>Cohen</last></author>
      <author><first>Ran</first><last>Ilan Ber</last></author>
      <author><first>Zehavi</first><last>Horowitz-Kugler</last></author>
      <author><first>Gabriel</first><last>Stanovsky</last></author>
      <pages>277–294</pages>
      <abstract>Ensuring the accuracy of responses provided by large language models (LLMs) is crucial, particularly in clinical settings where incorrect information may directly impact patient health. To address this challenge, we construct K-QA, a dataset containing 1,212 patient questions originating from real-world conversations held on a popular clinical online platform. We employ a panel of in-house physicians to answer and manually decompose a subset of K-QA into self-contained statements. Additionally, we formulate two NLI-based evaluation metrics approximating recall and precision: (1) comprehensiveness, measuring the percentage of essential clinical information in the generated answer and (2) hallucination rate, measuring the number of statements from the physician-curated response contradicted by the LLM answer. Finally, we use K-QA along with these metrics to evaluate several state-of-the-art models, as well as the effect of in-context learning and medically-oriented augmented retrieval schemes developed by the authors. Our findings indicate that in-context learning improves the comprehensiveness of the models, and augmented retrieval is effective in reducing hallucinations. We will make K-QA available to to the community to spur research into medically accurate NLP applications.</abstract>
      <url hash="4d5597bc">2024.bionlp-1.22</url>
      <bibkey>manes-etal-2024-k</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.22</doi>
    </paper>
    <paper id="23">
      <title>Large Language Models for Biomedical Knowledge Graph Construction: Information extraction from <fixed-case>EMR</fixed-case> notes</title>
      <author><first>Vahan</first><last>Arsenyan</last></author>
      <author><first>Spartak</first><last>Bughdaryan</last></author>
      <author><first>Fadi</first><last>Shaya</last></author>
      <author><first>Kent Wilson</first><last>Small</last></author>
      <author><first>Davit</first><last>Shahnazaryan</last></author>
      <pages>295–317</pages>
      <abstract>The automatic construction of knowledge graphs (KGs) is an important research area in medicine, with far-reaching applications spanning drug discovery and clinical trial design. These applications hinge on the accurate identification of interactions among medical and biological entities. In this study, we propose an end-to-end machine learning solution based on large language models (LLMs) that utilize electronic medical record notes to construct KGs. The entities used in the KG construction process are diseases, factors, treatments, as well as manifestations that coexist with the patient while experiencing the disease. Given the critical need for high-quality performance in medical applications, we embark on a comprehensive assessment of 12 LLMs of various architectures, evaluating their performance and safety attributes. To gauge the quantitative efficacy of our approach by assessing both precision and recall, we manually annotate a dataset provided by the Macula and Retina Institute. We also assess the qualitative performance of LLMs, such as the ability to generate structured outputs or the tendency to hallucinate. The results illustrate that in contrast to encoder-only and encoder-decoder, decoder-only LLMs require further investigation. Additionally, we provide guided prompt design to utilize such LLMs. The application of the proposed methodology is demonstrated on age-related macular degeneration.</abstract>
      <url hash="c4f81e82">2024.bionlp-1.23</url>
      <bibkey>arsenyan-etal-2024-large</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.23</doi>
    </paper>
    <paper id="24">
      <title>Document-level Clinical Entity and Relation extraction via Knowledge Base-Guided Generation</title>
      <author><first>Kriti</first><last>Bhattarai</last></author>
      <author><first>Inez Y.</first><last>Oh</last></author>
      <author><first>Zachary B.</first><last>Abrams</last></author>
      <author><first>Albert M.</first><last>Lai</last></author>
      <pages>318–327</pages>
      <abstract>Generative pre-trained transformer (GPT) models have shown promise in clinical entity and relation extraction tasks because of their precise extraction and contextual understanding capability. In this work, we further leverage the Unified Medical Language System (UMLS) knowledge base to accurately identify medical concepts and improve clinical entity and relation extraction at the document level. Our framework selects UMLS concepts relevant to the text and combines them with prompts to guide language models in extracting entities. Our experiments demonstrate that this initial concept mapping and the inclusion of these mapped concepts in the prompts improves extraction results compared to few-shot extraction tasks on generic language models that do not leverage UMLS. Further, our results show that this approach is more effective than the standard Retrieval Augmented Generation (RAG) technique, where retrieved data is compared with prompt embeddings to generate results. Overall, we find that integrating UMLS concepts with GPT models significantly improves entity and relation identification, outperforming the baseline and RAG models. By combining the precise concept mapping capability of knowledge-based approaches like UMLS with the contextual understanding capability of GPT, our method highlights the potential of these approaches in specialized domains like healthcare.</abstract>
      <url hash="bb681dc6">2024.bionlp-1.24</url>
      <bibkey>bhattarai-etal-2024-document</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.24</doi>
    </paper>
    <paper id="25">
      <title><fixed-case>B</fixed-case>i<fixed-case>CAL</fixed-case>: Bi-directional Contrastive Active Learning for Clinical Report Generation</title>
      <author><first>Tianyi</first><last>Wu</last></author>
      <author><first>Jingqing</first><last>Zhang</last></author>
      <author><first>Wenjia</first><last>Bai</last></author>
      <author><first>Kai</first><last>Sun</last></author>
      <pages>328–341</pages>
      <abstract>State-of-the-art performance by large pre-trained models in computer vision (CV) and natural language processing (NLP) suggests their potential for domain-specific tasks. However, training these models requires vast amounts of labelled data, a challenge in many domains due to the cost and expertise required for data labelling. Active Learning (AL) can mitigate this by selecting minimal yet informative data for model training. While AL has been mainly applied to single-modal tasks in the fields of NLP and CV, its application in multi-modal tasks remains underexplored. In this work, we proposed a novel AL strategy, Bidirectional Contrastive Active Learning strategy (BiCAL), that used both image and text latent spaces to identify contrastive samples to select batches to query for labels. BiCAL was robust to class imbalance data problems by its design, which is a problem that is commonly seen in training domain-specific models. We assessed BiCAL’s performance in domain-specific learning on the clinical report generation tasks from chest X-ray images. Our experiments showed that BiCAL outperforms State-of-the-art methods in clinical efficacy metrics, improving recall by 2.4% and F1 score by 9.5%, showcasing its effectiveness in actively training domain-specific multi-modal models.</abstract>
      <url hash="2fdf87c1">2024.bionlp-1.25</url>
      <bibkey>wu-etal-2024-bical</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.25</doi>
    </paper>
    <paper id="26">
      <title>Generation and De-Identification of <fixed-case>I</fixed-case>ndian Clinical Discharge Summaries using <fixed-case>LLM</fixed-case>s</title>
      <author><first>Sanjeet</first><last>Singh</last></author>
      <author><first>Shreya</first><last>Gupta</last></author>
      <author><first>Niralee</first><last>Gupta</last></author>
      <author><first>Naimish</first><last>Sharma</last></author>
      <author><first>Lokesh</first><last>Srivastava</last></author>
      <author><first>Vibhu</first><last>Agarwal</last></author>
      <author><first>Ashutosh</first><last>Modi</last></author>
      <pages>342–362</pages>
      <abstract>The consequences of a healthcare data breach can be devastating for the patients, providers, and payers. The average financial impact of a data breach in recent months has been estimated to be close to USD 10 million. This is especially significant for healthcare organizations in India that are managing rapid digitization while still establishing data governance procedures that align with the letter and spirit of the law. Computer-based systems for de-identification of personal information are vulnerable to data drift, often rendering them ineffective in cross-institution settings. Therefore, a rigorous assessment of existing de-identification against local health datasets is imperative to support the safe adoption of digital health initiatives in India. Using a small set of de-identified patient discharge summaries provided by an Indian healthcare institution, in this paper, we report the nominal performance of de-identification algorithms (based on language models) trained on publicly available non-Indian datasets, pointing towards a lack of cross-institutional generalization. Similarly, experimentation with off-the-shelf de-identification systems reveals potential risks associated with the approach. To overcome data scarcity, we explore generating synthetic clinical reports (using publicly available and Indian summaries) by performing in-context learning over Large Language Models (LLMs). Our experiments demonstrate the use of generated reports as an effective strategy for creating high-performing de-identification systems with good generalization capabilities.</abstract>
      <url hash="8740b4aa">2024.bionlp-1.26</url>
      <bibkey>singh-etal-2024-generation</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.26</doi>
    </paper>
    <paper id="27">
      <title>Pre-training data selection for biomedical domain adaptation using journal impact metrics</title>
      <author><first>Mathieu</first><last>Lai-king</last></author>
      <author><first>Patrick</first><last>Paroubek</last></author>
      <pages>363–369</pages>
      <abstract>Domain adaptation is a widely used method in natural language processing (NLP) to improve the performance of a language model within a specific domain. This method is particularly common in the biomedical domain, which sees regular publication of numerous scientific articles. PubMed, a significant corpus of text, is frequently used in the biomedical domain. The primary objective of this study is to explore whether refining a pre-training dataset using specific quality metrics for scientific papers can enhance the performance of the resulting model. To accomplish this, we employ two straightforward journal impact metrics and conduct experiments by continually pre-training BERT on various subsets of the complete PubMed training set, we then evaluate the resulting models on biomedical language understanding tasks from the BLURB benchmark. Our results show that pruning using journal impact metrics is not efficient. But we also show that pre-training using fewer abstracts (but with the same number of training steps) does not necessarily decrease the resulting model’s performance.</abstract>
      <url hash="782aef89">2024.bionlp-1.27</url>
      <bibkey>lai-king-paroubek-2024-pre</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.27</doi>
    </paper>
    <paper id="28">
      <title>Leveraging <fixed-case>LLM</fixed-case>s and Web-based Visualizations for Profiling Bacterial Host Organisms and Genetic Toolboxes</title>
      <author><first>Gilchan</first><last>Park</last></author>
      <author><first>Vivek</first><last>Mutalik</last></author>
      <author><first>Christopher</first><last>Neely</last></author>
      <author><first>Carlos</first><last>Soto</last></author>
      <author><first>Shinjae</first><last>Yoo</last></author>
      <author><first>Paramvir</first><last>Dehal</last></author>
      <pages>370–379</pages>
      <abstract>Building genetic tools to engineer microorganisms is at the core of understanding and redesigning natural biological systems for useful purposes. Every project to build such a genetic toolbox for an organism starts with a survey of available tools. Despite a decade-long investment and advancement in the field, it is still challenging to mine information about a genetic tool published in the literature and connect that information to microbial genomics and other microbial databases. This information gap not only limits our ability to identify and adopt available tools to a new chassis but also conceals available opportunities to engineer a new microbial host. Recent advances in natural language processing (NLP), particularly large language models (LLMs), offer solutions by enabling efficient extraction of genetic terms and biological entities from a vast array of publications. This work present a method to automate this process, using text-mining to refine models with data from bioRxiv and other databases. We evaluated various LLMs to investigate their ability to recognize bacterial host organisms and genetic toolboxes for engineering. We demonstrate our methodology with a web application that integrates a conversational LLM and visualization tool, connecting user inquiries to genetic resources and literature findings, thereby saving researchers time, money and effort in their laboratory work.</abstract>
      <url hash="2575e1b0">2024.bionlp-1.28</url>
      <bibkey>park-etal-2024-leveraging</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.28</doi>
    </paper>
    <paper id="29">
      <title><fixed-case>REAL</fixed-case>: A Retrieval-Augmented Entity Linking Approach for Biomedical Concept Recognition</title>
      <author><first>Darya</first><last>Shlyk</last></author>
      <author><first>Tudor</first><last>Groza</last></author>
      <author><first>Marco</first><last>Mesiti</last></author>
      <author><first>Stefano</first><last>Montanelli</last></author>
      <author><first>Emanuele</first><last>Cavalleri</last></author>
      <pages>380–389</pages>
      <abstract>Large Language Models (LLMs) offer an appealing alternative to training dedicated models for many Natural Language Processing (NLP) tasks. However, outdated knowledge and hallucination issues can be major obstacles in their application in knowledge-intensive biomedical scenarios. In this study, we consider the task of biomedical concept recognition (CR) from unstructured scientific literature and explore the use of Retrieval Augmented Generation (RAG) to improve accuracy and reliability of the LLM-based biomedical CR. Our approach, named REAL (Retrieval Augmented Entity Linking), combines the generative capabilities of LLMs with curated knowledge bases to automatically annotate natural language texts with concepts from bio-ontologies. By applying REAL to benchmark corpora on phenotype concept recognition, we show its effectiveness in improving LLM-based CR performance. This research highlights the potential of combining LLMs with external knowledge sources to advance biomedical text processing.</abstract>
      <url hash="352cf3d5">2024.bionlp-1.29</url>
      <bibkey>shlyk-etal-2024-real</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.29</doi>
    </paper>
    <paper id="30">
      <title>Is That the Right Dose? Investigating Generative Language Model Performance on Veterinary Prescription Text Analysis</title>
      <author><first>Brian</first><last>Hur</last></author>
      <author><first>Lucy Lu</first><last>Wang</last></author>
      <author><first>Laura</first><last>Hardefeldt</last></author>
      <author><first>Meliha</first><last>Yetisgen</last></author>
      <pages>390–397</pages>
      <abstract>Optimizing antibiotic dosing recommendations is a vital aspect of antimicrobial stewardship (AMS) programs aimed at combating antimicrobial resistance (AMR), a significant public health concern, where inappropriate dosing contributes to the selection of AMR pathogens. A key challenge is the extraction of dosing information, which is embedded in free-text clinical records and necessitates numerical transformations. This paper assesses the utility of Large Language Models (LLMs) in extracting essential prescription attributes such as dose, duration, active ingredient, and indication. We evaluate methods to optimize LLMs on this task against a baseline BERT-based ensemble model. Our findings reveal that LLMs can achieve exceptional accuracy by combining probabilistic predictions with deterministic calculations, enforced through functional prompting, to ensure data types and execute necessary arithmetic. This research demonstrates new prospects for automating aspects of AMS when no training data is available.</abstract>
      <url hash="098c59a0">2024.bionlp-1.30</url>
      <bibkey>hur-etal-2024-right</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.30</doi>
    </paper>
    <paper id="31">
      <title><fixed-case>M</fixed-case>i<fixed-case>DRED</fixed-case>: An Annotated Corpus for Microbiome Knowledge Base Construction</title>
      <author><first>William</first><last>Hogan</last></author>
      <author><first>Andrew</first><last>Bartko</last></author>
      <author><first>Jingbo</first><last>Shang</last></author>
      <author><first>Chun-Nan</first><last>Hsu</last></author>
      <pages>398–408</pages>
      <abstract>The interplay between microbiota and diseases has emerged as a significant area of research facilitated by the proliferation of cost-effective and precise sequencing technologies. To keep track of the many findings, domain experts manually review publications to extract reported microbe-disease associations and compile them into knowledge bases. However, manual curation efforts struggle to keep up with the pace of publications. Relation extraction has demonstrated remarkable success in other domains, yet the availability of datasets supporting such methods within the domain of microbiome research remains limited. To bridge this gap, we introduce the Microbe-Disease Relation Extraction Dataset (MiDRED); a human-annotated dataset containing 3,116 annotations of fine-grained relationships between microbes and diseases. We hope this dataset will help address the scarcity of data in this crucial domain and facilitate the development of advanced text-mining solutions to automate the creation and maintenance of microbiome knowledge bases.</abstract>
      <url hash="33100a4b">2024.bionlp-1.31</url>
      <bibkey>hogan-etal-2024-midred</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.31</doi>
    </paper>
    <paper id="32">
      <title>Do Numbers Matter? Types and Prevalence of Numbers in Clinical Texts</title>
      <author><first>Rahmad</first><last>Mahendra</last></author>
      <author><first>Damiano</first><last>Spina</last></author>
      <author><first>Lawrence</first><last>Cavedon</last></author>
      <author><first>Karin</first><last>Verspoor</last></author>
      <pages>409–415</pages>
      <abstract>In this short position paper, we highlight the importance of numbers in clinical text. We first present a taxonomy of number variants. We then perform corpus analysis to analyze characteristics of number use in several clinical corpora. Based on our findings of extensive use of numbers, and limited understanding of the impact of numbers on clinical NLP tasks, we identify the need for a public benchmark that will support investigation of numerical processing tasks for the clinical domain.</abstract>
      <url hash="5566b244">2024.bionlp-1.32</url>
      <bibkey>mahendra-etal-2024-numbers</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.32</doi>
    </paper>
    <paper id="33">
      <title>A Fine-grained citation graph for biomedical academic papers: the finding-citation graph</title>
      <author><first>Yuan</first><last>Liang</last></author>
      <author><first>Massimo</first><last>Poesio</last></author>
      <author><first>Roonak</first><last>Rezvani</last></author>
      <pages>416–426</pages>
      <abstract>Citations typically mention findings as well as papers. To model this richer notion of citation, we introduce a richer form of citation graph with nodes for both academic papers and their findings: the finding-citation graph (FCG). We also present a new pipeline to construct such a graph, which includes a finding identification module and a citation sentence extraction module. From each paper, it extracts rich basic information, abstract, and structured full text first. The abstract and vital sections, such as the results and discussion, are input into the finding identification module. This module identifies multiple findings from a paper, achieving an 80% accuracy in multiple findings evaluation. The full text is input into the citation sentence extraction module to identify inline citation sentences and citation markers, achieving 97.7% accuracy. Then, the graph is constructed using the outputs from the two modules mentioned above. We used the Europe PMC to build such a graph using the pipeline, resulting in a graph with 14.25 million nodes and 76 million edges.</abstract>
      <url hash="2e91c12c">2024.bionlp-1.33</url>
      <bibkey>liang-etal-2024-fine</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.33</doi>
    </paper>
    <paper id="34">
      <title>Evaluating Large Language Models for Predicting Protein Behavior under Radiation Exposure and Disease Conditions</title>
      <author><first>Ryan</first><last>Engel</last></author>
      <author><first>Gilchan</first><last>Park</last></author>
      <pages>427–439</pages>
      <abstract>The primary concern with exposure to ionizing radiation is the risk of developing diseases. While high doses of radiation can cause immediate damage leading to cancer, the effects of low-dose radiation (LDR) are less clear and more controversial. To further investigate this, it necessitates focusing on the underlying biological structures affected by radiation. Recent work has shown that Large Language Models (LLMs) can effectively predict protein structures and other biological properties. The aim of this research is to utilize open-source LLMs, such as Mistral, Llama 2, and Llama 3, to predict both radiation-induced alterations in proteins and the dynamics of protein-protein interactions (PPIs) within the presence of specific diseases. We show that fine-tuning these models yields state-of-the-art performance for predicting protein interactions in the context of neurodegenerative diseases, metabolic disorders, and cancer. Our findings contribute to the ongoing efforts to understand the complex relationships between radiation exposure and disease mechanisms, illustrating the nuanced capabilities and limitations of current computational models.</abstract>
      <url hash="5dcde95d">2024.bionlp-1.34</url>
      <bibkey>engel-park-2024-evaluating</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.34</doi>
    </paper>
    <paper id="35">
      <title><fixed-case>X</fixed-case>ray<fixed-case>GPT</fixed-case>: Chest Radiographs Summarization using Large Medical Vision-Language Models</title>
      <author><first>Omkar Chakradhar</first><last>Thawakar</last></author>
      <author><first>Abdelrahman M.</first><last>Shaker</last></author>
      <author><first>Sahal Shaji</first><last>Mullappilly</last></author>
      <author><first>Hisham</first><last>Cholakkal</last></author>
      <author><first>Rao Muhammad</first><last>Anwer</last></author>
      <author><first>Salman</first><last>Khan</last></author>
      <author><first>Jorma</first><last>Laaksonen</last></author>
      <author><first>Fahad</first><last>Khan</last></author>
      <pages>440–448</pages>
      <abstract>The latest breakthroughs in large language models (LLMs) and vision-language models (VLMs) have showcased promising capabilities toward performing a wide range of tasks. Such models are typically trained on massive datasets comprising billions of image-text pairs with diverse tasks. However, their performance on task-specific domains, such as radiology, is still under-explored. While few works have recently explored LLMs-based conversational medical models, they mainly focus on text-based analysis. In this paper, we introduce XrayGPT, a conversational medical vision-language (VLMs) model that can analyze and answer open-ended questions about chest radiographs. Specifically, we align both medical visual encoder with a fine-tuned LLM to possess visual conversation abilities, grounded in an understanding of radiographs and medical knowledge. For improved alignment of chest radiograph data, we generate ~217k interactive and high-quality summaries from free-text radiology reports. Extensive experiments are conducted to validate the merits of XrayGPT. To conduct an expert evaluation, certified medical doctors evaluated the output of our XrayGPT on a test subset and the results reveal that more than 70% of the responses are scientifically accurate, with an average score of 4/5. We hope our simple and effective method establishes a solid baseline, facilitating future research toward automated analysis and summarization of chest radiographs. Code, models, and instruction sets will be publicly released.</abstract>
      <url hash="8d8f36c0">2024.bionlp-1.35</url>
      <bibkey>thawakar-etal-2024-xraygpt</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.35</doi>
    </paper>
    <paper id="36">
      <title>Multilevel Analysis of Biomedical Domain Adaptation of Llama 2: What Matters the Most? A Case Study</title>
      <author><first>Vicente Ivan</first><last>Sanchez Carmona</last></author>
      <author><first>Shanshan</first><last>Jiang</last></author>
      <author><first>Takeshi</first><last>Suzuki</last></author>
      <author><first>Bin</first><last>Dong</last></author>
      <pages>449–456</pages>
      <abstract>Domain adaptation of Large Language Models (LLMs) leads to models better suited for a particular domain by capturing patterns from domain text which leads to improvements in downstream tasks. To the naked eye, these improvements are visible; however, the patterns are not so. How can we know which patterns and how much they contribute to changes in downstream scores? Through a Multilevel Analysis we discover and quantify the effect of text patterns on downstream scores of domain-adapted Llama 2 for the task of sentence similarity (BIOSSES dataset). We show that text patterns from PubMed abstracts such as clear writing and simplicity, as well as the amount of biomedical information, are the key for improving downstream scores. Also, we show how another factor not usually quantified contributes equally to downstream scores: choice of hyperparameters for both domain adaptation and fine-tuning.</abstract>
      <url hash="dd20b5bd">2024.bionlp-1.36</url>
      <attachment type="OptionalSupplementaryMaterial" hash="bbacf086">2024.bionlp-1.36.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>sanchez-carmona-etal-2024-multilevel-analysis</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.36</doi>
    </paper>
    <paper id="37">
      <title>Mention-Agnostic Information Extraction for Ontological Annotation of Biomedical Articles</title>
      <author><first>Oumaima</first><last>El Khettari</last></author>
      <author><first>Noriki</first><last>Nishida</last></author>
      <author><first>Shanshan</first><last>Liu</last></author>
      <author><first>Rumana Ferdous</first><last>Munne</last></author>
      <author><first>Yuki</first><last>Yamagata</last></author>
      <author><first>Solen</first><last>Quiniou</last></author>
      <author><first>Samuel</first><last>Chaffron</last></author>
      <author><first>Yuji</first><last>Matsumoto</last></author>
      <pages>457–473</pages>
      <abstract>Biomedical information extraction is crucial for advancing research, enhancing healthcare, and discovering treatments by efficiently analyzing extensive data. Given the extensive amount of biomedical data available, automated information extraction methods are necessary due to manual extraction’s labor-intensive, expertise-dependent, and costly nature. In this paper, we propose a novel two-stage system for information extraction where we annotate biomedical articles based on a specific ontology (HOIP). The major challenge is annotating relation between biomedical processes often not explicitly mentioned in text articles. Here, we first predict the candidate processes and then determine the relationships between these processes. The experimental results show promising outcomes in mention-agnostic process identification using Large Language Models (LLMs). In relation classification, BERT-based supervised models still outperform LLMs significantly. The end-to-end evaluation results suggest the difficulty of this task and room for improvement in both process identification and relation classification.</abstract>
      <url hash="5170930e">2024.bionlp-1.37</url>
      <bibkey>el-khettari-etal-2024-mention</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.37</doi>
    </paper>
    <paper id="38">
      <title>Automatic Extraction of Disease Risk Factors from Medical Publications</title>
      <author><first>Maxim</first><last>Rubchinsky</last></author>
      <author><first>Ella</first><last>Rabinovich</last></author>
      <author><first>Adi</first><last>Shribman</last></author>
      <author><first>Netanel</first><last>Golan</last></author>
      <author><first>Tali</first><last>Sahar</last></author>
      <author><first>Dorit</first><last>Shweiki</last></author>
      <pages>474–485</pages>
      <abstract>We present a novel approach to automating the identification of risk factors for diseases from medical literature, leveraging pre-trained models in the bio-medical domain, while tuning them for the specific task. Faced with the challenges of the diverse and unstructured nature of medical articles, our study introduces a multi-step system to first identify relevant articles, then classify them based on the presence of risk factor discussions and, finally, extract specific risk factor information for a disease through a question-answering model. Our contributions include the development of a comprehensive pipeline for the automated extraction of risk factors and the compilation of several datasets, which can serve as valuable resources for further research in this area. These datasets encompass a wide range of diseases, as well as their associated risk factors, meticulously identified and validated through a fine-grained evaluation scheme. We conducted both automatic and thorough manual evaluation, demonstrating encouraging results. We also highlight the importance of improving models and expanding dataset comprehensiveness to keep pace with the rapidly evolving field of medical research.</abstract>
      <url hash="7087d3db">2024.bionlp-1.38</url>
      <bibkey>rubchinsky-etal-2024-automatic</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.38</doi>
    </paper>
    <paper id="39">
      <title>Intervention extraction in preclinical animal studies of <fixed-case>A</fixed-case>lzheimer’s Disease: Enhancing regex performance with language model-based filtering</title>
      <author><first>Yiyuan</first><last>Pu</last></author>
      <author><first>Kaitlyn</first><last>Hair</last></author>
      <author><first>Daniel</first><last>Beck</last></author>
      <author><first>Mike</first><last>Conway</last></author>
      <author><first>Malcolm</first><last>MacLeod</last></author>
      <author><first>Karin</first><last>Verspoor</last></author>
      <pages>486–492</pages>
      <abstract>We explore different information extraction tools for annotation of interventions to support automated systematic reviews of preclinical AD animal studies. We compare two PICO (Population, Intervention, Comparison, and Outcome) extraction tools and two prompting-based learning strategies based on Large Language Models (LLMs). Motivated by the high recall of a dictionary-based approach, we define a two-stage method, removing false positives obtained from regexes with a pre-trained LM. With ChatGPT-based filtering using three-shot prompting, our approach reduces almost two-thirds of False Positives compared to the dictionary approach alone, while outperforming knowledge-free instructional prompting.</abstract>
      <url hash="a0c283fb">2024.bionlp-1.39</url>
      <bibkey>pu-etal-2024-intervention</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.39</doi>
    </paper>
    <paper id="40">
      <title>Efficient Biomedical Entity Linking: Clinical Text Standardization with Low-Resource Techniques</title>
      <author><first>Akshit</first><last>Achara</last></author>
      <author><first>Sanand</first><last>Sasidharan</last></author>
      <author><first>Gagan</first><last>N</last></author>
      <pages>493–505</pages>
      <abstract>Clinical text is rich in information, with mentions of treatment, medication and anatomy among many other clinical terms. Multiple terms can refer to the same core concepts which can be referred as a clinical entity. Ontologies like the Unified Medical Language System (UMLS) are developed and maintained to store millions of clinical entities including the definitions, relations and other corresponding information. These ontologies are used for standardization of clinical text by normalizing varying surface forms of a clinical term through Biomedical entity linking. With the introduction of transformer-based language models, there has been significant progress in Biomedical entity linking. In this work, we focus on learning through synonym pairs associated with the entities. As compared to the existing approaches, our approach significantly reduces the training data and resource consumption. Moreover, we propose a suite of context-based and context-less reranking techniques for performing the entity disambiguation. Overall, we achieve similar performance to the state-of-the-art zero-shot and distant supervised entity linking techniques on the Medmentions dataset, the largest annotated dataset on UMLS, without any domain-based training. Finally, we show that retrieval performance alone might not be sufficient as an evaluation metric and introduce an article level quantitative and qualitative analysis to reveal further insights on the performance of entity linking methods.</abstract>
      <url hash="75fd1a5b">2024.bionlp-1.40</url>
      <bibkey>achara-etal-2024-efficient</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.40</doi>
    </paper>
    <paper id="41">
      <title><fixed-case>XAI</fixed-case> for Better Exploitation of Text in Medical Decision Support</title>
      <author><first>Ajay Madhavan</first><last>Ravichandran</last></author>
      <author><first>Julianna</first><last>Grune</last></author>
      <author><first>Nils</first><last>Feldhus</last></author>
      <author><first>Aljoscha</first><last>Burchardt</last></author>
      <author><first>Roland</first><last>Roller</last></author>
      <author><first>Sebastian</first><last>Möller</last></author>
      <pages>506–513</pages>
      <abstract>In electronic health records, text data is considered a valuable resource as it complements a medical history and may contain information that cannot be easily included in tables. But why does the inclusion of clinical texts as additional input into multimodal models, not always significantly improve the performance of medical decision-support systems? Explainable AI (XAI) might provide the answer. We examine which information in text and structured data influences the performance of models in the context of multimodal decision support for biomedical tasks. Using data from an intensive care unit and targeting a mortality prediction task, we compare information that has been considered relevant by XAI methods to the opinion of a physician.</abstract>
      <url hash="2394fd2b">2024.bionlp-1.41</url>
      <bibkey>ravichandran-etal-2024-xai</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.41</doi>
    </paper>
    <paper id="42">
      <title>Optimizing Multimodal Large Language Models for Detection of Alcohol Advertisements via Adaptive Prompting</title>
      <author><first>Daniel</first><last>Cabrera Lozoya</last></author>
      <author><first>Jiahe</first><last>Liu</last></author>
      <author><first>Simon</first><last>D’Alfonso</last></author>
      <author><first>Mike</first><last>Conway</last></author>
      <pages>514–525</pages>
      <abstract>Adolescents exposed to advertisements promoting addictive substances exhibit a higher likelihood of subsequent substance use. The predominant source for youth exposure to such advertisements is through online content accessed via smartphones. Detecting these advertisements is crucial for establishing and maintaining a safer online environment for young people. In our study, we utilized Multimodal Large Language Models (MLLMs) to identify addictive substance advertisements in digital media. The performance of MLLMs depends on the quality of the prompt used to instruct the model. To optimize our prompts, an adaptive prompt engineering approach was implemented, leveraging a genetic algorithm to refine and enhance the prompts. To evaluate the model’s performance, we augmented the RICO dataset, consisting of Android user interface screenshots, by superimposing alcohol ads onto them. Our results indicate that the MLLM can detect advertisements promoting alcohol with a 0.94 accuracy and a 0.94 F1 score.</abstract>
      <url hash="6ad0ffcf">2024.bionlp-1.42</url>
      <bibkey>cabrera-lozoya-etal-2024-optimizing</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.42</doi>
    </paper>
    <paper id="43">
      <title>Extracting Epilepsy Patient Data with Llama 2</title>
      <author><first>Ben</first><last>Holgate</last></author>
      <author><first>Shichao</first><last>Fang</last></author>
      <author><first>Anthony</first><last>Shek</last></author>
      <author><first>Matthew</first><last>McWilliam</last></author>
      <author><first>Pedro</first><last>Viana</last></author>
      <author><first>Joel S.</first><last>Winston</last></author>
      <author><first>James T.</first><last>Teo</last></author>
      <author><first>Mark P.</first><last>Richardson</last></author>
      <pages>526–535</pages>
      <abstract>We fill a gap in scholarship by applying a generative Large Language Model (LLM) to extract information from clinical free text about the frequency of seizures experienced by people with epilepsy. Seizure frequency is difficult to determine across time from unstructured doctors’ and nurses’ reports of outpatients’ visits that are stored in Electronic Health Records (EHRs) in the United Kingdom’s National Health Service (NHS). We employ Meta’s Llama 2 to mine the EHRs of people with epilepsy and determine, where possible, a person’s seizure frequency at a given point in time. The results demonstrate that the new, powerful generative LLMs may improve outcomes for clinical NLP research in epilepsy and other areas.</abstract>
      <url hash="3c7f4b4d">2024.bionlp-1.43</url>
      <bibkey>holgate-etal-2024-extracting</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.43</doi>
    </paper>
    <paper id="44">
      <title>How do you know that? Teaching Generative Language Models to Reference Answers to Biomedical Questions</title>
      <author><first>Bojana</first><last>Bašaragin</last></author>
      <author><first>Adela</first><last>Ljajić</last></author>
      <author><first>Darija</first><last>Medvecki</last></author>
      <author><first>Lorenzo</first><last>Cassano</last></author>
      <author><first>Miloš</first><last>Košprdić</last></author>
      <author><first>Nikola</first><last>Milošević</last></author>
      <pages>536–547</pages>
      <abstract>Large language models (LLMs) have recently become the leading source of answers for users’ questions online. Despite their ability to offer eloquent answers, their accuracy and reliability can pose a significant challenge. This is especially true for sensitive domains such as biomedicine, where there is a higher need for factually correct answers. This paper introduces a biomedical retrieval-augmented generation (RAG) system designed to enhance the reliability of generated responses. The system is based on a fine-tuned LLM for the referenced question-answering, where retrieved relevant abstracts from PubMed are passed to LLM’s context as input through a prompt. Its output is an answer based on PubMed abstracts, where each statement is referenced accordingly, allowing the users to verify the answer. Our retrieval system achieves an absolute improvement of 23% compared to the PubMed search engine. Based on the manual evaluation on a small sample, our fine-tuned LLM component achieves comparable results to GPT-4 Turbo in referencing relevant abstracts. We make the dataset used to fine-tune the models and the fine-tuned models based on Mistral-7B-instruct-v0.1 and v0.2 publicly available.</abstract>
      <url hash="ea4fd91d">2024.bionlp-1.44</url>
      <bibkey>basaragin-etal-2024-know</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.44</doi>
    </paper>
    <paper id="45">
      <title>Low Resource <fixed-case>ICD</fixed-case> Coding of Hospital Discharge Summaries</title>
      <author><first>Ashton</first><last>Williamson</last></author>
      <author><first>David</first><last>de Hilster</last></author>
      <author><first>Amnon</first><last>Meyers</last></author>
      <author><first>Nina</first><last>Hubig</last></author>
      <author><first>Amy</first><last>Apon</last></author>
      <pages>548–558</pages>
      <abstract>Medical coding is the process by which standardized medical codes are assigned to patient health records. This is a complex and challenging task that typically requires an expert human coder to review health records and assign codes from a classification system based on a standard set of rules. Since health records typically consist of a large proportion of free-text documents, this problem has traditionally been approached as a natural language processing (NLP) task. While machine learning-based methods have seen recent popularity on this task, they tend to struggle with codes that are assigned less frequently, for which little or no training data exists. In this work we utilize the open-source NLP programming language, NLP++, to design and build an automated system to assign International Classification of Diseases (ICD) codes to discharge summaries that functions in the absence of labeled training data. We evaluate our system using the MIMIC-III dataset and find that for codes with little training data, our approach achieves competitive performance compared to state-of-the-art machine learning approaches.</abstract>
      <url hash="668342c9">2024.bionlp-1.45</url>
      <bibkey>williamson-etal-2024-low</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.45</doi>
    </paper>
    <paper id="46">
      <title>Towards <fixed-case>ML</fixed-case>-supported Triage Prediction in Real-World Emergency Room Scenarios</title>
      <author><first>Faraz</first><last>Maschhur</last></author>
      <author><first>Klaus</first><last>Netter</last></author>
      <author><first>Sven</first><last>Schmeier</last></author>
      <author><first>Katrin</first><last>Ostermann</last></author>
      <author><first>Rimantas</first><last>Palunis</last></author>
      <author><first>Tobias</first><last>Strapatsas</last></author>
      <author><first>Roland</first><last>Roller</last></author>
      <pages>559–569</pages>
      <abstract>In emergency wards, patients are prioritized by clinical staff according to the urgency of their medical condition. This can be achieved by categorizing patients into different labels of urgency ranging from immediate to not urgent. However, in order to train machine learning models offering support in this regard, there is more than approaching this as a multi-class problem. This work explores the challenges and obstacles of automatic triage using anonymized real-world multi-modal ambulance data in Germany.</abstract>
      <url hash="2621d92a">2024.bionlp-1.46</url>
      <bibkey>maschhur-etal-2024-towards</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.46</doi>
    </paper>
    <paper id="47">
      <title>Creating Ontology-annotated Corpora from <fixed-case>W</fixed-case>ikipedia for Medical Named-entity Recognition</title>
      <author><first>Johann</first><last>Frei</last></author>
      <author><first>Frank</first><last>Kramer</last></author>
      <pages>570–579</pages>
      <abstract>Acquiring annotated corpora for medical NLP is challenging due to legal and privacy constraints and costly annotation efforts, and using annotated public datasets may do not align well to the desired target application in terms of annotation style or language. We investigate the approach of utilizing Wikipedia and WikiData jointly to acquire an unsupervised annotated corpus for named-entity recognition (NER). By controlling the annotation ruleset through WikiData’s ontology, we extract custom-defined annotations and dynamically impute weak annotations by an adaptive loss scaling. Our validation on German medication detection datasets yields competitive results. The entire pipeline only relies on open models and data resources, enabling reproducibility and open sharing of models and corpora. All relevant assets are shared on GitHub.</abstract>
      <url hash="0c9df9d1">2024.bionlp-1.47</url>
      <bibkey>frei-kramer-2024-creating</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.47</doi>
    </paper>
    <paper id="48">
      <title>Paragraph Retrieval for Enhanced Question Answering in Clinical Documents</title>
      <author><first>Vojtech</first><last>Lanz</last></author>
      <author><first>Pavel</first><last>Pecina</last></author>
      <pages>580–590</pages>
      <abstract>Healthcare professionals often manually extract information from large clinical documents to address patient-related questions. The use of Natural Language Processing (NLP) techniques, particularly Question Answering (QA) models, is a promising direction for improving the efficiency of this process. However, document-level QA from large documents is often impractical or even infeasible (for model training and inference). In this work, we solve the document-level QA from clinical reports in a two-step approach: first, the entire report is split into segments and for a given question the most relevant segment is predicted by a NLP model; second, a QA model is applied to the question and the retrieved segment as context. We investigate the effectiveness of heading-based and naive paragraph segmentation approaches for various paragraph lengths on two subsets of the emrQA dataset. Our experiments reveal that an average paragraph length used as a parameter for the segmentation has no significant effect on performance during the whole document-level QA process. That means experiments focusing on segmentation into shorter paragraphs perform similarly to those focusing on entire unsegmented reports. Surprisingly, naive uniform segmentation is sufficient even though it is not based on prior knowledge of the clinical document’s characteristics.</abstract>
      <url hash="de97e7c7">2024.bionlp-1.48</url>
      <bibkey>lanz-pecina-2024-paragraph</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.48</doi>
    </paper>
    <paper id="49">
      <title><fixed-case>CID</fixed-case> at <fixed-case>RRG</fixed-case>24: Attempting in a Conditionally Initiated Decoding of Radiology Report Generation with Clinical Entities</title>
      <author><first>Yuxiang</first><last>Liao</last></author>
      <author><first>Yuanbang</first><last>Liang</last></author>
      <author><first>Yipeng</first><last>Qin</last></author>
      <author><first>Hantao</first><last>Liu</last></author>
      <author><first>Irena</first><last>Spasic</last></author>
      <pages>591–596</pages>
      <abstract>Radiology Report Generation (RRG) seeks to leverage deep learning techniques to automate the reporting process of radiologists. Current methods are typically modelling RRG as an image-to-text generation task that takes X-ray images as input and generates textual reports describing the corresponding clinical observations. However, the wording of the same clinical observation could have been influenced by the expression preference of radiologists. Nevertheless, such variability can be mitigated by normalizing textual reports into structured representations such as a graph structure. In this study, we attempt a novel paradigm for incorporating graph structural data into the RRG model. Our approach involves predicting graph labels based on visual features and subsequently initiating the decoding process through a template injection conditioned on the predicted labels. We trained and evaluated our model on the BioNLP 2024 Shared Task on Large-Scale Radiology Report Generation and submitted our results to the ViLMedic RRG leaderboard. Although our model showed a moderate ranking on the leaderboard, the results provide preliminary evidence for the feasibility of this new paradigm, warranting further exploration and refinement.</abstract>
      <url hash="d321ad6f">2024.bionlp-1.49</url>
      <bibkey>liao-etal-2024-cid</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.49</doi>
    </paper>
    <paper id="50">
      <title><fixed-case>MAIRA</fixed-case> at <fixed-case>RRG</fixed-case>24: A specialised large multimodal model for radiology report generation</title>
      <author><first>Shaury</first><last>Srivastav</last></author>
      <author><first>Mercy</first><last>Ranjit</last></author>
      <author><first>Fernando</first><last>Pérez-García</last></author>
      <author><first>Kenza</first><last>Bouzid</last></author>
      <author><first>Shruthi</first><last>Bannur</last></author>
      <author><first>Daniel C.</first><last>Castro</last></author>
      <author><first>Anton</first><last>Schwaighofer</last></author>
      <author><first>Harshita</first><last>Sharma</last></author>
      <author><first>Maximilian</first><last>Ilse</last></author>
      <author><first>Valentina</first><last>Salvatelli</last></author>
      <author><first>Sam</first><last>Bond-Taylor</last></author>
      <author><first>Fabian</first><last>Falck</last></author>
      <author><first>Anja</first><last>Thieme</last></author>
      <author><first>Hannah</first><last>Richardson</last></author>
      <author><first>Matthew P.</first><last>Lungren</last></author>
      <author><first>Stephanie L.</first><last>Hyland</last></author>
      <author><first>Javier</first><last>Alvarez-Valle</last></author>
      <pages>597–602</pages>
      <abstract>This paper discusses the participation of the MSR MAIRA team in the Large-Scale Radiology Report Generation Shared Task Challenge, as part of the BioNLP workshop at ACL 2024. We present a radiology-specific multimodal model designed to generate radiological reports from chest X-Rays (CXRs). Our proposed model combines a CXR-specific image encoder RAD-DINO with a Large Language Model (LLM) based on Vicuna-7B, via a multi-layer perceptron (MLP) adapter. Both the adapter and the LLM have been fine-tuned in a single-stage training setup to generate radiology reports. Experimental results indicate that a joint training setup with findings and impression sections improves findings prediction. Additionally, incorporating lateral images alongside frontal images when available further enhances all metrics. More information and resources about MAIRA can be found on the project website: http://aka.ms/maira.</abstract>
      <url hash="393fe6a5">2024.bionlp-1.50</url>
      <bibkey>srivastav-etal-2024-maira</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.50</doi>
    </paper>
    <paper id="51">
      <title><fixed-case>AIRI</fixed-case> at <fixed-case>RRG</fixed-case>24: <fixed-case>LL</fixed-case>a<fixed-case>V</fixed-case>a with specialised encoder and decoder</title>
      <author><first>Marina</first><last>Munkhoeva</last></author>
      <author><first>Dmitry</first><last>Umerenkov</last></author>
      <author><first>Valentin</first><last>Samokhin</last></author>
      <pages>603–607</pages>
      <abstract>We present a new approach to generating the ‘Findings’ and ‘Impression’ sections in the chest X-rays radiology reports, developed as part of the shared radiology task at BioNLP 2024. By integrating a DINOv2 vision encoder trained on medical data with specialized biomedical large language model using the LLaVA framework, our method addresses complex medical semantics and diverse findings in imaging. We use datasets from PadChest, BIMCV-COVID19, CheXpert, OpenI, and MIMIC-CXR. The evaluation metrics demonstrate our method’s effectiveness and the potential for automating the generation of radiology reports.</abstract>
      <url hash="9c060e4c">2024.bionlp-1.51</url>
      <bibkey>munkhoeva-etal-2024-airi</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.51</doi>
    </paper>
    <paper id="52">
      <title>i<fixed-case>H</fixed-case>ealth-<fixed-case>C</fixed-case>hile-1 at <fixed-case>RRG</fixed-case>24: In-context Learning and Finetuning of a Large Multimodal Model for Radiology Report Generation</title>
      <author><first>Diego</first><last>Campanini</last></author>
      <author><first>Oscar</first><last>Loch</last></author>
      <author><first>Pablo</first><last>Messina</last></author>
      <author><first>Rafael</first><last>Elberg</last></author>
      <author><first>Denis</first><last>Parra</last></author>
      <pages>608–613</pages>
      <abstract>This paper presents the approach of the iHealth-Chile-1 team for the shared task of Large-Scale Radiology Report Generation at the BioNLP workshop, inspired by progress in large multimodal models for processing images and text. In this work, we leverage LLaVA, a Visual-Language Model (VLM), composed of a vision-encoder, a vision-language connector or adapter, and a large language model able to process text and visual embeddings. We achieve our best result by enriching the input prompt of LLaVA with the text output of a simpler report generation model. With this enriched-prompt technique, we improve our results in 4 of 5 metrics (BLEU-4, Rouge-L, BertScore and F1-RadGraph,), only doing in-context learning. Moreover, we provide details about different architecture settings, fine-tuning strategies, and dataset configurations.</abstract>
      <url hash="5272c5bc">2024.bionlp-1.52</url>
      <bibkey>campanini-etal-2024-ihealth</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.52</doi>
    </paper>
    <paper id="53">
      <title>i<fixed-case>H</fixed-case>ealth-<fixed-case>C</fixed-case>hile-3&amp;2 at <fixed-case>RRG</fixed-case>24: Template Based Report Generation</title>
      <author><first>Oscar</first><last>Loch</last></author>
      <author><first>Pablo</first><last>Messina</last></author>
      <author><first>Rafael</first><last>Elberg</last></author>
      <author><first>Diego</first><last>Campanini</last></author>
      <author><first>Álvaro</first><last>Soto</last></author>
      <author><first>René</first><last>Vidal</last></author>
      <author><first>Denis</first><last>Parra</last></author>
      <pages>614–623</pages>
      <abstract>This paper presents the approaches of the iHealth-Chile-3 and iHealth-Chile-2 teams for the shared task of Large-Scale Radiology Report Generation at the BioNLP workshop. Inspired by prior work on template-based report generation, both teams focused on exploring various template-based strategies, using predictions from multi-label image classifiers as input. Our best approach achieved a modest F1-RadGraph score of 19.42 on the findings hidden test set, ranking 7th on the leaderboard. Notably, we consistently observed a discrepancy between our classification metrics and the F1-CheXbert metric reported on the leaderboard, which always showed lower scores. This suggests that the F1-CheXbert metric may be missing some of the labels mentioned by the templates.</abstract>
      <url hash="e0ef82ad">2024.bionlp-1.53</url>
      <bibkey>loch-etal-2024-ihealth</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.53</doi>
    </paper>
    <paper id="54">
      <title>Gla-<fixed-case>AI</fixed-case>4<fixed-case>B</fixed-case>io<fixed-case>M</fixed-case>ed at <fixed-case>RRG</fixed-case>24: Visual Instruction-tuned Adaptation for Radiology Report Generation</title>
      <author><first>Xi</first><last>Zhang</last></author>
      <author><first>Zaiqiao</first><last>Meng</last></author>
      <author><first>Jake</first><last>Lever</last></author>
      <author><first>Edmond S.L.</first><last>Ho</last></author>
      <pages>624–634</pages>
      <abstract>This paper introduces a radiology-focused visual language model designed to generate radiology reports from chest X-rays. Building on previous findings that large language models can acquire multimodal capabilities when aligned with pretrained vision encoders, we demonstrate similar potential with chest X-ray images. The model combines an image encoder (CLIP) with a fine-tuned large language model (LLM) based on the Vicuna-7B architecture. The training process involves a two-stage approach: initial alignment of chest X-ray features with the LLM, followed by fine-tuning for radiology report generation. The study highlights the importance of generating both FINDINGS and IMPRESSIONS sections in radiology reports and evaluates the model’s performance using various metrics, achieving notable accuracy in generating high-quality medical reports. The research also addresses the need for domain-specific fine-tuning to capture the intricate details necessary for accurate medical interpretations and reports.</abstract>
      <url hash="8404d20a">2024.bionlp-1.54</url>
      <bibkey>zhang-etal-2024-gla</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.54</doi>
    </paper>
    <paper id="55">
      <title><fixed-case>SICAR</fixed-case> at <fixed-case>RRG</fixed-case>2024: <fixed-case>GPU</fixed-case> Poor’s Guide to Radiology Report Generation</title>
      <author><first>Kiartnarin</first><last>Udomlapsakul</last></author>
      <author><first>Parinthapat</first><last>Pengpun</last></author>
      <author><first>Tossaporn</first><last>Saengja</last></author>
      <author><first>Kanyakorn</first><last>Veerakanjana</last></author>
      <author><first>Krittamate</first><last>Tiankanon</last></author>
      <author><first>Pitikorn</first><last>Khlaisamniang</last></author>
      <author><first>Pasit</first><last>Supholkhan</last></author>
      <author><first>Amrest</first><last>Chinkamol</last></author>
      <author><first>Pubordee</first><last>Aussavavirojekul</last></author>
      <author><first>Hirunkul</first><last>Phimsiri</last></author>
      <author><first>Tara</first><last>Sripo</last></author>
      <author><first>Chiraphat</first><last>Boonnag</last></author>
      <author><first>Trongtum</first><last>Tongdee</last></author>
      <author><first>Thanongchai</first><last>Siriapisith</last></author>
      <author><first>Pairash</first><last>Saiviroonporn</last></author>
      <author><first>Jiramet</first><last>Kinchagawat</last></author>
      <author><first>Piyalitt</first><last>Ittichaiwong</last></author>
      <pages>635–644</pages>
      <abstract>Radiology report generation (RRG) aims to create free-text radiology reports from clinical imaging. Our solution employs a lightweight multimodal language model (MLLM) enhanced with a two-stage post-processing strategy, utilizing a Large Language Model (LLM) to boost diagnostic accuracy and ensure patient safety. We introduce the “First, Do No Harm” SafetyNet, which incorporates Xraydar, an advanced X-ray classification model, to cross-verify the model outputs and specifically address false negatives from the MLLM. This comprehensive approach combines the efficiency of lightweight models with the robustness of thorough post-processing techniques, offering a reliable solution for radiology report generation. Our system achieved fourth place on the F1-Radgraph metric for findings generation in the Radiology Report Generation Shared Task (RRG24).</abstract>
      <url hash="50963690">2024.bionlp-1.55</url>
      <bibkey>udomlapsakul-etal-2024-sicar</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.55</doi>
    </paper>
    <paper id="56">
      <title>Shimo Lab at “Discharge Me!”: Discharge Summarization by Prompt-Driven Concatenation of Electronic Health Record Sections</title>
      <author><first>Yunzhen</first><last>He</last></author>
      <author><first>Hiroaki</first><last>Yamagiwa</last></author>
      <author><first>Hidetoshi</first><last>Shimodaira</last></author>
      <pages>645–657</pages>
      <abstract>In this paper, we present our approach to the shared task “Discharge Me!” at the BioNLP Workshop 2024. The primary goal of this task is to reduce the time and effort clinicians spend on writing detailed notes in the electronic health record (EHR). Participants develop a pipeline to generate the “Brief Hospital Course” and “Discharge Instructions” sections from the EHR. Our approach involves a first step of extracting the relevant sections from the EHR. We then add explanatory prompts to these sections and concatenate them with separate tokens to create the input text. To train a text generation model, we perform LoRA fine-tuning on the ClinicalT5-large model. On the final test data, our approach achieved a ROUGE-1 of 0.394, which is comparable to the top solutions.</abstract>
      <url hash="1c4c85b6">2024.bionlp-1.56</url>
      <bibkey>he-etal-2024-shimo</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.56</doi>
    </paper>
    <paper id="57">
      <title>Ixa-<fixed-case>M</fixed-case>ed at Discharge Me! Retrieval-Assisted Generation for Streamlining Discharge Documentation</title>
      <author><first>Jordan C.</first><last>Koontz</last></author>
      <author><first>Maite</first><last>Oronoz</last></author>
      <author><first>Alicia</first><last>Pérez</last></author>
      <pages>658–663</pages>
      <abstract>In this paper we present our system for the BioNLP ACL’24 “Discharge Me!” task on automating discharge summary section generation. Using Retrieval-Augmented Generation, we combine a Large Language Model (LLM) with external knowledge to guide the generation of the target sections. Our approach generates structured patient summaries from discharge notes using an instructed LLM, retrieves relevant “Brief Hospital Course” and “Discharge Instructions” examples via BM25 and SentenceBERT, and provides this context to a frozen LLM for generation. Our top system using SentenceBERT retrieval achieves an overall score of 0.183, outperforming zero-shot baselines. We analyze performance across different aspects, discussing limitations and future research directions.</abstract>
      <url hash="97fc3a99">2024.bionlp-1.57</url>
      <bibkey>koontz-etal-2024-ixa</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.57</doi>
    </paper>
    <paper id="58">
      <title><fixed-case>QUB</fixed-case>-Cirdan at “Discharge Me!”: Zero shot discharge letter generation by open-source <fixed-case>LLM</fixed-case></title>
      <author><first>Rui</first><last>Guo</last></author>
      <author><first>Greg</first><last>Farnan</last></author>
      <author><first>Niall</first><last>McLaughlin</last></author>
      <author><first>Barry</first><last>Devereux</last></author>
      <pages>664–674</pages>
      <abstract>The BioNLP ACL’24 Shared Task on Streamlining Discharge Documentation aims to reduce the administrative burden on clinicians by automating the creation of critical sections of patient discharge letters. This paper presents our approach using the Llama3 8B quantized model to generate the “Brief Hospital Course” and “Discharge Instructions” sections. We employ a zero-shot method combined with Retrieval-Augmented Generation (RAG) to produce concise, contextually accurate summaries. Our contributions include the development of a curated template-based approach to ensure reliability and consistency, as well as the integration of RAG for word count prediction. We also describe several unsuccessful experiments to provide insights into our pathway for the competition. Our results demonstrate the effectiveness and efficiency of our approach, achieving high scores across multiple evaluation metrics.</abstract>
      <url hash="f9d57e52">2024.bionlp-1.58</url>
      <bibkey>guo-etal-2024-qub</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.58</doi>
    </paper>
    <paper id="59">
      <title>e-Health <fixed-case>CSIRO</fixed-case> at “Discharge Me!” 2024: Generating Discharge Summary Sections with Fine-tuned Language Models</title>
      <author><first>Jinghui</first><last>Liu</last></author>
      <author><first>Aaron</first><last>Nicolson</last></author>
      <author><first>Jason</first><last>Dowling</last></author>
      <author><first>Bevan</first><last>Koopman</last></author>
      <author><first>Anthony</first><last>Nguyen</last></author>
      <pages>675–684</pages>
      <abstract>Clinical documentation is an important aspect of clinicians’ daily work and often demands a significant amount of time. The BioNLP 2024 Shared Task on Streamlining Discharge Documentation (Discharge Me!) aims to alleviate this documentation burden by automatically generating discharge summary sections, including brief hospital course and discharge instruction, which are often time-consuming to synthesize and write manually. We approach the generation task by fine-tuning multiple open-sourced language models (LMs), including both decoder-only and encoder-decoder LMs, with various configurations on input context. We also examine different setups for decoding algorithms, model ensembling or merging, and model specialization. Our results show that conditioning on the content of discharge summary prior to the target sections is effective for the generation task. Furthermore, we find that smaller encoder-decoder LMs can work as well or even slightly better than larger decoder-based LMs fine-tuned through LoRA. The model checkpoints from our team (aehrc) are openly available.</abstract>
      <url hash="515d7759">2024.bionlp-1.59</url>
      <bibkey>liu-etal-2024-e</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.59</doi>
    </paper>
    <paper id="60">
      <title><fixed-case>UF</fixed-case>-<fixed-case>HOBI</fixed-case> at “Discharge Me!”: A Hybrid Solution for Discharge Summary Generation Through Prompt-based Tuning of <fixed-case>G</fixed-case>ator<fixed-case>T</fixed-case>ron<fixed-case>GPT</fixed-case> Models</title>
      <author><first>Mengxian</first><last>Lyu</last></author>
      <author><first>Cheng</first><last>Peng</last></author>
      <author><first>Daniel</first><last>Paredes</last></author>
      <author><first>Ziyi</first><last>Chen</last></author>
      <author><first>Aokun</first><last>Chen</last></author>
      <author><first>Jiang</first><last>Bian</last></author>
      <author><first>Yonghui</first><last>Wu</last></author>
      <pages>685–695</pages>
      <abstract>Automatic generation of discharge summaries presents significant challenges due to the length of clinical documentation, the dispersed nature of patient information, and the diverse terminology used in healthcare. This paper presents a hybrid solution for generating discharge summary sections as part of our participation in the “Discharge Me!” Challenge at the BioNLP 2024 Shared Task. We developed a two-stage generation method using both extractive and abstractive techniques, in which we first apply name entity recognition (NER) to extract key clinical concepts, which are then used as input for a prompt-tuning based GatorTronGPT model to generate coherent text for two important sections including “Brief Hospital Course” and “Discharge Instructions”. Our system was ranked 5th in this challenge, achieving an overall score of 0.284. The results demonstrate the effectiveness of our hybrid solution in improving the quality of automated discharge section generation.</abstract>
      <url hash="7443c9ad">2024.bionlp-1.60</url>
      <bibkey>lyu-etal-2024-uf</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.60</doi>
    </paper>
    <paper id="61">
      <title><fixed-case>EPFL</fixed-case>-<fixed-case>MAKE</fixed-case> at “Discharge Me!”: An <fixed-case>LLM</fixed-case> System for Automatically Generating Discharge Summaries of Clinical Electronic Health Record</title>
      <author><first>Haotian</first><last>Wu</last></author>
      <author><first>Paul</first><last>Boulenger</last></author>
      <author><first>Antonin</first><last>Faure</last></author>
      <author><first>Berta</first><last>Céspedes</last></author>
      <author><first>Farouk</first><last>Boukil</last></author>
      <author><first>Nastasia</first><last>Morel</last></author>
      <author><first>Zeming</first><last>Chen</last></author>
      <author><first>Antoine</first><last>Bosselut</last></author>
      <pages>696–711</pages>
      <abstract>This paper presents our contribution to the Streamlining Discharge Documentation shared task organized as part of the ACL’24 workshop. We propose MEDISCHARGE (Meditron-7B Based Medical Summary Generation System for Discharge Me), an LLM-based system to generate Brief Hospital Course and Discharge Instruction summaries based on a patient’s Electronic Health Record. Our system is build on a Meditron-7B with context window extension, ensuring the system can handle cases of variable lengths with high quality. When the length of the input exceeds the system input limitation, we use a dynamic information selection framework to automatically extract important sections from the full discharge text. Then, extracted sections are removed in increasing order of importance until the input length requirement is met. We demonstrate our approach outperforms tripling the size of the context window of the model. Our system obtains a 0.289 overall score in the leaderboard, an improvement of 183% compared to the baseline, and a ROUGE-1 score of 0.444, achieving a second place performance in the shared task.</abstract>
      <url hash="40d772ef">2024.bionlp-1.61</url>
      <bibkey>wu-etal-2024-epfl</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.61</doi>
    </paper>
    <paper id="62">
      <title><fixed-case>U</fixed-case>o<fixed-case>G</fixed-case> Siephers at “Discharge Me!”: Exploring Ways to Generate Synthetic Patient Notes From Multi-Part Electronic Health Records</title>
      <author><first>Erlend</first><last>Frayling</last></author>
      <author><first>Jake</first><last>Lever</last></author>
      <author><first>Graham</first><last>McDonald</last></author>
      <pages>712–718</pages>
      <abstract>This paper presents the UoG Siephers team participation at the Discharge Me! Shared Task on Streamlining Discharge Documentation. For our participation, we investigate appropriately selecting and encoding specific sections of Electronic Health Records (EHR) as input data for sequence-to-sequence models, to generate the discharge instructions and brief hospital course sections of a patient’s EHR. We found that, despite the large volume of disparate information that is often available in EHRs, selectively choosing an appropriate EHR section for training and prompting sequence-to-sequence models resulted in improved generative quality. In particular, we found that using only the history of present illness section of an EHR as input often led to better performance than using multiple EHR sections.</abstract>
      <url hash="3f90160e">2024.bionlp-1.62</url>
      <bibkey>frayling-etal-2024-uog</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.62</doi>
    </paper>
    <paper id="63">
      <title>Roux-lette at “Discharge Me!”: Reducing <fixed-case>EHR</fixed-case> Chart Burden with a Simple, Scalable, Clinician-Driven <fixed-case>AI</fixed-case> Approach</title>
      <author><first>Suzanne</first><last>Wendelken</last></author>
      <author><first>Anson</first><last>Antony</last></author>
      <author><first>Rajashekar</first><last>Korutla</last></author>
      <author><first>Bhanu</first><last>Pachipala</last></author>
      <author><first>Dushyant</first><last>Mahajan</last></author>
      <author><first>James</first><last>Shanahan</last></author>
      <author><first>Walid</first><last>Saba</last></author>
      <pages>719–723</pages>
      <abstract>Healthcare providers spend a significant amount of time reading and synthesizing electronic health records (EHRs), negatively impacting patient outcomes and causing provider burnout. Traditional supervised machine learning approaches using large language models (LLMs) to summarize clinical text have struggled due to hallucinations and lack of relevant training data. Here, we present a novel, simplified solution for the “Discharge Me!” shared task. Our approach mimics human clinical workflow, using pre-trained LLMs to answer specific questions and summarize the answers obtained from discharge summaries and other EHR sections. This method (i) avoids hallucinations through hybrid-RAG/zero-shot contextualized prompting; (ii) requires no extensive training or fine-tuning; and (iii) is adaptable to various clinical tasks.</abstract>
      <url hash="59d73804">2024.bionlp-1.63</url>
      <bibkey>wendelken-etal-2024-roux</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.63</doi>
    </paper>
    <paper id="64">
      <title><fixed-case>Y</fixed-case>ale at “Discharge Me!”: Evaluating Constrained Generation of Discharge Summaries with Unstructured and Structured Information</title>
      <author><first>Vimig</first><last>Socrates</last></author>
      <author><first>Thomas</first><last>Huang</last></author>
      <author><first>Xuguang</first><last>Ai</last></author>
      <author><first>Soraya</first><last>Fereydooni</last></author>
      <author><first>Qingyu</first><last>Chen</last></author>
      <author><first>R Andrew</first><last>Taylor</last></author>
      <author><first>David</first><last>Chartash</last></author>
      <pages>724–730</pages>
      <abstract>In this work, we propose our top-ranking (2nd place) pipeline for the generation of discharge summary subsections as a part of the BioNLP 2024 Shared Task 2: “Discharge Me!”. We evaluate both encoder-decoder and state-of-the-art decoder-only language models on the generation of two key sections of the discharge summary. To evaluate the ability of NLP methods to further alleviate the documentation burden on physicians, we also design a novel pipeline to generate the brief hospital course directly from structured information found in the EHR. Finally, we evaluate a constrained beam search approach to inject external knowledge about relevant patient problems into the text generation process. We find that a BioBART model fine-tuned on a larger fraction of the data without constrained beam search outperforms all other models.</abstract>
      <url hash="394a282d">2024.bionlp-1.64</url>
      <bibkey>socrates-etal-2024-yale</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.64</doi>
    </paper>
    <paper id="65">
      <title><fixed-case>I</fixed-case>gnition<fixed-case>I</fixed-case>nnovators at “Discharge Me!”: Chain-of-Thought Instruction Finetuning Large Language Models for Discharge Summaries</title>
      <author><first>An Quang</first><last>Tang</last></author>
      <author><first>Xiuzhen</first><last>Zhang</last></author>
      <author><first>Minh Ngoc</first><last>Dinh</last></author>
      <pages>731–739</pages>
      <abstract>This paper presents our proposed approach to the Discharge Me! shared task, collocated with the 23th Workshop on Biomedical Natural Language Processing (BioNLP). In this work, we develop an LLM-based framework for solving the Discharge Summary Documentation (DSD) task, i.e., generating the two critical target sections ‘Brief Hospital Course’ and ‘Discharge Instructions’ in the discharge summary. By streamlining the recent instruction-finetuning process on LLMs, we explore several prompting strategies for optimally adapting LLMs to specific generation task of DSD. Experimental results show that providing a clear output structure, complimented by a set of comprehensive Chain-of-Thoughts (CoT) questions, effectively improves the model’s reasoning capability, and thereby, enhancing the structural correctness and faithfulness of clinical information in the generated text. Source code is available at: https://anonymous.4open.science/r/Discharge_LLM-A233</abstract>
      <url hash="608326a9">2024.bionlp-1.65</url>
      <bibkey>tang-etal-2024-ignitioninnovators</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.65</doi>
    </paper>
    <paper id="66">
      <title><fixed-case>MLBMIKABR</fixed-case> at “Discharge Me!”: Concept Based Clinical Text Description Generation</title>
      <author><first>Abir</first><last>Naskar</last></author>
      <author><first>Jane</first><last>Hocking</last></author>
      <author><first>Patty</first><last>Chondros</last></author>
      <author><first>Douglas</first><last>Boyle</last></author>
      <author><first>Mike</first><last>Conway</last></author>
      <pages>740–747</pages>
      <abstract>This paper presents a method called Concept Based Description Generation, aimed at creating summaries (Brief Hospital Course and Discharge Instructions) using source (Discharge and Radiology) texts. We propose a rule-based approach for segmenting both the source and target texts. In the target text, we not only segment the content but also identify the concept of each segment based on text patterns. Our methodology involves creating a combined summarized version of each text segment, extracting important information, and then fine-tuning a Large Language Model (LLM) to generate aspects. Subsequently, we fine-tune a new LLM using a specific aspect, the combined summary, and a list of all aspects to generate detailed descriptions for each task. This approach integrates segmentation, concept identification, summarization, and language modeling to achieve accurate and informative descriptions for medical documentation tasks. Due to lack to time, We could only train on 10000 training data.</abstract>
      <url hash="1dfec6b1">2024.bionlp-1.66</url>
      <bibkey>naskar-etal-2024-mlbmikabr</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.66</doi>
    </paper>
    <paper id="67">
      <title><fixed-case>D</fixed-case>eakin<fixed-case>NLP</fixed-case> at <fixed-case>B</fixed-case>io<fixed-case>L</fixed-case>ay<fixed-case>S</fixed-case>umm: Evaluating Fine-tuning Longformer and <fixed-case>GPT</fixed-case>-4 Prompting for Biomedical Lay Summarization</title>
      <author><first>Huy Quoc</first><last>To</last></author>
      <author><first>Ming</first><last>Liu</last></author>
      <author><first>Guangyan</first><last>Huang</last></author>
      <pages>748–754</pages>
      <abstract>This paper presents our approaches for the BioLaySumm 2024 Shared Task. We evaluate two methods for generating lay summaries based on biomedical articles: (1) fine-tuning the Longformer-Encoder-Decoder (LED) model, and (2) zero-shot and few-shot prompting on GPT-4. In the fine-tuning approach, we individually fine-tune the LED model using two datasets: PLOS and eLife. This process is conducted under two different settings: one utilizing 50% of the training dataset, and the other utilizing the entire 100% of the training dataset. We compare the results of both methods with GPT-4 in zero-shot and few-shot prompting. The experiment results demonstrate that fine-tuning with 100% of the training data achieves better performance than prompting with GPT-4. However, under data scarcity circumstances, prompting GPT-4 seems to be a better solution.</abstract>
      <url hash="273f67e4">2024.bionlp-1.67</url>
      <bibkey>to-etal-2024-deakinnlp</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.67</doi>
    </paper>
    <paper id="68">
      <title><fixed-case>EL</fixed-case>i<fixed-case>RF</fixed-case>-<fixed-case>VRAIN</fixed-case> at <fixed-case>B</fixed-case>io<fixed-case>L</fixed-case>ay<fixed-case>S</fixed-case>umm: Boosting Lay Summarization Systems Performance with Ranking Models</title>
      <author><first>Vicent</first><last>Ahuir</last></author>
      <author><first>Diego</first><last>Torres</last></author>
      <author><first>Encarna</first><last>Segarra</last></author>
      <author><first>Lluís-F.</first><last>Hurtado</last></author>
      <pages>755–761</pages>
      <abstract>This paper presents our contribution to the BioLaySumm 2024 shared task of the 23rd BioNLP Workshop. The task is to create a lay summary, given a biomedical research article and its technical summary. As the input to the system could be large, a Longformer Encoder-Decoder (LED) has been used. We continuously pre-trained a general domain LED model with biomedical data to adapt it to this specific domain. In the pre-training phase, several pre-training tasks were aggregated to inject linguistic knowledge and increase the abstractivity of the generated summaries. Since the distribution of samples between the two datasets, eLife and PLOS, is unbalanced, we fine-tuned two models: one for eLife and another for PLOS. To increase the quality of the lay summaries of the system, we developed a regression model that helps us rank the summaries generated by the summarization models. This regression model predicts the quality of the summary in three different aspects: Relevance, Readability, and Factuality. We present the results of our models and a study to measure the ranking capabilities of the regression model.</abstract>
      <url hash="def5fd29">2024.bionlp-1.68</url>
      <bibkey>ahuir-etal-2024-elirf</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.68</doi>
    </paper>
    <paper id="69">
      <title><fixed-case>B</fixed-case>io<fixed-case>L</fixed-case>ay_<fixed-case>AK</fixed-case>_<fixed-case>SS</fixed-case> at <fixed-case>B</fixed-case>io<fixed-case>L</fixed-case>ay<fixed-case>S</fixed-case>umm: Domain Adaptation by Two-Stage Fine-Tuning of Large Language Models used for Biomedical Lay Summary Generation</title>
      <author><first>Akanksha</first><last>Karotia</last></author>
      <author><first>Seba</first><last>Susan</last></author>
      <pages>762–768</pages>
      <abstract>Lay summarization is essential but challenging, as it simplifies scientific information for non-experts and keeps them updated with the latest scientific knowledge. In our participation in the Shared Task: Lay Summarization of Biomedical Research Articles @ BioNLP Workshop (Goldsack et al., 2024), ACL 2024, we conducted a comprehensive evaluation on abstractive summarization of biomedical literature using Large Language Models (LLMs) and assessed the performance using ten metrics across three categories: relevance, readability, and factuality, using eLife and PLOS datasets provided by the organizers. We developed a two-stage framework for lay summarization of biomedical scientific articles. In the first stage, we generated summaries using BART and PEGASUS LLMs by fine-tuning them on the given datasets. In the second stage, we combined the generated summaries and input them to BioBART, and then fine-tuned it on the same datasets. Our findings show that combining general and domain-specific LLMs enhances performance.</abstract>
      <url hash="6d3b14a7">2024.bionlp-1.69</url>
      <bibkey>karotia-susan-2024-biolay</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.69</doi>
    </paper>
    <paper id="70">
      <title><fixed-case>W</fixed-case>is<fixed-case>P</fixed-case>er<fixed-case>M</fixed-case>ed at <fixed-case>B</fixed-case>io<fixed-case>L</fixed-case>ay<fixed-case>S</fixed-case>umm: Adapting Autoregressive Large Language Models for Lay Summarization of Scientific Articles</title>
      <author><first>Tabea Margareta Grace</first><last>Pakull</last></author>
      <author><first>Hendrik</first><last>Damm</last></author>
      <author><first>Ahmad</first><last>Idrissi-Yaghir</last></author>
      <author><first>Henning</first><last>Schäfer</last></author>
      <author><first>Peter A.</first><last>Horn</last></author>
      <author><first>Christoph M.</first><last>Friedrich</last></author>
      <pages>769–779</pages>
      <abstract>This paper details the efforts of the WisPerMed team in the BioLaySumm2024 Shared Task on automatic lay summarization in the biomedical domain, aimed at making scientific publications accessible to non-specialists. Large language models (LLMs), specifically the BioMistral and Llama3 models, were fine-tuned and employed to create lay summaries from complex scientific texts. The summarization performance was enhanced through various approaches, including instruction tuning, few-shot learning, and prompt variations tailored to incorporate specific context information. The experiments demonstrated that fine-tuning generally led to the best performance across most evaluated metrics. Few-shot learning notably improved the models’ ability to generate relevant and factually accurate texts, particularly when using a well-crafted prompt. Additionally, a Dynamic Expert Selection (DES) mechanism to optimize the selection of text outputs based on readability and factuality metrics was developed. Out of 54 participants, the WisPerMed team reached the 4th place, measured by readability, factuality, and relevance. Determined by the overall score, our approach improved upon the baseline by approx. 5.5 percentage points and was only approx. 1.5 percentage points behind the first place.</abstract>
      <url hash="213370f7">2024.bionlp-1.70</url>
      <bibkey>pakull-etal-2024-wispermed</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.70</doi>
    </paper>
    <paper id="71">
      <title><fixed-case>HULAT</fixed-case>-<fixed-case>UC</fixed-case>3<fixed-case>M</fixed-case> at <fixed-case>B</fixed-case>iolay<fixed-case>S</fixed-case>umm: Adaptation of <fixed-case>B</fixed-case>io<fixed-case>BART</fixed-case> and Longformer models to summarizing biomedical documents</title>
      <author><first>Adrian</first><last>Gonzalez Sanchez</last></author>
      <author><first>Paloma</first><last>Martínez</last></author>
      <pages>780–785</pages>
      <abstract>This article presents our submission to the Bio- LaySumm 2024 shared task: Lay Summarization of Biomedical Research Articles. The objective of this task is to generate summaries that are simplified in a concise and less technical way, in order to facilitate comprehension by non-experts users. A pre-trained BioBART model was employed to fine-tune the articles from the two journals, thereby generating two models, one for each journal. The submission achieved the 12th best ranking in the task, attaining a meritorious first place in the Relevance ROUGE-1 metric.</abstract>
      <url hash="8cdbc95f">2024.bionlp-1.71</url>
      <bibkey>gonzalez-sanchez-martinez-2024-hulat</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.71</doi>
    </paper>
    <paper id="72">
      <title>Saama Technologies at <fixed-case>B</fixed-case>io<fixed-case>L</fixed-case>ay<fixed-case>S</fixed-case>umm: Abstract based fine-tuned models with <fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case></title>
      <author><first>Hwanmun</first><last>Kim</last></author>
      <author><first>Kamal raj</first><last>Kanakarajan</last></author>
      <author><first>Malaikannan</first><last>Sankarasubbu</last></author>
      <pages>786–792</pages>
      <abstract>Lay summarization of biomedical research articles is a challenging problem due to their use of technical terms and background knowledge requirements, despite the potential benefits of these research articles to the public. We worked on this problem as participating in BioLaySumm 2024. We experimented with various fine-tuning approaches to generate better lay summaries for biomedical research articles. After several experiments, we built a LoRA model with unsupervised fine-tuning based on the abstracts of the given articles, followed by a post-processing unit to take off repeated sentences. Our model was ranked 3rd overall in the BioLaySumm 2024 leaderboard. We analyzed the different approaches we experimented with and suggested several ideas to improve our model further.</abstract>
      <url hash="c05fa0cd">2024.bionlp-1.72</url>
      <bibkey>kim-etal-2024-saama-technologies</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.72</doi>
    </paper>
    <paper id="73">
      <title><fixed-case>AUTH</fixed-case> at <fixed-case>B</fixed-case>io<fixed-case>L</fixed-case>ay<fixed-case>S</fixed-case>umm 2024: Bringing Scientific Content to Kids</title>
      <author><first>Loukritia</first><last>Stefanou</last></author>
      <author><first>Tatiana</first><last>Passali</last></author>
      <author><first>Grigorios</first><last>Tsoumakas</last></author>
      <pages>793–803</pages>
      <abstract>The BioLaySumm 2024 shared task at the ACL 2024 BioNLP workshop aims to transform biomedical research articles into lay summaries suitable for a broad audience, including children. We utilize the BioBART model, designed for the biomedical sector, to convert complex scientific data into clear, concise summaries. Our dataset, which includes a range of scientific abstracts, enables us to address the diverse information needs of our audience. This focus ensures that our summaries are accessible to both general and younger lay audience. Additionally, we employ specialized tokens and augmentation techniques to optimize the model’s performance. Our methodology proved effective, earning us the 7th rank on the final leaderboard out of 57 participants.</abstract>
      <url hash="aec95f8e">2024.bionlp-1.73</url>
      <bibkey>stefanou-etal-2024-auth</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.73</doi>
    </paper>
    <paper id="74">
      <title><fixed-case>SINAI</fixed-case> at <fixed-case>B</fixed-case>io<fixed-case>L</fixed-case>ay<fixed-case>S</fixed-case>umm: Self-Play Fine-Tuning of Large Language Models for Biomedical Lay Summarisation</title>
      <author><first>Mariia</first><last>Chizhikova</last></author>
      <author><first>Manuel Carlos</first><last>Díaz-Galiano</last></author>
      <author><first>L. Alfonso</first><last>Ureña-López</last></author>
      <author><first>María-Teresa</first><last>Martín-Valdivia</last></author>
      <pages>804–809</pages>
      <abstract>An effective disclosure of scientific knowledge and advancements to the general public is often hindered by the complexity of the technical language used in research which often results very difficult, if not impossible, for non-experts to understand. In this paper we present the approach developed by the SINAI team as the result of our participation in BioLaySumm shared task hosted by the BioNLP workshop at ACL 2024. Our approach stems from the experimentation we performed in order to test the ability of state-of-the-art pre-trained large language models, namely GPT 3.5, GPT 4 and Llama-3, to tackle this task in a few-shot manner. In order to improve this baseline, we opted for fine-tuning Llama-3 by applying parameter-efficient methodologies. The best performing system which resulted from applying self-play fine tuning method which allows the model to improve while learning to distinguish between its own generations from the previous step from the gold standard summaries. This approach achieved 0.4205 ROUGE-1 score and 0.8583 BERTScore.</abstract>
      <url hash="d224ab5d">2024.bionlp-1.74</url>
      <bibkey>chizhikova-etal-2024-sinai</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.74</doi>
    </paper>
    <paper id="75">
      <title><fixed-case>RAG</fixed-case>-<fixed-case>RLRC</fixed-case>-<fixed-case>L</fixed-case>ay<fixed-case>S</fixed-case>um at <fixed-case>B</fixed-case>io<fixed-case>L</fixed-case>ay<fixed-case>S</fixed-case>umm: Integrating Retrieval-Augmented Generation and Readability Control for Layman Summarization of Biomedical Texts</title>
      <author><first>Yuelyu</first><last>Ji</last></author>
      <author><first>Zhuochun</first><last>Li</last></author>
      <author><first>Rui</first><last>Meng</last></author>
      <author><first>Sonish</first><last>Sivarajkumar</last></author>
      <author><first>Yanshan</first><last>Wang</last></author>
      <author><first>Zeshui</first><last>Yu</last></author>
      <author><first>Hui</first><last>Ji</last></author>
      <author><first>Yushui</first><last>Han</last></author>
      <author><first>Hanyu</first><last>Zeng</last></author>
      <author><first>Daqing</first><last>He</last></author>
      <pages>810–817</pages>
      <abstract>This paper introduces the RAG-RLRC-LaySum framework, designed to make complex biomedical research accessible to laymen through advanced Natural Language Processing (NLP) techniques. Our innovative Retrieval Augmentation Generation (RAG) solution, enhanced by a reranking method, utilizes multiple knowledge sources to ensure the precision and pertinence of lay summaries. Additionally, our Reinforcement Learning for Readability Control (RLRC) strategy improves readability, making scientific content comprehensible to non-specialists. Evaluations using the publicly accessible PLOS and eLife datasets show that our methods surpass Plain Gemini model, demonstrating a 20% increase in readability scores, a 15% improvement in ROUGE-2 relevance scores, and a 10% enhancement in factual accuracy. The RAG-RLRC-LaySum framework effectively democratizes scientific knowledge, enhancing public engagement with biomedical discoveries.</abstract>
      <url hash="36caa6e5">2024.bionlp-1.75</url>
      <bibkey>ji-etal-2024-rag</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.75</doi>
    </paper>
    <paper id="76">
      <title>Team <fixed-case>YXZ</fixed-case> at <fixed-case>B</fixed-case>io<fixed-case>L</fixed-case>ay<fixed-case>S</fixed-case>umm: Adapting Large Language Models for Biomedical Lay Summarization</title>
      <author><first>Jieli</first><last>Zhou</last></author>
      <author><first>Cheng</first><last>Ye</last></author>
      <author><first>Pengcheng</first><last>Xu</last></author>
      <author><first>Hongyi</first><last>Xin</last></author>
      <pages>818–825</pages>
      <abstract>Biomedical literature are crucial for disseminating new scientific findings. However, the complexity of these research articles often leads to misinterpretations by the public. To address this urgent issue, we participated in the BioLaySumm task at the 2024 ACL BioNLP workshop, which focuses on automatically simplifying technical biomedical articles for non-technical audiences. We conduct a systematic evaluation of the SOTA large language models (LLMs) in 2024 and found that LLMs can generally achieve better readability scores than smaller models like Bart. Then we iteratively developed techniques of title infusing, K-shot prompting , LLM rewriting and instruction finetuning to further boost readability while balancing factuality and relevance. Notably, our submission achieved the first place in readability at the workshop, and among the top-3 teams with the highest readability scores, we have the best overall rank. Here, we present our experiments and findings on how to effectively adapt LLMs for automatic lay summarization. Our code is available at https://github.com/zhoujieli/biolaysumm.</abstract>
      <url hash="5de7bc33">2024.bionlp-1.76</url>
      <bibkey>zhou-etal-2024-team</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.76</doi>
    </paper>
    <paper id="77">
      <title>Eulerian at <fixed-case>B</fixed-case>io<fixed-case>L</fixed-case>ay<fixed-case>S</fixed-case>umm: Preprocessing Over Abstract is All You Need</title>
      <author><first>Satyam</first><last>Modi</last></author>
      <author><first>T</first><last>Karthikeyan</last></author>
      <pages>826–830</pages>
      <abstract>In this paper, we present our approach to the BioLaySumm 2024 Shared Task on Lay Sum- marization of Biomedical Research Articles at BioNLP workshop 2024. The task aims to generate lay summaries from the abstract and main texts of biomedical research articles, making them understandable to lay audiences. We used some preprocessing techniques and finetuned FLAN-T5 models for the summarization task. Our method achieved an AlignScore of 0.9914 and a SummaC metric score of 0.944.</abstract>
      <url hash="c175b31b">2024.bionlp-1.77</url>
      <bibkey>modi-karthikeyan-2024-eulerian</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.77</doi>
    </paper>
    <paper id="78">
      <title><fixed-case>HGP</fixed-case>-<fixed-case>NLP</fixed-case> at <fixed-case>B</fixed-case>io<fixed-case>L</fixed-case>ay<fixed-case>S</fixed-case>umm: Leveraging <fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case> for Lay Summarization of Biomedical Research Articles using <fixed-case>S</fixed-case>eq2<fixed-case>S</fixed-case>eq Transformers</title>
      <author><first>Hemang</first><last>Malik</last></author>
      <author><first>Gaurav</first><last>Pradeep</last></author>
      <author><first>Pratinav</first><last>Seth</last></author>
      <pages>831–836</pages>
      <abstract>Lay summarization aims to generate summaries of technical articles for non-experts, enabling easy comprehension for a general audience. The technical language used in research often hinders effective communication of scientific knowledge, making it difficult for non-experts to understand. Automatic lay summarization can enhance access to scientific literature, promoting interdisciplinary knowledge sharing and public understanding. This has become especially important for biomedical articles, given the current global need for clear medical information. Large Language Models (LLMs), with their remarkable language understanding capabilities, are ideal for abstractive summarization, helping to make complex information accessible to the public. This paper details our submissions to the BioLaySumm 2024 Shared Task: Lay Summarization of Biomedical Research Articles. We fine-tune and evaluate sequence-to-sequence models like T5 across various training dataset settings and optimization methods such as LoRA for lay summarization. Our submission achieved the 53rd position overall.</abstract>
      <url hash="851f3408">2024.bionlp-1.78</url>
      <bibkey>malik-etal-2024-hgp</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.78</doi>
    </paper>
    <paper id="79">
      <title>Ctyun <fixed-case>AI</fixed-case> at <fixed-case>B</fixed-case>io<fixed-case>L</fixed-case>ay<fixed-case>S</fixed-case>umm: Enhancing Lay Summaries of Biomedical Articles Through Large Language Models and Data Augmentation</title>
      <author><first>Siyu</first><last>Bao</last></author>
      <author><first>Ruijing</first><last>Zhao</last></author>
      <author><first>Siqin</first><last>Zhang</last></author>
      <author><first>Jinghui</first><last>Zhang</last></author>
      <author><first>Weiyin</first><last>Wang</last></author>
      <author><first>Yunian</first><last>Ru</last></author>
      <pages>837–844</pages>
      <abstract>Lay summaries play a crucial role in making scientific research accessible to a wider audience. However, generating lay summaries from lengthy articles poses significant challenges. We consider two approaches to address this issue: Hard Truncation, which preserves the most informative initial portion of the article, and Text Chunking, which segments articles into smaller, manageable chunks. Our workflow encompasses data preprocessing, augmentation, prompt engineering, and fine-tuning large language models. We explore the influence of pretrained model selection, inference prompt design, and hyperparameter tuning on summarization performance. Our methods demonstrate effectiveness in generating high-quality, informative lay summaries, achieving the second-best performance in the BioLaySumm shared task at BioNLP 2024.</abstract>
      <url hash="dc893b10">2024.bionlp-1.79</url>
      <bibkey>bao-etal-2024-ctyun</bibkey>
      <doi>10.18653/v1/2024.bionlp-1.79</doi>
    </paper>
  </volume>
</collection>
