<?xml version='1.0' encoding='UTF-8'?>
<collection id="2023.nllp">
  <volume id="1" ingest-date="2023-11-30" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Natural Legal Language Processing Workshop 2023</booktitle>
      <editor><first>Daniel</first><last>Preoțiuc-Pietro</last></editor>
      <editor><first>Catalina</first><last>Goanta</last></editor>
      <editor><first>Ilias</first><last>Chalkidis</last></editor>
      <editor><first>Leslie</first><last>Barrett</last></editor>
      <editor><first>Gerasimos (Jerry)</first><last>Spanakis</last></editor>
      <editor><first>Nikolaos</first><last>Aletras</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Singapore</address>
      <month>December</month>
      <year>2023</year>
      <venue>nllp</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="bb91a61e">2023.nllp-1.0</url>
      <bibkey>nllp-ws-2023-natural</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Anthropomorphization of <fixed-case>AI</fixed-case>: Opportunities and Risks</title>
      <author><first>Ameet</first><last>Deshpande</last><affiliation>Princeton University</affiliation></author>
      <author><first>Tanmay</first><last>Rajpurohit</last><affiliation>Genpact</affiliation></author>
      <author><first>Karthik</first><last>Narasimhan</last><affiliation>Princeton University</affiliation></author>
      <author><first>Ashwin</first><last>Kalyan</last><affiliation>Allen Institute for Artificial Intelligence (AI2)</affiliation></author>
      <pages>1-7</pages>
      <abstract>Anthropomorphization is the tendency to attribute human-like traits to non-human entities. It is prevalent in many social contexts – children anthropomorphize toys, adults do so with brands, and it is a literary device. It is also a versatile tool in science, with behavioral psychology and evolutionary biology meticulously documenting its consequences. With widespread adoption of AI systems, and the push from stakeholders to make it human-like through alignment techniques, human voice, and pictorial avatars, the tendency for users to anthropomorphize it increases significantly. We take a dyadic approach to understanding this phenomenon with large language models (LLMs) by studying (1) the objective legal implications, as analyzed through the lens of the recent blueprint of AI bill of rights and the (2) subtle psychological aspects customization and anthropomorphization. We find that anthropomorphized LLMs customized for different user bases violate multiple provisions in the legislative blueprint. In addition, we point out that anthropomorphization of LLMs affects the influence they can have on their users, thus having the potential to fundamentally change the nature of human-AI interaction, with potential for manipulation and negative influence. With LLMs being hyper-personalized for vulnerable groups like children and patients among others, our work is a timely and important contribution. We propose a conservative strategy for the cautious use of anthropomorphization to improve trustworthiness of AI systems.</abstract>
      <url hash="9f95af30">2023.nllp-1.1</url>
      <bibkey>deshpande-etal-2023-anthropomorphization</bibkey>
      <doi>10.18653/v1/2023.nllp-1.1</doi>
    </paper>
    <paper id="2">
      <title><fixed-case>NOMOS</fixed-case>: Navigating Obligation Mining in Official Statutes</title>
      <author><first>Andrea</first><last>Pennisi</last><affiliation>Enhesa/Unibas</affiliation></author>
      <author><first>Elvira</first><last>González Hernández</last><affiliation>Enhesa</affiliation></author>
      <author><first>Nina</first><last>Koivula</last><affiliation>Enhesa</affiliation></author>
      <pages>8-16</pages>
      <abstract>The process of identifying obligations in a legal text is not a straightforward task, because not only are the documents long, but the sentences therein are long as well. As a result of long elements in the text, law is more difficult to interpret (Coupette et al., 2021). Moreover, the identification of obligations relies not only on the clarity and precision of the language used but also on the unique perspectives, experiences, and knowledge of the reader. In particular, this paper addresses the problem of identifyingobligations using machine and deep learning approaches showing a full comparison between both methodologies and proposing a new approach called NOMOS based on the combination of Positional Embeddings (PE) and Temporal Convolutional Networks (TCNs). Quantitative and qualitative experiments, conducted on legal regulations 1, demonstrate the effectiveness of the proposed approach.</abstract>
      <url hash="f9ee7cb4">2023.nllp-1.2</url>
      <bibkey>pennisi-etal-2023-nomos</bibkey>
      <doi>10.18653/v1/2023.nllp-1.2</doi>
    </paper>
    <paper id="3">
      <title>Long Text Classification using Transformers with Paragraph Selection Strategies</title>
      <author><first>Mohit</first><last>Tuteja</last><affiliation>Thomson Reuters</affiliation></author>
      <author><first>Daniel</first><last>González Juclà</last><affiliation>Thomson Reuters</affiliation></author>
      <pages>17-24</pages>
      <abstract>In the legal domain, we often perform classification tasks on very long documents, for example court judgements. These documents often contain thousands of words, so the length of these documents poses a challenge for this modelling task. In this research paper, we present a comprehensive evaluation of various strategies to perform long text classification using Transformers in conjunction with strategies to select document chunks using traditional NLP models. We conduct our experiments on 6 benchmark datasets comprising lengthy documents, 4 of which are publicly available. Each dataset has a median word count exceeding 1,000. Our evaluation encompasses state-of-the-art Transformer models, such as RoBERTa, Longformer, HAT, MEGA and LegalBERT and compares them with a traditional baseline TF-IDF + Neural Network (NN) model. We investigate the effectiveness of pre-training on large corpora, fine tuning strategies, and transfer learning techniques in the context of long text classification.</abstract>
      <url hash="c104cd83">2023.nllp-1.3</url>
      <bibkey>tuteja-gonzalez-jucla-2023-long</bibkey>
      <doi>10.18653/v1/2023.nllp-1.3</doi>
    </paper>
    <paper id="4">
      <title>Do Language Models Learn about Legal Entity Types during Pretraining?</title>
      <author><first>Claire</first><last>Barale</last><affiliation>The University of Edinburgh</affiliation></author>
      <author><first>Michael</first><last>Rovatsos</last><affiliation>The University of Edinburgh</affiliation></author>
      <author><first>Nehal</first><last>Bhuta</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>25-37</pages>
      <abstract>Language Models (LMs) have proven their ability to acquire diverse linguistic knowledge during the pretraining phase, potentially serving as a valuable source of incidental supervision for downstream tasks. However, there has been limited research conducted on the retrieval of domain-specific knowledge, and specifically legal knowledge. We propose to explore the task of Entity Typing, serving as a proxy for evaluating legal knowledge as an essential aspect of text comprehension, and a foundational task to numerous downstream legal NLP applications. Through systematic evaluation and analysis and two types of prompting (cloze sentences and QA-based templates) and to clarify the nature of these acquired cues, we compare diverse types and lengths of entities both general and domain-specific entities, semantics or syntax signals, and different LM pretraining corpus (generic and legal-oriented) and architectures (encoder BERT-based and decoder-only with Llama2). We show that (1) Llama2 performs well on certain entities and exhibits potential for substantial improvement with optimized prompt templates, (2) law-oriented LMs show inconsistent performance, possibly due to variations in their training corpus, (3) LMs demonstrate the ability to type entities even in the case of multi-token entities, (4) all models struggle with entities belonging to sub-domains of the law (5) Llama2 appears to frequently overlook syntactic cues, a shortcoming less present in BERT-based architectures.</abstract>
      <url hash="ae689dce">2023.nllp-1.4</url>
      <bibkey>barale-etal-2023-language</bibkey>
      <doi>10.18653/v1/2023.nllp-1.4</doi>
    </paper>
    <paper id="5">
      <title>Pretrained Language Models v. Court Ruling Predictions: A Case Study on a Small Dataset of <fixed-case>F</fixed-case>rench Court of Appeal Rulings</title>
      <author><first>Olivia</first><last>Vaudaux</last><affiliation>Université Grenoble Alpes</affiliation></author>
      <author><first>Caroline</first><last>Bazzoli</last><affiliation>Université Grenoble Alpes</affiliation></author>
      <author><first>Maximin</first><last>Coavoux</last><affiliation>CNRS, Univ Grenoble Alpes</affiliation></author>
      <author><first>Géraldine</first><last>Vial</last><affiliation>Université Grenoble Alpes</affiliation></author>
      <author><first>Étienne</first><last>Vergès</last><affiliation>Université Grenoble Alpes</affiliation></author>
      <pages>38-43</pages>
      <abstract>NLP systems are increasingly used in the law domain, either by legal institutions or by the industry. As a result there is a pressing need to characterize their strengths and weaknesses and understand their inner workings. This article presents a case study on the task of judicial decision prediction, on a small dataset from French Courts of Appeal. Specifically, our dataset of around 1000 decisions is about the habitual place of residency of children from divorced parents. The task consists in predicting, from the facts and reasons of the documents, whether the court rules that children should live with their mother or their father. Instead of feeding the whole document to a classifier, we carefully construct the dataset to make sure that the input to the classifier does not contain any ‘spoilers’ (it is often the case in court rulings that information all along the document mentions the final decision). Our results are mostly negative: even classifiers based on French pretrained language models (Flaubert, JuriBERT) do not classify the decisions with a reasonable accuracy. However, they can extract the decision when it is part of the input. With regards to these results, we argue that there is a strong caveat when constructing legal NLP datasets automatically.</abstract>
      <url hash="716c3d5b">2023.nllp-1.5</url>
      <bibkey>vaudaux-etal-2023-pretrained</bibkey>
      <doi>10.18653/v1/2023.nllp-1.5</doi>
    </paper>
    <paper id="6">
      <title><fixed-case>I</fixed-case>talian Legislative Text Classification for Gazzetta Ufficiale</title>
      <author><first>Marco</first><last>Rovera</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <author><first>Alessio</first><last>Palmero Aprosio</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <author><first>Francesco</first><last>Greco</last><affiliation>Istituto Poligrafico e Zecca dello Stato</affiliation></author>
      <author><first>Mariano</first><last>Lucchese</last><affiliation>Istituto Poligrafico e Zecca dello Stato</affiliation></author>
      <author><first>Sara</first><last>Tonelli</last><affiliation>FBK</affiliation></author>
      <author><first>Antonio</first><last>Antetomaso</last><affiliation>Istituto Poligrafico e Zecca dello Stato</affiliation></author>
      <pages>44-50</pages>
      <abstract>This work introduces a novel, extensive annotated corpus for multi-label legislative text classification in Italian, based on legal acts from the Gazzetta Ufficiale, the official source of legislative information of the Italian state. The annotated dataset, which we released to the community, comprises over 363,000 titles of legislative acts, spanning over 30 years from 1988 until 2022. Moreover, we evaluate four models for text classification on the dataset, demonstrating how using only the acts’ titles can achieve top-level classification performance, with a micro F1-score of 0.87. Also, our analysis shows how Italian domain-adapted legal models do not outperform general-purpose models on the task. Models’ performance can be checked by users via a demonstrator system provided in support of this work.</abstract>
      <url hash="8ceaaae0">2023.nllp-1.6</url>
      <bibkey>rovera-etal-2023-italian</bibkey>
      <doi>10.18653/v1/2023.nllp-1.6</doi>
    </paper>
    <paper id="7">
      <title>Mixed-domain Language Modeling for Processing Long Legal Documents</title>
      <author><first>Wenyue</first><last>Hua</last><affiliation>Rutgers University, New Brunswick</affiliation></author>
      <author><first>Yuchen</first><last>Zhang</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Zhe</first><last>Chen</last><affiliation>Claudius Legal Intelligence</affiliation></author>
      <author><first>Josie</first><last>Li</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Melanie</first><last>Weber</last><affiliation>Harvard University</affiliation></author>
      <pages>51-61</pages>
      <abstract>The application of Natural Language Processing (NLP) to specialized domains, such as the law, has recently received a surge of interest. As many legal services rely on processing and analyzing large collections of documents, automating such tasks with NLP tools such as language models emerges as a key challenge since legal documents may contain specialized vocabulary from other domains, such as medical terminology in personal injury text. However, most language models are general-purpose models, which either have limited reasoning capabilities on highly specialized legal terminology and syntax, such as BERT or ROBERTA, or are expensive to run and tune, such as GPT-3.5 and Claude. Thus, in this paper, we propose a specialized language model for personal injury text, LEGALRELECTRA, which is trained on mixed-domain legal and medical corpora. We show that as a small language model, our model improves over general-domain and single-domain medical and legal language models when processing mixed-domain (personal injury) text. Our training architecture implements the ELECTRA framework but utilizes REFORMER instead of BERT for its generator and discriminator. We show that this improves the model’s performance on processing long passages and results in better long-range text comprehension.</abstract>
      <url hash="f399640c">2023.nllp-1.7</url>
      <bibkey>hua-etal-2023-mixed</bibkey>
      <doi>10.18653/v1/2023.nllp-1.7</doi>
    </paper>
    <paper id="8">
      <title>Questions about Contracts: Prompt Templates for Structured Answer Generation</title>
      <author><first>Adam</first><last>Roegiest</last><affiliation>Zuva</affiliation></author>
      <author><first>Radha</first><last>Chitta</last><affiliation>Zuva</affiliation></author>
      <author><first>Jonathan</first><last>Donnelly</last><affiliation>Zuva</affiliation></author>
      <author><first>Maya</first><last>Lash</last><affiliation>Zuva</affiliation></author>
      <author><first>Alexandra</first><last>Vtyurina</last><affiliation>Zuva</affiliation></author>
      <author><first>Francois</first><last>Longtin</last><affiliation>Zuva</affiliation></author>
      <pages>62-72</pages>
      <abstract>Finding the answers to legal questions about specific clauses in contracts is an important analysis in many legal workflows (e.g., understanding market trends, due diligence, risk mitigation) but more important is being able to do this at scale. In this paper, we present an examination of using large language models to produce (partially) structured answers to legal questions; primarily in the form of multiple choice and multiple select. We first show that traditional semantic matching is unable to perform this task at acceptable accuracy and then show how question specific prompts can achieve reasonable accuracy across a range of generative models. Finally, we show that much of this effectiveness can be maintained when generalized prompt templates are used rather than question specific ones.</abstract>
      <url hash="72e1fcc7">2023.nllp-1.8</url>
      <bibkey>roegiest-etal-2023-questions</bibkey>
      <doi>10.18653/v1/2023.nllp-1.8</doi>
    </paper>
    <paper id="9">
      <title>Legal Judgment Prediction: If You Are Going to Do It, Do It Right</title>
      <author><first>Masha</first><last>Medvedeva</last><affiliation>Leiden University</affiliation></author>
      <author><first>Pauline</first><last>Mcbride</last><affiliation>Vrije Universiteit Brussel</affiliation></author>
      <pages>73-84</pages>
      <abstract>The field of Legal Judgment Prediction (LJP) has witnessed significant growth in the past decade, with over 100 papers published in the past three years alone. Our comprehensive survey of over 150 papers reveals a stark reality: only ~7% of published papers are doing what they set out to do - predict court decisions. We delve into the reasons behind the flawed and unreliable nature of the remaining experiments, emphasising their limited utility in the legal domain. We examine the distinctions between predicting court decisions and the practices of legal professionals in their daily work. We explore how a lack of attention to the identity and needs of end-users has fostered the misconception that LJP is a near-solved challenge suitable for practical application, and contributed to the surge in academic research in the field. To address these issues, we examine three different dimensions of ‘doing LJP right’: using data appropriate for the task; tackling explainability; and adopting an application-centric approach to model reporting and evaluation. We formulate a practical checklist of recommendations, delineating the characteristics that are required if a judgment prediction system is to be a valuable addition to the legal field.</abstract>
      <url hash="07c5ac92">2023.nllp-1.9</url>
      <bibkey>medvedeva-mcbride-2023-legal</bibkey>
      <doi>10.18653/v1/2023.nllp-1.9</doi>
    </paper>
    <paper id="10">
      <title>Beyond The Text: Analysis of Privacy Statements through Syntactic and Semantic Role Labeling</title>
      <author><first>Yan</first><last>Shvartzshanider</last><affiliation>York University</affiliation></author>
      <author><first>Ananth</first><last>Balashankar</last><affiliation>Google</affiliation></author>
      <author><first>Thomas</first><last>Wies</last><affiliation>New York University</affiliation></author>
      <author><first>Lakshminarayanan</first><last>Subramanian</last><affiliation>New York University</affiliation></author>
      <pages>85-98</pages>
      <abstract>This paper formulates a new task of extracting privacy parameters from a privacy policy, through the lens of Contextual Integrity (CI), an established social theory framework for reasoning about privacy norms. Through extensive experiments, we further show that incorporating CI-based domain-specific knowledge into a BERT-based SRL model results in the highest precision and recall, achieving an F1 score of 84%. With our work, we would like to motivate new research in building NLP applications for the privacy domain.</abstract>
      <url hash="e4c127e9">2023.nllp-1.10</url>
      <bibkey>shvartzshanider-etal-2023-beyond</bibkey>
      <revision id="1" href="2023.nllp-1.10v1" hash="fea1c897"/>
      <revision id="2" href="2023.nllp-1.10v2" hash="e4c127e9" date="2023-12-09">Typo correction in the CI labels in Section 5.4.</revision>
    </paper>
    <paper id="11">
      <title>Towards Mitigating Perceived Unfairness in Contracts from a Non-Legal Stakeholder’s Perspective</title>
      <author><first>Anmol</first><last>Singhal</last><affiliation>TCS Research</affiliation></author>
      <author><first>Preethu Rose</first><last>Anish</last><affiliation>TCS Research</affiliation></author>
      <author><first>Shirish</first><last>Karande</last><affiliation>TCS Research</affiliation></author>
      <author><first>Smita</first><last>Ghaisas</last><affiliation>TCs Research</affiliation></author>
      <pages>99-112</pages>
      <abstract>Commercial contracts are known to be a valuable source for deriving project-specific requirements. However, contract negotiations mainly occur among the legal counsel of the parties involved. The participation of non-legal stakeholders, including requirement analysts, engineers, and solution architects, whose primary responsibility lies in ensuring the seamless implementation of contractual terms, is often indirect and inadequate. Consequently, a significant number of sentences in contractual clauses, though legally accurate, can appear unfair from an implementation perspective to non-legal stakeholders. This perception poses a problem since requirements indicated in the clauses are obligatory and can involve punitive measures and penalties if not implemented as committed in the contract. Therefore, the identification of potentially unfair clauses in contracts becomes crucial. In this work, we conduct an empirical study to analyze the perspectives of different stakeholders regarding contractual fairness. We then investigate the ability of Pre-trained Language Models (PLMs) to identify unfairness in contractual sentences by comparing chain of thought prompting and semi-supervised fine-tuning approaches. Using BERT-based fine-tuning, we achieved an accuracy of 84% on a dataset consisting of proprietary contracts. It outperformed chain of thought prompting using Vicuna-13B by a margin of 9%.</abstract>
      <url hash="7d485463">2023.nllp-1.11</url>
      <bibkey>singhal-etal-2023-towards</bibkey>
      <doi>10.18653/v1/2023.nllp-1.11</doi>
    </paper>
    <paper id="12">
      <title>Connecting Symbolic Statutory Reasoning with Legal Information Extraction</title>
      <author><first>Nils</first><last>Holzenberger</last><affiliation>Télécom Paris</affiliation></author>
      <author><first>Benjamin</first><last>Van Durme</last><affiliation>Johns Hopkins University / Microsoft</affiliation></author>
      <pages>113-131</pages>
      <abstract>Statutory reasoning is the task of determining whether a given law – a part of a statute – applies to a given legal case. Previous work has shown that structured, logical representations of laws and cases can be leveraged to solve statutory reasoning, including on the StAtutory Reasoning Assessment dataset (SARA), but rely on costly human translation into structured representations. Here, we investigate a form of legal information extraction atop the SARA cases, illustrating how the task can be done with high performance. Further, we show how the performance of downstream symbolic reasoning directly correlates with the quality of the information extraction.</abstract>
      <url hash="dfcd2479">2023.nllp-1.12</url>
      <bibkey>holzenberger-van-durme-2023-connecting</bibkey>
      <doi>10.18653/v1/2023.nllp-1.12</doi>
    </paper>
    <paper id="13">
      <title>Retrieval-based Evaluation for <fixed-case>LLM</fixed-case>s: A Case Study in <fixed-case>K</fixed-case>orean Legal <fixed-case>QA</fixed-case></title>
      <author><first>Cheol</first><last>Ryu</last><affiliation>Linq Labs</affiliation></author>
      <author><first>Seolhwa</first><last>Lee</last><affiliation>Linq labs</affiliation></author>
      <author><first>Subeen</first><last>Pang</last><affiliation>Linq Labs</affiliation></author>
      <author><first>Chanyeol</first><last>Choi</last><affiliation>Linq Labs</affiliation></author>
      <author><first>Hojun</first><last>Choi</last><affiliation>Law&amp;Good</affiliation></author>
      <author><first>Myeonggee</first><last>Min</last><affiliation>Law&amp;Good</affiliation></author>
      <author><first>Jy-Yong</first><last>Sohn</last><affiliation>Yonsei University</affiliation></author>
      <pages>132-137</pages>
      <abstract>While large language models (LLMs) have demonstrated significant capabilities in text generation, their utilization in areas requiring domain-specific expertise, such as law, must be approached cautiously. This caution is warranted due to the inherent challenges associated with LLM-generated texts, including the potential presence of factual errors. Motivated by this issue, we propose Eval-RAG, a new evaluation method for LLM-generated texts. Unlike existing methods, Eval-RAG evaluates the validity of generated texts based on the related document that are collected by the retriever. In other words, Eval-RAG adopts the idea of retrieval augmented generation (RAG) for the purpose of evaluation. Our experimental results on Korean Legal Question-Answering (QA) tasks show that conventional LLM-based evaluation methods can be better aligned with Lawyers’ evaluations, by combining with Eval-RAG. In addition, our qualitative analysis show that Eval-RAG successfully finds the factual errors in LLM-generated texts, while existing evaluation methods cannot.</abstract>
      <url hash="00e36856">2023.nllp-1.13</url>
      <bibkey>ryu-etal-2023-retrieval</bibkey>
      <doi>10.18653/v1/2023.nllp-1.13</doi>
    </paper>
    <paper id="14">
      <title>Legal <fixed-case>NLP</fixed-case> Meets <fixed-case>M</fixed-case>i<fixed-case>CAR</fixed-case>: Advancing the Analysis of Crypto White Papers</title>
      <author><first>Carolina</first><last>Camassa</last><affiliation>Bank of Italy</affiliation></author>
      <pages>138-148</pages>
      <abstract>In the rapidly evolving field of crypto assets, white papers are essential documents for investor guidance, and are now subject to unprecedented content requirements under the European Union’s Markets in Crypto-Assets Regulation (MiCAR). Natural Language Processing (NLP) can serve as a powerful tool for both analyzing these documents and assisting in regulatory compliance. This paper delivers two contributions to the topic. First, we survey existing applications of textual analysis to unregulated crypto asset white papers, uncovering a research gap that could be bridged with interdisciplinary collaboration. We then conduct an analysis of the changes introduced by MiCAR, highlighting the opportunities and challenges of integrating NLP within the new regulatory framework. The findings set the stage for further research, with the potential to benefit regulators, crypto asset issuers, and investors.</abstract>
      <url hash="32f7010c">2023.nllp-1.14</url>
      <bibkey>camassa-2023-legal</bibkey>
      <doi>10.18653/v1/2023.nllp-1.14</doi>
    </paper>
    <paper id="15">
      <title>Low-Resource Deontic Modality Classification in <fixed-case>EU</fixed-case> Legislation</title>
      <author><first>Kristina</first><last>Minkova</last><affiliation>Brightlands Institute for Smart Society, Maastricht University</affiliation></author>
      <author><first>Shashank</first><last>Chakravarthy</last><affiliation>Brightlands Institute for Smart Society, Maastricht University</affiliation></author>
      <author><first>Gijs</first><last>Dijck</last><affiliation>Maastricht Law and Tech Lab, Maastricht University</affiliation></author>
      <pages>149-158</pages>
      <abstract>In law, it is important to distinguish between obligations, permissions, prohibitions, rights, and powers. These categories are called deontic modalities. This paper evaluates the performance of two deontic modality classification models, LEGAL-BERT and a Fusion model, in a low-resource setting. To create a generalized dataset for multi-class classification, we extracted random provisions from European Union (EU) legislation. By fine-tuning previously researched and published models, we evaluate their performance on our dataset against fusion models designed for low-resource text classification. We incorporate focal loss as an alternative for cross-entropy to tackle issues of class imbalance. The experiments indicate that the fusion model performs better for both balanced and imbalanced data with a macro F1-score of 0.61 for imbalanced data, 0.62 for balanced data, and 0.55 with focal loss for imbalanced data. When focusing on accuracy, our experiments indicate that the fusion model performs better with scores of 0.91 for imbalanced data, 0.78 for balanced data, and 0.90 for imbalanced data with focal loss.</abstract>
      <url hash="13f58685">2023.nllp-1.15</url>
      <bibkey>minkova-etal-2023-low</bibkey>
      <doi>10.18653/v1/2023.nllp-1.15</doi>
    </paper>
    <paper id="16">
      <title>Automatic Anonymization of <fixed-case>S</fixed-case>wiss Federal <fixed-case>S</fixed-case>upreme <fixed-case>C</fixed-case>ourt Rulings</title>
      <author><first>Joel</first><last>Niklaus</last><affiliation>University of Bern</affiliation></author>
      <author><first>Robin</first><last>Mamié</last><affiliation>Swiss Federal Supreme Court</affiliation></author>
      <author><first>Matthias</first><last>Stürmer</last><affiliation>University of Bern</affiliation></author>
      <author><first>Daniel</first><last>Brunner</last><affiliation>Swiss Federal Supreme Court</affiliation></author>
      <author><first>Marcel</first><last>Gygli</last><affiliation>Bern University of Applied Sciences</affiliation></author>
      <pages>159-165</pages>
      <abstract>Releasing court decisions to the public relies on proper anonymization to protect all involved parties, where necessary. The Swiss Federal Supreme Court relies on an existing system that combines different traditional computational methods with human experts. In this work, we enhance the existing anonymization software using a large dataset annotated with entities to be anonymized. We compared BERT-based models with models pre-trained on in-domain data. Our results show that using in-domain data to pre-train the models further improves the F1-score by more than 5% compared to existing models. Our work demonstrates that combining existing anonymization methods, such as regular expressions, with machine learning can further reduce manual labor and enhance automatic suggestions.</abstract>
      <url hash="187330fe">2023.nllp-1.16</url>
      <bibkey>niklaus-etal-2023-automatic</bibkey>
      <doi>10.18653/v1/2023.nllp-1.16</doi>
    </paper>
    <paper id="17">
      <title>Exploration of Open Large Language Models for e<fixed-case>D</fixed-case>iscovery</title>
      <author><first>Sumit</first><last>Pai</last><affiliation>Deloitte</affiliation></author>
      <author><first>Sounak</first><last>Lahiri</last><affiliation>Deloitte</affiliation></author>
      <author><first>Ujjwal</first><last>Kumar</last><affiliation>Deloitte</affiliation></author>
      <author><first>Krishanu</first><last>Baksi</last><affiliation>Deloitte</affiliation></author>
      <author><first>Elijah</first><last>Soba</last><affiliation>Deloitte</affiliation></author>
      <author><first>Michael</first><last>Suesserman</last><affiliation>Deloitte</affiliation></author>
      <author><first>Nirmala</first><last>Pudota</last><affiliation>Deloitte</affiliation></author>
      <author><first>Jon</first><last>Foster</last><affiliation>Deloitte</affiliation></author>
      <author><first>Edward</first><last>Bowen</last><affiliation>Deloitte</affiliation></author>
      <author><first>Sanmitra</first><last>Bhattacharya</last><affiliation>Deloitte</affiliation></author>
      <pages>166-177</pages>
      <abstract>The rapid advancement of Generative Artificial Intelligence (AI), particularly Large Language Models (LLMs), has led to their widespread adoption for various natural language processing (NLP) tasks. One crucial domain ripe for innovation is the Technology-Assisted Review (TAR) process in Electronic discovery (eDiscovery). Traditionally, TAR involves manual review and classification of documents for relevance over large document collections for litigations and investigations. This process is aided by machine learning and NLP tools which require extensive training and fine-tuning. In this paper, we explore the application of LLMs to TAR, specifically for predictive coding. We experiment with out-of-the-box prompting and fine-tuning of LLMs using parameter-efficient techniques. We conduct experiments using open LLMs and compare them to commercially-licensed ones. Our experiments demonstrate that open LLMs lag behind commercially-licensed models in relevance classification using out-of-the-box prompting. However, topic-specific instruction tuning of open LLMs not only improve their effectiveness but can often outperform their commercially-licensed counterparts in performance evaluations. Additionally, we conduct a user study to gauge the preferences of our eDiscovery Subject Matter Specialists (SMS) regarding human-authored versus model-generated reasoning. We demonstrate that instruction-tuned open LLMs can generate high quality reasonings that are comparable to commercial LLMs.</abstract>
      <url hash="25046765">2023.nllp-1.17</url>
      <bibkey>pai-etal-2023-exploration</bibkey>
      <doi>10.18653/v1/2023.nllp-1.17</doi>
    </paper>
    <paper id="18">
      <title>Retrieval-Augmented Chain-of-Thought in Semi-structured Domains</title>
      <author><first>Vaibhav</first><last>Mavi</last><affiliation>New York University</affiliation></author>
      <author><first>Abulhair</first><last>Saparov</last><affiliation>New York University</affiliation></author>
      <author><first>Chen</first><last>Zhao</last><affiliation>New York University</affiliation></author>
      <pages>178-191</pages>
      <abstract>Applying existing question answering (QA) systems to specialized domains like law and finance presents challenges that necessitate domain expertise. Although large language models (LLMs) have shown impressive language comprehension and in-context learning capabilities, their inability to handle very long inputs/contexts is well known. Tasks specific to these domains need significant background knowledge, leading to contexts that can often exceed the maximum length that existing LLMs can process. This study explores leveraging the semi-structured nature of legal and financial data to efficiently retrieve relevant context, enabling the use of LLMs for domain-specialized QA. The resulting system outperforms contemporary models and also provides useful explanations for the answers, encouraging the integration of LLMs into legal and financial NLP systems for future research.</abstract>
      <url hash="00b22041">2023.nllp-1.18</url>
      <bibkey>mavi-etal-2023-retrieval</bibkey>
      <doi>10.18653/v1/2023.nllp-1.18</doi>
    </paper>
    <paper id="19">
      <title>Joint Learning for Legal Text Retrieval and Textual Entailment: Leveraging the Relationship between Relevancy and Affirmation</title>
      <author><first>Nguyen</first><last>Hai Long</last><affiliation>UET - VNU</affiliation></author>
      <author><first>Thi Hai Yen</first><last>Vuong</last><affiliation>University of Engineering and Technology, Vietnam national university Hanoi</affiliation></author>
      <author><first>Ha Thanh</first><last>Nguyen</last><affiliation>National Institute of Informatics</affiliation></author>
      <author><first>Xuan-Hieu</first><last>Phan</last><affiliation>University of Engineering and Technology, Vietnam National University, Hanoi</affiliation></author>
      <pages>192-201</pages>
      <abstract>In legal text processing and reasoning, one normally performs information retrieval to find relevant documents of an input question, and then performs textual entailment to answer the question. The former is about relevancy whereas the latter is about affirmation (or conclusion). While relevancy and affirmation are two different concepts, there is obviously a connection between them. That is why performing retrieval and textual entailment sequentially and independently may not make the most of this mutually supportive relationship. This paper, therefore, propose a multi–task learning model for these two tasks to improve their performance. Technically, in the COLIEE dataset, we use the information of Task 4 (conclusions) to improve the performance of Task 3 (searching for legal provisions related to the question). Our empirical findings indicate that this supportive relationship truly exists. This important insight sheds light on how leveraging relationship between tasks can significantly enhance the effectiveness of our multi-task learning approach for legal text processing.</abstract>
      <url hash="b338cc86">2023.nllp-1.19</url>
      <bibkey>hai-long-etal-2023-joint</bibkey>
      <doi>10.18653/v1/2023.nllp-1.19</doi>
    </paper>
    <paper id="20">
      <title>Super-<fixed-case>SCOTUS</fixed-case>: A multi-sourced dataset for the <fixed-case>S</fixed-case>upreme <fixed-case>C</fixed-case>ourt of the <fixed-case>US</fixed-case></title>
      <author><first>Biaoyan</first><last>Fang</last><affiliation>The University of Melbourne</affiliation></author>
      <author><first>Trevor</first><last>Cohn</last><affiliation>University of Melbourne</affiliation></author>
      <author><first>Timothy</first><last>Baldwin</last><affiliation>MBZUAI</affiliation></author>
      <author><first>Lea</first><last>Frermann</last><affiliation>Melbourne University</affiliation></author>
      <pages>202-214</pages>
      <abstract>Given the complexity of the judiciary in the US Supreme Court, various procedures, along with various resources, contribute to the court system. However, most research focuses on a limited set of resources, e.g., court opinions or oral arguments, for analyzing a specific perspective in court, e.g., partisanship or voting. To gain a fuller understanding of these perspectives in the legal system of the US Supreme Court, a more comprehensive dataset, connecting different sources in different phases of the court procedure, is needed. To address this gap, we present a multi-sourced dataset for the Supreme Court, comprising court resources from different procedural phases, connecting language documents with extensive metadata. We showcase its utility through a case study on how different court documents reveal the decision direction (conservative vs. liberal) of the cases. We analyze performance differences across three protected attributes, indicating that different court resources encode different biases, and reinforcing that considering various resources provides a fuller picture of the court procedures. We further discuss how our dataset can contribute to future research directions.</abstract>
      <url hash="aa04616e">2023.nllp-1.20</url>
      <bibkey>fang-etal-2023-super</bibkey>
      <doi>10.18653/v1/2023.nllp-1.20</doi>
      <video href="2023.nllp-1.20.mp4"/>
    </paper>
    <paper id="21">
      <title>Transferring Legal Natural Language Inference Model from a <fixed-case>US</fixed-case> State to Another: What Makes It So Hard?</title>
      <author><first>Alice</first><last>Kwak</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Gaetano</first><last>Forte</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Derek</first><last>Bambauer</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Mihai</first><last>Surdeanu</last><affiliation>University of Arizona</affiliation></author>
      <pages>215-222</pages>
      <abstract>This study investigates whether a legal natural language inference (NLI) model trained on the data from one US state can be transferred to another state. We fine-tuned a pre-trained model on the task of evaluating the validity of legal will statements, once with the dataset containing the Tennessee wills and once with the dataset containing the Idaho wills. Each model’s performance on the in-domain setting and the out-of-domain setting are compared to see if the models can across the states. We found that the model trained on one US state can be mostly transferred to another state. However, it is clear that the model’s performance drops in the out-of-domain setting. The F1 scores of the Tennessee model and the Idaho model are 96.41 and 92.03 when predicting the data from the same state, but they drop to 66.32 and 81.60 when predicting the data from another state. Subsequent error analysis revealed that there are two major sources of errors. First, the model fails to recognize equivalent laws across states when there are stylistic differences between laws. Second, difference in statutory section numbering system between the states makes it difficult for the model to locate laws relevant to the cases being predicted on. This analysis provides insights on how the future NLI system can be improved. Also, our findings offer empirical support to legal experts advocating the standardization of legal documents.</abstract>
      <url hash="11a237c9">2023.nllp-1.21</url>
      <bibkey>kwak-etal-2023-transferring</bibkey>
      <doi>10.18653/v1/2023.nllp-1.21</doi>
    </paper>
    <paper id="22">
      <title>Large Language Models are legal but they are not: Making the case for a powerful <fixed-case>L</fixed-case>egal<fixed-case>LLM</fixed-case></title>
      <author><first>Thanmay</first><last>Jayakumar</last><affiliation>Visvesvaraya National Institute of Technology</affiliation></author>
      <author><first>Fauzan</first><last>Farooqui</last><affiliation>Visvesvaraya National Institute of Technology</affiliation></author>
      <author><first>Luqman</first><last>Farooqui</last><affiliation>Visvesvaraya National Institute of Technology</affiliation></author>
      <pages>223-229</pages>
      <abstract>Realizing the recent advances from Natural Language Processing (NLP) to the legal sector poses challenging problems such as extremely long sequence lengths, specialized vocabulary that is usually only understood by legal professionals, and high amounts of data imbalance. The recent surge of Large Language Models (LLM) has begun to provide new opportunities to apply NLP in the legal domain due to their ability to handle lengthy, complex sequences. Moreover, the emergence of domain-specific LLMs has displayed extremely promising results on various tasks. In this study, we aim to quantify how general LLMs perform in comparison to legal-domain models (be it an LLM or otherwise). Specifically, we compare the zero-shot performance of three general-purpose LLMs (ChatGPT-3.5, LLaMA-70b and Falcon-180b) on the LEDGAR subset of the LexGLUE benchmark for contract provision classification. Although the LLMs were not explicitly trained on legal data, we observe that they are still able to classify the theme correctly in most cases. However, we find that their mic-F1/mac-F1 performance are upto 19.2/26.8% lesser than smaller models fine-tuned on the legal domain, thus underscoring the need for more powerful legal-domain LLMs.</abstract>
      <url hash="19358e0d">2023.nllp-1.22</url>
      <bibkey>jayakumar-etal-2023-large</bibkey>
      <doi>10.18653/v1/2023.nllp-1.22</doi>
    </paper>
    <paper id="23">
      <title>On the Potential and Limitations of Few-Shot In-Context Learning to Generate Metamorphic Specifications for Tax Preparation Software</title>
      <author><first>Dananjay</first><last>Srinivas</last><affiliation>University of Colorado, Boulder</affiliation></author>
      <author><first>Rohan</first><last>Das</last><affiliation>University of Colorado, Boulder</affiliation></author>
      <author><first>Saeid</first><last>Tizpaz-Niari</last><affiliation>University of Texas, El Paso</affiliation></author>
      <author><first>Ashutosh</first><last>Trivedi</last><affiliation>University of Colorado, Boulder</affiliation></author>
      <author><first>Maria Leonor</first><last>Pacheco</last><affiliation>University of Colorado Boulder / Microsoft Research</affiliation></author>
      <pages>230-243</pages>
      <abstract>Due to the ever-increasing complexity of income tax laws in the United States, the number of US taxpayers filing their taxes using tax preparation software henceforth, tax software) continues to increase. According to the U.S. Internal Revenue Service (IRS), in FY22, nearly 50% of taxpayers filed their individual income taxes using tax software. Given the legal consequences of incorrectly filing taxes for the taxpayer, ensuring the correctness of tax software is of paramount importance. Metamorphic testing has emerged as a leading solution to test and debug legal-critical tax software due to the absence of correctness requirements and trustworthy datasets. The key idea behind metamorphic testing is to express the properties of a system in terms of the relationship between one input and its slightly metamorphosed twinned input. Extracting metamorphic properties from IRS tax publications is a tedious and time-consuming process. As a response, this paper formulates the task of generating metamorphic specifications as a translation task between properties extracted from tax documents - expressed in natural language - to a contrastive first-order logic form. We perform a systematic analysis on the potential and limitations of in-context learning with Large Language Models (LLMs) for this task, and outline a research agenda towards automating the generation of metamorphic specifications for tax preparation software.</abstract>
      <url hash="a57f86c8">2023.nllp-1.23</url>
      <bibkey>srinivas-etal-2023-potential</bibkey>
      <doi>10.18653/v1/2023.nllp-1.23</doi>
    </paper>
    <paper id="24">
      <title><fixed-case>A</fixed-case>sy<fixed-case>L</fixed-case>ex: A Dataset for Legal Language Processing of Refugee Claims</title>
      <author><first>Claire</first><last>Barale</last><affiliation>The University of Edinburgh</affiliation></author>
      <author><first>Mark</first><last>Klaisoongnoen</last><affiliation>EPCC at the University of Edinburgh</affiliation></author>
      <author><first>Pasquale</first><last>Minervini</last><affiliation>UCL</affiliation></author>
      <author><first>Michael</first><last>Rovatsos</last><affiliation>The University of Edinburgh</affiliation></author>
      <author><first>Nehal</first><last>Bhuta</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>244-257</pages>
      <abstract>Advancements in natural language processing (NLP) and language models have demonstrated immense potential in the legal domain, enabling automated analysis and comprehension of legal texts. However, developing robust models in Legal NLP is significantly challenged by the scarcity of resources. This paper presents AsyLex, the first dataset specifically designed for Refugee Law applications to address this gap. The dataset introduces 59,112 documents on refugee status determination in Canada from 1996 to 2022, providing researchers and practitioners with essential material for training and evaluating NLP models for legal research and case review. Case review is defined as entity extraction and outcome prediction tasks. The dataset includes 19,115 gold-standard human-labeled annotations for 20 legally relevant entity types curated with the help of legal experts and 1,682 gold-standard labeled documents for the case outcome. Furthermore, we supply the corresponding trained entity extraction models and the resulting labeled entities generated through the inference process on AsyLex. Four supplementary features are obtained through rule-based extraction. We demonstrate the usefulness of our dataset on the legal judgment prediction task to predict the binary outcome and test a set of baselines using the text of the documents and our annotations. We observe that models pretrained on similar legal documents reach better scores, suggesting that acquiring more datasets for specialized domains such as law is crucial.</abstract>
      <url hash="d055f921">2023.nllp-1.24</url>
      <bibkey>barale-etal-2023-asylex</bibkey>
      <doi>10.18653/v1/2023.nllp-1.24</doi>
    </paper>
    <paper id="25">
      <title>A Comparative Study of Prompting Strategies for Legal Text Classification</title>
      <author><first>Ali</first><last>Hakimi Parizi</last><affiliation>University of New Brunswick</affiliation></author>
      <author><first>Yuyang</first><last>Liu</last><affiliation>Thomson Reuters</affiliation></author>
      <author><first>Prudhvi</first><last>Nokku</last><affiliation>Thomson Reuters</affiliation></author>
      <author><first>Sina</first><last>Gholamian</last><affiliation>Thomson Reuters</affiliation></author>
      <author><first>David</first><last>Emerson</last><affiliation>Vector Institute</affiliation></author>
      <pages>258-265</pages>
      <abstract>In this study, we explore the performance oflarge language models (LLMs) using differ-ent prompt engineering approaches in the con-text of legal text classification. Prior researchhas demonstrated that various prompting tech-niques can improve the performance of a di-verse array of tasks done by LLMs. However,in this research, we observe that professionaldocuments, and in particular legal documents,pose unique challenges for LLMs. We experi-ment with several LLMs and various promptingtechniques, including zero/few-shot prompting,prompt ensembling, chain-of-thought, and ac-tivation fine-tuning and compare the perfor-mance on legal datasets. Although the newgeneration of LLMs and prompt optimizationtechniques have been shown to improve gener-ation and understanding of generic tasks, ourfindings suggest that such improvements maynot readily transfer to other domains. Specifi-cally, experiments indicate that not all prompt-ing approaches and models are well-suited forthe legal domain which involves complexitiessuch as long documents and domain-specificlanguage.</abstract>
      <url hash="05a254cb">2023.nllp-1.25</url>
      <bibkey>hakimi-parizi-etal-2023-comparative</bibkey>
      <doi>10.18653/v1/2023.nllp-1.25</doi>
      <video href="2023.nllp-1.25.mp4"/>
    </paper>
    <paper id="26">
      <title>Tracing Influence at Scale: A Contrastive Learning Approach to Linking Public Comments and Regulator Responses</title>
      <author><first>Linzi</first><last>Xing</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Brad</first><last>Hackinen</last><affiliation>Ivey Business School</affiliation></author>
      <author><first>Giuseppe</first><last>Carenini</last><affiliation>university of british columbia</affiliation></author>
      <pages>266-274</pages>
      <abstract>U.S. Federal Regulators receive over one million comment letters each year from businesses, interest groups, and members of the public, all advocating for changes to proposed regulations. These comments are believed to have wide-ranging impacts on public policy. However, measuring the impact of specific comments is challenging because regulators are required to respond to comments but they do not have to specify which comments they are addressing. In this paper, we propose a simple yet effective solution to this problem by using an iterative contrastive method to train a neural model aiming for matching text from public comments to responses written by regulators. We demonstrate that our proposal substantially outperforms a set of selected text-matching baselines on a human-annotated test set. Furthermore, it delivers performance comparable to the most advanced gigantic language model (i.e., GPT-4), and is more cost-effective when handling comments and regulator responses matching in larger scale.</abstract>
      <url hash="bcaec96b">2023.nllp-1.26</url>
      <bibkey>xing-etal-2023-tracing</bibkey>
      <doi>10.18653/v1/2023.nllp-1.26</doi>
    </paper>
  </volume>
</collection>
