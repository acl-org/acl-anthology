<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.queerinai">
  <volume id="main" ingest-date="2025-04-26" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Queer in AI Workshop</booktitle>
      <editor><first>A</first><last>Pranav</last><affiliation>Data Science Group, University of Hamburg</affiliation></editor>
      <editor><first>Alissa</first><last>Valentine</last><affiliation>Mount Sinai, NYC</affiliation></editor>
      <editor><first>Shaily</first><last>Bhatt</last><affiliation>Carnegie Mellon University</affiliation></editor>
      <editor><first>Yanan</first><last>Long</last><affiliation>University of Chicago</affiliation></editor>
      <editor><first>Arjun</first><last>Subramonian</last><affiliation>University of California, Los Angeles</affiliation></editor>
      <editor><first>Amanda</first><last>Bertsch</last><affiliation>Carnegie Mellon University</affiliation></editor>
      <editor><first>Anne</first><last>Lauscher</last><affiliation>Data Science Group, University of Hamburg</affiliation></editor>
      <editor><first>Ankush</first><last>Gupta</last><affiliation>IIIT Delhi</affiliation></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Hybrid format (in-person and virtual)</address>
      <month>May</month>
      <year>2025</year>
      <url hash="72e66610">2025.queerinai-main</url>
      <venue>queerinai</venue>
      <venue>ws</venue>
      <isbn>979-8-89176-244-2</isbn>
      <doi>10.18653/v1/2025.queerinai-main</doi>
    </meta>
    <frontmatter>
      <url hash="d02d1572">2025.queerinai-main.0</url>
      <bibkey>queerinai-ws-2025-main</bibkey>
      <doi>10.18653/v1/2025.queerinai-main.0</doi>
    </frontmatter>
    <paper id="1">
      <title>Studying the Representation of the <fixed-case>LGBTQ</fixed-case>+ Community in <fixed-case>R</fixed-case>u<fixed-case>P</fixed-case>aul’s Drag Race with <fixed-case>LLM</fixed-case>-Based Topic Modeling</title>
      <author><first>Mika</first><last>Hämäläinen</last><affiliation>Metropolia University of Applied Sciences</affiliation></author>
      <pages>1-5</pages>
      <abstract>This study investigates the representation of LGBTQ+ community in the widely acclaimed reality television series RuPaul’s Drag Race through a novel application of large language model (LLM)-based topic modeling. By analyzing subtitles from seasons 1 to 16, the research identifies a spectrum of topics ranging from empowering themes, such as self-expression through drag, community support and positive body image, to challenges faced by the LGBTQ+ community, including homophobia, HIV and mental health. Employing an LLM allowed for nuanced exploration of these themes, overcoming the limitations of traditional word-based topic modeling.</abstract>
      <url hash="b94156a2">2025.queerinai-main.1</url>
      <bibkey>hamalainen-2025-studying</bibkey>
      <doi>10.18653/v1/2025.queerinai-main.1</doi>
    </paper>
    <paper id="2">
      <title>Guardrails, not Guidance: Understanding Responses to <fixed-case>LGBTQ</fixed-case>+ Language in Large Language Models</title>
      <author><first>Joshua</first><last>Tint</last></author>
      <pages>6-16</pages>
      <abstract>Language models have integrated themselves into many aspects of digital life, shaping everything from social media to translation. This paper investigates how large language models (LLMs) respond to LGBTQ+ slang and heteronormative language. Through two experiments, the study assesses the emotional content and the impact of queer slang on responses from models including GPT-3.5, GPT-4o, Llama2, Llama3, Gemma and Mistral. The findings reveal that heteronormative prompts can trigger safety mechanisms, leading to neutral or corrective responses, while LGBTQ+ slang elicits more negative emotions. These insights punctuate the need to provide equitable outcomes for minority slangs and argots, in addition to eliminating explicit bigotry from language models.</abstract>
      <url hash="77cdebba">2025.queerinai-main.2</url>
      <bibkey>tint-2025-guardrails</bibkey>
      <doi>10.18653/v1/2025.queerinai-main.2</doi>
    </paper>
    <paper id="3">
      <title>Dehumanization of <fixed-case>LGBTQ</fixed-case>+ Groups in Sexual Interactions with <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case></title>
      <author><first>Alexandria</first><last>Leto</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <author><first>Juan</first><last>Vásquez</last></author>
      <author><first>Alexis</first><last>Palmer</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <author><first>Maria Leonor</first><last>Pacheco</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <pages>17-25</pages>
      <abstract>Given the widespread use of LLM-powered conversational agents such as ChatGPT, analyzing the ways people interact with them could provide valuable insights into human behavior. Prior work has shown that these agents are sometimes used in sexual contexts, such as to obtain advice, to role-play as sexual companions, or to generate erotica. While LGBTQ+ acceptance has increased in recent years, dehumanizing practices against minorities continue to prevail. In this paper, we hone in on this and perform an analysis of dehumanizing tendencies toward LGBTQ+ individuals by human users in their sexual interactions with ChatGPT. Through a series of experiments that model various concept vectors associated with distinct shades of dehumanization, we find evidence of the reproduction of harmful stereotypes. However, many user prompts lack indications of dehumanization, suggesting that the use of these agents is a complex and nuanced issue which warrants further investigation.</abstract>
      <url hash="83de7042">2025.queerinai-main.3</url>
      <bibkey>leto-etal-2025-dehumanization</bibkey>
      <doi>10.18653/v1/2025.queerinai-main.3</doi>
    </paper>
    <paper id="4">
      <title>Leveraging Large Language Models in Detecting Anti-<fixed-case>LGBTQIA</fixed-case>+ User-generated Texts</title>
      <author><first>Quoc-Toan</first><last>Nguyen</last></author>
      <author><first>Josh</first><last>Nguyen</last><affiliation>NA</affiliation></author>
      <author><first>Tuan</first><last>Pham</last><affiliation>State University of New York at Binghamton</affiliation></author>
      <author><first>William John</first><last>Teahan</last></author>
      <pages>26-34</pages>
      <abstract>Anti-LGBTQIA+ texts in user-generated content pose significant risks to online safety and inclusivity. This study investigates the capabilities and limitations of five widely adopted Large Language Models (LLMs)—DeepSeek-V3, GPT-4o, GPT-4o-mini, GPT-o1-mini, and Llama3.3-70B—in detecting such harmful content. Our findings reveal that while LLMs demonstrate potential in identifying offensive language, their effectiveness varies across models and metrics, with notable shortcomings in calibration. Furthermore, linguistic analysis exposes deeply embedded patterns of discrimination, reinforcing the urgency for improved detection mechanisms for this marginalised population. In summary, this study demonstrates the significant potential of LLMs for practical application in detecting anti-LGBTQIA+ user-generated texts and provides valuable insights from text analysis that can inform topic modelling. These findings contribute to developing safer digital platforms and enhancing protection for LGBTQIA+ individuals.</abstract>
      <url hash="f33e81ab">2025.queerinai-main.4</url>
      <bibkey>nguyen-etal-2025-leveraging</bibkey>
      <doi>10.18653/v1/2025.queerinai-main.4</doi>
    </paper>
    <paper id="5">
      <title>A <fixed-case>B</fixed-case>ayesian account of pronoun and neopronoun acquisition</title>
      <author><first>Cassandra L</first><last>Jacobs</last><affiliation>State University of New York, Buffalo</affiliation></author>
      <author><first>Morgan</first><last>Grobol</last><affiliation>Université Paris Nanterre</affiliation></author>
      <pages>35-40</pages>
      <abstract>A major challenge to equity among members of queer communities is the use of one’s chosen forms of reference, such as personal names or pronouns. Speakers often dimiss errors in pronominal use as unintentional, and claim that their errors reflect many decades of fossilized mainstream language use, including attitudes or expectations about the relationship between one’s appearance and acceptable forms of reference. Here, we propose a modeling framework that allows language use and speech communities to change over time, including the adoption of neopronouns and other forms for self-reference. We present a probabilistic graphical modeling approach to pronominal reference that is flexible in the face of change and experience while also moving beyond form-to-meaning mappings. The model critically also does not rely on lexical covariance structure to learn referring expressions. We show that such a model can account for individual differences in how quickly pronouns or names are integrated into symbolic knowledge and can empower computational systems to be both flexible and respectful of queer people with diverse gender expression.</abstract>
      <url hash="8132486f">2025.queerinai-main.5</url>
      <bibkey>jacobs-grobol-2025-bayesian</bibkey>
      <doi>10.18653/v1/2025.queerinai-main.5</doi>
    </paper>
  </volume>
</collection>
