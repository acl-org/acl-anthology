<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.winlp">
  <volume id="main" ingest-date="2025-10-28" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 9th Widening NLP Workshop</booktitle>
      <editor id="chen-zhang"><first>Chen</first><last>Zhang</last></editor>
      <editor><first>Emily</first><last>Allaway</last></editor>
      <editor><first>Hua</first><last>Shen</last></editor>
      <editor><first>Lesly</first><last>Miculicich</last></editor>
      <editor><first>Yinqiao</first><last>Li</last></editor>
      <editor><first>Meryem</first><last>M'hamdi</last></editor>
      <editor><first>Peerat</first><last>Limkonchotiwat</last></editor>
      <editor><first>Richard He</first><last>Bai</last></editor>
      <editor><first>Santosh</first><last>T.y.s.s.</last></editor>
      <editor><first>Sophia Simeng</first><last>Han</last></editor>
      <editor><first>Surendrabikram</first><last>Thapa</last></editor>
      <editor><first>Wiem Ben</first><last>Rim</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Suzhou, China</address>
      <month>November</month>
      <year>2025</year>
      <url hash="b4958c00">2025.winlp-main</url>
      <venue>winlp</venue>
      <venue>ws</venue>
      <isbn>979-8-89176-351-7</isbn>
    </meta>
    <frontmatter>
      <url hash="a121c32f">2025.winlp-main.0</url>
      <bibkey>winlp-ws-2025-main</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Seeing Symbols, Missing Cultures: Probing Vision-Language Models’ Reasoning on Fire Imagery and Cultural Meaning</title>
      <author orcid="0009-0001-2087-8879"><first>Haorui</first><last>Yu</last><affiliation>University of Dundee</affiliation></author>
      <author orcid="0009-0006-8952-1380"><first>Yang</first><last>Zhao</last></author>
      <author orcid="0009-0003-0414-7819"><first>Yijia</first><last>Chu</last></author>
      <author orcid="0000-0002-5611-5769"><first>Qiufeng</first><last>Yi</last></author>
      <pages>1-8</pages>
      <abstract>Vision-Language Models (VLMs) often appearculturally competent but rely on superficial pat.tern matching rather than genuine cultural understanding. We introduce a diagnostic framework to probe VLM reasoning on fire-themedcultural imagery through both classification andexplanation analysis. Testing multiple modelson Western festivals, non-Western traditions.and emergency scenes reveals systematic biases: models correctly identify prominent Western festivals but struggle with underrepresentedcultural events, frequently offering vague labelsor dangerously misclassifying emergencies ascelebrations. These failures expose the risksof symbolic shortcuts and highlight the needfor cultural evaluation beyond accuracy metrics to ensure interpretable and fair multimodalsystems.</abstract>
      <url hash="706afa4e">2025.winlp-main.1</url>
      <bibkey>yu-etal-2025-seeing</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>GPT</fixed-case>4<fixed-case>AMR</fixed-case>: Does <fixed-case>LLM</fixed-case>-based Paraphrasing Improve <fixed-case>AMR</fixed-case>-to-text Generation Fluency?</title>
      <author><first>Jiyuan</first><last>Ji</last></author>
      <author orcid="0000-0002-1062-0866"><first>Shira</first><last>Wein</last><affiliation>Amherst College</affiliation></author>
      <pages>9-18</pages>
      <abstract>Abstract Meaning Representation (AMR) is a graph-based semantic representation that has been incorporated into numerous downstream tasks, in particular due to substantial efforts developing text-to-AMR parsing and AMR-to-text generation models. However, there still exists a large gap between fluent, natural sentences and texts generated from AMR-to-text generation models. Prompt-based Large Language Models (LLMs), on the other hand, have demonstrated an outstanding ability to produce fluent text in a variety of languages and domains. In this paper, we investigate the extent to which LLMs can improve the AMR-to-text generated output fluency post-hoc via prompt engineering. We conduct automatic and human evaluations of the results, and ultimately have mixed findings: LLM-generated paraphrases generally do not exhibit improvement in automatic evaluation, but outperform baseline texts according to our human evaluation. Thus, we provide a detailed error analysis of our results to investigate the complex nature of generating highly fluent text from semantic representations.</abstract>
      <url hash="aa8640d9">2025.winlp-main.2</url>
      <bibkey>ji-wein-2025-gpt4amr</bibkey>
    </paper>
    <paper id="3">
      <title>Probing Gender Bias in Multilingual <fixed-case>LLM</fixed-case>s: A Case Study of Stereotypes in <fixed-case>P</fixed-case>ersian</title>
      <author orcid="0000-0002-6153-9048"><first>Ghazal</first><last>Kalhor</last><affiliation>University of Tehran, University of Tehran</affiliation></author>
      <author orcid="0000-0003-4429-2511"><first>Behnam</first><last>Bahrak</last><affiliation>Tehran Institute for Advanced Studies</affiliation></author>
      <pages>19-27</pages>
      <abstract>Multilingual Large Language Models (LLMs) are increasingly used worldwide, making it essential to ensure they are free from gender bias to prevent representational harm. While prior studies have examined such biases in high-resource languages, low-resource languages remain understudied. In this paper, we propose a template-based probing methodology, validated against real-world data, to uncover gender stereotypes in LLMs. As part of this framework, we introduce the Domain-Specific Gender Skew Index (DS-GSI), a metric that quantifies deviations from gender parity. We evaluate four prominent models, GPT-4o mini, DeepSeek R1, Gemini 2.0 Flash, and Qwen QwQ 32B, across four semantic domains, focusing on Persian, a low-resource language with distinct linguistic features. Our results show that all models exhibit gender stereotypes, with greater disparities in Persian than in English across all domains. Among these, sports reflect the most rigid gender biases. This study underscores the need for inclusive NLP practices and provides a framework for assessing bias in other low-resource languages.</abstract>
      <url hash="d6ce85e4">2025.winlp-main.3</url>
      <bibkey>kalhor-bahrak-2025-probing</bibkey>
    </paper>
    <paper id="7">
      <title>Whose <fixed-case>P</fixed-case>alestine Is It? A Topic Modelling Approach to National Framing in Academic Research</title>
      <author><first>Maida</first><last>Aizaz</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Taegyoon</first><last>Kim</last><affiliation>KAIST</affiliation></author>
      <author orcid="0000-0002-7381-4959"><first>Lanu</first><last>Kim</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <pages>28-40</pages>
      <abstract>In this study, we investigate how author affiliation shapes academic discourse, proposing it as an effective proxy for author perspective in understanding what topics are studied, how nations are framed, and whose realities are prioritised. Using Palestine as a case study, we apply BERTopic and Structural Topic Modelling (STM) to 29,536 English-language academic articles collected from the OpenAlex database. We find that domestic authors focus on practical, local issues like healthcare, education, and the environment, while foreign authors emphasise legal, historical, and geopolitical discussions. These differences, in our interpretation, reflect lived proximity to war and crisis. We also note that while BERTopic captures greater lexical nuance, STM enables covariate-aware comparisons, offering deeper insight into how affiliation correlates with thematic emphasis. We propose extending this framework to other underrepresented countries, including a future study focused on Gaza post-October 7.</abstract>
      <url hash="22e81fb7">2025.winlp-main.7</url>
      <bibkey>aizaz-etal-2025-whose</bibkey>
    </paper>
    <paper id="8">
      <title>Fine-tuning <fixed-case>XLM</fixed-case>-<fixed-case>R</fixed-case>o<fixed-case>BERT</fixed-case>a for Named Entity Recognition in <fixed-case>K</fixed-case>urmanji <fixed-case>K</fixed-case>urdish</title>
      <author><first>Akam</first><last>Nawzad</last></author>
      <author orcid="0000-0002-8899-4016"><first>Hossein</first><last>Hassani</last><affiliation>University of Kurdistan Hewlêr</affiliation></author>
      <pages>41-45</pages>
      <abstract>Named Entity Recognition (NER) is the information extraction task of identifying predefined named entities such as person names, location names, organization names and more. High-resource languages have made significant progress in NER tasks. However, low-resource languages such as Kurmanji Kurdish have not seen the same advancements, due to these languages having less available data online. This research aims to close this gap by developing an NER system via fine-tuning XLM-RoBERTa on a manually annotated dataset for Kurmanji. The dataset used for fine-tuning consists of 7,919 annotated sentences, which were manually annotated by three native Kurmanji speakers. The classes labeled in the dataset are Person (PER), Organization (ORG), and Location (LOC). A web-based application has also been developed using Streamlit to make the model more accessible. The model achieved an F1 score of 0.8735, precision of 0.8668, and recall of 0.8803, demonstrating the effectiveness of fine-tuning transformer-based models for NER tasks in low-resource languages. This work establishes a methodology that can be applied to other low-resource languages and Kurdish varieties.</abstract>
      <url hash="cd5b7b74">2025.winlp-main.8</url>
      <bibkey>hassani-2025-fine</bibkey>
    </paper>
    <paper id="10">
      <title>Human-<fixed-case>AI</fixed-case> Moral Judgment Congruence on Real-World Scenarios: A Cross-Lingual Analysis</title>
      <author orcid="0000-0002-9963-7794"><first>Nan</first><last>Li</last><affiliation>Universiteit Gent</affiliation></author>
      <author orcid="0000-0002-9895-9927"><first>Bo</first><last>Kang</last><affiliation>Ghent University</affiliation></author>
      <author orcid="0000-0002-2692-7504"><first>Tijl</first><last>De Bie</last><affiliation>Ghent University</affiliation></author>
      <pages>46-49</pages>
      <abstract>As Large Language Models (LLMs) are deployed in every aspect of our lives, understanding how they reason about moral issues becomes critical for AI safety. We investigate this using a dataset we curated from Reddit’s r/AmItheAsshole, comprising real-world moral dilemmas with crowd-sourced verdicts. Through experiments on five state-of-the-art LLMs across 847 posts, we find a significant and systematic divergence where LLMs are more lenient than humans. Moreover, we find that translating the posts into another language changes LLMs’ verdicts, indicating their judgments lack cross-lingual stability.</abstract>
      <url hash="f79f2222">2025.winlp-main.10</url>
      <bibkey>li-etal-2025-human</bibkey>
    </paper>
    <paper id="12">
      <title>Transfer learning for dependency parsing of <fixed-case>V</fixed-case>edic <fixed-case>S</fixed-case>anskrit</title>
      <author><first>Abhiram</first><last>Vinjamuri</last></author>
      <author><first>Weiwei</first><last>Sun</last><affiliation>University of Cambridge</affiliation></author>
      <pages>50-55</pages>
      <abstract>This paper focuses on data-driven dependency parsing for Vedic Sanskrit. We propose and evaluate a transfer learning approach that benefits from syntactic analysis of typologically related languages, including Ancient Greek and Latin, and a descendant language - Classical Sanskrit. Experiments on the Vedic TreeBank demonstrate the effectiveness of cross-lingual transfer, demonstrating improvements from the biaffine baseline as well as outperforming the current state of the art benchmark, the deep contextualised self-training algorithm, across a wide range of experimental setups.</abstract>
      <url hash="c5ae9809">2025.winlp-main.12</url>
      <bibkey>vinjamuri-sun-2025-transfer</bibkey>
    </paper>
    <paper id="13">
      <title>Debiasing Large Language Models in <fixed-case>T</fixed-case>hai Political Stance Detection via Counterfactual Calibration</title>
      <author><first>Kasidit</first><last>Sermsri</last></author>
      <author orcid="0000-0001-8464-4476"><first>Teerapong</first><last>Panboonyuen</last><affiliation>MARSAIL (Motor AI Recognition Solution Artificial Intelligence Laboratory) and Chulalongkorn University</affiliation></author>
      <pages>56-64</pages>
      <abstract>Political stance detection in low-resource and culturally complex settings poses a critical challenge for large language models (LLMs). In the Thai political landscape—rich with indirect expressions, polarized figures, and sentiment-stance entanglement—LLMs often exhibit systematic biases, including sentiment leakage and entity favoritism. These biases not only compromise model fairness but also degrade predictive reliability in real-world applications. We introduce ThaiFACTUAL, a lightweight, model-agnostic calibration framework that mitigates political bias without fine-tuning LLMs. ThaiFACTUAL combines counterfactual data augmentation with rationale-based supervision to disentangle sentiment from stance and neutralize political preferences. We curate and release the first high-quality Thai political stance dataset with stance, sentiment, rationale, and bias markers across diverse political entities and events. Our results show that ThaiFACTUAL substantially reduces spurious correlations, improves zero-shot generalization, and enhances fairness across multiple LLMs. This work underscores the need for culturally grounded bias mitigation and offers a scalable blueprint for debiasing LLMs in politically sensitive, underrepresented languages.</abstract>
      <url hash="78a70adf">2025.winlp-main.13</url>
      <bibkey>sermsri-panboonyuen-2025-debiasing</bibkey>
    </paper>
    <paper id="14">
      <title><fixed-case>ECCC</fixed-case>: Edge Code Cloak Coder for Privacy Code Agent</title>
      <author orcid="0009-0007-8646-8593"><first>Haoqi</first><last>He</last></author>
      <author><first>Wenzhi</first><last>Xu</last></author>
      <author><first>Ruoying</first><last>Liu</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Jiarui</first><last>Tang</last><affiliation>Chengdu University</affiliation></author>
      <author><first>Bairu</first><last>Li</last></author>
      <author><first>Xiaokai</first><last>Lin</last></author>
      <pages>65-74</pages>
      <abstract>Large language models (LLMs) have significantly advanced automated code generation and debugging, facilitating powerful multi-agent coding frameworks. However, deploying these sophisticated models on resource-constrained edge devices remains challenging due to high computational demands, limited adaptability, and significant privacy risks associated with cloud-based processing. Motivated by these constraints, we propose <b>Edge Code Cloak Coder (ECCC)</b>, a novel edge-cloud hybrid framework integrating lightweight quantized LLM with robust AST-based anonymization and edge-side privacy validation. ECCC enables high-performance, privacy-preserving LLM capabilities on consumer GPUs, anonymizing user code before securely delegating abstracted tasks to cloud LLMs. Experimental evaluations demonstrate that ECCC achieves competitive correctness (within 4–5pp of the GPT-4-based frameworks) and a perfect privacy score of 10/10, effectively balancing functionality and security for sensitive and proprietary code applications.</abstract>
      <url hash="160ecfb1">2025.winlp-main.14</url>
      <bibkey>he-etal-2025-eccc</bibkey>
    </paper>
    <paper id="15">
      <title><fixed-case>V</fixed-case>alue<fixed-case>C</fixed-case>ompass: A Framework for Measuring Contextual Value Alignment Between Human and <fixed-case>LLM</fixed-case>s</title>
      <author orcid="0000-0002-4928-525X"><first>Hua</first><last>Shen</last></author>
      <author orcid="0000-0003-4928-6225"><first>Tiffany</first><last>Knearem</last></author>
      <author><first>Reshmi</first><last>Ghosh</last><affiliation>Microsoft</affiliation></author>
      <author orcid="0009-0005-1989-0044"><first>Yu-Ju</first><last>Yang</last></author>
      <author><first>Nicholas</first><last>Clark</last><affiliation>University of Washington</affiliation></author>
      <author><first>Tanu</first><last>Mitra</last><affiliation>University of Washington</affiliation></author>
      <author><first>Yun</first><last>Huang</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <pages>75-86</pages>
      <abstract>As AI advances, aligning it with diverse human and societal values grows critical. But how do we define these values and measure AI’s adherence to them? We present ValueCompass, a framework grounded in psychological theories, to assess human-AI alignment. Applying it to five diverse LLMs and 112 humans from seven countries across four scenarios—collaborative writing, education, public sectors, and healthcare—we uncover key misalignments. For example, humans prioritize national security, while LLMs often reject it. Values also shift across contexts, demanding scenario-specific alignment strategies. This work advances AI design by mapping how systems can better reflect societal ethics.</abstract>
      <url hash="12fdafb2">2025.winlp-main.15</url>
      <bibkey>shen-etal-2025-valuecompass</bibkey>
    </paper>
    <paper id="16">
      <title><fixed-case>ASR</fixed-case> Under Noise: Exploring Robustness for <fixed-case>S</fixed-case>undanese and <fixed-case>J</fixed-case>avanese</title>
      <author><first>Salsabila Zahirah</first><last>Pranida</last></author>
      <author><first>Rifo Ahmad</first><last>Genadi</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Muhammad Cendekia</first><last>Airlangga</last></author>
      <author orcid="0000-0002-3258-6734"><first>Shady</first><last>Shehata</last><affiliation>University of Waterloo</affiliation></author>
      <pages>87-99</pages>
      <abstract>We investigate the robustness of Whisper-based automatic speech recognition (ASR) models for two major Indonesian regional languages: Javanese and Sundanese. While recent work has demonstrated strong ASR performance under clean conditions, their effectiveness in noisy environments remains unclear. To address this, we experiment with multiple training strategies, including synthetic noise augmentation and SpecAugment, and evaluate performance across a range of signal-to-noise ratios (SNRs). Our results show that noise-aware training substantially improves robustness, particularly for larger Whisper models. A detailed error analysis further reveals language-specific challenges, highlighting avenues for future improvements.</abstract>
      <url hash="5eae5ed2">2025.winlp-main.16</url>
      <bibkey>pranida-etal-2025-asr</bibkey>
    </paper>
    <paper id="17">
      <title>A Simple Data Augmentation Strategy for Text-in-Image Scientific <fixed-case>VQA</fixed-case></title>
      <author><first>Belal</first><last>Shoer</last></author>
      <author orcid="0009-0000-7465-5702"><first>Yova</first><last>Kementchedjhieva</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>100-105</pages>
      <abstract>Scientific visual question answering poses significant challenges for vision-language models due to the complexity of scientific figures and their multimodal context. Traditional approaches treat the figure and accompanying text (e.g., questions and answer options) as separate inputs. EXAMS-V introduced a new paradigm by embedding both visual and textual content into a single image. However, even state-of-the-art proprietary models perform poorly on this setup in zero-shot settings, underscoring the need for task-specific fine-tuning. To address the scarcity of training data in this “text-in-image” format, we synthesize a new dataset by converting existing separate image-text pairs into unified images. Fine-tuning a small multilingual multimodal model on a mix of our synthetic data and EXAMS-V yields notable gains across 13 languages, demonstrating strong average improvements and cross-lingual transfer.</abstract>
      <url hash="dd9a87f3">2025.winlp-main.17</url>
      <bibkey>shoer-kementchedjhieva-2025-simple</bibkey>
    </paper>
    <paper id="19">
      <title>Hybrid Fact-Checking that Integrates Knowledge Graphs, Large Language Models, and Search-Based Retrieval Agents Improves Interpretable Claim Verification</title>
      <author><first/><last>Shaghayeghkolli</last></author>
      <author orcid="0009-0006-0364-388X"><first>Richard</first><last>Rosenbaum</last></author>
      <author><first>Timo</first><last>Cavelius</last></author>
      <author orcid="0009-0004-4023-5776"><first>Lasse</first><last>Strothe</last></author>
      <author><first>Andrii</first><last>Lata</last><affiliation>NA</affiliation></author>
      <author orcid="0000-0001-8183-7109"><first>Jana</first><last>Diesner</last><affiliation>Technische Universität München</affiliation></author>
      <pages>106-115</pages>
      <abstract>Large language models (LLMs) excel in generating fluent utterances but can lack reliable grounding in verified information. At the same time, knowledge-graph-based fact-checkers deliver precise and interpretable evidence, yet suffer from limited coverage or latency. By integrating LLMs with knowledge graphs and real-time search agents, we introduce a hybrid fact-checking approach that leverages the individual strengths of each component. Our system comprises three autonomous steps: 1) a Knowledge Graph (KG) Retrieval for rapid one‐hop lookups in DBpedia, 2) an LM-based classification guided by a task-specific labeling prompt, producing outputs with internal rule-based logic, and 3) a Web Search Agent invoked only when KG coverage is insufficient. Our pipeline achieves an F1 score of 0.93 on the FEVER benchmark on the Supported/Refuted split without task‐specific fine‐tuning. To address Not enough information cases, we conduct a targeted reannotation study showing that our approach frequently uncovers valid evidence for claims originally labeled as Not Enough Information (NEI), as confirmed by both expert annotators and LLM reviewers. With this paper, we present a modular, open-source fact-checking pipeline with fallback strategies and generalization across datasets.</abstract>
      <url hash="b6bc5203">2025.winlp-main.19</url>
      <bibkey>shaghayeghkolli-etal-2025-hybrid</bibkey>
    </paper>
    <paper id="20">
      <title>Insights from a Disaggregated Analysis of Kinds of Biases in a Multicultural Dataset</title>
      <author orcid="0009-0006-3248-1461"><first>Guido</first><last>Ivetta</last><affiliation>Universidad Nacional de Córdoba</affiliation></author>
      <author><first>Hernán</first><last>Maina</last><affiliation>Universidad Nacional de Córdoba, Argentina</affiliation></author>
      <author orcid="0000-0001-7456-4333"><first>Luciana</first><last>Benotti</last><affiliation>Universidad nacional de Córdoba</affiliation></author>
      <pages>116-122</pages>
      <abstract>Warning: This paper contains explicit statements of offensive stereotypes which may be upsetting.Stereotypes vary across cultural contexts, making it essential to understand how language models encode social biases. MultiLingualCrowsPairs is a dataset of culturally adapted stereotypical and anti-stereotypical sentence pairs across nine languages. While prior work has primarily reported average fairness metrics on masked language models, this paper analyzes social biases in generative models by disaggregating results across specific bias types.We find that although most languages show an overall preference for stereotypical sentences, this masks substantial variation across different types of bias, such as gender, religion, and socioeconomic status. Our findings underscore that relying solely on aggregated metrics can obscure important patterns, and that fine-grained, bias-specific analysis is critical for meaningful fairness evaluation.</abstract>
      <url hash="a0028dc1">2025.winlp-main.20</url>
      <bibkey>ivetta-etal-2025-insights</bibkey>
    </paper>
    <paper id="21">
      <title>That Ain’t Right: Assessing <fixed-case>LLM</fixed-case> Performance on <fixed-case>QA</fixed-case> in <fixed-case>A</fixed-case>frican <fixed-case>A</fixed-case>merican and <fixed-case>W</fixed-case>est <fixed-case>A</fixed-case>frican <fixed-case>E</fixed-case>nglish Dialects</title>
      <author orcid="0009-0009-7850-7489"><first>William</first><last>Coggins</last><affiliation>University of Florida</affiliation></author>
      <author orcid="0000-0002-9518-6025"><first>Jasmine</first><last>McKenzie</last><affiliation>University of Florida</affiliation></author>
      <author orcid="0000-0001-7234-0395"><first>Sangpil</first><last>Youm</last><affiliation>University of Florida</affiliation></author>
      <author><first>Pradham</first><last>Mummaleti</last></author>
      <author><first>Juan</first><last>Gilbert</last><affiliation>University of Florida, University of Florida and Clemson University</affiliation></author>
      <author orcid="0000-0002-7192-3457"><first>Eric</first><last>Ragan</last><affiliation>University of Florida</affiliation></author>
      <author orcid="0000-0003-4356-5813"><first>Bonnie J</first><last>Dorr</last><affiliation>University of Florida</affiliation></author>
      <pages>123-129</pages>
      <abstract>As Large Language Models (LLMs) gain mainstream public usage, understanding how users interact with them becomes increasingly important. Limited variety in training data raises concerns about LLM reliability across different language inputs. To explore this, we test several LLMs using functionally equivalent prompts expressed in different English sublanguages. We frame this analysis using Question-Answer (QA) pairs, which allow us to detect and evaluate appropriate and anomalous model behavior. We contribute a cross-LLM testing method and a new QA dataset translated into AAVE and WAPE variants. Early results reveal a notable drop in accuracy for one sublanguage relative to the baseline.</abstract>
      <url hash="d4ad6bad">2025.winlp-main.21</url>
      <bibkey>coggins-etal-2025-aint</bibkey>
    </paper>
    <paper id="23">
      <title><fixed-case>A</fixed-case>mharic News Topic Classification: Dataset and Transformer-Based Model Benchmarks</title>
      <author orcid="0009-0004-0123-5140"><first>Dagnachew Mekonnen</first><last>Marilign</last></author>
      <author><first>Eyob Nigussie</first><last>Alemu</last><affiliation>Addis Ababa University</affiliation></author>
      <pages>130-135</pages>
      <abstract>News classification is a downstream task in Natural Language Processing (NLP) that involves the automatic categorization of news articles into predefined thematic categories. Although notable advancements have been made for high-resource languages, low-resource languages such as Amharic continue to encounter significant challenges, largely due to the scarcity of annotated corpora and the limited availability of language-specific, state-of-the-art model adaptations. To address these limitations, this study significantly expands an existing Amharic news dataset, increasing its size from 50,000 to 144,000 articles, thus enriching the linguistic and topical diversity available for the model training and evaluation. Using this expanded dataset, we systematically evaluated the performance of five transformer-based models: mBERT, XLM-R, DistilBERT, AfriBERTa, and AfroXLM in the context of Amharic news classification. Among these, AfriBERTa and XLM-R achieved the highest F1-scores of 90.25% and 90.11%, respectively, establishing a new performance baseline for the task. These findings underscore the efficacy of advanced multilingual and Africa-centric transformer architectures when applied to under-resourced languages, and further emphasize the critical importance of large-scale, high-quality datasets in enabling robust model generalization. This study offers a robust empirical foundation for advancing NLP research in low-resource languages, which remain underrepresented in current NLP resources and methodologies.</abstract>
      <url hash="b871079d">2025.winlp-main.23</url>
      <bibkey>marilign-alemu-2025-amharic</bibkey>
    </paper>
    <paper id="24">
      <title>Is this Chatbot Trying to Sell Something? Towards Oversight of Chatbot Sales Tactics</title>
      <author orcid="0000-0002-6785-9691"><first>Simrat</first><last>Deol</last></author>
      <author><first>Jack Luigi Henry</first><last>Contro</last></author>
      <author orcid="0000-0002-2003-0675"><first>Martim</first><last>Brandao</last><affiliation>King’s College London, University of London</affiliation></author>
      <pages>136-156</pages>
      <abstract>This research investigates the detection of covert sales tactics in human-chatbot interactions with a focus on the classification of solicited and unsolicited product recommendations. A custom dataset of 630 conversations was generated using a Large Language Model (LLM) to simulate chatbot-user interactions in various contexts, such as when interacting with users from different age groups, recommending different types of products and using different types of sales tactics. We then employ various approaches, including BiLSTM-based classification with sentence and word-level embeddings, as well as zero-shot, few-shot and CoT classification on large state-of-the-art LLMs. Our results show that few-shot GPT4 (86.44%) is the most accurate model on our dataset, followed by our compact SBERT+BiLSTM model (78.63%) - despite its small size.Our work demonstrates the feasibility of implementing oversight algorithms for monitoring chatbot conversations for undesired practices and that such monitoring could potentially be implemented locally on-device to mitigate privacy concerns. This research thus lays the groundwork for the development of auditing and oversight methods for virtual assistants such as chatbots, allowing consumer protection agencies to monitor the ethical use of conversational AI.</abstract>
      <url hash="07473b96">2025.winlp-main.24</url>
      <bibkey>deol-etal-2025-chatbot</bibkey>
    </paper>
    <paper id="25">
      <title>Sarc7: Evaluating Sarcasm Detection and Generation with Seven Types and Emotion-Informed Techniques</title>
      <author><first>Lang</first><last>Xiong</last></author>
      <author><first>Raina</first><last>Gao</last></author>
      <author><first>Alyssa</first><last>Jeong</last></author>
      <pages>157-166</pages>
      <abstract>Sarcasm is a complex linguistic and pragmatic phenomenon where expressions convey meanings that contrast with their literal interpretations, requiring sensitivity to the speaker’s intent and context. Misinterpreting sarcasm in collaborative human–AI settings can lead to under- or overreliance on LLM outputs, with consequences ranging from breakdowns in communication to critical safety failures. We introduce Sarc7, a benchmark for fine-grained sarcasm evaluation based on the MUStARD dataset, annotated with seven pragmatically defined sarcasm types: self-deprecating, brooding, deadpan, polite, obnoxious, raging, and manic. These categories are adapted from prior linguistic work and used to create a structured dataset suitable for LLM evaluation. For classification, we evaluate multiple prompting strategies—zero-shot, few-shot, chain-of-thought (CoT), and a novel emotion-based technique—across five major LLMs. Emotion-based prompting yields the highest macro-averaged F1 score of 0.3664 (Gemini 2.5), outperforming CoT for several models and demonstrating its effectiveness in sarcasm type recognition. For sarcasm generation, we design structured prompts using fixed values across four sarcasm-relevant dimensions: incongruity, shock value, context dependency, and emotion. Using Claude 3.5 Sonnet, this approach produces more subtype-aligned outputs, with human evaluators preferring emotion-based generations 38.46% more often than zero-shot baselines. Sarc7 offers a foundation for evaluating nuanced sarcasm understanding and controllable generation in LLMs, pushing beyond binary classification toward interpretable, emotion-informed language modeling.</abstract>
      <url hash="c0e8dcc4">2025.winlp-main.25</url>
      <bibkey>xiong-etal-2025-sarc7</bibkey>
    </paper>
    <paper id="26">
      <title>Emotionally Aware or Tone-Deaf? Evaluating Emotional Alignment in <fixed-case>LLM</fixed-case>-Based Conversational Recommendation Systems</title>
      <author orcid="0009-0006-0038-6128"><first>Darshna</first><last>Parmar</last><affiliation>Indian Institute of Information Technology, Vadodara</affiliation></author>
      <author orcid="0000-0003-0999-3689"><first>Pramit</first><last>Mazumdar</last></author>
      <pages>167-174</pages>
      <abstract>Recent advances in Large Language Models (LLMs) have enhanced the fluency and coherence of Conversational Recommendation Systems (CRSs), yet emotional intelligence remains a critical gap. In this study, we systematically evaluate the emotional behavior of six state-of-the-art LLMs in CRS settings using the ReDial and INSPIRED datasets. We propose an emotion-aware evaluation framework incorporating metrics such as Emotion Alignment, Emotion Flatness, and per-emotion F1-scores. Our analysis shows that most models frequently default to emotionally flat or mismatched responses, often misaligning with user affect (e.g., joy misread as neutral). We further examine patterns of emotional misalignment and their impact on user-centric qualities such as personalization, justification, and satisfaction. Through qualitative analysis, we demonstrate that emotionally aligned responses enhance user experience, while misalignments lead to loss of trust and relevance. This work highlights the need for emotion-aware design in CRS and provides actionable insights for improving affective sensitivity in LLM-generated recommendations.</abstract>
      <url hash="e330dd0c">2025.winlp-main.26</url>
      <bibkey>parmar-mazumdar-2025-emotionally</bibkey>
    </paper>
    <paper id="27">
      <title><fixed-case>MULBERE</fixed-case>: Multilingual Jailbreak Robustness Using Targeted Latent Adversarial Training</title>
      <author><first>Anastasia</first><last>Dunca</last></author>
      <author><first>Maanas Kumar</first><last>Sharma</last></author>
      <author><first>Olivia</first><last>Munoz</last></author>
      <author><first>Victor</first><last>Rosales</last><affiliation>NA</affiliation></author>
      <pages>175-181</pages>
      <abstract>Jailbreaking, the phenomenon where specific prompts cause LLMs to assist with harmful requests, remains a critical challenge in NLP, particularly in non-English and lower-resourced languages. To address this, we introduce MULBERE, a method that extends the method of Targeted Latent Adversarial Training (T-LAT) to a multilingual context. We first create and share a multilingual jailbreak dataset spanning high-, medium-, and low-resource languages, and then fine-tune LLaMA-2-7b-chat with interleaved T-LAT for jailbreak robustness and chat examples for model performance. Our evaluations show that MULBERE reduces average multilingual jailbreak success rates by 75% compared to the base LLaMA safety training and 71% compared to English-only T-LAT while maintaining or improving standard LLM performance.</abstract>
      <url hash="2ad4a252">2025.winlp-main.27</url>
      <bibkey>dunca-etal-2025-mulbere</bibkey>
    </paper>
    <paper id="30">
      <title>Investigating Motivated Inference in Large Language Models</title>
      <author><first>Nutchanon</first><last>Yongsatianchot</last><affiliation>Thammasat University</affiliation></author>
      <author><first>Stacy</first><last>Marsella</last><affiliation>Northeastern University</affiliation></author>
      <pages>182-196</pages>
      <abstract>Our desires often influence our beliefs and expectations. Humans tend to think good things are more likely to happen than they actually are, while believing bad things are less likely. This tendency has been referred to as wishful thinking in research on coping strategies. With large language models (LLMs) increasingly being considered as computational models of human cognition, we investigate whether they can simulate this distinctly human bias. We conducted two systematic experiments across multiple LLMs, manipulating outcome desirability and information uncertainty across multiple scenarios including probability games, natural disasters, and sports events. Our experiments revealed limited wishful thinking in LLMs. In Experiment 1, only two models showed the bias, and only in sports-related scenarios when role-playing characters. Models exhibited no wishful thinking in mathematical contexts. Experiment 2 found that explicit prompting about emotional states (being hopeful) was necessary to elicit wishful thinking in logical domains. These findings reveal a significant gap between human cognitive biases and LLMs’ default behavior patterns, suggesting that current models require explicit guidance to simulate wishful thinking influences on belief formation.</abstract>
      <url hash="a825d9e9">2025.winlp-main.30</url>
      <bibkey>yongsatianchot-marsella-2025-investigating</bibkey>
    </paper>
    <paper id="31">
      <title>Large Language Models as Detectors or Instigators of Hate Speech in Low-resource <fixed-case>E</fixed-case>thiopian Languages</title>
      <author><first>Nuhu</first><last>Ibrahim</last></author>
      <author><first>Felicity</first><last>Mulford</last></author>
      <author><first>Riza</first><last>Batista-Navarro</last><affiliation>University of Manchester</affiliation></author>
      <pages>197-209</pages>
      <abstract>We introduce a multilingual benchmark for evaluating large language models (LLMs) on hate speech detection and generation in low-resource Ethiopian languages: Afaan Oromo, Amharic and Tigrigna, and English (both monolingual and code-mixed). Using a balanced and expert-annotated dataset, we assess five state-of-the-art LLM families across both tasks. Our results show that while LLMs perform well on English detection, their performance on low-resource languages is significantly weaker, revealing that increasing model size alone does not ensure multilingual robustness. More critically, we find that all models, including closed and open-source variants, can be prompted to generate profiled hate speech with minimal resistance. These findings underscore the dual risk of exclusion and exploitation: LLMs fail to protect low-resource communities while enabling scalable harm against them. We make our evaluation framework available to facilitate future research on multilingual model safety and ethical robustness.</abstract>
      <url hash="35fdb92d">2025.winlp-main.31</url>
      <bibkey>ibrahim-etal-2025-large</bibkey>
    </paper>
    <paper id="32">
      <title>Brown Like Chocolate: How Vision-Language Models Associate Skin Tone with Food Colors</title>
      <author><first>Nutchanon</first><last>Yongsatianchot</last><affiliation>Thammasat University</affiliation></author>
      <author orcid="0000-0001-6647-870X"><first>Pachaya</first><last>Sailamul</last></author>
      <pages>210-223</pages>
      <abstract>We investigate how Vision-Language Models (VLMs) leverage visual features when making analogical comparisons about people. Using synthetic images of individuals varying in skin tone and nationality, we prompt GPT and Gemini models to make analogical associations with desserts and drinks. Results reveal that VLMs systematically associate darker-skinned individuals with brown-colored food items, with GPT showing stronger associations than Gemini. These patterns are amplified in Thai versus English prompts, suggesting language-dependent encoding of visual stereotypes. The associations persist across manipulation checks including position swapping and clothing changes, though presenting individuals alone yields divergent language-specific patterns. This work reveals concerning associations in VLMs’ visual reasoning that vary by language, with important implications for multilingual deployment.</abstract>
      <url hash="02226f70">2025.winlp-main.32</url>
      <bibkey>yongsatianchot-sailamul-2025-brown</bibkey>
    </paper>
    <paper id="33">
      <title>Improving <fixed-case>BGE</fixed-case>-<fixed-case>M</fixed-case>3 Multilingual Dense Embeddings for <fixed-case>N</fixed-case>igerian Low Resource Languages</title>
      <author><first>Abdulmatin</first><last>Omotoso</last><affiliation>Federal University of Agriculture Abeokutar</affiliation></author>
      <author orcid="0009-0005-5474-355X"><first>Habeeb</first><last>Shopeju</last><affiliation>Autodesk</affiliation></author>
      <author><first>Adejumobi Monjolaoluwa</first><last>Joshua</last></author>
      <author><first>Shiloh</first><last>Oni</last></author>
      <pages>224-229</pages>
      <abstract>Multilingual dense embedding models such as Multilingual E5, LaBSE, and BGE-M3 have shown promising results on diverse benchmarks for information retrieval in low-resource languages. But their result on low resource languages is not up to par with other high resource languages. This work improves the performance of BGE-M3 through contrastive fine-tuning; the model was selected because of its superior performance over other multilingual embedding models across MIRACL, MTEB, and SEB benchmarks. To fine-tune this model, we curated a comprehensive dataset comprising Yorùbá (32.9k rows), Igbo (18k rows) and Hausa (85k rows) from mainly news sources. We further augmented our multilingual dataset with English queries and mapped it to each of the Yoruba, Igbo, and Hausa documents, enabling cross-lingual semantic training. We evaluate on two settings: the Wura test set and the MIRACL benchmark. On Wura, the fine-tuned BGE-M3 raises mean reciprocal rank (MRR) to 0.9201 for Yorùbá, 0.8638 for Igbo, 0.9230 for Hausa, and 0.8617 for English queries matched to local documents, surpassing the BGE-M3 baselines of 0.7846, 0.7566, 0.8575, and 0.7377, respectively. On MIRACL (Yorùbá subset), the fine-tuned model attains 0.5996 MRR, slightly surpassing base BGE-M3 (0.5952) and outperforming ML-E5-large (0.5632) and LaBSE (0.4468).</abstract>
      <url hash="2f8fc9df">2025.winlp-main.33</url>
      <bibkey>omotoso-etal-2025-improving</bibkey>
    </paper>
    <paper id="34">
      <title>Challenges in Processing <fixed-case>C</fixed-case>hinese Texts Across Genres and Eras</title>
      <author><first>Minghao</first><last>Zheng</last></author>
      <author><first>Sarah</first><last>Moeller</last><affiliation>University of Florida</affiliation></author>
      <pages>230-234</pages>
      <abstract>Pre-trained Chinese Natural Language Processing (NLP) tools show reduced performance when analyzing poetry compared to prose. This study investigates the discrepancies between tools trained on either Classical or Modern Chinese prose when handling Classical Chinese prose and Classical Chinese poetry. Three experiments reveal error patterns that indicate the weaker performance on Classical Chinese poemsis due to challenges identifying word boundaries. Specifically, tools trained on Classical prose struggle recognizing word boundaries within Classical poetic structures and tools trained on Modern prose have difficulty with word segmentation in both Classical Chinese genres. These findings provide valuable insights into the limitations of current NLP tools for studying Classical Chinese literature.</abstract>
      <url hash="c2e12b5b">2025.winlp-main.34</url>
      <bibkey>zheng-moeller-2025-challenges</bibkey>
    </paper>
    <paper id="35">
      <title>The Gemma Sutras: Fine-Tuning Gemma 3 for <fixed-case>S</fixed-case>anskrit Sandhi Splitting</title>
      <author><first>Samarth</first><last>P</last></author>
      <author><first>Sanjay Balaji</first><last>Mahalingam</last></author>
      <pages>235-241</pages>
      <abstract>Sandhi, the phonological merging of morphemes, is a central feature of Sanskrit grammar. While Sandhi formation is well-defined by Pāṇini’s Aṣṭādhyāyī, the reverse task—Sandhi splitting—is substantially more complex due to inherent ambiguity and context-sensitive transformations. Accurate splitting is a critical precursor to tokenization in Sanskrit, which lacks explicit word boundaries and presents densely fused compounds. In this work, we present a data-driven approach, fine-tuning the Gemma-3 4B large language model on a dataset of over 49,000 training and 2,000 test examples of compound words and their morpheme-level decompositions. Leveraging the Unsloth framework with low-rank adaptation (LoRA) and 4-bit quantization, we train the model to predict these splits. Our work yields a scalable, Sandhi-aware system designed to enhance modern NLP pipelines for classical Sanskrit, demonstrating an effective application of LLMs to this linguistic challenge.</abstract>
      <url hash="fd865b3d">2025.winlp-main.35</url>
      <bibkey>p-mahalingam-2025-gemma</bibkey>
    </paper>
    <paper id="36">
      <title>Evaluation Sheet for Deep Research: A Use Case for Academic Survey Writing</title>
      <author orcid="0000-0001-9063-1137"><first>Israel Abebe</first><last>Azime</last></author>
      <author orcid="0000-0003-0883-984X"><first>Tadesse Destaw</first><last>Belay</last></author>
      <author orcid="0000-0002-3501-5136"><first>Atnafu Lambebo</first><last>Tonja</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>242-250</pages>
      <abstract>Large Language Models (LLMs) powered with argentic capabilities are able to do knowledge-intensive tasks without human involvement. A prime example of this tool is Deep research with the capability to browse the web, extract information and generate multi-page reports.In this work, we introduce an evaluation sheet that can be used for assessing the capability of Deep Research tools. In addition, we selected academic survey writing as a use case task and evaluated output reports based on the evaluation sheet we introduced. Our findings show the need to have carefully crafted evaluation standards. The evaluation done on OpenAI‘s Deep Search and Google’s Deep Search in generating an academic survey showed the huge gap between search engines and standalone Deep Research tools, as well as the shortcomings in representing the targeted area.</abstract>
      <url hash="532989cc">2025.winlp-main.36</url>
      <bibkey>azime-etal-2025-evaluation</bibkey>
    </paper>
    <paper id="37">
      <title>Reference-Guided Verdict: <fixed-case>LLM</fixed-case>s-as-Judges in Automatic Evaluation of Free-Form <fixed-case>QA</fixed-case></title>
      <author orcid="0000-0001-6780-3746"><first>Sher</first><last>Badshah</last></author>
      <author orcid="0000-0002-8584-6595"><first>Hassan</first><last>Sajjad</last><affiliation>Dalhousie University</affiliation></author>
      <pages>251-267</pages>
      <abstract>The emergence of Large Language Models (LLMs) as chat assistants capable of generating human-like conversations has amplified the need for robust evaluation methods, particularly for open-ended tasks. Conventional metrics such as EM and F1, while useful, are inadequate for capturing the full semantics and contextual depth of such generative outputs. We propose a reference-guided verdict method that automates the evaluation process by leveraging multiple LLMs as judges. Through experiments on free-form question-answering tasks, we demonstrate that combining multiple models improves the reliability and accuracy of evaluations, especially in tasks where a single model may struggle. The results indicate a strong correlation with human evaluations, establishing the proposed method as a reliable alternative to traditional metrics.</abstract>
      <url hash="34bd92d1">2025.winlp-main.37</url>
      <bibkey>badshah-sajjad-2025-reference</bibkey>
    </paper>
    <paper id="39">
      <title>No for Some, Yes for Others: Persona Prompts and Other Sources of False Refusal in Language Models</title>
      <author orcid="0000-0002-3020-5512"><first>Flor Miriam</first><last>Plaza-del-Arco</last><affiliation>Leiden University</affiliation></author>
      <author orcid="0009-0008-7115-6893"><first>Paul</first><last>Röttger</last><affiliation>Bocconi University</affiliation></author>
      <author><first>Nino</first><last>Scherrer</last><affiliation>Google</affiliation></author>
      <author><first>Emanuele</first><last>Borgonovo</last><affiliation>Bocconi University</affiliation></author>
      <author orcid="0000-0002-2019-9243"><first>Elmar</first><last>Plischke</last><affiliation>Helmholtz-Zentrum Dresden-Rossendorf</affiliation></author>
      <author orcid="0000-0002-4618-3127"><first>Dirk</first><last>Hovy</last><affiliation>Bocconi University</affiliation></author>
      <pages>268-282</pages>
      <abstract>Large language models (LLMs) are increasingly integrated into our daily lives and personalized. However, LLM personalization might also increase unintended side effects. Recent work suggests that persona prompting can lead models to falsely refuse user requests. However, no work has fully quantified the extent of this issue. To address this gap, we measure the impact of 15 sociodemographic personas (based on gender, race, religion, and disability) on false refusal. To control for other factors, we also test 16 different models, 3 tasks (Natural Language Inference, politeness, and offensiveness classification), and nine prompt paraphrases. We propose a Monte Carlo-based method to quantify this issue in a sample-efficient manner. Our results show that as models become more capable, personas impact the refusal rate less. However, we find that the choice of model significantly influence false refusals, especially in sensitive content tasks. The impact of certain sociodemographic personas further increases the false refusal effect in some models, which suggests that there are underlying biases in the alignment strategies or safety mechanisms.</abstract>
      <url hash="fa3db308">2025.winlp-main.39</url>
      <bibkey>plaza-del-arco-etal-2025-yes</bibkey>
    </paper>
  </volume>
</collection>
