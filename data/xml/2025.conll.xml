<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.conll">
  <volume id="1" ingest-date="2025-07-20" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 29th Conference on Computational Natural Language Learning</booktitle>
      <editor><first>Gemma</first><last>Boleda</last></editor>
      <editor><first>Michael</first><last>Roth</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Vienna, Austria</address>
      <month>July</month>
      <year>2025</year>
      <url hash="5ad9f0fe">2025.conll-1</url>
      <venue>conll</venue>
      <venue>ws</venue>
      <isbn>979-8-89176-271-8</isbn>
      <doi>10.18653/v1/2025.conll-1</doi>
    </meta>
    <frontmatter>
      <url hash="2c01511b">2025.conll-1.0</url>
      <bibkey>conll-ws-2025-1</bibkey>
      <doi>10.18653/v1/2025.conll-1.0</doi>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>HKC</fixed-case>anto-Eval: A Benchmark for Evaluating <fixed-case>C</fixed-case>antonese Language Understanding and Cultural Comprehension in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Tsz Chung</first><last>Cheng</last><affiliation>Kyushu University</affiliation></author>
      <author><first>Chung Shing</first><last>Cheng</last></author>
      <author><first>Chaak-ming</first><last>Lau</last><affiliation>The Education University of Hong Kong</affiliation></author>
      <author><first>Eugene</first><last>Lam</last></author>
      <author><first>Wong Chun</first><last>Yat</last></author>
      <author><first>Hoi On</first><last>Yu</last><affiliation>University of California, Berkeley</affiliation></author>
      <author><first>Cheuk Hei</first><last>Chong</last><affiliation>Beever AI and Votee AI</affiliation></author>
      <pages>1-11</pages>
      <abstract>The ability of language models to comprehend and interact in diverse linguistic and cultural landscapes is crucial. The Cantonese language used in Hong Kong presents unique challenges for natural language processing due to its rich cultural nuances and lack of dedicated evaluation datasets. The HKCanto-Eval benchmark addresses this gap by evaluating the performance of large language models (LLMs) on Cantonese language understanding tasks, extending to English and Written Chinese for cross-lingual evaluation. HKCanto-Eval integrates cultural and linguistic nuances intrinsic to Hong Kong, providing a robust framework for assessing language models in realistic scenarios. Additionally, the benchmark includes questions designed to tap into the underlying linguistic metaknowledge of the models. Our findings indicate that while proprietary models generally outperform open-weight models, significant limitations remain in handling Cantonese-specific linguistic and cultural knowledge, highlighting the need for more targeted training data and evaluation methods. The code can be accessed at https://github.com/hon9kon9ize/hkeval2025.</abstract>
      <url hash="c2dba9db">2025.conll-1.1</url>
      <attachment type="attachment" hash="6e911369">2025.conll-1.1.attachment.pdf</attachment>
      <bibkey>cheng-etal-2025-hkcanto</bibkey>
      <doi>10.18653/v1/2025.conll-1.1</doi>
    </paper>
    <paper id="2">
      <title>Quasi-symbolic Semantic Geometry over Transformer-based Variational <fixed-case>A</fixed-case>uto<fixed-case>E</fixed-case>ncoder</title>
      <author><first>Yingji</first><last>Zhang</last></author>
      <author><first>Danilo</first><last>Carvalho</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Andre</first><last>Freitas</last><affiliation>Idiap Research Institute and University of Manchester</affiliation></author>
      <pages>12-29</pages>
      <abstract>Formal/symbolic semantics can provide canonical, rigid controllability and interpretability to sentence representations due to their <i>localisation</i> or <i>composition</i> property. How can we deliver such property to the current distributional sentence representations to better control and interpret the generation of language models (LMs)? In this work, we theoretically frame the sentence semantics as the composition of <i>semantic role - word content</i> features and propose the formal semantic geometrical framework. To inject such geometry into Transformer-based LMs (i.e. GPT2), we deploy a supervised Transformer-based Variational AutoEncoder, where the sentence generation can be manipulated and explained over low-dimensional latent Gaussian space. In addition, we propose a new probing algorithm to guide the movement of sentence vectors over such geometry. Experimental results reveal that the formal semantic geometry can potentially deliver better control and interpretation to sentence generation.</abstract>
      <url hash="11cb23e1">2025.conll-1.2</url>
      <bibkey>zhang-etal-2025-quasi</bibkey>
      <doi>10.18653/v1/2025.conll-1.2</doi>
    </paper>
    <paper id="3">
      <title><fixed-case>L</fixed-case>aw<fixed-case>T</fixed-case>oken: a single token worth more than its constituents</title>
      <author><first>Yu-Hsiang</first><last>Tseng</last><affiliation>Eberhard-Karls-Universität Tübingen</affiliation></author>
      <author><first>Hsin-Yu</first><last>Chou</last><affiliation>deepq.com</affiliation></author>
      <author><first>Shu-Kai</first><last>Hsieh</last><affiliation>National Taiwan University</affiliation></author>
      <pages>30-46</pages>
      <abstract>Legal citations require correctly recalling the law references of complex law article names and article numbering, which large language models typically treat as multi-token sequences. Motivated by the form-meaning pair of constructionist approaches, we explore treating these multi-token law references as a single holistic law token and examining the implications for legal citation accuracy and differences in model interpretability. We train and compare two types of models: LawToken models, which encode the legal citations as a single law token, and LawBase models, which treat them as multi-token compounds. The results show that LawToken models outperform LawBase models on legal citation tasks, primarily due to fewer errors in the article numbering components. Further model representation analysis reveals that, while both models achieve comparable semantic representation quality, the multi-token-based LawBase suffers from degraded representations in multistep decoding, leading to more errors. Taken together, these findings suggest that form-meaning pairing can operate in a larger context, and this larger unit may offer advantages in future modeling of legal reasoning. In practice, this approach can significantly reduce the likelihood of hallucinations by anchoring legal citations as discrete, holistic tokens, thereby minimizing the risk of generating nonexistent or incorrect legal references.</abstract>
      <url hash="7bf566eb">2025.conll-1.3</url>
      <bibkey>tseng-etal-2025-lawtoken</bibkey>
      <doi>10.18653/v1/2025.conll-1.3</doi>
    </paper>
    <paper id="4">
      <title>Interpersonal Memory Matters: A New Task for Proactive Dialogue Utilizing Conversational History</title>
      <author><first>Bowen</first><last>Wu</last><affiliation>Peking University and Tencent</affiliation></author>
      <author><first>Wenqing</first><last>Wang</last></author>
      <author><first>Lihaoran</first><last>Lihaoran</last></author>
      <author><first>Yunhan</first><last>Deng</last><affiliation>Tencent</affiliation></author>
      <author><first>Ying</first><last>Li</last><affiliation>Peking University</affiliation></author>
      <author><first>Jingsong</first><last>Yu</last><affiliation>Peking University</affiliation></author>
      <author><first>Baoxun</first><last>Wang</last><affiliation>PCG, Tencent</affiliation></author>
      <pages>47-67</pages>
      <abstract>Proactive dialogue systems aim to empower chatbots with the capability of leading conversations towards specific targets, thereby enhancing user engagement and service autonomy. Existing systems typically target pre-defined keywords or entities, neglecting user attributes and preferences implicit in dialogue history, hindering the development of long-term user intimacy. To address these challenges, we take a radical step towards building a more human-like conversational agent by integrating proactive dialogue systems with long-term memory into a unified framework. Specifically, we define a novel task named Memory-aware Proactive Dialogue (MapDia). By decomposing the task, we then propose an automatic data construction method and create the first Chinese Memory-aware Proactive Dataset (ChMapData). Furthermore, we introduce a joint framework based on Retrieval Augmented Generation (RAG), featuring three modules: Topic Summarization, Topic Retrieval, and Proactive Topic-shifting Detection and Generation, designed to steer dialogues towards relevant historical topics at the right time. The effectiveness of our dataset and models is validated through both automatic and human evaluations. We release the open-source framework and dataset at https://github.com/FrontierLabs/MapDia.</abstract>
      <url hash="2bcf7f3c">2025.conll-1.4</url>
      <attachment type="software" hash="02c482c6">2025.conll-1.4.software.zip</attachment>
      <bibkey>wu-etal-2025-interpersonal</bibkey>
      <doi>10.18653/v1/2025.conll-1.4</doi>
    </paper>
    <paper id="5">
      <title><fixed-case>W</fixed-case>ino<fixed-case>W</fixed-case>hat: A Parallel Corpus of Paraphrased <fixed-case>W</fixed-case>ino<fixed-case>G</fixed-case>rande Sentences with Common Sense Categorization</title>
      <author><first>Ine</first><last>Gevers</last></author>
      <author><first>Victor</first><last>De Marez</last><affiliation>Universiteit Antwerpen</affiliation></author>
      <author><first>Luna</first><last>De Bruyne</last><affiliation>Universiteit Antwerpen</affiliation></author>
      <author><first>Walter</first><last>Daelemans</last><affiliation>University of Antwerp</affiliation></author>
      <pages>68-80</pages>
      <abstract>In this study, we take a closer look at how Winograd schema challenges can be used to evaluate common sense reasoning in LLMs. Specifically, we evaluate generative models of different sizes on the popular WinoGrande benchmark. We release WinoWhat, a new corpus, in which each instance of the WinoGrande validation set is paraphrased. Additionally, we evaluate the performance on the challenge across five common sense knowledge categories, giving more fine-grained insights on what types of knowledge are more challenging for LLMs. Surprisingly, all models perform significantly worse on WinoWhat, implying that LLM reasoning capabilities are overestimated on WinoGrande. To verify whether this is an effect of benchmark memorization, we match benchmark instances to LLM trainingdata and create two test-suites. We observe that memorization has a minimal effect on model performance on WinoGrande.</abstract>
      <url hash="7bc0af2a">2025.conll-1.5</url>
      <attachment type="attachment" hash="f02e058c">2025.conll-1.5.attachment.pdf</attachment>
      <bibkey>gevers-etal-2025-winowhat</bibkey>
      <doi>10.18653/v1/2025.conll-1.5</doi>
      <revision id="1" href="2025.conll-1.5v1" hash="5ebec676"/>
      <revision id="2" href="2025.conll-1.5v2" hash="7bc0af2a" date="2025-07-29">Minor update.</revision>
    </paper>
    <paper id="6">
      <title>Planning for Success: Exploring <fixed-case>LLM</fixed-case> Long-term Planning Capabilities in Table Understanding</title>
      <author><first>Thi-Nhung</first><last>Nguyen</last><affiliation>VinAI Research</affiliation></author>
      <author><first>Hoang</first><last>Ngo</last><affiliation>VinAI Research</affiliation></author>
      <author><first>Dinh</first><last>Phung</last><affiliation>Monash University</affiliation></author>
      <author><first>Thuy-Trang</first><last>Vu</last><affiliation>Monash University</affiliation></author>
      <author><first>Dat Quoc</first><last>Nguyen</last><affiliation>Qualcomm AI Research</affiliation></author>
      <pages>81-92</pages>
      <abstract>Table understanding is key to addressing challenging downstream tasks such as table-based question answering and fact verification. Recent works have focused on leveraging Chain-of-Thought and question decomposition to solve complex questions requiring multiple operations on tables. However, these methods often suffer from a lack of explicit long-term planning and weak inter-step connections, leading to miss constraints within questions. In this paper, we propose leveraging the long-term planning capabilities of large language models (LLMs) to enhance table understanding. Our approach enables the execution of a long-term plan, where the steps are tightly interconnected and serve the ultimate goal, an aspect that methods based on Chain-of-Thought and question decomposition lack. In addition, our method effectively minimizes the inclusion of unnecessary details in the process of solving the next short-term goals, a limitation of methods based on Chain-of-Thought. Extensive experiments demonstrate that our method outperforms strong baselines and achieves state-of-the-art performance on WikiTableQuestions and TabFact datasets.</abstract>
      <url hash="cd62982e">2025.conll-1.6</url>
      <bibkey>nguyen-etal-2025-planning</bibkey>
      <doi>10.18653/v1/2025.conll-1.6</doi>
    </paper>
    <paper id="7">
      <title>Derivational Probing: Unveiling the Layer-wise Derivation of Syntactic Structures in Neural Language Models</title>
      <author><first>Taiga</first><last>Someya</last></author>
      <author><first>Ryo</first><last>Yoshida</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Hitomi</first><last>Yanaka</last><affiliation>the University of Tokyo</affiliation></author>
      <author><first>Yohei</first><last>Oseki</last><affiliation>University of Tokyo</affiliation></author>
      <pages>93-104</pages>
      <abstract>Recent work has demonstrated that neural language models encode syntactic structures in their internal *representations*, yet the *derivations* by which these structures are constructed across layers remain poorly understood. In this paper, we propose *Derivational Probing* to investigate how micro-syntactic structures (e.g., subject noun phrases) and macro-syntactic structures (e.g., the relationship between the root verbs and their direct dependents) are constructed as word embeddings propagate upward across layers.Our experiments on BERT reveal a clear bottom-up derivation: micro-syntactic structures emerge in lower layers and are gradually integrated into a coherent macro-syntactic structure in higher layers.Furthermore, a targeted evaluation on subject-verb number agreement shows that the timing of constructing macro-syntactic structures is critical for downstream performance, suggesting an optimal timing for integrating global syntactic information.</abstract>
      <url hash="95e6e7f7">2025.conll-1.7</url>
      <bibkey>someya-etal-2025-derivational</bibkey>
      <doi>10.18653/v1/2025.conll-1.7</doi>
    </paper>
    <paper id="8">
      <title>Short-circuiting Shortcuts: Mechanistic Investigation of Shortcuts in Text Classification</title>
      <author><first>Leon</first><last>Eshuijs</last></author>
      <author><first>Shihan</first><last>Wang</last><affiliation>Utrecht University</affiliation></author>
      <author><first>Antske</first><last>Fokkens</last><affiliation>VU University Amsterdam</affiliation></author>
      <pages>105-125</pages>
      <abstract>Reliance on spurious correlations (shortcuts) has been shown to underlie many of the successes of language models. Previous work focused on identifying the input elements that impact prediction. We investigate how shortcuts are actually processed within the model’s decision-making mechanism.We use actor names in movie reviews as controllable shortcuts with known impact on the outcome. We use mechanistic interpretability methods and identify specific attention heads that focus on shortcuts. These heads gear the model towards a label before processing the complete input, effectively making premature decisions that bypass contextual analysis. Based on these findings, we introduce Head-based Token Attribution (HTA), which traces intermediate decisions back to input tokens. We show that HTA is effective in detecting shortcuts in LLMs and enables targeted mitigation by selectively deactivating shortcut-related attention heads.</abstract>
      <url hash="f6f5617d">2025.conll-1.8</url>
      <bibkey>eshuijs-etal-2025-short</bibkey>
      <doi>10.18653/v1/2025.conll-1.8</doi>
    </paper>
    <paper id="9">
      <title>A Linguistically Motivated Analysis of Intonational Phrasing in Text-to-Speech Systems: Revealing Gaps in Syntactic Sensitivity</title>
      <author><first>Charlotte</first><last>Pouw</last></author>
      <author><first>Afra</first><last>Alishahi</last><affiliation>Tilburg University</affiliation></author>
      <author><first>Willem</first><last>Zuidema</last><affiliation>University of Amsterdam</affiliation></author>
      <pages>126-140</pages>
      <abstract>We analyze the syntactic sensitivity of Text-to-Speech (TTS) systems using methods inspired by psycholinguistic research. Specifically, we focus on the generation of intonational phrase boundaries, which can often be predicted by identifying syntactic boundaries within a sentence. We find that TTS systems struggle to accurately generate intonational phrase boundaries in sentences where syntactic boundaries are ambiguous (e.g., garden path sentences or sentences with attachment ambiguity). In these cases, systems need superficial cues such as commas to place boundaries at the correct positions. In contrast, for sentences with simpler syntactic structures, we find that systems do incorporate syntactic cues beyond surface markers. Finally, we finetune models on sentences without commas at the syntactic boundary positions, encouraging them to focus on more subtle linguistic cues. Our findings indicate that this leads to more distinct intonation patterns that better reflect the underlying structure.</abstract>
      <url hash="7f942950">2025.conll-1.9</url>
      <bibkey>pouw-etal-2025-linguistically</bibkey>
      <doi>10.18653/v1/2025.conll-1.9</doi>
    </paper>
    <paper id="10">
      <title>Experiential Semantic Information and Brain Alignment: Are Multimodal Models Better than Language Models?</title>
      <author><first>Anna</first><last>Bavaresco</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Raquel</first><last>Fernández</last><affiliation>University of Amsterdam and University of Amsterdam</affiliation></author>
      <pages>141-155</pages>
      <abstract>A common assumption in Computational Linguistics is that text representations learnt by multimodal models are richer and more human-like than those by language-only models, as they are grounded in images or audio—similar to how human language is grounded in real-world experiences. However, empirical studies checking whether this is true are largely lacking. We address this gap by comparing word representations from contrastive multimodal models vs. language-only ones in the extent to which they capture experiential information—as defined by an existing norm-based ‘experiential model’—and align with human fMRI responses. Our results indicate that, surprisingly, language-only models are superior to multimodal ones in both respects. Additionally, they learn more unique brain-relevant semantic information beyond that shared with the experiential model. Overall, our study highlights the need to develop computational models that better integrate the complementary semantic information provided by multimodal data sources.</abstract>
      <url hash="6958ec00">2025.conll-1.10</url>
      <bibkey>bavaresco-fernandez-2025-experiential</bibkey>
      <doi>10.18653/v1/2025.conll-1.10</doi>
    </paper>
    <paper id="11">
      <title>What is an “Abstract Reasoner”? Revisiting Experiments and Arguments about Large Language Models</title>
      <author><first>Tian</first><last>Yun</last><affiliation>Brown University</affiliation></author>
      <author><first>Chen</first><last>Sun</last><affiliation>Brown University and Google</affiliation></author>
      <author><first>Ellie</first><last>Pavlick</last><affiliation>Brown University</affiliation></author>
      <pages>156-168</pages>
      <abstract>Recent work has argued that large language models (LLMs) are not “abstract reasoners”, citing their poor zero-shot performance on a variety of challenging tasks as evidence. We revisit these experiments in order to add nuance to the claim. First, we show that while LLMs indeed perform poorly in a zero-shot setting, even tuning a small subset of parameters for input encoding can enable near-perfect performance. However, we also show that this finetuning does not necessarily transfer across datasets. We take this collection of empirical results as an invitation to (re-)open the discussion of what it means to be an “abstract reasoner”, and why it matters whether LLMs fit the bill.</abstract>
      <url hash="e6a5f17c">2025.conll-1.11</url>
      <bibkey>yun-etal-2025-abstract</bibkey>
      <doi>10.18653/v1/2025.conll-1.11</doi>
    </paper>
    <paper id="12">
      <title>Do Construction Distributions Shape Formal Language Learning In <fixed-case>G</fixed-case>erman <fixed-case>B</fixed-case>aby<fixed-case>LM</fixed-case>s?</title>
      <author><first>Bastian</first><last>Bunzeck</last><affiliation>Universität Bielefeld</affiliation></author>
      <author><first>Daniel</first><last>Duran</last><affiliation>Universität Bielefeld</affiliation></author>
      <author><first>Sina</first><last>Zarrieß</last><affiliation>Bielefeld University</affiliation></author>
      <pages>169-186</pages>
      <abstract>We analyze the influence of utterance-level construction distributions in German child-directed/child-available speech on the resulting word-level, syntactic and semantic competence (and their underlying learning trajectories) in small LMs, which we train on a novel collection of developmentally plausible language data for German. We find that trajectories are surprisingly robust for markedly different distributions of constructions in the training data, which have little effect on final accuracies and almost no effect on global learning trajectories. While syntax learning benefits from more complex utterances, word-level learning culminates in better scores with more fragmentary utterances. We argue that LMs trained on developmentally plausible data can contribute to debates on how conducive different kinds of linguistic stimuli are to language learning.</abstract>
      <url hash="70f5a4b6">2025.conll-1.12</url>
      <bibkey>bunzeck-etal-2025-construction</bibkey>
      <doi>10.18653/v1/2025.conll-1.12</doi>
    </paper>
    <paper id="13">
      <title>Adapting Large Language Models for Movie Domain with Narrative Understanding Tasks</title>
      <author><first>Siqi</first><last>Shen</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>Amanmeet</first><last>Garg</last><affiliation>Amazon</affiliation></author>
      <pages>187-200</pages>
      <abstract>Large language models (LLMs) have been deployed in a wide spectrum of domains and applications due to their strong language understanding capabilities obtained through pretraining. However, their performance on specific domain is usually suboptimal due to limited exposure to domain-specific tasks. Adapting LLM to the cinematic domain post unique challenges as it consists of complicated stories with limited textual information accessible from the subtitle or script alone. In this paper, we decompose the movie understanding capability into a suite of narrative understanding tasks based on narrative theory. We construct a dataset for these tasks based on resources in the movie domain, and use it to examine the effect of different domain adaptation strategies. Both the dataset and the models are made publicly available.Our experiment results show the effectiveness of our approach in improving the narrative understanding of LLMs and highlight the trade-offs between domain-specific and general instruction capabilities.</abstract>
      <url hash="21de0efd">2025.conll-1.13</url>
      <bibkey>shen-garg-2025-adapting</bibkey>
      <doi>10.18653/v1/2025.conll-1.13</doi>
    </paper>
    <paper id="14">
      <title>From Stories to Statistics: Methodological Biases in <fixed-case>LLM</fixed-case>-Based Narrative Flow Quantification</title>
      <author><first>Amal</first><last>Sunny</last><affiliation>International Institute of Information Technology, Hyderabad, International Institute of Information Technology Hyderabad and International Institute of Information Technology, Hyderabad, International Institute of Information Technology Hyderabad</affiliation></author>
      <author><first>Advay</first><last>Gupta</last></author>
      <author><first>Yashashree</first><last>Chandak</last></author>
      <author><first>Vishnu</first><last>Sreekumar</last><affiliation>International Institute of Information Technology Hyderabad</affiliation></author>
      <pages>201-215</pages>
      <abstract>Large Language Models (LLMs) have made significant contributions to cognitive science research. One area of application is narrative understanding. Sap et al. (2022) introduced <tex-math>\textit{sequentiality}</tex-math>, an LLM-derived measure that assesses the coherence of a story based on word probability distributions. They reported that recalled stories flowed less sequentially than imagined stories. However, the robustness and generalizability of this narrative flow measure remain unverified. To assess generalizability, we apply <tex-math>\textit{sequentiality}</tex-math> derived from three different LLMs to a new dataset of matched autobiographical and biographical paragraphs. Contrary to previous results, we fail to find a significant difference in narrative flow between autobiographies and biographies. Further investigation reveals biases in the original data collection process, where topic selection systematically influences sequentiality scores. Adjusting for these biases substantially reduces the originally reported effect size. A validation exercise using LLM-generated stories with “good” and “poor” flow further highlights the flaws in the original formulation of sequentiality. Our findings suggest that LLM-based narrative flow quantification is susceptible to methodological artifacts. Finally, we provide some suggestions for modifying the <tex-math>\textit{sequentiality}</tex-math> formula to accurately capture narrative flow.</abstract>
      <url hash="885909bf">2025.conll-1.14</url>
      <bibkey>sunny-etal-2025-stories</bibkey>
      <doi>10.18653/v1/2025.conll-1.14</doi>
    </paper>
    <paper id="15">
      <title>Components of Creativity: Language Model-based Predictors for Clustering and Switching in Verbal Fluency</title>
      <author><first>Sina</first><last>Zarrieß</last><affiliation>Bielefeld University</affiliation></author>
      <author><first>Simeon</first><last>Junker</last><affiliation>Universität Bielefeld</affiliation></author>
      <author><first>Judith</first><last>Sieker</last><affiliation>Universität Bielefeld</affiliation></author>
      <author><first>Özge</first><last>Alacam</last><affiliation>Bielefeld University</affiliation></author>
      <pages>216-232</pages>
      <abstract>Verbal fluency is an experimental paradigm used to examine human knowledge retrieval, cognitive performance and creative abilities. This work investigates the psychometric capacities of LMs in this task. We focus on switching and clustering patterns and seek evidence to substantiate them as two distinct and separable components of lexical retrieval processes in LMs.We prompt different transformer-based LMs with verbal fluency items and ask whether metrics derived from the language models’ prediction probabilities or internal attention distributions offer reliable predictors of switching/clustering behaviors in verbal fluency. We find that token probabilities, but especially attention-based metrics have strong statistical power when separating between cases of switching and clustering, in line with prior research on human cognition.</abstract>
      <url hash="e81ad41a">2025.conll-1.15</url>
      <bibkey>zarriess-etal-2025-components</bibkey>
      <doi>10.18653/v1/2025.conll-1.15</doi>
    </paper>
    <paper id="16">
      <title>An Appraisal Theoretic Approach to Modelling Affect Flow in Conversation Corpora</title>
      <author><first>Alok</first><last>Debnath</last><affiliation>Trinity College, Dublin</affiliation></author>
      <author><first>Yvette</first><last>Graham</last></author>
      <author><first>Owen</first><last>Conlan</last></author>
      <pages>233-250</pages>
      <abstract>This paper presents a model of affect in conversations by leveraging Appraisal Theory as a generalizable framework. We propose that the multidimensional cognitive model of Appraisal Theory offers significant advantages for analyzing emotions in conversational contexts, addressing the current challenges of inconsistent annotation methodologies across corpora. To demonstrate this, we present AppraisePLM, a regression and classification model trained on the crowd-EnVent corpus that outperforms existing models in predicting 21 appraisal dimensions including <i>pleasantness</i>, <i>self-control</i>, and <i>alignment with social norms</i>. We apply AppraisePLM to diverse conversation datasets spanning task-oriented dialogues, general-domain chit-chat, affect-specific conversations, and domain-specific affect analysis. Our analysis reveals that AppraisePLM successfully extrapolates emotion labels across datasets, while capturing domain-specific patterns in affect flow – change in conversational emotion over the conversation. This work highlights the entangled nature of affective phenomena in conversation and positions affect flow as a promising model for holistic emotion analysis, offering a standardized approach to evaluate and benchmark affective capabilities in conversational agents.</abstract>
      <url hash="d95d852e">2025.conll-1.16</url>
      <bibkey>debnath-etal-2025-appraisal</bibkey>
      <doi>10.18653/v1/2025.conll-1.16</doi>
    </paper>
    <paper id="17">
      <title>Principal Parts Detection for Computational Morphology: Task, Models and Benchmark</title>
      <author><first>Dorin</first><last>Keshales</last><affiliation>Bar-Ilan University</affiliation></author>
      <author><first>Omer</first><last>Goldman</last><affiliation>Bar Ilan University</affiliation></author>
      <author><first>Reut</first><last>Tsarfaty</last><affiliation>Google and Bar-Ilan University, Technion</affiliation></author>
      <pages>251-267</pages>
      <abstract>Principal parts of an inflectional paradigm, defined as the minimal set of paradigm cells required to deduce all others, constitute an important concept in theoretical morphology. This concept, which outlines the minimal memorization needed for a perfect inflector, has been largely overlooked in computational morphology despite impressive advances in the field over the last decade. In this work, we posit Principal Parts Detection as a computational task and construct a multilingual dataset of verbal principal parts covering ten languages, based on Wiktionary entries. We evaluate an array of Principal Parts Detection methods, all of which follow the same schema: characterize the relationships between each pair of inflectional categories, cluster the resulting vector representations, and select a representative of each cluster as a predicted principal part. Our best-performing model, based on Edit Script between inflections and using Hierarchical K-Means, achieves an F1 score of 55.05%, significantly outperforming a random baseline of 21.20%. While our results demonstrate that some success is achievable, further work is needed to thoroughly solve Principal Parts Detection, a task that may be used to further optimize inputs for morphological inflection, and to promote research into the theoretical and practical importance of a compact representation of morphological paradigms.</abstract>
      <url hash="30b543ed">2025.conll-1.17</url>
      <bibkey>keshales-etal-2025-principal</bibkey>
      <doi>10.18653/v1/2025.conll-1.17</doi>
    </paper>
    <paper id="18">
      <title>Accelerating Large Language Model Pretraining via <fixed-case>LFR</fixed-case> Pedagogy: Learn, Focus, and Review</title>
      <author><first>Neha</first><last>Prakriya</last></author>
      <author><first>Jui-Nan</first><last>Yen</last></author>
      <author><first>Cho-Jui</first><last>Hsieh</last><affiliation>Google and University of California, Los Angeles</affiliation></author>
      <author><first>Jason</first><last>Cong</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>268-290</pages>
      <abstract>We introduce an effective and scalable data selection technique to accelerate the pretraining of large language models (LLMs). Given the variation in quality and informativeness of web-scale corpora, we present the Learn-Focus-Review (LFR) paradigm-a dynamic training approach that adapts to the model’s learning progress. Inspired by human learning techniques like spaced repetition, LFR tracks the model’s learning performance across data instances and prioritizes revisiting challenging and diverse regions of the dataset that are more prone to being forgotten, enabling better retention and more efficient learning. Through experiments spanning over 2200 GPU hours, we show that LFR significantly enhances data efficiency in pretraining while improving downstream performance across commonsense reasoning, question answering, problem-solving, language modeling, and translation tasks. LFR consistently achieves lower perplexity and higher accuracy using just 5%–19% of the training tokens as models trained on the full dataset. Notably, LFR matches the performance of industry-standard Pythia models with up to 2<tex-math>\times</tex-math> the parameter count while requiring only 3.2% of the training tokens. Unlike prior work on data selection, LFR models are Chinchilla-optimal demonstrating the effectiveness of our training methodology.</abstract>
      <url hash="509cc693">2025.conll-1.18</url>
      <bibkey>prakriya-etal-2025-accelerating</bibkey>
      <doi>10.18653/v1/2025.conll-1.18</doi>
    </paper>
    <paper id="19">
      <title>What does memory retrieval leave on the table? Modelling the Cost of Semi-Compositionality with <fixed-case>MINERVA</fixed-case>2 and s<fixed-case>BERT</fixed-case></title>
      <author><first>Sydelle</first><last>de Souza</last></author>
      <author><first>Ivan</first><last>Vegner</last></author>
      <author><first>Francis</first><last>Mollica</last></author>
      <author><first>Leonidas A. A.</first><last>Doumas</last></author>
      <pages>291-311</pages>
      <abstract>Despite being ubiquitous in natural language, collocations (e.g., kick+habit) incur a unique processing cost, compared to compositional phrases (kick+door) and idioms (kick+bucket). We confirm this cost with behavioural data as well as MINERVA2, a memory model, suggesting that collocations constitute a distinct linguistic category. While the model fails to fully capture the observed human processing patterns, we find that below a specific item frequency threshold, the model’s retrieval failures align with human reaction times across conditions. This suggests an alternative processing mechanism that activates when memory retrieval fails.</abstract>
      <url hash="aef51cc7">2025.conll-1.19</url>
      <bibkey>de-souza-etal-2025-memory</bibkey>
      <doi>10.18653/v1/2025.conll-1.19</doi>
    </paper>
    <paper id="20">
      <title>Polarity inversion operators in <fixed-case>PLM</fixed-case></title>
      <author><first>David</first><last>Kletz</last></author>
      <author><first>Pascal</first><last>Amsili</last><affiliation>Sorbonne Nouvelle (Paris 3)</affiliation></author>
      <author><first>Marie</first><last>Candito</last><affiliation>Université Paris Cité</affiliation></author>
      <pages>312-322</pages>
      <abstract>From a linguistic perspective, negation is a unique and inherently compositional operator. In this study, we investigate whether the bert-large-cased Pretrained Language Model (PLM) properly encodes this compositional aspect of negation when embedding a token that falls within the scope of negation.To explore this, we train two external Multi-Layer Perceptrons to modify contextual embeddings in a controlled manner. The goal is to reverse the polarity information encoded in the embedding while preserving all other token-related information. The first MLP, called the Negator, transforms a negative polarity into a positive one, while the second, the Affirmator, performs the reverse transformation.We then conduct a series of evaluations to assess the effectiveness of these operators. Our results indicate that while the Negator/Affirmator is functional, it only partially simulates the negation operator. Specifically, applying it recursively does not allow us to recover the original polarity, suggesting an incomplete representation of negation within the PLM’s embeddings.In addition, a downstream evaluation on the Negated LAMA dataset reveals that the modifications introduced by the Negator/Affirmator lead to a slight improvement in the model’s ability to account for negation in its predictions. However, applying the Negator/Affirmator recursively results in degraded representations, further reinforcing the idea that negation is not fully compositional within PLM embeddings.</abstract>
      <url hash="050b8ee5">2025.conll-1.20</url>
      <bibkey>kletz-etal-2025-polarity</bibkey>
      <doi>10.18653/v1/2025.conll-1.20</doi>
    </paper>
    <paper id="21">
      <title>Dynamic Epistemic Friction in Dialogue</title>
      <author><first>Timothy</first><last>Obiso</last><affiliation>Brandeis University</affiliation></author>
      <author><first>Kenneth</first><last>Lai</last><affiliation>Brandeis University and Mass General Brigham</affiliation></author>
      <author><first>Abhijnan</first><last>Nath</last></author>
      <author><first>Nikhil</first><last>Krishnaswamy</last><affiliation>Colorado State University</affiliation></author>
      <author><first>James</first><last>Pustejovsky</last><affiliation>Brandeis University</affiliation></author>
      <pages>323-333</pages>
      <abstract>Recent developments in aligning Large Language Models (LLMs) with human preferences have significantly enhanced their utility in human-AI collaborative scenarios. However, such approaches often neglect the critical role of “epistemic friction,” or the inherent resistance encountered when updating beliefs in response to new, conflicting, or ambiguous information. In this paper, we define *dynamic epistemic friction* as the resistance to epistemic integration, characterized by the misalignment between an agent’s current belief state and new propositions supported by external evidence. We position this within the framework of Dynamic Epistemic Logic, where friction emerges as nontrivial belief-revision during the interaction. We then present analyses from a situated collaborative task that demonstrate how this model of epistemic friction can effectively predict belief updates in dialogues, and we subsequently discuss how the model of belief alignment as a measure of epistemic resistance or friction can naturally be made more sophisticated to accommodate the complexities of real-world dialogue scenarios.</abstract>
      <url hash="575b0f3c">2025.conll-1.21</url>
      <bibkey>obiso-etal-2025-dynamic</bibkey>
      <doi>10.18653/v1/2025.conll-1.21</doi>
    </paper>
    <paper id="22">
      <title>A Three-Tier <fixed-case>LLM</fixed-case> Framework for Forecasting Student Engagement from Qualitative Longitudinal Data</title>
      <author><first>Ahatsham</first><last>Hayat</last><affiliation>University of Nebraska, Lincoln</affiliation></author>
      <author><first>Helen</first><last>Martinez</last></author>
      <author><first>Bilal</first><last>Khan</last><affiliation>Lehigh University</affiliation></author>
      <author><first>Mohammad Rashedul</first><last>Hasan</last><affiliation>University of Nebraska, Lincoln</affiliation></author>
      <pages>334-347</pages>
      <abstract>Forecasting nuanced shifts in student engagement from longitudinal experiential (LE) data—multi-modal, qualitative trajectories of academic experiences over time—remains challenging due to high dimensionality and missingness. We propose a natural language processing (NLP)-driven framework using large language models (LLMs) to forecast binary engagement levels across four dimensions: Lecture Engagement Disposition, Academic Self-Efficacy, Performance Self-Evaluation, and Academic Identity and Value Perception. Evaluated on 960 trajectories from 96 first-year STEM students, our three-tier approach—LLM-informed imputation to generate textual descriptors for missing-not-at-random (MNAR) patterns, zero-shot feature selection via ensemble voting, and fine-tuned LLMs—processes textual non-cognitive responses. LLMs substantially outperform numeric baselines (e.g., Random Forest, LSTM) by capturing contextual nuances in student responses. Encoder-only LLMs surpass decoder-only variants, highlighting architectural strengths for sparse, qualitative LE data. Our framework advances NLP solutions for modeling student engagement from complex LE data, excelling where traditional methods struggle.</abstract>
      <url hash="28e00c2e">2025.conll-1.22</url>
      <attachment type="atachment" hash="f7a665e2">2025.conll-1.22.atachment.pdf</attachment>
      <bibkey>hayat-etal-2025-three</bibkey>
      <doi>10.18653/v1/2025.conll-1.22</doi>
    </paper>
    <paper id="23">
      <title>Bridging the Socioeconomic Gap in Education: A Hybrid <fixed-case>AI</fixed-case> and Human Annotation Approach</title>
      <author><first>Nahed</first><last>Abdelgaber</last><affiliation>Southern Methodist University, Southern Methodist University</affiliation></author>
      <author><first>Labiba</first><last>Jahan</last><affiliation>Southern Methodist University</affiliation></author>
      <author><first>Arham Vinit</first><last>Doshi</last><affiliation>NA</affiliation></author>
      <author><first>Rishi</first><last>Suri</last></author>
      <author><first>Hamza Reza</first><last>Pavel</last><affiliation>NA</affiliation></author>
      <author><first>Jia</first><last>Zhang</last><affiliation>Southern Methodist University, Southern Methodist University</affiliation></author>
      <pages>348-364</pages>
      <abstract>Students’ academic performance is influenced by various demographic factors, with socioeconomic class being a prominently researched and debated factor. Computer Science research traditionally prioritizes computationally definable problems, yet challenges such as the scarcity of high-quality labeled data and ethical concerns surrounding the mining of personal information can pose barriers to exploring topics like the impact of SES on students’ education. Overcoming these barriers may involve automating the collection and annotation of high-quality language data from diverse social groups through human collaboration. Therefore, our focus is on gathering unstructured narratives from Internet forums written by students with low socioeconomic status (SES) using machine learning models and human insights. We developed a hybrid data collection model that semi-automatically retrieved narratives from the Reddit website and created a dataset five times larger than the seed dataset. Additionally, we compared the performance of traditional ML models with recent large language models (LLMs) in classifying narratives written by low-SES students, and analyzed the collected data to extract valuable insights into the socioeconomic challenges these students encounter and the solutions they pursue.</abstract>
      <url hash="065ab745">2025.conll-1.23</url>
      <attachment type="software" hash="00185625">2025.conll-1.23.software.zip</attachment>
      <bibkey>abdelgaber-etal-2025-bridging</bibkey>
      <doi>10.18653/v1/2025.conll-1.23</doi>
    </paper>
    <paper id="24">
      <title>Construction Identification and Disambiguation Using <fixed-case>BERT</fixed-case>: A Case Study of <fixed-case>NPN</fixed-case></title>
      <author><first>Wesley</first><last>Scivetti</last></author>
      <author><first>Nathan</first><last>Schneider</last><affiliation>Georgetown University</affiliation></author>
      <pages>365-376</pages>
      <abstract>Construction Grammar hypothesizes that knowledge of a language consists chiefly of knowledge of form–meaning pairs (“constructions”) that include vocabulary, general grammar rules, and even idiosyncratic patterns. Recent work has shown that transformer language models represent at least some constructional patterns, including ones where the construction is rare overall. In this work, we probe BERT’s representation of the form and meaning of a minor construction of English, the NPN (noun–preposition–noun) construction—exhibited in such expressions as face to face and day to day—which is known to be polysemous. We construct a benchmark dataset of semantically annotated corpus instances (including distractors that superficially resemble the construction). With this dataset, we train and evaluate probing classifiers. They achieve decent discrimination of the construction from distractors, as well as sense disambiguation among true instances of the construction, revealing that BERT embeddings carry indications of the construction’s semantics.Moreover, artificially permuting the word order of true construction instances causes them to be rejected, indicating sensitivity to matters of form. We conclude that BERT does latently encode at least some knowledge of the NPN construction going beyond a surface syntactic pattern and lexical cues.</abstract>
      <url hash="3c0cfdb9">2025.conll-1.24</url>
      <attachment type="software" hash="a6a6d803">2025.conll-1.24.software.zip</attachment>
      <bibkey>scivetti-schneider-2025-construction</bibkey>
      <doi>10.18653/v1/2025.conll-1.24</doi>
    </paper>
    <paper id="25">
      <title>Evidence of Generative Syntax in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Mary</first><last>Kennedy</last><affiliation>University of Southern California</affiliation></author>
      <pages>377-396</pages>
      <abstract>The syntactic probing literature has been largely limited to shallow structures like dependency trees, which are unable to capture the subtle differences in sub-surface syntactic structures that yield semantic nuances. These structures are captured by theories of syntax like generative syntax, but have not been researched in the LLM literature due to the difficulties in probing these complex structures with many silent, covert nodes. Our work presents a method for overcoming this limitation by deploying Hewitt and Manning’s (2019) dependency-trained probe on sentence constructions whose structural representation is identical in a dependency parse, but differs in theoretical syntax. If a pretrained language model has captured the theoretical syntax structure, then the probe’s predicted distances should vary in syntactically-predicted ways. Using this methodology and a novel dataset, we find evidence that LLMs have captured syntactic structures far richer than previously realized, indicating LLMs are able to capture the nuanced meanings that result from sub-surface differences in structural form.</abstract>
      <url hash="e159b6b9">2025.conll-1.25</url>
      <bibkey>kennedy-2025-evidence</bibkey>
      <doi>10.18653/v1/2025.conll-1.25</doi>
    </paper>
    <paper id="26">
      <title>Timestep Embeddings Trigger Collapse in Diffusion Text Generation</title>
      <author><first>Ryota</first><last>Nosaka</last></author>
      <author><first>Takuya</first><last>Matsuzaki</last></author>
      <pages>397-406</pages>
      <abstract>Diffusion models have achieved remarkable success in various generative tasks, particularly in image and audio synthesis, which work by iteratively refining random noise into realistic data. Recent studies have highlighted the potential of diffusion models for text generation, but several challenges remain unresolved. One significant issue is that the model begins to degrade a previous sample rather than improve it after a certain timestep in the generation process, resulting in broken text. In this paper, we reveal that timestep embeddings are a principal cause of the collapse problem by analyzing their interactions with word embeddings. Further, we propose two key methods: (a) a simple lightweight word embedding technique that enhances model analyzability as well as learning efficiency; (b) a novel regularization on both word and timestep embeddings. Experimental results demonstrate that our approach effectively mitigates the collapse problem and can lead to a considerable improvement in the quality of generated text.</abstract>
      <url hash="bb7829ae">2025.conll-1.26</url>
      <bibkey>nosaka-matsuzaki-2025-timestep</bibkey>
      <doi>10.18653/v1/2025.conll-1.26</doi>
    </paper>
    <paper id="27">
      <title>Investigating Psychometric Predictive Power of Syntactic Attention</title>
      <author><first>Ryo</first><last>Yoshida</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Yushi</first><last>Sugimoto</last><affiliation>Osaka University</affiliation></author>
      <author><first>Yohei</first><last>Oseki</last><affiliation>University of Tokyo</affiliation></author>
      <pages>407-418</pages>
      <abstract>In computational psycholinguistics, Merkx and Frank (2021) demonstrated that surprisal values from Transformers exhibit a closer fit to measures of human reading effort than those from Recurrent Neural Networks (RNNs), suggesting that Transformers’ attention mechanisms may capture cue-based retrieval-like operations in human sentence processing. Meanwhile, explicit integration of syntactic structures has been shown to improve language models’ ability to model human sentence processing—for example, Hale et al. (2018) demonstrated that Recurrent Neural Network Grammars (RNNGs), which integrate RNNs with explicit syntactic structures, account for human brain activities that vanilla RNNs cannot capture. In this paper, we investigate the psychometric predictive power of Composition Attention Grammars (CAGs), which integrate Transformers with explicit syntactic structures, to test whether they provide a better fit to human reading times than both vanilla Transformers and RNNGs. We hypothesized that CAGs’ syntactic attention mechanisms capture cue-based retrieval-like operations over syntactic memory representations—operations that may be involved in human sentence processing. The results of our strictly controlled experiments demonstrate that CAGs outperformed vanilla Transformers and RNNGs, suggesting that the syntactic attention mechanisms of CAGs may serve as a mechanistic implementation of cue-based retrieval from syntactic memory.</abstract>
      <url hash="e93cdde5">2025.conll-1.27</url>
      <bibkey>yoshida-etal-2025-investigating</bibkey>
      <doi>10.18653/v1/2025.conll-1.27</doi>
    </paper>
    <paper id="28">
      <title>A Continuous Approach to Metaphorically Motivated Regular Polysemy in Language Models</title>
      <author><first>Anna</first><last>Temerko</last><affiliation>CiTIUS - Centro Singular de Investigación en Tecnoloxías Intelixentes and Universidade de Santiago de Compostela</affiliation></author>
      <author><first>Marcos</first><last>Garcia</last><affiliation>Universidade de Santiago de Compostela</affiliation></author>
      <author><first>Pablo</first><last>Gamallo</last><affiliation>Universidad de Santiago de Compostela</affiliation></author>
      <pages>419-436</pages>
      <abstract>Linguistic accounts show that a word’s polysemy structure is largely governed by systematic sense alternations that form overarching patterns across the vocabulary. While psycholinguistic studies confirm the psychological validity of regularity in human language processing, in the research on large language models (LLMs) this phenomenon remains largely unaddressed. Revealing models’ sensitivity to systematic sense alternations of polysemous words can give us a better understanding of how LLMs process ambiguity and to what extent they emulate representations in the human mind. For this, we employ the measures of surprisal and semantic similarity as proxies of human judgment on the acceptability of novel senses. We focus on two aspects that have not received much attention previously —metaphorically motivated patterns and the continuous nature of regularity. We find evidence that surprisal from language models represents regularity of polysemic extensions in a human-like way, discriminating between different types of senses and varying regularity degrees, and overall strongly correlating with human acceptability scores.</abstract>
      <url hash="e06ec37b">2025.conll-1.28</url>
      <bibkey>temerko-etal-2025-continuous</bibkey>
      <doi>10.18653/v1/2025.conll-1.28</doi>
    </paper>
    <paper id="29">
      <title>Is Incremental Structure Prediction Process Universal across Languages?: Revisiting Parsing Strategy through Speculation</title>
      <author><first>Taiga</first><last>Ishii</last><affiliation>The University of Tokyo, Tokyo Institute of Technology</affiliation></author>
      <author><first>Yusuke</first><last>Miyao</last><affiliation>The University of Tokyo</affiliation></author>
      <pages>437-451</pages>
      <abstract>While natural language is processed incrementally, it is unclear whether the syntactic structure prediction process is universal across languages or language-specific. This study investigates this question by revisiting parsing strategies of syntactic language models that incrementally predict both the next token and the associated syntactic structure. Unlike previous studies that have focused on a few strategies, we examine a wide range of strategies by introducing different parameterizations of “speculation”, which quantifies the degree to which a model predicts syntactic structure before encountering the corresponding tokens. The experiments with 10 typologically diverse languages reveal that the optimal strategy differs depending on the language and the beam size.</abstract>
      <url hash="3ddb7229">2025.conll-1.29</url>
      <bibkey>ishii-miyao-2025-incremental</bibkey>
      <doi>10.18653/v1/2025.conll-1.29</doi>
    </paper>
    <paper id="30">
      <title>Lost in Variation? Evaluating <fixed-case>NLI</fixed-case> Performance in <fixed-case>B</fixed-case>asque and <fixed-case>S</fixed-case>panish Geographical Variants</title>
      <author><first>Jaione</first><last>Bengoetxea</last><affiliation>Universidad del País Vasco</affiliation></author>
      <author><first>Itziar</first><last>Gonzalez-Dios</last><affiliation>Universidad del País Vasco</affiliation></author>
      <author><first>Rodrigo</first><last>Agerri</last><affiliation>University of the Basque Country</affiliation></author>
      <pages>452-468</pages>
      <abstract>In this paper, we evaluate the capacity of current language technologies to understand Basque and Spanish language varieties. We use Natural Language Inference (NLI) as a pivot task and introduce a novel, manually-curated parallel dataset in Basque and Spanish, along with their respective variants. Our empirical analysis of crosslingual and in-context learning experiments using encoder-only and decoder-based Large Language Models (LLMs) shows a performance drop when handling linguistic variation, especially in Basque. Error analysis suggests that this decline is not due to lexical overlap, but rather to the linguistic variation itself. Further ablation experiments indicate that encoder-only models particularly struggle with Western Basque, which aligns with linguistic theory that identifies peripheral dialects (e.g., Western) as more distant from the standard. All data and code are publicly available.</abstract>
      <url hash="7fe80ce8">2025.conll-1.30</url>
      <bibkey>bengoetxea-etal-2025-lost</bibkey>
      <doi>10.18653/v1/2025.conll-1.30</doi>
    </paper>
    <paper id="31">
      <title>Compositionality and Event Retrieval in Complement Coercion: A Study of Language Models in a Low-resource Setting</title>
      <author><first>Matteo</first><last>Radaelli</last></author>
      <author><first>Emmanuele</first><last>Chersoni</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Alessandro</first><last>Lenci</last><affiliation>University of Pisa</affiliation></author>
      <author><first>Giosuè</first><last>Baggio</last><affiliation>Norwegian University of Science and Technology</affiliation></author>
      <pages>469-480</pages>
      <abstract>In sentences such as John began the book, the complement noun, lexically denoting an entity, is interpreted as an event. This phenomenon is known in linguistics as complement coercion: the event associated with the verb is not overtly expressed but can be recovered from the meanings of other constituents, context and world knowledge. We investigate whether language models (LMs) can exploit sentence structure and compositional meaning to recover plausible events in complement coercion. For the first time, we tested different LMs in Norwegian, a low-resource language with high syntactic variation in coercion constructions across aspectual verbs. Results reveal that LMs struggle with retrieving plausible events and with ranking them above less plausible ones. Moreover, we found that LMs do not exploit the compositional properties of coercion sentences in their predictions.</abstract>
      <url hash="4b1cf28c">2025.conll-1.31</url>
      <bibkey>radaelli-etal-2025-compositionality</bibkey>
      <doi>10.18653/v1/2025.conll-1.31</doi>
    </paper>
    <paper id="32">
      <title><fixed-case>DLU</fixed-case>: Dictionary Look-Up Data and Prediction</title>
      <author><first>David</first><last>Strohmaier</last><affiliation>Computer Laboratory</affiliation></author>
      <author><first>Gladys</first><last>Tyen</last><affiliation>Google</affiliation></author>
      <author><first>Hongyi</first><last>Gu</last></author>
      <author><first>Diane</first><last>Nicholls</last></author>
      <author id="zheng-yuan-cam"><first>Zheng</first><last>Yuan</last><affiliation>University of Sheffield</affiliation></author>
      <author><first>Paula</first><last>Buttery</last><affiliation>University of Cambridge</affiliation></author>
      <pages>481-501</pages>
      <abstract>Knowing which words language learners struggle with is crucial for developing personalised education technologies. In this paper, we advocate for the novel task of “dictionary look-up prediction” as a means for evaluating the complexity of words in reading tasks. We release the Dictionary Look-Up development dataset (DLU-dev) and the Dialogue Dictionary Look-Up dataset (D-DLU), which is based on chatbot dialogues. We demonstrate that dictionary look-up is a challenging task for LLMs (results are presented for LLaMA, Gemma, and Longformer models). We explore finetuning with the ROC* loss function as a more appropriate loss for this task than the commonly used Binary Cross Entropy (BCE). We show that a feature-based model outperforms the LLMs. Finally, we investigate the transfer between DLU and the related tasks of Complex Word Identification (CWI) and Semantic Error Prediction (SEP), establishing new state-of-the-art results for SEP.</abstract>
      <url hash="fd2d2d28">2025.conll-1.32</url>
      <bibkey>strohmaier-etal-2025-dlu</bibkey>
      <doi>10.18653/v1/2025.conll-1.32</doi>
    </paper>
    <paper id="33">
      <title><fixed-case>IPA</fixed-case> <fixed-case>CHILDES</fixed-case> &amp; <fixed-case>G</fixed-case>2<fixed-case>P</fixed-case>+: Feature-Rich Resources for Cross-Lingual Phonology and Phonemic Language Modeling</title>
      <author><first>Zebulon</first><last>Goriely</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Paula</first><last>Buttery</last><affiliation>University of Cambridge</affiliation></author>
      <pages>502-521</pages>
      <abstract>In this paper, we introduce two resources: (i) G2P+, a tool for converting orthographic datasets to a consistent phonemic representation; and (ii) IPA CHILDES, a phonemic dataset of child-centered speech across 31 languages. Prior tools for grapheme-to-phoneme conversion result in phonemic vocabularies that are inconsistent with established phonemic inventories, an issue which G2P+ addresses by leveraging the inventories in the Phoible database. Using this tool, we augment CHILDES with phonemic transcriptions to produce IPA CHILDES. This new resource fills several gaps in existing phonemic datasets, which often lack multilingual coverage, spontaneous speech, and a focus on child-directed language. We demonstrate the utility of this dataset for phonological research by training phoneme language models on 11 languages and probing them for distinctive features, finding that the distributional properties of phonemes are sufficient to learn major class and place features cross-lingually.</abstract>
      <url hash="a2d4805f">2025.conll-1.33</url>
      <bibkey>goriely-buttery-2025-ipa</bibkey>
      <doi>10.18653/v1/2025.conll-1.33</doi>
    </paper>
    <paper id="34">
      <title><fixed-case>B</fixed-case>aby<fixed-case>LM</fixed-case>’s First Words: Word Segmentation as a Phonological Probing Task</title>
      <author><first>Zebulon</first><last>Goriely</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Paula</first><last>Buttery</last><affiliation>University of Cambridge</affiliation></author>
      <pages>522-539</pages>
      <abstract>Language models provide a key framework for studying linguistic theories based on prediction, but phonological analysis using large language models (LLMs) is difficult; there are few phonological benchmarks beyond English and the standard input representation used in LLMs (subwords of graphemes) is not suitable for analyzing the representation of phonemes. In this work, we demonstrate how word segmentation can be used as a phonological probing task, allowing us to study the representations learned by phoneme-based language models trained on child-directed speech across 31 languages. Following computational models of word segmentation, we present unsupervised methods for extracting word boundaries from a trained model using the observation that prediction-error peaks at the start of words. We also use linear probes to identify that these models implicitly track word boundaries, even when they do not appear in training. This cross-lingual work corroborates statistical learning theories of acquisition and empirically motivates new methods for training subword tokenizers.</abstract>
      <url hash="0912942b">2025.conll-1.34</url>
      <bibkey>goriely-buttery-2025-babylms</bibkey>
      <doi>10.18653/v1/2025.conll-1.34</doi>
    </paper>
    <paper id="35">
      <title><fixed-case>GCG</fixed-case>-Based Artificial Languages for Evaluating Inductive Biases of Neural Language Models</title>
      <author><first>Nadine</first><last>El-Naggar</last></author>
      <author><first>Tatsuki</first><last>Kuribayashi</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Ted</first><last>Briscoe</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>540-556</pages>
      <abstract>Recent work has investigated whether extant neural language models (LMs) have an inbuilt inductive bias towards the acquisition of attested typologically-frequent grammatical patterns as opposed to infrequent, unattested, or impossible patterns using artificial languages (White and Cotterell, 2021; Kuribayashi et al., 2024). The use of artificial languages facilitates isolation of specific grammatical properties from other factors such as lexical or real-world knowledge, but also risks oversimplification of the problem.In this paper, we examine the use of Generalized Categorial Grammars (GCGs) (Wood, 2014) as a general framework to create artificial languages with a wider range of attested word order patterns, including those where the subject intervenes between verb and object (VSO, OSV) and unbounded dependencies in object relative clauses. In our experiments, we exemplify our approach by extending White and Cotterell (2021) and report some significant differences from existing results.</abstract>
      <url hash="3ba4d0ec">2025.conll-1.35</url>
      <bibkey>el-naggar-etal-2025-gcg</bibkey>
      <doi>10.18653/v1/2025.conll-1.35</doi>
    </paper>
    <paper id="36">
      <title>Beyond Accuracy: Revisiting Out-of-Distribution Generalization in <fixed-case>NLI</fixed-case> Models</title>
      <author><first>Zahra</first><last>Delbari</last></author>
      <author><first>Mohammad Taher</first><last>Pilehvar</last><affiliation>Cardiff University and TeIAS</affiliation></author>
      <pages>557-570</pages>
      <abstract>This study investigates how well discriminative transformers generalize in Natural Language Inference (NLI) tasks. We specifically focus on a well-studied bias in this task: the tendency of models to rely on superficial features and dataset biases rather than a true understanding of language. We argue that the performance differences observed between training and analysis datasets do not necessarily indicate a lack of knowledge within the model. Instead, the gap often points to a misalignment between the decision boundaries of the classifier head and the representations learned by the encoder for the analysis samples. By investigating the representation space of NLI models across different analysis datasets, we demonstrate that even when the accuracy is nearly random in some settings, still samples from opposing classes remain almost perfectly linearly separable in the encoder’s representation space. This suggests that, although the classifier head may fail on analysis data, the encoder still generalizes and encodes representations that allow for effective discrimination between NLI classes.</abstract>
      <url hash="20db80f0">2025.conll-1.36</url>
      <bibkey>delbari-pilehvar-2025-beyond</bibkey>
      <doi>10.18653/v1/2025.conll-1.36</doi>
    </paper>
    <paper id="37">
      <title>Spatial relation marking across languages: extraction, evaluation, analysis</title>
      <author><first>Barend</first><last>Beekhuizen</last><affiliation>University of Toronto</affiliation></author>
      <pages>571-585</pages>
      <abstract>This paper presents a novel task, detecting Spatial Relation Markers (SRMs, like English _**in** the bag_), across languages, alongside a model for this task, RUIMTE. Using a massively parallel corpus of Bible translations, the model is evaluated against existing and baseline models on the basis of a novel evaluation set. The model presents high quality SRM extraction, and an accurate identification of situations where language have zero-marked SRMs.</abstract>
      <url hash="3794c4e4">2025.conll-1.37</url>
      <bibkey>beekhuizen-2025-spatial</bibkey>
      <doi>10.18653/v1/2025.conll-1.37</doi>
    </paper>
    <paper id="38">
      <title>Human-likeness of <fixed-case>LLM</fixed-case>s in the Mental Lexicon</title>
      <author><first>Bei</first><last>Xiao</last></author>
      <author><first>Xufeng</first><last>Duan</last></author>
      <author><first>David A.</first><last>Haslett</last></author>
      <author><first>Zhenguang</first><last>Cai</last></author>
      <pages>586-601</pages>
      <abstract>Recent research has increasingly focused on the extent to which large language models (LLMs) exhibit human-like behavior. In this study, we investigate whether the mental lexicon in LLMs resembles that of humans in terms of lexical organization. Using a word association task—a direct and widely used method for probing word meaning and relationships in the human mind—we evaluated the lexical representations of GPT-4 and Llama-3.1. Our findings reveal that LLMs closely emulate human mental lexicons in capturing semantic relatedness but exhibit notable differences in other properties, such as association frequency and dominant lexical patterns (e.g., top associates). Specifically, LLM lexicons demonstrate greater clustering and reduced diversity compared to the human lexicon, with KL divergence analysis confirming significant deviations in word association patterns. Additionally, LLMs fail to fully capture word association response patterns in different demographic human groups. Among the models, GPT-4 consistently exhibited a slightly higher degree of human-likeness than Llama-3.1. This study highlights both the potential and limitations of LLMs in replicating human mental lexicons, offering valuable insights for applications in natural language processing and cognitive science research involving LLMs.</abstract>
      <url hash="5bd6badf">2025.conll-1.38</url>
      <bibkey>xiao-etal-2025-human</bibkey>
      <doi>10.18653/v1/2025.conll-1.38</doi>
    </paper>
    <paper id="39">
      <title>Vorm: Translations and a constrained hypothesis space support unsupervised morphological segmentation across languages</title>
      <author><first>Barend</first><last>Beekhuizen</last><affiliation>University of Toronto</affiliation></author>
      <pages>602-626</pages>
      <abstract>This paper introduces Vorm, an unsupervised morphological segmentation system, leveraging translation data to infer highly accurate morphological transformations, including less-frequently modeled processes such as infixation and reduplication. The system is evaluated on standard benchmark data and a novel, typologically diverse, dataset of 37 languages. Model performance is competitive and sometimes superior on canonical segmentation, but more limited on surface segmentation.</abstract>
      <url hash="5fac313f">2025.conll-1.39</url>
      <bibkey>beekhuizen-2025-vorm</bibkey>
      <doi>10.18653/v1/2025.conll-1.39</doi>
    </paper>
    <paper id="40">
      <title>Do large language models solve verbal analogies like children do?</title>
      <author><first>Tamar</first><last>Johnson</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Mathilde</first><last>ter Veen</last><affiliation>NA</affiliation></author>
      <author><first>Rochelle</first><last>Choenni</last></author>
      <author><first>Han</first><last>van der Maas</last><affiliation>NA</affiliation></author>
      <author><first>Ekaterina</first><last>Shutova</last><affiliation>Stanford University and University of Amsterdam</affiliation></author>
      <author><first>Claire E</first><last>Stevenson</last><affiliation>University of Amsterdam</affiliation></author>
      <pages>627-639</pages>
      <abstract>Analogy-making lies at the heart of human cognition. Adults solve analogies such as <tex-math>\textit{horse belongs to stable like chicken belongs to …?}</tex-math> by mapping relations (<tex-math>\textit{kept in}</tex-math>) and answering <tex-math>\textit{chicken coop}</tex-math>. In contrast, young children often use association, e.g., answering <tex-math>\textit{egg}</tex-math>. This paper investigates whether large language models (LLMs) solve verbal analogies in A:B::C:? form using associations, similar to what children do. We use verbal analogies extracted from an online learning environment, where 14,006 7-12 year-olds from the Netherlands solved 872 analogies in Dutch. The eight tested LLMs performed at or above the level of children, with some models approaching adult performance estimates. However, when we control for solving by association this picture changes. We conclude that the LLMs we tested rely heavily on association like young children do. However, LLMs make different errors than children, and association doesn’t fully explain their superior performance on this children’s verbal analogy task. Future work will investigate whether LLMs associations and errors are more similar to adult relational reasoning.</abstract>
      <url hash="6feade06">2025.conll-1.40</url>
      <bibkey>johnson-etal-2025-large</bibkey>
      <doi>10.18653/v1/2025.conll-1.40</doi>
    </paper>
  </volume>
</collection>
