<?xml version='1.0' encoding='UTF-8'?>
<collection id="J19">
  <volume id="1">
    <meta>
      <booktitle>Computational Linguistics, Volume 45, Issue 1 - March 2019</booktitle>
      <month>March</month>
      <year>2019</year>
    </meta>
    <frontmatter/>
    <paper id="1">
      <title>Unsupervised Compositionality Prediction of Nominal Compounds</title>
      <author><first>Silvio</first><last>Cordeiro</last></author>
      <author><first>Aline</first><last>Villavicencio</last></author>
      <author><first>Marco</first><last>Idiart</last></author>
      <author><first>Carlos</first><last>Ramisch</last></author>
      <doi>10.1162/coli_a_00341</doi>
      <abstract>Nominal compounds such as red wine and nut case display a continuum of compositionality, with varying contributions from the components of the compound to its semantics. This article proposes a framework for compound compositionality prediction using distributional semantic models, evaluating to what extent they capture idiomaticity compared to human judgments. For evaluation, we introduce data sets containing human judgments in three languages: English, French, and Portuguese. The results obtained reveal a high agreement between the models and human predictions, suggesting that they are able to incorporate information about idiomaticity. We also present an in-depth evaluation of various factors that can affect prediction, such as model and corpus parameters and compositionality operations. General crosslingual analyses reveal the impact of morphological variation and corpus size in the ability of the model to predict compositionality, and of a uniform combination of the components for best results.</abstract>
      <pages>1–57</pages>
      <url hash="1f3c1c4c">J19-1001</url>
    </paper>
    <paper id="2">
      <title>Learning an Executable Neural Semantic Parser</title>
      <author><first>Jianpeng</first><last>Cheng</last></author>
      <author><first>Siva</first><last>Reddy</last></author>
      <author><first>Vijay</first><last>Saraswat</last></author>
      <author><first>Mirella</first><last>Lapata</last></author>
      <doi>10.1162/coli_a_00342</doi>
      <abstract>This article describes a neural semantic parser that maps natural language utterances onto logical forms that can be executed against a task-specific environment, such as a knowledge base or a database, to produce a response. The parser generates tree-structured logical forms with a transition-based approach, combining a generic tree-generation algorithm with domain-general grammar defined by the logical language. The generation process is modeled by structured recurrent neural networks, which provide a rich encoding of the sentential context and generation history for making predictions. To tackle mismatches between natural language and logical form tokens, various attention mechanisms are explored. Finally, we consider different training settings for the neural semantic parser, including fully supervised training where annotated logical forms are given, weakly supervised training where denotations are provided, and distant supervision where only unlabeled sentences and a knowledge base are available. Experiments across a wide range of data sets demonstrate the effectiveness of our parser.</abstract>
      <pages>59–94</pages>
      <url hash="a5350168">J19-1002</url>
    </paper>
    <paper id="3">
      <title>Parsing <fixed-case>C</fixed-case>hinese Sentences with Grammatical Relations</title>
      <author><first>Weiwei</first><last>Sun</last></author>
      <author><first>Yufei</first><last>Chen</last></author>
      <author><first>Xiaojun</first><last>Wan</last></author>
      <author><first>Meichun</first><last>Liu</last></author>
      <doi>10.1162/coli_a_00343</doi>
      <abstract>We report our work on building linguistic resources and data-driven parsers in the grammatical relation (GR) analysis for Mandarin Chinese. Chinese, as an analytic language, encodes grammatical information in a highly configurational rather than morphological way. Accordingly, it is possible and reasonable to represent almost all grammatical relations as bilexical dependencies. In this work, we propose to represent grammatical information using general directed dependency graphs. Both only-local and rich long-distance dependencies are explicitly represented. To create high-quality annotations, we take advantage of an existing TreeBank, namely, Chinese TreeBank (CTB), which is grounded on the Government and Binding theory. We define a set of linguistic rules to explore CTB’s implicit phrase structural information and build deep dependency graphs. The reliability of this linguistically motivated GR extraction procedure is highlighted by manual evaluation. Based on the converted corpus, data-driven, including graph- and transition-based, models are explored for Chinese GR parsing. For graph-based parsing, a new perspective, graph merging, is proposed for building flexible dependency graphs: constructing complex graphs via constructing simple subgraphs. Two key problems are discussed in this perspective: (1) how to decompose a complex graph into simple subgraphs, and (2) how to combine subgraphs into a coherent complex graph. For transition-based parsing, we introduce a neural parser based on a list-based transition system. We also discuss several other key problems, including dynamic oracle and beam search for neural transition-based parsing. Evaluation gauges how successful GR parsing for Chinese can be by applying data-driven models. The empirical analysis suggests several directions for future study.</abstract>
      <pages>95–136</pages>
      <url hash="673f5e48">J19-1003</url>
    </paper>
    <paper id="4">
      <title>Automatic Inference of Sound Correspondence Patterns across Multiple Languages</title>
      <author><first>Johann-Mattis</first><last>List</last></author>
      <doi>10.1162/coli_a_00344</doi>
      <abstract>Sound correspondence patterns play a crucial role for linguistic reconstruction. Linguists use them to prove language relationship, to reconstruct proto-forms, and for classical phylogenetic reconstruction based on shared innovations. Cognate words that fail to conform with expected patterns can further point to various kinds of exceptions in sound change, such as analogy or assimilation of frequent words. Here I present an automatic method for the inference of sound correspondence patterns across multiple languages based on a network approach. The core idea is to represent all columns in aligned cognate sets as nodes in a network with edges representing the degree of compatibility between the nodes. The task of inferring all compatible correspondence sets can then be handled as the well-known minimum clique cover problem in graph theory, which essentially seeks to split the graph into the smallest number of cliques in which each node is represented by exactly one clique. The resulting partitions represent all correspondence patterns that can be inferred for a given data set. By excluding those patterns that occur in only a few cognate sets, the core of regularly recurring sound correspondences can be inferred. Based on this idea, the article presents a method for automatic correspondence pattern recognition, which is implemented as part of a Python library which supplements the article. To illustrate the usefulness of the method, I present how the inferred patterns can be used to predict words that have not been observed before.</abstract>
      <pages>137–161</pages>
      <url hash="22f63a92">J19-1004</url>
    </paper>
    <paper id="5">
      <title>A Sequential Matching Framework for Multi-Turn Response Selection in Retrieval-Based Chatbots</title>
      <author><first>Yu</first><last>Wu</last></author>
      <author><first>Wei</first><last>Wu</last></author>
      <author><first>Chen</first><last>Xing</last></author>
      <author><first>Can</first><last>Xu</last></author>
      <author><first>Zhoujun</first><last>Li</last></author>
      <author><first>Ming</first><last>Zhou</last></author>
      <doi>10.1162/coli_a_00345</doi>
      <abstract>We study the problem of response selection for multi-turn conversation in retrieval-based chatbots. The task involves matching a response candidate with a conversation context, the challenges for which include how to recognize important parts of the context, and how to model the relationships among utterances in the context. Existing matching methods may lose important information in contexts as we can interpret them with a unified framework in which contexts are transformed to fixed-length vectors without any interaction with responses before matching. This motivates us to propose a new matching framework that can sufficiently carry important information in contexts to matching and model relationships among utterances at the same time. The new framework, which we call a sequential matching framework (SMF), lets each utterance in a context interact with a response candidate at the first step and transforms the pair to a matching vector. The matching vectors are then accumulated following the order of the utterances in the context with a recurrent neural network (RNN) that models relationships among utterances. Context-response matching is then calculated with the hidden states of the RNN. Under SMF, we propose a sequential convolutional network and sequential attention network and conduct experiments on two public data sets to test their performance. Experiment results show that both models can significantly outperform state-of-the-art matching methods. We also show that the models are interpretable with visualizations that provide us insights on how they capture and leverage important information in contexts for matching.</abstract>
      <pages>163–197</pages>
      <url hash="1a517ee4">J19-1005</url>
    </paper>
  </volume>
  <volume id="2">
    <meta>
      <booktitle>Computational Linguistics, Volume 45, Issue 2 - June 2019</booktitle>
      <month>June</month>
      <year>2019</year>
    </meta>
    <paper id="1">
      <title><fixed-case>B</fixed-case>ayesian Learning of Latent Representations of Language Structures</title>
      <author><first>Yugo</first><last>Murawaki</last></author>
      <doi>10.1162/coli_a_00346</doi>
      <abstract>We borrow the concept of representation learning from deep learning research, and we argue that the quest for Greenbergian implicational universals can be reformulated as the learning of good latent representations of languages, or sequences of surface typological features. By projecting languages into latent representations and performing inference in the latent space, we can handle complex dependencies among features in an implicit manner. The most challenging problem in turning the idea into a concrete computational model is the alarmingly large number of missing values in existing typological databases. To address this problem, we keep the number of model parameters relatively small to avoid overfitting, adopt the Bayesian learning framework for its robustness, and exploit phylogenetically and/or spatially related languages as additional clues. Experiments show that the proposed model recovers missing values more accurately than others and that some latent variables exhibit phylogenetic and spatial signals comparable to those of surface features.</abstract>
      <pages>199–228</pages>
      <url hash="c87b1cef">J19-2001</url>
    </paper>
    <paper id="2">
      <title>Novel Event Detection and Classification for Historical Texts</title>
      <author><first>Rachele</first><last>Sprugnoli</last></author>
      <author><first>Sara</first><last>Tonelli</last></author>
      <doi>10.1162/coli_a_00347</doi>
      <abstract>Event processing is an active area of research in the Natural Language Processing community, but resources and automatic systems developed so far have mainly addressed contemporary texts. However, the recognition and elaboration of events is a crucial step when dealing with historical texts Particularly in the current era of massive digitization of historical sources: Research in this domain can lead to the development of methodologies and tools that can assist historians in enhancing their work, while having an impact also on the field of Natural Language Processing. Our work aims at shedding light on the complex concept of events when dealing with historical texts. More specifically, we introduce new annotation guidelines for event mentions and types, categorized into 22 classes. Then, we annotate a historical corpus accordingly, and compare two approaches for automatic event detection and classification following this novel scheme. We believe that this work can foster research in a field of inquiry as yet underestimated in the area of Temporal Information Processing. To this end, we release new annotation guidelines, a corpus, and new models for automatic annotation.</abstract>
      <pages>229–265</pages>
      <url hash="0e7ad813">J19-2002</url>
    </paper>
    <paper id="3">
      <title>Incorporating Source-Side Phrase Structures into Neural Machine Translation</title>
      <author><first>Akiko</first><last>Eriguchi</last></author>
      <author><first>Kazuma</first><last>Hashimoto</last></author>
      <author><first>Yoshimasa</first><last>Tsuruoka</last></author>
      <doi>10.1162/coli_a_00348</doi>
      <abstract>Neural machine translation (NMT) has shown great success as a new alternative to the traditional Statistical Machine Translation model in multiple languages. Early NMT models are based on sequence-to-sequence learning that encodes a sequence of source words into a vector space and generates another sequence of target words from the vector. In those NMT models, sentences are simply treated as sequences of words without any internal structure. In this article, we focus on the role of the syntactic structure of source sentences and propose a novel end-to-end syntactic NMT model, which we call a tree-to-sequence NMT model, extending a sequence-to-sequence model with the source-side phrase structure. Our proposed model has an attention mechanism that enables the decoder to generate a translated word while softly aligning it with phrases as well as words of the source sentence. We have empirically compared the proposed model with sequence-to-sequence models in various settings on Chinese-to-Japanese and English-to-Japanese translation tasks. Our experimental results suggest that the use of syntactic structure can be beneficial when the training data set is small, but is not as effective as using a bi-directional encoder. As the size of training data set increases, the benefits of using a syntactic tree tends to diminish.</abstract>
      <pages>267–292</pages>
      <url hash="505b1f07">J19-2003</url>
    </paper>
    <paper id="4">
      <title>Neural Models of Text Normalization for Speech Applications</title>
      <author><first>Hao</first><last>Zhang</last></author>
      <author><first>Richard</first><last>Sproat</last></author>
      <author><first>Axel H.</first><last>Ng</last></author>
      <author><first>Felix</first><last>Stahlberg</last></author>
      <author><first>Xiaochang</first><last>Peng</last></author>
      <author><first>Kyle</first><last>Gorman</last></author>
      <author><first>Brian</first><last>Roark</last></author>
      <doi>10.1162/coli_a_00349</doi>
      <abstract>Machine learning, including neural network techniques, have been applied to virtually every domain in natural language processing. One problem that has been somewhat resistant to effective machine learning solutions is text normalization for speech applications such as text-to-speech synthesis (TTS). In this application, one must decide, for example, that 123 is verbalized as one hundred twenty three in 123 pages but as one twenty three in 123 King Ave. For this task, state-of-the-art industrial systems depend heavily on hand-written language-specific grammars.We propose neural network models that treat text normalization for TTS as a sequence-to-sequence problem, in which the input is a text token in context, and the output is the verbalization of that token. We find that the most effective model, in accuracy and efficiency, is one where the sentential context is computed once and the results of that computation are combined with the computation of each token in sequence to compute the verbalization. This model allows for a great deal of flexibility in terms of representing the context, and also allows us to integrate tagging and segmentation into the process.These models perform very well overall, but occasionally they will predict wildly inappropriate verbalizations, such as reading 3 cm as three kilometers. Although rare, such verbalizations are a major issue for TTS applications. We thus use finite-state covering grammars to guide the neural models, either during training and decoding, or just during decoding, away from such “unrecoverable” errors. Such grammars can largely be learned from data.</abstract>
      <pages>293–337</pages>
      <url hash="6ddc519a">J19-2004</url>
    </paper>
    <paper id="5">
      <title>Ordered Tree Decomposition for <fixed-case>HRG</fixed-case> Rule Extraction</title>
      <author><first>Daniel</first><last>Gildea</last></author>
      <author><first>Giorgio</first><last>Satta</last></author>
      <author><first>Xiaochang</first><last>Peng</last></author>
      <doi>10.1162/coli_a_00350</doi>
      <abstract>We present algorithms for extracting Hyperedge Replacement Grammar (HRG) rules from a graph along with a vertex order. Our algorithms are based on finding a tree decomposition of smallest width, relative to the vertex order, and then extracting one rule for each node in this structure. The assumption of a fixed order for the vertices of the input graph makes it possible to solve the problem in polynomial time, in contrast to the fact that the problem of finding optimal tree decompositions for a graph is NP-hard. We also present polynomial-time algorithms for parsing based on our HRGs, where the input is a vertex sequence and the output is a graph structure. The intended application of our algorithms is grammar extraction and parsing for semantic representation of natural language. We apply our algorithms to data annotated with Abstract Meaning Representations and report on the characteristics of the resulting grammars.</abstract>
      <pages>339–379</pages>
      <url hash="f085cb28">J19-2005</url>
    </paper>
    <paper id="6">
      <title>What Do Language Representations Really Represent?</title>
      <author><first>Johannes</first><last>Bjerva</last></author>
      <author><first>Robert</first><last>Östling</last></author>
      <author><first>Maria Han</first><last>Veiga</last></author>
      <author><first>Jörg</first><last>Tiedemann</last></author>
      <author><first>Isabelle</first><last>Augenstein</last></author>
      <doi>10.1162/coli_a_00351</doi>
      <abstract>A neural language model trained on a text corpus can be used to induce distributed representations of words, such that similar words end up with similar representations. If the corpus is multilingual, the same model can be used to learn distributed representations of languages, such that similar languages end up with similar representations. We show that this holds even when the multilingual corpus has been translated into English, by picking up the faint signal left by the source languages. However, just as it is a thorny problem to separate semantic from syntactic similarity in word representations, it is not obvious what type of similarity is captured by language representations. We investigate correlations and causal relationships between language representations learned from translations on one hand, and genetic, geographical, and several levels of structural similarity between languages on the other. Of these, structural similarity is found to correlate most strongly with language representation similarity, whereas genetic relationships—a convenient benchmark used for evaluation in previous work—appears to be a confounding factor. Apart from implications about translation effects, we see this more generally as a case where NLP and linguistic typology can interact and benefit one another.</abstract>
      <pages>381–389</pages>
      <url hash="c8c6b867">J19-2006</url>
    </paper>
  </volume>
  <volume id="3">
    <meta>
      <booktitle>Computational Linguistics, Volume 45, Issue 3 - September 2019</booktitle>
      <month>September</month>
      <year>2019</year>
    </meta>
    <paper id="1">
      <title>Contextualized Translations of Phrasal Verbs with Distributional Compositional Semantics and Monolingual Corpora</title>
      <author><first>Pablo</first><last>Gamallo</last></author>
      <author><first>Susana</first><last>Sotelo</last></author>
      <author><first>José Ramom</first><last>Pichel</last></author>
      <author><first>Mikel</first><last>Artetxe</last></author>
      <doi>10.1162/coli_a_00353</doi>
      <abstract>This article describes a compositional distributional method to generate contextualized senses of words and identify their appropriate translations in the target language using monolingual corpora. Word translation is modeled in the same way as contextualization of word meaning, but in a bilingual vector space. The contextualization of meaning is carried out by means of distributional composition within a structured vector space with syntactic dependencies, and the bilingual space is created by means of transfer rules and a bilingual dictionary. A phrase in the source language, consisting of a head and a dependent, is translated into the target language by selecting both the nearest neighbor of the head given the dependent, and the nearest neighbor of the dependent given the head. This process is expanded to larger phrases by means of incremental composition. Experiments were performed on English and Spanish monolingual corpora in order to translate phrasal verbs in context. A new bilingual data set to evaluate strategies aimed at translating phrasal verbs in restricted syntactic domains has been created and released.</abstract>
      <pages>395–421</pages>
      <url hash="1c5a71d5">J19-3001</url>
    </paper>
    <paper id="2">
      <title>Watset: Local-Global Graph Clustering with Applications in Sense and Frame Induction</title>
      <author><first>Dmitry</first><last>Ustalov</last></author>
      <author><first>Alexander</first><last>Panchenko</last></author>
      <author><first>Chris</first><last>Biemann</last></author>
      <author><first>Simone Paolo</first><last>Ponzetto</last></author>
      <doi>10.1162/coli_a_00354</doi>
      <abstract>We present a detailed theoretical and computational analysis of the Watset meta-algorithm for fuzzy graph clustering, which has been found to be widely applicable in a variety of domains. This algorithm creates an intermediate representation of the input graph, which reflects the “ambiguity” of its nodes. Then, it uses hard clustering to discover clusters in this “disambiguated” intermediate graph. After outlining the approach and analyzing its computational complexity, we demonstrate that Watset shows competitive results in three applications: unsupervised synset induction from a synonymy graph, unsupervised semantic frame induction from dependency triples, and unsupervised semantic class induction from a distributional thesaurus. Our algorithm is generic and can also be applied to other networks of linguistic data.</abstract>
      <pages>423–479</pages>
      <url hash="b982208f">J19-3002</url>
    </paper>
    <paper id="3">
      <title>Evaluating Computational Language Models with Scaling Properties of Natural Language</title>
      <author><first>Shuntaro</first><last>Takahashi</last></author>
      <author><first>Kumiko</first><last>Tanaka-Ishii</last></author>
      <doi>10.1162/coli_a_00355</doi>
      <abstract>In this article, we evaluate computational models of natural language with respect to the universal statistical behaviors of natural language. Statistical mechanical analyses have revealed that natural language text is characterized by scaling properties, which quantify the global structure in the vocabulary population and the long memory of a text. We study whether five scaling properties (given by Zipf’s law, Heaps’ law, Ebeling’s method, Taylor’s law, and long-range correlation analysis) can serve for evaluation of computational models. Specifically, we test n-gram language models, a probabilistic context-free grammar, language models based on Simon/Pitman-Yor processes, neural language models, and generative adversarial networks for text generation. Our analysis reveals that language models based on recurrent neural networks with a gating mechanism (i.e., long short-term memory; a gated recurrent unit; and quasi-recurrent neural networks) are the only computational models that can reproduce the long memory behavior of natural language. Furthermore, through comparison with recently proposed model-based evaluation methods, we find that the exponent of Taylor’s law is a good indicator of model quality.</abstract>
      <pages>481–513</pages>
      <url hash="3f5ef36c">J19-3003</url>
    </paper>
    <paper id="4">
      <title>Taking <fixed-case>MT</fixed-case> Evaluation Metrics to Extremes: Beyond Correlation with Human Judgments</title>
      <author><first>Marina</first><last>Fomicheva</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <doi>10.1162/coli_a_00356</doi>
      <abstract>Automatic Machine Translation (MT) evaluation is an active field of research, with a handful of new metrics devised every year. Evaluation metrics are generally benchmarked against manual assessment of translation quality, with performance measured in terms of overall correlation with human scores. Much work has been dedicated to the improvement of evaluation metrics to achieve a higher correlation with human judgments. However, little insight has been provided regarding the weaknesses and strengths of existing approaches and their behavior in different settings. In this work we conduct a broad meta-evaluation study of the performance of a wide range of evaluation metrics focusing on three major aspects. First, we analyze the performance of the metrics when faced with different levels of translation quality, proposing a local dependency measure as an alternative to the standard, global correlation coefficient. We show that metric performance varies significantly across different levels of MT quality: Metrics perform poorly when faced with low-quality translations and are not able to capture nuanced quality distinctions. Interestingly, we show that evaluating low-quality translations is also more challenging for humans. Second, we show that metrics are more reliable when evaluating neural MT than the traditional statistical MT systems. Finally, we show that the difference in the evaluation accuracy for different metrics is maintained even if the gold standard scores are based on different criteria.</abstract>
      <pages>515–558</pages>
      <url hash="457f3608">J19-3004</url>
    </paper>
    <paper id="5">
      <title>Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing</title>
      <author><first>Edoardo Maria</first><last>Ponti</last></author>
      <author><first>Helen</first><last>O’Horan</last></author>
      <author><first>Yevgeni</first><last>Berzak</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <author><first>Roi</first><last>Reichart</last></author>
      <author><first>Thierry</first><last>Poibeau</last></author>
      <author><first>Ekaterina</first><last>Shutova</last></author>
      <author><first>Anna</first><last>Korhonen</last></author>
      <doi>10.1162/coli_a_00357</doi>
      <abstract>Linguistic typology aims to capture structural and semantic variation across the world’s languages. A large-scale typology could provide excellent guidance for multilingual Natural Language Processing (NLP), particularly for languages that suffer from the lack of human labeled resources. We present an extensive literature survey on the use of typological information in the development of NLP techniques. Our survey demonstrates that to date, the use of information in existing typological databases has resulted in consistent but modest improvements in system performance. We show that this is due to both intrinsic limitations of databases (in terms of coverage and feature granularity) and under-utilization of the typological features included in them. We advocate for a new approach that adapts the broad and discrete nature of typological categories to the contextual and continuous nature of machine learning algorithms used in contemporary NLP. In particular, we suggest that such an approach could be facilitated by recent developments in data-driven induction of typological knowledge.</abstract>
      <pages>559–601</pages>
      <url hash="9984b96f">J19-3005</url>
    </paper>
  </volume>
  <volume id="4">
    <meta>
      <booktitle>Computational Linguistics, Volume 45, Issue 4 - December 2019</booktitle>
      <month>December</month>
      <year>2019</year>
    </meta>
    <paper id="1">
      <title>Computational Psycholinguistics</title>
      <author><first>Ronald M.</first><last>Kaplan</last></author>
      <doi>10.1162/coli_a_00359</doi>
      <pages>607–626</pages>
      <url hash="7b042947">J19-4001</url>
    </paper>
    <paper id="2">
      <title>Discourse in Multimedia: A Case Study in Extracting Geometry Knowledge from Textbooks</title>
      <author><first>Mrinmaya</first><last>Sachan</last></author>
      <author><first>Avinava</first><last>Dubey</last></author>
      <author><first>Eduard H.</first><last>Hovy</last></author>
      <author><first>Tom M.</first><last>Mitchell</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <author><first>Eric P.</first><last>Xing</last></author>
      <doi>10.1162/coli_a_00360</doi>
      <abstract>To ensure readability, text is often written and presented with due formatting. These text formatting devices help the writer to effectively convey the narrative. At the same time, these help the readers pick up the structure of the discourse and comprehend the conveyed information. There have been a number of linguistic theories on discourse structure of text. However, these theories only consider unformatted text. Multimedia text contains rich formatting features that can be leveraged for various NLP tasks. In this article, we study some of these discourse features in multimedia text and what communicative function they fulfill in the context. As a case study, we use these features to harvest structured subject knowledge of geometry from textbooks. We conclude that the discourse and text layout features provide information that is complementary to lexical semantic information. Finally, we show that the harvested structured knowledge can be used to improve an existing solver for geometry problems, making it more accurate as well as more explainable.</abstract>
      <pages>627–665</pages>
      <url hash="42f3cefc">J19-4002</url>
    </paper>
    <paper id="3">
      <title>Automatic Identification and Production of Related Words for Historical Linguistics</title>
      <author><first>Alina Maria</first><last>Ciobanu</last></author>
      <author><first>Liviu P.</first><last>Dinu</last></author>
      <doi>10.1162/coli_a_00361</doi>
      <abstract>Language change across space and time is one of the main concerns in historical linguistics. In this article, we develop tools to assist researchers and domain experts in the study of language evolution.First, we introduce a method to automatically determine whether two words are cognates. We propose an algorithm for extracting cognates from electronic dictionaries that contain etymological information. Having built a data set of related words, we further develop machine learning methods based on orthographic alignment for identifying cognates. We use aligned subsequences as features for classification algorithms in order to infer rules for linguistic changes undergone by words when entering new languages and to discriminate between cognates and non-cognates.Second, we extend the method to a finer-grained level, to identify the type of relationship between words. Discriminating between cognates and borrowings provides a deeper insight into the history of a language and allows a better characterization of language relatedness. We show that orthographic features have discriminative power and we analyze the underlying linguistic factors that prove relevant in the classification task. To our knowledge, this is the first attempt of this kind.Third, we develop a machine learning method for automatically producing related words. We focus on reconstructing proto-words, but we also address two related sub-problems, producing modern word forms and producing cognates. The task of reconstructing proto-words consists of recreating the words in an ancient language from its modern daughter languages. Having modern word forms in multiple Romance languages, we infer the form of their common Latin ancestors. Our approach relies on the regularities that occurred when words entered the modern languages. We leverage information from several modern languages, building an ensemble system for reconstructing proto-words. We apply our method to multiple data sets, showing that our approach improves on previous results, also having the advantage of requiring less input data, which is essential in historical linguistics, where resources are generally scarce.</abstract>
      <pages>667–704</pages>
      <url hash="c31b066b">J19-4003</url>
    </paper>
    <paper id="4">
      <title>Syntactically Meaningful and Transferable Recursive Neural Networks for Aspect and Opinion Extraction</title>
      <author><first>Wenya</first><last>Wang</last></author>
      <author><first>Sinno Jialin</first><last>Pan</last></author>
      <doi>10.1162/coli_a_00362</doi>
      <abstract>In fine-grained opinion mining, extracting aspect terms (a.k.a. opinion targets) and opinion terms (a.k.a. opinion expressions) from user-generated texts is the most fundamental task in order to generate structured opinion summarization. Existing studies have shown that the syntactic relations between aspect and opinion words play an important role for aspect and opinion terms extraction. However, most of the works either relied on predefined rules or separated relation mining with feature learning. Moreover, these works only focused on single-domain extraction, which failed to adapt well to other domains of interest where only unlabeled data are available. In real-world scenarios, annotated resources are extremely scarce for many domains, motivating knowledge transfer strategies from labeled source domain(s) to any unlabeled target domain. We observe that syntactic relations among target words to be extracted are not only crucial for single-domain extraction, but also serve as invariant “pivot” information to bridge the gap between different domains. In this article, we explore the constructions of recursive neural networks based on the dependency tree of each sentence for associating syntactic structure with feature learning. Furthermore, we construct transferable recursive neural networks to automatically learn the domain-invariant fine-grained interactions among aspect words and opinion words. The transferability is built on an auxiliary task and a conditional domain adversarial network to reduce domain distribution difference in the hidden spaces effectively in word level through syntactic relations. Specifically, the auxiliary task builds structural correspondences across domains by predicting the dependency relation for each path of the dependency tree in the recursive neural network. The conditional domain adversarial network helps to learn domain-invariant hidden representation for each word conditioned on the syntactic structure. In the end, we integrate the recursive neural network with a sequence labeling classifier on top that models contextual influence in the final predictions. Extensive experiments and analysis are conducted to demonstrate the effectiveness of the proposed model and each component on three benchmark data sets.</abstract>
      <pages>705–736</pages>
      <url hash="efa9dcf1">J19-4004</url>
    </paper>
    <paper id="5">
      <title>Scalable Micro-planned Generation of Discourse from Structured Data</title>
      <author><first>Anirban</first><last>Laha</last></author>
      <author><first>Parag</first><last>Jain</last></author>
      <author><first>Abhijit</first><last>Mishra</last></author>
      <author><first>Karthik</first><last>Sankaranarayanan</last></author>
      <doi>10.1162/coli_a_00363</doi>
      <abstract>We present a framework for generating natural language description from structured data such as tables; the problem comes under the category of data-to-text natural language generation (NLG). Modern data-to-text NLG systems typically use end-to-end statistical and neural architectures that learn from a limited amount of task-specific labeled data, and therefore exhibit limited scalability, domain-adaptability, and interpretability. Unlike these systems, ours is a modular, pipeline-based approach, and does not require task-specific parallel data. Rather, it relies on monolingual corpora and basic off-the-shelf NLP tools. This makes our system more scalable and easily adaptable to newer domains.Our system utilizes a three-staged pipeline that: (i) converts entries in the structured data to canonical form, (ii) generates simple sentences for each atomic entry in the canonicalized representation, and (iii) combines the sentences to produce a coherent, fluent, and adequate paragraph description through sentence compounding and co-reference replacement modules. Experiments on a benchmark mixed-domain data set curated for paragraph description from tables reveals the superiority of our system over existing data-to-text approaches. We also demonstrate the robustness of our system in accepting other popular data sets covering diverse data types such as knowledge graphs and key-value maps.</abstract>
      <pages>737–763</pages>
      <url hash="b7d75c6e">J19-4005</url>
    </paper>
    <paper id="6">
      <title>Argument Mining: A Survey</title>
      <author><first>John</first><last>Lawrence</last></author>
      <author><first>Chris</first><last>Reed</last></author>
      <doi>10.1162/coli_a_00364</doi>
      <abstract>Argument mining is the automatic identification and extraction of the structure of inference and reasoning expressed as arguments presented in natural language. Understanding argumentative structure makes it possible to determine not only what positions people are adopting, but also why they hold the opinions they do, providing valuable insights in domains as diverse as financial market prediction and public relations. This survey explores the techniques that establish the foundations for argument mining, provides a review of recent advances in argument mining techniques, and discusses the challenges faced in automatically extracting a deeper understanding of reasoning expressed in language in general.</abstract>
      <pages>765–818</pages>
      <url hash="5416dc07">J19-4006</url>
    </paper>
    <paper id="7">
      <title>How to Distinguish Languages and Dialects</title>
      <author><first>Søren</first><last>Wichmann</last></author>
      <doi>10.1162/coli_a_00366</doi>
      <abstract>The terms “language” and “dialect” are ingrained, but linguists nevertheless tend to agree that it is impossible to apply a non-arbitrary distinction such that two speech varieties can be identified as either distinct languages or two dialects of one and the same language. A database of lexical information for more than 7,500 speech varieties, however, unveils a strong tendency for linguistic distances to be bimodally distributed. For a given language group the linguistic distances pertaining to either cluster can be teased apart, identifying a mixture of normal distributions within the data and then separating them fitting curves and finding the point where they cross. The thresholds identified are remarkably consistent across data sets, qualifying their mean as a universal criterion for distinguishing between language and dialect pairs. The mean of the thresholds identified translates into a temporal distance of around one to one-and-a-half millennia (1,075–1,635 years).</abstract>
      <pages>823–831</pages>
      <url hash="e299f64c">J19-4007</url>
    </paper>
  </volume>
</collection>
