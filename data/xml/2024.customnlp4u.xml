<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.customnlp4u">
  <volume id="1" ingest-date="2024-11-05" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 1st Workshop on Customizable NLP: Progress and Challenges in Customizing NLP for a Domain, Application, Group, or Individual (CustomNLP4U)</booktitle>
      <editor><first>Sachin</first><last>Kumar</last></editor>
      <editor><first>Vidhisha</first><last>Balachandran</last></editor>
      <editor><first>Chan Young</first><last>Park</last></editor>
      <editor><first>Weijia</first><last>Shi</last></editor>
      <editor><first>Shirley Anugrah</first><last>Hayati</last></editor>
      <editor><first>Yulia</first><last>Tsvetkov</last></editor>
      <editor><first>Noah</first><last>Smith</last></editor>
      <editor><first>Hannaneh</first><last>Hajishirzi</last></editor>
      <editor><first>Dongyeop</first><last>Kang</last></editor>
      <editor><first>David</first><last>Jurgens</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Miami, Florida, USA</address>
      <month>November</month>
      <year>2024</year>
      <url hash="dea123de">2024.customnlp4u-1</url>
      <venue>customnlp4u</venue>
    </meta>
    <frontmatter>
      <url hash="e7e4bdc5">2024.customnlp4u-1.0</url>
      <bibkey>customnlp4u-2024-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Navigate Complex Physical Worlds via Geometrically Constrained <fixed-case>LLM</fixed-case></title>
      <author><first>Yongqiang</first><last>Huang</last></author>
      <author><first>Wentao</first><last>Ye</last></author>
      <author><first>Liyao</first><last>Li</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Junbo</first><last>Zhao</last><affiliation>Zhejiang University</affiliation></author>
      <pages>1-11</pages>
      <abstract>This study investigates the potential of Large Language Models (LLMs) for reconstructing and understanding the physical world based solely on textual knowledge. It explores the impact of model performance on spatial understanding abilities by introducing a set of geometric conventions and developing a workflow based on multi-layer graphs and multi-agent systems. The study examines how LLMs achieve multi-step and multi-objective geometric inference in a spatial environment, using unified geometric conventions and a graph-driven framework. A genetic algorithm, inspired by large-scale model knowledge, is employed to solve geometric constraint problems, enhancing the spatial reasoning capabilities of LLMs. This work innovatively explores the feasibility of using text-based LLMs as builders of the physical world and designs a workflow to enhance their spatial comprehension and construction capabilities.</abstract>
      <url hash="cd048df7">2024.customnlp4u-1.1</url>
      <bibkey>huang-etal-2024-navigate</bibkey>
    </paper>
    <paper id="2">
      <title>Empowering <fixed-case>AAC</fixed-case> Users: A Systematic Integration of Personal Narratives with Conversational <fixed-case>AI</fixed-case></title>
      <author><first>Sayantan</first><last>Pal</last><affiliation>State University of New York at Buffalo</affiliation></author>
      <author><first>Souvik</first><last>Das</last><affiliation>State University of New York at Buffalo</affiliation></author>
      <author><first>Rohini</first><last>Srihari</last><affiliation>State University of New York at Buffalo</affiliation></author>
      <author><first>Jeff</first><last>Higginborham</last><affiliation>State University of New York at Buffalo</affiliation></author>
      <author><first>Jenna</first><last>Bizovi</last><affiliation>State University of New York at Buffalo</affiliation></author>
      <pages>12-25</pages>
      <abstract>Communication barriers have long posed challenges for users of Alternate and Augmentative Communication (AAC). In AAC, effective conversational aids are not solely about harnessing Artificial Intelligence (AI) capabilities but more about ensuring these technologies resonate deeply with AAC user’s unique communication challenges. We aim to bridge the gap between generic outputs and genuine human interactions by integrating advanced Conversational AI with personal narratives. While existing solutions offer generic responses, a considerable gap in tailoring outputs reflecting an AAC user’s intent must be addressed. Thus, we propose to create a custom conversational dataset centered on the experiences and words of a primary AAC user to fine-tune advanced language models. Additionally, we employ a Retrieval-Augmented Generation (RAG) method, drawing context from a summarized version of authored content by the AAC user. This combination ensures that responses are contextually relevant and deeply personal. Preliminary evaluations underscore its transformative potential, with automated metrics and human assessments showcasing significantly enhanced response quality.</abstract>
      <url hash="5ba67a1d">2024.customnlp4u-1.2</url>
      <bibkey>pal-etal-2024-empowering</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>LLM</fixed-case>-Based Robust Product Classification in Commerce and Compliance</title>
      <author><first>Sina</first><last>Gholamian</last></author>
      <author><first>Gianfranco</first><last>Romani</last><affiliation>Thomson Reuters</affiliation></author>
      <author><first>Bartosz</first><last>Rudnikowicz</last><affiliation>Thomson Reuters</affiliation></author>
      <author><first>Stavroula</first><last>Skylaki</last></author>
      <pages>26-36</pages>
      <abstract>Product classification is a crucial task in international trade, as compliance regulations are verified and taxes and duties are applied based on product categories. Manual classification of products is time-consuming and error-prone, and the sheer volume of products imported and exported renders the manual process infeasible. Consequently, e-commerce platforms and enterprises involved in international trade have turned to automatic product classification using machine learning. However, current approaches do not consider the real-world challenges associated with product classification, such as very abbreviated and incomplete product descriptions. In addition, recent advancements in generative Large Language Models (LLMs) and their reasoning capabilities are mainly untapped in product classification and e-commerce. In this research, we explore the real-life challenges of industrial classification and we propose data perturbations that allow for realistic data simulation. Furthermore, we employ LLM-based product classification to improve the robustness of the prediction in presence of incomplete data. Our research shows that LLMs with in-context learning outperform the supervised approaches in the clean-data scenario. Additionally, we illustrate that LLMs are significantly more robust than the supervised approaches when data attacks are present.</abstract>
      <url hash="a35796d7">2024.customnlp4u-1.3</url>
      <bibkey>gholamian-etal-2024-llm</bibkey>
    </paper>
    <paper id="4">
      <title>Less is Fed More: Sparsity Reduces Feature Distortion in Federated Learning</title>
      <author><first>Abhinav Sukumar</first><last>Rao</last></author>
      <author><first>Aashiq</first><last>Muhamed</last></author>
      <author><first>Harshita</first><last>Diddee</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>37-46</pages>
      <abstract>Our work studies Multilingual Federated Learning (FL), a decentralized paradigm that, although promising, grapples with issues such as client drift and suboptimal generalization in diverse, multilingual settings. We highlight limitations in existing approaches to generalize across both actively participating and inactive client language pairs. To mitigate these challenges, we introduce FedSparseNet, which incorporates sparse-network training, and LoRA, based on Low-Rank Adaptation. These approaches maintain the model’s fidelity to its pretraining distribution, thereby ensuring robust performance on both seen and unseen language pairs, while simultaneously enhancing communication efficiency by selectively transmitting trainable parameters. Our empirical evaluations demonstrate that FedSparseNet outperforms conventional FL models on both seen and unseen clients, while LoRA shows remarkable improvements in unseen client performance. Additionally, we propose the Continuous Relative Robustness Metric, a novel metric to uniformly assess a model’s performance across diverse language pairs. We open-source our code for reproducibility on GitHub.</abstract>
      <url hash="fdfe5a66">2024.customnlp4u-1.4</url>
      <bibkey>rao-etal-2024-less</bibkey>
    </paper>
    <paper id="5">
      <title>Understanding Players as if They Are Talking to the Game in a Customized Language: A Pilot Study</title>
      <author><first>Tianze</first><last>Wang</last></author>
      <author><first>Maryam</first><last>Honarijahromi</last><affiliation>Microsoft Xbox (king)</affiliation></author>
      <author><first>Styliani</first><last>Katsarou</last></author>
      <author><first>Olga</first><last>Mikheeva</last><affiliation>KTH Royal Institute of Technology, Stockholm, Sweden</affiliation></author>
      <author><first>Theodoros</first><last>Panagiotakopoulos</last><affiliation>King</affiliation></author>
      <author><first>Oleg</first><last>Smirnov</last><affiliation>Microsoft Gaming</affiliation></author>
      <author><first>Lele</first><last>Cao</last><affiliation>Microsoft (ABK)</affiliation></author>
      <author><first>Sahar</first><last>Asadi</last></author>
      <pages>47-52</pages>
      <abstract>This pilot study explores the application of language models (LMs) to model game event sequences, treating them as a customized natural language. We investigate a popular mobile game, transforming raw event data into textual sequences and pretraining a Longformer model on this data. Our approach captures the rich and nuanced interactions within game sessions, effectively identifying meaningful player segments. The results demonstrate the potential of self-supervised LMs in enhancing game design and personalization without relying on ground-truth labels.</abstract>
      <url hash="1a1f5f09">2024.customnlp4u-1.5</url>
      <bibkey>wang-etal-2024-understanding</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>L</fixed-case>3<fixed-case>M</fixed-case>asking: Multi-task Fine-tuning for Language Models by Leveraging Lessons Learned from Vanilla Models</title>
      <author><first>Yusuke</first><last>Kimura</last><affiliation>Doshisha University</affiliation></author>
      <author><first>Takahiro</first><last>Komamizu</last><affiliation>Nagoya University</affiliation></author>
      <author><first>Kenji</first><last>Hatano</last><affiliation>Doshisha University</affiliation></author>
      <pages>53-62</pages>
      <abstract>When distributional differences exist between pre-training and fine-tuning data, language models (LMs) may perform poorly on downstream tasks.Recent studies have reported that multi-task learning of downstream task and masked language modeling (MLM) task during the fine-tuning phase improves the performance of the downstream task.Typical MLM tasks (e.g., random token masking (RTM)) tend not to care tokens corresponding to the knowledge already acquired during the pre-training phase, therefore LMs may not notice the important clue or not effective to acquire linguistic knowledge of the task or domain.To overcome this limitation, we propose a new masking strategy for MLM task, called L3Masking, that leverages lessons (specifically, token-wise likelihood in a context) learned from the vanilla language model to be fine-tuned.L3Masking actively masks tokens with low likelihood on the vanilla model.Experimental evaluations on text classification tasks in different domains confirms a multi-task text classification method with L3Masking performed task adaptation more effectively than that with RTM.These results suggest the usefulness of assigning a preference to the tokens to be learned as the task or domain adaptation.</abstract>
      <url hash="778ef699">2024.customnlp4u-1.6</url>
      <bibkey>kimura-etal-2024-l3masking</bibkey>
    </paper>
    <paper id="7">
      <title>Grounded Language Agent for Product Search via Intelligent Web Interactions</title>
      <author><first>Moghis</first><last>Fereidouni</last></author>
      <author><first>Adib</first><last>Mosharrof</last><affiliation>University of Kentucky</affiliation></author>
      <author><first>A.b.</first><last>Siddique</last><affiliation>University of Kentucky</affiliation></author>
      <pages>63-75</pages>
      <abstract>Recent research has focused on developing agents powered by large language models (LLMs) to accomplish complex high-level user intents. However, employing LLMs with billions of parameters (e.g., GPT-4) may incur substantial costs on top of handcrafting extensive prompts. To address this, we introduce a Grounded Language Agent for Intelligent Web Interactions, named GLAINTEL. GLAINTEL employs Flan-T5 as its backbone and is flexible in training in various settings: unsupervised learning, supervised learning, and unsupervised domain adaptation. Specifically, we tackle both the challenge of learning without human demonstrations and the opportunity to leverage human demonstrations effectively when those are available. Additionally, we explore unsupervised domain adaptation for cases where demonstrations are limited to a specific domain. Experimental evaluations across diverse setups demonstrate the effectiveness of GLAINTEL in unsupervised settings, outperforming in-context learning-based approaches that employ larger models with up to 540 billion parameters. Surprisingly, behavioral cloning-based methods that straightforwardly use human demonstrations do not outperform unsupervised variants of GLAINTEL. Additionally, we show that combining human demonstrations with reinforcement learning-based training yields results comparable to methods utilizing GPT-4. The code is available at: https://github.com/MultifacetedNLP/Web-Agents-Unsupervised</abstract>
      <url hash="8f1dcd35">2024.customnlp4u-1.7</url>
      <bibkey>fereidouni-etal-2024-grounded</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>A</fixed-case>dapt<fixed-case>E</fixed-case>val: Evaluating Large Language Models on Domain Adaptation for Text Summarization</title>
      <author><first>Anum</first><last>Afzal</last><affiliation>Technische Universität München</affiliation></author>
      <author><first>Ribin</first><last>Chalumattu</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <author><first>Florian</first><last>Matthes</last><affiliation>Technische Universität München</affiliation></author>
      <author><first>Laura</first><last>Mascarell</last></author>
      <pages>76-85</pages>
      <abstract>Despite the advances in the abstractive summarization task using Large Language Models (LLM), there is a lack of research that asses their abilities to easily adapt to different domains. We evaluate the domain adaptation abilities of a wide range of LLMs on the summarization task across various domains in both fine-tuning and in-context learning settings. We also present AdaptEval, the first domain adaptation evaluation suite. AdaptEval includes a domain benchmark and a set of metrics to facilitate the analysis of domain adaptation. Our results demonstrate that LLMs exhibit comparable performance in the in-context learning setting, regardless of their parameter scale.</abstract>
      <url hash="aa50668b">2024.customnlp4u-1.8</url>
      <bibkey>afzal-etal-2024-adapteval</bibkey>
    </paper>
    <paper id="9">
      <title><fixed-case>CPS</fixed-case>-<fixed-case>T</fixed-case>ask<fixed-case>F</fixed-case>orge: Generating Collaborative Problem Solving Environments for Diverse Communication Tasks</title>
      <author><first>Nikita</first><last>Haduong</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <author><first>Irene</first><last>Wang</last><affiliation>NA</affiliation></author>
      <author><first>Bo-Ru</first><last>Lu</last><affiliation>University of Washington</affiliation></author>
      <author><first>Prithviraj</first><last>Ammanabrolu</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Noah A.</first><last>Smith</last><affiliation>University of Washington and Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>86-112</pages>
      <abstract>Teams can outperform individuals; could adding AI teammates further bolster performance of teams solving problems collaboratively? Collaborative problem solving (CPS) research commonly studies teams with two agents (human-human or human-AI), but team research literature finds that, for complex tasks, larger teams are more effective. Progress in studying collaboration with more than two agents, through textual records of team interactions, is hindered by a major data challenge: available CPS corpora are predominantly dyadic, and adapting pre-existing CPS tasks to more agents is non-trivial. We address this data challenge by developing a CPS task generator, CPS-TaskForge, that can produce environments for studying CPS under a wide array of conditions, and releasing a CPS task design checklist grounded in the theoretical PISA 2015 CPS framework to help facilitate the development of CPS corpora with more agents. CPS-TaskForge takes the form of a resource management (tower defense) game, and different CPS tasks can be studied by manipulating game design parameters. We conduct a case study with groups of 3–4 humans to validate production of diverse natural language CPS communication in a game instance produced by CPS-TaskForge. We discuss opportunities for advancing research in CPS (both with human-only and human-AI teams) using different task configurations. We release all data and code.</abstract>
      <url hash="1d118d6b">2024.customnlp4u-1.9</url>
      <bibkey>haduong-etal-2024-cps</bibkey>
    </paper>
    <paper id="10">
      <title>Active Learning for Robust and Representative <fixed-case>LLM</fixed-case> Generation in Safety-Critical Scenarios</title>
      <author><first>Sabit</first><last>Hassan</last><affiliation>University of Pittsburgh</affiliation></author>
      <author><first>Anthony</first><last>Sicilia</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Malihe</first><last>Alikhani</last><affiliation>Northeastern University</affiliation></author>
      <pages>113-123</pages>
      <abstract>Ensuring robust safety measures across a wide range of scenarios is crucial for user-facing systems. While Large Language Models (LLMs) can generate valuable data for safety measures, they often exhibit distributional biases, focusing on common scenarios and neglecting rare but critical cases. This can undermine the effectiveness of safety protocols developed using such data. To address this, we propose a novel framework that integrates active learning with clustering to guide LLM generation, enhancing their representativeness and robustness in safety scenarios. We demonstrate the effectiveness of our approach by constructing a dataset of 5.4K potential safety violations through an iterative process involving LLM generation and an active learner model’s feedback. Our results show that the proposed framework produces a more representative set of safety scenarios without requiring prior knowledge of the underlying data distribution. Additionally, data acquired through our method improves the accuracy and F1 score of both the active learner model as well models outside the scope of active learning process, highlighting its broad applicability.</abstract>
      <url hash="dd392938">2024.customnlp4u-1.10</url>
      <bibkey>hassan-etal-2024-active</bibkey>
    </paper>
    <paper id="11">
      <title>Exploring the Readiness of Prominent Small Language Models for the Democratization of Financial Literacy</title>
      <author><first>Tagore Rao</first><last>Kosireddy</last></author>
      <author><first>Jeffrey David</first><last>Wall</last><affiliation>Michigan Technological University</affiliation></author>
      <author><first>Evan</first><last>Lucas</last><affiliation>Michigan Technological University</affiliation></author>
      <pages>124-149</pages>
      <abstract>The use of <i>small language models</i> (SLMs), herein defined as models with less than three billion parameters, is increasing across various domains and applications. Due to their ability to run on more accessible hardware and preserve user privacy, SLMs possess the potential to democratize access to language models for individuals of different socioeconomic status and with different privacy preferences. This study assesses several state-of-the-art SLMs (e.g., Apple’s OpenELM, Microsoft’s Phi, Google’s Gemma, and the Tinyllama project) for use in the financial domain to support the development of financial literacy LMs. Democratizing access to quality financial information for those who are financially under educated is greatly needed in society, particularly as new financial markets and products emerge and participation in financial markets increases due to ease of access. We are the first to examine the use of open-source SLMs to democratize access to financial question answering capabilities for individuals and students. To this end, we provide an analysis of the memory usage, inference time, similarity comparisons to ground-truth answers, and output readability of prominent SLMs to determine which models are most accessible and capable of supporting access to financial information. We analyze zero-shot and few-shot learning variants of the models. The results suggest that some off-the-shelf SLMs merit further exploration and fine-tuning to prepare them for individual use, while others may have limits to their democratization. Code to replicate our experiments is shared.</abstract>
      <url hash="5afa9408">2024.customnlp4u-1.11</url>
      <bibkey>kosireddy-etal-2024-exploring</bibkey>
    </paper>
    <paper id="12">
      <title>Customized Style Transfer using Discrete Sampling</title>
      <author><first>Anugunj</first><last>Naman</last></author>
      <pages>150-155</pages>
      <abstract>Customizing text style or content typically involves extensive fine-tuning of large models, demanding significant data and training. Traditional unsupervised approaches using sampling often yield low diversity and creativity. We present a novel discrete Langevin proposal that samples directly from the categorical token distribution, overcoming these limitations. By adapting the continuous Langevin algorithm for discrete spaces, our approach enables efficient gradient-based sampling. Evaluations on style transfer tasks demonstrate superior performance over state-of-the-art methods in accuracy, BLEU, BERTScore, and diversity. Our proposed approach paves way for advanced customized text generation with desired styles as well as allows future scope for prompt generation for model safeguarding and jail-breaking.</abstract>
      <url hash="b707942b">2024.customnlp4u-1.12</url>
      <bibkey>naman-2024-customized</bibkey>
    </paper>
    <paper id="13">
      <title>Trustful <fixed-case>LLM</fixed-case>s: Customizing and Grounding Text Generation with knowledge bases and Dual Decoders</title>
      <author><first>Xiaofeng</first><last>Zhu</last></author>
      <author><first>Jaya Krishna</first><last>Mandivarapu</last><affiliation>Microsoft</affiliation></author>
      <pages>156-166</pages>
      <abstract>Although people are impressed by the content generation skills of large language models, the use of LLMs, such as ChatGPT, is limited by the domain grounding of the content. The correctness and groundedness of the generated content need to be based on a verified context, such as results from Retrieval-Augmented Generation (RAG). One important issue when adapting LLMs to a customized domain is that the generated responses are often incomplete, or the additions are not verified and may even be hallucinated. Prior studies on hallucination detection have focused on evaluation metrics, which are not easily adaptable to dynamic domains and can be vulnerable to attacks like jail-breaking. In this work, we propose 1) a post-processing algorithm of leveraging knowledge triplets in RAG context to correct hallucinations and 2) a dual-decoder model that fuses RAG context to guide the generation process.</abstract>
      <url hash="2899e3e2">2024.customnlp4u-1.13</url>
      <bibkey>zhu-mandivarapu-2024-trustful</bibkey>
    </paper>
    <paper id="14">
      <title>Constructing Domain-Specific Evaluation Sets for <fixed-case>LLM</fixed-case>-as-a-judge</title>
      <author><first>Ravi Shanker</first><last>Raju</last><affiliation>Sambanova Systems</affiliation></author>
      <author><first>Swayambhoo</first><last>Jain</last><affiliation>Sambanova Systems</affiliation></author>
      <author id="bo-li"><first>Bo</first><last>Li</last></author>
      <author><first>Jonathan Lingjie</first><last>Li</last></author>
      <author><first>Urmish</first><last>Thakker</last><affiliation>SambaNova Systems</affiliation></author>
      <pages>167-181</pages>
      <abstract>Large Language Models (LLMs) have revolutionized the landscape of machine learning, yet current benchmarks often fall short in capturing the diverse behavior of these models in real-world applications. A benchmark’s usefulness is determined by its ability to clearly differentiate between models of varying capabilities (separability) and closely align with human preferences. Existing frameworks like Alpaca-Eval 2.0 LC (CITATION) and Arena-Hard v0.1 (CITATION) are limited by their focus on general-purpose queries and lack of diversity across domains such as law, medicine, and multilingual contexts. In this paper, we address these limitations by introducing a novel data pipeline that curates diverse, domain-specific evaluation sets tailored for LLM-as-a-Judge frameworks. Our approach leverages a combination of manual curation, semi-supervised learning to generate clusters, and stratified sampling to ensure balanced representation across a wide range of domains and languages. The resulting evaluation set, which includes 1573 samples across 14 categories, demonstrates high separability (84%) across ten top-ranked models, and agreement (84%) with Chatbot Arena and (0.915) Spearman correlation. The agreement values are 9% better than Arena Hard and 20% better than AlpacaEval 2.0 LC, while the Spearman coefficient is 0.7 more than the next best benchmark, showcasing a significant improvement in the usefulness of the benchmark. We further provide an open-source evaluation tool that enables fine-grained analysis of model performance across user-defined categories, offering valuable insights for practitioners. This work contributes to the ongoing effort to enhance the transparency, diversity, and effectiveness of LLM evaluation methodologies.</abstract>
      <url hash="782ae059">2024.customnlp4u-1.14</url>
      <bibkey>raju-etal-2024-constructing</bibkey>
    </paper>
    <paper id="15">
      <title>Learning to Adapt Large Language Models to One-Shot In-Context Intent Classification on Unseen Domains</title>
      <author><first>Joongbo</first><last>Shin</last><affiliation>LG AI Research</affiliation></author>
      <author><first>Youbin</first><last>Ahn</last><affiliation>LG Corporation</affiliation></author>
      <author><first>Seungpil</first><last>Won</last><affiliation>LG Corporation</affiliation></author>
      <author><first>Stanley Jungkyu</first><last>Choi</last><affiliation>Language Lab, LG AI Research</affiliation></author>
      <pages>182-197</pages>
      <abstract>In this paper, we explore one-shot in-context intent classification using large language models (LLMs) with the goal of minimizing the effort required to adapt models to unseen domains. To enhance the one-shot in-context learning capabilities of LLMs, we employ in-context tuning, leveraging its cross-domain transferability to unseen domains.To this end, we introduce the IC-collection, a compilation of open-source intent classification datasets from diverse domains, which are meticulously divided into held-in and held-out datasets.Our experiments demonstrate the effectiveness of the proposed method, showing that our model, with only 7B parameters, not only outperforms GPT-4 on intent classification but also achieves state-of-the-art in unseen domains with only one-shot demonstrations.Both our benchmark and model will be made publicly available to advance research in the chatbot systems.</abstract>
      <url hash="b443c48f">2024.customnlp4u-1.15</url>
      <bibkey>shin-etal-2024-learning</bibkey>
    </paper>
    <paper id="16">
      <title>Pearl: Personalizing Large Language Model Writing Assistants with Generation-Calibrated Retrievers</title>
      <author><first>Sheshera</first><last>Mysore</last></author>
      <author><first>Zhuoran</first><last>Lu</last></author>
      <author><first>Mengting</first><last>Wan</last><affiliation>Microsoft</affiliation></author>
      <author><first>Longqi</first><last>Yang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Bahareh</first><last>Sarrafzadeh</last><affiliation>Microsoft</affiliation></author>
      <author><first>Steve</first><last>Menezes</last><affiliation>Microsoft</affiliation></author>
      <author><first>Tina</first><last>Baghaee</last><affiliation>Microsoft</affiliation></author>
      <author><first>Emmanuel Barajas</first><last>Gonzalez</last><affiliation>Centro de Enseñanza Técnica Industrial</affiliation></author>
      <author><first>Jennifer</first><last>Neville</last><affiliation>Purdue University and Purdue University</affiliation></author>
      <author><first>Tara</first><last>Safavi</last><affiliation>Microsoft Research</affiliation></author>
      <pages>198-219</pages>
      <abstract>Powerful large language models have facilitated the development of writing assistants that promise to significantly improve the quality and efficiency of composition and communication. However, a barrier to effective assistance is the lack of personalization in LLM outputs to the author’s communication style, specialized knowledge, and values. In this paper, we address this challenge by proposing Pearl, a LLM writing assistant personalized with a retriever that is trained to be generation-calibrated for personalization. Generation calibration ensures that our retriever selects historic user authored documents to augment an LLM prompt such that they are likely to help an LLM generation better adhere to a users’ preferences. We propose two key novelties for training such a retriever: (1) A training data selection method that identifies user requests likely to benefit from personalization and documents that provide that benefit; and (2) A scale-calibrating KL-divergence objective that ensures that our retriever scores remain proportional to the downstream generation quality from using the document for personalized generation. In a series of holistic evaluations, we demonstrate the effectiveness of Pearl in generating long-form texts on multiple social media datasets. Finally, we demonstrate how a generation-calibrated retriever can double as a performance predictor – detecting low quality retrieval, and improving potentially under-performing outputs via revision with LLMs.</abstract>
      <url hash="b347742d">2024.customnlp4u-1.16</url>
      <bibkey>mysore-etal-2024-pearl</bibkey>
    </paper>
    <paper id="17">
      <title>Evaluating and Training Long-Context Large Language Models for Question Answering on Scientific Papers</title>
      <author><first>Lukas</first><last>Hilgert</last><affiliation>Karlsruher Institut für Technologie</affiliation></author>
      <author><first>Danni</first><last>Liu</last><affiliation>Karlsruher Institut für Technologie</affiliation></author>
      <author><first>Jan</first><last>Niehues</last></author>
      <pages>220-236</pages>
      <abstract>With the number of scientific papers published every year growing and current large language models (LLMs) showing state-of-the-art performance on natural language processing (NLP) tasks, we ask the question if LLMs could be utilized to answer questions on scientific papers.We investigate how well state-of-the-art large language models (LLMs) can answer questions on scientific paper by experimenting with long-context versions of the LLaMA 2 model and evaluating and training on the Qasper dataset.We analyze how well the LLMs handle longer papers and questions that can only be answered by accessing information from far out paragraphs. During our experiments, we see that the performance of these LLMs drops with growing length and position of relevant information.We employ different measures from simple prompts to chain-of-thought prompts and zero-shot usage to fine-tuning with QLoRA.While we still observe a performance loss with increased context length, our measures reduce the effects of this flaw, and we can achieve <tex-math>F_{1}</tex-math> scores similar to bigger models like GPT-4.</abstract>
      <url hash="90997a25">2024.customnlp4u-1.17</url>
      <bibkey>hilgert-etal-2024-evaluating</bibkey>
    </paper>
    <paper id="18">
      <title><fixed-case>H</fixed-case>y<fixed-case>PA</fixed-case>-<fixed-case>RAG</fixed-case>: A Hybrid Parameter Adaptive Retrieval-Augmented Generation System for <fixed-case>AI</fixed-case> Legal and Policy Applications</title>
      <author><first>Rishi</first><last>Kalra</last></author>
      <author><first>Zekun</first><last>Wu</last></author>
      <author><first>Ayesha</first><last>Gulley</last><affiliation>NA</affiliation></author>
      <author><first>Airlie</first><last>Hilliard</last></author>
      <author><first>Xin</first><last>Guan</last></author>
      <author><first>Adriano</first><last>Koshiyama</last></author>
      <author><first>Philip Colin</first><last>Treleaven</last><affiliation>University College London, University of London</affiliation></author>
      <pages>237-256</pages>
      <abstract>While Large Language Models (LLMs) excel in text generation and question-answering, their effectiveness in AI legal and policy applications is limited by outdated knowledge, hallucinations, and inadequate reasoning in complex contexts. Retrieval-Augmented Generation (RAG) systems improve response accuracy by integrating external knowledge but struggle with retrieval errors, poor context integration, and high costs, particularly in interpreting AI legal texts. This paper introduces a Hybrid Parameter-Adaptive RAG (HyPA-RAG) system tailored for AI legal and policy, exemplified by NYC Local Law 144 (LL144). HyPA-RAG uses a query complexity classifier for adaptive parameter tuning, a hybrid retrieval strategy combining dense, sparse, and knowledge graph methods, and an evaluation framework with specific question types and metrics. By dynamically adjusting parameters, HyPA-RAG significantly improves retrieval accuracy and response fidelity. Testing on LL144 shows enhanced correctness, faithfulness, and contextual precision, addressing the need for adaptable NLP systems in complex, high-stakes AI legal and policy applications.</abstract>
      <url hash="766ff314">2024.customnlp4u-1.18</url>
      <bibkey>kalra-etal-2024-hypa</bibkey>
    </paper>
    <paper id="19">
      <title>What Kind of Sourcery is This? Evaluating <fixed-case>GPT</fixed-case>-4’s Performance on Linking Scientific Fact to Citations</title>
      <author><first>Autumn</first><last>Toney</last><affiliation>Georgetown University</affiliation></author>
      <pages>257-268</pages>
      <abstract>From document summarization to code generation, chabots have disrupted various aspects of scientific research and writing. While chabots are useful research resources for ideation, information retrieval, and editing, their generative pre-trained transformer (GPT) models’ underlying knowledge infrastructure is opaque. This has raised questions about the reliability of generative chatbot responses, as GPT models are known to respond with misleading information that appears to be accurate. Prior research has investigated the utility of OpenAI’s public chatbot, ChatGPT, to generate reliable bibliographic information with a focus on small-scale medical-related scientific facts. We present an expanded study that analyzes GPT-4’s ability to accurately identify 1,326 scientific facts and link them to academic sources. Using both the API and UI service, we experimented with open-ended and close-ended prompts to establish an understanding of GPT-4’s general ability at this domain-specific task, as well as study the real-world scenario of an average user interacting with ChatGPT using its UI. GPT-4 accurately identified 96% of the scientific facts and generated relevant and existent academic citations with 78% accuracy. Using the claims that GPT-4 mislabeled and provided incorrect sources via the API, we prompt two public GPTs customized for academic writing to evaluate if they correctly label the scientific claims and provide accurate sources. We find that these GPTs are able to accurately label 38% of the mislabeled claims, with 95% of the corresponding citations being accurate and relevant.</abstract>
      <url hash="cbc222b9">2024.customnlp4u-1.19</url>
      <bibkey>toney-2024-kind</bibkey>
    </paper>
    <paper id="20">
      <title>“Let’s Argue Both Sides”: Argument Generation Can Force Small Models to Utilize Previously Inaccessible Reasoning Capabilities</title>
      <author><first>Kaveh</first><last>Eskandari Miandoab</last></author>
      <author><first>Vasanth</first><last>Sarathy</last><affiliation>Tufts University</affiliation></author>
      <pages>269-283</pages>
      <abstract>Large Language Models (LLMs), despite achieving state-of-the-art results in a number of evaluation tasks, struggle to maintain their performance when logical reasoning is strictly required to correctly infer a prediction. In this work, we propose Argument Generation as a method of forcing models to utilize their reasoning capabilities when other approaches such as chain-of-thought reasoning prove insufficient. Our method involves the generation of arguments for each possible inference result, and asking the end model to rank the generated arguments. We show that Argument Generation can serve as an appropriate substitute for zero-shot prompting techniques without the requirement to add layers of complexity. Furthermore, we argue that knowledge-probing techniques such as chain-of-thought reasoning and Argument Generation are only useful when further reasoning is required to infer a prediction, making them auxiliary to more common zero-shot approaches. Finally, we demonstrate that our approach forces larger gains in smaller language models, showcasing a complex relationship between model size and prompting methods in foundation models.</abstract>
      <url hash="8014d845">2024.customnlp4u-1.20</url>
      <bibkey>eskandari-miandoab-sarathy-2024-lets</bibkey>
    </paper>
    <paper id="21">
      <title><fixed-case>LLM</fixed-case>-as-a-tutor in <fixed-case>EFL</fixed-case> Writing Education: Focusing on Evaluation of Student-<fixed-case>LLM</fixed-case> Interaction</title>
      <author><first>Jieun</first><last>Han</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Haneul</first><last>Yoo</last><affiliation>KAIST</affiliation></author>
      <author><first>Junho</first><last>Myung</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Minsun</first><last>Kim</last></author>
      <author><first>Hyunseung</first><last>Lim</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Yoonsu</first><last>Kim</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Tak Yeon</first><last>Lee</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Hwajung</first><last>Hong</last></author>
      <author><first>Juho</first><last>Kim</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>So-Yeon</first><last>Ahn</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Alice</first><last>Oh</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>284-293</pages>
      <abstract>In the context of English as a Foreign Language (EFL) writing education, LLM-as-a-tutor can assist students by providing real-time feedback on their essays. However, challenges arise in assessing LLM-as-a-tutor due to differing standards between educational and general use cases. To bridge this gap, we integrate pedagogical principles to assess student-LLM interaction. First, we explore how LLMs can function as English tutors, providing effective essay feedback tailored to students. Second, we propose three criteria to evaluate LLM-as-a-tutor specifically designed for EFL writing education, emphasizing pedagogical aspects. In this process, EFL experts evaluate the feedback from LLM-as-a-tutor regarding (1) quality and (2) characteristics. On the other hand, EFL learners assess their (3) learning outcomes from interaction with LLM-as-a-tutor. This approach lays the groundwork for developing LLMs-as-a-tutor tailored to the needs of EFL learners, advancing the effectiveness of writing education in this context.</abstract>
      <url hash="6202961b">2024.customnlp4u-1.21</url>
      <bibkey>han-etal-2024-llm</bibkey>
    </paper>
    <paper id="22">
      <title><fixed-case>E</fixed-case>-Commerce Product Categorization with <fixed-case>LLM</fixed-case>-based Dual-Expert Classification Paradigm</title>
      <author><first>Zhu</first><last>Cheng</last><affiliation>Amazon</affiliation></author>
      <author><first>Wen</first><last>Zhang</last><affiliation>Amazon</affiliation></author>
      <author><first>Chih-Chi</first><last>Chou</last><affiliation>NA</affiliation></author>
      <author><first>You-Yi</first><last>Jau</last><affiliation>NA</affiliation></author>
      <author><first>Archita</first><last>Pathak</last><affiliation>NA</affiliation></author>
      <author><first>Peng</first><last>Gao</last><affiliation>NA</affiliation></author>
      <author><first>Umit</first><last>Batur</last><affiliation>NA</affiliation></author>
      <pages>294-304</pages>
      <abstract>Accurate product categorization in e-commerce is critical for delivering a satisfactory online shopping experience to customers. With the vast number of available products and the numerous potential categories, it becomes crucial to develop a classification system capable of assigning products to their correct categories with high accuracy. We present a dual-expert classification system that utilizes the power of large language models (LLMs). This framework integrates domain-specific knowledge and pre-trained LLM’s general knowledge through effective model fine-tuning and prompting techniques. First, the fine-tuned domain-specific expert recommends top K candidate categories for a given input product. Then, the more general LLM-based expert, through prompting techniques, analyzes the nuanced differences between candidate categories and selects the most suitable target category. We introduce a new in-context learning approach that utilizes LLM self-generated summarization to provide clearer instructions and enhance its performance. Experiments on e-commerce datasets demonstrate the effectiveness of our LLM-based Dual-Expert classification system.</abstract>
      <url hash="9a8d36c7">2024.customnlp4u-1.22</url>
      <bibkey>cheng-etal-2024-e</bibkey>
    </paper>
    <paper id="23">
      <title>Adapting <fixed-case>LLM</fixed-case> Predictions in In-Context Learning with Data Priors</title>
      <author><first>Javier</first><last>Chiyah-Garcia</last></author>
      <author><first>Prasoon</first><last>Goyal</last><affiliation>Amazon</affiliation></author>
      <author><first>Michael</first><last>Johnston</last><affiliation>Amazon</affiliation></author>
      <author><first>Reza</first><last>Ghanadan</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>305-316</pages>
      <abstract>In-Context Learning (ICL) has enabled Large Language Models (LLMs) to excel as general-purpose models in zero and few-shot task settings. However, since LLMs are often not trained on the downstream tasks, they lack crucial contextual knowledge from the data distributions, which limits their task adaptability.This paper explores using data priors to automatically customize prompts in ICL. We extract these priors in a dataset-agnostic way basedon historical information, enabling LLMs to personalize their output towards users or tasks at inference time. We find that they improve LLM’s output by injecting latent dataset-specific information for the task of rating prediction. Throughout a series of experiments, we show replicable results across LLMs and datasets on what information and methods are most effective for adapting ICL outputs with priors. Our findings offer a systematic approach to customizing prompts with additional information in a privacy-friendly manner, requiring only aggregated data that is computationally efficient.</abstract>
      <url hash="6b93e243">2024.customnlp4u-1.23</url>
      <bibkey>chiyah-garcia-etal-2024-adapting</bibkey>
    </paper>
    <paper id="24">
      <title><fixed-case>V</fixed-case>-<fixed-case>G</fixed-case>lór<fixed-case>IA</fixed-case> - Customizing Large Vision and Language Models to <fixed-case>E</fixed-case>uropean <fixed-case>P</fixed-case>ortuguese</title>
      <author><first>Afonso</first><last>Simplício</last></author>
      <author><first>David</first><last>Semedo</last><affiliation>Universidade NOVA de Lisboa and Universidade NOVA de Lisboa</affiliation></author>
      <author><first>Joao</first><last>Magalhaes</last><affiliation>Universidade Nova de Lisboa</affiliation></author>
      <pages>317-326</pages>
      <abstract>Generative Vision and Language models have obtained remarkable results recently, thanks to the use of robust pre-trained Visual encoders and Large Language Models (LLMs), together with efficient model adaptation training strategies, requiring minimal architecturalmodifications, while preserving LLMs’ original capabilities. With these advances focusing mainly on the English language, there is a gap in customization methodologies for other languages. In this paper, we propose a customization methodology that adapts existingstate-of-the-art vision and language architectures to European Portuguese (PT-PT). As a result of applying this methodology, we introduce V-GlórIA , the first Large Vision and Language generative model specifically customized for European Portuguese. V-GlórIA supports multimodal tasks such as image captioning, retrieval, and dialogue. To deliver V-GlórIA, we leverage state-of-the-art V&amp;L architectures, and contribute with PT-PT machine-translated pre-training (CC3M PT-PT) and benchmark (MSCOCO PT-PT and VisDial PT-PT) datasets.Our experiments show that V-GlórIA delivers promising performance in text-image retrieval and downstream tasks in a zero-shot setting, such as image captioning and visual dialogue tasks, highlighting the effectiveness of our customization approach.</abstract>
      <url hash="7ba22181">2024.customnlp4u-1.24</url>
      <bibkey>simplicio-etal-2024-v</bibkey>
    </paper>
  </volume>
</collection>
