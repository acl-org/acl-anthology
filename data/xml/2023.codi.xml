<?xml version='1.0' encoding='UTF-8'?>
<collection id="2023.codi">
  <volume id="1" ingest-date="2023-07-08" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 4th Workshop on Computational Approaches to Discourse (CODI 2023)</booktitle>
      <editor><first>Michael</first><last>Strube</last></editor>
      <editor><first>Chloe</first><last>Braud</last></editor>
      <editor><first>Christian</first><last>Hardmeier</last></editor>
      <editor><first>Junyi Jessy</first><last>Li</last></editor>
      <editor><first>Sharid</first><last>Loaiciga</last></editor>
      <editor><first>Amir</first><last>Zeldes</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Toronto, Canada</address>
      <month>July</month>
      <year>2023</year>
      <url hash="1fdc50ef">2023.codi-1</url>
      <venue>codi</venue>
    </meta>
    <frontmatter>
      <url hash="4428953a">2023.codi-1.0</url>
      <bibkey>codi-2023-approaches</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>M</fixed-case>u<fixed-case>LMS</fixed-case>-<fixed-case>AZ</fixed-case>: An Argumentative Zoning Dataset for the Materials Science Domain</title>
      <author><first>Timo</first><last>Schrader</last><affiliation>Robert Bosch GmbH</affiliation></author>
      <author><first>Teresa</first><last>Bürkle</last><affiliation>Bosch Center for Artificial Intelligence</affiliation></author>
      <author><first>Sophie</first><last>Henning</last><affiliation>Bosch Center for Artificial Intelligence; Ludwig-Maximilians-Universitt Mnchen</affiliation></author>
      <author><first>Sherry</first><last>Tan</last><affiliation>TU Darmstadt</affiliation></author>
      <author><first>Matteo</first><last>Finco</last><affiliation>Robert Bosch GmbH</affiliation></author>
      <author><first>Stefan</first><last>Grünewald</last><affiliation>University of Stuttgart</affiliation></author>
      <author><first>Maira</first><last>Indrikova</last><affiliation>Robert Bosch GmbH</affiliation></author>
      <author><first>Felix</first><last>Hildebrand</last><affiliation>Robert Bosch GmbH</affiliation></author>
      <author><first>Annemarie</first><last>Friedrich</last><affiliation>University of Augsburg</affiliation></author>
      <pages>1-15</pages>
      <abstract>Scientific publications follow conventionalized rhetorical structures. Classifying the Argumentative Zone (AZ), e.g., identifying whether a sentence states a Motivation, a Result or Background information, has been proposed to improve processing of scholarly documents. In this work, we adapt and extend this idea to the domain of materials science research. We present and release a new dataset of 50 manually annotated research articles. The dataset spans seven sub-topics and is annotated with a materials-science focused multi-label annotation scheme for AZ. We detail corpus statistics and demonstrate high inter-annotator agreement. Our computational experiments show that using domain-specific pre-trained transformer-based text encoders is key to high classification performance. We also find that AZ categories from existing datasets in other domains are transferable to varying degrees.</abstract>
      <url hash="f21ae74a">2023.codi-1.1</url>
      <bibkey>schrader-etal-2023-mulms</bibkey>
      <doi>10.18653/v1/2023.codi-1.1</doi>
    </paper>
    <paper id="2">
      <title>A Side-by-side Comparison of Transformers for Implicit Discourse Relation Classification</title>
      <author><first>Bruce W.</first><last>Lee</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Bongseok</first><last>Yang</last><affiliation>LXPER AI Research</affiliation></author>
      <author><first>Jason</first><last>Lee</last><affiliation>Lxper Ai</affiliation></author>
      <pages>16-23</pages>
      <abstract>Though discourse parsing can help multiple NLP fields, there has been no wide language model search done on implicit discourse relation classification. This hinders researchers from fully utilizing public-available models in discourse analysis. This work is a straightforward, fine-tuned discourse performance comparison of 7 pre-trained language models. We use PDTB-3, a popular discourse relation annotated dataset. Through our model search, we raise SOTA to 0.671 ACC and obtain novel observations. Some are contrary to what has been reported before (Shi and Demberg, 2019b), that sentence-level pre-training objectives (NSP, SBO, SOP) generally fail to produce the best-performing model for implicit discourse relation classification. Counterintuitively, similar-sized PLMs with MLM and full attention led to better performance. Our code is publicly released.</abstract>
      <url hash="78e00d12">2023.codi-1.2</url>
      <bibkey>lee-etal-2023-side</bibkey>
      <doi>10.18653/v1/2023.codi-1.2</doi>
      <video href="2023.codi-1.2.mp4"/>
    </paper>
    <paper id="3">
      <title>Ensemble Transfer Learning for Multilingual Coreference Resolution</title>
      <author><first>Tuan</first><last>Lai</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Heng</first><last>Ji</last><affiliation>University of Illinois at Urbana-Champaign and Amazon (Amazon Scholar)</affiliation></author>
      <pages>24-36</pages>
      <abstract>Entity coreference resolution is an important research problem with many applications, including information extraction and question answering. Coreference resolution for English has been studied extensively. However, there is relatively little work for other languages. A problem that frequently occurs when working with a non-English language is the scarcity of annotated training data. To overcome this challenge, we design a simple but effective ensemble-based framework that combines various transfer learning (TL) techniques. We first train several models using different TL methods. Then, during inference, we compute the unweighted average scores of the models’ predictions to extract the final set of predicted clusters. Furthermore, we also propose a low-cost TL method that bootstraps coreference resolution models by utilizing Wikipedia anchor texts. Leveraging the idea that the coreferential links naturally exist between anchor texts pointing to the same article, our method builds a sizeable distantly-supervised dataset for the target language that consists of tens of thousands of documents. We can pre-train a model on the pseudo-labeled dataset before finetuning it on the final target dataset. Experimental results on two benchmark datasets, OntoNotes and SemEval, confirm the effectiveness of our methods. Our best ensembles consistently outperform the baseline approach of simple training by up to 7.68% in the F1 score. These ensembles also achieve new state-of-the-art results for three languages: Arabic, Dutch, and Spanish.</abstract>
      <url hash="4fc434aa">2023.codi-1.3</url>
      <bibkey>lai-ji-2023-ensemble</bibkey>
      <doi>10.18653/v1/2023.codi-1.3</doi>
    </paper>
    <paper id="4">
      <title>Contrastive Hierarchical Discourse Graph for Scientific Document Summarization</title>
      <author><first>Haopeng</first><last>Zhang</last><affiliation>University of California, Davis</affiliation></author>
      <author><first>Xiao</first><last>Liu</last><affiliation>Univeristy of California, Davis</affiliation></author>
      <author><first>Jiawei</first><last>Zhang</last><affiliation>UC Davis</affiliation></author>
      <pages>37-47</pages>
      <abstract>The extended structural context has made scientific paper summarization a challenging task. This paper proposes CHANGES, a contrastive hierarchical graph neural network for extractive scientific paper summarization. CHANGES represents a scientific paper with a hierarchical discourse graph and learns effective sentence representations with dedicated designed hierarchical graph information aggregation. We also propose a graph contrastive learning module to learn global theme-aware sentence representations. Extensive experiments on the PubMed and arXiv benchmark datasets prove the effectiveness of CHANGES and the importance of capturing hierarchical structure information in modeling scientific papers.</abstract>
      <url hash="ceb7aa36">2023.codi-1.4</url>
      <bibkey>zhang-etal-2023-contrastive-hierarchical</bibkey>
      <doi>10.18653/v1/2023.codi-1.4</doi>
    </paper>
    <paper id="5">
      <title>Leveraging Structural Discourse Information for Event Coreference Resolution in <fixed-case>D</fixed-case>utch</title>
      <author><first>Loic</first><last>De Langhe</last><affiliation>Ghent University</affiliation></author>
      <author><first>Orphee</first><last>De Clercq</last><affiliation>LT3, Ghent University</affiliation></author>
      <author><first>Veronique</first><last>Hoste</last><affiliation>LT3, Ghent University</affiliation></author>
      <pages>48-53</pages>
      <url hash="fa0e9374">2023.codi-1.5</url>
      <bibkey>de-langhe-etal-2023-leveraging</bibkey>
      <abstract>We directly embed easily extractable discourse structure information (subsection, paragraph and text type) in a transformer-based Dutch event coreference resolution model in order to more explicitly provide it with structural information that is known to be important in coreferential relationships. Results show that integrating this type of knowledge leads to a significant improvement in CONLL F1 for within-document settings (+ 8.6\%) and a minor improvement for cross-document settings (+ 1.1\%).</abstract>
      <doi>10.18653/v1/2023.codi-1.5</doi>
    </paper>
    <paper id="6">
      <title>Entity Coreference and Co-occurrence Aware Argument Mining from Biomedical Literature</title>
      <author><first>Boyang</first><last>Liu</last><affiliation>the University of Manchester</affiliation></author>
      <author><first>Viktor</first><last>Schlegel</last><affiliation>Asus Aics</affiliation></author>
      <author><first>Riza</first><last>Batista-navarro</last><affiliation>Department of Computer Science, The University of Manchester</affiliation></author>
      <author><first>Sophia</first><last>Ananiadou</last><affiliation>University of Manchester</affiliation></author>
      <pages>54-60</pages>
      <abstract>Biomedical argument mining (BAM) aims at automatically identifying the argumentative structure in biomedical texts. However, identifying and classifying argumentative relations (AR) between argumentative components (AC) is challenging since it not only needs to understand the semantics of ACs but also need to capture the interactions between them. We argue that entities can serve as bridges that connect different ACs since entities and their mentions convey significant semantic information in biomedical argumentation. For example, it is common that related AC pairs share a common entity. Capturing such entity information can be beneficial for the Relation Identification (RI) task. In order to incorporate this entity information into BAM, we propose an Entity Coreference and Co-occurrence aware Argument Mining (ECCAM) framework based on an edge-oriented graph model for BAM. We evaluate our model on a benchmark dataset and from the experimental results we find that our method improves upon state-of-the-art methods.</abstract>
      <url hash="26535bcb">2023.codi-1.6</url>
      <bibkey>liu-etal-2023-entity</bibkey>
      <doi>10.18653/v1/2023.codi-1.6</doi>
    </paper>
    <paper id="8">
      <title>A Weakly-Supervised Learning Approach to the Identification of “Alternative Lexicalizations” in Shallow Discourse Parsing</title>
      <author><first>René</first><last>Knaebel</last><affiliation>University of Potsdam</affiliation></author>
      <pages>61-69</pages>
      <abstract>Recently, the identification of free connective phrases as signals for discourse relations has received new attention with the introduction of statistical models for their automatic extraction. The limited amount of annotations makes it still challenging to develop well-performing models. In our work, we want to overcome this limitation with semi-supervised learning from unlabeled news texts. We implement a self-supervised sequence labeling approach and filter its predictions by a second model trained to disambiguate signal candidates. With our novel model design, we report state-of-the-art results and in addition, achieve an average improvement of about 5% for both exactly and partially matched alternativelylexicalized discourse signals due to weak supervision.</abstract>
      <url hash="d84acb07">2023.codi-1.8</url>
      <bibkey>knaebel-2023-weakly</bibkey>
      <doi>10.18653/v1/2023.codi-1.8</doi>
    </paper>
    <paper id="9">
      <title>Entity-based <fixed-case>S</fixed-case>pan<fixed-case>C</fixed-case>opy for Abstractive Summarization to Improve the Factual Consistency</title>
      <author><first>Wen</first><last>Xiao</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Giuseppe</first><last>Carenini</last><affiliation>University Of British Columbia</affiliation></author>
      <pages>70-81</pages>
      <abstract>Discourse-aware techniques, including entity-aware approaches, play a crucial role in summarization. In this paper, we propose an entity-based SpanCopy mechanism to tackle the entity-level factual inconsistency problem in abstractive summarization, i.e. reducing the mismatched entities between the generated summaries and the source documents. Complemented by a Global Relevance component to identify summary-worthy entities, our approach demonstrates improved factual consistency while preserving saliency on four summarization datasets, contributing to the effective application of discourse-aware methods summarization tasks.</abstract>
      <url hash="1b6f0b65">2023.codi-1.9</url>
      <bibkey>xiao-carenini-2023-entity</bibkey>
      <doi>10.18653/v1/2023.codi-1.9</doi>
    </paper>
    <paper id="10">
      <title>Discourse Information for Document-Level Temporal Dependency Parsing</title>
      <author><first>Jingcheng</first><last>Niu</last><affiliation>University of Toronto</affiliation></author>
      <author><first>Victoria</first><last>Ng</last><affiliation>Public Health Agency of Canada</affiliation></author>
      <author><first>Erin</first><last>Rees</last><affiliation>Public Health Agency of Canada</affiliation></author>
      <author><first>Simon</first><last>De Montigny</last><affiliation>University of Montreal</affiliation></author>
      <author><first>Gerald</first><last>Penn</last><affiliation>University of Toronto</affiliation></author>
      <pages>82-88</pages>
      <abstract>In this study, we examine the benefits of incorporating discourse information into document-level temporal dependency parsing. Specifically, we evaluate the effectiveness of integrating both high-level discourse profiling information, which describes the discourse function of sentences, and surface-level sentence position information into temporal dependency graph (TDG) parsing. Unexpectedly, our results suggest that simple sentence position information, particularly when encoded using our novel sentence-position embedding method, performs the best, perhaps because it does not rely on noisy model-generated feature inputs. Our proposed system surpasses the current state-of-the-art TDG parsing systems in performance. Furthermore, we aim to broaden the discussion on the relationship between temporal dependency parsing and discourse analysis, given the substantial similarities shared between the two tasks. We argue that discourse analysis results should not be merely regarded as an additional input feature for temporal dependency parsing. Instead, adopting advanced discourse analysis techniques and research insights can lead to more effective and comprehensive approaches to temporal information extraction tasks.</abstract>
      <url hash="8de4b0d6">2023.codi-1.10</url>
      <bibkey>niu-etal-2023-discourse</bibkey>
      <doi>10.18653/v1/2023.codi-1.10</doi>
    </paper>
    <paper id="11">
      <title>Encoding Discourse Structure: Comparison of <fixed-case>RST</fixed-case> and <fixed-case>QUD</fixed-case></title>
      <author><first>Sara</first><last>Shahmohammadi</last><affiliation>Universitt Potsdam</affiliation></author>
      <author><first>Hannah</first><last>Seemann</last><affiliation>Ruhr-Universitt</affiliation></author>
      <author><first>Manfred</first><last>Stede</last><affiliation>Universitt Potsdam</affiliation></author>
      <author><first>Tatjana</first><last>Scheffler</last><affiliation>Ruhr-Universitt</affiliation></author>
      <pages>89-98</pages>
      <abstract>We present a quantitative and qualitative comparison of the discourse trees defined by the Rhetorical Structure Theory and Questions under Discussion models. Based on an empirical analysis of parallel annotations for 28 texts (blog posts and podcast transcripts), we conclude that both discourse frameworks capture similar structural information. The qualitative analysis shows that while complex discourse units often match between analyses, QUD structures do not indicate the centrality of segments.</abstract>
      <url hash="15121d2f">2023.codi-1.11</url>
      <bibkey>shahmohammadi-etal-2023-encoding</bibkey>
      <doi>10.18653/v1/2023.codi-1.11</doi>
    </paper>
    <paper id="13">
      <title>Exploiting Knowledge about Discourse Relations for Implicit Discourse Relation Classification</title>
      <author><first>Nobel</first><last>Varghese</last><affiliation>Saarland University</affiliation></author>
      <author><first>Frances</first><last>Yung</last><affiliation>Saarland University</affiliation></author>
      <author><first>Kaveri</first><last>Anuranjana</last><affiliation>Saarland University</affiliation></author>
      <author><first>Vera</first><last>Demberg</last><affiliation>Saarland University</affiliation></author>
      <pages>99-105</pages>
      <abstract>In discourse relation recognition, the classification labels are typically represented as one-hot vectors. However, the categories are in fact not all independent of one another on the contrary, there are several frameworks that describe the labels’ similarities (by e.g. sorting them into a hierarchy or describing them interms of features (Sanders et al., 2021)). Recently, several methods for representing the similarities between labels have been proposed (Zhang et al., 2018; Wang et al., 2018; Xiong et al., 2021). We here explore and extend the Label Confusion Model (Guo et al., 2021) for learning a representation for discourse relation labels. We explore alternative ways of informing the model about the similarities between relations, by representing relations in terms of their names (and parent category), their typical markers, or in terms of CCR features that describe the relations. Experimental results show that exploiting label similarity improves classification results.</abstract>
      <url hash="e2b92136">2023.codi-1.13</url>
      <bibkey>varghese-etal-2023-exploiting</bibkey>
      <doi>10.18653/v1/2023.codi-1.13</doi>
    </paper>
    <paper id="14">
      <title><fixed-case>SAE</fixed-case>-<fixed-case>NTM</fixed-case>: Sentence-Aware Encoder for Neural Topic Modeling</title>
      <author><first>Hao</first><last>Liu</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Jingsheng</first><last>Gao</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Suncheng</first><last>Xiang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Ting</first><last>Liu</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Yuzhuo</first><last>Fu</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <pages>106-111</pages>
      <abstract>Incorporating external knowledge, such as pre-trained language models (PLMs), into neural topic modeling has achieved great success in recent years. However, employing PLMs for topic modeling generally ignores the maximum sequence length of PLMs and the interaction between external knowledge and bag-of-words (BOW). To this end, we propose a sentence-aware encoder for neural topic modeling, which adopts fine-grained sentence embeddings as external knowledge to entirely utilize the semantic information of input documents. We introduce sentence-aware attention for document representation, where BOW enables the model to attend on topical sentences that convey topic-related cues. Experiments on three benchmark datasets show that our framework outperforms other state-of-the-art neural topic models in topic coherence. Further, we demonstrate that the proposed approach can yield better latent document-topic features through improvement on the document classification.</abstract>
      <url hash="351ac862">2023.codi-1.14</url>
      <bibkey>liu-etal-2023-sae</bibkey>
      <doi>10.18653/v1/2023.codi-1.14</doi>
    </paper>
    <paper id="15">
      <title>Improving Long Context Document-Level Machine Translation</title>
      <author><first>Christian</first><last>Herold</last><affiliation>RWTH Aachen University</affiliation></author>
      <author><first>Hermann</first><last>Ney</last><affiliation>RWTH Aachen University</affiliation></author>
      <pages>112-125</pages>
      <abstract>Document-level context for neural machine translation (NMT) is crucial to improve the translation consistency and cohesion, the translation of ambiguous inputs, as well as several other linguistic phenomena. Many works have been published on the topic of document-level NMT, but most restrict the system to only local context, typically including just the one or two preceding sentences as additional information. This might be enough to resolve some ambiguous inputs, but it is probably not sufficient to capture some document-level information like the topic or style of a conversation. When increasing the context size beyond just the local context, there are two challenges: (i) the memory usage increases exponentially (ii) the translation performance starts to degrade. We argue that the widely-used attention mechanism is responsible for both issues. Therefore, we propose a constrained attention variant that focuses the attention on the most relevant parts of the sequence, while simultaneously reducing the memory consumption. For evaluation, we utilize targeted test sets in combination with novel evaluation techniques to analyze the translations in regards to specific discourse-related phenomena. We find that our approach is a good compromise between sentence-level NMT vs attending to the full context, especially in low resource scenarios.</abstract>
      <url hash="01bd6e9c">2023.codi-1.15</url>
      <bibkey>herold-ney-2023-improving</bibkey>
      <doi>10.18653/v1/2023.codi-1.15</doi>
      <video href="2023.codi-1.15.mp4"/>
    </paper>
    <paper id="16">
      <title>Unpacking Ambiguous Structure: A Dataset for Ambiguous Implicit Discourse Relations for <fixed-case>E</fixed-case>nglish and <fixed-case>E</fixed-case>gyptian <fixed-case>A</fixed-case>rabic</title>
      <author><first>Ahmed</first><last>Ruby</last><affiliation>Uppsala university</affiliation></author>
      <author><first>Sara</first><last>Stymne</last><affiliation>Uppsala University</affiliation></author>
      <author><first>Christian</first><last>Hardmeier</last><affiliation>IT University of Copenhagen/Uppsala University</affiliation></author>
      <pages>126-144</pages>
      <abstract>In this paper, we present principles of constructing and resolving ambiguity in implicit discourse relations. Following these principles, we created a dataset in both English and Egyptian Arabic that controls for semantic disambiguation, enabling the investigation of prosodic features in future work. In these datasets, examples are two-part sentences with an implicit discourse relation that can be ambiguously read as either causal or concessive, paired with two different preceding context sentences forcing either the causal or the concessive reading. We also validated both datasets by humans and language models (LMs) to study whether context can help humans or LMs resolve ambiguities of implicit relations and identify the intended relation. As a result, this task posed no difficulty for humans, but proved challenging for BERT/CamelBERT and ELECTRA/AraELECTRA models.</abstract>
      <url hash="a14f7a05">2023.codi-1.16</url>
      <bibkey>ruby-etal-2023-unpacking</bibkey>
      <doi>10.18653/v1/2023.codi-1.16</doi>
    </paper>
    <paper id="20">
      <title>Two-step Text Summarization for Long-form Biographical Narrative Genre</title>
      <author><first>Avi</first><last>Bleiweiss</last><affiliation>BShalem</affiliation></author>
      <pages>145-155</pages>
      <abstract>Transforming narrative structure to implicit discourse relations in long-form text has recently seen a mindset shift toward assessing generation consistency. To this extent, summarization of lengthy biographical discourse is of practical benefit to readers, as it helps them decide whether immersing for days or weeks in a bulky book turns a rewarding experience. Machine-generated summaries can reduce the cognitive load and the time spent by authors to write the summary. Nevertheless, summarization faces significant challenges of factual inconsistencies with respect to the inputs. In this paper, we explored a two-step summary generation aimed to retain source-summary faithfulness. Our method uses a graph representation to rank sentence saliency in each of the novel chapters, leading to distributing summary segments in distinct regions of the chapter. Basing on the previously extracted sentences we produced an abstractive summary in a manner more computationally tractable for detecting inconsistent information. We conducted a series of quantitative analyses on a test set of four long biographical novels and showed to improve summarization quality in automatic evaluation over both single-tier settings and external baselines.</abstract>
      <url hash="58180c45">2023.codi-1.20</url>
      <attachment type="SupplementaryMaterial" hash="e12adef4">2023.codi-1.20.SupplementaryMaterial.zip</attachment>
      <bibkey>bleiweiss-2023-two</bibkey>
      <doi>10.18653/v1/2023.codi-1.20</doi>
    </paper>
    <paper id="21">
      <title>The distribution of discourse relations within and across turns in spontaneous conversation</title>
      <author><first>S. Magalí</first><last>López Cortez</last><affiliation>University at Buffalo</affiliation></author>
      <author><first>Cassandra L.</first><last>Jacobs</last><affiliation>University at Buffalo</affiliation></author>
      <pages>156-162</pages>
      <abstract>Time pressure and topic negotiation may impose constraints on how people leverage discourse relations (DRs) in spontaneous conversational contexts. In this work, we adapt a system of DRs for written language to spontaneous dialogue using crowdsourced annotations from novice annotators. We then test whether discourse relations are used differently across several types of multi-utterance contexts. We compare the patterns of DR annotation within and across speakers and within and across turns. Ultimately, we find that different discourse contexts produce distinct distributions of discourse relations, with single-turn annotations creating the most uncertainty for annotators. Additionally, we find that the discourse relation annotations are of sufficient quality to predict from embeddings of discourse units.</abstract>
      <url hash="9216eeca">2023.codi-1.21</url>
      <bibkey>lopez-cortez-jacobs-2023-distribution</bibkey>
      <doi>10.18653/v1/2023.codi-1.21</doi>
      <video href="2023.codi-1.21.mp4"/>
    </paper>
    <paper id="22">
      <title>Embedding Mental Health Discourse for Community Recommendation</title>
      <author><first>Hy</first><last>Dang</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Bang</first><last>Nguyen</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Noah</first><last>Ziems</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Meng</first><last>Jiang</last><affiliation>University of Notre Dame</affiliation></author>
      <pages>163-172</pages>
      <abstract>Our paper investigates the use of discourse embedding techniques to develop a community recommendation system that focuses on mental health support groups on social media. Social media platforms provide a means for users to anonymously connect with communities that cater to their specific interests. However, with the vast number of online communities available, users may face difficulties in identifying relevant groups to address their mental health concerns. To address this challenge, we explore the integration of discourse information from various subreddit communities using embedding techniques to develop an effective recommendation system. Our approach involves the use of content-based and collaborative filtering techniques to enhance the performance of the recommendation system. Our findings indicate that the proposed approach outperforms the use of each technique separately and provides interpretability in the recommendation process.</abstract>
      <url hash="32ac4b43">2023.codi-1.22</url>
      <bibkey>dang-etal-2023-embedding</bibkey>
      <doi>10.18653/v1/2023.codi-1.22</doi>
      <video href="2023.codi-1.22.mp4"/>
    </paper>
    <paper id="23">
      <title><fixed-case>APA</fixed-case>-<fixed-case>RST</fixed-case>: A Text Simplification Corpus with <fixed-case>RST</fixed-case> Annotations</title>
      <author><first>Freya</first><last>Hewett</last><affiliation>University of Potsdam</affiliation></author>
      <pages>173-179</pages>
      <abstract>We present a corpus of parallel German-language simplified newspaper articles. The articles have been aligned at sentence level and annotated according to the Rhetorical Structure Theory (RST) framework. These RST annotated texts could shed light on structural aspects of text complexity and how simplifications work on a text-level.</abstract>
      <url hash="894bdc93">2023.codi-1.23</url>
      <bibkey>hewett-2023-apa</bibkey>
      <doi>10.18653/v1/2023.codi-1.23</doi>
    </paper>
  </volume>
</collection>
