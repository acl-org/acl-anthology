<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.nlp4convai">
  <volume id="1" ingest-date="2024-07-31" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 6th Workshop on NLP for Conversational AI (NLP4ConvAI 2024)</booktitle>
      <editor><first>Elnaz</first><last>Nouri</last></editor>
      <editor><first>Abhinav</first><last>Rastogi</last></editor>
      <editor><first>Georgios</first><last>Spithourakis</last></editor>
      <editor><first>Bing</first><last>Liu</last></editor>
      <editor><first>Yun-Nung</first><last>Chen</last></editor>
      <editor><first>Yu</first><last>Li</last></editor>
      <editor><first>Alon</first><last>Albalak</last></editor>
      <editor><first>Hiromi</first><last>Wakaki</last></editor>
      <editor><first>Alexandros</first><last>Papangelis</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Bangkok, Thailand</address>
      <month>August</month>
      <year>2024</year>
      <url hash="d414ca8b">2024.nlp4convai-1</url>
      <venue>nlp4convai</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="86ea0505">2024.nlp4convai-1.0</url>
      <bibkey>nlp4convai-1-2024</bibkey>
    </frontmatter>
    <paper id="1">
      <title>On the Benchmarking of <fixed-case>LLM</fixed-case>s for Open-Domain Dialogue Evaluation</title>
      <author><first>John</first><last>Mendonça</last><affiliation>Instituto Superior Técnico</affiliation></author>
      <author><first>Alon</first><last>Lavie</last><affiliation>Phrase and School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Isabel</first><last>Trancoso</last><affiliation>Instituto Superior Técnico</affiliation></author>
      <pages>1-12</pages>
      <abstract>Large Language Models (LLMs) have showcased remarkable capabilities in various Natural Language Processing tasks. For automatic open-domain dialogue evaluation in particular, LLMs have been seamlessly integrated into evaluation frameworks, and together with human evaluation, compose the backbone of most evaluations. However, existing evaluation benchmarks often rely on outdated datasets and evaluate aspects like Fluency and Relevance, which fail to adequately capture the capabilities and limitations of state-of-the-art chatbot models. This paper critically examines current evaluation benchmarks, highlighting that the use of older response generators and quality aspects fail to accurately reflect modern chatbot capabilities. A small annotation experiment on a recent LLM-generated dataset (SODA) reveals that LLM evaluators such as GPT-4 struggle to detect actual deficiencies in dialogues generated by current LLM chatbots.</abstract>
      <url hash="98e02f5d">2024.nlp4convai-1.1</url>
      <bibkey>mendonca-etal-2024-benchmarking</bibkey>
    </paper>
    <paper id="2">
      <title>Exploring Description-Augmented Dataless Intent Classification</title>
      <author><first>Ruoyu</first><last>Hu</last></author>
      <author><first>Foaad</first><last>Khosmood</last><affiliation>California Polytechnic State University, San Luis Obispo</affiliation></author>
      <author><first>Abbas</first><last>Edalat</last><affiliation>Imperial College London</affiliation></author>
      <pages>13-36</pages>
      <abstract>In this work, we introduce several schemes to leverage description-augmented embedding similarity for dataless intent classification using current state-of-the-art (SOTA) text embedding models. We report results of our methods on four commonly used intent classification datasets and compare against previous works of a similar nature. Our work shows promising results for dataless classification scaling to a large number of unseen intents. We show competitive results and significant improvements (+6.12% Avg.) over strong zero-shot baselines, all without training on labelled or task-specific data. Furthermore, we provide qualitative error analysis of the shortfalls of this methodology to help guide future research in this area.</abstract>
      <url hash="eb941b9e">2024.nlp4convai-1.2</url>
      <bibkey>hu-etal-2024-exploring</bibkey>
    </paper>
    <paper id="3">
      <title>Revealing User Familiarity Bias in Task-Oriented Dialogue via Interactive Evaluation</title>
      <author><first>Takyoung</first><last>Kim</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Jamin</first><last>Shin</last><affiliation>NAVER</affiliation></author>
      <author><first>Young-Ho</first><last>Kim</last><affiliation>NAVER AI Lab</affiliation></author>
      <author><first>Sanghwan</first><last>Bae</last><affiliation>NAVER Cloud</affiliation></author>
      <author><first>Sungdong</first><last>Kim</last><affiliation>KAIST AI and NAVER</affiliation></author>
      <pages>37-55</pages>
      <abstract>Most task-oriented dialogue (TOD) benchmarks assume users that know exactly how to use the system by constraining the user behaviors within the system’s capabilities via strict user goals, namely “user familiarity” bias. This data bias deepens when it combines with data-driven TOD systems, as it is impossible to fathom the effect of it with existing static evaluations. Hence, we conduct an interactive user study to unveil how vulnerable TOD systems are against realistic scenarios. In particular, we compare users with 1) detailed goal instructions that conform to the system boundaries (closed-goal) and 2) vague goal instructions that are often unsupported but realistic (open-goal). Our study reveals that conversations in open-goal settings lead to catastrophic failures of the system, in which 92% of the dialogues had significant issues. Moreover, we conduct a thorough analysis to identify distinctive features between the two settings through error annotation. From this, we discover a novel “pretending” behavior, in which the system pretends to handle the user requests even though they are beyond the system’s capabilities. We discuss its characteristics and toxicity while showing recent large language models can also suffer from this behavior.</abstract>
      <url hash="e8df30fb">2024.nlp4convai-1.3</url>
      <bibkey>kim-etal-2024-revealing</bibkey>
    </paper>
    <paper id="4">
      <title>Evaluating Robustness of Open Dialogue Summarization Models in the Presence of Naturally Occurring Variations</title>
      <author><first>Ankita</first><last>Gupta</last></author>
      <author><first>Chulaka</first><last>Gunasekara</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Hui</first><last>Wan</last><affiliation>IBM Research AI</affiliation></author>
      <author><first>Jatin</first><last>Ganhotra</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Sachindra</first><last>Joshi</last></author>
      <author><first>Marina</first><last>Danilevsky</last><affiliation>International Business Machines</affiliation></author>
      <pages>56-72</pages>
      <abstract>Dialogue summarization involves summarizing long conversations while preserving the most salient information. Real-life dialogues often involve naturally occurring variations (e.g., repetitions, hesitations). In this study, we systematically investigate the impact of such variations on state-of-the-art open dialogue summarization models whose details are publicly known (e.g., architectures, weights, and training corpora). To simulate real-life variations, we introduce two types of perturbations: utterance-level perturbations that modify individual utterances with errors and language variations, and dialogue-level perturbations that add non-informative exchanges (e.g., repetitions, greetings). We perform our analysis along three dimensions of robustness: consistency, saliency, and faithfulness, which aim to capture different aspects of performance of a summarization model. We find that both fine-tuned and instruction-tuned models are affected by input variations, with the latter being more susceptible, particularly to dialogue-level perturbations. We also validate our findings via human evaluation. Finally, we investigate whether the robustness of fine-tuned models can be improved by training them with a fraction of perturbed data. We find that this approach does not yield consistent performance gains, warranting further research. Overall, our work highlights robustness challenges in current open encoder-decoder summarization models and provides insights for future research.</abstract>
      <url hash="e5567c0b">2024.nlp4convai-1.4</url>
      <bibkey>gupta-etal-2024-evaluating</bibkey>
      <revision id="1" href="2024.nlp4convai-1.4v1" hash="98287004"/>
      <revision id="2" href="2024.nlp4convai-1.4v2" hash="e5567c0b" date="2024-10-25">This revision provides a de-anonymized, camera-ready version of the paper.</revision>
    </paper>
    <paper id="5">
      <title>Engineering Conversational Search Systems: A Review of Applications, Architectures, and Functional Components</title>
      <author><first>Phillip</first><last>Schneider</last></author>
      <author><first>Wessel</first><last>Poelman</last><affiliation>KU Leuven</affiliation></author>
      <author><first>Michael</first><last>Rovatsos</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Florian</first><last>Matthes</last><affiliation>Technische Universität München</affiliation></author>
      <pages>73-88</pages>
      <abstract>Conversational search systems enable information retrieval via natural language interactions, with the goal of maximizing users’ information gain over multiple dialogue turns. The increasing prevalence of conversational interfaces adopting this search paradigm challenges traditional information retrieval approaches, stressing the importance of better understanding the engineering process of developing these systems. We undertook a systematic literature review to investigate the links between theoretical studies and technical implementations of conversational search systems. Our review identifies real-world application scenarios, system architectures, and functional components. We consolidate our results by presenting a layered architecture framework and explaining the core functions of conversational search systems. Furthermore, we reflect on our findings in light of the rapid progress in large language models, discussing their capabilities, limitations, and directions for future research.</abstract>
      <url hash="d36b0544">2024.nlp4convai-1.5</url>
      <bibkey>schneider-etal-2024-engineering</bibkey>
      <revision id="1" href="2024.nlp4convai-1.5v1" hash="b9359051"/>
      <revision id="2" href="2024.nlp4convai-1.5v2" hash="d36b0544" date="2024-09-09">This revision corrects the page numbering.</revision>
    </paper>
    <paper id="6">
      <title>Efficient Dynamic Hard Negative Sampling for Dialogue Selection</title>
      <author><first>Janghoon</first><last>Han</last><affiliation>LG AI Research</affiliation></author>
      <author><first>Dongkyu</first><last>Lee</last></author>
      <author><first>Joongbo</first><last>Shin</last><affiliation>LG AI Research</affiliation></author>
      <author><first>Hyunkyung</first><last>Bae</last><affiliation>LG AI Research</affiliation></author>
      <author><first>Jeesoo</first><last>Bang</last><affiliation>LG AI Research</affiliation></author>
      <author><first>Seonghwan</first><last>Kim</last><affiliation>LG AI Research</affiliation></author>
      <author><first>Stanley Jungkyu</first><last>Choi</last><affiliation>Language Lab, LG AI Research</affiliation></author>
      <author><first>Honglak</first><last>Lee</last><affiliation>University of Michigan - Ann Arbor and LG AI Research</affiliation></author>
      <pages>89-100</pages>
      <abstract>Recent studies have demonstrated significant improvements in selection tasks, and a considerable portion of this success is attributed to incorporating informative negative samples during training. While traditional methods for constructing hard negatives provide meaningful supervision, they depend on static samples that do not evolve during training, leading to sub-optimal performance. Dynamic hard negative sampling addresses this limitation by continuously adapting to the model’s changing state throughout training. However, the high computational demands of this method restrict its applicability to certain model architectures. To overcome these challenges, we introduce an efficient dynamic hard negative sampling (EDHNS). EDHNS enhances efficiency by pre-filtering easily discriminable negatives, thereby reducing the number of candidates the model needs to compute during training. Additionally, it excludes question-candidate pairs where the model already exhibits high confidence from loss computations, further reducing training time. These approaches maintain learning quality while minimizing computation and streamlining the training process. Extensive experiments on DSTC9, DSTC10, Ubuntu, and E-commerce benchmarks demonstrate that EDHNS significantly outperforms baseline models, proving its effectiveness in dialogue selection tasks.</abstract>
      <url hash="dd9518c8">2024.nlp4convai-1.6</url>
      <bibkey>han-etal-2024-efficient</bibkey>
    </paper>
    <paper id="7">
      <title>Chamain: Harmonizing Character Persona Integrity with Domain-Adaptive Knowledge in Dialogue Generation</title>
      <author><first>Seung-Moo</first><last>Yang</last><affiliation>Seoul National University of Science and Technology</affiliation></author>
      <author><first>Jeehyun</first><last>Lee</last></author>
      <author><first>Won Ik</first><last>Cho</last><affiliation>Samsung Advanced Institute of Technology</affiliation></author>
      <pages>101-113</pages>
      <abstract>Recent advances in large language models (LLMs) have shown their capacity for generating natural dialogues, leveraging extensive pre-trained knowledge. However, the seamless integration of domain-specific knowledge into dialogue agents, without undermining their personas or unique textual style, remains a challenging task. Traditional approaches, such as constructing knowledge-aware character dialogue datasets or training LLMs from the ground up, require considerable resources. Sequentially fine-tuning character chatbots across multiple datasets or applying existing merging techniques often leads to catastrophic forgetting, resulting in the loss of both knowledge and the character’s distinct persona. This compromises the model’s ability to consistently generate character-driven dialogues within a user-centric framework. In this context, we introduce a novel model merging method, Chamain, which effortlessly enhances the performance of character models, much like finding a “free lunch”. Chamain merges domain-specific knowledge into a character model by parameter-wise weight combination of instruction-tuned models and learns to reflect persona’s unique characteristics and style through Layer-wise merging. Our experiments demonstrate that Chamain effectively maintains style while also solving domain-specific problems to a certain extent compared to the baselines, even showing a higher style probability compared to the character model in legal QA.</abstract>
      <url hash="1a3738c8">2024.nlp4convai-1.7</url>
      <bibkey>yang-etal-2024-chamain</bibkey>
    </paper>
    <paper id="8">
      <title>Faithful Persona-based Conversational Dataset Generation with Large Language Models</title>
      <author><first>Pegah</first><last>Jandaghi</last></author>
      <author><first>Xianghai</first><last>Sheng</last><affiliation>Google</affiliation></author>
      <author><first>Xinyi</first><last>Bai</last><affiliation>Google</affiliation></author>
      <author><first>Jay</first><last>Pujara</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Hakim</first><last>Sidahmed</last></author>
      <pages>114-139</pages>
      <abstract>High-quality conversational datasets are essential for developing AI models that can communicate with users. One way to foster deeper interactions between a chatbot and its user is through personas, aspects of the user’s character that provide insights into their personality, motivations, and behaviors. Training Natural Language Processing (NLP) models on a diverse and comprehensive persona-based dataset can lead to conversational models that create a deeper connection with the user, and maintain their engagement. In this paper, we leverage the power of Large Language Models (LLMs) to create a large, high-quality conversational dataset from a seed dataset. We propose a Generator-Critic architecture framework to expand the initial dataset, while improving the quality of its conversations. The Generator is an LLM prompted to output conversations. The Critic consists of a mixture of expert LLMs that control the quality of the generated conversations. These experts select the best generated conversations, which we then use to improve the Generator. We release Synthetic-Persona-Chat, consisting of 20k conversations seeded from Persona-Chat. We evaluate the quality of Synthetic-Persona-Chat and our generation framework on different dimensions through extensive experiments, and observe that the losing rate of Synthetic-Persona-Chat against Persona-Chat during an AI detection test decreases from 17.2% to 8.8% over three iterations.</abstract>
      <url hash="0ff53865">2024.nlp4convai-1.8</url>
      <bibkey>jandaghi-etal-2024-faithful</bibkey>
    </paper>
  </volume>
</collection>
