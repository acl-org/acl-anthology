<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.trl">
  <volume id="1" ingest-date="2025-07-23" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 4th Table Representation Learning Workshop</booktitle>
      <editor><first>Shuaichen</first><last>Chang</last></editor>
      <editor><first>Madelon</first><last>Hulsebos</last></editor>
      <editor><first>Qian</first><last>Liu</last></editor>
      <editor><first>Wenhu</first><last>Chen</last></editor>
      <editor><first>Huan</first><last>Sun</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Vienna, Austria</address>
      <month>July</month>
      <year>2025</year>
      <url hash="71905cff">2025.trl-1</url>
      <venue>trl</venue>
      <venue>ws</venue>
      <isbn>979-8-89176-268-8</isbn>
      <doi>10.18653/v1/2025.trl-1</doi>
    </meta>
    <frontmatter>
      <url hash="819784d3">2025.trl-1.0</url>
      <bibkey>trl-ws-2025-1</bibkey>
      <doi>10.18653/v1/2025.trl-1.0</doi>
    </frontmatter>
    <paper id="1">
      <title>Theme-Explanation Structure for Table Summarization using Large Language Models: A Case Study on <fixed-case>K</fixed-case>orean Tabular Data</title>
      <author><first>TaeYoon</first><last>Kwack</last></author>
      <author><first>Jisoo</first><last>Kim</last></author>
      <author><first>Ki Yong</first><last>Jung</last><affiliation>Sung Kyun Kwan University</affiliation></author>
      <author><first>DongGeon</first><last>Lee</last></author>
      <author><first>Heesun</first><last>Park</last><affiliation>Sung Kyun Kwan University</affiliation></author>
      <pages>1-12</pages>
      <abstract>Tables are a primary medium for conveying critical information in administrative domains, yet their complexity hinders utilization by Large Language Models (LLMs). This paper introduces the Theme-Explanation Structure-based Table Summarization (Tabular-TX) pipeline, a novel approach designed to generate highly interpretable summaries from tabular data, with a specific focus on Korean administrative documents. Current table summarization methods often neglect the crucial aspect of human-friendly output. Tabular-TX addresses this by first employing a multi-step reasoning process to ensure deep table comprehension by LLMs, followed by a journalist persona prompting strategy for clear sentence generation. Crucially, it then structures the output into a Theme Part (an adverbial phrase) and an Explanation Part (a predicative clause), significantly enhancing readability. Our approach leverages in-context learning, obviating the need for extensive fine-tuning and associated labeled data or computational resources. Experimental results show that Tabular-TX effectively processes complex table structures and metadata, offering a robust and efficient solution for generating human-centric table summaries, especially in low-resource scenarios.</abstract>
      <url hash="1fed9a5e">2025.trl-1.1</url>
      <bibkey>kwack-etal-2025-theme</bibkey>
      <doi>10.18653/v1/2025.trl-1.1</doi>
    </paper>
    <paper id="2">
      <title>Generating Synthetic Relational Tabular Data via Structural Causal Models</title>
      <author><first>Frederik</first><last>Hoppe</last><affiliation>CONTACT-Software GmbH and Rheinisch Westfälische Technische Hochschule Aachen</affiliation></author>
      <author><first>Astrid</first><last>Franz</last><affiliation>CONTACT Software GmbH</affiliation></author>
      <author><first>Lars</first><last>Kleinemeier</last><affiliation>Contact Software GmbH</affiliation></author>
      <author><first>Udo</first><last>Göbel</last><affiliation>CONTACT Software</affiliation></author>
      <pages>13-18</pages>
      <abstract>Synthetic tabular data generation has received increasing attention in recent years, particularly with the emergence of foundation models for tabular data. The breakthrough success of TabPFN (Hollmann et al.,2025), which leverages vast quantities of synthetic tabular datasets derived from structural causal models (SCMs), demonstrates the critical role synthetic data plays in developing powerful tabular foundation models. However, most real-world tabular data exists in relational formats spanning multiple interconnected tables — a structure not adequately addressed by current generation methods. In this work, we extend the SCM-based approach by developing a novel framework that generates realistic synthetic relational tabular data including causal relationships across tables. Our experiments confirm that this framework is able to construct relational datasets with complex inter-table dependencies mimicking real-world scenarios.</abstract>
      <url hash="d45dface">2025.trl-1.2</url>
      <bibkey>hoppe-etal-2025-generating</bibkey>
      <doi>10.18653/v1/2025.trl-1.2</doi>
    </paper>
    <paper id="3">
      <title>Tables as Thought: Exploring Structured Thoughts in <fixed-case>LLM</fixed-case> Reasoning</title>
      <author><first>Zhenjie</first><last>Sun</last></author>
      <author><first>Naihao</first><last>Deng</last></author>
      <author><first>Haofei</first><last>Yu</last></author>
      <author><first>Jiaxuan</first><last>You</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <pages>19-33</pages>
      <abstract>Large language models’ reasoning abilities benefit from methods that organize their thought processes, such as chain-of-thought prompting, which employs a sequential structure to guide the reasoning process step-by-step. However, existing approaches focus primarily on organizing the sequence of thoughts, leaving structure in individual thought steps underexplored. To address this gap, we propose Table as Thought, a framework inspired by cognitive neuroscience theories on human thought. Table as Thought organizes reasoning within a tabular schema, where rows represent sequential thought steps and columns capture critical constraints and contextual information to enhance reasoning. The reasoning process iteratively populates the table until self-verification ensures completeness and correctness. Our experiments show that Table as Thought excels in planning tasks and demonstrates a strong potential for enhancing LLM performance in mathematical reasoning compared to unstructured thought baselines. This work provides a novel exploration of refining thought representation within LLMs, paving the way for advancements in reasoning and AI cognition.</abstract>
      <url hash="f5bc05bc">2025.trl-1.3</url>
      <bibkey>sun-etal-2025-tables</bibkey>
      <doi>10.18653/v1/2025.trl-1.3</doi>
    </paper>
    <paper id="4">
      <title><tex-math>R^3</tex-math>: “This is My <fixed-case>SQL</fixed-case>, Are You With Me?” A Consensus-Based Multi-Agent System for Text-to-<fixed-case>SQL</fixed-case> Tasks</title>
      <author><first>Hanchen</first><last>Xia</last></author>
      <author><first>Feng</first><last>Jiang</last></author>
      <author><first>Naihao</first><last>Deng</last></author>
      <author><first>Cunxiang</first><last>Wang</last></author>
      <author><first>Guojiang</first><last>Zhao</last></author>
      <author><first>Rada</first><last>Mihalcea</last><affiliation>University of Michigan</affiliation></author>
      <author><first>Yue</first><last>Zhang</last><affiliation>Westlake University</affiliation></author>
      <pages>34-46</pages>
      <abstract>Large Language Models (LLMs) have demon- strated exceptional performance across diverse tasks. To harness their capabilities for Text- to-SQL, we introduce R3 (Review-Rebuttal- Revision), a consensus-based multi-agent sys- tem for Text-to-SQL tasks. R3 achieves the new state-of-the-art performance of 89.9 on the Spider test set. In the meantime, R3 achieves 61.80 on the Bird development set. R3 out- performs existing single-LLM and multi-agent Text-to-SQL systems by 1.3% to 8.1% on Spi- der and Bird, respectively. Surprisingly, we find that for Llama-3-8B, R3 outperforms chain-of- thought prompting by over 20%, even outper- forming GPT-3.5 on the Spider development set. We open-source our codebase at https: //github.com/1ring2rta/R3.</abstract>
      <url hash="acf73e4a">2025.trl-1.4</url>
      <bibkey>xia-etal-2025-r3</bibkey>
      <doi>10.18653/v1/2025.trl-1.4</doi>
    </paper>
    <paper id="5">
      <title><fixed-case>SQL</fixed-case>ong: Enhanced <fixed-case>NL</fixed-case>2<fixed-case>SQL</fixed-case> for Longer Contexts with <fixed-case>LLM</fixed-case>s</title>
      <author><first>Dai Quoc</first><last>Nguyen</last><affiliation>Oracle</affiliation></author>
      <author><first>Cong Duy Vu</first><last>Hoang</last><affiliation>Oracle Corporation</affiliation></author>
      <author><first>Duy Quang</first><last>Vu</last><affiliation>Oracle</affiliation></author>
      <author><first>Gioacchino</first><last>Tangari</last><affiliation>Oracle</affiliation></author>
      <author><first>Thanh</first><last>Vu</last><affiliation>Oracle</affiliation></author>
      <author><first>Don</first><last>Dharmasiri</last><affiliation>Oracle</affiliation></author>
      <author><first>Yuan-Fang</first><last>Li</last><affiliation>Monash University and Oracle</affiliation></author>
      <author><first>Long</first><last>Duong</last><affiliation>Oracle</affiliation></author>
      <pages>47-55</pages>
      <abstract>Open-weight large language models (LLMs) have significantly advanced performance in the Natural Language to SQL (NL2SQL) task. However, their effectiveness diminishes when dealing with large database schemas, as the context length increases. To address this limitation, we present SQLong, a novel and efficient data augmentation framework designed to enhance LLM performance in long-context scenarios for the NL2SQL task. SQLong generates augmented datasets by extending existing database schemas with additional synthetic CREATE TABLE commands and corresponding data rows, sampled from diverse schemas in the training data. This approach effectively simulates long-context scenarios during finetuning and evaluation. Through experiments on the Spider and BIRD datasets, we demonstrate that LLMs finetuned with SQLong-augmented data significantly outperform those trained on standard datasets. These imply SQLong’s practical implementation and its impact on improving NL2SQL capabilities in real-world settings with complex database schemas.</abstract>
      <url hash="e21d57a9">2025.trl-1.5</url>
      <bibkey>nguyen-etal-2025-sqlong</bibkey>
      <doi>10.18653/v1/2025.trl-1.5</doi>
    </paper>
    <paper id="6">
      <title>i<fixed-case>TBLS</fixed-case>: A Dataset of Interactive Conversations Over Tabular Information</title>
      <author><first>Anirudh</first><last>Sundar</last></author>
      <author><first>Christopher Gordon</first><last>Richardson</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Larry</first><last>Heck</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Adar</first><last>Avsian</last></author>
      <pages>56-70</pages>
      <abstract>This paper introduces Interactive Tables (iTBLS), a dataset of interactive conversations that focuses on natural-language manipulation of tabular information sourced from academic pre-prints on ArXiv. The iTBLS dataset consists of three types of tabular tasks – interpretation, modification, and generation. Interpretation focuses on tabular understanding, modification focuses on manipulating tabular information, and generation focuses on the addition of new natural-language evidence. In addition, the paper presents a novel framework that reformulates tabular operations as question-answering, where an appropriate question is formulated based on the nature of interaction and the question is answered using the user request as evidence. The developed approach results in an improvement on all tasks on a sequence-to-sequence modeling baseline on iTBLS. In addition, the question-answering-based reformulation is applied to datasets from prior work for the text-to-table task where textual paragraphs are summarized into tables. The novel approach results in up to 13% improvement in Exact-Match accuracy and up to 16% improvement in BERTScores compared to the prior state-of-the-art.</abstract>
      <url hash="f83f9cd9">2025.trl-1.6</url>
      <bibkey>sundar-etal-2025-itbls</bibkey>
      <doi>10.18653/v1/2025.trl-1.6</doi>
    </paper>
    <paper id="7">
      <title>Something’s Fishy in the Data Lake: A Critical Re-evaluation of Table Union Search Benchmarks</title>
      <author><first>Allaa</first><last>Boutaleb</last></author>
      <author><first>Bernd</first><last>Amann</last><affiliation>Sorbonne Université</affiliation></author>
      <author><first>Hubert</first><last>Naacke</last><affiliation>Sorbonne Université</affiliation></author>
      <author><first>Rafael</first><last>Angarita</last><affiliation>Université Paris Ouest Paris X Nanterre</affiliation></author>
      <pages>71-85</pages>
      <abstract>Recent table representation learning and data discovery methods tackle table union search (TUS) within data lakes, which involves identifying tables that can be unioned with a given query table to enrich its content. These methods are commonly evaluated using benchmarks that aim to assess semantic understanding in real-world TUS tasks. However, our analysis of prominent TUS benchmarks reveals several limitations that allow simple baselines to perform surprisingly well, often outperforming more sophisticated approaches. This suggests that current benchmark scores are heavily influenced by dataset-specific characteristics and fail to effectively isolate the gains from semantic understanding. To address this, we propose essential criteria for future benchmarks to enable a more realistic and reliable evaluation of progress in semantic table union search.</abstract>
      <url hash="e04e159c">2025.trl-1.7</url>
      <bibkey>boutaleb-etal-2025-somethings</bibkey>
      <doi>10.18653/v1/2025.trl-1.7</doi>
    </paper>
    <paper id="8">
      <title><fixed-case>RITT</fixed-case>: A Retrieval-Assisted Framework with Image and Text Table Representations for Table Question Answering</title>
      <author><first>Wei</first><last>Zhou</last></author>
      <author><first>Mohsen</first><last>Mesgar</last><affiliation>Bosch</affiliation></author>
      <author><first>Heike</first><last>Adel</last><affiliation>Hochschule der Medien (University of Applied Sciences)</affiliation></author>
      <author><first>Annemarie</first><last>Friedrich</last><affiliation>University of Augsburg</affiliation></author>
      <pages>86-97</pages>
      <abstract>Tables can be represented either as text or as images. Previous works on table question answering (TQA) typically rely on only one representation, neglecting the potential benefits of combining both. In this work, we explore integrating textual and visual table representations using multi-modal large language models (MLLMs) for TQA. Specifically, we propose RITT, a retrieval-assisted framework that first identifies the most relevant part of a table for a given question, then dynamically selects the optimal table representations based on the question type. Experiments demonstrate that our framework significantly outperforms the baseline MLLMs by an average of 13 Exact Match and surpasses two text-only state-of-the-art TQA methods on four TQA benchmarks, highlighting the benefits of leveraging both textual and visual table representations.</abstract>
      <url hash="eaed29ce">2025.trl-1.8</url>
      <bibkey>zhou-etal-2025-ritt</bibkey>
      <doi>10.18653/v1/2025.trl-1.8</doi>
    </paper>
    <paper id="9">
      <title>Ask Me Like <fixed-case>I</fixed-case>’m Human: <fixed-case>LLM</fixed-case>-based Evaluation with For-Human Instructions Correlates Better with Human Evaluations than Human Judges</title>
      <author><first>Rudali</first><last>Huidrom</last></author>
      <author><first>Anya</first><last>Belz</last><affiliation>Dublin City University</affiliation></author>
      <pages>98-108</pages>
      <abstract>Human evaluation in NLP has high cost and expertise requirements, and instruction-tuned LLMs are increasingly seen as a viable alternative. Reported correlations with human judgements vary across evaluation contexts and prompt types, and it is hard currently to predict if an LLM-as-judge metric will work equally well for new evaluation contexts and prompts, unless human evaluations are also carried out for comparison. Addressing two main factors contributing to this uncertainty, model suitability and prompt engineering, in the work reported in this focused contribution, we test four LLMs and different ways of combining them, in conjunction with a standard approach to prompt formulation, namely using written-for-human instructions verbatim. We meta-evaluate performance against human evaluations on two data-to-text tasks, and eight evaluation measures, also comparing against more conventional LLM prompt formulations. We find that the best LLM (combination)s are excellent predictors of mean human judgements, and are particularly good at content-related evaluation (in contrast to form-related criteria such as Fluency). Moreover, the best LLMs correlate far more strongly with human evaluations than individual human judges across all scenarios.</abstract>
      <url hash="ec9effd6">2025.trl-1.9</url>
      <bibkey>huidrom-belz-2025-ask</bibkey>
      <doi>10.18653/v1/2025.trl-1.9</doi>
    </paper>
    <paper id="10">
      <title>Table Understanding and (Multimodal) <fixed-case>LLM</fixed-case>s: A Cross-Domain Case Study on Scientific vs. Non-Scientific Data</title>
      <author id="ekaterina-borisova-aarhus"><first>Ekaterina</first><last>Borisova</last><affiliation>Technische Universität Berlin and German Research Center for AI</affiliation></author>
      <author><first>Fabio</first><last>Barth</last><affiliation>German Research Center for AI</affiliation></author>
      <author><first>Nils</first><last>Feldhus</last></author>
      <author><first>Raia</first><last>Abu Ahmad</last><affiliation>German Research Center for AI</affiliation></author>
      <author><first>Malte</first><last>Ostendorff</last><affiliation>Deutsche Telekom</affiliation></author>
      <author><first>Pedro</first><last>Ortiz Suarez</last><affiliation>Common Crawl Foundation</affiliation></author>
      <author><first>Georg</first><last>Rehm</last><affiliation>Humboldt-Universität zu Berlin and Deutsches Forschungszentrum für Künstliche Intelligenz</affiliation></author>
      <author><first>Sebastian</first><last>Möller</last></author>
      <pages>109-142</pages>
      <abstract>Tables are among the most widely used tools for representing structured data in research, business, medicine, and education. Although LLMs demonstrate strong performance in downstream tasks, their efficiency in processing tabular data remains underexplored. In this paper, we investigate the effectiveness of both text-based and multimodal LLMs on table understanding tasks through a cross-domain and cross-modality evaluation. Specifically, we compare their performance on tables from scientific vs. non-scientific contexts and examine their robustness on tables represented as images vs. text. Additionally, we conduct an interpretability analysis to measure context usage and input relevance. We also introduce the TableEval benchmark, comprising 3017 tables from scholarly publications, Wikipedia, and financial reports, where each table is provided in five different formats: Image, Dictionary, HTML, XML, and LaTeX. Our findings indicate that while LLMs maintain robustness across table modalities, they face significant challenges when processing scientific tables.</abstract>
      <url hash="65b351c4">2025.trl-1.10</url>
      <bibkey>borisova-etal-2025-table</bibkey>
      <doi>10.18653/v1/2025.trl-1.10</doi>
    </paper>
    <paper id="11">
      <title>Perspective: Leveraging Domain Knowledge for Tabular Machine Learning in the Medical Domain</title>
      <author><first>Arijana</first><last>Bohr</last></author>
      <author><first>Thomas</first><last>Altstidl</last><affiliation>Friedrich-Alexander Universität Erlangen-Nürnberg</affiliation></author>
      <author><first>Bjoern</first><last>Eskofier</last><affiliation>Friedrich-Alexander-Universität Erlangen-Nürnberg</affiliation></author>
      <author><first>Emmanuelle</first><last>Salin</last><affiliation>Friedrich-Alexander Universität Erlangen-Nürnberg</affiliation></author>
      <pages>143-155</pages>
      <abstract>There has been limited exploration of how to effectively integrate domain knowledge into machine learning for medical tabular data. Traditional approaches often rely on non-generalizable processes tailored to specific datasets. In contrast, recent advances in deep learning for language and tabular data are leading the way toward more generalizable and scalable methods of domain knowledge inclusion. In this paper, we first explore the need for domain knowledge in medical tabular data, categorize types of medical domain knowledge, and discuss how each can be leveraged in tabular machine learning. We then outline strategies for integrating this knowledge at various stages of the machine learning pipeline. Finally, building on recent advances in tabular deep learning, we propose future research directions to support the integration of domain knowledge.</abstract>
      <url hash="2e76a585">2025.trl-1.11</url>
      <bibkey>bohr-etal-2025-perspective</bibkey>
      <doi>10.18653/v1/2025.trl-1.11</doi>
    </paper>
    <paper id="12">
      <title><fixed-case>LLM</fixed-case>-Mixer: Multiscale Mixing in <fixed-case>LLM</fixed-case>s for Time Series Forecasting</title>
      <author><first>Md</first><last>Kowsher</last><affiliation>University of Central Florida</affiliation></author>
      <author><first>Md. Shohanur Islam</first><last>Sobuj</last></author>
      <author><first>Nusrat Jahan</first><last>Prottasha</last></author>
      <author><first>E. Alejandro</first><last>Alanis</last><affiliation>Microsoft</affiliation></author>
      <author><first>Ozlem</first><last>Garibay</last><affiliation>University of Central Florida</affiliation></author>
      <author><first>Niloofar</first><last>Yousefi</last><affiliation>University of Central Florida</affiliation></author>
      <pages>156-165</pages>
      <abstract>Time series forecasting is a challenging task, especially when dealing with data that contains both short-term variations and long-term trends. In this study, we introduce LLM-Mixer, a novel framework that combines multiscale time-series decomposition with the power of pre-trained Large Language Models (LLMs). LLM-Mixer breaks down time-series data into multiple temporal resolutions using downsampling and processes these multiscale representations with a frozen LLM, guided by a carefully designed text prompt that encodes information about the dataset’s features and structure. To understand the role of downsampling, we conduct a detailed analysis using Neural Tangent Kernel (NTK) distance, showing that incorporating multiple scales improves the model’s learning dynamics.We evaluate LLM-Mixer across a diverse set of forecasting tasks, including long-term multivariate, short-term multivariate, and long-term univariate scenarios. Experimental results demonstrate that LLM-Mixer achieves competitive performance compared to recent state-of-the-art models across various forecasting horizons.</abstract>
      <url hash="1acefeaa">2025.trl-1.12</url>
      <bibkey>kowsher-etal-2025-llm</bibkey>
      <doi>10.18653/v1/2025.trl-1.12</doi>
    </paper>
    <paper id="13">
      <title><fixed-case>T</fixed-case>able<fixed-case>KV</fixed-case>: <fixed-case>KV</fixed-case> Cache Compression for In-Context Table Processing</title>
      <author><first>Giulio</first><last>Corallo</last></author>
      <author><first>Elia</first><last>Faure-Rolland</last></author>
      <author><first>Miriam</first><last>Lamari</last></author>
      <author><first>Paolo</first><last>Papotti</last><affiliation>Eurecom</affiliation></author>
      <pages>166-171</pages>
      <abstract>Processing large tables provided in-context to LLMs is challenging due to token limits and information overload. While Retrieval-Augmented Generation can select relevant subsets externally, this work explores Key-Value (KV) cache compression as an alternative, applied directly to the linearized table during inference. We show that the LLM’s internal attention scores over the table context guides the retention of essential KV pairs, effectively compressing the processing context while preserving crucial relational information needed for complex queries. Experiments on Spider, WikitableQA, and QTSumm datasets validate the compression approach for in-context table processing, offering a promising path for improved table representation learning in LLMs.</abstract>
      <url hash="6f4f9417">2025.trl-1.13</url>
      <bibkey>corallo-etal-2025-tablekv</bibkey>
      <doi>10.18653/v1/2025.trl-1.13</doi>
    </paper>
    <paper id="14">
      <title><fixed-case>O</fixed-case>r<fixed-case>QA</fixed-case> – Open Data Retrieval for Question Answering dataset generation</title>
      <author><first>Giovanni</first><last>Malaguti</last></author>
      <author><first>Angelo</first><last>Mozzillo</last></author>
      <author><first>Giovanni</first><last>Simonini</last><affiliation>Università degli Studi di Modena e Reggio Emilia</affiliation></author>
      <pages>172-181</pages>
      <abstract>We present OrQA, a novel agentic framework to generate large-scale tabular question-answering (TQA) datasets based on real-world open data.Such datasets are needed to overcome the limitations of existing benchmark datasets, which rely on synthetic questions or limited web tables.OrQA employs LLM agents to retrieve related open data tables, generate natural questions, and synthesize executable <tex-math>\texttt{SQL}</tex-math> queries—involving joins, unions, and other non-trivial operations.By leveraging hundreds of GPU hours on four NVIDIA A100, we applied OrQA to Canadian and UK government open data to produce 1,000 question-tables–SQL triples, a representative sample of which has been human‐validated.This open‐source dataset is now publicly available to drive transparency, reproducibility, and progress in table‐based question answering.</abstract>
      <url hash="c1a8101d">2025.trl-1.14</url>
      <bibkey>malaguti-etal-2025-orqa</bibkey>
      <doi>10.18653/v1/2025.trl-1.14</doi>
    </paper>
    <paper id="15">
      <title>In-Context Learning of Soft Nearest Neighbor Classifiers for Intelligible Tabular Machine Learning</title>
      <author><first>Mykhailo</first><last>Koshil</last></author>
      <author><first>Matthias</first><last>Feurer</last></author>
      <author><first>Katharina</first><last>Eggensperger</last><affiliation>Eberhard-Karls-Universität Tübingen</affiliation></author>
      <pages>182-191</pages>
      <abstract>With in-context learning foundation models like TabPFN excelling on small supervised tabular learning tasks, it has been argued that “boosted trees are not the best default choice when working with data in tables”. However, such foundation models are inherently black-box models that do not provide interpretable predictions. We introduce a novel learning task to train ICL models to act as a nearest neighbor algorithm, which enables intelligible inference and does not decrease performance empirically.</abstract>
      <url hash="ec57cee9">2025.trl-1.15</url>
      <bibkey>koshil-etal-2025-context</bibkey>
      <doi>10.18653/v1/2025.trl-1.15</doi>
    </paper>
    <paper id="16">
      <title>Retrieval-Augmented Forecasting with Tabular Time Series Data</title>
      <author><first>Zichao</first><last>Li</last><affiliation>University of Waterloo</affiliation></author>
      <pages>192-199</pages>
      <abstract>This paper presents Retrieval-Augmented Forecasting (RAF), a novel framework for tabular time-series prediction that dynamically retrieves and integrates relevant historical table slices. RAF addresses three key limitations of existing methods: 1) schema rigidity through dynamic hashing of column metadata, 2) temporal myopia via cross-attention with learned decay, and 3) pipeline sub-optimality via end-to-end retriever-forecaster co-training. Experiments across macroeconomic (FRED-MD), financial (Yahoo Finance), and development (WorldBank) benchmarks demonstrate RAF’s superiority over six baselines, reducing sMAPE by 19.1-26.5% while maintaining robustness to schema changes (+3.2% sMAPE increase vs. +6.7-12.7% for alternatives). The architecture’s computational overhead (1.8 vs. 1.2 hours/epoch vs. TFT) is justified by significant accuracy gains in critical scenarios like market shocks (61.7% vs. 55.1% directional accuracy).</abstract>
      <url hash="bab82967">2025.trl-1.16</url>
      <bibkey>li-2025-retrieval</bibkey>
      <doi>10.18653/v1/2025.trl-1.16</doi>
    </paper>
    <paper id="17">
      <title>Resolution-Alignment-Completion of Tabular Electronic Health Records via Meta-Path Generative Sampling</title>
      <author><first>S</first><last>Mehryar</last><affiliation>University of Toronto</affiliation></author>
      <pages>200-207</pages>
      <abstract>The increasing availability of electronic health records (EHR) offers significant opportunities in data-driven healthcare, yet much of this data remains fragmented, semantically inconsistent, or incomplete. These issues are particularly evident in tabular patient records where important contextual information are lacking from the input for effective modeling. In this work, we introduce a system that performs ontology-based entity alignment to resolve and complete tabular data used in real-world clinical units. We transform patient records into a knowledge graph and capture its hidden structures through graph embeddings. We further propose a meta-path sample generation approach for completing the missing information. Our experiments demonstrate the system’s ability to augment cardiovascular disease (CVD) data for lab event detection, diagnosis prediction, and drug recommendation, enabling more robust and precise predictive models in clinical decision-making.</abstract>
      <url hash="78c49a39">2025.trl-1.17</url>
      <bibkey>mehryar-2025-resolution</bibkey>
      <doi>10.18653/v1/2025.trl-1.17</doi>
    </paper>
    <paper id="18">
      <title>Embeddings for Numerical Features Using tanh Activation</title>
      <author><first>Bingyan</first><last>Liu</last></author>
      <author><first>Charles</first><last>Elkan</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Anil N.</first><last>Hirani</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <pages>208-216</pages>
      <abstract>Recent advances in tabular deep learning have demonstrated the importance of embeddings for numerical features, where scalar values are mapped to high-dimensional spaces before being processed by the main model. Here, we propose an embedding method using the hyperbolic tangent (tanh) activation function that allows neural networks to achieve better accuracy on tabular data via an inductive bias similar to that of decision trees. To make training with the new embedding method reliable and efficient, we additionally propose a principled initialization method. Experiments demonstrate that the new approach improves upon or matches accuracy results from previously proposed embedding methods across multiple tabular datasets and model architectures.</abstract>
      <url hash="2f01efc1">2025.trl-1.18</url>
      <bibkey>liu-etal-2025-embeddings</bibkey>
      <doi>10.18653/v1/2025.trl-1.18</doi>
    </paper>
    <paper id="19">
      <title>Improving Table Retrieval with Question Generation from Partial Tables</title>
      <author><first>Hsing-Ping</first><last>Liang</last></author>
      <author><first>Che-Wei</first><last>Chang</last></author>
      <author><first>Yao-Chung</first><last>Fan</last><affiliation>National Chung Hsing University</affiliation></author>
      <pages>217-228</pages>
      <abstract>Recent advances in open-domain question answering over tables have widely adopted large language models (LLMs) under the Retriever-Reader architecture. Prior works have effectively leveraged LLMs to tackle the complex reasoning demands of the Reader component, such as text-to-text, text-to-SQL, and multi-hop reasoning. In contrast, the Retriever component has primarily focused on optimizing the query representation—training retrievers to retrieve relevant tables based on questions, or to select keywords from questions for matching table segments. However, little attention has been given to enhancing how tables themselves are represented in embedding space to better align with questions. To address this, we propose QGpT (Question Generation from Partial Tables), a simple yet effective method that uses an LLM to generate synthetic questions based on small portions of a table. These questions are generated to simulate how a user might query the content of the table currently under consideration. The generated questions are then jointly embedded with the partial table segments used for generation, enhancing semantic alignment with user queries. Without the need to embed entire tables, our method significantly improves retrieval performance across multiple benchmarks for both dense and late-interaction retrievers.</abstract>
      <url hash="36e53f94">2025.trl-1.19</url>
      <bibkey>liang-etal-2025-improving-table</bibkey>
      <doi>10.18653/v1/2025.trl-1.19</doi>
    </paper>
    <paper id="20">
      <title>Sparks of Tabular Reasoning via <fixed-case>T</fixed-case>ext2<fixed-case>SQL</fixed-case> Reinforcement Learning</title>
      <author><first>Josefa Lia</first><last>Stoisser</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Marc Boubnovski</first><last>Martell</last><affiliation>Novo Nordisk</affiliation></author>
      <author><first>Julien</first><last>Fauqueur</last><affiliation>Novo Nordisk</affiliation></author>
      <pages>229-240</pages>
      <abstract>This work reframes the Text-to-SQL task as a pathway for teaching large language models (LLMs) to reason over and manipulate tabular data—moving beyond the traditional focus on query generation. We propose a two-stage framework that leverages SQL supervision to develop transferable table reasoning capabilities. First, we synthesize detailed chain-of-thought (CoT) traces from real-world SQL queries, providing step-by-step, clause-level supervision that teaches the model how to traverse, filter, and aggregate table fields. Second, we introduce a Group Relative Policy Optimization (GRPO) reinforcement learning objective that connects SQL execution accuracy to generalizable reasoning by encouraging steps that extend beyond task-specific syntax and transfer across datasets.Empirically, our approach improves performance on standard Text-to-SQL benchmarks and achieves substantial gains on reasoning-intensive datasets such as BIRD, CRT-QA and Tablebench, demonstrating enhanced generalization and interpretability. Specifically, the distilled-quantized LLaMA-8B model achieved a 34% relative increase in exact match scores on CRT-QA when trained on Text-to-SQL tasks, while Qwen-2.5-7B achieved a 10% and Qwen-2.5-14B a 6% relative increase. These results suggest that SQL can serve not only as a target formalism but also as an effective scaffold for learning robust, transferable reasoning over structured data.</abstract>
      <url hash="440d08a1">2025.trl-1.20</url>
      <bibkey>stoisser-etal-2025-sparks</bibkey>
      <doi>10.18653/v1/2025.trl-1.20</doi>
    </paper>
    <paper id="21">
      <title>How well do <fixed-case>LLM</fixed-case>s reason over tabular data, really?</title>
      <author><first>Cornelius</first><last>Wolff</last><affiliation>Centrum voor Wiskunde en Informatica</affiliation></author>
      <author><first>Madelon</first><last>Hulsebos</last><affiliation>Centrum voor Wiskunde en Informatica</affiliation></author>
      <pages>241-250</pages>
      <abstract>Large Language Models (LLMs) excel in natural language tasks, but less is known about their reasoning capabilities over tabular data. Prior analyses devise evaluation strategies that poorly reflect an LLM’s realistic performance on tabular queries. Moreover, we have a limited understanding of the robustness of LLMs towards realistic variations in tabular inputs. Therefore, we ask: Can general-purpose LLMs reason over tabular data, really?, and focus on two questions 1) are tabular reasoning capabilities of general-purpose LLMs robust to real-world characteristics of tabular inputs, and 2) how can we realistically evaluate an LLM’s performance on analytical tabular queries?Building on a recent tabular reasoning benchmark, we first surface shortcomings of its multiple-choice prompt evaluation strategy, as well as commonly used free-form text metrics such as SacreBleu and BERT-score. We show that an LLM-as-a-judge procedure yields more reliable performance insights and unveil a significant deficit in tabular reasoning performance of LLMs. We then extend the tabular inputs reflecting three common characteristics in practice: 1) missing values, 2) duplicate entities, and 3) structural variations. Experiments show that the tabular reasoning capabilities of general-purpose LLMs suffer from these variations, stressing the importance of improving their robustness for realistic tabular inputs.</abstract>
      <url hash="89b46eed">2025.trl-1.21</url>
      <bibkey>wolff-hulsebos-2025-well</bibkey>
      <doi>10.18653/v1/2025.trl-1.21</doi>
    </paper>
  </volume>
</collection>
