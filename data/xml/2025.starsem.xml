<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.starsem">
  <volume id="1" ingest-date="2025-10-28" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 14th Joint Conference on Lexical and Computational Semantics (*SEM 2025)</booktitle>
      <editor><first>Lea</first><last>Frermann</last></editor>
      <editor><first>Mark</first><last>Stevenson</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Suzhou, China</address>
      <month>November</month>
      <year>2025</year>
      <url hash="17d3482a">2025.starsem-1</url>
      <venue>starsem</venue>
      <isbn>979-8-89176-340-1</isbn>
      <doi>10.18653/v1/2025.starsem-1</doi>
    </meta>
    <frontmatter>
      <url hash="ff568269">2025.starsem-1.0</url>
      <bibkey>sem-2025-1</bibkey>
      <doi>10.18653/v1/2025.starsem-1.0</doi>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>C</fixed-case>hengyu<fixed-case>STS</fixed-case>: An Intrinsic Perspective on <fixed-case>M</fixed-case>andarin Idiom Representation</title>
      <author id="le-qiu" orcid="0000-0002-5990-1116"><first>Le</first><last>Qiu</last></author>
      <author id="emmanuele-chersoni" orcid="0000-0001-8742-0451"><first>Emmanuele</first><last>Chersoni</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <author id="aline-villavicencio" orcid="0000-0002-3731-9168"><first>Aline</first><last>Villavicencio</last><affiliation>University of Exeter and University of Sheffield</affiliation></author>
      <pages>1-12</pages>
      <abstract>Chengyu, or four-character idioms, are ubiquitous in both spoken and written Chinese. Despite their importance, chengyu are often underexplored in NLP tasks, and existing evaluation frameworks are primarily based on extrinsic measures. In this paper, we introduce an intrinsic evaluation task for Chinese idiomatic understanding: idiomatic semantic textual similarity (iSTS), which evaluates how well models can capture the semantic similarity of sentences containing idioms. To this purpose, we present a curated dataset: ChengyuSTS. Our experiments show that current pre-trained sentence Transformer models generally fail to capture the idiomaticity of chengyu in a zero-shot setting. We then show results of fine-tuned models using the SimCSE contrastive learning framework, which demonstrate promising results for handling idiomatic expressions. We also presented the results of DeepSeek for reference</abstract>
      <url hash="ad9f3325">2025.starsem-1.1</url>
      <bibkey>qiu-etal-2025-chengyusts</bibkey>
      <doi>10.18653/v1/2025.starsem-1.1</doi>
    </paper>
    <paper id="2">
      <title>Bridging Information Gaps with Comprehensive Answers: Improving the Diversity and Informativeness of Follow-Up Questions</title>
      <author id="zhe-liu-9045" orcid="0000-0002-1904-9045"><first>Zhe</first><last>Liu</last></author>
      <author><first>Taekyu</first><last>Kang</last><affiliation>, University of British Columbia</affiliation></author>
      <author><first>Haoyu</first><last>Wang</last></author>
      <author><first>Seyed Hossein</first><last>Alavi</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Vered</first><last>Shwartz</last></author>
      <pages>13-30</pages>
      <abstract>Generating diverse follow-up questions that uncover missing information remains challenging for conversational agents, particularly when they run on small, locally hosted models. To tackle this problem, we develop an information-gap–driven pipeline that contrasts the initial answer with an LLM-generated comprehensive answer, identifies the information gaps, and formulates gap-bridging follow-up questions. Applying the pipeline, we augment an existing dataset–FollowupQG–tenfold. Experiments show that models fine-tuned on the augmented dataset achieve significantly higher informativeness and diversity than variations trained on the original dataset. These findings indicate that our pipeline, which mirrors the human cognitive process of information seeking, provides an efficient distillation channel from state-of-the-art LLMs to smaller models, enabling resource-constrained conversational systems to generate more diverse and informative follow-up questions.</abstract>
      <url hash="adbf6939">2025.starsem-1.2</url>
      <bibkey>liu-etal-2025-bridging</bibkey>
      <doi>10.18653/v1/2025.starsem-1.2</doi>
    </paper>
    <paper id="3">
      <title>Injecting Frame Semantics into Large Language Models via Prompt-Based Fine-Tuning</title>
      <author><first>Shahid Iqbal</first><last>Rai</last></author>
      <author id="danilo-croce" orcid="0000-0001-9111-1950"><first>Danilo</first><last>Croce</last></author>
      <author id="roberto-basili" orcid="0000-0001-5140-0694"><first>Roberto</first><last>Basili</last><affiliation>University of Roma, Tor Vergata</affiliation></author>
      <pages>31-47</pages>
      <abstract>Large Language Models (LLMs) have demonstrated remarkable generalization across diverse NLP tasks, yet they often produce outputs lacking semantic coherence due to insufficient grounding in structured linguistic knowledge. This paper proposes a novel method for injecting Frame Semantics into a pretrained LLaMA model using Low-Rank Adaptation (LoRA). Leveraging FrameNet (a rich resource of over 1,000 semantic frames) we construct a training corpus comprising structured triples of frame definitions, frame elements, and lexical units. Our method encodes these examples into the model via LoRA adapters and evaluates performance using zero-shot prompting for textual entailment and semantic role labeling (SRL) over Framenet. Experimental results show that our adapted frame-aware LLM substantially outperforms the baseline across closed, open-ended, and multiple-choice prompts. Moreover, we observe significant improvements in SRL accuracy, demonstrating the efficacy of combining frame-semantic theory with parameter-efficient pretraining.</abstract>
      <url hash="ecf9fbcb">2025.starsem-1.3</url>
      <bibkey>rai-etal-2025-injecting</bibkey>
      <doi>10.18653/v1/2025.starsem-1.3</doi>
    </paper>
    <paper id="4">
      <title>From Complex Word Identification to Substitution: Instruction-Tuned Language Models for Lexical Simplification</title>
      <author><first>Tonghui</first><last>Han</last><affiliation>Binzhou Polytechnic</affiliation></author>
      <author id="xinru-zhang" orcid="0009-0008-7762-7473"><first>Xinru</first><last>Zhang</last></author>
      <author id="yaxin-bi" orcid="0000-0002-0979-4084"><first>Yaxin</first><last>Bi</last></author>
      <author id="maurice-d-mulvenna" orcid="0000-0002-1554-0785"><first>Maurice D.</first><last>Mulvenna</last><affiliation>Ulster University</affiliation></author>
      <author id="dongqiang-yang" orcid="0000-0002-3053-6610"><first>Dongqiang</first><last>Yang</last><affiliation>Shandong Jianzhu University</affiliation></author>
      <pages>48-58</pages>
      <abstract>Lexical-level sentence simplification is essential for improving text accessibility, yet traditional methods often struggle to dynamically identify complex terms and generate contextually appropriate substitutions, resulting in limited generalization. While prompt-based approaches with large language models (LLMs) have shown strong performance and adaptability, they often lack interpretability and are prone to hallucinating. This study proposes a fine-tuning approach for mid-sized LLMs to emulate the lexical simplification pipeline. We transform complex word identification datasets into an instruction–response format to support instruction tuning. Experimental results show that our method substantially enhances complex word identification accuracy with reduced hallucinations while achieving competitive performance on lexical simplification benchmarks. Furthermore, we find that integrating fine-tuning with prompt engineering reduces dependency on manual prompt optimization, leading to a more efficient simplification framework.</abstract>
      <url hash="70bae2cc">2025.starsem-1.4</url>
      <bibkey>han-etal-2025-complex</bibkey>
      <doi>10.18653/v1/2025.starsem-1.4</doi>
    </paper>
    <paper id="5">
      <title>Semantic Prosody in Machine Translation: the <fixed-case>E</fixed-case>nglish-<fixed-case>C</fixed-case>hinese Case of Passive Structures</title>
      <author><first>Xinyue</first><last>Ma</last><affiliation>Universitat de Barcelona</affiliation></author>
      <author id="pol-pastells" orcid="0000-0003-1302-1372"><first>Pol</first><last>Pastells</last></author>
      <author id="mariona-taule-delor" orcid="0000-0003-0089-940X"><first>Mariona Taulé</first><last>Delor</last><affiliation>Universitat de Barcelona</affiliation></author>
      <author id="mireia-farrus" orcid="0000-0002-7160-9513"><first>Mireia</first><last>Farrús</last><affiliation>Universitat de Barcelona</affiliation></author>
      <pages>59-69</pages>
      <abstract>Semantic prosody is a collocational meaning formed through the co-occurrence of a linguistic unit and a consistent series of collocates, which should be treated separately from semantic meaning. Since words that are literal translation of each other may have different semantic prosody, more attention should be paid to this linguistic property in order to generate accurate translation. However, current machine translation models cannot handle this problem. To bridge the gap, we propose an approach to teach machine translation models about semantic prosody of a specific structure. We focus on Chinese BEI passives and create a dataset of English-Chinese sentence pairs with the purpose of demonstrating the negative semantic prosody of BEI passives. Then we fine-tune OPUS-MT, NLLB-600M and mBART50-mmt models with our dataset for the English-Chinese translation task. Our results show that fine-tuned MT models perform better on using BEI passives for translating unfavourable content and avoid using it for neutral and favourable content. Also, in NLLB-600M, which is a multilingual model, this knowledge of semantic prosody can be transferred from English-Chinese translation to other language pairs, such as Spanish-Chinese.</abstract>
      <url hash="a423ba79">2025.starsem-1.5</url>
      <bibkey>ma-etal-2025-semantic</bibkey>
      <doi>10.18653/v1/2025.starsem-1.5</doi>
    </paper>
    <paper id="6">
      <title>Semantic Differentiation in Speech Emotion Recognition: Insights from Descriptive and Expressive Speech Roles</title>
      <author><first>Rongchen</first><last>Guo</last></author>
      <author><first>Vincent</first><last>Francoeur</last><affiliation>University of Ottawa</affiliation></author>
      <author><first>Isar</first><last>Nejadgholi</last><affiliation>National Research Council Canada and University of Ottawa</affiliation></author>
      <author id="sylvain-gagnon" orcid="0000-0003-4944-7519"><first>Sylvain</first><last>Gagnon</last></author>
      <author><first>Miodrag</first><last>Bolic</last><affiliation>University of Ottawa</affiliation></author>
      <pages>70-82</pages>
      <abstract>Speech Emotion Recognition (SER) is essential for improving human-computer interaction, yet its accuracy remains constrained by the complexity of emotional nuances in speech. In this study, we distinguish between <tex-math>descriptive\ semantics</tex-math>, which represents the contextual content of speech, and <tex-math>expressive\ semantics</tex-math>, which reflects the speaker’s emotional state. After watching emotionally charged movie segments, we recorded audio clips of participants describing their experiences, along with the intended emotion tags for each clip, participants’ self-rated emotional responses, and their valence/arousal scores. Through experiments we show that descriptive semantics align with intended emotions, while expressive semantics correlate with evoked emotions. Our findings inform SER applications in human-AI interaction and pave the way for more context-aware AI systems.</abstract>
      <url hash="6b5c795d">2025.starsem-1.6</url>
      <bibkey>guo-etal-2025-semantic</bibkey>
      <doi>10.18653/v1/2025.starsem-1.6</doi>
    </paper>
    <paper id="7">
      <title>Generalizability of Media Frames: Corpus creation and analysis across countries</title>
      <author id="agnese-daffara" orcid="0009-0009-6706-0095"><first>Agnese</first><last>Daffara</last></author>
      <author><first>Sourabh</first><last>Dattawad</last></author>
      <author id="sebastian-pado"><first>Sebastian</first><last>Padó</last><affiliation>University of Stuttgart, Universität Stuttgart</affiliation></author>
      <author id="tanise-ceron" orcid="0009-0002-4845-2789"><first>Tanise</first><last>Ceron</last></author>
      <pages>83-99</pages>
      <abstract>Frames capture aspects of an issue that are emphasized in a debate by interlocutors and can help us understand how political language conveys different perspectives and ultimately shapes people’s opinions. The Media Frame Corpus (MFC) is the most commonly used framework with categories and detailed guidelines for operationalizing frames. It is, however, focused on a few salient U.S. news issues, making it unclear how well these frames can capture news issues in other cultural contexts. To explore this, we introduce <tex-math>\texttt{FrameNews-PT}</tex-math>, a dataset of Brazilian Portuguese news articles covering political and economic news and annotate it within the MFC framework.Through several annotation rounds, we evaluate the extent to which MFC frames generalize to the Brazilian debate issues. We further evaluate how fine-tuned and zero-shot models perform on out-of-domain data.Results show that the 15 MFC frames remain broadly applicable with minor revisions of the guidelines. However, some MFC frames are rarely used, and novel news issues are analyzed using general ‘fallback’ frames. We conclude that cross-cultural frame use requires careful consideration.</abstract>
      <url hash="a3dbd786">2025.starsem-1.7</url>
      <bibkey>daffara-etal-2025-generalizability</bibkey>
      <doi>10.18653/v1/2025.starsem-1.7</doi>
    </paper>
    <paper id="8">
      <title>Cross-Lingual Extractive Question Answering with Unanswerable Questions</title>
      <author><first>Yuval</first><last>Gorodissky</last><affiliation>Ben Gurion University of the Negev</affiliation></author>
      <author id="elior-sulem" orcid="0000-0002-9859-7313"><first>Elior</first><last>Sulem</last><affiliation>Ben-Gurion University of the Negev</affiliation></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>100-121</pages>
      <abstract>Cross-lingual Extractive Question Answering (EQA) extends standard EQA by requiring models to find answers in passages written in languages different from the questions. The Generalized Cross-Lingual Transfer (G-XLT) task evaluates models’ zero-shot ability to transfer question answering capabilities across languages using only English training data. While previous research has primarily focused on scenarios where answers are always present, real-world applications often encounter situations where no answer exists within the given context. This paper introduces an enhanced G-XLT task definition that explicitly handles unanswerable questions, bridging a critical gap in current research. To address this challenge, we present two new datasets: miXQuAD and MLQA-IDK, which address both answerable and unanswerable questions and respectively cover 12 and 7 language pairs. Our study evaluates state-of-the-art large language models using fine-tuning, parameter-efficient techniques, and in-context learning approaches, revealing interesting trade-offs between a smaller fine-tuned model’s performance on answerable questions versus a larger in-context learning model’s capability on unanswerable questions. We also examine language similarity patterns based on model performance, finding alignments with known language families.</abstract>
      <url hash="1a0a7138">2025.starsem-1.8</url>
      <bibkey>gorodissky-etal-2025-cross</bibkey>
      <doi>10.18653/v1/2025.starsem-1.8</doi>
    </paper>
    <paper id="9">
      <title>Evaluating Compositional Generalisation in <fixed-case>VLM</fixed-case>s and Diffusion Models</title>
      <author><first>Beth</first><last>Pearson</last><affiliation>University of Bristol</affiliation></author>
      <author><first>Bilal</first><last>Boulbarss</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Michael</first><last>Wray</last><affiliation>University of Bristol</affiliation></author>
      <author id="martha-lewis" orcid="0000-0002-9951-9413"><first>Martha</first><last>Lewis</last><affiliation>University of Amsterdam</affiliation></author>
      <pages>122-133</pages>
      <abstract>A fundamental aspect of the semantics of natural language is that novel meanings can be formed from the composition of previously known parts.Vision-language models (VLMs) have made significant progress in recent years, however, there is evidence that they are unable to perform this kind of composition. For example, given an image of a red cube and a blue cylinder, a VLM such as CLIP is likely to incorrectly label the image as a red cylinder or a blue cube, indicating it represents the image as a ‘bag-of-words’ and fails to capture compositional semantics. Diffusion models have recently gained significant attention for their impressive generative abilities, and zero-shot classifiers based on diffusion models have been shown to perform competitively with CLIP in certain compositional tasks. We explore whether the generative Diffusion Classifier has improved compositional generalisation abilities compared to discriminative models. We assess three models—Diffusion Classifier, CLIP, and ViLT—on their ability to bind objects with attributes and relations in both zero-shot learning (ZSL) and generalised zero-shot learning (GZSL) settings. Our results show that the Diffusion Classifier and ViLT perform well at concept binding tasks, but that all models struggle significantly with the relational GZSL task, underscoring the broader challenges VLMs face with relational reasoning. Analysis of CLIP embeddings suggests that the difficulty may stem from overly similar representations of relational concepts such as left and right. Code and dataset are available at [link redacted for anonymity].</abstract>
      <url hash="dd8e9400">2025.starsem-1.9</url>
      <bibkey>pearson-etal-2025-evaluating</bibkey>
      <doi>10.18653/v1/2025.starsem-1.9</doi>
    </paper>
    <paper id="10">
      <title>On the Distinctive Co-occurrence Characteristics of Antonymy</title>
      <author><first>Zhihan</first><last>Cao</last><affiliation>Institute of Science Tokyo</affiliation></author>
      <author id="hiroaki-yamada" orcid="0000-0002-1963-958X"><first>Hiroaki</first><last>Yamada</last><affiliation>Institute of Science Tokyo</affiliation></author>
      <author id="takenobu-tokunaga" orcid="0000-0002-1399-9517"><first>Takenobu</first><last>Tokunaga</last><affiliation>Tokyo Institute of Technology, Tokyo Institute of Technology</affiliation></author>
      <pages>134-141</pages>
      <abstract>Antonymy has long received particular attention in lexical semantics.Previous studies have shown that antonym pairs frequently co-occur in text, across genres and parts of speech, more often than would be expected by chance. However, whether this co-occurrence pattern is distinctive of antonymy remains unclear, due to a lack of comparison with other semantic relations. This work fills the gap by comparing antonymy with three other relations across parts of speech using robust co-occurrence metrics. We find that antonymy is distinctive in three respects: antonym pairs co-occur with high strength, in a preferred linear order, and within short spans. All results are available online.</abstract>
      <url hash="20724774">2025.starsem-1.10</url>
      <bibkey>cao-etal-2025-distinctive</bibkey>
      <doi>10.18653/v1/2025.starsem-1.10</doi>
    </paper>
    <paper id="11">
      <title>Evaluating Textual and Visual Semantic Neighborhoods of Abstract and Concrete Concepts</title>
      <author><first>Sven</first><last>Naber</last><affiliation>Universität Stuttgart</affiliation></author>
      <author id="diego-frassinelli" orcid="0000-0002-1517-2185"><first>Diego</first><last>Frassinelli</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author id="sabine-schulte-im-walde" orcid="0000-0002-8975-6255"><first>Sabine</first><last>Schulte Im Walde</last><affiliation>University of Stuttgart</affiliation></author>
      <pages>142-151</pages>
      <abstract>This paper presents a systematic evaluation of nearest neighbors across semantic representation spaces in both textual and visual modalities. We focus on nominal concepts with varying concreteness levels, and apply a neighborhood overlap measure to compare these target concepts differing in their linguistic and perceptual nature. We find that alignment is primarily determined by modality, and additionally by level of concreteness: Models from the same modality show stronger alignment than cross-modal models, and spaces of concrete concepts show stronger alignment than those of abstract ones. Overall, larger neighborhood size strengthens the alignment between spaces.</abstract>
      <url hash="316676d1">2025.starsem-1.11</url>
      <bibkey>naber-etal-2025-evaluating</bibkey>
      <doi>10.18653/v1/2025.starsem-1.11</doi>
    </paper>
    <paper id="12">
      <title><fixed-case>A</fixed-case>mbi<fixed-case>S</fixed-case>tory: A Challenging Dataset of Lexically Ambiguous Short Stories</title>
      <author><first>Janosch</first><last>Gehring</last></author>
      <author><first>Michael</first><last>Roth</last><affiliation>University of Technology Nuremberg</affiliation></author>
      <pages>152-171</pages>
      <abstract>Word sense disambiguation is the task of selecting a word’s applicable word sense in a given context. However, ambiguous texts may lack the information necessary to disambiguate words completely, resulting in multiple word senses with varying degrees of plausibility. We design a dataset around this premise: Our samples consist of 4–5 sentence short stories, where the sentence with the word to be disambiguated is itself ambiguous and surrounding sentences only contain indirect clues towards the more plausible word sense. We collect annotations from humans who rate the plausibility of a given word sense on a scale from 1–5. In total, our dataset contains 19,701 human word sense annotations on 1,899 stories. We investigate the performance of large language models on our data and find that many poorly correlate with human judgments. We also find that fine-tuning on our data can increase performance.</abstract>
      <url hash="93395076">2025.starsem-1.12</url>
      <bibkey>gehring-roth-2025-ambistory</bibkey>
      <doi>10.18653/v1/2025.starsem-1.12</doi>
    </paper>
    <paper id="13">
      <title><fixed-case>W</fixed-case>i<fixed-case>C</fixed-case> Evaluation in <fixed-case>G</fixed-case>alician and <fixed-case>S</fixed-case>panish: Effects of Dataset Quality and Composition</title>
      <author id="marta-vazquez-abuin" orcid="0000-0002-2134-9493"><first>Marta Vázquez</first><last>Abuín</last></author>
      <author id="marcos-garcia" orcid="0000-0002-6557-0210"><first>Marcos</first><last>Garcia</last><affiliation>Universidade de Santiago de Compostela</affiliation></author>
      <pages>172-178</pages>
      <abstract>This work explores the impact of dataset quality and composition on Word-in-Context performance for Galician and Spanish. We assess existing datasets, validate their test sets, and create new manually constructed evaluation data. Across five experiments with controlled variations in training and test data, we find that while the validation of test data tends to yield better model performance, evaluations on manually created datasets suggest that contextual embeddings are not sufficient on their own to reliably capture word meaning variation. Regarding training data, our results suggest that performance is influenced not only by size and human validation but also by deeper factors related to the semantic properties of the datasets. All new resources will be freely released.</abstract>
      <url hash="903768ba">2025.starsem-1.13</url>
      <bibkey>abuin-garcia-2025-wic</bibkey>
      <doi>10.18653/v1/2025.starsem-1.13</doi>
    </paper>
    <paper id="14">
      <title>Math Natural Language Inference: this should be easy!</title>
      <author id="valeria-de-paiva" orcid="0000-0002-1078-6970"><first>Valeria</first><last>de Paiva</last><affiliation>Topos Institute and School of Computer Science, University of Birmingham, UK</affiliation></author>
      <author><first>Qiyue</first><last>Gao</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Hai</first><last>Hu</last></author>
      <author id="pavel-kovalev" orcid="0009-0003-7609-7590"><first>Pavel</first><last>Kovalev</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Yikang</first><last>Liu</last></author>
      <author id="lawrence-s-moss" orcid="0000-0002-9908-5774"><first>Lawrence S.</first><last>Moss</last><affiliation>Indiana University at Bloomington</affiliation></author>
      <author id="zhiheng-qian" orcid="0009-0004-0351-2279"><first>Zhiheng</first><last>Qian</last></author>
      <pages>179-188</pages>
      <abstract>We ask whether contemporary LLMs are able to perform natural language inference (NLI) tasks on mathematical texts. We call this the Math NLI problem. We construct a corpus of Math NLI pairs whose premises are from extant mathematical text and whose hypotheses and gold labels were provided by people with experience in both research-level mathematics and also in the NLI field. We also investigate the quality of corpora using the same premises but whose hypotheses are provided by LLMs themselves. We not only investigate the performance but also the inter-group consistency of the diverse group of LLMs. We have both positive and negative findings. Among our positive findings: in some settings, using a majority vote of LLMs is approximately equivalent to using human-labeled data in the Math NLI area. On the negative side: LLMs still struggle with mathematical language. They occasionally fail at even basic inferences. Current models are not as prone to hypothesis-only “inference” in our data the way the previous generation had been. In addition to our findings, we also provide our corpora as data to support future work on Math NLI.</abstract>
      <url hash="8aa9d766">2025.starsem-1.14</url>
      <bibkey>de-paiva-etal-2025-math</bibkey>
      <doi>10.18653/v1/2025.starsem-1.14</doi>
    </paper>
    <paper id="15">
      <title>All Entities are Not Created Equal: Examining the Long Tail for Ultra-Fine Entity Typing</title>
      <author><first>Advait</first><last>Deshmukh</last></author>
      <author><first>Ashwin</first><last>Umadi</last></author>
      <author><first>Dananjay</first><last>Srinivas</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <author id="maria-leonor-pacheco"><first>Maria Leonor</first><last>Pacheco</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <pages>189-201</pages>
      <abstract>Due to their capacity to acquire world knowledge from large corpora, pre-trained language models (PLMs) are extensively used in ultra-fine entity typing tasks where the space of labels is extremely large. In this work, we explore the limitations of the knowledge acquired by PLMs by proposing a novel heuristic to approximate the pre-training distribution of entities when the pre-training data is unknown. Then, we systematically demonstrate that entity-typing approaches that rely solely on the parametric knowledge of PLMs struggle significantly with entities at the long tail of the pre-training distribution, and that knowledge-infused approaches can account for some of these shortcomings. Our findings suggest that we need to go beyond PLMs to produce solutions that perform well for infrequent entities.</abstract>
      <url hash="c024d06b">2025.starsem-1.15</url>
      <bibkey>deshmukh-etal-2025-entities</bibkey>
      <doi>10.18653/v1/2025.starsem-1.15</doi>
    </paper>
    <paper id="16">
      <title>When Does Meaning Backfire? Investigating the Role of <fixed-case>AMR</fixed-case>s in <fixed-case>NLI</fixed-case></title>
      <author id="junghyun-min" orcid="0000-0002-1778-2163"><first>Junghyun</first><last>Min</last><affiliation>Georgetown University</affiliation></author>
      <author id="xiulin-yang" orcid="0009-0005-4647-2235"><first>Xiulin</first><last>Yang</last></author>
      <author id="shira-wein" orcid="0000-0002-1062-0866"><first>Shira</first><last>Wein</last><affiliation>Amherst College</affiliation></author>
      <pages>202-211</pages>
      <abstract>Natural Language Inference (NLI) relies heavily on adequately parsing the semantic content of the premise and hypothesis.In this work, we investigate whether adding semantic information in the form of an Abstract Meaning Representation (AMR) helps pretrained language models better generalize in NLI. Our experiments integrating AMR into NLI in both fine-tuning and prompting settings show that the presence of AMR in fine-tuning hinders model generalization while prompting with AMR leads to slight gains in GPT-4o.However, an ablation study reveals that the improvement comes from amplifying surface-level differences rather than aiding semantic reasoning. This amplification can mislead models to predict non-entailment even when the core meaning is preserved.</abstract>
      <url hash="3f1fa1d2">2025.starsem-1.16</url>
      <bibkey>min-etal-2025-meaning</bibkey>
      <doi>10.18653/v1/2025.starsem-1.16</doi>
    </paper>
    <paper id="17">
      <title>Explanations explained. Influence of Free-text Explanations on <fixed-case>LLM</fixed-case>s and the Role of Implicit Knowledge</title>
      <author><first>Andrea</first><last>Zaninello</last></author>
      <author><first>Roberto</first><last>Dessi</last><affiliation>Samaya AI</affiliation></author>
      <author><first>Malvina</first><last>Nissim</last><affiliation>University of Groningen</affiliation></author>
      <author id="bernardo-magnini"><first>Bernardo</first><last>Magnini</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <pages>212-224</pages>
      <abstract>In this work, we investigate the relationship between the quality of explanations produced by different models and the amount of implicit knowledge the are able to provide beyond the input. We approximate explanation quality via accuracy on a downstream task with a standardized pipeline (GEISER) and study its correlation with three different association measures, each capturing different aspects of implicitness, defined as a combination of relevance and novelty. We conduct experiments with three SOTA LLMs on four tasks involving implicit knowledge, with explanations either confirming or contradicting the correct label. Our results demonstrate that providing quality explanations consistently improves the accuracy of LLM predictions, even when the models are not explicitly trained to take explanations as input, and underline the correlation between implicit content delivered by the explanation and its effectiveness.</abstract>
      <url hash="a18fdc2c">2025.starsem-1.17</url>
      <bibkey>zaninello-etal-2025-explanations</bibkey>
      <doi>10.18653/v1/2025.starsem-1.17</doi>
    </paper>
    <paper id="18">
      <title>Latent Traits and Cross-Task Transfer: Deconstructing Dataset Interactions in <fixed-case>LLM</fixed-case> Fine-tuning</title>
      <author><first>Shambhavi</first><last>Krishna</last><affiliation>University of Massachusetts at Amherst</affiliation></author>
      <author><first>Atharva</first><last>Naik</last></author>
      <author><first>Chaitali</first><last>Agarwal</last></author>
      <author><first>Sudharshan</first><last>Govindan</last></author>
      <author id="haw-shiuan-chang" orcid="0000-0003-4607-936X"><first>Haw-Shiuan</first><last>Chang</last><affiliation>Department of Computer Science, University of Massachusetts at Amherst</affiliation></author>
      <author><first>Taesung</first><last>Lee</last></author>
      <pages>225-241</pages>
      <abstract>Large language models are increasingly deployed across diverse applications. This often includes tasks LLMs have not encountered during training.This implies that enumerating and obtaining the high-quality training data for all tasks is infeasible. Thus, we often need to rely on transfer learning using datasets with different characteristics, and anticipate out-of-distribution requests.Motivated by this practical need, we propose an analysis framework, building a transfer learning matrix and dimensionality reduction, to dissect these cross-task interactions.We train and analyze 10 models to identify latent abilities (e.g., Reasoning, Sentiment Classification, NLU, Arithmetic)and discover the side effects of the transfer learning.Our findings reveal that performance improvements often defy explanations based on surface-level dataset similarity or source data quality. Instead, hidden statistical factors of the source dataset, such as class distribution and generation length proclivities, alongside specific linguistic features, are actually more influential.This work offers insights into the complex dynamics of transfer learning, paving the way for more predictable and effective LLM adaptation.</abstract>
      <url hash="87412ec8">2025.starsem-1.18</url>
      <bibkey>krishna-etal-2025-latent</bibkey>
      <doi>10.18653/v1/2025.starsem-1.18</doi>
    </paper>
    <paper id="19">
      <title><fixed-case>LLM</fixed-case>s as annotators of argumentation</title>
      <author><first>Anna</first><last>Lindahl</last><affiliation>Göteborg University</affiliation></author>
      <pages>242-252</pages>
      <abstract>Annotated data is essential for most NLP tasks, but creating it can be time-consuming and challenging. Argumentation annotation is especially complex, often resulting in moderate human agreement. While large language models (LLMs) have excelled in increasingly complex tasks, their application to argumentation annotation has been limited. This paper investigates how well GPT-4o and Claude can annotate three types of argumentation in Swedish data compared to human annotators. Using full annotation guidelines, we evaluate the models on argumentation schemes, argumentative spans, and attitude annotation. Both models perform similarly to humans across all tasks, with Claude showing better human agreement than GPT-4o. Agreement between models is higher than human agreement in argumentation scheme and span annotation.</abstract>
      <url hash="dcd9bc37">2025.starsem-1.19</url>
      <bibkey>lindahl-2025-llms</bibkey>
      <doi>10.18653/v1/2025.starsem-1.19</doi>
    </paper>
    <paper id="20">
      <title>If We May De-Presuppose: Robustly Verifying Claims through Presupposition-Free Question Decomposition</title>
      <author id="shubhashis-roy-dipta" orcid="0000-0002-9176-1782"><first>Shubhashis</first><last>Roy Dipta</last></author>
      <author id="francis-ferraro" orcid="0000-0003-2413-9368"><first>Francis</first><last>Ferraro</last><affiliation>University of Maryland, Baltimore County</affiliation></author>
      <pages>253-266</pages>
      <abstract>Prior work has shown that presupposition in generated questions can introduce unverified assumptions, leading to inconsistencies in claim verification. Additionally, prompt sensitivity remains a significant challenge for large language models (LLMs), resulting in performance variance as high as **3–6%**. While recent advancements have reduced this gap, our study demonstrates that prompt sensitivity remains a persistent issue. To address this, we propose a structured and robust claim verification framework that reasons through presupposition-free, decomposed questions. Extensive experiments across multiple prompts, datasets, and LLMs reveal that even state-of-the-art models remain susceptible to prompt variance and presupposition. Our method consistently mitigates these issues, achieving up to a **2–5%** improvement.</abstract>
      <url hash="5e5d72ee">2025.starsem-1.20</url>
      <bibkey>roy-dipta-ferraro-2025-may</bibkey>
      <doi>10.18653/v1/2025.starsem-1.20</doi>
    </paper>
    <paper id="21">
      <title>Modeling Language Learning in Corrective Feedback Interactions</title>
      <author><first>Juan Luis</first><last>Castro-Garcia</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Parisa</first><last>Kordjamshidi</last><affiliation>Michigan State University</affiliation></author>
      <pages>267-279</pages>
      <abstract>To study computational models for language acquisition, we propose an interactive computational framework that utilizes a miniature language acquisition dataset in a controlled environment. In this framework, a neural learner model interacts with a teacher model that provides corrective feedback. Within this framework, we investigate various corrective feedback strategies, specifically focusing on reformulations and their effect on the learner model during their interactions. We design experimental settings to evaluate the learner’s production of syntactically and semantically correct linguistic utterances and perception of concepts and word-meaning associations.These results offer insights into the effectiveness of different feedback strategies in language acquisition using artificial neural networks. The outcome of this research is establishing a framework with a dataset for the systematic evaluation of various aspects of language acquisition in a controlled environment.</abstract>
      <url hash="206b622f">2025.starsem-1.21</url>
      <bibkey>castro-garcia-kordjamshidi-2025-modeling</bibkey>
      <doi>10.18653/v1/2025.starsem-1.21</doi>
    </paper>
    <paper id="22">
      <title>Relation-Aware Prompting Makes Large Language Models Effective Zero-shot Relation Extractors</title>
      <author><first>Mahdi</first><last>Rahimi</last><affiliation>Computer Science Department, University of Arizona</affiliation></author>
      <author><first>Razvan-Gabriel</first><last>Dumitru</last></author>
      <author><first>Mihai</first><last>Surdeanu</last><affiliation>University of Arizona</affiliation></author>
      <pages>280-292</pages>
      <abstract>While supervised relation extraction (RE) models have considerably advanced the state-of-the-art, they often perform poorly in low-resource settings. Zero-shot RE is vital when annotations are not available either due to costs or time constraints. As a result, zero-shot RE has garnered interest in the research community. With the advent of large language models (LLMs) many approaches have been proposed for prompting LLMs for RE, but these methods often either rely on an accompanying small language model (e.g., for finetuning on synthetic data generated by LLMs) or require complex post-prompt processing. In this paper, we propose an effective prompt-based method that does not require any additional resources. Instead, we use an LLM to perform a two-step process. In the first step, we perform a targeted summarization of the text with respect to the underlying relation, reduce the applicable label space, and synthesize examples. Then, we combine the products of these processes with other elements into a final prompt. We evaluate our approach with various LLMs on four real-world RE datasets. Our evaluation shows that our method outperforms the previous state-of-the-art zero-shot methods by a large margin. This work can also be considered as a new strong baseline for zero-shot RE that is compatible with any LLM.</abstract>
      <url hash="91133b31">2025.starsem-1.22</url>
      <bibkey>rahimi-etal-2025-relation</bibkey>
      <doi>10.18653/v1/2025.starsem-1.22</doi>
    </paper>
    <paper id="23">
      <title>Enhancing Readability-Controlled Text Modification with Readability Assessment and Target Span Prediction</title>
      <author><first>Liu</first><last>Fengkai</last></author>
      <author id="john-sie-yuen-lee" orcid="0000-0003-2505-2678"><first>John Sie Yuen</first><last>Lee</last><affiliation>City University of Hong Kong</affiliation></author>
      <pages>293-303</pages>
      <abstract>Readability-controlled text modification aims to rewrite an input text so that it reaches a target level of difficulty. This task is closely related to automatic readability assessment (ARA) since, depending on the difficulty level of the input text, it may need to be simplified or complexified. Most previous research in LLM-based text modification has focused on zero-shot prompting, without further input from ARA or guidance on text spans that most likely require revision. This paper shows that ARA models for texts and sentences, as well as predictions of text spans that should be edited, can enhance performance in readability-controlled text modification.</abstract>
      <url hash="5b2b85ee">2025.starsem-1.23</url>
      <bibkey>fengkai-lee-2025-enhancing</bibkey>
      <doi>10.18653/v1/2025.starsem-1.23</doi>
    </paper>
    <paper id="24">
      <title><fixed-case>TAG</fixed-case>–<fixed-case>EQA</fixed-case>: Text–And–Graph for Event Question Answering via Structured Prompting Strategies</title>
      <author><first>Maithili Sanjay</first><last>Kadam</last><affiliation>University of Maryland, Baltimore County</affiliation></author>
      <author id="francis-ferraro" orcid="0000-0003-2413-9368"><first>Francis</first><last>Ferraro</last><affiliation>University of Maryland, Baltimore County</affiliation></author>
      <pages>304-315</pages>
      <abstract>Large language models (LLMs) excel at general language tasks but often struggle with event-based questions—especially those requiring causal or temporal reasoning. We introduce TAG-EQA (Text-And-Graph for Event Question Answering), a prompting framework that injects causal event graphs into LLM inputs by converting structured relations into natural-language statements. TAG-EQA spans nine prompting configurations, combining three strategies (zero-shot, few-shot, chain-of-thought) with three input modalities (text-only, graph-only, text+graph), enabling a systematic analysis of when and how structured knowledge aids inference. On the TORQUESTRA benchmark, TAG-EQA improves accuracy by ~5% on average over text-only baselines, with gains up to ~12% in zero-shot settings and ~18% when graph-augmented CoT prompting is effective. While performance varies by model and configuration, our findings show that causal graphs can enhance event reasoning in LLMs without fine-tuning, offering a flexible way to encode structure in prompt-based QA.</abstract>
      <url hash="49a7e100">2025.starsem-1.24</url>
      <bibkey>kadam-ferraro-2025-tag</bibkey>
      <doi>10.18653/v1/2025.starsem-1.24</doi>
    </paper>
    <paper id="25">
      <title><fixed-case>D</fixed-case>is<fixed-case>C</fixed-case>o<fixed-case>CLIP</fixed-case>: A Distributional Compositional Tensor Network Encoder for Vision-Language Understanding</title>
      <author id="kin-ian-lo" orcid="0000-0002-1300-396X"><first>Kin Ian</first><last>Lo</last><affiliation>University College London, University of London</affiliation></author>
      <author><first>Hala</first><last>Hawashin</last><affiliation>University College London, University of London</affiliation></author>
      <author><first>Mina</first><last>Abbaszadeh</last></author>
      <author><first>Tilen Gaetano</first><last>Limbäck-Stokin</last></author>
      <author><first>Hadi</first><last>Wazni</last></author>
      <author id="mehrnoosh-sadrzadeh"><first>Mehrnoosh</first><last>Sadrzadeh</last><affiliation>University College London</affiliation></author>
      <pages>316-327</pages>
      <abstract>Recent vision–language models excel at large-scale image–text alignment but often neglect the compositional structure of language, leading to failures on tasks that hinge on word order and predicate–argument structure. We introduce DisCoCLIP, a multimodal encoder that combines a frozen CLIP vision transformer with a novel tensor network text encoder that explicitly encodes syntactic structure. Sentences are parsed with a Combinatory Categorial Grammar parser to yield distributional word tensors whose contractions mirror the sentence’s grammatical derivation. To keep the model efficient, high-order tensors are factorized with tensor decompositions, reducing parameter count from tens of millions to under one million. Trained end-to-end with a self-supervised contrastive loss, DisCoCLIP markedly improves sensitivity to verb semantics and word order: it raises CLIP’s SVO-Probes verb accuracy from 77.6% to 82.4%, boosts ARO attribution and relation scores by over 9% and 4%, and achieves 93.7% on a newly introduced SVO-Swap benchmark. These results demonstrate that embedding explicit linguistic structure via tensor networks yields interpretable, parameter-efficient representations that substantially improve compositional reasoning in vision–language tasks.</abstract>
      <url hash="8a5f1c54">2025.starsem-1.25</url>
      <bibkey>lo-etal-2025-discoclip</bibkey>
      <doi>10.18653/v1/2025.starsem-1.25</doi>
    </paper>
    <paper id="26">
      <title><fixed-case>HSGM</fixed-case>: Hierarchical Segment-Graph Memory for Scalable Long-Text Semantics</title>
      <author><first>Dong</first><last>Liu</last><affiliation>University of California, Los Angeles and Yale University</affiliation></author>
      <author><first>Yanxuan</first><last>Yu</last></author>
      <pages>328-337</pages>
      <abstract>Semantic parsing of long documents remains challenging due to quadratic growth in pairwise composition and memory requirements. We introduce <b>Hierarchical Segment-Graph Memory (HSGM)</b>, a novel framework that decomposes an input of length <tex-math>N</tex-math> into <tex-math>M</tex-math> meaningful segments, constructs <i>Local Semantic Graphs</i> on each segment, and extracts compact <i>summary nodes</i> to form a <i>Global Graph Memory</i>. HSGM supports <i>incremental updates</i>—only newly arrived segments incur local graph construction and summary-node integration—while <i>Hierarchical Query Processing</i> locates relevant segments via top-<tex-math>K</tex-math> retrieval over summary nodes and then performs fine-grained reasoning within their local graphs.Theoretically, HSGM reduces worst-case complexity from <tex-math>O(N^2)</tex-math> to <tex-math>O\bigl(N\,k + (N/k)^2\bigr)</tex-math>,with segment size <tex-math>k \ll N</tex-math>, and we derive Frobenius-norm bounds on the approximation error introduced by node summarization and sparsification thresholds. Empirically, on three benchmarks—long-document AMR parsing, segment-level semantic role labeling (OntoNotes), and legal event extraction—HSGM achieves <i>2–4× inference speedup</i>, <i>
          <tex-math>&gt;</tex-math>60% reduction</i> in peak memory, and <i>
          <tex-math>\ge95\%</tex-math></i> of baseline accuracy. Our approach unlocks scalable, accurate semantic modeling for ultra-long texts, enabling real-time and resource-constrained NLP applications.</abstract>
      <url hash="a052109f">2025.starsem-1.26</url>
      <bibkey>liu-yu-2025-hsgm</bibkey>
      <doi>10.18653/v1/2025.starsem-1.26</doi>
    </paper>
    <paper id="27">
      <title>Knowledge Editing Induces Underconfidence in Language Models</title>
      <author><first>Ryo</first><last>Hasegawa</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Yusuke</first><last>Sakai</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author id="hidetaka-kamigaito" orcid="0000-0002-5249-5813"><first>Hidetaka</first><last>Kamigaito</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author id="taro-watanabe" orcid="0000-0001-8349-3522"><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <pages>338-347</pages>
      <abstract>As language models continue to scale, the demand for knowledge editing, a retraining-free knowledge update method, has increased. However, since knowledge editing directly alters token prediction probabilities acquired during pretraining, the probabilities may diverge from the empirical distribution. In this study, we analyze the impact of knowledge editing to compare the alignment between token prediction probabilities and task accuracy by calculating confidence calibration before and after knowledge editing. Our results reveal that, for tasks requiring semantic understanding, the range of increase in token prediction probabilities tends to be smaller than that of accuracy improvement, suggesting that knowledge editing methods lead to less confidence in prediction.</abstract>
      <url hash="41640f6e">2025.starsem-1.27</url>
      <bibkey>hasegawa-etal-2025-knowledge</bibkey>
      <doi>10.18653/v1/2025.starsem-1.27</doi>
    </paper>
    <paper id="28">
      <title>How Do Large Language Models Evaluate Lexical Complexity?</title>
      <author><first>Abdelhak</first><last>Kelious</last></author>
      <author id="matthieu-constant"><first>Mathieu</first><last>Constant</last><affiliation>Université de Lorraine, CNRS, ATILF</affiliation></author>
      <author><first>Christophe</first><last>Coeur</last></author>
      <pages>348-361</pages>
      <abstract>In this work, we explore the prediction of lexical complexity by combining supervised approaches and the use of large language models (LLMs). We first evaluate the impact of different prompting strategies (zero-shot, one-shot, and chain-of-thought) on the quality of the predictions, comparing the results with human annotations from the CompLex 2.0 corpus. Our results indicate that LLMs, and in particular gpt-4o, benefit from explicit instructions to better approximate human judgments, although some discrepancies remain. Moreover, a calibration approach to better align LLMs predictions and human judgements based on few manually annotated data appears as a promising solution to improve the reliability of the annotations in a supervised scenario.</abstract>
      <url hash="a603ba4b">2025.starsem-1.28</url>
      <bibkey>kelious-etal-2025-large</bibkey>
      <doi>10.18653/v1/2025.starsem-1.28</doi>
    </paper>
    <paper id="29">
      <title><fixed-case>SAG</fixed-case>: Enhancing Domain-Specific Information Retrieval with Semantic-Augmented Graphs</title>
      <author><first>Carol-Luca</first><last>Gasan</last></author>
      <author id="vasile-pais" orcid="0000-0002-0019-7574"><first>Vasile</first><last>Pais</last><affiliation>Research Institute for Artificial Intelligence “Mihai Draganescu”, Romanian Academy</affiliation></author>
      <pages>362-371</pages>
      <abstract>Retrieval-Augmented Generation (RAG) systems rely on high-quality embeddings to retrieve relevant context for large language models. This paper introduces the Semantic-Augmented Graph (SAG), a new architecture that improves domain-specific embeddings by capturing hierarchical semantic relationships between text segments. Inspired by human information processing, SAG organizes content from general to specific concepts using a graph-based structure. By combining static embeddings with dynamic semantic graphs, it generates context-aware representations that reflect both lexical and conceptual links. Experiments on text similarity and domain-specific question answering show that SAG consistently outperforms standard embedding methods within RAG pipelines.</abstract>
      <url hash="222d9f82">2025.starsem-1.29</url>
      <bibkey>gasan-pais-2025-sag</bibkey>
      <doi>10.18653/v1/2025.starsem-1.29</doi>
    </paper>
    <paper id="30">
      <title>Cross-Domain Persuasion Detection with Argumentative Features</title>
      <author><first>Bagyasree</first><last>Sudharsan</last></author>
      <author id="maria-leonor-pacheco"><first>Maria Leonor</first><last>Pacheco</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <pages>372-380</pages>
      <abstract>The main challenge in cross-domain persuasion detection lies in the vast differences in vocabulary observed across different outlets and contexts. Superficially, an argument made on social media will look nothing like an opinion presented in the Supreme Court, but the latent factors that make an argument persuasive are common across all settings. Regardless of domain, persuasive arguments tend to use sound reasoning and present solid evidence, build on the credibility and authority of the source, or appeal to the emotions and beliefs of the audience. In this paper, we show that simply encoding the different argumentative components and their semantic types can significantly improve a language model’s ability to detect persuasion across vastly different domains.</abstract>
      <url hash="fff318d6">2025.starsem-1.30</url>
      <bibkey>sudharsan-pacheco-2025-cross</bibkey>
      <doi>10.18653/v1/2025.starsem-1.30</doi>
    </paper>
    <paper id="31">
      <title>Hallucinated Span Detection with Multi-View Attention Features</title>
      <author><first>Yuya</first><last>Ogasa</last><affiliation>LY Corporation and Graduate School of Information Science and Technology, Osaka UniversityOsaka University</affiliation></author>
      <author id="yuki-arase"><first>Yuki</first><last>Arase</last><affiliation>Tokyo Institute of Technology, Tokyo Institute of Technology, RIKEN and AIST, National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <pages>381-394</pages>
      <abstract>This study addresses the problem of hallucinated span detection in the outputs of large language models. It has received less attention than output-level hallucination detection despite its practical importance. Prior work has shown that attentions often exhibit irregular patterns when hallucinations occur. Motivated by these findings, we extract features from the attention matrix that provide complementary views capturing (a) whether certain tokens are influential or ignored, (b) whether attention is biased toward specific subsets, and (c) whether a token is generated referring to a narrow or broad context, in the generation. These features are input to a Transformer-based classifier to conduct sequential labelling to identify hallucinated spans. Experimental results indicate that the proposed method outperforms strong baselines on hallucinated span detection with longer input contexts, such as data-to-text and summarisation tasks.</abstract>
      <url hash="45ed874e">2025.starsem-1.31</url>
      <bibkey>ogasa-arase-2025-hallucinated</bibkey>
      <doi>10.18653/v1/2025.starsem-1.31</doi>
    </paper>
    <paper id="32">
      <title><fixed-case>A</fixed-case>dv<fixed-case>ERSEM</fixed-case>: Adversarial Robustness Testing and Training of <fixed-case>LLM</fixed-case>-based Groundedness Evaluators via Semantic Structure Manipulation</title>
      <author id="kaustubh-dhole" orcid="0000-0002-8875-176X"><first>Kaustubh</first><last>Dhole</last><affiliation>Emory University</affiliation></author>
      <author><first>Ramraj</first><last>Chandradevan</last><affiliation>Amazon</affiliation></author>
      <author id="eugene-agichtein" orcid="0000-0002-3148-5448"><first>Eugene</first><last>Agichtein</last><affiliation>Emory University</affiliation></author>
      <pages>395-408</pages>
      <abstract>Evaluating outputs from large language models (LLMs) presents significant challenges, especially as hallucinations and adversarial manipulations are often difficult to detect. Existing evaluation methods lack robustness against subtle yet intentional linguistic alterations, necessitating novel techniques for reliably assessing model-generated content. Training accurate and robust groundedness evaluators is key for mitigating hallucinations and ensuring the alignment of model or human-generated claims to real-world evidence. However, as we show, many models, while optimizing for accuracy, lack robustness to subtle variations of claims, making them unsuitable and brittle in real-world settings where adversaries employ purposeful and deceitful tactics like hedging to deceive readers, which go beyond surface-level variations. To address this problem, we propose AdvERSem, a controllable adversarial approach to manipulating LLM output via Abstract Meaning Representations (AMR) to generate attack claims of multiple fine-grained types, followed by automatic verification of the correct label. By systematically manipulating a unique linguistic facet AdvERSem provides an interpretable testbed for gauging robustness as well as useful training data. We demonstrate that utilizing these AMR manipulations during training across multiple fact verification datasets helps improve the accuracy and robustness of groundedness evaluation while also minimizing the requirement of costly annotated data. To encourage further systematic evaluation, we release AdvERSem-Test, a manually verified groundedness test-bed.</abstract>
      <url hash="819b5282">2025.starsem-1.32</url>
      <bibkey>dhole-etal-2025-adversem</bibkey>
      <doi>10.18653/v1/2025.starsem-1.32</doi>
    </paper>
    <paper id="33">
      <title>Connecting Concept Layers and Rationales to Enhance Language Model Interpretability</title>
      <author><first>Thomas</first><last>Bailleux</last><affiliation>Université d’Artois</affiliation></author>
      <author id="tanmoy-mukherjee" orcid="0000-0002-9540-5398"><first>Tanmoy</first><last>Mukherjee</last><affiliation>Vrije Universiteit Brussel</affiliation></author>
      <author id="pierre-marquis" orcid="0000-0002-7979-6608"><first>Pierre</first><last>Marquis</last><affiliation>Université d’Artois</affiliation></author>
      <author id="zied-bouraoui" orcid="0000-0002-1662-4163"><first>Zied</first><last>Bouraoui</last><affiliation>CRIL Univ-Artois &amp; CNRS</affiliation></author>
      <pages>409-429</pages>
      <abstract>With the introduction of large language models, NLP has undergone a paradigm shift where these models now serve as the backbone of most developed systems. However, while highly effective, they remain opaque and difficult to interpret, which limits their adoption in critical applications that require transparency and trust. Two major approaches aim to address this: rationale extraction, which highlights input spans that justify predictions, and concept bottleneck models, which make decisions through human-interpretable concepts. Yet each has limitations. Crucially, current models lack a unified framework that connects where a model looks (rationales) with why it makes a decision (concepts). We introduce CLARITY, a model that first selects key input spans, maps them to interpretable concepts, and then predicts using only those concepts. This design supports faithful, multi-level explanations and allows users to intervene at both the rationale and concept levels. CLARITY, achieves competitive accuracy while offering improved transparency and controllability.</abstract>
      <url hash="2c91a990">2025.starsem-1.33</url>
      <bibkey>bailleux-etal-2025-connecting</bibkey>
      <doi>10.18653/v1/2025.starsem-1.33</doi>
    </paper>
    <paper id="34">
      <title>Towards Evaluation of Language Models with Skill Dimensions: A Case Study on Narrative Question Answering</title>
      <author id="emil-kalbaliyev" orcid="0000-0003-0704-0470"><first>Emil</first><last>Kalbaliyev</last></author>
      <author><first>Kairit</first><last>Sirts</last><affiliation>institute of computer science, University of Tartu</affiliation></author>
      <pages>430-440</pages>
      <abstract>Large language models have demonstrated varying levels of competence across a range of reasoning tasks, but coarse-grained evaluations often do not reflect their specific strengths and weaknesses, particularly in complex tasks such as Narrative Question Answering. In this paper, we advocate for a multi-dimensional skill-based evaluation that assesses models across distinct core skill dimensions. Our proposed skill-focused evaluation framework offers a granular and more realistic measure of model performance, revealing targeted areas for improvement and guiding future development. Experiments on Narrative Question Answering demonstrate that dimension-level analysis captures the multifaceted nature of the task and informs more effective model evaluation.</abstract>
      <url hash="23bae696">2025.starsem-1.34</url>
      <bibkey>kalbaliyev-sirts-2025-towards</bibkey>
      <doi>10.18653/v1/2025.starsem-1.34</doi>
    </paper>
    <paper id="35">
      <title>Potentially Problematic Word Usages and How to Detect Them: A Survey</title>
      <author id="aina-gari-soler"><first>Aina</first><last>Garí Soler</last><affiliation>Télécom-Paris</affiliation></author>
      <author><first>Matthieu</first><last>Labeau</last><affiliation>Télécom ParisTech</affiliation></author>
      <author><first>Chloé</first><last>Clavel</last><affiliation>INRIA</affiliation></author>
      <pages>441-463</pages>
      <abstract>We introduce and explore the concept of potentially problematic word usages (PPWUs): word occurrences that are likely to cause communication breakdowns of a semantic nature. While much research has been devoted to lexical complexity, ambiguity, vagueness and related issues, no work has attempted to fully capture the intricate nature of PPWUs. We review linguistic factors, datasets and metrics that can be helpful for PPWU detection. We also discuss challenges to their study, such as their complexity and subjectivity, and highlight the need for future work on this phenomenon.</abstract>
      <url hash="0deb65ef">2025.starsem-1.35</url>
      <bibkey>gari-soler-etal-2025-potentially</bibkey>
      <doi>10.18653/v1/2025.starsem-1.35</doi>
    </paper>
  </volume>
</collection>
