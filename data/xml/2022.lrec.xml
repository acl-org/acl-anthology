<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.lrec">
  <volume id="1" ingest-date="2022-09-20">
    <meta>
      <booktitle>Proceedings of the Thirteenth Language Resources and Evaluation Conference</booktitle>
      <editor><first>Nicoletta</first><last>Calzolari</last></editor>
      <editor><first>Frédéric</first><last>Béchet</last></editor>
      <editor><first>Philippe</first><last>Blache</last></editor>
      <editor><first>Khalid</first><last>Choukri</last></editor>
      <editor><first>Christopher</first><last>Cieri</last></editor>
      <editor><first>Thierry</first><last>Declerck</last></editor>
      <editor><first>Sara</first><last>Goggi</last></editor>
      <editor><first>Hitoshi</first><last>Isahara</last></editor>
      <editor><first>Bente</first><last>Maegaard</last></editor>
      <editor><first>Joseph</first><last>Mariani</last></editor>
      <editor><first>Hélène</first><last>Mazo</last></editor>
      <editor><first>Jan</first><last>Odijk</last></editor>
      <editor><first>Stelios</first><last>Piperidis</last></editor>
      <publisher>European Language Resources Association</publisher>
      <address>Marseille, France</address>
      <month>June</month>
      <year>2022</year>
      <url hash="75a65238">2022.lrec-1</url>
      <venue>lrec</venue>
    </meta>
    <frontmatter>
      <url hash="7222a938">2022.lrec-1.0</url>
      <bibkey>lrec-2022-language</bibkey>
      <revision id="1" href="2022.lrec-1.0v1" hash="6e3cbb6b"/>
      <revision id="2" href="2022.lrec-1.0v2" hash="7222a938" date="2022-10-24">Corrected a typo.</revision>
    </frontmatter>
    <paper id="1">
      <title>Domain Adaptation in Neural Machine Translation using a Qualia-Enriched <fixed-case>F</fixed-case>rame<fixed-case>N</fixed-case>et</title>
      <author><first>Alexandre Diniz da</first><last>Costa</last></author>
      <author><first>Mateus</first><last>Coutinho Marim</last></author>
      <author><first>Ely</first><last>Matos</last></author>
      <author><first>Tiago</first><last>Timponi Torrent</last></author>
      <pages>1–12</pages>
      <abstract>In this paper we present Scylla, a methodology for domain adaptation of Neural Machine Translation (NMT) systems that make use of a multilingual FrameNet enriched with qualia relations as an external knowledge base. Domain adaptation techniques used in NMT usually require fine-tuning and in-domain training data, which may pose difficulties for those working with lesser-resourced languages and may also lead to performance decay of the NMT system for out-of-domain sentences. Scylla does not require fine-tuning of the NMT model, avoiding the risk of model over-fitting and consequent decrease in performance for out-of-domain translations. Two versions of Scylla are presented: one using the source sentence as input, and another one using the target sentence. We evaluate Scylla in comparison to a state-of-the-art commercial NMT system in an experiment in which 50 sentences from the Sports domain are translated from Brazilian Portuguese to English. The two versions of Scylla significantly outperform the baseline commercial system in HTER.</abstract>
      <url hash="ebd276a0">2022.lrec-1.1</url>
      <bibkey>costa-etal-2022-domain</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/framenet">FrameNet</pwcdataset>
    </paper>
    <paper id="2">
      <title><fixed-case>HOPE</fixed-case>: A Task-Oriented and Human-Centric Evaluation Framework Using Professional Post-Editing Towards More Effective <fixed-case>MT</fixed-case> Evaluation</title>
      <author><first>Serge</first><last>Gladkoff</last></author>
      <author><first>Lifeng</first><last>Han</last></author>
      <pages>13–21</pages>
      <abstract>Traditional automatic evaluation metrics for machine translation have been widely criticized by linguists due to their low accuracy, lack of transparency, focus on language mechanics rather than semantics, and low agreement with human quality evaluation. Human evaluations in the form of MQM-like scorecards have always been carried out in real industry setting by both clients and translation service providers (TSPs). However, traditional human translation quality evaluations are costly to perform and go into great linguistic detail, raise issues as to inter-rater reliability (IRR) and are not designed to measure quality of worse than premium quality translations. In this work, we introduce <b>HOPE</b>, a task-oriented and <i>
          <b>h</b>
        </i>uman-centric evaluation framework for machine translation output based <i>
          <b>o</b>
        </i>n professional <i>
          <b>p</b>
        </i>ost-<i>
          <b>e</b>
        </i>diting annotations. It contains only a limited number of commonly occurring error types, and uses a scoring model with geometric progression of error penalty points (EPPs) reflecting error severity level to each translation unit. The initial experimental work carried out on English-Russian language pair MT outputs on marketing content type of text from highly technical domain reveals that our evaluation framework is quite effective in reflecting the MT output quality regarding both overall system-level performance and segment-level transparency, and it increases the IRR for error type interpretation. The approach has several key advantages, such as ability to measure and compare less than perfect MT output from different systems, ability to indicate human perception of quality, immediate estimation of the labor effort required to bring MT output to premium quality, low-cost and faster application, as well as higher IRR. Our experimental data is available at <url>https://github.com/lHan87/HOPE</url>.</abstract>
      <url hash="a50ccfaa">2022.lrec-1.2</url>
      <bibkey>gladkoff-han-2022-hope</bibkey>
      <pwccode url="https://github.com/lhan87/hope" additional="false">lhan87/hope</pwccode>
    </paper>
    <paper id="3">
      <title>Priming <fixed-case>A</fixed-case>ncient <fixed-case>K</fixed-case>orean Neural Machine Translation</title>
      <author><first>Chanjun</first><last>Park</last></author>
      <author><first>Seolhwa</first><last>Lee</last></author>
      <author><first>Jaehyung</first><last>Seo</last></author>
      <author><first>Hyeonseok</first><last>Moon</last></author>
      <author><first>Sugyeong</first><last>Eo</last></author>
      <author><first>Heuiseok</first><last>Lim</last></author>
      <pages>22–28</pages>
      <abstract>In recent years, there has been an increasing need for the restoration and translation of historical languages. In this study, we attempt to translate historical records in ancient Korean language based on neural machine translation (NMT). Inspired by priming, a cognitive science theory that two different stimuli influence each other, we propose novel priming ancient-Korean NMT (AKNMT) using bilingual subword embedding initialization with structural property awareness in the ancient documents. Finally, we obtain state-of-the-art results in the AKNMT task. To the best of our knowledge, we confirm the possibility of developing a human-centric model that incorporates the concepts of cognitive science and analyzes the result from the perspective of interference and cognitive dissonance theory for the first time.</abstract>
      <url hash="b2e20ab3">2022.lrec-1.3</url>
      <bibkey>park-etal-2022-priming</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>GECO</fixed-case>-<fixed-case>MT</fixed-case>: The Ghent Eye-tracking Corpus of Machine Translation</title>
      <author><first>Toon</first><last>Colman</last></author>
      <author><first>Margot</first><last>Fonteyne</last></author>
      <author><first>Joke</first><last>Daems</last></author>
      <author><first>Nicolas</first><last>Dirix</last></author>
      <author><first>Lieve</first><last>Macken</last></author>
      <pages>29–38</pages>
      <abstract>In the present paper, we describe a large corpus of eye movement data, collected during natural reading of a human translation and a machine translation of a full novel. This data set, called GECO-MT (Ghent Eye tracking Corpus of Machine Translation) expands upon an earlier corpus called GECO (Ghent Eye-tracking Corpus) by Cop et al. (2017). The eye movement data in GECO-MT will be used in future research to investigate the effect of machine translation on the reading process and the effects of various error types on reading. In this article, we describe in detail the materials and data collection procedure of GECO-MT. Extensive information on the language proficiency of our participants is given, as well as a comparison with the participants of the original GECO. We investigate the distribution of a selection of important eye movement variables and explore the possibilities for future analyses of the data. GECO-MT is freely available at https://www.lt3.ugent.be/resources/geco-mt.</abstract>
      <url hash="a1b1f5d0">2022.lrec-1.4</url>
      <bibkey>colman-etal-2022-geco</bibkey>
    </paper>
    <paper id="5">
      <title>Introducing Frege to <fixed-case>F</fixed-case>illmore: A <fixed-case>F</fixed-case>rame<fixed-case>N</fixed-case>et Dataset that Captures both Sense and Reference</title>
      <author><first>Levi</first><last>Remijnse</last></author>
      <author><first>Piek</first><last>Vossen</last></author>
      <author><first>Antske</first><last>Fokkens</last></author>
      <author><first>Sam</first><last>Titarsolej</last></author>
      <pages>39–50</pages>
      <abstract>This article presents the first output of the Dutch FrameNet annotation tool, which facilitates both referential- and frame annotations of language-independent corpora. On the referential level, the tool links in-text mentions to structured data, grounding the text in the real world. On the frame level, those same mentions are annotated with respect to their semantic sense. This way of annotating not only generates a rich linguistic dataset that is grounded in real-world event instances, but also guides the annotators in frame identification, resulting in high inter-annotator-agreement and consistent annotations across documents and at discourse level, exceeding traditional sentence level annotations of frame elements. Moreover, the annotation tool features a dynamic lexical lookup that increases the development of a cross-domain FrameNet lexicon.</abstract>
      <url hash="7287b3ba">2022.lrec-1.5</url>
      <bibkey>remijnse-etal-2022-introducing</bibkey>
    </paper>
    <paper id="6">
      <title>Compiling a Suitable Level of Sense Granularity in a Lexicon for <fixed-case>AI</fixed-case> Purposes: The Open Source <fixed-case>COR</fixed-case> Lexicon</title>
      <author><first>Bolette</first><last>Pedersen</last></author>
      <author><first>Nathalie Carmen Hau</first><last>Sørensen</last></author>
      <author><first>Sanni</first><last>Nimb</last></author>
      <author><first>Ida</first><last>Flørke</last></author>
      <author><first>Sussi</first><last>Olsen</last></author>
      <author><first>Thomas</first><last>Troelsgård</last></author>
      <pages>51–60</pages>
      <abstract>We present The Central Word Register for Danish (COR), which is an open source lexicon project for general AI purposes funded and initiated by the Danish Agency for Digitisation as part of an AI initiative embarked by the Danish Government in 2020. We focus here on the lexical semantic part of the project (COR-S) and describe how we – based on the existing fine-grained sense inventory from Den Danske Ordbog (DDO) – compile a more AI suitable sense granularity level of the vocabulary. A three-step methodology is applied: We establish a set of linguistic principles for defining core senses in COR-S and from there, we generate a hand-crafted gold standard of 6,000 lemmas depicting how to come from the fine-grained DDO sense to the COR inventory. Finally, we experiment with a number of language models in order to automatize the sense reduction of the rest of the lexicon. The models comprise a ruled-based model that applies our linguistic principles in terms of features, a word2vec model using cosine similarity to measure the sense proximity, and finally a deep neural BERT model fine-tuned on our annotations. The rule-based approach shows best results, in particular on adjectives, however, when focusing on the average polysemous vocabulary, the BERT model shows promising results too.</abstract>
      <url hash="c3e6a6ba">2022.lrec-1.6</url>
      <bibkey>pedersen-etal-2022-compiling</bibkey>
    </paper>
    <paper id="7">
      <title>Sense and Sentiment</title>
      <author><first>Francis</first><last>Bond</last></author>
      <author><first>Merrick</first><last>Choo</last></author>
      <pages>61–69</pages>
      <abstract>In this paper we examine existing sentiment lexicons and sense-based sentiment-tagged corpora to find out how sense and concept-based semantic relations effect sentiment scores (for polarity and valence). We show that some relations are good predictors of sentiment of related words: antonyms have similar valence and opposite polarity, synonyms similar valence and polarity, as do many derivational relations. We use this knowledge and existing resources to build a sentiment annotated wordnet of English, and show how it can be used to produce sentiment lexicons for other languages using the Open Multilingual Wordnet.</abstract>
      <url hash="6b083305">2022.lrec-1.7</url>
      <bibkey>bond-choo-2022-sense</bibkey>
      <pwccode url="https://github.com/bond-lab/sensitive" additional="true">bond-lab/sensitive</pwccode>
    </paper>
    <paper id="8">
      <title>Enriching Linguistic Representation in the <fixed-case>C</fixed-case>antonese <fixed-case>W</fixed-case>ordnet and Building the New <fixed-case>C</fixed-case>antonese <fixed-case>W</fixed-case>ordnet Corpus</title>
      <author><first>Ut Seong</first><last>Sio</last></author>
      <author><first>Luís</first><last>Morgado da Costa</last></author>
      <pages>70–78</pages>
      <abstract>This paper reports on the most recent improvements on the Cantonese Wordnet, a wordnet project started in 2019 (Sio and Morgado da Costa, 2019) with the aim of capturing and organizing lexico-semantic information of Hong Kong Cantonese. The improvements we present here extend both the breadth and depth of the Cantonese Wordnet: increasing the general coverage, adding functional categories, enriching verbal representations, as well as creating the Cantonese Wordnet Corpus – a corpus of handcrafted examples where individual senses are shown in context.</abstract>
      <url hash="01aeea3c">2022.lrec-1.8</url>
      <bibkey>sio-morgado-da-costa-2022-enriching</bibkey>
    </paper>
    <paper id="9">
      <title><fixed-case>ZAEBUC</fixed-case>: An Annotated <fixed-case>A</fixed-case>rabic-<fixed-case>E</fixed-case>nglish Bilingual Writer Corpus</title>
      <author><first>Nizar</first><last>Habash</last></author>
      <author><first>David</first><last>Palfreyman</last></author>
      <pages>79–88</pages>
      <abstract>We present ZAEBUC, an annotated Arabic-English bilingual writer corpus comprising short essays by first-year university students at Zayed University in the United Arab Emirates. We describe and discuss the various guidelines and pipeline processes we followed to create the annotations and quality check them. The annotations include spelling and grammar correction, morphological tokenization, Part-of-Speech tagging, lemmatization, and Common European Framework of Reference (CEFR) ratings. All of the annotations are done on Arabic and English texts using consistent guidelines as much as possible, with tracked alignments among the different annotations, and to the original raw texts. For morphological tokenization, POS tagging, and lemmatization, we use existing automatic annotation tools followed by manual correction. We also present various measurements and correlations with preliminary insights drawn from the data and annotations. The publicly available ZAEBUC corpus and its annotations are intended to be the stepping stones for additional annotations.</abstract>
      <url hash="add1c4d2">2022.lrec-1.9</url>
      <bibkey>habash-palfreyman-2022-zaebuc</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="10">
      <title><fixed-case>T</fixed-case>urkish <fixed-case>U</fixed-case>niversal <fixed-case>C</fixed-case>onceptual <fixed-case>C</fixed-case>ognitive <fixed-case>A</fixed-case>nnotation</title>
      <author><first>Necva</first><last>Bölücü</last></author>
      <author><first>Burcu</first><last>Can</last></author>
      <pages>89–99</pages>
      <abstract>Universal Conceptual Cognitive Annotation (UCCA) (Abend and Rappoport, 2013a) is a cross-lingual semantic annotation framework that provides an easy annotation without any requirement for linguistic background. UCCA-annotated datasets have been already released in English, French, and German. In this paper, we introduce the first UCCA-annotated Turkish dataset that currently involves 50 sentences obtained from the METU-Sabanci Turkish Treebank (Atalay et al., 2003; Oflazeret al., 2003). We followed a semi-automatic annotation approach, where an external semantic parser is utilised for an initial annotation of the dataset, which is partially accurate and requires refinement. We manually revised the annotations obtained from the semantic parser that are not in line with the UCCA rules that we defined for Turkish. We used the same external semantic parser for evaluation purposes and conducted experiments with both zero-shot and few-shot learning. While the parser cannot predict remote edges in zero-shot setting, using even a small subset of training data in few-shot setting increased the overall F-1 score including the remote edges. This is the initial version of the annotated dataset and we are currently extending the dataset. We will release the current Turkish UCCA annotation guideline along with the annotated dataset.</abstract>
      <url hash="1b1d9422">2022.lrec-1.10</url>
      <bibkey>bolucu-can-2022-turkish</bibkey>
    </paper>
    <paper id="11">
      <title>Introducing the <fixed-case>CURLICAT</fixed-case> Corpora: Seven-language Domain Specific Annotated Corpora from Curated Sources</title>
      <author><first>Tamás</first><last>Váradi</last></author>
      <author><first>Bence</first><last>Nyéki</last></author>
      <author><first>Svetla</first><last>Koeva</last></author>
      <author><first>Marko</first><last>Tadić</last></author>
      <author><first>Vanja</first><last>Štefanec</last></author>
      <author><first>Maciej</first><last>Ogrodniczuk</last></author>
      <author><first>Bartłomiej</first><last>Nitoń</last></author>
      <author><first>Piotr</first><last>Pęzik</last></author>
      <author><first>Verginica</first><last>Barbu Mititelu</last></author>
      <author><first>Elena</first><last>Irimia</last></author>
      <author><first>Maria</first><last>Mitrofan</last></author>
      <author><first>Dan</first><last>Tufiș</last></author>
      <author><first>Radovan</first><last>Garabík</last></author>
      <author><first>Simon</first><last>Krek</last></author>
      <author><first>Andraž</first><last>Repar</last></author>
      <pages>100–108</pages>
      <abstract>This article presents the current outcomes of the CURLICAT CEF Telecom project, which aims to collect and deeply annotate a set of large corpora from selected domains. The CURLICAT corpus includes 7 monolingual corpora (Bulgarian, Croatian, Hungarian, Polish, Romanian, Slovak and Slovenian) containing selected samples from respective national corpora. These corpora are automatically tokenized, lemmatized and morphologically analysed and the named entities annotated. The annotations are uniformly provided for each language specific corpus while the common metadata schema is harmonised across the languages. Additionally, the corpora are annotated for IATE terms in all languages. The file format is CoNLL-U Plus format, containing the ten columns specific to the CoNLL-U format and three extra columns specific to our corpora as defined by Varádi et al. (2020). The CURLICAT corpora represent a rich and valuable source not just for training NMT models, but also for further studies and developments in machine learning, cross-lingual terminological data extraction and classification.</abstract>
      <url hash="6a9f9346">2022.lrec-1.11</url>
      <bibkey>varadi-etal-2022-introducing</bibkey>
    </paper>
    <paper id="12">
      <title><fixed-case>RU</fixed-case>-<fixed-case>ADEPT</fixed-case>: <fixed-case>R</fixed-case>ussian Anonymized Dataset with Eight Personality Traits</title>
      <author><first>C. Anton</first><last>Rytting</last></author>
      <author><first>Valerie</first><last>Novak</last></author>
      <author><first>James R.</first><last>Hull</last></author>
      <author><first>Victor M.</first><last>Frank</last></author>
      <author><first>Paul</first><last>Rodrigues</last></author>
      <author><first>Jarrett G. W.</first><last>Lee</last></author>
      <author><first>Laurel</first><last>Miller-Sims</last></author>
      <pages>109–118</pages>
      <abstract>Social media has provided a platform for many individuals to easily express themselves naturally and publicly, and researchers have had the opportunity to utilize large quantities of this data to improve author trait analysis techniques and to improve author trait profiling systems. The majority of the work in this area, however, has been narrowly spent on English and other Western European languages, and generally focuses on a single social network at a time, despite the large quantity of data now available across languages and differences that have been found across platforms. This paper introduces RU-ADEPT, a dataset of Russian authors’ personality trait scores–Big Five and Dark Triad, demographic information (e.g. age, gender), with associated corpus of the authors’ cross-contributions to (up to) four different social media platforms–VKontakte (VK), LiveJournal, Blogger, and Moi Mir. We believe this to be the first publicly-available dataset associating demographic and personality trait data with Russian-language social media content, the first paper to describe the collection of Dark Triad scores with texts across multiple Russian-language social media platforms, and to a limited extent, the first publicly-available dataset of personality traits to author content across several different social media sites.</abstract>
      <url hash="fd98f621">2022.lrec-1.12</url>
      <bibkey>rytting-etal-2022-ru</bibkey>
    </paper>
    <paper id="13">
      <title><fixed-case>C</fixed-case>o<fixed-case>QAR</fixed-case>: Question Rewriting on <fixed-case>C</fixed-case>o<fixed-case>QA</fixed-case></title>
      <author><first>Quentin</first><last>Brabant</last></author>
      <author><first>Gwénolé</first><last>Lecorvé</last></author>
      <author><first>Lina M.</first><last>Rojas Barahona</last></author>
      <pages>119–126</pages>
      <abstract>Questions asked by humans during a conversation often contain contextual dependencies, i.e., explicit or implicit references to previous dialogue turns. These dependencies take the form of coreferences (e.g., via pronoun use) or ellipses, and can make the understanding difficult for automated systems. One way to facilitate the understanding and subsequent treatments of a question is to rewrite it into an out-of-context form, i.e., a form that can be understood without the conversational context. We propose CoQAR, a corpus containing 4.5K conversations from the Conversational Question-Answering dataset CoQA, for a total of 53K follow-up question-answer pairs. Each original question was manually annotated with at least 2 at most 3 out-of-context rewritings. CoQA originally contains 8k conversations, which sum up to 127k question-answer pairs. CoQAR can be used in the supervised learning of three tasks: question paraphrasing, question rewriting and conversational question answering. In order to assess the quality of CoQAR’s rewritings, we conduct several experiments consisting in training and evaluating models for these three tasks. Our results support the idea that question rewriting can be used as a preprocessing step for (conversational and non-conversational) question answering models, thereby increasing their performances.</abstract>
      <url hash="ebdf9a31">2022.lrec-1.13</url>
      <bibkey>brabant-etal-2022-coqar</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/canard">CANARD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/coqa">CoQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/quac">QuAC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="14">
      <title>User Interest Modelling in Argumentative Dialogue Systems</title>
      <author><first>Annalena</first><last>Aicher</last></author>
      <author><first>Nadine</first><last>Gerstenlauer</last></author>
      <author><first>Wolfgang</first><last>Minker</last></author>
      <author><first>Stefan</first><last>Ultes</last></author>
      <pages>127–136</pages>
      <abstract>Most systems helping to provide structured information and support opinion building, discuss with users without considering their individual interest. The scarce existing research on user interest in dialogue systems depends on explicit user feedback. Such systems require user responses that are not content-related and thus, tend to disturb the dialogue flow. In this paper, we present a novel model for implicitly estimating user interest during argumentative dialogues based on semantically clustered data. Therefore, an online user study was conducted to acquire training data which was used to train a binary neural network classifier in order to predict whether or not users are still interested in the content of the ongoing dialogue. We achieved a classification accuracy of 74.9% and furthermore investigated with different Artificial Neural Networks (ANN) which new argument would fit the user interest best.</abstract>
      <url hash="389ab718">2022.lrec-1.14</url>
      <bibkey>aicher-etal-2022-user</bibkey>
    </paper>
    <paper id="15">
      <title>Every time <fixed-case>I</fixed-case> fire a conversational designer, the performance of the dialogue system goes down</title>
      <author><first>Giancarlo</first><last>Xompero</last></author>
      <author><first>Michele</first><last>Mastromattei</last></author>
      <author><first>Samir</first><last>Salman</last></author>
      <author><first>Cristina</first><last>Giannone</last></author>
      <author><first>Andrea</first><last>Favalli</last></author>
      <author><first>Raniero</first><last>Romagnoli</last></author>
      <author><first>Fabio Massimo</first><last>Zanzotto</last></author>
      <pages>137–145</pages>
      <abstract>Incorporating handwritten domain scripts into neural-based task-oriented dialogue systems may be an effective way to reduce the need for large sets of annotated dialogues. In this paper, we investigate how the use of domain scripts written by conversational designers affects the performance of neural-based dialogue systems. To support this investigation, we propose the Conversational-Logic-Injection-in-Neural-Network system (CLINN) where domain scripts are coded in semi-logical rules. By using CLINN, we evaluated semi-logical rules produced by a team of differently-skilled conversational designers. We experimented with the Restaurant domain of the MultiWOZ dataset. Results show that external knowledge is extremely important for reducing the need for annotated examples for conversational systems. In fact, rules from conversational designers used in CLINN significantly outperform a state-of-the-art neural-based dialogue system when trained with smaller sets of annotated dialogues.</abstract>
      <url hash="c1fe25c5">2022.lrec-1.15</url>
      <bibkey>xompero-etal-2022-every</bibkey>
    </paper>
    <paper id="16">
      <title>An Empirical Study on the Overlapping Problem of Open-Domain Dialogue Datasets</title>
      <author><first>Yuqiao</first><last>Wen</last></author>
      <author><first>Guoqing</first><last>Luo</last></author>
      <author><first>Lili</first><last>Mou</last></author>
      <pages>146–153</pages>
      <abstract>Open-domain dialogue systems aim to converse with humans through text, and dialogue research has heavily relied on benchmark datasets. In this work, we observe the overlapping problem in DailyDialog and OpenSubtitles, two popular open-domain dialogue benchmark datasets. Our systematic analysis then shows that such overlapping can be exploited to obtain fake state-of-the-art performance. Finally, we address this issue by cleaning these datasets and setting up a proper data processing procedure for future research.</abstract>
      <url hash="ff270ccb">2022.lrec-1.16</url>
      <bibkey>wen-etal-2022-empirical</bibkey>
      <pwccode url="https://github.com/yq-wen/overlapping-datasets" additional="false">yq-wen/overlapping-datasets</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/atis">ATIS</pwcdataset>
    </paper>
    <paper id="17">
      <title>Language Technologies for the Creation of Multilingual Terminologies. Lessons Learned from the <fixed-case>SSHOC</fixed-case> Project</title>
      <author><first>Federica</first><last>Gamba</last></author>
      <author><first>Francesca</first><last>Frontini</last></author>
      <author><first>Daan</first><last>Broeder</last></author>
      <author><first>Monica</first><last>Monachini</last></author>
      <pages>154–163</pages>
      <abstract>This paper is framed in the context of the SSHOC project and aims at exploring how Language Technologies can help in promoting and facilitating multilingualism in the Social Sciences and Humanities (SSH). Although most SSH researchers produce culturally and societally relevant work in their local languages, metadata and vocabularies used in the SSH domain to describe and index research data are currently mostly in English. We thus investigate Natural Language Processing and Machine Translation approaches in view of providing resources and tools to foster multilingual access and discovery to SSH content across different languages. As case studies, we create and deliver as freely, openly available data a set of multilingual metadata concepts and an automatically extracted multilingual Data Stewardship terminology. The two case studies allow as well to evaluate performances of state-of-the-art tools and to derive a set of recommendations as to how best apply them. Although not adapted to the specific domain, the employed tools prove to be a valid asset to translation tasks. Nonetheless, validation of results by domain experts proficient in the language is an unavoidable phase of the whole workflow.</abstract>
      <url hash="1d333c48">2022.lrec-1.17</url>
      <bibkey>gamba-etal-2022-language</bibkey>
    </paper>
    <paper id="18">
      <title>How to be <fixed-case>FAIR</fixed-case> when you <fixed-case>CARE</fixed-case>: The <fixed-case>DGS</fixed-case> <fixed-case>C</fixed-case>orpus as a Case Study of Open Science Resources for Minority Languages</title>
      <author><first>Marc</first><last>Schulder</last></author>
      <author><first>Thomas</first><last>Hanke</last></author>
      <pages>164–173</pages>
      <abstract>The publication of resources for minority languages requires a balance between making data open and accessible and respecting the rights and needs of its language community. The FAIR principles were introduced as a guide to good open data practices and they have since been complemented by the CARE principles for indigenous data governance. This article describes how the DGS Corpus implemented these principles and how the two sets of principles affected each other. The DGS Corpus is a large collection of recordings of members of the deaf community in Germany communicating in their primary language, German Sign Language (DGS); it was created to be both as a resource for linguistic research and as a record of the life experiences of deaf people in Germany. The corpus was designed with CARE in mind to respect and empower the language community and FAIR data publishing was used to enhance its usefulness as a scientific resource.</abstract>
      <url hash="56fb091b">2022.lrec-1.18</url>
      <bibkey>schulder-hanke-2022-fair</bibkey>
    </paper>
    <paper id="19">
      <title><fixed-case>I</fixed-case>talian <fixed-case>NLP</fixed-case> for Everyone: Resources and Models from <fixed-case>EVALITA</fixed-case> to the <fixed-case>E</fixed-case>uropean Language Grid</title>
      <author><first>Valerio</first><last>Basile</last></author>
      <author><first>Cristina</first><last>Bosco</last></author>
      <author><first>Michael</first><last>Fell</last></author>
      <author><first>Viviana</first><last>Patti</last></author>
      <author><first>Rossella</first><last>Varvara</last></author>
      <pages>174–180</pages>
      <abstract>The European Language Grid enables researchers and practitioners to easily distribute and use NLP resources and models, such as corpora and classifiers. We describe in this paper how, during the course of our EVALITA4ELG project, we have integrated datasets and systems for the Italian language. We show how easy it is to use the integrated systems, and demonstrate in case studies how seamless the application of the platform is, providing Italian NLP for everyone.</abstract>
      <url hash="874f68f3">2022.lrec-1.19</url>
      <bibkey>basile-etal-2022-italian</bibkey>
    </paper>
    <paper id="20">
      <title>Cross-Lingual Link Discovery for Under-Resourced Languages</title>
      <author><first>Michael</first><last>Rosner</last></author>
      <author><first>Sina</first><last>Ahmadi</last></author>
      <author><first>Elena-Simona</first><last>Apostol</last></author>
      <author><first>Julia</first><last>Bosque-Gil</last></author>
      <author><first>Christian</first><last>Chiarcos</last></author>
      <author><first>Milan</first><last>Dojchinovski</last></author>
      <author><first>Katerina</first><last>Gkirtzou</last></author>
      <author><first>Jorge</first><last>Gracia</last></author>
      <author><first>Dagmar</first><last>Gromann</last></author>
      <author><first>Chaya</first><last>Liebeskind</last></author>
      <author><first>Giedrė</first><last>Valūnaitė Oleškevičienė</last></author>
      <author><first>Gilles</first><last>Sérasset</last></author>
      <author><first>Ciprian-Octavian</first><last>Truică</last></author>
      <pages>181–192</pages>
      <abstract>In this paper, we provide an overview of current technologies for cross-lingual link discovery, and we discuss challenges, experiences and prospects of their application to under-resourced languages. We rst introduce the goals of cross-lingual linking and associated technologies, and in particular, the role that the Linked Data paradigm (Bizer et al., 2011) applied to language data can play in this context. We de ne under-resourced languages with a speci c focus on languages actively used on the internet, i.e., languages with a digitally versatile speaker community, but limited support in terms of language technology. We argue that languages for which considerable amounts of textual data and (at least) a bilingual word list are available, techniques for cross-lingual linking can be readily applied, and that these enable the implementation of downstream applications for under-resourced languages via the localisation and adaptation of existing technologies and resources.</abstract>
      <url hash="be98f941">2022.lrec-1.20</url>
      <bibkey>rosner-etal-2022-cross</bibkey>
    </paper>
    <paper id="21">
      <title>Angry or Sad ? Emotion Annotation for Extremist Content Characterisation</title>
      <author><first>Valentina</first><last>Dragos</last></author>
      <author><first>Delphine</first><last>Battistelli</last></author>
      <author><first>Aline</first><last>Etienne</last></author>
      <author><first>Yolène</first><last>Constable</last></author>
      <pages>193–201</pages>
      <abstract>This paper examines the role of emotion annotations to characterize extremist content released on social platforms. The analysis of extremist content is important to identify user emotions towards some extremist ideas and to highlight the root cause of where emotions and extremist attitudes merge together. To address these issues our methodology combines knowledge from sociological and linguistic annotations to explore French extremist content collected online. For emotion linguistic analysis, the solution presented in this paper relies on a complex linguistic annotation scheme. The scheme was used to annotate extremist text corpora in French. Data sets were collected online by following semi-automatic procedures for content selection and validation. The paper describes the integrated annotation scheme, the annotation protocol that was set-up for French corpora annotation and the results, e.g. agreement measures and remarks on annotation disagreements. The aim of this work is twofold: first, to provide a characterization of extremist contents; second, to validate the annotation scheme and to test its capacity to capture and describe various aspects of emotions.</abstract>
      <url hash="81369e27">2022.lrec-1.21</url>
      <bibkey>dragos-etal-2022-angry</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/emobank">EmoBank</pwcdataset>
    </paper>
    <paper id="22">
      <title>Identification of Multiword Expressions in Tweets for Hate Speech Detection</title>
      <author><first>Nicolas</first><last>Zampieri</last></author>
      <author><first>Carlos</first><last>Ramisch</last></author>
      <author><first>Irina</first><last>Illina</last></author>
      <author><first>Dominique</first><last>Fohr</last></author>
      <pages>202–210</pages>
      <abstract>Multiword expression (MWE) identification in tweets is a complex task due to the complex linguistic nature of MWEs combined with the non-standard language use in social networks. MWE features were shown to be helpful for hate speech detection (HSD). In this article, we present joint experiments on these two related tasks on English Twitter data: first we focus on the MWE identification task, and then we observe the influence of MWE-based features on the HSD task. For MWE identification, we compare the performance of two systems: lexicon-based and deep neural networks-based (DNN). We experimentally evaluate seven configurations of a state-of-the-art DNN system based on recurrent networks using pre-trained contextual embeddings from BERT. The DNN-based system outperforms the lexicon-based one thanks to its superior generalisation power, yielding much better recall. For the HSD task, we propose a new DNN architecture for incorporating MWE features. We confirm that MWE features are helpful for the HSD task. Moreover, the proposed DNN architecture beats previous MWE-based HSD systems by 0.4 to 1.1 F-measure points on average on four Twitter HSD corpora.</abstract>
      <url hash="fbf6b40c">2022.lrec-1.22</url>
      <bibkey>zampieri-etal-2022-identification-multiword</bibkey>
    </paper>
    <paper id="23">
      <title>Causal Investigation of Public Opinion during the <fixed-case>COVID</fixed-case>-19 Pandemic via Social Media Text</title>
      <author><first>Michael</first><last>Jantscher</last></author>
      <author><first>Roman</first><last>Kern</last></author>
      <pages>211–226</pages>
      <abstract>Understanding the needs and fears of citizens, especially during a pandemic such as COVID-19, is essential for any government or legislative entity. An effective COVID-19 strategy further requires that the public understand and accept the restriction plans imposed by these entities. In this paper, we explore a causal mediation scenario in which we want to emphasize the use of NLP methods in combination with methods from economics and social sciences. Based on sentiment analysis of Tweets towards the current COVID-19 situation in the UK and Sweden, we conduct several causal inference experiments and attempt to decouple the effect of government restrictions on mobility behavior from the effect that occurs due to public perception of the COVID-19 strategy in a country. To avoid biased results we control for valid country specific epidemiological and time-varying confounders. Comprehensive experiments show that not all changes in mobility are caused by countries implemented policies but also by the support of individuals in the fight against this pandemic. We find that social media texts are an important source to capture citizens’ concerns and trust in policy makers and are suitable to evaluate the success of government policies.</abstract>
      <url hash="dbd029ae">2022.lrec-1.23</url>
      <bibkey>jantscher-kern-2022-causal</bibkey>
    </paper>
    <paper id="24">
      <title>Misspelling Semantics in <fixed-case>T</fixed-case>hai</title>
      <author><first>Pakawat</first><last>Nakwijit</last></author>
      <author><first>Matthew</first><last>Purver</last></author>
      <pages>227–236</pages>
      <abstract>User-generated content is full of misspellings. Rather than being just random noise, we hypothesise that many misspellings contain hidden semantics that can be leveraged for language understanding tasks. This paper presents a fine-grained annotated corpus of misspelling in Thai, together with an analysis of misspelling intention and its possible semantics to get a better understanding of the misspelling patterns observed in the corpus. In addition, we introduce two approaches to incorporate the semantics of misspelling: Misspelling Average Embedding (MAE) and Misspelling Semantic Tokens (MST). Experiments on a sentiment analysis task confirm our overall hypothesis: additional semantics from misspelling can boost the micro F1 score up to 0.4-2%, while blindly normalising misspelling is harmful and suboptimal.</abstract>
      <url hash="949fe581">2022.lrec-1.24</url>
      <bibkey>nakwijit-purver-2022-misspelling</bibkey>
    </paper>
    <paper id="25">
      <title>Automatic Detection of Stigmatizing Uses of Psychiatric Terms on <fixed-case>T</fixed-case>witter</title>
      <author><first>Véronique</first><last>Moriceau</last></author>
      <author><first>Farah</first><last>Benamara</last></author>
      <author><first>Abdelmoumene</first><last>Boumadane</last></author>
      <pages>237–243</pages>
      <abstract>Psychiatry and people suffering from mental disorders have often been given a pejorative label that induces social rejection.Many studies have addressed discourse content about psychiatry on social media, suggesting that they convey stigmatizingrepresentations of mental health disorders. In this paper, we focus for the first time on the use of psychiatric terms in tweetsin French. We first describe the annotated dataset that we use. Then we propose several deep learning models to detectautomatically (1) the different types of use of psychiatric terms (medical use, misuse or irrelevant use), and (2) the polarityof the tweet. We show that polarity detection can be improved when done in a multitask framework in combination with typeof use detection. This confirms the observations made manually on several datasets, namely that the polarity of a tweet iscorrelated to the type of term use (misuses are mostly negative whereas medical uses are neutral). The results are interesting forboth tasks and it allows to consider the possibility for performant automatic approaches in order to conduct real-time surveyson social media, larger and less expensive than existing manual ones</abstract>
      <url hash="c63ed97c">2022.lrec-1.25</url>
      <bibkey>moriceau-etal-2022-automatic</bibkey>
    </paper>
    <paper id="26">
      <title><fixed-case>C</fixed-case>o<fixed-case>VERT</fixed-case>: A Corpus of Fact-checked Biomedical <fixed-case>COVID</fixed-case>-19 Tweets</title>
      <author><first>Isabelle</first><last>Mohr</last></author>
      <author><first>Amelie</first><last>Wührl</last></author>
      <author><first>Roman</first><last>Klinger</last></author>
      <pages>244–257</pages>
      <abstract>During the first two years of the COVID-19 pandemic, large volumes of biomedical information concerning this new disease have been published on social media. Some of this information can pose a real danger, particularly when false information is shared, for instance recommendations how to treat diseases without professional medical advice. Therefore, automatic fact-checking resources and systems developed specifically for medical domain are crucial. While existing fact-checking resources cover COVID-19 related information in news or quantify the amount of misinformation in tweets, there is no dataset providing fact-checked COVID-19 related Twitter posts with detailed annotations for biomedical entities, relations and relevant evidence. We contribute CoVERT, a fact-checked corpus of tweets with a focus on the domain of biomedicine and COVID-19 related (mis)information. The corpus consists of 300 tweets, each annotated with named entities and relations. We employ a novel crowdsourcing methodology to annotate all tweets with fact-checking labels and supporting evidence, which crowdworkers search for online. This methodology results in substantial inter-annotator agreement. Furthermore, we use the retrieved evidence extracts as part of a fact-checking pipeline, finding that the real-world evidence is more useful than the knowledge directly available in pretrained language models.</abstract>
      <url hash="ca5dd32b">2022.lrec-1.26</url>
      <bibkey>mohr-etal-2022-covert</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/covert">CoVERT</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/biolama">BioLAMA</pwcdataset>
    </paper>
    <paper id="27">
      <title><fixed-case>XLM</fixed-case>-<fixed-case>T</fixed-case>: Multilingual Language Models in <fixed-case>T</fixed-case>witter for Sentiment Analysis and Beyond</title>
      <author><first>Francesco</first><last>Barbieri</last></author>
      <author><first>Luis</first><last>Espinosa Anke</last></author>
      <author><first>Jose</first><last>Camacho-Collados</last></author>
      <pages>258–266</pages>
      <abstract>Language models are ubiquitous in current NLP, and their multilingual capacity has recently attracted considerable attention. However, current analyses have almost exclusively focused on (multilingual variants of) standard benchmarks, and have relied on clean pre-training and task-specific corpora as multilingual signals. In this paper, we introduce XLM-T, a model to train and evaluate multilingual language models in Twitter. In this paper we provide: (1) a new strong multilingual baseline consisting of an XLM-R (Conneau et al. 2020) model pre-trained on millions of tweets in over thirty languages, alongside starter code to subsequently fine-tune on a target task; and (2) a set of unified sentiment analysis Twitter datasets in eight different languages and a XLM-T model trained on this dataset.</abstract>
      <url hash="5a05483a">2022.lrec-1.27</url>
      <bibkey>barbieri-etal-2022-xlm</bibkey>
      <pwccode url="https://github.com/cardiffnlp/xlm-t" additional="false">cardiffnlp/xlm-t</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/tweeteval">TweetEval</pwcdataset>
    </paper>
    <paper id="28">
      <title>‘Am <fixed-case>I</fixed-case> the Bad One’? Predicting the Moral Judgement of the Crowd Using Pre–trained Language Models</title>
      <author><first>Areej</first><last>Alhassan</last></author>
      <author><first>Jinkai</first><last>Zhang</last></author>
      <author><first>Viktor</first><last>Schlegel</last></author>
      <pages>267–276</pages>
      <abstract>Natural language processing (NLP) has been shown to perform well in various tasks, such as answering questions, ascertaining natural language inference and anomaly detection. However, there are few NLP-related studies that touch upon the moral context conveyed in text. This paper studies whether state-of-the-art, pre-trained language models are capable of passing moral judgments on posts retrieved from a popular Reddit user board. Reddit is a social discussion website and forum where posts are promoted by users through a voting system. In this work, we construct a dataset that can be used for moral judgement tasks by collecting data from the AITA? (Am I the A*******?) subreddit. To model our task, we harnessed the power of pre-trained language models, including BERT, RoBERTa, RoBERTa-large, ALBERT and Longformer. We then fine-tuned these models and evaluated their ability to predict the correct verdict as judged by users for each post in the datasets. RoBERTa showed relative improvements across the three datasets, exhibiting a rate of 87% accuracy and a Matthews correlation coefficient (MCC) of 0.76, while the use of the Longformer model slightly improved the performance when used with longer sequences, achieving 87% accuracy and 0.77 MCC.</abstract>
      <url hash="0bf908d5">2022.lrec-1.28</url>
      <bibkey>alhassan-etal-2022-bad</bibkey>
    </paper>
    <paper id="29">
      <title>Generating Questions from <fixed-case>W</fixed-case>ikidata Triples</title>
      <author><first>Kelvin</first><last>Han</last></author>
      <author><first>Thiago</first><last>Castro Ferreira</last></author>
      <author><first>Claire</first><last>Gardent</last></author>
      <pages>277–290</pages>
      <abstract>Question generation from knowledge bases (or knowledge base question generation, KBQG) is the task of generating questions from structured database information, typically in the form of triples representing facts. To handle rare entities and generalize to unseen properties, previous work on KBQG resorted to extensive, often ad-hoc pre- and post-processing of the input triple. We revisit KBQG – using pre training, a new (triple, question) dataset and taking question type into account – and show that our approach outperforms previous work both in a standard and in a zero-shot setting. We also show that the extended KBQG dataset (also helpful for knowledge base question answering) we provide allows not only for better coverage in terms of knowledge base (KB) properties but also for increased output variability in that it permits the generation of multiple questions from the same KB triple.</abstract>
      <url hash="790cb146">2022.lrec-1.29</url>
      <bibkey>han-etal-2022-generating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/dbpedia">DBpedia</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/simplequestions">SimpleQuestions</pwcdataset>
    </paper>
    <paper id="30">
      <title>Evaluating Transformer Language Models on Arithmetic Operations Using Number Decomposition</title>
      <author><first>Matteo</first><last>Muffo</last></author>
      <author><first>Aldo</first><last>Cocco</last></author>
      <author><first>Enrico</first><last>Bertino</last></author>
      <pages>291–297</pages>
      <abstract>In recent years, Large Language Models such as GPT-3 showed remarkable capabilities in performing NLP tasks in the zero and few shot settings. On the other hand, the experiments highlighted the difficulty of GPT-3 in carrying out tasks that require a certain degree of reasoning, such as arithmetic operations. In this paper we evaluate the ability of Transformer Language Models to perform arithmetic operations following a pipeline that, before performing computations, decomposes numbers in units, tens, and so on. We denote the models fine-tuned with this pipeline with the name Calculon and we test them in the task of performing additions, subtractions and multiplications on the same test sets of GPT-3. Results show an increase of accuracy of 63% in the five-digit addition task. Moreover, we demonstrate the importance of the decomposition pipeline introduced, since fine-tuning the same Language Model without decomposing numbers results in 0% accuracy in the five-digit addition task.</abstract>
      <url hash="25e0efcb">2022.lrec-1.30</url>
      <bibkey>muffo-etal-2022-evaluating</bibkey>
      <pwccode url="https://github.com/mmuffo94/TransformerLM_arithmetics" additional="false">mmuffo94/TransformerLM_arithmetics</pwccode>
    </paper>
    <paper id="31">
      <title>Evaluating the Effects of Embedding with Speaker Identity Information in Dialogue Summarization</title>
      <author><first>Yuji</first><last>Naraki</last></author>
      <author><first>Tetsuya</first><last>Sakai</last></author>
      <author><first>Yoshihiko</first><last>Hayashi</last></author>
      <pages>298–304</pages>
      <abstract>Automatic dialogue summarization is a task used to succinctly summarize a dialogue transcript while correctly linking the speakers and their speech, which distinguishes this task from a conventional document summarization. To address this issue and reduce the “who said what”-related errors in a summary, we propose embedding the speaker identity information in the input embedding into the dialogue transcript encoder. Unlike the speaker embedding proposed by Gu et al. (2020), our proposal takes into account the informativeness of position embedding. By experimentally comparing several embedding methods, we confirmed that the scores of ROUGE and a human evaluation of the generated summaries were substantially increased by embedding speaker information at the less informative part of the fixed position embedding with sinusoidal functions.</abstract>
      <url hash="ca0bc2c3">2022.lrec-1.31</url>
      <bibkey>naraki-etal-2022-evaluating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/samsum-corpus">SAMSum Corpus</pwcdataset>
    </paper>
    <paper id="32">
      <title>Perceived Text Quality and Readability in Extractive and Abstractive Summaries</title>
      <author><first>Julius</first><last>Monsen</last></author>
      <author><first>Evelina</first><last>Rennes</last></author>
      <pages>305–312</pages>
      <abstract>We present results from a study investigating how users perceive text quality and readability in extractive and abstractive summaries. We trained two summarisation models on Swedish news data and used these to produce summaries of articles. With the produced summaries, we conducted an online survey in which the extractive summaries were compared to the abstractive summaries in terms of fluency, adequacy and simplicity. We found statistically significant differences in perceived fluency and adequacy between abstractive and extractive summaries but no statistically significant difference in simplicity. Extractive summaries were preferred in most cases, possibly due to the types of errors the summaries tend to have.</abstract>
      <url hash="4660612f">2022.lrec-1.32</url>
      <bibkey>monsen-rennes-2022-perceived</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
    </paper>
    <paper id="33">
      <title>Learning to Prioritize: Precision-Driven Sentence Filtering for Long Text Summarization</title>
      <author><first>Alex</first><last>Mei</last></author>
      <author><first>Anisha</first><last>Kabir</last></author>
      <author><first>Rukmini</first><last>Bapat</last></author>
      <author><first>John</first><last>Judge</last></author>
      <author><first>Tony</first><last>Sun</last></author>
      <author><first>William Yang</first><last>Wang</last></author>
      <pages>313–318</pages>
      <abstract>Neural text summarization has shown great potential in recent years. However, current state-of-the-art summarization models are limited by their maximum input length, posing a challenge to summarizing longer texts comprehensively. As part of a layered summarization architecture, we introduce PureText, a simple yet effective pre-processing layer that removes low- quality sentences in articles to improve existing summarization models. When evaluated on popular datasets like WikiHow and Reddit TIFU, we show up to 3.84 and 8.57 point ROUGE-1 absolute improvement on the full test set and the long article subset, respectively, for state-of-the-art summarization models such as BertSum and BART. Our approach provides downstream models with higher-quality sentences for summarization, improving overall model performance, especially on long text articles.</abstract>
      <url hash="caa3d355">2022.lrec-1.33</url>
      <bibkey>mei-etal-2022-learning</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/reddit-tifu">Reddit TIFU</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikihow">WikiHow</pwcdataset>
    </paper>
    <paper id="34">
      <title>Automating Horizon Scanning in Future Studies</title>
      <author><first>Tatsuya</first><last>Ishigaki</last></author>
      <author><first>Suzuko</first><last>Nishino</last></author>
      <author><first>Sohei</first><last>Washino</last></author>
      <author><first>Hiroki</first><last>Igarashi</last></author>
      <author><first>Yukari</first><last>Nagai</last></author>
      <author><first>Yuichi</first><last>Washida</last></author>
      <author><first>Akihiko</first><last>Murai</last></author>
      <pages>319–327</pages>
      <abstract>We introduce document retrieval and comment generation tasks for automating horizon scanning. This is an important task in the field of futurology that collects sufficient information for predicting drastic societal changes in the mid- or long-term future. The steps used are: 1) retrieving news articles that imply drastic changes, and 2) writing subjective comments on each article for others’ ease of understanding. As a first step in automating these tasks, we create a dataset that contains 2,266 manually collected news articles with comments written by experts. We analyze the collected documents and comments regarding characteristic words, the distance to general articles, and contents in the comments. Furthermore, we compare several methods for automating horizon scanning. Our experiments show that 1) manually collected articles are different from general articles regarding the words used and semantic distances, 2) the contents in the comment can be classified into several categories, and 3) a supervised model trained on our dataset achieves a better performance. The contributions are: 1) we propose document retrieval and comment generation tasks for horizon scanning, 2) create and analyze a new dataset, and 3) report the performance of several models and show that comment generation tasks are challenging.</abstract>
      <url hash="afaa0ce9">2022.lrec-1.34</url>
      <bibkey>ishigaki-etal-2022-automating</bibkey>
    </paper>
    <paper id="35">
      <title><fixed-case>V</fixed-case>i<fixed-case>H</fixed-case>ealth<fixed-case>BERT</fixed-case>: Pre-trained Language Models for <fixed-case>V</fixed-case>ietnamese in Health Text Mining</title>
      <author><first>Nguyen</first><last>Minh</last></author>
      <author><first>Vu Hoang</first><last>Tran</last></author>
      <author><first>Vu</first><last>Hoang</last></author>
      <author><first>Huy Duc</first><last>Ta</last></author>
      <author><first>Trung Huu</first><last>Bui</last></author>
      <author><first>Steven Quoc Hung</first><last>Truong</last></author>
      <pages>328–337</pages>
      <abstract>Pre-trained language models have become crucial to achieving competitive results across many Natural Language Processing (NLP) problems. For monolingual pre-trained models in low-resource languages, the quantity has been significantly increased. However, most of them relate to the general domain, and there are limited strong baseline language models for domain-specific. We introduce ViHealthBERT, the first domain-specific pre-trained language model for Vietnamese healthcare. The performance of our model shows strong results while outperforming the general domain language models in all health-related datasets. Moreover, we also present Vietnamese datasets for the healthcare domain for two tasks are Acronym Disambiguation (AD) and Frequently Asked Questions (FAQ) Summarization. We release our ViHealthBERT to facilitate future research and downstream application for Vietnamese NLP in domain-specific. Our dataset and code are available in https://github.com/demdecuong/vihealthbert.</abstract>
      <url hash="b98bb8eb">2022.lrec-1.35</url>
      <bibkey>minh-etal-2022-vihealthbert</bibkey>
      <pwccode url="https://github.com/demdecuong/vihealthbert" additional="false">demdecuong/vihealthbert</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/vimq">ViMQ</pwcdataset>
    </paper>
    <paper id="36">
      <title>Privacy-Preserving Graph Convolutional Networks for Text Classification</title>
      <author><first>Timour</first><last>Igamberdiev</last></author>
      <author><first>Ivan</first><last>Habernal</last></author>
      <pages>338–350</pages>
      <abstract>Graph convolutional networks (GCNs) are a powerful architecture for representation learning on documents that naturally occur as graphs, e.g., citation or social networks. However, sensitive personal information, such as documents with people’s profiles or relationships as edges, are prone to privacy leaks, as the trained model might reveal the original input. Although differential privacy (DP) offers a well-founded privacy-preserving framework, GCNs pose theoretical and practical challenges due to their training specifics. We address these challenges by adapting differentially-private gradient-based training to GCNs and conduct experiments using two optimizers on five NLP datasets in two languages. We propose a simple yet efficient method based on random graph splits that not only improves the baseline privacy bounds by a factor of 2.7 while retaining competitive F1 scores, but also provides strong privacy guarantees of epsilon = 1.0. We show that, under certain modeling choices, privacy-preserving GCNs perform up to 90% of their non-private variants, while formally guaranteeing strong privacy measures.</abstract>
      <url hash="4bf99e13">2022.lrec-1.36</url>
      <bibkey>igamberdiev-habernal-2022-privacy</bibkey>
      <pwccode url="https://github.com/trusthlt/privacy-preserving-gcn" additional="false">trusthlt/privacy-preserving-gcn</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/citeseer">Citeseer</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cora">Cora</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/pubmed">Pubmed</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/reddit">Reddit</pwcdataset>
    </paper>
    <paper id="37">
      <title><fixed-case>A</fixed-case>r<fixed-case>MATH</fixed-case>: a Dataset for Solving <fixed-case>A</fixed-case>rabic Math Word Problems</title>
      <author><first>Reem</first><last>Alghamdi</last></author>
      <author><first>Zhenwen</first><last>Liang</last></author>
      <author><first>Xiangliang</first><last>Zhang</last></author>
      <pages>351–362</pages>
      <abstract>This paper studies solving Arabic Math Word Problems by deep learning. A Math Word Problem (MWP) is a text description of a mathematical problem that can be solved by deriving a math equation to reach the answer. Effective models have been developed for solving MWPs in English and Chinese. However, Arabic MWPs are rarely studied. This paper contributes the first large-scale dataset for Arabic MWPs, which contains 6,000 samples of primary-school math problems, written in Modern Standard Arabic (MSA). Arabic MWP solvers are then built with deep learning models and evaluated on this dataset. In addition, a transfer learning model is built to let the high-resource Chinese MWP solver promote the performance of the low-resource Arabic MWP solver. This work is the first to use deep learning methods to solve Arabic MWP and the first to use transfer learning to solve MWP across different languages. The transfer learning enhanced solver has an accuracy of 74.15%, which is 3% higher than the solver without using transfer learning. We make the dataset and solvers available in public for encouraging more research of Arabic MWPs: https://github.com/reem-codes/ArMATH</abstract>
      <url hash="a218e90a">2022.lrec-1.37</url>
      <bibkey>alghamdi-etal-2022-armath</bibkey>
      <pwccode url="https://github.com/reem-codes/armath" additional="false">reem-codes/armath</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mawps">MAWPS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/math23k">Math23K</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mathqa">MathQA</pwcdataset>
    </paper>
    <paper id="38">
      <title><fixed-case>KIMERA</fixed-case>: Injecting Domain Knowledge into Vacant Transformer Heads</title>
      <author><first>Benjamin</first><last>Winter</last></author>
      <author><first>Alexei Figueroa</first><last>Rosero</last></author>
      <author><first>Alexander</first><last>Löser</last></author>
      <author><first>Felix Alexander</first><last>Gers</last></author>
      <author><first>Amy</first><last>Siu</last></author>
      <pages>363–373</pages>
      <abstract>Training transformer language models requires vast amounts of text and computational resources. This drastically limits the usage of these models in niche domains for which they are not optimized, or where domain-specific training data is scarce. We focus here on the clinical domain because of its limited access to training data in common tasks, while structured ontological data is often readily available. Recent observations in model compression of transformer models show optimization potential in improving the representation capacity of attention heads. We propose KIMERA (Knowledge Injection via Mask Enforced Retraining of Attention) for detecting, retraining and instilling attention heads with complementary structured domain knowledge. Our novel multi-task training scheme effectively identifies and targets individual attention heads that are least useful for a given downstream task and optimizes their representation with information from structured data. KIMERA generalizes well, thereby building the basis for an efficient fine-tuning. KIMERA achieves significant performance boosts on seven datasets in the medical domain in Information Retrieval and Clinical Outcome Prediction settings. We apply KIMERA to BERT-base to evaluate the extent of the domain transfer and also improve on the already strong results of BioBERT in the clinical domain.</abstract>
      <url hash="3342a32d">2022.lrec-1.38</url>
      <bibkey>winter-etal-2022-kimera</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mimic-iii">MIMIC-III</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/medquad">MedQuAD</pwcdataset>
    </paper>
    <paper id="39">
      <title>Distilling the Knowledge of <fixed-case>R</fixed-case>omanian <fixed-case>BERT</fixed-case>s Using Multiple Teachers</title>
      <author><first>Andrei-Marius</first><last>Avram</last></author>
      <author><first>Darius</first><last>Catrina</last></author>
      <author><first>Dumitru-Clementin</first><last>Cercel</last></author>
      <author><first>Mihai</first><last>Dascalu</last></author>
      <author><first>Traian</first><last>Rebedea</last></author>
      <author><first>Vasile</first><last>Pais</last></author>
      <author><first>Dan</first><last>Tufis</last></author>
      <pages>374–384</pages>
      <abstract>Running large-scale pre-trained language models in computationally constrained environments remains a challenging problem yet to be addressed, while transfer learning from these models has become prevalent in Natural Language Processing tasks. Several solutions, including knowledge distillation, network quantization, or network pruning have been previously proposed; however, these approaches focus mostly on the English language, thus widening the gap when considering low-resource languages. In this work, we introduce three light and fast versions of distilled BERT models for the Romanian language: Distil-BERT-base-ro, Distil-RoBERT-base, and DistilMulti-BERT-base-ro. The first two models resulted from the individual distillation of knowledge from two base versions of Romanian BERTs available in literature, while the last one was obtained by distilling their ensemble. To our knowledge, this is the first attempt to create publicly available Romanian distilled BERT models, which were thoroughly evaluated on five tasks: part-of-speech tagging, named entity recognition, sentiment analysis, semantic textual similarity, and dialect identification. Our experimental results argue that the three distilled models offer performance comparable to their teachers, while being twice as fast on a GPU and ~35% smaller. In addition, we further test the similarity between the predictions of our students versus their teachers by measuring their label and probability loyalty, together with regression loyalty - a new metric introduced in this work.</abstract>
      <url hash="881bcd6d">2022.lrec-1.39</url>
      <bibkey>avram-etal-2022-distilling</bibkey>
      <pwccode url="https://github.com/racai-ai/romanian-distilbert" additional="false">racai-ai/romanian-distilbert</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/moroco">MOROCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ronec">RONEC</pwcdataset>
    </paper>
    <paper id="40">
      <title>Personalized Filled-pause Generation with Group-wise Prediction Models</title>
      <author><first>Yuta</first><last>Matsunaga</last></author>
      <author><first>Takaaki</first><last>Saeki</last></author>
      <author><first>Shinnosuke</first><last>Takamichi</last></author>
      <author><first>Hiroshi</first><last>Saruwatari</last></author>
      <pages>385–392</pages>
      <abstract>In this paper, we propose a method to generate personalized filled pauses (FPs) with group-wise prediction models. Compared with fluent text generation, disfluent text generation has not been widely explored. To generate more human-like texts, we addressed disfluent text generation. The usage of disfluency, such as FPs, rephrases, and word fragments, differs from speaker to speaker, and thus, the generation of personalized FPs is required. However, it is difficult to predict them because of the sparsity of position and the frequency difference between more and less frequently used FPs. Moreover, it is sometimes difficult to adapt FP prediction models to each speaker because of the large variation of the tendency within each speaker. To address these issues, we propose a method to build group-dependent prediction models by grouping speakers on the basis of their tendency to use FPs. This method does not require a large amount of data and time to train each speaker model. We further introduce a loss function and a word embedding model suitable for FP prediction. Our experimental results demonstrate that group-dependent models can predict FPs with higher scores than a non-personalized one and the introduced loss function and word embedding model improve the prediction performance.</abstract>
      <url hash="685c0849">2022.lrec-1.40</url>
      <bibkey>matsunaga-etal-2022-personalized</bibkey>
      <pwccode url="https://github.com/ndkgit339/filledpause_prediction_group" additional="false">ndkgit339/filledpause_prediction_group</pwccode>
    </paper>
    <paper id="41">
      <title>Transformer versus <fixed-case>LSTM</fixed-case> Language Models trained on Uncertain <fixed-case>ASR</fixed-case> Hypotheses in Limited Data Scenarios</title>
      <author><first>Imran</first><last>Sheikh</last></author>
      <author><first>Emmanuel</first><last>Vincent</last></author>
      <author><first>Irina</first><last>Illina</last></author>
      <pages>393–399</pages>
      <abstract>In several ASR use cases, training and adaptation of domain-specific LMs can only rely on a small amount of manually verified text transcriptions and sometimes a limited amount of in-domain speech. Training of LSTM LMs in such limited data scenarios can benefit from alternate uncertain ASR hypotheses, as observed in our recent work. In this paper, we propose a method to train Transformer LMs on ASR confusion networks. We evaluate whether these self-attention based LMs are better at exploiting alternate ASR hypotheses as compared to LSTM LMs. Evaluation results show that Transformer LMs achieve 3-6% relative reduction in perplexity on the AMI scenario meetings but perform similar to LSTM LMs on the smaller Verbmobil conversational corpus. Evaluation on ASR N-best rescoring shows that LSTM and Transformer LMs trained on ASR confusion networks do not bring significant WER reductions. However, a qualitative analysis reveals that they are better at predicting less frequent words.</abstract>
      <url hash="9c357316">2022.lrec-1.41</url>
      <bibkey>sheikh-etal-2022-transformer</bibkey>
    </paper>
    <paper id="42">
      <title>Out of Thin Air: Is Zero-Shot Cross-Lingual Keyword Detection Better Than Unsupervised?</title>
      <author><first>Boshko</first><last>Koloski</last></author>
      <author><first>Senja</first><last>Pollak</last></author>
      <author><first>Blaž</first><last>Škrlj</last></author>
      <author><first>Matej</first><last>Martinc</last></author>
      <pages>400–409</pages>
      <abstract>Keyword extraction is the task of retrieving words that are essential to the content of a given document. Researchers proposed various approaches to tackle this problem. At the top-most level, approaches are divided into ones that require training - supervised and ones that do not - unsupervised. In this study, we are interested in settings, where for a language under investigation, no training data is available. More specifically, we explore whether pretrained multilingual language models can be employed for zero-shot cross-lingual keyword extraction on low-resource languages with limited or no available labeled training data and whether they outperform state-of-the-art unsupervised keyword extractors. The comparison is conducted on six news article datasets covering two high-resource languages, English and Russian, and four low-resource languages, Croatian, Estonian, Latvian, and Slovenian. We find that the pretrained models fine-tuned on a multilingual corpus covering languages that do not appear in the test set (i.e. in a zero-shot setting), consistently outscore unsupervised models in all six languages.</abstract>
      <url hash="4989d7b1">2022.lrec-1.42</url>
      <bibkey>koloski-etal-2022-thin</bibkey>
    </paper>
    <paper id="43">
      <title>Evaluating Pretraining Strategies for Clinical <fixed-case>BERT</fixed-case> Models</title>
      <author><first>Anastasios</first><last>Lamproudis</last></author>
      <author><first>Aron</first><last>Henriksson</last></author>
      <author><first>Hercules</first><last>Dalianis</last></author>
      <pages>410–416</pages>
      <abstract>Research suggests that using generic language models in specialized domains may be sub-optimal due to significant domain differences. As a result, various strategies for developing domain-specific language models have been proposed, including techniques for adapting an existing generic language model to the target domain, e.g. through various forms of vocabulary modifications and continued domain-adaptive pretraining with in-domain data. Here, an empirical investigation is carried out in which various strategies for adapting a generic language model to the clinical domain are compared to pretraining a pure clinical language model. Three clinical language models for Swedish, pretrained for up to ten epochs, are fine-tuned and evaluated on several downstream tasks in the clinical domain. A comparison of the language models’ downstream performance over the training epochs is conducted. The results show that the domain-specific language models outperform a general-domain language model; however, there is little difference in performance of the various clinical language models. However, compared to pretraining a pure clinical language model with only in-domain data, leveraging and adapting an existing general-domain language model requires fewer epochs of pretraining with in-domain data.</abstract>
      <url hash="5b4dc9c2">2022.lrec-1.43</url>
      <bibkey>lamproudis-etal-2022-evaluating</bibkey>
    </paper>
    <paper id="44">
      <title><fixed-case>K</fixed-case>az<fixed-case>NERD</fixed-case>: <fixed-case>K</fixed-case>azakh Named Entity Recognition Dataset</title>
      <author><first>Rustem</first><last>Yeshpanov</last></author>
      <author><first>Yerbolat</first><last>Khassanov</last></author>
      <author><first>Huseyin Atakan</first><last>Varol</last></author>
      <pages>417–426</pages>
      <abstract>We present the development of a dataset for Kazakh named entity recognition. The dataset was built as there is a clear need for publicly available annotated corpora in Kazakh, as well as annotation guidelines containing straightforward—but rigorous—rules and examples. The dataset annotation, based on the IOB2 scheme, was carried out on television news text by two native Kazakh speakers under the supervision of the first author. The resulting dataset contains 112,702 sentences and 136,333 annotations for 25 entity classes. State-of-the-art machine learning models to automatise Kazakh named entity recognition were also built, with the best-performing model achieving an exact match F1-score of 97.22% on the test set. The annotated dataset, guidelines, and codes used to train the models are freely available for download under the CC BY 4.0 licence from https://github.com/IS2AI/KazNERD.</abstract>
      <url hash="139fe160">2022.lrec-1.44</url>
      <bibkey>yeshpanov-etal-2022-kaznerd</bibkey>
      <pwccode url="https://github.com/is2ai/kaznerd" additional="false">is2ai/kaznerd</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/kaznerd">KazNERD</pwcdataset>
    </paper>
    <paper id="45">
      <title>Mitigating Dataset Artifacts in Natural Language Inference Through Automatic Contextual Data Augmentation and Learning Optimization</title>
      <author><first>Michail</first><last>Mersinias</last></author>
      <author><first>Panagiotis</first><last>Valvis</last></author>
      <pages>427–435</pages>
      <abstract>In recent years, natural language inference has been an emerging research area. In this paper, we present a novel data augmentation technique and combine it with a unique learning procedure for that task. Our so-called automatic contextual data augmentation (acda) method manages to be fully automatic, non-trivially contextual, and computationally efficient at the same time. When compared to established data augmentation methods, it is substantially more computationally efficient and requires no manual annotation by a human expert as they usually do. In order to increase its efficiency, we combine acda with two learning optimization techniques: contrastive learning and a hybrid loss function. The former maximizes the benefit of the supervisory signal generated by acda, while the latter incentivises the model to learn the nuances of the decision boundary. Our combined approach is shown experimentally to provide an effective way for mitigating spurious data correlations within a dataset, called dataset artifacts, and as a result improves performance. Specifically, our experiments verify that acda-boosted pre-trained language models that employ our learning optimization techniques, consistently outperform the respective fine-tuned baseline pre-trained language models across both benchmark datasets and adversarial examples.</abstract>
      <url hash="29b90ed6">2022.lrec-1.45</url>
      <bibkey>mersinias-valvis-2022-mitigating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="46">
      <title>Kompetencer: Fine-grained Skill Classification in <fixed-case>D</fixed-case>anish Job Postings via Distant Supervision and Transfer Learning</title>
      <author><first>Mike</first><last>Zhang</last></author>
      <author><first>Kristian Nørgaard</first><last>Jensen</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <pages>436–447</pages>
      <abstract>Skill Classification (SC) is the task of classifying job competences from job postings. This work is the first in SC applied to Danish job vacancy data. We release the first Danish job posting dataset: *Kompetencer* (_en_: competences), annotated for nested spans of competences. To improve upon coarse-grained annotations, we make use of The European Skills, Competences, Qualifications and Occupations (ESCO; le Vrang et al., (2014)) taxonomy API to obtain fine-grained labels via distant supervision. We study two setups: The zero-shot and few-shot classification setting. We fine-tune English-based models and RemBERT (Chung et al., 2020) and compare them to in-language Danish models. Our results show RemBERT significantly outperforms all other models in both the zero-shot and the few-shot setting.</abstract>
      <url hash="3bf5879a">2022.lrec-1.46</url>
      <bibkey>zhang-etal-2022-kompetencer</bibkey>
      <pwccode url="https://github.com/Kaleidophon/deep-significance" additional="true">Kaleidophon/deep-significance</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/kompetencer">Kompetencer</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/skillspan">SkillSpan</pwcdataset>
    </paper>
    <paper id="47">
      <title>Semantic Role Labelling for <fixed-case>D</fixed-case>utch Law Texts</title>
      <author><first>Roos</first><last>Bakker</last></author>
      <author><first>Romy A.N.</first><last>van Drie</last></author>
      <author><first>Maaike</first><last>de Boer</last></author>
      <author><first>Robert</first><last>van Doesburg</last></author>
      <author><first>Tom</first><last>van Engers</last></author>
      <pages>448–457</pages>
      <abstract>Legal texts are often difficult to interpret, and people who interpret them need to make choices about the interpretation. To improve transparency, the interpretation of a legal text can be made explicit by formalising it. However, creating formalised representations of legal texts manually is quite labour-intensive. In this paper, we describe a method to extract structured representations in the Flint language (van Doesburg and van Engers, 2019) from natural language. Automated extraction of knowledge representation not only makes the interpretation and modelling efforts more efficient, it also contributes to reducing inter-coder dependencies. The Flint language offers a formal model that enables the interpretation of legal text by describing the norms in these texts as acts, facts and duties. To extract the components of a Flint representation, we use a rule-based method and a transformer-based method. In the transformer-based method we fine-tune the last layer with annotated legal texts. The results show that the transformed-based method (80% accuracy) outperforms the rule-based method (42% accuracy) on the Dutch Aliens Act. This indicates that the transformer-based method is a promising approach of automatically extracting Flint frames.</abstract>
      <url hash="35c0fd27">2022.lrec-1.47</url>
      <bibkey>bakker-etal-2022-semantic</bibkey>
    </paper>
    <paper id="48">
      <title><fixed-case>E</fixed-case>nglish Language Spelling Correction as an Information Retrieval Task Using <fixed-case>W</fixed-case>ikipedia Search Statistics</title>
      <author><first>Kyle</first><last>Goslin</last></author>
      <author><first>Markus</first><last>Hofmann</last></author>
      <pages>458–464</pages>
      <abstract>Spelling correction utilities have become commonplace during the writing process, however, many spelling correction utilities suffer due to the size and quality of dictionaries available to aid correction. Many terms, acronyms, and morphological variations of terms are often missing, leaving potential spelling errors unidentified and potentially uncorrected. This research describes the implementation of WikiSpell, a dynamic spelling correction tool that relies on the Wikipedia dataset search API functionality as the sole source of knowledge to aid misspelled term identification and automatic replacement. Instead of a traditional matching process to select candidate replacement terms, the replacement process is treated as a natural language information retrieval process harnessing wildcard string matching and search result statistics. The aims of this research include: 1) the implementation of a spelling correction algorithm that utilizes the wildcard operators in the Wikipedia dataset search API, 2) a review of the current spell correction tools and approaches being utilized, and 3) testing and validation of the developed algorithm against the benchmark spelling correction tool, Hunspell. The key contribution of this research is a robust, dynamic information retrieval-based spelling correction algorithm that does not require prior training. Results of this research show that the proposed spelling correction algorithm, WikiSpell, achieved comparable results to an industry-standard spelling correction algorithm, Hunspell.</abstract>
      <url hash="ebb59f10">2022.lrec-1.48</url>
      <bibkey>goslin-hofmann-2022-english</bibkey>
    </paper>
    <paper id="49">
      <title><fixed-case>C</fixed-case>rude<fixed-case>O</fixed-case>il<fixed-case>N</fixed-case>ews: An Annotated Crude Oil News Corpus for Event Extraction</title>
      <author><first>Meisin</first><last>Lee</last></author>
      <author><first>Lay-Ki</first><last>Soon</last></author>
      <author><first>Eu Gene</first><last>Siew</last></author>
      <author><first>Ly Fie</first><last>Sugianto</last></author>
      <pages>465–479</pages>
      <abstract>In this paper, we present CrudeOilNews, a corpus of English Crude Oil news for event extraction. It is the first of its kind for Commodity News and serves to contribute towards resource building for economic and financial text mining. This paper describes the data collection process, the annotation methodology, and the event typology used in producing the corpus. Firstly, a seed set of 175 news articles were manually annotated, of which a subset of 25 news was used as the adjudicated reference test set for inter-annotator and system evaluation. The inter-annotator agreement was generally substantial, and annotator performance was adequate, indicating that the annotation scheme produces consistent event annotations of high quality. Subsequently, the dataset is expanded through (1) data augmentation and (2) Human-in-the-loop active learning. The resulting corpus has 425 news articles with approximately 11k events annotated. As part of the active learning process, the corpus was used to train basic event extraction models for machine labeling; the resulting models also serve as a validation or as a pilot study demonstrating the use of the corpus in machine learning purposes. The annotated corpus is made available for academic research purpose at https://github.com/meisin/CrudeOilNews-Corpus</abstract>
      <url hash="887619c3">2022.lrec-1.49</url>
      <bibkey>lee-etal-2022-crudeoilnews</bibkey>
      <pwccode url="https://github.com/meisin/crudeoilnews-corpus" additional="false">meisin/crudeoilnews-corpus</pwccode>
    </paper>
    <paper id="50">
      <title>Claim Extraction and Law Matching for <fixed-case>COVID</fixed-case>-19-related Legislation</title>
      <author><first>Niklas</first><last>Dehio</last></author>
      <author><first>Malte</first><last>Ostendorff</last></author>
      <author><first>Georg</first><last>Rehm</last></author>
      <pages>480–490</pages>
      <abstract>To cope with the COVID-19 pandemic, many jurisdictions have introduced new or altered existing legislation. Even though these new rules are often communicated to the public in news articles, it remains challenging for laypersons to learn about what is currently allowed or forbidden since news articles typically do not reference underlying laws. We investigate an automated approach to extract legal claims from news articles and to match the claims with their corresponding applicable laws. We examine the feasibility of the two tasks concerning claims about COVID-19-related laws from Berlin, Germany. For both tasks, we create and make publicly available the data sets and report the results of initial experiments. We obtain promising results with Transformer-based models that achieve 46.7 F1 for claim extraction and 91.4 F1 for law matching, albeit with some conceptual limitations. Furthermore, we discuss challenges of current machine learning approaches for legal language processing and their ability for complex legal reasoning tasks.</abstract>
      <url hash="8673cf6a">2022.lrec-1.50</url>
      <bibkey>dehio-etal-2022-claim</bibkey>
      <pwccode url="https://github.com/dfki-nlp/covid19-law-matching" additional="false">dfki-nlp/covid19-law-matching</pwccode>
    </paper>
    <paper id="51">
      <title>Constructing A Dataset of Support and Attack Relations in Legal Arguments in Court Judgements using Linguistic Rules</title>
      <author><first>Basit</first><last>Ali</last></author>
      <author><first>Sachin</first><last>Pawar</last></author>
      <author><first>Girish</first><last>Palshikar</last></author>
      <author><first>Rituraj</first><last>Singh</last></author>
      <pages>491–500</pages>
      <abstract>Argumentation mining is a growing area of research and has several interesting practical applications of mining legal arguments. Support and Attack relations are the backbone of any legal argument. However, there is no publicly available dataset of these relations in the context of legal arguments expressed in court judgements. In this paper, we focus on automatically constructing such a dataset of Support and Attack relations between sentences in a court judgment with reasonable accuracy. We propose three sets of rules based on linguistic knowledge and distant supervision to identify such relations from Indian Supreme Court judgments. The first rule set is based on multiple discourse connectors, the second rule set is based on common semantic structures between argumentative sentences in a close neighbourhood, and the third rule set uses the information about the source of the argument. We also explore a BERT-based sentence pair classification model which is trained on this dataset. We release the dataset of 20506 sentence pairs - 10746 Support (precision 77.3%) and 9760 Attack (precision 65.8%). We believe that this dataset and the ideas explored in designing the linguistic rules and will boost the argumentation mining research for legal arguments.</abstract>
      <url hash="93ca7dff">2022.lrec-1.51</url>
      <bibkey>ali-etal-2022-constructing</bibkey>
    </paper>
    <paper id="52">
      <title><fixed-case>KIND</fixed-case>: an <fixed-case>I</fixed-case>talian Multi-Domain Dataset for Named Entity Recognition</title>
      <author><first>Teresa</first><last>Paccosi</last></author>
      <author><first>Alessio</first><last>Palmero Aprosio</last></author>
      <pages>501–507</pages>
      <abstract>In this paper we present KIND, an Italian dataset for Named-entity recognition. It contains more than one million tokens with annotation covering three classes: person, location, and organization. The dataset (around 600K tokens) mostly contains manual gold annotations in three different domains (news, literature, and political discourses) and a semi-automatically annotated part. The multi-domain feature is the main strength of the present work, offering a resource which covers different styles and language uses, as well as the largest Italian NER dataset with manual gold annotations. It represents an important resource for the training of NER systems in Italian. Texts and annotations are freely downloadable from the Github repository.</abstract>
      <url hash="d02c581d">2022.lrec-1.52</url>
      <bibkey>paccosi-palmero-aprosio-2022-kind</bibkey>
      <pwccode url="https://github.com/dhfbk/kind" additional="false">dhfbk/kind</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/kind">KIND</pwcdataset>
    </paper>
    <paper id="53">
      <title><fixed-case>R</fixed-case>ussian Jeopardy! Data Set for Question-Answering Systems</title>
      <author><first>Elena</first><last>Mikhalkova</last></author>
      <author><first>Alexander A.</first><last>Khlyupin</last></author>
      <pages>508–514</pages>
      <abstract>Question answering (QA) is one of the most common NLP tasks that relates to named entity recognition, fact extraction, semantic search and some other fields. In industry, it is much valued in chat-bots and corporate information systems. It is also a challenging task that attracted the attention of a very general audience at the quiz show Jeopardy! In this article we describe a Jeopardy!-like Russian QA data set collected from the official Russian quiz database Ch-g-k. The data set includes 379,284 quiz-like questions with 29,375 from the Russian analogue of Jeopardy! (Own Game). We observe its linguistic features and the related QA-task. We conclude about perspectives of a QA challenge based on the collected data set.</abstract>
      <url hash="c4bd7f3e">2022.lrec-1.53</url>
      <bibkey>mikhalkova-khlyupin-2022-russian</bibkey>
      <pwccode url="https://github.com/evrog/russian-qa-jeopardy" additional="false">evrog/russian-qa-jeopardy</pwccode>
    </paper>
    <paper id="54">
      <title>Know Better – A Clickbait Resolving Challenge</title>
      <author><first>Benjamin</first><last>Hättasch</last></author>
      <author><first>Carsten</first><last>Binnig</last></author>
      <pages>515–523</pages>
      <abstract>In this paper, we present a new corpus of clickbait articles annotated by university students along with a corresponding shared task: clickbait articles use a headline or teaser that hides information from the reader to make them curious to open the article. We therefore propose to construct approaches that can automatically extract the relevant information from such an article, which we call clickbait resolving. We show why solving this task might be relevant for end users, and why clickbait can probably not be defeated with clickbait detection alone. Additionally, we argue that this task, although similar to question answering and some automatic summarization approaches, needs to be tackled with specialized models. We analyze the performance of some basic approaches on this task and show that models fine-tuned on our data can outperform general question answering models, while providing a systematic approach to evaluate the results. We hope that the data set and the task will help in giving users tools to counter clickbait in the future.</abstract>
      <url hash="b575355c">2022.lrec-1.54</url>
      <bibkey>hattasch-binnig-2022-know</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="55">
      <title>Valet: Rule-Based Information Extraction for Rapid Deployment</title>
      <author><first>Dayne</first><last>Freitag</last></author>
      <author><first>John</first><last>Cadigan</last></author>
      <author><first>Robert</first><last>Sasseen</last></author>
      <author><first>Paul</first><last>Kalmar</last></author>
      <pages>524–533</pages>
      <abstract>We present VALET, a framework for rule-based information extraction written in Python. VALET departs from legacy approaches predicated on cascading finite-state transducers, instead offering direct support for mixing heterogeneous information–lexical, orthographic, syntactic, corpus-analytic–in a succinct syntax that supports context-free idioms. We show how a handful of rules suffices to implement sophisticated matching, and describe a user interface that facilitates exploration for development and maintenance of rule sets. Arguing that rule-based information extraction is an important methodology early in the development cycle, we describe an experiment in which a VALET model is used to annotate examples for a machine learning extraction model. While learning to emulate the extraction rules, the resulting model generalizes them, recognizing valid extraction targets the rules failed to detect.</abstract>
      <url hash="c1026827">2022.lrec-1.55</url>
      <bibkey>freitag-etal-2022-valet</bibkey>
    </paper>
    <paper id="56">
      <title>Negation Detection in <fixed-case>D</fixed-case>utch Spoken Human-Computer Conversations</title>
      <author><first>Tom</first><last>Sweers</last></author>
      <author><first>Iris</first><last>Hendrickx</last></author>
      <author><first>Helmer</first><last>Strik</last></author>
      <pages>534–542</pages>
      <abstract>Proper recognition and interpretation of negation signals in text or communication is crucial for any form of full natural language understanding. It is also essential for computational approaches to natural language processing. In this study we focus on negation detection in Dutch spoken human-computer conversations. Since there exists no Dutch (dialogue) corpus annotated for negation we have annotated a Dutch corpus sample to evaluate our method for automatic negation detection. We use transfer learning and trained NegBERT (an existing BERT implementation used for negation detection) on English data with multilingual BERT to detect negation in Dutch dialogues. Our results show that adding in-domain training material improves the results. We show that we can detect both negation cues and scope in Dutch dialogues with high precision and recall. We provide a detailed error analysis and discuss the effects of cross-lingual and cross-domain transfer learning on automatic negation detection.</abstract>
      <url hash="a5f37ba0">2022.lrec-1.56</url>
      <bibkey>sweers-etal-2022-negation</bibkey>
    </paper>
    <paper id="57">
      <title>Reflections on 30 Years of Language Resource Development and Sharing</title>
      <author><first>Christopher</first><last>Cieri</last></author>
      <author><first>Mark</first><last>Liberman</last></author>
      <author><first>Sunghye</first><last>Cho</last></author>
      <author><first>Stephanie</first><last>Strassel</last></author>
      <author><first>James</first><last>Fiumara</last></author>
      <author><first>Jonathan</first><last>Wright</last></author>
      <pages>543–550</pages>
      <abstract>The Linguistic Data Consortium was founded in 1992 to solve the problem that limitations in access to shareable data was impeding progress in Human Language Technology research and development. At the time, DARPA had adopted the common task research management paradigm to impose additional rigor on their programs by also providing shared objectives, data and evaluation methods. Early successes underscored the promise of this paradigm but also the need for a standing infrastructure to host and distribute the shared data. During LDC’s initial five year grant, it became clear that the demand for linguistic data could not easily be met by the existing providers and that a dedicated data center could add capacity first for data collection and shortly thereafter for annotation. The expanding purview required expansions of LDC’s technical infrastructure including systems support and software development. An open question for the center would be its role in other kinds of research beyond data development. Over its 30 years history, LDC has performed multiple roles ranging from neutral, independent data provider to multisite programs, to creator of exploratory data in tight collaboration with system developers, to research group focused on data intensive investigations.</abstract>
      <url hash="9921e61a">2022.lrec-1.57</url>
      <bibkey>cieri-etal-2022-reflections</bibkey>
    </paper>
    <paper id="58">
      <title>Language Resources to Support Language Diversity – the <fixed-case>ELRA</fixed-case> Achievements</title>
      <author><first>Valérie</first><last>Mapelli</last></author>
      <author><first>Victoria</first><last>Arranz</last></author>
      <author><first>Khalid</first><last>Choukri</last></author>
      <author><first>Hélène</first><last>Mazo</last></author>
      <pages>551–558</pages>
      <abstract>This article highlights ELRA’s latest achievements in the field of Language Resources (LRs) identification, sharing and production. It also reports on ELRA’s involvement in several national and international projects, as well as in the organization of events for the support of LRs and related Language Technologies, including for under-resourced languages. Over the past few years, ELRA, together with its operational agency ELDA, has continued to increase its catalogue offer of LRs, establishing worldwide partnerships for the production of various types of LRs (SMS, tweets, crawled data, MT aligned data, speech LRs, sentiment-based data, etc.). Through their consistent involvement in EU-funded projects, ELRA and ELDA have contributed to improve the access to multilingual information in the context of the pandemic, develop tools for the de-identification of texts in the legal and medical domains, support the EU eTranslation Machine Translation system, and set up a European platform providing access to both resources and services. In December 2019, ELRA co-organized the LT4All conference, whose main topics were Language Technologies for enabling linguistic diversity and multilingualism worldwide. Moreover, although LREC was cancelled in 2020, ELRA published the LREC 2020 proceedings for the Main conference and Workshops papers, and carried on its dissemination activities while targeting the new LREC edition for 2022.</abstract>
      <url hash="3512df8a">2022.lrec-1.58</url>
      <bibkey>mapelli-etal-2022-language</bibkey>
    </paper>
    <paper id="59">
      <title>Ethical Issues in Language Resources and Language Technology – Tentative Categorisation</title>
      <author><first>Pawel</first><last>Kamocki</last></author>
      <author><first>Andreas</first><last>Witt</last></author>
      <pages>559–563</pages>
      <abstract>Ethical issues in Language Resources and Language Technology are often invoked, but rarely discussed. This is at least partly because little work has been done to systematize ethical issues and principles applicable in the fields of Language Resources and Language Technology. This paper provides an overview of ethical issues that arise at different stages of Language Resources and Language Technology development, from the conception phase through the construction phase to the use phase. Based on this overview, the authors propose a tentative taxonomy of ethical issues in Language Resources and Language Technology, built around five principles: Privacy, Property, Equality, Transparency and Freedom. The authors hope that this tentative taxonomy will facilitate ethical assessment of projects in the field of Language Resources and Language Technology, and structure the discussion on ethical issues in this domain, which may eventually lead to the adoption of a universally accepted Code of Ethics of the Language Resources and Language Technology community.</abstract>
      <url hash="9740f0e8">2022.lrec-1.59</url>
      <bibkey>kamocki-witt-2022-ethical</bibkey>
    </paper>
    <paper id="60">
      <title>Do we Name the Languages we Study? The #<fixed-case>B</fixed-case>ender<fixed-case>R</fixed-case>ule in <fixed-case>LREC</fixed-case> and <fixed-case>ACL</fixed-case> articles</title>
      <author><first>Fanny</first><last>Ducel</last></author>
      <author><first>Karën</first><last>Fort</last></author>
      <author><first>Gaël</first><last>Lejeune</last></author>
      <author><first>Yves</first><last>Lepage</last></author>
      <pages>564–573</pages>
      <abstract>This article studies the application of the #BenderRule in Natural Language Processing (NLP) articles according to two dimensions. Firstly, in a contrastive manner, by considering two major international conferences, LREC and ACL, and secondly, in a diachronic manner, by inspecting nearly 14,000 articles over a period of time ranging from 2000 to 2020 for LREC and from 1979 to 2020 for ACL. For this purpose, we created a corpus from LREC and ACL articles from the above-mentioned periods, from which we manually annotated nearly 1,000. We then developed two classifiers to automatically annotate the rest of the corpus. Our results show that LREC articles tend to respect the #BenderRule (80 to 90% of them respect it), whereas 30 to 40% of ACL articles do not. Interestingly, over the considered periods, the results appear to be stable for the two conferences, even though a rebound in ACL 2020 could be a sign of the influence of the blog post about the #BenderRule.</abstract>
      <url hash="03955e04">2022.lrec-1.60</url>
      <bibkey>ducel-etal-2022-name</bibkey>
    </paper>
    <paper id="61">
      <title>Aspect-Based Emotion Analysis and Multimodal Coreference: A Case Study of Customer Comments on Adidas <fixed-case>I</fixed-case>nstagram Posts</title>
      <author><first>Luna</first><last>De Bruyne</last></author>
      <author><first>Akbar</first><last>Karimi</last></author>
      <author><first>Orphee</first><last>De Clercq</last></author>
      <author><first>Andrea</first><last>Prati</last></author>
      <author><first>Veronique</first><last>Hoste</last></author>
      <pages>574–580</pages>
      <abstract>While aspect-based sentiment analysis of user-generated content has received a lot of attention in the past years, emotion detection at the aspect level has been relatively unexplored. Moreover, given the rise of more visual content on social media platforms, we want to meet the ever-growing share of multimodal content. In this paper, we present a multimodal dataset for Aspect-Based Emotion Analysis (ABEA). Additionally, we take the first steps in investigating the utility of multimodal coreference resolution in an ABEA framework. The presented dataset consists of 4,900 comments on 175 images and is annotated with aspect and emotion categories and the emotional dimensions of valence and arousal. Our preliminary experiments suggest that ABEA does not benefit from multimodal coreference resolution, and that aspect and emotion classification only requires textual information. However, when more specific information about the aspects is desired, image recognition could be essential.</abstract>
      <url hash="ec4cfb29">2022.lrec-1.61</url>
      <bibkey>de-bruyne-etal-2022-aspect</bibkey>
    </paper>
    <paper id="62">
      <title>Multi-source Multi-domain Sentiment Analysis with <fixed-case>BERT</fixed-case>-based Models</title>
      <author><first>Gabriel</first><last>Roccabruna</last></author>
      <author><first>Steve</first><last>Azzolin</last></author>
      <author><first>Giuseppe</first><last>Riccardi</last></author>
      <pages>581–589</pages>
      <abstract>Sentiment analysis is one of the most widely studied tasks in natural language processing. While BERT-based models have achieved state-of-the-art results in this task, little attention has been given to its performance variability across class labels, multi-source and multi-domain corpora. In this paper, we present an improved state-of-the-art and comparatively evaluate BERT-based models for sentiment analysis on Italian corpora. The proposed model is evaluated over eight sentiment analysis corpora from different domains (social media, finance, e-commerce, health, travel) and sources (Twitter, YouTube, Facebook, Amazon, Tripadvisor, Opera and Personal Healthcare Agent) on the prediction of positive, negative and neutral classes. Our findings suggest that BERT-based models are confident in predicting positive and negative examples but not as much with neutral examples. We release the sentiment analysis model as well as a newly financial domain sentiment corpus.</abstract>
      <url hash="cc707322">2022.lrec-1.62</url>
      <bibkey>roccabruna-etal-2022-multi</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="63">
      <title><fixed-case>N</fixed-case>aija<fixed-case>S</fixed-case>enti: A <fixed-case>N</fixed-case>igerian <fixed-case>T</fixed-case>witter Sentiment Corpus for Multilingual Sentiment Analysis</title>
      <author><first>Shamsuddeen Hassan</first><last>Muhammad</last></author>
      <author><first>David Ifeoluwa</first><last>Adelani</last></author>
      <author><first>Sebastian</first><last>Ruder</last></author>
      <author><first>Ibrahim Sa’id</first><last>Ahmad</last></author>
      <author><first>Idris</first><last>Abdulmumin</last></author>
      <author><first>Bello Shehu</first><last>Bello</last></author>
      <author><first>Monojit</first><last>Choudhury</last></author>
      <author><first>Chris Chinenye</first><last>Emezue</last></author>
      <author><first>Saheed Salahudeen</first><last>Abdullahi</last></author>
      <author><first>Anuoluwapo</first><last>Aremu</last></author>
      <author><first>Alípio</first><last>Jorge</last></author>
      <author><first>Pavel</first><last>Brazdil</last></author>
      <pages>590–602</pages>
      <abstract>Sentiment analysis is one of the most widely studied applications in NLP, but most work focuses on languages with large amounts of data. We introduce the first large-scale human-annotated Twitter sentiment dataset for the four most widely spoken languages in Nigeria—Hausa, Igbo, Nigerian-Pidgin, and Yorùbá—consisting of around 30,000 annotated tweets per language, including a significant fraction of code-mixed tweets. We propose text collection, filtering, processing and labeling methods that enable us to create datasets for these low-resource languages. We evaluate a range of pre-trained models and transfer strategies on the dataset. We find that language-specific models and language-adaptive fine-tuning generally perform best. We release the datasets, trained models, sentiment lexicons, and code to incentivize research on sentiment analysis in under-represented languages.</abstract>
      <url hash="9bd48a35">2022.lrec-1.63</url>
      <bibkey>muhammad-etal-2022-naijasenti</bibkey>
      <pwccode url="https://github.com/hausanlp/naijasenti" additional="true">hausanlp/naijasenti</pwccode>
    </paper>
    <paper id="64">
      <title>A (Psycho-)Linguistically Motivated Scheme for Annotating and Exploring Emotions in a Genre-Diverse Corpus</title>
      <author><first>Aline</first><last>Etienne</last></author>
      <author><first>Delphine</first><last>Battistelli</last></author>
      <author><first>Gwénolé</first><last>Lecorvé</last></author>
      <pages>603–612</pages>
      <abstract>This paper presents a scheme for emotion annotation and its manual application on a genre-diverse corpus of texts written in French. The methodology introduced here emphasizes the necessity of clarifying the main concepts implied by the analysis of emotions as they are expressed in texts, before conducting a manual annotation campaign. After explaining whatentails a deeply linguistic perspective on emotion expression modeling, we present a few NLP works that share some common points with this perspective and meticulously compare our approach with them. We then highlight some interesting quantitative results observed on our annotated corpus. The most notable interactions are on the one hand between emotion expression modes and genres of texts, and on the other hand between emotion expression modes and emotional categories. These observation corroborate and clarify some of the results already mentioned in other NLP works on emotion annotation.</abstract>
      <url hash="268547c8">2022.lrec-1.64</url>
      <bibkey>etienne-etal-2022-psycho</bibkey>
    </paper>
    <paper id="65">
      <title>Integrating a Phrase Structure Corpus Grammar and a Lexical-Semantic Network: the <fixed-case>HOLINET</fixed-case> Knowledge Graph</title>
      <author><first>Jean-Philippe</first><last>Prost</last></author>
      <pages>613–622</pages>
      <abstract>In this paper we address the question of how to integrate grammar and lexical-semantic knowledge within a single and homogeneous knowledge graph. We introduce a graph modelling of grammar knowledge which enables its merging with a lexical-semantic network. Such an integrated representation is expected, for instance, to provide new material for language-related graph embeddings in order to model interactions between Syntax and Semantics. Our base model relies on a phrase structure grammar. The phrase structure is accounted for by both a Proof-Theoretical representation, through a Context-Free Grammar, and a Model-Theoretical one, through a constraint-based grammar. The constraint types colour the grammar layer with syntactic relationships such as Immediate Dominance, Linear Precedence, and more. We detail a creation process which infers the grammar layer from a corpus annotated in constituency and integrates it with a lexical-semantic network through a shared POS tagset. We implement the process, and experiment with the French Treebank and the JeuxDeMots lexical-semantic network. The outcome is the HOLINET knowledge graph.</abstract>
      <url hash="7327888f">2022.lrec-1.65</url>
      <bibkey>prost-2022-integrating</bibkey>
    </paper>
    <paper id="66">
      <title>On the Impact of Temporal Representations on Metaphor Detection</title>
      <author><first>Giorgio</first><last>Ottolina</last></author>
      <author><first>Matteo Luigi</first><last>Palmonari</last></author>
      <author><first>Manuel</first><last>Vimercati</last></author>
      <author><first>Mehwish</first><last>Alam</last></author>
      <pages>623–632</pages>
      <abstract>State-of-the-art approaches for metaphor detection compare their literal - or core - meaning and their contextual meaning using metaphor classifiers based on neural networks. However, metaphorical expressions evolve over time due to various reasons, such as cultural and societal impact. Metaphorical expressions are known to co-evolve with language and literal word meanings, and even drive, to some extent, this evolution. This poses the question of whether different, possibly time-specific, representations of literal meanings may impact the metaphor detection task. To the best of our knowledge, this is the first study that examines the metaphor detection task with a detailed exploratory analysis where different temporal and static word embeddings are used to account for different representations of literal meanings. Our experimental analysis is based on three popular benchmarks used for metaphor detection and word embeddings extracted from different corpora and temporally aligned using different state-of-the-art approaches. The results suggest that the usage of different static word embedding methods does impact the metaphor detection task and some temporal word embeddings slightly outperform static methods. However, the results also suggest that temporal word embeddings may provide representations of the core meaning of the metaphor even too close to their contextual meaning, thus confusing the classifier. Overall, the interaction between temporal language evolution and metaphor detection appears tiny in the benchmark datasets used in our experiments. This suggests that future work for the computational analysis of this important linguistic phenomenon should first start by creating a new dataset where this interaction is better represented.</abstract>
      <url hash="50b0224b">2022.lrec-1.66</url>
      <bibkey>ottolina-etal-2022-impact</bibkey>
      <pwccode url="https://github.com/vinid/cade" additional="false">vinid/cade</pwccode>
    </paper>
    <paper id="67">
      <title>Analysis and Prediction of <fixed-case>NLP</fixed-case> Models via Task Embeddings</title>
      <author><first>Damien</first><last>Sileo</last></author>
      <author><first>Marie-Francine</first><last>Moens</last></author>
      <pages>633–647</pages>
      <abstract>Task embeddings are low-dimensional representations that are trained to capture task properties. In this paper, we propose MetaEval, a collection of 101 NLP tasks. We fit a single transformer to all MetaEval tasks jointly while conditioning it on learned embeddings. The resulting task embeddings enable a novel analysis of the space of tasks. We then show that task aspects can be mapped to task embeddings for new tasks without using any annotated examples. Predicted embeddings can modulate the encoder for zero-shot inference and outperform a zero-shot baseline on GLUE tasks. The provided multitask setup can function as a benchmark for future transfer learning research.</abstract>
      <url hash="92bd5846">2022.lrec-1.67</url>
      <bibkey>sileo-moens-2022-analysis</bibkey>
      <pwccode url="https://github.com/sileod/metaeval" additional="false">sileod/metaeval</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/metaeval">MetaEval</pwcdataset>
    </paper>
    <paper id="68">
      <title>Cross-lingual and Cross-domain Transfer Learning for Automatic Term Extraction from Low Resource Data</title>
      <author><first>Amir</first><last>Hazem</last></author>
      <author><first>Merieme</first><last>Bouhandi</last></author>
      <author><first>Florian</first><last>Boudin</last></author>
      <author><first>Beatrice</first><last>Daille</last></author>
      <pages>648–662</pages>
      <abstract>Automatic Term Extraction (ATE) is a key component for domain knowledge understanding and an important basis for further natural language processing applications. Even with persistent improvements, ATE still exhibits weak results exacerbated by small training data inherent to specialized domain corpora. Recently, transformers-based deep neural models, such as BERT, have proven to be efficient in many downstream NLP tasks. However, no systematic evaluation of ATE has been conducted so far. In this paper, we run an extensive study on fine-tuning pre-trained BERT models for ATE. We propose strategies that empirically show BERT’s effectiveness using cross-lingual and cross-domain transfer learning to extract single and multi-word terms. Experiments have been conducted on four specialized domains in three languages. The obtained results suggest that BERT can capture cross-domain and cross-lingual terminologically-marked contexts shared by terms, opening a new design-pattern for ATE.</abstract>
      <url hash="17683fcd">2022.lrec-1.68</url>
      <bibkey>hazem-etal-2022-cross</bibkey>
    </paper>
    <paper id="69">
      <title>Few-Shot Learning for Argument Aspects of the Nuclear Energy Debate</title>
      <author><first>Lena</first><last>Jurkschat</last></author>
      <author><first>Gregor</first><last>Wiedemann</last></author>
      <author><first>Maximilian</first><last>Heinrich</last></author>
      <author><first>Mattes</first><last>Ruckdeschel</last></author>
      <author><first>Sunna</first><last>Torge</last></author>
      <pages>663–672</pages>
      <abstract>We approach aspect-based argument mining as a supervised machine learning task to classify arguments into semantically coherent groups referring to the same defined aspect categories. As an exemplary use case, we introduce the Argument Aspect Corpus - Nuclear Energy that separates arguments about the topic of nuclear energy into nine major aspects. Since the collection of training data for further aspects and topics is costly, we investigate the potential for current transformer-based few-shot learning approaches to accurately classify argument aspects. The best approach is applied to a British newspaper corpus covering the debate on nuclear energy over the past 21 years. Our evaluation shows that a stable prediction of shares of argument aspects in this debate is feasible with 50 to 100 training samples per aspect. Moreover, we see signals for a clear shift in the public discourse in favor of nuclear energy in recent years. This revelation of changing patterns of pro and contra arguments related to certain aspects over time demonstrates the potential of supervised argument aspect detection for tracking issue-specific media discourses.</abstract>
      <url hash="a95bd635">2022.lrec-1.69</url>
      <bibkey>jurkschat-etal-2022-shot</bibkey>
      <pwccode url="https://github.com/leibniz-hbi/aac-ne_experiments" additional="false">leibniz-hbi/aac-ne_experiments</pwccode>
    </paper>
    <paper id="70">
      <title><fixed-case>M</fixed-case>u<fixed-case>LVE</fixed-case>, A Multi-Language Vocabulary Evaluation Data Set</title>
      <author><first>Anik</first><last>Jacobsen</last></author>
      <author><first>Salar</first><last>Mohtaj</last></author>
      <author><first>Sebastian</first><last>Möller</last></author>
      <pages>673–679</pages>
      <abstract>Vocabulary learning is vital to foreign language learning. Correct and adequate feedback is essential to successful and satisfying vocabulary training. However, many vocabulary and language evaluation systems perform on simple rules and do not account for real-life user learning data. This work introduces Multi-Language Vocabulary Evaluation Data Set (MuLVE), a data set consisting of vocabulary cards and real-life user answers, labeled indicating whether the user answer is correct or incorrect. The data source is user learning data from the Phase6 vocabulary trainer. The data set contains vocabulary questions in German and English, Spanish, and French as target language and is available in four different variations regarding pre-processing and deduplication. We experiment to fine-tune pre-trained BERT language models on the downstream task of vocabulary evaluation with the proposed MuLVE data set. The results provide outstanding results of &gt; 95.5 accuracy and F2-score. The data set is available on the European Language Grid.</abstract>
      <url hash="c73c2ead">2022.lrec-1.70</url>
      <bibkey>jacobsen-etal-2022-mulve</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mulve">MuLVE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paws">PAWS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paws-x">PAWS-X</pwcdataset>
    </paper>
    <paper id="71">
      <title><fixed-case>PLOD</fixed-case>: An Abbreviation Detection Dataset for Scientific Documents</title>
      <author><first>Leonardo</first><last>Zilio</last></author>
      <author><first>Hadeel</first><last>Saadany</last></author>
      <author><first>Prashant</first><last>Sharma</last></author>
      <author><first>Diptesh</first><last>Kanojia</last></author>
      <author><first>Constantin</first><last>Orăsan</last></author>
      <pages>680–688</pages>
      <abstract>The detection and extraction of abbreviations from unstructured texts can help to improve the performance of Natural Language Processing tasks, such as machine translation and information retrieval. However, in terms of publicly available datasets, there is not enough data for training deep-neural-networks-based models to the point of generalising well over data. This paper presents PLOD, a large-scale dataset for abbreviation detection and extraction that contains 160k+ segments automatically annotated with abbreviations and their long forms. We performed manual validation over a set of instances and a complete automatic validation for this dataset. We then used it to generate several baseline models for detecting abbreviations and long forms. The best models achieved an F1-score of 0.92 for abbreviations and 0.89 for detecting their corresponding long forms. We release this dataset along with our code and all the models publicly at https://github.com/surrey-nlp/PLOD-AbbreviationDetection</abstract>
      <url hash="43c499d1">2022.lrec-1.71</url>
      <bibkey>zilio-etal-2022-plod</bibkey>
      <pwccode url="https://github.com/surrey-nlp/PLOD-AbbreviationDetection" additional="false">surrey-nlp/PLOD-AbbreviationDetection</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/plod-filtered">PLOD-filtered</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/plod-an-abbreviation-detection-dataset-for">PLOD-unfiltered</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/acronym-identification">Acronym Identification</pwcdataset>
    </paper>
    <paper id="72">
      <title>Potential Idiomatic Expression (<fixed-case>PIE</fixed-case>)-<fixed-case>E</fixed-case>nglish: Corpus for Classes of Idioms</title>
      <author><first>Tosin</first><last>Adewumi</last></author>
      <author><first>Roshanak</first><last>Vadoodi</last></author>
      <author><first>Aparajita</first><last>Tripathy</last></author>
      <author><first>Konstantina</first><last>Nikolaido</last></author>
      <author><first>Foteini</first><last>Liwicki</last></author>
      <author><first>Marcus</first><last>Liwicki</last></author>
      <pages>689–696</pages>
      <abstract>We present a fairly large, Potential Idiomatic Expression (PIE) dataset for Natural Language Processing (NLP) in English. The challenges with NLP systems with regards to tasks such as Machine Translation (MT), word sense disambiguation (WSD) and information retrieval make it imperative to have a labelled idioms dataset with classes such as it is in this work. To the best of the authors’ knowledge, this is the first idioms corpus with classes of idioms beyond the literal and the general idioms classification. In particular, the following classes are labelled in the dataset: metaphor, simile, euphemism, parallelism, personification, oxymoron, paradox, hyperbole, irony and literal. We obtain an overall inter-annotator agreement (IAA) score, between two independent annotators, of 88.89%. Many past efforts have been limited in the corpus size and classes of samples but this dataset contains over 20,100 samples with almost 1,200 cases of idioms (with their meanings) from 10 classes (or senses). The corpus may also be extended by researchers to meet specific needs. The corpus has part of speech (PoS) tagging from the NLTK library. Classification experiments performed on the corpus to obtain a baseline and comparison among three common models, including the BERT model, give good results. We also make publicly available the corpus and the relevant codes for working with it for NLP tasks.</abstract>
      <url hash="0be6c47a">2022.lrec-1.72</url>
      <bibkey>adewumi-etal-2022-potential</bibkey>
      <pwccode url="https://github.com/tosingithub/idesk" additional="true">tosingithub/idesk</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/epie">EPIE</pwcdataset>
    </paper>
    <paper id="73">
      <title><fixed-case>L</fixed-case>e<fixed-case>S</fixed-case>pell - A Multi-Lingual Benchmark Corpus of Spelling Errors to Develop Spellchecking Methods for Learner Language</title>
      <author><first>Marie</first><last>Bexte</last></author>
      <author><first>Ronja</first><last>Laarmann-Quante</last></author>
      <author><first>Andrea</first><last>Horbach</last></author>
      <author><first>Torsten</first><last>Zesch</last></author>
      <pages>697–706</pages>
      <abstract>Spellchecking text written by language learners is especially challenging because errors made by learners differ both quantitatively and qualitatively from errors made by already proficient learners. We introduce LeSpell, a multi-lingual (English, German, Italian, and Czech) evaluation data set of spelling mistakes in context that we compiled from seven underlying learner corpora. Our experiments show that existing spellcheckers do not work well with learner data. Thus, we introduce a highly customizable spellchecking component for the DKPro architecture, which improves performance in many settings.</abstract>
      <url hash="3ec62d0c">2022.lrec-1.73</url>
      <bibkey>bexte-etal-2022-lespell</bibkey>
      <pwccode url="https://github.com/ltl-ude/ltl-spelling" additional="false">ltl-ude/ltl-spelling</pwccode>
    </paper>
    <paper id="74">
      <title>Subjective Text Complexity Assessment for <fixed-case>G</fixed-case>erman</title>
      <author><first>Laura</first><last>Seiffe</last></author>
      <author><first>Fares</first><last>Kallel</last></author>
      <author><first>Sebastian</first><last>Möller</last></author>
      <author><first>Babak</first><last>Naderi</last></author>
      <author><first>Roland</first><last>Roller</last></author>
      <pages>707–714</pages>
      <abstract>For different reasons, text can be difficult to read and understand for many people, especially if the text’s language is too complex. In order to provide suitable text for the target audience, it is necessary to measure its complexity. In this paper we describe subjective experiments to assess the readability of German text. We compile a new corpus of sentences provided by a German IT service provider. The sentences are annotated with the subjective complexity ratings by two groups of participants, namely experts and non-experts for that text domain. We then extract an extensive set of linguistically motivated features that are supposedly interacting with complexity perception. We show that a linear regression model with a subset of these features can be a very good predictor of text complexity.</abstract>
      <url hash="b51853ed">2022.lrec-1.74</url>
      <bibkey>seiffe-etal-2022-subjective</bibkey>
    </paper>
    <paper id="75">
      <title>Querying Interaction Structure: Approaches to Overlap in Spoken Language Corpora</title>
      <author><first>Elena</first><last>Frick</last></author>
      <author><first>Thomas</first><last>Schmidt</last></author>
      <author><first>Henrike</first><last>Helmer</last></author>
      <pages>715–722</pages>
      <abstract>In this paper, we address two problems in indexing and querying spoken language corpora with overlapping speaker contributions. First, we look into how token distance and token precedence can be measured when multiple primary data streams are available and when transcriptions happen to be tokenized, but are not synchronized with the sound at the level of individual tokens. We propose and experiment with a speaker-based search mode that enables any speaker’s transcription tier to be the basic tokenization layer whereby the contributions of other speakers are mapped to this given tier. Secondly, we address two distinct methods of how speaker overlaps can be captured in the TEI-based ISO Standard for Spoken Language Transcriptions (ISO 24624:2016) and how they can be queried by MTAS – an open source Lucene-based search engine for querying text with multilevel annotations. We illustrate the problems, introduce possible solutions and discuss their benefits and drawbacks.</abstract>
      <url hash="0294a726">2022.lrec-1.75</url>
      <bibkey>frick-etal-2022-querying</bibkey>
    </paper>
    <paper id="76">
      <title><fixed-case>D</fixed-case>ia<fixed-case>B</fixed-case>iz – an Annotated Corpus of <fixed-case>P</fixed-case>olish Call Center Dialogs</title>
      <author><first>Piotr</first><last>Pęzik</last></author>
      <author><first>Gosia</first><last>Krawentek</last></author>
      <author><first>Sylwia</first><last>Karasińska</last></author>
      <author><first>Paweł</first><last>Wilk</last></author>
      <author><first>Paulina</first><last>Rybińska</last></author>
      <author><first>Anna</first><last>Cichosz</last></author>
      <author><first>Angelika</first><last>Peljak-Łapińska</last></author>
      <author><first>Mikołaj</first><last>Deckert</last></author>
      <author><first>Michał</first><last>Adamczyk</last></author>
      <pages>723–726</pages>
      <abstract>This paper introduces DiaBiz, a large, annotated, multimodal corpus of Polish telephone conversations conducted in varied business settings, comprising 4036 call centre interactions from nine different domains, i.e. banking, energy services, telecommunications, insurance, medical care, debt collection, tourism, retail and car rental. The corpus was developed to boost the development of third-party speech recognition engines, dialog systems and conversational intelligence tools for Polish. Its current size amounts to nearly 410 hours of recordings and over 3 million words of transcribed speech. We present the structure of the corpus, data collection and transcription procedures, challenges of punctuating and truecasing speech transcripts, dialog structure annotation and discuss some of the ecological validity considerations involved in the development of such resources.</abstract>
      <url hash="7af08678">2022.lrec-1.76</url>
      <bibkey>pezik-etal-2022-diabiz</bibkey>
    </paper>
    <paper id="77">
      <title><fixed-case>L</fixed-case>a<fixed-case>VA</fixed-case> – <fixed-case>L</fixed-case>atvian Language Learner corpus</title>
      <author><first>Roberts</first><last>Darģis</last></author>
      <author><first>Ilze</first><last>Auziņa</last></author>
      <author><first>Inga</first><last>Kaija</last></author>
      <author><first>Kristīne</first><last>Levāne-Petrova</last></author>
      <author><first>Kristīne</first><last>Pokratniece</last></author>
      <pages>727–731</pages>
      <abstract>This paper presents the Latvian Language Learner Corpus (LaVA) developed at the Institute of Mathematics and Computer Science, University of Latvia. LaVA corpus contains 1015 essays (190k tokens and 790k characters excluding whitespaces) from foreigners studying at Latvian higher education institutions and who are learning Latvian as a foreign language in the first or second semester, reaching the A1 (possibly A2) Latvian language proficiency level. The corpus has morphological and error annotations. Error analysis and the statistics of the LaVA corpus are also provided in the paper. The corpus is publicly available at: http://www.korpuss.lv/id/LaVA.</abstract>
      <url hash="f0504b2e">2022.lrec-1.77</url>
      <bibkey>dargis-etal-2022-lava</bibkey>
    </paper>
    <paper id="78">
      <title>The <fixed-case>E</fixed-case>uro<fixed-case>P</fixed-case>at Corpus: A Parallel Corpus of <fixed-case>E</fixed-case>uropean Patent Data</title>
      <author><first>Kenneth</first><last>Heafield</last></author>
      <author><first>Elaine</first><last>Farrow</last></author>
      <author><first>Jelmer</first><last>van der Linde</last></author>
      <author><first>Gema</first><last>Ramírez-Sánchez</last></author>
      <author><first>Dion</first><last>Wiggins</last></author>
      <pages>732–740</pages>
      <abstract>We present the EuroPat corpus of patent-specific parallel data for 6 official European languages paired with English: German, Spanish, French, Croatian, Norwegian, and Polish. The filtered parallel corpora range in size from 51 million sentences (Spanish-English) to 154k sentences (Croatian-English), with the unfiltered (raw) corpora being up to 2 times larger. Access to clean, high quality, parallel data in technical domains such as science, engineering, and medicine is needed for training neural machine translation systems for tasks like online dispute resolution and eProcurement. Our evaluation found that the addition of EuroPat data to a generic baseline improved the performance of machine translation systems on in-domain test data in German, Spanish, French, and Polish; and in translating patent data from Croatian to English. The corpus has been released under Creative Commons Zero, and is expected to be widely useful for training high-quality machine translation systems, and particularly for those targeting technical documents such as patents and contracts.</abstract>
      <url hash="5adfc8e4">2022.lrec-1.78</url>
      <bibkey>heafield-etal-2022-europat</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/paracrawl">ParaCrawl</pwcdataset>
    </paper>
    <paper id="79">
      <title>“Beste Grüße, Maria Meyer” — Pseudonymization of Privacy-Sensitive Information in Emails</title>
      <author><first>Elisabeth</first><last>Eder</last></author>
      <author><first>Michael</first><last>Wiegand</last></author>
      <author><first>Ulrike</first><last>Krieg-Holz</last></author>
      <author><first>Udo</first><last>Hahn</last></author>
      <pages>741–752</pages>
      <abstract>The exploding amount of user-generated content has spurred NLP research to deal with documents from various digital communication formats (tweets, chats, emails, etc.). Using these texts as language resources implies complying with legal data privacy regulations. To protect the personal data of individuals and preclude their identification, we employ pseudonymization. More precisely, we identify those text spans that carry information revealing an individual’s identity (e.g., names of persons, locations, phone numbers, or dates) and subsequently substitute them with synthetically generated surrogates. Based on CodE Alltag, a German-language email corpus, we address two tasks. The first task is to evaluate various architectures for the automatic recognition of privacy-sensitive entities in raw data. The second task examines the applicability of pseudonymized data as training data for such systems since models learned on original data cannot be published for reasons of privacy protection. As outputs of both tasks, we, first, generate a new pseudonymized version of CodE Alltag compliant with the legal requirements of the General Data Protection Regulation (GDPR). Second, we make accessible a tagger for recognizing privacy-sensitive information in German emails and similar text genres, which is trained on already pseudonymized data.</abstract>
      <url hash="fcbea06e">2022.lrec-1.79</url>
      <bibkey>eder-etal-2022-beste</bibkey>
    </paper>
    <paper id="80">
      <title>Criteria for the Annotation of Implicit Stereotypes</title>
      <author><first>Wolfgang</first><last>Schmeisser-Nieto</last></author>
      <author><first>Montserrat</first><last>Nofre</last></author>
      <author><first>Mariona</first><last>Taulé</last></author>
      <pages>753–762</pages>
      <abstract>The growth of social media has brought with it a massive channel for spreading and reinforcing stereotypes. This issue becomes critical when the affected targets are minority groups such as women, the LGBT+ community and immigrants. Although from the perspective of computational linguistics, the detection of this kind of stereotypes is steadily improving, most stereotypes are expressed implicitly and identifying them automatically remains a challenge. One of the problems we found for tackling this issue is the lack of an operationalised definition of implicit stereotypes that would allow us to annotate consistently new corpora by characterising the different forms in which stereotypes appear. In this paper, we present thirteen criteria for annotating implicitness which were elaborated to facilitate the subjective task of identifying the presence of stereotypes. We also present NewsCom-Implicitness, a corpus of 1,911 sentences, of which 426 comprise explicit and implicit racial stereotypes. An experiment was carried out to evaluate the applicability of these criteria. The results indicate that different criteria obtain different inter-annotator agreement values and that there is a greater agreement when more criteria can be identified in one sentence.</abstract>
      <url hash="e04d5f16">2022.lrec-1.80</url>
      <bibkey>schmeisser-nieto-etal-2022-criteria</bibkey>
    </paper>
    <paper id="81">
      <title>Common Phone: A Multilingual Dataset for Robust Acoustic Modelling</title>
      <author><first>Philipp</first><last>Klumpp</last></author>
      <author><first>Tomas</first><last>Arias</last></author>
      <author><first>Paula Andrea</first><last>Pérez-Toro</last></author>
      <author><first>Elmar</first><last>Noeth</last></author>
      <author><first>Juan</first><last>Orozco-Arroyave</last></author>
      <pages>763–768</pages>
      <abstract>Current state of the art acoustic models can easily comprise more than 100 million parameters. This growing complexity demands larger training datasets to maintain a decent generalization of the final decision function. An ideal dataset is not necessarily large in size, but large with respect to the amount of unique speakers, utilized hardware and varying recording conditions. This enables a machine learning model to explore as much of the domain-specific input space as possible during parameter estimation. This work introduces Common Phone, a gender-balanced, multilingual corpus recorded from more than 76.000 contributors via Mozilla’s Common Voice project. It comprises around 116 hours of speech enriched with automatically generated phonetic segmentation. A Wav2Vec 2.0 acoustic model was trained with the Common Phone to perform phonetic symbol recognition and validate the quality of the generated phonetic annotation. The architecture achieved a PER of 18.1 % on the entire test set, computed with all 101 unique phonetic symbols, showing slight differences between the individual languages. We conclude that Common Phone provides sufficient variability and reliable phonetic annotation to help bridging the gap between research and application of acoustic models.</abstract>
      <url hash="701b0fae">2022.lrec-1.81</url>
      <bibkey>klumpp-etal-2022-common</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/common-phone">Common Phone</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/common-voice">Common Voice</pwcdataset>
    </paper>
    <paper id="82">
      <title>Curras + Baladi: Towards a <fixed-case>L</fixed-case>evantine Corpus</title>
      <author><first>Karim</first><last>Al-Haff</last></author>
      <author><first>Mustafa</first><last>Jarrar</last></author>
      <author><first>Tymaa</first><last>Hammouda</last></author>
      <author><first>Fadi</first><last>Zaraket</last></author>
      <pages>769–778</pages>
      <abstract>This paper presents two-fold contributions: a full revision of the Palestinian morphologically annotated corpus (Curras), and a newly annotated Lebanese corpus (Baladi). Both corpora can be used as a more general Levantine corpus. Baladi consists of around 9.6K morphologically annotated tokens. Each token was manually annotated with several morphological features and using LDC’s SAMA lemmas and tags. The inter-annotator evaluation on most features illustrates 78.5% Kappa and 90.1% F1-Score. Curras was revised by refining all annotations for accuracy, normalization and unification of POS tags, and linking with SAMA lemmas. This revision was also important to ensure that both corpora are compatible and can help to bridge the nuanced linguistic gaps that exist between the two highly mutually intelligible dialects. Both corpora are publicly available through a web portal.</abstract>
      <url hash="3f457364">2022.lrec-1.82</url>
      <bibkey>al-haff-etal-2022-curras</bibkey>
    </paper>
    <paper id="83">
      <title>Annotation Study of <fixed-case>J</fixed-case>apanese Judgments on Tort for Legal Judgment Prediction with Rationales</title>
      <author><first>Hiroaki</first><last>Yamada</last></author>
      <author><first>Takenobu</first><last>Tokunaga</last></author>
      <author><first>Ryutaro</first><last>Ohara</last></author>
      <author><first>Keisuke</first><last>Takeshita</last></author>
      <author><first>Mihoko</first><last>Sumida</last></author>
      <pages>779–790</pages>
      <abstract>This paper describes a comprehensive annotation study on Japanese judgment documents in civil cases. We aim to build an annotated corpus designed for Legal Judgment Prediction (LJP), especially for torts. Our annotation scheme contains annotations of whether tort is accepted by judges as well as its corresponding rationales for explainability purpose. Our annotation scheme extracts decisions and rationales at character-level. Moreover, the scheme can capture the explicit causal relation between judge’s decisions and their corresponding rationales, allowing multiple decisions in a document. To obtain high-quality annotation, we developed an annotation scheme with legal experts, and confirmed its reliability by agreement studies with Krippendorff’s alpha metric. The result of the annotation study suggests the proposed annotation scheme can produce a dataset of Japanese LJP at reasonable reliability.</abstract>
      <url hash="4bdb2cf8">2022.lrec-1.83</url>
      <bibkey>yamada-etal-2022-annotation</bibkey>
      <revision id="1" href="2022.lrec-1.83v1" hash="dade2fbf"/>
      <revision id="2" href="2022.lrec-1.83v2" hash="4bdb2cf8" date="2022-10-07">Author name correction.</revision>
      <pwcdataset url="https://paperswithcode.com/dataset/echr">ECHR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ecthr">ECtHR</pwcdataset>
    </paper>
    <paper id="84">
      <title>Placing <fixed-case>M</fixed-case>-Phasis on the Plurality of Hate: A Feature-Based Corpus of Hate Online</title>
      <author><first>Dana</first><last>Ruiter</last></author>
      <author><first>Liane</first><last>Reiners</last></author>
      <author><first>Ashwin Geet</first><last>D’Sa</last></author>
      <author><first>Thomas</first><last>Kleinbauer</last></author>
      <author><first>Dominique</first><last>Fohr</last></author>
      <author><first>Irina</first><last>Illina</last></author>
      <author><first>Dietrich</first><last>Klakow</last></author>
      <author><first>Christian</first><last>Schemer</last></author>
      <author><first>Angeliki</first><last>Monnier</last></author>
      <pages>791–804</pages>
      <abstract>Even though hate speech (HS) online has been an important object of research in the last decade, most HS-related corpora over-simplify the phenomenon of hate by attempting to label user comments as “hate” or “neutral”. This ignores the complex and subjective nature of HS, which limits the real-life applicability of classifiers trained on these corpora. In this study, we present the M-Phasis corpus, a corpus of ~9k German and French user comments collected from migration-related news articles. It goes beyond the “hate”-“neutral” dichotomy and is instead annotated with 23 features, which in combination become descriptors of various types of speech, ranging from critical comments to implicit and explicit expressions of hate. The annotations are performed by 4 native speakers per language and achieve high (0.77 &lt;= k &lt;= 1) inter-annotator agreements. Besides describing the corpus creation and presenting insights from a content, error and domain analysis, we explore its data characteristics by training several classification baselines.</abstract>
      <url hash="65813497">2022.lrec-1.84</url>
      <bibkey>ruiter-etal-2022-placing</bibkey>
      <pwccode url="https://github.com/uds-lsv/mphasis" additional="false">uds-lsv/mphasis</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/m-phasis">M-Phasis</pwcdataset>
    </paper>
    <paper id="85">
      <title><fixed-case>P</fixed-case>ar<fixed-case>C</fixed-case>or<fixed-case>F</fixed-case>ull2.0: a Parallel Corpus Annotated with Full Coreference</title>
      <author><first>Ekaterina</first><last>Lapshinova-Koltunski</last></author>
      <author><first>Pedro Augusto</first><last>Ferreira</last></author>
      <author><first>Elina</first><last>Lartaud</last></author>
      <author><first>Christian</first><last>Hardmeier</last></author>
      <pages>805–813</pages>
      <abstract>In this paper, we describe ParCorFull2.0, a parallel corpus annotated with full coreference chains for multiple languages, which is an extension of the existing corpus ParCorFull (Lapshinova-Koltunski et al., 2018). Similar to the previous version, this corpus has been created to address translation of coreference across languages, a phenomenon still challenging for machine translation (MT) and other multilingual natural language processing (NLP) applications. The current version of the corpus that we present here contains not only parallel texts for the language pair English-German, but also for English-French and English-Portuguese, which are all major European languages. The new language pairs belong to the Romance languages. The addition of a new language group creates a need of extension not only in terms of texts added, but also in terms of the annotation guidelines. Both French and Portuguese contain structures not found in English and German. Moreover, Portuguese is a pro-drop language bringing even more systemic differences in the realisation of coreference into our cross-lingual resources. These differences cause problems for multilingual coreference resolution and machine translation. Our parallel corpus with full annotation of coreference will be a valuable resource with a variety of uses not only for NLP applications, but also for contrastive linguists and researchers in translation studies.</abstract>
      <url hash="c5e94873">2022.lrec-1.85</url>
      <bibkey>lapshinova-koltunski-etal-2022-parcorfull2</bibkey>
    </paper>
    <paper id="86">
      <title>A Multi-Party Dialogue Ressource in <fixed-case>F</fixed-case>rench</title>
      <author><first>Maria</first><last>Boritchev</last></author>
      <author><first>Maxime</first><last>Amblard</last></author>
      <pages>814–823</pages>
      <abstract>We presentDialogues in Games(DinG), a corpus of manual transcriptions of real-life, oral, spontaneous multi-party dialogues between French-speaking players of the board game Catan. Our objective is to make available a quality resource for French, composed of long dialogues, to facilitate their study in the style of (Asher et al., 2016). In a general dialogue setting, participants share personal information, which makes it impossible to disseminate the resource freely and openly. In DinG, the attention of the participants is focused on the game, which prevents them from talking about themselves. In addition, we are conducting a study on the nature of the questions in dialogue, through annotation (Cruz Blandon et al., 2019), in order to develop more natural automatic dialogue systems</abstract>
      <url hash="65ef913e">2022.lrec-1.86</url>
      <bibkey>boritchev-amblard-2022-multi</bibkey>
    </paper>
    <paper id="87">
      <title>Bicleaner <fixed-case>AI</fixed-case>: Bicleaner Goes Neural</title>
      <author><first>Jaume</first><last>Zaragoza-Bernabeu</last></author>
      <author><first>Gema</first><last>Ramírez-Sánchez</last></author>
      <author><first>Marta</first><last>Bañón</last></author>
      <author><first>Sergio</first><last>Ortiz Rojas</last></author>
      <pages>824–831</pages>
      <abstract>This paper describes the experiments carried out during the development of the latest version of Bicleaner, named Bicleaner AI, a tool that aims at detecting noisy sentences in parallel corpora. The tool, which now implements a new neural classifier, uses state-of-the-art techniques based on pre-trained transformer-based language models fine-tuned on a binary classification task. After that, parallel corpus filtering is performed, discarding the sentences that have lower probability of being mutual translations. Our experiments, based on the training of neural machine translation (NMT) with corpora filtered using Bicleaner AI for two different scenarios, show significant improvements in translation quality compared to the previous version of the tool which implemented a classifier based on Extremely Randomized Trees.</abstract>
      <url hash="177c50a3">2022.lrec-1.87</url>
      <bibkey>zaragoza-bernabeu-etal-2022-bicleaner</bibkey>
      <pwccode url="https://github.com/bitextor/bicleaner-ai" additional="false">bitextor/bicleaner-ai</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/paracrawl">ParaCrawl</pwcdataset>
    </paper>
    <paper id="88">
      <title>Semi-automatically Annotated Learner Corpus for <fixed-case>R</fixed-case>ussian</title>
      <author><first>Anisia</first><last>Katinskaia</last></author>
      <author><first>Maria</first><last>Lebedeva</last></author>
      <author><first>Jue</first><last>Hou</last></author>
      <author><first>Roman</first><last>Yangarber</last></author>
      <pages>832–839</pages>
      <abstract>We present ReLCo— the Revita Learner Corpus—a new semi-automatically annotated learner corpus for Russian. The corpus was collected while several thousand L2 learners were performing exercises using the Revita language-learning system. All errors were detected automatically by the system and annotated by type. Part of the corpus was annotated manually—this part was created for further experiments on automatic assessment of grammatical correctness. The Learner Corpus provides valuable data for studying patterns of grammatical errors, experimenting with grammatical error detection and grammatical error correction, and developing new exercises for language learners. Automating the collection and annotation makes the process of building the learner corpus much cheaper and faster, in contrast to the traditional approach of building learner corpora. We make the data publicly available.</abstract>
      <url hash="93e9f953">2022.lrec-1.88</url>
      <bibkey>katinskaia-etal-2022-semi</bibkey>
    </paper>
    <paper id="89">
      <title><fixed-case>U</fixed-case>ni<fixed-case>M</fixed-case>orph 4.0: <fixed-case>U</fixed-case>niversal <fixed-case>M</fixed-case>orphology</title>
      <author><first>Khuyagbaatar</first><last>Batsuren</last></author>
      <author><first>Omer</first><last>Goldman</last></author>
      <author><first>Salam</first><last>Khalifa</last></author>
      <author><first>Nizar</first><last>Habash</last></author>
      <author><first>Witold</first><last>Kieraś</last></author>
      <author><first>Gábor</first><last>Bella</last></author>
      <author><first>Brian</first><last>Leonard</last></author>
      <author><first>Garrett</first><last>Nicolai</last></author>
      <author><first>Kyle</first><last>Gorman</last></author>
      <author><first>Yustinus Ghanggo</first><last>Ate</last></author>
      <author><first>Maria</first><last>Ryskina</last></author>
      <author><first>Sabrina</first><last>Mielke</last></author>
      <author><first>Elena</first><last>Budianskaya</last></author>
      <author><first>Charbel</first><last>El-Khaissi</last></author>
      <author><first>Tiago</first><last>Pimentel</last></author>
      <author><first>Michael</first><last>Gasser</last></author>
      <author><first>William Abbott</first><last>Lane</last></author>
      <author><first>Mohit</first><last>Raj</last></author>
      <author><first>Matt</first><last>Coler</last></author>
      <author><first>Jaime Rafael Montoya</first><last>Samame</last></author>
      <author><first>Delio Siticonatzi</first><last>Camaiteri</last></author>
      <author><first>Esaú Zumaeta</first><last>Rojas</last></author>
      <author><first>Didier</first><last>López Francis</last></author>
      <author><first>Arturo</first><last>Oncevay</last></author>
      <author><first>Juan</first><last>López Bautista</last></author>
      <author><first>Gema Celeste Silva</first><last>Villegas</last></author>
      <author><first>Lucas Torroba</first><last>Hennigen</last></author>
      <author><first>Adam</first><last>Ek</last></author>
      <author><first>David</first><last>Guriel</last></author>
      <author><first>Peter</first><last>Dirix</last></author>
      <author><first>Jean-Philippe</first><last>Bernardy</last></author>
      <author><first>Andrey</first><last>Scherbakov</last></author>
      <author><first>Aziyana</first><last>Bayyr-ool</last></author>
      <author><first>Antonios</first><last>Anastasopoulos</last></author>
      <author><first>Roberto</first><last>Zariquiey</last></author>
      <author><first>Karina</first><last>Sheifer</last></author>
      <author><first>Sofya</first><last>Ganieva</last></author>
      <author><first>Hilaria</first><last>Cruz</last></author>
      <author><first>Ritván</first><last>Karahóǧa</last></author>
      <author><first>Stella</first><last>Markantonatou</last></author>
      <author><first>George</first><last>Pavlidis</last></author>
      <author><first>Matvey</first><last>Plugaryov</last></author>
      <author><first>Elena</first><last>Klyachko</last></author>
      <author><first>Ali</first><last>Salehi</last></author>
      <author><first>Candy</first><last>Angulo</last></author>
      <author><first>Jatayu</first><last>Baxi</last></author>
      <author><first>Andrew</first><last>Krizhanovsky</last></author>
      <author><first>Natalia</first><last>Krizhanovskaya</last></author>
      <author><first>Elizabeth</first><last>Salesky</last></author>
      <author><first>Clara</first><last>Vania</last></author>
      <author><first>Sardana</first><last>Ivanova</last></author>
      <author><first>Jennifer</first><last>White</last></author>
      <author><first>Rowan Hall</first><last>Maudslay</last></author>
      <author><first>Josef</first><last>Valvoda</last></author>
      <author><first>Ran</first><last>Zmigrod</last></author>
      <author><first>Paula</first><last>Czarnowska</last></author>
      <author><first>Irene</first><last>Nikkarinen</last></author>
      <author><first>Aelita</first><last>Salchak</last></author>
      <author><first>Brijesh</first><last>Bhatt</last></author>
      <author><first>Christopher</first><last>Straughn</last></author>
      <author><first>Zoey</first><last>Liu</last></author>
      <author><first>Jonathan North</first><last>Washington</last></author>
      <author><first>Yuval</first><last>Pinter</last></author>
      <author><first>Duygu</first><last>Ataman</last></author>
      <author><first>Marcin</first><last>Wolinski</last></author>
      <author><first>Totok</first><last>Suhardijanto</last></author>
      <author><first>Anna</first><last>Yablonskaya</last></author>
      <author><first>Niklas</first><last>Stoehr</last></author>
      <author><first>Hossep</first><last>Dolatian</last></author>
      <author><first>Zahroh</first><last>Nuriah</last></author>
      <author><first>Shyam</first><last>Ratan</last></author>
      <author><first>Francis M.</first><last>Tyers</last></author>
      <author><first>Edoardo M.</first><last>Ponti</last></author>
      <author><first>Grant</first><last>Aiton</last></author>
      <author><first>Aryaman</first><last>Arora</last></author>
      <author><first>Richard J.</first><last>Hatcher</last></author>
      <author><first>Ritesh</first><last>Kumar</last></author>
      <author><first>Jeremiah</first><last>Young</last></author>
      <author><first>Daria</first><last>Rodionova</last></author>
      <author><first>Anastasia</first><last>Yemelina</last></author>
      <author><first>Taras</first><last>Andrushko</last></author>
      <author><first>Igor</first><last>Marchenko</last></author>
      <author><first>Polina</first><last>Mashkovtseva</last></author>
      <author><first>Alexandra</first><last>Serova</last></author>
      <author><first>Emily</first><last>Prud’hommeaux</last></author>
      <author><first>Maria</first><last>Nepomniashchaya</last></author>
      <author><first>Fausto</first><last>Giunchiglia</last></author>
      <author><first>Eleanor</first><last>Chodroff</last></author>
      <author><first>Mans</first><last>Hulden</last></author>
      <author><first>Miikka</first><last>Silfverberg</last></author>
      <author><first>Arya D.</first><last>McCarthy</last></author>
      <author><first>David</first><last>Yarowsky</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <author><first>Reut</first><last>Tsarfaty</last></author>
      <author><first>Ekaterina</first><last>Vylomova</last></author>
      <pages>840–855</pages>
      <abstract>The Universal Morphology (UniMorph) project is a collaborative effort providing broad-coverage instantiated normalized morphological inflection tables for hundreds of diverse world languages. The project comprises two major thrusts: a language-independent feature schema for rich morphological annotation, and a type-level resource of annotated data in diverse languages realizing that schema. This paper presents the expansions and improvements on several fronts that were made in the last couple of years (since McCarthy et al. (2020)). Collaborative efforts by numerous linguists have added 66 new languages, including 24 endangered languages. We have implemented several improvements to the extraction pipeline to tackle some issues, e.g., missing gender and macrons information. We have amended the schema to use a hierarchical structure that is needed for morphological phenomena like multiple-argument agreement and case stacking, while adding some missing morphological features to make the schema more inclusive.In light of the last UniMorph release, we also augmented the database with morpheme segmentation for 16 languages. Lastly, this new release makes a push towards inclusion of derivational morphology in UniMorph by enriching the data and annotation schema with instances representing derivational processes from MorphyNet.</abstract>
      <url hash="c54ff0be">2022.lrec-1.89</url>
      <bibkey>batsuren-etal-2022-unimorph</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/unimorph">UniMorph 4.0</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="90">
      <title>Textinator: an Internationalized Tool for Annotation and Human Evaluation in Natural Language Processing and Generation</title>
      <author><first>Dmytro</first><last>Kalpakchi</last></author>
      <author><first>Johan</first><last>Boye</last></author>
      <pages>856–866</pages>
      <abstract>We release an internationalized annotation and human evaluation bundle, called Textinator, along with documentation and video tutorials. Textinator allows annotating data for a wide variety of NLP tasks, and its user interface is offered in multiple languages, lowering the entry threshold for domain experts. The latter is, in fact, quite a rare feature among the annotation tools, that allows controlling for possible unintended biases introduced due to hiring only English-speaking annotators. We illustrate the rarity of this feature by presenting a thorough systematic comparison of Textinator to previously published annotation tools along 9 different axes (with internationalization being one of them). To encourage researchers to design their human evaluation before starting to annotate data, Textinator offers an easy-to-use tool for human evaluations allowing importing surveys with potentially hundreds of evaluation items in one click. We finish by presenting several use cases of annotation and evaluation projects conducted using pre-release versions of Textinator. The presented use cases do not represent Textinator’s full annotation or evaluation capabilities, and interested readers are referred to the online documentation for more information.</abstract>
      <url hash="c9a27757">2022.lrec-1.90</url>
      <bibkey>kalpakchi-boye-2022-textinator</bibkey>
    </paper>
    <paper id="91">
      <title><fixed-case>C</fixed-case>yber<fixed-case>A</fixed-case>gression<fixed-case>A</fixed-case>do-v1: a Dataset of Annotated Online Aggressions in <fixed-case>F</fixed-case>rench Collected through a Role-playing Game</title>
      <author><first>Anaïs</first><last>Ollagnier</last></author>
      <author><first>Elena</first><last>Cabrio</last></author>
      <author><first>Serena</first><last>Villata</last></author>
      <author><first>Catherine</first><last>Blaya</last></author>
      <pages>867–875</pages>
      <abstract>Over the past decades, the number of episodes of cyber aggression occurring online has grown substantially, especially among teens. Most solutions investigated by the NLP community to curb such online abusive behaviors consist of supervised approaches relying on annotated data extracted from social media. However, recent studies have highlighted that private instant messaging platforms are major mediums of cyber aggression among teens. As such interactions remain invisible due to the app privacy policies, very few datasets collecting aggressive conversations are available for the computational analysis of language. In order to overcome this limitation, in this paper we present the CyberAgressionAdo-V1 dataset, containing aggressive multiparty chats in French collected through a role-playing game in high-schools, and annotated at different layers. We describe the data collection and annotation phases, carried out in the context of a EU and a national research projects, and provide insightful analysis on the different types of aggression and verbal abuse depending on the targeted victims (individuals or communities) emerging from the collected data.</abstract>
      <url hash="fd9cd71d">2022.lrec-1.91</url>
      <bibkey>ollagnier-etal-2022-cyberagressionado</bibkey>
    </paper>
    <paper id="92">
      <title><fixed-case>F</fixed-case>innish Hate-Speech Detection on Social Media Using <fixed-case>CNN</fixed-case> and <fixed-case>F</fixed-case>in<fixed-case>BERT</fixed-case></title>
      <author><first>Md Saroar</first><last>Jahan</last></author>
      <author><first>Mourad</first><last>Oussalah</last></author>
      <author><first>Nabil</first><last>Arhab</last></author>
      <pages>876–882</pages>
      <abstract>There has been a lot of research in identifying hate posts from social media because of their detrimental effects on both individuals and society. The majority of this research has concentrated on English, although one notices the emergence of multilingual detection tools such as multilingual-BERT (mBERT). However, there is a lack of hate speech datasets compared to English, and a multilingual pre-trained model often contains fewer tokens for other languages. This paper attempts to contribute to hate speech identification in Finnish by constructing a new hate speech dataset that is collected from a popular forum (Suomi24). Furthermore, we have experimented with FinBERT pre-trained model performance for Finnish hate speech detection compared to state-of-the-art mBERT and other practices. In addition, we tested the performance of FinBERT compared to fastText as embedding, which employed with Convolution Neural Network (CNN). Our results showed that FinBERT yields a 91.7% accuracy and 90.8% F1 score value, which outperforms all state-of-art models, including multilingual-BERT and CNN.</abstract>
      <url hash="1bae774e">2022.lrec-1.92</url>
      <bibkey>jahan-etal-2022-finnish</bibkey>
    </paper>
    <paper id="93">
      <title>Empirical Analysis of Noising Scheme based Synthetic Data Generation for Automatic Post-editing</title>
      <author><first>Hyeonseok</first><last>Moon</last></author>
      <author><first>Chanjun</first><last>Park</last></author>
      <author><first>Seolhwa</first><last>Lee</last></author>
      <author><first>Jaehyung</first><last>Seo</last></author>
      <author><first>Jungseob</first><last>Lee</last></author>
      <author><first>Sugyeong</first><last>Eo</last></author>
      <author><first>Heuiseok</first><last>Lim</last></author>
      <pages>883–891</pages>
      <abstract>Automatic post-editing (APE) refers to a research field that aims to automatically correct errors included in the translation sentences derived by the machine translation system. This study has several limitations, considering the data acquisition, because there is no official dataset for most language pairs. Moreover, the amount of data is restricted even for language pairs in which official data has been released, such as WMT. To solve this problem and promote universal APE research regardless of APE data existence, this study proposes a method for automatically generating APE data based on a noising scheme from a parallel corpus. Particularly, we propose a human mimicking errors-based noising scheme that considers a practical correction process at the human level. We propose a precise inspection to attain high performance, and we derived the optimal noising schemes that show substantial effectiveness. Through these, we also demonstrate that depending on the type of noise, the noising scheme-based APE data generation may lead to inferior performance. In addition, we propose a dynamic noise injection strategy that enables the acquisition of a robust error correction capability and demonstrated its effectiveness by comparative analysis. This study enables obtaining a high performance APE model without human-generated data and can promote universal APE research for all language pairs targeting English.</abstract>
      <url hash="f1231d93">2022.lrec-1.93</url>
      <bibkey>moon-etal-2022-empirical</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/escape">eSCAPE</pwcdataset>
    </paper>
    <paper id="94">
      <title>Domain Mismatch Doesn’t Always Prevent Cross-lingual Transfer Learning</title>
      <author><first>Daniel</first><last>Edmiston</last></author>
      <author><first>Phillip</first><last>Keung</last></author>
      <author><first>Noah A.</first><last>Smith</last></author>
      <pages>892–899</pages>
      <abstract>Cross-lingual transfer learning without labeled target language data or parallel text has been surprisingly effective in zero-shot cross-lingual classification, question answering, unsupervised machine translation, etc. However, some recent publications have claimed that domain mismatch prevents cross-lingual transfer, and their results show that unsupervised bilingual lexicon induction (UBLI) and unsupervised neural machine translation (UNMT) do not work well when the underlying monolingual corpora come from different domains (e.g., French text from Wikipedia but English text from UN proceedings). In this work, we show how a simple initialization regimen can overcome much of the effect of domain mismatch in cross-lingual transfer. We pre-train word and contextual embeddings on the concatenated domain-mismatched corpora, and use these as initializations for three tasks: MUSE UBLI, UN Parallel UNMT, and the SemEval 2017 cross-lingual word similarity task. In all cases, our results challenge the conclusions of prior work by showing that proper initialization can recover a large portion of the losses incurred by domain mismatch.</abstract>
      <url hash="8c772d20">2022.lrec-1.94</url>
      <bibkey>edmiston-etal-2022-domain</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/europarl">Europarl</pwcdataset>
    </paper>
    <paper id="95">
      <title>Cross-Lingual Knowledge Transfer for Clinical Phenotyping</title>
      <author><first>Jens-Michalis</first><last>Papaioannou</last></author>
      <author><first>Paul</first><last>Grundmann</last></author>
      <author><first>Betty</first><last>van Aken</last></author>
      <author><first>Athanasios</first><last>Samaras</last></author>
      <author><first>Ilias</first><last>Kyparissidis</last></author>
      <author><first>George</first><last>Giannakoulas</last></author>
      <author><first>Felix</first><last>Gers</last></author>
      <author><first>Alexander</first><last>Loeser</last></author>
      <pages>900–909</pages>
      <abstract>Clinical phenotyping enables the automatic extraction of clinical conditions from patient records, which can be beneficial to doctors and clinics worldwide. However, current state-of-the-art models are mostly applicable to clinical notes written in English. We therefore investigate cross-lingual knowledge transfer strategies to execute this task for clinics that do not use the English language and have a small amount of in-domain data available. Our results reveal two strategies that outperform the state-of-the-art: Translation-based methods in combination with domain-specific encoders and cross-lingual encoders plus adapters. We find that these strategies perform especially well for classifying rare phenotypes and we advise on which method to prefer in which situation. Our results show that using multilingual data overall improves clinical phenotyping models and can compensate for data sparseness.</abstract>
      <url hash="a4fc76f8">2022.lrec-1.95</url>
      <bibkey>papaioannou-etal-2022-cross</bibkey>
      <pwccode url="https://github.com/neuron1682/cross-lingual-phenotype-prediction" additional="false">neuron1682/cross-lingual-phenotype-prediction</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mimic-iii">MIMIC-III</pwcdataset>
    </paper>
    <paper id="96">
      <title>The Multilingual Microblog Translation Corpus: Improving and Evaluating Translation of User-Generated Text</title>
      <author><first>Paul</first><last>McNamee</last></author>
      <author><first>Kevin</first><last>Duh</last></author>
      <pages>910–918</pages>
      <abstract>Translation of the noisy, informal language found in social media has been an understudied problem, with a principal factor being the limited availability of translation corpora in many languages. To address this need we have developed a new corpus containing over 200,000 translations of microblog posts that supports translation of thirteen languages into English. The languages are: Arabic, Chinese, Farsi, French, German, Hindi, Korean, Pashto, Portuguese, Russian, Spanish, Tagalog, and Urdu. We are releasing these data as the Multilingual Microblog Translation Corpus to support futher research in translation of informal language. We establish baselines using this new resource, and we further demonstrate the utility of the corpus by conducting experiments with fine-tuning to improve translation quality from a high performing neural machine translation (NMT) system. Fine-tuning provided substantial gains, ranging from +3.4 to +11.1 BLEU. On average, a relative gain of 21% was observed, demonstrating the utility of the corpus.</abstract>
      <url hash="858856a1">2022.lrec-1.96</url>
      <bibkey>mcnamee-duh-2022-multilingual</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/flores-101">FLORES-101</pwcdataset>
    </paper>
    <paper id="97">
      <title>Multilingual and Multimodal Learning for <fixed-case>B</fixed-case>razilian <fixed-case>P</fixed-case>ortuguese</title>
      <author><first>Júlia</first><last>Sato</last></author>
      <author><first>Helena</first><last>Caseli</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <pages>919–927</pages>
      <abstract>Humans constantly deal with multimodal information, that is, data from different modalities, such as texts and images. In order for machines to process information similarly to humans, they must be able to process multimodal data and understand the joint relationship between these modalities. This paper describes the work performed on the VTLM (Visual Translation Language Modelling) framework from (Caglayan et al., 2021) to test its generalization ability for other language pairs and corpora. We use the multimodal and multilingual corpus How2 (Sanabria et al., 2018) in three parallel streams with aligned English-Portuguese-Visual information to investigate the effectiveness of the model for this new language pair and in more complex scenarios, where the sentence associated with each image is not a simple description of it. Our experiments on the Portuguese-English multimodal translation task using the How2 dataset demonstrate the efficacy of cross-lingual visual pretraining. We achieved a BLEU score of 51.8 and a METEOR score of 78.0 on the test set, outperforming the MMT baseline by about 14 BLEU and 14 METEOR. The good BLEU and METEOR values obtained for this new language pair, regarding the original English-German VTLM, establish the suitability of the model to other languages.</abstract>
      <url hash="56815780">2022.lrec-1.97</url>
      <bibkey>sato-etal-2022-multilingual</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/how2">How2</pwcdataset>
    </paper>
    <paper id="98">
      <title><fixed-case>L</fixed-case>ibri<fixed-case>S</fixed-case>2<fixed-case>S</fixed-case>: A <fixed-case>G</fixed-case>erman-<fixed-case>E</fixed-case>nglish Speech-to-Speech Translation Corpus</title>
      <author><first>Pedro</first><last>Jeuris</last></author>
      <author><first>Jan</first><last>Niehues</last></author>
      <pages>928–935</pages>
      <abstract>Recently, we have seen an increasing interest in the area of speech-to-text translation. This has led to astonishing improvements in this area. In contrast, the activities in the area of speech-to-speech translation is still limited, although it is essential to overcome the language barrier. We believe that one of the limiting factors is the availability of appropriate training data. We address this issue by creating LibriS2S, to our knowledge the first publicly available speech-to-speech training corpus between German and English. For this corpus, we used independently created audio for German and English leading to an unbiased pronunciation of the text in both languages. This allows the creation of a new text-to-speech and speech-to-speech translation model that directly learns to generate the speech signal based on the pronunciation of the source language. Using this created corpus, we propose Text-to-Speech models based on the example of the recently proposed FastSpeech 2 model that integrates source language information. We do this by adapting the model to take information such as the pitch, energy or transcript from the source speech as additional input.</abstract>
      <url hash="176eef37">2022.lrec-1.98</url>
      <bibkey>jeuris-niehues-2022-libris2s</bibkey>
      <pwccode url="https://github.com/pedrodke/libris2s" additional="false">pedrodke/libris2s</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/libris2s">LibriS2S</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/librivoxdeen">LibriVoxDeEn</pwcdataset>
    </paper>
    <paper id="99">
      <title>A Linguistically Motivated Test Suite to Semi-Automatically Evaluate <fixed-case>G</fixed-case>erman–<fixed-case>E</fixed-case>nglish Machine Translation Output</title>
      <author><first>Vivien</first><last>Macketanz</last></author>
      <author><first>Eleftherios</first><last>Avramidis</last></author>
      <author><first>Aljoscha</first><last>Burchardt</last></author>
      <author><first>He</first><last>Wang</last></author>
      <author><first>Renlong</first><last>Ai</last></author>
      <author><first>Shushen</first><last>Manakhimova</last></author>
      <author><first>Ursula</first><last>Strohriegel</last></author>
      <author><first>Sebastian</first><last>Möller</last></author>
      <author><first>Hans</first><last>Uszkoreit</last></author>
      <pages>936–947</pages>
      <abstract>This paper presents a fine-grained test suite for the language pair German–English. The test suite is based on a number of linguistically motivated categories and phenomena and the semi-automatic evaluation is carried out with regular expressions. We describe the creation and implementation of the test suite in detail, providing a full list of all categories and phenomena. Furthermore, we present various exemplary applications of our test suite that have been implemented in the past years, like contributions to the Conference of Machine Translation, the usage of the test suite and MT outputs for quality estimation, and the expansion of the test suite to the language pair Portuguese–English. We describe how we tracked the development of the performance of various systems MT systems over the years with the help of the test suite and which categories and phenomena are prone to resulting in MT errors. For the first time, we also make a large part of our test suite publicly available to the research community.</abstract>
      <url hash="f47174f8">2022.lrec-1.99</url>
      <bibkey>macketanz-etal-2022-linguistically</bibkey>
      <pwccode url="https://github.com/dfki-nlp/mt-testsuite" additional="false">dfki-nlp/mt-testsuite</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2020">WMT 2020</pwcdataset>
    </paper>
    <paper id="100">
      <title>Cross-lingual Transfer of Monolingual Models</title>
      <author><first>Evangelia</first><last>Gogoulou</last></author>
      <author><first>Ariel</first><last>Ekgren</last></author>
      <author><first>Tim</first><last>Isbister</last></author>
      <author><first>Magnus</first><last>Sahlgren</last></author>
      <pages>948–955</pages>
      <abstract>Recent studies in cross-lingual learning using multilingual models have cast doubt on the previous hypothesis that shared vocabulary and joint pre-training are the keys to cross-lingual generalization. We introduce a method for transferring monolingual models to other languages through continuous pre-training and study the effects of such transfer from four different languages to English. Our experimental results on GLUE show that the transferred models outperform an English model trained from scratch, independently of the source language. After probing the model representations, we find that model knowledge from the source language enhances the learning of syntactic and semantic knowledge in English.</abstract>
      <url hash="1d743a04">2022.lrec-1.100</url>
      <bibkey>gogoulou-etal-2022-cross</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
    <paper id="101">
      <title>Dataset of Student Solutions to Algorithm and Data Structure Programming Assignments</title>
      <author><first>Fynn</first><last>Petersen-Frey</last></author>
      <author><first>Marcus</first><last>Soll</last></author>
      <author><first>Louis</first><last>Kobras</last></author>
      <author><first>Melf</first><last>Johannsen</last></author>
      <author><first>Peter</first><last>Kling</last></author>
      <author><first>Chris</first><last>Biemann</last></author>
      <pages>956–962</pages>
      <abstract>We present a dataset containing source code solutions to algorithmic programming exercises solved by hundreds of Bachelor-level students at the University of Hamburg. These solutions were collected during the winter semesters 2019/2020, 2020/2021 and 2021/2022. The dataset contains a set of solutions to a total of 21 tasks written in Java as well as Python and a total of over 1500 individual solutions. All solutions were submitted through Moodle and the Coderunner plugin and passed a number of test cases (including randomized tests), such that they can be considered as working correctly. All students whose solutions are included in the dataset gave their consent into publishing their solutions. The solutions are pseudonymized with a random solution ID. Included in this paper is a short analysis of the dataset containing statistical data and highlighting a few anomalies (e.g. the number of solutions per task decreases for the last few tasks due to grading rules). We plan to extend the dataset with tasks and solutions from upcoming courses.</abstract>
      <url hash="b322b01c">2022.lrec-1.101</url>
      <bibkey>petersen-frey-etal-2022-dataset</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/nlc2cmd">NLC2CMD</pwcdataset>
    </paper>
    <paper id="102">
      <title>Language Patterns and Behaviour of the Peer Supporters in Multilingual Healthcare Conversational Forums</title>
      <author><first>Ishani</first><last>Mondal</last></author>
      <author><first>Kalika</first><last>Bali</last></author>
      <author><first>Mohit</first><last>Jain</last></author>
      <author><first>Monojit</first><last>Choudhury</last></author>
      <author><first>Jacki</first><last>O’Neill</last></author>
      <author><first>Millicent</first><last>Ochieng</last></author>
      <author><first>Kagnoya</first><last>Awori</last></author>
      <author><first>Keshet</first><last>Ronen</last></author>
      <pages>963–975</pages>
      <abstract>In this work, we conduct a quantitative linguistic analysis of the language usage patterns of multilingual peer supporters in two health-focused WhatsApp groups in Kenya comprising of youth living with HIV. Even though the language of communication for the group was predominantly English, we observe frequent use of Kiswahili, Sheng and code-mixing among the three languages. We present an analysis of language choice and its accommodation, different functions of code-mixing, and relationship between sentiment and code-mixing. To explore the effectiveness of off-the-shelf Language Technologies (LT) in such situations, we attempt to build a sentiment analyzer for this dataset. Our experiments demonstrate the challenges of developing LT and therefore effective interventions for such forums and languages. We provide recommendations for language resources that should be built to address these challenges.</abstract>
      <url hash="c4922ac4">2022.lrec-1.102</url>
      <bibkey>mondal-etal-2022-language</bibkey>
    </paper>
    <paper id="103">
      <title>Frame Shift Prediction</title>
      <author><first>Zheng Xin</first><last>Yong</last></author>
      <author><first>Patrick D.</first><last>Watson</last></author>
      <author><first>Tiago</first><last>Timponi Torrent</last></author>
      <author><first>Oliver</first><last>Czulo</last></author>
      <author><first>Collin</first><last>Baker</last></author>
      <pages>976–986</pages>
      <abstract>Frame shift is a cross-linguistic phenomenon in translation which results in corresponding pairs of linguistic material evoking different frames. The ability to predict frame shifts would enable (semi-)automatic creation of multilingual frame annotations and thus speeding up FrameNet creation through annotation projection. Here, we first characterize how frame shifts result from other linguistic divergences such as translational divergences and construal differences. Our analysis also shows that many pairs of frames in frame shifts are multi-hop away from each other in Berkeley FrameNet’s net-like configuration. Then, we propose the Frame Shift Prediction task and demonstrate that our graph attention networks, combined with auxiliary training, can learn cross-linguistic frame-to-frame correspondence and predict frame shifts.</abstract>
      <url hash="5004d13c">2022.lrec-1.103</url>
      <bibkey>yong-etal-2022-frame</bibkey>
    </paper>
    <paper id="104">
      <title><fixed-case>CL</fixed-case>e<fixed-case>L</fixed-case>f<fixed-case>PC</fixed-case>: a Large Open Multi-Speaker Corpus of <fixed-case>F</fixed-case>rench Cued Speech</title>
      <author><first>Brigitte</first><last>Bigi</last></author>
      <author><first>Maryvonne</first><last>Zimmermann</last></author>
      <author><first>Carine</first><last>André</last></author>
      <pages>987–994</pages>
      <abstract>Cued Speech is a communication system developed for deaf people to complement speechreading at the phonetic level with hands. This visual communication mode uses handshapes in different placements near the face in combination with the mouth movements of speech to make the phonemes of spoken language look different from each other. This paper describes CLeLfPC - Corpus de Lecture en Langue française Parlée Complétée, a corpus of French Cued Speech. It consists in about 4 hours of audio and HD video recordings of 23 participants. The recordings are 160 different isolated ‘CV’ syllables repeated 5 times, 320 words or phrases repeated 2-3 times and about 350 sentences repeated 2-3 times. The corpus is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. It can be used for any further research or teaching purpose. The corpus includes orthographic transliteration and other phonetic annotations on 5 of the recorded topics, i.e. syllables, words, isolated sentences and a text. The early results are encouraging: it seems that 1/ the hand position has a high influence on the key audio duration; and 2/ the hand shape has not.</abstract>
      <url hash="50fe04c2">2022.lrec-1.104</url>
      <bibkey>bigi-etal-2022-clelfpc</bibkey>
    </paper>
    <paper id="105">
      <title>Samrómur Children: An <fixed-case>I</fixed-case>celandic Speech Corpus</title>
      <author><first>Carlos Daniel</first><last>Hernandez Mena</last></author>
      <author><first>David Erik</first><last>Mollberg</last></author>
      <author><first>Michal</first><last>Borský</last></author>
      <author><first>Jón</first><last>Guðnason</last></author>
      <pages>995–1002</pages>
      <abstract>Samrómur Children is an Icelandic speech corpus intended for the field of automatic speech recognition. It contains 131 hours of read speech from Icelandic children aged between 4 to 17 years. The test portion was meticulously selected to cover a wide range of ages as possible; we aimed to have exactly the same amount of data per age range. The speech was collected with the crowd-sourcing platform Samrómur.is, which is inspired on the “Mozilla’s Common Voice Project”. The corpus was developed within the framework of the “Language Technology Programme for Icelandic 2019 − 2023”; the goal of the project is to make Icelandic available in language-technology applications. Samrómur Children is the first corpus in Icelandic with children’s voices for public use under a Creative Commons license. Additionally, we present baseline experiments and results using Kaldi.</abstract>
      <url hash="338af98b">2022.lrec-1.105</url>
      <bibkey>hernandez-mena-etal-2022-samromur</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/common-voice">Common Voice</pwcdataset>
    </paper>
    <paper id="106">
      <title>The <fixed-case>N</fixed-case>orwegian Parliamentary Speech Corpus</title>
      <author><first>Per Erik</first><last>Solberg</last></author>
      <author><first>Pablo</first><last>Ortiz</last></author>
      <pages>1003–1008</pages>
      <abstract>The Norwegian Parliamentary Speech Corpus (NPSC) is a speech dataset with recordings of meetings from Stortinget, the Norwegian parliament. It is the first, publicly available dataset containing unscripted, Norwegian speech designed for training of automatic speech recognition (ASR) systems. The recordings are manually transcribed and annotated with language codes and speakers, and there are detailed metadata about the speakers. The transcriptions exist in both normalized and non-normalized form, and non-standardized words are explicitly marked and annotated with standardized equivalents. To test the usefulness of this dataset, we have compared an ASR system trained on the NPSC with a baseline system trained on only manuscript-read speech. These systems were tested on an independent dataset containing spontaneous, dialectal speech. The NPSC-trained system performed significantly better, with a 22.9% relative improvement in word error rate (WER). Moreover, training on the NPSC is shown to have a “democratizing” effects in terms of dialects, as improvements are generally larger for dialects with higher WER from the baseline system.</abstract>
      <url hash="7b3c78fa">2022.lrec-1.106</url>
      <bibkey>solberg-ortiz-2022-norwegian</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/npsc">NPSC</pwcdataset>
    </paper>
    <paper id="107">
      <title>A Speech Recognizer for <fixed-case>F</fixed-case>risian/<fixed-case>D</fixed-case>utch Council Meetings</title>
      <author><first>Martijn</first><last>Bentum</last></author>
      <author><first>Louis</first><last>ten Bosch</last></author>
      <author><first>Henk</first><last>van den Heuvel</last></author>
      <author><first>Simone</first><last>Wills</last></author>
      <author><first>Domenique</first><last>van der Niet</last></author>
      <author><first>Jelske</first><last>Dijkstra</last></author>
      <author><first>Hans</first><last>Van de Velde</last></author>
      <pages>1009–1015</pages>
      <abstract>We developed a bilingual Frisian/Dutch speech recognizer for council meetings in Fryslân (the Netherlands). During these meetings both Frisian and Dutch are spoken, and code switching between both languages shows up frequently. The new speech recognizer is based on an existing speech recognizer for Frisian and Dutch named FAME!, which was trained and tested on historical radio broadcasts. Adapting a speech recognizer for the council meeting domain is challenging because of acoustic background noise, speaker overlap and the jargon typically used in council meetings. To train the new recognizer, we used the radio broadcast materials utilized for the development of the FAME! recognizer and added newly created manually transcribed audio recordings of council meetings from eleven Frisian municipalities, the Frisian provincial council and the Frisian water board. The council meeting recordings consist of 49 hours of speech, with 26 hours of Frisian speech and 23 hours of Dutch speech. Furthermore, from the same sources, we obtained texts in the domain of council meetings containing 11 million words; 1.1 million Frisian words and 9.9 million Dutch words. We describe the methods used to train the new recognizer, report the observed word error rates, and perform an error analysis on remaining errors.</abstract>
      <url hash="b0b38c0e">2022.lrec-1.107</url>
      <bibkey>bentum-etal-2022-speech</bibkey>
    </paper>
    <paper id="108">
      <title>Elderly Conversational Speech Corpus with Cognitive Impairment Test and Pilot Dementia Detection Experiment Using Acoustic Characteristics of Speech in <fixed-case>J</fixed-case>apanese Dialects</title>
      <author><first>Meiko</first><last>Fukuda</last></author>
      <author><first>Ryota</first><last>Nishimura</last></author>
      <author><first>Maina</first><last>Umezawa</last></author>
      <author><first>Kazumasa</first><last>Yamamoto</last></author>
      <author><first>Yurie</first><last>Iribe</last></author>
      <author><first>Norihide</first><last>Kitaoka</last></author>
      <pages>1016–1022</pages>
      <abstract>There is a need for a simple method of detecting early signs of dementia which is not burdensome to patients, since early diagnosis and treatment can often slow the advance of the disease. Several studies have explored using only the acoustic and linguistic information of conversational speech as diagnostic material, with some success. To accelerate this research, we recorded natural conversations between 128 elderly people living in four different regions of Japan and interviewers, who also administered the Hasegawa’s Dementia Scale-Revised (HDS-R), a cognitive impairment test. Using our elderly speech corpus and dementia test results, we propose an SVM-based screening method which can detect dementia using the acoustic features of conversational speech even when regional dialects are present. We accomplish this by omitting some acoustic features, to limit the negative effect of differences between dialects. When using our proposed method, a dementia detection accuracy rate of about 91% was achieved for speakers from two regions. When speech from four regions was used in a second experiment, the discrimination rate fell to 76.6%, but this may have been due to using only sentence-level acoustic features in the second experiment, instead of sentence and phoneme-level features as in the previous experiment. This is an on-going research project, and additional investigation is needed to understand differences in the acoustic characteristics of phoneme units in the conversational speech collected from these four regions, to determine whether the removal of formants and other features can improve the dementia detection rate.</abstract>
      <url hash="b8c69fa7">2022.lrec-1.108</url>
      <bibkey>fukuda-etal-2022-elderly</bibkey>
    </paper>
    <paper id="109">
      <title>A Spoken Drug Prescription Dataset in <fixed-case>F</fixed-case>rench for Spoken Language Understanding</title>
      <author><first>Ali Can</first><last>Kocabiyikoglu</last></author>
      <author><first>François</first><last>Portet</last></author>
      <author><first>Prudence</first><last>Gibert</last></author>
      <author><first>Hervé</first><last>Blanchon</last></author>
      <author><first>Jean-Marc</first><last>Babouchkine</last></author>
      <author><first>Gaëtan</first><last>Gavazzi</last></author>
      <pages>1023–1031</pages>
      <abstract>Spoken medical dialogue systems are increasingly attracting interest to enhance access to healthcare services and improve quality and traceability of patient care. In this paper, we focus on medical drug prescriptions acquired on smartphones through spoken dialogue. Such systems would facilitate the traceability of care and would free the clinicians’ time. However, there is a lack of speech corpora to develop such systems since most of the related corpora are in text form and in English. To facilitate the research and development of spoken medical dialogue systems, we present, to the best of our knowledge, the first spoken medical drug prescriptions corpus, named PxNLU. It contains 4 hours of transcribed and annotated dialogues of drug prescriptions in French acquired through an experiment with 55 participants experts and non-experts in prescriptions. We also present some experiments that demonstrate the interest of this corpus for the evaluation and development of medical dialogue systems.</abstract>
      <url hash="916a1167">2022.lrec-1.109</url>
      <bibkey>kocabiyikoglu-etal-2022-spoken</bibkey>
    </paper>
    <paper id="110">
      <title>Towards an Open-Source <fixed-case>D</fixed-case>utch Speech Recognition System for the Healthcare Domain</title>
      <author><first>Cristian</first><last>Tejedor-García</last></author>
      <author><first>Berrie</first><last>van der Molen</last></author>
      <author><first>Henk</first><last>van den Heuvel</last></author>
      <author><first>Arjan</first><last>van Hessen</last></author>
      <author><first>Toine</first><last>Pieters</last></author>
      <pages>1032–1039</pages>
      <abstract>The current largest open-source generic automatic speech recognition (ASR) system for Dutch, Kaldi_NL, does not include a domain-specific healthcare jargon in the lexicon. Commercial alternatives (e.g., Google ASR system) are also not suitable for this purpose, not only because of the lexicon issue, but they do not safeguard privacy of sensitive data sufficiently and reliably. These reasons motivate that just a small amount of medical staff employs speech technology in the Netherlands. This paper proposes an innovative ASR training method developed within the Homo Medicinalis (HoMed) project. On the semantic level it specifically targets automatic transcription of doctor-patient consultation recordings with a focus on the use of medicines. In the first stage of HoMed, the Kaldi_NL language model (LM) is fine-tuned with lists of Dutch medical terms and transcriptions of Dutch online healthcare news bulletins. Despite the acoustic challenges and linguistic complexity of the domain, we reduced the word error rate (WER) by 5.2%. The proposed method could be employed for ASR domain adaptation to other domains with sensitive and special category data. These promising results allow us to apply this methodology on highly sensitive audiovisual recordings of patient consultations at the Netherlands Institute for Health Services Research (Nivel).</abstract>
      <url hash="76c63790">2022.lrec-1.110</url>
      <bibkey>tejedor-garcia-etal-2022-towards</bibkey>
    </paper>
    <paper id="111">
      <title>A Dataset for Speech Emotion Recognition in <fixed-case>G</fixed-case>reek Theatrical Plays</title>
      <author><first>Maria</first><last>Moutti</last></author>
      <author><first>Sofia</first><last>Eleftheriou</last></author>
      <author><first>Panagiotis</first><last>Koromilas</last></author>
      <author><first>Theodoros</first><last>Giannakopoulos</last></author>
      <pages>1040–1046</pages>
      <abstract>Machine learning methodologies can be adopted in cultural applications and propose new ways to distribute or even present the cultural content to the public. For instance, speech analytics can be adopted to automatically generate subtitles in theatrical plays, in order to (among other purposes) help people with hearing loss. Apart from a typical speech-to-text transcription with Automatic Speech Recognition (ASR), Speech Emotion Recognition (SER) can be used to automatically predict the underlying emotional content of speech dialogues in theatrical plays, and thus to provide a deeper understanding how the actors utter their lines. However, real-world datasets from theatrical plays are not available in the literature. In this work we present GreThE, the Greek Theatrical Emotion dataset, a new publicly available data collection for speech emotion recognition in Greek theatrical plays. The dataset contains utterances from various actors and plays, along with respective valence and arousal annotations. Towards this end, multiple annotators have been asked to provide their input for each speech recording and inter-annotator agreement is taken into account in the final ground truth generation. In addition, we discuss the results of some indicative experiments that have been conducted with machine and deep learning frameworks, using the dataset, along with some widely used databases in the field of speech emotion recognition.</abstract>
      <url hash="6b77875a">2022.lrec-1.111</url>
      <bibkey>moutti-etal-2022-dataset</bibkey>
      <pwccode url="https://github.com/magcil/grethe" additional="false">magcil/grethe</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/iemocap">IEMOCAP</pwcdataset>
    </paper>
    <paper id="112">
      <title>Audiobook Dialogues as Training Data for Conversational Style Synthetic Voices</title>
      <author><first>Liisi</first><last>Piits</last></author>
      <author><first>Hille</first><last>Pajupuu</last></author>
      <author><first>Heete</first><last>Sahkai</last></author>
      <author><first>Rene</first><last>Altrov</last></author>
      <author><first>Liis</first><last>Ermus</last></author>
      <author><first>Kairi</first><last>Tamuri</last></author>
      <author><first>Indrek</first><last>Hein</last></author>
      <author><first>Meelis</first><last>Mihkla</last></author>
      <author><first>Indrek</first><last>Kiissel</last></author>
      <author><first>Egert</first><last>Männisalu</last></author>
      <author><first>Kristjan</first><last>Suluste</last></author>
      <author><first>Jaan</first><last>Pajupuu</last></author>
      <pages>1047–1053</pages>
      <abstract>Synthetic voices are increasingly used in applications that require a conversational speaking style, raising the question as to which type of training data yields the most suitable speaking style for such applications. This study compares voices trained on three corpora of equal size recorded by the same speaker: an audiobook character speech (dialogue) corpus, an audiobook narrator speech corpus, and a neutral-style sentence-based corpus. The voices were trained with three text-to-speech synthesisers: two hidden Markov model-based synthesisers and a neural synthesiser. An evaluation study tested the suitability of their speaking style for use in customer service voice chatbots. Independently of the synthesiser used, the voices trained on the character speech corpus received the lowest, and those trained on the neutral-style corpus the highest scores. However, the evaluation results may have been confounded by the greater acoustic variability, less balanced sentence length distribution, and poorer phonemic coverage of the character speech corpus, especially compared to the neutral-style corpus. Therefore, the next step will be the creation of a more uniform, balanced, and representative audiobook dialogue corpus, and the evaluation of its suitability for further conversational-style applications besides customer service chatbots.</abstract>
      <url hash="aee066e8">2022.lrec-1.112</url>
      <bibkey>piits-etal-2022-audiobook</bibkey>
    </paper>
    <paper id="113">
      <title>Using a Knowledge Base to Automatically Annotate Speech Corpora and to Identify Sociolinguistic Variation</title>
      <author><first>Yaru</first><last>Wu</last></author>
      <author><first>Fabian</first><last>Suchanek</last></author>
      <author><first>Ioana</first><last>Vasilescu</last></author>
      <author><first>Lori</first><last>Lamel</last></author>
      <author><first>Martine</first><last>Adda-Decker</last></author>
      <pages>1054–1060</pages>
      <abstract>Speech characteristics vary from speaker to speaker. While some variation phenomena are due to the overall communication setting, others are due to diastratic factors such as gender, provenance, age, and social background. The analysis of these factors, although relevant for both linguistic and speech technology communities, is hampered by the need to annotate existing corpora or to recruit, categorise, and record volunteers as a function of targeted profiles. This paper presents a methodology that uses a knowledge base to provide speaker-specific information. This can facilitate the enrichment of existing corpora with new annotations extracted from the knowledge base. The method also helps the large scale analysis by automatically extracting instances of speech variation to correlate with diastratic features. We apply our method to an over 120-hour corpus of broadcast speech in French and investigate variation patterns linked to reduction phenomena and/or specific to connected speech such as disfluencies. We find significant differences in speech rate, the use of filler words, and the rate of non-canonical realisations of frequent segments as a function of different professional categories and age groups.</abstract>
      <url hash="362c16f0">2022.lrec-1.113</url>
      <bibkey>wu-etal-2022-using</bibkey>
    </paper>
    <paper id="114">
      <title>Phone Inventories and Recognition for Every Language</title>
      <author><first>Xinjian</first><last>Li</last></author>
      <author><first>Florian</first><last>Metze</last></author>
      <author><first>David R.</first><last>Mortensen</last></author>
      <author><first>Alan W</first><last>Black</last></author>
      <author><first>Shinji</first><last>Watanabe</last></author>
      <pages>1061–1067</pages>
      <abstract>Identifying phone inventories is a crucial component in language documentation and the preservation of endangered languages. However, even the largest collection of phone inventory only covers about 2000 languages, which is only 1/4 of the total number of languages in the world. A majority of the remaining languages are endangered. In this work, we attempt to solve this problem by estimating the phone inventory for any language listed in Glottolog, which contains phylogenetic information regarding 8000 languages. In particular, we propose one probabilistic model and one non-probabilistic model, both using phylogenetic trees (“language family trees”) to measure the distance between languages. We show that our best model outperforms baseline models by 6.5 F1. Furthermore, we demonstrate that, with the proposed inventories, the phone recognition model can be customized for every language in the set, which improved the PER (phone error rate) in phone recognition by 25%.</abstract>
      <url hash="ada79042">2022.lrec-1.114</url>
      <bibkey>li-etal-2022-phone</bibkey>
    </paper>
    <paper id="115">
      <title>Constructing Parallel Corpora from <fixed-case>COVID</fixed-case>-19 News using <fixed-case>M</fixed-case>edi<fixed-case>S</fixed-case>ys Metadata</title>
      <author><first>Dimitrios</first><last>Roussis</last></author>
      <author><first>Vassilis</first><last>Papavassiliou</last></author>
      <author><first>Sokratis</first><last>Sofianopoulos</last></author>
      <author><first>Prokopis</first><last>Prokopidis</last></author>
      <author><first>Stelios</first><last>Piperidis</last></author>
      <pages>1068–1072</pages>
      <abstract>This paper presents a collection of parallel corpora generated by exploiting the COVID-19 related dataset of metadata created with the Europe Media Monitor (EMM) / Medical Information System (MediSys) processing chain of news articles. We describe how we constructed comparable monolingual corpora of news articles related to the current pandemic and used them to mine about 11.2 million segment alignments in 26 EN-X language pairs, covering most official EU languages plus Albanian, Arabic, Icelandic, Macedonian, and Norwegian. Subsets of this collection have been used in shared tasks (e.g. Multilingual Semantic Search, Machine Translation) aimed at accelerating the creation of resources and tools needed to facilitate access to information in the COVID-19 emergency situation.</abstract>
      <url hash="d8b5f31e">2022.lrec-1.115</url>
      <bibkey>roussis-etal-2022-constructing</bibkey>
    </paper>
    <paper id="116">
      <title>A Distant Supervision Corpus for Extracting Biomedical Relationships Between Chemicals, Diseases and Genes</title>
      <author><first>Dongxu</first><last>Zhang</last></author>
      <author><first>Sunil</first><last>Mohan</last></author>
      <author><first>Michaela</first><last>Torkar</last></author>
      <author><first>Andrew</first><last>McCallum</last></author>
      <pages>1073–1082</pages>
      <abstract>We introduce ChemDisGene, a new dataset for training and evaluating multi-class multi-label biomedical relation extraction models. Our dataset contains 80k biomedical research abstracts labeled with mentions of chemicals, diseases, and genes, portions of which human experts labeled with 18 types of biomedical relationships between these entities (intended for evaluation), and the remainder of which (intended for training) has been distantly labeled via the CTD database with approximately 78% accuracy. In comparison to similar preexisting datasets, ours is both substantially larger and cleaner; it also includes annotations linking mentions to their entities. We also provide three baseline deep neural network relation extraction models trained and evaluated on our new dataset.</abstract>
      <url hash="9a529d9d">2022.lrec-1.116</url>
      <bibkey>zhang-etal-2022-distant</bibkey>
      <pwccode url="https://github.com/chanzuckerberg/chemdisgene" additional="false">chanzuckerberg/chemdisgene</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/chemdisgene">ChemDisGene</pwcdataset>
    </paper>
    <paper id="117">
      <title><fixed-case>D</fixed-case>rug<fixed-case>EHRQA</fixed-case>: A Question Answering Dataset on Structured and Unstructured Electronic Health Records For Medicine Related Queries</title>
      <author><first>Jayetri</first><last>Bardhan</last></author>
      <author><first>Anthony</first><last>Colas</last></author>
      <author><first>Kirk</first><last>Roberts</last></author>
      <author><first>Daisy Zhe</first><last>Wang</last></author>
      <pages>1083–1097</pages>
      <abstract>This paper develops the first question answering dataset (DrugEHRQA) containing question-answer pairs from both structured tables and unstructured notes from a publicly available Electronic Health Record (EHR). EHRs contain patient records, stored in structured tables and unstructured clinical notes. The information in structured and unstructured EHRs is not strictly disjoint: information may be duplicated, contradictory, or provide additional context between these sources. Our dataset has medication-related queries, containing over 70,000 question-answer pairs. To provide a baseline model and help analyze the dataset, we have used a simple model (MultimodalEHRQA) which uses the predictions of a modality selection network to choose between EHR tables and clinical notes to answer the questions. This is used to direct the questions to the table-based or text-based state-of-the-art QA model. In order to address the problem arising from complex, nested queries, this is the first time Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers (RAT-SQL) has been used to test the structure of query templates in EHR data. Our goal is to provide a benchmark dataset for multi-modal QA systems, and to open up new avenues of research in improving question answering over EHR structured data by using context from unstructured clinical data.</abstract>
      <url hash="f5cc6612">2022.lrec-1.117</url>
      <bibkey>bardhan-etal-2022-drugehrqa</bibkey>
      <pwccode url="https://github.com/jayetri/DrugEHRQA-A-Question-Answering-Dataset-on-Structured-and-Unstructured-Electronic-Health-Records" additional="false">jayetri/DrugEHRQA-A-Question-Answering-Dataset-on-Structured-and-Unstructured-Electronic-Health-Records</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/drugehrqa">DrugEHRQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/clicr">CliCR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mimic-iii">MIMIC-III</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/emrqa">emrQA</pwcdataset>
    </paper>
    <paper id="118">
      <title>Efficiently and Thoroughly Anonymizing a Transformer Language Model for <fixed-case>D</fixed-case>utch Electronic Health Records: a Two-Step Method</title>
      <author><first>Stella</first><last>Verkijk</last></author>
      <author><first>Piek</first><last>Vossen</last></author>
      <pages>1098–1103</pages>
      <abstract>Neural Network (NN) architectures are used more and more to model large amounts of data, such as text data available online. Transformer-based NN architectures have shown to be very useful for language modelling. Although many researchers study how such Language Models (LMs) work, not much attention has been paid to the privacy risks of training LMs on large amounts of data and publishing them online. This paper presents a new method for anonymizing a language model by presenting the way in which MedRoBERTa.nl, a Dutch language model for hospital notes, was anonymized. The two-step method involves i) automatic anonymization of the training data and ii) semi-automatic anonymization of the LM’s vocabulary. Adopting the fill-mask task where the model predicts what tokens are most probable in a certain context, it was tested how often the model will predict a name in a context where a name should be. It was shown that it predicts a name-like token 0.2% of the time. Any name-like token that was predicted was never the name originally present in the training data. By explaining how a LM trained on highly private real-world medical data can be published, we hope that more language resources will be published openly and responsibly so the scientific community can profit from them.</abstract>
      <url hash="9705200d">2022.lrec-1.118</url>
      <bibkey>verkijk-vossen-2022-efficiently</bibkey>
    </paper>
    <paper id="119">
      <title><fixed-case>BERT</fixed-case>rade: Using Contextual Embeddings to Parse <fixed-case>O</fixed-case>ld <fixed-case>F</fixed-case>rench</title>
      <author><first>Loïc</first><last>Grobol</last></author>
      <author><first>Mathilde</first><last>Regnault</last></author>
      <author><first>Pedro</first><last>Ortiz Suarez</last></author>
      <author><first>Benoît</first><last>Sagot</last></author>
      <author><first>Laurent</first><last>Romary</last></author>
      <author><first>Benoit</first><last>Crabbé</last></author>
      <pages>1104–1113</pages>
      <abstract>The successes of contextual word embeddings learned by training large-scale language models, while remarkable, have mostly occurred for languages where significant amounts of raw texts are available and where annotated data in downstream tasks have a relatively regular spelling. Conversely, it is not yet completely clear if these models are also well suited for lesser-resourced and more irregular languages. We study the case of Old French, which is in the interesting position of having relatively limited amount of available raw text, but enough annotated resources to assess the relevance of contextual word embedding models for downstream NLP tasks. In particular, we use POS-tagging and dependency parsing to evaluate the quality of such models in a large array of configurations, including models trained from scratch from small amounts of raw text and models pre-trained on other languages but fine-tuned on Medieval French data.</abstract>
      <url hash="abcb4ba8">2022.lrec-1.119</url>
      <bibkey>grobol-etal-2022-bertrade</bibkey>
    </paper>
    <paper id="120">
      <title>Out-of-Domain Evaluation of <fixed-case>F</fixed-case>innish Dependency Parsing</title>
      <author><first>Jenna</first><last>Kanerva</last></author>
      <author><first>Filip</first><last>Ginter</last></author>
      <pages>1114–1124</pages>
      <abstract>The prevailing practice in the academia is to evaluate the model performance on in-domain evaluation data typically set aside from the training corpus. However, in many real world applications the data on which the model is applied may very substantially differ from the characteristics of the training data. In this paper, we focus on Finnish out-of-domain parsing by introducing a novel UD Finnish-OOD out-of-domain treebank including five very distinct data sources (web documents, clinical, online discussions, tweets, and poetry), and a total of 19,382 syntactic words in 2,122 sentences released under the Universal Dependencies framework. Together with the new treebank, we present extensive out-of-domain parsing evaluation utilizing the available section-level information from three different Finnish UD treebanks (TDT, PUD, OOD). Compared to the previously existing treebanks, the new Finnish-OOD is shown include sections more challenging for the general parser, creating an interesting evaluation setting and yielding valuable information for those applying the parser outside of its training domain.</abstract>
      <url hash="c143975d">2022.lrec-1.120</url>
      <bibkey>kanerva-ginter-2022-domain</bibkey>
      <pwccode url="https://github.com/turkunlp/finnish-tweets-lang-identification" additional="false">turkunlp/finnish-tweets-lang-identification</pwccode>
    </paper>
    <paper id="121">
      <title><fixed-case>TA</fixed-case>r<fixed-case>C</fixed-case>: <fixed-case>T</fixed-case>unisian <fixed-case>A</fixed-case>rabish Corpus, First complete release</title>
      <author><first>Elisa</first><last>Gugliotta</last></author>
      <author><first>Marco</first><last>Dinarelli</last></author>
      <pages>1125–1136</pages>
      <abstract>In this paper we present the final result of a project focused on Tunisian Arabic encoded in Arabizi, the Latin-based writing system for digital conversations. The project led to the realization of two integrated and independent tools: a linguistic corpus and a neural network architecture created to annotate the former with various levels of linguistic information (code-switching classification, transliteration, tokenization, POS-tagging, lemmatization). We discuss the choices made in terms of computational and linguistic methodology and the strategies adopted to improve our results. We report on the experiments performed in order to outline our research path. Finally, we explain the reasons why we believe in the potential of these tools for both computational and linguistic researches.</abstract>
      <url hash="a2afe465">2022.lrec-1.121</url>
      <bibkey>gugliotta-dinarelli-2022-tarc</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/tarc">TArC</pwcdataset>
    </paper>
    <paper id="122">
      <title>Towards Universal Segmentations: <fixed-case>U</fixed-case>ni<fixed-case>S</fixed-case>egments 1.0</title>
      <author><first>Zdeněk</first><last>Žabokrtský</last></author>
      <author><first>Niyati</first><last>Bafna</last></author>
      <author><first>Jan</first><last>Bodnár</last></author>
      <author><first>Lukáš</first><last>Kyjánek</last></author>
      <author><first>Emil</first><last>Svoboda</last></author>
      <author><first>Magda</first><last>Ševčíková</last></author>
      <author><first>Jonáš</first><last>Vidra</last></author>
      <pages>1137–1149</pages>
      <abstract>Our work aims at developing a multilingual data resource for morphological segmentation. We present a survey of 17 existing data resources relevant for segmentation in 32 languages, and analyze diversity of how individual linguistic phenomena are captured across them. Inspired by the success of Universal Dependencies, we propose a harmonized scheme for segmentation representation, and convert the data from the studied resources into this common scheme. Harmonized versions of resources available under free licenses are published as a collection called UniSegments 1.0.</abstract>
      <url hash="99f6c0dd">2022.lrec-1.122</url>
      <bibkey>zabokrtsky-etal-2022-towards</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/celex">CELEX</pwcdataset>
    </paper>
    <paper id="123">
      <title><fixed-case>T</fixed-case>e<fixed-case>DD</fixed-case>i Sample: Text Data Diversity Sample for Language Comparison and Multilingual <fixed-case>NLP</fixed-case></title>
      <author><first>Steven</first><last>Moran</last></author>
      <author><first>Christian</first><last>Bentz</last></author>
      <author><first>Ximena</first><last>Gutierrez-Vasques</last></author>
      <author><first>Olga</first><last>Pelloni</last></author>
      <author><first>Tanja</first><last>Samardzic</last></author>
      <pages>1150–1158</pages>
      <abstract>We present the TeDDi sample, a diversity sample of text data for language comparison and multilingual Natural Language Processing. The TeDDi sample currently features 89 languages based on the typological diversity sample in the World Atlas of Language Structures. It consists of more than 20k texts and is accompanied by open-source corpus processing tools. The aim of TeDDi is to facilitate text-based quantitative analysis of linguistic diversity. We describe in detail the TeDDi sample, how it was created, data availability, and its added value through for NLP and linguistic research.</abstract>
      <url hash="0551f185">2022.lrec-1.123</url>
      <bibkey>moran-etal-2022-teddi</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
    </paper>
    <paper id="124">
      <title>Leveraging a Bilingual Dictionary to Learn Wolastoqey Word Representations</title>
      <author><first>Diego</first><last>Bear</last></author>
      <author><first>Paul</first><last>Cook</last></author>
      <pages>1159–1166</pages>
      <abstract>Word embeddings (Mikolov et al., 2013; Pennington et al., 2014) have been used to bolster the performance of natural language processing systems in a wide variety of tasks, including information retrieval (Roy et al., 2018) and machine translation (Qi et al., 2018). However, approaches to learning word embeddings typically require large corpora of running text to learn high quality representations. For many languages, such resources are unavailable. This is the case for Wolastoqey, also known as Passamaquoddy-Maliseet, an endangered low-resource Indigenous language. As there exist no large corpora of running text for Wolastoqey, in this paper, we leverage a bilingual dictionary to learn Wolastoqey word embeddings by encoding their corresponding English definitions into vector representations using pretrained English word and sequence representation models. Specifically, we consider representations based on pretrained word2vec (Mikolov et al., 2013), RoBERTa (Liu et al., 2019) and sentence-BERT (Reimers and Gurevych, 2019) models. We evaluate these embeddings in word prediction tasks focused on part-of-speech, animacy, and transitivity; semantic clustering; and reverse dictionary search. In all evaluations we demonstrate that approaches using these embeddings outperform task-specific baselines, without requiring any language-specific training or fine-tuning.</abstract>
      <url hash="bfe8e92f">2022.lrec-1.124</url>
      <bibkey>bear-cook-2022-leveraging</bibkey>
    </paper>
    <paper id="125">
      <title>Unmasking the Myth of Effortless Big Data - Making an Open Source Multi-lingual Infrastructure and Building Language Resources from Scratch</title>
      <author><first>Linda</first><last>Wiechetek</last></author>
      <author><first>Katri</first><last>Hiovain-Asikainen</last></author>
      <author><first>Inga Lill Sigga</first><last>Mikkelsen</last></author>
      <author><first>Sjur</first><last>Moshagen</last></author>
      <author><first>Flammie</first><last>Pirinen</last></author>
      <author><first>Trond</first><last>Trosterud</last></author>
      <author><first>Børre</first><last>Gaup</last></author>
      <pages>1167–1177</pages>
      <abstract>Machine learning (ML) approaches have dominated NLP during the last two decades. From machine translation and speech technology, ML tools are now also in use for spellchecking and grammar checking, with a blurry distinction between the two. We unmask the myth of effortless big data by illuminating the efforts and time that lay behind building a multi-purpose corpus with regard to collecting, mark-up and building from scratch. We also discuss what kind of language technology minority languages actually need, and to what extent the dominating paradigm has been able to deliver these tools. In this context we present our alternative to corpus-based language technology, which is knowledge-based language technology, and we show how this approach can provide language technology solutions for languages being outside the reach of machine learning procedures. We present a stable and mature infrastructure (GiellaLT) containing more than hundred languages and building a number of language technology tools that are useful for language communities.</abstract>
      <url hash="c1ed04ae">2022.lrec-1.125</url>
      <bibkey>wiechetek-etal-2022-unmasking</bibkey>
    </paper>
    <paper id="126">
      <title>Building and curating conversational corpora for diversity-aware language science and technology</title>
      <author><first>Andreas</first><last>Liesenfeld</last></author>
      <author><first>Mark</first><last>Dingemanse</last></author>
      <pages>1178–1192</pages>
      <abstract>We present an analysis pipeline and best practice guidelines for building and curating corpora of everyday conversation in diverse languages. Surveying language documentation corpora and other resources that cover 67 languages and varieties from 28 phyla, we describe the compilation and curation process, specify minimal properties of a unified format for interactional data, and develop methods for quality control that take into account turn-taking and timing. Two case studies show the broad utility of conversational data for (i) charting human interactional infrastructure and (ii) tracing challenges and opportunities for current ASR solutions. Linguistically diverse conversational corpora can provide new insights for the language sciences and stronger empirical foundations for language technology.</abstract>
      <url hash="e70d2161">2022.lrec-1.126</url>
      <bibkey>liesenfeld-dingemanse-2022-building</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/librispeech">LibriSpeech</pwcdataset>
    </paper>
    <paper id="127">
      <title><fixed-case>EPIC</fixed-case> <fixed-case>U</fixed-case>d<fixed-case>S</fixed-case> - Creation and Applications of a Simultaneous Interpreting Corpus</title>
      <author><first>Heike</first><last>Przybyl</last></author>
      <author><first>Ekaterina</first><last>Lapshinova-Koltunski</last></author>
      <author><first>Katrin</first><last>Menzel</last></author>
      <author><first>Stefan</first><last>Fischer</last></author>
      <author><first>Elke</first><last>Teich</last></author>
      <pages>1193–1200</pages>
      <abstract>In this paper, we describe the creation and annotation of EPIC UdS, a multilingual corpus of simultaneous interpreting for English, German and Spanish. We give an overview of the comparable and parallel, aligned corpus variants and explore various applications of the corpus. What makes EPIC UdS relevant is that it is one of the rare interpreting corpora that includes transcripts suitable for research on more than one language pair and on interpreting with regard to German. It not only contains transcribed speeches, but also rich metadata and fine-grained linguistic annotations tailored for diverse applications across a broad range of linguistic subfields.</abstract>
      <url hash="d46bda83">2022.lrec-1.127</url>
      <bibkey>przybyl-etal-2022-epic</bibkey>
    </paper>
    <paper id="128">
      <title>Development of a Benchmark Corpus to Support Entity Recognition in Job Descriptions</title>
      <author><first>Thomas</first><last>Green</last></author>
      <author><first>Diana</first><last>Maynard</last></author>
      <author><first>Chenghua</first><last>Lin</last></author>
      <pages>1201–1208</pages>
      <abstract>We present the development of a benchmark suite consisting of an annotation schema, training corpus and baseline model for Entity Recognition (ER) in job descriptions, published under a Creative Commons license. This was created to address the distinct lack of resources available to the community for the extraction of salient entities, such as skills, from job descriptions. The dataset contains 18.6k entities comprising five types (Skill, Qualification, Experience, Occupation, and Domain). We include a benchmark CRF-based ER model which achieves an F1 score of 0.59. Through the establishment of a standard definition of entities and training/testing corpus, the suite is designed as a foundation for future work on tasks such as the development of job recommender systems.</abstract>
      <url hash="8435309a">2022.lrec-1.128</url>
      <bibkey>green-etal-2022-development</bibkey>
    </paper>
    <paper id="129">
      <title><fixed-case>CAMIO</fixed-case>: A Corpus for <fixed-case>OCR</fixed-case> in Multiple Languages</title>
      <author><first>Michael</first><last>Arrigo</last></author>
      <author><first>Stephanie</first><last>Strassel</last></author>
      <author><first>Nolan</first><last>King</last></author>
      <author><first>Thao</first><last>Tran</last></author>
      <author><first>Lisa</first><last>Mason</last></author>
      <pages>1209–1216</pages>
      <abstract>CAMIO (Corpus of Annotated Multilingual Images for OCR) is a new corpus created by Linguistic Data Consortium to serve as a resource to support the development and evaluation of optical character recognition (OCR) and related technologies for 35 languages across 24 unique scripts. The corpus comprises nearly 70,000 images of machine printed text, covering a wide variety of topics and styles, document domains, attributes and scanning/capture artifacts. Most images have been exhaustively annotated for text localization, resulting in over 2.3M line-level bounding boxes. For 13 of the 35 languages, 1250 images/language have been further annotated with orthographic transcriptions of each line plus specification of reading order, yielding over 2.4M tokens of transcribed text. The resulting annotations are represented in a comprehensive XML output format defined for this corpus. The paper discusses corpus design and implementation, challenges encountered, baseline performance results obtained on the corpus for text localization and OCR decoding, and plans for corpus publication.</abstract>
      <url hash="1a6742ed">2022.lrec-1.129</url>
      <bibkey>arrigo-etal-2022-camio</bibkey>
    </paper>
    <paper id="130">
      <title><fixed-case>FABRA</fixed-case>: <fixed-case>F</fixed-case>rench Aggregator-Based Readability Assessment toolkit</title>
      <author><first>Rodrigo</first><last>Wilkens</last></author>
      <author><first>David</first><last>Alfter</last></author>
      <author><first>Xiaoou</first><last>Wang</last></author>
      <author><first>Alice</first><last>Pintard</last></author>
      <author><first>Anaïs</first><last>Tack</last></author>
      <author><first>Kevin P.</first><last>Yancey</last></author>
      <author><first>Thomas</first><last>François</last></author>
      <pages>1217–1233</pages>
      <abstract>In this paper, we present the FABRA: readability toolkit based on the aggregation of a large number of readability predictor variables. The toolkit is implemented as a service-oriented architecture, which obviates the need for installation, and simplifies its integration into other projects. We also perform a set of experiments to show which features are most predictive on two different corpora, and how the use of aggregators improves performance over standard feature-based readability prediction. Our experiments show that, for the explored corpora, the most important predictors for native texts are measures of lexical diversity, dependency counts and text coherence, while the most important predictors for foreign texts are syntactic variables illustrating language development, as well as features linked to lexical sophistication. FABRA: have the potential to support new research on readability assessment for French.</abstract>
      <url hash="b4cab0b1">2022.lrec-1.130</url>
      <bibkey>wilkens-etal-2022-fabra</bibkey>
    </paper>
    <paper id="131">
      <title>Towards Building a Spoken Dialogue System for Argument Exploration</title>
      <author><first>Annalena</first><last>Aicher</last></author>
      <author><first>Nadine</first><last>Gerstenlauer</last></author>
      <author><first>Isabel</first><last>Feustel</last></author>
      <author><first>Wolfgang</first><last>Minker</last></author>
      <author><first>Stefan</first><last>Ultes</last></author>
      <pages>1234–1241</pages>
      <abstract>Speech interfaces for argumentative dialogue systems (ADS) are rather scarce. The complex task they pursue hinders the application of common natural language understanding (NLU) approaches in this domain. To address this issue we include an adaption of a recently introduced NLU framework tailored to argumentative tasks into a complete ADS. We evaluate the likeability and motivation of users to interact with the new system in a user study. Therefore, we compare it to a solid baseline utilizing a drop-down menu. The results indicate that the integration of a flexible NLU framework enables a far more natural and satisfying interaction with human users in real-time. Even though the drop-down menu convinces regarding its robustness, the willingness to use the new system is significantly higher. Hence, the featured NLU framework provides a sound basis to build an intuitive interface which can be extended to adapt its behavior to the individual user.</abstract>
      <url hash="8599d6ba">2022.lrec-1.131</url>
      <bibkey>aicher-etal-2022-towards</bibkey>
    </paper>
    <paper id="132">
      <title><fixed-case>F</fixed-case>ree<fixed-case>T</fixed-case>alky: Don’t Be Afraid! Conversations Made Easier by a Humanoid Robot using Persona-based Dialogue</title>
      <author><first>Chanjun</first><last>Park</last></author>
      <author><first>Yoonna</first><last>Jang</last></author>
      <author><first>Seolhwa</first><last>Lee</last></author>
      <author><first>Sungjin</first><last>Park</last></author>
      <author><first>Heuiseok</first><last>Lim</last></author>
      <pages>1242–1248</pages>
      <abstract>We propose a deep learning-based foreign language learning platform, named FreeTalky, for people who experience anxiety dealing with foreign languages, by employing a humanoid robot NAO and various deep learning models. A persona-based dialogue system that is embedded in NAO provides an interesting and consistent multi-turn dialogue for users. Also, an grammar error correction system promotes improvement in grammar skills of the users. Thus, our system enables personalized learning based on persona dialogue and facilitates grammar learning of a user using grammar error feedback. Furthermore, we verified whether FreeTalky provides practical help in alleviating xenoglossophobia by replacing the real human in the conversation with a NAO robot, through human evaluation.</abstract>
      <url hash="6d1a241a">2022.lrec-1.132</url>
      <bibkey>park-etal-2022-freetalky</bibkey>
    </paper>
    <paper id="133">
      <title>Self-Contained Utterance Description Corpus for <fixed-case>J</fixed-case>apanese Dialog</title>
      <author><first>Yuta</first><last>Hayashibe</last></author>
      <pages>1249–1255</pages>
      <abstract>Often both an utterance and its context must be read to understand its intent in a dialog. Herein we propose a task, Self- Contained Utterance Description (SCUD), to describe the intent of an utterance in a dialog with multiple simple natural sentences without the context. If a task can be performed concurrently with high accuracy as the conversation continues such as in an accommodation search dialog, the operator can easily suggest candidates to the customer by inputting SCUDs of the customer’s utterances to the accommodation search system. SCUDs can also describe the transition of customer requests from the dialog log. We construct a Japanese corpus to train and evaluate automatic SCUD generation. The corpus consists of 210 dialogs containing 10,814 sentences. We conduct an experiment to verify that SCUDs can be automatically generated. Additionally, we investigate the influence of the amount of training data on the automatic generation performance using 8,200 additional examples.</abstract>
      <url hash="b67c2a15">2022.lrec-1.133</url>
      <bibkey>hayashibe-2022-self</bibkey>
    </paper>
    <paper id="134">
      <title><fixed-case>D</fixed-case>ial<fixed-case>C</fixed-case>rowd 2.0: A Quality-Focused Dialog System Crowdsourcing Toolkit</title>
      <author><first>Jessica</first><last>Huynh</last></author>
      <author><first>Ting-Rui</first><last>Chiang</last></author>
      <author><first>Jeffrey</first><last>Bigham</last></author>
      <author><first>Maxine</first><last>Eskenazi</last></author>
      <pages>1256–1263</pages>
      <abstract>Dialog system developers need high-quality data to train, fine-tune and assess their systems. They often use crowdsourcing for this since it provides large quantities of data from many workers. However, the data may not be of sufficiently good quality. This can be due to the way that the requester presents a task and how they interact with the workers. This paper introduces DialCrowd 2.0 to help requesters obtain higher quality data by, for example, presenting tasks more clearly and facilitating effective communication with workers. DialCrowd 2.0 guides developers in creating improved Human Intelligence Tasks (HITs) and is directly applicable to the workflows used currently by developers and researchers.</abstract>
      <url hash="c2a50924">2022.lrec-1.134</url>
      <bibkey>huynh-etal-2022-dialcrowd</bibkey>
    </paper>
    <paper id="135">
      <title>A Brief Survey of Textual Dialogue Corpora</title>
      <author><first>Hugo</first><last>Gonçalo Oliveira</last></author>
      <author><first>Patrícia</first><last>Ferreira</last></author>
      <author><first>Daniel</first><last>Martins</last></author>
      <author><first>Catarina</first><last>Silva</last></author>
      <author><first>Ana</first><last>Alves</last></author>
      <pages>1264–1274</pages>
      <abstract>Several dialogue corpora are currently available for research purposes, but they still fall short for the growing interest in the development of dialogue systems with their own specific requirements. In order to help those requiring such a corpus, this paper surveys a range of available options, in terms of aspects like speakers, size, languages, collection, annotations, and domains. Some trends are identified and possible approaches for the creation of new corpora are also discussed.</abstract>
      <url hash="ea1918ec">2022.lrec-1.135</url>
      <bibkey>goncalo-oliveira-etal-2022-brief</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/coqa">CoQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/emotyda">EMOTyDA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/emotionlines">EmotionLines</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/meld">MELD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multiwoz">MultiWOZ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/quac">QuAC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/redial">ReDial</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/samsum-corpus">SAMSum Corpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sgd">SGD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/taskmaster-1">Taskmaster-1</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/topical-chat">Topical-Chat</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ubuntu-dialogue-corpus">UDC</pwcdataset>
    </paper>
    <paper id="136">
      <title>A Unified Approach to Entity-Centric Context Tracking in Social Conversations</title>
      <author><first>Ulrich</first><last>Rückert</last></author>
      <author><first>Srinivas</first><last>Sunkara</last></author>
      <author><first>Abhinav</first><last>Rastogi</last></author>
      <author><first>Sushant</first><last>Prakash</last></author>
      <author><first>Pranav</first><last>Khaitan</last></author>
      <pages>1275–1285</pages>
      <abstract>In human-human conversations, Context Tracking deals with identifying important entities and keeping track of their properties and relationships. This is a challenging problem that encompasses several subtasks such as slot tagging, coreference resolution, resolving plural mentions and entity linking. We approach this problem as an end-to-end modeling task where the conversational context is represented by an entity repository containing the entity references mentioned so far, their properties and the relationships between them. The repository is updated turn-by-turn, thus making training and inference computationally efficient even for long conversations. This paper lays the groundwork for an investigation of this framework in two ways. First, we release Contrack, a large scale human-human conversation corpus for context tracking with people and location annotations. It contains over 7000 conversations with an average of 11.8 turns, 5.8 entities and 15.2 references per conversation. Second, we open-source a neural network architecture for context tracking. Finally we compare this network to state-of-the-art approaches for the subtasks it subsumes and report results on the involved tradeoffs.</abstract>
      <url hash="2c007313">2022.lrec-1.136</url>
      <bibkey>ruckert-etal-2022-unified</bibkey>
      <pwccode url="https://github.com/google-research-datasets/contrack" additional="false">google-research-datasets/contrack</pwccode>
    </paper>
    <paper id="137">
      <title>A Unifying View On Task-oriented Dialogue Annotation</title>
      <author><first>Vojtěch</first><last>Hudeček</last></author>
      <author><first>Leon-paul</first><last>Schaub</last></author>
      <author><first>Daniel</first><last>Stancl</last></author>
      <author><first>Patrick</first><last>Paroubek</last></author>
      <author><first>Ondřej</first><last>Dušek</last></author>
      <pages>1286–1296</pages>
      <abstract>Every model is only as strong as the data that it is trained on. In this paper, we present a new dataset, obtained by merging four publicly available annotated corpora for task-oriented dialogues in several domains (MultiWOZ 2.2, CamRest676, DSTC2 and Schema-Guided Dialogue Dataset). This way, we assess the feasibility of providing a unified ontology and annotation schema covering several domains with a relatively limited effort. We analyze the characteristics of the resulting dataset along three main dimensions: language, information content and performance. We focus on aspects likely to be pertinent for improving dialogue success, e.g. dialogue consistency. Furthermore, to assess the usability of this new corpus, we thoroughly evaluate dialogue generation performance under various conditions with the help of two prominent recent end-to-end dialogue models: MarCo and GPT-2. These models were selected as popular open implementations representative of the two main dimensions of dialogue modelling. While we did not observe a significant gain for dialogue state tracking performance, we show that using more training data from different sources can improve language modelling capabilities and positively impact dialogue flow (consistency). In addition, we provide the community with one of the largest open dataset for machine learning experiments.</abstract>
      <url hash="117d215d">2022.lrec-1.137</url>
      <bibkey>hudecek-etal-2022-unifying</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/sgd">SGD</pwcdataset>
    </paper>
    <paper id="138">
      <title>A Multi-source Graph Representation of the Movie Domain for Recommendation Dialogues Analysis</title>
      <author><first>Antonio</first><last>Origlia</last></author>
      <author><first>Martina</first><last>Di Bratto</last></author>
      <author><first>Maria</first><last>Di Maro</last></author>
      <author><first>Sabrina</first><last>Mennella</last></author>
      <pages>1297–1306</pages>
      <abstract>In dialogue analysis, characterising named entities in the domain of interest is relevant in order to understand how people are making use of them for argumentation purposes. The movie recommendation domain is a frequently considered case study for many applications and by linguistic studies and, since many different resources have been collected throughout the years to describe it, a single database combining all these data sources is a valuable asset for cross-disciplinary investigations. We propose an integrated graph-based structure of multiple resources, enriched with the results of the application of graph analytics approaches to provide an encompassing view of the domain and of the way people talk about it during the recommendation task. While we cannot distribute the final resource because of licensing issues, we share the code to assemble and process it once the reference data have been obtained from the original sources.</abstract>
      <url hash="a64c618d">2022.lrec-1.138</url>
      <bibkey>origlia-etal-2022-multi</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/inspired">Inspired</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/movielens">MovieLens</pwcdataset>
    </paper>
    <paper id="139">
      <title><fixed-case>SHARE</fixed-case>: A Lexicon of Harmful Expressions by <fixed-case>S</fixed-case>panish Speakers</title>
      <author><first>Flor Miriam</first><last>Plaza-del-Arco</last></author>
      <author><first>Ana Belén</first><last>Parras Portillo</last></author>
      <author><first>Pilar</first><last>López Úbeda</last></author>
      <author><first>Beatriz</first><last>Gil</last></author>
      <author><first>María-Teresa</first><last>Martín-Valdivia</last></author>
      <pages>1307–1316</pages>
      <abstract>In this paper we present SHARE, a new lexical resource with 10,125 offensive terms and expressions collected from Spanish speakers. We retrieve this vocabulary using an existing chatbot developed to engage a conversation with users and collect insults via Telegram, named Fiero. This vocabulary has been manually labeled by five annotators obtaining a kappa coefficient agreement of 78.8%. In addition, we leverage the lexicon to release the first corpus in Spanish for offensive span identification research named OffendES_spans. Finally, we show the utility of our resource as an interpretability tool to explain why a comment may be considered offensive.</abstract>
      <url hash="ccf1639f">2022.lrec-1.139</url>
      <bibkey>plaza-del-arco-etal-2022-share</bibkey>
    </paper>
    <paper id="140">
      <title>Wiktextract: <fixed-case>W</fixed-case>iktionary as Machine-Readable Structured Data</title>
      <author><first>Tatu</first><last>Ylonen</last></author>
      <pages>1317–1325</pages>
      <abstract>We present a machine-readable structured data version of Wiktionary. Unlike previous Wiktionary extractions, the new extractor, Wiktextract, fully interprets and expands templates and Lua modules in Wiktionary. This enables it to perform a more complete, robust, and maintainable extraction. The extracted data is multilingual and includes lemmas, inflected forms, translations, etymology, usage examples, pronunciations (including URLs of sound files), lexical and semantic relations, and various morphological, syntactic, semantic, topical, and dialectal annotations. We extract all data from the English Wiktionary. Comparing against previous extractions from language-specific dictionaries, we find that its coverage for non-English languages often matches or exceeds the coverage in the language-specific editions, with the added benefit that all glosses are in English. The data is freely available and regularly updated, enabling anyone to add more data and correct errors by editing Wiktionary. The extracted data is in JSON format and designed to be easy to use by researchers, downstream resources, and application developers.</abstract>
      <url hash="564e79a7">2022.lrec-1.140</url>
      <bibkey>ylonen-2022-wiktextract</bibkey>
    </paper>
    <paper id="141">
      <title><fixed-case>N</fixed-case>y<fixed-case>LL</fixed-case>ex: A Novel Resource of <fixed-case>S</fixed-case>wedish Words Annotated with Reading Proficiency Level</title>
      <author><first>Daniel</first><last>Holmer</last></author>
      <author><first>Evelina</first><last>Rennes</last></author>
      <pages>1326–1331</pages>
      <abstract>What makes a text easy to read or not, depends on a variety of factors. One of the most prominent is, however, if the text contains easy, and avoids difficult, words. Deciding if a word is easy or difficult is not a trivial task, since it depends on characteristics of the word in itself as well as the reader, but it can be facilitated by the help of a corpus annotated with word frequencies and reading proficiency levels. In this paper, we present NyLLex, a novel lexical resource derived from books published by Sweden’s largest publisher for easy language texts. NyLLex consists of 6,668 entries, with frequency counts distributed over six reading proficiency levels. We show that NyLLex, with its novel source material aimed at individuals of different reading proficiency levels, can serve as a complement to already existing resources for Swedish.</abstract>
      <url hash="a319c43d">2022.lrec-1.141</url>
      <bibkey>holmer-rennes-2022-nyllex</bibkey>
    </paper>
    <paper id="142">
      <title>Making a Semantic Event-type Ontology Multilingual</title>
      <author><first>Zdenka</first><last>Uresova</last></author>
      <author><first>Karolina</first><last>Zaczynska</last></author>
      <author><first>Peter</first><last>Bourgonje</last></author>
      <author><first>Eva</first><last>Fučíková</last></author>
      <author><first>Georg</first><last>Rehm</last></author>
      <author><first>Jan</first><last>Hajic</last></author>
      <pages>1332–1343</pages>
      <abstract>We present an extension of the SynSemClass Event-type Ontology, originally conceived as a bilingual Czech-English resource. We added German entries to the classes representing the concepts of the ontology. Having a different starting point than the original work (unannotated parallel corpus without links to a valency lexicon and, of course, different existing lexical resources), it was a challenge to adapt the annotation guidelines, the data model and the tools used for the original version. We describe the process and results of working in such a setup. We also show the next steps to adapt the annotation process, data structures and formats and tools necessary to make the addition of a new language in the future more smooth and efficient, and possibly to allow for various teams to work on SynSemClass extensions to many languages concurrently. We also present the latest release which contains the results of adding German, freely available for download as well as for online access.</abstract>
      <url hash="f94da44f">2022.lrec-1.142</url>
      <bibkey>uresova-etal-2022-making</bibkey>
      <pwccode url="https://github.com/linatal/synsemclass_ger" additional="false">linatal/synsemclass_ger</pwccode>
    </paper>
    <paper id="143">
      <title><fixed-case>N</fixed-case>om<fixed-case>V</fixed-case>allex: A Valency Lexicon of <fixed-case>C</fixed-case>zech Nouns and Adjectives</title>
      <author><first>Veronika</first><last>Kolářová</last></author>
      <author><first>Anna</first><last>Vernerová</last></author>
      <pages>1344–1352</pages>
      <abstract>We present NomVallex, a manually annotated valency lexicon of Czech nouns and adjectives. The lexicon is created in the theoretical framework of the Functional Generative Description and based on corpus data. In total, NomVallex 2.0 is comprised of 1027 lexical units contained in 570 lexemes, covering the following part-of-speech and derivational categories: deverbal and deadjectival nouns, and deverbal, denominal, deadjectival and primary adjectives. Valency properties of a lexical unit are captured in a valency frame which is modeled as a sequence of valency slots, supplemented with a list of morphemic forms. In order to make it possible to study the relationship between valency behavior of base words and their derivatives, lexical units of nouns and adjectives in NomVallex are linked to their respective base words, contained either in NomVallex itself or, in case of verbs, in a valency lexicon of Czech verbs called VALLEX. NomVallex enables a comparison of valency properties of a significant number of Czech nominals with their base words, both manually and in an automatic way; as such, we can address the theoretical question of argument inheritance, concentrating on systemic and non-systemic valency behavior.</abstract>
      <url hash="d3b9faa8">2022.lrec-1.143</url>
      <bibkey>kolarova-vernerova-2022-nomvallex</bibkey>
    </paper>
    <paper id="144">
      <title><fixed-case>TZOS</fixed-case>: an Online Terminology Database Aimed at Working on <fixed-case>B</fixed-case>asque Academic Terminology Collaboratively</title>
      <author><first>Izaskun</first><last>Aldezabal</last></author>
      <author><first>Jose Mari</first><last>Arriola</last></author>
      <author><first>Arantxa</first><last>Otegi</last></author>
      <pages>1353–1359</pages>
      <abstract>Terminology databases are highly useful for the dissemination of specialized knowledge. In this paper we present TZOS, an online terminology database to work on Basque academic terminology collaboratively. We show how this resource integrates the Communicative Theory of Terminology, together with the methodological matters, how it is connected with real corpus GARATERM, which terminology issues arise when terms are collected and future perspectives. The main objectives of this work are to develop basic tools to research academic registers and make the terminology collected by expert users available to the community. Even though TZOS has been designed for an educational context, its flexible structure makes possible to extend it also to the professional area. In this way, we have built IZIBI-TZOS which is a Civil Engineering oriented version of TZOS. These resources are already publicly available, and the ongoing work is towards the interlinking with other lexical resources by applying linking data principles.</abstract>
      <url hash="ca5b50df">2022.lrec-1.144</url>
      <bibkey>aldezabal-etal-2022-tzos</bibkey>
    </paper>
    <paper id="145">
      <title><fixed-case>A</fixed-case>nimacy Denoting <fixed-case>G</fixed-case>erman Nouns: Annotation and Classification</title>
      <author><first>Manfred</first><last>Klenner</last></author>
      <author><first>Anne</first><last>Göhring</last></author>
      <pages>1360–1364</pages>
      <abstract>In this paper, we introduce a gold standard for animacy detection comprising almost 14,500 German nouns that might be used to denote either animate entities or non-animate entities. We present inter-annotator agreement of our crowd-sourced seed annotations (9,000 nouns) and discuss the results of machine learning models applied to this data.</abstract>
      <url hash="26be35db">2022.lrec-1.145</url>
      <bibkey>klenner-gohring-2022-animacy</bibkey>
    </paper>
    <paper id="146">
      <title>x-en<fixed-case>VENT</fixed-case>: A Corpus of Event Descriptions with Experiencer-specific Emotion and Appraisal Annotations</title>
      <author><first>Enrica</first><last>Troiano</last></author>
      <author><first>Laura Ana Maria</first><last>Oberlaender</last></author>
      <author><first>Maximilian</first><last>Wegge</last></author>
      <author><first>Roman</first><last>Klinger</last></author>
      <pages>1365–1375</pages>
      <abstract>Emotion classification is often formulated as the task to categorize texts into a predefined set of emotion classes. So far, this task has been the recognition of the emotion of writers and readers, as well as that of entities mentioned in the text. We argue that a classification setup for emotion analysis should be performed in an integrated manner, including the different semantic roles that participate in an emotion episode. Based on appraisal theories in psychology, which treat emotions as reactions to events, we compile an English corpus of written event descriptions. The descriptions depict emotion-eliciting circumstances, and they contain mentions of people who responded emotionally. We annotate all experiencers, including the original author, with the emotions they likely felt. In addition, we link them to the event they found salient (which can be different for different experiencers in a text) by annotating event properties, or appraisals (e.g., the perceived event undesirability, the uncertainty of its outcome). Our analysis reveals patterns in the co-occurrence of people’s emotions in interaction. Hence, this richly-annotated resource provides useful data to study emotions and event evaluations from the perspective of different roles, and it enables the development of experiencer-specific emotion and appraisal classification systems.</abstract>
      <url hash="5b180b39">2022.lrec-1.146</url>
      <bibkey>troiano-etal-2022-x</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/isear">ISEAR</pwcdataset>
    </paper>
    <paper id="147">
      <title>Polar Quantification of Actor Noun Phrases for <fixed-case>G</fixed-case>erman</title>
      <author><first>Anne</first><last>Göhring</last></author>
      <author><first>Manfred</first><last>Klenner</last></author>
      <pages>1376–1380</pages>
      <abstract>In this paper, we discuss work that strives to measure the degree of negativity - the negative polar load - of noun phrases, especially those denoting actors. Since no gold standard data is available for German for this quantification task, we generated a silver standard and used it to fine-tune a BERT-based intensity regressor. We evaluated the quality of the silver standard empirically and found that our lexicon-based quantification metric showed a strong correlation with human annotators.</abstract>
      <url hash="c9132421">2022.lrec-1.147</url>
      <bibkey>gohring-klenner-2022-polar</bibkey>
    </paper>
    <paper id="148">
      <title><fixed-case>C</fixed-case>zech Dataset for Cross-lingual Subjectivity Classification</title>
      <author><first>Pavel</first><last>Přibáň</last></author>
      <author><first>Josef</first><last>Steinberger</last></author>
      <pages>1381–1391</pages>
      <abstract>In this paper, we introduce a new Czech subjectivity dataset of 10k manually annotated subjective and objective sentences from movie reviews and descriptions. Our prime motivation is to provide a reliable dataset that can be used with the existing English dataset as a benchmark to test the ability of pre-trained multilingual models to transfer knowledge between Czech and English and vice versa. Two annotators annotated the dataset reaching 0.83 of the Cohen’s K inter-annotator agreement. To the best of our knowledge, this is the first subjectivity dataset for the Czech language. We also created an additional dataset that consists of 200k automatically labeled sentences. Both datasets are freely available for research purposes. Furthermore, we fine-tune five pre-trained BERT-like models to set a monolingual baseline for the new dataset and we achieve 93.56% of accuracy. We fine-tune models on the existing English dataset for which we obtained results that are on par with the current state-of-the-art results. Finally, we perform zero-shot cross-lingual subjectivity classification between Czech and English to verify the usability of our dataset as the cross-lingual benchmark. We compare and discuss the cross-lingual and monolingual results and the ability of multilingual models to transfer knowledge between languages.</abstract>
      <url hash="1331b0b5">2022.lrec-1.148</url>
      <bibkey>priban-steinberger-2022-czech</bibkey>
      <pwccode url="https://github.com/pauli31/czech-subjectivity-dataset" additional="true">pauli31/czech-subjectivity-dataset</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/czech-subjectivity-dataset">Czech Subjectivity Dataset</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/subj">SUBJ</pwcdataset>
    </paper>
    <paper id="149">
      <title><fixed-case>RED</fixed-case> v2: Enhancing <fixed-case>RED</fixed-case> Dataset for Multi-Label Emotion Detection</title>
      <author><first>Alexandra</first><last>Ciobotaru</last></author>
      <author><first>Mihai Vlad</first><last>Constantinescu</last></author>
      <author><first>Liviu P.</first><last>Dinu</last></author>
      <author><first>Stefan</first><last>Dumitrescu</last></author>
      <pages>1392–1399</pages>
      <abstract>RED (Romanian Emotion Dataset) is a machine learning-based resource developed for the automatic detection of emotions in Romanian texts, containing single-label annotated tweets with one of the following emotions: joy, fear, sadness, anger and neutral. In this work, we propose REDv2, an open-source extension of RED by adding two more emotions, trust and surprise, and by widening the annotation schema so that the resulted novel dataset is multi-label. We show the overall reliability of our dataset by computing inter-annotator agreements per tweet using a formula suitable for our annotation setup and we aggregate all annotators’ opinions into two variants of ground truth, one suitable for multi-label classification and the other suitable for text regression. We propose strong baselines with two transformer models, the Romanian BERT and the multilingual XLM-Roberta model, in both categorical and regression settings.</abstract>
      <url hash="5523d8f3">2022.lrec-1.149</url>
      <bibkey>ciobotaru-etal-2022-red</bibkey>
    </paper>
    <paper id="150">
      <title>Fine-Grained Error Analysis and Fair Evaluation of Labeled Spans</title>
      <author><first>Katrin</first><last>Ortmann</last></author>
      <pages>1400–1407</pages>
      <abstract>The traditional evaluation of labeled spans with precision, recall, and F1-score has undesirable effects due to double penalties. Annotations with incorrect label or boundaries count as two errors instead of one, despite being closer to the target annotation than false positives or false negatives. In this paper, new error types are introduced, which more accurately reflect true annotation quality and ensure that every annotation counts only once. An algorithm for error identification in flat and multi-level annotations is presented and complemented with a proposal on how to calculate meaningful precision, recall, and F1-scores based on the more fine-grained error types. The exemplary application to three different annotation tasks (NER, chunking, parsing) shows that the suggested procedure not only prevents double penalties but also allows for a more detailed error analysis, thereby providing more insight into the actual weaknesses of a system.</abstract>
      <url hash="56e6d5ef">2022.lrec-1.150</url>
      <bibkey>ortmann-2022-fine</bibkey>
      <pwccode url="https://github.com/rubcompling/faireval" additional="false">rubcompling/faireval</pwccode>
    </paper>
    <paper id="151">
      <title>Probing Pre-trained Auto-regressive Language Models for Named Entity Typing and Recognition</title>
      <author><first>Elena V.</first><last>Epure</last></author>
      <author><first>Romain</first><last>Hennequin</last></author>
      <pages>1408–1417</pages>
      <abstract>Multiple works have proposed to probe language models (LMs) for generalization in named entity (NE) typing (NET) and recognition (NER). However, little has been done in this direction for auto-regressive models despite their popularity and potential to express a wide variety of NLP tasks in the same unified format. We propose a new methodology to probe auto-regressive LMs for NET and NER generalization, which draws inspiration from human linguistic behavior, by resorting to meta-learning. We study NEs of various types individually by designing a zero-shot transfer strategy for NET. Then, we probe the model for NER by providing a few examples at inference. We introduce a novel procedure to assess the model’s memorization of NEs and report the memorization’s impact on the results. Our findings show that: 1) GPT2, a common pre-trained auto-regressive LM, without any fine-tuning for NET or NER, performs the tasksfairly well; 2) name irregularity when common for a NE type could be an effective exploitable cue; 3) the model seems to rely more on NE than contextual cues in few-shot NER; 4) NEs with words absent during LM pre-training are very challenging for both NET and NER.</abstract>
      <url hash="14a396d9">2022.lrec-1.151</url>
      <bibkey>epure-hennequin-2022-probing</bibkey>
      <pwccode url="https://github.com/deezer/net-ner-probing" additional="false">deezer/net-ner-probing</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/dbpedia">DBpedia</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wnut-2017-emerging-and-rare-entity">WNUT 2017</pwcdataset>
    </paper>
    <paper id="152">
      <title>Frustratingly Easy Performance Improvements for Low-resource Setups: A Tale on <fixed-case>BERT</fixed-case> and Segment Embeddings</title>
      <author><first>Rob</first><last>van der Goot</last></author>
      <author><first>Max</first><last>Müller-Eberstein</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <pages>1418–1427</pages>
      <abstract>As input representation for each sub-word, the original BERT architecture proposes the sum of the sub-word embedding, position embedding and a segment embedding. Sub-word and position embeddings are well-known and studied, and encode lexical information and word position, respectively. In contrast, segment embeddings are less known and have so far received no attention, despite being ubiquitous in large pre-trained language models. The key idea of segment embeddings is to encode to which of the two sentences (segments) a word belongs to — the intuition is to inform the model about the separation of sentences for the next sentence prediction pre-training task. However, little is known on whether the choice of segment impacts performance. In this work, we try to fill this gap and empirically study the impact of the segment embedding during inference time for a variety of pre-trained embeddings and target tasks. We hypothesize that for single-sentence prediction tasks performance is not affected — neither in mono- nor multilingual setups — while it matters when swapping segment IDs in paired-sentence tasks. To our surprise, this is not the case. Although for classification tasks and monolingual BERT models no large differences are observed, particularly word-level multilingual prediction tasks are heavily impacted. For low-resource syntactic tasks, we observe impacts of segment embedding and multilingual BERT choice. We find that the default setting for the most used multilingual BERT model underperforms heavily, and a simple swap of the segment embeddings yields an average improvement of 2.5 points absolute LAS score for dependency parsing over 9 different treebanks.</abstract>
      <url hash="fac87db3">2022.lrec-1.152</url>
      <bibkey>van-der-goot-etal-2022-frustratingly</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
    <paper id="153">
      <title>The Subject Annotations of the <fixed-case>D</fixed-case>anish Parliament Corpus (2009-2017) - Evaluated with Automatic Multi-label Classification</title>
      <author><first>Costanza</first><last>Navarretta</last></author>
      <author><first>Dorte</first><last>Haltrup Hansen</last></author>
      <pages>1428–1436</pages>
      <abstract>This paper addresses the semi-automatic annotation of subjects, also called policy areas, in the Danish Parliament Corpus (2009-2017) v.2. Recently, the corpus has been made available through the CLARIN-DK repository, the Danish node of the European CLARIN infrastructure. The paper also contains an analysis of the subjects in the corpus, and a description of multi-label classification experiments act to verify the consistency of the subject annotation and the utility of the corpus for training classifiers on this type of data. The analysis of the corpus comprises an investigation of how often the parliament members addressed each subject and the relation between subjects and gender of the speaker. The classification experiments show that classifiers can determine the two co-occurring subjects of the speeches from the agenda titles with a performance similar to that of human annotators. Moreover, a multilayer perceptron achieved an F1-score of 0.68 on the same task when trained on bag of words vectors obtained from the speeches’ lemmas. This is an improvement of more than 0.6 with respect to the baseline, a majority classifier that accounts for the frequency of the classes. The result is promising given the high number of subject combinations (186) and the skewness of the data.</abstract>
      <url hash="cff805d8">2022.lrec-1.153</url>
      <bibkey>navarretta-haltrup-hansen-2022-subject</bibkey>
    </paper>
    <paper id="154">
      <title>A Systematic Study Reveals Unexpected Interactions in Pre-Trained Neural Machine Translation</title>
      <author><first>Ashleigh</first><last>Richardson</last></author>
      <author><first>Janet</first><last>Wiles</last></author>
      <pages>1437–1443</pages>
      <abstract>A significant challenge in developing translation systems for the world’s ∼7,000 languages is that very few have sufficient data for state-of-the-art techniques. Transfer learning is a promising direction for low-resource neural machine translation (NMT), but introduces many new variables which are often selected through ablation studies, costly trial-and-error, or niche expertise. When pre-training an NMT system for low-resource translation, the pre-training task is often chosen based on data abundance and similarity to the main task. Factors such as dataset sizes and similarity have typically been analysed independently in previous studies, due to the computational cost associated with systematic studies. However, these factors are not independent. We conducted a three-factor experiment to examine how language similarity, pre-training dataset size and main dataset size interacted in their effect on performance in pre-trained transformer-based low-resource NMT. We replicated the common finding that more data was beneficial in bilingual systems, but also found a statistically significant interaction between the three factors, which reduced the effectiveness of large pre-training datasets for some main task dataset sizes (p-value &lt; 0.0018). The surprising trends identified in these interactions indicate that systematic studies of interactions may be a promising long-term direction for guiding research in low-resource neural methods.</abstract>
      <url hash="239c5a17">2022.lrec-1.154</url>
      <bibkey>richardson-wiles-2022-systematic</bibkey>
    </paper>
    <paper id="155">
      <title>Holistic Evaluation of Automatic <fixed-case>T</fixed-case>ime<fixed-case>ML</fixed-case> Annotators</title>
      <author><first>Mustafa</first><last>Ocal</last></author>
      <author><first>Adrian</first><last>Perez</last></author>
      <author><first>Antonela</first><last>Radas</last></author>
      <author><first>Mark</first><last>Finlayson</last></author>
      <pages>1444–1453</pages>
      <abstract>TimeML is a scheme for representing temporal information (times, events, &amp; temporal relations) in texts. Although automatic TimeML annotation is challenging, there has been notable progress, with F1s of 0.8–0.9 for events and time detection subtasks, and F1s of 0.5–0.7 for relation extraction. Individually, these subtask results are reasonable, even good, but when combined to generate a full TimeML graph, is overall performance still acceptable? We present a novel suite of eight metrics, combined with a new graph-transformation experimental design, for holistic evaluation of TimeML graphs. We apply these metrics to four automatic TimeML annotation systems (CAEVO, TARSQI, CATENA, and ClearTK). We show that on average 1/3 of the TimeML graphs produced using these systems are inconsistent, and there is on average 1/5 more temporal indeterminacy than the gold-standard. We also show that the automatically generated graphs are on average 109 edits from the gold-standard, which is 1/3 toward complete replacement. Finally, we show that the relationship individual subtask performance and graph quality is non-linear: small errors in TimeML subtasks result in rapid degradation of final graph quality. These results suggest current automatic TimeML annotators are far from optimal and significant further improvement would be useful.</abstract>
      <url hash="496a7c29">2022.lrec-1.155</url>
      <bibkey>ocal-etal-2022-holistic</bibkey>
    </paper>
    <paper id="156">
      <title>Measuring Uncertainty in Translation Quality Evaluation (<fixed-case>TQE</fixed-case>)</title>
      <author><first>Serge</first><last>Gladkoff</last></author>
      <author><first>Irina</first><last>Sorokina</last></author>
      <author><first>Lifeng</first><last>Han</last></author>
      <author><first>Alexandra</first><last>Alekseeva</last></author>
      <pages>1454–1461</pages>
      <abstract>From both human translators (HT) and machine translation (MT) researchers’ point of view, translation quality evaluation (TQE) is an essential task. Translation service providers (TSPs) have to deliver large volumes of translations which meet customer specifications with harsh constraints of required quality level in tight time-frames and costs. MT researchers strive to make their models better, which also requires reliable quality evaluation. While automatic machine translation evaluation (MTE) metrics and quality estimation (QE) tools are widely available and easy to access, existing automated tools are not good enough, and human assessment from professional translators (HAP) are often chosen as the golden standard (CITATION). Human evaluations, however, are often accused of having low reliability and agreement. Is this caused by subjectivity or statistics is at play? How to avoid the entire text to be checked and be more efficient with TQE from cost and efficiency perspectives, and what is the optimal sample size of the translated text, so as to reliably estimate the translation quality of the entire material? This work carries out such a motivated research to correctly estimate the confidence intervals (CITATION) depending on the sample size of translated text, e.g. the amount of words or sentences, that needs to be processed on TQE workflow step for confident and reliable evaluation of overall translation quality. The methodology we applied for this work is from Bernoulli Statistical Distribution Modelling (BSDM) and Monte Carlo Sampling Analysis (MCSA).</abstract>
      <url hash="cb95bf68">2022.lrec-1.156</url>
      <bibkey>gladkoff-etal-2022-measuring</bibkey>
    </paper>
    <paper id="157">
      <title>Challenging the Transformer-based models with a Classical <fixed-case>A</fixed-case>rabic dataset: <fixed-case>Q</fixed-case>uran and <fixed-case>H</fixed-case>adith</title>
      <author><first>Shatha</first><last>Altammami</last></author>
      <author><first>Eric</first><last>Atwell</last></author>
      <pages>1462–1471</pages>
      <abstract>Transformer-based models showed near-perfect results on several downstream tasks. However, their performance on classical Arabic texts is largely unexplored. To fill this gap, we evaluate monolingual, bilingual, and multilingual state-of-the-art models to detect relatedness between the Quran (Muslim holy book) and the Hadith (Prophet Muhammed teachings), which are complex classical Arabic texts with underlying meanings that require deep human understanding. To do this, we carefully built a dataset of Quran-verse and Hadith-teaching pairs by consulting sources of reputable religious experts. This study presents the methodology of creating the dataset, which we make available on our repository, and discusses the models’ performance that calls for the imminent need to explore avenues for improving the quality of these models to capture the semantics in such complex, low-resource texts.</abstract>
      <url hash="366b8f80">2022.lrec-1.157</url>
      <bibkey>altammami-atwell-2022-challenging</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/sick">SICK</pwcdataset>
    </paper>
    <paper id="158">
      <title>Question Modifiers in Visual Question Answering</title>
      <author><first>William</first><last>Britton</last></author>
      <author><first>Somdeb</first><last>Sarkhel</last></author>
      <author><first>Deepak</first><last>Venugopal</last></author>
      <pages>1472–1479</pages>
      <abstract>Visual Question Answering (VQA) is a challenge problem that can advance AI by integrating several important sub-disciplines including natural language understanding and computer vision. Large VQA datasets that are publicly available for training and evaluation have driven the growth of VQA models that have obtained increasingly larger accuracy scores. However, it is also important to understand how much a model understands the details that are provided in a question. For example, studies in psychology have shown that syntactic complexity places a larger cognitive load on humans. Analogously, we want to understand if models have the perceptual capability to handle modifications to questions. Therefore, we develop a new dataset using Amazon Mechanical Turk where we asked workers to add modifiers to questions based on object properties and spatial relationships. We evaluate this data on LXMERT which is a state-of-the-art model in VQA that focuses more extensively on language processing. Our conclusions indicate that there is a significant negative impact on the performance of the model when the questions are modified to include more detailed information.</abstract>
      <url hash="099fa293">2022.lrec-1.158</url>
      <bibkey>britton-etal-2022-question</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering-v2-0">Visual Question Answering v2.0</pwcdataset>
    </paper>
    <paper id="159">
      <title>Multimodal Pipeline for Collection of Misinformation Data from Telegram</title>
      <author><first>Jose</first><last>Sosa</last></author>
      <author><first>Serge</first><last>Sharoff</last></author>
      <pages>1480–1489</pages>
      <abstract>The paper presents the outcomes of AI-COVID19, our project aimed at better understanding of misinformation flow about COVID-19 across social media platforms. The specific focus of the study reported in this paper is on collecting data from Telegram groups which are active in promotion of COVID-related misinformation. Our corpus collected so far contains around 28 million words, from almost one million messages. Given that a substantial portion of misinformation flow in social media is spread via multimodal means, such as images and video, we have also developed a mechanism for utilising such channels via producing automatic transcripts for videos and automatic classification for images into such categories as memes, screenshots of posts and other kinds of images. The accuracy of the image classification pipeline is around 87%.</abstract>
      <url hash="22670e1f">2022.lrec-1.159</url>
      <bibkey>sosa-sharoff-2022-multimodal</bibkey>
      <pwccode url="https://github.com/josesosajs/telegram-data-collection" additional="false">josesosajs/telegram-data-collection</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cord-19">CORD-19</pwcdataset>
    </paper>
    <paper id="160">
      <title>Identifying Tension in Holocaust Survivors’ Interview: Code-switching/Code-mixing as Cues</title>
      <author><first>Xinyuan</first><last>Xia</last></author>
      <author><first>Lu</first><last>Xiao</last></author>
      <author><first>Kun</first><last>Yang</last></author>
      <author><first>Yueyue</first><last>Wang</last></author>
      <pages>1490–1495</pages>
      <abstract>In this study, we thrive on finding out how code-switching and code-mixing (CS/CM) as a linguistic phenomenon could be a sign of tension in Holocaust survivors’ interviews. We first created an interview corpus (a total of 39 interviews) that contains manually annotated CS/CM codes (a total of 802 quotations). We then compared our annotations with the tension places in the corpus. The tensions are identified by a computational tool. We found that most of our annotations were captured in the tension places, and it showed a relatively outstanding performance. The finding implies that CS/CM can be appropriate cues for detecting tension in this communication context. Our CS/CM annotated interview corpus is openly accessible. Aside from annotating and examining CS/CM occurrences, we annotated silence situations in this open corpus. Silence is shown to be an indicator of tension in interpersonal communications. Making this corpus openly accessible, we call for more research endeavors on tension detection.</abstract>
      <url hash="1af89990">2022.lrec-1.160</url>
      <bibkey>xia-etal-2022-identifying</bibkey>
    </paper>
    <paper id="161">
      <title>Fine-tuning vs From Scratch: Do Vision &amp; Language Models Have Similar Capabilities on Out-of-Distribution Visual Question Answering?</title>
      <author><first>Kristian Nørgaard</first><last>Jensen</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <pages>1496–1508</pages>
      <abstract>Fine-tuning general-purpose pre-trained models has become a de-facto standard, also for Vision and Language tasks such as Visual Question Answering (VQA). In this paper, we take a step back and ask whether a fine-tuned model has superior linguistic and reasoning capabilities than a prior state-of-the-art architecture trained from scratch on the training data alone. We perform a fine-grained evaluation on out-of-distribution data, including an analysis on robustness due to linguistic variation (rephrasings). Our empirical results confirm the benefit of pre-training on overall performance and rephrasing in particular. But our results also uncover surprising limitations, particularly for answering questions involving boolean operations. To complement the empirical evaluation, this paper also surveys relevant earlier work on 1) available VQA data sets, 2) models developed for VQA, 3) pre-trained Vision+Language models, and 4) earlier fine-grained evaluation of pre-trained Vision+Language models.</abstract>
      <url hash="f5d9970b">2022.lrec-1.161</url>
      <bibkey>jensen-plank-2022-fine</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/c4">C4</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/clevr">CLEVR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/daquar">DAQUAR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/gqa">GQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/textvqa">TextVQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-genome">Visual Genome</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering-v2-0">Visual Question Answering v2.0</pwcdataset>
    </paper>
    <paper id="162">
      <title>Multilingual Image Corpus – Towards a Multimodal and Multilingual Dataset</title>
      <author><first>Svetla</first><last>Koeva</last></author>
      <author><first>Ivelina</first><last>Stoyanova</last></author>
      <author><first>Jordan</first><last>Kralev</last></author>
      <pages>1509–1518</pages>
      <abstract>One of the processing tasks for large multimodal data streams is automatic image description (image classification, object segmentation and classification). Although the number and the diversity of image datasets is constantly expanding, still there is a huge demand for more datasets in terms of variety of domains and object classes covered. The goal of the project Multilingual Image Corpus (MIC 21) is to provide a large image dataset with annotated objects and object descriptions in 24 languages. The Multilingual Image Corpus consists of an Ontology of visual objects (based on WordNet) and a collection of thematically related images whose objects are annotated with segmentation masks and labels describing the ontology classes. The dataset is designed both for image classification and object detection and for semantic segmentation. The main contributions of our work are: a) the provision of large collection of high quality copyright-free images; b) the formulation of the Ontology of visual objects based on WordNet noun hierarchies; c) the precise manual correction of automatic object segmentation within the images and the annotation of object classes; and d) the association of objects and images with extended multilingual descriptions based on WordNet inner- and interlingual relations. The dataset can be used also for multilingual image caption generation, image-to-text alignment and automatic question answering for images and videos.</abstract>
      <url hash="135fbbdb">2022.lrec-1.162</url>
      <bibkey>koeva-etal-2022-multilingual</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
    </paper>
    <paper id="163">
      <title>Sign Language Production With Avatar Layering: A Critical Use Case over Rare Words</title>
      <author><first>Jung-Ho</first><last>Kim</last></author>
      <author><first>Eui Jun</first><last>Hwang</last></author>
      <author><first>Sukmin</first><last>Cho</last></author>
      <author><first>Du Hui</first><last>Lee</last></author>
      <author><first>Jong</first><last>Park</last></author>
      <pages>1519–1528</pages>
      <abstract>Sign language production (SLP) is the process of generating sign language videos from spoken language expressions. Since sign languages are highly under-resourced, existing vision-based SLP approaches suffer from out-of-vocabulary (OOV) and test-time generalization problems and thus generate low-quality translations. To address these problems, we introduce an avatar-based SLP system composed of a sign language translation (SLT) model and an avatar animation generation module. Our Transformer-based SLT model utilizes two additional strategies to resolve these problems: named entity transformation to reduce OOV tokens and context vector generation using a pretrained language model (e.g., BERT) to reliably train the decoder. Our system is validated on a new Korean-Korean Sign Language (KSL) dataset of weather forecasts and emergency announcements. Our SLT model achieves an 8.77 higher BLEU-4 score and a 4.57 higher ROUGE-L score over those of our baseline model. In a user evaluation, 93.48% of named entities were successfully identified by participants, demonstrating marked improvement on OOV issues.</abstract>
      <url hash="e617265b">2022.lrec-1.163</url>
      <bibkey>kim-etal-2022-sign</bibkey>
    </paper>
    <paper id="164">
      <title>The <fixed-case>V</fixed-case>ox<fixed-case>W</fixed-case>orld Platform for Multimodal Embodied Agents</title>
      <author><first>Nikhil</first><last>Krishnaswamy</last></author>
      <author><first>William</first><last>Pickard</last></author>
      <author><first>Brittany</first><last>Cates</last></author>
      <author><first>Nathaniel</first><last>Blanchard</last></author>
      <author><first>James</first><last>Pustejovsky</last></author>
      <pages>1529–1541</pages>
      <abstract>We present a five-year retrospective on the development of the VoxWorld platform, first introduced as a multimodal platform for modeling motion language, that has evolved into a platform for rapidly building and deploying embodied agents with contextual and situational awareness, capable of interacting with humans in multiple modalities, and exploring their environments. In particular, we discuss the evolution from the theoretical underpinnings of the VoxML modeling language to a platform that accommodates both neural and symbolic inputs to build agents capable of multimodal interaction and hybrid reasoning. We focus on three distinct agent implementations and the functionality needed to accommodate all of them: Diana, a virtual collaborative agent; Kirby, a mobile robot; and BabyBAW, an agent who self-guides its own exploration of the world.</abstract>
      <url hash="6085ed69">2022.lrec-1.164</url>
      <bibkey>krishnaswamy-etal-2022-voxworld</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/openai-gym">OpenAI Gym</pwcdataset>
    </paper>
    <paper id="165">
      <title><fixed-case>M</fixed-case>emo<fixed-case>S</fixed-case>en: A Multimodal Dataset for Sentiment Analysis of Memes</title>
      <author><first>Eftekhar</first><last>Hossain</last></author>
      <author><first>Omar</first><last>Sharif</last></author>
      <author><first>Mohammed Moshiul</first><last>Hoque</last></author>
      <pages>1542–1554</pages>
      <abstract>Posting and sharing memes have become a powerful expedient of expressing opinions on social media in recent days. Analysis of sentiment from memes has gained much attention to researchers due to its substantial implications in various domains like finance and politics. Past studies on sentiment analysis of memes have primarily been conducted in English, where low-resource languages gain little or no attention. However, due to the proliferation of social media usage in recent years, sentiment analysis of memes is also a crucial research issue in low resource languages. The scarcity of benchmark datasets is a significant barrier to performing multimodal sentiment analysis research in resource-constrained languages like Bengali. This paper presents a novel multimodal dataset (named MemoSen) for Bengali containing 4417 memes with three annotated labels positive, negative, and neutral. A detailed annotation guideline is provided to facilitate further resource development in this domain. Additionally, a set of experiments are carried out on MemoSen by constructing twelve unimodal (i.e., visual, textual) and ten multimodal (image+text) models. The evaluation exhibits that the integration of multimodal information significantly improves (about 1.2%) the meme sentiment classification compared to the unimodal counterparts and thus elucidate the novel aspects of multimodality.</abstract>
      <url hash="ebac0642">2022.lrec-1.165</url>
      <bibkey>hossain-etal-2022-memosen</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
    </paper>
    <paper id="166">
      <title><fixed-case>RUSAVIC</fixed-case> Corpus: <fixed-case>R</fixed-case>ussian Audio-Visual Speech in Cars</title>
      <author><first>Denis</first><last>Ivanko</last></author>
      <author><first>Alexandr</first><last>Axyonov</last></author>
      <author><first>Dmitry</first><last>Ryumin</last></author>
      <author><first>Alexey</first><last>Kashevnik</last></author>
      <author><first>Alexey</first><last>Karpov</last></author>
      <pages>1555–1559</pages>
      <abstract>We present a new audio-visual speech corpus (RUSAVIC) recorded in a car environment and designed for noise-robust speech recognition. Our goal was to produce a speech corpus which is natural (recorded in real driving conditions), controlled (providing different SNR levels by windows open/closed, moving/parked vehicle, etc.), and adequate size (the amount of data is enough to train state-of-the-art NN approaches). We focus on the problem of audio-visual speech recognition: with the use of automated lip-reading to improve the performance of audio-based speech recognition in the presence of severe acoustic noise caused by road traffic. We also describe the equipment and procedures used to create RUSAVIC corpus. Data are collected in a synchronous way through several smartphones located at different angles and equipped with FullHD video camera and microphone. The corpus includes the recordings of 20 drivers with minimum of 10 recording sessions for each. Besides providing a detailed description of the dataset and its collection pipeline, we evaluate several popular audio and visual speech recognition methods and present a set of baseline recognition results. At the moment RUSAVIC is a unique audio-visual corpus for the Russian language that is recorded in-the-wild condition and we make it publicly available.</abstract>
      <url hash="c71f45f9">2022.lrec-1.166</url>
      <bibkey>ivanko-etal-2022-rusavic</bibkey>
    </paper>
    <paper id="167">
      <title>A First Corpus of <fixed-case>AZ</fixed-case>ee Discourse Expressions</title>
      <author><first>Camille</first><last>Challant</last></author>
      <author><first>Michael</first><last>Filhol</last></author>
      <pages>1560–1565</pages>
      <abstract>This paper presents a corpus of AZee discourse expressions, i.e. expressions which formally describe Sign Language utterances of any length using the AZee approach and language. The construction of this corpus had two main goals: a first reference corpus for AZee, and a test of its coverage on a significant sample of real-life utterances. We worked on productions from an existing corpus, namely the “40 breves”, containing an hour of French Sign Language. We wrote the corresponding AZee discourse expressions for the entire video content, i.e. expressions capturing the forms produced by the signers and their associated meaning by combining known production rules, a basic building block for these expressions. These are made available as a version 2 extension of the “40 breves”. We explain the way in which these expressions can be built, present the resulting corpus and set of production rules used, and perform first measurements on it. We also propose an evaluation of our corpus: for one hour of discourse, AZee allows to describe 94% of it, while ongoing studies are increasing this coverage. This corpus offers a lot of future prospects, for instance concerning synthesis with virtual signers, machine translation or formal grammars for Sign Language.</abstract>
      <url hash="534f3a2f">2022.lrec-1.167</url>
      <bibkey>challant-filhol-2022-first</bibkey>
    </paper>
    <paper id="168">
      <title><fixed-case>BERTHA</fixed-case>: Video Captioning Evaluation Via Transfer-Learned Human Assessment</title>
      <author><first>Luis</first><last>Lebron</last></author>
      <author><first>Yvette</first><last>Graham</last></author>
      <author><first>Kevin</first><last>McGuinness</last></author>
      <author><first>Konstantinos</first><last>Kouramas</last></author>
      <author><first>Noel E.</first><last>O’Connor</last></author>
      <pages>1566–1575</pages>
      <abstract>Evaluating video captioning systems is a challenging task as there are multiple factors to consider; for instance: the fluency of the caption, multiple actions happening in a single scene, and the human bias of what is considered important. Most metrics try to measure how similar the system generated captions are to a single or a set of human-annotated captions. This paper presents a new method based on a deep learning model to evaluate these systems. The model is based on BERT, which is a language model that has been shown to work well in multiple NLP tasks. The aim is for the model to learn to perform an evaluation similar to that of a human. To do so, we use a dataset that contains human evaluations of system generated captions. The dataset consists of the human judgments of the captions produces by the system participating in various years of the TRECVid video to text task. BERTHA obtain favourable results, outperforming the commonly used metrics in some setups.</abstract>
      <url hash="2af2dac2">2022.lrec-1.168</url>
      <bibkey>lebron-etal-2022-bertha</bibkey>
      <pwccode url="https://github.com/LLebronC/TRECvid-VTT_HA" additional="false">LLebronC/TRECvid-VTT_HA</pwccode>
    </paper>
    <paper id="169">
      <title><fixed-case>A</fixed-case>bstract <fixed-case>M</fixed-case>eaning <fixed-case>R</fixed-case>epresentation for Gesture</title>
      <author><first>Richard</first><last>Brutti</last></author>
      <author><first>Lucia</first><last>Donatelli</last></author>
      <author><first>Kenneth</first><last>Lai</last></author>
      <author><first>James</first><last>Pustejovsky</last></author>
      <pages>1576–1583</pages>
      <abstract>This paper presents Gesture AMR, an extension to Abstract Meaning Representation (AMR), that captures the meaning of gesture. In developing Gesture AMR, we consider how gesture form and meaning relate; how gesture packages meaning both independently and in interaction with speech; and how the meaning of gesture is temporally and contextually determined. Our case study for developing Gesture AMR is a focused human-human shared task to build block structures. We develop an initial taxonomy of gesture act relations that adheres to AMR’s existing focus on predicate-argument structure while integrating meaningful elements unique to gesture. Pilot annotation shows Gesture AMR to be more challenging than standard AMR, and illustrates the need for more work on representation of dialogue and multimodal meaning. We discuss challenges of adapting an existing meaning representation to non-speech-based modalities and outline several avenues for expanding Gesture AMR.</abstract>
      <url hash="f7155346">2022.lrec-1.169</url>
      <bibkey>brutti-etal-2022-abstract</bibkey>
    </paper>
    <paper id="170">
      <title>The <fixed-case>GINCO</fixed-case> Training Dataset for Web Genre Identification of Documents Out in the Wild</title>
      <author><first>Taja</first><last>Kuzman</last></author>
      <author><first>Peter</first><last>Rupnik</last></author>
      <author><first>Nikola</first><last>Ljubešić</last></author>
      <pages>1584–1594</pages>
      <abstract>This paper presents a new training dataset for automatic genre identification GINCO, which is based on 1,125 crawled Slovenian web documents that consist of 650,000 words. Each document was manually annotated for genre with a new annotation schema that builds upon existing schemata, having primarily clarity of labels and inter-annotator agreement in mind. The dataset consists of various challenges related to web-based data, such as machine translated content, encoding errors, multiple contents presented in one document etc., enabling evaluation of classifiers in realistic conditions. The initial machine learning experiments on the dataset show that (1) pre-Transformer models are drastically less able to model the phenomena, with macro F1 metrics ranging around 0.22, while Transformer-based models achieve scores of around 0.58, and (2) multilingual Transformer models work as well on the task as the monolingual models that were previously proven to be superior to multilingual models on standard NLP tasks.</abstract>
      <url hash="cbcddb89">2022.lrec-1.170</url>
      <bibkey>kuzman-etal-2022-ginco</bibkey>
    </paper>
    <paper id="171">
      <title>The Spoken Language Understanding <fixed-case>MEDIA</fixed-case> Benchmark Dataset in the Era of Deep Learning: data updates, training and evaluation tools</title>
      <author><first>Gaëlle</first><last>Laperrière</last></author>
      <author><first>Valentin</first><last>Pelloin</last></author>
      <author><first>Antoine</first><last>Caubrière</last></author>
      <author><first>Salima</first><last>Mdhaffar</last></author>
      <author><first>Nathalie</first><last>Camelin</last></author>
      <author><first>Sahar</first><last>Ghannay</last></author>
      <author><first>Bassam</first><last>Jabaian</last></author>
      <author><first>Yannick</first><last>Estève</last></author>
      <pages>1595–1602</pages>
      <abstract>With the emergence of neural end-to-end approaches for spoken language understanding (SLU), a growing number of studies have been presented during these last three years on this topic. The major part of these works addresses the spoken language understanding domain through a simple task like speech intent detection. In this context, new benchmark datasets have also been produced and shared with the community related to this task. In this paper, we focus on the French MEDIA SLU dataset, distributed since 2005 and used as a benchmark dataset for a large number of research works. This dataset has been shown as being the most challenging one among those accessible to the research community. Distributed by ELRA, this corpus is free for academic research since 2019. Unfortunately, the MEDIA dataset is not really used beyond the French research community. To facilitate its use, a complete recipe, including data preparation, training and evaluation scripts, has been built and integrated to SpeechBrain, an already popular open-source and all-in-one conversational AI toolkit based on PyTorch. This recipe is presented in this paper. In addition, based on the feedback of some researchers who have worked on this dataset for several years, some corrections have been brought to the initial manual annotation: the new version of the data will also be integrated into the ELRA catalogue, as the original one. More, a significant amount of data collected during the construction of the MEDIA corpus in the 2000s was never used until now: we present the first results reached on this subset — also included in the MEDIA SpeechBrain recipe — , that will be used for now as the MEDIA test2. Last, we discuss evaluation issues.</abstract>
      <url hash="d75cd078">2022.lrec-1.171</url>
      <bibkey>laperriere-etal-2022-spoken</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/atis">ATIS</pwcdataset>
    </paper>
    <paper id="172">
      <title><fixed-case>B</fixed-case>asque<fixed-case>GLUE</fixed-case>: A Natural Language Understanding Benchmark for <fixed-case>B</fixed-case>asque</title>
      <author><first>Gorka</first><last>Urbizu</last></author>
      <author><first>Iñaki</first><last>San Vicente</last></author>
      <author><first>Xabier</first><last>Saralegi</last></author>
      <author><first>Rodrigo</first><last>Agerri</last></author>
      <author><first>Aitor</first><last>Soroa</last></author>
      <pages>1603–1612</pages>
      <abstract>Natural Language Understanding (NLU) technology has improved significantly over the last few years and multitask benchmarks such as GLUE are key to evaluate this improvement in a robust and general way. These benchmarks take into account a wide and diverse set of NLU tasks that require some form of language understanding, beyond the detection of superficial, textual clues. However, they are costly to develop and language-dependent, and therefore they are only available for a small number of languages. In this paper, we present BasqueGLUE, the first NLU benchmark for Basque, a less-resourced language, which has been elaborated from previously existing datasets and following similar criteria to those used for the construction of GLUE and SuperGLUE. We also report the evaluation of two state-of-the-art language models for Basque on BasqueGLUE, thus providing a strong baseline to compare upon. BasqueGLUE is freely available under an open license.</abstract>
      <url hash="e6478608">2022.lrec-1.172</url>
      <bibkey>urbizu-etal-2022-basqueglue</bibkey>
      <pwccode url="https://github.com/elhuyar/basqueglue" additional="false">elhuyar/basqueglue</pwccode>
    </paper>
    <paper id="173">
      <title>Resources and Experiments on Sentiment Classification for <fixed-case>G</fixed-case>eorgian</title>
      <author><first>Nicolas</first><last>Stefanovitch</last></author>
      <author><first>Jakub</first><last>Piskorski</last></author>
      <author><first>Sopho</first><last>Kharazi</last></author>
      <pages>1613–1621</pages>
      <abstract>This paper presents, to the best of our knowledge, the first ever publicly available annotated dataset for sentiment classification and semantic polarity dictionary for Georgian. The characteristics of these resources and the process of their creation are described in detail. The results of various experiments on the performance of both lexicon- and machine learning-based models for Georgian sentiment classification are also reported. Both 3-label (positive, neutral, negative) and 4-label settings (same labels + mixed) are considered. The machine learning models explored include, i.a., logistic regression, SVMs, and transformed-based models. We also explore transfer learning- and translation-based (to a well-supported language) approaches. The obtained results for Georgian are on par with the state-of-the-art results in sentiment classification for well studied languages when using training data of comparable size.</abstract>
      <url hash="affe10d5">2022.lrec-1.173</url>
      <bibkey>stefanovitch-etal-2022-resources</bibkey>
    </paper>
    <paper id="174">
      <title><fixed-case>C</fixed-case>o<fixed-case>F</fixed-case>i<fixed-case>F</fixed-case> Plus: A <fixed-case>F</fixed-case>rench Financial Narrative Summarisation Corpus</title>
      <author><first>Nadhem</first><last>Zmandar</last></author>
      <author><first>Tobias</first><last>Daudert</last></author>
      <author><first>Sina</first><last>Ahmadi</last></author>
      <author><first>Mahmoud</first><last>El-Haj</last></author>
      <author><first>Paul</first><last>Rayson</last></author>
      <pages>1622–1639</pages>
      <abstract>Natural Language Processing is increasingly being applied in the finance and business industry to analyse the text of many different types of financial documents. Given the increasing growth of firms around the world, the volume of financial disclosures and financial texts in different languages and forms is increasing sharply and therefore the study of language technology methods that automatically summarise content has grown rapidly into a major research area. Corpora for financial narrative summarisation exists in English, but there is a significant lack of financial text resources in the French language. To remedy this, we present CoFiF Plus, the first French financial narrative summarisation dataset providing a comprehensive set of financial text written in French. The dataset has been extracted from French financial reports published in PDF file format. It is composed of 1,703 reports from the most capitalised companies in France (Euronext Paris) covering a time frame from 1995 to 2021. This paper describes the collection, annotation and validation of the financial reports and their summaries. It also describes the dataset and gives the results of some baseline summarisers. Our datasets will be openly available upon the acceptance of the paper.</abstract>
      <url hash="260c0eb6">2022.lrec-1.174</url>
      <bibkey>zmandar-etal-2022-cofif</bibkey>
    </paper>
    <paper id="175">
      <title>Generating Extended and Multilingual Summaries with Pre-trained Transformers</title>
      <author><first>Rémi</first><last>Calizzano</last></author>
      <author><first>Malte</first><last>Ostendorff</last></author>
      <author><first>Qian</first><last>Ruan</last></author>
      <author><first>Georg</first><last>Rehm</last></author>
      <pages>1640–1650</pages>
      <abstract>Almost all summarisation methods and datasets focus on a single language and short summaries. We introduce a new dataset called WikinewsSum for English, German, French, Spanish, Portuguese, Polish, and Italian summarisation tailored for extended summaries of approx. 11 sentences. The dataset comprises 39,626 summaries which are news articles from Wikinews and their sources. We compare three multilingual transformer models on the extractive summarisation task and three training scenarios on which we fine-tune mT5 to perform abstractive summarisation. This results in strong baselines for both extractive and abstractive summarisation on WikinewsSum. We also show how the combination of an extractive model with an abstractive one can be used to create extended abstractive summaries from long input documents. Finally, our results show that fine-tuning mT5 on all the languages combined significantly improves the summarisation performance on low-resource languages.</abstract>
      <url hash="03e3f901">2022.lrec-1.175</url>
      <bibkey>calizzano-etal-2022-generating</bibkey>
      <pwccode url="https://github.com/airklizz/mdmls" additional="true">airklizz/mdmls</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multi-news">Multi-News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikisum">WikiSum</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xl-sum">XL-Sum</pwcdataset>
    </paper>
    <paper id="176">
      <title><fixed-case>MUSS</fixed-case>: Multilingual Unsupervised Sentence Simplification by Mining Paraphrases</title>
      <author><first>Louis</first><last>Martin</last></author>
      <author><first>Angela</first><last>Fan</last></author>
      <author><first>Éric</first><last>de la Clergerie</last></author>
      <author><first>Antoine</first><last>Bordes</last></author>
      <author><first>Benoît</first><last>Sagot</last></author>
      <pages>1651–1664</pages>
      <abstract>Progress in sentence simplification has been hindered by a lack of labeled parallel simplification data, particularly in languages other than English. We introduce MUSS, a Multilingual Unsupervised Sentence Simplification system that does not require labeled simplification data. MUSS uses a novel approach to sentence simplification that trains strong models using sentence-level paraphrase data instead of proper simplification data. These models leverage unsupervised pretraining and controllable generation mechanisms to flexibly adjust attributes such as length and lexical complexity at inference time. We further present a method to mine such paraphrase data in any language from Common Crawl using semantic sentence embeddings, thus removing the need for labeled data. We evaluate our approach on English, French, and Spanish simplification benchmarks and closely match or outperform the previous best supervised results, despite not using any labeled simplification data. We push the state of the art further by incorporating labeled simplification data.</abstract>
      <url hash="c35475fd">2022.lrec-1.176</url>
      <bibkey>martin-etal-2022-muss</bibkey>
      <pwccode url="https://github.com/facebookresearch/muss" additional="false">facebookresearch/muss</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/asset">ASSET</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsela">Newsela</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/turkcorpus">TurkCorpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikilarge">WikiLarge</pwcdataset>
    </paper>
    <paper id="177">
      <title>Towards Understanding Gender-Seniority Compound Bias in Natural Language Generation</title>
      <author><first>Samhita</first><last>Honnavalli</last></author>
      <author><first>Aesha</first><last>Parekh</last></author>
      <author><first>Lily</first><last>Ou</last></author>
      <author><first>Sophie</first><last>Groenwold</last></author>
      <author><first>Sharon</first><last>Levy</last></author>
      <author><first>Vicente</first><last>Ordonez</last></author>
      <author><first>William Yang</first><last>Wang</last></author>
      <pages>1665–1670</pages>
      <abstract>Women are often perceived as junior to their male counterparts, even within the same job titles. While there has been significant progress in the evaluation of gender bias in natural language processing (NLP), existing studies seldom investigate how biases toward gender groups change when compounded with other societal biases. In this work, we investigate how seniority impacts the degree of gender bias exhibited in pretrained neural generation models by introducing a novel framework for probing compound bias. We contribute a benchmark robustness-testing dataset spanning two domains, U.S. senatorship and professorship, created using a distant-supervision method. Our dataset includes human-written text with underlying ground truth and paired counterfactuals. We then examine GPT-2 perplexity and the frequency of gendered language in generated text. Our results show that GPT-2 amplifies bias by considering women as junior and men as senior more often than the ground truth in both domains. These results suggest that NLP applications built using GPT-2 may harm women in professional capacities.</abstract>
      <url hash="75bf6b26">2022.lrec-1.177</url>
      <bibkey>honnavalli-etal-2022-towards</bibkey>
      <pwccode url="https://github.com/aeshapar/gender-seniority-compound-bias-dataset" additional="false">aeshapar/gender-seniority-compound-bias-dataset</pwccode>
    </paper>
    <paper id="178">
      <title>Combining <fixed-case>ELECTRA</fixed-case> and Adaptive Graph Encoding for Frame Identification</title>
      <author><first>Fabio</first><last>Tamburini</last></author>
      <pages>1671–1679</pages>
      <abstract>This paper presents contributions in two directions: first we propose a new system for Frame Identification (FI), based on pre-trained text encoders trained discriminatively and graphs embedding, producing state of the art performance and, second, we take in consideration all the extremely different procedures used to evaluate systems for this task performing a complete evaluation over two benchmarks and all possible splits and cleaning procedures used in the FI literature.</abstract>
      <url hash="16fdf9c1">2022.lrec-1.178</url>
      <bibkey>tamburini-2022-combining</bibkey>
      <pwccode url="https://github.com/ftamburin/electra-age_fe" additional="false">ftamburin/electra-age_fe</pwccode>
    </paper>
    <paper id="179">
      <title>Polysemy in Spoken Conversations and Written Texts</title>
      <author><first>Aina</first><last>Garí Soler</last></author>
      <author><first>Matthieu</first><last>Labeau</last></author>
      <author><first>Chloé</first><last>Clavel</last></author>
      <pages>1680–1690</pages>
      <abstract>Our discourses are full of potential lexical ambiguities, due in part to the pervasive use of words having multiple senses. Sometimes, one word may even be used in more than one sense throughout a text. But, to what extent is this true for different kinds of texts? Does the use of polysemous words change when a discourse involves two people, or when speakers have time to plan what to say? We investigate these questions by comparing the polysemy level of texts of different nature, with a focus on spontaneous spoken dialogs; unlike previous work which examines solely scripted, written, monolog-like data. We compare multiple metrics that presuppose different conceptualizations of text polysemy, i.e., they consider the observed or the potential number of senses of words, or their sense distribution in a discourse. We show that the polysemy level of texts varies greatly depending on the kind of text considered, with dialog and spoken discourses having generally a higher polysemy level than written monologs. Additionally, our results emphasize the need for relaxing the popular “one sense per discourse” hypothesis.</abstract>
      <url hash="189c31e7">2022.lrec-1.179</url>
      <bibkey>gari-soler-etal-2022-polysemy</bibkey>
      <pwccode url="https://github.com/ainagari/spoken_poly" additional="false">ainagari/spoken_poly</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/iemocap">IEMOCAP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/senseval-2-1">Senseval-2</pwcdataset>
    </paper>
    <paper id="180">
      <title>Cross-Level Semantic Similarity for <fixed-case>S</fixed-case>erbian Newswire Texts</title>
      <author><first>Vuk</first><last>Batanović</last></author>
      <author><first>Maja</first><last>Miličević Petrović</last></author>
      <pages>1691–1699</pages>
      <abstract>Cross-Level Semantic Similarity (CLSS) is a measure of the level of semantic overlap between texts of different lengths. Although this problem was formulated almost a decade ago, research on it has been sparse, and limited exclusively to the English language. In this paper, we present the first CLSS dataset in another language, in the form of CLSS.news.sr – a corpus of 1000 phrase-sentence and 1000 sentence-paragraph newswire text pairs in Serbian, manually annotated with fine-grained semantic similarity scores using a 0–4 similarity scale. We describe the methodology of data collection and annotation, and compare the resulting corpus to its preexisting counterpart in English, SemEval CLSS, following up with a preliminary linguistic analysis of the newly created dataset. State-of-the-art pre-trained language models are then fine-tuned and evaluated on the CLSS task in Serbian using the produced data, and their settings and results are discussed. The CLSS.news.sr corpus and the guidelines used in its creation are made publicly available.</abstract>
      <url hash="18d830c3">2022.lrec-1.180</url>
      <bibkey>batanovic-milicevic-petrovic-2022-cross</bibkey>
    </paper>
    <paper id="181">
      <title>Universal <fixed-case>P</fixed-case>roposition <fixed-case>B</fixed-case>ank 2.0</title>
      <author><first>Ishan</first><last>Jindal</last></author>
      <author><first>Alexandre</first><last>Rademaker</last></author>
      <author><first>Michał</first><last>Ulewicz</last></author>
      <author><first>Ha</first><last>Linh</last></author>
      <author><first>Huyen</first><last>Nguyen</last></author>
      <author><first>Khoi-Nguyen</first><last>Tran</last></author>
      <author><first>Huaiyu</first><last>Zhu</last></author>
      <author><first>Yunyao</first><last>Li</last></author>
      <pages>1700–1711</pages>
      <abstract>Semantic role labeling (SRL) represents the meaning of a sentence in the form of predicate-argument structures. Such shallow semantic analysis is helpful in a wide range of downstream NLP tasks and real-world applications. As treebanks enabled the development of powerful syntactic parsers, the accurate predicate-argument analysis demands training data in the form of propbanks. Unfortunately, most languages simply do not have corresponding propbanks due to the high cost required to construct such resources. To overcome such challenges, Universal Proposition Bank 1.0 (UP1.0) was released in 2017, with high-quality propbank data generated via a two-stage method exploiting monolingual SRL and multilingual parallel data. In this paper, we introduce Universal Proposition Bank 2.0 (UP2.0), with significant enhancements over UP1.0: (1) propbanks with higher quality by using a state-of-the-art monolingual SRL and improved auto-generation of annotations; (2) expanded language coverage (from 7 to 9 languages); (3) span annotation for the decoupling of syntactic analysis; and (4) Gold data for a subset of the languages. We also share our experimental results that confirm the significant quality improvements of the generated propbanks. In addition, we present a comprehensive experimental evaluation on how different implementation choices impact the quality of the resulting data. We release these resources to the research community and hope to encourage more research on cross-lingual SRL.</abstract>
      <url hash="0a9433d6">2022.lrec-1.181</url>
      <bibkey>jindal-etal-2022-universal</bibkey>
    </paper>
    <paper id="182">
      <title>The Copenhagen Corpus of Eye Tracking Recordings from Natural Reading of <fixed-case>D</fixed-case>anish Texts</title>
      <author><first>Nora</first><last>Hollenstein</last></author>
      <author><first>Maria</first><last>Barrett</last></author>
      <author><first>Marina</first><last>Björnsdóttir</last></author>
      <pages>1712–1720</pages>
      <abstract>Eye movement recordings from reading are one of the richest signals of human language processing. Corpora of eye movements during reading of contextualized running text is a way of making such records available for natural language processing purposes. Such corpora already exist in some languages. We present CopCo, the Copenhagen Corpus of eye tracking recordings from natural reading of Danish texts. It is the first eye tracking corpus of its kind for the Danish language. CopCo includes 1,832 sentences with 34,897 tokens of Danish text extracted from a collection of speech manuscripts. This first release of the corpus contains eye tracking data from 22 participants. It will be extended continuously with more participants and texts from other genres. We assess the data quality of the recorded eye movements and find that the extracted features are in line with related research. The dataset available here: https://osf.io/ud8s5/.</abstract>
      <url hash="a37cd39e">2022.lrec-1.182</url>
      <bibkey>hollenstein-etal-2022-copenhagen</bibkey>
    </paper>
    <paper id="183">
      <title>The Brooklyn Multi-Interaction Corpus for Analyzing Variation in Entrainment Behavior</title>
      <author><first>Andreas</first><last>Weise</last></author>
      <author><first>Matthew</first><last>McNeill</last></author>
      <author><first>Rivka</first><last>Levitan</last></author>
      <pages>1721–1731</pages>
      <abstract>We present the Brooklyn Multi-Interaction Corpus (B-MIC), a collection of dyadic conversations designed to identify speaker traits and conversation contexts that cause variations in entrainment behavior. B-MIC pairs each participant with multiple partners for an object placement game and open-ended discussions, as well as with a Wizard of Oz for a baseline of their speech. In addition to fully transcribed recordings, it includes demographic information and four completed psychological questionnaires for each subject and turn annotations for perceived emotion and acoustic outliers. This enables the study of speakers’ entrainment behavior in different contexts and the sources of variation in this behavior. In this paper, we introduce B-MIC and describe our collection, annotation, and preprocessing methodologies. We report a preliminary study demonstrating varied entrainment behavior across different conversation types and discuss the rich potential for future work on the corpus.</abstract>
      <url hash="9523861b">2022.lrec-1.183</url>
      <bibkey>weise-etal-2022-brooklyn</bibkey>
    </paper>
    <paper id="184">
      <title>Pro-<fixed-case>TEXT</fixed-case>: an Annotated Corpus of Keystroke Logs</title>
      <author><first>Aleksandra</first><last>Miletic</last></author>
      <author><first>Christophe</first><last>Benzitoun</last></author>
      <author><first>Georgeta</first><last>Cislaru</last></author>
      <author><first>Santiago</first><last>Herrera-Yanez</last></author>
      <pages>1732–1739</pages>
      <abstract>Pro-TEXT is a corpus of keystroke logs written in French. Keystroke logs are recordings of the writing process executed through a keyboard, which keep track of all actions taken by the writer (character additions, deletions, substitutions). As such, the Pro-TEXT corpus offers new insights into text genesis and underlying cognitive processes from the production perspective. A subset of the corpus is linguistically annotated with parts of speech, lemmas and syntactic dependencies, making it suitable for the study of interactions between linguistic and behavioural aspects of the writing process. The full corpus contains 202K tokens, while the annotated portion is currently 30K tokens large. The annotated content is progressively being made available in a database-like CSV format and in CoNLL format, and the work on an HTML-based visualisation tool is currently under way. To the best of our knowledge, Pro-TEXT is the first corpus of its kind in French.</abstract>
      <url hash="7e336533">2022.lrec-1.184</url>
      <bibkey>miletic-etal-2022-pro</bibkey>
    </paper>
    <paper id="185">
      <title>Work Hard, Play Hard: Collecting Acceptability Annotations through a 3<fixed-case>D</fixed-case> Game</title>
      <author><first>Federico</first><last>Bonetti</last></author>
      <author><first>Elisa</first><last>Leonardelli</last></author>
      <author><first>Daniela</first><last>Trotta</last></author>
      <author><first>Raffaele</first><last>Guarasci</last></author>
      <author><first>Sara</first><last>Tonelli</last></author>
      <pages>1740–1750</pages>
      <abstract>Corpus-based studies on acceptability judgements have always stimulated the interest of researchers, both in theoretical and computational fields. Some approaches focused on spontaneous judgements collected through different types of tasks, others on data annotated through crowd-sourcing platforms, still others relied on expert annotated data available from the literature. The release of CoLA corpus, a large-scale corpus of sentences extracted from linguistic handbooks as examples of acceptable/non acceptable phenomena in English, has revived interest in the reliability of judgements of linguistic experts vs. non-experts. Several issues are still open. In this work, we contribute to this debate by presenting a 3D video game that was used to collect acceptability judgments on Italian sentences. We analyse the resulting annotations in terms of agreement among players and by comparing them with experts’ acceptability judgments. We also discuss different game settings to assess their impact on participants’ motivation and engagement. The final dataset containing 1,062 sentences, which were selected based on majority voting, is released for future research and comparisons.</abstract>
      <url hash="5e6f2a69">2022.lrec-1.185</url>
      <bibkey>bonetti-etal-2022-work</bibkey>
      <pwccode url="https://github.com/dhfbk/itacola-dataset" additional="false">dhfbk/itacola-dataset</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/itacola">ItaCoLA</pwcdataset>
    </paper>
    <paper id="186">
      <title><fixed-case>D</fixed-case>i<fixed-case>H</fixed-case>u<fixed-case>T</fixed-case>ra: a Parallel Corpus to Analyse Differences between Human Translations</title>
      <author><first>Ekaterina</first><last>Lapshinova-Koltunski</last></author>
      <author><first>Maja</first><last>Popović</last></author>
      <author><first>Maarit</first><last>Koponen</last></author>
      <pages>1751–1760</pages>
      <abstract>This paper describes a new corpus of human translations which contains both professional and students translations. The data consists of English sources – texts from news and reviews – and their translations into Russian and Croatian, as well as of the subcorpus containing translations of the review texts into Finnish. All target languages represent mid-resourced and less or mid-investigated ones. The corpus will be valuable for studying variation in translation as it allows a direct comparison between human translations of the same source texts. The corpus will also be a valuable resource for evaluating machine translation systems. We believe that this resource will facilitate understanding and improvement of the quality issues in both human and machine translation. In the paper, we describe how the data was collected, provide information on translator groups and summarise the differences between the human translations at hand based on our preliminary results with shallow features.</abstract>
      <url hash="8a650c30">2022.lrec-1.186</url>
      <bibkey>lapshinova-koltunski-etal-2022-dihutra-parallel</bibkey>
      <pwccode url="https://github.com/katjakaterina/dihutra" additional="false">katjakaterina/dihutra</pwccode>
    </paper>
    <paper id="187">
      <title>Data Expansion Using <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et-based Semantic Expansion and Word Disambiguation for Cyberbullying Detection</title>
      <author><first>Md Saroar</first><last>Jahan</last></author>
      <author><first>Djamila Romaissa</first><last>Beddiar</last></author>
      <author><first>Mourad</first><last>Oussalah</last></author>
      <author><first>Muhidin</first><last>Mohamed</last></author>
      <pages>1761–1770</pages>
      <abstract>Automatic identification of cyberbullying from textual content is known to be a challenging task. The challenges arise from the inherent structure of cyberbullying and the lack of labeled large-scale corpus, enabling efficient machine-learning-based tools including neural networks. This paper advocates a data augmentation-based approach that could enhance the automatic detection of cyberbullying in social media texts. We use both word sense disambiguation and synonymy relation in WordNet lexical database to generate coherent equivalent utterances of cyberbullying input data. The disambiguation and semantic expansion are intended to overcome the inherent limitations of social media posts, such as an abundance of unstructured constructs and limited semantic content. Besides, to test the feasibility, a novel protocol has been employed to collect cyberbullying traces data from AskFm forum, where about a 10K-size dataset has been manually labeled. Next, the problem of cyberbullying identification is viewed as a binary classification problem using an elaborated data augmentation strategy and an appropriate classifier. For the latter, a Convolutional Neural Network (CNN) architecture with FastText and BERT was put forward, whose results were compared against commonly employed Naïve Bayes (NB) and Logistic Regression (LR) classifiers with and without data augmentation. The research outcomes were promising and yielded almost 98.4% of classifier accuracy, an improvement of more than 4% over baseline results.</abstract>
      <url hash="272c8da3">2022.lrec-1.187</url>
      <bibkey>jahan-etal-2022-data</bibkey>
    </paper>
    <paper id="188">
      <title><fixed-case>ALIGNMEET</fixed-case>: A Comprehensive Tool for Meeting Annotation, Alignment, and Evaluation</title>
      <author><first>Peter</first><last>Polák</last></author>
      <author><first>Muskaan</first><last>Singh</last></author>
      <author><first>Anna</first><last>Nedoluzhko</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <pages>1771–1779</pages>
      <abstract>Summarization is a challenging problem, and even more challenging is to manually create, correct, and evaluate the summaries. The severity of the problem grows when the inputs are multi-party dialogues in a meeting setup. To facilitate the research in this area, we present ALIGNMEET, a comprehensive tool for meeting annotation, alignment, and evaluation. The tool aims to provide an efficient and clear interface for fast annotation while mitigating the risk of introducing errors. Moreover, we add an evaluation mode that enables a comprehensive quality evaluation of meeting minutes. To the best of our knowledge, there is no such tool available. We release the tool as open source. It is also directly installable from PyPI.</abstract>
      <url hash="1875667b">2022.lrec-1.188</url>
      <bibkey>polak-etal-2022-alignmeet</bibkey>
    </paper>
    <paper id="189">
      <title><fixed-case>KS</fixed-case>o<fixed-case>F</fixed-case>: The Kassel State of Fluency Dataset – A Therapy Centered Dataset of Stuttering</title>
      <author><first>Sebastian</first><last>Bayerl</last></author>
      <author><first>Alexander</first><last>Wolff von Gudenberg</last></author>
      <author><first>Florian</first><last>Hönig</last></author>
      <author><first>Elmar</first><last>Noeth</last></author>
      <author><first>Korbinian</first><last>Riedhammer</last></author>
      <pages>1780–1787</pages>
      <abstract>Stuttering is a complex speech disorder that negatively affects an individual’s ability to communicate effectively. Persons who stutter (PWS) often suffer considerably under the condition and seek help through therapy. Fluency shaping is a therapy approach where PWSs learn to modify their speech to help them to overcome their stutter. Mastering such speech techniques takes time and practice, even after therapy. Shortly after therapy, success is evaluated highly, but relapse rates are high. To be able to monitor speech behavior over a long time, the ability to detect stuttering events and modifications in speech could help PWSs and speech pathologists to track the level of fluency. Monitoring could create the ability to intervene early by detecting lapses in fluency. To the best of our knowledge, no public dataset is available that contains speech from people who underwent stuttering therapy that changed the style of speaking. This work introduces the Kassel State of Fluency (KSoF), a therapy-based dataset containing over 5500 clips of PWSs. The clips were labeled with six stuttering-related event types: blocks, prolongations, sound repetitions, word repetitions, interjections, and – specific to therapy – speech modifications. The audio was recorded during therapy sessions at the Institut der Kasseler Stottertherapie. The data will be made available for research purposes upon request.</abstract>
      <url hash="e56eac46">2022.lrec-1.189</url>
      <bibkey>bayerl-etal-2022-ksof</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ksof">KSoF</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/librispeech">LibriSpeech</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sep-28k">SEP-28k</pwcdataset>
    </paper>
    <paper id="190">
      <title><fixed-case>EZCAT</fixed-case>: an Easy Conversation Annotation Tool</title>
      <author><first>Gaël</first><last>Guibon</last></author>
      <author><first>Luce</first><last>Lefeuvre</last></author>
      <author><first>Matthieu</first><last>Labeau</last></author>
      <author><first>Chloé</first><last>Clavel</last></author>
      <pages>1788–1797</pages>
      <abstract>Users generate content constantly, leading to new data requiring annotation. Among this data, textual conversations are created every day and come with some specificities: they are mostly private through instant messaging applications, requiring the conversational context to be labeled. These specificities led to several annotation tools dedicated to conversation, and mostly dedicated to dialogue tasks, requiring complex annotation schemata, not always customizable and not taking into account conversation-level labels. In this paper, we present EZCAT, an easy-to-use interface to annotate conversations in a two-level configurable schema, leveraging message-level labels and conversation-level labels. Our interface is characterized by the voluntary absence of a server and accounts management, enhancing its availability to anyone, and the control over data, which is crucial to confidential conversations. We also present our first usage of EZCAT along with our annotation schema we used to annotate confidential customer service conversations. EZCAT is freely available at https://gguibon.github.io/ezcat.</abstract>
      <url hash="db5da0e8">2022.lrec-1.190</url>
      <bibkey>guibon-etal-2022-ezcat</bibkey>
    </paper>
    <paper id="191">
      <title>Spoken Language Treebanks in <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies: an Overview</title>
      <author><first>Kaja</first><last>Dobrovoljc</last></author>
      <pages>1798–1806</pages>
      <abstract>Given the benefits of syntactically annotated collections of transcribed speech in spoken language research and applications, many spoken language treebanks have been developed in the last decades, with divergent annotation schemes posing important limitations to cross-resource explorations, such as comparing data across languages, grammatical frameworks, and language domains. As a consequence, there has been a growing number of spoken language treebanks adopting the Universal Dependencies (UD) annotation scheme, aimed at cross-linguistically consistent morphosyntactic annotation. In view of the non-central role of spoken language data within the scheme and with little in-domain consolidation to date, this paper presents a comparative overview of spoken language treebanks in UD to support cross-treebank data explorations on the one hand, and encourage further treebank harmonization on the other. Our results show that the spoken language treebanks differ considerably with respect to the inventory and the format of transcribed phenomena, as well as the principles adopted in their morphosyntactic annotation. This is particularly true for the dependency annotation of speech disfluencies, where conflicting data annotations suggest an underspecification of the guidelines pertaining to speech repairs in general and the reparandum dependency relation in particular.</abstract>
      <url hash="fb1a64dd">2022.lrec-1.191</url>
      <bibkey>dobrovoljc-2022-spoken</bibkey>
    </paper>
    <paper id="192">
      <title><fixed-case>L</fixed-case>e<fixed-case>C</fixed-case>on<fixed-case>T</fixed-case>ra: A Learner Corpus of <fixed-case>E</fixed-case>nglish-to-<fixed-case>D</fixed-case>utch News Translation</title>
      <author><first>Bram</first><last>Vanroy</last></author>
      <author><first>Lieve</first><last>Macken</last></author>
      <pages>1807–1816</pages>
      <abstract>We present LeConTra, a learner corpus consisting of English-to-Dutch news translations enriched with translation process data. Three students of a Master’s programme in Translation were asked to translate 50 different English journalistic texts of approximately 250 tokens each. Because we also collected translation process data in the form of keystroke logging, our dataset can be used as part of different research strands such as translation process research, learner corpus research, and corpus-based translation studies. Reference translations, without process data, are also included. The data has been manually segmented and tokenized, and manually aligned at both segment and word level, leading to a high-quality corpus with token-level process data. The data is freely accessible via the Translation Process Research DataBase, which emphasises our commitment of distributing our dataset. The tool that was built for manual sentence segmentation and tokenization, Mantis, is also available as an open-source aid for data processing.</abstract>
      <url hash="0d9d42af">2022.lrec-1.192</url>
      <bibkey>vanroy-macken-2022-lecontra</bibkey>
      <pwccode url="https://github.com/bramvanroy/lecontra" additional="true">bramvanroy/lecontra</pwccode>
    </paper>
    <paper id="193">
      <title>Annotating Attribution in <fixed-case>C</fixed-case>zech News Server Articles</title>
      <author><first>Barbora</first><last>Hladka</last></author>
      <author><first>Jiří</first><last>Mírovský</last></author>
      <author><first>Matyáš</first><last>Kopp</last></author>
      <author><first>Václav</first><last>Moravec</last></author>
      <pages>1817–1823</pages>
      <abstract>This paper focuses on detection of sources in the Czech articles published on a news server of Czech public radio. In particular, we search for attribution in sentences and we recognize attributed sources and their sentence context (signals). We organized a crowdsourcing annotation task that resulted in a data set of 2,167 stories with manually recognized signals and sources. In addition, the sources were classified into the classes of named and unnamed sources.</abstract>
      <url hash="b1fa2166">2022.lrec-1.193</url>
      <bibkey>hladka-etal-2022-annotating</bibkey>
    </paper>
    <paper id="194">
      <title>Xposition: An Online Multilingual Database of Adpositional Semantics</title>
      <author><first>Luke</first><last>Gessler</last></author>
      <author><first>Austin</first><last>Blodgett</last></author>
      <author><first>Joseph C.</first><last>Ledford</last></author>
      <author><first>Nathan</first><last>Schneider</last></author>
      <pages>1824–1830</pages>
      <abstract>We present Xposition, an online platform for documenting adpositional semantics across languages in terms of supersenses (Schneider et al., 2018). More than just a lexical database, Xposition houses annotation guidelines, structured lexicographic documentation, and annotated corpora. Guidelines and documentation are stored as wiki pages for ease of editing, and described elements (supersenses, adpositions, etc.) are hyperlinked for ease of browsing. We describe how the platform structures information; its current contents across several languages; and aspects of the design of the web application that supports it, with special attention to how it supports datasets and standards that evolve over time.</abstract>
      <url hash="13eeab97">2022.lrec-1.194</url>
      <bibkey>gessler-etal-2022-xposition</bibkey>
    </paper>
    <paper id="195">
      <title>A Study in Contradiction: Data and Annotation for <fixed-case>AIDA</fixed-case> Focusing on Informational Conflict in <fixed-case>R</fixed-case>ussia-<fixed-case>U</fixed-case>kraine Relations</title>
      <author><first>Jennifer</first><last>Tracey</last></author>
      <author><first>Ann</first><last>Bies</last></author>
      <author><first>Jeremy</first><last>Getman</last></author>
      <author><first>Kira</first><last>Griffitt</last></author>
      <author><first>Stephanie</first><last>Strassel</last></author>
      <pages>1831–1838</pages>
      <abstract>This paper describes data resources created for Phase 1 of the DARPA Active Interpretation of Disparate Alternatives (AIDA) program, which aims to develop language technology that can help humans manage large volumes of sometimes conflicting information to develop a comprehensive understanding of events around the world, even when such events are described in multiple media and languages. Especially important is the need for the technology to be capable of building multiple hypotheses to account for alternative interpretations of data imbued with informational conflict. The corpus described here is designed to support these goals. It focuses on the domain of Russia-Ukraine relations and contains multimedia source data in English, Russian and Ukrainian, annotated to support development and evaluation of systems that perform extraction of entities, events, and relations from individual multimedia documents, aggregate the information across documents and languages, and produce multiple “hypotheses” about what has happened. This paper describes source data collection, annotation, and assessment.</abstract>
      <url hash="e342b731">2022.lrec-1.195</url>
      <bibkey>tracey-etal-2022-study</bibkey>
    </paper>
    <paper id="196">
      <title>Annotating Verbal Multiword Expressions in <fixed-case>A</fixed-case>rabic: Assessing the Validity of a Multilingual Annotation Procedure</title>
      <author><first>Najet</first><last>Hadj Mohamed</last></author>
      <author><first>Cherifa</first><last>Ben Khelil</last></author>
      <author><first>Agata</first><last>Savary</last></author>
      <author><first>Iskandar</first><last>Keskes</last></author>
      <author><first>Jean-Yves</first><last>Antoine</last></author>
      <author><first>Lamia</first><last>Hadrich-Belguith</last></author>
      <pages>1839–1848</pages>
      <abstract>This paper describes our efforts to extend the PARSEME framework to Modern Standard Arabic. Theapplicability of the PARSEME guidelines was tested by measuring the inter-annotator agreement in theearly annotation stage. A subset of 1,062 sentences from the Prague Arabic Dependency Treebank PADTwas selected and annotated by two Arabic native speakers independently. Following their annotations, anew Arabic corpus with over 1,250 annotated VMWEs has been built. This corpus already exceeds thesmallest corpora of the PARSEME suite, and enables first observations. We discuss our annotation guide-line schema that shows full MWE annotation is realizable in Arabic where we get good inter-annotator agreement.</abstract>
      <url hash="842e2d04">2022.lrec-1.196</url>
      <bibkey>hadj-mohamed-etal-2022-annotating</bibkey>
    </paper>
    <paper id="197">
      <title>Annotation of Communicative Functions of Short Feedback Tokens in Switchboard</title>
      <author><first>Carol</first><last>Figueroa</last></author>
      <author><first>Adaeze</first><last>Adigwe</last></author>
      <author><first>Magalie</first><last>Ochs</last></author>
      <author><first>Gabriel</first><last>Skantze</last></author>
      <pages>1849–1859</pages>
      <abstract>There has been a lot of work on predicting the timing of feedback in conversational systems. However, there has been less focus on predicting the prosody and lexical form of feedback given their communicative function. Therefore, in this paper we present our preliminary annotations of the communicative functions of 1627 short feedback tokens from the Switchboard corpus and an analysis of their lexical realizations and prosodic characteristics. Since there is no standard scheme for annotating the communicative function of feedback we propose our own annotation scheme. Although our work is ongoing, our preliminary analysis revealed lexical tokens such as “yeah” are ambiguous and therefore lexical forms alone are not indicative of the function. Both the lexical form and prosodic characteristics need to be taken into account in order to predict the communicative function. We also found that feedback functions have distinguishable prosodic characteristics in terms of duration, mean pitch, pitch slope, and pitch range.</abstract>
      <url hash="745a3acb">2022.lrec-1.197</url>
      <bibkey>figueroa-etal-2022-annotation</bibkey>
    </paper>
    <paper id="198">
      <title>A Dataset of Offensive Language in Kosovo Social Media</title>
      <author><first>Adem</first><last>Ajvazi</last></author>
      <author><first>Christian</first><last>Hardmeier</last></author>
      <pages>1860–1869</pages>
      <abstract>Social media are a central part of people’s lives. Unfortunately, many public social media spaces are rife with bullying and offensive language, creating an unsafe environment for their users. In this paper, we present a new dataset for offensive language detection in Albanian. The dataset is composed of user-generated comments on Facebook and YouTube from the channels of selected Kosovo news platforms. It is annotated according to the three levels of the OLID annotation scheme. We also show results of a baseline system for offensive language classification based on a fine-tuned BERT model and compare with the Danish DKhate dataset, which is similar in scope and size. In a transfer learning setting, we find that merging the Albanian and Danish training sets leads to improved performance for prediction on Danish, but not Albanian, on both offensive language recognition and distinguishing targeted and untargeted offence.</abstract>
      <url hash="4ada88e7">2022.lrec-1.198</url>
      <bibkey>ajvazi-hardmeier-2022-dataset</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/dkhate">DKhate</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/olid">OLID</pwcdataset>
    </paper>
    <paper id="199">
      <title>The <fixed-case>A</fixed-case>rabic Parallel Gender Corpus 2.0: Extensions and Analyses</title>
      <author><first>Bashar</first><last>Alhafni</last></author>
      <author><first>Nizar</first><last>Habash</last></author>
      <author><first>Houda</first><last>Bouamor</last></author>
      <pages>1870–1884</pages>
      <abstract>Gender bias in natural language processing (NLP) applications, particularly machine translation, has been receiving increasing attention. Much of the research on this issue has focused on mitigating gender bias in English NLP models and systems. Addressing the problem in poorly resourced, and/or morphologically rich languages has lagged behind, largely due to the lack of datasets and resources. In this paper, we introduce a new corpus for gender identification and rewriting in contexts involving one or two target users (I and/or You) – first and second grammatical persons with independent grammatical gender preferences. We focus on Arabic, a gender-marking morphologically rich language. The corpus has multiple parallel components: four combinations of 1st and 2nd person in feminine and masculine grammatical genders, as well as English, and English to Arabic machine translation output. This corpus expands on Habash et al. (2019)’s Arabic Parallel Gender Corpus (APGC v1.0) by adding second person targets as well as increasing the total number of sentences over 6.5 times, reaching over 590K words. Our new dataset will aid the research and development of gender identification, controlled text generation, and post-editing rewrite systems that could be used to personalize NLP applications and provide users with the correct outputs based on their grammatical gender preferences. We make the Arabic Parallel Gender Corpus (APGC v2.0) publicly available</abstract>
      <url hash="55725fe7">2022.lrec-1.199</url>
      <bibkey>alhafni-etal-2022-arabic</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
    </paper>
    <paper id="200">
      <title>The Engage Corpus: A Social Media Dataset for Text-Based Recommender Systems</title>
      <author><first>Daniel</first><last>Cheng</last></author>
      <author><first>Kyle</first><last>Yan</last></author>
      <author><first>Phillip</first><last>Keung</last></author>
      <author><first>Noah A.</first><last>Smith</last></author>
      <pages>1885–1889</pages>
      <abstract>Social media platforms play an increasingly important role as forums for public discourse. Many platforms use recommendation algorithms that funnel users to online groups with the goal of maximizing user engagement, which many commentators have pointed to as a source of polarization and misinformation. Understanding the role of NLP in recommender systems is an interesting research area, given the role that social media has played in world events. However, there are few standardized resources which researchers can use to build models that predict engagement with online groups on social media; each research group constructs datasets from scratch without releasing their version for reuse. In this work, we present a dataset drawn from posts and comments on the online message board Reddit. We develop baseline models for recommending subreddits to users, given the user’s post and comment history. We also study the behavior of our recommender models on subreddits that were banned in June 2020 as part of Reddit’s efforts to stop the dissemination of hate speech.</abstract>
      <url hash="d59fa8fc">2022.lrec-1.200</url>
      <bibkey>cheng-etal-2022-engage</bibkey>
    </paper>
    <paper id="201">
      <title>Annotating Arguments in a Corpus of Opinion Articles</title>
      <author><first>Gil</first><last>Rocha</last></author>
      <author><first>Luís</first><last>Trigo</last></author>
      <author><first>Henrique</first><last>Lopes Cardoso</last></author>
      <author><first>Rui</first><last>Sousa-Silva</last></author>
      <author><first>Paula</first><last>Carvalho</last></author>
      <author><first>Bruno</first><last>Martins</last></author>
      <author><first>Miguel</first><last>Won</last></author>
      <pages>1890–1899</pages>
      <abstract>Interest in argument mining has resulted in an increasing number of argument annotated corpora. However, most focus on English texts with explicit argumentative discourse markers, such as persuasive essays or legal documents. Conversely, we report on the first extensive and consolidated Portuguese argument annotation project focused on opinion articles. We briefly describe the annotation guidelines based on a multi-layered process and analyze the manual annotations produced, highlighting the main challenges of this textual genre. We then conduct a comprehensive inter-annotator agreement analysis, including argumentative discourse units, their classes and relations, and resulting graphs. This analysis reveals that each of these aspects tackles very different kinds of challenges. We observe differences in annotator profiles, motivating our aim of producing a non-aggregated corpus containing the insights of every annotator. We note that the interpretation and identification of token-level arguments is challenging; nevertheless, tasks that focus on higher-level components of the argument structure can obtain considerable agreement. We lay down perspectives on corpus usage, exploiting its multi-faceted nature.</abstract>
      <url hash="d9ea45a8">2022.lrec-1.201</url>
      <bibkey>rocha-etal-2022-annotating</bibkey>
      <pwccode url="https://github.com/dargmints/op-articles-arg-pt" additional="false">dargmints/op-articles-arg-pt</pwccode>
    </paper>
    <paper id="202">
      <title><fixed-case>G</fixed-case>erman Parliamentary Corpus (<fixed-case>G</fixed-case>er<fixed-case>P</fixed-case>ar<fixed-case>C</fixed-case>or)</title>
      <author><first>Giuseppe</first><last>Abrami</last></author>
      <author><first>Mevlüt</first><last>Bagci</last></author>
      <author><first>Leon</first><last>Hammerla</last></author>
      <author><first>Alexander</first><last>Mehler</last></author>
      <pages>1900–1906</pages>
      <abstract>Parliamentary debates represent a large and partly unexploited treasure trove of publicly accessible texts. In the German-speaking area, there is a certain deficit of uniformly accessible and annotated corpora covering all German-speaking parliaments at the national and federal level. To address this gap, we introduce the German Parliamentary Corpus (GerParCor). GerParCor is a genre-specific corpus of (predominantly historical) German-language parliamentary protocols from three centuries and four countries, including state and federal level data. In addition, GerParCor contains conversions of scanned protocols and, in particular, of protocols in Fraktur converted via an OCR process based on Tesseract. All protocols were preprocessed by means of the NLP pipeline of spaCy3 and automatically annotated with metadata regarding their session date. GerParCor is made available in the XMI format of the UIMA project. In this way, GerParCor can be used as a large corpus of historical texts in the field of political communication for various tasks in NLP.</abstract>
      <url hash="3872e47c">2022.lrec-1.202</url>
      <bibkey>abrami-etal-2022-german</bibkey>
      <pwccode url="https://github.com/texttechnologylab/gerparcor" additional="false">texttechnologylab/gerparcor</pwccode>
    </paper>
    <paper id="203">
      <title><fixed-case>N</fixed-case>er<fixed-case>K</fixed-case>or+<fixed-case>C</fixed-case>ars-<fixed-case>O</fixed-case>nto<fixed-case>N</fixed-case>otes++</title>
      <author><first>Attila</first><last>Novák</last></author>
      <author><first>Borbála</first><last>Novák</last></author>
      <pages>1907–1916</pages>
      <abstract>In this paper, we present an upgraded version of the Hungarian NYTK-NerKor named entity corpus, which contains about twice as many annotated spans and 7 times as many distinct entity types as the original version. We used an extended version of the OntoNotes 5 annotation scheme including time and numerical expressions. NerKor is the newest and biggest NER corpus for Hungarian containing diverse domains. We applied cross-lingual transfer of NER models trained for other languages based on multilingual contextual language models to preannotate the corpus. We corrected the annotation semi-automatically and manually. Zero-shot preannotation was very effective with about 0.82 F1 score for the best model. We also added a 12000-token subcorpus on cars and other motor vehicles. We trained and release a transformer-based NER tagger for Hungarian using the annotation in the new corpus version, which provides similar performance to an identical model trained on the original version of the corpus.</abstract>
      <url hash="bb7b3a14">2022.lrec-1.203</url>
      <bibkey>novak-novak-2022-nerkor</bibkey>
      <pwccode url="https://github.com/ppke-nlpg/nytk-nerkor-cars-ontonotespp" additional="false">ppke-nlpg/nytk-nerkor-cars-ontonotespp</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/dan">DaN+</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/genia">GENIA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/nne">NNE</pwcdataset>
    </paper>
    <paper id="204">
      <title>A Comparative Cross Language View On Acted Databases Portraying Basic Emotions Utilising Machine Learning</title>
      <author><first>Felix</first><last>Burkhardt</last></author>
      <author><first>Anabell</first><last>Hacker</last></author>
      <author><first>Uwe</first><last>Reichel</last></author>
      <author><first>Hagen</first><last>Wierstorf</last></author>
      <author><first>Florian</first><last>Eyben</last></author>
      <author><first>Björn</first><last>Schuller</last></author>
      <pages>1917–1924</pages>
      <abstract>Since several decades emotional databases have been recorded by various laboratories. Many of them contain acted portrays of Darwin’s famous “big four” basic emotions. In this paper, we investigate in how far a selection of them are comparable by two approaches: on the one hand modeling similarity as performance in cross database machine learning experiments and on the other by analyzing a manually picked set of four acoustic features that represent different phonetic areas. It is interesting to see in how far specific databases (we added a synthetic one) perform well as a training set for others while some do not. Generally speaking, we found indications for both similarity as well as specificiality across languages.</abstract>
      <url hash="6ddbdf60">2022.lrec-1.204</url>
      <bibkey>burkhardt-etal-2022-comparative</bibkey>
    </paper>
    <paper id="205">
      <title>Nkululeko: A Tool For Rapid Speaker Characteristics Detection</title>
      <author><first>Felix</first><last>Burkhardt</last></author>
      <author><first>Johannes</first><last>Wagner</last></author>
      <author><first>Hagen</first><last>Wierstorf</last></author>
      <author><first>Florian</first><last>Eyben</last></author>
      <author><first>Björn</first><last>Schuller</last></author>
      <pages>1925–1932</pages>
      <abstract>We present advancements with a software tool called Nkululeko, that lets users perform (semi-) supervised machine learning experiments in the speaker characteristics domain. It is based on audformat, a format for speech database metadata description. Due to an interface based on configurable templates, it supports best practise and very fast setup of experiments without the need to be proficient in the underlying language: Python. The paper explains the handling of Nkululeko and presents two typical experiments: comparing the expert acoustic features with artificial neural net embeddings for emotion classification and speaker age regression.</abstract>
      <url hash="b86c39a1">2022.lrec-1.205</url>
      <bibkey>burkhardt-etal-2022-nkululeko</bibkey>
    </paper>
    <paper id="206">
      <title>Speech Aerodynamics Database, Tools and Visualisation</title>
      <author><first>Shi</first><last>Yu</last></author>
      <author><first>Clara</first><last>Ponchard</last></author>
      <author><first>Roland</first><last>Trouville</last></author>
      <author><first>Sergio</first><last>Hassid</last></author>
      <author><first>Didier</first><last>Demolin</last></author>
      <pages>1933–1938</pages>
      <abstract>Aerodynamic processes underlie the characteristics of the acoustic signal of speech sounds. The aerodynamics of speech give insights on acoustic outcome and help explain the mechanisms of speech production. This database was designed during an ARC project ”Dynamique des systèmes phonologiques” in which the study of aerodynamic constraints on speech production was an important target. Data were recorded between 1996 and 1999 at the Erasmus Hospital (Hôpital Erasme) of Université Libre de Bruxelles, Belgium and constitute one of the few datasets available on direct measurement of subglottal pressure and other aerodynamic parameters. The goal was to obtain a substantial amount of data with simultaneous recording, in various context, of the speech acoustic signal, subglottal pressure (Ps), intraoral pressure (Po), oral airflow (Qo) and nasal airflow (Qn). This database contains recordings of 2 English, 1 Amharic, and 7 French speakers and is provided with data conversion and visualisation tools. Another aim of this project was to obtain some reference values of the aerodynamics of speech production for female and male speakers uttering different types of segments and sentences in French.</abstract>
      <url hash="7747f519">2022.lrec-1.206</url>
      <bibkey>yu-etal-2022-speech</bibkey>
    </paper>
    <paper id="207">
      <title><fixed-case>PATATRA</fixed-case> and <fixed-case>PATAF</fixed-case>req: two <fixed-case>F</fixed-case>rench databases for the documentation of within-speaker variability in speech</title>
      <author><first>Cécile</first><last>Fougeron</last></author>
      <author><first>Nicolas</first><last>Audibert</last></author>
      <author><first>Cedric</first><last>Gendrot</last></author>
      <author><first>Estelle</first><last>Chardenon</last></author>
      <author><first>Louise</first><last>Wohmann</last></author>
      <pages>1939–1944</pages>
      <abstract>Our knowledge on speech is historically built on data comparing different speakers or data averaged across speakers. Consequently, little is known on the variability in the speech of a single individual. Experimental studies have shown that speakers adapt to the linguistic and the speaking contexts, and modify their speech according to their emotional or biological condition, etc. However, it is unclear how much speakers vary from one repetition to the next, and how comparable are recordings that are collected days, months or years apart. In this paper, we introduce two French databases which contain recordings of 9 to 11 speakers recorded over 9 to 18 sessions, allowing comparisons of speech tasks with a different delay between the repetitions: 3 repetitions within the same session, 6 to 10 repetitions on different days during a two months period, 5 to 9 repetitions on different years. Speakers are recorded on a large set of speech tasks including read and spontaneous speech as well as speech-like performance tasks. In this paper, we provide detailed descriptions of the two databases and available annotations. We conclude by an illustration on how these data can inform on within-speaker variability of speech.</abstract>
      <url hash="e66c3b09">2022.lrec-1.207</url>
      <bibkey>fougeron-etal-2022-patatra</bibkey>
    </paper>
    <paper id="208">
      <title>The Makerere Radio Speech Corpus: A <fixed-case>L</fixed-case>uganda Radio Corpus for Automatic Speech Recognition</title>
      <author><first>Jonathan</first><last>Mukiibi</last></author>
      <author><first>Andrew</first><last>Katumba</last></author>
      <author><first>Joyce</first><last>Nakatumba-Nabende</last></author>
      <author><first>Ali</first><last>Hussein</last></author>
      <author><first>Joshua</first><last>Meyer</last></author>
      <pages>1945–1954</pages>
      <abstract>Building a usable radio monitoring automatic speech recognition (ASR) system is a challenging task for under-resourced languages and yet this is paramount in societies where radio is the main medium of public communication and discussions. Initial efforts by the United Nations in Uganda have proved how understanding the perceptions of rural people who are excluded from social media is important in national planning. However, these efforts are being challenged by the absence of transcribed speech datasets. In this paper, The Makerere Artificial Intelligence research lab releases a Luganda radio speech corpus of 155 hours. To our knowledge, this is the first publicly available radio dataset in sub-Saharan Africa. The paper describes the development of the voice corpus and presents baseline Luganda ASR performance results using Coqui STT toolkit, an open-source speech recognition toolkit.</abstract>
      <url hash="248a1887">2022.lrec-1.208</url>
      <bibkey>mukiibi-etal-2022-makerere</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/common-voice">Common Voice</pwcdataset>
    </paper>
    <paper id="209">
      <title>Far-Field Speaker Recognition Benchmark Derived From The <fixed-case>D</fixed-case>i<fixed-case>PC</fixed-case>o Corpus</title>
      <author><first>Mickael</first><last>Rouvier</last></author>
      <author><first>Mohammad</first><last>Mohammadamini</last></author>
      <pages>1955–1959</pages>
      <abstract>In this paper, we present a far-field speaker verification benchmark derived from the publicly-available DiPCo corpus. This corpus comprise three different tasks that involve enrollment and test conditions with single- and/or multi-channels recordings. The main goal of this corpus is to foster research in far-field and multi-channel text-independent speaker verification. Also, it can be used for other speaker recognition tasks such as dereverberation, denoising and speech enhancement. In addition, we release a Kaldi and SpeechBrain system to facilitate further research. And we validate the evaluation design with a single-microphone state-of-the-art speaker recognition system (i.e. ResNet-101). The results show that the proposed tasks are very challenging. And we hope these resources will inspire the speech community to develop new methods and systems for this challenging domain.</abstract>
      <url hash="23cafd32">2022.lrec-1.209</url>
      <bibkey>rouvier-mohammadamini-2022-far</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/voxceleb2">VoxCeleb2</pwcdataset>
    </paper>
    <paper id="210">
      <title>Evaluating Sampling-based Filler Insertion with Spontaneous <fixed-case>TTS</fixed-case></title>
      <author><first>Siyang</first><last>Wang</last></author>
      <author><first>Joakim</first><last>Gustafson</last></author>
      <author><first>Éva</first><last>Székely</last></author>
      <pages>1960–1969</pages>
      <abstract>Inserting fillers (such as “um”, “like”) to clean speech text has a rich history of study. One major application is to make dialogue systems sound more spontaneous. The ambiguity of filler occurrence and inter-speaker difference make both modeling and evaluation difficult. In this paper, we study sampling-based filler insertion, a simple yet unexplored approach to inserting fillers. We propose an objective score called Filler Perplexity (FPP). We build three models trained on two single-speaker spontaneous corpora, and evaluate them with FPP and perceptual tests. We implement two innovations in perceptual tests, (1) evaluating filler insertion on dialogue systems output, (2) synthesizing speech with neural spontaneous TTS engines. FPP proves to be useful in analysis but does not correlate well with perceptual MOS. Perceptual results show little difference between compared filler insertion models including with ground-truth, which may be due to the ambiguity of what is good filler insertion and a strong neural spontaneous TTS that produces natural speech irrespective of input. Results also show preference for filler-inserted speech synthesized with spontaneous TTS. The same test using TTS based on read speech obtains the opposite results, which shows the importance of using spontaneous TTS in evaluating filler insertions. Audio samples: www.speech.kth.se/tts-demos/LREC22</abstract>
      <url hash="9fbd9989">2022.lrec-1.210</url>
      <bibkey>wang-etal-2022-evaluating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ljspeech">LJSpeech</pwcdataset>
    </paper>
    <paper id="211">
      <title><fixed-case>BEA</fixed-case>-Base: A Benchmark for <fixed-case>ASR</fixed-case> of Spontaneous <fixed-case>H</fixed-case>ungarian</title>
      <author><first>Peter</first><last>Mihajlik</last></author>
      <author><first>Andras</first><last>Balog</last></author>
      <author><first>Tekla Etelka</first><last>Graczi</last></author>
      <author><first>Anna</first><last>Kohari</last></author>
      <author><first>Balázs</first><last>Tarján</last></author>
      <author><first>Katalin</first><last>Mady</last></author>
      <pages>1970–1977</pages>
      <abstract>Hungarian is spoken by 15 million people, still, easily accessible Automatic Speech Recognition (ASR) benchmark datasets – especially for spontaneous speech – have been practically unavailable. In this paper, we introduce BEA-Base, a subset of the BEA spoken Hungarian database comprising mostly spontaneous speech of 140 speakers. It is built specifically to assess ASR, primarily for conversational AI applications. After defining the speech recognition subsets and task, several baselines – including classic HMM-DNN hybrid and end-to-end approaches augmented by cross-language transfer learning – are developed using open-source toolkits. The best results obtained are based on multilingual self-supervised pretraining, achieving a 45% recognition error rate reduction as compared to the classical approach – without the application of an external language model or additional supervised data. The results show the feasibility of using BEA-Base for training and evaluation of Hungarian speech recognition systems.</abstract>
      <url hash="4960a08a">2022.lrec-1.211</url>
      <bibkey>mihajlik-etal-2022-bea</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/common-voice">Common Voice</pwcdataset>
    </paper>
    <paper id="212">
      <title><fixed-case>SN</fixed-case>u<fixed-case>C</fixed-case>: The <fixed-case>S</fixed-case>heffield Numbers Spoken Language Corpus</title>
      <author><first>Emma</first><last>Barker</last></author>
      <author><first>Jon</first><last>Barker</last></author>
      <author><first>Robert</first><last>Gaizauskas</last></author>
      <author><first>Ning</first><last>Ma</last></author>
      <author><first>Monica Lestari</first><last>Paramita</last></author>
      <pages>1978–1984</pages>
      <abstract>We present SNuC, the first published corpus of spoken alphanumeric identifiers of the sort typically used as serial and part numbers in the manufacturing sector. The dataset contains recordings and transcriptions of over 50 native British English speakers, speaking over 13,000 multi-character alphanumeric sequences and totalling almost 20 hours of recorded speech. We describe requirements taken into account in the designing the corpus and the methodology used to construct it. We present summary statistics describing the corpus contents, as well as a preliminary investigation into errors in spoken alphanumeric identifiers. We validate the corpus by showing how it can be used to adapt a deep learning neural network based ASR system, resulting in improved recognition accuracy on the task of spoken alphanumeric identifier recognition. Finally, we discuss further potential uses for the corpus and for the tools developed to construct it.</abstract>
      <url hash="f9e1c122">2022.lrec-1.212</url>
      <bibkey>barker-etal-2022-snuc</bibkey>
    </paper>
    <paper id="213">
      <title>The <fixed-case>M</fixed-case>an<fixed-case>D</fixed-case>i Corpus: A Spoken Corpus of <fixed-case>M</fixed-case>andarin Regional Dialects</title>
      <author><first>Liang</first><last>Zhao</last></author>
      <author><first>Eleanor</first><last>Chodroff</last></author>
      <pages>1985–1990</pages>
      <abstract>In the present paper, we introduce the ManDi Corpus, a spoken corpus of regional Mandarin dialects and Standard Mandarin. The corpus currently contains 357 recordings (about 9.6 hours) of monosyllabic words, disyllabic words, short sentences, a short passage and a poem, each produced in Standard Mandarin and in one of six regional Mandarin dialects: Beijing, Chengdu, Jinan, Taiyuan, Wuhan, and Xi’an Mandarin from 36 speakers. The corpus was collected remotely using participant-controlled smartphone recording apps. Word- and phone-level alignments were generated using Praat and the Montreal Forced Aligner. The pilot study of dialect-specific tone systems showed that with practicable design and decent recording quality, remotely collected speech data can be suitable for analysis of relative patterns in acoustic-phonetic realization. The corpus is available on OSF (https://osf.io/fgv4w/) for non-commercial use under a CC BY-NC 3.0 license.</abstract>
      <url hash="70759cbc">2022.lrec-1.213</url>
      <bibkey>zhao-chodroff-2022-mandi</bibkey>
    </paper>
    <paper id="214">
      <title>The Speed-Vel Project: a Corpus of Acoustic and Aerodynamic Data to Measure Droplets Emission During Speech Interaction</title>
      <author><first>Francesca</first><last>Carbone</last></author>
      <author><first>Gilles</first><last>Bouchet</last></author>
      <author><first>Alain</first><last>Ghio</last></author>
      <author><first>Thierry</first><last>Legou</last></author>
      <author><first>Carine</first><last>André</last></author>
      <author><first>Muriel</first><last>Lalain</last></author>
      <author><first>Sabrina</first><last>Kadri</last></author>
      <author><first>Caterina</first><last>Petrone</last></author>
      <author><first>Federica</first><last>Procino</last></author>
      <author><first>Antoine</first><last>Giovanni</last></author>
      <pages>1991–1999</pages>
      <abstract>Conversations (normal speech) or professional interactions (e.g., projected speech in the classroom) have been identified as situations with increased risk of exposure to SARS-CoV-2 due to the high production of droplets in the exhaled air. However, it is still unclear to what extent speech properties influence droplets emission during everyday life conversations. Here, we report the experimental protocol of three experiments aiming at measuring the velocity and the direction of the airflow, the number and size of droplets spread during speech interactions in French. We consider different phonetic conditions, potentially leading to a modulation of speech droplets production, such as voice intensity (normal vs. loud voice), articulation manner of phonemes (type of consonants and vowels) and prosody (i.e., the melody of the speech). Findings from these experiments will allow future simulation studies to predict the transport, dispersion and evaporation of droplets emitted under different speech conditions.</abstract>
      <url hash="cdb651ce">2022.lrec-1.214</url>
      <bibkey>carbone-etal-2022-speed</bibkey>
    </paper>
    <paper id="215">
      <title>Towards Speech-only Opinion-level Sentiment Analysis</title>
      <author><first>Annalena</first><last>Aicher</last></author>
      <author><first>Alisa</first><last>Gazizullina</last></author>
      <author><first>Aleksei</first><last>Gusev</last></author>
      <author><first>Yuri</first><last>Matveev</last></author>
      <author><first>Wolfgang</first><last>Minker</last></author>
      <pages>2000–2006</pages>
      <abstract>The growing popularity of various forms of Spoken Dialogue Systems (SDS) raises the demand for their capability of implicitly assessing the speaker’s sentiment from speech only. Mapping the latter on user preferences enables to adapt to the user and individualize the requested information while increasing user satisfaction. In this paper, we explore the integration of rank consistent ordinal regression into a speech-only sentiment prediction task performed by ResNet-like systems. Furthermore, we use speaker verification extractors trained on larger datasets as low-level feature extractors. An improvement of performance is shown by fusing sentiment and pre-extracted speaker embeddings reducing the speaker bias of sentiment predictions. Numerous experiments on Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) databases show that we beat the baselines of state-of-the-art unimodal approaches. Using speech as the only modality combined with optimizing an order-sensitive objective function gets significantly closer to the sentiment analysis results of state-of-the-art multimodal systems.</abstract>
      <url hash="a1cefc65">2022.lrec-1.215</url>
      <bibkey>aicher-etal-2022-towards-speech</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cmu-mosei">CMU-MOSEI</pwcdataset>
    </paper>
    <paper id="216">
      <title>At the Intersection of <fixed-case>NLP</fixed-case> and Sustainable Development: Exploring the Impact of Demographic-Aware Text Representations in Modeling Value on a Corpus of Interviews</title>
      <author><first>Goya</first><last>van Boven</last></author>
      <author><first>Stephanie</first><last>Hirmer</last></author>
      <author><first>Costanza</first><last>Conforti</last></author>
      <pages>2007–2021</pages>
      <abstract>This research explores automated text classification using data from Low– and Middle–Income Countries (LMICs). In particular, we explore enhancing text representations with demographic information of speakers in a privacy-preserving manner. We introduce the Demographic-Rich Qualitative UPV-Interviews Dataset (DR-QI), a rich dataset of qualitative interviews from rural communities in India and Uganda. The interviews were conducted following the latest standards for respectful interactions with illiterate speakers (Hirmer et al., 2021a). The interviews were later sentence-annotated for Automated User-Perceived Value (UPV) Classification (Conforti et al., 2020), a schema that classifies values expressed by speakers, resulting in a dataset of 5,333 sentences. We perform the UPV classification task, which consists of predicting which values are expressed in a given sentence, on the new DR-QI dataset. We implement a classification model using DistilBERT (Sanh et al., 2019), which we extend with demographic information. In order to preserve the privacy of speakers, we investigate encoding demographic information using autoencoders. We find that adding demographic information improves performance, even if such information is encoded. In addition, we find that the performance per UPV is linked to the number of occurrences of that value in our data.</abstract>
      <url hash="04ba637c">2022.lrec-1.216</url>
      <bibkey>van-boven-etal-2022-intersection</bibkey>
      <pwccode url="https://github.com/gvanboven/dr-qi" additional="false">gvanboven/dr-qi</pwccode>
    </paper>
    <paper id="217">
      <title>A Study on the Ambiguity in Human Annotation of <fixed-case>G</fixed-case>erman Oral History Interviews for Perceived Emotion Recognition and Sentiment Analysis</title>
      <author><first>Michael</first><last>Gref</last></author>
      <author><first>Nike</first><last>Matthiesen</last></author>
      <author><first>Sreenivasa</first><last>Hikkal Venugopala</last></author>
      <author><first>Shalaka</first><last>Satheesh</last></author>
      <author><first>Aswinkumar</first><last>Vijayananth</last></author>
      <author><first>Duc Bach</first><last>Ha</last></author>
      <author><first>Sven</first><last>Behnke</last></author>
      <author><first>Joachim</first><last>Köhler</last></author>
      <pages>2022–2031</pages>
      <abstract>For research in audiovisual interview archives often it is not only of interest what is said but also how. Sentiment analysis and emotion recognition can help capture, categorize and make these different facets searchable. In particular, for oral history archives, such indexing technologies can be of great interest. These technologies can help understand the role of emotions in historical remembering. However, humans often perceive sentiments and emotions ambiguously and subjectively. Moreover, oral history interviews have multi-layered levels of complex, sometimes contradictory, sometimes very subtle facets of emotions. Therefore, the question arises of the chance machines and humans have capturing and assigning these into predefined categories. This paper investigates the ambiguity in human perception of emotions and sentiment in German oral history interviews and the impact on machine learning systems. Our experiments reveal substantial differences in human perception for different emotions. Furthermore, we report from ongoing machine learning experiments with different modalities. We show that the human perceptual ambiguity and other challenges, such as class imbalance and lack of training data, currently limit the opportunities of these technologies for oral history archives. Nonetheless, our work uncovers promising observations and possibilities for further research.</abstract>
      <url hash="ccd0ade3">2022.lrec-1.217</url>
      <bibkey>gref-etal-2022-study</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cmu-mosei">CMU-MOSEI</pwcdataset>
    </paper>
    <paper id="218">
      <title>Detecting Optimism in Tweets using Knowledge Distillation and Linguistic Analysis of Optimism</title>
      <author><first>Ștefan</first><last>Cobeli</last></author>
      <author><first>Ioan-Bogdan</first><last>Iordache</last></author>
      <author><first>Shweta</first><last>Yadav</last></author>
      <author><first>Cornelia</first><last>Caragea</last></author>
      <author><first>Liviu P.</first><last>Dinu</last></author>
      <author><first>Dragoș</first><last>Iliescu</last></author>
      <pages>2032–2041</pages>
      <abstract>Finding the polarity of feelings in texts is a far-reaching task. Whilst the field of natural language processing has established sentiment analysis as an alluring problem, many feelings are left uncharted. In this study, we analyze the optimism and pessimism concepts from Twitter posts to effectively understand the broader dimension of psychological phenomenon. Towards this, we carried a systematic study by first exploring the linguistic peculiarities of optimism and pessimism in user-generated content. Later, we devised a multi-task knowledge distillation framework to simultaneously learn the target task of optimism detection with the help of the auxiliary task of sentiment analysis and hate speech detection. We evaluated the performance of our proposed approach on the benchmark Optimism/Pessimism Twitter dataset. Our extensive experiments show the superior- ity of our approach in correctly differentiating between optimistic and pessimistic users. Our human and automatic evaluation shows that sentiment analysis and hate speech detection are beneficial for optimism/pessimism detection.</abstract>
      <url hash="a4d986cb">2022.lrec-1.218</url>
      <bibkey>cobeli-etal-2022-detecting</bibkey>
    </paper>
    <paper id="219">
      <title>Dataset and Baseline for Automatic Student Feedback Analysis</title>
      <author><first>Missaka</first><last>Herath</last></author>
      <author><first>Kushan</first><last>Chamindu</last></author>
      <author><first>Hashan</first><last>Maduwantha</last></author>
      <author><first>Surangika</first><last>Ranathunga</last></author>
      <pages>2042–2049</pages>
      <abstract>In this paper, we present a student feedback corpus, which contains 3000 instances of feedback written by university students. This dataset has been annotated for aspect terms, opinion terms, polarities of the opinion terms towards targeted aspects, document-level opinion polarities and sentence separations. We develop a hierarchical taxonomy for aspect categorization, which covers all the areas of the teaching-learning process. We annotated both implicit and explicit aspects using this taxonomy. Annotation methodology, difficulties faced during the annotation, and the details about the aspect term categorization have been discussed in detail. This annotated corpus can be used for Aspect Extraction, Aspect Level Sentiment Analysis, and Document Level Sentiment Analysis. Also the baseline results for all three tasks are given in the paper.</abstract>
      <url hash="bbc33f05">2022.lrec-1.219</url>
      <bibkey>herath-etal-2022-dataset</bibkey>
    </paper>
    <paper id="220">
      <title><fixed-case>EENLP</fixed-case>: Cross-lingual <fixed-case>E</fixed-case>astern <fixed-case>E</fixed-case>uropean <fixed-case>NLP</fixed-case> Index</title>
      <author><first>Alexey</first><last>Tikhonov</last></author>
      <author><first>Alex</first><last>Malkhasov</last></author>
      <author><first>Andrey</first><last>Manoshin</last></author>
      <author><first>George-Andrei</first><last>Dima</last></author>
      <author><first>Réka</first><last>Cserháti</last></author>
      <author><first>Md.Sadek</first><last>Hossain Asif</last></author>
      <author><first>Matt</first><last>Sárdi</last></author>
      <pages>2050–2057</pages>
      <abstract>Motivated by the sparsity of NLP resources for Eastern European languages, we present a broad index of existing Eastern European language resources (90+ datasets and 45+ models) published as a github repository open for updates from the community. Furthermore, to support the evaluation of commonsense reasoning tasks, we provide hand-crafted cross-lingual datasets for five different semantic tasks (namely news categorization, paraphrase detection, Natural Language Inference (NLI) task, tweet sentiment detection, and news sentiment detection) for some of the Eastern European languages. We perform several experiments with the existing multilingual models on these datasets to define the performance baselines and compare them to the existing results for other languages.</abstract>
      <url hash="81610e13">2022.lrec-1.220</url>
      <bibkey>tikhonov-etal-2022-eenlp</bibkey>
      <pwccode url="https://github.com/altsoph/EENLP" additional="false">altsoph/EENLP</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tapaco">TaPaCo</pwcdataset>
    </paper>
    <paper id="221">
      <title><fixed-case>S</fixed-case>lovene <fixed-case>S</fixed-case>uper<fixed-case>GLUE</fixed-case> Benchmark: Translation and Evaluation</title>
      <author><first>Aleš</first><last>Žagar</last></author>
      <author><first>Marko</first><last>Robnik-Šikonja</last></author>
      <pages>2058–2065</pages>
      <abstract>We present SuperGLUE benchmark adapted and translated into Slovene using a combination of human and machine translation. We describe the translation process and problems arising due to differences in morphology and grammar. We evaluate the translated datasets in several modes: monolingual, cross-lingual, and multilingual, taking into account differences between machine and human translated training sets. The results show that the monolingual Slovene SloBERTa model is superior to massively multilingual and trilingual BERT models, but these also show a good cross-lingual performance on certain tasks. The performance of Slovene models still lags behind the best English models.</abstract>
      <url hash="470474fc">2022.lrec-1.221</url>
      <bibkey>zagar-robnik-sikonja-2022-slovene</bibkey>
    </paper>
    <paper id="222">
      <title>Speech Resources in the <fixed-case>T</fixed-case>amasheq Language</title>
      <author><first>Marcely</first><last>Zanon Boito</last></author>
      <author><first>Fethi</first><last>Bougares</last></author>
      <author><first>Florentin</first><last>Barbier</last></author>
      <author><first>Souhir</first><last>Gahbiche</last></author>
      <author><first>Loïc</first><last>Barrault</last></author>
      <author><first>Mickael</first><last>Rouvier</last></author>
      <author><first>Yannick</first><last>Estève</last></author>
      <pages>2066–2071</pages>
      <abstract>In this paper we present two datasets for Tamasheq, a developing language mainly spoken in Mali and Niger. These two datasets were made available for the IWSLT 2022 low-resource speech translation track, and they consist of collections of radio recordings from daily broadcast news in Niger (Studio Kalangou) and Mali (Studio Tamani). We share (i) a massive amount of unlabeled audio data (671 hours) in five languages: French from Niger, Fulfulde, Hausa, Tamasheq and Zarma, and (ii) a smaller 17 hours parallel corpus of audio recordings in Tamasheq, with utterance-level translations in the French language. All this data is shared under the Creative Commons BY-NC-ND 3.0 license. We hope these resources will inspire the speech community to develop and benchmark models using the Tamasheq language.</abstract>
      <url hash="3b84e44b">2022.lrec-1.222</url>
      <bibkey>zanon-boito-etal-2022-speech</bibkey>
      <pwccode url="https://github.com/mzboito/iwslt2022_tamasheq_data" additional="false">mzboito/iwslt2022_tamasheq_data</pwccode>
    </paper>
    <paper id="223">
      <title><fixed-case>A</fixed-case>esop’s fable “The North Wind and the Sun” Used as a Rosetta Stone to Extract and Map Spoken Words in Under-resourced Languages</title>
      <author><first>Elena</first><last>Knyazeva</last></author>
      <author><first>Philippe</first><last>Boula de Mareüil</last></author>
      <author><first>Frédéric</first><last>Vernier</last></author>
      <pages>2072–2079</pages>
      <abstract>This paper describes a method of semi-automatic word spotting in minority languages, from one and the same Aesop fable “The North Wind and the Sun” translated in Romance languages/dialects from Hexagonal (i.e. Metropolitan) France and languages from French Polynesia. The first task consisted of finding out how a dozen words such as “wind” and “sun” were translated in over 200 versions collected in the field — taking advantage of orthographic similarity, word position and context. Occurrences of the translations were then extracted from the phone-aligned recordings. The results were judged accurate in 96–97% of cases, both on the development corpus and a test set of unseen data. Corrected alignments were then mapped and basemaps were drawn to make various linguistic phenomena immediately visible. The paper exemplifies how regular expressions may be used for this purpose. The final result, which takes the form of an online speaking atlas (enriching the https://atlas.limsi.fr website), enables us to illustrate lexical, morphological or phonetic variation.</abstract>
      <url hash="ec3dcd54">2022.lrec-1.223</url>
      <bibkey>knyazeva-etal-2022-aesops</bibkey>
    </paper>
    <paper id="224">
      <title>Multilingual Open Text Release 1: Public Domain News in 44 Languages</title>
      <author><first>Chester</first><last>Palen-Michel</last></author>
      <author><first>June</first><last>Kim</last></author>
      <author><first>Constantine</first><last>Lignos</last></author>
      <pages>2080–2089</pages>
      <abstract>We present a Multilingual Open Text (MOT), a new multilingual corpus containing text in 44 languages, many of which have limited existing text resources for natural language processing. The first release of the corpus contains over 2.8 million news articles and an additional 1 million short snippets (photo captions, video descriptions, etc.) published between 2001–2022 and collected from Voice of America’s news websites. We describe our process for collecting, filtering, and processing the data. The source material is in the public domain, our collection is licensed using a creative commons license (CC BY 4.0), and all software used to create the corpus is released under the MIT License. The corpus will be regularly updated as additional documents are published.</abstract>
      <url hash="011f55d4">2022.lrec-1.224</url>
      <bibkey>palen-michel-etal-2022-multilingual</bibkey>
      <pwccode url="https://github.com/VietHoang1710/khmer-nltk" additional="true">VietHoang1710/khmer-nltk</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/oscar">OSCAR</pwcdataset>
    </paper>
    <paper id="225">
      <title><fixed-case>T</fixed-case>weet<fixed-case>T</fixed-case>aglish: A Dataset for Investigating <fixed-case>T</fixed-case>agalog-<fixed-case>E</fixed-case>nglish Code-Switching</title>
      <author><first>Megan</first><last>Herrera</last></author>
      <author><first>Ankit</first><last>Aich</last></author>
      <author><first>Natalie</first><last>Parde</last></author>
      <pages>2090–2097</pages>
      <abstract>Deploying recent natural language processing innovations to low-resource settings allows for state-of-the-art research findings and applications to be accessed across cultural and linguistic borders. One low-resource setting of increasing interest is code-switching, the phenomenon of combining, swapping, or alternating the use of two or more languages in continuous dialogue. In this paper, we introduce a large dataset (20k+ instances) to facilitate investigation of Tagalog-English code-switching, which has become a popular mode of discourse in Philippine culture. Tagalog is an Austronesian language and former official language of the Philippines spoken by over 23 million people worldwide, but it and Tagalog-English are under-represented in NLP research and practice. We describe our methods for data collection, as well as our labeling procedures. We analyze our resulting dataset, and finally conclude by providing results from a proof-of-concept regression task to establish dataset validity, achieving a strong performance benchmark (R2=0.797-0.909; RMSE=0.068-0.057).</abstract>
      <url hash="6bcfd25d">2022.lrec-1.225</url>
      <bibkey>herrera-etal-2022-tweettaglish</bibkey>
      <pwccode url="https://github.com/meg2121/tweettaglish-dataset" additional="false">meg2121/tweettaglish-dataset</pwccode>
    </paper>
    <paper id="226">
      <title>Jojajovai: A Parallel <fixed-case>G</fixed-case>uarani-<fixed-case>S</fixed-case>panish Corpus for <fixed-case>MT</fixed-case> Benchmarking</title>
      <author><first>Luis</first><last>Chiruzzo</last></author>
      <author><first>Santiago</first><last>Góngora</last></author>
      <author><first>Aldo</first><last>Alvarez</last></author>
      <author><first>Gustavo</first><last>Giménez-Lugo</last></author>
      <author><first>Marvin</first><last>Agüero-Torales</last></author>
      <author><first>Yliana</first><last>Rodríguez</last></author>
      <pages>2098–2107</pages>
      <abstract>This work presents a parallel corpus of Guarani-Spanish text aligned at sentence level. The corpus contains about 30,000 sentence pairs, and is structured as a collection of subsets from different sources, further split into training, development and test sets. A sample of sentences from the test set was manually annotated by native speakers in order to incorporate meta-linguistic annotations about the Guarani dialects present in the corpus and also the correctness of the alignment and translation. We also present some baseline MT experiments and analyze the results in terms of the subsets. We hope this corpus can be used as a benchmark for testing Guarani-Spanish MT systems, and aim to expand and improve the quality of the corpus in future iterations.</abstract>
      <url hash="55c7aa30">2022.lrec-1.226</url>
      <bibkey>chiruzzo-etal-2022-jojajovai</bibkey>
      <pwccode url="https://github.com/pln-fing-udelar/jojajovai" additional="false">pln-fing-udelar/jojajovai</pwccode>
    </paper>
    <paper id="227">
      <title>Assessing Multilinguality of Publicly Accessible Websites</title>
      <author><first>Rinalds</first><last>Vīksna</last></author>
      <author><first>Inguna</first><last>Skadiņa</last></author>
      <author><first>Raivis</first><last>Skadiņš</last></author>
      <author><first>Andrejs</first><last>Vasiļjevs</last></author>
      <author><first>Roberts</first><last>Rozis</last></author>
      <pages>2108–2116</pages>
      <abstract>Although information on the Internet can be shared in many languages, the language presence on the World Wide Web is very disproportionate. The problem of multilingualism on the Web, in particular access, availability and quality of information in the world’s languages, has been the subject of UNESCO focus for several decades. Making European websites more multilingual is also one of the focal targets of the Connecting Europe Facility Automated Translation (CEF AT) digital service infrastructure. In order to monitor this goal, alongside other possible solutions, CEF AT needs a methodology and easy to use tool to assess the degree of multilingualism of a given website. In this paper we investigate methods and tools that automatically analyse the language diversity of the Web and propose indicators and methodology on how to measure the multilingualism of European websites. We also introduce a prototype tool based on open-source software that helps to assess multilingualism of the Web and can be independently run at set intervals. We also present initial results obtained with our tool that allows us to conclude that multilingualism on the Web is still a problem not only at the world level, but also at the European and regional level.</abstract>
      <url hash="32799353">2022.lrec-1.227</url>
      <bibkey>viksna-etal-2022-assessing</bibkey>
    </paper>
    <paper id="228">
      <title>A Methodology for Building a Diachronic Dataset of Semantic Shifts and its Application to <fixed-case>QC</fixed-case>-<fixed-case>FR</fixed-case>-Diac-V1.0, a Free Reference for <fixed-case>F</fixed-case>rench</title>
      <author><first>David</first><last>Kletz</last></author>
      <author><first>Philippe</first><last>Langlais</last></author>
      <author><first>François</first><last>Lareau</last></author>
      <author><first>Patrick</first><last>Drouin</last></author>
      <pages>2117–2125</pages>
      <abstract>Different algorithms have been proposed to detect semantic shifts (changes in a word meaning over time) in a diachronic corpus. Yet, and somehow surprisingly, no reference corpus has been designed so far to evaluate them, leaving researchers to fallback to troublesome evaluation strategies. In this work, we introduce a methodology for the construction of a reference dataset for the evaluation of semantic shift detection, that is, a list of words where we know for sure whether they present a word meaning change over a period of interest. We leverage a state-of-the-art word-sense disambiguation model to associate a date of first appearance to all the senses of a word. Significant changes in sense distributions as well as clear stability are detected and the resulting words are inspected by experts using a dedicated interface before populating a reference dataset. As a proof of concept, we apply this methodology to a corpus of newspapers from Quebec covering the whole 20th century. We manually verified a subset of candidates, leading to QC-FR-Diac-V1.0, a corpus of 151 words allowing one to evaluate the identification of semantic shifts in French between 1910 and 1990.</abstract>
      <url hash="76eca5ee">2022.lrec-1.228</url>
      <bibkey>kletz-etal-2022-methodology</bibkey>
    </paper>
    <paper id="229">
      <title><fixed-case>CRASS</fixed-case>: A Novel Data Set and Benchmark to Test Counterfactual Reasoning of Large Language Models</title>
      <author><first>Jörg</first><last>Frohberg</last></author>
      <author><first>Frank</first><last>Binder</last></author>
      <pages>2126–2140</pages>
      <abstract>We introduce the CRASS (counterfactual reasoning assessment) data set and benchmark utilizing questionized counterfactual conditionals as a novel and powerful tool to evaluate large language models. We present the data set design and benchmark. We test six state-of-the-art models against our benchmark. Our results show that it poses a valid challenge for these models and opens up considerable room for their improvement.</abstract>
      <url hash="20287000">2022.lrec-1.229</url>
      <bibkey>frohberg-binder-2022-crass</bibkey>
      <pwccode url="https://github.com/apergo-ai/crass-data-set" additional="false">apergo-ai/crass-data-set</pwccode>
    </paper>
    <paper id="230">
      <title>Evaluating Gender Bias in Speech Translation</title>
      <author><first>Marta R.</first><last>Costa-jussà</last></author>
      <author><first>Christine</first><last>Basta</last></author>
      <author><first>Gerard I.</first><last>Gállego</last></author>
      <pages>2141–2147</pages>
      <abstract>The scientific community is increasingly aware of the necessity to embrace pluralism and consistently represent major and minor social groups. Currently, there are no standard evaluation techniques for different types of biases. Accordingly, there is an urgent need to provide evaluation sets and protocols to measure existing biases in our automatic systems. Evaluating the biases should be an essential step towards mitigating them in the systems. This paper introduces WinoST, a new freely available challenge set for evaluating gender bias in speech translation. WinoST is the speech version of WinoMT, an MT challenge set, and both follow an evaluation protocol to measure gender accuracy. Using an S-Transformer end-to-end speech translation system, we report the gender bias evaluation on four language pairs, and we reveal the inaccuracies in translations generating gender-stereotyped translations.</abstract>
      <url hash="8351b6e5">2022.lrec-1.230</url>
      <bibkey>costa-jussa-etal-2022-evaluating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/must-c">MuST-C</pwcdataset>
    </paper>
    <paper id="231">
      <title>Design Choices in Crowdsourcing Discourse Relation Annotations: The Effect of Worker Selection and Training</title>
      <author><first>Merel</first><last>Scholman</last></author>
      <author><first>Valentina</first><last>Pyatkin</last></author>
      <author><first>Frances</first><last>Yung</last></author>
      <author><first>Ido</first><last>Dagan</last></author>
      <author><first>Reut</first><last>Tsarfaty</last></author>
      <author><first>Vera</first><last>Demberg</last></author>
      <pages>2148–2156</pages>
      <abstract>Obtaining linguistic annotation from novice crowdworkers is far from trivial. A case in point is the annotation of discourse relations, which is a complicated task. Recent methods have obtained promising results by extracting relation labels from either discourse connectives (DCs) or question-answer (QA) pairs that participants provide. The current contribution studies the effect of worker selection and training on the agreement on implicit relation labels between workers and gold labels, for both the DC and the QA method. In Study 1, workers were not specifically selected or trained, and the results show that there is much room for improvement. Study 2 shows that a combination of selection and training does lead to improved results, but the method is cost- and time-intensive. Study 3 shows that a selection-only approach is a viable alternative; it results in annotations of comparable quality compared to annotations from trained participants. The results generalized over both the DC and QA method and therefore indicate that a selection-only approach could also be effective for other crowdsourced discourse annotation tasks.</abstract>
      <url hash="8f2ce5e3">2022.lrec-1.231</url>
      <bibkey>scholman-etal-2022-design</bibkey>
    </paper>
    <paper id="232">
      <title><fixed-case>TBD</fixed-case>3: A Thresholding-Based Dynamic Depression Detection from Social Media for Low-Resource Users</title>
      <author><first>Hrishikesh</first><last>Kulkarni</last></author>
      <author><first>Sean</first><last>MacAvaney</last></author>
      <author><first>Nazli</first><last>Goharian</last></author>
      <author><first>Ophir</first><last>Frieder</last></author>
      <pages>2157–2165</pages>
      <abstract>Social media are heavily used by many users to share their mental health concerns and diagnoses. This trend has turned social media into a large-scale resource for researchers focused on detecting mental health conditions. Social media usage varies considerably across individuals. Thus, classification of patterns, including detecting signs of depression, must account for such variation. We address the disparity in classification effectiveness for users with little activity (e.g., new users). Our evaluation, performed on a large-scale dataset, shows considerable detection discrepancy based on user posting frequency. For instance, the F1 detection score of users with an above-median versus below-median number of posts is greater than double (0.803 vs 0.365) using a conventional CNN-based model; similar results were observed on lexical and transformer-based classifiers. To complement this evaluation, we propose a dynamic thresholding technique that adjusts the classifier’s sensitivity as a function of the number of posts a user has. This technique alone reduces the margin between users with many and few posts, on average, by 45% across all methods and increases overall performance, on average, by 33%. These findings emphasize the importance of evaluating and tuning natural language systems for potentially vulnerable populations.</abstract>
      <url hash="96261162">2022.lrec-1.232</url>
      <bibkey>kulkarni-etal-2022-tbd3</bibkey>
      <pwccode url="https://github.com/georgetown-ir-lab/lrec2022-tbd3" additional="false">georgetown-ir-lab/lrec2022-tbd3</pwccode>
    </paper>
    <paper id="233">
      <title><fixed-case>S</fixed-case>pec<fixed-case>NFS</fixed-case>: A Challenge Dataset Towards Extracting Formal Models from Natural Language Specifications</title>
      <author><first>Sayontan</first><last>Ghosh</last></author>
      <author><first>Amanpreet</first><last>Singh</last></author>
      <author><first>Alex</first><last>Merenstein</last></author>
      <author><first>Wei</first><last>Su</last></author>
      <author><first>Scott A.</first><last>Smolka</last></author>
      <author><first>Erez</first><last>Zadok</last></author>
      <author><first>Niranjan</first><last>Balasubramanian</last></author>
      <pages>2166–2176</pages>
      <abstract>Can NLP assist in building formal models for verifying complex systems? We study this challenge in the context of parsing Network File System (NFS) specifications. We define a semantic-dependency problem over SpecIR, a representation language we introduce to model sentences appearing in NFS specification documents (RFCs) as IF-THEN statements, and present an annotated dataset of 1,198 sentences. We develop and evaluate semantic-dependency parsing systems for this problem. Evaluations show that even when using a state-of-the-art language model, there is significant room for improvement, with the best models achieving an F1 score of only 60.5 and 33.3 in the named-entity-recognition and dependency-link-prediction sub-tasks, respectively. We also release additional unlabeled data and other domain-related texts. Experiments show that these additional resources increase the F1 measure when used for simple domain-adaption and transfer-learning-based approaches, suggesting fruitful directions for further research</abstract>
      <url hash="476dec1d">2022.lrec-1.233</url>
      <bibkey>ghosh-etal-2022-specnfs</bibkey>
      <pwccode url="https://github.com/stonybrooknlp/specnfs" additional="false">stonybrooknlp/specnfs</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bio">Bio</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="234">
      <title>Argument Similarity Assessment in <fixed-case>G</fixed-case>erman for Intelligent Tutoring: Crowdsourced Dataset and First Experiments</title>
      <author><first>Xiaoyu</first><last>Bai</last></author>
      <author><first>Manfred</first><last>Stede</last></author>
      <pages>2177–2187</pages>
      <abstract>NLP technologies such as text similarity assessment, question answering and text classification are increasingly being used to develop intelligent educational applications. The long-term goal of our work is an intelligent tutoring system for German secondary schools, which will support students in a school exercise that requires them to identify arguments in an argumentative source text. The present paper presents our work on a central subtask, viz. the automatic assessment of similarity between a pair of argumentative text snippets in German. In the designated use case, students write out key arguments from a given source text; the tutoring system then evaluates them against a target reference, assessing the similarity level between student work and the reference. We collect a dataset for our similarity assessment task through crowdsourcing as authentic German student data are scarce; we label the collected text pairs with similarity scores on a 5-point scale and run first experiments on the task. We see that a model based on BERT shows promising results, while we also discuss some challenges that we observe.</abstract>
      <url hash="efd90a1e">2022.lrec-1.234</url>
      <bibkey>bai-stede-2022-argument</bibkey>
    </paper>
    <paper id="235">
      <title>Leveraging Pre-trained Language Models for Gender Debiasing</title>
      <author><first>Nishtha</first><last>Jain</last></author>
      <author><first>Declan</first><last>Groves</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <author><first>Maja</first><last>Popović</last></author>
      <pages>2188–2195</pages>
      <abstract>Studying and mitigating gender and other biases in natural language have become important areas of research from both algorithmic and data perspectives. This paper explores the idea of reducing gender bias in a language generation context by generating gender variants of sentences. Previous work in this field has either been rule-based or required large amounts of gender balanced training data. These approaches are however not scalable across multiple languages, as creating data or rules for each language is costly and time-consuming. This work explores a light-weight method to generate gender variants for a given text using pre-trained language models as the resource, without any task-specific labelled data. The approach is designed to work on multiple languages with minimal changes in the form of heuristics. To showcase that, we have tested it on a high-resourced language, namely Spanish, and a low-resourced language from a different family, namely Serbian. The approach proved to work very well on Spanish, and while the results were less positive for Serbian, it showed potential even for languages where pre-trained models are less effective.</abstract>
      <url hash="ce132ed7">2022.lrec-1.235</url>
      <bibkey>jain-etal-2022-leveraging</bibkey>
    </paper>
    <paper id="236">
      <title>Unsupervised Embeddings with Graph Auto-Encoders for Multi-domain and Multilingual Hate Speech Detection</title>
      <author><first>Gretel Liz</first><last>De la Peña Sarracén</last></author>
      <author><first>Paolo</first><last>Rosso</last></author>
      <pages>2196–2204</pages>
      <abstract>Hate speech detection is a prominent and challenging task, since hate messages are often expressed in subtle ways and with characteristics that may vary depending on the author. Hence, many models suffer from the generalization problem. However, retrieving and monitoring hateful content on social media is a current necessity. In this paper, we propose an unsupervised approach using Graph Auto-Encoders (GAE), which allows us to avoid using labeled data when training the representation of the texts. Specifically, we represent texts as nodes of a graph, and use a transformer layer together with a convolutional layer to encode these nodes in a low-dimensional space. As a result, we obtain embeddings that can be decoded into a reconstruction of the original network. Our main idea is to learn a model with a set of texts without supervision, in order to generate embeddings for the nodes: nodes with the same label should be close in the embedding space, which, in turn, should allow us to distinguish among classes. We employ this strategy to detect hate speech in multi-domain and multilingual sets of texts, where our method shows competitive results on small datasets.</abstract>
      <url hash="ad9da628">2022.lrec-1.236</url>
      <bibkey>de-la-pena-sarracen-rosso-2022-unsupervised</bibkey>
    </paper>
    <paper id="237">
      <title><fixed-case>FQ</fixed-case>u<fixed-case>AD</fixed-case>2.0: <fixed-case>F</fixed-case>rench Question Answering and Learning When You Don’t Know</title>
      <author><first>Quentin</first><last>Heinrich</last></author>
      <author><first>Gautier</first><last>Viaud</last></author>
      <author><first>Wacim</first><last>Belblidia</last></author>
      <pages>2205–2214</pages>
      <abstract>Question Answering, including Reading Comprehension, is one of the NLP research areas that has seen significant scientific breakthroughs over the past few years, thanks to the concomitant advances in Language Modeling. Most of these breakthroughs, however, are centered on the English language. In 2020, as a first strong initiative to bridge the gap to the French language, Illuin Technology introduced FQuAD1.1, a French Native Reading Comprehension dataset composed of 60,000+ questions and answers samples extracted from Wikipedia articles. Nonetheless, Question Answering models trained on this dataset have a major drawback: they are not able to predict when a given question has no answer in the paragraph of interest, therefore making unreliable predictions in various industrial use-cases. We introduce FQuAD2.0, which extends FQuAD with 17,000+ unanswerable questions, annotated adversarially, in order to be similar to answerable ones. This new dataset, comprising a total of almost 80,000 questions, makes it possible to train French Question Answering models with the ability of distinguishing unanswerable questions from answerable ones. We benchmark several models with this dataset: our best model, a fine-tuned CamemBERT-large, achieves a F1 score of 82.3% on this classification task, and a F1 score of 83% on the Reading Comprehension task.</abstract>
      <url hash="e3ddeff4">2022.lrec-1.237</url>
      <bibkey>heinrich-etal-2022-fquad2</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/boolq">BoolQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/coqa">CoQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fquad">FQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="238">
      <title>Large-Scale Hate Speech Detection with Cross-Domain Transfer</title>
      <author><first>Cagri</first><last>Toraman</last></author>
      <author><first>Furkan</first><last>Şahinuç</last></author>
      <author><first>Eyup</first><last>Yilmaz</last></author>
      <pages>2215–2225</pages>
      <abstract>The performance of hate speech detection models relies on the datasets on which the models are trained. Existing datasets are mostly prepared with a limited number of instances or hate domains that define hate topics. This hinders large-scale analysis and transfer learning with respect to hate domains. In this study, we construct large-scale tweet datasets for hate speech detection in English and a low-resource language, Turkish, consisting of human-labeled 100k tweets per each. Our datasets are designed to have equal number of tweets distributed over five domains. The experimental results supported by statistical tests show that Transformer-based language models outperform conventional bag-of-words and neural models by at least 5% in English and 10% in Turkish for large-scale hate speech detection. The performance is also scalable to different training sizes, such that 98% of performance in English, and 97% in Turkish, are recovered when 20% of training instances are used. We further examine the generalization ability of cross-domain transfer among hate domains. We show that 96% of the performance of a target domain in average is recovered by other domains for English, and 92% for Turkish. Gender and religion are more successful to generalize to other domains, while sports fail most.</abstract>
      <url hash="386c28ee">2022.lrec-1.238</url>
      <bibkey>toraman-etal-2022-large</bibkey>
      <pwccode url="https://github.com/avaapm/hatespeech" additional="false">avaapm/hatespeech</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hatexplain">HateXplain</pwcdataset>
    </paper>
    <paper id="239">
      <title><fixed-case>GL</fixed-case>o<fixed-case>HBCD</fixed-case>: A Naturalistic <fixed-case>G</fixed-case>erman Dataset for Language of Health Behaviour Change on Online Support Forums</title>
      <author><first>Selina</first><last>Meyer</last></author>
      <author><first>David</first><last>Elsweiler</last></author>
      <pages>2226–2235</pages>
      <abstract>Health behaviour change is a difficult and prolonged process that requires sustained motivation and determination. Conversa- tional agents have shown promise in supporting the change process in the past. One therapy approach that facilitates change and has been used as a framework for conversational agents is motivational interviewing. However, existing implementations of this therapy approach lack the deep understanding of user utterances that is essential to the spirit of motivational interviewing. To address this lack of understanding, we introduce the GLoHBCD, a German dataset of naturalistic language around health behaviour change. Data was sourced from a popular German weight loss forum and annotated using theoretically grounded motivational interviewing categories. We describe the process of dataset construction and present evaluation results. Initial experiments suggest a potential for broad applicability of the data and the resulting classifiers across different behaviour change domains. We make code to replicate the dataset and experiments available on Github.</abstract>
      <url hash="ef8d26d4">2022.lrec-1.239</url>
      <bibkey>meyer-elsweiler-2022-glohbcd</bibkey>
      <pwccode url="https://github.com/selinameyer/glohbcd" additional="false">selinameyer/glohbcd</pwccode>
    </paper>
    <paper id="240">
      <title>Creating a Data Set of Abstractive Summaries of Turn-labeled Spoken Human-Computer Conversations</title>
      <author><first>Iris</first><last>Hendrickx</last></author>
      <pages>2236–2244</pages>
      <abstract>Digital recorded written and spoken dialogues are becoming increasingly available as an effect of the technological advances such as online messenger services and the use of chatbots. Summaries are a natural way of presenting the important information gathered from dialogues. We present a unique data set that consists of Dutch spoken human-computer conversations, an annotation layer of turn labels, and conversational abstractive summaries of user answers. The data set is publicly available for research purposes.</abstract>
      <url hash="d06ce2b6">2022.lrec-1.240</url>
      <bibkey>hendrickx-2022-creating</bibkey>
    </paper>
    <paper id="241">
      <title><fixed-case>O</fixed-case>pen<fixed-case>EL</fixed-case>: An Annotated Corpus for Entity Linking and Discourse in Open Domain Dialogue</title>
      <author><first>Wen</first><last>Cui</last></author>
      <author><first>Leanne</first><last>Rolston</last></author>
      <author><first>Marilyn</first><last>Walker</last></author>
      <author><first>Beth Ann</first><last>Hockey</last></author>
      <pages>2245–2256</pages>
      <abstract>Entity linking in dialogue is the task of mapping entity mentions in utterances to a target knowledge base. Prior work on entity linking has mainly focused on well-written articles such as Wikipedia, annotated newswire, or domain-specific datasets. We extend the study of entity linking to open domain dialogue by presenting the OpenEL corpus: an annotated multi-domain corpus for linking entities in natural conversation to Wikidata. Each dialogic utterance in 179 dialogues over 12 topics from the EDINA dataset has been annotated for entities realized by definite referring expressions as well as anaphoric forms such as he, she, it and they. This dataset supports training and evaluation of entity linking in open-domain dialogue, as well as analysis of the effect of using dialogue context and anaphora resolution in model training. It could also be used for fine-tuning a coreference resolution algorithm. To the best of our knowledge, this is the first substantial entity linking corpus publicly available for open-domain dialogue. We also establish baselines for this task using several existing entity linking systems. We found that the Transformer-based system Flair + BLINK has the best performance with a 0.65 F1 score. Our results show that dialogue context is extremely beneficial for entity linking in conversations, with Flair + Blink achieving an F1 of 0.61 without discourse context. These results also demonstrate the remaining performance gap between the baselines and human performance, highlighting the challenges of entity linking in open-domain dialogue, and suggesting many avenues for future research using OpenEL.</abstract>
      <url hash="838ebd42">2022.lrec-1.241</url>
      <bibkey>cui-etal-2022-openel</bibkey>
      <pwccode url="https://github.com/wenzi3241/openel_corpus" additional="false">wenzi3241/openel_corpus</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wizard-of-wikipedia">Wizard of Wikipedia</pwcdataset>
    </paper>
    <paper id="242">
      <title>Collecting Visually-Grounded Dialogue with A Game Of Sorts</title>
      <author><first>Bram</first><last>Willemsen</last></author>
      <author><first>Dmytro</first><last>Kalpakchi</last></author>
      <author><first>Gabriel</first><last>Skantze</last></author>
      <pages>2257–2268</pages>
      <abstract>An idealized, though simplistic, view of the referring expression production and grounding process in (situated) dialogue assumes that a speaker must merely appropriately specify their expression so that the target referent may be successfully identified by the addressee. However, referring in conversation is a collaborative process that cannot be aptly characterized as an exchange of minimally-specified referring expressions. Concerns have been raised regarding assumptions made by prior work on visually-grounded dialogue that reveal an oversimplified view of conversation and the referential process. We address these concerns by introducing a collaborative image ranking task, a grounded agreement game we call “A Game Of Sorts”. In our game, players are tasked with reaching agreement on how to rank a set of images given some sorting criterion through a largely unrestricted, role-symmetric dialogue. By putting emphasis on the argumentation in this mixed-initiative interaction, we collect discussions that involve the collaborative referential process. We describe results of a small-scale data collection experiment with the proposed task. All discussed materials, which includes the collected data, the codebase, and a containerized version of the application, are publicly available.</abstract>
      <url hash="3009f8f4">2022.lrec-1.242</url>
      <bibkey>willemsen-etal-2022-collecting</bibkey>
      <pwccode url="https://github.com/willemsenbram/a-game-of-sorts" additional="false">willemsenbram/a-game-of-sorts</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/photobook">PhotoBook</pwcdataset>
    </paper>
    <paper id="243">
      <title><fixed-case>C</fixed-case>o<fixed-case>R</fixed-case>o<fixed-case>S</fixed-case>e<fixed-case>O</fixed-case>f - An Annotated Corpus of <fixed-case>R</fixed-case>omanian Sexist and Offensive Tweets</title>
      <author><first>Diana Constantina</first><last>Hoefels</last></author>
      <author><first>Çağrı</first><last>Çöltekin</last></author>
      <author><first>Irina Diana</first><last>Mădroane</last></author>
      <pages>2269–2281</pages>
      <abstract>This paper introduces CoRoSeOf, a large corpus of Romanian social media manually annotated for sexist and offensive language. We describe the annotation process of the corpus, provide initial analyses, and baseline classification results for sexism detection on this data set. The resulting corpus contains 39 245 tweets, annotated by multiple annotators (with an agreement rate of Fleiss’κ= 0.45), following the sexist label set of a recent study. The automatic sexism detection yields scores similar to some of the earlier studies (macro averaged F1 score of 83.07% on binary classification task). We release the corpus with a permissive license.</abstract>
      <url hash="9eedad9e">2022.lrec-1.243</url>
      <bibkey>hoefels-etal-2022-coroseof</bibkey>
      <pwccode url="https://github.com/dianahoefels/coroseof" additional="false">dianahoefels/coroseof</pwccode>
    </paper>
    <paper id="244">
      <title><fixed-case>A</fixed-case>r<fixed-case>MIS</fixed-case> - The <fixed-case>A</fixed-case>rabic Misogyny and Sexism Corpus with Annotator Subjective Disagreements</title>
      <author><first>Dina</first><last>Almanea</last></author>
      <author><first>Massimo</first><last>Poesio</last></author>
      <pages>2282–2291</pages>
      <abstract>The use of misogynistic and sexist language has increased in recent years in social media, and is increasing in the Arabic world in reaction to reforms attempting to remove restrictions on women lives. However, there are few benchmarks for Arabic misogyny and sexism detection, and in those the annotations are in aggregated form even though misogyny and sexism judgments are found to be highly subjective. In this paper we introduce an Arabic misogyny and sexism dataset (ArMIS) characterized by providing annotations from annotators with different degree of religious beliefs, and provide evidence that such differences do result in disagreements. To the best of our knowledge, this is the first dataset to study in detail the effect of beliefs on misogyny and sexism annotation. We also discuss proof-of-concept experiments showing that a dataset in which disagreements have not been reconciled can be used to train state-of-the-art models for misogyny and sexism detection; and consider different ways in which such models could be evaluated.</abstract>
      <url hash="c8a4321d">2022.lrec-1.244</url>
      <bibkey>almanea-poesio-2022-armis</bibkey>
    </paper>
    <paper id="245">
      <title>Annotating Interruption in Dyadic Human Interaction</title>
      <author><first>Liu</first><last>Yang</last></author>
      <author><first>Catherine</first><last>Achard</last></author>
      <author><first>Catherine</first><last>Pelachaud</last></author>
      <pages>2292–2297</pages>
      <abstract>Integrating the existing interruption and turn switch classification methods, we propose a new annotation schema to annotate different types of interruptions through timeliness, switch accomplishment and speech content level. The proposed method is able to distinguish smooth turn exchange, backchannel and interruption (including interruption types) and to annotate dyadic conversation. We annotated the French part of NoXi corpus with the proposed structure and use these annotations to study the probability distribution and duration of each turn switch type.</abstract>
      <url hash="69f1fc62">2022.lrec-1.245</url>
      <bibkey>yang-etal-2022-annotating</bibkey>
    </paper>
    <paper id="246">
      <title>The Causal News Corpus: Annotating Causal Relations in Event Sentences from News</title>
      <author><first>Fiona Anting</first><last>Tan</last></author>
      <author><first>Ali</first><last>Hürriyetoğlu</last></author>
      <author><first>Tommaso</first><last>Caselli</last></author>
      <author><first>Nelleke</first><last>Oostdijk</last></author>
      <author><first>Tadashi</first><last>Nomoto</last></author>
      <author><first>Hansi</first><last>Hettiarachchi</last></author>
      <author><first>Iqra</first><last>Ameer</last></author>
      <author><first>Onur</first><last>Uca</last></author>
      <author><first>Farhana Ferdousi</first><last>Liza</last></author>
      <author><first>Tiancheng</first><last>Hu</last></author>
      <pages>2298–2310</pages>
      <abstract>Despite the importance of understanding causality, corpora addressing causal relations are limited. There is a discrepancy between existing annotation guidelines of event causality and conventional causality corpora that focus more on linguistics. Many guidelines restrict themselves to include only explicit relations or clause-based arguments. Therefore, we propose an annotation schema for event causality that addresses these concerns. We annotated 3,559 event sentences from protest event news with labels on whether it contains causal relations or not. Our corpus is known as the Causal News Corpus (CNC). A neural network built upon a state-of-the-art pre-trained language model performed well with 81.20% F1 score on test set, and 83.46% in 5-folds cross-validation. CNC is transferable across two external corpora: CausalTimeBank (CTB) and Penn Discourse Treebank (PDTB). Leveraging each of these external datasets for training, we achieved up to approximately 64% F1 on the CNC test set without additional fine-tuning. CNC also served as an effective training and pre-training dataset for the two external corpora. Lastly, we demonstrate the difficulty of our task to the layman in a crowd-sourced annotation exercise. Our annotated corpus is publicly available, providing a valuable resource for causal text mining researchers.</abstract>
      <url hash="3745dff0">2022.lrec-1.246</url>
      <bibkey>tan-etal-2022-causal</bibkey>
      <pwccode url="https://github.com/tanfiona/causalnewscorpus" additional="false">tanfiona/causalnewscorpus</pwccode>
    </paper>
    <paper id="247">
      <title>Samrómur: Crowd-sourcing large amounts of data</title>
      <author><first>Staffan</first><last>Hedström</last></author>
      <author><first>David Erik</first><last>Mollberg</last></author>
      <author><first>Ragnheiður</first><last>Þórhallsdóttir</last></author>
      <author><first>Jón</first><last>Guðnason</last></author>
      <pages>2311–2316</pages>
      <abstract>This contribution describes the collection of a large and diverse corpus for speech recognition and similar tools using crowd-sourced donations. We have built a collection platform inspired by Mozilla Common Voice and specialized it to our needs. We discuss the importance of engaging the community and motivating it to contribute, in our case through competitions. Given the incentive and a platform to easily read in large amounts of utterances, we have observed four cases of speakers freely donating over 10 thousand utterances. We have also seen that women are keener to participate in these events throughout all age groups. Manually verifying a large corpus is a monumental task and we attempt to automatically verify parts of the data using tools like Marosijo and the Montreal Forced Aligner. The method proved helpful, especially for detecting invalid utterances and halving the work needed from crowd-sourced verification.</abstract>
      <url hash="c511bdbc">2022.lrec-1.247</url>
      <bibkey>hedstrom-etal-2022-samromur</bibkey>
    </paper>
    <paper id="248">
      <title>An Annotated Corpus of Textual Explanations for Clinical Decision Support</title>
      <author><first>Roland</first><last>Roller</last></author>
      <author><first>Aljoscha</first><last>Burchardt</last></author>
      <author><first>Nils</first><last>Feldhus</last></author>
      <author><first>Laura</first><last>Seiffe</last></author>
      <author><first>Klemens</first><last>Budde</last></author>
      <author><first>Simon</first><last>Ronicke</last></author>
      <author><first>Bilgin</first><last>Osmanodja</last></author>
      <pages>2317–2326</pages>
      <abstract>In recent years, machine learning for clinical decision support has gained more and more attention. In order to introduce such applications into clinical practice, a good performance might be essential, however, the aspect of trust should not be underestimated. For the treating physician using such a system and being (legally) responsible for the decision made, it is particularly important to understand the system’s recommendation. To provide insights into a model’s decision, various techniques from the field of explainability (XAI) have been proposed whose output is often enough not targeted to the domain experts that want to use the model. To close this gap, in this work, we explore how explanations could possibly look like in future. To this end, this work presents a dataset of textual explanations in context of decision support. Within a reader study, human physicians estimated the likelihood of possible negative patient outcomes in the near future and justified each decision with a few sentences. Using those sentences, we created a novel corpus, annotated with different semantic layers. Moreover, we provide an analysis of how those explanations are constructed, and how they change depending on physician, on the estimated risk and also in comparison to an automatic clinical decision support system with feature importance.</abstract>
      <url hash="9fdef490">2022.lrec-1.248</url>
      <bibkey>roller-etal-2022-annotated</bibkey>
      <pwccode url="https://github.com/dfki-nlp/ex4cds" additional="false">dfki-nlp/ex4cds</pwccode>
    </paper>
    <paper id="249">
      <title><fixed-case>LARD</fixed-case>: Large-scale Artificial Disfluency Generation</title>
      <author><first>Tatiana</first><last>Passali</last></author>
      <author><first>Thanassis</first><last>Mavropoulos</last></author>
      <author><first>Grigorios</first><last>Tsoumakas</last></author>
      <author><first>Georgios</first><last>Meditskos</last></author>
      <author><first>Stefanos</first><last>Vrochidis</last></author>
      <pages>2327–2336</pages>
      <abstract>Disfluency detection is a critical task in real-time dialogue systems. However, despite its importance, it remains a relatively unexplored field, mainly due to the lack of appropriate datasets. At the same time, existing datasets suffer from various issues, including class imbalance issues, which can significantly affect the performance of the model on rare classes, as it is demonstrated in this paper. To this end, we propose LARD, a method for generating complex and realistic artificial disfluencies with little effort. The proposed method can handle three of the most common types of disfluencies: repetitions, replacements, and restarts. In addition, we release a new large-scale dataset with disfluencies that can be used on four different tasks: disfluency detection, classification, extraction, and correction. Experimental results on the LARD dataset demonstrate that the data produced by the proposed method can be effectively used for detecting and removing disfluencies, while also addressing limitations of existing datasets.</abstract>
      <url hash="619213d3">2022.lrec-1.249</url>
      <bibkey>passali-etal-2022-lard</bibkey>
      <pwccode url="https://github.com/tatianapassali/artificial-disfluency-generation" additional="false">tatianapassali/artificial-disfluency-generation</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/sgd">SGD</pwcdataset>
    </paper>
    <paper id="250">
      <title>The <fixed-case>CRECIL</fixed-case> Corpus: a New Dataset for Extraction of Relations between Characters in <fixed-case>C</fixed-case>hinese Multi-party Dialogues</title>
      <author><first>Yuru</first><last>Jiang</last></author>
      <author><first>Yang</first><last>Xu</last></author>
      <author><first>Yuhang</first><last>Zhan</last></author>
      <author><first>Weikai</first><last>He</last></author>
      <author><first>Yilin</first><last>Wang</last></author>
      <author><first>Zixuan</first><last>Xi</last></author>
      <author><first>Meiyun</first><last>Wang</last></author>
      <author><first>Xinyu</first><last>Li</last></author>
      <author><first>Yu</first><last>Li</last></author>
      <author><first>Yanchao</first><last>Yu</last></author>
      <pages>2337–2344</pages>
      <abstract>We describe a new freely available Chinese multi-party dialogue dataset for automatic extraction of dialogue-based character relationships. The data has been extracted from the original TV scripts of a Chinese sitcom called “I Love My Home” with complex family-based human daily spoken conversations in Chinese. First, we introduced human annotation scheme for both global Character relationship map and character reference relationship. And then we generated the dialogue-based character relationship triples. The corpus annotates relationships between 140 entities in total. We also carried out a data exploration experiment by deploying a BERT-based model to extract character relationships on the CRECIL corpus and another existing relation extraction corpus (DialogRE (CITATION)).The results demonstrate that extracting character relationships is more challenging in CRECIL than in DialogRE.</abstract>
      <url hash="f3841f2f">2022.lrec-1.250</url>
      <bibkey>jiang-etal-2022-crecil</bibkey>
      <pwccode url="https://github.com/bistu-nlp-lab/crecil" additional="false">bistu-nlp-lab/crecil</pwccode>
    </paper>
    <paper id="251">
      <title>The <fixed-case>B</fixed-case>ahrain Corpus: A Multi-genre Corpus of Bahraini <fixed-case>A</fixed-case>rabic</title>
      <author><first>Dana</first><last>Abdulrahim</last></author>
      <author><first>Go</first><last>Inoue</last></author>
      <author><first>Latifa</first><last>Shamsan</last></author>
      <author><first>Salam</first><last>Khalifa</last></author>
      <author><first>Nizar</first><last>Habash</last></author>
      <pages>2345–2352</pages>
      <abstract>In recent years, the focus on developing natural language processing (NLP) tools for Arabic has shifted from Modern Standard Arabic to various Arabic dialects. Various corpora of various sizes and representing different genres, have been created for a number of Arabic dialects. As far as Gulf Arabic is concerned, Gumar Corpus (Khalifa et al., 2016) is the largest corpus, to date, that includes data representing the dialectal Arabic of the six Gulf Cooperation Council countries (Bahrain, Kuwait, Saudi Arabia, Qatar, United Arab Emirates, and Oman), particularly in the genre of “online forum novels”. In this paper, we present the Bahrain Corpus. Our objective is to create a specialized corpus of the Bahraini Arabic dialect, which includes written texts as well as transcripts of audio files, belonging to a different genre (folktales, comedy shows, plays, cooking shows, etc.). The corpus comprises 620K words, carefully curated. We provide automatic morphological annotations of the full corpus using state-of-the-art morphosyntactic disambiguation for Gulf Arabic. We validate the quality of the annotations on a 7.6K word sample. We plan to make the annotated sample as well as the full corpus publicly available to support researchers interested in Arabic NLP.</abstract>
      <url hash="412b5836">2022.lrec-1.251</url>
      <bibkey>abdulrahim-etal-2022-bahrain</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/gumar-corpus">Gumar Corpus</pwcdataset>
    </paper>
    <paper id="252">
      <title>A <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies Treebank of <fixed-case>A</fixed-case>ncient <fixed-case>H</fixed-case>ebrew</title>
      <author><first>Daniel</first><last>Swanson</last></author>
      <author><first>Francis</first><last>Tyers</last></author>
      <pages>2353–2361</pages>
      <abstract>In this paper we present the initial construction of a Universal Dependencies treebank with morphological annotations of Ancient Hebrew containing portions of the Hebrew Scriptures (1579 sentences, 27K tokens) for use in comparative study with ancient translations and for analysis of the development of Hebrew syntax. We construct this treebank by applying a rule-based parser (300 rules) to an existing morphologically-annotated corpus with minimal constituency structure and manually verifying the output and present the results of this semi-automated annotation process and some of the annotation decisions made in the process of applying the UD guidelines to a new language.</abstract>
      <url hash="c9a4140a">2022.lrec-1.252</url>
      <bibkey>swanson-tyers-2022-universal</bibkey>
    </paper>
    <paper id="253">
      <title>Hate Speech Dynamics Against <fixed-case>A</fixed-case>frican descent, <fixed-case>R</fixed-case>oma and <fixed-case>LGBTQI</fixed-case> Communities in <fixed-case>P</fixed-case>ortugal</title>
      <author><first>Paula</first><last>Carvalho</last></author>
      <author><first>Bernardo</first><last>Cunha</last></author>
      <author><first>Raquel</first><last>Santos</last></author>
      <author><first>Fernando</first><last>Batista</last></author>
      <author><first>Ricardo</first><last>Ribeiro</last></author>
      <pages>2362–2370</pages>
      <abstract>This paper introduces FIGHT, a dataset containing 63,450 tweets, posted before and after the official declaration of Covid-19 as a pandemic by online users in Portugal. This resource aims at contributing to the analysis of online hate speech targeting the most representative minorities in Portugal, namely the African descent and the Roma communities, and the LGBTQI community, the most commonly reported target of hate speech in social media at the European context. We present the methods for collecting the data, and provide insightful statistics on the distribution of tweets included in FIGHT, considering both the temporal and spatial dimensions. We also analyze the availability over time of tweets targeting the above-mentioned communities, distinguishing public, private and deleted tweets. We believe this study will contribute to better understand the dynamics of online hate speech in Portugal, particularly in adverse contexts, such as a pandemic outbreak, allowing the development of more informed and accurate hate speech resources for Portuguese.</abstract>
      <url hash="37d1136b">2022.lrec-1.253</url>
      <bibkey>carvalho-etal-2022-hate</bibkey>
    </paper>
    <paper id="254">
      <title>Evolving Large Text Corpora: Four Versions of the <fixed-case>I</fixed-case>celandic <fixed-case>G</fixed-case>igaword Corpus</title>
      <author><first>Starkaður</first><last>Barkarson</last></author>
      <author><first>Steinþór</first><last>Steingrímsson</last></author>
      <author><first>Hildur</first><last>Hafsteinsdóttir</last></author>
      <pages>2371–2381</pages>
      <abstract>The Icelandic Gigaword Corpus was first published in 2018. Since then new versions have been published annually, containing new texts from additional sources as well as from previous sources. This paper describes the evolution of the corpus in its first four years. All versions are made available under permissive licenses and with each new version the texts are annotated with the latest and most accurate tools. We show how the corpus has grown almost 50% in size from the first version to the fourth and how it was restructured in order to better accommodate different meta-data for different subcorpora. Furthermore, other services have been set up to facilitate usage of the corpus for different use cases. These include a keyword-in-context concordance tool, an n-gram viewer, a word frequency database and pre-trained word embeddings.</abstract>
      <url hash="49848589">2022.lrec-1.254</url>
      <bibkey>barkarson-etal-2022-evolving</bibkey>
    </paper>
    <paper id="255">
      <title>A Pragmatics-Centered Evaluation Framework for Natural Language Understanding</title>
      <author><first>Damien</first><last>Sileo</last></author>
      <author><first>Philippe</first><last>Muller</last></author>
      <author><first>Tim</first><last>Van de Cruys</last></author>
      <author><first>Camille</first><last>Pradel</last></author>
      <pages>2382–2394</pages>
      <abstract>New models for natural language understanding have recently made an unparalleled amount of progress, which has led some researchers to suggest that the models induce universal text representations. However, current benchmarks are predominantly targeting semantic phenomena; we make the case that pragmatics needs to take center stage in the evaluation of natural language understanding. We introduce PragmEval, a new benchmark for the evaluation of natural language understanding, that unites 11 pragmatics-focused evaluation datasets for English. PragmEval can be used as supplementary training data in a multi-task learning setup, and is publicly available, alongside the code for gathering and preprocessing the datasets. Using our evaluation suite, we show that natural language inference, a widely used pretraining task, does not result in genuinely universal representations, which presents a new challenge for multi-task learning.</abstract>
      <url hash="993e91c4">2022.lrec-1.255</url>
      <bibkey>sileo-etal-2022-pragmatics</bibkey>
      <pwccode url="https://github.com/synapse-developpement/DiscEval" additional="true">synapse-developpement/DiscEval</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/emobank">EmoBank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
    </paper>
    <paper id="256">
      <title>Conversational Analysis of Daily Dialog Data using Polite Emotional Dialogue Acts</title>
      <author><first>Chandrakant</first><last>Bothe</last></author>
      <author><first>Stefan</first><last>Wermter</last></author>
      <pages>2395–2400</pages>
      <abstract>Many socio-linguistic cues are used in conversational analysis, such as emotion, sentiment, and dialogue acts. One of the fundamental social cues is politeness, which linguistically possesses properties such as social manners useful in conversational analysis. This article presents findings of polite emotional dialogue act associations, where we can correlate the relationships between the socio-linguistic cues. We confirm our hypothesis that the utterances with the emotion classes Anger and Disgust are more likely to be impolite. At the same time, Happiness and Sadness are more likely to be polite. A less expectable phenomenon occurs with dialogue acts Inform and Commissive which contain more polite utterances than Question and Directive. Finally, we conclude on the future work of these findings to extend the learning of social behaviours using politeness.</abstract>
      <url hash="4682d70c">2022.lrec-1.256</url>
      <bibkey>bothe-wermter-2022-conversational</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/emotional-dialogue-acts">Emotional Dialogue Acts</pwcdataset>
    </paper>
    <paper id="257">
      <title>Inducing Discourse Marker Inventories from Lexical Knowledge Graphs</title>
      <author><first>Christian</first><last>Chiarcos</last></author>
      <pages>2401–2412</pages>
      <abstract>Discourse marker inventories are important tools for the development of both discourse parsers and corpora with discourse annotations. In this paper we explore the potential of massively multilingual lexical knowledge graphs to induce multilingual discourse marker lexicons using concept propagation methods as previously developed in the context of translation inference across dictionaries. Given one or multiple source languages with discourse marker inventories that discourse relations as senses of potential discourse markers, as well as a large number of bilingual dictionaries that link them – directly or indirectly – with the target language, we specifically study to what extent discourse marker induction can benefit from the integration of information from different sources, the impact of sense granularity and what limiting factors may need to be considered. Our study uses discourse marker inventories from nine European languages normalized against the discourse relation inventory of the Penn Discourse Treebank (PDTB), as well as three collections of machine-readable dictionaries with different characteristics, so that the interplay of a large number of factors can be studied.</abstract>
      <url hash="b9ad2d8e">2022.lrec-1.257</url>
      <bibkey>chiarcos-2022-inducing</bibkey>
      <pwccode url="https://github.com/acoli-repo/rdf4discourse" additional="false">acoli-repo/rdf4discourse</pwccode>
    </paper>
    <paper id="258">
      <title>Story Trees: Representing Documents using Topological Persistence</title>
      <author><first>Pantea</first><last>Haghighatkhah</last></author>
      <author><first>Antske</first><last>Fokkens</last></author>
      <author><first>Pia</first><last>Sommerauer</last></author>
      <author><first>Bettina</first><last>Speckmann</last></author>
      <author><first>Kevin</first><last>Verbeek</last></author>
      <pages>2413–2429</pages>
      <abstract>Topological Data Analysis (TDA) focuses on the inherent shape of (spatial) data. As such, it may provide useful methods to explore spatial representations of linguistic data (embeddings) which have become central in NLP. In this paper we aim to introduce TDA to researchers in language technology. We use TDA to represent document structure as so-called story trees. Story trees are hierarchical representations created from semantic vector representations of sentences via persistent homology. They can be used to identify and clearly visualize prominent components of a story line. We showcase their potential by using story trees to create extractive summaries for news stories.</abstract>
      <url hash="808f51d3">2022.lrec-1.258</url>
      <bibkey>haghighatkhah-etal-2022-story</bibkey>
    </paper>
    <paper id="259">
      <title>Extracting and Analysing Metaphors in Migration Media Discourse: towards a Metaphor Annotation Scheme</title>
      <author><first>Ana</first><last>Zwitter Vitez</last></author>
      <author><first>Mojca</first><last>Brglez</last></author>
      <author><first>Marko</first><last>Robnik Šikonja</last></author>
      <author><first>Tadej</first><last>Škvorc</last></author>
      <author><first>Andreja</first><last>Vezovnik</last></author>
      <author><first>Senja</first><last>Pollak</last></author>
      <pages>2430–2439</pages>
      <abstract>The study of metaphors in media discourse is an increasingly researched topic as media are an important shaper of social reality and metaphors are an indicator of how we think about certain issues through references to other things. We present a neural transfer learning method for detecting metaphorical sentences in Slovene and evaluate its performance on a gold standard corpus of metaphors (classification accuracy of 0.725), as well as on a sample of a domain specific corpus of migrations (precision of 0.40 for extracting domain metaphors and 0.74 if evaluated only on a set of migration related sentences). Based on empirical results and findings of our analysis, we propose a novel metaphor annotation scheme containing linguistic level, conceptual level, and stance information. The new scheme can be used for future metaphor annotations of other socially relevant topics.</abstract>
      <url hash="bff22952">2022.lrec-1.259</url>
      <bibkey>zwitter-vitez-etal-2022-extracting</bibkey>
      <pwccode url="https://github.com/tadejskvorc/metaphor-detection" additional="false">tadejskvorc/metaphor-detection</pwccode>
    </paper>
    <paper id="260">
      <title><fixed-case>DD</fixed-case>is<fixed-case>C</fixed-case>o: A Discourse Coherence Dataset for <fixed-case>D</fixed-case>anish</title>
      <author><first>Linea</first><last>Flansmose Mikkelsen</last></author>
      <author><first>Oliver</first><last>Kinch</last></author>
      <author><first>Anders</first><last>Jess Pedersen</last></author>
      <author><first>Ophélie</first><last>Lacroix</last></author>
      <pages>2440–2445</pages>
      <abstract>To date, there has been no resource for studying discourse coherence on real-world Danish texts. Discourse coherence has mostly been approached with the assumption that incoherent texts can be represented by coherent texts in which sentences have been shuffled. However, incoherent real-world texts rarely resemble that. We thus present DDisCo, a dataset including text from the Danish Wikipedia and Reddit annotated for discourse coherence. We choose to annotate real-world texts instead of relying on artificially incoherent text for training and testing models. Then, we evaluate the performance of several methods, including neural networks, on the dataset.</abstract>
      <url hash="22c8023f">2022.lrec-1.260</url>
      <bibkey>flansmose-mikkelsen-etal-2022-ddisco</bibkey>
    </paper>
    <paper id="261">
      <title><fixed-case>LPA</fixed-case>ttack: A Feasible Annotation Scheme for Capturing Logic Pattern of Attacks in Arguments</title>
      <author><first>Farjana Sultana</first><last>Mim</last></author>
      <author><first>Naoya</first><last>Inoue</last></author>
      <author><first>Shoichi</first><last>Naito</last></author>
      <author><first>Keshav</first><last>Singh</last></author>
      <author><first>Kentaro</first><last>Inui</last></author>
      <pages>2446–2459</pages>
      <abstract>In argumentative discourse, persuasion is often achieved by refuting or attacking others’ arguments. Attacking an argument is not always straightforward and often consists of complex rhetorical moves in which arguers may agree with a logic of an argument while attacking another logic. Furthermore, an arguer may neither deny nor agree with any logics of an argument, instead ignore them and attack the main stance of the argument by providing new logics and presupposing that the new logics have more value or importance than the logics presented in the attacked argument. However, there are no studies in computational argumentation that capture such complex rhetorical moves in attacks or the presuppositions or value judgments in them. To address this gap, we introduce LPAttack, a novel annotation scheme that captures the common modes and complex rhetorical moves in attacks along with the implicit presuppositions and value judgments. Our annotation study shows moderate inter-annotator agreement, indicating that human annotation for the proposed scheme is feasible. We publicly release our annotated corpus and the annotation guidelines.</abstract>
      <url hash="181a3fbf">2022.lrec-1.261</url>
      <bibkey>mim-etal-2022-lpattack</bibkey>
    </paper>
    <paper id="262">
      <title><fixed-case>B</fixed-case>e<fixed-case>S</fixed-case>t: The Belief and Sentiment Corpus</title>
      <author><first>Jennifer</first><last>Tracey</last></author>
      <author><first>Owen</first><last>Rambow</last></author>
      <author><first>Claire</first><last>Cardie</last></author>
      <author><first>Adam</first><last>Dalton</last></author>
      <author><first>Hoa Trang</first><last>Dang</last></author>
      <author><first>Mona</first><last>Diab</last></author>
      <author><first>Bonnie</first><last>Dorr</last></author>
      <author><first>Louise</first><last>Guthrie</last></author>
      <author><first>Magdalena</first><last>Markowska</last></author>
      <author><first>Smaranda</first><last>Muresan</last></author>
      <author><first>Vinodkumar</first><last>Prabhakaran</last></author>
      <author><first>Samira</first><last>Shaikh</last></author>
      <author><first>Tomek</first><last>Strzalkowski</last></author>
      <pages>2460–2467</pages>
      <abstract>We present the BeSt corpus, which records cognitive state: who believes what (i.e., factuality), and who has what sentiment towards what. This corpus is inspired by similar source-and-target corpora, specifically MPQA and FactBank. The corpus comprises two genres, newswire and discussion forums, in three languages, Chinese (Mandarin), English, and Spanish. The corpus is distributed through the LDC.</abstract>
      <url hash="55a6b229">2022.lrec-1.262</url>
      <bibkey>tracey-etal-2022-best</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mpqa-opinion-corpus">MPQA Opinion Corpus</pwcdataset>
    </paper>
    <paper id="263">
      <title><fixed-case>MOTIF</fixed-case>: Contextualized Images for Complex Words to Improve Human Reading</title>
      <author><first>Xintong</first><last>Wang</last></author>
      <author><first>Florian</first><last>Schneider</last></author>
      <author><first>Özge</first><last>Alacam</last></author>
      <author><first>Prateek</first><last>Chaudhury</last></author>
      <author><first>Chris</first><last>Biemann</last></author>
      <pages>2468–2477</pages>
      <abstract>MOTIF (MultimOdal ConTextualized Images For Language Learners) is a multimodal dataset that consists of 1125 comprehension texts retrieved from Wikipedia Simple Corpus. Allowing multimodal processing or enriching the context with multimodal information has proven imperative for many learning tasks, specifically for second language (L2) learning. In this respect, several traditional NLP approaches can assist L2 readers in text comprehension processes, such as simplifying text or giving dictionary descriptions for complex words. As nicely stated in the well-known proverb, sometimes “a picture is worth a thousand words” and an image can successfully complement the verbal message by enriching the representation, like in Pictionary books. This multimodal support can also assist on-the-fly text reading experience by providing a multimodal tool that chooses and displays the most relevant images for the difficult words, given the text context. This study mainly focuses on one of the key components to achieving this goal; collecting a multimodal dataset enriched with complex word annotation and validated image match.</abstract>
      <url hash="6dc0077d">2022.lrec-1.263</url>
      <bibkey>wang-etal-2022-motif</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
    </paper>
    <paper id="264">
      <title>Challenges with Sign Language Datasets for Sign Language Recognition and Translation</title>
      <author><first>Mirella</first><last>De Sisto</last></author>
      <author><first>Vincent</first><last>Vandeghinste</last></author>
      <author><first>Santiago</first><last>Egea Gómez</last></author>
      <author><first>Mathieu</first><last>De Coster</last></author>
      <author><first>Dimitar</first><last>Shterionov</last></author>
      <author><first>Horacio</first><last>Saggion</last></author>
      <pages>2478–2487</pages>
      <abstract>Sign Languages (SLs) are the primary means of communication for at least half a million people in Europe alone. However, the development of SL recognition and translation tools is slowed down by a series of obstacles concerning resource scarcity and standardization issues in the available data. The former challenge relates to the volume of data available for machine learning as well as the time required to collect and process new data. The latter obstacle is linked to the variety of the data, i.e., annotation formats are not unified and vary amongst different resources. The available data formats are often not suitable for machine learning, obstructing the provision of automatic tools based on neural models. In the present paper, we give an overview of these challenges by comparing various SL corpora and SL machine learning datasets. Furthermore, we propose a framework to address the lack of standardization at format level, unify the available resources and facilitate SL research for different languages. Our framework takes ELAN files as inputs and returns textual and visual data ready to train SL recognition and translation models. We present a proof of concept, training neural translation models on the data produced by the proposed framework.</abstract>
      <url hash="bd08d39e">2022.lrec-1.264</url>
      <bibkey>de-sisto-etal-2022-challenges</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/bobsl">BOBSL</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/content4all">Content4All</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/how2sign">How2Sign</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/rwth-phoenix-weather-2014-t">RWTH-PHOENIX-Weather 2014 T</pwcdataset>
    </paper>
    <paper id="265">
      <title>A Low-Cost Motion Capture Corpus in <fixed-case>F</fixed-case>rench <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage for Interpreting Iconicity and Spatial Referencing Mechanisms</title>
      <author><first>Clémence</first><last>Mertz</last></author>
      <author><first>Vincent</first><last>Barreaud</last></author>
      <author><first>Thibaut</first><last>Le Naour</last></author>
      <author><first>Damien</first><last>Lolive</last></author>
      <author><first>Sylvie</first><last>Gibet</last></author>
      <pages>2488–2497</pages>
      <abstract>The automatic translation of sign language videos into transcribed texts is rarely approached in its whole, as it implies to finely model the grammatical mechanisms that govern these languages. The presented work is a first step towards the interpretation of French sign language (LSF) by specifically targeting iconicity and spatial referencing. This paper describes the LSF-SHELVES corpus as well as the original technology that was designed and implemented to collect it. Our goal is to use deep learning methods to circumvent the use of models in spatial referencing recognition. In order to obtain training material with sufficient variability, we designed a light-weight (and low-cost) capture protocol that enabled us to collect data from a large panel of LSF signers. This protocol involves the use of a portable device providing a 3D skeleton, and of a software developed specifically for this application to facilitate the post-processing of handshapes. The LSF-SHELVES includes simple and compound iconic and spatial dynamics, organized in 6 complexity levels, representing a total of 60 sequences signed by 15 LSF signers.</abstract>
      <url hash="ceee50d9">2022.lrec-1.265</url>
      <bibkey>mertz-etal-2022-low</bibkey>
    </paper>
    <paper id="266">
      <title>The <fixed-case>CLAMS</fixed-case> Platform at Work: Processing Audiovisual Data from the <fixed-case>A</fixed-case>merican Archive of Public Broadcasting</title>
      <author><first>Marc</first><last>Verhagen</last></author>
      <author><first>Kelley</first><last>Lynch</last></author>
      <author><first>Kyeongmin</first><last>Rim</last></author>
      <author><first>James</first><last>Pustejovsky</last></author>
      <pages>2498–2506</pages>
      <abstract>The Computational Linguistics Applications for Multimedia Services (CLAMS) platform provides access to computational content analysis tools for multimedia material. The version we present here is a robust update of an initial prototype implementation from 2019. The platform now sports a variety of image, video, audio and text processing tools that interact via a common multi-modal representation language named MMIF (Multi-Media Interchange Format). We describe the overall architecture, the MMIF format, some of the tools included in the platform, the process to set up and run a workflow, visualizations included in CLAMS, and evaluate aspects of the platform on data from the American Archive of Public Broadcasting, showing how CLAMS can add metadata to mass-digitized multimedia collections, metadata that are typically only available implicitly in now largely unsearchable digitized media in archives and libraries.</abstract>
      <url hash="362fb887">2022.lrec-1.266</url>
      <bibkey>verhagen-etal-2022-clams</bibkey>
    </paper>
    <paper id="267">
      <title><fixed-case>BU</fixed-case>-<fixed-case>NE</fixed-case>mo: an Affective Dataset of Gun Violence News</title>
      <author><first>Carley</first><last>Reardon</last></author>
      <author><first>Sejin</first><last>Paik</last></author>
      <author><first>Ge</first><last>Gao</last></author>
      <author><first>Meet</first><last>Parekh</last></author>
      <author><first>Yanling</first><last>Zhao</last></author>
      <author><first>Lei</first><last>Guo</last></author>
      <author><first>Margrit</first><last>Betke</last></author>
      <author><first>Derry Tanti</first><last>Wijaya</last></author>
      <pages>2507–2516</pages>
      <abstract>Given our society’s increased exposure to multimedia formats on social media platforms, efforts to understand how digital content impacts people’s emotions are burgeoning. As such, we introduce a U.S. gun violence news dataset that contains news headline and image pairings from 840 news articles with 15K high-quality, crowdsourced annotations on emotional responses to the news pairings. We created three experimental conditions for the annotation process: two with a single modality (headline or image only), and one multimodal (headline and image together). In contrast to prior works on affectively-annotated data, our dataset includes annotations on the dominant emotion experienced with the content, the intensity of the selected emotion and an open-ended, written component. By collecting annotations on different modalities of the same news content pairings, we explore the relationship between image and text influence on human emotional response. We offer initial analysis on our dataset, showing the nuanced affective differences that appear due to modality and individual factors such as political leaning and media consumption habits. Our dataset is made publicly available to facilitate future research in affective computing.</abstract>
      <url hash="e721d4ed">2022.lrec-1.267</url>
      <bibkey>reardon-etal-2022-bu</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/gvfc">GVFC</pwcdataset>
    </paper>
    <paper id="268">
      <title><fixed-case>R</fixed-case>oom<fixed-case>R</fixed-case>eader: A Multimodal Corpus of Online Multiparty Conversational Interactions</title>
      <author><first>Justine</first><last>Reverdy</last></author>
      <author><first>Sam</first><last>O’Connor Russell</last></author>
      <author><first>Louise</first><last>Duquenne</last></author>
      <author><first>Diego</first><last>Garaialde</last></author>
      <author><first>Benjamin R.</first><last>Cowan</last></author>
      <author><first>Naomi</first><last>Harte</last></author>
      <pages>2517–2527</pages>
      <abstract>We present RoomReader, a corpus of multimodal, multiparty conversational interactions in which participants followed a collaborative student-tutor scenario designed to elicit spontaneous speech. The corpus was developed within the wider RoomReader Project to explore multimodal cues of conversational engagement and behavioural aspects of collaborative interaction in online environments. However, the corpus can be used to study a wide range of phenomena in online multimodal interaction. The publicly-shared corpus consists of over 8 hours of video and audio recordings from 118 participants in 30 gender-balanced sessions, in the “in-the-wild” online environment of Zoom. The recordings have been edited, synchronised, and fully transcribed. Student participants have been continuously annotated for engagement with a novel continuous scale. We provide questionnaires measuring engagement and group cohesion collected from the annotators, tutors and participants themselves. We also make a range of accompanying data available such as personality tests and behavioural assessments. The dataset and accompanying psychometrics present a rich resource enabling the exploration of a range of downstream tasks across diverse fields including linguistics and artificial intelligence. This could include the automatic detection of student engagement, analysis of group interaction and collaboration in online conversation, and the analysis of conversational behaviours in an online setting.</abstract>
      <url hash="427cf977">2022.lrec-1.268</url>
      <bibkey>reverdy-etal-2022-roomreader</bibkey>
    </paper>
    <paper id="269">
      <title>Quevedo: Annotation and Processing of Graphical Languages</title>
      <author><first>Antonio F. G.</first><last>Sevilla</last></author>
      <author><first>Alberto</first><last>Díaz Esteban</last></author>
      <author><first>José María</first><last>Lahoz-Bengoechea</last></author>
      <pages>2528–2535</pages>
      <abstract>In this article, we present Quevedo, a software tool we have developed for the task of automatic processing of graphical languages. These are languages which use images to convey meaning, relying not only on the shape of symbols but also on their spatial arrangement in the page, and relative to each other. When presented in image form, these languages require specialized computational processing which is not the same as usually done either for natural language processing or for artificial vision. Quevedo enables this specialized processing, focusing on a data-based approach. As a command line application and library, it provides features for the collection and management of image datasets, and their machine learning recognition using neural networks and recognizer pipelines. This processing requires careful annotation of the source data, for which Quevedo offers an extensive and visual web-based annotation interface. In this article, we also briefly present a case study centered on the task of SignWriting recognition, the original motivation for writing the software. Quevedo is written in Python, and distributed freely under the Open Software License version 3.0.</abstract>
      <url hash="8dfc9f6e">2022.lrec-1.269</url>
      <bibkey>sevilla-etal-2022-quevedo</bibkey>
    </paper>
    <paper id="270">
      <title>Merkel Podcast Corpus: A Multimodal Dataset Compiled from 16 Years of Angela Merkel’s Weekly Video Podcasts</title>
      <author><first>Debjoy</first><last>Saha</last></author>
      <author><first>Shravan</first><last>Nayak</last></author>
      <author><first>Timo</first><last>Baumann</last></author>
      <pages>2536–2540</pages>
      <abstract>We introduce the Merkel Podcast Corpus, an audio-visual-text corpus in German collected from 16 years of (almost) weekly Internet podcasts of former German chancellor Angela Merkel. To the best of our knowledge, this is the first single speaker corpus in the German language consisting of audio, visual and text modalities of comparable size and temporal extent. We describe the methods used with which we have collected and edited the data which involves downloading the videos, transcripts and other metadata, forced alignment, performing active speaker recognition and face detection to finally curate the single speaker dataset consisting of utterances spoken by Angela Merkel. The proposed pipeline is general and can be used to curate other datasets of similar nature, such as talk show contents. Through various statistical analyses and applications of the dataset in talking face generation and TTS, we show the utility of the dataset. We argue that it is a valuable contribution to the research community, in particular, due to its realistic and challenging material at the boundary between prepared and spontaneous speech.</abstract>
      <url hash="b492fe5c">2022.lrec-1.270</url>
      <bibkey>saha-etal-2022-merkel</bibkey>
      <pwccode url="https://github.com/deeplsd/merkel-podcast-corpus" additional="false">deeplsd/merkel-podcast-corpus</pwccode>
    </paper>
    <paper id="271">
      <title>Crowdsourcing <fixed-case>K</fixed-case>azakh-<fixed-case>R</fixed-case>ussian <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage: <fixed-case>F</fixed-case>luent<fixed-case>S</fixed-case>igners-50</title>
      <author><first>Medet</first><last>Mukushev</last></author>
      <author><first>Aigerim</first><last>Kydyrbekova</last></author>
      <author><first>Alfarabi</first><last>Imashev</last></author>
      <author><first>Vadim</first><last>Kimmelman</last></author>
      <author><first>Anara</first><last>Sandygulova</last></author>
      <pages>2541–2547</pages>
      <abstract>This paper presents the methodology we used to crowdsource a data collection of a new large-scale signer independent dataset for Kazakh-Russian Sign Language (KRSL) created for Sign Language Processing. By involving the Deaf community throughout the research process, we firstly designed a research protocol and then performed an efficient crowdsourcing campaign that resulted in a new FluentSigners-50 dataset. The FluentSigners-50 dataset consists of 173 sentences performed by 50 KRSL signers for 43,250 video samples. Dataset contributors recorded videos in real-life settings on various backgrounds using various devices such as smartphones and web cameras. Therefore, each dataset contribution has a varying distance to the camera, camera angles and aspect ratio, video quality, and frame rates. Additionally, the proposed dataset contains a high degree of linguistic and inter-signer variability and thus is a better training set for recognizing a real-life signed speech. FluentSigners-50 is publicly available at https://krslproject.github.io/fluentsigners-50/</abstract>
      <url hash="6b118e57">2022.lrec-1.271</url>
      <bibkey>mukushev-etal-2022-crowdsourcing</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/how2sign">How2Sign</pwcdataset>
    </paper>
    <paper id="272">
      <title>Connecting a <fixed-case>F</fixed-case>rench Dictionary from the Beginning of the 20th Century to <fixed-case>W</fixed-case>ikidata</title>
      <author><first>Pierre</first><last>Nugues</last></author>
      <pages>2548–2555</pages>
      <abstract>The Petit Larousse illustré is a French dictionary first published in 1905. Its division in two main parts on language and on history and geography corresponds to a major milestone in French lexicography as well as a repository of general knowledge from this period. Although the value of many entries from 1905 remains intact, some descriptions now have a dimension that is more historical than contemporary. They are nonetheless significant to analyze and understand cultural representations from this time. A comparison with more recent information or a verification of these entries would require a tedious manual work. In this paper, we describe a new lexical resource, where we connected all the dictionary entries of the history and geography part to current data sources. For this, we linked each of these entries to a wikidata identifier. Using the wikidata links, we can automate more easily the identification, comparison, and verification of historically-situated representations. We give a few examples on how to process wikidata identifiers and we carried out a small analysis of the entities described in the dictionary to outline possible applications. The resource, i.e. the annotation of 20,245 dictionary entries with wikidata links, is available from GitHub (https://github.com/pnugues/petit_larousse_1905/)</abstract>
      <url hash="d63dd810">2022.lrec-1.272</url>
      <bibkey>nugues-2022-connecting</bibkey>
      <pwccode url="https://github.com/pnugues/petit_larousse_1905" additional="false">pnugues/petit_larousse_1905</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/larousse-1905-wd">larousse_1905_wd</pwcdataset>
    </paper>
    <paper id="273">
      <title>Metaphor annotation for <fixed-case>G</fixed-case>erman</title>
      <author><first>Markus</first><last>Egg</last></author>
      <author><first>Valia</first><last>Kordoni</last></author>
      <pages>2556–2562</pages>
      <abstract>The paper presents current work on a German corpus annotated for metaphor. Metaphors denote entities or situations that are in some sense similar to the literal referent, e.g., when “Handschrift” ‘signature’ is used in the sense of ‘distinguishing mark’ or the suppression of hopes is introduced by the verb “verschütten” ‘bury’. The corpus is part of a project on register, hence, includes material from different registers that represent register variation along a number of important dimensions, but we believe that it is of interest to research on metaphor in general. The corpus extends previous annotation initiatives in that it not only annotates the metaphoric expressions themselves but also their respective relevant contexts that trigger a metaphorical interpretation of the expressions. For the corpus, we developed extended annotation guidelines, which specifically focus not only on the identification of these metaphoric contexts but also analyse in detail specific linguistic challenges for metaphor annotation that emerge due to the grammar of German.</abstract>
      <url hash="b8fa3018">2022.lrec-1.273</url>
      <bibkey>egg-kordoni-2022-metaphor</bibkey>
    </paper>
    <paper id="274">
      <title><fixed-case>N</fixed-case>or<fixed-case>D</fixed-case>ia<fixed-case>C</fixed-case>hange: Diachronic Semantic Change Dataset for <fixed-case>N</fixed-case>orwegian</title>
      <author><first>Andrey</first><last>Kutuzov</last></author>
      <author><first>Samia</first><last>Touileb</last></author>
      <author><first>Petter</first><last>Mæhlum</last></author>
      <author><first>Tita</first><last>Enstad</last></author>
      <author><first>Alexandra</first><last>Wittemann</last></author>
      <pages>2563–2572</pages>
      <abstract>We describe NorDiaChange: the first diachronic semantic change dataset for Norwegian. NorDiaChange comprises two novel subsets, covering about 80 Norwegian nouns manually annotated with graded semantic change over time. Both datasets follow the same annotation procedure and can be used interchangeably as train and test splits for each other. NorDiaChange covers the time periods related to pre- and post-war events, oil and gas discovery in Norway, and technological developments. The annotation was done using the DURel framework and two large historical Norwegian corpora. NorDiaChange is published in full under a permissive licence, complete with raw annotation data and inferred diachronic word usage graphs (DWUGs).</abstract>
      <url hash="9a10d2e8">2022.lrec-1.274</url>
      <bibkey>kutuzov-etal-2022-nordiachange</bibkey>
    </paper>
    <paper id="275">
      <title>Exploring Transformers for Ranking <fixed-case>P</fixed-case>ortuguese Semantic Relations</title>
      <author><first>Hugo</first><last>Gonçalo Oliveira</last></author>
      <pages>2573–2582</pages>
      <abstract>We explored transformer-based language models for ranking instances of Portuguese lexico-semantic relations. Weights were based on the likelihood of natural language sequences that transmitted the relation instances, and expectations were that they would be useful for filtering out noisier instances. However, after analysing the weights, no strong conclusions were taken. They are not correlated with redundancy, but are lower for instances with longer and more specific arguments, which may nevertheless be a consequence of their sensitivity to the frequency of such arguments. They did also not reveal to be useful when computing word similarity with network embeddings. Despite the negative results, we see the reported experiments and insights as another contribution for better understanding transformer language models like BERT and GPT, and we make the weighted instances publicly available for further research.</abstract>
      <url hash="bb879d3e">2022.lrec-1.275</url>
      <bibkey>goncalo-oliveira-2022-exploring</bibkey>
    </paper>
    <paper id="276">
      <title>Building Static Embeddings from Contextual Ones: Is It Useful for Building Distributional Thesauri?</title>
      <author><first>Olivier</first><last>Ferret</last></author>
      <pages>2583–2590</pages>
      <abstract>While contextual language models are now dominant in the field of Natural Language Processing, the representations they build at the token level are not always suitable for all uses. In this article, we propose a new method for building word or type-level embeddings from contextual models. This method combines the generalization and the aggregation of token representations. We evaluate it for a large set of English nouns from the perspective of the building of distributional thesauri for extracting semantic similarity relations. Moreover, we analyze the differences between static embeddings and type-level embeddings according to features such as the frequency of words or the type of semantic relations these embeddings account for, showing that the properties of these two types of embeddings can be complementary and exploited for further improving distributional thesauri.</abstract>
      <url hash="4fac9b35">2022.lrec-1.276</url>
      <bibkey>ferret-2022-building</bibkey>
    </paper>
    <paper id="277">
      <title>Sentence Selection Strategies for Distilling Word Embeddings from <fixed-case>BERT</fixed-case></title>
      <author><first>Yixiao</first><last>Wang</last></author>
      <author><first>Zied</first><last>Bouraoui</last></author>
      <author><first>Luis</first><last>Espinosa Anke</last></author>
      <author><first>Steven</first><last>Schockaert</last></author>
      <pages>2591–2600</pages>
      <abstract>Many applications crucially rely on the availability of high-quality word vectors. To learn such representations, several strategies based on language models have been proposed in recent years. While effective, these methods typically rely on a large number of contextualised vectors for each word, which makes them impractical. In this paper, we investigate whether similar results can be obtained when only a few contextualised representations of each word can be used. To this end, we analyse a range of strategies for selecting the most informative sentences. Our results show that with a careful selection strategy, high-quality word vectors can be learned from as few as 5 to 10 sentences.</abstract>
      <url hash="3b0c48e1">2022.lrec-1.277</url>
      <bibkey>wang-etal-2022-sentence</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/genericskb">GenericsKB</pwcdataset>
    </paper>
    <paper id="278">
      <title><fixed-case>D</fixed-case>ia<fixed-case>WUG</fixed-case>: A Dataset for Diatopic Lexical Semantic Variation in <fixed-case>S</fixed-case>panish</title>
      <author><first>Gioia</first><last>Baldissin</last></author>
      <author><first>Dominik</first><last>Schlechtweg</last></author>
      <author><first>Sabine</first><last>Schulte im Walde</last></author>
      <pages>2601–2609</pages>
      <abstract>We provide a novel dataset – DiaWUG – with judgements on diatopic lexical semantic variation for six Spanish variants in Europe and Latin America. In contrast to most previous meaning-based resources and studies on semantic diatopic variation, we collect annotations on semantic relatedness for Spanish target words in their contexts from both a semasiological perspective (i.e., exploring the meanings of a word given its form, thus including polysemy) and an onomasiological perspective (i.e., exploring identical meanings of words with different forms, thus including synonymy). In addition, our novel dataset exploits and extends the existing framework DURel for annotating word senses in context (Erk et al., 2013; Schlechtweg et al., 2018) and the framework-embedded Word Usage Graphs (WUGs) – which up to now have mainly be used for semasiological tasks and resources – in order to distinguish, visualize and interpret lexical semantic variation of contextualized words in Spanish from these two perspectives, i.e., semasiological and onomasiological language variation.</abstract>
      <url hash="bf8f3b2a">2022.lrec-1.278</url>
      <bibkey>baldissin-etal-2022-diawug</bibkey>
    </paper>
    <paper id="279">
      <title>My Case, For an Adposition: Lexical Polysemy of Adpositions and Case Markers in <fixed-case>F</fixed-case>innish and <fixed-case>L</fixed-case>atin</title>
      <author><first>Daniel</first><last>Chen</last></author>
      <author><first>Mans</first><last>Hulden</last></author>
      <pages>2610–2616</pages>
      <abstract>Adpositions and case markers contain a high degree of polysemy and participate in unique semantic role configurations. We present a novel application of the SNACS supersense hierarchy to Finnish and Latin data by manually annotating adposition and case marker tokens in Finnish and Latin translations of Chapters IV-V of Le Petit Prince (The Little Prince). We evaluate the computational validity of the semantic role annotation categories by grouping raw, contextualized Multilingual BERT embeddings using k-means clustering.</abstract>
      <url hash="9d48d265">2022.lrec-1.279</url>
      <bibkey>chen-hulden-2022-case</bibkey>
    </paper>
    <paper id="280">
      <title><fixed-case>W</fixed-case>i<fixed-case>C</fixed-case>-<fixed-case>TSV</fixed-case>-de: <fixed-case>G</fixed-case>erman Word-in-Context Target-Sense-Verification Dataset and Cross-Lingual Transfer Analysis</title>
      <author><first>Anna</first><last>Breit</last></author>
      <author><first>Artem</first><last>Revenko</last></author>
      <author><first>Narayani</first><last>Blaschke</last></author>
      <pages>2617–2625</pages>
      <abstract>Target Sense Verification (TSV) describes the binary disambiguation task of deciding whether the intended sense of a target word in a context corresponds to a given target sense. In this paper, we introduce WiC-TSV-de, a multi-domain dataset for German Target Sense Verification. While the training and development sets consist of domain-independent instances only, the test set contains domain-bound subsets, originating from four different domains, being Gastronomy, Medicine, Hunting, and Zoology. The domain-bound subsets incorporate adversarial examples such as in-domain ambiguous target senses and context-mixing (i.e., using the target sense in an out-of-domain context) which contribute to the challenging nature of the presented dataset. WiC-TSV-de allows for the development of sense-inventory-independent disambiguation models that can generalise their knowledge for different domain settings. By combining it with the original English WiC-TSV benchmark, we performed monolingual and cross-lingual analysis, where the evaluated baseline models were not able to solve the dataset to a satisfying degree, leaving a big gap to human performance.</abstract>
      <url hash="68573f75">2022.lrec-1.280</url>
      <bibkey>breit-etal-2022-wic</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wic">WiC</pwcdataset>
    </paper>
    <paper id="281">
      <title>Re-train or Train from Scratch? Comparing Pre-training Strategies of <fixed-case>BERT</fixed-case> in the Medical Domain</title>
      <author><first>Hicham</first><last>El Boukkouri</last></author>
      <author><first>Olivier</first><last>Ferret</last></author>
      <author><first>Thomas</first><last>Lavergne</last></author>
      <author><first>Pierre</first><last>Zweigenbaum</last></author>
      <pages>2626–2633</pages>
      <abstract>BERT models used in specialized domains all seem to be the result of a simple strategy: initializing with the original BERT and then resuming pre-training on a specialized corpus. This method yields rather good performance (e.g. BioBERT (Lee et al., 2020), SciBERT (Beltagy et al., 2019), BlueBERT (Peng et al., 2019)). However, it seems reasonable to think that training directly on a specialized corpus, using a specialized vocabulary, could result in more tailored embeddings and thus help performance. To test this hypothesis, we train BERT models from scratch using many configurations involving general and medical corpora. Based on evaluations using four different tasks, we find that the initial corpus only has a weak influence on the performance of BERT models when these are further pre-trained on a medical corpus.</abstract>
      <url hash="5944884c">2022.lrec-1.281</url>
      <bibkey>el-boukkouri-etal-2022-train</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mimic-iii">MIMIC-III</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/openwebtext">OpenWebText</pwcdataset>
    </paper>
    <paper id="282">
      <title>Universal Semantic Annotator: the First Unified <fixed-case>API</fixed-case> for <fixed-case>WSD</fixed-case>, <fixed-case>SRL</fixed-case> and Semantic Parsing</title>
      <author><first>Riccardo</first><last>Orlando</last></author>
      <author><first>Simone</first><last>Conia</last></author>
      <author><first>Stefano</first><last>Faralli</last></author>
      <author><first>Roberto</first><last>Navigli</last></author>
      <pages>2634–2641</pages>
      <abstract>In this paper, we present the Universal Semantic Annotator (USeA), which offers the first unified API for high-quality automatic annotations of texts in 100 languages through state-of-the-art systems for Word Sense Disambiguation, Semantic Role Labeling and Semantic Parsing. Together, such annotations can be used to provide users with rich and diverse semantic information, help second-language learners, and allow researchers to integrate explicit semantic knowledge into downstream tasks and real-world applications.</abstract>
      <url hash="173ab0e5">2022.lrec-1.282</url>
      <bibkey>orlando-etal-2022-universal</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2012-1">CoNLL-2012</pwcdataset>
    </paper>
    <paper id="283">
      <title>D3: A Massive Dataset of Scholarly Metadata for Analyzing the State of Computer Science Research</title>
      <author><first>Jan Philip</first><last>Wahle</last></author>
      <author><first>Terry</first><last>Ruas</last></author>
      <author><first>Saif</first><last>Mohammad</last></author>
      <author><first>Bela</first><last>Gipp</last></author>
      <pages>2642–2651</pages>
      <abstract>DBLP is the largest open-access repository of scientific articles on computer science and provides metadata associated with publications, authors, and venues. We retrieved more than 6 million publications from DBLP and extracted pertinent metadata (e.g., abstracts, author affiliations, citations) from the publication texts to create the DBLP Discovery Dataset (D3). D3 can be used to identify trends in research activity, productivity, focus, bias, accessibility, and impact of computer science research. We present an initial analysis focused on the volume of computer science research (e.g., number of papers, authors, research activity), trends in topics of interest, and citation patterns. Our findings show that computer science is a growing research field (15% annually), with an active and collaborative researcher community. While papers in recent years present more bibliographical entries in comparison to previous decades, the average number of citations has been declining. Investigating papers’ abstracts reveals that recent topic trends are clearly reflected in D3. Finally, we list further applications of D3 and pose supplemental research questions. The D3 dataset, our findings, and source code are publicly available for research purposes.</abstract>
      <url hash="0230c351">2022.lrec-1.283</url>
      <bibkey>wahle-etal-2022-d3</bibkey>
      <pwccode url="https://github.com/gipplab/d3-dataset/blob/main/README.md" additional="false">gipplab/d3-dataset</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/d3">D3</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/s2orc">S2ORC</pwcdataset>
    </paper>
    <paper id="284">
      <title><fixed-case>S</fixed-case>ci<fixed-case>P</fixed-case>ar: A Collection of Parallel Corpora from Scientific Abstracts</title>
      <author><first>Dimitrios</first><last>Roussis</last></author>
      <author><first>Vassilis</first><last>Papavassiliou</last></author>
      <author><first>Prokopis</first><last>Prokopidis</last></author>
      <author><first>Stelios</first><last>Piperidis</last></author>
      <author><first>Vassilis</first><last>Katsouros</last></author>
      <pages>2652–2657</pages>
      <abstract>This paper presents SciPar, a new collection of parallel corpora created from openly available metadata of bachelor theses, master theses and doctoral dissertations hosted in institutional repositories, digital libraries of universities and national archives. We describe first how we harvested and processed metadata from 86, mainly European, repositories to extract bilingual titles and abstracts, and then how we mined high quality sentence pairs in a wide range of scientific areas and sub-disciplines. In total, the resource includes 9.17 million segment alignments in 31 language pairs and is publicly available via the ELRC-SHARE repository. The bilingual corpora in this collection could prove valuable in various applications, such as cross-lingual plagiarism detection or adapting Machine Translation systems for the translation of scientific texts and academic writing in general, especially for language pairs which include English.</abstract>
      <url hash="ce528d2d">2022.lrec-1.284</url>
      <bibkey>roussis-etal-2022-scipar</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/aspec">ASPEC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikimatrix">WikiMatrix</pwcdataset>
    </paper>
    <paper id="285">
      <title><fixed-case>CAT</fixed-case>s are Fuzzy <fixed-case>PET</fixed-case>s: A Corpus and Analysis of Potentially Euphemistic Terms</title>
      <author><first>Martha</first><last>Gavidia</last></author>
      <author><first>Patrick</first><last>Lee</last></author>
      <author><first>Anna</first><last>Feldman</last></author>
      <author><first>JIng</first><last>Peng</last></author>
      <pages>2658–2671</pages>
      <abstract>Euphemisms have not received much attention in natural language processing, despite being an important element of polite and figurative language. Euphemisms prove to be a difficult topic, not only because they are subject to language change, but also because humans may not agree on what is a euphemism and what is not. Nonetheless, the first step to tackling the issue is to collect and analyze examples of euphemisms. We present a corpus of potentially euphemistic terms (PETs) along with example texts from the GloWbE corpus. Additionally, we present a subcorpus of texts where these PETs are not being used euphemistically, which may be useful for future applications. We also discuss the results of multiple analyses run on the corpus. Firstly, we find that sentiment analysis on the euphemistic texts supports that PETs generally decrease negative and offensive sentiment. Secondly, we observe cases of disagreement in an annotation task, where humans are asked to label PETs as euphemistic or not in a subset of our corpus text examples. We attribute the disagreement to a variety of potential reasons, including if the PET was a commonly accepted term (CAT).</abstract>
      <url hash="a2d68140">2022.lrec-1.285</url>
      <bibkey>gavidia-etal-2022-cats</bibkey>
    </paper>
    <paper id="286">
      <title>Camel Treebank: An Open Multi-genre <fixed-case>A</fixed-case>rabic Dependency Treebank</title>
      <author><first>Nizar</first><last>Habash</last></author>
      <author><first>Muhammed</first><last>AbuOdeh</last></author>
      <author><first>Dima</first><last>Taji</last></author>
      <author><first>Reem</first><last>Faraj</last></author>
      <author><first>Jamila</first><last>El Gizuli</last></author>
      <author><first>Omar</first><last>Kallas</last></author>
      <pages>2672–2681</pages>
      <abstract>We present the Camel Treebank (CAMELTB), a 188K word open-source dependency treebank of Modern Standard and Classical Arabic. CAMELTB 1.0 includes 13 sub-corpora comprising selections of texts from pre-Islamic poetry to social media online commentaries, and covering a range of genres from religious and philosophical texts to news, novels, and student essays. The texts are all publicly available (out of copyright, creative commons, or under open licenses). The texts were morphologically tokenized and syntactically parsed automatically, and then manually corrected by a team of trained annotators. The annotations follow the guidelines of the Columbia Arabic Treebank (CATiB) dependency representation. We discuss our annotation process and guideline extensions, and we present some initial observations on lexical and syntactic differences among the annotated sub-corpora. This corpus will be publicly available to support and encourage research on Arabic NLP in general and on new, previously unexplored genres that are of interest to a wider spectrum of researchers, from historical linguistics and digital humanities to computer-assisted language pedagogy.</abstract>
      <url hash="88fd214f">2022.lrec-1.286</url>
      <bibkey>habash-etal-2022-camel</bibkey>
    </paper>
    <paper id="287">
      <title><fixed-case>M</fixed-case>ent<fixed-case>S</fixed-case>um: A Resource for Exploring Summarization of Mental Health Online Posts</title>
      <author><first>Sajad</first><last>Sotudeh</last></author>
      <author><first>Nazli</first><last>Goharian</last></author>
      <author><first>Zachary</first><last>Young</last></author>
      <pages>2682–2692</pages>
      <abstract>Mental health remains a significant challenge of public health worldwide. With increasing popularity of online platforms, many use the platforms to share their mental health conditions, express their feelings, and seek help from the community and counselors. Some of these platforms, such as Reachout, are dedicated forums where the users register to seek help. Others such as Reddit provide subreddits where the users publicly but anonymously post their mental health distress. Although posts are of varying length, it is beneficial to provide a short, but informative summary for fast processing by the counselors. To facilitate research in summarization of mental health online posts, we introduce Mental Health Summarization dataset, MentSum, containing over 24k carefully selected user posts from Reddit, along with their short user-written summary (called TLDR) in English from 43 mental health subreddits. This domain-specific dataset could be of interest not only for generating short summaries on Reddit, but also for generating summaries of posts on the dedicated mental health forums such as Reachout. We further evaluate both extractive and abstractive state-of-the-art summarization baselines in terms of Rouge scores, and finally conduct an in-depth human evaluation study of both user-written and system-generated summaries, highlighting challenges in this research.</abstract>
      <url hash="d5664867">2022.lrec-1.287</url>
      <bibkey>sotudeh-etal-2022-mentsum</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mentsum">MentSum</pwcdataset>
    </paper>
    <paper id="288">
      <title>Klexikon: A <fixed-case>G</fixed-case>erman Dataset for Joint Summarization and Simplification</title>
      <author><first>Dennis</first><last>Aumiller</last></author>
      <author><first>Michael</first><last>Gertz</last></author>
      <pages>2693–2701</pages>
      <abstract>Traditionally, Text Simplification is treated as a monolingual translation task where sentences between source texts and their simplified counterparts are aligned for training. However, especially for longer input documents, summarizing the text (or dropping less relevant content altogether) plays an important role in the simplification process, which is currently not reflected in existing datasets. Simultaneously, resources for non-English languages are scarce in general and prohibitive for training new solutions. To tackle this problem, we pose core requirements for a system that can jointly summarize and simplify long source documents. We further describe the creation of a new dataset for joint Text Simplification and Summarization based on German Wikipedia and the German children’s encyclopedia “Klexikon”, consisting of almost 2,900 documents. We release a document-aligned version that particularly highlights the summarization aspect, and provide statistical evidence that this resource is well suited to simplification as well. Code and data are available on Github: https://github.com/dennlinger/klexikon</abstract>
      <url hash="1b144a8a">2022.lrec-1.288</url>
      <bibkey>aumiller-gertz-2022-klexikon</bibkey>
      <pwccode url="https://github.com/dennlinger/klexikon" additional="true">dennlinger/klexikon</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/klexikon">Klexikon</pwcdataset>
    </paper>
    <paper id="289">
      <title>Applying Automatic Text Summarization for Fake News Detection</title>
      <author><first>Philipp</first><last>Hartl</last></author>
      <author><first>Udo</first><last>Kruschwitz</last></author>
      <pages>2702–2713</pages>
      <abstract>The distribution of fake news is not a new but a rapidly growing problem. The shift to news consumption via social media has been one of the drivers for the spread of misleading and deliberately wrong information, as in addition to its ease of use there is rarely any veracity monitoring. Due to the harmful effects of such fake news on society, the detection of these has become increasingly important. We present an approach to the problem that combines the power of transformer-based language models while simultaneously addressing one of their inherent problems. Our framework, CMTR-BERT, combines multiple text representations, with the goal of circumventing sequential limits and related loss of information the underlying transformer architecture typically suffers from. Additionally, it enables the incorporation of contextual information. Extensive experiments on two very different, publicly available datasets demonstrates that our approach is able to set new state-of-the-art performance benchmarks. Apart from the benefit of using automatic text summarization techniques we also find that the incorporation of contextual information contributes to performance gains.</abstract>
      <url hash="88a1bc7c">2022.lrec-1.289</url>
      <bibkey>hartl-kruschwitz-2022-applying</bibkey>
      <pwccode url="https://github.com/phhartl/lrec_2022" additional="false">phhartl/lrec_2022</pwccode>
    </paper>
    <paper id="290">
      <title>Increasing <fixed-case>CMDI</fixed-case>’s Semantic Interoperability with schema.org</title>
      <author><first>Nino</first><last>Meisinger</last></author>
      <author><first>Thorsten</first><last>Trippel</last></author>
      <author><first>Claus</first><last>Zinn</last></author>
      <pages>2714–2720</pages>
      <abstract>The CLARIN Concept Registry (CCR) is the common semantic ground for most CMDI-based profiles to describe language-related resources in the CLARIN universe. While the CCR supports semantic interoperability within this universe, it does not extend beyond it. The flexibility of CMDI, however, allows users to use other term or concept registries when defining their metadata components. In this paper, we describe our use of schema.org, a light ontology used by many parties across disciplines.</abstract>
      <url hash="cd7133db">2022.lrec-1.290</url>
      <bibkey>meisinger-etal-2022-increasing</bibkey>
    </paper>
    <paper id="291">
      <title><fixed-case>R</fixed-case>ef<fixed-case>C</fixed-case>o and its Checker: Improving Language Documentation Corpora’s Reusability Through a Semi-Automatic Review Process</title>
      <author><first>Herbert</first><last>Lange</last></author>
      <author><first>Jocelyn</first><last>Aznar</last></author>
      <pages>2721–2729</pages>
      <abstract>The QUEST (QUality ESTablished) project aims at ensuring the reusability of audio-visual datasets (Wamprechtshammer et al., 2022) by devising quality criteria and curating processes. RefCo (Reference Corpora) is an initiative within QUEST in collaboration with DoReCo (Documentation Reference Corpus, Paschen et al. (2020)) focusing on language documentation projects. Previously, Aznar and Seifart (2020) introduced a set of quality criteria dedicated to documenting fieldwork corpora. Based on these criteria, we establish a semi-automatic review process for existing and work-in-progress corpora, in particular for language documentation. The goal is to improve the quality of a corpus by increasing its reusability. A central part of this process is a template for machine-readable corpus documentation and automatic data verification based on this documentation. In addition to the documentation and automatic verification, the process involves a human review and potentially results in a RefCo certification of the corpus. For each of these steps, we provide guidelines and manuals. We describe the evaluation process in detail, highlight the current limits for automatic evaluation and how the manual review is organized accordingly.</abstract>
      <url hash="5b13b1c4">2022.lrec-1.291</url>
      <bibkey>lange-aznar-2022-refco</bibkey>
    </paper>
    <paper id="292">
      <title>Identification and Analysis of Personification in <fixed-case>H</fixed-case>ungarian: The <fixed-case>P</fixed-case>er<fixed-case>SEC</fixed-case>orp project</title>
      <author><first>Gábor</first><last>Simon</last></author>
      <pages>2730–2738</pages>
      <abstract>Despite the recent findings on the conceptual and linguistic organization of personification, we have relatively little knowledge about its lexical patterns and grammatical templates. It is especially true in the case of Hungarian which has remained an understudied language regarding the constructions of figurative meaning generation. The present paper aims to provide a corpus-driven approach to personification analysis in the framework of cognitive linguistics. This approach is based on the building of a semi-automatically processed research corpus (the PerSE corpus) in which personifying linguistic structures are annotated manually. The present test version of the corpus consists of online car reviews written in Hungarian (10468 words altogether): the texts were tokenized, lemmatized, morphologically analyzed, syntactically parsed, and PoS-tagged with the e-magyar NLP tool. For the identification of personifications, the adaptation of the MIPVU protocol was used and combined with additional analysis of semantic relations within personifying multi-word expressions. The paper demonstrates the structure of the corpus as well as the levels of the annotation. Furthermore, it gives an overview of possible data types emerging from the analysis: lexical pattern, grammatical characteristics, and the construction-like behavior of personifications in Hungarian.</abstract>
      <url hash="604bb564">2022.lrec-1.292</url>
      <bibkey>simon-2022-identification</bibkey>
    </paper>
    <paper id="293">
      <title><fixed-case>ISO</fixed-case>-based Annotated Multilingual Parallel Corpus for Discourse Markers</title>
      <author><first>Purificação</first><last>Silvano</last></author>
      <author><first>Mariana</first><last>Damova</last></author>
      <author><first>Giedrė Valūnaitė</first><last>Oleškevičienė</last></author>
      <author><first>Chaya</first><last>Liebeskind</last></author>
      <author><first>Christian</first><last>Chiarcos</last></author>
      <author><first>Dimitar</first><last>Trajanov</last></author>
      <author><first>Ciprian-Octavian</first><last>Truică</last></author>
      <author><first>Elena-Simona</first><last>Apostol</last></author>
      <author><first>Anna</first><last>Baczkowska</last></author>
      <pages>2739–2749</pages>
      <abstract>Discourse markers carry information about the discourse structure and organization, and also signal local dependencies or epistemological stance of speaker. They provide instructions on how to interpret the discourse, and their study is paramount to understand the mechanism underlying discourse organization. This paper presents a new language resource, an ISO-based annotated multilingual parallel corpus for discourse markers. The corpus comprises nine languages, Bulgarian, Lithuanian, German, European Portuguese, Hebrew, Romanian, Polish, and Macedonian, with English as a pivot language. In order to represent the meaning of the discourse markers, we propose an annotation scheme of discourse relations from ISO 24617-8 with a plug-in to ISO 24617-2 for communicative functions. We describe an experiment in which we applied the annotation scheme to assess its validity. The results reveal that, although some extensions are required to cover all the multilingual data, it provides a proper representation of discourse markers value. Additionally, we report some relevant contrastive phenomena concerning discourse markers interpretation and role in discourse. This first step will allow us to develop deep learning methods to identify and extract discourse relations and communicative functions, and to represent that information as Linguistic Linked Open Data (LLOD).</abstract>
      <url hash="3b8f3b80">2022.lrec-1.293</url>
      <bibkey>silvano-etal-2022-iso</bibkey>
    </paper>
    <paper id="294">
      <title><fixed-case>LIP</fixed-case>-<fixed-case>RTVE</fixed-case>: An Audiovisual Database for Continuous <fixed-case>S</fixed-case>panish in the Wild</title>
      <author><first>David</first><last>Gimeno-Gómez</last></author>
      <author><first>Carlos-D.</first><last>Martínez-Hinarejos</last></author>
      <pages>2750–2758</pages>
      <abstract>Speech is considered as a multi-modal process where hearing and vision are two fundamentals pillars. In fact, several studies have demonstrated that the robustness of Automatic Speech Recognition systems can be improved when audio and visual cues are combined to represent the nature of speech. In addition, Visual Speech Recognition, an open research problem whose purpose is to interpret speech by reading the lips of the speaker, has been a focus of interest in the last decades. Nevertheless, in order to estimate these systems in the currently Deep Learning era, large-scale databases are required. On the other hand, while most of these databases are dedicated to English, other languages lack sufficient resources. Thus, this paper presents a semi-automatically annotated audiovisual database to deal with unconstrained natural Spanish, providing 13 hours of data extracted from Spanish television. Furthermore, baseline results for both speaker-dependent and speaker-independent scenarios are reported using Hidden Markov Models, a traditional paradigm that has been widely used in the field of Speech Technologies.</abstract>
      <url hash="af859548">2022.lrec-1.294</url>
      <bibkey>gimeno-gomez-martinez-hinarejos-2022-lip</bibkey>
      <pwccode url="https://github.com/david-gimeno/lip-rtve" additional="false">david-gimeno/lip-rtve</pwccode>
    </paper>
    <paper id="295">
      <title>Modality Alignment between Deep Representations for Effective Video-and-Language Learning</title>
      <author><first>Hyeongu</first><last>Yun</last></author>
      <author><first>Yongil</first><last>Kim</last></author>
      <author><first>Kyomin</first><last>Jung</last></author>
      <pages>2759–2770</pages>
      <abstract>Video-and-Language learning, such as video question answering or video captioning, is the next challenge in the deep learning society, as it pursues the way how human intelligence perceives everyday life. These tasks require the ability of multi-modal reasoning which is to handle both visual information and text information simultaneously across time. In this point of view, a cross-modality attention module that fuses video representation and text representation takes a critical role in most recent approaches. However, existing Video-and-Language models merely compute the attention weights without considering the different characteristics of video modality and text modality. Such na ̈ıve attention module hinders the current models to fully enjoy the strength of cross-modality. In this paper, we propose a novel Modality Alignment method that benefits the cross-modality attention module by guiding it to easily amalgamate multiple modalities. Specifically, we exploit Centered Kernel Alignment (CKA) which was originally proposed to measure the similarity between two deep representations. Our method directly optimizes CKA to make an alignment between video and text embedding representations, hence it aids the cross-modality attention module to combine information over different modalities. Experiments on real-world Video QA tasks demonstrate that our method outperforms conventional multi-modal methods significantly with +3.57% accuracy increment compared to the baseline in a popular benchmark dataset. Additionally, in a synthetic data environment, we show that learning the alignment with our method boosts the performance of the cross-modality attention.</abstract>
      <url hash="84b8f7aa">2022.lrec-1.295</url>
      <bibkey>yun-etal-2022-modality</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/tvqa">TVQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tvqa-1">TVQA+</pwcdataset>
    </paper>
    <paper id="296">
      <title>Mutual Gaze and Linguistic Repetition in a Multimodal Corpus</title>
      <author><first>Anais</first><last>Murat</last></author>
      <author><first>Maria</first><last>Koutsombogera</last></author>
      <author><first>Carl</first><last>Vogel</last></author>
      <pages>2771–2780</pages>
      <abstract>This paper investigates the correlation between mutual gaze and linguistic repetition, a form of alignment, which we take as evidence of mutual understanding. We focus on a multimodal corpus made of three-party conversations and explore the question of whether mutual gaze events correspond to moments of repetition or non-repetition. Our results, although mainly significant on word unigrams and bigrams, suggest positive correlations between the presence of mutual gaze and the repetitions of tokens, lemmas, or parts-of-speech, but negative correlations when it comes to paired levels of representation (tokens or lemmas associated with their part-of-speech). No compelling correlation is found with duration of mutual gaze. Results are strongest when ignoring punctuation as representations of pauses, intonation, etc. in counting aligned tokens.</abstract>
      <url hash="39871c2e">2022.lrec-1.296</url>
      <bibkey>murat-etal-2022-mutual</bibkey>
    </paper>
    <paper id="297">
      <title>Multidimensional Coding of Multimodal Languaging in Multi-Party Settings</title>
      <author><first>Christophe</first><last>Parisse</last></author>
      <author><first>Marion</first><last>Blondel</last></author>
      <author><first>Stéphanie</first><last>Caët</last></author>
      <author><first>Claire</first><last>Danet</last></author>
      <author><first>Coralie</first><last>Vincent</last></author>
      <author><first>Aliyah</first><last>Morgenstern</last></author>
      <pages>2781–2787</pages>
      <abstract>In natural language settings, many interactions include more than two speakers, and real-life interpretation is based on all types of information available in all modalities. This constitutes a challenge for corpus-based analyses because the information in the audio and visual channels must be included in the coding. The goal of the DINLANG project is to tackle that challenge and analyze spontaneous interactions in family dinner settings (two adults and two to three children). The families use either French, or LSF (French sign language). Our aim is to compare how participants share language across the range of modalities found in vocal and visual languaging in coordination with dining. In order to pinpoint similarities and differences, we had to find a common coding tool for all situations (variations from one family to another) and modalities. Our coding procedure incorporates the use of the ELAN software. We created a template organized around participants, situations, and modalities, rather than around language forms. Spoken language transcription can be integrated, when it exists, but it is not mandatory. Data that has been created with another software can be injected in ELAN files if it is linked using time stamps. Analyses performed with the coded files rely on ELAN’s structured search functionalities, which allow to achieve fine-grained temporal analyses and which can be completed by using spreadsheets or R language.</abstract>
      <url hash="1d6a8482">2022.lrec-1.297</url>
      <bibkey>parisse-etal-2022-multidimensional</bibkey>
    </paper>
    <paper id="298">
      <title>Constructing a Lexical Resource of <fixed-case>R</fixed-case>ussian Derivational Morphology</title>
      <author><first>Lukáš</first><last>Kyjánek</last></author>
      <author><first>Olga</first><last>Lyashevskaya</last></author>
      <author><first>Anna</first><last>Nedoluzhko</last></author>
      <author><first>Daniil</first><last>Vodolazsky</last></author>
      <author><first>Zdeněk</first><last>Žabokrtský</last></author>
      <pages>2788–2797</pages>
      <abstract>Words of any language are to some extent related thought the ways they are formed. For instance, the verb ‘exempl-ify’ and the noun ‘example-s’ are both based on the word ‘example’, but the verb is derived from it, while the noun is inflected. In Natural Language Processing of Russian, the inflection is satisfactorily processed; however, there are only a few machine-trackable resources that capture derivations even though Russian has both of these morphological processes very rich. Therefore, we devote this paper to improving one of the methods of constructing such resources and to the application of the method to a Russian lexicon, which results in the creation of the largest lexical resource of Russian derivational relations. The resulting database dubbed DeriNet.RU includes more than 300 thousand lexemes connected with more than 164 thousand binary derivational relations. To create such data, we combined the existing machine-learning methods that we improved to manage this goal. The whole approach is evaluated on our newly created data set of manual, parallel annotation. The resulting DeriNet.RU is freely available under an open license agreement.</abstract>
      <url hash="4d3bf6e9">2022.lrec-1.298</url>
      <bibkey>kyjanek-etal-2022-constructing</bibkey>
    </paper>
    <paper id="299">
      <title>Using Linguistic Typology to Enrich Multilingual Lexicons: the Case of Lexical Gaps in Kinship</title>
      <author><first>Temuulen</first><last>Khishigsuren</last></author>
      <author><first>Gábor</first><last>Bella</last></author>
      <author><first>Khuyagbaatar</first><last>Batsuren</last></author>
      <author><first>Abed Alhakim</first><last>Freihat</last></author>
      <author><first>Nandu</first><last>Chandran Nair</last></author>
      <author><first>Amarsanaa</first><last>Ganbold</last></author>
      <author><first>Hadi</first><last>Khalilia</last></author>
      <author><first>Yamini</first><last>Chandrashekar</last></author>
      <author><first>Fausto</first><last>Giunchiglia</last></author>
      <pages>2798–2807</pages>
      <abstract>This paper describes a method to enrich lexical resources with content relating to linguistic diversity, based on knowledge from the field of lexical typology. We capture the phenomenon of diversity through the notion of lexical gap and use a systematic method to infer gaps semi-automatically on a large scale, which we demonstrate on the kinship domain. The resulting free diversity-aware terminological resource consists of 198 concepts, 1,911 words, and 37,370 gaps in 699 languages. We see great potential in the use of resources such as ours for the improvement of a variety of cross-lingual NLP tasks, which we illustrate through an application in the evaluation of machine translation systems.</abstract>
      <url hash="cf68e94b">2022.lrec-1.299</url>
      <bibkey>khishigsuren-etal-2022-using</bibkey>
      <pwccode url="https://github.com/kbatsuren/kindiv" additional="false">kbatsuren/kindiv</pwccode>
    </paper>
    <paper id="300">
      <title>Towards <fixed-case>L</fixed-case>atvian <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et</title>
      <author><first>Peteris</first><last>Paikens</last></author>
      <author><first>Mikus</first><last>Grasmanis</last></author>
      <author><first>Agute</first><last>Klints</last></author>
      <author><first>Ilze</first><last>Lokmane</last></author>
      <author><first>Lauma</first><last>Pretkalniņa</last></author>
      <author><first>Laura</first><last>Rituma</last></author>
      <author><first>Madara</first><last>Stāde</last></author>
      <author><first>Laine</first><last>Strankale</last></author>
      <pages>2808–2815</pages>
      <abstract>In this paper we describe our current work on creating a WordNet for Latvian based on the principles of the Princeton WordNet. The chosen methodology for word sense definition and sense linking is based on corpus evidence and the existing Tezaurs.lv online dictionary, ensuring a foundation that fits the Latvian language usage and existing linguistic tradition. We cover a wide set of semantic relations, including gradation sets. Currently the dataset consists of 6432 words linked in 5528 synsets, out of which 2717 synsets are considered fully completed as they have all the outgoing semantic links annotated, annotated with corpus examples for each sense and links to the English Princeton WordNet.</abstract>
      <url hash="cd46e416">2022.lrec-1.300</url>
      <bibkey>paikens-etal-2022-towards</bibkey>
    </paper>
    <paper id="301">
      <title>Building Sentiment Lexicons for <fixed-case>M</fixed-case>ainland <fixed-case>S</fixed-case>candinavian Languages Using Machine Translation and Sentence Embeddings</title>
      <author><first>Peng</first><last>Liu</last></author>
      <author><first>Cristina</first><last>Marco</last></author>
      <author><first>Jon Atle</first><last>Gulla</last></author>
      <pages>2816–2825</pages>
      <abstract>This paper presents a simple but effective method to build sentiment lexicons for the three Mainland Scandinavian languages: Danish, Norwegian and Swedish. This method benefits from the English Sentiwordnet and a thesaurus in one of the target languages. Sentiment information from the English resource is mapped to the target languages by using machine translation and similarity measures based on sentence embeddings. A number of experiments with Scandinavian languages are performed in order to determine the best working sentence embedding algorithm for this task. A careful extrinsic evaluation on several datasets yields state-of-the-art results using a simple rule-based sentiment analysis algorithm. The resources are made freely available under an MIT License.</abstract>
      <url hash="2da25348">2022.lrec-1.301</url>
      <bibkey>liu-etal-2022-building</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/norec">NoReC</pwcdataset>
    </paper>
    <paper id="302">
      <title>A Thesaurus-based Sentiment Lexicon for <fixed-case>D</fixed-case>anish: The <fixed-case>D</fixed-case>anish Sentiment Lexicon</title>
      <author><first>Sanni</first><last>Nimb</last></author>
      <author><first>Sussi</first><last>Olsen</last></author>
      <author><first>Bolette</first><last>Pedersen</last></author>
      <author><first>Thomas</first><last>Troelsgård</last></author>
      <pages>2826–2832</pages>
      <abstract>This paper describes how a newly published Danish sentiment lexicon with a high lexical coverage was compiled by use of lexicographic methods and based on the links between groups of words listed in semantic order in a thesaurus and the corresponding word sense descriptions in a comprehensive monolingual dictionary. The overall idea was to identify negative and positive sections in a thesaurus, extract the words from these sections and combine them with the dictionary information via the links. The annotation task of the dataset included several steps, and was based on the comparison of synonyms and near synonyms within a semantic field. In the cases where one of the words were included in the smaller Danish sentiment lexicon AFINN, its value there was used as inspiration and expanded to the synonyms when appropriate. In order to obtain a more practical lexicon with overall polarity values at lemma level, all the senses of the lemma were afterwards compared, taking into consideration dictionary information such as usage, style and frequency. The final lexicon contains 13,859 Danish polarity lemmas and includes morphological information. It is freely available at https://github.com/dsldk/danish-sentiment-lexicon (licence CC-BY-SA 4.0 International).</abstract>
      <url hash="2ac42b8b">2022.lrec-1.302</url>
      <bibkey>nimb-etal-2022-thesaurus</bibkey>
    </paper>
    <paper id="303">
      <title><fixed-case>I</fixed-case>ndo<fixed-case>UKC</fixed-case>: A Concept-Centered <fixed-case>I</fixed-case>ndian Multilingual Lexical Resource</title>
      <author><first>Nandu</first><last>Chandran Nair</last></author>
      <author><first>Rajendran S.</first><last>Velayuthan</last></author>
      <author><first>Yamini</first><last>Chandrashekar</last></author>
      <author><first>Gábor</first><last>Bella</last></author>
      <author><first>Fausto</first><last>Giunchiglia</last></author>
      <pages>2833–2840</pages>
      <abstract>We introduce the IndoUKC, a new multilingual lexical database comprised of eighteen Indian languages, with a focus on formally capturing words and word meanings specific to Indian languages and cultures. The IndoUKC reuses content from the existing IndoWordNet resource while providing a new model for the cross-lingual mapping of lexical meanings that allows for a richer, diversity-aware representation. Accordingly, beyond a thorough syntactic and semantic cleaning, the IndoWordNet lexical content has been thoroughly remodeled in order to allow a more precise expression of language-specific meaning. The resulting database is made available both for browsing through a graphical web interface and for download through the LiveLanguage data catalogue.</abstract>
      <url hash="98e184ea">2022.lrec-1.303</url>
      <bibkey>chandran-nair-etal-2022-indoukc</bibkey>
    </paper>
    <paper id="304">
      <title><fixed-case>K</fixed-case>orean Language Modeling via Syntactic Guide</title>
      <author><first>Hyeondey</first><last>Kim</last></author>
      <author><first>Seonhoon</first><last>Kim</last></author>
      <author><first>Inho</first><last>Kang</last></author>
      <author><first>Nojun</first><last>Kwak</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <pages>2841–2849</pages>
      <abstract>While pre-trained language models play a vital role in modern language processing tasks, but not every language can benefit from them. Most existing research on pre-trained language models focuses primarily on widely-used languages such as English, Chinese, and Indo-European languages. Additionally, such schemes usually require extensive computational resources alongside a large amount of data, which is infeasible for less-widely used languages. We aim to address this research niche by building a language model that understands the linguistic phenomena in the target language which can be trained with low-resources. In this paper, we discuss Korean language modeling, specifically methods for language representation and pre-training methods. With our Korean-specific language representation, we are able to build more powerful language models for Korean understanding, even with fewer resources. The paper proposes chunk-wise reconstruction of the Korean language based on a widely used transformer architecture and bidirectional language representation. We also introduce morphological features such as Part-of-Speech (PoS) into the language understanding by leveraging such information during the pre-training. Our experiment results prove that the proposed methods improve the model performance of the investigated Korean language understanding tasks.</abstract>
      <url hash="15e78c6f">2022.lrec-1.304</url>
      <bibkey>kim-etal-2022-korean</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/kornli">KorNLI</pwcdataset>
    </paper>
    <paper id="305">
      <title>A Whole-Person Function Dictionary for the Mobility, Self-Care and Domestic Life Domains: a Seedset Expansion Approach</title>
      <author><first>Ayah</first><last>Zirikly</last></author>
      <author><first>Bart</first><last>Desmet</last></author>
      <author><first>Julia</first><last>Porcino</last></author>
      <author><first>Jonathan</first><last>Camacho Maldonado</last></author>
      <author><first>Pei-Shu</first><last>Ho</last></author>
      <author><first>Rafael</first><last>Jimenez Silva</last></author>
      <author><first>Maryanne</first><last>Sacco</last></author>
      <pages>2850–2855</pages>
      <abstract>Whole-person functional limitations in the areas of mobility, self-care and domestic life affect a majority of individuals with disabilities. Detecting, recording and monitoring such limitations would benefit those individuals, as well as research on whole-person functioning and general public health. Dictionaries of terms related to whole-person function would enable automated identification and extraction of relevant information. However, no such terminologies currently exist, due in part to a lack of standardized coding and their availability mainly in free text clinical notes. In this paper, we introduce terminologies of whole-person function in the domains of mobility, self-care and domestic life, built and evaluated using a small set of manually annotated clinical notes, which provided a seedset that was expanded using a mix of lexical and deep learning approaches.</abstract>
      <url hash="2b840ac3">2022.lrec-1.305</url>
      <bibkey>zirikly-etal-2022-whole</bibkey>
      <pwccode url="https://github.com/cc-rmd-epibio/terminologies" additional="false">cc-rmd-epibio/terminologies</pwccode>
    </paper>
    <paper id="306">
      <title>Placing multi-modal, and multi-lingual Data in the Humanities Domain on the Map: the Mythotopia Geo-tagged Corpus</title>
      <author><first>Voula</first><last>Giouli</last></author>
      <author><first>Anna</first><last>Vacalopoulou</last></author>
      <author><first>Nikolaos</first><last>Sidiropoulos</last></author>
      <author><first>Christina</first><last>Flouda</last></author>
      <author><first>Athanasios</first><last>Doupas</last></author>
      <author><first>Giorgos</first><last>Giannopoulos</last></author>
      <author><first>Nikos</first><last>Bikakis</last></author>
      <author><first>Vassilis</first><last>Kaffes</last></author>
      <author><first>Gregory</first><last>Stainhaouer</last></author>
      <pages>2856–2864</pages>
      <abstract>The paper gives an account of an infrastructure that will be integrated into a platform aimed at providing a multi-faceted experience to visitors of Northern Greece using mythology as a starting point. This infrastructure comprises a multi-lingual and multi-modal corpus (i.e., a corpus of textual data supplemented with images, and video) that belongs to the humanities domain along with a dedicated database (content management system) with advanced indexing, linking and search functionalities. We will present the corpus itself focusing on the content, the methodology adopted for its development, and the steps taken towards rendering it accessible via the database in a way that also facilitates useful visualizations. In this context, we tried to address three main challenges: (a) to add a novel annotation layer, namely geotagging, (b) to ensure the long-term maintenance of and accessibility to the highly heterogeneous primary data – even after the life cycle of the current project – by adopting a metadata schema that is compatible to existing standards; and (c) to render the corpus a useful resource to scholarly research in the digital humanities by adding a minimum set of linguistic annotations.</abstract>
      <url hash="34adbde3">2022.lrec-1.306</url>
      <bibkey>giouli-etal-2022-placing</bibkey>
    </paper>
    <paper id="307">
      <title>An Architecture of resolving a multiple link path in a standoff-style data format to enhance the mobility of language resources</title>
      <author><first>Kazushi</first><last>Ohya</last></author>
      <pages>2865–2873</pages>
      <abstract>The present data formats proposed by authentic organizations are based on a so-called standoff-style data format in XML, which represents a semantic data model through an instance structure and a link structure. However, this type of data formats intended to enhance the power of representation of an XML format injures the mobility of data because an abstract data structure denoted by multiple link paths is hard to be converted into other data structures. This difficulty causes a problem in the reuse of data to convert into other data formats especially in a personal data management environment. In this paper, in order to compensate for the drawback, we propose a new concept of transforming a link structure to an instance structure on a new marked-up scheme. This approach to language data brings a new architecture of language data management to realize a personal data management environment in daily and long-life use.</abstract>
      <url hash="e1d96bd9">2022.lrec-1.307</url>
      <bibkey>ohya-2022-architecture</bibkey>
    </paper>
    <paper id="308">
      <title>A Corpus of <fixed-case>G</fixed-case>erman Citizen Contributions in Mobility Planning: Supporting Evaluation Through Multidimensional Classification</title>
      <author><first>Julia</first><last>Romberg</last></author>
      <author><first>Laura</first><last>Mark</last></author>
      <author><first>Tobias</first><last>Escher</last></author>
      <pages>2874–2883</pages>
      <abstract>Political authorities in democratic countries regularly consult the public in order to allow citizens to voice their ideas and concerns on specific issues. When trying to evaluate the (often large number of) contributions by the public in order to inform decision-making, authorities regularly face challenges due to restricted resources. We identify several tasks whose automated support can help in the evaluation of public participation. These are i) the recognition of arguments, more precisely premises and their conclusions, ii) the assessment of the concreteness of arguments, iii) the detection of textual descriptions of locations in order to assign citizens’ ideas to a spatial location, and iv) the thematic categorization of contributions. To enable future research efforts to develop techniques addressing these four tasks, we introduce the CIMT PartEval Corpus, a new publicly-available German-language corpus that includes several thousand citizen contributions from six mobility-related planning processes in five German municipalities. The corpus provides annotations for each of these tasks which have not been available in German for the domain of public participation before either at all or in this scope and variety.</abstract>
      <url hash="852ffca6">2022.lrec-1.308</url>
      <bibkey>romberg-etal-2022-corpus</bibkey>
      <pwccode url="https://github.com/juliaromberg/cimt-geographic-location-dataset" additional="true">juliaromberg/cimt-geographic-location-dataset</pwccode>
    </paper>
    <paper id="309">
      <title>Overlooked Data in Typological Databases: What Grambank Teaches Us About Gaps in Grammars</title>
      <author><first>Jakob</first><last>Lesage</last></author>
      <author><first>Hannah J.</first><last>Haynie</last></author>
      <author><first>Hedvig</first><last>Skirgård</last></author>
      <author><first>Tobias</first><last>Weber</last></author>
      <author><first>Alena</first><last>Witzlack-Makarevich</last></author>
      <pages>2884–2890</pages>
      <abstract>Typological databases can contain a wealth of information beyond the collection of linguistic properties across languages. This paper shows how information often overlooked in typological databases can inform the research community about the state of description of the world’s languages. We illustrate this using Grambank, a morphosyntactic typological database covering 2,467 language varieties and based on 3,951 grammatical descriptions. We classify and quantify the comments that accompany coded values in Grambank. We then aggregate these comments and the coded values to derive a level of description for 17 grammatical domains that Grambank covers (negation, adnominal modification, participant marking, tense, aspect, etc.). We show that the description level of grammatical domains varies across space and time. Information about gaps and uncertainties in the descriptive knowledge of grammatical domains within and across languages is essential for a correct analysis of data in typological databases and for the study of grammatical diversity more generally. When collected in a database, such information feeds into disciplines that focus on primary data collection, such as grammaticography and language documentation.</abstract>
      <url hash="87ebc21e">2022.lrec-1.309</url>
      <bibkey>lesage-etal-2022-overlooked</bibkey>
    </paper>
    <paper id="310">
      <title><fixed-case>H</fixed-case>ong <fixed-case>K</fixed-case>ong: Longitudinal and Synchronic Characterisations of Protest News between 1998 and 2020</title>
      <author><first>Arya D.</first><last>McCarthy</last></author>
      <author><first>Giovanna Maria Dora</first><last>Dore</last></author>
      <pages>2891–2900</pages>
      <abstract>This paper showcases the utility and timeliness of the Hong Kong Protest News Dataset, a highly curated collection of news articles from diverse news sources, to investigate longitudinal and synchronic news characterisations of protests in Hong Kong between 1998 and 2020. The properties of the dataset enable us to apply natural language processing to its 4522 articles and thereby study patterns of journalistic practice across newspapers. This paper sheds light on whether depth and/or manner of reporting changed over time, and if so, in what ways, or in response to what. In its focus and methodology, this paper helps bridge the gap between “validity-focused methodological debates” and the use of computational methods of analysis in the social sciences.</abstract>
      <url hash="358b17ae">2022.lrec-1.310</url>
      <bibkey>mccarthy-dore-2022-hong</bibkey>
    </paper>
    <paper id="311">
      <title>Nunc profana tractemus. Detecting Code-Switching in a Large Corpus of 16th Century Letters</title>
      <author><first>Martin</first><last>Volk</last></author>
      <author><first>Lukas</first><last>Fischer</last></author>
      <author><first>Patricia</first><last>Scheurer</last></author>
      <author><first>Bernard Silvan</first><last>Schroffenegger</last></author>
      <author><first>Raphael</first><last>Schwitter</last></author>
      <author><first>Phillip</first><last>Ströbel</last></author>
      <author><first>Benjamin</first><last>Suter</last></author>
      <pages>2901–2908</pages>
      <abstract>This paper is based on a collection of 16th century letters from and to the Zurich reformer Heinrich Bullinger. Around 12,000 letters of this exchange have been preserved, out of which 3100 have been professionally edited, and another 5500 are available as provisional transcriptions. We have investigated code-switching in these 8600 letters, first on the sentence-level and then on the word-level. In this paper we give an overview of the corpus and its language mix (mostly Early New High German and Latin, but also French, Greek, Italian and Hebrew). We report on our experiences with a popular language identifier and present our results when training an alternative identifier on a very small training corpus of only 150 sentences per language. We use the automatically labeled sentences in order to bootstrap a word-based language classifier which works with high accuracy. Our research around the corpus building and annotation involves automatic handwritten text recognition, text normalisation for ENH German, and machine translation from medieval Latin into modern German.</abstract>
      <url hash="6c569eb8">2022.lrec-1.311</url>
      <bibkey>volk-etal-2022-nunc</bibkey>
    </paper>
    <paper id="312">
      <title>Quality and Efficiency of Manual Annotation: Pre-annotation Bias</title>
      <author><first>Marie</first><last>Mikulová</last></author>
      <author><first>Milan</first><last>Straka</last></author>
      <author><first>Jan</first><last>Štěpánek</last></author>
      <author><first>Barbora</first><last>Štěpánková</last></author>
      <author><first>Jan</first><last>Hajic</last></author>
      <pages>2909–2918</pages>
      <abstract>This paper presents an analysis of annotation using an automatic pre-annotation for a mid-level annotation complexity task - dependency syntax annotation. It compares the annotation efforts made by annotators using a pre-annotated version (with a high-accuracy parser) and those made by fully manual annotation. The aim of the experiment is to judge the final annotation quality when pre-annotation is used. In addition, it evaluates the effect of automatic linguistically-based (rule-formulated) checks and another annotation on the same data available to the annotators, and their influence on annotation quality and efficiency. The experiment confirmed that the pre-annotation is an efficient tool for faster manual syntactic annotation which increases the consistency of the resulting annotation without reducing its quality.</abstract>
      <url hash="f0741319">2022.lrec-1.312</url>
      <bibkey>mikulova-etal-2022-quality</bibkey>
    </paper>
    <paper id="313">
      <title>A Comprehensive Evaluation and Correction of the <fixed-case>T</fixed-case>ime<fixed-case>B</fixed-case>ank Corpus</title>
      <author><first>Mustafa</first><last>Ocal</last></author>
      <author><first>Antonela</first><last>Radas</last></author>
      <author><first>Jared</first><last>Hummer</last></author>
      <author><first>Karine</first><last>Megerdoomian</last></author>
      <author><first>Mark</first><last>Finlayson</last></author>
      <pages>2919–2927</pages>
      <abstract>TimeML is an annotation scheme for capturing temporal information in text. The developers of TimeML built the TimeBank corpus to both validate the scheme and provide a rich dataset of events, temporal expressions, and temporal relationships for training and testing temporal analysis systems. In our own work we have been developing methods aimed at TimeML graphs for detecting (and eventually automatically correcting) temporal inconsistencies, extracting timelines, and assessing temporal indeterminacy. In the course of this investigation we identified numerous previously unrecognized issues in the TimeBank corpus, including multiple violations of TimeML annotation guide rules, incorrectly disconnected temporal graphs, as well as inconsistent, redundant, missing, or otherwise incorrect annotations. We describe our methods for detecting and correcting these problems, which include: (a) automatic guideline checking (109 violations); (b) automatic inconsistency checking (65 inconsistent files); (c) automatic disconnectivity checking (625 incorrect breakpoints); and (d) manual comparison with the output of state-of-the-art automatic annotators to identify missing annotations (317 events, 52 temporal expressions). We provide our code as well as a set of patch files that can be applied to the TimeBank corpus to produce a corrected version for use by other researchers in the field.</abstract>
      <url hash="d59c52a8">2022.lrec-1.313</url>
      <bibkey>ocal-etal-2022-comprehensive</bibkey>
    </paper>
    <paper id="314">
      <title>Evaluating Multilingual Sentence Representation Models in a Real Case Scenario</title>
      <author><first>Rocco</first><last>Tripodi</last></author>
      <author><first>Rexhina</first><last>Blloshmi</last></author>
      <author><first>Simon</first><last>Levis Sullam</last></author>
      <pages>2928–2939</pages>
      <abstract>In this paper, we present an evaluation of sentence representation models on the paraphrase detection task. The evaluation is designed to simulate a real-world problem of plagiarism and is based on one of the most important cases of forgery in modern history: the so-called “Protocols of the Elders of Zion”. The sentence pairs for the evaluation are taken from the infamous forged text “Protocols of the Elders of Zion” (Protocols) by unknown authors; and by “Dialogue in Hell between Machiavelli and Montesquieu” by Maurice Joly. Scholars have demonstrated that the first text plagiarizes from the second, indicating all the forged parts on qualitative grounds. Following this evidence, we organized the rephrased texts and asked native speakers to quantify the level of similarity between each pair. We used this material to evaluate sentence representation models in two languages: English and French, and on three tasks: similarity correlation, paraphrase identification, and paraphrase retrieval. Our evaluation aims at encouraging the development of benchmarks based on real-world problems, as a means to prevent problems connected to AI hypes, and to use NLP technologies for social good. Through our evaluation, we are able to confirm that the infamous Protocols are actually a plagiarized text but, as we will show, we encounter several problems connected with the convoluted nature of the task, that is very different from the one reported in standard benchmarks of paraphrase detection and sentence similarity. Code and data available at https://github.com/roccotrip/protocols.</abstract>
      <url hash="cfe41977">2022.lrec-1.314</url>
      <bibkey>tripodi-etal-2022-evaluating</bibkey>
      <pwccode url="https://github.com/roccotrip/protocols" additional="false">roccotrip/protocols</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paws">PAWS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paws-x">PAWS-X</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sick">SICK</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="315">
      <title>Validity, Agreement, Consensuality and Annotated Data Quality</title>
      <author><first>Anaëlle</first><last>Baledent</last></author>
      <author><first>Yann</first><last>Mathet</last></author>
      <author><first>Antoine</first><last>Widlöcher</last></author>
      <author><first>Christophe</first><last>Couronne</last></author>
      <author><first>Jean-Luc</first><last>Manguin</last></author>
      <pages>2940–2948</pages>
      <abstract>Reference annotated (or gold-standard) datasets are required for various common tasks such as training for machine learning systems or system validation. They are necessary to analyse or compare occurrences or items annotated by experts, or to compare objects resulting from any computational process to objects annotated by experts. But, even if reference annotated gold-standard corpora are required, their production is known as a difficult problem, from both a theoretical and practical point of view. Many studies devoted to theses issues conclude that multi-annotation is most of the time a necessity. That inter-annotator agreement measure, which is required to check the reliability of data and the reproducibility of an annotation task, and thus to establish a gold standard, is another thorny problem. Fine analysis of available metrics for this specific task then becomes essential. Our work is part of this effort and more precisely focuses on several problems, which are rarely discussed, although they are intrinsically linked with the interpretation of metrics. In particular, we focus here on the complex relations between agreement and reference (of which agreement among annotators is supposed to be an indicator), and the emergence of consensus. We also introduce the notion of consensuality as another relevant indicator.</abstract>
      <url hash="c78c5800">2022.lrec-1.315</url>
      <bibkey>baledent-etal-2022-validity</bibkey>
    </paper>
    <paper id="316">
      <title>Impact Analysis of the Use of Speech and Language Models Pretrained by Self-Supersivion for Spoken Language Understanding</title>
      <author><first>Salima</first><last>Mdhaffar</last></author>
      <author><first>Valentin</first><last>Pelloin</last></author>
      <author><first>Antoine</first><last>Caubrière</last></author>
      <author><first>Gaëlle</first><last>Laperriere</last></author>
      <author><first>Sahar</first><last>Ghannay</last></author>
      <author><first>Bassam</first><last>Jabaian</last></author>
      <author><first>Nathalie</first><last>Camelin</last></author>
      <author><first>Yannick</first><last>Estève</last></author>
      <pages>2949–2956</pages>
      <abstract>Pretrained models through self-supervised learning have been recently introduced for both acoustic and language modeling. Applied to spoken language understanding tasks, these models have shown their great potential by improving the state-of-the-art performances on challenging benchmark datasets. In this paper, we present an error analysis reached by the use of such models on the French MEDIA benchmark dataset, known as being one of the most challenging benchmarks for the slot filling task among all the benchmarks accessible to the entire research community. One year ago, the state-of-art system reached a Concept Error Rate (CER) of 13.6% through the use of a end-to-end neural architecture. Some months later, a cascade approach based on the sequential use of a fine-tuned wav2vec2.0 model and a fine-tuned BERT model reaches a CER of 11.2%. This significant improvement raises questions about the type of errors that remain difficult to treat, but also about those that have been corrected using these models pre-trained through self-supervision learning on a large amount of data. This study brings some answers in order to better understand the limits of such models and open new perspectives to continue improving the performance.</abstract>
      <url hash="c796f640">2022.lrec-1.316</url>
      <bibkey>mdhaffar-etal-2022-impact</bibkey>
    </paper>
    <paper id="317">
      <title><fixed-case>JGLUE</fixed-case>: <fixed-case>J</fixed-case>apanese General Language Understanding Evaluation</title>
      <author><first>Kentaro</first><last>Kurihara</last></author>
      <author><first>Daisuke</first><last>Kawahara</last></author>
      <author><first>Tomohide</first><last>Shibata</last></author>
      <pages>2957–2966</pages>
      <abstract>To develop high-performance natural language understanding (NLU) models, it is necessary to have a benchmark to evaluate and analyze NLU ability from various perspectives. While the English NLU benchmark, GLUE, has been the forerunner, benchmarks are now being released for languages other than English, such as CLUE for Chinese and FLUE for French; but there is no such benchmark for Japanese. We build a Japanese NLU benchmark, JGLUE, from scratch without translation to measure the general NLU ability in Japanese. We hope that JGLUE will facilitate NLU research in Japanese.</abstract>
      <url hash="179d34c3">2022.lrec-1.317</url>
      <bibkey>kurihara-etal-2022-jglue</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/marc-ja">JGLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/clue">CLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/commonsenseqa">CommonsenseQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/flue-french-language-understanding-evaluation">FLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/klue">KLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sick">SICK</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="318">
      <title>Using the <fixed-case>LARA</fixed-case> Little Prince to compare human and <fixed-case>TTS</fixed-case> audio quality</title>
      <author><first>Elham</first><last>Akhlaghi</last></author>
      <author><first>Ingibjörg Iða</first><last>Auðunardóttir</last></author>
      <author><first>Anna</first><last>Bączkowska</last></author>
      <author><first>Branislav</first><last>Bédi</last></author>
      <author><first>Hakeem</first><last>Beedar</last></author>
      <author><first>Harald</first><last>Berthelsen</last></author>
      <author><first>Cathy</first><last>Chua</last></author>
      <author><first>Catia</first><last>Cucchiarin</last></author>
      <author><first>Hanieh</first><last>Habibi</last></author>
      <author><first>Ivana</first><last>Horváthová</last></author>
      <author><first>Junta</first><last>Ikeda</last></author>
      <author><first>Christèle</first><last>Maizonniaux</last></author>
      <author><first>Neasa</first><last>Ní Chiaráin</last></author>
      <author><first>Chadi</first><last>Raheb</last></author>
      <author><first>Manny</first><last>Rayner</last></author>
      <author><first>John</first><last>Sloan</last></author>
      <author><first>Nikos</first><last>Tsourakis</last></author>
      <author><first>Chunlin</first><last>Yao</last></author>
      <pages>2967–2975</pages>
      <abstract>A popular idea in Computer Assisted Language Learning (CALL) is to use multimodal annotated texts, with annotations typically including embedded audio and translations, to support L2 learning through reading. An important question is how to create good quality audio, which can be done either through human recording or by a Text-To-Speech (TTS) engine. We may reasonably expect TTS to be quicker and easier, but human to be of higher quality. Here, we report a study using the open source LARA platform and ten languages. Samples of audio totalling about five minutes, representing the same four passages taken from LARA versions of Saint-Exupèry’s “Le petit prince”, were provided for each language in both human and TTS form; the passages were chosen to instantiate the 2x2 cross product of the conditions dialogue, not-dialogue and humour, not-humour. 251 subjects used a web form to compare human and TTS versions of each item and rate the voices as a whole. For the three languages where TTS did best, English, French and Irish, the evidence from this study and the previous one it extended suggest that TTS audio is now pedagogically adequate and roughly comparable with a non-professional human voice in terms of exemplifying correct pronunciation and prosody. It was however still judged substantially less natural and less pleasant to listen to. No clear evidence was found to support the hypothesis that dialogue and humour pose special problems for TTS. All data and software will be made freely available.</abstract>
      <url hash="87f087d3">2022.lrec-1.318</url>
      <bibkey>akhlaghi-etal-2022-using</bibkey>
    </paper>
    <paper id="319">
      <title>Cyberbullying Classifiers are Sensitive to Model-Agnostic Perturbations</title>
      <author><first>Chris</first><last>Emmery</last></author>
      <author><first>Ákos</first><last>Kádár</last></author>
      <author><first>Grzegorz</first><last>Chrupała</last></author>
      <author><first>Walter</first><last>Daelemans</last></author>
      <pages>2976–2988</pages>
      <abstract>A limited amount of studies investigates the role of model-agnostic adversarial behavior in toxic content classification. As toxicity classifiers predominantly rely on lexical cues, (deliberately) creative and evolving language-use can be detrimental to the utility of current corpora and state-of-the-art models when they are deployed for content moderation. The less training data is available, the more vulnerable models might become. This study is, to our knowledge, the first to investigate the effect of adversarial behavior and augmentation for cyberbullying detection. We demonstrate that model-agnostic lexical substitutions significantly hurt classifier performance. Moreover, when these perturbed samples are used for augmentation, we show models become robust against word-level perturbations at a slight trade-off in overall task performance. Augmentations proposed in prior work on toxicity prove to be less effective. Our results underline the need for such evaluations in online harm areas with small corpora.</abstract>
      <url hash="185bdad4">2022.lrec-1.319</url>
      <bibkey>emmery-etal-2022-cyberbullying</bibkey>
      <pwccode url="https://github.com/cmry/augtox" additional="false">cmry/augtox</pwccode>
    </paper>
    <paper id="320">
      <title>Constructing Distributions of Variation in Referring Expression Type from Corpora for Model Evaluation</title>
      <author><first>T. Mark</first><last>Ellison</last></author>
      <author><first>Fahime</first><last>Same</last></author>
      <pages>2989–2997</pages>
      <abstract>The generation of referring expressions (REs) is a non-deterministic task. However, the algorithms for the generation of REs are standardly evaluated against corpora of written texts which include only one RE per each reference. Our goal in this work is firstly to reproduce one of the few studies taking the distributional nature of the RE generation into account. We add to this work, by introducing a method for exploring variation in human RE choice on the basis of longitudinal corpora - substantial corpora with a single human judgement (in the process of composition) per RE. We focus on the prediction of RE types, proper name, description and pronoun. We compare evaluations made against distributions over these types with evaluations made against parallel human judgements. Our results show agreement in the evaluation of learning algorithms against distributions constructed from parallel human evaluations and from longitudinal data.</abstract>
      <url hash="cb63aa17">2022.lrec-1.320</url>
      <bibkey>ellison-same-2022-constructing</bibkey>
    </paper>
    <paper id="321">
      <title>Knowledge Graph Question Answering Leaderboard: A Community Resource to Prevent a Replication Crisis</title>
      <author><first>Aleksandr</first><last>Perevalov</last></author>
      <author><first>Xi</first><last>Yan</last></author>
      <author><first>Liubov</first><last>Kovriguina</last></author>
      <author><first>Longquan</first><last>Jiang</last></author>
      <author><first>Andreas</first><last>Both</last></author>
      <author><first>Ricardo</first><last>Usbeck</last></author>
      <pages>2998–3007</pages>
      <abstract>Data-driven systems need to be evaluated to establish trust in the scientific approach and its applicability. In particular, this is true for Knowledge Graph (KG) Question Answering (QA), where complex data structures are made accessible via natural-language interfaces. Evaluating the capabilities of these systems has been a driver for the community for more than ten years while establishing different KGQA benchmark datasets. However, comparing different approaches is cumbersome. The lack of existing and curated leaderboards leads to a missing global view over the research field and could inject mistrust into the results. In particular, the latest and most-used datasets in the KGQA community, LC-QuAD and QALD, miss providing central and up-to-date points of trust. In this paper, we survey and analyze a wide range of evaluation results with significant coverage of 100 publications and 98 systems from the last decade. We provide a new central and open leaderboard for any KGQA benchmark dataset as a focal point for the community - https://kgqa.github.io/leaderboard/. Our analysis highlights existing problems during the evaluation of KGQA systems. Thus, we will point to possible improvements for future evaluations.</abstract>
      <url hash="08e53b27">2022.lrec-1.321</url>
      <bibkey>perevalov-etal-2022-knowledge</bibkey>
      <pwccode url="https://github.com/kgqa/leaderboard" additional="false">kgqa/leaderboard</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/webquestions">WebQuestions</pwcdataset>
    </paper>
    <paper id="322">
      <title>Multi-Task Learning for Cross-Lingual Abstractive Summarization</title>
      <author><first>Sho</first><last>Takase</last></author>
      <author><first>Naoaki</first><last>Okazaki</last></author>
      <pages>3008–3016</pages>
      <abstract>We present a multi-task learning framework for cross-lingual abstractive summarization to augment training data. Recent studies constructed pseudo cross-lingual abstractive summarization data to train their neural encoder-decoders. Meanwhile, we introduce existing genuine data such as translation pairs and monolingual abstractive summarization data into training. Our proposed method, Transum, attaches a special token to the beginning of the input sentence to indicate the target task. The special token enables us to incorporate the genuine data into the training data easily. The experimental results show that Transum achieves better performance than the model trained with only pseudo cross-lingual summarization data. In addition, we achieve the top ROUGE score on Chinese-English and Arabic-English abstractive summarization. Moreover, Transum also has a positive effect on machine translation. Experimental results indicate that Transum improves the performance from the strong baseline, Transformer, in Chinese-English, Arabic-English, and English-Japanese translation datasets.</abstract>
      <url hash="2af34ced">2022.lrec-1.322</url>
      <bibkey>takase-okazaki-2022-multi</bibkey>
    </paper>
    <paper id="323">
      <title>How Much Context Span is Enough? Examining Context-Related Issues for Document-level <fixed-case>MT</fixed-case></title>
      <author><first>Sheila</first><last>Castilho</last></author>
      <pages>3017–3025</pages>
      <abstract>This paper analyses how much context span is necessary to solve different context-related issues, namely, reference, ellipsis, gender, number, lexical ambiguity, and terminology when translating from English into Portuguese. We use the DELA corpus, which consists of 60 documents and six different domains (subtitles, literary, news, reviews, medical, and legislation). We find that the shortest context span to disambiguate issues can appear in different positions in the document including preceding, following, global, world knowledge. Moreover, the average length depends on the issue types as well as the domain. Moreover, we show that the standard approach of relying on only two preceding sentences as context might not be enough depending on the domain and issue types.</abstract>
      <url hash="ddad9fef">2022.lrec-1.323</url>
      <bibkey>castilho-2022-much</bibkey>
    </paper>
    <paper id="324">
      <title><fixed-case>TANDO</fixed-case>: A Corpus for Document-level Machine Translation</title>
      <author><first>Harritxu</first><last>Gete</last></author>
      <author><first>Thierry</first><last>Etchegoyhen</last></author>
      <author><first>David</first><last>Ponce</last></author>
      <author><first>Gorka</first><last>Labaka</last></author>
      <author><first>Nora</first><last>Aranberri</last></author>
      <author><first>Ander</first><last>Corral</last></author>
      <author><first>Xabier</first><last>Saralegi</last></author>
      <author><first>Igor</first><last>Ellakuria</last></author>
      <author><first>Maite</first><last>Martin</last></author>
      <pages>3026–3037</pages>
      <abstract>Document-level Neural Machine Translation aims to increase the quality of neural translation models by taking into account contextual information. Properly modelling information beyond the sentence level can result in improved machine translation output in terms of coherence, cohesion and consistency. Suitable corpora for context-level modelling are necessary to both train and evaluate context-aware systems, but are still relatively scarce. In this work we describe TANDO, a document-level corpus for the under-resourced Basque-Spanish language pair, which we share with the scientific community. The corpus is composed of parallel data from three different domains and has been prepared with context-level information. Additionally, the corpus includes contrastive test sets for fine-grained evaluations of gender and register contextual phenomena on both source and target language sides. To establish the usefulness of the corpus, we trained and evaluated baseline Transformer models and context-aware variants based on context concatenation. Our results indicate that the corpus is suitable for fine-grained evaluation of document-level machine translation systems.</abstract>
      <url hash="4cc550ac">2022.lrec-1.324</url>
      <bibkey>gete-etal-2022-tando</bibkey>
      <pwccode url="https://github.com/vicomtech/tando" additional="false">vicomtech/tando</pwccode>
    </paper>
    <paper id="325">
      <title>Unsupervised Machine Translation in Real-World Scenarios</title>
      <author><first>Ona</first><last>de Gibert Bonet</last></author>
      <author><first>Iakes</first><last>Goenaga</last></author>
      <author><first>Jordi</first><last>Armengol-Estapé</last></author>
      <author><first>Olatz</first><last>Perez-de-Viñaspre</last></author>
      <author><first>Carla</first><last>Parra Escartín</last></author>
      <author><first>Marina</first><last>Sanchez</last></author>
      <author><first>Mārcis</first><last>Pinnis</last></author>
      <author><first>Gorka</first><last>Labaka</last></author>
      <author><first>Maite</first><last>Melero</last></author>
      <pages>3038–3047</pages>
      <abstract>In this work, we present the work that has been carried on in the MT4All CEF project and the resources that it has generated by leveraging recent research carried out in the field of unsupervised learning. In the course of the project 18 monolingual corpora for specific domains and languages have been collected, and 12 bilingual dictionaries and translation models have been generated. As part of the research, the unsupervised MT methodology based only on monolingual corpora (Artetxe et al., 2017) has been tested on a variety of languages and domains. Results show that in specialised domains, when there is enough monolingual in-domain data, unsupervised results are comparable to those of general domain supervised translation, and that, at any rate, unsupervised techniques can be used to boost results whenever very little data is available.</abstract>
      <url hash="c04bb371">2022.lrec-1.325</url>
      <bibkey>de-gibert-bonet-etal-2022-unsupervised</bibkey>
    </paper>
    <paper id="326">
      <title><fixed-case>COVID</fixed-case>-19 Mythbusters in World Languages</title>
      <author><first>Mana</first><last>Ashida</last></author>
      <author><first>Jin-Dong</first><last>Kim</last></author>
      <author><first>Seunghun</first><last>Lee</last></author>
      <pages>3048–3055</pages>
      <abstract>This paper introduces a multi-lingual database containing translated texts of COVID-19 mythbusters. The database has translations into 115 languages as well as the original English texts, of which the original texts are published by World Health Organization (WHO). This paper then presents preliminary analyses on latin-alphabet-based texts to see the potential of the database as a resource for multilingual linguistic analyses. The analyses on latin-alphabet-based texts gave interesting insights into the resource. While the amount of translated texts in each language was small, character bi-grams with normalization (lowercasing and removal of diacritics) was turned out to be an effective proxy for measuring the similarity of the languages, and the affinity ranking of language pairs could be obtained. Additionally, the hierarchical clustering analysis is performed using the character bigram overlap ratio of every possible pair of languages. The result shows the cluster of Germanic languages, Romance languages, and Southern Bantu languages. In sum, the multilingual database not only offers fixed set of materials in numerous languages, but also serves as a preliminary tool to identify the language family using text-based similarity measure of bigram overlap ratio.</abstract>
      <url hash="ef5dd5b3">2022.lrec-1.326</url>
      <bibkey>ashida-etal-2022-covid</bibkey>
    </paper>
    <paper id="327">
      <title>On the Multilingual Capabilities of Very Large-Scale <fixed-case>E</fixed-case>nglish Language Models</title>
      <author><first>Jordi</first><last>Armengol-Estapé</last></author>
      <author><first>Ona</first><last>de Gibert Bonet</last></author>
      <author><first>Maite</first><last>Melero</last></author>
      <pages>3056–3068</pages>
      <abstract>Generative Pre-trained Transformers (GPTs) have recently been scaled to unprecedented sizes in the history of machine learning. These models, solely trained on the language modeling objective, have been shown to exhibit outstanding zero, one, and few-shot learning capabilities in a number of different tasks. Nevertheless, aside from anecdotal experiences, little is known regarding their multilingual capabilities, given the fact that the pre-training corpus is almost entirely composed of English text. In this work, we investigate its potential and limits in three tasks: extractive question-answering, text summarization and natural language generation for five different languages, as well as the effect of scale in terms of model size. Our results show that GPT-3 can be almost as useful for many languages as it is for English, with room for improvement if optimization of the tokenization is addressed.</abstract>
      <url hash="c04a04e6">2022.lrec-1.327</url>
      <bibkey>armengol-estape-etal-2022-multilingual</bibkey>
      <pwccode url="https://github.com/temu-bsc/gpt3-queries" additional="true">temu-bsc/gpt3-queries</pwccode>
    </paper>
    <paper id="328">
      <title>Evaluating Subtitle Segmentation for End-to-end Generation Systems</title>
      <author><first>Alina</first><last>Karakanta</last></author>
      <author><first>François</first><last>Buet</last></author>
      <author><first>Mauro</first><last>Cettolo</last></author>
      <author><first>François</first><last>Yvon</last></author>
      <pages>3069–3078</pages>
      <abstract>Subtitles appear on screen as short pieces of text, segmented based on formal constraints (length) and syntactic/semantic criteria. Subtitle segmentation can be evaluated with sequence segmentation metrics against a human reference. However, standard segmentation metrics cannot be applied when systems generate outputs different than the reference, e.g. with end-to-end subtitling systems. In this paper, we study ways to conduct reference-based evaluations of segmentation accuracy irrespective of the textual content. We first conduct a systematic analysis of existing metrics for evaluating subtitle segmentation. We then introduce Sigma, a Subtitle Segmentation Score derived from an approximate upper-bound of BLEU on segmentation boundaries, which allows us to disentangle the effect of good segmentation from text quality. To compare Sigma with existing metrics, we further propose a boundary projection method from imperfect hypotheses to the true reference. Results show that all metrics are able to reward high quality output but for similar outputs system ranking depends on each metric’s sensitivity to error type. Our thorough analyses suggest Sigma is a promising segmentation candidate but its reliability over other segmentation metrics remains to be validated through correlations with human judgements.</abstract>
      <url hash="42160ec8">2022.lrec-1.328</url>
      <bibkey>karakanta-etal-2022-evaluating</bibkey>
      <pwccode url="https://github.com/fyvo/evalsubtitle" additional="false">fyvo/evalsubtitle</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/must-cinema">MuST-Cinema</pwcdataset>
    </paper>
    <paper id="329">
      <title>Using Semantic Role Labeling to Improve Neural Machine Translation</title>
      <author><first>Reinhard</first><last>Rapp</last></author>
      <pages>3079–3083</pages>
      <abstract>Despite impressive progress in machine translation in recent years, it has occasionally been argued that current systems are still mainly based on pattern recognition and that further progress may be possible by using text understanding techniques, thereby e.g. looking at semantics of the type “Who is doing what to whom?”. In the current research we aim to take a small step into this direction. Assuming that semantic role labeling (SRL) grasps some of the relevant semantics, we automatically annotate the source language side of a standard parallel corpus, namely Europarl, with semantic roles. We then train a neural machine translation (NMT) system using the annotated corpus on the source language side, and the original unannotated corpus on the target language side. New text to be translated is first annotated by the same SRL system and then fed into the translation system. We compare the results to those of a baseline NMT system trained with unannotated text on both sides and find that the SRL-based system yields small improvements in terms of BLEU scores for each of the four language pairs under investigation, involving English, French, German, Greek and Spanish.</abstract>
      <url hash="4471a710">2022.lrec-1.329</url>
      <bibkey>rapp-2022-using</bibkey>
    </paper>
    <paper id="330">
      <title>A Deep Transfer Learning Method for Cross-Lingual Natural Language Inference</title>
      <author><first>Dibyanayan</first><last>Bandyopadhyay</last></author>
      <author><first>Arkadipta</first><last>De</last></author>
      <author><first>Baban</first><last>Gain</last></author>
      <author><first>Tanik</first><last>Saikh</last></author>
      <author><first>Asif</first><last>Ekbal</last></author>
      <pages>3084–3092</pages>
      <abstract>Natural Language Inference (NLI), also known as Recognizing Textual Entailment (RTE), has been one of the central tasks in Artificial Intelligence (AI) and Natural Language Processing (NLP). RTE between the two pieces of texts is a crucial problem, and it adds further challenges when involving two different languages, i.e., in the cross-lingual scenario. This paper proposes an effective transfer learning approach for cross-lingual NLI. We perform experiments on English-Hindi language pairs in the cross-lingual setting to find out that our novel loss formulation could enhance the performance of the baseline model by up to 2%. To assess the effectiveness of our method further, we perform additional experiments on every possible language pair using four European languages, namely French, German, Bulgarian, and Turkish, on top of XNLI dataset. Evaluation results yield up to 10% performance improvement over the respective baseline models, in some cases surpassing the state-of-the-art (SOTA). It is also to be noted that our proposed model has 110M parameters which is much lesser than the SOTA model having 220M parameters. Finally, we argue that our transfer learning-based loss objective is model agnostic and thus can be used with other deep learning-based architectures for cross-lingual NLI.</abstract>
      <url hash="116b7d1d">2022.lrec-1.330</url>
      <bibkey>bandyopadhyay-etal-2022-deep</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
    </paper>
    <paper id="331">
      <title>Simple <fixed-case>TICO</fixed-case>-19: A Dataset for Joint Translation and Simplification of <fixed-case>COVID</fixed-case>-19 Texts</title>
      <author><first>Matthew</first><last>Shardlow</last></author>
      <author><first>Fernando</first><last>Alva-Manchego</last></author>
      <pages>3093–3102</pages>
      <abstract>Specialist high-quality information is typically first available in English, and it is written in a language that may be difficult to understand by most readers. While Machine Translation technologies contribute to mitigate the first issue, the translated content will most likely still contain complex language. In order to investigate and address both problems simultaneously, we introduce Simple TICO-19, a new language resource containing manual simplifications of the English and Spanish portions of the TICO-19 corpus for Machine Translation of COVID-19 literature. We provide an in-depth description of the annotation process, which entailed designing an annotation manual and employing four annotators (two native English speakers and two native Spanish speakers) who simplified over 6,000 sentences from the English and Spanish portions of the TICO-19 corpus. We report several statistics on the new dataset, focusing on analysing the improvements in readability from the original texts to their simplified versions. In addition, we propose baseline methodologies for automatically generating the simplifications, translations and joint translation and simplifications contained in our dataset.</abstract>
      <url hash="fbb6e489">2022.lrec-1.331</url>
      <bibkey>shardlow-alva-manchego-2022-simple</bibkey>
    </paper>
    <paper id="332">
      <title>Building Comparable Corpora for Assessing Multi-Word Term Alignment</title>
      <author><first>Omar</first><last>Adjali</last></author>
      <author><first>Emmanuel</first><last>Morin</last></author>
      <author><first>Pierre</first><last>Zweigenbaum</last></author>
      <pages>3103–3112</pages>
      <abstract>Recent work has demonstrated the importance of dealing with Multi-Word Terms (MWTs) in several Natural Language Processing applications. In particular, MWTs pose serious challenges for alignment and machine translation systems because of their syntactic and semantic properties. Thus, developing algorithms that handle MWTs is becoming essential for many NLP tasks. However, the availability of bilingual and more generally multi-lingual resources is limited, especially for low-resourced languages and in specialized domains. In this paper, we propose an approach for building comparable corpora and bilingual term dictionaries that help evaluate bilingual term alignment in comparable corpora. To that aim, we exploit parallel corpora to perform automatic bilingual MWT extraction and comparable corpus construction. Parallel information helps to align bilingual MWTs and makes it easier to build comparable specialized sub-corpora. Experimental validation on an existing dataset and on manually annotated data shows the interest of the proposed methodology.</abstract>
      <url hash="d8fcc8fc">2022.lrec-1.332</url>
      <bibkey>adjali-etal-2022-building</bibkey>
    </paper>
    <paper id="333">
      <title>Mean Machine Translations: On Gender Bias in <fixed-case>I</fixed-case>celandic Machine Translations</title>
      <author><first>Agnes</first><last>Sólmundsdóttir</last></author>
      <author><first>Dagbjört</first><last>Guðmundsdóttir</last></author>
      <author><first>Lilja Björk</first><last>Stefánsdóttir</last></author>
      <author><first>Anton</first><last>Ingason</last></author>
      <pages>3113–3121</pages>
      <abstract>This paper examines machine bias in language technology. Machine bias can affect machine learning algorithms when language models trained on large corpora include biased human decisions or reflect historical or social inequities, e.g. regarding gender and race. The focus of the paper is on gender bias in machine translation and we discuss a study conducted on Icelandic translations in the translation systems Google Translate and Vélþýðing.is. The results show a pattern which corresponds to certain societal ideas about gender. For example it seems to depend on the meaning of adjectives referring to people whether they appear in the masculine or feminine form. Adjectives describing positive personality traits were more likely to appear in masculine gender whereas the negative ones frequently appear in feminine gender. However, the opposite applied to appearance related adjectives. These findings unequivocally demonstrate the importance of being vigilant towards technology so as not to maintain societal inequalities and outdated views — especially in today’s digital world.</abstract>
      <url hash="7a873ba6">2022.lrec-1.333</url>
      <bibkey>solmundsdottir-etal-2022-mean</bibkey>
    </paper>
    <paper id="334">
      <title>An Analysis of Dialogue Act Sequence Similarity Across Multiple Domains</title>
      <author><first>Ayesha</first><last>Enayet</last></author>
      <author><first>Gita</first><last>Sukthankar</last></author>
      <pages>3122–3130</pages>
      <abstract>This paper presents an analysis of how dialogue act sequences vary across different datasets in order to anticipate the potential degradation in the performance of learned models during domain adaptation. We hypothesize the following: 1) dialogue sequences from related domains will exhibit similar n-gram frequency distributions 2) this similarity can be expressed by measuring the average Hamming distance between subsequences drawn from different datasets. Our experiments confirm that when dialogue acts sequences from two datasets are dissimilar they lie further away in embedding space, making it possible to train a classifier to discriminate between them even when the datasets are corrupted with noise. We present results from eight different datasets: SwDA, AMI (DialSum), GitHub, Hate Speech, Teams, Diplomacy Betrayal, SAMsum, and Military (Army). Our datasets were collected from many types of human communication including strategic planning, informal discussion, and social media exchanges. Our methodology provides intuition on the generalizability of dialogue models trained on different datasets. Based on our analysis, it is problematic to assume that machine learning models trained on one type of discourse will generalize well to other settings, due to contextual differences.</abstract>
      <url hash="73315e24">2022.lrec-1.334</url>
      <bibkey>enayet-sukthankar-2022-analysis</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/samsum-corpus">SAMSum Corpus</pwcdataset>
    </paper>
    <paper id="335">
      <title>Constructing a Culinary Interview Dialogue Corpus with Video Conferencing Tool</title>
      <author><first>Taro</first><last>Okahisa</last></author>
      <author><first>Ribeka</first><last>Tanaka</last></author>
      <author><first>Takashi</first><last>Kodama</last></author>
      <author><first>Yin Jou</first><last>Huang</last></author>
      <author><first>Sadao</first><last>Kurohashi</last></author>
      <pages>3131–3139</pages>
      <abstract>Interview is an efficient way to elicit knowledge from experts of different domains. In this paper, we introduce CIDC, an interview dialogue corpus in the culinary domain in which interviewers play an active role to elicit culinary knowledge from the cooking expert. The corpus consists of 308 interview dialogues (each about 13 minutes in length), which add up to a total of 69,000 utterances. We use a video conferencing tool for data collection, which allows us to obtain the facial expressions of the interlocutors as well as the screen-sharing contents. To understand the impact of the interlocutors’ skill level, we divide the experts into “semi-professionals’” and “enthusiasts” and the interviewers into “skilled interviewers” and “unskilled interviewers.” For quantitative analysis, we report the statistics and the results of the post-interview questionnaire. We also conduct qualitative analysis on the collected interview dialogues and summarize the salient patterns of how interviewers elicit knowledge from the experts. The corpus serves the purpose to facilitate future research on the knowledge elicitation mechanism in interview dialogues.</abstract>
      <url hash="3ed57f77">2022.lrec-1.335</url>
      <bibkey>okahisa-etal-2022-constructing</bibkey>
    </paper>
    <paper id="336">
      <title><fixed-case>U</fixed-case>g<fixed-case>C</fixed-case>h<fixed-case>D</fixed-case>ial: A <fixed-case>U</fixed-case>yghur Chat-based Dialogue Corpus for Response Space Classification</title>
      <author><first>Zulipiye</first><last>Yusupujiang</last></author>
      <author><first>Jonathan</first><last>Ginzburg</last></author>
      <pages>3140–3149</pages>
      <abstract>In this paper, we introduce a carefully designed and collected language resource: UgChDial – a Uyghur dialogue corpus based on a chatroom environment. The Uyghur Chat-based Dialogue Corpus (UgChDial) is divided into two parts: (1). Two-party dialogues and (2). Multi-party dialogues. We ran a series of 25, 120-minutes each, two-party chat sessions, totaling 7323 turns and 1581 question-response pairs. We created 16 different scenarios and topics to gather these two-party conversations. The multi-party conversations were compiled from chitchats in general channels as well as free chats in topic-oriented public channels, yielding 5588 unique turns and 838 question-response pairs. The initial purpose of this corpus is to study query-response pairs in Uyghur, building on an existing fine-grained response space taxonomy for English. We provide here initial annotation results on the Uyghur response space classification task using UgChDial.</abstract>
      <url hash="89012e4f">2022.lrec-1.336</url>
      <bibkey>yusupujiang-ginzburg-2022-ugchdial</bibkey>
    </paper>
    <paper id="337">
      <title>A Speculative and Tentative Common Ground Handling for Efficient Composition of Uncertain Dialogue</title>
      <author><first>Saki</first><last>Sudo</last></author>
      <author><first>Kyoshiro</first><last>Asano</last></author>
      <author><first>Koh</first><last>Mitsuda</last></author>
      <author><first>Ryuichiro</first><last>Higashinaka</last></author>
      <author><first>Yugo</first><last>Takeuchi</last></author>
      <pages>3150–3157</pages>
      <abstract>This study investigates how the grounding process is composed and explores new interaction approaches that adapt to human cognitive processes that have not yet been significantly studied. The results of an experiment indicate that grounding through dialogue is mutually accepted among participants through holistic expressions and suggest that common ground among participants may not necessarily be formed in a bottom-up way through analytic expressions. These findings raise the possibility of a promising new approach to creating a human-like dialogue system that may be more suitable for natural human communication.</abstract>
      <url hash="7d17bd16">2022.lrec-1.337</url>
      <bibkey>sudo-etal-2022-speculative</bibkey>
    </paper>
    <paper id="338">
      <title><fixed-case>B</fixed-case>a<fixed-case>SC</fixed-case>o: An Annotated <fixed-case>B</fixed-case>asque-<fixed-case>S</fixed-case>panish Code-Switching Corpus for Natural Language Understanding</title>
      <author><first>Maia</first><last>Aguirre</last></author>
      <author><first>Laura</first><last>García-Sardiña</last></author>
      <author><first>Manex</first><last>Serras</last></author>
      <author><first>Ariane</first><last>Méndez</last></author>
      <author><first>Jacobo</first><last>López</last></author>
      <pages>3158–3163</pages>
      <abstract>The main objective of this work is the elaboration and public release of BaSCo, the first corpus with annotated linguistic resources encompassing Basque-Spanish code-switching. The mixture of Basque and Spanish languages within the same utterance is popularly referred to as Euskañol, a widespread phenomenon among bilingual speakers in the Basque Country. Thus, this corpus has been created to meet the demand of annotated linguistic resources in Euskañol in research areas such as multilingual dialogue systems. The presented resource is the result of translating to Euskañol a compilation of texts in Basque and Spanish that were used for training the Natural Language Understanding (NLU) models of several task-oriented bilingual chatbots. Those chatbots were meant to answer specific questions associated with the administration, fiscal, and transport domains. In addition, they had the transverse potential to answer to greetings, requests for help, and chit-chat questions asked to chatbots. BaSCo is a compendium of 1377 tagged utterances with every sample annotated at three levels: (i) NLU semantic labels, considering intents and entities, (ii) code-switching proportion, and (iii) domain of origin.</abstract>
      <url hash="7b0de47f">2022.lrec-1.338</url>
      <bibkey>aguirre-etal-2022-basco</bibkey>
    </paper>
    <paper id="339">
      <title><fixed-case>P</fixed-case>ro<fixed-case>D</fixed-case>ial – An Annotated Proactive Dialogue Act Corpus for Conversational Assistants using Crowdsourcing</title>
      <author><first>Matthias</first><last>Kraus</last></author>
      <author><first>Nicolas</first><last>Wagner</last></author>
      <author><first>Wolfgang</first><last>Minker</last></author>
      <pages>3164–3173</pages>
      <abstract>Robots will eventually enter our daily lives and assist with a variety of tasks. Especially in the household domain, robots may become indispensable helpers by overtaking tedious tasks, e.g. keeping the place tidy. Their effectiveness and efficiency, however, depend on their ability to adapt to our needs, routines, and personal characteristics. Otherwise, they may not be accepted and trusted in our private domain. For enabling adaptation, the interaction between a human and a robot needs to be personalized. Therefore, the robot needs to collect personal information from the user. However, it is unclear how such sensitive data can be collected in an understandable way without losing a user’s trust in the system. In this paper, we present a conversational approach for explicitly collecting personal user information using natural dialogue. For creating a sound interactive personalization, we have developed an empathy-augmented dialogue strategy. In an online study, the empathy-augmented strategy was compared to a baseline dialogue strategy for interactive personalization. We have found the empathy-augmented strategy to perform notably friendlier. Overall, using dialogue for interactive personalization has generally shown positive user reception.</abstract>
      <url hash="ac99f189">2022.lrec-1.339</url>
      <bibkey>kraus-etal-2022-prodial</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/multiwoz">MultiWOZ</pwcdataset>
    </paper>
    <paper id="340">
      <title><fixed-case>ELITR</fixed-case> Minuting Corpus: A Novel Dataset for Automatic Minuting from Multi-Party Meetings in <fixed-case>E</fixed-case>nglish and <fixed-case>C</fixed-case>zech</title>
      <author><first>Anna</first><last>Nedoluzhko</last></author>
      <author><first>Muskaan</first><last>Singh</last></author>
      <author><first>Marie</first><last>Hledíková</last></author>
      <author><first>Tirthankar</first><last>Ghosal</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <pages>3174–3182</pages>
      <abstract>Taking minutes is an essential component of every meeting, although the goals, style, and procedure of this activity (“minuting” for short) can vary. Minuting is a rather unstructured writing activity and is affected by who is taking the minutes and for whom the intended minutes are. With the rise of online meetings, automatic minuting would be an important benefit for the meeting participants as well as for those who might have missed the meeting. However, automatically generating meeting minutes is a challenging problem due to a variety of factors including the quality of automatic speech recorders (ASRs), availability of public meeting data, subjective knowledge of the minuter, etc. In this work, we present the first of its kind dataset on <i>Automatic Minuting</i>. We develop a dataset of English and Czech technical project meetings which consists of transcripts generated from ASRs, manually corrected, and minuted by several annotators. Our dataset, AutoMin, consists of 113 (English) and 53 (Czech) meetings, covering more than 160 hours of meeting content. Upon acceptance, we will publicly release (aaa.bbb.ccc) the dataset as a set of meeting transcripts and minutes, excluding the recordings for privacy reasons. A unique feature of our dataset is that most meetings are equipped with more than one minute, each created independently. Our corpus thus allows studying differences in what people find important while taking the minutes. We also provide baseline experiments for the community to explore this novel problem further. To the best of our knowledge <b>AutoMin</b> is probably the first resource on minuting in English and also in a language other than English (Czech).</abstract>
      <url hash="32f0bbf5">2022.lrec-1.340</url>
      <bibkey>nedoluzhko-etal-2022-elitr</bibkey>
    </paper>
    <paper id="341">
      <title>Extracting Age-Related Stereotypes from Social Media Texts</title>
      <author><first>Kathleen C.</first><last>Fraser</last></author>
      <author><first>Svetlana</first><last>Kiritchenko</last></author>
      <author><first>Isar</first><last>Nejadgholi</last></author>
      <pages>3183–3194</pages>
      <abstract>Age-related stereotypes are pervasive in our society, and yet have been under-studied in the NLP community. Here, we present a method for extracting age-related stereotypes from Twitter data, generating a corpus of 300,000 over-generalizations about four contemporary generations (baby boomers, generation X, millennials, and generation Z), as well as “old” and “young” people more generally. By employing word-association metrics, semi-supervised topic modelling, and density-based clustering, we uncover many common stereotypes as reported in the media and in the psychological literature, as well as some more novel findings. We also observe trends consistent with the existing literature, namely that definitions of “young” and “old” age appear to be context-dependent, stereotypes for different generations vary across different topics (e.g., work versus family life), and some age-based stereotypes are distinct from generational stereotypes. The method easily extends to other social group labels, and therefore can be used in future work to study stereotypes of different social categories. By better understanding how stereotypes are formed and spread, and by tracking emerging stereotypes, we hope to eventually develop mitigating measures against such biased statements.</abstract>
      <url hash="fbe0b61f">2022.lrec-1.341</url>
      <bibkey>fraser-etal-2022-extracting</bibkey>
    </paper>
    <paper id="342">
      <title>Borrowing or Codeswitching? Annotating for Finer-Grained Distinctions in Language Mixing</title>
      <author><first>Elena</first><last>Alvarez-Mellado</last></author>
      <author><first>Constantine</first><last>Lignos</last></author>
      <pages>3195–3201</pages>
      <abstract>We present a new corpus of Twitter data annotated for codeswitching and borrowing between Spanish and English. The corpus contains 9,500 tweets annotated at the token level with codeswitches, borrowings, and named entities. This corpus differs from prior corpora of codeswitching in that we attempt to clearly define and annotate the boundary between codeswitching and borrowing and do not treat common “internet-speak” (lol, etc.) as codeswitching when used in an otherwise monolingual context. The result is a corpus that enables the study and modeling of Spanish-English borrowing and codeswitching on Twitter in one dataset. We present baseline scores for modeling the labels of this corpus using Transformer-based language models. The annotation itself is released with a CC BY 4.0 license, while the text it applies to is distributed in compliance with the Twitter terms of service.</abstract>
      <url hash="b682cdb9">2022.lrec-1.342</url>
      <bibkey>alvarez-mellado-lignos-2022-borrowing</bibkey>
      <pwccode url="https://github.com/lirondos/borrowing-or-codeswitching" additional="false">lirondos/borrowing-or-codeswitching</pwccode>
    </paper>
    <paper id="343">
      <title>Multi-Aspect Transfer Learning for Detecting Low Resource Mental Disorders on Social Media</title>
      <author><first>Ana Sabina</first><last>Uban</last></author>
      <author><first>Berta</first><last>Chulvi</last></author>
      <author><first>Paolo</first><last>Rosso</last></author>
      <pages>3202–3219</pages>
      <abstract>Mental disorders are a serious and increasingly relevant public health issue. NLP methods have the potential to assist with automatic mental health disorder detection, but building annotated datasets for this task can be challenging; moreover, annotated data is very scarce for disorders other than depression. Understanding the commonalities between certain disorders is also important for clinicians who face the problem of shifting standards of diagnosis. We propose that transfer learning with linguistic features can be useful for approaching both the technical problem of improving mental disorder detection in the context of data scarcity, and the clinical problem of understanding the overlapping symptoms between certain disorders. In this paper, we target four disorders: depression, PTSD, anorexia and self-harm. We explore multi-aspect transfer learning for detecting mental disorders from social media texts, using deep learning models with multi-aspect representations of language (including multiple types of interpretable linguistic features). We explore different transfer learning strategies for cross-disorder and cross-platform transfer, and show that transfer learning can be effective for improving prediction performance for disorders where little annotated data is available. We offer insights into which linguistic features are the most useful vehicles for transferring knowledge, through ablation experiments, as well as error analysis.</abstract>
      <url hash="d6771f03">2022.lrec-1.343</url>
      <bibkey>uban-etal-2022-multi</bibkey>
      <pwccode url="https://github.com/ananana/mental-disorders" additional="false">ananana/mental-disorders</pwccode>
    </paper>
    <paper id="344">
      <title><fixed-case>A</fixed-case>r<fixed-case>C</fixed-case>ovid<fixed-case>V</fixed-case>ac: Analyzing <fixed-case>A</fixed-case>rabic Tweets About <fixed-case>COVID</fixed-case>-19 Vaccination</title>
      <author><first>Hamdy</first><last>Mubarak</last></author>
      <author><first>Sabit</first><last>Hassan</last></author>
      <author><first>Shammur Absar</first><last>Chowdhury</last></author>
      <author><first>Firoj</first><last>Alam</last></author>
      <pages>3220–3230</pages>
      <abstract>The emergence of the COVID-19 pandemic and the first global infodemic have changed our lives in many different ways. We relied on social media to get the latest information about COVID-19 pandemic and at the same time to disseminate information. The content in social media consisted not only health related advice, plans, and informative news from policymakers, but also contains conspiracies and rumors. It became important to identify such information as soon as they are posted to make an actionable decision (e.g., debunking rumors, or taking certain measures for traveling). To address this challenge, we develop and publicly release the first largest manually annotated Arabic tweet dataset, ArCovidVac, for COVID-19 vaccination campaign, covering many countries in the Arab region. The dataset is enriched with different layers of annotation, including, (i) Informativeness more vs. less importance of the tweets); (ii) fine-grained tweet content types (e.g., advice, rumors, restriction, authenticate news/information); and (iii) stance towards vaccination (pro-vaccination, neutral, anti-vaccination). Further, we performed in-depth analysis of the data, exploring the popularity of different vaccines, trending hashtags, topics, and presence of offensiveness in the tweets. We studied the data for individual types of tweets and temporal changes in stance towards vaccine. We benchmarked the ArCovidVac dataset using transformer architectures for informativeness, content types, and stance detection.</abstract>
      <url hash="dd6c7727">2022.lrec-1.344</url>
      <bibkey>mubarak-etal-2022-arcovidvac</bibkey>
    </paper>
    <paper id="345">
      <title><fixed-case>FACTOID</fixed-case>: A New Dataset for Identifying Misinformation Spreaders and Political Bias</title>
      <author><first>Flora</first><last>Sakketou</last></author>
      <author><first>Joan</first><last>Plepi</last></author>
      <author><first>Riccardo</first><last>Cervero</last></author>
      <author><first>Henri Jacques</first><last>Geiss</last></author>
      <author><first>Paolo</first><last>Rosso</last></author>
      <author><first>Lucie</first><last>Flek</last></author>
      <pages>3231–3241</pages>
      <abstract>Proactively identifying misinformation spreaders is an important step towards mitigating the impact of fake news on our society. In this paper, we introduce a new contemporary Reddit dataset for fake news spreader analysis, called FACTOID, monitoring political discussions on Reddit since the beginning of 2020. The dataset contains over 4K users with 3.4M Reddit posts, and includes, beyond the users’ binary labels, also their fine-grained credibility level (very low to very high) and their political bias strength (extreme right to extreme left). As far as we are aware, this is the first fake news spreader dataset that simultaneously captures both the long-term context of users’ historical posts and the interactions between them. To create the first benchmark on our data, we provide methods for identifying misinformation spreaders by utilizing the social connections between the users along with their psycho-linguistic features. We show that the users’ social interactions can, on their own, indicate misinformation spreading, while the psycho-linguistic features are mostly informative in non-neural classification settings. In a qualitative analysis we observe that detecting affective mental processes correlates negatively with right-biased users, and that the openness to experience factor is lower for those who spread fake news.</abstract>
      <url hash="57890467">2022.lrec-1.345</url>
      <bibkey>sakketou-etal-2022-factoid</bibkey>
      <pwccode url="https://github.com/caisa-lab/factoid-dataset" additional="false">caisa-lab/factoid-dataset</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/realnews">RealNews</pwcdataset>
    </paper>
    <paper id="346">
      <title>Multitask Learning for Grapheme-to-Phoneme Conversion of Anglicisms in <fixed-case>G</fixed-case>erman Speech Recognition</title>
      <author><first>Julia</first><last>Pritzen</last></author>
      <author><first>Michael</first><last>Gref</last></author>
      <author><first>Dietlind</first><last>Zühlke</last></author>
      <author><first>Christoph Andreas</first><last>Schmidt</last></author>
      <pages>3242–3249</pages>
      <abstract>Anglicisms are a challenge in German speech recognition. Due to their irregular pronunciation compared to native German words, automatically generated pronunciation dictionaries often contain incorrect phoneme sequences for Anglicisms. In this work, we propose a multitask sequence-to-sequence approach for grapheme-to-phoneme conversion to improve the phonetization of Anglicisms. We extended a grapheme-to-phoneme model with a classification task to distinguish Anglicisms from native German words. With this approach, the model learns to generate different pronunciations depending on the classification result. We used our model to create supplementary Anglicism pronunciation dictionaries to be added to an existing German speech recognition model. Tested on a special Anglicism evaluation set, we improved the recognition of Anglicisms compared to a baseline model, reducing the word error rate by a relative 1 % and the Anglicism error rate by a relative 3 %. With our experiment, we show that multitask learning can help solving the challenge of Anglicisms in German speech recognition.</abstract>
      <url hash="8b604c9a">2022.lrec-1.346</url>
      <bibkey>pritzen-etal-2022-multitask</bibkey>
    </paper>
    <paper id="347">
      <title><fixed-case>SDS</fixed-case>-200: A <fixed-case>S</fixed-case>wiss <fixed-case>G</fixed-case>erman Speech to <fixed-case>S</fixed-case>tandard <fixed-case>G</fixed-case>erman Text Corpus</title>
      <author><first>Michel</first><last>Plüss</last></author>
      <author><first>Manuela</first><last>Hürlimann</last></author>
      <author><first>Marc</first><last>Cuny</last></author>
      <author><first>Alla</first><last>Stöckli</last></author>
      <author><first>Nikolaos</first><last>Kapotis</last></author>
      <author><first>Julia</first><last>Hartmann</last></author>
      <author><first>Malgorzata Anna</first><last>Ulasik</last></author>
      <author><first>Christian</first><last>Scheller</last></author>
      <author><first>Yanick</first><last>Schraner</last></author>
      <author><first>Amit</first><last>Jain</last></author>
      <author><first>Jan</first><last>Deriu</last></author>
      <author><first>Mark</first><last>Cieliebak</last></author>
      <author><first>Manfred</first><last>Vogel</last></author>
      <pages>3250–3256</pages>
      <abstract>We present SDS-200, a corpus of Swiss German dialectal speech with Standard German text translations, annotated with dialect, age, and gender information of the speakers. The dataset allows for training speech translation, dialect recognition, and speech synthesis systems, among others. The data was collected using a web recording tool that is open to the public. Each participant was given a text in Standard German and asked to translate it to their Swiss German dialect before recording it. To increase the corpus quality, recordings were validated by other participants. The data consists of 200 hours of speech by around 4000 different speakers and covers a large part of the Swiss German dialect landscape. We release SDS-200 alongside a baseline speech translation model, which achieves a word error rate (WER) of 30.3 and a BLEU score of 53.1 on the SDS-200 test set. Furthermore, we use SDS-200 to fine-tune a pre-trained XLS-R model, achieving 21.6 WER and 64.0 BLEU.</abstract>
      <url hash="1d9c1b6f">2022.lrec-1.347</url>
      <bibkey>pluss-etal-2022-sds</bibkey>
      <pwccode url="https://github.com/stt4sg/dialektsammlung-public" additional="false">stt4sg/dialektsammlung-public</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/common-voice">Common Voice</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/must-c">MuST-C</pwcdataset>
    </paper>
    <paper id="348">
      <title>Extracting Linguistic Knowledge from Speech: A Study of Stop Realization in 5 <fixed-case>R</fixed-case>omance Languages</title>
      <author><first>Yaru</first><last>Wu</last></author>
      <author><first>Mathilde</first><last>Hutin</last></author>
      <author><first>Ioana</first><last>Vasilescu</last></author>
      <author><first>Lori</first><last>Lamel</last></author>
      <author><first>Martine</first><last>Adda-Decker</last></author>
      <pages>3257–3263</pages>
      <abstract>This paper builds upon recent work in leveraging the corpora and tools originally used to develop speech technologies for corpus-based linguistic studies. We address the non-canonical realization of consonants in connected speech and we focus on voicing alternation phenomena of stops in 5 standard varieties of Romance languages (French, Italian, Spanish, Portuguese, Romanian). For these languages, both large scale corpora and speech recognition systems were available for the study. We use forced alignment with pronunciation variants and machine learning techniques to examine to what extent such frequent phenomena characterize languages and what are the most triggering factors. The results confirm that voicing alternations occur in all Romance languages. Automatic classification underlines that surrounding contexts and segment duration are recurring contributing factors for modeling voicing alternation. The results of this study also demonstrate the new role that machine learning techniques such as classification algorithms can play in helping to extract linguistic knowledge from speech and to suggest interesting research directions.</abstract>
      <url hash="21069df6">2022.lrec-1.348</url>
      <bibkey>wu-etal-2022-extracting</bibkey>
    </paper>
    <paper id="349">
      <title>Overlaps and Gender Analysis in the Context of Broadcast Media</title>
      <author><first>Martin</first><last>Lebourdais</last></author>
      <author><first>Marie</first><last>Tahon</last></author>
      <author><first>Antoine</first><last>Laurent</last></author>
      <author><first>Sylvain</first><last>Meignier</last></author>
      <author><first>Anthony</first><last>Larcher</last></author>
      <pages>3264–3270</pages>
      <abstract>Our main goal is to study the interactions between speakers according to their gender and role in broadcast media. In this paper, we propose an extensive study of gender and overlap annotations in various speech corpora mainly dedicated to diarisation or transcription tasks. We point out the issue of the heterogeneity of the annotation guidelines for both overlapping speech and gender categories. On top of that, we analyse how the speech content (casual speech, meetings, debate, interviews, etc.) impacts the distribution of overlapping speech segments. On a small dataset of 93 recordings from LCP French channel, we intend to characterise the interactions between speakers according to their gender. Finally, we propose a method which aims to highlight active speech areas in terms of interactions between speakers. Such a visualisation tool could improve the efficiency of qualitative studies conducted by researchers in human sciences.</abstract>
      <url hash="3b3afc05">2022.lrec-1.349</url>
      <bibkey>lebourdais-etal-2022-overlaps</bibkey>
    </paper>
    <paper id="350">
      <title>A Semi-Automatic Approach to Create Large Gender- and Age-Balanced Speaker Corpora: Usefulness of Speaker Diarization &amp; Identification.</title>
      <author><first>Rémi</first><last>Uro</last></author>
      <author><first>David</first><last>Doukhan</last></author>
      <author><first>Albert</first><last>Rilliard</last></author>
      <author><first>Laetitia</first><last>Larcher</last></author>
      <author><first>Anissa-Claire</first><last>Adgharouamane</last></author>
      <author><first>Marie</first><last>Tahon</last></author>
      <author><first>Antoine</first><last>Laurent</last></author>
      <pages>3271–3280</pages>
      <abstract>This paper presents a semi-automatic approach to create a diachronic corpus of voices balanced for speaker’s age, gender, and recording period, according to 32 categories (2 genders, 4 age ranges and 4 recording periods). Corpora were selected at French National Institute of Audiovisual (INA) to obtain at least 30 speakers per category (a total of 960 speakers; only 874 have be found yet). For each speaker, speech excerpts were extracted from audiovisual documents using an automatic pipeline consisting of speech detection, background music and overlapped speech removal and speaker diarization, used to present clean speaker segments to human annotators identifying target speakers. This pipeline proved highly effective, cutting down manual processing by a factor of ten. Evaluation of the quality of the automatic processing and of the final output is provided. It shows the automatic processing compare to up-to-date process, and that the output provides high quality speech for most of the selected excerpts. This method is thus recommendable for creating large corpora of known target speakers.</abstract>
      <url hash="f7b6dabe">2022.lrec-1.350</url>
      <bibkey>uro-etal-2022-semi</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/dihard-ii">DIHARD II</pwcdataset>
    </paper>
    <paper id="351">
      <title><fixed-case>D</fixed-case>isco<fixed-case>G</fixed-case>e<fixed-case>M</fixed-case>: A Crowdsourced Corpus of Genre-Mixed Implicit Discourse Relations</title>
      <author><first>Merel</first><last>Scholman</last></author>
      <author><first>Tianai</first><last>Dong</last></author>
      <author><first>Frances</first><last>Yung</last></author>
      <author><first>Vera</first><last>Demberg</last></author>
      <pages>3281–3290</pages>
      <abstract>We present DiscoGeM, a crowdsourced corpus of 6,505 implicit discourse relations from three genres: political speech, literature, and encyclopedic texts. Each instance was annotated by 10 crowd workers. Various label aggregation methods were explored to evaluate how to obtain a label that best captures the meaning inferred by the crowd annotators. The results show that a significant proportion of discourse relations in DiscoGeM are ambiguous and can express multiple relation senses. Probability distribution labels better capture these interpretations than single labels. Further, the results emphasize that text genre crucially affects the distribution of discourse relations, suggesting that genre should be included as a factor in automatic relation classification. We make available the newly created DiscoGeM corpus, as well as the dataset with all annotator-level labels. Both the corpus and the dataset can facilitate a multitude of applications and research purposes, for example to function as training data to improve the performance of automatic discourse relation parsers, as well as facilitate research into non-connective signals of discourse relations.</abstract>
      <url hash="248ff2f0">2022.lrec-1.351</url>
      <bibkey>scholman-etal-2022-discogem</bibkey>
      <pwccode url="https://github.com/merelscholman/discogem" additional="false">merelscholman/discogem</pwccode>
    </paper>
    <paper id="352">
      <title><fixed-case>QT</fixed-case>30: A Corpus of Argument and Conflict in Broadcast Debate</title>
      <author><first>Annette</first><last>Hautli-Janisz</last></author>
      <author><first>Zlata</first><last>Kikteva</last></author>
      <author><first>Wassiliki</first><last>Siskou</last></author>
      <author><first>Kamila</first><last>Gorska</last></author>
      <author><first>Ray</first><last>Becker</last></author>
      <author><first>Chris</first><last>Reed</last></author>
      <pages>3291–3300</pages>
      <abstract>Broadcast political debate is a core pillar of democracy: it is the public’s easiest access to opinions that shape policies and enables the general public to make informed choices. With QT30, we present the largest corpus of analysed dialogical argumentation ever created (19,842 utterances, 280,000 words) and also the largest corpus of analysed broadcast political debate to date, using 30 episodes of BBC’s ‘Question Time’ from 2020 and 2021. Question Time is the prime institution in UK broadcast political debate and features questions from the public on current political issues, which are responded to by a weekly panel of five figures of UK politics and society. QT30 is highly argumentative and combines language of well-versed political rhetoric with direct, often combative, justification-seeking of the general public. QT30 is annotated with Inference Anchoring Theory, a framework well-known in argument mining, which encodes the way arguments and conflicts are created and reacted to in dialogical settings. The resource is freely available at http://corpora.aifdb.org/qt30.</abstract>
      <url hash="f97abaf7">2022.lrec-1.352</url>
      <bibkey>hautli-janisz-etal-2022-qt30</bibkey>
    </paper>
    <paper id="353">
      <title>Scaling up Discourse Quality Annotation for Political Science</title>
      <author><first>Neele</first><last>Falk</last></author>
      <author><first>Gabriella</first><last>Lapesa</last></author>
      <pages>3301–3318</pages>
      <abstract>The empirical quantification of the quality of a contribution to a political discussion is at the heart of deliberative theory, the subdiscipline of political science which investigates decision-making in deliberative democracy. Existing annotation on deliberative quality is time-consuming and carried out by experts, typically resulting in small datasets which also suffer from strong class imbalance. Scaling up such annotations with automatic tools is desirable, but very challenging. We take up this challenge and explore different strategies to improve the prediction of deliberative quality dimensions (justification, common good, interactivity, respect) in a standard dataset. Our results show that simple data augmentation techniques successfully alleviate data imbalance. Classifiers based on linguistic features (textual complexity and sentiment/polarity) and classifiers integrating argument quality annotations (from the argument mining community in NLP) were consistently outperformed by transformer-based models, with or without data augmentation.</abstract>
      <url hash="2d9d9243">2022.lrec-1.353</url>
      <bibkey>falk-lapesa-2022-scaling</bibkey>
      <pwccode url="https://github.com/blubberli/empiricaldqi" additional="false">blubberli/empiricaldqi</pwccode>
    </paper>
    <paper id="354">
      <title>Clarifying Implicit and Underspecified Phrases in Instructional Text</title>
      <author><first>Talita</first><last>Anthonio</last></author>
      <author><first>Anna</first><last>Sauer</last></author>
      <author><first>Michael</first><last>Roth</last></author>
      <pages>3319–3330</pages>
      <abstract>Natural language inherently consists of implicit and underspecified phrases, which represent potential sources of misunderstanding. In this paper, we present a data set of such phrases in English from instructional texts together with multiple possible clarifications. Our data set, henceforth called CLAIRE, is based on a corpus of revision histories from wikiHow, from which we extract human clarifications that resolve an implicit or underspecified phrase. We show how language modeling can be used to generate alternate clarifications, which may or may not be compatible with the human clarification. Based on plausibility judgements for each clarification, we define the task of distinguishing between plausible and implausible clarifications. We provide several baseline models for this task and analyze to what extent different clarifications represent multiple readings as a first step to investigate misunderstandings caused by implicit/underspecified language in instructional texts.</abstract>
      <url hash="28ccca6b">2022.lrec-1.354</url>
      <bibkey>anthonio-etal-2022-clarifying</bibkey>
    </paper>
    <paper id="355">
      <title>Multilingual Pragmaticon: Database of Discourse Formulae</title>
      <author><first>Anton</first><last>Buzanov</last></author>
      <author><first>Polina</first><last>Bychkova</last></author>
      <author><first>Arina</first><last>Molchanova</last></author>
      <author><first>Anna</first><last>Postnikova</last></author>
      <author><first>Daria</first><last>Ryzhova</last></author>
      <pages>3331–3336</pages>
      <abstract>The paper presents a multilingual database aimed to be used as a tool for typological analysis of response constructions called discourse formulae (DF), cf. English ‘No way¡ or French ‘Ça va¡ ( ‘all right’). The two primary qualities that make DF of theoretical interest for linguists are their idiomaticity and the special nature of their meanings (cf. consent, refusal, negation), determined by their dialogical function. The formal and semantic structures of these items are language-specific. Compiling a database with DF from various languages would help estimate the diversity of DF in both of these aspects, and, at the same time, establish some frequently occurring patterns. The DF in the database are accompanied with glosses and assigned with multiple tags, such as pragmatic function, additional semantics, the illocutionary type of the context, etc. As a starting point, Russian, Serbian and Slovene DF are included into the database. This data already shows substantial grammatical and lexical variability.</abstract>
      <url hash="c6a9ff91">2022.lrec-1.355</url>
      <bibkey>buzanov-etal-2022-multilingual</bibkey>
    </paper>
    <paper id="356">
      <title>Distant Reading in Digital Humanities: Case Study on the <fixed-case>S</fixed-case>erbian Part of the <fixed-case>ELT</fixed-case>e<fixed-case>C</fixed-case> Collection</title>
      <author><first>Ranka</first><last>Stanković</last></author>
      <author><first>Cvetana</first><last>Krstev</last></author>
      <author><first>Branislava</first><last>Šandrih Todorović</last></author>
      <author><first>Dusko</first><last>Vitas</last></author>
      <author><first>Mihailo</first><last>Skoric</last></author>
      <author><first>Milica</first><last>Ikonić Nešić</last></author>
      <pages>3337–3345</pages>
      <abstract>In this paper we present the Serbian part of the ELTeC multilingual corpus of novels written in the time period 1840-1920. The corpus is being built in order to test various distant reading methods and tools with the aim of re-thinking the European literary history. We present the various steps that led to the production of the Serbian sub-collection: the novel selection and retrieval, text preparation, structural annotation, POS-tagging, lemmatization and named entity recognition. The Serbian sub-collection was published on different platforms in order to make it freely available to various users. Several use examples show that this sub-collection is usefull for both close and distant reading approaches.</abstract>
      <url hash="4b965c4a">2022.lrec-1.356</url>
      <bibkey>stankovic-etal-2022-distant</bibkey>
    </paper>
    <paper id="357">
      <title>Exploring Text Recombination for Automatic Narrative Level Detection</title>
      <author><first>Nils</first><last>Reiter</last></author>
      <author><first>Judith</first><last>Sieker</last></author>
      <author><first>Svenja</first><last>Guhr</last></author>
      <author><first>Evelyn</first><last>Gius</last></author>
      <author><first>Sina</first><last>Zarrieß</last></author>
      <pages>3346–3353</pages>
      <abstract>Automatizing the process of understanding the global narrative structure of long texts and stories is still a major challenge for state-of-the-art natural language understanding systems, particularly because annotated data is scarce and existing annotation workflows do not scale well to the annotation of complex narrative phenomena. In this work, we focus on the identification of narrative levels in texts corresponding to stories that are embedded in stories. Lacking sufficient pre-annotated training data, we explore a solution to deal with data scarcity that is common in machine learning: the automatic augmentation of an existing small data set of annotated samples with the help of data synthesis. We present a workflow for narrative level detection, that includes the operationalization of the task, a model, and a data augmentation protocol for automatically generating narrative texts annotated with breaks between narrative levels. Our experiments suggest that narrative levels in long text constitute a challenging phenomenon for state-of-the-art NLP models, but generating training data synthetically does improve the prediction results considerably.</abstract>
      <url hash="6a09902d">2022.lrec-1.357</url>
      <bibkey>reiter-etal-2022-exploring</bibkey>
    </paper>
    <paper id="358">
      <title>Automatic Normalisation of Early <fixed-case>M</fixed-case>odern <fixed-case>F</fixed-case>rench</title>
      <author><first>Rachel</first><last>Bawden</last></author>
      <author><first>Jonathan</first><last>Poinhos</last></author>
      <author><first>Eleni</first><last>Kogkitsidou</last></author>
      <author><first>Philippe</first><last>Gambette</last></author>
      <author><first>Benoît</first><last>Sagot</last></author>
      <author><first>Simon</first><last>Gabay</last></author>
      <pages>3354–3366</pages>
      <abstract>Spelling normalisation is a useful step in the study and analysis of historical language texts, whether it is manual analysis by experts or automatic analysis using downstream natural language processing (NLP) tools. Not only does it help to homogenise the variable spelling that often exists in historical texts, but it also facilitates the use of off-the-shelf contemporary NLP tools, if contemporary spelling conventions are used for normalisation. We present FREEMnorm, a new benchmark for the normalisation of Early Modern French (from the 17th century) into contemporary French and provide a thorough comparison of three different normalisation methods: ABA, an alignment-based approach and MT-approaches, (both statistical and neural), including extensive parameter searching, which is often missing in the normalisation literature.</abstract>
      <url hash="c337ef47">2022.lrec-1.358</url>
      <bibkey>bawden-etal-2022-automatic</bibkey>
      <pwccode url="https://github.com/rbawden/modfr-norm" additional="false">rbawden/modfr-norm</pwccode>
    </paper>
    <paper id="359">
      <title>From <fixed-case>F</fixed-case>re<fixed-case>EM</fixed-case> to D’<fixed-case>A</fixed-case>lem<fixed-case>BERT</fixed-case>: a Large Corpus and a Language Model for Early <fixed-case>M</fixed-case>odern <fixed-case>F</fixed-case>rench</title>
      <author><first>Simon</first><last>Gabay</last></author>
      <author><first>Pedro</first><last>Ortiz Suarez</last></author>
      <author><first>Alexandre</first><last>Bartz</last></author>
      <author><first>Alix</first><last>Chagué</last></author>
      <author><first>Rachel</first><last>Bawden</last></author>
      <author><first>Philippe</first><last>Gambette</last></author>
      <author><first>Benoît</first><last>Sagot</last></author>
      <pages>3367–3374</pages>
      <abstract>anguage models for historical states of language are becoming increasingly important to allow the optimal digitisation and analysis of old textual sources. Because these historical states are at the same time more complex to process and more scarce in the corpora available, this paper presents recent efforts to overcome this difficult situation. These efforts include producing a corpus, creating the model, and evaluating it with an NLP task currently used by scholars in other ongoing projects.</abstract>
      <url hash="97d82b10">2022.lrec-1.359</url>
      <bibkey>gabay-etal-2022-freem</bibkey>
    </paper>
    <paper id="360">
      <title>Detecting Multiple Transitions in Literary Texts</title>
      <author><first>Nuette</first><last>Heyns</last></author>
      <author><first>Menno</first><last>van Zaanen</last></author>
      <pages>3375–3381</pages>
      <abstract>Identifying the high level structure of texts provides important information when performing distant reading analysis. The structure of texts is not necessarily linear, as transitions, such as changes in the scenery or flashbacks, can be present. As a first step in identifying this structure, we aim to identify transitions in texts. Previous work (Heyns and van Zaanen, 2021) proposed a system that can successfully identify one transition in literary texts. The text is split in snippets and LDA is applied, resulting in a sequence of topics. A transition is introduced at the point that separates the topics (before and after the point) best. In this article, we extend the existing system such that it can detect multiple transitions. Additionally, we introduce a new system that inherently handles multiple transitions in texts. The new system also relies on LDA information, but is more robust than the previous system. We apply these systems to texts with known transitions (as they are constructed by concatenating text snippets stemming from different source texts) and evaluation both systems on texts with one transition and texts with two transitions. As both systems rely on LDA to identify transitions between snippets, we also show the impact of varying the number of LDA topics on the results as well. The new system consistently outperforms the previous system, not only on texts with multiple transitions, but also on single boundary texts.</abstract>
      <url hash="cc379d64">2022.lrec-1.360</url>
      <bibkey>heyns-van-zaanen-2022-detecting</bibkey>
    </paper>
    <paper id="361">
      <title><fixed-case>B</fixed-case>asque<fixed-case>P</fixed-case>arl: A Bilingual Corpus of <fixed-case>B</fixed-case>asque Parliamentary Transcriptions</title>
      <author><first>Nayla</first><last>Escribano</last></author>
      <author><first>Jon Ander</first><last>Gonzalez</last></author>
      <author><first>Julen</first><last>Orbegozo-Terradillos</last></author>
      <author><first>Ainara</first><last>Larrondo-Ureta</last></author>
      <author><first>Simón</first><last>Peña-Fernández</last></author>
      <author><first>Olatz</first><last>Perez-de-Viñaspre</last></author>
      <author><first>Rodrigo</first><last>Agerri</last></author>
      <pages>3382–3390</pages>
      <abstract>Parliamentary transcripts provide a valuable resource to understand the reality and know about the most important facts that occur over time in our societies. Furthermore, the political debates captured in these transcripts facilitate research on political discourse from a computational social science perspective. In this paper we release the first version of a newly compiled corpus from Basque parliamentary transcripts. The corpus is characterized by heavy Basque-Spanish code-switching, and represents an interesting resource to study political discourse in contrasting languages such as Basque and Spanish. We enrich the corpus with metadata related to relevant attributes of the speakers and speeches (language, gender, party...) and process the text to obtain named entities and lemmas. The obtained metadata is then used to perform a detailed corpus analysis which provides interesting insights about the language use of the Basque political representatives across time, parties and gender.</abstract>
      <url hash="042982b0">2022.lrec-1.361</url>
      <bibkey>escribano-etal-2022-basqueparl</bibkey>
      <pwccode url="https://github.com/ixa-ehu/basqueparl" additional="false">ixa-ehu/basqueparl</pwccode>
    </paper>
    <paper id="362">
      <title><fixed-case>G</fixed-case>er<fixed-case>EO</fixed-case>: A Large-Scale Resource on the Syntactic Distribution of <fixed-case>G</fixed-case>erman Experiencer-Object Verbs</title>
      <author><first>Johanna M.</first><last>Poppek</last></author>
      <author><first>Simon</first><last>Masloch</last></author>
      <author><first>Tibor</first><last>Kiss</last></author>
      <pages>3391–3397</pages>
      <abstract>Although studied for several decades, the syntactic properties of experiencer-object (EO) verbs are still under discussion, while most analyses are not supported by substantial corpus data. With GerEO, we intend to fill this lacuna for German EO-verbs by presenting a large-scale database of more than 10,000 examples for 64 verbs (up to 200 per verb) from a newspaper corpus annotated for several syntactic and semantic features relevant for their analysis, including the overall syntactic construction, the semantic stimulus type, and the form of a possible stimulus preposition, i.e. a preposition heading a PP that indicates (a part/aspect of) the stimulus. Non-psych occurrences of the verbs are not excluded from the database but marked as such to make a comparison possible. Data of this kind can be used to develop and test theoretical hypotheses on the properties of EO-verbs, aid in the construction of experiments as well as provide training and test data for AI systems.</abstract>
      <url hash="63474bd2">2022.lrec-1.362</url>
      <bibkey>poppek-etal-2022-gereo</bibkey>
    </paper>
    <paper id="363">
      <title><fixed-case>ACT</fixed-case>2: A multi-disciplinary semi-structured dataset for importance and purpose classification of citations</title>
      <author><first>Suchetha</first><last>Nambanoor Kunnath</last></author>
      <author><first>Valentin</first><last>Stauber</last></author>
      <author><first>Ronin</first><last>Wu</last></author>
      <author><first>David</first><last>Pride</last></author>
      <author><first>Viktor</first><last>Botev</last></author>
      <author><first>Petr</first><last>Knoth</last></author>
      <pages>3398–3406</pages>
      <abstract>Classifying citations according to their purpose and importance is a challenging task that has gained considerable interest in recent years. This interest has been primarily driven by the need to create more transparent, efficient, merit-based reward systems in academia; a system that goes beyond simple bibliometric measures and considers the semantics of citations. Such systems that quantify and classify the influence of citations can act as edges that link knowledge nodes to a graph and enable efficient knowledge discovery. While a number of researchers have experimented with a variety of models, these experiments are typically limited to single-domain applications and the resulting models are hardly comparable. Recently, two Citation Context Classification (3C) shared tasks (at WOSP2020 and SDP2021) created the first benchmark enabling direct comparison of citation classification approaches, revealing the crucial impact of supplementary data on the performance of models. Reflecting from the findings of these shared tasks, we are releasing a new multi-disciplinary dataset, ACT2, an extended SDP 3C shared task dataset. This modified corpus has annotations for both citation function and importance classes newly enriched with supplementary contextual and non-contextual feature sets the selection of which follows from the lists of features used by the more successful teams in these shared tasks. Additionally, we include contextual features for cited papers (e.g. Abstract of the cited paper), which most existing datasets lack, but which have a lot of potential to improve results. We describe the methodology used for feature extraction and the challenges involved in the process. The feature enriched ACT2 dataset is available at https://github.com/oacore/ACT2.</abstract>
      <url hash="28e7c08a">2022.lrec-1.363</url>
      <bibkey>nambanoor-kunnath-etal-2022-act2</bibkey>
      <pwccode url="https://github.com/oacore/act2" additional="false">oacore/act2</pwccode>
    </paper>
    <paper id="364">
      <title>Quantification Annotation in <fixed-case>ISO</fixed-case> 24617-12, Second Draft</title>
      <author><first>Harry</first><last>Bunt</last></author>
      <author><first>Maxime</first><last>Amblard</last></author>
      <author><first>Johan</first><last>Bos</last></author>
      <author><first>Karën</first><last>Fort</last></author>
      <author><first>Bruno</first><last>Guillaume</last></author>
      <author><first>Philippe</first><last>de Groote</last></author>
      <author><first>Chuyuan</first><last>Li</last></author>
      <author><first>Pierre</first><last>Ludmann</last></author>
      <author><first>Michel</first><last>Musiol</last></author>
      <author><first>Siyana</first><last>Pavlova</last></author>
      <author><first>Guy</first><last>Perrier</last></author>
      <author><first>Sylvain</first><last>Pogodalla</last></author>
      <pages>3407–3416</pages>
      <abstract>This paper describes the continuation of a project that aims at establishing an interoperable annotation schema for quantification phenomena as part of the ISO suite of standards for semantic annotation, known as the Semantic Annotation Framework. After a break, caused by the Covid-19 pandemic, the project was relaunched in early 2022 with a second working draft of an annotation scheme, which is discussed in this paper. Keywords: semantic annotation, quantification, interoperability, annotation schema, ISO standard</abstract>
      <url hash="216f2d26">2022.lrec-1.364</url>
      <bibkey>bunt-etal-2022-quantification</bibkey>
    </paper>
    <paper id="365">
      <title>The <fixed-case>LTRC</fixed-case> <fixed-case>H</fixed-case>indi-<fixed-case>T</fixed-case>elugu Parallel Corpus</title>
      <author><first>Vandan</first><last>Mujadia</last></author>
      <author><first>Dipti</first><last>Sharma</last></author>
      <pages>3417–3424</pages>
      <abstract>We present the Hindi-Telugu Parallel Corpus of different technical domains such as Natural Science, Computer Science, Law and Healthcare along with the General domain. The qualitative corpus consists of 700K parallel sentences of which 535K sentences were created using multiple methods such as extract, align and review of Hindi-Telugu corpora, end-to-end human translation, iterative back-translation driven post-editing and around 165K parallel sentences were collected from available sources in the public domain. We present the comparative assessment of created parallel corpora for representativeness and diversity. The corpus has been pre-processed for machine translation, and we trained a neural machine translation system using it and report state-of-the-art baseline results on the developed development set over multiple domains and on available benchmarks. With this, we define a new task on Domain Machine Translation for low resource language pairs such as Hindi and Telugu. The developed corpus (535K) is freely available for non-commercial research and to the best of our knowledge, this is the well curated, largest, publicly available domain parallel corpus for Hindi-Telugu.</abstract>
      <url hash="fb03375a">2022.lrec-1.365</url>
      <bibkey>mujadia-sharma-2022-ltrc</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/flores-101">FLORES-101</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/samanantar">Samanantar</pwcdataset>
    </paper>
    <paper id="366">
      <title><fixed-case>MHE</fixed-case>: Code-Mixed Corpora for Similar Language Identification</title>
      <author><first>Priya</first><last>Rani</last></author>
      <author><first>John P.</first><last>McCrae</last></author>
      <author><first>Theodorus</first><last>Fransen</last></author>
      <pages>3425–3433</pages>
      <abstract>This paper introduces a new Magahi-Hindi-English (MHE) code-mixed data-set for similar language identification (SMLID), where Magahi is a less-resourced minority language. This corpus provides a language id at two levels: word and sentence. This data-set is the first Magahi-Hindi-English code-mixed data-set for similar language identification task. Furthermore, we will discuss the complexity of the data-set and provide a few baselines for the language identification task.</abstract>
      <url hash="62534e80">2022.lrec-1.366</url>
      <bibkey>rani-etal-2022-mhe</bibkey>
    </paper>
    <paper id="367">
      <title>Bazinga! A Dataset for Multi-Party Dialogues Structuring</title>
      <author><first>Paul</first><last>Lerner</last></author>
      <author><first>Juliette</first><last>Bergoënd</last></author>
      <author><first>Camille</first><last>Guinaudeau</last></author>
      <author><first>Hervé</first><last>Bredin</last></author>
      <author><first>Benjamin</first><last>Maurice</last></author>
      <author><first>Sharleyne</first><last>Lefevre</last></author>
      <author><first>Martin</first><last>Bouteiller</last></author>
      <author><first>Aman</first><last>Berhe</last></author>
      <author><first>Léo</first><last>Galmant</last></author>
      <author><first>Ruiqing</first><last>Yin</last></author>
      <author><first>Claude</first><last>Barras</last></author>
      <pages>3434–3441</pages>
      <abstract>We introduce a dataset built around a large collection of TV (and movie) series. Those are filled with challenging multi-party dialogues. Moreover, TV series come with a very active fan base that allows the collection of metadata and accelerates annotation. With 16 TV and movie series, Bazinga! amounts to 400+ hours of speech and 8M+ tokens, including 500K+ tokens annotated with the speaker, addressee, and entity linking information. Along with the dataset, we also provide a baseline for speaker diarization, punctuation restoration, and person entity recognition. The results demonstrate the difficulty of the tasks and of transfer learning from models trained on mono-speaker audio or written text, which is more widely available. This work is a step towards better multi-party dialogue structuring and understanding. Bazinga! is available at hf.co/bazinga. Because (a large) part of Bazinga! is only partially annotated, we also expect this dataset to foster research towards self- or weakly-supervised learning methods.</abstract>
      <url hash="f4aaac3f">2022.lrec-1.367</url>
      <bibkey>lerner-etal-2022-bazinga</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/serial-speakers">Serial Speakers</pwcdataset>
    </paper>
    <paper id="368">
      <title>The Ellogon Web Annotation Tool: Annotating Moral Values and Arguments</title>
      <author><first>Alexandros Fotios</first><last>Ntogramatzis</last></author>
      <author><first>Anna</first><last>Gradou</last></author>
      <author><first>Georgios</first><last>Petasis</last></author>
      <author><first>Marko</first><last>Kokol</last></author>
      <pages>3442–3450</pages>
      <abstract>In this paper, we present the Ellogon Web Annotation Tool. It is a collaborative, web-based annotation tool built upon the Ellogon infrastructure offering an improved user experience and adaptability to various annotation scenarios by making good use of the latest design practices and web development frameworks. Being in development for many years, this paper describes its current architecture, along with the recent modifications that extend the existing functionalities and the new features that were added. The new version of the tool offers document analytics, annotation inspection and comparison features, a modern UI, and formatted text import (e.g. TEI XML documents, rendered with simple markup). We present two use cases that serve as two examples of different annotation scenarios to demonstrate the new functionalities. An appropriate (user-supplied, XML-based) annotation schema is used for each scenario. The first schema contains the relevant components for representing concepts, moral values, and ideas. The second includes all the necessary elements for annotating argumentative units in a document and their binary relations.</abstract>
      <url hash="8c736940">2022.lrec-1.368</url>
      <bibkey>ntogramatzis-etal-2022-ellogon</bibkey>
    </paper>
    <paper id="369">
      <title><fixed-case>W</fixed-case>e<fixed-case>C</fixed-case>an<fixed-case>T</fixed-case>alk: A New Multi-language, Multi-modal Resource for Speaker Recognition</title>
      <author><first>Karen</first><last>Jones</last></author>
      <author><first>Kevin</first><last>Walker</last></author>
      <author><first>Christopher</first><last>Caruso</last></author>
      <author><first>Jonathan</first><last>Wright</last></author>
      <author><first>Stephanie</first><last>Strassel</last></author>
      <pages>3451–3456</pages>
      <abstract>The WeCanTalk (WCT) Corpus is a new multi-language, multi-modal resource for speaker recognition. The corpus contains Cantonese, Mandarin and English telephony and video speech data from over 200 multilingual speakers located in Hong Kong. Each speaker contributed at least 10 telephone conversations of 8-10 minutes’ duration collected via a custom telephone platform based in Hong Kong. Speakers also uploaded at least 3 videos in which they were both speaking and visible, along with one selfie image. At least half of the calls and videos for each speaker were in Cantonese, while their remaining recordings featured one or more different languages. Both calls and videos were made in a variety of noise conditions. All speech and video recordings were audited by experienced multilingual annotators for quality including presence of the expected language and for speaker identity. The WeCanTalk Corpus has been used to support the NIST 2021 Speaker Recognition Evaluation and will be published in the LDC catalog.</abstract>
      <url hash="7bc330c3">2022.lrec-1.369</url>
      <bibkey>jones-etal-2022-wecantalk</bibkey>
    </paper>
    <paper id="370">
      <title>Using <fixed-case>W</fixed-case>iktionary to Create Specialized Lexical Resources and Datasets</title>
      <author><first>Lenka</first><last>Bajčetić</last></author>
      <author><first>Thierry</first><last>Declerck</last></author>
      <pages>3457–3460</pages>
      <abstract>This paper describes an approach aiming at utilizing Wiktionary data for creating specialized lexical datasets which can be used for enriching other lexical (semantic) resources or for generating datasets that can be used for evaluating or improving NLP tasks, like Word Sense Disambiguation, Word-in-Context challenges, or Sense Linking across lexicons and dictionaries. We have focused on Wiktionary data about pronunciation information in English, and grammatical number and grammatical gender in German.</abstract>
      <url hash="bedf1316">2022.lrec-1.370</url>
      <bibkey>bajcetic-declerck-2022-using</bibkey>
    </paper>
    <paper id="371">
      <title><fixed-case>STAPI</fixed-case>: An Automatic Scraper for Extracting Iterative Title-Text Structure from Web Documents</title>
      <author><first>Nan</first><last>Zhang</last></author>
      <author><first>Shomir</first><last>Wilson</last></author>
      <author><first>Prasenjit</first><last>Mitra</last></author>
      <pages>3461–3470</pages>
      <abstract>Formal documents often are organized into sections of text, each with a title, and extracting this structure remains an under-explored aspect of natural language processing. This iterative title-text structure is valuable data for building models for headline generation and section title generation, but there is no corpus that contains web documents annotated with titles and prose texts. Therefore, we propose the first title-text dataset on web documents that incorporates a wide variety of domains to facilitate downstream training. We also introduce STAPI (Section Title And Prose text Identifier), a two-step system for labeling section titles and prose text in HTML documents. To filter out unrelated content like document footers, its first step involves a filter that reads HTML documents and proposes a set of textual candidates. In the second step, a typographic classifier takes the candidates from the filter and categorizes each one into one of the three pre-defined classes (title, prose text, and miscellany). We show that STAPI significantly outperforms two baseline models in terms of title-text identification. We release our dataset along with a web application to facilitate supervised and semi-supervised training in this domain.</abstract>
      <url hash="1726f663">2022.lrec-1.371</url>
      <bibkey>zhang-etal-2022-stapi</bibkey>
    </paper>
    <paper id="372">
      <title><fixed-case>ELTE</fixed-case> Poetry Corpus: A Machine Annotated Database of Canonical <fixed-case>H</fixed-case>ungarian Poetry</title>
      <author><first>Péter</first><last>Horváth</last></author>
      <author><first>Péter</first><last>Kundráth</last></author>
      <author><first>Balázs</first><last>Indig</last></author>
      <author><first>Zsófia</first><last>Fellegi</last></author>
      <author><first>Eszter</first><last>Szlávich</last></author>
      <author><first>Tímea Borbála</first><last>Bajzát</last></author>
      <author><first>Zsófia</first><last>Sárközi-Lindner</last></author>
      <author><first>Bence</first><last>Vida</last></author>
      <author><first>Aslihan</first><last>Karabulut</last></author>
      <author><first>Mária</first><last>Timári</last></author>
      <author><first>Gábor</first><last>Palkó</last></author>
      <pages>3471–3478</pages>
      <abstract>ELTE Poetry Corpus is a database that stores canonical Hungarian poetry with automatically generated annotations of the poems’ structural units, grammatical features and sound devices, i.e. rhyme patterns, rhyme pairs, rhythm, alliterations and the main phonological features of words. The corpus has an open access online query tool with several search functions. The paper presents the main stages of the annotation process and the tools used for each stage. The TEI XML format of the different versions of the corpus, each of which contains an increasing number of annotation layers, is presented as well. We have also specified our own XML format for the corpus, slightly different from TEI, in order to make it easier and faster to execute queries on the corpus. We discuss the results of a manual evaluation of the quality of automatic annotation of rhythm, as well as the results of an automatic evaluation of different rule sets used for the automatic annotation of rhyme patterns. Finally, the paper gives an overview of the main functions of the online query tool developed for the corpus.</abstract>
      <url hash="61436aba">2022.lrec-1.372</url>
      <bibkey>horvath-etal-2022-elte</bibkey>
      <pwccode url="https://github.com/elte-dh/poetry-corpus" additional="false">elte-dh/poetry-corpus</pwccode>
    </paper>
    <paper id="373">
      <title><fixed-case>HAWP</fixed-case>: a Dataset for <fixed-case>H</fixed-case>indi Arithmetic Word Problem Solving</title>
      <author><first>Harshita</first><last>Sharma</last></author>
      <author><first>Pruthwik</first><last>Mishra</last></author>
      <author><first>Dipti</first><last>Sharma</last></author>
      <pages>3479–3490</pages>
      <abstract>Word Problem Solving remains a challenging and interesting task in NLP. A lot of research has been carried out to solve different genres of word problems with various complexity levels in recent years. However, most of the publicly available datasets and work has been carried out for English. Recently there has been a surge in this area of word problem solving in Chinese with the creation of large benchmark datastes. Apart from these two languages, labeled benchmark datasets for low resource languages are very scarce. This is the first attempt to address this issue for any Indian Language, especially Hindi. In this paper, we present HAWP (Hindi Arithmetic Word Problems), a dataset consisting of 2336 arithmetic word problems in Hindi. We also developed baseline systems for solving these word problems. We also propose a new evaluation technique for word problem solvers taking equation equivalence into account.</abstract>
      <url hash="b38abc81">2022.lrec-1.373</url>
      <bibkey>sharma-etal-2022-hawp</bibkey>
      <pwccode url="https://github.com/Pruthwik/Hindi-Word-Problem-Solver" additional="false">Pruthwik/Hindi-Word-Problem-Solver</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mawps">MAWPS</pwcdataset>
    </paper>
    <paper id="374">
      <title>The <fixed-case>B</fixed-case>ulgarian Event Corpus: Overview and Initial <fixed-case>NER</fixed-case> Experiments</title>
      <author><first>Petya</first><last>Osenova</last></author>
      <author><first>Kiril</first><last>Simov</last></author>
      <author><first>Iva</first><last>Marinova</last></author>
      <author><first>Melania</first><last>Berbatova</last></author>
      <pages>3491–3499</pages>
      <abstract>The paper describes the Bulgarian Event Corpus (BEC). The annotation scheme is based on CIDOC-CRM ontology and on the English Framenet, adjusted for our task. It includes two main layers: named entities and events with their roles. The corpus is multi-domain and mainly oriented towards Social Sciences and Humanities (SSH). It will be used for: extracting knowledge and making it available through the Bulgaria-centric Knowledge Graph; further developing an annotation scheme that handles multiple domains in SSH; training automatic modules for the most important knowledge-based tasks, such as domain-specific and nested NER, NEL, event detection and profiling. Initial experiments were conducted on standard NER task due to complexity of the dataset and the rich NE annotation scheme. The results are promising with respect to some labels and give insights on handling better other ones. These experiments serve also as error detection modules that would help us in scheme re-design. They are a basis for further and more complex tasks, such as nested NER, NEL and event detection.</abstract>
      <url hash="f0c4a301">2022.lrec-1.374</url>
      <bibkey>osenova-etal-2022-bulgarian</bibkey>
    </paper>
    <paper id="375">
      <title>A Corpus for Commonsense Inference in Story Cloze Test</title>
      <author><first>Bingsheng</first><last>Yao</last></author>
      <author><first>Ethan</first><last>Joseph</last></author>
      <author><first>Julian</first><last>Lioanag</last></author>
      <author><first>Mei</first><last>Si</last></author>
      <pages>3500–3508</pages>
      <abstract>The Story Cloze Test (SCT) is designed for training and evaluating machine learning algorithms for narrative understanding and inferences. The SOTA models can achieve over 90% accuracy on predicting the last sentence. However, it has been shown that high accuracy can be achieved by merely using surface-level features. We suspect these models may not <i>truly</i> understand the story. Based on the SCT dataset, we constructed a human-labeled and human-verified commonsense knowledge inference dataset. Given the first four sentences of a story, we asked crowd-source workers to choose from four types of narrative inference for deciding the ending sentence and which sentence contributes most to the inference. We accumulated data on 1871 stories, and three human workers labeled each story. Analysis of the intra-category and inter-category agreements show a high level of consensus. We present two new tasks for predicting the narrative inference categories and contributing sentences. Our results show that transformer-based models can reach SOTA performance on the original SCT task using transfer learning but don’t perform well on these new and more challenging tasks.</abstract>
      <url hash="1abb457b">2022.lrec-1.375</url>
      <bibkey>yao-etal-2022-corpus</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/storycloze">StoryCloze</pwcdataset>
    </paper>
    <paper id="376">
      <title>Lessons Learned from <fixed-case>GPT</fixed-case>-<fixed-case>SW</fixed-case>3: Building the First Large-Scale Generative Language Model for <fixed-case>S</fixed-case>wedish</title>
      <author><first>Ariel</first><last>Ekgren</last></author>
      <author><first>Amaru</first><last>Cuba Gyllensten</last></author>
      <author><first>Evangelia</first><last>Gogoulou</last></author>
      <author><first>Alice</first><last>Heiman</last></author>
      <author><first>Severine</first><last>Verlinden</last></author>
      <author><first>Joey</first><last>Öhman</last></author>
      <author><first>Fredrik</first><last>Carlsson</last></author>
      <author><first>Magnus</first><last>Sahlgren</last></author>
      <pages>3509–3518</pages>
      <abstract>We present GTP-SW3, a 3.5 billion parameter autoregressive language model, trained on a newly created 100 GB Swedish corpus. This paper provides insights with regards to data collection and training, while highlights the challenges of proper model evaluation. The results of quantitive evaluation through perplexity indicate that GPT-SW3 is a competent model in comparison with existing autoregressive models of similar size. Additionally, we perform an extensive prompting study which reveals the good text generation capabilities of GTP-SW3.</abstract>
      <url hash="3c725190">2022.lrec-1.376</url>
      <bibkey>ekgren-etal-2022-lessons</bibkey>
    </paper>
    <paper id="377">
      <title>Constrained Language Models for Interactive Poem Generation</title>
      <author><first>Andrei</first><last>Popescu-Belis</last></author>
      <author><first>Àlex</first><last>Atrio</last></author>
      <author><first>Valentin</first><last>Minder</last></author>
      <author><first>Aris</first><last>Xanthos</last></author>
      <author><first>Gabriel</first><last>Luthier</last></author>
      <author><first>Simon</first><last>Mattei</last></author>
      <author><first>Antonio</first><last>Rodriguez</last></author>
      <pages>3519–3529</pages>
      <abstract>This paper describes a system for interactive poem generation, which combines neural language models (LMs) for poem generation with explicit constraints that can be set by users on form, topic, emotion, and rhyming scheme. LMs cannot learn such constraints from the data, which is scarce with respect to their needs even for a well-resourced language such as French. We propose a method to generate verses and stanzas by combining LMs with rule-based algorithms, and compare several approaches for adjusting the words of a poem to a desired combination of topics or emotions. An approach to automatic rhyme setting using a phonetic dictionary is proposed as well. Our system has been demonstrated at public events, and log analysis shows that users found it engaging.</abstract>
      <url hash="7949f6b1">2022.lrec-1.377</url>
      <bibkey>popescu-belis-etal-2022-constrained</bibkey>
    </paper>
    <paper id="378">
      <title><fixed-case>ELF</fixed-case>22: A Context-based Counter Trolling Dataset to Combat <fixed-case>I</fixed-case>nternet Trolls</title>
      <author><first>Huije</first><last>Lee</last></author>
      <author><first>Young Ju</first><last>Na</last></author>
      <author><first>Hoyun</first><last>Song</last></author>
      <author><first>Jisu</first><last>Shin</last></author>
      <author><first>Jong</first><last>Park</last></author>
      <pages>3530–3541</pages>
      <abstract>Online trolls increase social costs and cause psychological damage to individuals. With the proliferation of automated accounts making use of bots for trolling, it is difficult for targeted individual users to handle the situation both quantitatively and qualitatively. To address this issue, we focus on automating the method to counter trolls, as counter responses to combat trolls encourage community users to maintain ongoing discussion without compromising freedom of expression. For this purpose, we propose a novel dataset for automatic counter response generation. In particular, we constructed a pair-wise dataset that includes troll comments and counter responses with labeled response strategies, which enables models fine-tuned on our dataset to generate responses by varying counter responses according to the specified strategy. We conducted three tasks to assess the effectiveness of our dataset and evaluated the results through both automatic and human evaluation. In human evaluation, we demonstrate that the model fine-tuned with our dataset shows a significantly improved performance in strategy-controlled sentence generation.</abstract>
      <url hash="fe551ba4">2022.lrec-1.378</url>
      <bibkey>lee-etal-2022-elf22</bibkey>
      <pwccode url="https://github.com/huijelee/elf22" additional="false">huijelee/elf22</pwccode>
    </paper>
    <paper id="379">
      <title>Generating Textual Explanations for Machine Learning Models Performance: A Table-to-Text Task</title>
      <author><first>Isaac</first><last>Ampomah</last></author>
      <author><first>James</first><last>Burton</last></author>
      <author><first>Amir</first><last>Enshaei</last></author>
      <author><first>Noura</first><last>Al Moubayed</last></author>
      <pages>3542–3551</pages>
      <abstract>Numerical tables are widely employed to communicate or report the classification performance of machine learning (ML) models with respect to a set of evaluation metrics. For non-experts, domain knowledge is required to fully understand and interpret the information presented by numerical tables. This paper proposes a new natural language generation (NLG) task where neural models are trained to generate textual explanations, analytically describing the classification performance of ML models based on the metrics’ scores reported in the tables. Presenting the generated texts along with the numerical tables will allow for a better understanding of the classification performance of ML models. We constructed a dataset comprising numerical tables paired with their corresponding textual explanations written by experts to facilitate this NLG task. Experiments on the dataset are conducted by fine-tuning pre-trained language models (T5 and BART) to generate analytical textual explanations conditioned on the information in the tables. Furthermore, we propose a neural module, Metrics Processing Unit (MPU), to improve the performance of the baselines in terms of correctly verbalising the information in the corresponding table. Evaluation and analysis conducted indicate, that exploring pre-trained models for data-to-text generation leads to better generalisation performance and can produce high-quality textual explanations.</abstract>
      <url hash="900e9723">2022.lrec-1.379</url>
      <bibkey>ampomah-etal-2022-generating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/e2e">E2E</pwcdataset>
    </paper>
    <paper id="380">
      <title>Barch: an <fixed-case>E</fixed-case>nglish Dataset of Bar Chart Summaries</title>
      <author><first>Iza</first><last>Škrjanec</last></author>
      <author><first>Muhammad Salman</first><last>Edhi</last></author>
      <author><first>Vera</first><last>Demberg</last></author>
      <pages>3552–3560</pages>
      <abstract>We present Barch, a new English dataset of human-written summaries describing bar charts. This dataset contains 47 charts based on a selection of 18 topics. Each chart is associated with one of the four intended messages expressed in the chart title. Using crowdsourcing, we collected around 20 summaries per chart, or one thousand in total. The text of the summaries is aligned with the chart data as well as with analytical inferences about the data drawn by humans. Our datasets is one of the first to explore the effect of intended messages on the data descriptions in chart summaries. Additionally, it lends itself well to the task of training data-driven systems for chart-to-text generation. We provide results on the performance of state-of-the-art neural generation models trained on this dataset and discuss the strengths and shortcomings of different models.</abstract>
      <url hash="b07217d2">2022.lrec-1.380</url>
      <bibkey>skrjanec-etal-2022-barch</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/autochart">AutoChart</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/chart2text">Chart2Text</pwcdataset>
    </paper>
    <paper id="381">
      <title>Effectiveness of Data Augmentation and Pretraining for Improving Neural Headline Generation in Low-Resource Settings</title>
      <author><first>Matej</first><last>Martinc</last></author>
      <author><first>Syrielle</first><last>Montariol</last></author>
      <author><first>Lidia</first><last>Pivovarova</last></author>
      <author><first>Elaine</first><last>Zosa</last></author>
      <pages>3561–3570</pages>
      <abstract>We tackle the problem of neural headline generation in a low-resource setting, where only limited amount of data is available to train a model. We compare the ideal high-resource scenario on English with results obtained on a smaller subset of the same data and also run experiments on two small news corpora covering low-resource languages, Croatian and Estonian. Two options for headline generation in a multilingual low-resource scenario are investigated: a pretrained multilingual encoder-decoder model and a combination of two pretrained language models, one used as an encoder and the other as a decoder, connected with a cross-attention layer that needs to be trained from scratch. The results show that the first approach outperforms the second one by a large margin. We explore several data augmentation and pretraining strategies in order to improve the performance of both models and show that while we can drastically improve the second approach using these strategies, they have little to no effect on the performance of the pretrained encoder-decoder model. Finally, we propose two new measures for evaluating the performance of the models besides the classic ROUGE scores.</abstract>
      <url hash="f0f6e88e">2022.lrec-1.381</url>
      <bibkey>martinc-etal-2022-effectiveness</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/kptimes">KPTimes</pwcdataset>
    </paper>
    <paper id="382">
      <title>Effectiveness of <fixed-case>F</fixed-case>rench Language Models on Abstractive Dialogue Summarization Task</title>
      <author><first>Yongxin</first><last>Zhou</last></author>
      <author><first>François</first><last>Portet</last></author>
      <author><first>Fabien</first><last>Ringeval</last></author>
      <pages>3571–3581</pages>
      <abstract>Pre-trained language models have established the state-of-the-art on various natural language processing tasks, including dialogue summarization, which allows the reader to quickly access key information from long conversations in meetings, interviews or phone calls. However, such dialogues are still difficult to handle with current models because the spontaneity of the language involves expressions that are rarely present in the corpora used for pre-training the language models. Moreover, the vast majority of the work accomplished in this field has been focused on English. In this work, we present a study on the summarization of spontaneous oral dialogues in French using several language specific pre-trained models: BARThez, and BelGPT-2, as well as multilingual pre-trained models: mBART, mBARThez, and mT5. Experiments were performed on the DECODA (Call Center) dialogue corpus whose task is to generate abstractive synopses from call center conversations between a caller and one or several agents depending on the situation. Results show that the BARThez models offer the best performance far above the previous state-of-the-art on DECODA. We further discuss the limits of such pre-trained models and the challenges that must be addressed for summarizing spontaneous dialogues.</abstract>
      <url hash="a3d16d8d">2022.lrec-1.382</url>
      <bibkey>zhou-etal-2022-effectiveness</bibkey>
    </paper>
    <paper id="383">
      <title><fixed-case>ALEXSIS</fixed-case>: A Dataset for Lexical Simplification in <fixed-case>S</fixed-case>panish</title>
      <author><first>Daniel</first><last>Ferrés</last></author>
      <author><first>Horacio</first><last>Saggion</last></author>
      <pages>3582–3594</pages>
      <abstract>Lexical Simplification is the process of reducing the lexical complexity of a text by replacing difficult words with easier to read (or understand) expressions while preserving the original information and meaning. In this paper we introduce ALEXSIS, a new dataset for this task, and we use ALEXSIS to benchmark Lexical Simplification systems in Spanish. The paper describes the evaluation of three kind of approaches to Lexical Simplification, a thesaurus-based approach, a single transformers-based approach, and a combination of transformers. We also report state of the art results on a previous Lexical Simplification dataset for Spanish.</abstract>
      <url hash="b32c640d">2022.lrec-1.383</url>
      <bibkey>ferres-saggion-2022-alexsis</bibkey>
      <pwccode url="https://github.com/lastus-taln-upf/alexsis" additional="false">lastus-taln-upf/alexsis</pwccode>
    </paper>
    <paper id="384">
      <title>The <fixed-case>IARPA</fixed-case> <fixed-case>BETTER</fixed-case> Program Abstract Task Four New Semantically Annotated Corpora from <fixed-case>IARPA</fixed-case>’s <fixed-case>BETTER</fixed-case> Program</title>
      <author><first>Timothy</first><last>Mckinnon</last></author>
      <author><first>Carl</first><last>Rubino</last></author>
      <pages>3595–3600</pages>
      <abstract>IARPA’s Better Extraction from Text Towards Enhanced Retrieval (BETTER) Program created multiple multilingual datasets to spawn and evaluate cross-language information extraction and information retrieval research and development in zero-shot conditions. The first set of these resources for information extraction, the “Abstract” data will be released to the public at LREC 2022 in four languages to champion further information extraction work in this area. This paper presents the event and argument annotation in the Abstract Evaluation phase of BETTER, as well as the data collection, preparation, partitioning and mark-up of the datasets.</abstract>
      <url hash="a3f49785">2022.lrec-1.384</url>
      <bibkey>mckinnon-rubino-2022-iarpa</bibkey>
    </paper>
    <paper id="385">
      <title>A Named Entity Recognition Corpus for <fixed-case>V</fixed-case>ietnamese Biomedical Texts to Support Tuberculosis Treatment</title>
      <author><first>Uyen</first><last>Phan</last></author>
      <author><first>Phuong N.V</first><last>Nguyen</last></author>
      <author><first>Nhung</first><last>Nguyen</last></author>
      <pages>3601–3609</pages>
      <abstract>Named Entity Recognition (NER) is an important task in information extraction. However, due to the lack of labelled corpora, biomedical NER has scarcely been studied in Vietnamese compared to English. To address this situation, we have constructed VietBioNER, a labelled NER corpus of Vietnamese academic biomedical text. The corpus focuses specifically on supporting tuberculosis surveillance, and was constructed by collecting scientific papers and grey literature related to tuberculosis symptoms and diagnostics. We manually annotated a small set of the collected documents with five categories of named entities: Organisation, Location, Date and Time, Symptom and Disease, and Diagnostic Procedure. Inter-annotator agreement ranges from 70.59% and 95.89% F-score according to entity category. In this paper, we make available two splits of the corpus, corresponding to traditional supervised learning and few-shot learning settings. We also provide baseline results for both of these settings, in addition to a dictionary-based approach, as a means to stimulate further research into Vietnamese biomedical NER. Although supervised methods produce results that are far superior to the other two approaches, the fact that even one-shot learning can outperform the dictionary-based method provides evidence that further research into few-shot learning on this text type would be worthwhile.</abstract>
      <url hash="181b3ebc">2022.lrec-1.385</url>
      <bibkey>phan-etal-2022-named</bibkey>
      <pwccode url="https://github.com/ptpuyen1511/vietbioner" additional="false">ptpuyen1511/vietbioner</pwccode>
    </paper>
    <paper id="386">
      <title><fixed-case>R</fixed-case>a<fixed-case>F</fixed-case>o<fixed-case>L</fixed-case>a: A Rationale-Annotated Corpus for Detecting Indicators of Forced Labour</title>
      <author><first>Erick</first><last>Mendez Guzman</last></author>
      <author><first>Viktor</first><last>Schlegel</last></author>
      <author><first>Riza</first><last>Batista-Navarro</last></author>
      <pages>3610–3625</pages>
      <abstract>Forced labour is the most common type of modern slavery, and it is increasingly gaining the attention of the research and social community. Recent studies suggest that artificial intelligence (AI) holds immense potential for augmenting anti-slavery action. However, AI tools need to be developed transparently in cooperation with different stakeholders. Such tools are contingent on the availability and access to domain-specific data, which are scarce due to the near-invisible nature of forced labour. To the best of our knowledge, this paper presents the first openly accessible English corpus annotated for multi-class and multi-label forced labour detection. The corpus consists of 989 news articles retrieved from specialised data sources and annotated according to risk indicators defined by the International Labour Organization (ILO). Each news article was annotated for two aspects: (1) indicators of forced labour as classification labels and (2) snippets of the text that justify labelling decisions. We hope that our data set can help promote research on explainability for multi-class and multi-label text classification. In this work, we explain our process for collecting the data underpinning the proposed corpus, describe our annotation guidelines and present some statistical analysis of its content. Finally, we summarise the results of baseline experiments based on different variants of the Bidirectional Encoder Representation from Transformer (BERT) model.</abstract>
      <url hash="f750b998">2022.lrec-1.386</url>
      <bibkey>mendez-guzman-etal-2022-rafola</bibkey>
    </paper>
    <paper id="387">
      <title>Wojood: Nested <fixed-case>A</fixed-case>rabic Named Entity Corpus and Recognition using <fixed-case>BERT</fixed-case></title>
      <author><first>Mustafa</first><last>Jarrar</last></author>
      <author><first>Mohammed</first><last>Khalilia</last></author>
      <author><first>Sana</first><last>Ghanem</last></author>
      <pages>3626–3636</pages>
      <abstract>This paper presents Wojood, a corpus for Arabic nested Named Entity Recognition (NER). Nested entities occur when one entity mention is embedded inside another entity mention. Wojood consists of about 550K Modern Standard Arabic (MSA) and dialect tokens that are manually annotated with 21 entity types including person, organization, location, event and date. More importantly, the corpus is annotated with nested entities instead of the more common flat annotations. The data contains about 75K entities and 22.5% of which are nested. The inter-annotator evaluation of the corpus demonstrated a strong agreement with Cohen’s Kappa of 0.979 and an F1-score of 0.976. To validate our data, we used the corpus to train a nested NER model based on multi-task learning using the pre-trained AraBERT (Arabic BERT). The model achieved an overall micro F1-score of 0.884. Our corpus, the annotation guidelines, the source code and the pre-trained model are publicly available.</abstract>
      <url hash="ed9708b8">2022.lrec-1.387</url>
      <bibkey>jarrar-etal-2022-wojood</bibkey>
      <pwccode url="https://github.com/SinaLab/ArabicNER" additional="false">SinaLab/ArabicNER</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/nne">NNE</pwcdataset>
    </paper>
    <paper id="388">
      <title>Cross-lingual Approaches for the Detection of Adverse Drug Reactions in <fixed-case>G</fixed-case>erman from a Patient’s Perspective</title>
      <author><first>Lisa</first><last>Raithel</last></author>
      <author><first>Philippe</first><last>Thomas</last></author>
      <author><first>Roland</first><last>Roller</last></author>
      <author><first>Oliver</first><last>Sapina</last></author>
      <author><first>Sebastian</first><last>Möller</last></author>
      <author><first>Pierre</first><last>Zweigenbaum</last></author>
      <pages>3637–3649</pages>
      <abstract>In this work, we present the first corpus for German Adverse Drug Reaction (ADR) detection in patient-generated content. The data consists of 4,169 binary annotated documents from a German patient forum, where users talk about health issues and get advice from medical doctors. As is common in social media data in this domain, the class labels of the corpus are very imbalanced. This and a high topic imbalance make it a very challenging dataset, since often, the same symptom can have several causes and is not always related to a medication intake. We aim to encourage further multi-lingual efforts in the domain of ADR detection and provide preliminary experiments for binary classification using different methods of zero- and few-shot learning based on a multi-lingual model. When fine-tuning XLM-RoBERTa first on English patient forum data and then on the new German data, we achieve an F1-score of 37.52 for the positive class. We make the dataset and models publicly available for the community.</abstract>
      <url hash="34de7366">2022.lrec-1.388</url>
      <bibkey>raithel-etal-2022-cross</bibkey>
      <pwccode url="https://github.com/dfki-nlp/cross-ling-adr" additional="false">dfki-nlp/cross-ling-adr</pwccode>
    </paper>
    <paper id="389">
      <title><fixed-case>GGPONC</fixed-case> 2.0 - The <fixed-case>G</fixed-case>erman Clinical Guideline Corpus for Oncology: Curation Workflow, Annotation Policy, Baseline <fixed-case>NER</fixed-case> Taggers</title>
      <author><first>Florian</first><last>Borchert</last></author>
      <author><first>Christina</first><last>Lohr</last></author>
      <author><first>Luise</first><last>Modersohn</last></author>
      <author><first>Jonas</first><last>Witt</last></author>
      <author><first>Thomas</first><last>Langer</last></author>
      <author><first>Markus</first><last>Follmann</last></author>
      <author><first>Matthias</first><last>Gietzelt</last></author>
      <author><first>Bert</first><last>Arnrich</last></author>
      <author><first>Udo</first><last>Hahn</last></author>
      <author><first>Matthieu-P.</first><last>Schapranow</last></author>
      <pages>3650–3660</pages>
      <abstract>Despite remarkable advances in the development of language resources over the recent years, there is still a shortage of annotated, publicly available corpora covering (German) medical language. With the initial release of the German Guideline Program in Oncology NLP Corpus (GGPONC), we have demonstrated how such corpora can be built upon clinical guidelines, a widely available resource in many natural languages with a reasonable coverage of medical terminology. In this work, we describe a major new release for GGPONC. The corpus has been substantially extended in size and re-annotated with a new annotation scheme based on SNOMED CT top level hierarchies, reaching high inter-annotator agreement (γ=.94). Moreover, we annotated elliptical coordinated noun phrases and their resolutions, a common language phenomenon in (not only German) scientific documents. We also trained BERT-based named entity recognition models on this new data set, which achieve high performance on short, coarse-grained entity spans (F1=.89), while the rate of boundary errors increases for long entity spans. GGPONC is freely available through a data use agreement. The trained named entity recognition models, as well as the detailed annotation guide, are also made publicly available.</abstract>
      <url hash="d364f2e8">2022.lrec-1.389</url>
      <bibkey>borchert-etal-2022-ggponc</bibkey>
    </paper>
    <paper id="390">
      <title><fixed-case>C</fixed-case>lin<fixed-case>IDM</fixed-case>ap: Towards a Clinical <fixed-case>ID</fixed-case>s Mapping for Data Interoperability</title>
      <author><first>Elena</first><last>Zotova</last></author>
      <author><first>Montse</first><last>Cuadros</last></author>
      <author><first>German</first><last>Rigau</last></author>
      <pages>3661–3669</pages>
      <abstract>This paper presents ClinIDMap, a tool for mapping identifiers between clinical ontologies and lexical resources. ClinIDMap interlinks identifiers from UMLS, SMOMED-CT, ICD-10 and the corresponding Wikipedia articles for concepts from the UMLS Metathesaurus. Our main goal is to provide semantic interoperability across the clinical concepts from various knowledge bases. As a side effect, the mapping enriches already annotated corpora in multiple languages with new labels. For instance, spans manually annotated with IDs from UMLS can be annotated with Semantic Types and Groups, and its corresponding SNOMED CT and ICD-10 IDs. We also experiment with sequence labelling models for detecting Diagnosis and Procedures concepts and for detecting UMLS Semantic Groups trained on Spanish, English, and bilingual corpora obtained with the new mapping procedure. The ClinIDMap tool is publicly available.</abstract>
      <url hash="1e203c3c">2022.lrec-1.390</url>
      <bibkey>zotova-etal-2022-clinidmap</bibkey>
      <pwccode url="https://github.com/vicomtech/clinidmap" additional="false">vicomtech/clinidmap</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/medmentions">MedMentions</pwcdataset>
    </paper>
    <paper id="391">
      <title>Identifying Draft Bills Impacting Existing Legislation: a Case Study on <fixed-case>R</fixed-case>omanian</title>
      <author><first>Corina</first><last>Ceausu</last></author>
      <author><first>Sergiu</first><last>Nisioi</last></author>
      <pages>3670–3674</pages>
      <abstract>In our paper, we present a novel corpus of historical legal documents on the Romanian public procurement legislation and an annotated subset of draft bills that have been screened by legal experts and identified as impacting past public procurement legislation. Using the manual annotations provided by the experts, we attempt to automatically identify future draft bills that have the potential to impact existing policies on public procurement.</abstract>
      <url hash="1fad2061">2022.lrec-1.391</url>
      <bibkey>ceausu-nisioi-2022-identifying</bibkey>
    </paper>
    <paper id="392">
      <title><fixed-case>M</fixed-case>u<fixed-case>LD</fixed-case>: The Multitask Long Document Benchmark</title>
      <author><first>George</first><last>Hudson</last></author>
      <author><first>Noura</first><last>Al Moubayed</last></author>
      <pages>3675–3685</pages>
      <abstract>The impressive progress in NLP techniques has been driven by the development of multi-task benchmarks such as GLUE and SuperGLUE. While these benchmarks focus on tasks for one or two input sentences, there has been exciting work in designing efficient techniques for processing much longer inputs. In this paper, we present MuLD: a new long document benchmark consisting of only documents over 10,000 tokens. By modifying existing NLP tasks, we create a diverse benchmark which requires models to successfully model long-term dependencies in the text. We evaluate how existing models perform, and find that our benchmark is much more challenging than their ‘short document’ equivalents. Furthermore, by evaluating both regular and efficient transformers, we show that models with increased context length are better able to solve the tasks presented, suggesting that future improvements in these models are vital for solving similar long document problems. We release the data and code for baselines to encourage further research on efficient NLP models.</abstract>
      <url hash="06923cf3">2022.lrec-1.392</url>
      <bibkey>hudson-al-moubayed-2022-muld</bibkey>
      <pwccode url="https://github.com/ghomashudson/muld" additional="false">ghomashudson/muld</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/muld">MuLD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/narrativeqa">NarrativeQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/quality">QuALITY</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/scrolls">SCROLLS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="393">
      <title>A Cross-document Coreference Dataset for Longitudinal Tracking across Radiology Reports</title>
      <author><first>Surabhi</first><last>Datta</last></author>
      <author><first>Hio Cheng</first><last>Lam</last></author>
      <author><first>Atieh</first><last>Pajouhi</last></author>
      <author><first>Sunitha</first><last>Mogalla</last></author>
      <author><first>Kirk</first><last>Roberts</last></author>
      <pages>3686–3695</pages>
      <abstract>This paper proposes a new cross-document coreference resolution (CDCR) dataset for identifying co-referring radiological findings and medical devices across a patient’s radiology reports. Our annotated corpus contains 5872 mentions (findings and devices) spanning 638 MIMIC-III radiology reports across 60 patients, covering multiple imaging modalities and anatomies. There are a total of 2292 mention chains. We describe the annotation process in detail, highlighting the complexities involved in creating a sizable and realistic dataset for radiology CDCR. We apply two baseline methods–string matching and transformer language models (BERT)–to identify cross-report coreferences. Our results indicate the requirement of further model development targeting better understanding of domain language and context to address this challenging and unexplored task. This dataset can serve as a resource to develop more advanced natural language processing CDCR methods in the future. This is one of the first attempts focusing on CDCR in the clinical domain and holds potential in benefiting physicians and clinical research through long-term tracking of radiology findings.</abstract>
      <url hash="f7fb7672">2022.lrec-1.393</url>
      <bibkey>datta-etal-2022-cross</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mimic-iii">MIMIC-III</pwcdataset>
    </paper>
    <paper id="394">
      <title>How’s Business Going Worldwide ? A Multilingual Annotated Corpus for Business Relation Extraction</title>
      <author><first>Hadjer</first><last>Khaldi</last></author>
      <author><first>Farah</first><last>Benamara</last></author>
      <author><first>Camille</first><last>Pradel</last></author>
      <author><first>Grégoire</first><last>Sigel</last></author>
      <author><first>Nathalie</first><last>Aussenac-Gilles</last></author>
      <pages>3696–3705</pages>
      <abstract>The business world has changed due to the 21st century economy, where borders have melted and trades became free. Nowadays,competition is no longer only at the local market level but also at the global level. In this context, the World Wide Web has become a major source of information for companies and professionals to keep track of their complex, rapidly changing, and competitive business environment. A lot of effort is nonetheless needed to collect and analyze this information due to information overload problem and the huge number of web pages to process and analyze. In this paper, we propose the BizRel resource, the first multilingual (French,English, Spanish, and Chinese) dataset for automatic extraction of binary business relations involving organizations from the web.This dataset is used to train several monolingual and cross-lingual deep learning models to detect these relations in texts. Our results are encouraging, demonstrating the effectiveness of such a resource for both research and business communities. In particular, we believe multilingual business relation extraction systems are crucial tools for decision makers to identify links between specific market stakeholders and build business networks which enable to anticipate changes and discover new threats or opportunities. Our work is therefore an important direction toward such tools.</abstract>
      <url hash="f64b8fbf">2022.lrec-1.394</url>
      <bibkey>khaldi-etal-2022-hows</bibkey>
    </paper>
    <paper id="395">
      <title>Do Transformer Networks Improve the Discovery of Rules from Text?</title>
      <author><first>Mahdi</first><last>Rahimi</last></author>
      <author><first>Mihai</first><last>Surdeanu</last></author>
      <pages>3706–3714</pages>
      <abstract>With their Discovery of Inference Rules from Text (DIRT) algorithm, Lin and Pantel (2001) made a seminal contribution to the field of rule acquisition from text, by adapting the distributional hypothesis of Harris (1954) to rules that model binary relations such as X treat Y. DIRT’s relevance is renewed in today’s neural era given the recent focus on interpretability in the field of natural language processing. We propose a novel take on the DIRT algorithm, where we implement the distributional hypothesis using the contextualized embeddings provided by BERT, a transformer-network-based language model (Vaswani et al. 2017; Devlin et al. 2018). In particular, we change the similarity measure between pairs of slots (i.e., the set of words matched by a rule) from the original formula that relies on lexical items to a formula computed using contextualized embeddings. We empirically demonstrate that this new similarity method yields a better implementation of the distributional hypothesis, and this, in turn, yields rules that outperform the original algorithm in the question answering-based evaluation proposed by Lin and Pantel (2001).</abstract>
      <url hash="4d9da47c">2022.lrec-1.395</url>
      <bibkey>rahimi-surdeanu-2022-transformer</bibkey>
    </paper>
    <paper id="396">
      <title>Offensive language detection in <fixed-case>H</fixed-case>ebrew: can other languages help?</title>
      <author><first>Marina</first><last>Litvak</last></author>
      <author><first>Natalia</first><last>Vanetik</last></author>
      <author><first>Chaya</first><last>Liebeskind</last></author>
      <author><first>Omar</first><last>Hmdia</last></author>
      <author><first>Rizek Abu</first><last>Madeghem</last></author>
      <pages>3715–3723</pages>
      <abstract>Unfortunately, offensive language in social media is a common phenomenon nowadays. It harms many people and vulnerable groups. Therefore, automated detection of offensive language is in high demand and it is a serious challenge in multilingual domains. Various machine learning approaches combined with natural language techniques have been applied for this task lately. This paper contributes to this area from several aspects: (1) it introduces a new dataset of annotated Facebook comments in Hebrew; (2) it describes a case study with multiple supervised models and text representations for a task of offensive language detection in three languages, including two Semitic (Hebrew and Arabic) languages; (3) it reports evaluation results of cross-lingual and multilingual learning for detection of offensive content in Semitic languages; and (4) it discusses the limitations of these settings.</abstract>
      <url hash="2fa538de">2022.lrec-1.396</url>
      <bibkey>litvak-etal-2022-offensive</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/olid">OLID</pwcdataset>
    </paper>
    <paper id="397">
      <title><fixed-case>J</fixed-case>a<fixed-case>MIE</fixed-case>: A Pipeline <fixed-case>J</fixed-case>apanese Medical Information Extraction System with Novel Relation Annotation</title>
      <author><first>Fei</first><last>Cheng</last></author>
      <author><first>Shuntaro</first><last>Yada</last></author>
      <author><first>Ribeka</first><last>Tanaka</last></author>
      <author><first>Eiji</first><last>Aramaki</last></author>
      <author><first>Sadao</first><last>Kurohashi</last></author>
      <pages>3724–3731</pages>
      <abstract>In the field of Japanese medical information extraction, few analyzing tools are available and relation extraction is still an under-explored topic. In this paper, we first propose a novel relation annotation schema for investigating the medical and temporal relations between medical entities in Japanese medical reports. We experiment with the practical annotation scenarios by separately annotating two different types of reports. We design a pipeline system with three components for recognizing medical entities, classifying entity modalities, and extracting relations. The empirical results show accurate analyzing performance and suggest the satisfactory annotation quality, the superiority of the latest contextual embedding models. and the feasible annotation strategy for high-accuracy demand.</abstract>
      <url hash="0ec02ae5">2022.lrec-1.397</url>
      <bibkey>cheng-etal-2022-jamie</bibkey>
    </paper>
    <paper id="398">
      <title>Enhanced Entity Annotations for Multilingual Corpora</title>
      <author><first>Michael</first><last>Strobl</last></author>
      <author><first>Amine</first><last>Trabelsi</last></author>
      <author><first>Osmar</first><last>Zaïane</last></author>
      <pages>3732–3740</pages>
      <abstract>Modern approaches in Natural Language Processing (NLP) require, ideally, large amounts of labelled data for model training. However, new language resources, for example, for Named Entity Recognition (NER), Co-reference Resolution (CR), Entity Linking (EL) and Relation Extraction (RE), naming a few of the most popular tasks in NLP, have always been challenging to create since manual text annotations can be very time-consuming to acquire. While there may be an acceptable amount of labelled data available for some of these tasks in one language, there may be a lack of datasets in another. WEXEA is a tool to exhaustively annotate entities in the English Wikipedia. Guidelines for editors of Wikipedia articles result, on the one hand, in only a few annotations through hyperlinks, but on the other hand, make it easier to exhaustively annotate the rest of these articles with entities than starting from scratch. We propose the following main improvements to WEXEA: Creating multi-lingual corpora, improved entity annotations using a proven NER system, annotating dates and times. A brief evaluation of the annotation quality of WEXEA is added.</abstract>
      <url hash="8d9c9927">2022.lrec-1.398</url>
      <bibkey>strobl-etal-2022-enhanced</bibkey>
    </paper>
    <paper id="399">
      <title>Enriching Epidemiological Thematic Features For Disease Surveillance Corpora Classification</title>
      <author><first>Edmond</first><last>Menya</last></author>
      <author><first>Mathieu</first><last>Roche</last></author>
      <author><first>Roberto</first><last>Interdonato</last></author>
      <author><first>Dickson</first><last>Owuor</last></author>
      <pages>3741–3750</pages>
      <abstract>We present EpidBioBERT, a biosurveillance epidemiological document tagger for disease surveillance over PADI-Web system. Our model is trained on PADI-Web corpus which contains news articles on Animal Diseases Outbreak extracted from the web. We train a classifier to discriminate between relevant and irrelevant documents based on their epidemiological thematic feature content in preparation for further epidemiology information extraction. Our approach proposes a new way to perform epidemiological document classification by enriching epidemiological thematic features namely disease, host, location and date, which are used as inputs to our epidemiological document classifier. We adopt a pre-trained biomedical language model with a novel fine tuning approach that enriches these epidemiological thematic features. We find these thematic features rich enough to improve epidemiological document classification over a smaller data set than initially used in PADI-Web classifier. This improves the classifiers ability to avoid false positive alerts on disease surveillance systems. To further understand information encoded in EpidBioBERT, we experiment the impact of each epidemiology thematic feature on the classifier under ablation studies. We compare our biomedical pre-trained approach with a general language model based model finding that thematic feature embeddings pre-trained on general English documents are not rich enough for epidemiology classification task. Our model achieves an F1-score of 95.5% over an unseen test set, with an improvement of +5.5 points on F1-Score on the PADI-Web classifier with nearly half the training data set.</abstract>
      <url hash="77dbb11f">2022.lrec-1.399</url>
      <bibkey>menya-etal-2022-enriching</bibkey>
    </paper>
    <paper id="400">
      <title><fixed-case>S</fixed-case>panish Datasets for Sensitive Entity Detection in the Legal Domain</title>
      <author><first>Ona</first><last>de Gibert Bonet</last></author>
      <author><first>Aitor</first><last>García Pablos</last></author>
      <author><first>Montse</first><last>Cuadros</last></author>
      <author><first>Maite</first><last>Melero</last></author>
      <pages>3751–3760</pages>
      <abstract>The de-identification of sensible data, also known as automatic textual anonymisation, is essential for data sharing and reuse, both for research and commercial purposes. The first step for data anonymisation is the detection of sensible entities. In this work, we present four new datasets for named entity detection in Spanish in the legal domain. These datasets have been generated in the framework of the MAPA project, three smaller datasets have been manually annotated and one large dataset has been automatically annotated, with an estimated error rate of around 14%. In order to assess the quality of the generated datasets, we have used them to fine-tune a battery of entity-detection models, using as foundation different pre-trained language models: one multilingual, two general-domain monolingual and one in-domain monolingual. We compare the results obtained, which validate the datasets as a valuable resource to fine-tune models for the task of named entity detection. We further explore the proposed methodology by applying it to a real use case scenario.</abstract>
      <url hash="54a146e4">2022.lrec-1.400</url>
      <bibkey>de-gibert-bonet-etal-2022-spanish</bibkey>
    </paper>
    <paper id="401">
      <title><fixed-case>C</fixed-case>onv<fixed-case>T</fixed-case>ext<fixed-case>TM</fixed-case>: An Explainable Convolutional Tsetlin Machine Framework for Text Classification</title>
      <author><first>Bimal</first><last>Bhattarai</last></author>
      <author><first>Ole-Christoffer</first><last>Granmo</last></author>
      <author><first>Lei</first><last>Jiao</last></author>
      <pages>3761–3770</pages>
      <abstract>Recent advancements in natural language processing (NLP) have reshaped the industry, with powerful language models such as GPT-3 achieving superhuman performance on various tasks. However, the increasing complexity of such models turns them into “black boxes”, creating uncertainty about their internal operation and decision-making. Tsetlin Machine (TM) employs human-interpretable conjunctive clauses in propositional logic to solve complex pattern recognition problems and has demonstrated competitive performance in various NLP tasks. In this paper, we propose ConvTextTM, a novel convolutional TM architecture for text classification. While legacy TM solutions treat the whole text as a corpus-specific set-of-words (SOW), ConvTextTM breaks down the text into a sequence of text fragments. The convolution over the text fragments opens up for local position-aware analysis. Further, ConvTextTM eliminates the dependency on a corpus-specific vocabulary. Instead, it employs a generic SOW formed by the tokenization scheme of the Bidirectional Encoder Representations from Transformers (BERT). The convolution binds together the tokens, allowing ConvTextTM to address the out-of-vocabulary problem as well as spelling errors. We investigate the local explainability of our proposed method using clause-based features. Extensive experiments are conducted on seven datasets, to demonstrate that the accuracy of ConvTextTM is either superior or comparable to state-of-the-art baselines.</abstract>
      <url hash="f5387a25">2022.lrec-1.401</url>
      <bibkey>bhattarai-etal-2022-convtexttm</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/politifact">PolitiFact</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/web-of-science-dataset">WOS</pwcdataset>
    </paper>
    <paper id="402">
      <title>Elvis vs. <fixed-case>M</fixed-case>. <fixed-case>J</fixed-case>ackson: Who has More Albums? Classification and Identification of Elements in Comparative Questions</title>
      <author><first>Meriem</first><last>Beloucif</last></author>
      <author><first>Seid Muhie</first><last>Yimam</last></author>
      <author><first>Steffen</first><last>Stahlhacke</last></author>
      <author><first>Chris</first><last>Biemann</last></author>
      <pages>3771–3779</pages>
      <abstract>Comparative Question Answering (cQA) is the task of providing concrete and accurate responses to queries such as: “Is Lyft cheaper than a regular taxi?” or “What makes a mortgage different from a regular loan?”. In this paper, we propose two new open-domain real-world datasets for identifying and labeling comparative questions. While the first dataset contains instances of English questions labeled as comparative vs. non-comparative, the second dataset provides additional labels including the objects and the aspects of comparison. We conduct several experiments that evaluate the soundness of our datasets. The evaluation of our datasets using various classifiers show promising results that reach close-to-human results on a binary classification task with a neural model using ALBERT embeddings. When approaching the unsupervised sequence labeling task, some headroom remains.</abstract>
      <url hash="b1e9156c">2022.lrec-1.402</url>
      <bibkey>beloucif-etal-2022-elvis</bibkey>
    </paper>
    <paper id="403">
      <title>Decorate the Examples: A Simple Method of Prompt Design for Biomedical Relation Extraction</title>
      <author><first>Hui-Syuan</first><last>Yeh</last></author>
      <author><first>Thomas</first><last>Lavergne</last></author>
      <author><first>Pierre</first><last>Zweigenbaum</last></author>
      <pages>3780–3787</pages>
      <abstract>Relation extraction is a core problem for natural language processing in the biomedical domain. Recent research on relation extraction showed that prompt-based learning improves the performance on both fine-tuning on full training set and few-shot training. However, less effort has been made on domain-specific tasks where good prompt design can be even harder. In this paper, we investigate prompting for biomedical relation extraction, with experiments on the ChemProt dataset. We present a simple yet effective method to systematically generate comprehensive prompts that reformulate the relation extraction task as a cloze-test task under a simple prompt formulation. In particular, we experiment with different ranking scores for prompt selection. With BioMed-RoBERTa-base, our results show that prompting-based fine-tuning obtains gains by 14.21 F1 over its regular fine-tuning baseline, and 1.14 F1 over SciFive-Large, the current state-of-the-art on ChemProt. Besides, we find prompt-based learning requires fewer training examples to make reasonable predictions. The results demonstrate the potential of our methods in such a domain-specific relation extraction task.</abstract>
      <url hash="95785d0a">2022.lrec-1.403</url>
      <bibkey>yeh-etal-2022-decorate</bibkey>
      <revision id="1" href="2022.lrec-1.403v1" hash="f476e39b"/>
      <revision id="2" href="2022.lrec-1.403v2" hash="95785d0a" date="2022-10-07">Updated abstract and section 4.4.</revision>
    </paper>
    <paper id="404">
      <title>Comparing Annotated Datasets for Named Entity Recognition in <fixed-case>E</fixed-case>nglish Literature</title>
      <author><first>Rositsa</first><last>Ivanova</last></author>
      <author><first>Marieke</first><last>van Erp</last></author>
      <author><first>Sabrina</first><last>Kirrane</last></author>
      <pages>3788–3797</pages>
      <abstract>The growing interest in named entity recognition (NER) in various domains has led to the creation of different benchmark datasets, often with slightly different annotation guidelines. To better understand the different NER benchmark datasets for the domain of English literature and their impact on the evaluation of NER tools, we analyse two existing annotated datasets and create two additional gold standard datasets. Following on from this, we evaluate the performance of two NER tools, one domain-specific and one general-purpose NER tool, using the four gold standards, and analyse the sources for the differences in the measured performance. Our results show that the performance of the two tools varies significantly depending on the gold standard used for the individual evaluations.</abstract>
      <url hash="67ec8d36">2022.lrec-1.404</url>
      <bibkey>ivanova-etal-2022-comparing</bibkey>
      <pwccode url="https://github.com/therosko/annotated_datasets_en_comparisson" additional="false">therosko/annotated_datasets_en_comparisson</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll">CoNLL++</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/litbank">LitBank</pwcdataset>
    </paper>
    <paper id="405">
      <title>Investigating User Radicalization: A Novel Dataset for Identifying Fine-Grained Temporal Shifts in Opinion</title>
      <author><first>Flora</first><last>Sakketou</last></author>
      <author><first>Allison</first><last>Lahnala</last></author>
      <author><first>Liane</first><last>Vogel</last></author>
      <author><first>Lucie</first><last>Flek</last></author>
      <pages>3798–3808</pages>
      <abstract>There is an increasing need for the ability to model fine-grained opinion shifts of social media users, as concerns about the potential polarizing social effects increase. However, the lack of publicly available datasets that are suitable for the task presents a major challenge. In this paper, we introduce an innovative annotated dataset for modeling subtle opinion fluctuations and detecting fine-grained stances. The dataset includes a sufficient amount of stance polarity and intensity labels per user over time and within entire conversational threads, thus making subtle opinion fluctuations detectable both in long term and in short term. All posts are annotated by non-experts and a significant portion of the data is also annotated by experts. We provide a strategy for recruiting suitable non-experts. Our analysis of the inter-annotator agreements shows that the resulting annotations obtained from the majority vote of the non-experts are of comparable quality to the annotations of the experts. We provide analyses of the stance evolution in short term and long term levels, a comparison of language usage between users with vacillating and resolute attitudes, and fine-grained stance detection baselines.</abstract>
      <url hash="b97a41df">2022.lrec-1.405</url>
      <bibkey>sakketou-etal-2022-investigating</bibkey>
      <pwccode url="https://github.com/caisa-lab/spinos-dataset" additional="false">caisa-lab/spinos-dataset</pwccode>
    </paper>
    <paper id="406">
      <title><fixed-case>APPR</fixed-case>eddit: a Corpus of <fixed-case>R</fixed-case>eddit Posts Annotated for Appraisal</title>
      <author><first>Marco Antonio</first><last>Stranisci</last></author>
      <author><first>Simona</first><last>Frenda</last></author>
      <author><first>Eleonora</first><last>Ceccaldi</last></author>
      <author><first>Valerio</first><last>Basile</last></author>
      <author><first>Rossana</first><last>Damiano</last></author>
      <author><first>Viviana</first><last>Patti</last></author>
      <pages>3809–3818</pages>
      <abstract>Despite the large number of computational resources for emotion recognition, there is a lack of data sets relying on appraisal models. According to Appraisal theories, emotions are the outcome of a multi-dimensional evaluation of events. In this paper, we present APPReddit, the first corpus of non-experimental data annotated according to this theory. After describing its development, we compare our resource with enISEAR, a corpus of events created in an experimental setting and annotated for appraisal. Results show that the two corpora can be mapped notwithstanding different typologies of data and annotations schemes. A SVM model trained on APPReddit predicts four appraisal dimensions without significant loss. Merging both corpora in a single training set increases the prediction of 3 out of 4 dimensions. Such findings pave the way to a better performing classification model for appraisal prediction.</abstract>
      <url hash="69f9ffa3">2022.lrec-1.406</url>
      <bibkey>stranisci-etal-2022-appreddit</bibkey>
    </paper>
    <paper id="407">
      <title>Evaluating Methods for Extraction of Aspect Terms in Opinion Texts in <fixed-case>P</fixed-case>ortuguese - the Challenges of Implicit Aspects</title>
      <author><first>Mateus</first><last>Machado</last></author>
      <author><first>Thiago Alexandre Salgueiro</first><last>Pardo</last></author>
      <pages>3819–3828</pages>
      <abstract>One of the challenges of aspect-based sentiment analysis is the implicit mention of aspects. These are more difficult to identify and may require world knowledge to do so. In this work, we evaluate frequency-based, hybrid, and machine learning methods, including the use of the pre-trained BERT language model, in the task of extracting aspect terms in opinionated texts in Portuguese, emphasizing the analysis of implicit aspects. Besides the comparative evaluation of methods, the differential of this work lies in the analysis’s novelty using a typology of implicit aspects that shows the knowledge needed to identify each implicit aspect term, thus allowing a mapping of the strengths and weaknesses of each method.</abstract>
      <url hash="b1feec4d">2022.lrec-1.407</url>
      <bibkey>machado-pardo-2022-evaluating</bibkey>
      <pwccode url="https://github.com/mtarcinalli/lrec-extraction-of-aspect-terms" additional="false">mtarcinalli/lrec-extraction-of-aspect-terms</pwccode>
    </paper>
    <paper id="408">
      <title><fixed-case>S</fixed-case>entic<fixed-case>N</fixed-case>et 7: A Commonsense-based Neurosymbolic <fixed-case>AI</fixed-case> Framework for Explainable Sentiment Analysis</title>
      <author><first>Erik</first><last>Cambria</last></author>
      <author><first>Qian</first><last>Liu</last></author>
      <author><first>Sergio</first><last>Decherchi</last></author>
      <author><first>Frank</first><last>Xing</last></author>
      <author><first>Kenneth</first><last>Kwok</last></author>
      <pages>3829–3839</pages>
      <abstract>In recent years, AI research has demonstrated enormous potential for the benefit of humanity and society. While often better than its human counterparts in classification and pattern recognition tasks, however, AI still struggles with complex tasks that require commonsense reasoning such as natural language understanding. In this context, the key limitations of current AI models are: dependency, reproducibility, trustworthiness, interpretability, and explainability. In this work, we propose a commonsense-based neurosymbolic framework that aims to overcome these issues in the context of sentiment analysis. In particular, we employ unsupervised and reproducible subsymbolic techniques such as auto-regressive language models and kernel methods to build trustworthy symbolic representations that convert natural language to a sort of protolanguage and, hence, extract polarity from text in a completely interpretable and explainable manner.</abstract>
      <url hash="778b600d">2022.lrec-1.408</url>
      <bibkey>cambria-etal-2022-senticnet</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="409">
      <title>Building an Endangered Language Resource in the Classroom: <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies for Kakataibo</title>
      <author><first>Roberto</first><last>Zariquiey</last></author>
      <author><first>Claudia</first><last>Alvarado</last></author>
      <author><first>Ximena</first><last>Echevarría</last></author>
      <author><first>Luisa</first><last>Gomez</last></author>
      <author><first>Rosa</first><last>Gonzales</last></author>
      <author><first>Mariana</first><last>Illescas</last></author>
      <author><first>Sabina</first><last>Oporto</last></author>
      <author><first>Frederic</first><last>Blum</last></author>
      <author><first>Arturo</first><last>Oncevay</last></author>
      <author><first>Javier</first><last>Vera</last></author>
      <pages>3840–3851</pages>
      <abstract>In this paper, we launch a new Universal Dependencies treebank for an endangered language from Amazonia: Kakataibo, a Panoan language spoken in Peru. We first discuss the collaborative methodology implemented, which proved effective to create a treebank in the context of a Computational Linguistic course for undergraduates. Then, we describe the general details of the treebank and the language-specific considerations implemented for the proposed annotation. We finally conduct some experiments on part-of-speech tagging and syntactic dependency parsing. We focus on monolingual and transfer learning settings, where we study the impact of a Shipibo-Konibo treebank, another Panoan language resource.</abstract>
      <url hash="2366acb9">2022.lrec-1.409</url>
      <bibkey>zariquiey-etal-2022-building</bibkey>
      <pwccode url="https://github.com/tarotis/building-an-endangered-language-resource-in-the-classroom" additional="false">tarotis/building-an-endangered-language-resource-in-the-classroom</pwccode>
    </paper>
    <paper id="410">
      <title>The <fixed-case>N</fixed-case>orwegian Colossal Corpus: A Text Corpus for Training Large <fixed-case>N</fixed-case>orwegian Language Models</title>
      <author><first>Per</first><last>Kummervold</last></author>
      <author><first>Freddy</first><last>Wetjen</last></author>
      <author><first>Javier</first><last>de la Rosa</last></author>
      <pages>3852–3860</pages>
      <abstract>Norwegian has been one of many languages lacking sufficient available text to train quality language models. In an attempt to bridge this gap, we introduce the Norwegian Colossal Corpus (NCC), which comprises 49GB of clean Norwegian textual data containing over 7B words. The NCC is composed of different and varied sources, ranging from books and newspapers to government documents and public reports, showcasing the various uses of the Norwegian language in society. The corpus contains mainly Norwegian Bokmål and Norwegian Nynorsk. Each document in the corpus is tagged with metadata that enables the creation of sub-corpora for specific needs. Its structure makes it easy to combine with large web archives that for licensing reasons could not be distributed together with the NCC. By releasing this corpus openly to the public, we hope to foster the creation of both better Norwegian language models and multilingual language models with support for Norwegian.</abstract>
      <url hash="f4d63527">2022.lrec-1.410</url>
      <bibkey>kummervold-etal-2022-norwegian</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mc4">mC4</pwcdataset>
    </paper>
    <paper id="411">
      <title>Embeddings models for Buddhist <fixed-case>S</fixed-case>anskrit</title>
      <author><first>Ligeia</first><last>Lugli</last></author>
      <author><first>Matej</first><last>Martinc</last></author>
      <author><first>Andraž</first><last>Pelicon</last></author>
      <author><first>Senja</first><last>Pollak</last></author>
      <pages>3861–3871</pages>
      <abstract>The paper presents novel resources and experiments for Buddhist Sanskrit, broadly defined here including all the varieties of Sanskrit in which Buddhist texts have been transmitted. We release a novel corpus of Buddhist texts, a novel corpus of general Sanskrit and word similarity and word analogy datasets for intrinsic evaluation of Buddhist Sanskrit embeddings models. We compare the performance of word2vec and fastText static embeddings models, with default and optimized parameter settings, as well as contextual models BERT and GPT-2, with different training regimes (including a transfer learning approach using the general Sanskrit corpus) and different embeddings construction regimes (given the encoder layers). The results show that for semantic similarity the fastText embeddings yield the best results, while for word analogy tasks BERT embeddings work the best. We also show that for contextual models the optimal layer combination for embedding construction is task dependant, and that pretraining the contextual embeddings models on a reference corpus of general Sanskrit is beneficial, which is a promising finding for future development of embeddings for less-resourced languages and domains.</abstract>
      <url hash="951104f5">2022.lrec-1.411</url>
      <bibkey>lugli-etal-2022-embeddings</bibkey>
    </paper>
    <paper id="412">
      <title>Development of Automatic Speech Recognition for the Documentation of <fixed-case>C</fixed-case>ook <fixed-case>I</fixed-case>slands <fixed-case>M</fixed-case>āori</title>
      <author><first>Rolando</first><last>Coto-Solano</last></author>
      <author><first>Sally Akevai</first><last>Nicholas</last></author>
      <author><first>Samiha</first><last>Datta</last></author>
      <author><first>Victoria</first><last>Quint</last></author>
      <author><first>Piripi</first><last>Wills</last></author>
      <author><first>Emma Ngakuravaru</first><last>Powell</last></author>
      <author><first>Liam</first><last>Koka’ua</last></author>
      <author><first>Syed</first><last>Tanveer</last></author>
      <author><first>Isaac</first><last>Feldman</last></author>
      <pages>3872–3882</pages>
      <abstract>This paper describes the process of data processing and training of an automatic speech recognition (ASR) system for Cook Islands Māori (CIM), an Indigenous language spoken by approximately 22,000 people in the South Pacific. We transcribed four hours of speech from adults and elderly speakers of the language and prepared two experiments. First, we trained three ASR systems: one statistical, Kaldi; and two based on Deep Learning, DeepSpeech and XLSR-Wav2Vec2. Wav2Vec2 tied with Kaldi for lowest character error rate (CER=6±1) and was slightly behind in word error rate (WER=23±2 versus WER=18±2 for Kaldi). This provides evidence that Deep Learning ASR systems are reaching the performance of statistical methods on small datasets, and that they can work effectively with extremely low-resource Indigenous languages like CIM. In the second experiment we used Wav2Vec2 to train models with held-out speakers. While the performance decreased (CER=15±7, WER=46±16), the system still showed considerable learning. We intend to use ASR to accelerate the documentation of CIM, using newly transcribed texts to improve the ASR and also generate teaching and language revitalization materials. The trained model is available under a license based on the Kaitiakitanga License, which provides for non-commercial use while retaining control of the model by the Indigenous community.</abstract>
      <url hash="836bea3e">2022.lrec-1.412</url>
      <bibkey>coto-solano-etal-2022-development</bibkey>
    </paper>
    <paper id="413">
      <title>A Generalized Approach to Protest Event Detection in <fixed-case>G</fixed-case>erman Local News</title>
      <author><first>Gregor</first><last>Wiedemann</last></author>
      <author><first>Jan Matti</first><last>Dollbaum</last></author>
      <author><first>Sebastian</first><last>Haunss</last></author>
      <author><first>Priska</first><last>Daphi</last></author>
      <author><first>Larissa Daria</first><last>Meier</last></author>
      <pages>3883–3891</pages>
      <abstract>Protest events provide information about social and political conflicts, the state of social cohesion and democratic conflict management, as well as the state of civil society in general. Social scientists are therefore interested in the systematic observation of protest events. With this paper, we release the first German language resource of protest event related article excerpts published in local news outlets. We use this dataset to train and evaluate transformer-based text classifiers to automatically detect relevant newspaper articles. Our best approach reaches a binary F1-score of 93.3 %, which is a promising result for our goal to support political science research. However, in a second experiment, we show that our model does not generalize equally well when applied to data from time periods and localities other than our training sample. To make protest event detection more robust, we test two ways of alternative preprocessing. First, we find that letting the classifier concentrate on sentences around protest keywords improves the F1-score for out-of-sample data up to +4 percentage points. Second, against our initial intuition, masking of named entities during preprocessing does not improve the generalization in terms of F1-scores. However, it leads to a significantly improved recall of the models.</abstract>
      <url hash="17bae56e">2022.lrec-1.413</url>
      <bibkey>wiedemann-etal-2022-generalized</bibkey>
    </paper>
    <paper id="414">
      <title>Evaluation of Transfer Learning and Domain Adaptation for Analyzing <fixed-case>G</fixed-case>erman-Speaking Job Advertisements</title>
      <author><first>Ann-Sophie</first><last>Gnehm</last></author>
      <author><first>Eva</first><last>Bühlmann</last></author>
      <author><first>Simon</first><last>Clematide</last></author>
      <pages>3892–3901</pages>
      <abstract>This paper presents text mining approaches on German-speaking job advertisements to enable social science research on the development of the labour market over the last 30 years. In order to build text mining applications providing information about profession and main task of a job, as well as experience and ICT skills needed, we experiment with transfer learning and domain adaptation. Our main contribution consists in building language models which are adapted to the domain of job advertisements, and their assessment on a broad range of machine learning problems. Our findings show the large value of domain adaptation in several respects. First, it boosts the performance of fine-tuned task-specific models consistently over all evaluation experiments. Second, it helps to mitigate rapid data shift over time in our special domain, and enhances the ability to learn from small updates with new, labeled task data. Third, domain-adaptation of language models is efficient: With continued in-domain pre-training we are able to outperform general-domain language models pre-trained on ten times more data. We share our domain-adapted language models and data with the research community.</abstract>
      <url hash="c6b894f6">2022.lrec-1.414</url>
      <bibkey>gnehm-etal-2022-evaluation</bibkey>
    </paper>
    <paper id="415">
      <title>Pre-Training Language Models for Identifying Patronizing and Condescending Language: An Analysis</title>
      <author><first>Carla</first><last>Perez Almendros</last></author>
      <author><first>Luis</first><last>Espinosa Anke</last></author>
      <author><first>Steven</first><last>Schockaert</last></author>
      <pages>3902–3911</pages>
      <abstract>Patronizing and Condescending Language (PCL) is a subtle but harmful type of discourse, yet the task of recognizing PCL remains under-studied by the NLP community. Recognizing PCL is challenging because of its subtle nature, because available datasets are limited in size, and because this task often relies on some form of commonsense knowledge. In this paper, we study to what extent PCL detection models can be improved by pre-training them on other, more established NLP tasks. We find that performance gains are indeed possible in this way, in particular when pre-training on tasks focusing on sentiment, harmful language and commonsense morality. In contrast, for tasks focusing on political speech and social justice, no or only very small improvements were witnessed. These findings improve our understanding of the nature of PCL.</abstract>
      <url hash="f79e78e9">2022.lrec-1.415</url>
      <bibkey>perez-almendros-etal-2022-pre</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ethics-1">ETHICS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/stereoset">StereoSet</pwcdataset>
    </paper>
    <paper id="416">
      <title><fixed-case>H</fixed-case>e<fixed-case>LI</fixed-case>-<fixed-case>OTS</fixed-case>, Off-the-shelf Language Identifier for Text</title>
      <author><first>Tommi</first><last>Jauhiainen</last></author>
      <author><first>Heidi</first><last>Jauhiainen</last></author>
      <author><first>Krister</first><last>Lindén</last></author>
      <pages>3912–3922</pages>
      <abstract>This paper introduces HeLI-OTS, an off-the-shelf text language identification tool using the HeLI language identification method. The HeLI-OTS language identifier is equipped with language models for 200 languages and licensed for academic as well as commercial use. We present the HeLI method and its use in our previous research. Then we compare the performance of the HeLI-OTS language identifier with that of fastText on two different data sets, showing that fastText favors the recall of common languages, whereas HeLI-OTS reaches both high recall and high precision for all languages. While introducing existing off-the-shelf language identification tools, we also give a picture of digital humanities-related research that uses such tools. The validity of the results of such research depends on the results given by the language identifier used, and especially for research focusing on the less common languages, the tendency to favor widely used languages might be very detrimental, which Heli-OTS is now able to remedy.</abstract>
      <url hash="f84316ec">2022.lrec-1.416</url>
      <bibkey>jauhiainen-etal-2022-heli</bibkey>
    </paper>
    <paper id="417">
      <title>Towards a Broad Coverage Named Entity Resource: A Data-Efficient Approach for Many Diverse Languages</title>
      <author><first>Silvia</first><last>Severini</last></author>
      <author><first>Ayyoob</first><last>ImaniGooghari</last></author>
      <author><first>Philipp</first><last>Dufter</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <pages>3923–3933</pages>
      <abstract>Parallel corpora are ideal for extracting a multilingual named entity (MNE) resource, i.e., a dataset of names translated into multiple languages. Prior work on extracting MNE datasets from parallel corpora required resources such as large monolingual corpora or word aligners that are unavailable or perform poorly for underresourced languages. We present CLC-BN, a new method for creating an MNE resource, and apply it to the Parallel Bible Corpus, a corpus of more than 1000 languages. CLC-BN learns a neural transliteration model from parallel-corpus statistics, without requiring any other bilingual resources, word aligners, or seed data. Experimental results show that CLC-BN clearly outperforms prior work. We release an MNE resource for 1340 languages and demonstrate its effectiveness in two downstream tasks: knowledge graph augmentation and bilingual lexicon induction.</abstract>
      <url hash="43de353d">2022.lrec-1.417</url>
      <bibkey>severini-etal-2022-towards</bibkey>
    </paper>
    <paper id="418">
      <title>Towards the Construction of a <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et for <fixed-case>O</fixed-case>ld <fixed-case>E</fixed-case>nglish</title>
      <author><first>Fahad</first><last>Khan</last></author>
      <author><first>Francisco J.</first><last>Minaya Gómez</last></author>
      <author><first>Rafael</first><last>Cruz González</last></author>
      <author><first>Harry</first><last>Diakoff</last></author>
      <author><first>Javier E.</first><last>Diaz Vera</last></author>
      <author><first>John P.</first><last>McCrae</last></author>
      <author><first>Ciara</first><last>O’Loughlin</last></author>
      <author><first>William Michael</first><last>Short</last></author>
      <author><first>Sander</first><last>Stolk</last></author>
      <pages>3934–3941</pages>
      <abstract>In this paper we will discuss our preliminary work towards the construction of a WordNet for Old English, taking our inspiration from other similar WN construction projects for ancient languages such as Ancient Greek, Latin and Sanskrit. The Old English WordNet (OldEWN) will build upon this innovative work in a number of different ways which we articulate in the article, most importantly by treateating figurative meaning as a ‘first-class citizen’ in the structuring of the semantic system. From a more practical perspective we will describe our plan to utilize a pre-existing lexicographic resource and the naisc system to automatically compile a provisional version of the WordNet which will then be checked and enriched by Old English experts.</abstract>
      <url hash="52106e06">2022.lrec-1.418</url>
      <bibkey>khan-etal-2022-towards</bibkey>
    </paper>
    <paper id="419">
      <title>A Framenet and Frame Annotator for <fixed-case>G</fixed-case>erman Social Media</title>
      <author><first>Eckhard</first><last>Bick</last></author>
      <pages>3942–3949</pages>
      <abstract>This paper presents PFN-DE, a new, parsing- and annotation-oriented framenet for German, with almost 15,000 frames, covering 11,300 verb lemmas. The resource was developed in the context of a Danish/German social-media study on hate speech and has a strong focus on coverage, robustness and cross-language comparability. A simple annotation scheme for argument roles meshes directly with the output of a syntactic parser, facilitating frame disambiguation through slot-filler conditions based on valency, syntactic function and semantic noun class. We discuss design principles for the framenet and the frame tagger using it, and present statistics for frame and role distribution at both the lexicon (type) and corpus (token) levels. In an evaluation run on Twitter data, the parser-based frame annotator achieved an overall F-score for frame senses of 93.6%.</abstract>
      <url hash="b8df8dfd">2022.lrec-1.419</url>
      <bibkey>bick-2022-framenet</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/framenet">FrameNet</pwcdataset>
    </paper>
    <paper id="420">
      <title>The Robotic Surgery Procedural Framebank</title>
      <author><first>Marco</first><last>Bombieri</last></author>
      <author><first>Marco</first><last>Rospocher</last></author>
      <author><first>Simone Paolo</first><last>Ponzetto</last></author>
      <author><first>Paolo</first><last>Fiorini</last></author>
      <pages>3950–3959</pages>
      <abstract>Robot-Assisted minimally invasive robotic surgery is the gold standard for the surgical treatment of many pathological conditions, and several manuals and academic papers describe how to perform these interventions. These high-quality, often peer-reviewed texts are the main study resource for medical personnel and consequently contain essential procedural domain-specific knowledge. The procedural knowledge therein described could be extracted, e.g., on the basis of semantic parsing models, and used to develop clinical decision support systems or even automation methods for some procedure’s steps. However, natural language understanding algorithms such as, for instance, semantic role labelers have lower efficacy and coverage issues when applied to domain others than those they are typically trained on (i.e., newswire text). To overcome this problem, starting from PropBank frames, we propose a new linguistic resource specific to the robotic-surgery domain, named Robotic Surgery Procedural Framebank (RSPF). We extract from robotic-surgical texts verbs and nouns that describe surgical actions and extend PropBank frames by adding any of new lemmas, frames or role sets required to cover missing lemmas, specific frames describing the surgical significance, or new semantic roles used in procedural surgical language. Our resource is publicly available and can be used to annotate corpora in the surgical domain to train and evaluate Semantic Role Labeling (SRL) systems in a challenging fine-grained domain setting.</abstract>
      <url hash="c478d08f">2022.lrec-1.420</url>
      <bibkey>bombieri-etal-2022-robotic</bibkey>
      <pwccode url="https://gitlab.com/altairlab/robotic-surgery-propositional-bank" additional="false">altairlab/robotic-surgery-propositional-bank</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/framenet">FrameNet</pwcdataset>
    </paper>
    <paper id="421">
      <title>Representing the Toddler Lexicon: Do the Corpus and Semantics Matter?</title>
      <author><first>Jennifer</first><last>Weber</last></author>
      <author><first>Eliana</first><last>Colunga</last></author>
      <pages>3960–3968</pages>
      <abstract>Understanding child language development requires accurately representing children’s lexicons. However, much of the past work modeling children’s vocabulary development has utilized adult-based measures. The present investigation asks whether using corpora that captures the language input of young children more accurately represents children’s vocabulary knowledge. We present a newly-created toddler corpus that incorporates transcripts of child-directed conversations, the text of picture books written for preschoolers, and dialog from G-rated movies to approximate the language input a North American preschooler might hear. We evaluate the utility of the new corpus for modeling children’s vocabulary development by building and analyzing different semantic network models and comparing them to norms based on vocabulary norms for toddlers in this age range. More specifically, the relations between words in our semantic networks were derived from skip-gram neural networks (Word2Vec) trained on our toddler corpus or on Google news. Results revealed that the models built from the toddler corpus were more accurate at predicting toddler vocabulary growth than the adult-based corpus. These results speak to the importance of selecting a corpus that matches the population of interest.</abstract>
      <url hash="a7aa2e02">2022.lrec-1.421</url>
      <bibkey>weber-colunga-2022-representing</bibkey>
    </paper>
    <paper id="422">
      <title>Organizing and Improving a Database of <fixed-case>F</fixed-case>rench Word Formation Using Formal Concept Analysis</title>
      <author><first>Nyoman</first><last>Juniarta</last></author>
      <author><first>Olivier</first><last>Bonami</last></author>
      <author><first>Nabil</first><last>Hathout</last></author>
      <author><first>Fiammetta</first><last>Namer</last></author>
      <author><first>Yannick</first><last>Toussaint</last></author>
      <pages>3969–3976</pages>
      <abstract>We apply Formal Concept Analysis (FCA) to organize and to improve the quality of Démonette2, a French derivational database, through a detection of both missing and spurious derivations in the database. We represent each derivational family as a graph. Given that the subgraph relation exists among derivational families, FCA can group families and represent them in a partially ordered set (poset). This poset is also useful for improving the database. A family is regarded as a possible anomaly (meaning that it may have missing and/or spurious derivations) if its derivational graph is almost, but not completely identical to a large number of other families.</abstract>
      <url hash="2c49ebbb">2022.lrec-1.422</url>
      <bibkey>juniarta-etal-2022-organizing</bibkey>
    </paper>
    <paper id="423">
      <title>Towards a new Ontology for Sign Languages</title>
      <author><first>Thierry</first><last>Declerck</last></author>
      <pages>3977–3983</pages>
      <abstract>We present the current status of a new ontology for representing constitutive elements of Sign Languages (SL). This development emerged from investigations on how to represent multimodal lexical data in the OntoLex-Lemon framework, with the goal to publish such data in the Linguistic Linked Open Data (LLOD) cloud. While studying the literature and various sites dealing with sign languages, we saw the need to harmonise all the data categories (or features) defined and used in those sources, and to organise them in an ontology to which lexical descriptions in OntoLex-Lemon could be linked. We make the code of the first version of this ontology available, so that it can be further developed collaboratively by both the Linked Data and the SL communities</abstract>
      <url hash="9a619b29">2022.lrec-1.423</url>
      <bibkey>declerck-2022-towards</bibkey>
    </paper>
    <paper id="424">
      <title>Towards the Detection of a Semantic Gap in the Chain of Commonsense Knowledge Triples</title>
      <author><first>Yoshihiko</first><last>Hayashi</last></author>
      <pages>3984–3993</pages>
      <abstract>A commonsense knowledge resource organizes common sense that is not necessarily correct all the time, but most people are expected to know or believe. Such knowledge resources have recently been actively built and utilized in artificial intelligence, particularly natural language processing. In this paper, we discuss an important but not significantly discussed the issue of semantic gaps potentially existing in a commonsense knowledge graph and propose a machine learning-based approach to detect a semantic gap that may inhibit the proper chaining of knowledge triples. In order to establish this line of research, we created a pilot dataset from ConceptNet, in which chains consisting of two adjacent triples are sampled, and the validity of each chain is human-annotated. We also devised a few baseline methods for detecting the semantic gaps and compared them in small-scale experiments. Although the experimental results suggest that the detection of semantic gaps may not be a trivial task, we achieved several insights to further push this research direction, including the potential efficacy of sense embeddings and contextualized word representations enabled by a pre-trained language model.</abstract>
      <url hash="a78330d1">2022.lrec-1.424</url>
      <bibkey>hayashi-2022-towards</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
    </paper>
    <paper id="425">
      <title><fixed-case>COPA</fixed-case>-<fixed-case>SSE</fixed-case>: Semi-structured Explanations for Commonsense Reasoning</title>
      <author><first>Ana</first><last>Brassard</last></author>
      <author><first>Benjamin</first><last>Heinzerling</last></author>
      <author><first>Pride</first><last>Kavumba</last></author>
      <author><first>Kentaro</first><last>Inui</last></author>
      <pages>3994–4000</pages>
      <abstract>We present Semi-Structured Explanations for COPA (COPA-SSE), a new crowdsourced dataset of 9,747 semi-structured, English common sense explanations for Choice of Plausible Alternatives (COPA) questions. The explanations are formatted as a set of triple-like common sense statements with ConceptNet relations but freely written concepts. This semi-structured format strikes a balance between the high quality but low coverage of structured data and the lower quality but high coverage of free-form crowdsourcing. Each explanation also includes a set of human-given quality ratings. With their familiar format, the explanations are geared towards commonsense reasoners operating on knowledge graphs and serve as a starting point for ongoing work on improving such systems. The dataset is available at https://github.com/a-brassard/copa-sse.</abstract>
      <url hash="874284fb">2022.lrec-1.425</url>
      <bibkey>brassard-etal-2022-copa</bibkey>
      <pwccode url="https://github.com/a-brassard/copa-sse" additional="false">a-brassard/copa-sse</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/copa-sse">COPA-SSE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/copa">COPA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
    </paper>
    <paper id="426">
      <title><fixed-case>GR</fixed-case>h<fixed-case>OOT</fixed-case>: Ontology of Rhetorical Figures in <fixed-case>G</fixed-case>erman</title>
      <author><first>Ramona</first><last>Kühn</last></author>
      <author><first>Jelena</first><last>Mitrović</last></author>
      <author><first>Michael</first><last>Granitzer</last></author>
      <pages>4001–4010</pages>
      <abstract>GRhOOT, the German RhetOrical OnTology, is a domain ontology of 110 rhetorical figures in the German language. The overall goal of building an ontology of rhetorical figures in German is not only the formal representation of different rhetorical figures, but also allowing for their easier detection, thus improving sentiment analysis, argument mining, detection of hate speech and fake news, machine translation, and many other tasks in which recognition of non-literal language plays an important role. The challenge of building such ontologies lies in classifying the figures and assigning adequate characteristics to group them, while considering their distinctive features. The ontology of rhetorical figures in the Serbian language was used as a basis for our work. Besides transferring and extending the concepts of the Serbian ontology, we ensured completeness and consistency by using description logic and SPARQL queries. Furthermore, we show a decision tree to identify figures and suggest a usage scenario on how the ontology can be utilized to collect and annotate data.</abstract>
      <url hash="daeee747">2022.lrec-1.426</url>
      <bibkey>kuhn-etal-2022-grhoot</bibkey>
      <pwccode url="https://github.com/kuehnram/grhoot-ontology" additional="false">kuehnram/grhoot-ontology</pwccode>
    </paper>
    <paper id="427">
      <title>Querying a Dozen Corpora and a Thousand Years with Fintan</title>
      <author><first>Christian</first><last>Chiarcos</last></author>
      <author><first>Christian</first><last>Fäth</last></author>
      <author><first>Maxim</first><last>Ionov</last></author>
      <pages>4011–4021</pages>
      <abstract>Large-scale diachronic corpus studies covering longer time periods are difficult if more than one corpus are to be consulted and, as a result, different formats and annotation schemas need to be processed and queried in a uniform, comparable and replicable manner. We describes the application of the Flexible Integrated Transformation and Annotation eNgineering (Fintan) platform for studying word order in German using syntactically annotated corpora that represent its entire written history. Focusing on nominal dative and accusative arguments, this study hints at two major phases in the development of scrambling in modern German. Against more recent assumptions, it supports the traditional view that word order flexibility decreased over time, but it also indicates that this was a relatively sharp transition in Early New High German. The successful case study demonstrates the potential of Fintan and the underlying LLOD technology for historical linguistics, linguistic typology and corpus linguistics. The technological contribution of this paper is to demonstrate the applicability of Fintan for querying across heterogeneously annotated corpora, as previously, it had only been applied for transformation tasks. With its focus on quantitative analysis, Fintan is a natural complement for existing multi-layer technologies that focus on query and exploration.</abstract>
      <url hash="2709c216">2022.lrec-1.427</url>
      <bibkey>chiarcos-etal-2022-querying</bibkey>
    </paper>
    <paper id="428">
      <title>The Index <fixed-case>T</fixed-case>homisticus Treebank as Linked Data in the <fixed-case>L</fixed-case>i<fixed-case>L</fixed-case>a Knowledge Base</title>
      <author><first>Francesco</first><last>Mambrini</last></author>
      <author><first>Marco</first><last>Passarotti</last></author>
      <author><first>Giovanni</first><last>Moretti</last></author>
      <author><first>Matteo</first><last>Pellegrini</last></author>
      <pages>4022–4029</pages>
      <abstract>Although the Universal Dependencies initiative today allows for cross-linguistically consistent annotation of morphology and syntax in treebanks for several languages, syntactically annotated corpora are not yet interoperable with many lexical resources that describe properties of the words that occur therein. In order to cope with such limitation, we propose to adopt the principles of the Linguistic Linked Open Data community, to describe and publish dependency treebanks as LLOD. In particular, this paper illustrates the approach pursued in the LiLa Knowledge Base, which enables interoperability between corpora and lexical resources for Latin, to publish as Linguistic Linked Open Data the annotation layers of two versions of a Medieval Latin treebank (the Index Thomisticus Treebank).</abstract>
      <url hash="2337a56f">2022.lrec-1.428</url>
      <bibkey>mambrini-etal-2022-index</bibkey>
    </paper>
    <paper id="429">
      <title>Building a Multilingual Taxonomy of Olfactory Terms with Timestamps</title>
      <author><first>Stefano</first><last>Menini</last></author>
      <author><first>Teresa</first><last>Paccosi</last></author>
      <author><first>Serra Sinem</first><last>Tekiroğlu</last></author>
      <author><first>Sara</first><last>Tonelli</last></author>
      <pages>4030–4039</pages>
      <abstract>Olfactory references play a crucial role in our memory and, more generally, in our experiences, since researchers have shown that smell is the sense that is most directly connected with emotions. Nevertheless, only few works in NLP have tried to capture this sensory dimension from a computational perspective. One of the main challenges is the lack of a systematic and consistent taxonomy of olfactory information, where concepts are organised also in a multi-lingual perspective. WordNet represents a valuable starting point in this direction, which can be semi-automatically extended taking advantage of Google n-grams and of existing language models. In this work we describe the process that has led to the semi-automatic development of a taxonomy for olfactory information in four languages (English, French, German and Italian), detailing the different steps and the intermediate evaluations. Along with being multi-lingual, the taxonomy also encloses temporal marks for olfactory terms thus making it a valuable resource for historical content analysis. The resource has been released and is freely available.</abstract>
      <url hash="c73463ee">2022.lrec-1.429</url>
      <bibkey>menini-etal-2022-building</bibkey>
    </paper>
    <paper id="430">
      <title>Attention Understands Semantic Relations</title>
      <author><first>Anastasia</first><last>Chizhikova</last></author>
      <author><first>Sanzhar</first><last>Murzakhmetov</last></author>
      <author><first>Oleg</first><last>Serikov</last></author>
      <author><first>Tatiana</first><last>Shavrina</last></author>
      <author><first>Mikhail</first><last>Burtsev</last></author>
      <pages>4040–4050</pages>
      <abstract>Today, natural language processing heavily relies on pre-trained large language models. Even though such models are criticized for the poor interpretability, they still yield state-of-the-art solutions for a wide set of very different tasks. While lots of probing studies have been conducted to measure the models’ awareness of grammatical knowledge, semantic probing is less popular. In this work, we introduce the probing pipeline to study the representedness of semantic relations in transformer language models. We show that in this task, attention scores are nearly as expressive as the layers’ output activations, despite their lesser ability to represent surface cues. This supports the hypothesis that attention mechanisms are focusing not only on the syntactic relational information but also on the semantic one.</abstract>
      <url hash="0c49111e">2022.lrec-1.430</url>
      <bibkey>chizhikova-etal-2022-attention</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/docred">DocRED</pwcdataset>
    </paper>
    <paper id="431">
      <title>Analysis of Dialogue in Human-Human Collaboration in <fixed-case>M</fixed-case>inecraft</title>
      <author><first>Takuma</first><last>Ichikawa</last></author>
      <author><first>Ryuichiro</first><last>Higashinaka</last></author>
      <pages>4051–4059</pages>
      <abstract>Recently, many studies have focused on developing dialogue systems that enable collaborative work; however, they rarely focus on creative tasks. Collaboration for creative work, in which humans and systems collaborate to create new value, will be essential for future dialogue systems. In this study, we collected 500 dialogues of human-human collaboration in Minecraft as a basis for developing a dialogue system that enables creative collaborative work. We conceived the Collaborative Garden Task, where two workers interact and collaborate in Minecraft to create a garden, and we collected dialogue, action logs, and subjective evaluations. We also collected third-person evaluations of the gardens and analyzed the relationship between dialogue and collaborative work that received high scores on the subjective and third-person evaluations in order to identify dialogic factors for high-quality collaborative work. We found that two essential aspects in creative collaborative work are performing more processes to ask for and agree on suggestions between workers and agreeing on a particular image of the final product in the early phase of work and then discussing changes and details.</abstract>
      <url hash="06ad5195">2022.lrec-1.431</url>
      <bibkey>ichikawa-higashinaka-2022-analysis</bibkey>
    </paper>
    <paper id="432">
      <title>Data Collection for Empirically Determining the Necessary Information for Smooth Handover in Dialogue</title>
      <author><first>Sanae</first><last>Yamashita</last></author>
      <author><first>Ryuichiro</first><last>Higashinaka</last></author>
      <pages>4060–4068</pages>
      <abstract>Despite recent advances, dialogue systems still struggle to achieve fully autonomous transactions. Therefore, when a system encounters a problem, human operators need to take over the dialogue to complete the transaction. However, it is unclear what information should be presented to the operator when this handover takes place. In this study, we conducted a data collection experiment in which one of two operators talked to a user and switched with the other operator periodically while exchanging notes when the handovers took place. By examining these notes, it is possible to identify the information necessary for handing over the dialogue. We collected 60 dialogues in which two operators switched periodically while performing chat, consultation, and sales tasks in dialogue. We found that adjacency pairs are a useful representation for recording conversation history. In addition, we found that key-value-pair representation is also useful when there are underlying tasks, such as consultation and sales.</abstract>
      <url hash="7e9766bb">2022.lrec-1.432</url>
      <bibkey>yamashita-higashinaka-2022-data</bibkey>
    </paper>
    <paper id="433">
      <title>The slurk Interaction Server Framework: Better Data for Better Dialog Models</title>
      <author><first>Jana</first><last>Götze</last></author>
      <author><first>Maike</first><last>Paetzel-Prüsmann</last></author>
      <author><first>Wencke</first><last>Liermann</last></author>
      <author><first>Tim</first><last>Diekmann</last></author>
      <author><first>David</first><last>Schlangen</last></author>
      <pages>4069–4078</pages>
      <abstract>This paper presents the slurk software, a lightweight interaction server for setting up dialog data collections and running experiments. slurk enables a multitude of settings including text-based, speech and video interaction between two or more humans or humans and bots, and a multimodal display area for presenting shared or private interactive context. The software is implemented in Python with an HTML and JavaScript frontend that can easily be adapted to individual needs. It also provides a setup for pairing participants on common crowdworking platforms such as Amazon Mechanical Turk and some example bot scripts for common interaction scenarios.</abstract>
      <url hash="7c8fa310">2022.lrec-1.433</url>
      <bibkey>gotze-etal-2022-slurk</bibkey>
    </paper>
    <paper id="434">
      <title>Corpus Design for Studying Linguistic Nudges in Human-Computer Spoken Interactions</title>
      <author><first>Natalia</first><last>Kalashnikova</last></author>
      <author><first>Serge</first><last>Pajak</last></author>
      <author><first>Fabrice</first><last>Le Guel</last></author>
      <author><first>Ioana</first><last>Vasilescu</last></author>
      <author><first>Gemma</first><last>Serrano</last></author>
      <author><first>Laurence</first><last>Devillers</last></author>
      <pages>4079–4087</pages>
      <abstract>In this paper, we present the methodology of corpus design that will be used to study the comparison of influence between linguistic nudges with positive or negative influences and three conversational agents: robot, smart speaker, and human. We recruited forty-nine participants to form six groups. The conversational agents first asked the participants about their willingness to adopt five ecological habits and invest time and money in ecological problems. The participants were then asked the same questions but preceded by one linguistic nudge with positive or negative influence. The comparison of standard deviation and mean metrics of differences between these two notes (before the nudge and after) showed that participants were mainly affected by nudges with positive influence, even though several nudges with negative influence decreased the average note. In addition, participants from all groups were willing to spend more money than time on ecological problems. In general, our experiment’s early results suggest that a machine agent can influence participants to the same degree as a human agent. A better understanding of the power of influence of different conversational machines and the potential of influence of nudges of different polarities will lead to the development of ethical norms of human-computer interactions.</abstract>
      <url hash="c7c60e29">2022.lrec-1.434</url>
      <bibkey>kalashnikova-etal-2022-corpus</bibkey>
    </paper>
    <paper id="435">
      <title>Dialogue Corpus Construction Considering Modality and Social Relationships in Building Common Ground</title>
      <author><first>Yuki</first><last>Furuya</last></author>
      <author><first>Koki</first><last>Saito</last></author>
      <author><first>Kosuke</first><last>Ogura</last></author>
      <author><first>Koh</first><last>Mitsuda</last></author>
      <author><first>Ryuichiro</first><last>Higashinaka</last></author>
      <author><first>Kazunori</first><last>Takashio</last></author>
      <pages>4088–4095</pages>
      <abstract>Building common ground with users is essential for dialogue agent systems and robots to interact naturally with people. While a few previous studies have investigated the process of building common ground in human-human dialogue, most of them have been conducted on the basis of text chat. In this study, we constructed a dialogue corpus to investigate the process of building common ground with a particular focus on the modality of dialogue and the social relationship between the participants in the process of building common ground, which are important but have not been investigated in the previous work. The results of our analysis suggest that adding the modality or developing the relationship between workers speeds up the building of common ground. Specifically, regarding the modality, the presence of video rather than only audio may unconsciously facilitate work, and as for the relationship, it is easier to convey information about emotions and turn-taking among friends than in first meetings. These findings and the corpus should prove useful for developing a system to support remote communication.</abstract>
      <url hash="0214ee84">2022.lrec-1.435</url>
      <bibkey>furuya-etal-2022-dialogue</bibkey>
    </paper>
    <paper id="436">
      <title><fixed-case>E</fixed-case>mo<fixed-case>WOZ</fixed-case>: A Large-Scale Corpus and Labelling Scheme for Emotion Recognition in Task-Oriented Dialogue Systems</title>
      <author><first>Shutong</first><last>Feng</last></author>
      <author><first>Nurul</first><last>Lubis</last></author>
      <author><first>Christian</first><last>Geishauser</last></author>
      <author><first>Hsien-chin</first><last>Lin</last></author>
      <author><first>Michael</first><last>Heck</last></author>
      <author><first>Carel</first><last>van Niekerk</last></author>
      <author><first>Milica</first><last>Gasic</last></author>
      <pages>4096–4113</pages>
      <abstract>The ability to recognise emotions lends a conversational artificial intelligence a human touch. While emotions in chit-chat dialogues have received substantial attention, emotions in task-oriented dialogues remain largely unaddressed. This is despite emotions and dialogue success having equally important roles in a natural system. Existing emotion-annotated task-oriented corpora are limited in size, label richness, and public availability, creating a bottleneck for downstream tasks. To lay a foundation for studies on emotions in task-oriented dialogues, we introduce EmoWOZ, a large-scale manually emotion-annotated corpus of task-oriented dialogues. EmoWOZ is based on MultiWOZ, a multi-domain task-oriented dialogue dataset. It contains more than 11K dialogues with more than 83K emotion annotations of user utterances. In addition to Wizard-of-Oz dialogues from MultiWOZ, we collect human-machine dialogues within the same set of domains to sufficiently cover the space of various emotions that can happen during the lifetime of a data-driven dialogue system. To the best of our knowledge, this is the first large-scale open-source corpus of its kind. We propose a novel emotion labelling scheme, which is tailored to task-oriented dialogues. We report a set of experimental results to show the usability of this corpus for emotion recognition and state tracking in task-oriented dialogues.</abstract>
      <url hash="ebb2fadd">2022.lrec-1.436</url>
      <bibkey>feng-etal-2022-emowoz</bibkey>
      <pwccode url="https://gitlab.cs.uni-duesseldorf.de/general/dsml/emowoz-public" additional="false">dsml/emowoz-public</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/emowoz-1">EmoWOZ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/emorynlp">EmoryNLP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/meld">MELD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multiwoz">MultiWOZ</pwcdataset>
    </paper>
    <paper id="437">
      <title>Data Augmentation with Paraphrase Generation and Entity Extraction for Multimodal Dialogue System</title>
      <author><first>Eda</first><last>Okur</last></author>
      <author><first>Saurav</first><last>Sahay</last></author>
      <author><first>Lama</first><last>Nachman</last></author>
      <pages>4114–4125</pages>
      <abstract>Contextually aware intelligent agents are often required to understand the users and their surroundings in real-time. Our goal is to build Artificial Intelligence (AI) systems that can assist children in their learning process. Within such complex frameworks, Spoken Dialogue Systems (SDS) are crucial building blocks to handle efficient task-oriented communication with children in game-based learning settings. We are working towards a multimodal dialogue system for younger kids learning basic math concepts. Our focus is on improving the Natural Language Understanding (NLU) module of the task-oriented SDS pipeline with limited datasets. This work explores the potential benefits of data augmentation with paraphrase generation for the NLU models trained on small task-specific datasets. We also investigate the effects of extracting entities for conceivably further data expansion. We have shown that paraphrasing with model-in-the-loop (MITL) strategies using small seed data is a promising approach yielding improved performance results for the Intent Recognition task.</abstract>
      <url hash="e51d7080">2022.lrec-1.437</url>
      <bibkey>okur-etal-2022-data</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paranmt-50m">PARANMT-50M</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paws">PAWS</pwcdataset>
    </paper>
    <paper id="438">
      <title>Towards Modelling Self-imposed Filter Bubbles in Argumentative Dialogue Systems</title>
      <author><first>Annalena</first><last>Aicher</last></author>
      <author><first>Wolfgang</first><last>Minker</last></author>
      <author><first>Stefan</first><last>Ultes</last></author>
      <pages>4126–4134</pages>
      <abstract>To build a well-founded opinion it is natural for humans to gather and exchange new arguments. Especially when being confronted with an overwhelming amount of information, people tend to focus on only the part of the available information that fits into their current beliefs or convenient opinions. To overcome this “self-imposed filter bubble” (SFB) in the information seeking process, it is crucial to identify influential indicators for the former. Within this paper we propose and investigate indicators for the the user’s SFB, mainly their Reflective User Engagement (RUE), their Personal Relevance (PR) ranking of content-related subtopics as well as their False (FK) and True Knowledge (TK) on the topic. Therefore, we analysed the answers of 202 participants of an online conducted user study, who interacted with our argumentative dialogue system BEA (“Building Engaging Argumentation”). Moreover, also the influence of different input/output modalities (speech/speech and drop-down menu/text) on the interaction with regard to the suggested indicators was investigated.</abstract>
      <url hash="77896566">2022.lrec-1.438</url>
      <bibkey>aicher-etal-2022-towards-modelling</bibkey>
    </paper>
    <paper id="439">
      <title>Telling a Lie: Analyzing the Language of Information and Misinformation during Global Health Events</title>
      <author><first>Ankit</first><last>Aich</last></author>
      <author><first>Natalie</first><last>Parde</last></author>
      <pages>4135–4141</pages>
      <abstract>The COVID-19 pandemic and other global health events are unfortunately excellent environments for the creation and spread of misinformation, and the language associated with health misinformation may be typified by unique patterns and linguistic markers. Allowing health misinformation to spread unchecked can have devastating ripple effects; however, detecting and stopping its spread requires careful analysis of these linguistic characteristics at scale. We analyze prior investigations focusing on health misinformation, associated datasets, and detection of misinformation during health crises. We also introduce a novel dataset designed for analyzing such phenomena, comprised of 2.8 million news articles and social media posts spanning the early 1900s to the present. Our annotation guidelines result in strong agreement between independent annotators. We describe our methods for collecting this data and follow this with a thorough analysis of the themes and linguistic features that appear in information versus misinformation. Finally, we demonstrate a proof-of-concept misinformation detection task to establish dataset validity, achieving a strong performance benchmark (accuracy = 75%; F1 = 0.7).</abstract>
      <url hash="7f99d8b6">2022.lrec-1.439</url>
      <bibkey>aich-parde-2022-telling</bibkey>
      <pwccode url="https://github.com/ankitaich09/misinformation" additional="false">ankitaich09/misinformation</pwccode>
    </paper>
    <paper id="440">
      <title>Misogyny and Aggressiveness Tend to Come Together and Together We Address Them</title>
      <author><first>Arianna</first><last>Muti</last></author>
      <author><first>Francesco</first><last>Fernicola</last></author>
      <author><first>Alberto</first><last>Barrón-Cedeño</last></author>
      <pages>4142–4148</pages>
      <abstract>We target the complementary binary tasks of identifying whether a tweet is misogynous and, if that is the case, whether it is also aggressive. We compare two ways to address these problems: one multi-class model that discriminates between all the classes at once: not misogynous, non aggressive-misogynous and aggressive-misogynous; as well as a cascaded approach where the binary classification is carried out separately (misogynous vs non-misogynous and aggressive vs non-aggressive) and then joined together. For the latter, two training and three testing scenarios are considered. Our models are built on top of AlBERTo and are evaluated on the framework of Evalita’s 2020 shared task on automatic misogyny and aggressiveness identification in Italian tweets. Our cascaded models —including the strong naïve baseline— outperform significantly the top submissions to Evalita, reaching state-of-the-art performance without relying on any external information.</abstract>
      <url hash="4704d9a3">2022.lrec-1.440</url>
      <bibkey>muti-etal-2022-misogyny</bibkey>
    </paper>
    <paper id="441">
      <title>The <fixed-case>C</fixed-case>om<fixed-case>MA</fixed-case> Dataset V0.2: Annotating Aggression and Bias in Multilingual Social Media Discourse</title>
      <author><first>Ritesh</first><last>Kumar</last></author>
      <author><first>Shyam</first><last>Ratan</last></author>
      <author><first>Siddharth</first><last>Singh</last></author>
      <author><first>Enakshi</first><last>Nandi</last></author>
      <author><first>Laishram Niranjana</first><last>Devi</last></author>
      <author><first>Akash</first><last>Bhagat</last></author>
      <author><first>Yogesh</first><last>Dawer</last></author>
      <author><first>Bornini</first><last>Lahiri</last></author>
      <author><first>Akanksha</first><last>Bansal</last></author>
      <author><first>Atul Kr.</first><last>Ojha</last></author>
      <pages>4149–4161</pages>
      <abstract>In this paper, we discuss the development of a multilingual dataset annotated with a hierarchical, fine-grained tagset marking different types of aggression and the “context” in which they occur. The context, here, is defined by the conversational thread in which a specific comment occurs and also the “type” of discursive role that the comment is performing with respect to the previous comment. The initial dataset, being discussed here consists of a total 59,152 annotated comments in four languages - Meitei, Bangla, Hindi, and Indian English - collected from various social media platforms such as YouTube, Facebook, Twitter and Telegram. As is usual on social media websites, a large number of these comments are multilingual, mostly code-mixed with English. The paper gives a detailed description of the tagset being used for annotation and also the process of developing a multi-label, fine-grained tagset that has been used for marking comments with aggression and bias of various kinds including sexism (called gender bias in the tagset), religious intolerance (called communal bias in the tagset), class/caste bias and ethnic/racial bias. We also define and discuss the tags that have been used for marking the different discursive role being performed through the comments, such as attack, defend, etc. Finally, we present a basic statistical analysis of the dataset. The dataset is being incrementally made publicly available on the project website.</abstract>
      <url hash="73b9d979">2022.lrec-1.441</url>
      <bibkey>kumar-etal-2022-comma</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/the-comma-dataset-v0-2">The ComMA Dataset v0.2</pwcdataset>
    </paper>
    <paper id="442">
      <title><fixed-case>Tweet Emotion Dynamics</fixed-case>: Emotion Word Usage in Tweets from <fixed-case>US</fixed-case> and <fixed-case>C</fixed-case>anada</title>
      <author><first>Krishnapriya</first><last>Vishnubhotla</last></author>
      <author><first>Saif M.</first><last>Mohammad</last></author>
      <pages>4162–4176</pages>
      <abstract>Over the last decade, Twitter has emerged as one of the most influential forums for social, political, and health discourse. In this paper, we introduce a massive dataset of more than 45 million geo-located tweets posted between 2015 and 2021 from US and Canada (TUSC), especially curated for natural language analysis. We also introduce Tweet Emotion Dynamics (TED) — metrics to capture patterns of emotions associated with tweets over time. We use TED and TUSC to explore the use of emotion-associated words across US and Canada; across 2019 (pre-pandemic), 2020 (the year the pandemic hit), and 2021 (the second year of the pandemic); and across individual tweeters. We show that Canadian tweets tend to have higher valence, lower arousal, and higher dominance than the US tweets. Further, we show that the COVID-19 pandemic had a marked impact on the emotional signature of tweets posted in 2020, when compared to the adjoining years. Finally, we determine metrics of TED for 170,000 tweeters to benchmark characteristics of TED metrics at an aggregate level. TUSC and the metrics for TED will enable a wide variety of research on studying how we use language to express ourselves, persuade, communicate, and influence, with particularly promising applications in public health, affective science, social science, and psychology.</abstract>
      <url hash="ceaa5915">2022.lrec-1.442</url>
      <bibkey>vishnubhotla-mohammad-2022-tusc</bibkey>
      <pwccode url="https://github.com/priya22/emotiondynamics" additional="false">priya22/emotiondynamics</pwccode>
    </paper>
    <paper id="443">
      <title>A <fixed-case>T</fixed-case>urkish Hate Speech Dataset and Detection System</title>
      <author><first>Fatih</first><last>Beyhan</last></author>
      <author><first>Buse</first><last>Çarık</last></author>
      <author><first>İnanç</first><last>Arın</last></author>
      <author><first>Ayşecan</first><last>Terzioğlu</last></author>
      <author><first>Berrin</first><last>Yanikoglu</last></author>
      <author><first>Reyyan</first><last>Yeniterzi</last></author>
      <pages>4177–4185</pages>
      <abstract>Social media posts containing hate speech are reproduced and redistributed at an accelerated pace, reaching greater audiences at a higher speed. We present a machine learning system for automatic detection of hate speech in Turkish, along with a hate speech dataset consisting of tweets collected in two separate domains. We first adopted a definition for hate speech that is in line with our goals and amenable to easy annotation; then designed the annotation schema for annotating the collected tweets. The Istanbul Convention dataset consists of tweets posted following the withdrawal of Turkey from the Istanbul Convention. The Refugees dataset was created by collecting tweets about immigrants by filtering based on commonly used keywords related to immigrants. Finally, we have developed a hate speech detection system using the transformer architecture (BERTurk), to be used as a baseline for the collected dataset. The binary classification accuracy is 77% when the system is evaluated using 5-fold cross-validation on the Istanbul Convention dataset and 71% for the Refugee dataset. We also tested a regression model with 0.66 and 0.83 RMSE on a scale of [0-4], for the Istanbul Convention and Refugees datasets.</abstract>
      <url hash="b5b010a4">2022.lrec-1.443</url>
      <bibkey>beyhan-etal-2022-turkish</bibkey>
    </paper>
    <paper id="444">
      <title>Life is not Always Depressing: Exploring the Happy Moments of People Diagnosed with Depression</title>
      <author><first>Ana-Maria</first><last>Bucur</last></author>
      <author><first>Adrian</first><last>Cosma</last></author>
      <author><first>Liviu P.</first><last>Dinu</last></author>
      <pages>4186–4192</pages>
      <abstract>In this work, we explore the relationship between depression and manifestations of happiness in social media. While the majority of works surrounding depression focus on symptoms, psychological research shows that there is a strong link between seeking happiness and being diagnosed with depression. We make use of Positive-Unlabeled learning paradigm to automatically extract happy moments from social media posts of both controls and users diagnosed with depression, and qualitatively analyze them with linguistic tools such as LIWC and keyness information. We show that the life of depressed individuals is not always bleak, with positive events related to friends and family being more noteworthy to their lives compared to the more mundane happy events reported by control users.</abstract>
      <url hash="c52d46cd">2022.lrec-1.444</url>
      <bibkey>bucur-etal-2022-life</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/happydb">HappyDB</pwcdataset>
    </paper>
    <paper id="445">
      <title>Evaluating Tokenizers Impact on <fixed-case>OOV</fixed-case>s Representation with Transformers Models</title>
      <author><first>Alexandra</first><last>Benamar</last></author>
      <author><first>Cyril</first><last>Grouin</last></author>
      <author><first>Meryl</first><last>Bothua</last></author>
      <author><first>Anne</first><last>Vilnat</last></author>
      <pages>4193–4204</pages>
      <abstract>Transformer models have achieved significant improvements in multiple downstream tasks in recent years. One of the main contributions of Transformers is their ability to create new representations for out-of-vocabulary (OOV) words. In this paper, we have evaluated three categories of OOVs: (A) new domain-specific terms (e.g., “eucaryote’” in microbiology), (B) misspelled words containing typos, and (C) cross-domain homographs (e.g., “arm” has different meanings in a clinical trial and anatomy). We use three French domain-specific datasets on the legal, medical, and energetical domains to robustly analyze these categories. Our experiments have led to exciting findings that showed: (1) It is easier to improve the representation of new words (A and B) than it is for words that already exist in the vocabulary of the Transformer models (C), (2) To ameliorate the representation of OOVs, the most effective method relies on adding external morpho-syntactic context rather than improving the semantic understanding of the words directly (fine-tuning) and (3) We cannot foresee the impact of minor misspellings in words because similar misspellings have different impacts on their representation. We believe that tackling the challenges of processing OOVs regarding their specificities will significantly help the domain adaptation aspect of BERT.</abstract>
      <url hash="c0ffba57">2022.lrec-1.445</url>
      <bibkey>benamar-etal-2022-evaluating</bibkey>
      <pwccode url="https://github.com/alexandrabenamar/evaluating_tokenizers_oov" additional="false">alexandrabenamar/evaluating_tokenizers_oov</pwccode>
    </paper>
    <paper id="446">
      <title>Assessing the Quality of an <fixed-case>I</fixed-case>talian Crowdsourced Idiom Corpus:the Dodiom Experiment</title>
      <author><first>Giuseppina</first><last>Morza</last></author>
      <author><first>Raffaele</first><last>Manna</last></author>
      <author><first>Johanna</first><last>Monti</last></author>
      <pages>4205–4211</pages>
      <abstract>This paper describes how idiom-related language resources, collected through a crowdsourcing experiment carried out by means of Dodiom, a Game-with-a-purpose, have been analysed by language experts. The paper focuses on the criteria adopted for the data annotation and evaluation process. The main scope of this project is, indeed, the evaluation of the quality of the linguistic data obtained through a crowdsourcing project, namely to assess if the data provided and evaluated by the players who joined the game are actually considered of good quality by the language experts. Finally, results of the annotation and evaluation processes as well as future work are presented.</abstract>
      <url hash="572d6f7f">2022.lrec-1.446</url>
      <bibkey>morza-etal-2022-assessing</bibkey>
    </paper>
    <paper id="447">
      <title>Medical Crossing: a Cross-lingual Evaluation of Clinical Entity Linking</title>
      <author><first>Anton</first><last>Alekseev</last></author>
      <author><first>Zulfat</first><last>Miftahutdinov</last></author>
      <author><first>Elena</first><last>Tutubalina</last></author>
      <author><first>Artem</first><last>Shelmanov</last></author>
      <author><first>Vladimir</first><last>Ivanov</last></author>
      <author><first>Vladimir</first><last>Kokh</last></author>
      <author><first>Alexander</first><last>Nesterov</last></author>
      <author><first>Manvel</first><last>Avetisian</last></author>
      <author><first>Andrei</first><last>Chertok</last></author>
      <author><first>Sergey</first><last>Nikolenko</last></author>
      <pages>4212–4220</pages>
      <abstract>Medical data annotation requires highly qualified expertise. Despite the efforts devoted to medical entity linking in different languages, available data is very sparse in terms of both data volume and languages. In this work, we establish benchmarks for cross-lingual medical entity linking using clinical reports, clinical guidelines, and medical research papers. We present a test set filtering procedure designed to analyze the “hard cases” of entity linking approaching zero-shot cross-lingual transfer learning, evaluate state-of-the-art models, and draw several interesting conclusions based on our evaluation results.</abstract>
      <url hash="3540b4aa">2022.lrec-1.447</url>
      <bibkey>alekseev-etal-2022-medical</bibkey>
      <pwccode url="https://github.com/airi-institute/medical_crossing" additional="false">airi-institute/medical_crossing</pwccode>
    </paper>
    <paper id="448">
      <title><fixed-case>MTL</fixed-case>ens: Machine Translation Output Debugging</title>
      <author><first>Shreyas</first><last>Sharma</last></author>
      <author><first>Kareem</first><last>Darwish</last></author>
      <author><first>Lucas</first><last>Pavanelli</last></author>
      <author><first>Thiago</first><last>Castro Ferreira</last></author>
      <author><first>Mohamed</first><last>Al-Badrashiny</last></author>
      <author><first>Kamer Ali</first><last>Yuksel</last></author>
      <author><first>Hassan</first><last>Sawaf</last></author>
      <pages>4221–4226</pages>
      <abstract>The performance of Machine Translation (MT) systems varies significantly with inputs of diverging features such as topics, genres, and surface properties. Though there are many MT evaluation metrics that generally correlate with human judgments, they are not directly useful in identifying specific shortcomings of MT systems. In this demo, we present a benchmarking interface that enables improved evaluation of specific MT systems in isolation or multiple MT systems collectively by quantitatively evaluating their performance on many tasks across multiple domains and evaluation metrics. Further, it facilitates effective debugging and error analysis of MT output via the use of dynamic filters that help users hone in on problem sentences with specific properties, such as genre, topic, sentence length, etc. The interface can be extended to include additional filters such as lexical, morphological, and syntactic features. Aside from helping debug MT output, it can also help in identifying problems in reference translations and evaluation metrics.</abstract>
      <url hash="62bae907">2022.lrec-1.448</url>
      <bibkey>sharma-etal-2022-mtlens</bibkey>
    </paper>
    <paper id="449">
      <title><fixed-case>I</fixed-case>ce<fixed-case>BATS</fixed-case>: An <fixed-case>I</fixed-case>celandic Adaptation of the Bigger Analogy Test Set</title>
      <author><first>Steinunn Rut</first><last>Friðriksdóttir</last></author>
      <author><first>Hjalti</first><last>Daníelsson</last></author>
      <author><first>Steinþór</first><last>Steingrímsson</last></author>
      <author><first>Einar</first><last>Sigurdsson</last></author>
      <pages>4227–4234</pages>
      <abstract>Word embedding models have become commonplace in a wide range of NLP applications. In order to train and use the best possible models, accurate evaluation is needed. For extrinsic evaluation of word embedding models, analogy evaluation sets have been shown to be a good quality estimator. We introduce an Icelandic adaptation of a large analogy dataset, BATS, evaluate it on three different word embedding models and show that our evaluation set is apt at measuring the capabilities of such models.</abstract>
      <url hash="ba85c70e">2022.lrec-1.449</url>
      <bibkey>fridriksdottir-etal-2022-icebats</bibkey>
    </paper>
    <paper id="450">
      <title>Transfer Learning Methods for Domain Adaptation in Technical Logbook Datasets</title>
      <author><first>Farhad</first><last>Akhbardeh</last></author>
      <author><first>Marcos</first><last>Zampieri</last></author>
      <author><first>Cecilia Ovesdotter</first><last>Alm</last></author>
      <author><first>Travis</first><last>Desell</last></author>
      <pages>4235–4244</pages>
      <abstract>Event identification in technical logbooks poses challenges given the limited logbook data available in specific technical domains, the large set of possible classes, and logbook entries typically being in short form and non-standard technical language. Technical logbook data typically has both a domain, the field it comes from (e.g., automotive), and an application, what it is used for (e.g., maintenance). In order to better handle the problem of data scarcity, using a variety of technical logbook datasets, this paper investigates the benefits of using transfer learning from sources within the same domain (but different applications), from within the same application (but different domains) and from all available data. Results show that performing transfer learning within a domain provides statistically significant improvements, and in all cases but one the best performance. Interestingly, transfer learning from within the application or across the global dataset degrades results in all cases but one, which benefited from adding as much data as possible. A further analysis of the dataset similarities shows that the datasets with higher similarity scores performed better in transfer learning tasks, suggesting that this can be utilized to determine the effectiveness of adding a dataset in a transfer learning task for technical logbooks.</abstract>
      <url hash="5eb1acbc">2022.lrec-1.450</url>
      <bibkey>akhbardeh-etal-2022-transfer</bibkey>
    </paper>
    <paper id="451">
      <title>Downstream Task Performance of <fixed-case>BERT</fixed-case> Models Pre-Trained Using Automatically De-Identified Clinical Data</title>
      <author><first>Thomas</first><last>Vakili</last></author>
      <author><first>Anastasios</first><last>Lamproudis</last></author>
      <author><first>Aron</first><last>Henriksson</last></author>
      <author><first>Hercules</first><last>Dalianis</last></author>
      <pages>4245–4252</pages>
      <abstract>Automatic de-identification is a cost-effective and straightforward way of removing large amounts of personally identifiable information from large and sensitive corpora. However, these systems also introduce errors into datasets due to their imperfect precision. These corruptions of the data may negatively impact the utility of the de-identified dataset. This paper de-identifies a very large clinical corpus in Swedish either by removing entire sentences containing sensitive data or by replacing sensitive words with realistic surrogates. These two datasets are used to perform domain adaptation of a general Swedish BERT model. The impact of the de-identification techniques is assessed by training and evaluating the models using six clinical downstream tasks. The results are then compared to a similar BERT model domain-adapted using an unaltered version of the clinical corpus. The results show that using an automatically de-identified corpus for domain adaptation does not negatively impact downstream performance. We argue that automatic de-identification is an efficient way of reducing the privacy risks of domain-adapted models and that the models created in this paper should be safe to distribute to other academic researchers.</abstract>
      <url hash="dd083155">2022.lrec-1.451</url>
      <bibkey>vakili-etal-2022-downstream</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mimic-iii">MIMIC-III</pwcdataset>
    </paper>
    <paper id="452">
      <title>Dilated Convolutional Neural Networks for Lightweight Diacritics Restoration</title>
      <author><first>Bálint</first><last>Csanády</last></author>
      <author><first>András</first><last>Lukács</last></author>
      <pages>4253–4259</pages>
      <abstract>Diacritics restoration has become a ubiquitous task in the Latin-alphabet-based English-dominated Internet language environment. In this paper, we describe a small footprint 1D dilated convolution-based approach which operates on a character-level. We find that neural networks based on 1D dilated convolutions are competitive alternatives to solutions based on recurrent neural networks or linguistic modeling for the task of diacritics restoration. Our approach surpasses the performance of similarly sized models and is also competitive with larger models. A special feature of our solution is that it even runs locally in a web browser. We also provide a working example of this browser-based implementation. Our model is evaluated on different corpora, with emphasis on the Hungarian language. We performed comparative measurements about the generalization power of the model in relation to three Hungarian corpora. We also analyzed the errors to understand the limitation of corpus-based self-supervised training.</abstract>
      <url hash="fa051426">2022.lrec-1.452</url>
      <bibkey>csanady-lukacs-2022-dilated</bibkey>
      <pwccode url="https://github.com/aielte-research/diacritics_restoration" additional="false">aielte-research/diacritics_restoration</pwccode>
    </paper>
    <paper id="453">
      <title>Generating Artificial Texts as Substitution or Complement of Training Data</title>
      <author><first>Vincent</first><last>Claveau</last></author>
      <author><first>Antoine</first><last>Chaffin</last></author>
      <author><first>Ewa</first><last>Kijak</last></author>
      <pages>4260–4269</pages>
      <abstract>The quality of artificially generated texts has considerably improved with the advent of transformers. The question of using these models to generate learning data for supervised learning tasks naturally arises, especially when the original language resource cannot be distributed, or when it is small. In this article, this question is explored under 3 aspects: (i) are artificial data an efficient complement? (ii) can they replace the original data when those are not available or cannot be distributed for confidentiality reasons? (iii) can they improve the explainability of classifiers? Different experiments are carried out on classification tasks - namely sentiment analysis on product reviews and Fake News detection - using artificially generated data by fine-tuned GPT-2 models. The results show that such artificial data can be used in a certain extend but require pre-processing to significantly improve performance. We also show that bag-of-words approaches benefit the most from such data augmentation.</abstract>
      <url hash="0915bb2b">2022.lrec-1.453</url>
      <bibkey>claveau-etal-2022-generating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/flue-french-language-understanding-evaluation">FLUE</pwcdataset>
    </paper>
    <paper id="454">
      <title>From Pattern to Interpretation. Using Colibri Core to Detect Translation Patterns in the Peshitta.</title>
      <author><first>Mathias</first><last>Coeckelbergs</last></author>
      <pages>4270–4274</pages>
      <abstract>This article presents the first results of the CLARIAH-funded project ‘Patterns in Translation: Using Colibri Core for the Syriac Bible’ (PaTraCoSy). This project seeks to use Colibri Core to detect translation patterns in the Peshitta, the Syriac translation of the Hebrew Bible. We first describe how we constructed word and phrase alignment between these two texts. This step is necessary to succesfully implement the functionalities of Colibri Core. After this, we further describe our first investigations with the software. We describe how we use the built-in pattern modeller to detect n-gram and skipgram patterns in both Hebrew and Syriac texts. Colibri Core does not allow the creation of a bilingual model, which is why we compare the separate models. After a presentation of a few general insights on the overall translation behaviour of the Peshitta, we delve deeper into the concrete patterns we can detect by the n-gram/skipgram analysis. We provide multiple examples from the book of Genesis, a book which has been treated broadly in scholarly research into the Syriac translation, but which also appears to have interesting features based on our Colibri Core research.</abstract>
      <url hash="f8dc6ff3">2022.lrec-1.454</url>
      <bibkey>coeckelbergs-2022-pattern</bibkey>
    </paper>
    <paper id="455">
      <title><fixed-case>PAG</fixed-case>nol: An Extra-Large <fixed-case>F</fixed-case>rench Generative Model</title>
      <author><first>Julien</first><last>Launay</last></author>
      <author><first>E.l.</first><last>Tommasone</last></author>
      <author><first>Baptiste</first><last>Pannier</last></author>
      <author><first>François</first><last>Boniface</last></author>
      <author><first>Amélie</first><last>Chatelain</last></author>
      <author><first>Alessandro</first><last>Cappelli</last></author>
      <author><first>Iacopo</first><last>Poli</last></author>
      <author><first>Djamé</first><last>Seddah</last></author>
      <pages>4275–4284</pages>
      <abstract>Access to large pre-trained models of varied architectures, in many different languages, is central to the democratization of NLP. We introduce PAGnol, a collection of French GPT models. Using scaling laws, we efficiently train PAGnol-XL (1.5B parameters) with the same computational budget as CamemBERT, a model 13 times smaller. PAGnol-XL is the largest model trained from scratch for the French language. We plan to train increasingly large and performing versions of PAGnol, exploring the capabilities of French extreme-scale models. For this first release, we focus on the pre-training and scaling calculations underlining PAGnol. We fit a scaling law for compute for the French language, and compare it with its English counterpart. We find the pre-training dataset significantly conditions the quality of the outputs, with common datasets such as OSCAR leading to low-quality offensive text. We evaluate our models on discriminative and generative tasks in French, comparing to other state-of-the-art French and multilingual models, and reaching the state of the art in the abstract summarization task. Our research was conducted on the public GENCI Jean Zay supercomputer, and our models up to the Large are made publicly available.</abstract>
      <url hash="9c09388f">2022.lrec-1.455</url>
      <bibkey>launay-etal-2022-pagnol</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ccnet">CCNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/flue-french-language-understanding-evaluation">FLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fquad">FQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/oscar">OSCAR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/orangesum">OrangeSum</pwcdataset>
    </paper>
    <paper id="456">
      <title><fixed-case>CEPOC</fixed-case>: The <fixed-case>C</fixed-case>ambridge Exams Publishing Open Cloze dataset</title>
      <author><first>Mariano</first><last>Felice</last></author>
      <author><first>Shiva</first><last>Taslimipoor</last></author>
      <author><first>Øistein E.</first><last>Andersen</last></author>
      <author><first>Paula</first><last>Buttery</last></author>
      <pages>4285–4290</pages>
      <abstract>Open cloze tests are a standard type of exercise where examinees must complete a text by filling in the gaps without any given options to choose from. This paper presents the Cambridge Exams Publishing Open Cloze (CEPOC) dataset, a collection of open cloze tests from world-renowned English language proficiency examinations. The tests in CEPOC have been expertly designed and validated using standard principles in language research and assessment. They are prepared for language learners at different proficiency levels and hence classified into different CEFR levels (A2, B1, B2, C1, C2). This resource can be a valuable testbed for various NLP tasks. We perform a complete set of experiments on three tasks: gap filling, gap prediction, and CEFR text classification. We implement transformer-based systems based on pre-trained language models to model each task and use our dataset as a test set, providing promising benchmark results.</abstract>
      <url hash="8486c549">2022.lrec-1.456</url>
      <bibkey>felice-etal-2022-cepoc</bibkey>
      <pwccode url="https://github.com/cambridgealta/cepoc" additional="false">cambridgealta/cepoc</pwccode>
    </paper>
    <paper id="457">
      <title><fixed-case>ALBETO</fixed-case> and <fixed-case>D</fixed-case>istil<fixed-case>BETO</fixed-case>: Lightweight <fixed-case>S</fixed-case>panish Language Models</title>
      <author><first>José</first><last>Cañete</last></author>
      <author><first>Sebastian</first><last>Donoso</last></author>
      <author><first>Felipe</first><last>Bravo-Marquez</last></author>
      <author><first>Andrés</first><last>Carvallo</last></author>
      <author><first>Vladimir</first><last>Araujo</last></author>
      <pages>4291–4298</pages>
      <abstract>In recent years there have been considerable advances in pre-trained language models, where non-English language versions have also been made available. Due to their increasing use, many lightweight versions of these models (with reduced parameters) have also been released to speed up training and inference times. However, versions of these lighter models (e.g., ALBERT, DistilBERT) for languages other than English are still scarce. In this paper we present ALBETO and DistilBETO, which are versions of ALBERT and DistilBERT pre-trained exclusively on Spanish corpora. We train several versions of ALBETO ranging from 5M to 223M parameters and one of DistilBETO with 67M parameters. We evaluate our models in the GLUES benchmark that includes various natural language understanding tasks in Spanish. The results show that our lightweight models achieve competitive results to those of BETO (Spanish-BERT) despite having fewer parameters. More specifically, our larger ALBETO model outperforms all other models on the MLDoc, PAWS-X, XNLI, MLQA, SQAC and XQuAD datasets. However, BETO remains unbeaten for POS and NER. As a further contribution, all models are publicly available to the community for future research.</abstract>
      <url hash="f2258b34">2022.lrec-1.457</url>
      <bibkey>canete-etal-2022-albeto</bibkey>
      <pwccode url="https://github.com/dccuchile/lightweight-spanish-language-models" additional="true">dccuchile/lightweight-spanish-language-models</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2002">CoNLL 2002</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mldoc">MLDoc</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mlqa">MLQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paws-x">PAWS-X</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad-es">SQuAD-es</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xquad">XQuAD</pwcdataset>
    </paper>
    <paper id="458">
      <title>On the Robustness of Cognate Generation Models</title>
      <author><first>Winston</first><last>Wu</last></author>
      <author><first>David</first><last>Yarowsky</last></author>
      <pages>4299–4305</pages>
      <abstract>We evaluate two popular neural cognate generation models’ robustness to several types of human-plausible noise (deletion, duplication, swapping, and keyboard errors, as well as a new type of error, phonological errors). We find that duplication and phonological substitution is least harmful, while the other types of errors are harmful. We present an in-depth analysis of the models’ results with respect to each error type to explain how and why these models perform as they do.</abstract>
      <url hash="7da9ae9a">2022.lrec-1.458</url>
      <bibkey>wu-yarowsky-2022-robustness</bibkey>
    </paper>
    <paper id="459">
      <title><fixed-case>CLISTER</fixed-case> : A Corpus for Semantic Textual Similarity in <fixed-case>F</fixed-case>rench Clinical Narratives</title>
      <author><first>Nicolas</first><last>Hiebel</last></author>
      <author><first>Olivier</first><last>Ferret</last></author>
      <author><first>Karën</first><last>Fort</last></author>
      <author><first>Aurélie</first><last>Névéol</last></author>
      <pages>4306–4315</pages>
      <abstract>Modern Natural Language Processing relies on the availability of annotated corpora for training and evaluating models. Such resources are scarce, especially for specialized domains in languages other than English. In particular, there are very few resources for semantic similarity in the clinical domain in French. This can be useful for many biomedical natural language processing applications, including text generation. We introduce a definition of similarity that is guided by clinical facts and apply it to the development of a new French corpus of 1,000 sentence pairs manually annotated according to similarity scores. This new sentence similarity corpus is made freely available to the community. We further evaluate the corpus through experiments of automatic similarity measurement. We show that a model of sentence embeddings can capture similarity with state-of-the-art performance on the DEFT STS shared task evaluation data set (Spearman=0.8343). We also show that the corpus is complementary to DEFT STS.</abstract>
      <url hash="ef7948d8">2022.lrec-1.459</url>
      <bibkey>hiebel-etal-2022-clister-corpus</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/biosses">BIOSSES</pwcdataset>
    </paper>
    <paper id="460">
      <title>The <fixed-case>C</fixed-case>hinese Causative-Passive Homonymy Disambiguation: an adversarial Dataset for <fixed-case>NLI</fixed-case> and a Probing Task</title>
      <author><first>Shanshan</first><last>Xu</last></author>
      <author><first>Katja</first><last>Markert</last></author>
      <pages>4316–4323</pages>
      <abstract>The disambiguation of causative-passive homonymy (CPH) is potentially tricky for machines, as the causative and the passive are not distinguished by the sentences’ syntactic structure. By transforming CPH disambiguation to a challenging natural language inference (NLI) task, we present the first Chinese Adversarial NLI challenge set (CANLI). We show that the pretrained transformer model RoBERTa, fine-tuned on an existing large-scale Chinese NLI benchmark dataset, performs poorly on CANLI. We also employ Word Sense Disambiguation as a probing task to investigate to what extent the CPH feature is captured in the model’s internal representation. We find that the model’s performance on CANLI does not correspond to its internal representation of CPH, which is the crucial linguistic ability central to the CANLI dataset. CANLI is available on Hugging Face Datasets (Lhoest et al., 2021) at https://huggingface.co/datasets/sxu/CANLI</abstract>
      <url hash="f716f8c7">2022.lrec-1.460</url>
      <bibkey>xu-markert-2022-chinese</bibkey>
    </paper>
    <paper id="461">
      <title>Modeling Noise in Paraphrase Detection</title>
      <author><first>Teemu</first><last>Vahtola</last></author>
      <author><first>Eetu</first><last>Sjöblom</last></author>
      <author><first>Jörg</first><last>Tiedemann</last></author>
      <author><first>Mathias</first><last>Creutz</last></author>
      <pages>4324–4332</pages>
      <abstract>Noisy labels in training data present a challenging issue in classification tasks, misleading a model towards incorrect decisions during training. In this paper, we propose the use of a linear noise model to augment pre-trained language models to account for label noise in fine-tuning. We test our approach in a paraphrase detection task with various levels of noise and five different languages. Our experiments demonstrate the effectiveness of the additional noise model in making the training procedures more robust and stable. Furthermore, we show that this model can be applied without further knowledge about annotation confidence and reliability of individual training examples and we analyse our results in light of data selection and sampling strategies.</abstract>
      <url hash="3a0288cd">2022.lrec-1.461</url>
      <bibkey>vahtola-etal-2022-modeling</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/opusparcus">Opusparcus</pwcdataset>
    </paper>
    <paper id="462">
      <title>Give me your Intentions, <fixed-case>I</fixed-case>’ll Predict our Actions: A Two-level Classification of Speech Acts for Crisis Management in Social Media</title>
      <author><first>Enzo</first><last>Laurenti</last></author>
      <author><first>Nils</first><last>Bourgon</last></author>
      <author><first>Farah</first><last>Benamara</last></author>
      <author><first>Alda</first><last>Mari</last></author>
      <author><first>Véronique</first><last>Moriceau</last></author>
      <author><first>Camille</first><last>Courgeon</last></author>
      <pages>4333–4343</pages>
      <abstract>Discovered by (Austin,1962) and extensively promoted by (Searle, 1975), speech acts (SA) have been the object of extensive discussion in the philosophical and the linguistic literature, as well as in computational linguistics where the detection of SA have shown to be an important step in many down stream NLP applications. In this paper, we attempt to measure for the first time the role of SA on urgency detection in tweets, focusing on natural disasters. Indeed, SA are particularly relevant to identify intentions, desires, plans and preferences towards action, providing therefore actionable information that will help to set priorities for the human teams and decide appropriate rescue actions. To this end, we come up here with four main contributions: (1) A two-layer annotation scheme of SA both at the tweet and subtweet levels, (2) A new French dataset of 6,669 tweets annotated for both urgency and SA, (3) An in-depth analysis of the annotation campaign, highlighting the correlation between SA and urgency categories, and (4) A set of deep learning experiments to detect SA in a crisis corpus. Our results show that SA are correlated with urgency which is a first important step towards SA-aware NLP-based crisis management on social media.</abstract>
      <url hash="53e20ae4">2022.lrec-1.462</url>
      <bibkey>laurenti-etal-2022-give</bibkey>
    </paper>
    <paper id="463">
      <title>Towards a Cleaner Document-Oriented Multilingual Crawled Corpus</title>
      <author><first>Julien</first><last>Abadji</last></author>
      <author><first>Pedro</first><last>Ortiz Suarez</last></author>
      <author><first>Laurent</first><last>Romary</last></author>
      <author><first>Benoît</first><last>Sagot</last></author>
      <pages>4344–4355</pages>
      <abstract>The need for large corpora raw corpora has dramatically increased in recent years with the introduction of transfer learning and semi-supervised learning methods to Natural Language Processing. And while there have been some recent attempts to manually curate the amount of data necessary to train large language models, the main way to obtain this data is still through automatic web crawling. In this paper we take the existing multilingual web corpus OSCAR and its pipeline Ungoliant that extracts and classifies data from Common Crawl at the line level, and propose a set of improvements and automatic annotations in order to produce a new document-oriented version of OSCAR that could prove more suitable to pre-train large generative language models as well as hopefully other applications in Natural Language Processing and Digital Humanities.</abstract>
      <url hash="4d69e739">2022.lrec-1.463</url>
      <bibkey>abadji-etal-2022-towards</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/c4">C4</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ccnet">CCNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/oscar">OSCAR</pwcdataset>
    </paper>
    <paper id="464">
      <title>A Warm Start and a Clean Crawled Corpus - A Recipe for Good Language Models</title>
      <author><first>Vésteinn</first><last>Snæbjarnarson</last></author>
      <author><first>Haukur Barri</first><last>Símonarson</last></author>
      <author><first>Pétur Orri</first><last>Ragnarsson</last></author>
      <author><first>Svanhvít Lilja</first><last>Ingólfsdóttir</last></author>
      <author><first>Haukur</first><last>Jónsson</last></author>
      <author><first>Vilhjalmur</first><last>Thorsteinsson</last></author>
      <author><first>Hafsteinn</first><last>Einarsson</last></author>
      <pages>4356–4366</pages>
      <abstract>We train several language models for Icelandic, including IceBERT, that achieve state-of-the-art performance in a variety of downstream tasks, including part-of-speech tagging, named entity recognition, grammatical error detection and constituency parsing. To train the models we introduce a new corpus of Icelandic text, the Icelandic Common Crawl Corpus (IC3), a collection of high quality texts found online by targeting the Icelandic top-level-domain .is. Several other public data sources are also collected for a total of 16GB of Icelandic text. To enhance the evaluation of model performance and to raise the bar in baselines for Icelandic, we manually translate and adapt the WinoGrande commonsense reasoning dataset. Through these efforts we demonstrate that a properly cleaned crawled corpus is sufficient to achieve state-of-the-art results in NLP applications for low to medium resource languages, by comparison with models trained on a curated corpus. We further show that initializing models using existing multilingual models can lead to state-of-the-art results for some downstream tasks.</abstract>
      <url hash="0bf02307">2022.lrec-1.464</url>
      <bibkey>snaebjarnarson-etal-2022-warm</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/winogrande">WinoGrande</pwcdataset>
    </paper>
    <paper id="465">
      <title>Adapting Language Models When Training on Privacy-Transformed Data</title>
      <author><first>Tugtekin</first><last>Turan</last></author>
      <author><first>Dietrich</first><last>Klakow</last></author>
      <author><first>Emmanuel</first><last>Vincent</last></author>
      <author><first>Denis</first><last>Jouvet</last></author>
      <pages>4367–4373</pages>
      <abstract>In recent years, voice-controlled personal assistants have revolutionized the interaction with smart devices and mobile applications. The collected data are then used by system providers to train language models (LMs). Each spoken message reveals personal information, hence removing private information from the input sentences is necessary. Our data sanitization process relies on recognizing and replacing named entities by other words from the same class. However, this may harm LM training because privacy-transformed data is unlikely to match the test distribution. This paper aims to fill the gap by focusing on the adaptation of LMs initially trained on privacy-transformed sentences using a small amount of original untransformed data. To do so, we combine class-based LMs, which provide an effective approach to overcome data sparsity in the context of n-gram LMs, and neural LMs, which handle longer contexts and can yield better predictions. Our experiments show that training an LM on privacy-transformed data result in a relative 11% word error rate (WER) increase compared to training on the original untransformed data, and adapting that model on a limited amount of original untransformed data leads to a relative 8% WER improvement over the model trained solely on privacy-transformed data.</abstract>
      <url hash="c6be2417">2022.lrec-1.465</url>
      <bibkey>turan-etal-2022-adapting</bibkey>
    </paper>
    <paper id="466">
      <title>Evaluation of Transfer Learning for <fixed-case>P</fixed-case>olish with a Text-to-Text Model</title>
      <author><first>Aleksandra</first><last>Chrabrowa</last></author>
      <author><first>Łukasz</first><last>Dragan</last></author>
      <author><first>Karol</first><last>Grzegorczyk</last></author>
      <author><first>Dariusz</first><last>Kajtoch</last></author>
      <author><first>Mikołaj</first><last>Koszowski</last></author>
      <author><first>Robert</first><last>Mroczkowski</last></author>
      <author><first>Piotr</first><last>Rybak</last></author>
      <pages>4374–4394</pages>
      <abstract>We introduce a new benchmark for assessing the quality of text-to-text models for Polish. The benchmark consists of diverse tasks and datasets: KLEJ benchmark adapted for text-to-text, en-pl translation, summarization, and question answering. In particular, since summarization and question answering lack benchmark datasets for the Polish language, we describe in detail their construction and make them publicly available. Additionally, we present plT5 - a general-purpose text-to-text model for Polish that can be fine-tuned on various Natural Language Processing (NLP) tasks with a single training objective. Unsupervised denoising pre-training is performed efficiently by initializing the model weights with a multi-lingual T5 (mT5) counterpart. We evaluate the performance of plT5, mT5, Polish BART (plBART), and Polish GPT-2 (papuGaPT2). The plT5 scores top on all of these tasks except summarization, where plBART is best. In general (except summarization), the larger the model, the better the results. The encoder-decoder architectures prove to be better than the decoder-only equivalent.</abstract>
      <url hash="cbcaba05">2022.lrec-1.466</url>
      <bibkey>chrabrowa-etal-2022-evaluation</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/klej">KLEJ</pwcdataset>
    </paper>
    <paper id="467">
      <title>Evaluation of <fixed-case>HTR</fixed-case> models without Ground Truth Material</title>
      <author><first>Phillip Benjamin</first><last>Ströbel</last></author>
      <author><first>Martin</first><last>Volk</last></author>
      <author><first>Simon</first><last>Clematide</last></author>
      <author><first>Raphael</first><last>Schwitter</last></author>
      <author><first>Tobias</first><last>Hodel</last></author>
      <author><first>David</first><last>Schoch</last></author>
      <pages>4395–4404</pages>
      <abstract>The evaluation of Handwritten Text Recognition (HTR) models during their development is straightforward: because HTR is a supervised problem, the usual data split into training, validation, and test data sets allows the evaluation of models in terms of accuracy or error rates. However, the evaluation process becomes tricky as soon as we switch from development to application. A compilation of a new (and forcibly smaller) ground truth (GT) from a sample of the data that we want to apply the model on and the subsequent evaluation of models thereon only provides hints about the quality of the recognised text, as do confidence scores (if available) the models return. Moreover, if we have several models at hand, we face a model selection problem since we want to obtain the best possible result during the application phase. This calls for GT-free metrics to select the best model, which is why we (re-)introduce and compare different metrics, from simple, lexicon-based to more elaborate ones using standard language models and masked language models (MLM). We show that MLM-based evaluation can compete with lexicon-based methods, with the advantage that large and multilingual transformers are readily available, thus making compiling lexical resources for other metrics superfluous.</abstract>
      <url hash="178b0174">2022.lrec-1.467</url>
      <bibkey>strobel-etal-2022-evaluation</bibkey>
      <pwccode url="https://github.com/pstroe/atr-eval" additional="false">pstroe/atr-eval</pwccode>
    </paper>
    <paper id="468">
      <title>A Semi-Automated Live Interlingual Communication Workflow Featuring Intralingual Respeaking: Evaluation and Benchmarking</title>
      <author><first>Tomasz</first><last>Korybski</last></author>
      <author><first>Elena</first><last>Davitti</last></author>
      <author><first>Constantin</first><last>Orasan</last></author>
      <author><first>Sabine</first><last>Braun</last></author>
      <pages>4405–4413</pages>
      <abstract>In this paper, we present a semi-automated workflow for live interlingual speech-to-text communication which seeks to reduce the shortcomings of existing ASR systems: a human respeaker works with a speaker-dependent speech recognition software (e.g., Dragon Naturally Speaking) to deliver punctuated same-language output of superior quality than obtained using out-of-the-box automatic speech recognition of the original speech. This is fed into a machine translation engine (the EU’s eTranslation) to produce live-caption ready text. We benchmark the quality of the output against the output of best-in-class (human) simultaneous interpreters working with the same source speeches from plenary sessions of the European Parliament. To evaluate the accuracy and facilitate the comparison between the two types of output, we use a tailored annotation approach based on the NTR model (Romero-Fresco and Pöchhacker, 2017). We find that the semi-automated workflow combining intralingual respeaking and machine translation is capable of generating outputs that are similar in terms of accuracy and completeness to the outputs produced in the benchmarking workflow, although the small scale of our experiment requires caution in interpreting this result.</abstract>
      <url hash="81322a36">2022.lrec-1.468</url>
      <bibkey>korybski-etal-2022-semi</bibkey>
    </paper>
    <paper id="469">
      <title>Are Embedding Spaces Interpretable? Results of an Intrusion Detection Evaluation on a Large <fixed-case>F</fixed-case>rench Corpus</title>
      <author><first>Thibault</first><last>Prouteau</last></author>
      <author><first>Nicolas</first><last>Dugué</last></author>
      <author><first>Nathalie</first><last>Camelin</last></author>
      <author><first>Sylvain</first><last>Meignier</last></author>
      <pages>4414–4419</pages>
      <abstract>Word embedding methods allow to represent words as vectors in a space that is structured using word co-occurrences so that words with close meanings are close in this space. These vectors are then provided as input to automatic systems to solve natural language processing problems. Because interpretability is a necessary condition to trusting such systems, interpretability of embedding spaces, the first link in the chain is an important issue. In this paper, we thus evaluate the interpretability of vectors extracted with two approaches: SPINE a k-sparse auto-encoder, and SINr, a graph-based method. This evaluation is based on a Word Intrusion Task with human annotators. It is operated using a large French corpus, and is thus, as far as we know, the first large-scale experiment regarding word embedding interpretability on this language. Furthermore, contrary to the approaches adopted in the literature where the evaluation is done on a small sample of frequent words, we consider a more realistic use-case where most of the vocabulary is kept for the evaluation. This allows to show how difficult this task is, even though SPINE and SINr show some promising results. In particular, SINr results are obtained with a very low amount of computation compared to SPINE, while being similarly interpretable.</abstract>
      <url hash="532b5377">2022.lrec-1.469</url>
      <bibkey>prouteau-etal-2022-embedding</bibkey>
    </paper>
    <paper id="470">
      <title>Corpus for Automatic Structuring of Legal Documents</title>
      <author><first>Prathamesh</first><last>Kalamkar</last></author>
      <author><first>Aman</first><last>Tiwari</last></author>
      <author><first>Astha</first><last>Agarwal</last></author>
      <author><first>Saurabh</first><last>Karn</last></author>
      <author><first>Smita</first><last>Gupta</last></author>
      <author><first>Vivek</first><last>Raghavan</last></author>
      <author><first>Ashutosh</first><last>Modi</last></author>
      <pages>4420–4429</pages>
      <abstract>In populous countries, pending legal cases have been growing exponentially. There is a need for developing techniques for processing and organizing legal documents. In this paper, we introduce a new corpus for structuring legal documents. In particular, we introduce a corpus of legal judgment documents in English that are segmented into topical and coherent parts. Each of these parts is annotated with a label coming from a list of pre-defined Rhetorical Roles. We develop baseline models for automatically predicting rhetorical roles in a legal document based on the annotated corpus. Further, we show the application of rhetorical roles to improve performance on the tasks of summarization and legal judgment prediction. We release the corpus and baseline model code along with the paper.</abstract>
      <url hash="5f1b3746">2022.lrec-1.470</url>
      <bibkey>kalamkar-etal-2022-corpus</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ildc">ILDC</pwcdataset>
    </paper>
    <paper id="471">
      <title>The Search for Agreement on Logical Fallacy Annotation of an Infodemic</title>
      <author><first>Claire</first><last>Bonial</last></author>
      <author><first>Austin</first><last>Blodgett</last></author>
      <author><first>Taylor</first><last>Hudson</last></author>
      <author><first>Stephanie M.</first><last>Lukin</last></author>
      <author><first>Jeffrey</first><last>Micher</last></author>
      <author><first>Douglas</first><last>Summers-Stay</last></author>
      <author><first>Peter</first><last>Sutor</last></author>
      <author><first>Clare</first><last>Voss</last></author>
      <pages>4430–4438</pages>
      <abstract>We evaluate an annotation schema for labeling logical fallacy types, originally developed for a crowd-sourcing annotation paradigm, now using an annotation paradigm of two trained linguist annotators. We apply the schema to a variety of different genres of text relating to the COVID-19 pandemic. Our linguist (as opposed to crowd-sourced) annotation of logical fallacies allows us to evaluate whether the annotation schema category labels are sufficiently clear and non-overlapping for both manual and, later, system assignment. We report inter-annotator agreement results over two annotation phases as well as a preliminary assessment of the corpus for training and testing a machine learning algorithm (Pattern-Exploiting Training) for fallacy detection and recognition. The agreement results and system performance underscore the challenging nature of this annotation task and suggest that the annotation schema and paradigm must be iteratively evaluated and refined in order to arrive at a set of annotation labels that can be reproduced by human annotators and, in turn, provide reliable training data for automatic detection and recognition systems.</abstract>
      <url hash="9dd4d5f5">2022.lrec-1.471</url>
      <bibkey>bonial-etal-2022-search</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/record">ReCoRD</pwcdataset>
    </paper>
    <paper id="472">
      <title>Recovering Patient Journeys: A Corpus of Biomedical Entities and Relations on <fixed-case>T</fixed-case>witter (<fixed-case>BEAR</fixed-case>)</title>
      <author><first>Amelie</first><last>Wührl</last></author>
      <author><first>Roman</first><last>Klinger</last></author>
      <pages>4439–4450</pages>
      <abstract>Text mining and information extraction for the medical domain has focused on scientific text generated by researchers. However, their access to individual patient experiences or patient-doctor interactions is limited. On social media, doctors, patients and their relatives also discuss medical information. Individual information provided by laypeople complements the knowledge available in scientific text. It reflects the patient’s journey making the value of this type of data twofold: It offers direct access to people’s perspectives, and it might cover information that is not available elsewhere, including self-treatment or self-diagnose. Named entity recognition and relation extraction are methods to structure information that is available in unstructured text. However, existing medical social media corpora focused on a comparably small set of entities and relations. In contrast, we provide rich annotation layers to model patients’ experiences in detail. The corpus consists of medical tweets annotated with a fine-grained set of medical entities and relations between them, namely 14 entity (incl. environmental factors, diagnostics, biochemical processes, patients’ quality-of-life descriptions, pathogens, medical conditions, and treatments) and 20 relation classes (incl. prevents, influences, interactions, causes). The dataset consists of 2,100 tweets with approx. 6,000 entities and 2,200 relations.</abstract>
      <url hash="5bdc597b">2022.lrec-1.472</url>
      <bibkey>wuhrl-klinger-2022-recovering</bibkey>
    </paper>
    <paper id="473">
      <title>Improving Event Duration Question Answering by Leveraging Existing Temporal Information Extraction Data</title>
      <author><first>Felix</first><last>Virgo</last></author>
      <author><first>Fei</first><last>Cheng</last></author>
      <author><first>Sadao</first><last>Kurohashi</last></author>
      <pages>4451–4457</pages>
      <abstract>Understanding event duration is essential for understanding natural language. However, the amount of training data for tasks like duration question answering, i.e., McTACO, is very limited, suggesting a need for external duration information to improve this task. The duration information can be obtained from existing temporal information extraction tasks, such as UDS-T and TimeBank, where more duration data is available. A straightforward two-stage fine-tuning approach might be less likely to succeed given the discrepancy between the target duration question answering task and the intermediary duration classification task. This paper resolves this discrepancy by automatically recasting an existing event duration classification task from UDS-T to a question answering task similar to the target McTACO. We investigate the transferability of duration information by comparing whether the original UDS-T duration classification or the recast UDS-T duration question answering can be transferred to the target task. Our proposed model achieves a 13% Exact Match score improvement over the baseline on the McTACO duration question answering task, showing that the two-stage fine-tuning approach succeeds when the discrepancy between the target and intermediary tasks are resolved.</abstract>
      <url hash="521cd9a4">2022.lrec-1.473</url>
      <bibkey>virgo-etal-2022-improving</bibkey>
      <pwccode url="https://github.com/felixgiov/udst-durationqa" additional="false">felixgiov/udst-durationqa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mc-taco">MC-TACO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
    </paper>
    <paper id="474">
      <title>Entity Linking over Nested Named Entities for <fixed-case>R</fixed-case>ussian</title>
      <author><first>Natalia</first><last>Loukachevitch</last></author>
      <author><first>Pavel</first><last>Braslavski</last></author>
      <author><first>Vladimir</first><last>Ivanov</last></author>
      <author><first>Tatiana</first><last>Batura</last></author>
      <author><first>Suresh</first><last>Manandhar</last></author>
      <author><first>Artem</first><last>Shelmanov</last></author>
      <author><first>Elena</first><last>Tutubalina</last></author>
      <pages>4458–4466</pages>
      <abstract>In this paper, we describe entity linking annotation over nested named entities in the recently released Russian NEREL dataset for information extraction. The NEREL collection is currently the largest Russian dataset annotated with entities and relations. It includes 933 news texts with annotation of 29 entity types and 49 relation types. The paper describes the main design principles behind NEREL’s entity linking annotation, provides its statistics, and reports evaluation results for several entity linking baselines. To date, 38,152 entity mentions in 933 documents are linked to Wikidata. The NEREL dataset is publicly available.</abstract>
      <url hash="dad3c91d">2022.lrec-1.474</url>
      <bibkey>loukachevitch-etal-2022-entity</bibkey>
      <pwccode url="https://github.com/nerel-ds/nerel" additional="false">nerel-ds/nerel</pwccode>
    </paper>
    <paper id="475">
      <title><fixed-case>H</fixed-case>i<fixed-case>NER</fixed-case>: A large <fixed-case>H</fixed-case>indi Named Entity Recognition Dataset</title>
      <author><first>Rudra</first><last>Murthy</last></author>
      <author><first>Pallab</first><last>Bhattacharjee</last></author>
      <author><first>Rahul</first><last>Sharnagat</last></author>
      <author><first>Jyotsana</first><last>Khatri</last></author>
      <author><first>Diptesh</first><last>Kanojia</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>4467–4476</pages>
      <abstract>Named Entity Recognition (NER) is a foundational NLP task that aims to provide class labels like Person, Location, Organisation, Time, and Number to words in free text. Named Entities can also be multi-word expressions where the additional I-O-B annotation information helps label them during the NER annotation process. While English and European languages have considerable annotated data for the NER task, Indian languages lack on that front- both in terms of quantity and following annotation standards. This paper releases a significantly sized standard-abiding Hindi NER dataset containing 109,146 sentences and 2,220,856 tokens, annotated with 11 tags. We discuss the dataset statistics in all their essential detail and provide an in-depth analysis of the NER tag-set used with our data. The statistics of tag-set in our dataset shows a healthy per-tag distribution especially for prominent classes like Person, Location and Organisation. Since the proof of resource-effectiveness is in building models with the resource and testing the model on benchmark data and against the leader-board entries in shared tasks, we do the same with the aforesaid data. We use different language models to perform the sequence labelling task for NER and show the efficacy of our data by performing a comparative evaluation with models trained on another dataset available for the Hindi NER task. Our dataset helps achieve a weighted F1 score of 88.78 with all the tags and 92.22 when we collapse the tag-set, as discussed in the paper. To the best of our knowledge, no available dataset meets the standards of volume (amount) and variability (diversity), as far as Hindi NER is concerned. We fill this gap through this work, which we hope will significantly help NLP for Hindi. We release this dataset with our code and models for further research at https://github.com/cfiltnlp/HiNER</abstract>
      <url hash="39dc5311">2022.lrec-1.475</url>
      <bibkey>murthy-etal-2022-hiner</bibkey>
      <pwccode url="https://github.com/cfiltnlp/hiner" additional="false">cfiltnlp/hiner</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hiner-collapsed-1">HiNER-collapsed</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hiner-original-1">HiNER-original</pwcdataset>
    </paper>
    <paper id="476">
      <title>Bootstrapping Text Anonymization Models with Distant Supervision</title>
      <author><first>Anthi</first><last>Papadopoulou</last></author>
      <author><first>Pierre</first><last>Lison</last></author>
      <author><first>Lilja</first><last>Øvrelid</last></author>
      <author><first>Ildikó</first><last>Pilán</last></author>
      <pages>4477–4487</pages>
      <abstract>We propose a novel method to bootstrap text anonymization models based on distant supervision. Instead of requiring manually labeled training data, the approach relies on a knowledge graph expressing the background information assumed to be publicly available about various individuals. This knowledge graph is employed to automatically annotate text documents including personal data about a subset of those individuals. More precisely, the method determines which text spans ought to be masked in order to guarantee k-anonymity, assuming an adversary with access to both the text documents and the background information expressed in the knowledge graph. The resulting collection of labeled documents is then used as training data to fine-tune a pre-trained language model for text anonymization. We illustrate this approach using a knowledge graph extracted from Wikidata and short biographical texts from Wikipedia. Evaluation results with a RoBERTa-based model and a manually annotated collection of 553 summaries showcase the potential of the approach, but also unveil a number of issues that may arise if the knowledge graph is noisy or incomplete. The results also illustrate that, contrary to most sequence labeling problems, the text anonymization task may admit several alternative solutions.</abstract>
      <url hash="120c9487">2022.lrec-1.476</url>
      <bibkey>papadopoulou-etal-2022-bootstrapping</bibkey>
    </paper>
    <paper id="477">
      <title>Natural Questions in <fixed-case>I</fixed-case>celandic</title>
      <author><first>Vésteinn</first><last>Snæbjarnarson</last></author>
      <author><first>Hafsteinn</first><last>Einarsson</last></author>
      <pages>4488–4496</pages>
      <abstract>We present the first extractive question answering (QA) dataset for Icelandic, Natural Questions in Icelandic (NQiI). Developing such datasets is important for the development and evaluation of Icelandic QA systems. It also aids in the development of QA methods that need to work for a wide range of morphologically and grammatically different languages in a multilingual setting. The dataset was created by asking contributors to come up with questions they would like to know the answer to. Later, they were tasked with finding answers to each others questions following a previously published methodology. The questions are Natural in the sense that they are real questions posed out of interest in knowing the answer. The complete dataset contains 18 thousand labeled entries of which 5,568 are directly suitable for training an extractive QA system for Icelandic. The dataset is a valuable resource for Icelandic which we demonstrate by creating and evaluating a system capable of extractive QA in Icelandic.</abstract>
      <url hash="26f5c3da">2022.lrec-1.477</url>
      <bibkey>snaebjarnarson-einarsson-2022-natural</bibkey>
    </paper>
    <paper id="478">
      <title><fixed-case>QA</fixed-case>4<fixed-case>IE</fixed-case>: A Quality Assurance Tool for Information Extraction</title>
      <author><first>Rafael Jimenez</first><last>Silva</last></author>
      <author><first>Kaushik</first><last>Gedela</last></author>
      <author><first>Alex</first><last>Marr</last></author>
      <author><first>Bart</first><last>Desmet</last></author>
      <author><first>Carolyn</first><last>Rose</last></author>
      <author><first>Chunxiao</first><last>Zhou</last></author>
      <pages>4497–4503</pages>
      <abstract>Quality assurance (QA) is an essential though underdeveloped part of the data annotation process. Although QA is supported to some extent in existing annotation tools, comprehensive support for QA is not standardly provided. In this paper we contribute QA4IE, a comprehensive QA tool for information extraction, which can (1) detect potential problems in text annotations in a timely manner, (2) accurately assess the quality of annotations, (3) visually display and summarize annotation discrepancies among annotation team members, (4) provide a comprehensive statistics report, and (5) support viewing of annotated documents interactively. This paper offers a competitive analysis comparing QA4IE and other popular annotation tools and demonstrates its features, usage, and effectiveness through a case study. The Python code, documentation, and demonstration video are available publicly at https://github.com/CC-RMD-EpiBio/QA4IE.</abstract>
      <url hash="1a1a84d5">2022.lrec-1.478</url>
      <bibkey>silva-etal-2022-qa4ie</bibkey>
      <pwccode url="https://github.com/cc-rmd-epibio/qa4ie" additional="false">cc-rmd-epibio/qa4ie</pwccode>
    </paper>
    <paper id="479">
      <title>A New Dataset for Topic-Based Paragraph Classification in Genocide-Related Court Transcripts</title>
      <author><first>Miriam</first><last>Schirmer</last></author>
      <author><first>Udo</first><last>Kruschwitz</last></author>
      <author><first>Gregor</first><last>Donabauer</last></author>
      <pages>4504–4512</pages>
      <abstract>Recent progress in natural language processing has been impressive in many different areas with transformer-based approaches setting new benchmarks for a wide range of applications. This development has also lowered the barriers for people outside the NLP community to tap into the tools and resources applied to a variety of domain-specific applications. The bottleneck however still remains the lack of annotated gold-standard collections as soon as one’s research or professional interest falls outside the scope of what is readily available. One such area is genocide-related research (also including the work of experts who have a professional interest in accessing, exploring and searching large-scale document collections on the topic, such as lawyers). We present GTC (Genocide Transcript Corpus), the first annotated corpus of genocide-related court transcripts which serves three purposes: (1) to provide a first reference corpus for the community, (2) to establish benchmark performances (using state-of-the-art transformer-based approaches) for the new classification task of paragraph identification of violence-related witness statements, (3) to explore first steps towards transfer learning within the domain. We consider our contribution to be addressing in particular this year’s hot topic on Language Technology for All.</abstract>
      <url hash="e4379dc7">2022.lrec-1.479</url>
      <bibkey>schirmer-etal-2022-new</bibkey>
      <pwccode url="https://github.com/miriamschirmer/genocide-transcript-corpus" additional="false">miriamschirmer/genocide-transcript-corpus</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/gtc-topic-based-paragraph-classification">Genocide Transcript Corpus (GTC): Topic-Based Paragraph Classification in Genocide-Related Court Transcripts</pwcdataset>
    </paper>
    <paper id="480">
      <title><fixed-case>D</fixed-case>eep<fixed-case>REF</fixed-case>: A Framework for Optimized Deep Learning-based Relation Classification</title>
      <author><first>Igor</first><last>Nascimento</last></author>
      <author><first>Rinaldo</first><last>Lima</last></author>
      <author><first>Adrian-Gabriel</first><last>Chifu</last></author>
      <author><first>Bernard</first><last>Espinasse</last></author>
      <author><first>Sébastien</first><last>Fournier</last></author>
      <pages>4513–4522</pages>
      <abstract>The Relation Extraction (RE) is an important basic Natural Language Processing (NLP) for many applications, such as search engines, recommender systems, question-answering systems and others. There are many studies in this subarea of NLP that continue to be explored, such as SemEval campaigns (2010 to 2018), or DDI Extraction (2013).For more than ten years, different RE systems using mainly statistical models have been proposed as well as the frameworks to develop them. This paper focuses on frameworks allowing to develop such RE systems using deep learning models. Such frameworks should make it possible to reproduce experiments of various deep learning models and pre-processing techniques proposed in various publications. Currently, there are very few frameworks of this type, and we propose a new open and optimizable framework, called DeepREF, which is inspired by the OpenNRE and REflex existing frameworks. DeepREF allows the employment of various deep learning models, to optimize their use, to identify the best inputs and to get better results with each data set for RE and compare with other experiments, making ablation studies possible. The DeepREF Framework is evaluated on several reference corpora from various application domains.</abstract>
      <url hash="aec10555">2022.lrec-1.480</url>
      <bibkey>nascimento-etal-2022-deepref</bibkey>
      <pwccode url="https://github.com/igorvlnascimento/deepref" additional="false">igorvlnascimento/deepref</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/tacred">TACRED</pwcdataset>
    </paper>
    <paper id="481">
      <title>Exploring Data Augmentation Strategies for Hate Speech Detection in <fixed-case>R</fixed-case>oman <fixed-case>U</fixed-case>rdu</title>
      <author><first>Ubaid</first><last>Azam</last></author>
      <author><first>Hammad</first><last>Rizwan</last></author>
      <author><first>Asim</first><last>Karim</last></author>
      <pages>4523–4531</pages>
      <abstract>In an era where social media platform users are growing rapidly, there has been a marked increase in hateful content being generated; to combat this, automatic hate speech detection systems are a necessity. For this purpose, researchers have recently focused their efforts on developing datasets, however, the vast majority of them have been generated for the English language, with only a few available for low-resource languages such as Roman Urdu. Furthermore, what few are available have small number of samples that pertain to hateful classes and these lack variations in topics and content. Thus, deep learning models trained on such datasets perform poorly when deployed in the real world. To improve performance the option of collecting and annotating more data can be very costly and time consuming. Thus, data augmentation techniques need to be explored to exploit already available datasets to improve model generalizability. In this paper, we explore different data augmentation techniques for the improvement of hate speech detection in Roman Urdu. We evaluate these augmentation techniques on two datasets. We are able to improve performance in the primary metric of comparison (F1 and Macro F1) as well as in recall, which is impertinent for human-in-the-loop AI systems.</abstract>
      <url hash="ac903825">2022.lrec-1.481</url>
      <bibkey>azam-etal-2022-exploring</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/hate-speech-and-offensive-language">Hate Speech and Offensive Language</pwcdataset>
    </paper>
    <paper id="482">
      <title>Incorporating <fixed-case>LIWC</fixed-case> in Neural Networks to Improve Human Trait and Behavior Analysis in Low Resource Scenarios</title>
      <author><first>Isil</first><last>Yakut Kilic</last></author>
      <author><first>Shimei</first><last>Pan</last></author>
      <pages>4532–4539</pages>
      <abstract>Psycholinguistic knowledge resources have been widely used in constructing features for text-based human trait and behavior analysis. Recently, deep neural network (NN)-based text analysis methods have gained dominance due to their high prediction performance. However, NN-based methods may not perform well in low resource scenarios where the ground truth data is limited (e.g., only a few hundred labeled training instances are available). In this research, we investigate diverse methods to incorporate Linguistic Inquiry and Word Count (LIWC), a widely-used psycholinguistic lexicon, in NN models to improve human trait and behavior analysis in low resource scenarios. We evaluate the proposed methods in two tasks: predicting delay discounting and predicting drug use based on social media posts. The results demonstrate that our methods perform significantly better than baselines that use only LIWC or only NN-based feature learning methods. They also performed significantly better than published results on the same dataset.</abstract>
      <url hash="29530636">2022.lrec-1.482</url>
      <bibkey>yakut-kilic-pan-2022-incorporating</bibkey>
    </paper>
    <paper id="483">
      <title>Using Sentence-level Classification Helps Entity Extraction from Material Science Literature</title>
      <author><first>Ankan</first><last>Mullick</last></author>
      <author><first>Shubhraneel</first><last>Pal</last></author>
      <author><first>Tapas</first><last>Nayak</last></author>
      <author><first>Seung-Cheol</first><last>Lee</last></author>
      <author><first>Satadeep</first><last>Bhattacharjee</last></author>
      <author><first>Pawan</first><last>Goyal</last></author>
      <pages>4540–4545</pages>
      <abstract>In the last few years, several attempts have been made on extracting information from material science research domain. Material Science research articles are a rich source of information about various entities related to material science such as names of the materials used for experiments, the computational software used along with its parameters, the method used in the experiments, etc. But the distribution of these entities is not uniform across different sections of research articles. Most of the sentences in the research articles do not contain any entity. In this work, we first use a sentence-level classifier to identify sentences containing at least one entity mention. Next, we apply the information extraction models only on the filtered sentences, to extract various entities of interest. Our experiments for named entity recognition in the material science research articles show that this additional sentence-level classification step helps to improve the F1 score by more than 4%.</abstract>
      <url hash="a12a7f50">2022.lrec-1.483</url>
      <bibkey>mullick-etal-2022-using</bibkey>
    </paper>
    <paper id="484">
      <title>A <fixed-case>T</fixed-case>witter Corpus for Named Entity Recognition in <fixed-case>T</fixed-case>urkish</title>
      <author><first>Buse</first><last>Çarık</last></author>
      <author><first>Reyyan</first><last>Yeniterzi</last></author>
      <pages>4546–4551</pages>
      <abstract>This paper introduces a new Turkish Twitter Named Entity Recognition dataset. The dataset, which consists of 5000 tweets from a year-long period, was labeled by multiple annotators with a high agreement score. The dataset is also diverse in terms of the named entity types as it contains not only person, organization, and location but also time, money, product, and tv-show categories. Our initial experiments with pretrained language models (like BertTurk) over this dataset returned F1 scores of around 80%. We share this dataset publicly.</abstract>
      <url hash="78364b82">2022.lrec-1.484</url>
      <bibkey>carik-yeniterzi-2022-twitter</bibkey>
      <pwccode url="https://github.com/su-nlp/sunlp-twitter-ner-dataset" additional="false">su-nlp/sunlp-twitter-ner-dataset</pwccode>
    </paper>
    <paper id="485">
      <title>A <fixed-case>STEP</fixed-case> towards Interpretable Multi-Hop Reasoning:Bridge Phrase Identification and Query Expansion</title>
      <author><first>Fan</first><last>Luo</last></author>
      <author><first>Mihai</first><last>Surdeanu</last></author>
      <pages>4552–4560</pages>
      <abstract>We propose an unsupervised method for the identification of bridge phrases in multi-hop question answering (QA). Our method constructs a graph of noun phrases from the question and the available context, and applies the Steiner tree algorithm to identify the minimal sub-graph that connects all question phrases. Nodes in the sub-graph that bridge loosely-connected or disjoint subsets of question phrases due to low-strength semantic relations are extracted as bridge phrases. The identified bridge phrases are then used to expand the query based on the initial question, helping in increasing the relevance of evidence that has little lexical overlap or semantic relation with the question. Through an evaluation on HotpotQA, a popular dataset for multi-hop QA, we show that our method yields: (a) improved evidence retrieval, (b) improved QA performance when using the retrieved sentences; and (c) effective and faithful explanations when answers are provided.</abstract>
      <url hash="d4bf1137">2022.lrec-1.485</url>
      <bibkey>luo-surdeanu-2022-step</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
    </paper>
    <paper id="486">
      <title>Question Generation and Answering for exploring Digital Humanities collections</title>
      <author><first>Frederic</first><last>Bechet</last></author>
      <author><first>Elie</first><last>Antoine</last></author>
      <author><first>Jérémy</first><last>Auguste</last></author>
      <author><first>Géraldine</first><last>Damnati</last></author>
      <pages>4561–4568</pages>
      <abstract>This paper introduces the question answering paradigm as a way to explore digitized archive collections for Social Science studies. In particular, we are interested in evaluating largely studied question generation and question answering approaches on a new type of documents, as a step forward beyond traditional benchmark evaluations. Question generation can be used as a way to provide enhanced training material for Machine Reading Question Answering algorithms but also has its own purpose in this paradigm, where relevant questions can be used as a way to create explainable links between documents. To this end, generating large amounts of question is not the only motivation, but we need to include qualitative and semantic control to the generation process. We propose a new approach for question generation, relying on a BART Transformer based generative model, for which input data are enriched by semantic constraints. Question generation and answering are evaluated on several French corpora, and the whole approach is validated on a new corpus of digitized archive collection of a French Social Science journal.</abstract>
      <url hash="e41459c2">2022.lrec-1.486</url>
      <bibkey>bechet-etal-2022-question</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/framenet">FrameNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="487">
      <title>Evaluating Retrieval for Multi-domain Scientific Publications</title>
      <author><first>Nancy</first><last>Ide</last></author>
      <author><first>Keith</first><last>Suderman</last></author>
      <author><first>Jingxuan</first><last>Tu</last></author>
      <author><first>Marc</first><last>Verhagen</last></author>
      <author><first>Shanan</first><last>Peters</last></author>
      <author><first>Ian</first><last>Ross</last></author>
      <author><first>John</first><last>Lawson</last></author>
      <author><first>Andrew</first><last>Borg</last></author>
      <author><first>James</first><last>Pustejovsky</last></author>
      <pages>4569–4576</pages>
      <abstract>This paper provides an overview of the xDD/LAPPS Grid framework and provides results of evaluating the AskMe retrievalengine using the BEIR benchmark datasets. Our primary goal is to determine a solid baseline of performance to guide furtherdevelopment of our retrieval capabilities. Beyond this, we aim to dig deeper to determine when and why certain approachesperform well (or badly) on both in-domain and out-of-domain data, an issue that has to date received relatively little attention.</abstract>
      <url hash="8d9a5944">2022.lrec-1.487</url>
      <bibkey>ide-etal-2022-evaluating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/beir">BEIR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/bioasq">BioASQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/trec-covid">TREC-COVID</pwcdataset>
    </paper>
    <paper id="488">
      <title>Modeling <fixed-case>D</fixed-case>utch Medical Texts for Detecting Functional Categories and Levels of <fixed-case>COVID</fixed-case>-19 Patients</title>
      <author><first>Jenia</first><last>Kim</last></author>
      <author><first>Stella</first><last>Verkijk</last></author>
      <author><first>Edwin</first><last>Geleijn</last></author>
      <author><first>Marieke</first><last>van der Leeden</last></author>
      <author><first>Carel</first><last>Meskers</last></author>
      <author><first>Caroline</first><last>Meskers</last></author>
      <author><first>Sabina</first><last>van der Veen</last></author>
      <author><first>Piek</first><last>Vossen</last></author>
      <author><first>Guy</first><last>Widdershoven</last></author>
      <pages>4577–4585</pages>
      <abstract>Electronic Health Records contain a lot of information in natural language that is not expressed in the structured clinical data. Especially in the case of new diseases such as COVID-19, this information is crucial to get a better understanding of patient recovery patterns and factors that may play a role in it. However, the language in these records is very different from standard language and generic natural language processing tools cannot easily be applied out-of-the-box. In this paper, we present a fine-tuned Dutch language model specifically developed for the language in these health records that can determine the functional level of patients according to a standard coding framework from the World Health Organization. We provide evidence that our classification performs at a sufficient level to generate patient recovery patterns that can be used in the future to analyse factors that contribute to the rehabilitation of COVID-19 patients and to predict individual patient recovery of functioning.</abstract>
      <url hash="711bef07">2022.lrec-1.488</url>
      <bibkey>kim-etal-2022-modeling</bibkey>
    </paper>
    <paper id="489">
      <title>Hierarchical Aggregation of Dialectal Data for <fixed-case>A</fixed-case>rabic Dialect Identification</title>
      <author><first>Nurpeiis</first><last>Baimukan</last></author>
      <author><first>Houda</first><last>Bouamor</last></author>
      <author><first>Nizar</first><last>Habash</last></author>
      <pages>4586–4596</pages>
      <abstract>Arabic is a collection of dialectal variants that are historically related but significantly different. These differences can be seen across regions, countries, and even cities in the same countries. Previous work on Arabic Dialect identification has focused mainly on specific dialect levels (region, country, province, or city) using level-specific resources; and different efforts used different schemas and labels. In this paper, we present the first effort aiming at defining a standard unified three-level hierarchical schema (region-country-city) for dialectal Arabic classification. We map 29 different data sets to this unified schema, and use the common mapping to facilitate aggregating these data sets. We test the value of such aggregation by building language models and using them in dialect identification. We make our label mapping code and aggregated language models publicly available.</abstract>
      <url hash="cba35ea2">2022.lrec-1.489</url>
      <bibkey>baimukan-etal-2022-hierarchical</bibkey>
    </paper>
    <paper id="490">
      <title>Investigating Active Learning Sampling Strategies for Extreme Multi Label Text Classification</title>
      <author><first>Lukas</first><last>Wertz</last></author>
      <author><first>Katsiaryna</first><last>Mirylenka</last></author>
      <author><first>Jonas</first><last>Kuhn</last></author>
      <author><first>Jasmina</first><last>Bogojeska</last></author>
      <pages>4597–4605</pages>
      <abstract>Large scale, multi-label text datasets with high numbers of different classes are expensive to annotate, even more so if they deal with domain specific language. In this work, we aim to build classifiers on these datasets using Active Learning in order to reduce the labeling effort. We outline the challenges when dealing with extreme multi-label settings and show the limitations of existing Active Learning strategies by focusing on their effectiveness as well as efficiency in terms of computational cost. In addition, we present five multi-label datasets which were compiled from hierarchical classification tasks to serve as benchmarks in the context of extreme multi-label classification for future experiments. Finally, we provide insight into multi-class, multi-label evaluation and present an improved classifier architecture on top of pre-trained transformer language models.</abstract>
      <url hash="9d3d6574">2022.lrec-1.490</url>
      <bibkey>wertz-etal-2022-investigating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/rcv1">RCV1</pwcdataset>
    </paper>
    <paper id="491">
      <title><fixed-case>G</fixed-case>erman Light Verb Constructions in Business Process Models</title>
      <author><first>Kristin</first><last>Kutzner</last></author>
      <author><first>Ralf</first><last>Laue</last></author>
      <pages>4606–4610</pages>
      <abstract>We present a resource of German light verb constructions extracted from textual labels in graphical business process models. Those models depict the activities in processes in an organization in a semi-formal way. From a large range of sources, we compiled a repository of 2,301 business process models. Their textual labels (altogether 52,963 labels) were analyzed. This produced a list of 5,246 occurrences of 846 light verb constructions. We found that the light verb constructions that occur in business process models differ from light verb constructions that have been analyzed in other texts. Hence, we conclude that texts in graphical business process models represent a specific type of texts that is worth to be studied on its own. We think that our work is a step towards better automatic analysis of business process models because understanding the actual meaning of activity labels is a prerequisite for detecting certain types of modelling problems.</abstract>
      <url hash="47cefbfd">2022.lrec-1.491</url>
      <bibkey>kutzner-laue-2022-german</bibkey>
    </paper>
    <paper id="492">
      <title><fixed-case>P</fixed-case>hys<fixed-case>NLU</fixed-case>: A Language Resource for Evaluating Natural Language Understanding and Explanation Coherence in Physics</title>
      <author><first>Jordan</first><last>Meadows</last></author>
      <author><first>Zili</first><last>Zhou</last></author>
      <author><first>André</first><last>Freitas</last></author>
      <pages>4611–4619</pages>
      <abstract>In order for language models to aid physics research, they must first encode representations of mathematical and natural language discourse which lead to coherent explanations, with correct ordering and relevance of statements. We present a collection of datasets developed to evaluate the performance of language models in this regard, which measure capabilities with respect to sentence ordering, position, section prediction, and discourse coherence. Analysis of the data reveals the classes of arguments and sub-disciplines which are most common in physics discourse, as well as the sentence-level frequency of equations and expressions. We present baselines that demonstrate how contemporary language models are challenged by coherence related tasks in physics, even when trained on mathematical natural language objectives.</abstract>
      <url hash="4cbeff7a">2022.lrec-1.492</url>
      <bibkey>meadows-etal-2022-physnlu</bibkey>
      <pwccode url="https://github.com/jmeadows17/physnlu" additional="false">jmeadows17/physnlu</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/physnlu">PhysNLU</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-2">WikiText-2</pwcdataset>
    </paper>
    <paper id="493">
      <title><fixed-case>HECTOR</fixed-case>: A Hybrid <fixed-case>TE</fixed-case>xt <fixed-case>S</fixed-case>implifi<fixed-case>C</fixed-case>ation <fixed-case>TO</fixed-case>ol for Raw Texts in <fixed-case>F</fixed-case>rench</title>
      <author><first>Amalia</first><last>Todirascu</last></author>
      <author><first>Rodrigo</first><last>Wilkens</last></author>
      <author><first>Eva</first><last>Rolin</last></author>
      <author><first>Thomas</first><last>François</last></author>
      <author><first>Delphine</first><last>Bernhard</last></author>
      <author><first>Núria</first><last>Gala</last></author>
      <pages>4620–4630</pages>
      <abstract>Reducing the complexity of texts by applying an Automatic Text Simplification (ATS) system has been sparking interest inthe area of Natural Language Processing (NLP) for several years and a number of methods and evaluation campaigns haveemerged targeting lexical and syntactic transformations. In recent years, several studies exploit deep learning techniques basedon very large comparable corpora. Yet the lack of large amounts of corpora (original-simplified) for French has been hinderingthe development of an ATS tool for this language. In this paper, we present our system, which is based on a combination ofmethods relying on word embeddings for lexical simplification and rule-based strategies for syntax and discourse adaptations. We present an evaluation of the lexical, syntactic and discourse-level simplifications according to automatic and humanevaluations. We discuss the performances of our system at the lexical, syntactic, and discourse levels</abstract>
      <url hash="507355f0">2022.lrec-1.493</url>
      <bibkey>todirascu-etal-2022-hector</bibkey>
    </paper>
    <paper id="494">
      <title><fixed-case>A</fixed-case>i<fixed-case>RO</fixed-case> - an Interactive Learning Tool for Children at Risk of Dyslexia</title>
      <author><first>Peter Juel</first><last>Henrichsen</last></author>
      <author><first>Stine</first><last>Fuglsang Engmose</last></author>
      <pages>4631–4636</pages>
      <abstract>This paper presents the AiRO learning tool, which is designed for use in classrooms and homes by children at risk of developing dyslexia. The tool is based on the client-server architecture with a graphical and auditive front end (providing the interaction with the learner) and all NLP-related components located at the back end (analysing the pupil’s input, deciding on the system’s response, preparing speech synthesis and other feedback, logging the pupil’s performance etc). AiRO software consists of independent modules for easy maintenance, e.g., upgrading the didactics or preparing AiROs for other languages. This paper also reports on our first tests ‘in vivo’ (November 2021) with 49 pupils (aged 6). The subjects completed 16 AiRO sessions over a four-week period. The subjects were pre- and post-tested on spelling and reading. The experimental group significantly out-performed the control group, suggesting that a new IT-supported teaching strategy may be within reach. A collection of AiRO resources (language materials, software, synthetic voice) are available as open source. At LREC, we shall present a demo of the AiRO learning tool.</abstract>
      <url hash="f2769479">2022.lrec-1.494</url>
      <bibkey>henrichsen-fuglsang-engmose-2022-airo</bibkey>
    </paper>
    <paper id="495">
      <title>Creating a Basic Language Resource Kit for <fixed-case>F</fixed-case>aroese</title>
      <author><first>Annika</first><last>Simonsen</last></author>
      <author><first>Sandra Saxov</first><last>Lamhauge</last></author>
      <author><first>Iben Nyholm</first><last>Debess</last></author>
      <author><first>Peter Juel</first><last>Henrichsen</last></author>
      <pages>4637–4643</pages>
      <abstract>The biggest challenges we face in developing LR and LT for Faroese is the lack of existing resources. A few resources already exist for Faroese, but many of them are either of insufficient size and quality or are not easily accessible. Therefore, the Faroese ASR project, Ravnur, set out to make a BLARK for Faroese. The BLARK is still in the making, but many of its resources have already been produced or collected. The LR status is framed by mentioning existing LR of relevant size and quality. The specific components of the BLARK are presented as well as the working principles behind the BLARK. The BLARK will be a pillar in Faroese LR, being relatively substantial in both size, quality, and diversity. It will be open-source, inviting other small languages to use it as an inspiration to create their own BLARK. We comment on the faulty yet sprouting LT situation in the Faroe Islands. The LR and LT challenges are not solved with just a BLARK. Some initiatives are therefore proposed to better the prospects of Faroese LT. The open-source principle of the project should facilitate further development.</abstract>
      <url hash="f30693c4">2022.lrec-1.495</url>
      <bibkey>simonsen-etal-2022-creating</bibkey>
    </paper>
    <paper id="496">
      <title>Developing a Spell and Grammar Checker for <fixed-case>I</fixed-case>celandic using an Error Corpus</title>
      <author><first>Hulda</first><last>Óladóttir</last></author>
      <author><first>Þórunn</first><last>Arnardóttir</last></author>
      <author><first>Anton</first><last>Ingason</last></author>
      <author><first>Vilhjálmur</first><last>Þorsteinsson</last></author>
      <pages>4644–4653</pages>
      <abstract>A lack of datasets for spelling and grammatical error correction in Icelandic, along with language-specific issues, has caused a dearth of spell and grammar checking systems for the language. We present the first open-source spell and grammar checking tool for Icelandic, using an error corpus at all stages. This error corpus was in part created to aid in the development of the tool. The system is built with a rule-based tool stack comprising a tokenizer, a morphological tagger, and a parser. For token-level error annotation, tokenization rules, word lists, and a trigram model are used in error detection and correction. For sentence-level error annotation, we use specific error grammar rules in the parser as well as regex-like patterns to search syntax trees. The error corpus gives valuable insight into the errors typically made when Icelandic text is written, and guided each development phase in a test-driven manner. We assess the system’s performance with both automatic and human evaluation, using the test set in the error corpus as a reference in the automatic evaluation. The data in the error corpus development set proved useful in various ways for error detection and correction.</abstract>
      <url hash="ec35502d">2022.lrec-1.496</url>
      <bibkey>oladottir-etal-2022-developing</bibkey>
    </paper>
    <paper id="497">
      <title>The <fixed-case>T</fixed-case>alk<fixed-case>M</fixed-case>oves Dataset: K-12 Mathematics Lesson Transcripts Annotated for Teacher and Student Discursive Moves</title>
      <author><first>Abhijit</first><last>Suresh</last></author>
      <author><first>Jennifer</first><last>Jacobs</last></author>
      <author><first>Charis</first><last>Harty</last></author>
      <author><first>Margaret</first><last>Perkoff</last></author>
      <author><first>James H.</first><last>Martin</last></author>
      <author><first>Tamara</first><last>Sumner</last></author>
      <pages>4654–4662</pages>
      <abstract>Transcripts of teaching episodes can be effective tools to understand discourse patterns in classroom instruction. According to most educational experts, sustained classroom discourse is a critical component of equitable, engaging, and rich learning environments for students. This paper describes the TalkMoves dataset, composed of 567 human-annotated K-12 mathematics lesson transcripts (including entire lessons or portions of lessons) derived from video recordings. The set of transcripts primarily includes in-person lessons with whole-class discussions and/or small group work, as well as some online lessons. All of the transcripts are human-transcribed, segmented by the speaker (teacher or student), and annotated at the sentence level for ten discursive moves based on accountable talk theory. In addition, the transcripts include utterance-level information in the form of dialogue act labels based on the Switchboard Dialog Act Corpus. The dataset can be used by educators, policymakers, and researchers to understand the nature of teacher and student discourse in K-12 math classrooms. Portions of this dataset have been used to develop the TalkMoves application, which provides teachers with automated, immediate, and actionable feedback about their mathematics instruction.</abstract>
      <url hash="c26a0770">2022.lrec-1.497</url>
      <bibkey>suresh-etal-2022-talkmoves</bibkey>
      <pwccode url="https://github.com/sumnerlab/talkmoves" additional="false">sumnerlab/talkmoves</pwccode>
    </paper>
    <paper id="498">
      <title>Automating Idea Unit Segmentation and Alignment for Assessing Reading Comprehension via Summary Protocol Analysis</title>
      <author><first>Marcello</first><last>Gecchele</last></author>
      <author><first>Hiroaki</first><last>Yamada</last></author>
      <author><first>Takenobu</first><last>Tokunaga</last></author>
      <author><first>Yasuyo</first><last>Sawaki</last></author>
      <author><first>Mika</first><last>Ishizuka</last></author>
      <pages>4663–4673</pages>
      <abstract>In this paper, we approach summary evaluation from an applied linguistics (AL) point of view. We provide computational tools to AL researchers to simplify the process of Idea Unit (IU) segmentation. The IU is a segmentation unit that can identify chunks of information. These chunks can be compared across documents to measure the content overlap between a summary and its source text. We propose a full revision of the annotation guidelines to allow machine implementation. The new guideline also improves the inter-annotator agreement, rising from 0.547 to 0.785 (Cohen’s Kappa). We release L2WS 2021, a IU gold standard corpus composed of 40 manually annotated student summaries. We propose IUExtract; i.e. the first automatic segmentation algorithm based on the IU. The algorithm was tested over the L2WS 2021 corpus. Our results are promising, achieving a precision of 0.789 and a recall of 0.844. We tested an existing approach to IU alignment via word embeddings with the state of the art model SBERT. The recorded precision for the top 1 aligned pair of IUs was 0.375. We deemed this result insufficient for effective automatic alignment. We propose “SAT”, an online tool to facilitate the collection of alignment gold standards for future training.</abstract>
      <url hash="4fe162cc">2022.lrec-1.498</url>
      <bibkey>gecchele-etal-2022-automating</bibkey>
    </paper>
    <paper id="499">
      <title><fixed-case>IRAC</fixed-case>: A Domain-Specific Annotated Corpus of Implicit Reasoning in Arguments</title>
      <author><first>Keshav</first><last>Singh</last></author>
      <author><first>Naoya</first><last>Inoue</last></author>
      <author><first>Farjana Sultana</first><last>Mim</last></author>
      <author><first>Shoichi</first><last>Naito</last></author>
      <author><first>Kentaro</first><last>Inui</last></author>
      <pages>4674–4683</pages>
      <abstract>The task of implicit reasoning generation aims to help machines understand arguments by inferring plausible reasonings (usually implicit) between argumentative texts. While this task is easy for humans, machines still struggle to make such inferences and deduce the underlying reasoning. To solve this problem, we hypothesize that as human reasoning is guided by innate collection of domain-specific knowledge, it might be beneficial to create such a domain-specific corpus for machines. As a starting point, we create the first domain-specific resource of implicit reasonings annotated for a wide range of arguments, which can be leveraged to empower machines with better implicit reasoning generation ability. We carefully design an annotation framework to collect them on a large scale through crowdsourcing and show the feasibility of creating a such a corpus at a reasonable cost and high-quality. Our experiments indicate that models trained with domain-specific implicit reasonings significantly outperform domain-general models in both automatic and human evaluations. To facilitate further research towards implicit reasoning generation in arguments, we present an in-depth analysis of our corpus and crowdsourcing methodology, and release our materials (i.e., crowdsourcing guidelines and domain-specific resource of implicit reasonings).</abstract>
      <url hash="46c8d1ab">2022.lrec-1.499</url>
      <bibkey>singh-etal-2022-irac</bibkey>
      <pwccode url="https://github.com/cl-tohoku/irac_2022" additional="false">cl-tohoku/irac_2022</pwccode>
    </paper>
    <paper id="500">
      <title>Conversational Speech Recognition Needs Data? Experiments with <fixed-case>A</fixed-case>ustrian <fixed-case>G</fixed-case>erman</title>
      <author><first>Julian</first><last>Linke</last></author>
      <author><first>Philip N.</first><last>Garner</last></author>
      <author><first>Gernot</first><last>Kubin</last></author>
      <author><first>Barbara</first><last>Schuppler</last></author>
      <pages>4684–4691</pages>
      <abstract>Conversational speech represents one of the most complex of automatic speech recognition (ASR) tasks owing to the high inter-speaker variation in both pronunciation and conversational dynamics. Such complexity is particularly sensitive to low-resourced (LR) scenarios. Recent developments in self-supervision have allowed such scenarios to take advantage of large amounts of otherwise unrelated data. In this study, we characterise an (LR) Austrian German conversational task. We begin with a non-pre-trained baseline and show that fine-tuning of a model pre-trained using self-supervision leads to improvements consistent with those in the literature; this extends to cases where a lexicon and language model are included. We also show that the advantage of pre-training indeed arises from the larger database rather than the self-supervision. Further, by use of a leave-one-conversation out technique, we demonstrate that robustness problems remain with respect to inter-speaker and inter-conversation variation. This serves to guide where future research might best be focused in light of the current state-of-the-art.</abstract>
      <url hash="8e51a11e">2022.lrec-1.500</url>
      <bibkey>linke-etal-2022-conversational</bibkey>
    </paper>
    <paper id="501">
      <title>A Benchmark Corpus for the Detection of Automatically Generated Text in Academic Publications</title>
      <author><first>Vijini</first><last>Liyanage</last></author>
      <author><first>Davide</first><last>Buscaldi</last></author>
      <author><first>Adeline</first><last>Nazarenko</last></author>
      <pages>4692–4700</pages>
      <abstract>Automatic text generation based on neural language models has achieved performance levels that make the generated text almost indistinguishable from those written by humans. Despite the value that text generation can have in various applications, it can also be employed for malicious tasks. The diffusion of such practices represent a threat to the quality of academic publishing. To address these problems, we propose in this paper two datasets comprised of artificially generated research content: a completely synthetic dataset and a partial text substitution dataset. In the first case, the content is completely generated by the GPT-2 model after a short prompt extracted from original papers. The partial or hybrid dataset is created by replacing several sentences of abstracts with sentences that are generated by the Arxiv-NLP model. We evaluate the quality of the datasets comparing the generated texts to aligned original texts using fluency metrics such as BLEU and ROUGE. The more natural the artificial texts seem, the more difficult they are to detect and the better is the benchmark. We also evaluate the difficulty of the task of distinguishing original from generated text by using state-of-the-art classification models.</abstract>
      <url hash="c1c26430">2022.lrec-1.501</url>
      <bibkey>liyanage-etal-2022-benchmark</bibkey>
    </paper>
    <paper id="502">
      <title>Building a Dataset for Automatically Learning to Detect Questions Requiring Clarification</title>
      <author><first>Ivano</first><last>Lauriola</last></author>
      <author><first>Kevin</first><last>Small</last></author>
      <author><first>Alessandro</first><last>Moschitti</last></author>
      <pages>4701–4707</pages>
      <abstract>Question Answering (QA) systems aim to return correct and concise answers in response to user questions. QA research generally assumes all questions are intelligible and unambiguous, which is unrealistic in practice as questions frequently encountered by virtual assistants are ambiguous or noisy. In this work, we propose to make QA systems more robust via the following two-step process: (1) classify if the input question is intelligible and (2) for such questions with contextual ambiguity, return a clarification question. We describe a new open-domain clarification corpus containing user questions sampled from Quora, which is useful for building machine learning approaches to solving these tasks.</abstract>
      <url hash="f5d0532a">2022.lrec-1.502</url>
      <bibkey>lauriola-etal-2022-building</bibkey>
    </paper>
    <paper id="503">
      <title>The <fixed-case>ALPIN</fixed-case> Sentiment Dictionary: <fixed-case>A</fixed-case>ustrian Language Polarity in Newspapers</title>
      <author><first>Thomas</first><last>Kolb</last></author>
      <author><first>Sekanina</first><last>Katharina</last></author>
      <author><first>Bettina Manuela Johanna</first><last>Kern</last></author>
      <author><first>Julia</first><last>Neidhardt</last></author>
      <author><first>Tanja</first><last>Wissik</last></author>
      <author><first>Andreas</first><last>Baumann</last></author>
      <pages>4708–4716</pages>
      <abstract>This paper introduces the Austrian German sentiment dictionary ALPIN to account for the lack of resources for dictionary-based sentiment analysis in this specific variety of German, which is characterized by lexical idiosyncrasies that also affect word sentiment. The proposed language resource is based on Austrian news media in the field of politics, an austriacism list based on different resources and a posting data set based on a popular Austrian news media. Different resources are used to increase the diversity of the resulting language resource. Extensive crowd-sourcing is performed followed by evaluation and automatic conversion into sentiment scores. We show that crowd-sourcing enables the creation of a sentiment dictionary for the Austrian German domain. Additionally, the different parts of the sentiment dictionary are evaluated to show their impact on the resulting resource. Furthermore, the proposed dictionary is utilized in a web application and available for future research and free to use for anyone.</abstract>
      <url hash="3a2a8051">2022.lrec-1.503</url>
      <bibkey>kolb-etal-2022-alpin</bibkey>
    </paper>
    <paper id="504">
      <title>Text Classification and Prediction in the Legal Domain</title>
      <author><first>Minh-Quoc</first><last>Nghiem</last></author>
      <author><first>Paul</first><last>Baylis</last></author>
      <author><first>André</first><last>Freitas</last></author>
      <author><first>Sophia</first><last>Ananiadou</last></author>
      <pages>4717–4722</pages>
      <abstract>We present a case study on the application of text classification and legal judgment prediction for flight compensation. We combine transformer-based classification models to classify responses from airlines and incorporate text data with other data types to predict a legal claim being successful. Our experimental evaluations show that our models achieve consistent and significant improvements over baselines and even outperformed human prediction when predicting a claim being successful. These models were integrated into an existing claim management system, providing substantial productivity gains for handling the case lifecycle, currently supporting several thousands of monthly processes.</abstract>
      <url hash="cbf4788d">2022.lrec-1.504</url>
      <bibkey>nghiem-etal-2022-text</bibkey>
    </paper>
    <paper id="505">
      <title><fixed-case>I</fixed-case> still have Time(s): Extending <fixed-case>H</fixed-case>eidel<fixed-case>T</fixed-case>ime for <fixed-case>G</fixed-case>erman Texts</title>
      <author><first>Andy</first><last>Luecking</last></author>
      <author><first>Manuel</first><last>Stoeckel</last></author>
      <author><first>Giuseppe</first><last>Abrami</last></author>
      <author><first>Alexander</first><last>Mehler</last></author>
      <pages>4723–4728</pages>
      <abstract>HeidelTime is one of the most widespread and successful tools for detecting temporal expressions in texts. Since HeidelTime’s pattern matching system is based on regular expression, it can be extended in a convenient way. We present such an extension for the German resources of HeidelTime: HeidelTimeExt. The extension has been brought about by means of observing false negatives within real world texts and various time banks. The gain in coverage is 2.7 % or 8.5 %, depending on the admitted degree of potential overgeneralization. We describe the development of HeidelTimeExt, its evaluation on text samples from various genres, and share some linguistic observations. HeidelTimeExt can be obtained from https://github.com/texttechnologylab/heideltime.</abstract>
      <url hash="4e2405f0">2022.lrec-1.505</url>
      <bibkey>luecking-etal-2022-still</bibkey>
      <pwccode url="https://github.com/texttechnologylab/heideltime" additional="false">texttechnologylab/heideltime</pwccode>
    </paper>
    <paper id="506">
      <title>Morphological Complexity of Children Narratives in Eight Languages</title>
      <author><first>Gordana</first><last>Hržica</last></author>
      <author><first>Chaya</first><last>Liebeskind</last></author>
      <author><first>Kristina Š.</first><last>Despot</last></author>
      <author><first>Olga</first><last>Dontcheva-Navratilova</last></author>
      <author><first>Laura</first><last>Kamandulytė-Merfeldienė</last></author>
      <author><first>Sara</first><last>Košutar</last></author>
      <author><first>Matea</first><last>Kramarić</last></author>
      <author><first>Giedrė</first><last>Valūnaitė Oleškevičienė</last></author>
      <pages>4729–4738</pages>
      <abstract>The aim of this study was to compare the morphological complexity in a corpus representing the language production of younger and older children across different languages. The language samples were taken from the Frog Story subcorpus of the CHILDES corpora, which comprises oral narratives collected by various researchers between 1990 and 2005. We extracted narratives by typically developing, monolingual, middle-class children. Additionally, samples of Lithuanian language, collected according to the same principles, were added. The corpus comprises 249 narratives evenly distributed across eight languages: Croatian, English, French, German, Italian, Lithuanian, Russian and Spanish. Two subcorpora were formed for each language: a younger children corpus and an older children corpus. Four measures of morphological complexity were calculated for each subcorpus: Bane, Kolmogorov, Word entropy and Relative entropy of word structure. The results showed that younger children corpora had lower morphological complexity than older children corpora for all four measures for Spanish and Russian. Reversed results were obtained for English and French, and the results for the remaining four languages showed variation. Relative entropy of word structure proved to be indicative of age differences. Word entropy and relative entropy of word structure show potential to demonstrate typological differences.</abstract>
      <url hash="0002a55e">2022.lrec-1.506</url>
      <bibkey>hrzica-etal-2022-morphological</bibkey>
    </paper>
    <paper id="507">
      <title><fixed-case>EXPRES</fixed-case> Corpus for A Field-specific Automated Exploratory Study of <fixed-case>L</fixed-case>2 <fixed-case>E</fixed-case>nglish Expert Scientific Writing</title>
      <author><first>Ana-Maria</first><last>Bucur</last></author>
      <author><first>Madalina</first><last>Chitez</last></author>
      <author><first>Valentina</first><last>Muresan</last></author>
      <author><first>Andreea</first><last>Dinca</last></author>
      <author><first>Roxana</first><last>Rogobete</last></author>
      <pages>4739–4746</pages>
      <abstract>Field Specific Expert Scientific Writing in English as a Lingua Franca is essential for the effective research networking and dissemination worldwide. Extracting the linguistic profile of the research articles written in L2 English can help young researchers and expert scholars in various disciplines adapt to the scientific writing norms of their communities of practice. In this exploratory study, we present and test an automated linguistic assessment model that includes features relevant for the cross-disciplinary second language framework: Text Complexity Analysis features, such as Syntactic and Lexical Complexity, and Field Specific Academic Word Lists. We analyse how these features vary across four disciplinary fields (Economics, IT, Linguistics and Political Science) in a corpus of L2-English Expert Scientific Writing, part of the EXPRES corpus (Corpus of Expert Writing in Romanian and English). The variation in field specific writing is also analysed in groups of linguistic features extracted from the higher visibility (Hv) versus lower visibility (Lv) journals. After applying lexical sophistication, lexical variation and syntactic complexity formulae, significant differences between disciplines were identified, mainly that research articles from Lv journals have higher lexical complexity, but lower syntactic complexity than articles from Hv journals; while academic vocabulary proved to have discipline specific variation.</abstract>
      <url hash="4c0cf76f">2022.lrec-1.507</url>
      <bibkey>bucur-etal-2022-expres</bibkey>
    </paper>
    <paper id="508">
      <title>An Evaluation Framework for Legal Document Summarization</title>
      <author><first>Ankan</first><last>Mullick</last></author>
      <author><first>Abhilash</first><last>Nandy</last></author>
      <author><first>Manav</first><last>Kapadnis</last></author>
      <author><first>Sohan</first><last>Patnaik</last></author>
      <author><first>Raghav</first><last>R</last></author>
      <author><first>Roshni</first><last>Kar</last></author>
      <pages>4747–4753</pages>
      <abstract>A law practitioner has to go through numerous lengthy legal case proceedings for their practices of various categories, such as land dispute, corruption, etc. Hence, it is important to summarize these documents, and ensure that summaries contain phrases with intent matching the category of the case. To the best of our knowledge, there is no evaluation metric that evaluates a summary based on its intent. We propose an automated intent-based summarization metric, which shows a better agreement with human evaluation as compared to other automated metrics like BLEU, ROUGE-L etc. in terms of human satisfaction. We also curate a dataset by annotating intent phrases in legal documents, and show a proof of concept as to how this system can be automated.</abstract>
      <url hash="e9102680">2022.lrec-1.508</url>
      <bibkey>mullick-etal-2022-evaluation</bibkey>
      <pwccode url="https://github.com/manavkapadnis/legalevaluation_lrec2022" additional="false">manavkapadnis/legalevaluation_lrec2022</pwccode>
    </paper>
    <paper id="509">
      <title>Complex Labelling and Similarity Prediction in Legal Texts: Automatic Analysis of <fixed-case>F</fixed-case>rance’s Court of Cassation Rulings</title>
      <author><first>Thibault</first><last>Charmet</last></author>
      <author><first>Inès</first><last>Cherichi</last></author>
      <author><first>Matthieu</first><last>Allain</last></author>
      <author><first>Urszula</first><last>Czerwinska</last></author>
      <author><first>Amaury</first><last>Fouret</last></author>
      <author><first>Benoît</first><last>Sagot</last></author>
      <author><first>Rachel</first><last>Bawden</last></author>
      <pages>4754–4766</pages>
      <abstract>Detecting divergences in the applications of the law (where the same legal text is applied differently by two rulings) is an important task. It is the mission of the French Cour de Cassation. The first step in the detection of divergences is to detect similar cases, which is currently done manually by experts. They rely on summarised versions of the rulings (syntheses and keyword sequences), which are currently produced manually and are not available for all rulings. There is also a high degree of variability in the keyword choices and the level of granularity used. In this article, we therefore aim to provide automatic tools to facilitate the search for similar rulings. We do this by (i) providing automatic keyword sequence generation models, which can be used to improve the coverage of the analysis, and (ii) providing measures of similarity based on the available texts and augmented with predicted keyword sequences. Our experiments show that the predictions improve correlations of automatically obtained similarities against our specially colelcted human judgments of similarity.</abstract>
      <url hash="1f8f9949">2022.lrec-1.509</url>
      <bibkey>charmet-etal-2022-complex</bibkey>
      <pwccode url="https://github.com/rbawden/similarity-cour-de-cassation" additional="false">rbawden/similarity-cour-de-cassation</pwccode>
    </paper>
    <paper id="510">
      <title><fixed-case>C</fixed-case>yrillic-<fixed-case>MNIST</fixed-case>: a <fixed-case>C</fixed-case>yrillic Version of the <fixed-case>MNIST</fixed-case> Dataset</title>
      <author><first>Bolat</first><last>Tleubayev</last></author>
      <author><first>Zhanel</first><last>Zhexenova</last></author>
      <author><first>Kenessary</first><last>Koishybay</last></author>
      <author><first>Anara</first><last>Sandygulova</last></author>
      <pages>4767–4773</pages>
      <abstract>This paper presents a new handwritten dataset, Cyrillic-MNIST, a Cyrillic version of the MNIST dataset, comprising of 121,234 samples of 42 Cyrillic letters. The performance of Cyrillic-MNIST is evaluated using standard deep learning approaches and is compared to the Extended MNIST (EMNIST) dataset. The dataset is available at https://github.com/bolattleubayev/cmnist</abstract>
      <url hash="36b92170">2022.lrec-1.510</url>
      <bibkey>tleubayev-etal-2022-cyrillic</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/how2sign">How2Sign</pwcdataset>
    </paper>
    <paper id="511">
      <title>ga<fixed-case>BERT</fixed-case> — an <fixed-case>I</fixed-case>rish Language Model</title>
      <author><first>James</first><last>Barry</last></author>
      <author><first>Joachim</first><last>Wagner</last></author>
      <author><first>Lauren</first><last>Cassidy</last></author>
      <author><first>Alan</first><last>Cowap</last></author>
      <author><first>Teresa</first><last>Lynn</last></author>
      <author><first>Abigail</first><last>Walsh</last></author>
      <author><first>Mícheál J.</first><last>Ó Meachair</last></author>
      <author><first>Jennifer</first><last>Foster</last></author>
      <pages>4774–4788</pages>
      <abstract>The BERT family of neural language models have become highly popular due to their ability to provide sequences of text with rich context-sensitive token encodings which are able to generalise well to many NLP tasks. We introduce gaBERT, a monolingual BERT model for the Irish language. We compare our gaBERT model to multilingual BERT and the monolingual Irish WikiBERT, and we show that gaBERT provides better representations for a downstream parsing task. We also show how different filtering criteria, vocabulary size and the choice of subword tokenisation model affect downstream performance. We compare the results of fine-tuning a gaBERT model with an mBERT model for the task of identifying verbal multiword expressions, and show that the fine-tuned gaBERT model also performs better at this task. We release gaBERT and related code to the community.</abstract>
      <url hash="be0e9935">2022.lrec-1.511</url>
      <bibkey>barry-etal-2022-gabert</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/paracrawl">ParaCrawl</pwcdataset>
    </paper>
    <paper id="512">
      <title><fixed-case>P</fixed-case>o<fixed-case>S</fixed-case> Tagging, Lemmatization and Dependency Parsing of <fixed-case>W</fixed-case>est <fixed-case>F</fixed-case>risian</title>
      <author><first>Wilbert</first><last>Heeringa</last></author>
      <author><first>Gosse</first><last>Bouma</last></author>
      <author><first>Martha</first><last>Hofman</last></author>
      <author><first>Jelle</first><last>Brouwer</last></author>
      <author><first>Eduard</first><last>Drenth</last></author>
      <author><first>Jan</first><last>Wijffels</last></author>
      <author><first>Hans</first><last>Van de Velde</last></author>
      <pages>4789–4798</pages>
      <abstract>We present a lemmatizer/PoS tagger/dependency parser for West Frisian using a corpus of 44,714 words in 3,126 sentences that were annotated according to the guidelines of Universal Dependencies version 2. PoS tags were assigned to words by using a Dutch PoS tagger that was applied to a Dutch word-by-word translation, or to sentences of a Dutch parallel text. Best results were obtained when using word-by-word translations that were created by using the previous version of the Frisian translation program Oersetter. Morphologic and syntactic annotations were generated on the basis of a Dutch word-by-word translation as well. The performance of the lemmatizer/tagger/annotator when it was trained using default parameters was compared to the performance that was obtained when using the parameter values that were used for training the LassySmall UD 2.5 corpus. We study the effects of different hyperparameter settings on the accuracy of the annotation pipeline. The Frisian lemmatizer/PoS tagger/dependency parser is released as a web app and as a web service.</abstract>
      <url hash="d3e04155">2022.lrec-1.512</url>
      <bibkey>heeringa-etal-2022-pos</bibkey>
    </paper>
    <paper id="513">
      <title>A Dataset of Offensive <fixed-case>G</fixed-case>erman Language Tweets Annotated for Speech Acts</title>
      <author><first>Melina</first><last>Plakidis</last></author>
      <author><first>Georg</first><last>Rehm</last></author>
      <pages>4799–4807</pages>
      <abstract>We present a dataset consisting of German offensive and non-offensive tweets, annotated for speech acts. These 600 tweets are a subset of the dataset by Struß et al. (2019) and comprises three levels of annotation, i.e., six coarse-grained speech acts, 23 fine-grained speech acts and 14 different sentence types. Furthermore, we provide an evaluation in both qualitative and quantitative terms. The dataset is made publicly available under a CC-BY-4.0 license.</abstract>
      <url hash="ffef5114">2022.lrec-1.513</url>
      <bibkey>plakidis-rehm-2022-dataset</bibkey>
    </paper>
    <paper id="514">
      <title>Tracing Syntactic Change in the Scientific Genre: Two <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependency-parsed Diachronic Corpora of Scientific <fixed-case>E</fixed-case>nglish and <fixed-case>G</fixed-case>erman</title>
      <author><first>Marie-Pauline</first><last>Krielke</last></author>
      <author><first>Luigi</first><last>Talamo</last></author>
      <author><first>Mahmoud</first><last>Fawzi</last></author>
      <author><first>Jörg</first><last>Knappen</last></author>
      <pages>4808–4816</pages>
      <abstract>We present two comparable diachronic corpora of scientific English and German from the Late Modern Period (17th c.–19th c.) annotated with Universal Dependencies. We describe several steps of data pre-processing and evaluate the resulting parsing accuracy showing how our pre-processing steps significantly improve output quality. As a sanity check for the representativity of our data, we conduct a case study comparing previously gained insights on grammatical change in the scientific genre with our data. Our results reflect the often reported trend of English scientific discourse towards heavy noun phrases and a simplification of the sentence structure (Halliday, 1988; Halliday and Martin, 1993; Biber and Gray, 2011; Biber and Gray, 2016). We also show that this trend applies to German scientific discourse as well. The presented corpora are valuable resources suitable for the contrastive analysis of syntactic diachronic change in the scientific genre between 1650 and 1900. The presented pre-processing procedures and their evaluations are applicable to other languages and can be useful for a variety of Natural Language Processing tasks such as syntactic parsing.</abstract>
      <url hash="e71bce1a">2022.lrec-1.514</url>
      <bibkey>krielke-etal-2022-tracing</bibkey>
    </paper>
    <paper id="515">
      <title>The Tembusu Treebank: An <fixed-case>E</fixed-case>nglish Learner Treebank</title>
      <author><first>Luís</first><last>Morgado da Costa</last></author>
      <author><first>Francis</first><last>Bond</last></author>
      <author><first>Roger V. P.</first><last>Winder</last></author>
      <pages>4817–4826</pages>
      <abstract>This paper reports on the creation and development of the Tembusu Learner Treebank — an open treebank created from the NTU Corpus of Learner English, unique for incorporating mal-rules in the annotation of ungrammatical sentences. It describes the motivation and development of the treebank, as well as its exploitation to build a new parse-ranking model for the English Resource Grammar, designed to help improve the parse selection of ungrammatical sentences and diagnose these sentences through mal-rules. The corpus contains 25,000 sentences, of which 4,900 are treebanked. The paper concludes with an evaluation experiment that shows the usefulness of this new treebank in the tasks of grammatical error detection and diagnosis.</abstract>
      <url hash="93f40758">2022.lrec-1.515</url>
      <bibkey>morgado-da-costa-etal-2022-tembusu</bibkey>
      <pwccode url="https://github.com/lmorgadodacosta/the-tembusu-treebank" additional="false">lmorgadodacosta/the-tembusu-treebank</pwccode>
    </paper>
    <paper id="516">
      <title>The <fixed-case>N</fixed-case>orwegian Dialect Corpus Treebank</title>
      <author><first>Andre</first><last>Kåsen</last></author>
      <author><first>Kristin</first><last>Hagen</last></author>
      <author><first>Anders</first><last>Nøklestad</last></author>
      <author><first>Joel</first><last>Priestly</last></author>
      <author><first>Per Erik</first><last>Solberg</last></author>
      <author><first>Dag Trygve Truslew</first><last>Haug</last></author>
      <pages>4827–4832</pages>
      <abstract>This paper presents the NDC Treebank of spoken Norwegian dialects in the Bokmål variety of Norwegian. It consists of dialect recordings made between 2006 and 2012 which have been digitised, segmented, transcribed and subsequently annotated with morphological and syntactic analysis. The nature of the spoken data gives rise to various challenges both in segmentation and annotation. We follow earlier efforts for Norwegian, in particular the LIA Treebank of spoken dialects transcribed in the Nynorsk variety of Norwegian, in the annotation principles to ensure interusability of the resources. We have developed a spoken language parser on the basis of the annotated material and report on its accuracy both on a test set across the dialects and by holding out single dialects.</abstract>
      <url hash="2bee2b05">2022.lrec-1.516</url>
      <bibkey>kasen-etal-2022-norwegian</bibkey>
    </paper>
    <paper id="517">
      <title><fixed-case>RRG</fixed-case>parbank: A Parallel Role and Reference Grammar Treebank</title>
      <author><first>Tatiana</first><last>Bladier</last></author>
      <author><first>Kilian</first><last>Evang</last></author>
      <author><first>Valeria</first><last>Generalova</last></author>
      <author><first>Zahra</first><last>Ghane</last></author>
      <author><first>Laura</first><last>Kallmeyer</last></author>
      <author><first>Robin</first><last>Möllemann</last></author>
      <author><first>Natalia</first><last>Moors</last></author>
      <author><first>Rainer</first><last>Osswald</last></author>
      <author><first>Simon</first><last>Petitjean</last></author>
      <pages>4833–4841</pages>
      <abstract>This paper describes the first release of RRGparbank, a multilingual parallel treebank for Role and Reference Grammar (RRG) containing annotations of George Orwell’s novel 1984 and its translations. The release comprises the entire novel for English and a constructionally diverse and highly parallel sample (“seed”) for German, French and Russian. The paper gives an overview of annotation decisions that have been taken and describes the adopted treebanking methodology. Finally, as a possible application, a multilingual parser is trained on the treebank data. RRGparbank is one of the first resources to apply RRG to large amounts of real-world data. Furthermore, it enables comparative and typological corpus studies in RRG. And, finally, it creates new possibilities of data-driven NLP applications based on RRG.</abstract>
      <url hash="8c3be0c8">2022.lrec-1.517</url>
      <bibkey>bladier-etal-2022-rrgparbank</bibkey>
      <pwccode url="https://github.com/taniabladier/transformer-based-twg-parsing" additional="false">taniabladier/transformer-based-twg-parsing</pwccode>
    </paper>
    <paper id="518">
      <title>Unifying Morphology Resources with <fixed-case>O</fixed-case>nto<fixed-case>L</fixed-case>ex-Morph. A Case Study in <fixed-case>G</fixed-case>erman</title>
      <author><first>Christian</first><last>Chiarcos</last></author>
      <author><first>Christian</first><last>Fäth</last></author>
      <author><first>Maxim</first><last>Ionov</last></author>
      <pages>4842–4850</pages>
      <abstract>The OntoLex vocabulary has become a widely used community standard for machine-readable lexical resources on the web. The primary motivation to use OntoLex in favor of tool- or application-specific formalisms is to facilitate interoperability and information integration across different resources. One of its extension that is currently being developed is a module for representing morphology, OntoLex-Morph. In this paper, we show how OntoLex-Morph can be used for the encoding and integration of different types of morphological resources on a unified basis. With German as the example, we demonstrate it for (a) a full-form dictionary with inflection information (Unimorph), (b) a dictionary of base forms and their derivations (UDer), (c) a dictionary of compounds (from GermaNet), and (d) lexicon and inflection rules of a finite-state parser/generator (SMOR/Morphisto). These data are converted to OntoLex-Morph, their linguistic information is consolidated and corresponding lexical entries are linked with each other.</abstract>
      <url hash="9d0da5b3">2022.lrec-1.518</url>
      <bibkey>chiarcos-etal-2022-unifying</bibkey>
    </paper>
    <paper id="519">
      <title>Building Dataset for Grounding of Formulae — Annotating Coreference Relations Among Math Identifiers</title>
      <author><first>Takuto</first><last>Asakura</last></author>
      <author><first>Yusuke</first><last>Miyao</last></author>
      <author><first>Akiko</first><last>Aizawa</last></author>
      <pages>4851–4858</pages>
      <abstract>Grounding the meaning of each symbol in math formulae is important for automated understanding of scientific documents. Generally speaking, the meanings of math symbols are not necessarily constant, and the same symbol is used in multiple meanings. Therefore, coreference relations between symbols need to be identified for grounding, and the task has aspects of both description alignment and coreference analysis. In this study, we annotated 15 papers selected from arXiv.org with the grounding information. In total, 12,352 occurrences of math identifiers in these papers were annotated, and all coreference relations between them were made explicit in each paper. The constructed dataset shows that regardless of the ambiguity of symbols in math formulae, coreference relations can be labeled with a high inter-annotator agreement. The constructed dataset enables us to achieve automation of formula grounding, and in turn, make deeper use of the knowledge in scientific documents using techniques such as math information extraction. The built grounding dataset is available at https://sigmathling.kwarc.info/resources/grounding- dataset/.</abstract>
      <url hash="64b4ca4a">2022.lrec-1.519</url>
      <bibkey>asakura-etal-2022-building</bibkey>
      <pwccode url="https://github.com/wtsnjp/MioGatto" additional="false">wtsnjp/MioGatto</pwccode>
    </paper>
    <paper id="520">
      <title><fixed-case>C</fixed-case>oref<fixed-case>UD</fixed-case> 1.0: Coreference Meets <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies</title>
      <author><first>Anna</first><last>Nedoluzhko</last></author>
      <author><first>Michal</first><last>Novák</last></author>
      <author><first>Martin</first><last>Popel</last></author>
      <author><first>Zdeněk</first><last>Žabokrtský</last></author>
      <author><first>Amir</first><last>Zeldes</last></author>
      <author><first>Daniel</first><last>Zeman</last></author>
      <pages>4859–4872</pages>
      <abstract>Recent advances in standardization for annotated language resources have led to successful large scale efforts, such as the Universal Dependencies (UD) project for multilingual syntactically annotated data. By comparison, the important task of coreference resolution, which clusters multiple mentions of entities in a text, has yet to be standardized in terms of data formats or annotation guidelines. In this paper we present CorefUD, a multilingual collection of corpora and a standardized format for coreference resolution, compatible with morphosyntactic annotations in the UD framework and including facilities for related tasks such as named entity recognition, which forms a first step in the direction of convergence for coreference resolution across languages.</abstract>
      <url hash="20c31b8c">2022.lrec-1.520</url>
      <bibkey>nedoluzhko-etal-2022-corefud</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/parcorfull">ParCorFull</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="521">
      <title>The Universal Anaphora Scorer</title>
      <author><first>Juntao</first><last>Yu</last></author>
      <author><first>Sopan</first><last>Khosla</last></author>
      <author><first>Nafise Sadat</first><last>Moosavi</last></author>
      <author><first>Silviu</first><last>Paun</last></author>
      <author><first>Sameer</first><last>Pradhan</last></author>
      <author><first>Massimo</first><last>Poesio</last></author>
      <pages>4873–4883</pages>
      <abstract>The aim of the Universal Anaphora initiative is to push forward the state of the art in anaphora and anaphora resolution by expanding the aspects of anaphoric interpretation which are or can be reliably annotated in anaphoric corpora, producing unified standards to annotate and encode these annotations, deliver datasets encoded according to these standards, and developing methods for evaluating models carrying out this type of interpretation. Such expansion of the scope of anaphora resolution requires a comparable expansion of the scope of the scorers used to evaluate this work. In this paper, we introduce an extended version of the Reference Coreference Scorer (Pradhan et al., 2014) that can be used to evaluate the extended range of anaphoric interpretation included in the current Universal Anaphora proposal. The UA scorer supports the evaluation of identity anaphora resolution and of bridging reference resolution, for which scorers already existed but not integrated in a single package. It also supports the evaluation of split antecedent anaphora and discourse deixis, for which no tools existed. The proposed approach to the evaluation of split antecedent anaphora is entirely novel; the proposed approach to the evaluation of discourse deixis leverages the encoding of discourse deixis proposed in Universal Anaphora to enable the use for discourse deixis of the same metrics already used for identity anaphora. The scorer was tested in the recent CODI-CRAC 2021 Shared Task on Anaphora Resolution in Dialogues.</abstract>
      <url hash="3c72f0f5">2022.lrec-1.521</url>
      <bibkey>yu-etal-2022-universal</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2012-1">CoNLL-2012</pwcdataset>
    </paper>
    <paper id="522">
      <title>Towards Evaluation of Cross-document Coreference Resolution Models Using Datasets with Diverse Annotation Schemes</title>
      <author><first>Anastasia</first><last>Zhukova</last></author>
      <author><first>Felix</first><last>Hamborg</last></author>
      <author><first>Bela</first><last>Gipp</last></author>
      <pages>4884–4893</pages>
      <abstract>Established cross-document coreference resolution (CDCR) datasets contain event-centric coreference chains of events and entities with identity relations. These datasets establish strict definitions of the coreference relations across related tests but typically ignore anaphora with more vague context-dependent loose coreference relations. In this paper, we qualitatively and quantitatively compare the annotation schemes of ECB+, a CDCR dataset with identity coreference relations, and NewsWCL50, a CDCR dataset with a mix of loose context-dependent and strict coreference relations. We propose a phrasing diversity metric (PD) that encounters for the diversity of full phrases unlike the previously proposed metrics and allows to evaluate lexical diversity of the CDCR datasets in a higher precision. The analysis shows that coreference chains of NewsWCL50 are more lexically diverse than those of ECB+ but annotating of NewsWCL50 leads to the lower inter-coder reliability. We discuss the different tasks that both CDCR datasets create for the CDCR models, i.e., lexical disambiguation and lexical diversity. Finally, to ensure generalizability of the CDCR models, we propose a direction for CDCR evaluation that combines CDCR datasets with multiple annotation schemes that focus of various properties of the coreference chains.</abstract>
      <url hash="d3bac379">2022.lrec-1.522</url>
      <bibkey>zhukova-etal-2022-towards</bibkey>
      <pwccode url="https://github.com/anastasia-zhukova/diverse_cdcr_datasets" additional="false">anastasia-zhukova/diverse_cdcr_datasets</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ecb">ECB+</pwcdataset>
    </paper>
    <paper id="523">
      <title>Explainable Tsetlin Machine Framework for Fake News Detection with Credibility Score Assessment</title>
      <author><first>Bimal</first><last>Bhattarai</last></author>
      <author><first>Ole-Christoffer</first><last>Granmo</last></author>
      <author><first>Lei</first><last>Jiao</last></author>
      <pages>4894–4903</pages>
      <abstract>The proliferation of fake news, i.e., news intentionally spread for misinformation, poses a threat to individuals and society. Despite various fact-checking websites such as PolitiFact, robust detection techniques are required to deal with the increase in fake news. Several deep learning models show promising results for fake news classification, however, their black-box nature makes it difficult to explain their classification decisions and quality-assure the models. We here address this problem by proposing a novel interpretable fake news detection framework based on the recently introduced Tsetlin Machine (TM). In brief, we utilize the conjunctive clauses of the TM to capture lexical and semantic properties of both true and fake news text. Further, we use clause ensembles to calculate the credibility of fake news. For evaluation, we conduct experiments on two publicly available datasets, PolitiFact and GossipCop, and demonstrate that the TM framework significantly outperforms previously published baselines by at least 5% in terms of accuracy, with the added benefit of an interpretable logic-based representation. In addition, our approach provides a higher F1-score than BERT and XLNet, however, we obtain slightly lower accuracy. We finally present a case study on our model’s explainability, demonstrating how it decomposes into meaningful words and their negations.</abstract>
      <url hash="40e6b243">2022.lrec-1.523</url>
      <bibkey>bhattarai-etal-2022-explainable</bibkey>
      <pwccode url="" additional="true"/>
    </paper>
    <paper id="524">
      <title>Enhancing Deep Learning with Embedded Features for <fixed-case>A</fixed-case>rabic Named Entity Recognition</title>
      <author><first>Ali L.</first><last>Hatab</last></author>
      <author><first>Caroline</first><last>Sabty</last></author>
      <author><first>Slim</first><last>Abdennadher</last></author>
      <pages>4904–4912</pages>
      <abstract>The introduction of word embedding models has remarkably changed many Natural Language Processing tasks. Word embeddings can automatically capture the semantics of words and other hidden features. Nonetheless, the Arabic language is highly complex, which results in the loss of important information. This paper uses Madamira, an external knowledge source, to generate additional word features. We evaluate the utility of adding these features to conventional word and character embeddings to perform the Named Entity Recognition (NER) task on Modern Standard Arabic (MSA). Our NER model is implemented using Bidirectional Long Short Term Memory and Conditional Random Fields (BiLSTM-CRF). We add morphological and syntactical features to different word embeddings to train the model. The added features improve the performance by different values depending on the used embedding model. The best performance is achieved by using Bert embeddings. Moreover, our best model outperforms the previous systems to the best of our knowledge.</abstract>
      <url hash="fce080c9">2022.lrec-1.524</url>
      <bibkey>hatab-etal-2022-enhancing</bibkey>
    </paper>
    <paper id="525">
      <title><fixed-case>SCAI</fixed-case>-<fixed-case>QR</fixed-case>e<fixed-case>CC</fixed-case> Shared Task on Conversational Question Answering</title>
      <author><first>Svitlana</first><last>Vakulenko</last></author>
      <author><first>Johannes</first><last>Kiesel</last></author>
      <author><first>Maik</first><last>Fröbe</last></author>
      <pages>4913–4922</pages>
      <abstract>Search-Oriented Conversational AI (SCAI) is an established venue that regularly puts a spotlight upon the recent work advancing the field of conversational search. SCAI’21 was organised as an independent online event and featured a shared task on conversational question answering, on which this paper reports. The shared task featured three subtasks that correspond to three steps in conversational question answering: question rewriting, passage retrieval, and answer generation. This report discusses each subtask, but emphasizes the answer generation subtask as it attracted the most attention from the participants and we identified evaluation of answer correctness in the conversational settings as a major challenge and acurrent research gap. Alongside the automatic evaluation, we conducted two crowdsourcing experiments to collect annotations for answer plausibility and faithfulness. As a result of this shared task, the original conversational QA dataset used for evaluation was further extended with alternative correct answers produced by the participant systems.</abstract>
      <url hash="0be4921c">2022.lrec-1.525</url>
      <bibkey>vakulenko-etal-2022-scai</bibkey>
      <pwccode url="https://github.com/scai-conf/scai-qrecc-21" additional="false">scai-conf/scai-qrecc-21</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/qrecc">QReCC</pwcdataset>
    </paper>
    <paper id="526">
      <title>Semantic Relations between Text Segments for Semantic Storytelling: Annotation Tool - Dataset - Evaluation</title>
      <author><first>Michael</first><last>Raring</last></author>
      <author><first>Malte</first><last>Ostendorff</last></author>
      <author><first>Georg</first><last>Rehm</last></author>
      <pages>4923–4932</pages>
      <abstract>Semantic Storytelling describes the goal to automatically and semi-automatically generate stories based on extracted, processed, classified and annotated information from large content resources. Essential is the automated processing of text segments extracted from different content resources by identifying the relevance of a text segment to a topic and its semantic relation to other text segments. In this paper we present an approach to create an automatic classifier for semantic relations between extracted text segments from different news articles. We devise custom annotation guidelines based on various discourse structure theories and annotate a dataset of 2,501 sentence pairs extracted from 2,638 Wikinews articles. For the annotation, we developed a dedicated annotation tool. Based on the constructed dataset, we perform initial experiments with Transformer language models that are trained for the automatic classification of semantic relations. Our results with promising high accuracy scores suggest the validity and applicability of our approach for future Semantic Storytelling solutions.</abstract>
      <url hash="4261b3a2">2022.lrec-1.526</url>
      <bibkey>raring-etal-2022-semantic</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="527">
      <title>Evaluating Pre-training Objectives for Low-Resource Translation into Morphologically Rich Languages</title>
      <author><first>Prajit</first><last>Dhar</last></author>
      <author><first>Arianna</first><last>Bisazza</last></author>
      <author><first>Gertjan</first><last>van Noord</last></author>
      <pages>4933–4943</pages>
      <abstract>The scarcity of parallel data is a major limitation for Neural Machine Translation (NMT) systems, in particular for translation into morphologically rich languages (MRLs). An important way to overcome the lack of parallel data is to leverage target monolingual data, which is typically more abundant and easier to collect. We evaluate a number of techniques to achieve this, ranging from back-translation to random token masking, on the challenging task of translating English into four typologically diverse MRLs, under low-resource settings. Additionally, we introduce Inflection Pre-Training (or PT-Inflect), a novel pre-training objective whereby the NMT system is pre-trained on the task of re-inflecting lemmatized target sentences before being trained on standard source-to-target language translation. We conduct our evaluation on four typologically diverse target MRLs, and find that PT-Inflect surpasses NMT systems trained only on parallel data. While PT-Inflect is outperformed by back-translation overall, combining the two techniques leads to gains in some of the evaluated language pairs.</abstract>
      <url hash="1a654a35">2022.lrec-1.527</url>
      <bibkey>dhar-etal-2022-evaluating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2020">WMT 2020</pwcdataset>
    </paper>
    <paper id="528">
      <title>Aligning Images and Text with Semantic Role Labels for Fine-Grained Cross-Modal Understanding</title>
      <author><first>Abhidip</first><last>Bhattacharyya</last></author>
      <author><first>Cecilia</first><last>Mauceri</last></author>
      <author><first>Martha</first><last>Palmer</last></author>
      <author><first>Christoffer</first><last>Heckman</last></author>
      <pages>4944–4954</pages>
      <abstract>As vision processing and natural language processing continue to advance, there is increasing interest in multimodal applications, such as image retrieval, caption generation, and human-robot interaction. These tasks require close alignment between the information in the images and text. In this paper, we present a new multimodal dataset that combines state of the art semantic annotation for language with the bounding boxes of corresponding images. This richer multimodal labeling supports cross-modal inference for applications in which such alignment is useful. Our semantic representations, developed in the natural language processing community, abstract away from the surface structure of the sentence, focusing on specific actions and the roles of their participants, a level that is equally relevant to images. We then utilize these representations in the form of semantic role labels in the captions and the images and demonstrate improvements in standard tasks such as image retrieval. The potential contributions of these additional labels is evaluated using a role-aware retrieval system based on graph convolutional and recurrent neural networks. The addition of semantic roles into this system provides a significant increase in capability and greater flexibility for these tasks, and could be extended to state-of-the-art techniques relying on transformers with larger amounts of annotated data.</abstract>
      <url hash="3756daf2">2022.lrec-1.528</url>
      <bibkey>bhattacharyya-etal-2022-aligning</bibkey>
    </paper>
    <paper id="529">
      <title>Rosetta-<fixed-case>LSF</fixed-case>: an Aligned Corpus of <fixed-case>F</fixed-case>rench <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage and <fixed-case>F</fixed-case>rench for Text-to-Sign Translation</title>
      <author><first>Elise</first><last>Bertin-Lemée</last></author>
      <author><first>Annelies</first><last>Braffort</last></author>
      <author><first>Camille</first><last>Challant</last></author>
      <author><first>Claire</first><last>Danet</last></author>
      <author><first>Boris</first><last>Dauriac</last></author>
      <author><first>Michael</first><last>Filhol</last></author>
      <author><first>Emmanuella</first><last>Martinod</last></author>
      <author><first>Jérémie</first><last>Segouat</last></author>
      <pages>4955–4962</pages>
      <abstract>This article presents a new French Sign Language (LSF) corpus called “Rosetta-LSF”. It was created to support future studies on the automatic translation of written French into LSF, rendered through the animation of a virtual signer. An overview of the field highlights the importance of a quality representation of LSF. In order to obtain quality animations understandable by signers, it must surpass the simple “gloss transcription” of the LSF lexical units to use in the discourse. To achieve this, we designed a corpus composed of four types of aligned data, and evaluated its usability. These are: news headlines in French, translations of these headlines into LSF in the form of videos showing animations of a virtual signer, gloss annotations of the “traditional” type—although including additional information on the context in which each gestural unit is performed as well as their potential for adaptation to another context—and AZee representations of the videos, i.e. formal expressions capturing the necessary and sufficient linguistic information. This article describes this data, exhibiting an example from the corpus. It is available online for public research.</abstract>
      <url hash="33a12bb6">2022.lrec-1.529</url>
      <bibkey>bertin-lemee-etal-2022-rosetta</bibkey>
    </paper>
    <paper id="530">
      <title><fixed-case>MLQE</fixed-case>-<fixed-case>PE</fixed-case>: A Multilingual Quality Estimation and Post-Editing Dataset</title>
      <author><first>Marina</first><last>Fomicheva</last></author>
      <author><first>Shuo</first><last>Sun</last></author>
      <author><first>Erick</first><last>Fonseca</last></author>
      <author><first>Chrysoula</first><last>Zerva</last></author>
      <author><first>Frédéric</first><last>Blain</last></author>
      <author><first>Vishrav</first><last>Chaudhary</last></author>
      <author><first>Francisco</first><last>Guzmán</last></author>
      <author><first>Nina</first><last>Lopatina</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <author><first>André F. T.</first><last>Martins</last></author>
      <pages>4963–4974</pages>
      <abstract>We present MLQE-PE, a new dataset for Machine Translation (MT) Quality Estimation (QE) and Automatic Post-Editing (APE). The dataset contains annotations for eleven language pairs, including both high- and low-resource languages. Specifically, it is annotated for translation quality with human labels for up to 10,000 translations per language pair in the following formats: sentence-level direct assessments and post-editing effort, and word-level binary good/bad labels. Apart from the quality-related scores, each source-translation sentence pair is accompanied by the corresponding post-edited sentence, as well as titles of the articles where the sentences were extracted from, and information on the neural MT models used to translate the text. We provide a thorough description of the data collection and annotation process as well as an analysis of the annotation distribution for each language pair. We also report the performance of baseline systems trained on the MLQE-PE dataset. The dataset is freely available and has already been used for several WMT shared tasks.</abstract>
      <url hash="c3959ea4">2022.lrec-1.530</url>
      <bibkey>fomicheva-etal-2022-mlqe</bibkey>
      <pwccode url="https://github.com/sheffieldnlp/mlqe-pe" additional="false">sheffieldnlp/mlqe-pe</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mlqe-pe">MLQE-PE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/flores">FLoRes</pwcdataset>
    </paper>
    <paper id="531">
      <title><fixed-case>O</fixed-case>pen<fixed-case>K</fixed-case>or<fixed-case>POS</fixed-case>: Democratizing <fixed-case>K</fixed-case>orean Tokenization with Voting-Based Open Corpus Annotation</title>
      <author><first>Sangwhan</first><last>Moon</last></author>
      <author><first>Won Ik</first><last>Cho</last></author>
      <author><first>Hye Joo</first><last>Han</last></author>
      <author><first>Naoaki</first><last>Okazaki</last></author>
      <author><first>Nam Soo</first><last>Kim</last></author>
      <pages>4975–4983</pages>
      <abstract>Korean is a language with complex morphology that uses spaces at larger-than-word boundaries, unlike other East-Asian languages. While morpheme-based text generation can provide significant semantic advantages compared to commonly used character-level approaches, Korean morphological analyzers only provide a sequence of morpheme-level tokens, losing information in the tokenization process. Two crucial issues are the loss of spacing information and subcharacter level morpheme normalization, both of which make the tokenization result challenging to reconstruct the original input string, deterring the application to generative tasks. As this problem originates from the conventional scheme used when creating a POS tagging corpus, we propose an improvement to the existing scheme, which makes it friendlier to generative tasks. On top of that, we suggest a fully-automatic annotation of a corpus by leveraging public analyzers. We vote the surface and POS from the outcome and fill the sequence with the selected morphemes, yielding tokenization with a decent quality that incorporates space information. Our scheme is verified via an evaluation done on an external corpus, and subsequently, it is adapted to Korean Wikipedia to construct an open, permissive resource. We compare morphological analyzer performance trained on our corpus with existing methods, then perform an extrinsic evaluation on a downstream task.</abstract>
      <url hash="e81cd1cb">2022.lrec-1.531</url>
      <bibkey>moon-etal-2022-openkorpos</bibkey>
    </paper>
    <paper id="532">
      <title>Enriching Grammatical Error Correction Resources for <fixed-case>M</fixed-case>odern <fixed-case>G</fixed-case>reek</title>
      <author><first>Katerina</first><last>Korre</last></author>
      <author><first>John</first><last>Pavlopoulos</last></author>
      <pages>4984–4991</pages>
      <abstract>Grammatical Error Correction (GEC), a task of Natural Language Processing (NLP), is challenging for underepresented languages. This issue is most prominent in languages other than English. This paper addresses the issue of data and system sparsity for GEC purposes in the modern Greek Language. Following the most popular current approaches in GEC, we develop and test an MT5 multilingual text-to-text transformer for Greek. To our knowledge this the first attempt to create a fully-fledged GEC model for Greek. Our evaluation shows that our system reaches up to 52.63% F0.5 score on part of the Greek Native Corpus (GNC), which is 16% below the winning system of the BEA-19 shared task on English GEC. In addition, we provide an extended version of the Greek Learner Corpus (GLC), on which our model reaches up to 22.76% F0.5. Previous versions did not include corrections with the annotations which hindered the potential development of efficient GEC systems. For that reason we provide a new set of corrections. This new dataset facilitates an exploration of the generalisation abilities and robustness of our system, given that the assessment is conducted on learner data while the training on native data.</abstract>
      <url hash="7060c32b">2022.lrec-1.532</url>
      <bibkey>korre-pavlopoulos-2022-enriching</bibkey>
    </paper>
    <paper id="533">
      <title>A <fixed-case>H</fixed-case>mong Corpus with Elaborate Expression Annotations</title>
      <author><first>David R.</first><last>Mortensen</last></author>
      <author><first>Xinyu</first><last>Zhang</last></author>
      <author><first>Chenxuan</first><last>Cui</last></author>
      <author><first>Katherine J.</first><last>Zhang</last></author>
      <pages>4992–5000</pages>
      <abstract>This paper describes the first publicly available corpus of Hmong, a minority language of China, Vietnam, Laos, Thailand, and various countries in Europe and the Americas. The corpus has been scraped from a long-running Usenet newsgroup called soc.culture.hmong and consists of approximately 12 million tokens. This corpus (called SCH) is also the first substantial corpus to be annotated for elaborate expressions, a kind of four-part coordinate construction that is common and important in the languages of mainland Southeast Asia. We show that word embeddings trained on SCH can benefit tasks in Hmong (solving analogies) and that a model trained on it can label previously unseen elaborate expressions, in context, with an F1 of 90.79 (precision: 87.36, recall: 94.52). [ISO 639-3: mww, hmj]</abstract>
      <url hash="a4c16996">2022.lrec-1.533</url>
      <bibkey>mortensen-etal-2022-hmong</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="534">
      <title><fixed-case>ELAL</fixed-case>: An Emotion Lexicon for the Analysis of <fixed-case>A</fixed-case>lsatian Theatre Plays</title>
      <author><first>Delphine</first><last>Bernhard</last></author>
      <author><first>Pablo</first><last>Ruiz Fabo</last></author>
      <pages>5001–5010</pages>
      <abstract>In this work, we present a novel and manually corrected emotion lexicon for the Alsatian dialects, including graphical variants of Alsatian lexical items. These High German dialects are spoken in the North-East of France. They are used mainly orally, and thus lack a stable and consensual spelling convention. There has nevertheless been a continuous literary production since the middle of the 17th century and, in particular, theatre plays. A large sample of Alsatian theatre plays is currently being encoded according to the Text Encoding Initiative (TEI) Guidelines. The emotion lexicon will be used to perform automatic emotion analysis in this corpus of theatre plays. We used a graph-based approach to deriving emotion scores and translations, relying only on bilingual lexicons, cognates and spelling variants. The source lexicons for emotion scores are the NRC Valence Arousal and Dominance and NRC Emotion Intensity lexicons.</abstract>
      <url hash="b798e6f0">2022.lrec-1.534</url>
      <bibkey>bernhard-ruiz-fabo-2022-elal</bibkey>
    </paper>
    <paper id="535">
      <title><fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies for Western Sierra <fixed-case>P</fixed-case>uebla <fixed-case>N</fixed-case>ahuatl</title>
      <author><first>Robert</first><last>Pugh</last></author>
      <author><first>Marivel</first><last>Huerta Mendez</last></author>
      <author><first>Mitsuya</first><last>Sasaki</last></author>
      <author><first>Francis</first><last>Tyers</last></author>
      <pages>5011–5020</pages>
      <abstract>We present a morpho-syntactically-annotated corpus of Western Sierra Puebla Nahuatl that conforms to the annotation guidelines of the Universal Dependencies project. We describe the sources of the texts that make up the corpus, the annotation process, and important annotation decisions made throughout the development of the corpus. As the first indigenous language of Mexico to be added to the Universal Dependencies project, this corpus offers a good opportunity to test and more clearly define annotation guidelines for the Meso-american linguistic area, spontaneous and elicited spoken data, and code-switching.</abstract>
      <url hash="c2cb3a13">2022.lrec-1.535</url>
      <bibkey>pugh-etal-2022-universal</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="536">
      <title>The Construction and Evaluation of the <fixed-case>LEAFTOP</fixed-case> Dataset of Automatically Extracted Nouns in 1480 Languages</title>
      <author><first>Gregory</first><last>Baker</last></author>
      <author><first>Diego</first><last>Molla</last></author>
      <pages>5021–5028</pages>
      <abstract>The LEAFTOP (language extracted automatically from thousands of passages) dataset consists of nouns that appear in multiple places in the four gospels of the New Testament. We use a naive approach — probabilistic inference — to identify likely translations in 1480 other languages. We evaluate this process and find that it provides lexiconaries with accuracy from 42% (Korafe) to 99% (Runyankole), averaging 72% correct across evaluated languages. The process translates up to 161 distinct lemmas from Koine Greek (average 159). We identify nouns which appear to be easy and hard to translate, language families where this technique works, and future possible improvements and extensions. The claims to novelty are: the use of a Koine Greek New Testament as the source language; using a fully-annotated manually-created grammatically parse of the source text; a custom scraper for texts in the target languages; a new metric for language similarity; a novel strategy for evaluation on low-resource languages.</abstract>
      <url hash="80bb0600">2022.lrec-1.536</url>
      <bibkey>baker-molla-2022-construction</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/panlex">Panlex</pwcdataset>
    </paper>
    <paper id="537">
      <title>Huqariq: A Multilingual Speech Corpus of Native Languages of <fixed-case>P</fixed-case>eru for<fixed-case>S</fixed-case>peech Recognition</title>
      <author><first>Rodolfo</first><last>Zevallos</last></author>
      <author><first>Luis</first><last>Camacho</last></author>
      <author><first>Nelsi</first><last>Melgarejo</last></author>
      <pages>5029–5034</pages>
      <abstract>The Huqariq corpus is a multilingual collection of speech from native Peruvian languages. The transcribed corpus is intended for the research and development of speech technologies to preserve endangered languages in Peru. Huqariq is primarily designed for the development of automatic speech recognition, language identification and text-to-speech tools. In order to achieve corpus collection sustainably, we employs the crowdsourcing methodology. Huqariq includes four native languages of Peru, and it is expected that by the year 2022, it can reach up to 20 native languages out of the 48 native languages in Peru. The corpus has 220 hours of transcribed audio recorded by more than 500 volunteers, making it the largest speech corpus for native languages in Peru. In order to verify the quality of the corpus, we present speech recognition experiments using 220 hours of fully transcribed audio.</abstract>
      <url hash="bab3c45e">2022.lrec-1.537</url>
      <bibkey>zevallos-etal-2022-huqariq</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/common-voice">Common Voice</pwcdataset>
    </paper>
    <paper id="538">
      <title>Writing System and Speaker Metadata for 2,800+ Language Varieties</title>
      <author><first>Daan</first><last>van Esch</last></author>
      <author><first>Tamar</first><last>Lucassen</last></author>
      <author><first>Sebastian</first><last>Ruder</last></author>
      <author><first>Isaac</first><last>Caswell</last></author>
      <author><first>Clara</first><last>Rivera</last></author>
      <pages>5035–5046</pages>
      <abstract>We describe an open-source dataset providing metadata for about 2,800 language varieties used in the world today. Specifically, the dataset provides the attested writing system(s) for each of these 2,800+ varieties, as well as an estimated speaker count for each variety. This dataset was developed through internal research and has been used for analyses around language technologies. This is the largest publicly-available, machine-readable resource with writing system and speaker information for the world’s languages. We analyze the distribution of languages and writing systems in our data and compare it to their representation in current NLP. We hope the availability of this data will catalyze research in under-represented languages.</abstract>
      <url hash="c912808b">2022.lrec-1.538</url>
      <bibkey>van-esch-etal-2022-writing</bibkey>
      <pwccode url="https://github.com/google-research/url-nlp" additional="false">google-research/url-nlp</pwccode>
    </paper>
    <paper id="539">
      <title>The <fixed-case>PALMA</fixed-case> Corpora of <fixed-case>A</fixed-case>frican Varieties of <fixed-case>P</fixed-case>ortuguese</title>
      <author><first>Tjerk</first><last>Hagemeijer</last></author>
      <author><first>Amália</first><last>Mendes</last></author>
      <author><first>Rita</first><last>Gonçalves</last></author>
      <author><first>Catarina</first><last>Cornejo</last></author>
      <author><first>Raquel</first><last>Madureira</last></author>
      <author><first>Michel</first><last>Généreux</last></author>
      <pages>5047–5053</pages>
      <abstract>We present three new corpora of urban varieties of Portuguese spoken in Angola, Mozambique, and São Tomé and Príncipe, where Portuguese is increasingly being spoken as first and second language in different multilingual settings. Given the scarcity of linguistic resources available for the African varieties of Portuguese, these corpora provide new, contemporary data for the study of each variety and for comparative research on African, Brazilian and European varieties, hereby improving our understanding of processes of language variation and change in postcolonial societies. The corpora consist of transcribed spoken data, complemented by a rich set of metadata describing the setting of the audio recordings and sociolinguistic information about the speakers. They are annotated with POS and lemma information and made available on the CQPweb platform, which allows for sophisticated data searches. The corpora are already being used for comparative research on constructions in the domain of possession and location involving the argument structure of intransitive, monotransitive and ditransitive verbs that select Goals, Locatives, and Recipients.</abstract>
      <url hash="85336192">2022.lrec-1.539</url>
      <bibkey>hagemeijer-etal-2022-palma</bibkey>
    </paper>
    <paper id="540">
      <title>A Learning-Based Dependency to Constituency Conversion Algorithm for the <fixed-case>T</fixed-case>urkish Language</title>
      <author><first>Büşra</first><last>Marşan</last></author>
      <author><first>Oğuz K.</first><last>Yıldız</last></author>
      <author><first>Aslı</first><last>Kuzgun</last></author>
      <author><first>Neslihan</first><last>Cesur</last></author>
      <author><first>Arife B.</first><last>Yenice</last></author>
      <author><first>Ezgi</first><last>Sanıyar</last></author>
      <author><first>Oğuzhan</first><last>Kuyrukçu</last></author>
      <author><first>Bilge N.</first><last>Arıcan</last></author>
      <author><first>Olcay Taner</first><last>Yıldız</last></author>
      <pages>5054–5062</pages>
      <abstract>This study aims to create the very first dependency-to-constituency conversion algorithm optimised for Turkish language. For this purpose, a state-of-the-art morphologic analyser and a feature-based machine learning model was used. In order to enhance the performance of the conversion algorithm, bootstrap aggregating meta-algorithm was integrated. While creating the conversation algorithm, typological properties of Turkish were carefully considered. A comprehensive and manually annotated UD-style dependency treebank was the input, and constituency trees were the output of the conversion algorithm. A team of linguists manually annotated a set of constituency trees. These manually annotated trees were used as the gold standard to assess the performance of the algorithm. The conversion process yielded more than 8000 constituency trees whose UD-style dependency trees are also available on GitHub. In addition to its contribution to Turkish treebank resources, this study also offers a viable and easy-to-implement conversion algorithm that can be used to generate new constituency treebanks and training data for NLP resources like constituency parsers.</abstract>
      <url hash="79e91c29">2022.lrec-1.540</url>
      <bibkey>marsan-etal-2022-learning</bibkey>
    </paper>
    <paper id="541">
      <title>Standard <fixed-case>G</fixed-case>erman Subtitling of <fixed-case>S</fixed-case>wiss <fixed-case>G</fixed-case>erman <fixed-case>TV</fixed-case> content: the <fixed-case>PASSAGE</fixed-case> Project</title>
      <author><first>Jonathan David</first><last>Mutal</last></author>
      <author><first>Pierrette</first><last>Bouillon</last></author>
      <author><first>Johanna</first><last>Gerlach</last></author>
      <author><first>Veronika</first><last>Haberkorn</last></author>
      <pages>5063–5070</pages>
      <abstract>In Switzerland, two thirds of the population speak Swiss German, a primarily spoken language with no standardised written form. It is widely used on Swiss TV, for example in news reports, interviews or talk shows, and subtitles are required for people who cannot understand this spoken language. This paper focuses on the task of automatic Standard German subtitling of spoken Swiss German, and more specifically on the translation of a normalised Swiss German speech recognition result into Standard German suitable for subtitles. Our contribution consists of a comparison of different statistical and deep learning MT systems for this task and an aligned corpus of normalised Swiss German and Standard German subtitles. Results of two evaluations, automatic and human, show that the systems succeed in improving the content, but are currently not capable of producing entirely correct Standard German.</abstract>
      <url hash="a8c4cbb1">2022.lrec-1.541</url>
      <bibkey>mutal-etal-2022-standard</bibkey>
    </paper>
    <paper id="542">
      <title>A Survey of Multilingual Models for Automatic Speech Recognition</title>
      <author><first>Hemant</first><last>Yadav</last></author>
      <author><first>Sunayana</first><last>Sitaram</last></author>
      <pages>5071–5079</pages>
      <abstract>Although Automatic Speech Recognition (ASR) systems have achieved human-like performance for a few languages, the majority of the world’s languages do not have usable systems due to the lack of large speech datasets to train these models. Cross-lingual transfer is an attractive solution to this problem, because low-resource languages can potentially benefit from higher-resource languages either through transfer learning, or being jointly trained in the same multilingual model. The problem of cross-lingual transfer has been well studied in ASR, however, recent advances in Self Supervised Learning are opening up avenues for unlabeled speech data to be used in multilingual ASR models, which can pave the way for improved performance on low-resource languages. In this paper, we survey the state of the art in multilingual ASR models that are built with cross-lingual transfer in mind. We present best practices for building multilingual models from research across diverse languages and techniques, discuss open questions and provide recommendations for future work.</abstract>
      <url hash="40e32bf9">2022.lrec-1.542</url>
      <bibkey>yadav-sitaram-2022-survey</bibkey>
    </paper>
    <paper id="543">
      <title><fixed-case>L</fixed-case>uxem<fixed-case>BERT</fixed-case>: Simple and Practical Data Augmentation in Language Model Pre-Training for <fixed-case>L</fixed-case>uxembourgish</title>
      <author><first>Cedric</first><last>Lothritz</last></author>
      <author><first>Bertrand</first><last>Lebichot</last></author>
      <author><first>Kevin</first><last>Allix</last></author>
      <author><first>Lisa</first><last>Veiber</last></author>
      <author><first>Tegawende</first><last>Bissyande</last></author>
      <author><first>Jacques</first><last>Klein</last></author>
      <author><first>Andrey</first><last>Boytsov</last></author>
      <author><first>Clément</first><last>Lefebvre</last></author>
      <author><first>Anne</first><last>Goujon</last></author>
      <pages>5080–5089</pages>
      <abstract>Pre-trained Language Models such as BERT have become ubiquitous in NLP where they have achieved state-of-the-art performance in most NLP tasks. While these models are readily available for English and other widely spoken languages, they remain scarce for low-resource languages such as Luxembourgish. In this paper, we present LuxemBERT, a BERT model for the Luxembourgish language that we create using the following approach: we augment the pre-training dataset by considering text data from a closely related language that we partially translate using a simple and straightforward method. We are then able to produce the LuxemBERT model, which we show to be effective for various NLP tasks: it outperforms a simple baseline built with the available Luxembourgish text data as well the multilingual mBERT model, which is currently the only option for transformer-based language models in Luxembourgish. Furthermore, we present datasets for various downstream NLP tasks that we created for this study and will make available to researchers on request.</abstract>
      <url hash="df2362ea">2022.lrec-1.543</url>
      <bibkey>lothritz-etal-2022-luxembert</bibkey>
    </paper>
    <paper id="544">
      <title><fixed-case>P</fixed-case>er<fixed-case>P</fixed-case>a<fixed-case>D</fixed-case>a: A <fixed-case>P</fixed-case>ersian Paraphrase Dataset based on Implicit Crowdsourcing Data Collection</title>
      <author><first>Salar</first><last>Mohtaj</last></author>
      <author><first>Fatemeh</first><last>Tavakkoli</last></author>
      <author><first>Habibollah</first><last>Asghari</last></author>
      <pages>5090–5096</pages>
      <abstract>In this paper we introduce PerPaDa, a Persian paraphrase dataset that is collected from users’ input in a plagiarism detection system. As an implicit crowdsourcing experience, we have gathered a large collection of original and paraphrased sentences from Hamtajoo; a Persian plagiarism detection system, in which users try to conceal cases of text re-use in their documents by paraphrasing and re-submitting manuscripts for analysis. The compiled dataset contains 2446 instances of paraphrasing. In order to improve the overall quality of the collected data, some heuristics have been used to exclude sentences that don’t meet the proposed criteria. The introduced corpus is much larger than the available datasets for the task of paraphrase identification in Persian. Moreover, there is less bias in the data compared to the similar datasets, since the users did not try some fixed predefined rules in order to generate similar texts to their original inputs.</abstract>
      <url hash="da9637aa">2022.lrec-1.544</url>
      <bibkey>mohtaj-etal-2022-perpada</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/perpada">PerPaDa</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/rumedbench">RuMedBench</pwcdataset>
    </paper>
    <paper id="545">
      <title>Introducing the <fixed-case>W</fixed-case>elsh Text Summarisation Dataset and Baseline Systems</title>
      <author><first>Ignatius</first><last>Ezeani</last></author>
      <author><first>Mahmoud</first><last>El-Haj</last></author>
      <author><first>Jonathan</first><last>Morris</last></author>
      <author><first>Dawn</first><last>Knight</last></author>
      <pages>5097–5106</pages>
      <abstract>Welsh is an official language in Wales and is spoken by an estimated 884,300 people (29.2% of the population of Wales). Despite this status and estimated increase in speaker numbers since the last (2011) census, Welsh remains a minority language undergoing revitalisation and promotion by Welsh Government and relevant stakeholders. As part of the effort to increase the availability of Welsh digital technology, this paper introduces the first Welsh summarisation dataset, which we provide freely for research purposes to help advance the work on Welsh summarisation. The dataset was created by Welsh speakers through manually summarising Welsh Wikipedia articles. In addition, the paper discusses the implementation and evaluation of different summarisation systems for Welsh. The summarisation systems and results will serve as benchmarks for the development of summarisers in other minority language contexts.</abstract>
      <url hash="522f3e33">2022.lrec-1.545</url>
      <bibkey>ezeani-etal-2022-introducing</bibkey>
      <pwccode url="https://github.com/ucrel/welsh-summarization-dataset" additional="false">ucrel/welsh-summarization-dataset</pwccode>
    </paper>
    <paper id="546">
      <title>A Systematic Approach to Derive a Refined Speech Corpus for <fixed-case>S</fixed-case>inhala</title>
      <author><first>Disura</first><last>Warusawithana</last></author>
      <author><first>Nilmani</first><last>Kulaweera</last></author>
      <author><first>Lakshan</first><last>Weerasinghe</last></author>
      <author><first>Buddhika</first><last>Karunarathne</last></author>
      <pages>5107–5113</pages>
      <abstract>Speech Recognition is an active research area where advances of technology have continuously driven the development of research work. However, due to the lack of adequate resources, certain languages such as Sinhala, are left to underutilize the technology. With techniques such as crowdsourcing and web scraping, several Sinhala corpora have been created and made publicly available. Despite them being large and generic, the correctness and consistency in their text data remain questionable, especially due to the lack of uniformity in the language used in the different sources of web scraped text. Addressing that requires a thorough understanding of technical and linguistic particulars pertaining to the language, which often leaves the issue unattended. We have followed a systematic approach to derive a refined corpus using a publicly available corpus for Sinhala speech recognition. In particular, we standardized the transcriptions of the corpus by removing noise in the text. Further, we applied corrections based on Sinhala linguistics. A comparative experiment shows a promising effect of the linguistic corrections by having a relative reduction of the Word-Error-Rate by 15.9%.</abstract>
      <url hash="c4cffc03">2022.lrec-1.546</url>
      <bibkey>warusawithana-etal-2022-systematic</bibkey>
    </paper>
    <paper id="547">
      <title><fixed-case>I</fixed-case>gbo<fixed-case>BERT</fixed-case> Models: Building and Training Transformer Models for the <fixed-case>I</fixed-case>gbo Language</title>
      <author><first>Chiamaka</first><last>Chukwuneke</last></author>
      <author><first>Ignatius</first><last>Ezeani</last></author>
      <author><first>Paul</first><last>Rayson</last></author>
      <author><first>Mahmoud</first><last>El-Haj</last></author>
      <pages>5114–5122</pages>
      <abstract>This work presents a standard Igbo named entity recognition (IgboNER) dataset as well as the results from training and fine-tuning state-of-the-art transformer IgboNER models. We discuss the process of our dataset creation - data collection and annotation and quality checking. We also present experimental processes involved in building an IgboBERT language model from scratch as well as fine-tuning it along with other non-Igbo pre-trained models for the downstream IgboNER task. Our results show that, although the IgboNER task benefited hugely from fine-tuning large transformer model, fine-tuning a transformer model built from scratch with comparatively little Igbo text data seems to yield quite decent results for the IgboNER task. This work will contribute immensely to IgboNLP in particular as well as the wider African and low-resource NLP efforts Keywords: Igbo, named entity recognition, BERT models, under-resourced, dataset</abstract>
      <url hash="973c31c9">2022.lrec-1.547</url>
      <bibkey>chukwuneke-etal-2022-igbobert</bibkey>
      <pwccode url="https://github.com/chiamakac/igboner-models" additional="false">chiamakac/igboner-models</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/masakhaner">MasakhaNER</pwcdataset>
    </paper>
    <paper id="548">
      <title><fixed-case>L</fixed-case>atvian National Corpora Collection – Korpuss.lv</title>
      <author><first>Baiba</first><last>Saulite</last></author>
      <author><first>Roberts</first><last>Darģis</last></author>
      <author><first>Normunds</first><last>Gruzitis</last></author>
      <author><first>Ilze</first><last>Auzina</last></author>
      <author><first>Kristīne</first><last>Levāne-Petrova</last></author>
      <author><first>Lauma</first><last>Pretkalniņa</last></author>
      <author><first>Laura</first><last>Rituma</last></author>
      <author><first>Peteris</first><last>Paikens</last></author>
      <author><first>Arturs</first><last>Znotins</last></author>
      <author><first>Laine</first><last>Strankale</last></author>
      <author><first>Kristīne</first><last>Pokratniece</last></author>
      <author><first>Ilmārs</first><last>Poikāns</last></author>
      <author><first>Guntis</first><last>Barzdins</last></author>
      <author><first>Inguna</first><last>Skadiņa</last></author>
      <author><first>Anda</first><last>Baklāne</last></author>
      <author><first>Valdis</first><last>Saulespurēns</last></author>
      <author><first>Jānis</first><last>Ziediņš</last></author>
      <pages>5123–5129</pages>
      <abstract>LNCC is a diverse collection of Latvian language corpora representing both written and spoken language and is useful for both linguistic research and language modelling. The collection is intended to cover diverse Latvian language use cases and all the important text types and genres (e.g. news, social media, blogs, books, scientific texts, debates, essays, etc.), taking into account both quality and size aspects. To reach this objective, LNCC is a continuous multi-institutional and multi-project effort, supported by the Digital Humanities and Language Technology communities in Latvia. LNCC includes a broad range of Latvian texts from the Latvian National Library, Culture Information Systems Centre, Latvian National News Agency, Latvian Parliament, Latvian web crawl, various Latvian publishers, and from the Latvian language corpora created by Institute of Mathematics and Computer Science and its partners, including spoken language corpora. All corpora of LNCC are re-annotated with a uniform morpho-syntactic annotation scheme which enables federated search and consistent linguistics analysis in all the LNCC corpora, as well as facilitates to select and mix various corpora for pre-training large Latvian language models like BERT and GPT.</abstract>
      <url hash="0265b9f7">2022.lrec-1.548</url>
      <bibkey>saulite-etal-2022-latvian</bibkey>
    </paper>
    <paper id="549">
      <title>Investigating the Relationship Between <fixed-case>R</fixed-case>omanian Financial News and Closing Prices from the Bucharest Stock Exchange</title>
      <author><first>Ioan-Bogdan</first><last>Iordache</last></author>
      <author><first>Ana Sabina</first><last>Uban</last></author>
      <author><first>Catalin</first><last>Stoean</last></author>
      <author><first>Liviu P.</first><last>Dinu</last></author>
      <pages>5130–5136</pages>
      <abstract>A new data set is gathered from a Romanian financial news website for the duration of four years. It is further refined to extract only information related to one company by selecting only paragraphs and even sentences that referred to it. The relation between the extracted sentiment scores of the texts and the stock prices from the corresponding dates is investigated using various approaches like the lexicon-based Vader tool, Financial BERT, as well as Transformer-based models. Automated translation is used, since some models could be only applied for texts in English. It is encouraging that all models, be that they are applied to Romanian or English texts, indicate a correlation between the sentiment scores and the increase or decrease of the stock closing prices.</abstract>
      <url hash="211987c5">2022.lrec-1.549</url>
      <bibkey>iordache-etal-2022-investigating</bibkey>
    </paper>
    <paper id="550">
      <title>A Free/Open-Source Morphological Analyser and Generator for Sakha</title>
      <author><first>Sardana</first><last>Ivanova</last></author>
      <author><first>Jonathan</first><last>Washington</last></author>
      <author><first>Francis</first><last>Tyers</last></author>
      <pages>5137–5142</pages>
      <abstract>We present, to our knowledge, the first ever published morphological analyser and generator for Sakha, a marginalised language of Siberia. The transducer, developed using HFST, has coverage of solidly above 90%, and high precision. In the development of the analyser, we have expanded linguistic knowledge about Sakha, and developed strategies for complex grammatical patterns. The transducer is already being used in downstream tasks, including computer assisted language learning applications for linguistic maintenance and computational linguistic shared tasks.</abstract>
      <url hash="595d8dac">2022.lrec-1.550</url>
      <bibkey>ivanova-etal-2022-free</bibkey>
    </paper>
    <paper id="551">
      <title>An Expanded Finite-State Transducer for Tsuut’ina Verbs</title>
      <author><first>Joshua</first><last>Holden</last></author>
      <author><first>Christopher</first><last>Cox</last></author>
      <author><first>Antti</first><last>Arppe</last></author>
      <pages>5143–5152</pages>
      <abstract>This paper describes the expansion of a finite state transducer (FST) for the transitive verb system of Tsuut’ina (ISO 639-3: srs), a Dene (Athabaskan) language spoken in Alberta, Canada. Dene languages have unique templatic morphology, in which lexical, inflectional and derivational tiers are interlaced. Drawing on data from close to 9,000 verbal forms, the expanded model can handle a great range of common and rare argument structure types, including ditransitive and uniquely Dene object experiencer verbs. While challenges of speed remain, this expansion shows the ability of FST modelling to handle morphology of this type, and the expnded FST shows great promise for community language applications such as a morphologically informed online dictionary and word predictor, and for further FST development.This paper describes the expansion of a finite state transducer (FST) for the transitive verb system of Tsuut’ina (ISO 639-3: srs), a Dene (Athabaskan) language spoken in Alberta, Canada. Dene languages have unique templatic morphology, in which lexical, inflectional and derivational tiers are interlaced. Drawing on data from over 12,000 verbs forms, the expanded model can handle a great range of common and rare argument structure types, including ditransitive and uniquely Dene object experiencer verbs. While challenges of speed remain, this expansion shows the ability of FST modelling to handle morphology of this type, and the expnded FST shows great promise for community language applications such as a morphologically informed online dictionary and word predictor, and for further FST development.</abstract>
      <url hash="96ec4dad">2022.lrec-1.551</url>
      <bibkey>holden-etal-2022-expanded</bibkey>
    </paper>
    <paper id="552">
      <title><fixed-case>BD</fixed-case>-<fixed-case>SHS</fixed-case>: A Benchmark Dataset for Learning to Detect Online <fixed-case>B</fixed-case>angla Hate Speech in Different Social Contexts</title>
      <author><first>Nauros</first><last>Romim</last></author>
      <author><first>Mosahed</first><last>Ahmed</last></author>
      <author><first>Md Saiful</first><last>Islam</last></author>
      <author><first>Arnab</first><last>Sen Sharma</last></author>
      <author><first>Hriteshwar</first><last>Talukder</last></author>
      <author><first>Mohammad Ruhul</first><last>Amin</last></author>
      <pages>5153–5162</pages>
      <abstract>Social media platforms and online streaming services have spawned a new breed of Hate Speech (HS). Due to the massive amount of user-generated content on these sites, modern machine learning techniques are found to be feasible and cost-effective to tackle this problem. However, linguistically diverse datasets covering different social contexts in which offensive language is typically used are required to train generalizable models. In this paper, we identify the shortcomings of existing Bangla HS datasets and introduce a large manually labeled dataset BD-SHS that includes HS in different social contexts. The labeling criteria were prepared following a hierarchical annotation process, which is the first of its kind in Bangla HS to the best of our knowledge. The dataset includes more than 50,200 offensive comments crawled from online social networking sites and is at least 60% larger than any existing Bangla HS datasets. We present the benchmark result of our dataset by training different NLP models resulting in the best one achieving an F1-score of 91.0%. In our experiments, we found that a word embedding trained exclusively using 1.47 million comments from social media and streaming sites consistently resulted in better modeling of HS detection in comparison to other pre-trained embeddings. Our dataset and all accompanying codes is publicly available at github.com/naurosromim/hate-speech-dataset-for-Bengali-social-media</abstract>
      <url hash="1c47f77e">2022.lrec-1.552</url>
      <bibkey>romim-etal-2022-bd</bibkey>
      <pwccode url="https://github.com/naurosromim/hate-speech-dataset-for-bengali-social-media" additional="false">naurosromim/hate-speech-dataset-for-bengali-social-media</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hate-speech">Hate Speech</pwcdataset>
    </paper>
    <paper id="553">
      <title>Introducing <fixed-case>R</fixed-case>ezo<fixed-case>JDM</fixed-case>16k: a <fixed-case>F</fixed-case>rench <fixed-case>K</fixed-case>nowledge<fixed-case>G</fixed-case>raph <fixed-case>D</fixed-case>ata<fixed-case>S</fixed-case>et for Link Prediction</title>
      <author><first>Mehdi</first><last>Mirzapour</last></author>
      <author><first>Waleed</first><last>Ragheb</last></author>
      <author><first>Mohammad Javad</first><last>Saeedizade</last></author>
      <author><first>Kevin</first><last>Cousot</last></author>
      <author><first>Helene</first><last>Jacquenet</last></author>
      <author><first>Lawrence</first><last>Carbon</last></author>
      <author><first>Mathieu</first><last>Lafourcade</last></author>
      <pages>5163–5169</pages>
      <abstract>Knowledge graphs applications, in industry and academia, motivate substantial research directions towards large-scale information extraction from various types of resources. Nowadays, most of the available knowledge graphs are either in English or multilingual. In this paper, we introduce RezoJDM16k, a French knowledge graph dataset based on RezoJDM. With 16k nodes, 832k triplets, and 53 relation types, RezoJDM16k can be employed in many NLP downstream tasks for the French language such as machine translation, question-answering, and recommendation systems. Moreover, we provide strong knowledge graph embedding baselines that are used in link prediction tasks for future benchmarking. Compared to the state-of-the-art English knowledge graph datasets used in link prediction, RezoJDM16k shows a similar promising predictive behavior.</abstract>
      <url hash="48ff9e27">2022.lrec-1.553</url>
      <bibkey>mirzapour-etal-2022-introducing</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fb15k">FB15k</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wn18">WN18</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wn18rr">WN18RR</pwcdataset>
    </paper>
    <paper id="554">
      <title>The Badalona Corpus - An Audio, Video and Neuro-Physiological Conversational Dataset</title>
      <author><first>Philippe</first><last>Blache</last></author>
      <author><first>Salomé</first><last>Antoine</last></author>
      <author><first>Dorina</first><last>De Jong</last></author>
      <author><first>Lena-Marie</first><last>Huttner</last></author>
      <author><first>Emilia</first><last>Kerr</last></author>
      <author><first>Thierry</first><last>Legou</last></author>
      <author><first>Eliot</first><last>Maës</last></author>
      <author><first>Clément</first><last>François</last></author>
      <pages>5170–5177</pages>
      <abstract>We present in this paper the first natural conversation corpus recorded with all modalities and neuro-physiological signals. 5 dyads (10 participants) have been recorded three times, during three sessions (30mns each) with 4 days interval. During each session, audio and video are captured as well as the neural signal (EEG with Emotiv-EPOC) and the electro-physiological one (with Empatica-E4). This resource original in several respects. Technically, it is the first one gathering all these types of data in a natural conversation situation. Moreover, the recording of the same dyads at different periods opens the door to new longitudinal investigations such as the evolution of interlocutors’ alignment during the time. The paper situates this new type of resources with in the literature, presents the experimental setup and describes different annotations enriching the corpus.</abstract>
      <url hash="cdc82082">2022.lrec-1.554</url>
      <bibkey>blache-etal-2022-badalona</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/k-emocon">K-EmoCon</pwcdataset>
    </paper>
    <paper id="555">
      <title>Reading Time and Vocabulary Rating in the <fixed-case>J</fixed-case>apanese Language: Large-Scale <fixed-case>J</fixed-case>apanese Reading Time Data Collection Using Crowdsourcing</title>
      <author><first>Masayuki</first><last>Asahara</last></author>
      <pages>5178–5187</pages>
      <abstract>This study examines how differences in human vocabulary affect reading time. Specifically, we assumed vocabulary to be the random effect of research participants when applying a generalized linear mixed model to the ratings of participants in the word familiarity survey. Thereafter, we asked the participants to take part in a self-paced reading task to collect their reading times. Through fixed effect of vocabulary when applying a generalized linear mixed model to reading time, we clarified the tendency that vocabulary differences give to reading time.</abstract>
      <url hash="fc4ac6ed">2022.lrec-1.555</url>
      <bibkey>asahara-2022-reading</bibkey>
      <pwccode url="https://github.com/masayu-a/bccwj-spr2" additional="false">masayu-a/bccwj-spr2</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-stories">Natural Stories</pwcdataset>
    </paper>
    <paper id="556">
      <title>Thematic Fit Bits: Annotation Quality and Quantity Interplay for Event Participant Representation</title>
      <author><first>Yuval</first><last>Marton</last></author>
      <author><first>Asad</first><last>Sayeed</last></author>
      <pages>5188–5197</pages>
      <abstract>Modeling thematic fit (a verb-argument compositional semantics task) currently requires a very large burden of labeled data. We take a linguistically machine-annotated large corpus and replace corpus layers with output from higher-quality, more modern taggers. We compare the old and new corpus versions’ impact on a verb-argument fit modeling task, using a high-performing neural approach. We discover that higher annotation quality dramatically reduces our data requirement while demonstrating better supervised predicate-argument classification. But in applying the model to psycholinguistic tasks outside the training objective, we see clear gains at scale, but only in one of two thematic fit estimation tasks, and no clear gains on the other. We also see that quality improves with training size, but perhaps plateauing or even declining in one task. Last, we tested the effect of role set size. All this suggests that the quality/quantity interplay is not all you need. We replicate previous studies while modifying certain role representation details and set a new state-of-the-art in event modeling, using a fraction of the data. We make the new corpus version public.</abstract>
      <url hash="7f98c0eb">2022.lrec-1.556</url>
      <bibkey>marton-sayeed-2022-thematic</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/framenet">FrameNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="557">
      <title><fixed-case>C</fixed-case>hi<fixed-case>S</fixed-case>ense-12: An <fixed-case>E</fixed-case>nglish Sense-Annotated Child-Directed Speech Corpus</title>
      <author><first>Francesco</first><last>Cabiddu</last></author>
      <author><first>Lewis</first><last>Bott</last></author>
      <author><first>Gary</first><last>Jones</last></author>
      <author><first>Chiara</first><last>Gambi</last></author>
      <pages>5198–5205</pages>
      <abstract>Language acquisition research has benefitted from the use of annotated corpora of child-directed speech to examine key questions about how children learn and process language in real-world contexts. However, a lack of sense-annotated corpora has limited investigations of child word sense disambiguation in naturalistic contexts. In this work, we sense-tagged 53 corpora of American and English speech directed to 958 target children up to 59 months of age, comprising a large-scale sample of 15,581 utterances for 12 ambiguous words. Importantly, we carefully selected target senses that we know - from previous investigations - young children understand. As such work was part of a project focused on investigating the role of verbs in child word sense disambiguation, we additionally coded for verb instances which took a target ambiguous word as verb object. We present experimental work where we leveraged our sense-tagged corpus ChiSense-12 to examine the role of verb-event structure in child word sense disambiguation, and we outline our plan to use Transformer-based computational architectures to test hypotheses on the role of different learning mechanisms underlying children word sense disambiguation performance.</abstract>
      <url hash="25e1ca35">2022.lrec-1.557</url>
      <bibkey>cabiddu-etal-2022-chisense</bibkey>
      <pwccode url="https://gitlab.com/francescocabiddu/chisense-12" additional="false">francescocabiddu/chisense-12</pwccode>
    </paper>
    <paper id="558">
      <title>Making People Laugh like a Pro: Analysing Humor Through Stand-Up Comedy</title>
      <author><first>Beatrice</first><last>Turano</last></author>
      <author><first>Carlo</first><last>Strapparava</last></author>
      <pages>5206–5211</pages>
      <abstract>The analysis of humor using computational tools has gained popularity in the past few years, and a lot of resources have been built for this purpose. However, most of these resources focus on standalone jokes or on occasional humorous sentences during presentations. In this paper I present a new dataset, SCRIPTS, built using stand-up comedy shows transcripts: the humor that this dataset collects is inserted in a larger narrative, composed of daily events made humorous by the ability of the comedian. This different perspective on the humor problem can allow us to think and study humor in a different way and possibly to open the path to new lines of research.</abstract>
      <url hash="9619d187">2022.lrec-1.558</url>
      <bibkey>turano-strapparava-2022-making</bibkey>
    </paper>
    <paper id="559">
      <title>Testing Focus and Non-at-issue Frameworks with a Question-under-Discussion-Annotated Corpus</title>
      <author><first>Christoph</first><last>Hesse</last></author>
      <author><first>Maurice</first><last>Langner</last></author>
      <author><first>Ralf</first><last>Klabunde</last></author>
      <author><first>Anton</first><last>Benz</last></author>
      <pages>5212–5219</pages>
      <abstract>We present an annotated corpus of German driving reports for the analysis of Question-under-Discussion (QUD) based information structural distinctions. Since QUDs can hardly be defined in advance for providing a corresponding tagset, several theoretical issues arise concerning the scope and quality of the corpus and the development of an appropriate annotation tool for creating the corpus. We developed the corpus for testing the adequacy of QUD-based pragmatic frameworks of information structure. First analyses of the annotated information structures show that focus-related meaning aspects are essentially confirmed, indicating a sufficent accuracy of the annotations. Assumptions on non-at-issueness expressed by non-restrictive relative clauses made in the literature seem to be too strong, given the corpus data.</abstract>
      <url hash="34b9a90a">2022.lrec-1.559</url>
      <bibkey>hesse-etal-2022-testing</bibkey>
    </paper>
    <paper id="560">
      <title>Development of a Multilingual <fixed-case>CCG</fixed-case> Treebank via <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies Conversion</title>
      <author><first>Tu-Anh</first><last>Tran</last></author>
      <author><first>Yusuke</first><last>Miyao</last></author>
      <pages>5220–5233</pages>
      <abstract>This paper introduces an algorithm to convert Universal Dependencies (UD) treebanks to Combinatory Categorial Grammar (CCG) treebanks. As CCG encodes almost all grammatical information into the lexicon, obtaining a high-quality CCG derivation from a dependency tree is a challenging task. Our algorithm relies on hand-crafted rules to assign categories to constituents, and a non-statistical parser to derive full CCG parses given the assigned categories. To evaluate our converted treebanks, we perform lexical, sentential, and syntactic rule coverage analysis, as well as CCG parsing experiments. Finally, we discuss how our method handles complex constructions, and propose possible future extensions.</abstract>
      <url hash="a7c6f3ed">2022.lrec-1.560</url>
      <bibkey>tran-miyao-2022-development</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="561">
      <title>The Automatic Extraction of Linguistic Biomarkers as a Viable Solution for the Early Diagnosis of Mental Disorders</title>
      <author><first>Gloria</first><last>Gagliardi</last></author>
      <author><first>Fabio</first><last>Tamburini</last></author>
      <pages>5234–5242</pages>
      <abstract>Digital Linguistic Biomarkers extracted from spontaneous language productions proved to be very useful for the early detection of various mental disorders. This paper presents a computational pipeline for the automatic processing of oral and written texts: the tool enables the computation of a rich set of linguistic features at the acoustic, rhythmic, lexical, and morphosyntactic levels. Several applications of the instrument - for the detection of Mild Cognitive Impairments, Anorexia Nervosa, and Developmental Language Disorders - are also briefly discussed.</abstract>
      <url hash="a558bb62">2022.lrec-1.561</url>
      <bibkey>gagliardi-tamburini-2022-automatic</bibkey>
    </paper>
    <paper id="562">
      <title><fixed-case>S</fixed-case>inglish Where Got Rules One? Constructing a Computational Grammar for <fixed-case>S</fixed-case>inglish</title>
      <author><first>Siew Yeng</first><last>Chow</last></author>
      <author><first>Francis</first><last>Bond</last></author>
      <pages>5243–5250</pages>
      <abstract>Singlish is a variety of English spoken in Singapore. In this paper, we share some of its grammar features and how they are implemented in the construction of a computational grammar of Singlish as a branch of English grammar. New rules were created and existing ones from standard English grammar of the English Resource Grammar (ERG) were changed in this branch to cater to how Singlish works. In addition, Singlish lexicon was added into the grammar together with some new lexical types. We used Head-driven Phrase Structure Grammar (HPSG) as the framework for this project of a creating a working computational grammar. As part of building the language resource, we also collected and formatted some data from the internet as part of a test suite for Singlish. Finally, the computational grammar was tested against a set of gold standard trees and compared with the standard English grammar to find out how well the grammar fares in analysing Singlish.</abstract>
      <url hash="b9fe6847">2022.lrec-1.562</url>
      <bibkey>chow-bond-2022-singlish</bibkey>
    </paper>
    <paper id="563">
      <title><fixed-case>COSMOS</fixed-case>: Experimental and Comparative Studies of Concept Representations in Schoolchildren</title>
      <author><first>Jeanne</first><last>Villaneau</last></author>
      <author><first>Farida</first><last>Said</last></author>
      <pages>5251–5260</pages>
      <abstract>COSMOS is a multidisciplinary research project investigating schoolchildren’s beliefs and representations of specific concepts under control variables (age, gender, language spoken at home). Seven concepts are studied: <i>friend, father, mother, villain, work, television</i> and <i>dog</i>. We first present the protocol used and the data collected from a survey of 184 children in two age groups (6-7 and 9-11 years) in four schools in Brittany (France). A word-level lexical study shows that children’s linguistic proficiency and lexical diversity increase with age, and we observe an interaction effect between gender and age on lexical diversity as measured with MLR (Measure of Lexical Richness). In contrast, none of the control variables affects lexical density. We also present the lemmas that schoolchildren most often associate with each concept. Generalized linear mixed-effects models reveal significant effects of age, gender, and home language on some concept-lemma associations and specific interactions between age and gender. Most of the identified effects are documented in the child development literature. To better understand the process of semantic construction in children, additional lexical analyses at the n-gram, chunk, and clause levels would be helpful. We briefly present ongoing and planned work in this direction. The COSMOS data will soon be made freely available to the scientific community.</abstract>
      <url hash="f9268d71">2022.lrec-1.563</url>
      <bibkey>villaneau-said-2022-cosmos</bibkey>
    </paper>
    <paper id="564">
      <title>Features of Perceived Metaphoricity on the Discourse Level: Abstractness and Emotionality</title>
      <author><first>Prisca</first><last>Piccirilli</last></author>
      <author><first>Sabine</first><last>Schulte im Walde</last></author>
      <pages>5261–5273</pages>
      <abstract>Research on metaphorical language has shown ties between abstractness and emotionality with regard to metaphoricity; prior work is however limited to the word and sentence levels, and up to date there is no empirical study establishing the extent to which this is also true on the discourse level. This paper explores which textual and perceptual features human annotators perceive as important for the metaphoricity of discourses and expressions, and addresses two research questions more specifically. First, is a metaphorically-perceived discourse more abstract and more emotional in comparison to a literally- perceived discourse? Second, is a metaphorical expression preceded by a more metaphorical/abstract/emotional context than a synonymous literal alternative? We used a dataset of 1,000 corpus-extracted discourses for which crowdsourced annotators (1) provided judgements on whether they perceived the discourses as more metaphorical or more literal, and (2) systematically listed lexical terms which triggered their decisions in (1). Our results indicate that metaphorical discourses are more emotional and to a certain extent more abstract than literal discourses. However, neither the metaphoricity nor the abstractness and emotionality of the preceding discourse seem to play a role in triggering the choice between synonymous metaphorical vs. literal expressions. Our dataset is available at https://www.ims.uni-stuttgart.de/data/discourse-met-lit.</abstract>
      <url hash="6da1c3ad">2022.lrec-1.564</url>
      <bibkey>piccirilli-schulte-im-walde-2022-features</bibkey>
    </paper>
    <paper id="565">
      <title>Hollywood Identity Bias Dataset: A Context Oriented Bias Analysis of Movie Dialogues</title>
      <author><first>Sandhya</first><last>Singh</last></author>
      <author><first>Prapti</first><last>Roy</last></author>
      <author><first>Nihar</first><last>Sahoo</last></author>
      <author><first>Niteesh</first><last>Mallela</last></author>
      <author><first>Himanshu</first><last>Gupta</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <author><first>Milind</first><last>Savagaonkar</last></author>
      <author><first>Nidhi</first><last>Sultan</last></author>
      <author><first>Roshni</first><last>Ramnani</last></author>
      <author><first>Anutosh</first><last>Maitra</last></author>
      <author><first>Shubhashis</first><last>Sengupta</last></author>
      <pages>5274–5285</pages>
      <abstract>Movies reflect society and also hold power to transform opinions. Social biases and stereotypes present in movies can cause extensive damage due to their reach. These biases are not always found to be the need of storyline but can creep in as the author’s bias. Movie production houses would prefer to ascertain that the bias present in a script is the story’s demand. Today, when deep learning models can give human-level accuracy in multiple tasks, having an AI solution to identify the biases present in the script at the writing stage can help them avoid the inconvenience of stalled release, lawsuits, etc. Since AI solutions are data intensive and there exists no domain specific data to address the problem of biases in scripts, we introduce a new dataset of movie scripts that are annotated for identity bias. The dataset contains dialogue turns annotated for (i) bias labels for seven categories, viz., gender, race/ethnicity, religion, age, occupation, LGBTQ, and other, which contains biases like body shaming, personality bias, etc. (ii) labels for sensitivity, stereotype, sentiment, emotion, emotion intensity, (iii) all labels annotated with context awareness, (iv) target groups and reason for bias labels and (v) expert-driven group-validation process for high quality annotations. We also report various baseline performances for bias identification and category detection on our dataset.</abstract>
      <url hash="2576fee3">2022.lrec-1.565</url>
      <bibkey>singh-etal-2022-hollywood</bibkey>
    </paper>
    <paper id="566">
      <title><fixed-case>V</fixed-case>ox<fixed-case>C</fixed-case>ommunis: A Corpus for Cross-linguistic Phonetic Analysis</title>
      <author><first>Emily</first><last>Ahn</last></author>
      <author><first>Eleanor</first><last>Chodroff</last></author>
      <pages>5286–5294</pages>
      <abstract>Cross-linguistic phonetic analysis has long been limited by data scarcity and insufficient computational resources. In the past few years, the availability of large-scale cross-linguistic spoken corpora has increased dramatically, but the data still require considerable computational power and processing for downstream phonetic analysis. To facilitate large-scale cross-linguistic phonetic research in the field, we release the VoxCommunis Corpus, which contains acoustic models, pronunciation lexicons, and word- and phone-level alignments, derived from the publicly available Mozilla Common Voice Corpus. The current release includes data from 36 languages. The corpus also contains acoustic-phonetic measurements, which currently consist of formant frequencies (F1–F4) from all vowel quartiles. Major advantages of this corpus for phonetic analysis include the number of available languages, the large amount of speech per language, as well as the fact that most language datasets have dozens to hundreds of contributing speakers. We demonstrate the utility of this corpus for downstream phonetic research in a descriptive analysis of language-specific vowel systems, as well as an analysis of “uniformity” in vowel realization across languages. The VoxCommunis Corpus is free to download and use under a CC0 license.</abstract>
      <url hash="e83ea3a1">2022.lrec-1.566</url>
      <bibkey>ahn-chodroff-2022-voxcommunis</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/common-voice">Common Voice</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multilingual-librispeech">Multilingual LibriSpeech</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/voxclamantis">VoxClamantis</pwcdataset>
    </paper>
    <paper id="567">
      <title>Tracking Textual Similarities in Neo-<fixed-case>L</fixed-case>atin Drama Networks</title>
      <author><first>Andrea</first><last>Peverelli</last></author>
      <author><first>Marieke</first><last>van Erp</last></author>
      <author><first>Jan</first><last>Bloemendal</last></author>
      <pages>5295–5303</pages>
      <abstract>This paper describes the first experiments towards tracking the complex and international network of text reuse within the Early Modern (XV-XVII centuries) community of Neo-Latin humanists. Our research, conducted within the framework of the TransLatin project, aims at gaining more evidence on the topic of textual similarities and semi-conscious reuse of literary models. It consists of two experiments conveyed through two main research fields (Information Retrieval and Stylometry), as a means to a better understanding of the complex and subtle literary mechanisms underlying the drama production of Modern Age authors and their transnational network of relations. The experiments led to the construction of networks of works and authors that fashion different patterns of similarity and models of evolution and interaction between texts.</abstract>
      <url hash="f372dcc0">2022.lrec-1.567</url>
      <bibkey>peverelli-etal-2022-tracking</bibkey>
    </paper>
    <paper id="568">
      <title>Named Entity Recognition in <fixed-case>E</fixed-case>stonian 19th Century Parish Court Records</title>
      <author><first>Siim</first><last>Orasmaa</last></author>
      <author><first>Kadri</first><last>Muischnek</last></author>
      <author><first>Kristjan</first><last>Poska</last></author>
      <author><first>Anna</first><last>Edela</last></author>
      <pages>5304–5313</pages>
      <abstract>This paper presents a new historical language resource, a corpus of Estonian Parish Court records from the years 1821-1920, annotated for named entities (NE), and reports on named entity recognition (NER) experiments using this corpus. The hand-written records have been transcribed manually via a crowdsourcing project, so the transcripts are of high quality, but the variation of language and spelling is high in these documents due to dialectal variation and the fact that there was a considerable change in Estonian spelling conventions during the time of their writing. The typology of NEs for manual annotation includes 7 categories, but the inter-annotator agreement is as good as 95.0 (mean F1-score). We experimented with fine-tuning BERT-like transfer learning approaches for NER, and found modern Estonian BERT models highly applicable, despite the difficulty of the historical material. Our best model, finetuned Est-RoBERTa, achieved microaverage F1 score of 93.6, which is comparable to state-of-the-art NER performance on the contemporary Estonian.</abstract>
      <url hash="cd5ea47e">2022.lrec-1.568</url>
      <bibkey>orasmaa-etal-2022-named</bibkey>
      <pwccode url="https://github.com/soras/vk_ner_lrec_2022" additional="false">soras/vk_ner_lrec_2022</pwccode>
    </paper>
    <paper id="569">
      <title>Investigating Independence vs. Control: Agenda-Setting in <fixed-case>R</fixed-case>ussian News Coverage on Social Media</title>
      <author><first>Annerose</first><last>Eichel</last></author>
      <author><first>Gabriella</first><last>Lapesa</last></author>
      <author><first>Sabine</first><last>Schulte im Walde</last></author>
      <pages>5314–5323</pages>
      <abstract>Agenda-setting is a widely explored phenomenon in political science: powerful stakeholders (governments or their financial supporters) have control over the media and set their agenda: political and economical powers determine which news should be salient. This is a clear case of targeted manipulation to divert the public attention from serious issues affecting internal politics (such as economic downturns and scandals) by flooding the media with potentially distracting information. We investigate agenda-setting in the Russian social media landscape, exploring the relation between economic indicators and mentions of foreign geopolitical entities, as well as of Russia itself. Our contributions are at three levels: at the level of the domain of the investigation, our study is the first to substructure the Russian media landscape in state-controlled vs. independent outlets in the context of strategic distraction from negative economic trends; at the level of the scope of the investigation, we involve a large set of geopolitical entities (while previous work has focused on the U.S.); at the qualitative level, our analysis of posts on Ukraine, whose relationship with Russia is of high geopolitical relevance, provides further insights into the contrast between state-controlled and independent outlets.</abstract>
      <url hash="962662b6">2022.lrec-1.569</url>
      <bibkey>eichel-etal-2022-investigating</bibkey>
      <pwccode url="https://github.com/anneroseeichel/agenda-setting" additional="false">anneroseeichel/agenda-setting</pwccode>
    </paper>
    <paper id="570">
      <title><fixed-case>SL</fixed-case>ä<fixed-case>ND</fixed-case>a version 2.0: Improved and Extended Annotation of Narrative and Dialogue in <fixed-case>S</fixed-case>wedish Literature</title>
      <author><first>Sara</first><last>Stymne</last></author>
      <author><first>Carin</first><last>Östman</last></author>
      <pages>5324–5333</pages>
      <abstract>In this paper, we describe version 2.0 of the SLäNDa corpus. SLäNDa, the Swedish Literary corpus of Narrative and Dialogue, now contains excerpts from 19 novels, written between 1809–1940. The main focus of the SLäNDa corpus is to distinguish between direct speech and the main narrative. In order to isolate the narrative, we also annotate everything else which does not belong to the narrative, such as thoughts, quotations, and letters. SLäNDa version 2.0 has a slightly updated annotation scheme from version 1.0. In addition, we added new texts from eleven authors and performed quality control on the previous version. We are specifically interested in different ways of marking speech segments, such as quotation marks, dashes, or no marking at all. To allow a detailed evaluation of this aspect, we added dedicated test sets to SLäNDa for these different types of speech marking. In a pilot experiment, we explore the impact of typographic speech marking by using these test sets, as well as artificially stripping the training data of speech markers.</abstract>
      <url hash="92348ac9">2022.lrec-1.570</url>
      <bibkey>stymne-ostman-2022-slanda</bibkey>
    </paper>
    <paper id="571">
      <title><fixed-case>AGIL</fixed-case>e: The First Lemmatizer for <fixed-case>A</fixed-case>ncient <fixed-case>G</fixed-case>reek Inscriptions</title>
      <author><first>Evelien</first><last>de Graaf</last></author>
      <author><first>Silvia</first><last>Stopponi</last></author>
      <author><first>Jasper K.</first><last>Bos</last></author>
      <author><first>Saskia</first><last>Peels-Matthey</last></author>
      <author><first>Malvina</first><last>Nissim</last></author>
      <pages>5334–5344</pages>
      <abstract>To facilitate corpus searches by classicists as well as to reduce data sparsity when training models, we focus on the automatic lemmatization of ancient Greek inscriptions, which have not received as much attention in this sense as literary text data has. We show that existing lemmatizers for ancient Greek, trained on literary data, are not performant on epigraphic data, due to major language differences between the two types of texts. We thus train the first inscription-specific lemmatizer achieving above 80% accuracy, and make both the models and the lemmatized data available to the community. We also provide a detailed error analysis highlighting peculiarities of inscriptions which again highlights the importance of a lemmatizer dedicated to inscriptions.</abstract>
      <url hash="096fd3a7">2022.lrec-1.571</url>
      <bibkey>de-graaf-etal-2022-agile</bibkey>
    </paper>
    <paper id="572">
      <title>»textklang« – Towards a Multi-Modal Exploration Platform for <fixed-case>G</fixed-case>erman Poetry</title>
      <author><first>Nadja</first><last>Schauffler</last></author>
      <author><first>Toni</first><last>Bernhart</last></author>
      <author><first>Andre</first><last>Blessing</last></author>
      <author><first>Gunilla</first><last>Eschenbach</last></author>
      <author><first>Markus</first><last>Gärtner</last></author>
      <author><first>Kerstin</first><last>Jung</last></author>
      <author><first>Anna</first><last>Kinder</last></author>
      <author><first>Julia</first><last>Koch</last></author>
      <author><first>Sandra</first><last>Richter</last></author>
      <author><first>Gabriel</first><last>Viehhauser</last></author>
      <author><first>Ngoc Thang</first><last>Vu</last></author>
      <author><first>Lorenz</first><last>Wesemann</last></author>
      <author><first>Jonas</first><last>Kuhn</last></author>
      <pages>5345–5355</pages>
      <abstract>We present the steps taken towards an exploration platform for a multi-modal corpus of German lyric poetry from the Romantic era developed in the project »textklang«. This interdisciplinary project develops a mixed-methods approach for the systematic investigation of the relationship between written text (here lyric poetry) and its potential and actual sonic realisation (in recitations, musical performances etc.). The multi-modal »textklang« platform will be designed to technically and analytically combine three modalities: the poetic text, the audio signal of a recorded recitation and, at a later stage, music scores of a musical setting of a poem. The methodological workflow will enable scholars to develop hypotheses about the relationship between textual form and sonic/prosodic realisation based on theoretical considerations, text interpretation and evidence from recorded recitations. The full workflow will support hypothesis testing either through systematic corpus analysis alone or with addtional contrastive perception experiments. For the experimental track, researchers will be enabled to manipulate prosodic parameters in (re-)synthesised variants of the original recordings. The focus of this paper is on the design of the base corpus and on tools for systematic exploration – placing special emphasis on our response to challenges stemming from multi-modality and the methodologically diverse interdisciplinary setup.</abstract>
      <url hash="2e5cc905">2022.lrec-1.572</url>
      <bibkey>schauffler-etal-2022-textklang</bibkey>
    </paper>
    <paper id="573">
      <title>Predicting the Proficiency Level of Nonnative <fixed-case>H</fixed-case>ebrew Authors</title>
      <author><first>Isabelle</first><last>Nguyen</last></author>
      <author><first>Shuly</first><last>Wintner</last></author>
      <pages>5356–5365</pages>
      <abstract>We present classifiers that can accurately predict the proficiency level of nonnative Hebrew learners. This is important for practical (mainly educational) applications, but the endeavor also sheds light on the features that support the classification, thereby improving our understanding of learner language in general, and transfer effects from Arabic, French, and Russian on nonnative Hebrew in particular.</abstract>
      <url hash="3b74c9ae">2022.lrec-1.573</url>
      <bibkey>nguyen-wintner-2022-predicting</bibkey>
    </paper>
    <paper id="574">
      <title>Trends, Limitations and Open Challenges in Automatic Readability Assessment Research</title>
      <author><first>Sowmya</first><last>Vajjala</last></author>
      <pages>5366–5377</pages>
      <abstract>Readability assessment is the task of evaluating the reading difficulty of a given piece of text. This article takes a closer look at contemporary NLP research on developing computational models for readability assessment, identifying the common approaches used for this task, their shortcomings, and some challenges for the future. Where possible, the survey also connects computational research with insights from related work in other disciplines such as education and psychology.</abstract>
      <url hash="686ea59f">2022.lrec-1.574</url>
      <bibkey>vajjala-2022-trends</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/newsela">Newsela</pwcdataset>
    </paper>
    <paper id="575">
      <title><fixed-case>H</fixed-case>ate<fixed-case>C</fixed-case>heck<fixed-case>HI</fixed-case>n: Evaluating <fixed-case>H</fixed-case>indi Hate Speech Detection Models</title>
      <author><first>Mithun</first><last>Das</last></author>
      <author><first>Punyajoy</first><last>Saha</last></author>
      <author><first>Binny</first><last>Mathew</last></author>
      <author><first>Animesh</first><last>Mukherjee</last></author>
      <pages>5378–5387</pages>
      <abstract>Due to the sheer volume of online hate, the AI and NLP communities have started building models to detect such hateful content. Recently, multilingual hate is a major emerging challenge for automated detection where code-mixing or more than one language have been used for conversation in social media. Typically, hate speech detection models are evaluated by measuring their performance on the held-out test data using metrics such as accuracy and F1-score. While these metrics are useful, it becomes difficult to identify using them where the model is failing, and how to resolve it. To enable more targeted diagnostic insights of such multilingual hate speech models, we introduce a set of functionalities for the purpose of evaluation. We have been inspired to design this kind of functionalities based on real-world conversation on social media. Considering Hindi as a base language, we craft test cases for each functionality. We name our evaluation dataset HateCheckHIn. To illustrate the utility of these functionalities , we test state-of-the-art transformer based m-BERT model and the Perspective API.</abstract>
      <url hash="2df72fb9">2022.lrec-1.575</url>
      <bibkey>das-etal-2022-hatecheckhin</bibkey>
      <pwccode url="https://github.com/hate-alert/hatecheckhin" additional="false">hate-alert/hatecheckhin</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hate-speech">Hate Speech</pwcdataset>
    </paper>
    <paper id="576">
      <title>Surfer100: Generating Surveys From Web Resources, <fixed-case>W</fixed-case>ikipedia-style</title>
      <author><first>Irene</first><last>Li</last></author>
      <author><first>Alex</first><last>Fabbri</last></author>
      <author><first>Rina</first><last>Kawamura</last></author>
      <author><first>Yixin</first><last>Liu</last></author>
      <author><first>Xiangru</first><last>Tang</last></author>
      <author><first>Jaesung</first><last>Tae</last></author>
      <author><first>Chang</first><last>Shen</last></author>
      <author><first>Sally</first><last>Ma</last></author>
      <author><first>Tomoe</first><last>Mizutani</last></author>
      <author><first>Dragomir</first><last>Radev</last></author>
      <pages>5388–5392</pages>
      <abstract>Fast-developing fields such as Artificial Intelligence (AI) often outpace the efforts of encyclopedic sources such as Wikipedia, which either do not completely cover recently-introduced topics or lack such content entirely. As a result, methods for automatically producing content are valuable tools to address this information overload. We show that recent advances in pretrained language modeling can be combined for a two-stage extractive and abstractive approach for Wikipedia lead paragraph generation. We extend this approach to generate longer Wikipedia-style summaries with sections and examine how such methods struggle in this application through detailed studies with 100 reference human-collected surveys. This is the first study on utilizing web resources for long Wikipedia-style summaries to the best of our knowledge.</abstract>
      <url hash="153a8657">2022.lrec-1.576</url>
      <bibkey>li-etal-2022-surfer100</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wikisum">WikiSum</pwcdataset>
    </paper>
    <paper id="577">
      <title><fixed-case>MS</fixed-case>-<fixed-case>L</fixed-case>a<fixed-case>TTE</fixed-case>: A Dataset of Where and When To-do Tasks are Completed</title>
      <author><first>Sujay Kumar</first><last>Jauhar</last></author>
      <author><first>Nirupama</first><last>Chandrasekaran</last></author>
      <author><first>Michael</first><last>Gamon</last></author>
      <author><first>Ryen</first><last>White</last></author>
      <pages>5393–5403</pages>
      <abstract>Tasks are a fundamental unit of work in the daily lives of people, who are increasingly using digital means to keep track of, organize, triage, and act on them. These digital tools – such as task management applications – provide a unique opportunity to study and understand tasks and their connection to the real world, and through intelligent assistance, help people be more productive. By logging signals such as text, timestamp information, and social connectivity graphs, an increasingly rich and detailed picture of how tasks are created and organized, what makes them important, and who acts on them, can be progressively developed. Yet the context around actual task completion remains fuzzy, due to the basic disconnect between actions taken in the real world and telemetry recorded in the digital world. Thus, in this paper we compile and release a novel, real-life, large-scale dataset called MS-LaTTE that captures two core aspects of the context surrounding task completion: location and time. We describe our annotation framework and conduct a number of analyses on the data that were collected, demonstrating that it captures intuitive contextual properties for common tasks. Finally, we test the dataset on the two problems of predicting spatial and temporal task co-occurrence, concluding that predictors for co-location and co-time are both learnable, with a BERT fine-tuned model outperforming several other baselines. The MS-LaTTE dataset provides an opportunity to tackle many new modeling challenges in contextual task understanding and we hope that its release will spur future research in task intelligence more broadly.</abstract>
      <url hash="f45ac3f8">2022.lrec-1.577</url>
      <bibkey>jauhar-etal-2022-ms</bibkey>
      <pwccode url="https://github.com/microsoft/ms-latte" additional="false">microsoft/ms-latte</pwccode>
    </paper>
    <paper id="578">
      <title><fixed-case>K</fixed-case>azakh<fixed-case>TTS</fixed-case>2: Extending the Open-Source <fixed-case>K</fixed-case>azakh <fixed-case>TTS</fixed-case> Corpus With More Data, Speakers, and Topics</title>
      <author><first>Saida</first><last>Mussakhojayeva</last></author>
      <author><first>Yerbolat</first><last>Khassanov</last></author>
      <author><first>Huseyin Atakan</first><last>Varol</last></author>
      <pages>5404–5411</pages>
      <abstract>We present an expanded version of our previously released Kazakh text-to-speech (KazakhTTS) synthesis corpus. In the new KazakhTTS2 corpus, the overall size has increased from 93 hours to 271 hours, the number of speakers has risen from two to five (three females and two males), and the topic coverage has been diversified with the help of new sources, including a book and Wikipedia articles. This corpus is necessary for building high-quality TTS systems for Kazakh, a Central Asian agglutinative language from the Turkic family, which presents several linguistic challenges. We describe the corpus construction process and provide the details of the training and evaluation procedures for the TTS system. Our experimental results indicate that the constructed corpus is sufficient to build robust TTS models for real-world applications, with a subjective mean opinion score ranging from 3.6 to 4.2 for all the five speakers. We believe that our corpus will facilitate speech and language research for Kazakh and other Turkic languages, which are widely considered to be low-resource due to the limited availability of free linguistic data. The constructed corpus, code, and pretrained models are publicly available in our GitHub repository.</abstract>
      <url hash="06cc44b3">2022.lrec-1.578</url>
      <bibkey>mussakhojayeva-etal-2022-kazakhtts2</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/kazakhtts">KazakhTTS</pwcdataset>
    </paper>
    <paper id="579">
      <title>A Graph-Based Method for Unsupervised Knowledge Discovery from Financial Texts</title>
      <author><first>Joel</first><last>Oksanen</last></author>
      <author><first>Abhilash</first><last>Majumder</last></author>
      <author><first>Kumar</first><last>Saunack</last></author>
      <author><first>Francesca</first><last>Toni</last></author>
      <author><first>Arun</first><last>Dhondiyal</last></author>
      <pages>5412–5417</pages>
      <abstract>The need for manual review of various financial texts, such as company filings and news, presents a major bottleneck in financial analysts’ work. Thus, there is great potential for the application of NLP methods, tools and resources to fulfil a genuine industrial need in finance. In this paper, we show how this potential can be fulfilled by presenting an end-to-end, fully unsupervised method for knowledge discovery from financial texts. Our method creatively integrates existing resources to construct automatically a knowledge graph of companies and related entities as well as to carry out unsupervised analysis of the resulting graph to provide quantifiable and explainable insights from the produced knowledge. The graph construction integrates entity processing and semantic expansion, before carrying out open relation extraction. We illustrate our method by calculating automatically the environmental rating for companies in the S&amp;P 500, based on company filings with the SEC (Securities and Exchange Commission). We then show the usefulness of our method in this setting by providing an assessment of our method’s outputs with an independent MSCI source.</abstract>
      <url hash="5dcd57d3">2022.lrec-1.579</url>
      <bibkey>oksanen-etal-2022-graph</bibkey>
    </paper>
    <paper id="580">
      <title>Leveraging Mental Health Forums for User-level Depression Detection on Social Media</title>
      <author><first>Sravani</first><last>Boinepelli</last></author>
      <author><first>Tathagata</first><last>Raha</last></author>
      <author><first>Harika</first><last>Abburi</last></author>
      <author><first>Pulkit</first><last>Parikh</last></author>
      <author><first>Niyati</first><last>Chhaya</last></author>
      <author><first>Vasudeva</first><last>Varma</last></author>
      <pages>5418–5427</pages>
      <abstract>The number of depression and suicide risk cases on social media platforms is ever-increasing, and the lack of depression detection mechanisms on these platforms is becoming increasingly apparent. A majority of work in this area has focused on leveraging linguistic features while dealing with small-scale datasets. However, one faces many obstacles when factoring into account the vastness and inherent imbalance of social media content. In this paper, we aim to optimize the performance of user-level depression classification to lessen the burden on computational resources. The resulting system executes in a quicker, more efficient manner, in turn making it suitable for deployment. To simulate a platform agnostic framework, we simultaneously replicate the size and composition of social media to identify victims of depression. We systematically design a solution that categorizes post embeddings, obtained by fine-tuning transformer models such as RoBERTa, and derives user-level representations using hierarchical attention networks. We also introduce a novel mental health dataset to enhance the performance of depression categorization. We leverage accounts of depression taken from this dataset to infuse domain-specific elements into our framework. Our proposed methods outperform numerous baselines across standard metrics for the task of depression detection in text.</abstract>
      <url hash="951b21a2">2022.lrec-1.580</url>
      <bibkey>boinepelli-etal-2022-leveraging</bibkey>
    </paper>
    <paper id="581">
      <title>Classifying Implant-Bearing Patients via their Medical Histories: a Pre-Study on <fixed-case>S</fixed-case>wedish <fixed-case>EMR</fixed-case>s with Semi-Supervised <fixed-case>G</fixed-case>an<fixed-case>BERT</fixed-case></title>
      <author><first>Benjamin</first><last>Danielsson</last></author>
      <author><first>Marina</first><last>Santini</last></author>
      <author><first>Peter</first><last>Lundberg</last></author>
      <author><first>Yosef</first><last>Al-Abasse</last></author>
      <author><first>Arne</first><last>Jonsson</last></author>
      <author><first>Emma</first><last>Eneling</last></author>
      <author><first>Magnus</first><last>Stridsman</last></author>
      <pages>5428–5435</pages>
      <abstract>In this paper, we compare the performance of two BERT-based text classifiers whose task is to classify patients (more precisely, their medical histories) as having or not having implant(s) in their body. One classifier is a fully-supervised BERT classifier. The other one is a semi-supervised GAN-BERT classifier. Both models are compared against a fully-supervised SVM classifier. Since fully-supervised classification is expensive in terms of data annotation, with the experiments presented in this paper, we investigate whether we can achieve a competitive performance with a semi-supervised classifier based only on a small amount of annotated data. Results are promising and show that the semi-supervised classifier has a competitive performance with the fully-supervised classifier.</abstract>
      <url hash="703255f5">2022.lrec-1.581</url>
      <bibkey>danielsson-etal-2022-classifying</bibkey>
    </paper>
    <paper id="582">
      <title>Standardisation of Dialect Comments in Social Networks in View of Sentiment Analysis : Case of <fixed-case>T</fixed-case>unisian Dialect</title>
      <author><first>Saméh</first><last>Kchaou</last></author>
      <author><first>Rahma</first><last>Boujelbane</last></author>
      <author><first>Emna</first><last>Fsih</last></author>
      <author><first>Lamia</first><last>Hadrich-Belguith</last></author>
      <pages>5436–5443</pages>
      <abstract>With the growing access to the internet, the spoken Arabic dialect language becomes informal languages written in social media. Most users post comments using their own dialect. This linguistic situation inhibits mutual understanding between internet users and makes difficult to use computational approaches since most Arabic resources are intended for the formal language: Modern Standard Arabic (MSA). In this paper, we present a pipeline to standardize the written texts in social networks by translating them to the standard language MSA. We fine-tun at first an identification bert-based model to select Tunisian Dialect (TD) from MSA and other dialects. Then, we learned transformer model to translate TD to MSA. The final system includes the translated TD text and the originally text written in MSA. Each of these steps was evaluated on the same test corpus. In order to test the effectiveness of the approach, we compared two opinion analysis models, the first intended for the Sentiment Analysis (SA) of dialect texts and the second for the MSA texts. We concluded that through standardization we obtain the best score.</abstract>
      <url hash="33a23f01">2022.lrec-1.582</url>
      <bibkey>kchaou-etal-2022-standardisation</bibkey>
    </paper>
    <paper id="583">
      <title><fixed-case>E</fixed-case>nsy<fixed-case>N</fixed-case>et: A Dataset for Encouragement and Sympathy Detection</title>
      <author><first>Tiberiu</first><last>Sosea</last></author>
      <author><first>Cornelia</first><last>Caragea</last></author>
      <pages>5444–5449</pages>
      <abstract>More and more people turn to Online Health Communities to seek social support during their illnesses. By interacting with peers with similar medical conditions, users feel emotionally and socially supported, which in turn leads to better adherence to therapy. Current studies in Online Health Communities focus only on the presence or absence of emotional support, while the available datasets are scarce or limited in terms of size. To enable development on emotional support detection, we introduce EnsyNet, a dataset of 6,500 sentences annotated with two types of support: encouragement and sympathy. We train BERT-based classifiers on this dataset, and apply our best BERT model in two large scale experiments. The results of these experiments show that receiving encouragements or sympathy improves users’ emotional state, while the lack of emotional support negatively impacts patients’ emotional state.</abstract>
      <url hash="c98f302f">2022.lrec-1.583</url>
      <bibkey>sosea-caragea-2022-ensynet</bibkey>
      <pwccode url="https://github.com/tsosea2/ensynet" additional="false">tsosea2/ensynet</pwccode>
    </paper>
    <paper id="584">
      <title>Preliminary Results on the Evaluation of Computational Tools for the Analysis of <fixed-case>Q</fixed-case>uechua and <fixed-case>A</fixed-case>ymara</title>
      <author><first>Marcelo Yuji</first><last>Himoro</last></author>
      <author><first>Antonio</first><last>Pareja-Lora</last></author>
      <pages>5450–5459</pages>
      <abstract>This research has focused on evaluating the existing open-source morphological analyzers for two of the most widely spoken indigenous macrolanguages in South America, namely Quechua and Aymara. Firstly, we have evaluated their performance (precision, recall and F1 score) for the individual languages for which they were developed (Cuzco Quechua and Aymara). Secondly, in order to assess how these tools handle other individual languages of the macrolanguage, we have extracted some sample text from school textbooks and educational resources. This sample text was edited in the different countries where these macrolanguages are spoken (Colombia, Ecuador, Peru, Bolivia, Chile and Argentina for Quechua; and Bolivia, Peru and Chile for Aymara), and it includes their different standardized forms (10 individual languages of Quechua and 3 of Aymara). Processing this text by means of the tools, we have (i) calculated their coverage (number of words recognized and analyzed) and (ii) studied in detail the cases for which each tool was unable to generate any output. Finally, we discuss different ways in which these tools could be optimized, either to improve their performances or, in the specific case of Quechua, to cover more individual languages of this macrolanguage in future works as well.</abstract>
      <url hash="72fe1608">2022.lrec-1.584</url>
      <bibkey>himoro-pareja-lora-2022-preliminary</bibkey>
    </paper>
    <paper id="585">
      <title>A Tale of Two Regulatory Regimes: Creation and Analysis of a Bilingual Privacy Policy Corpus</title>
      <author><first>Siddhant</first><last>Arora</last></author>
      <author><first>Henry</first><last>Hosseini</last></author>
      <author><first>Christine</first><last>Utz</last></author>
      <author><first>Vinayshekhar</first><last>Bannihatti Kumar</last></author>
      <author><first>Tristan</first><last>Dhellemmes</last></author>
      <author><first>Abhilasha</first><last>Ravichander</last></author>
      <author><first>Peter</first><last>Story</last></author>
      <author><first>Jasmine</first><last>Mangat</last></author>
      <author><first>Rex</first><last>Chen</last></author>
      <author><first>Martin</first><last>Degeling</last></author>
      <author><first>Thomas</first><last>Norton</last></author>
      <author><first>Thomas</first><last>Hupperich</last></author>
      <author><first>Shomir</first><last>Wilson</last></author>
      <author><first>Norman</first><last>Sadeh</last></author>
      <pages>5460–5472</pages>
      <abstract>Over the past decade, researchers have started to explore the use of NLP to develop tools aimed at helping the public, vendors, and regulators analyze disclosures made in privacy policies. With the introduction of new privacy regulations, the language of privacy policies is also evolving, and disclosures made by the same organization are not always the same in different languages, especially when used to communicate with users who fall under different jurisdictions. This work explores the use of language technologies to capture and analyze these differences at scale. We introduce an annotation scheme designed to capture the nuances of two new landmark privacy regulations, namely the EU’s GDPR and California’s CCPA/CPRA. We then introduce the first bilingual corpus of mobile app privacy policies consisting of 64 privacy policies in English (292K words) and 91 privacy policies in German (478K words), respectively with manual annotations for 8K and 19K fine-grained data practices. The annotations are used to develop computational methods that can automatically extract “disclosures” from privacy policies. Analysis of a subset of 59 “semi-parallel” policies reveals differences that can be attributed to different regulatory regimes, suggesting that systematic analysis of policies using automated language technologies is indeed a worthwhile endeavor.</abstract>
      <url hash="c605f57a">2022.lrec-1.585</url>
      <bibkey>arora-etal-2022-tale</bibkey>
    </paper>
    <paper id="586">
      <title><fixed-case>M</fixed-case>e<fixed-case>SH</fixed-case>up: Corpus for Full Text Biomedical Document Indexing</title>
      <author><first>Xindi</first><last>Wang</last></author>
      <author><first>Robert E.</first><last>Mercer</last></author>
      <author><first>Frank</first><last>Rudzicz</last></author>
      <pages>5473–5483</pages>
      <abstract>Medical Subject Heading (MeSH) indexing refers to the problem of assigning a given biomedical document with the most relevant labels from an extremely large set of MeSH terms. Currently, the vast number of biomedical articles in the PubMed database are manually annotated by human curators, which is time consuming and costly; therefore, a computational system that can assist the indexing is highly valuable. When developing supervised MeSH indexing systems, the availability of a large-scale annotated text corpus is desirable. A publicly available, large corpus that permits robust evaluation and comparison of various systems is important to the research community. We release a large scale annotated MeSH indexing corpus, MeSHup, which contains 1,342,667 full text articles, together with the associated MeSH labels and metadata, authors and publication venues that are collected from the MEDLINE database. We train an end-to-end model that combines features from documents and their associated labels on our corpus and report the new baseline.</abstract>
      <url hash="46db8c86">2022.lrec-1.586</url>
      <bibkey>wang-etal-2022-meshup</bibkey>
      <pwccode url="https://github.com/xdwang0726/meshup" additional="false">xdwang0726/meshup</pwccode>
    </paper>
    <paper id="587">
      <title>Hierarchical Annotation for Building A Suite of Clinical Natural Language Processing Tasks: Progress Note Understanding</title>
      <author><first>Yanjun</first><last>Gao</last></author>
      <author><first>Dmitriy</first><last>Dligach</last></author>
      <author><first>Timothy</first><last>Miller</last></author>
      <author><first>Samuel</first><last>Tesch</last></author>
      <author><first>Ryan</first><last>Laffin</last></author>
      <author><first>Matthew M.</first><last>Churpek</last></author>
      <author><first>Majid</first><last>Afshar</last></author>
      <pages>5484–5493</pages>
      <abstract>Applying methods in natural language processing on electronic health records (EHR) data has attracted rising interests. Existing corpus and annotation focus on modeling textual features and relation prediction. However, there are a paucity of annotated corpus built to model clinical diagnostic thinking, a processing involving text understanding, domain knowledge abstraction and reasoning. In this work, we introduce a hierarchical annotation schema with three stages to address clinical text understanding, clinical reasoning and summarization. We create an annotated corpus based on a large collection of publicly available daily progress notes, a type of EHR that is time-sensitive, problem-oriented, and well-documented by the format of Subjective, Objective, Assessment and Plan (SOAP). We also define a new suite of tasks, Progress Note Understanding, with three tasks utilizing the three annotation stages. This new suite aims at training and evaluating future NLP models for clinical text understanding, clinical knowledge representation, inference and summarization.</abstract>
      <url hash="49d2084d">2022.lrec-1.587</url>
      <bibkey>gao-etal-2022-hierarchical</bibkey>
    </paper>
    <paper id="588">
      <title><fixed-case>KC</fixed-case>4<fixed-case>MT</fixed-case>: A High-Quality Corpus for Multilingual Machine Translation</title>
      <author><first>Vinh Van</first><last>Nguyen</last></author>
      <author><first>Ha</first><last>Nguyen</last></author>
      <author><first>Huong Thanh</first><last>Le</last></author>
      <author><first>Thai Phuong</first><last>Nguyen</last></author>
      <author><first>Tan Van</first><last>Bui</last></author>
      <author><first>Luan Nghia</first><last>Pham</last></author>
      <author><first>Anh Tuan</first><last>Phan</last></author>
      <author><first>Cong Hoang-Minh</first><last>Nguyen</last></author>
      <author><first>Viet Hong</first><last>Tran</last></author>
      <author><first>Anh Huu</first><last>Tran</last></author>
      <pages>5494–5502</pages>
      <abstract>The multilingual parallel corpus is an important resource for many applications of natural language processing (NLP). For machine translation, the size and quality of the training corpus mainly affects the quality of the translation models. In this work, we present the method for building high-quality multilingual parallel corpus in the news domain and for some low-resource languages, including Vietnamese, Laos, and Khmer, to improve the quality of multilingual machine translation in these areas. We also publicized this one that includes 500.000 Vietnamese-Chinese bilingual sentence pairs; 150.000 Vietnamese-Laos bilingual sentence pairs, and 150.000 Vietnamese-Khmer bilingual sentence pairs.</abstract>
      <url hash="dce9ca1b">2022.lrec-1.588</url>
      <bibkey>nguyen-etal-2022-kc4mt</bibkey>
    </paper>
    <paper id="589">
      <title>Developing A Multilabel Corpus for the Quality Assessment of Online Political Talk</title>
      <author><first>Kokil</first><last>Jaidka</last></author>
      <pages>5503–5510</pages>
      <abstract>This paper motivates and presents the Twitter Deliberative Politics dataset, a corpus of political tweets labeled for its deliberative characteristics. The corpus was randomly sampled from replies to US congressmen and women. It is expected to be useful to a general community of computational linguists, political scientists, and social scientists interested in the study of online political expression, computer-mediated communication, and political deliberation. The data sampling and annotation methods are discussed and classical machine learning approaches are evaluated for their predictive performance on the different deliberative facets. The paper concludes with a discussion of future work aimed at developing dictionaries for the quality assessment of online political talk in English. The dataset and a demo dashboard are available at https://github.com/kj2013/twitter-deliberative-politics.</abstract>
      <url hash="d6b016ba">2022.lrec-1.589</url>
      <bibkey>jaidka-2022-developing</bibkey>
    </paper>
    <paper id="590">
      <title><fixed-case>BIL</fixed-case>in<fixed-case>MID</fixed-case>: A <fixed-case>S</fixed-case>panish-<fixed-case>E</fixed-case>nglish Corpus of the <fixed-case>US</fixed-case> Midwest</title>
      <author><first>Irati</first><last>Hurtado</last></author>
      <pages>5511–5516</pages>
      <abstract>This paper describes the Bilinguals in the Midwest (BILinMID) Corpus, a comparable text corpus of the Spanish and English spoken in the US Midwest by various types of bilinguals. Unlike other areas within the US where language contact has been widely documented (e.g., the Southwest), Spanish-English bilingualism in the Midwest has been understudied despite an increase in its Hispanic population. The BILinMID Corpus contains short stories narrated in Spanish and in English by 72 speakers representing different types of bilinguals: early simultaneous bilinguals, early sequential bilinguals, and late second language learners. All stories have been transcribed and annotated using various natural language processing tools. Additionally, a user interface has also been created to facilitate searching for specific patterns in the corpus as well as to filter out results according to specified criteria. Guidelines and procedures followed to create the corpus and the user interface are described in detail in the paper. The corpus is fully available online and it might be particularly interesting for researchers working on language variation and contact.</abstract>
      <url hash="4676b98d">2022.lrec-1.590</url>
      <bibkey>hurtado-2022-bilinmid</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="591">
      <title>One Document, Many Revisions: A Dataset for Classification and Description of Edit Intents</title>
      <author><first>Dheeraj</first><last>Rajagopal</last></author>
      <author><first>Xuchao</first><last>Zhang</last></author>
      <author><first>Michael</first><last>Gamon</last></author>
      <author><first>Sujay Kumar</first><last>Jauhar</last></author>
      <author><first>Diyi</first><last>Yang</last></author>
      <author><first>Eduard</first><last>Hovy</last></author>
      <pages>5517–5524</pages>
      <abstract>Document authoring involves a lengthy revision process, marked by individual edits that are frequently linked to comments. Modeling the relationship between edits and comments leads to a better understanding of document evolution, potentially benefiting applications such as content summarization, and task triaging. Prior work on understanding revisions has primarily focused on classifying edit intents, but falling short of a deeper understanding of the nature of these edits. In this paper, we present explore the challenge of describing an edit at two levels: identifying the edit intent, and describing the edit using free-form text. We begin by defining a taxonomy of general edit intents and introduce a new dataset of full revision histories of Wikipedia pages, annotated with each revision’s edit intent. Using this dataset, we train a classifier that achieves a 90% accuracy in identifying edit intent. We use this classifier to train a distantly-supervised model that generates a high-level description of a revision in free-form text. Our experimental results show that incorporating edit intent information aids in generating better edit descriptions. We establish a set of baselines for the edit description task, achieving a best score of 28 ROUGE, thus demonstrating the effectiveness of our layered approach to edit understanding.</abstract>
      <url hash="f7ace795">2022.lrec-1.591</url>
      <bibkey>rajagopal-etal-2022-one</bibkey>
    </paper>
    <paper id="592">
      <title><fixed-case>CTAP</fixed-case> for <fixed-case>C</fixed-case>hinese:A Linguistic Complexity Feature Automatic Calculation Platform</title>
      <author><first>Yue</first><last>Cui</last></author>
      <author><first>Junhui</first><last>Zhu</last></author>
      <author><first>Liner</first><last>Yang</last></author>
      <author><first>Xuezhi</first><last>Fang</last></author>
      <author><first>Xiaobin</first><last>Chen</last></author>
      <author><first>Yujie</first><last>Wang</last></author>
      <author><first>Erhong</first><last>Yang</last></author>
      <pages>5525–5538</pages>
      <abstract>The construct of linguistic complexity has been widely used in language learning research. Several text analysis tools have been created to automatically analyze linguistic complexity. However, the indexes supported by several existing Chinese text analysis tools are limited and different because of different research purposes. CTAP is an open-source linguistic complexity measurement extraction tool, which prompts any research purposes. Although it was originally developed for English, the Unstructured Information Management (UIMA) framework it used allows the integration of other languages. In this study, we integrated the Chinese component into CTAP, describing the index sets it incorporated and comparing it with three linguistic complexity tools for Chinese. The index set includes four levels of 196 linguistic complexity indexes: character level, word level, sentence level, and discourse level. So far, CTAP has implemented automatic calculation of complexity characteristics for four languages, aiming to help linguists without NLP background study language complexity.</abstract>
      <url hash="1e03bcd7">2022.lrec-1.592</url>
      <bibkey>cui-etal-2022-ctap</bibkey>
    </paper>
    <paper id="593">
      <title>A Corpus for Suggestion Mining of <fixed-case>G</fixed-case>erman Peer Feedback</title>
      <author><first>Dominik</first><last>Pfütze</last></author>
      <author><first>Eva</first><last>Ritz</last></author>
      <author><first>Julius</first><last>Janda</last></author>
      <author><first>Roman</first><last>Rietsche</last></author>
      <pages>5539–5547</pages>
      <abstract>Peer feedback in online education becomes increasingly important to meet the demand for feedback in large scale classes, such as e.g. Massive Open Online Courses (MOOCs). However, students are often not experts in how to write helpful feedback to their fellow students. In this paper, we introduce a corpus compiled from university students’ peer feedback to be able to detect suggestions on how to improve the students’ work and therefore being able to capture peer feedback helpfulness. To the best of our knowledge, this corpus is the first student peer feedback corpus in German which additionally was labelled with a new annotation scheme. The corpus consists of more than 600 written feedback (about 7,500 sentences). The utilisation of the corpus is broadly ranged from Dependency Parsing to Sentiment Analysis to Suggestion Mining, etc. We applied the latter to empirically validate the utility of the new corpus. Suggestion Mining is the extraction of sentences that contain suggestions from unstructured text. In this paper, we present a new annotation scheme to label sentences for Suggestion Mining. Two independent annotators labelled the corpus and achieved an inter-annotator agreement of 0.71. With the help of an expert arbitrator a gold standard was created. An automatic classification using BERT achieved an accuracy of 75.3%.</abstract>
      <url hash="7ca9c8c9">2022.lrec-1.593</url>
      <bibkey>pfutze-etal-2022-corpus</bibkey>
    </paper>
    <paper id="594">
      <title><fixed-case>CLGC</fixed-case>: A Corpus for <fixed-case>C</fixed-case>hinese Literary Grace Evaluation</title>
      <author><first>Yi</first><last>Li</last></author>
      <author><first>Dong</first><last>Yu</last></author>
      <author><first>Pengyuan</first><last>Liu</last></author>
      <pages>5548–5556</pages>
      <abstract>In this paper, we construct a Chinese literary grace corpus, CLGC, with 10,000 texts and more than 1.85 million tokens. Multi-level annotations are provided for each text in our corpus, including literary grace level, sentence category, and figure-of-speech type. Based on the corpus, we dig deep into the correlation between fine-grained features (semantic information, part-of-speech and figure-of-speech, etc.) and literary grace level. We also propose a new Literary Grace Evaluation (LGE) task, which aims at making a comprehensive assessment of the literary grace level according to the text. In the end, we build some classification models with machine learning algorithms (such as SVM, TextCNN) to prove the effectiveness of our features and corpus for LGE. The results of our preliminary classification experiments have achieved 79.71% on the weighted average F1-score.</abstract>
      <url hash="ea1f36fa">2022.lrec-1.594</url>
      <bibkey>li-etal-2022-clgc</bibkey>
      <pwccode url="https://github.com/blcunlp/clgc" additional="false">blcunlp/clgc</pwccode>
    </paper>
    <paper id="595">
      <title>Anonymising the <fixed-case>SAGT</fixed-case> Speech Corpus and Treebank</title>
      <author><first>Özlem</first><last>Çetinoğlu</last></author>
      <author><first>Antje</first><last>Schweitzer</last></author>
      <pages>5557–5564</pages>
      <abstract>Anonymisation, that is identifying and neutralising sensitive references, is a crucial part of dataset creation. In this paper, we describe the anonymisation process of a Turkish-German code-switching corpus, namely SAGT, which consists of speech data and a treebank that is built on its transcripts. We employed a selective pseudonymisation approach where we manually identified sensitive references to anonymise and replaced them with surrogate values on the treebank side. In addition to maintaining data privacy, our primary concerns in surrogate selection were keeping the integrity of code-switching properties, morphosyntactic annotation layers, and semantics. After the treebank anonymisation, we anonymised the speech data by mapping between the treebank sentences and audio transcripts with the help of Praat scripts. The treebank is publicly available for research purposes and the audio files can be obtained via an individual licence agreement.</abstract>
      <url hash="3ac6cdff">2022.lrec-1.595</url>
      <bibkey>cetinoglu-schweitzer-2022-anonymising</bibkey>
    </paper>
    <paper id="596">
      <title>Construction of a Quality Estimation Dataset for Automatic Evaluation of <fixed-case>J</fixed-case>apanese Grammatical Error Correction</title>
      <author><first>Daisuke</first><last>Suzuki</last></author>
      <author><first>Yujin</first><last>Takahashi</last></author>
      <author><first>Ikumi</first><last>Yamashita</last></author>
      <author><first>Taichi</first><last>Aida</last></author>
      <author><first>Tosho</first><last>Hirasawa</last></author>
      <author><first>Michitaka</first><last>Nakatsuji</last></author>
      <author><first>Masato</first><last>Mita</last></author>
      <author><first>Mamoru</first><last>Komachi</last></author>
      <pages>5565–5572</pages>
      <abstract>In grammatical error correction (GEC), automatic evaluation is considered as an important factor for research and development of GEC systems. Previous studies on automatic evaluation have shown that quality estimation models built from datasets with manual evaluation can achieve high performance in automatic evaluation of English GEC. However, quality estimation models have not yet been studied in Japanese, because there are no datasets for constructing quality estimation models. In this study, therefore, we created a quality estimation dataset with manual evaluation to build an automatic evaluation model for Japanese GEC. By building a quality estimation model using this dataset and conducting a meta-evaluation, we verified the usefulness of the quality estimation model for Japanese GEC.</abstract>
      <url hash="14a3b423">2022.lrec-1.596</url>
      <bibkey>suzuki-etal-2022-construction</bibkey>
    </paper>
    <paper id="597">
      <title>Enhanced Distant Supervision with State-Change Information for Relation Extraction</title>
      <author><first>Jui</first><last>Shah</last></author>
      <author><first>Dongxu</first><last>Zhang</last></author>
      <author><first>Sam</first><last>Brody</last></author>
      <author><first>Andrew</first><last>McCallum</last></author>
      <pages>5573–5579</pages>
      <abstract>In this work, we introduce a method for enhancing distant supervision with state-change information for relation extraction. We provide a training dataset created via this process, along with manually annotated development and test sets. We present an analysis of the curation process and data, and compare it to standard distant supervision. We demonstrate that the addition of state-change information reduces noise when used for static relation extraction, and can also be used to train a relation-extraction system that detects a change of state in relations.</abstract>
      <url hash="66fa1714">2022.lrec-1.597</url>
      <bibkey>shah-etal-2022-enhanced</bibkey>
      <pwccode url="https://github.com/iesl/state-change-re" additional="false">iesl/state-change-re</pwccode>
    </paper>
    <paper id="598">
      <title>The <fixed-case>H</fixed-case>ebrew Essay Corpus</title>
      <author><first>Chen</first><last>Gafni</last></author>
      <author><first>Anat</first><last>Prior</last></author>
      <author><first>Shuly</first><last>Wintner</last></author>
      <pages>5580–5586</pages>
      <abstract>We present the Hebrew Essay Corpus: an annotated corpus of Hebrew language argumentative essays authored by prospective higher-education students. The corpus includes both essays by native speakers, written as part of the psychometric exam that is used to assess their future success in academic studies; and essays authored by non-native speakers, with three different native languages, that were written as part of a language aptitude test. The corpus is uniformly encoded and stored. The non-native essays were annotated with target hypotheses whose main goal is to make the texts amenable to automatic processing (morphological and syntactic analysis). The corpus is available for academic purposes upon request. We describe the corpus and the error correction and annotation schemes used in its analysis. In addition to introducing this new resource, we discuss the challenges of identifying and analyzing non-native language use in general, and propose various ways for dealing with these challenges.</abstract>
      <url hash="82101f33">2022.lrec-1.598</url>
      <bibkey>gafni-etal-2022-hebrew</bibkey>
    </paper>
    <paper id="599">
      <title>Design and Evaluation of the Corpus of Everyday <fixed-case>J</fixed-case>apanese Conversation</title>
      <author><first>Hanae</first><last>Koiso</last></author>
      <author><first>Haruka</first><last>Amatani</last></author>
      <author><first>Yasuharu</first><last>Den</last></author>
      <author><first>Yuriko</first><last>Iseki</last></author>
      <author><first>Yuichi</first><last>Ishimoto</last></author>
      <author><first>Wakako</first><last>Kashino</last></author>
      <author><first>Yoshiko</first><last>Kawabata</last></author>
      <author><first>Ken’ya</first><last>Nishikawa</last></author>
      <author><first>Yayoi</first><last>Tanaka</last></author>
      <author><first>Yasuyuki</first><last>Usuda</last></author>
      <author><first>Yuka</first><last>Watanabe</last></author>
      <pages>5587–5594</pages>
      <abstract>We have constructed the Corpus of Everyday Japanese Conversation (CEJC) and published it in March 2022. The CEJC is designed to contain various kinds of everyday conversations in a balanced manner to capture their diversity. The CEJC features not only audio but also video data to facilitate precise understanding of the mechanism of real-life social behavior. The publication of a large-scale corpus of everyday conversations that includes video data is a new approach. The CEJC contains 200 hours of speech, 577 conversations, about 2.4 million words, and a total of 1675 conversants. In this paper, we present an overview of the corpus, including the recording method and devices, structure of the corpus, formats of video and audio files, transcription, and annotations. We then report some results of the evaluation of the CEJC in terms of conversant and conversation attributes. We show that the CEJC includes a good balance of adult conversants in terms of gender and age, as well as a variety of conversations in terms of conversation forms, places, activities, and numbers of conversants.</abstract>
      <url hash="74ccf686">2022.lrec-1.599</url>
      <bibkey>koiso-etal-2022-design</bibkey>
    </paper>
    <paper id="600">
      <title>Developing Language Resources and <fixed-case>NLP</fixed-case> Tools for the <fixed-case>N</fixed-case>orth <fixed-case>K</fixed-case>orean Language</title>
      <author><first>Arda</first><last>Akdemir</last></author>
      <author><first>Yeojoo</first><last>Jeon</last></author>
      <author><first>Tetsuo</first><last>Shibuya</last></author>
      <pages>5595–5600</pages>
      <abstract>Since the division of Korea, the two Korean languages have diverged significantly over the last 70 years. However, due to the lack of linguistic source of the North Korean language, there is no DPRK-based language model. Consequently, scholars rely on the Korean language model by utilizing South Korean linguistic data. In this paper, we first present a large-scale dataset for the North Korean language. We use the dataset to train a BERT-based language model, DPRK-BERT. Second, we annotate a subset of this dataset for the sentiment analysis task. Finally, we compare the performance of different language models for masked language modeling and sentiment analysis tasks.</abstract>
      <url hash="6fd9dbbf">2022.lrec-1.600</url>
      <bibkey>akdemir-etal-2022-developing</bibkey>
    </paper>
    <paper id="601">
      <title>Developing a Dataset of Overridden Information in <fixed-case>W</fixed-case>ikipedia</title>
      <author><first>Masatoshi</first><last>Tsuchiya</last></author>
      <author><first>Yasutaka</first><last>Yokoi</last></author>
      <pages>5601–5608</pages>
      <abstract>This paper proposes a new task of detecting information override. Since all information on the Web is not updated in a timely manner, the necessity is created for information that is overridden by another information source to be discarded. The task is formalized as a binary classification problem to determine whether a reference sentence has overridden a target sentence. In investigating this task, this paper describes a construction procedure for the dataset of overridden information by collecting sentence pairs from the difference between two versions of Wikipedia. Our developing dataset shows that the old version of Wikipedia contains much overridden information and that the detection of information override is necessary.</abstract>
      <url hash="e0ceb1df">2022.lrec-1.601</url>
      <bibkey>tsuchiya-yokoi-2022-developing</bibkey>
    </paper>
    <paper id="602">
      <title><fixed-case>BRATECA</fixed-case> (<fixed-case>B</fixed-case>razilian Tertiary Care Dataset): a Clinical Information Dataset for the <fixed-case>P</fixed-case>ortuguese Language</title>
      <author><first>Bernardo</first><last>Consoli</last></author>
      <author><first>Henrique D. P.</first><last>dos Santos</last></author>
      <author><first>Ana Helena D. P. S.</first><last>Ulbrich</last></author>
      <author><first>Renata</first><last>Vieira</last></author>
      <author><first>Rafael H.</first><last>Bordini</last></author>
      <pages>5609–5616</pages>
      <abstract>Computational medicine research requires clinical data for training and testing purposes, so the development of datasets composed of real hospital data is of utmost importance in this field. Most such data collections are in the English language, were collected in anglophone countries, and do not reflect other clinical realities, which increases the importance of national datasets for projects that hope to positively impact public health. This paper presents a new Brazilian Clinical Dataset containing over 70,000 admissions from 10 hospitals in two Brazilian states, composed of a sum total of over 2.5 million free-text clinical notes alongside data pertaining to patient information, prescription information, and exam results. This data was collected, organized, deidentified, and is being distributed via credentialed access for the use of the research community. In the course of presenting the new dataset, this paper will explore the new dataset’s structure, population, and potential benefits of using this dataset in clinical AI tasks.</abstract>
      <url hash="4c9e3bc3">2022.lrec-1.602</url>
      <bibkey>consoli-etal-2022-brateca</bibkey>
    </paper>
    <paper id="603">
      <title>Universal Grammatical Dependencies for <fixed-case>P</fixed-case>ortuguese with <fixed-case>CINTIL</fixed-case> Data, <fixed-case>LX</fixed-case> Processing and <fixed-case>CLARIN</fixed-case> support</title>
      <author><first>António</first><last>Branco</last></author>
      <author><first>João Ricardo</first><last>Silva</last></author>
      <author><first>Luís</first><last>Gomes</last></author>
      <author><first>João</first><last>António Rodrigues</last></author>
      <pages>5617–5626</pages>
      <abstract>The grammatical framework for the mapping between linguistic form and meaning representation known as Universal Dependencies relies on a non-constituency syntactic analysis that is centered on the notion of grammatical relation (e.g. Subject, Object, etc.). Given its core goal of providing a common set of analysis primitives suitable to every natural language, and its practical objective of fostering their computational grammatical processing, it keeps being an active domain of research in science and technology of language. This paper presents a new collection of quality language resources for the computational processing of the Portuguese language under the Universal Dependencies framework (UD). This is an all-encompassing, publicly available open collection of mutually consistent and inter-operable scientific resources that includes reliably annotated corpora, top-performing processing tools and expert support services: a new UPOS-annotated corpus, CINTIL-UPos, with 675K tokens and a new UD treebank, CINTIL-UDep Treebank, with nearly 38K sentences; a UPOS tagger, LX-UTagger, and a UD parser, LX-UDParser, trained on these corpora, available both as local stand-alone tools and as remote web-based services; and helpdesk support ensured by the Knowledge Center for the Science and Technology of Portuguese of the CLARIN research infrastructure.</abstract>
      <url hash="1f391d7a">2022.lrec-1.603</url>
      <bibkey>branco-etal-2022-universal</bibkey>
      <pwccode url="https://github.com/nlx-group/ud-portuguese" additional="false">nlx-group/ud-portuguese</pwccode>
    </paper>
    <paper id="604">
      <title><fixed-case>CWID</fixed-case>-hi: A Dataset for Complex Word Identification in <fixed-case>H</fixed-case>indi Text</title>
      <author><first>Gayatri</first><last>Venugopal</last></author>
      <author><first>Dhanya</first><last>Pramod</last></author>
      <author><first>Ravi</first><last>Shekhar</last></author>
      <pages>5627–5636</pages>
      <abstract>Text simplification is a method for improving the accessibility of text by converting complex sentences into simple sentences. Multiple studies have been done to create datasets for text simplification. However, most of these datasets focus on high-resource languages only. In this work, we proposed a complex word dataset for Hindi, a language largely ignored in text simplification literature. We used various Hindi knowledge annotators for annotation to capture the annotator’s language knowledge. Our analysis shows a significant difference between native and non-native annotators’ perception of word complexity. We also built an automatic complex word classifier using a soft voting approach based on the predictions from tree-based ensemble classifiers. These models behave differently for annotations made by different categories of users, such as native and non-native speakers. Our dataset and analysis will help simplify Hindi text depending on the user’s language understanding. The dataset is available at https://zenodo.org/record/5229160.</abstract>
      <url hash="16a26013">2022.lrec-1.604</url>
      <bibkey>venugopal-etal-2022-cwid</bibkey>
    </paper>
    <paper id="605">
      <title>Automatic Classification of <fixed-case>R</fixed-case>ussian Learner Errors</title>
      <author><first>Alla</first><last>Rozovskaya</last></author>
      <pages>5637–5647</pages>
      <abstract>Grammatical Error Correction systems are typically evaluated overall, without taking into consideration performance on individual error types because system output is not annotated with respect to error type. We introduce a tool that automatically classifies errors in Russian learner texts. The tool takes an edit pair consisting of the original token(s) and the corresponding replacement and provides a grammatical error category. Manual evaluation of the output reveals that in more than 93% of cases the error categories are judged as correct or acceptable. We apply the tool to carry out a fine-grained evaluation on the performance of two error correction systems for Russian.</abstract>
      <url hash="5c12a4c2">2022.lrec-1.605</url>
      <bibkey>rozovskaya-2022-automatic</bibkey>
    </paper>
    <paper id="606">
      <title>Annotation of metaphorical expressions in the Basic Corpus of <fixed-case>P</fixed-case>olish Metaphors</title>
      <author><first>Elżbieta</first><last>Hajnicz</last></author>
      <pages>5648–5653</pages>
      <abstract>This paper presents a corpus of Polish texts annotated with metaphorical expressions. It is composed of two parts of comparable size, selected from two subcorpora of the Polish National Corpus: the subcorpus manually annotated on morphosyntactic level, named entities level etc., and the Polish Coreference Corpus, with manually annotated mentions and the coreference relations between them, but automatically annotated on the morphosyntactic level (only the second part is actually annotated). In the paper we briefly outline the method for identifying metaphorical expressions in a text, based on the MIPVU procedure. The main difference is the stress put on novel metaphors and considering neologistic derivatives that have metaphorical properties. The annotation procedure is based on two notions: vehicle – a part of an expression used metaphorically, representing a source domain and its topic – a part referring to reality, representing a target domain. Next, we propose several features (text form, conceptual structure, conventionality and contextuality) to classify metaphorical expressions identified in texts. Additionally, some metaphorical expressions are identified as concerning personal identity matters and classified w.r.t. their properties. Finally, we analyse and evaluate the results of the annotation.</abstract>
      <url hash="dca94889">2022.lrec-1.606</url>
      <bibkey>hajnicz-2022-annotation</bibkey>
    </paper>
    <paper id="607">
      <title><fixed-case>C</fixed-case>hi<fixed-case>MST</fixed-case>: A <fixed-case>C</fixed-case>hinese Medical Corpus for Word Segmentation and Medical Term Recognition</title>
      <author><first>Yuanhe</first><last>Tian</last></author>
      <author><first>Han</first><last>Qin</last></author>
      <author><first>Fei</first><last>Xia</last></author>
      <author><first>Yan</first><last>Song</last></author>
      <pages>5654–5664</pages>
      <abstract>Chinese word segmentation (CWS) and named entity recognition (NER) are two important tasks in Chinese natural language processing. To achieve good model performance on these tasks, existing neural approaches normally require a large amount of labeled training data, which is often unavailable for specific domains such as the Chinese medical domain due to privacy and legal issues. To address this problem, we have developed a Chinese medical corpus named ChiMST which consists of question-answer pairs collected from an online medical healthcare platform and is annotated with word boundary and medical term information. For word boundary, we mainly follow the word segmentation guidelines for the Penn Chinese Treebank (Xia, 2000); for medical terms, we define 9 categories and 18 sub-categories after consulting medical experts. To provide baselines on this corpus, we train existing state-of-the-art models on it and achieve good performance. We believe that the corpus and the baseline systems will be a valuable resource for CWS and NER research on the medical domain.</abstract>
      <url hash="718ef475">2022.lrec-1.607</url>
      <bibkey>tian-etal-2022-chimst</bibkey>
      <pwccode url="https://github.com/synlp/chimst" additional="false">synlp/chimst</pwccode>
    </paper>
    <paper id="608">
      <title>Building a Synthetic Biomedical Research Article Citation Linkage Corpus</title>
      <author><first>Sudipta</first><last>Singha Roy</last></author>
      <author><first>Robert E.</first><last>Mercer</last></author>
      <pages>5665–5672</pages>
      <abstract>Citations are frequently used in publications to support the presented results and to demonstrate the previous discoveries while also assisting the reader in following the chronological progression of information through publications. In scientific publications, a citation refers to the referenced document, but it makes no mention of the exact span of text that is being referred to. Connecting the citation to this span of text is called citation linkage. In this paper, to find these citation linkages in biomedical research publications using deep learning, we provide a synthetic silver standard corpus as well as the method to build this corpus. The motivation for building this corpus is to provide a training set for deep learning models that will locate the text spans in a reference article, given a citing statement, based on semantic similarity. This corpus is composed of sentence pairs, where one sentence in each pair is the citing statement and the other one is a candidate cited statement from the referenced paper. The corpus is annotated using an unsupervised sentence embedding method. The effectiveness of this silver standard corpus for training citation linkage models is validated against a human-annotated gold standard corpus.</abstract>
      <url hash="9d746d51">2022.lrec-1.608</url>
      <bibkey>singha-roy-mercer-2022-building</bibkey>
    </paper>
    <paper id="609">
      <title>Dataset Construction for Scientific-Document Writing Support by Extracting Related Work Section and Citations from <fixed-case>PDF</fixed-case> Papers</title>
      <author><first>Keita</first><last>Kobayashi</last></author>
      <author><first>Kohei</first><last>Koyama</last></author>
      <author><first>Hiromi</first><last>Narimatsu</last></author>
      <author><first>Yasuhiro</first><last>Minami</last></author>
      <pages>5673–5682</pages>
      <abstract>To augment datasets used for scientific-document writing support research, we extract texts from “Related Work” sections and citation information in PDF-formatted papers published in English. The previous dataset was constructed entirely with Tex-formatted papers, from which it is easy to extract citation information. However, since many publicly available papers in various fields are provided only in PDF format, a dataset constructed using only Tex papers has limited utility. To resolve this problem, we augment the existing dataset by extracting the titles of sections using the visual features of PDF documents and extracting the Related Work section text using the explicit title information. Since text generated from the figures and footnotes appearing in the extraction target areas is considered noise, we remove instances of such text. Moreover, we map the cited paper’s information obtained using existing tools to citation marks detected by regular expression rules, resulting in pairs of cited paper information and text of the Related Work section. By evaluating body text extraction and citation mapping in the constructed dataset, the accuracy of the proposed dataset was found to be close to that of the previous dataset. Accordingly, we demonstrated the possibility of building a significantly augmented dataset.</abstract>
      <url hash="74dea5a7">2022.lrec-1.609</url>
      <bibkey>kobayashi-etal-2022-dataset</bibkey>
    </paper>
    <paper id="610">
      <title><fixed-case>R</fixed-case>u<fixed-case>PAWS</fixed-case>: A <fixed-case>R</fixed-case>ussian Adversarial Dataset for Paraphrase Identification</title>
      <author><first>Nikita</first><last>Martynov</last></author>
      <author><first>Irina</first><last>Krotova</last></author>
      <author><first>Varvara</first><last>Logacheva</last></author>
      <author><first>Alexander</first><last>Panchenko</last></author>
      <author><first>Olga</first><last>Kozlova</last></author>
      <author><first>Nikita</first><last>Semenov</last></author>
      <pages>5683–5691</pages>
      <abstract>Paraphrase identification task can be easily challenged by changing word order, e.g. as in “Can a good person become bad?”. While for English this problem was tackled by the PAWS dataset (Zhang et al., 2019), datasets for Russian paraphrase detection lack non-paraphrase examples with high lexical overlap. We present RuPAWS, the first adversarial dataset for Russian paraphrase identification. Our dataset consists of examples from PAWS translated to the Russian language and manually annotated by native speakers. We compare it to the largest available dataset for Russian ParaPhraser and show that the best available paraphrase identifiers for the Russian language fail on the RuPAWS dataset. At the same time, the state-of-the-art paraphrasing model RuBERT trained on both RuPAWS and ParaPhraser obtains high performance on the RuPAWS dataset while maintaining its accuracy on the ParaPhraser benchmark. We also show that RuPAWS can measure the sensitivity of models to word order and syntax structure since simple baselines fail even when given RuPAWS training samples.</abstract>
      <url hash="605d8cc3">2022.lrec-1.610</url>
      <bibkey>martynov-etal-2022-rupaws</bibkey>
      <pwccode url="https://github.com/mts-ai/rupaws-dataset" additional="false">mts-ai/rupaws-dataset</pwccode>
    </paper>
    <paper id="611">
      <title>Atril: an <fixed-case>XML</fixed-case> Visualization System for Corpus Texts</title>
      <author><first>Andressa</first><last>Rodrigues Gomide</last></author>
      <author><first>Conceição</first><last>Carapinha</last></author>
      <author><first>Cornelia</first><last>Plag</last></author>
      <pages>5692–5695</pages>
      <abstract>This paper presents Atril, an XML visualization system for corpus texts, developed for, but not restricted to, the project Corpus de Audiências (CorAuDis), a corpus composed of transcripts of sessions of criminal proceedings recorded at the Coimbra Court. The main aim of the tool is to provide researchers with a web-based environment that allows for an easily customizable visualization of corpus texts with heavy structural annotation. Existing corpus analysis tools such as SketchEngine, TEITOK and CQPweb offer some kind of visualization mechanisms, but, to our knowledge, none meets our project’s main needs. Our requirements are a system that is open-source; that can be easily connected to CQPweb and TEITOK, that provides a full text-view with switchable visualization templates, that allows for the visualization of overlapping utterances. To meet those requirements, we created Atril, a module with a corpus XML file viewer, a visualization management system, and a word alignment tool.</abstract>
      <url hash="dfcaadb0">2022.lrec-1.611</url>
      <bibkey>rodrigues-gomide-etal-2022-atril</bibkey>
    </paper>
    <paper id="612">
      <title><fixed-case>MASALA</fixed-case>: Modelling and Analysing the Semantics of Adpositions in Linguistic Annotation of <fixed-case>H</fixed-case>indi</title>
      <author><first>Aryaman</first><last>Arora</last></author>
      <author><first>Nitin</first><last>Venkateswaran</last></author>
      <author><first>Nathan</first><last>Schneider</last></author>
      <pages>5696–5704</pages>
      <abstract>We present a completed, publicly available corpus of annotated semantic relations of adpositions and case markers in Hindi. We used the multilingual SNACS annotation scheme, which has been applied to a variety of typologically diverse languages. Building on past work examining linguistic problems in SNACS annotation, we use language models to attempt automatic labelling of SNACS supersenses in Hindi and achieve results competitive with past work on English. We look towards upstream applications in semantic role labelling and extension to related languages such as Gujarati.</abstract>
      <url hash="16cae07b">2022.lrec-1.612</url>
      <bibkey>arora-etal-2022-masala</bibkey>
    </paper>
    <paper id="613">
      <title><fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies for <fixed-case>P</fixed-case>unjabi</title>
      <author><first>Aryaman</first><last>Arora</last></author>
      <pages>5705–5711</pages>
      <abstract>We introduce the first Universal Dependencies treebank for Punjabi (written in the Gurmukhi script) and discuss corpus design and linguistic phenomena encountered in annotation. The treebank covers a variety of genres and has been annotated for POS tags, dependency relations, and graph-based Enhanced Dependencies. We aim to expand the diversity of coverage of Indo-Aryan languages in UD.</abstract>
      <url hash="84c1b386">2022.lrec-1.613</url>
      <bibkey>arora-2022-universal</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/indiccorp">IndicCorp</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="614">
      <title><fixed-case>T</fixed-case>e<fixed-case>S</fixed-case>um: Human-Generated Abstractive Summarization Corpus for <fixed-case>T</fixed-case>elugu</title>
      <author><first>Ashok</first><last>Urlana</last></author>
      <author><first>Nirmal</first><last>Surange</last></author>
      <author><first>Pavan</first><last>Baswani</last></author>
      <author><first>Priyanka</first><last>Ravva</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <pages>5712–5722</pages>
      <abstract>Expert human annotation for summarization is definitely an expensive task, and can not be done on huge scales. But with this work, we show that even with a crowd sourced summary generation approach, quality can be controlled by aggressive expert informed filtering and sampling-based human evaluation. We propose a pipeline that crowd-sources summarization data and then aggressively filters the content via: automatic and partial expert evaluation. Using this pipeline we create a high-quality Telugu Abstractive Summarization dataset (TeSum) which we validate with sampling-based human evaluation. We also provide baseline numbers for various models commonly used for summarization. A number of recently released datasets for summarization, scraped the web-content relying on the assumption that summary is made available with the article by the publishers. While this assumption holds for multiple resources (or news-sites) in English, it should not be generalised across languages without thorough analysis and verification. Our analysis clearly shows that this assumption does not hold true for most Indian language news resources. We show that our proposed filtration pipeline can even be applied to these large-scale scraped datasets to extract better quality article-summary pairs.</abstract>
      <url hash="9a04438b">2022.lrec-1.614</url>
      <bibkey>urlana-etal-2022-tesum</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/xl-sum">XL-Sum</pwcdataset>
    </paper>
    <paper id="615">
      <title>A Corpus of Simulated Counselling Sessions with Dialog Act Annotation</title>
      <author><first>John</first><last>Lee</last></author>
      <author><first>Haley</first><last>Fong</last></author>
      <author><first>Lai Shuen Judy</first><last>Wong</last></author>
      <author><first>Chun Chung</first><last>Mak</last></author>
      <author><first>Chi Hin</first><last>Yip</last></author>
      <author><first>Ching Wah Larry</first><last>Ng</last></author>
      <pages>5723–5730</pages>
      <abstract>We present a corpus of simulated counselling sessions consisting of speech- and text-based dialogs in Cantonese. Consisting of 152K Chinese characters, the corpus labels the dialog act of both client and counsellor utterances, segments each dialog into stages, and identifies the forward and backward links in the dialog. We analyze the distribution of client and counsellor communicative intentions in the various stages, and discuss significant patterns of the dialog flow.</abstract>
      <url hash="073a41ab">2022.lrec-1.615</url>
      <bibkey>lee-etal-2022-corpus</bibkey>
    </paper>
    <paper id="616">
      <title>Interactive Evaluation of Dialog Track at <fixed-case>DSTC</fixed-case>9</title>
      <author><first>Shikib</first><last>Mehri</last></author>
      <author><first>Yulan</first><last>Feng</last></author>
      <author><first>Carla</first><last>Gordon</last></author>
      <author><first>Seyed Hossein</first><last>Alavi</last></author>
      <author><first>David</first><last>Traum</last></author>
      <author><first>Maxine</first><last>Eskenazi</last></author>
      <pages>5731–5738</pages>
      <abstract>The ultimate goal of dialog research is to develop systems that can be effectively used in interactive settings by real users. To this end, we introduced the Interactive Evaluation of Dialog Track at the 9th Dialog System Technology Challenge. This track consisted of two sub-tasks. The first sub-task involved building knowledge-grounded response generation models. The second sub-task aimed to extend dialog models beyond static datasets by assessing them in an interactive setting with real users. Our track challenges participants to develop strong response generation models and explore strategies that extend them to back-and-forth interactions with real users. The progression from static corpora to interactive evaluation introduces unique challenges and facilitates a more thorough assessment of open-domain dialog systems. This paper provides an overview of the track, including the methodology and results. Furthermore, it provides insights into how to best evaluate open-domain dialog models.</abstract>
      <url hash="0087ee31">2022.lrec-1.616</url>
      <bibkey>mehri-etal-2022-interactive</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/fed">FED</pwcdataset>
    </paper>
    <paper id="617">
      <title><fixed-case>HADREB</fixed-case>: Human Appraisals and (<fixed-case>E</fixed-case>nglish) Descriptions of Robot Emotional Behaviors</title>
      <author><first>Josue</first><last>Torres-Fonseca</last></author>
      <author><first>Casey</first><last>Kennington</last></author>
      <pages>5739–5748</pages>
      <abstract>Humans sometimes anthropomorphize everyday objects, but especially robots that have human-like qualities and that are often able to interact with and respond to humans in ways that other objects cannot. Humans especially attribute emotion to robot behaviors, partly because humans often use and interpret emotions when interacting with other humans, and they apply that capability when interacting with robots. Moreover, emotions are a fundamental part of the human language system and emotions are used as scaffolding for language learning, making them an integral part of language learning and meaning. However, there are very few datasets that explore how humans perceive the emotional states of robots and how emotional behaviors relate to human language. To address this gap we have collected HADREB, a dataset of human appraisals and English descriptions of robot emotional behaviors collected from over 30 participants. These descriptions and human emotion appraisals are collected using the Mistyrobotics Misty II and the Digital Dream Labs Cozmo (formerly Anki) robots. The dataset contains English descriptions and emotion appraisals of more than 500 descriptions and graded valence labels of 8 emotion pairs for each behavior and each robot. In this paper we describe the process of collecting and cleaning the data, give a general analysis of the data, and evaluate the usefulness of the dataset in two experiments, one using a language model to map descriptions to emotions, the other maps robot behaviors to emotions.</abstract>
      <url hash="19fce756">2022.lrec-1.617</url>
      <bibkey>torres-fonsesca-kennington-2022-hadreb</bibkey>
    </paper>
    <paper id="618">
      <title>Dialogue Collection for Recording the Process of Building Common Ground in a Collaborative Task</title>
      <author><first>Koh</first><last>Mitsuda</last></author>
      <author><first>Ryuichiro</first><last>Higashinaka</last></author>
      <author><first>Yuhei</first><last>Oga</last></author>
      <author><first>Sen</first><last>Yoshida</last></author>
      <pages>5749–5758</pages>
      <abstract>To develop a dialogue system that can build common ground with users, the process of building common ground through dialogue needs to be clarified. However, the studies on the process of building common ground have not been well conducted; much work has focused on finding the relationship between a dialogue in which users perform a collaborative task and its task performance represented by the final result of the task. In this study, to clarify the process of building common ground, we propose a data collection method for automatically recording the process of building common ground through a dialogue by using the intermediate result of a task. We collected 984 dialogues, and as a result of investigating the process of building common ground, we found that the process can be classified into several typical patterns and that conveying each worker’s understanding through affirmation of a counterpart’s utterances especially contributes to building common ground. In addition, toward dialogue systems that can build common ground, we conducted an automatic estimation of the degree of built common ground and found that its degree can be estimated quite accurately.</abstract>
      <url hash="ac5a2d5e">2022.lrec-1.618</url>
      <bibkey>mitsuda-etal-2022-dialogue</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mindcraft">MindCraft</pwcdataset>
    </paper>
    <paper id="619">
      <title>Collection and Analysis of Travel Agency Task Dialogues with Age-Diverse Speakers</title>
      <author><first>Michimasa</first><last>Inaba</last></author>
      <author><first>Yuya</first><last>Chiba</last></author>
      <author><first>Ryuichiro</first><last>Higashinaka</last></author>
      <author><first>Kazunori</first><last>Komatani</last></author>
      <author><first>Yusuke</first><last>Miyao</last></author>
      <author><first>Takayuki</first><last>Nagai</last></author>
      <pages>5759–5767</pages>
      <abstract>When individuals communicate with each other, they use different vocabulary, speaking speed, facial expressions, and body language depending on the people they talk to. This paper focuses on the speaker’s age as a factor that affects the change in communication. We collected a multimodal dialogue corpus with a wide range of speaker ages. As a dialogue task, we focus on travel, which interests people of all ages, and we set up a task based on a tourism consultation between an operator and a customer at a travel agency. This paper provides details of the dialogue task, the collection procedure and annotations, and the analysis on the characteristics of the dialogues and facial expressions focusing on the age of the speakers. Results of the analysis suggest that the adult speakers have more independent opinions, the older speakers more frequently express their opinions frequently compared with other age groups, and the operators expressed a smile more frequently to the minor speakers.</abstract>
      <url hash="78ccc524">2022.lrec-1.619</url>
      <bibkey>inaba-etal-2022-collection</bibkey>
    </paper>
    <paper id="620">
      <title>Strategy-level Entrainment of Dialogue System Users in a Creative Visual Reference Resolution Task</title>
      <author><first>Deepthi</first><last>Karkada</last></author>
      <author><first>Ramesh</first><last>Manuvinakurike</last></author>
      <author><first>Maike</first><last>Paetzel-Prüsmann</last></author>
      <author><first>Kallirroi</first><last>Georgila</last></author>
      <pages>5768–5777</pages>
      <abstract>In this work, we study entrainment of users playing a creative reference resolution game with an autonomous dialogue system. The language understanding module in our dialogue system leverages annotated human-wizard conversational data, openly available knowledge graphs, and crowd-augmented data. Unlike previous entrainment work, our dialogue system does not attempt to make the human conversation partner adopt lexical items in their dialogue, but rather to adapt their descriptive strategy to one that is simpler to parse for our natural language understanding unit. By deploying this dialogue system through a crowd-sourced study, we show that users indeed entrain on a “strategy-level” without the change of strategy impinging on their creativity. Our work thus presents a promising future research direction for developing dialogue management systems that can strategically influence people’s descriptive strategy to ease the system’s language understanding in creative tasks.</abstract>
      <url hash="a9dfc428">2022.lrec-1.620</url>
      <bibkey>karkada-etal-2022-strategy</bibkey>
    </paper>
    <paper id="621">
      <title><fixed-case>MMC</fixed-case>hat: Multi-Modal Chat Dataset on Social Media</title>
      <author><first>Yinhe</first><last>Zheng</last></author>
      <author><first>Guanyi</first><last>Chen</last></author>
      <author><first>Xin</first><last>Liu</last></author>
      <author><first>Jian</first><last>Sun</last></author>
      <pages>5778–5786</pages>
      <abstract>Incorporating multi-modal contexts in conversation is an important step for developing more engaging dialogue systems. In this work, we explore this direction by introducing MMChat: a large scale Chinese multi-modal dialogue corpus (32.4M raw dialogues and 120.84K filtered dialogues). Unlike previous corpora that are crowd-sourced or collected from fictitious movies, MMChat contains image-grounded dialogues collected from real conversations on social media, in which the sparsity issue is observed. Specifically, image-initiated dialogues in common communications may deviate to some non-image-grounded topics as the conversation proceeds. To better investigate this issue, we manually annotate 100K dialogues from MMChat and further filter the corpus accordingly, which yields MMChat-hf. We develop a benchmark model to address the sparsity issue in dialogue generation tasks by adapting the attention routing mechanism on image features. Experiments demonstrate the usefulness of incorporating image features and the effectiveness in handling the sparsity of image features.</abstract>
      <url hash="ee924afe">2022.lrec-1.621</url>
      <bibkey>zheng-etal-2022-mmchat</bibkey>
      <pwccode url="https://github.com/silverriver/mmchat" additional="false">silverriver/mmchat</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mmchat">MMChat</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-genome">Visual Genome</pwcdataset>
    </paper>
    <paper id="622">
      <title><fixed-case>E</fixed-case>-<fixed-case>C</fixed-case>onv<fixed-case>R</fixed-case>ec: A Large-Scale Conversational Recommendation Dataset for <fixed-case>E</fixed-case>-Commerce Customer Service</title>
      <author><first>Meihuizi</first><last>Jia</last></author>
      <author><first>Ruixue</first><last>Liu</last></author>
      <author><first>Peiying</first><last>Wang</last></author>
      <author><first>Yang</first><last>Song</last></author>
      <author><first>Zexi</first><last>Xi</last></author>
      <author><first>Haobin</first><last>Li</last></author>
      <author><first>Xin</first><last>Shen</last></author>
      <author><first>Meng</first><last>Chen</last></author>
      <author><first>Jinhui</first><last>Pang</last></author>
      <author><first>Xiaodong</first><last>He</last></author>
      <pages>5787–5796</pages>
      <abstract>There has been a growing interest in developing conversational recommendation system (CRS), which provides valuable recommendations to users through conversations. Compared to the traditional recommendation, it advocates wealthier interactions and provides possibilities to obtain users’ exact preferences explicitly. Nevertheless, the corresponding research on this topic is limited due to the lack of broad-coverage dialogue corpus, especially real-world dialogue corpus. To handle this issue and facilitate our exploration, we construct E-ConvRec, an authentic Chinese dialogue dataset consisting of over 25k dialogues and 770k utterances, which contains user profile, product knowledge base (KB), and multiple sequential real conversations between users and recommenders. Next, we explore conversational recommendation in a real scene from multiple facets based on the dataset. Therefore, we particularly design three tasks: user preference recognition, dialogue management, and personalized recommendation. In the light of the three tasks, we establish baseline results on E-ConvRec to facilitate future studies.</abstract>
      <url hash="18e48650">2022.lrec-1.622</url>
      <bibkey>jia-etal-2022-e</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/coached-conversational-preference-elicitation">Coached Conversational Preference Elicitation</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cookie">Cookie</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/durecdial">DuRecDial</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/inspired">Inspired</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/opendialkg">OpenDialKG</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/redial">ReDial</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tg-redial">TG-ReDial</pwcdataset>
    </paper>
    <paper id="623">
      <title><fixed-case>SHONGLAP</fixed-case>: A Large <fixed-case>B</fixed-case>engali Open-Domain Dialogue Corpus</title>
      <author><first>Syed Mostofa</first><last>Monsur</last></author>
      <author><first>Sakib</first><last>Chowdhury</last></author>
      <author><first>Md Shahrar</first><last>Fatemi</last></author>
      <author><first>Shafayat</first><last>Ahmed</last></author>
      <pages>5797–5804</pages>
      <abstract>We introduce SHONGLAP, a large annotated open-domain dialogue corpus in Bengali language. Due to unavailability of high-quality dialogue datasets for low-resource languages like Bengali, existing neural open-domain dialogue systems suffer from data scarcity. We propose a framework to prepare large-scale open-domain dialogue datasets from publicly available multi-party discussion podcasts, talk-shows and label them based on weak-supervision techniques which is particularly suitable for low-resource settings. Using this framework, we prepared our corpus, the first reported Bengali open-domain dialogue corpus (7.7k+ fully annotated dialogues in total) which can serve as a strong baseline for future works. Experimental results show that our corpus improves performance of large language models (BanglaBERT) in case of downstream classification tasks during fine-tuning.</abstract>
      <url hash="a2fd2310">2022.lrec-1.623</url>
      <bibkey>monsur-etal-2022-shonglap</bibkey>
    </paper>
    <paper id="624">
      <title>A Comparison of Praising Skills in Face-to-Face and Remote Dialogues</title>
      <author><first>Toshiki</first><last>Onishi</last></author>
      <author><first>Asahi</first><last>Ogushi</last></author>
      <author><first>Yohei</first><last>Tahara</last></author>
      <author><first>Ryo</first><last>Ishii</last></author>
      <author><first>Atsushi</first><last>Fukayama</last></author>
      <author><first>Takao</first><last>Nakamura</last></author>
      <author><first>Akihiro</first><last>Miyata</last></author>
      <pages>5805–5812</pages>
      <abstract>Praising behavior is considered to an important method of communication in daily life and social activities. An engineering analysis of praising behavior is therefore valuable. However, a dialogue corpus for this analysis has not yet been developed. Therefore, we develop corpuses for face-to-face and remote two-party dialogues with ratings of praising skills. The corpuses enable us to clarify how to use verbal and nonverbal behaviors for successfully praise. In this paper, we analyze the differences between the face-to-face and remote corpuses, in particular the expressions in adjudged praising scenes in both corpuses, and also evaluated praising skills. We also compare differences in head motion, gaze behavior, facial expression in high-rated praising scenes in both corpuses. The results showed that the distribution of praising scores was similar in face-to-face and remote dialogues, although the ratio of the number of praising scenes to the number of utterances was different. In addition, we confirmed differences in praising behavior in face-to-face and remote dialogues.</abstract>
      <url hash="3ceb0cd3">2022.lrec-1.624</url>
      <bibkey>onishi-etal-2022-comparison</bibkey>
    </paper>
    <paper id="625">
      <title>Comparing Approaches to Language Understanding for Human-Robot Dialogue: An Error Taxonomy and Analysis</title>
      <author><first>Ada</first><last>Tur</last></author>
      <author><first>David</first><last>Traum</last></author>
      <pages>5813–5820</pages>
      <abstract>In this paper, we compare two different approaches to language understanding for a human-robot interaction domain in which a human commander gives navigation instructions to a robot. We contrast a relevance-based classifier with a GPT-2 model, using about 2000 input-output examples as training data. With this level of training data, the relevance-based model outperforms the GPT-2 based model 79% to 8%. We also present a taxonomy of types of errors made by each model, indicating that they have somewhat different strengths and weaknesses, so we also examine the potential for a combined model.</abstract>
      <url hash="644b29c2">2022.lrec-1.625</url>
      <bibkey>tur-traum-2022-comparing</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/room-to-room">R2R</pwcdataset>
    </paper>
    <paper id="626">
      <title><fixed-case>SPORTSINTERVIEW</fixed-case>: A Large-Scale Sports Interview Benchmark for Entity-centric Dialogues</title>
      <author><first>Hanfei</first><last>Sun</last></author>
      <author><first>Ziyuan</first><last>Cao</last></author>
      <author><first>Diyi</first><last>Yang</last></author>
      <pages>5821–5828</pages>
      <abstract>We propose a novel knowledge grounded dialogue (interview) dataset SPORTSINTERVIEW set in the domain of sports interview. Our dataset contains two types of external knowledge sources as knowledge grounding, and is rich in content, containing about 150K interview sessions and 34K distinct interviewees. Compared to existing knowledge grounded dialogue datasets, our interview dataset is larger in size, comprises natural dialogues revolving around real-world sports matches, and have more than one dimension of external knowledge linking. We performed several experiments on SPORTSINTERVIEW and found that models such as BART fine-tuned on our dataset are able to learn lots of relevant domain knowledge and generate meaningful sentences (questions or responses). However, their performance is still far from humans (by comparing to gold sentences in the dataset) and hence encourages future research utilizing SPORTSINTERVIEW.</abstract>
      <url hash="708f5e4e">2022.lrec-1.626</url>
      <bibkey>sun-etal-2022-sportsinterview</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cmu-dog">CMU DoG</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wizard-of-wikipedia">Wizard of Wikipedia</pwcdataset>
    </paper>
    <paper id="627">
      <title><fixed-case>E</fixed-case>mo<fixed-case>I</fixed-case>n<fixed-case>H</fixed-case>indi: A Multi-label Emotion and Intensity Annotated Dataset in <fixed-case>H</fixed-case>indi for Emotion Recognition in Dialogues</title>
      <author><first>Gopendra Vikram</first><last>Singh</last></author>
      <author><first>Priyanshu</first><last>Priya</last></author>
      <author><first>Mauajama</first><last>Firdaus</last></author>
      <author><first>Asif</first><last>Ekbal</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>5829–5837</pages>
      <abstract>The long-standing goal of Artificial Intelligence (AI) has been to create human-like conversational systems. Such systems should have the ability to develop an emotional connection with the users, consequently, emotion recognition in dialogues has gained popularity. Emotion detection in dialogues is a challenging task because humans usually convey multiple emotions with varying degrees of intensities in a single utterance. Moreover, emotion in an utterance of a dialogue may be dependent on previous utterances making the task more complex. Recently, emotion recognition in low-resource languages like Hindi has been in great demand. However, most of the existing datasets for multi-label emotion and intensity detection in conversations are in English. To this end, we propose a large conversational dataset in Hindi named EmoInHindi for multi-label emotion and intensity recognition in conversations containing 1,814 dialogues with a total of 44,247 utterances. We prepare our dataset in a Wizard-of-Oz manner for mental health and legal counselling of crime victims. Each utterance of dialogue is annotated with one or more emotion categories from 16 emotion labels including neutral and their corresponding intensity. We further propose strong contextual baselines that can detect the emotion(s) and corresponding emotional intensity of an utterance given the conversational context.</abstract>
      <url hash="0db44bbe">2022.lrec-1.627</url>
      <bibkey>singh-etal-2022-emoinhindi</bibkey>
    </paper>
    <paper id="628">
      <title>The Project Dialogism Novel Corpus: A Dataset for Quotation Attribution in Literary Texts</title>
      <author><first>Krishnapriya</first><last>Vishnubhotla</last></author>
      <author><first>Adam</first><last>Hammond</last></author>
      <author><first>Graeme</first><last>Hirst</last></author>
      <pages>5838–5848</pages>
      <abstract>We present the Project Dialogism Novel Corpus, or PDNC, an annotated dataset of quotations for English literary texts. PDNC contains annotations for 35,978 quotations across 22 full-length novels, and is by an order of magnitude the largest corpus of its kind. Each quotation is annotated for the speaker, addressees, type of quotation, referring expression, and character mentions within the quotation text. The annotated attributes allow for a comprehensive evaluation of models of quotation attribution and coreference for literary texts.</abstract>
      <url hash="18457516">2022.lrec-1.628</url>
      <bibkey>vishnubhotla-etal-2022-project</bibkey>
      <pwccode url="https://github.com/priya22/pdnc-lrec2022" additional="false">priya22/pdnc-lrec2022</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/pdnc">PDNC</pwcdataset>
    </paper>
    <paper id="629">
      <title>Who’s in, who’s out? Predicting the Inclusiveness or Exclusiveness of Personal Pronouns in Parliamentary Debates</title>
      <author><first>Ines</first><last>Rehbein</last></author>
      <author><first>Josef</first><last>Ruppenhofer</last></author>
      <pages>5849–5858</pages>
      <abstract>This paper presents a compositional annotation scheme to capture the clusivity properties of personal pronouns in context, that is their ability to construct and manage in-groups and out-groups by including/excluding the audience and/or non-speech act participants in reference to groups that also include the speaker. We apply and test our schema on pronoun instances in speeches taken from the German parliament. The speeches cover a time period from 2017-2021 and comprise manual annotations for 3,126 sentences. We achieve high inter-annotator agreement for our new schema, with a Cohen’s κ in the range of 89.7-93.2 and a percentage agreement of &gt; 96%. Our exploratory analysis of in/exclusive pronoun use in the parliamentary setting provides some face validity for our new schema. Finally, we present baseline experiments for automatically predicting clusivity in political debates, with promising results for many referential constellations, yielding an overall 84.9% micro F1 for all pronouns.</abstract>
      <url hash="892d7224">2022.lrec-1.629</url>
      <bibkey>rehbein-ruppenhofer-2022-whos</bibkey>
    </paper>
    <paper id="630">
      <title>A Language Modelling Approach to Quality Assessment of <fixed-case>OCR</fixed-case>’ed Historical Text</title>
      <author><first>Callum</first><last>Booth</last></author>
      <author><first>Robert</first><last>Shoemaker</last></author>
      <author><first>Robert</first><last>Gaizauskas</last></author>
      <pages>5859–5864</pages>
      <abstract>We hypothesise and evaluate a language model-based approach for scoring the quality of OCR transcriptions in the British Library Newspapers (BLN) corpus parts 1 and 2, to identify the best quality OCR for use in further natural language processing tasks, with a wider view to link individual newspaper reports of crime in nineteenth-century London to the Digital Panopticon—a structured repository of criminal lives. We mitigate the absence of gold standard transcriptions of the BLN corpus by utilising a corpus of genre-adjacent texts that capture the common and legal parlance of nineteenth-century London—the Proceedings of the Old Bailey Online—with a view to rank the BLN transcriptions by their OCR quality.</abstract>
      <url hash="60d7ad8a">2022.lrec-1.630</url>
      <bibkey>booth-etal-2022-language</bibkey>
    </paper>
    <paper id="631">
      <title>Identifying Copied Fragments in a 18th Century <fixed-case>D</fixed-case>utch Chronicle</title>
      <author><first>Roser</first><last>Morante</last></author>
      <author><first>Eleanor L. T.</first><last>Smith</last></author>
      <author><first>Lianne</first><last>Wilhelmus</last></author>
      <author><first>Alie</first><last>Lassche</last></author>
      <author><first>Erika</first><last>Kuijpers</last></author>
      <pages>5865–5878</pages>
      <abstract>We apply computational stylometric techniques to an 18th century Dutch chronicle to determine which fragments of the manuscript represent the author’s own original work and which show signs of external source use through either direct copying or paraphrasing. Through stylometric methods the majority of text fragments in the chronicle can be correctly labelled as either the author’s own words, direct copies from sources or paraphrasing. Our results show that clustering text fragments based on stylometric measures is an effective methodology for authorship verification of this document; however, this approach is less effective when personal writing style is masked by author independent styles or when applied to paraphrased text.</abstract>
      <url hash="17f9d8cc">2022.lrec-1.631</url>
      <bibkey>morante-etal-2022-identifying</bibkey>
      <pwccode url="https://github.com/chroniclingnovelty/stylometry-lrec22" additional="false">chroniclingnovelty/stylometry-lrec22</pwccode>
    </paper>
    <paper id="632">
      <title>A Study of Distant Viewing of ukiyo-e prints</title>
      <author><first>Konstantina</first><last>Liagkou</last></author>
      <author><first>John</first><last>Pavlopoulos</last></author>
      <author><first>Ewa</first><last>Machotka</last></author>
      <pages>5879–5888</pages>
      <abstract>This paper contributes to studying relationships between Japanese topography and places featured in early modern landscape prints, so-called ukiyo-e or ‘pictures of the floating world’. The printed inscriptions on these images feature diverse place-names, both man-made and natural formations. However, due to the corpus’s richness and diversity, the precise nature of artistic mediation of the depicted places remains little understood. In this paper, we explored a new analytical approach based on the macroanalysis of images facilitated by Natural Language Processing technologies. This paper presents a small dataset with inscriptions on prints that have been annotated by an art historian for included place-name entities. Our dataset is released for public use. By fine-tuning and applying a Japanese BERT-based Name Entity Recogniser, we provide a use-case of a macroanalysis of a visual dataset that is hosted by the digital database of the Art Research Center at the Ritsumeikan University, Kyoto. Our work studies the relationship between topography and its visual renderings in early modern Japanese ukiyo-e landscape prints, demonstrating how an art historian’s work can be improved with Natural Language Processing toward distant viewing of visual datasets. We release our dataset and code for public use: https://github.com/connalia/ukiyo-e_meisho_nlp</abstract>
      <url hash="db02bce2">2022.lrec-1.632</url>
      <bibkey>liagkou-etal-2022-study</bibkey>
      <pwccode url="https://github.com/connalia/ukiyo-e_meisho_nlp" additional="false">connalia/ukiyo-e_meisho_nlp</pwccode>
    </paper>
    <paper id="633">
      <title><fixed-case>CCTAA</fixed-case>: A Reproducible Corpus for <fixed-case>C</fixed-case>hinese Authorship Attribution Research</title>
      <author><first>Haining</first><last>Wang</last></author>
      <author><first>Allen</first><last>Riddell</last></author>
      <pages>5889–5893</pages>
      <abstract>Authorship attribution infers the likely author of an unsigned, single-authored document from a pool of candidates. Despite recent advances, a lack of standard, reproducible testbeds for Chinese language documents impedes progress. In this paper, we present the Chinese Cross-Topic Authorship Attribution (CCTAA) corpus. It is the first standard testbed for authorship attribution on contemporary Chinese prose. The cross-topic design and relatively inflexible genre of newswire contribute to an appropriate level of difficulty. It supports reproducible research by using pre-defined data splits. We show that a sequence classifier based on pre-trained Chinese RoBERTa embedding and a support vector machine classifier using function character n-gram frequency features perform below expectations on this task. The code for generating the corpus and reproducing the baselines is freely available at https://codeberg.org/haining/cctaa.</abstract>
      <url hash="19a9dc41">2022.lrec-1.633</url>
      <bibkey>wang-riddell-2022-cctaa</bibkey>
    </paper>
    <paper id="634">
      <title>An automatic model and Gold Standard for translation alignment of <fixed-case>A</fixed-case>ncient <fixed-case>G</fixed-case>reek</title>
      <author><first>Tariq</first><last>Yousef</last></author>
      <author><first>Chiara</first><last>Palladino</last></author>
      <author><first>Farnoosh</first><last>Shamsian</last></author>
      <author><first>Anise</first><last>d’Orange Ferreira</last></author>
      <author><first>Michel</first><last>Ferreira dos Reis</last></author>
      <pages>5894–5905</pages>
      <abstract>This paper illustrates a workflow for developing and evaluating automatic translation alignment models for Ancient Greek. We designed an annotation Style Guide and a gold standard for the alignment of Ancient Greek-English and Ancient Greek-Portuguese, measured inter-annotator agreement and used the resulting dataset to evaluate the performance of various translation alignment models. We proposed a fine-tuning strategy that employs unsupervised training with mono- and bilingual texts and supervised training using manually aligned sentences. The results indicate that the fine-tuned model based on XLM-Roberta is superior in performance, and it achieved good results on language pairs that were not part of the training data.</abstract>
      <url hash="cc367b84">2022.lrec-1.634</url>
      <bibkey>yousef-etal-2022-automatic</bibkey>
      <pwccode url="https://github.com/ugaritalignment/alignment-gold-standards" additional="false">ugaritalignment/alignment-gold-standards</pwccode>
    </paper>
    <paper id="635">
      <title>Rhetorical Structure Approach for Online Deception Detection: A Survey</title>
      <author><first>Francielle</first><last>Vargas</last></author>
      <author><first>Jonas</first><last>D‘Alessandro</last></author>
      <author><first>Zohar</first><last>Rabinovich</last></author>
      <author><first>Fabrício</first><last>Benevenuto</last></author>
      <author><first>Thiago</first><last>Pardo</last></author>
      <pages>5906–5915</pages>
      <abstract>Most information is passed on in the form of language. Therefore, research on how people use language to inform and misinform, and how this knowledge may be automatically extracted from large amounts of text is surely relevant. This survey provides first-hand experiences and a comprehensive review of rhetorical-level structure analysis for online deception detection. We systematically analyze how discourse structure, aligned or not with other approaches, is applied to automatic fake news and fake reviews detection on the web and social media. Moreover, we categorize discourse-tagged corpora along with results, hence offering a summary and accessible introductions to new researchers.</abstract>
      <url hash="964cfccc">2022.lrec-1.635</url>
      <bibkey>vargas-etal-2022-rhetorical</bibkey>
    </paper>
    <paper id="636">
      <title><fixed-case>TYPIC</fixed-case>: A Corpus of Template-Based Diagnostic Comments on Argumentation</title>
      <author><first>Shoichi</first><last>Naito</last></author>
      <author><first>Shintaro</first><last>Sawada</last></author>
      <author><first>Chihiro</first><last>Nakagawa</last></author>
      <author><first>Naoya</first><last>Inoue</last></author>
      <author><first>Kenshi</first><last>Yamaguchi</last></author>
      <author><first>Iori</first><last>Shimizu</last></author>
      <author><first>Farjana Sultana</first><last>Mim</last></author>
      <author><first>Keshav</first><last>Singh</last></author>
      <author><first>Kentaro</first><last>Inui</last></author>
      <pages>5916–5928</pages>
      <abstract>Providing feedback on the argumentation of the learner is essential for developing critical thinking skills, however, it requires a lot of time and effort. To mitigate the overload on teachers, we aim to automate a process of providing feedback, especially giving diagnostic comments which point out the weaknesses inherent in the argumentation. It is recommended to give specific diagnostic comments so that learners can recognize the diagnosis without misinterpretation. However, it is not obvious how the task of providing specific diagnostic comments should be formulated. We present a formulation of the task as template selection and slot filling to make an automatic evaluation easier and the behavior of the model more tractable. The key to the formulation is the possibility of creating a template set that is sufficient for practical use. In this paper, we define three criteria that a template set should satisfy: expressiveness, informativeness, and uniqueness, and verify the feasibility of creating a template set that satisfies these criteria as a first trial. We will show that it is feasible through an annotation study that converts diagnostic comments given in a text to a template format. The corpus used in the annotation study is publicly available.</abstract>
      <url hash="f3f322a9">2022.lrec-1.636</url>
      <bibkey>naito-etal-2022-typic</bibkey>
      <pwccode url="https://github.com/cl-tohoku/typic" additional="false">cl-tohoku/typic</pwccode>
    </paper>
    <paper id="637">
      <title>Towards Speaker Verification for Crowdsourced Speech Collections</title>
      <author><first>John</first><last>Mendonca</last></author>
      <author><first>Rui</first><last>Correia</last></author>
      <author><first>Mariana</first><last>Lourenço</last></author>
      <author><first>João</first><last>Freitas</last></author>
      <author><first>Isabel</first><last>Trancoso</last></author>
      <pages>5929–5937</pages>
      <abstract>Crowdsourcing the collection of speech provides a scalable setting to access a customisable demographic according to each dataset’s needs. The correctness of speaker metadata is especially relevant for speaker-centred collections - ones that require the collection of a fixed amount of data per speaker. This paper identifies two different types of misalignment present in these collections: Multiple Accounts misalignment (different contributors map to the same speaker), and Multiple Speakers misalignment (multiple speakers map to the same contributor). Based on state-of-the-art approaches to Speaker Verification, this paper proposes an unsupervised method for measuring speaker metadata plausibility of a collection, i.e., evaluating the match (or lack thereof) between contributors and speakers. The solution presented is composed of an embedding extractor and a clustering module. Results indicate high precision in automatically classifying contributor alignment (&gt;0.94).</abstract>
      <url hash="a22889bc">2022.lrec-1.637</url>
      <bibkey>mendonca-etal-2022-towards</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/musan">MUSAN</pwcdataset>
    </paper>
    <paper id="638">
      <title>Align-smatch: A Novel Evaluation Method for <fixed-case>C</fixed-case>hinese <fixed-case>A</fixed-case>bstract <fixed-case>M</fixed-case>eaning <fixed-case>R</fixed-case>epresentation Parsing based on Alignment of Concept and Relation</title>
      <author><first>Liming</first><last>Xiao</last></author>
      <author><first>Bin</first><last>Li</last></author>
      <author><first>Zhixing</first><last>Xu</last></author>
      <author><first>Kairui</first><last>Huo</last></author>
      <author><first>Minxuan</first><last>Feng</last></author>
      <author><first>Junsheng</first><last>Zhou</last></author>
      <author><first>Weiguang</first><last>Qu</last></author>
      <pages>5938–5945</pages>
      <abstract>Abstract Meaning Representation is a sentence-level meaning representation, which abstracts the meaning of sentences into a rooted acyclic directed graph. With the continuous expansion of Chinese AMR corpus, more and more scholars have developed parsing systems to automatically parse sentences into Chinese AMR. However, the current parsers can’t deal with concept alignment and relation alignment, let alone the evaluation methods for AMR parsing. Therefore, to make up for the vacancy of Chinese AMR parsing evaluation methods, based on AMR evaluation metric smatch, we have improved the algorithm of generating triples so that to make it compatible with concept alignment and relation alignment. Finally, we obtain a new integrity metric align-smatch for paring evaluation. A comparative research then was conducted on 20 manually annotated AMR and gold AMR, with the result that align-smatch works well in alignments and more robust in evaluating arcs. We also put forward some fine-grained metric for evaluating concept alignment, relation alignment and implicit concepts, in order to further measure parsers’ performance in subtasks.</abstract>
      <url hash="186db9f8">2022.lrec-1.638</url>
      <bibkey>xiao-etal-2022-align</bibkey>
    </paper>
    <paper id="639">
      <title>Dynamic Human Evaluation for Relative Model Comparisons</title>
      <author><first>Thórhildur</first><last>Thorleiksdóttir</last></author>
      <author><first>Cedric</first><last>Renggli</last></author>
      <author><first>Nora</first><last>Hollenstein</last></author>
      <author><first>Ce</first><last>Zhang</last></author>
      <pages>5946–5955</pages>
      <abstract>Collecting human judgements is currently the most reliable evaluation method for natural language generation systems. Automatic metrics have reported flaws when applied to measure quality aspects of generated text and have been shown to correlate poorly with human judgements. However, human evaluation is time and cost-intensive, and we lack consensus on designing and conducting human evaluation experiments. Thus there is a need for streamlined approaches for efficient collection of human judgements when evaluating natural language generation systems. Therefore, we present a dynamic approach to measure the required number of human annotations when evaluating generated outputs in relative comparison settings. We propose an agent-based framework of human evaluation to assess multiple labelling strategies and methods to decide the better model in a simulation and a crowdsourcing case study. The main results indicate that a decision about the superior model can be made with high probability across different labelling strategies, where assigning a single random worker per task requires the least overall labelling effort and thus the least cost.</abstract>
      <url hash="07492e14">2022.lrec-1.639</url>
      <bibkey>thorleiksdottir-etal-2022-dynamic</bibkey>
      <pwccode url="https://github.com/thorhildurt/dynamic-human-evaluation" additional="false">thorhildurt/dynamic-human-evaluation</pwccode>
    </paper>
    <paper id="640">
      <title>Please, Don’t Forget the Difference and the Confidence Interval when Seeking for the State-of-the-Art Status</title>
      <author><first>Yves</first><last>Bestgen</last></author>
      <pages>5956–5962</pages>
      <abstract>This paper argues for the widest possible use of bootstrap confidence intervals for comparing NLP system performances instead of the state-of-the-art status (SOTA) and statistical significance testing. Their main benefits are to draw attention to the difference in performance between two systems and to help assessing the degree of superiority of one system over another. Two cases studies, one comparing several systems and the other based on a K-fold cross-validation procedure, illustrate these benefits.</abstract>
      <url hash="6cd3fdca">2022.lrec-1.640</url>
      <bibkey>bestgen-2022-please</bibkey>
      <pwccode url="https://github.com/ybestgen/bootcirealdata" additional="false">ybestgen/bootcirealdata</pwccode>
    </paper>
    <paper id="641">
      <title><fixed-case>PCR</fixed-case>4<fixed-case>ALL</fixed-case>: A Comprehensive Evaluation Benchmark for Pronoun Coreference Resolution in <fixed-case>E</fixed-case>nglish</title>
      <author><first>Xinran</first><last>Zhao</last></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Yangqiu</first><last>Song</last></author>
      <pages>5963–5973</pages>
      <abstract>Pronoun Coreference Resolution (PCR) is the task of resolving pronominal expressions to all mentions they refer to. The correct resolution of pronouns typically involves the complex inference over both linguistic knowledge and general world knowledge. Recently, with the help of pre-trained language representation models, the community has made significant progress on various PCR tasks. However, as most existing works focus on developing PCR models for specific datasets and measuring the accuracy or F1 alone, it is still unclear whether current PCR systems are reliable in real applications. Motivated by this, we propose PCR4ALL, a new benchmark and a toolbox that evaluates and analyzes the performance of PCR systems from different perspectives (i.e., knowledge source, domain, data size, frequency, relevance, and polarity). Experiments demonstrate notable performance differences when the models are examined from different angles. We hope that PCR4ALL can motivate the community to pay more attention to solving the overall PCR problem and understand the performance comprehensively. All data and codes are available at: https://github.com/HKUST-KnowComp/PCR4ALL.</abstract>
      <url hash="eb15f574">2022.lrec-1.641</url>
      <bibkey>zhao-etal-2022-pcr4all</bibkey>
      <pwccode url="https://github.com/hkust-knowcomp/pcr4all" additional="false">hkust-knowcomp/pcr4all</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2012-1">CoNLL-2012</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/definite-pronoun-resolution-dataset">Definite Pronoun Resolution Dataset</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wsc">WSC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/winogrande">WinoGrande</pwcdataset>
    </paper>
    <paper id="642">
      <title>Estimating Confidence of Predictions of Individual Classifiers and <fixed-case>T</fixed-case>heir<fixed-case>E</fixed-case>nsembles for the Genre Classification Task</title>
      <author><first>Mikhail</first><last>Lepekhin</last></author>
      <author><first>Serge</first><last>Sharoff</last></author>
      <pages>5974–5982</pages>
      <abstract>Genre identification is a kind of non-topic text classification. The main difference between this task and topic classification is that genre, unlike topic, usually cannot be expressed just by some keywords and is defined as a functional space. Neural models based on pre-trained transformers, such as BERT or XLM-RoBERTa, demonstrate SOTA results in many NLP tasks, including non-topical classification. However, in many cases, their downstream application to very large corpora, such as those extracted from social media, can lead to unreliable results because of dataset shifts, when some raw texts do not match the profile of the training set. To mitigate this problem, we experiment with individual models as well as with their ensembles. To evaluate the robustness of all models we use a prediction confidence metric, which estimates the reliability of a prediction in the absence of a gold standard label. We can evaluate robustness via the confidence gap between the correctly classified texts and the misclassified ones on a labeled test corpus, higher gaps make it easier to identify whether a text is classified correctly. Our results show that for all of the classifiers tested in this study, there is a confidence gap, but for the ensembles, the gap is wider, meaning that ensembles are more robust than their individual models.</abstract>
      <url hash="8d26fe9c">2022.lrec-1.642</url>
      <bibkey>lepekhin-sharoff-2022-estimating</bibkey>
    </paper>
    <paper id="643">
      <title>What do we really know about State of the Art <fixed-case>NER</fixed-case>?</title>
      <author><first>Sowmya</first><last>Vajjala</last></author>
      <author><first>Ramya</first><last>Balasubramaniam</last></author>
      <pages>5983–5993</pages>
      <abstract>Named Entity Recognition (NER) is a well researched NLP task and is widely used in real world NLP scenarios. NER research typically focuses on the creation of new ways of training NER, with relatively less emphasis on resources and evaluation. Further, state of the art (SOTA) NER models, trained on standard datasets, typically report only a single performance measure (F-score) and we don’t really know how well they do for different entity types and genres of text, or how robust are they to new, unseen entities. In this paper, we perform a broad evaluation of NER using a popular dataset, that takes into consideration various text genres and sources constituting the dataset at hand. Additionally, we generate six new adversarial test sets through small perturbations in the original test set, replacing select entities while retaining the context. We also train and test our models on randomly generated train/dev/test splits followed by an experiment where the models are trained on a select set of genres but tested genres not seen in training. These comprehensive evaluation strategies were performed using three SOTA NER models. Based on our results, we recommend some useful reporting practices for NER researchers, that could help in providing a better understanding of a SOTA model’s performance in future.</abstract>
      <url hash="f927b7e4">2022.lrec-1.643</url>
      <bibkey>vajjala-balasubramaniam-2022-really</bibkey>
    </paper>
    <paper id="644">
      <title><fixed-case>P</fixed-case>ro<fixed-case>QE</fixed-case>: Proficiency-wise Quality Estimation dataset for Grammatical Error Correction</title>
      <author><first>Yujin</first><last>Takahashi</last></author>
      <author><first>Masahiro</first><last>Kaneko</last></author>
      <author><first>Masato</first><last>Mita</last></author>
      <author><first>Mamoru</first><last>Komachi</last></author>
      <pages>5994–6000</pages>
      <abstract>This study investigates how supervised quality estimation (QE) models of grammatical error correction (GEC) are affected by the learners’ proficiency with the data. QE models for GEC evaluations in prior work have obtained a high correlation with manual evaluations. However, when functioning in a real-world context, the data used for the reported results have limitations because prior works were biased toward data by learners with relatively high proficiency levels. To address this issue, we created a QE dataset that includes multiple proficiency levels and explored the necessity of performing proficiency-wise evaluation for QE of GEC. Our experiments demonstrated that differences in evaluation dataset proficiency affect the performance of QE models, and proficiency-wise evaluation helps create more robust models.</abstract>
      <url hash="01b82662">2022.lrec-1.644</url>
      <bibkey>takahashi-etal-2022-proqe</bibkey>
    </paper>
    <paper id="645">
      <title>Evaluation of Off-the-shelf Speech Recognizers on Different Accents in a Dialogue Domain</title>
      <author><first>Divya</first><last>Tadimeti</last></author>
      <author><first>Kallirroi</first><last>Georgila</last></author>
      <author><first>David</first><last>Traum</last></author>
      <pages>6001–6008</pages>
      <abstract>We evaluate several publicly available off-the-shelf (commercial and research) automatic speech recognition (ASR) systems on dialogue agent-directed English speech from speakers with General American vs. non-American accents. Our results show that the performance of the ASR systems for non-American accents is considerably worse than for General American accents. Depending on the recognizer, the absolute difference in performance between General American accents and all non-American accents combined can vary approximately from 2% to 12%, with relative differences varying approximately between 16% and 49%. This drop in performance becomes even larger when we consider specific categories of non-American accents indicating a need for more diligent collection of and training on non-native English speaker data in order to narrow this performance gap. There are performance differences across ASR systems, and while the same general pattern holds, with more errors for non-American accents, there are some accents for which the best recognizer is different than in the overall case. We expect these results to be useful for dialogue system designers in developing more robust inclusive dialogue systems, and for ASR providers in taking into account performance requirements for different accents.</abstract>
      <url hash="0da7430c">2022.lrec-1.645</url>
      <bibkey>tadimeti-etal-2022-evaluation</bibkey>
    </paper>
    <paper id="646">
      <title>Sentence Pair Embeddings Based Evaluation Metric for Abstractive and Extractive Summarization</title>
      <author><first>Ramya</first><last>Akula</last></author>
      <author><first>Ivan</first><last>Garibay</last></author>
      <pages>6009–6017</pages>
      <abstract>The development of an automatic evaluation metric remains an open problem in text generation. Widely used evaluation metrics, like ROUGE and BLEU, are based on exact word matching and fail to capture semantic similarity. Recent works, such as BERTScore, MoverScore and, Sentence Mover’s Similarity, are an improvement over these standard metrics as they use the contextualized word or sentence embeddings to capture semantic similarity. We in this work, propose a novel evaluation metric, Sentence Pair EmbEDdings (SPEED) Score, for text generation which is based on semantic similarity between sentence pairs as opposed to earlier approaches. To find semantic similarity between a pair of sentences, we obtain sentence-level embeddings from multiple transformer models pre-trained specifically on various sentence pair tasks such as Paraphrase Detection (PD), Semantic Text Similarity (STS), and Natural Language Inference (NLI). As these sentence pair tasks involve capturing the semantic similarity between a pair of input texts, we leverage these models in our metric computation. Our proposed evaluation metric shows an impressive performance in evaluating both abstractive and extractive summarization models and achieves state-of-the-art results on the SummEval dataset, demonstrating the effectiveness of our approach. Also, we perform the run-time analysis to show that our proposed metric is faster than the current state-of-the-art.</abstract>
      <url hash="67c1a3d7">2022.lrec-1.646</url>
      <bibkey>akula-garibay-2022-sentence</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
    </paper>
    <paper id="647">
      <title>On “Human Parity” and “Super Human Performance” in Machine Translation Evaluation</title>
      <author><first>Thierry</first><last>Poibeau</last></author>
      <pages>6018–6023</pages>
      <abstract>In this paper, we reassess claims of human parity and super human performance in machine translation. Although these terms have already been discussed, as well as the evaluation protocols used to achieved these conclusions (human-parity is achieved i) only for a very reduced number of languages, ii) on very specific types of documents and iii) with very literal translations), we show that the terms used are themselves problematic, and that human translation involves much more than what is embedded in automatic systems. We also discuss ethical issues related to the way results are presented and advertised. Finally, we claim that a better assessment of human capacities should be put forward and that the goal of replacing humans by machines is not a desirable one.</abstract>
      <url hash="8b306dfa">2022.lrec-1.647</url>
      <bibkey>poibeau-2022-human</bibkey>
    </paper>
    <paper id="648">
      <title>Evaluation Benchmarks for <fixed-case>S</fixed-case>panish Sentence Representations</title>
      <author><first>Vladimir</first><last>Araujo</last></author>
      <author><first>Andrés</first><last>Carvallo</last></author>
      <author><first>Souvik</first><last>Kundu</last></author>
      <author><first>José</first><last>Cañete</last></author>
      <author><first>Marcelo</first><last>Mendoza</last></author>
      <author><first>Robert E.</first><last>Mercer</last></author>
      <author><first>Felipe</first><last>Bravo-Marquez</last></author>
      <author><first>Marie-Francine</first><last>Moens</last></author>
      <author><first>Alvaro</first><last>Soto</last></author>
      <pages>6024–6034</pages>
      <abstract>Due to the success of pre-trained language models, versions of languages other than English have been released in recent years. This fact implies the need for resources to evaluate these models. In the case of Spanish, there are few ways to systematically assess the models’ quality. In this paper, we narrow the gap by building two evaluation benchmarks. Inspired by previous work (Conneau and Kiela, 2018; Chen et al., 2019), we introduce Spanish SentEval and Spanish DiscoEval, aiming to assess the capabilities of stand-alone and discourse-aware sentence representations, respectively. Our benchmarks include considerable pre-existing and newly constructed datasets that address different tasks from various domains. In addition, we evaluate and analyze the most recent pre-trained Spanish language models to exhibit their capabilities and limitations. As an example, we discover that for the case of discourse evaluation tasks, mBERT, a language model trained on multiple languages, usually provides a richer latent representation than models trained only with documents in Spanish. We hope our contribution will motivate a fairer, more comparable, and less cumbersome way to evaluate future Spanish language models.</abstract>
      <url hash="ae6ebc33">2022.lrec-1.648</url>
      <bibkey>araujo-etal-2022-evaluation</bibkey>
      <pwccode url="https://github.com/opencenia/spanish-sentence-evaluation" additional="false">opencenia/spanish-sentence-evaluation</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mlsum">MLSUM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/senteval">SentEval</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/esxnli">esXNLI</pwcdataset>
    </paper>
    <paper id="649">
      <title><fixed-case>UMUT</fixed-case>ext<fixed-case>S</fixed-case>tats: A linguistic feature extraction tool for <fixed-case>S</fixed-case>panish</title>
      <author><first>José Antonio</first><last>García-Díaz</last></author>
      <author><first>Pedro José</first><last>Vivancos-Vicente</last></author>
      <author><first>Ángela</first><last>Almela</last></author>
      <author><first>Rafael</first><last>Valencia-García</last></author>
      <pages>6035–6044</pages>
      <abstract>Feature Engineering consists in the application of domain knowledge to select and transform relevant features to build efficient machine learning models. In the Natural Language Processing field, the state of the art concerning automatic document classification tasks relies on word and sentence embeddings built upon deep learning models based on transformers that have outperformed the competition in several tasks. However, the models built from these embeddings are usually difficult to interpret. On the contrary, linguistic features are easy to understand, they result in simpler models, and they usually achieve encouraging results. Moreover, both linguistic features and embeddings can be combined with different strategies which result in more reliable machine-learning models. The de facto tool for extracting linguistic features in Spanish is LIWC. However, this software does not consider specific linguistic phenomena of Spanish such as grammatical gender and lacks certain verb tenses. In order to solve these drawbacks, we have developed UMUTextStats, a linguistic extraction tool designed from scratch for Spanish. Furthermore, this tool has been validated to conduct different experiments in areas such as infodemiology, hate-speech detection, author profiling, authorship verification, humour or irony detection, among others. The results indicate that the combination of linguistic features and embeddings based on transformers are beneficial in automatic document classification.</abstract>
      <url hash="9d8dfb8e">2022.lrec-1.649</url>
      <bibkey>garcia-diaz-etal-2022-umutextstats</bibkey>
    </paper>
    <paper id="650">
      <title>Problem-solving Recognition in Scientific Text</title>
      <author><first>Kevin</first><last>Heffernan</last></author>
      <author><first>Simone</first><last>Teufel</last></author>
      <pages>6045–6058</pages>
      <abstract>As far back as Aristotle, problems and solutions have been recognised as a core pattern of thought, and in particular of the scientific method. In this work, we present the novel task of problem-solving recognition in scientific text. Previous work on problem-solving either is not computational, is not adapted to scientific text, or has been narrow in scope. This work provides a new annotation scheme of problem-solving tailored to the scientific domain. We validate the scheme with an annotation study, and model the task using state-of-the-art baselines such as a Neural Relational Topic Model. The agreement study indicates that our annotation is reliable, and results from modelling show that problem-solving expressions in text can be recognised to a high degree of accuracy.</abstract>
      <url hash="6d3b3af2">2022.lrec-1.650</url>
      <bibkey>heffernan-teufel-2022-problem</bibkey>
    </paper>
    <paper id="651">
      <title><fixed-case>HRCA</fixed-case>+: Advanced Multiple-choice Machine Reading Comprehension Method</title>
      <author><first>Yuxiang</first><last>Zhang</last></author>
      <author><first>Hayato</first><last>Yamana</last></author>
      <pages>6059–6068</pages>
      <abstract>Multiple-choice question answering (MCQA) for machine reading comprehension (MRC) is challenging. It requires a model to select a correct answer from several candidate options related to text passages or dialogue. To select the correct answer, such models must have the ability to understand natural languages, comprehend textual representations, and infer the relationship between candidate options, questions, and passages. Previous models calculated representations between passages and question-option pairs separately, thereby ignoring the effect of other relation-pairs. In this study, we propose a human reading comprehension attention (HRCA) model and a passage-question-option (PQO) matrix-guided HRCA model called HRCA+ to increase accuracy. The HRCA model updates the information learned from the previous relation-pair to the next relation-pair. HRCA+ utilizes the textual information and the interior relationship between every two parts in a passage, a question, and the corresponding candidate options. Our proposed method outperforms other state-of-the-art methods. On the Semeval-2018 Task 11 dataset, our proposed method improved accuracy levels from 95.8% to 97.2%, and on the DREAM dataset, it improved accuracy levels from 90.4% to 91.6% without extra training data, from 91.8% to 92.6% with extra training data.</abstract>
      <url hash="48fad91d">2022.lrec-1.651</url>
      <bibkey>zhang-yamana-2022-hrca</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/dream">DREAM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mctest">MCTest</pwcdataset>
    </paper>
    <paper id="652">
      <title><fixed-case>H</fixed-case>yper<fixed-case>B</fixed-case>ox: A Supervised Approach for Hypernym Discovery using Box Embeddings</title>
      <author><first>Maulik</first><last>Parmar</last></author>
      <author><first>Apurva</first><last>Narayan</last></author>
      <pages>6069–6076</pages>
      <abstract>Hypernymy plays a fundamental role in many AI tasks like taxonomy learning, ontology learning, etc. This has motivated the development of many automatic identification methods for extracting this relation, most of which rely on word distribution. We present a novel model HyperBox to learn box embeddings for hypernym discovery. Given an input term, HyperBox retrieves its suitable hypernym from a target corpus. For this task, we use the dataset published for SemEval 2018 Shared Task on Hypernym Discovery. We compare the performance of our model on two specific domains of knowledge: medical and music. Experimentally, we show that our model outperforms existing methods on the majority of the evaluation metrics. Moreover, our model generalize well over unseen hypernymy pairs using only a small set of training data.</abstract>
      <url hash="6c2676f8">2022.lrec-1.652</url>
      <bibkey>parmar-narayan-2022-hyperbox</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/semeval-2018-task-9-hypernym-discovery">SemEval-2018 Task 9: Hypernym Discovery</pwcdataset>
    </paper>
    <paper id="653">
      <title>Extracting Space Situational Awareness Events from News Text</title>
      <author><first>Zhengnan</first><last>Xie</last></author>
      <author><first>Alice Saebom</first><last>Kwak</last></author>
      <author><first>Enfa</first><last>George</last></author>
      <author><first>Laura W.</first><last>Dozal</last></author>
      <author><first>Hoang</first><last>Van</last></author>
      <author><first>Moriba</first><last>Jah</last></author>
      <author><first>Roberto</first><last>Furfaro</last></author>
      <author><first>Peter</first><last>Jansen</last></author>
      <pages>6077–6082</pages>
      <abstract>Space situational awareness typically makes use of physical measurements from radar, telescopes, and other assets to monitor satellites and other spacecraft for operational, navigational, and defense purposes. In this work we explore using textual input for the space situational awareness task. We construct a corpus of 48.5k news articles spanning all known active satellites between 2009 and 2020. Using a dependency-rule-based extraction system designed to target three high-impact events – spacecraft launches, failures, and decommissionings, we identify 1,787 space-event sentences that are then annotated by humans with 15.9k labels for event slots. We empirically demonstrate a state-of-the-art neural extraction system achieves an overall F1 between 53 and 91 per slot for event extraction in this low-resource, high-impact domain.</abstract>
      <url hash="e3006f8f">2022.lrec-1.653</url>
      <bibkey>xie-etal-2022-extracting</bibkey>
      <pwccode url="https://github.com/cognitiveailab/ssa-corpus" additional="false">cognitiveailab/ssa-corpus</pwccode>
    </paper>
    <paper id="654">
      <title><fixed-case>P</fixed-case>er<fixed-case>CQA</fixed-case>: <fixed-case>P</fixed-case>ersian Community Question Answering Dataset</title>
      <author><first>Naghme</first><last>Jamali</last></author>
      <author><first>Yadollah</first><last>Yaghoobzadeh</last></author>
      <author><first>Heshaam</first><last>Faili</last></author>
      <pages>6083–6092</pages>
      <abstract>Community Question Answering (CQA) forums provide answers to many real-life questions. These forums are trendy among machine learning researchers due to their large size. Automatic answer selection, answer ranking, question retrieval, expert finding, and fact-checking are example learning tasks performed using CQA data. This paper presents PerCQA, the first Persian dataset for CQA. This dataset contains the questions and answers crawled from the most well-known Persian forum. After data acquisition, we provide rigorous annotation guidelines in an iterative process and then the annotation of question-answer pairs in SemEvalCQA format. PerCQA contains 989 questions and 21,915 annotated answers. We make PerCQA publicly available to encourage more research in Persian CQA. We also build strong benchmarks for the task of answer selection in PerCQA by using mono- and multi-lingual pre-trained language models.</abstract>
      <url hash="3843e42e">2022.lrec-1.654</url>
      <bibkey>jamali-etal-2022-percqa</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/percqa">PerCQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/insuranceqa">InsuranceQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/trecqa">TrecQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikiqa">WikiQA</pwcdataset>
    </paper>
    <paper id="655">
      <title><fixed-case>G</fixed-case>r<fixed-case>ASP</fixed-case>: A Library for Extracting and Exploring Human-Interpretable Textual Patterns</title>
      <author><first>Piyawat</first><last>Lertvittayakumjorn</last></author>
      <author><first>Leshem</first><last>Choshen</last></author>
      <author><first>Eyal</first><last>Shnarch</last></author>
      <author><first>Francesca</first><last>Toni</last></author>
      <pages>6093–6103</pages>
      <abstract>Data exploration is an important step of every data science and machine learning project, including those involving textual data. We provide a novel language tool, in the form of a publicly available Python library for extracting patterns from textual data. The library integrates a first public implementation of the existing GrASP algorithm. It allows users to extract patterns using a number of general-purpose built-in linguistic attributes (such as hypernyms, part-of-speech tags, and syntactic dependency tags), as envisaged for the original algorithm, as well as domain-specific custom attributes which can be incorporated into the library by implementing two functions. The library is equipped with a web-based interface empowering human users to conveniently explore data via the extracted patterns, using complementary pattern-centric and example-centric views: the former includes a reading in natural language and statistics of each extracted pattern; the latter shows applications of each extracted pattern to training examples. We demonstrate the usefulness of the library in classification (spam detection and argument mining), model analysis (machine translation), and artifact discovery in datasets (SNLI and 20Newsgroups).</abstract>
      <url hash="0031c4f1">2022.lrec-1.655</url>
      <bibkey>lertvittayakumjorn-etal-2022-grasp</bibkey>
      <pwccode url="https://github.com/plkumjorn/GrASP" additional="false">plkumjorn/GrASP</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="656">
      <title>Recurrent Neural Networks with Mixed Hierarchical Structures and <fixed-case>EM</fixed-case> Algorithm for Natural Language Processing</title>
      <author><first>Zhaoxin</first><last>Luo</last></author>
      <author><first>Michael</first><last>Zhu</last></author>
      <pages>6104–6113</pages>
      <abstract>How to obtain hierarchical representations with an increasing level of abstraction becomes one of the key issues of learning with deep neural networks. A variety of RNN models have recently been proposed to incorporate both explicit and implicit hierarchical information in modeling languages in the literature. In this paper, we propose a novel approach called the latent indicator layer to identify and learn implicit hierarchical information (e.g., phrases), and further develop an EM algorithm to handle the latent indicator layer in training. The latent indicator layer further simplifies a text’s hierarchical structure, which allows us to seamlessly integrate different levels of attention mechanisms into the structure. We called the resulting architecture as the EM-HRNN model. Furthermore, we develop two bootstrap strategies to effectively and efficiently train the EM-HRNN model on long text documents. Simulation studies and real data applications demonstrate that the EM-HRNN model with bootstrap training outperforms other RNN-based models in document classification tasks. The performance of the EM-HRNN model is comparable to a Transformer-based method called Bert-base, though the former is much smaller model and does not require pre-training.</abstract>
      <url hash="01f548e1">2022.lrec-1.656</url>
      <bibkey>luo-zhu-2022-recurrent</bibkey>
    </paper>
    <paper id="657">
      <title><fixed-case>K</fixed-case>orean-Specific Dataset for Table Question Answering</title>
      <author><first>Changwook</first><last>Jun</last></author>
      <author><first>Jooyoung</first><last>Choi</last></author>
      <author><first>Myoseop</first><last>Sim</last></author>
      <author><first>Hyun</first><last>Kim</last></author>
      <author><first>Hansol</first><last>Jang</last></author>
      <author><first>Kyungkoo</first><last>Min</last></author>
      <pages>6114–6120</pages>
      <abstract>Existing question answering systems mainly focus on dealing with text data. However, much of the data produced daily is stored in the form of tables that can be found in documents and relational databases, or on the web. To solve the task of question answering over tables, there exist many datasets for table question answering written in English, but few Korean datasets. In this paper, we demonstrate how we construct Korean-specific datasets for table question answering: Korean tabular dataset is a collection of 1.4M tables with corresponding descriptions for unsupervised pre-training language models. Korean table question answering corpus consists of 70k pairs of questions and answers created by crowd-sourced workers. Subsequently, we then build a pre-trained language model based on Transformer and fine-tune the model for table question answering with these datasets. We then report the evaluation results of our model. We make our datasets publicly available via our GitHub repository and hope that those datasets will help further studies for question answering over tables, and for the transformation of table formats.</abstract>
      <url hash="34705116">2022.lrec-1.657</url>
      <bibkey>jun-etal-2022-korean</bibkey>
      <pwccode url="https://github.com/lg-nlp/korwikitablequestions" additional="false">lg-nlp/korwikitablequestions</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/korean-table-question-answering">Korean Table Question Answering</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitablequestions">WikiTableQuestions</pwcdataset>
    </paper>
    <paper id="658">
      <title><fixed-case>G</fixed-case>er<fixed-case>CCT</fixed-case>: An Annotated Corpus for Mining Arguments in <fixed-case>G</fixed-case>erman Tweets on Climate Change</title>
      <author><first>Robin</first><last>Schaefer</last></author>
      <author><first>Manfred</first><last>Stede</last></author>
      <pages>6121–6130</pages>
      <abstract>While the field of argument mining has grown notably in the last decade, research on the Twitter medium remains relatively understudied. Given the difficulty of mining arguments in tweets, recent work on creating annotated resources mainly utilized simplified annotation schemes that focus on single argument components, i.e., on claim or evidence. In this paper we strive to fill this research gap by presenting GerCCT, a new corpus of German tweets on climate change, which was annotated for a set of different argument components and properties. Additionally, we labelled sarcasm and toxic language to facilitate the development of tools for filtering out non-argumentative content. This, to the best of our knowledge, renders our corpus the first tweet resource annotated for argumentation, sarcasm and toxic language. We show that a comparatively complex annotation scheme can still yield promising inter-annotator agreement. We further present first good supervised classification results yielded by a fine-tuned BERT architecture.</abstract>
      <url hash="9b373b53">2022.lrec-1.658</url>
      <bibkey>schaefer-stede-2022-gercct</bibkey>
    </paper>
    <paper id="659">
      <title>Budget Argument Mining Dataset Using <fixed-case>J</fixed-case>apanese Minutes from the National Diet and Local Assemblies</title>
      <author><first>Yasutomo</first><last>Kimura</last></author>
      <author><first>Hokuto</first><last>Ototake</last></author>
      <author><first>Minoru</first><last>Sasaki</last></author>
      <pages>6131–6138</pages>
      <abstract>Budget argument mining attempts to identify argumentative components related to a budget item, and then classifies these argumentative components, given budget information and minutes. We describe the construction of the dataset for budget argument mining, a subtask of QA Lab-PoliInfo-3 in NTCIR-16. Budget argument mining analyses the argument structure of the minutes, focusing on monetary expressions (amount of money). In this task, given sufficient budget information (budget item, budget amount, etc.), relevant argumentative components in the minutes are identified and argument labels (claim, premise, and other) are assigned their components. In this paper, we describe the design of the data format, the annotation procedure, and release information of budget argument mining dataset, to link budget information to minutes.</abstract>
      <url hash="5ee03cf0">2022.lrec-1.659</url>
      <bibkey>kimura-etal-2022-budget</bibkey>
    </paper>
    <paper id="660">
      <title>Context-based Virtual Adversarial Training for Text Classification with Noisy Labels</title>
      <author><first>Do-Myoung</first><last>Lee</last></author>
      <author><first>Yeachan</first><last>Kim</last></author>
      <author><first>Chang gyun</first><last>Seo</last></author>
      <pages>6139–6146</pages>
      <abstract>Deep neural networks (DNNs) have a high capacity to completely memorize noisy labels given sufficient training time, and its memorization unfortunately leads to performance degradation. Recently, virtual adversarial training (VAT) attracts attention as it could further improve the generalization of DNNs in semi-supervised learning. The driving force behind VAT is to prevent the models from overffiting to data points by enforcing consistency between the inputs and the perturbed inputs. These strategy could be helpful in learning from noisy labels if it prevents neural models from learning noisy samples while encouraging the models to generalize clean samples. In this paper, we propose context-based virtual adversarial training (ConVAT) to prevent a text classifier from overfitting to noisy labels. Unlike the previous works, the proposed method performs the adversarial training in the context level rather than the inputs. It makes the classifier not only learn its label but also its contextual neighbors, which alleviate the learning from noisy labels by preserving contextual semantics on each data point. We conduct extensive experiments on four text classification datasets with two types of label noises. Comprehensive experimental results clearly show that the proposed method works quite well even with extremely noisy settings.</abstract>
      <url hash="70e8b176">2022.lrec-1.660</url>
      <bibkey>lee-etal-2022-context</bibkey>
    </paper>
    <paper id="661">
      <title><fixed-case>F</fixed-case>in<fixed-case>M</fixed-case>ath: Injecting a Tree-structured Solver for Question Answering over Financial Reports</title>
      <author><first>Chenying</first><last>Li</last></author>
      <author><first>Wenbo</first><last>Ye</last></author>
      <author><first>Yilun</first><last>Zhao</last></author>
      <pages>6147–6152</pages>
      <abstract>Answering questions over financial reports containing both tabular and textual data (hybrid data) is challenging as it requires models to select information from financial reports and perform complex quantitative analyses. Although current models have demonstrated a solid capability to solve simple questions, they struggle with complex questions that require a multiple-step numerical reasoning process. This paper proposes a new framework named FinMath, which improves the model’s numerical reasoning capacity by injecting a tree-structured neural model to perform multi-step numerical reasoning. Specifically, FinMath extracts supporting evidence from the financial reports given the question in the first phase. In the second phase, a tree-structured neural model is applied to generate a tree expression in a top-down recursive way. Experiments on the TAT-QA dataset show that our proposed approach improves the previous best result by 8.5% absolute for Exact Match (EM) score (50.1% to 58.6%) and 6.1% absolute for numeracy-focused F1 score (58.0% to 64.1%).</abstract>
      <url hash="0f6f7a95">2022.lrec-1.661</url>
      <bibkey>li-etal-2022-finmath</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/math">MATH</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tat-qa">TAT-QA</pwcdataset>
    </paper>
    <paper id="662">
      <title><fixed-case>H</fixed-case>eadline<fixed-case>C</fixed-case>ause: A Dataset of News Headlines for Detecting Causalities</title>
      <author><first>Ilya</first><last>Gusev</last></author>
      <author><first>Alexey</first><last>Tikhonov</last></author>
      <pages>6153–6161</pages>
      <abstract>Detecting implicit causal relations in texts is a task that requires both common sense and world knowledge. Existing datasets are focused either on commonsense causal reasoning or explicit causal relations. In this work, we present HeadlineCause, a dataset for detecting implicit causal relations between pairs of news headlines. The dataset includes over 5000 headline pairs from English news and over 9000 headline pairs from Russian news labeled through crowdsourcing. The pairs vary from totally unrelated or belonging to the same general topic to the ones including causation and refutation relations. We also present a set of models and experiments that demonstrates the dataset validity, including a multilingual XLM-RoBERTa based model for causality detection and a GPT-2 based model for possible effects prediction.</abstract>
      <url hash="cc4765d4">2022.lrec-1.662</url>
      <bibkey>gusev-tikhonov-2022-headlinecause</bibkey>
      <pwccode url="https://github.com/ilyagusev/headlinecause" additional="false">ilyagusev/headlinecause</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/headlinecause">HeadlineCause</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/copa">COPA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="663">
      <title>Incorporating Zoning Information into Argument Mining from Biomedical Literature</title>
      <author><first>Boyang</first><last>Liu</last></author>
      <author><first>Viktor</first><last>Schlegel</last></author>
      <author><first>Riza</first><last>Batista-Navarro</last></author>
      <author><first>Sophia</first><last>Ananiadou</last></author>
      <pages>6162–6169</pages>
      <abstract>The goal of text zoning is to segment a text into zones (i.e., Background, Conclusion) that serve distinct functions. Argumentative zoning, a specific text zoning scheme for the scientific domain, is considered as the antecedent for argument mining by many researchers. Surprisingly, however, little work is concerned with exploiting zoning information to improve the performance of argument mining models, despite the relatedness of the two tasks. In this paper, we propose two transformer-based models to incorporate zoning information into argumentative component identification and classification tasks. One model is for the sentence-level argument mining task and the other is for the token-level task. In particular, we add the zoning labels predicted by an off-the-shelf model to the beginning of each sentence, inspired by the convention commonly used biomedical abstracts. Moreover, we employ multi-head attention to transfer the sentence-level zoning information to each token in a sentence. Based on experiment results, we find a significant improvement in F1-scores for both sentence- and token-level tasks. It is worth mentioning that these zoning labels can be obtained with high accuracy by utilising readily available automated methods. Thus, existing argument mining models can be improved by incorporating zoning information without any additional annotation cost.</abstract>
      <url hash="f65b6377">2022.lrec-1.663</url>
      <bibkey>liu-etal-2022-incorporating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/pubmed-rct">PubMed RCT</pwcdataset>
    </paper>
    <paper id="664">
      <title><fixed-case>MAKED</fixed-case>: Multi-lingual Automatic Keyword Extraction Dataset</title>
      <author><first>Yash</first><last>Verma</last></author>
      <author><first>Anubhav</first><last>Jangra</last></author>
      <author><first>Sriparna</first><last>Saha</last></author>
      <author><first>Adam</first><last>Jatowt</last></author>
      <author><first>Dwaipayan</first><last>Roy</last></author>
      <pages>6170–6179</pages>
      <abstract>Keyword extraction is an integral task for many downstream problems like clustering, recommendation, search and classification. Development and evaluation of keyword extraction techniques require an exhaustive dataset; however, currently, the community lacks large-scale multi-lingual datasets. In this paper, we present MAKED, a large-scale multi-lingual keyword extraction dataset comprising of 540K+ news articles from British Broadcasting Corporation News (BBC News) spanning 20 languages. It is the first keyword extraction dataset for 11 of these 20 languages. The quality of the dataset is examined by experimentation with several baselines. We believe that the proposed dataset will help advance the field of automatic keyword extraction given its size, diversity in terms of languages used, topics covered and time periods as well as its focus on under-studied languages.</abstract>
      <url hash="1f84527a">2022.lrec-1.664</url>
      <bibkey>verma-etal-2022-maked</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/xl-sum">XL-Sum</pwcdataset>
    </paper>
    <paper id="665">
      <title>From Examples to Rules: Neural Guided Rule Synthesis for Information Extraction</title>
      <author><first>Robert</first><last>Vacareanu</last></author>
      <author><first>Marco A.</first><last>Valenzuela-Escárcega</last></author>
      <author><first>George Caique</first><last>Gouveia Barbosa</last></author>
      <author><first>Rebecca</first><last>Sharp</last></author>
      <author><first>Gustave</first><last>Hahn-Powell</last></author>
      <author><first>Mihai</first><last>Surdeanu</last></author>
      <pages>6180–6189</pages>
      <abstract>While deep learning approaches to information extraction have had many successes, they can be difficult to augment or maintain as needs shift. Rule-based methods, on the other hand, can be more easily modified. However, crafting rules requires expertise in linguistics and the domain of interest, making it infeasible for most users. Here we attempt to combine the advantages of these two directions while mitigating their drawbacks. We adapt recent advances from the adjacent field of program synthesis to information extraction, synthesizing rules from provided examples. We use a transformer-based architecture to guide an enumerative search, and show that this reduces the number of steps that need to be explored before a rule is found. Further, we show that without training the synthesis algorithm on the specific domain, our synthesized rules achieve state-of-the-art performance on the 1-shot scenario of a task that focuses on few-shot learning for relation classification, and competitive performance in the 5-shot scenario.</abstract>
      <url hash="9540b841">2022.lrec-1.665</url>
      <bibkey>vacareanu-etal-2022-examples</bibkey>
      <pwccode url="https://github.com/clulab/releases" additional="false">clulab/releases</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/tacred">TACRED</pwcdataset>
    </paper>
    <paper id="666">
      <title>Enhancing Relation Extraction via Adversarial Multi-task Learning</title>
      <author><first>Han</first><last>Qin</last></author>
      <author><first>Yuanhe</first><last>Tian</last></author>
      <author><first>Yan</first><last>Song</last></author>
      <pages>6190–6199</pages>
      <abstract>Relation extraction (RE) is a sub-field of information extraction, which aims to extract the relation between two given named entities (NEs) in a sentence and thus requires a good understanding of contextual information, especially the entities and their surrounding texts. However, limited attention is paid by most existing studies to re-modeling the given NEs and thus lead to inferior RE results when NEs are sometimes ambiguous. In this paper, we propose a RE model with two training stages, where adversarial multi-task learning is applied to the first training stage to explicitly recover the given NEs so as to enhance the main relation extractor, which is trained alone in the second stage. In doing so, the RE model is optimized by named entity recognition (NER) and thus obtains a detailed understanding of entity-aware context. We further propose the adversarial mechanism to enhance the process, which controls the effect of NER on the main relation extractor and allows the extractor to benefit from NER while keep focusing on RE rather than the entire multi-task learning. Experimental results on two English benchmark datasets for RE demonstrate the effectiveness of our approach, where state-of-the-art performance is observed on both datasets.</abstract>
      <url hash="019cdb42">2022.lrec-1.666</url>
      <bibkey>qin-etal-2022-enhancing</bibkey>
      <pwccode url="https://github.com/synlp/re-amt" additional="false">synlp/re-amt</pwccode>
    </paper>
    <paper id="667">
      <title>Query Obfuscation by Semantic Decomposition</title>
      <author><first>Danushka</first><last>Bollegala</last></author>
      <author><first>Tomoya</first><last>Machide</last></author>
      <author><first>Ken-ichi</first><last>Kawarabayashi</last></author>
      <pages>6200–6211</pages>
      <abstract>We propose a method to protect the privacy of search engine users by decomposing the queries using semantically <i>related</i> and unrelated <i>distractor</i> terms. Instead of a single query, the search engine receives multiple decomposed query terms. Next, we reconstruct the search results relevant to the original query term by aggregating the search results retrieved for the decomposed query terms. We show that the word embeddings learnt using a distributed representation learning method can be used to find semantically related and distractor query terms. We derive the relationship between the <i>obfuscity</i> achieved through the proposed query anonymisation method and the <i>reconstructability</i> of the original search results using the decomposed queries. We analytically study the risk of discovering the search engine users’ information intents under the proposed query obfuscation method, and empirically evaluate its robustness against clustering-based attacks. Our experimental results show that the proposed method can accurately reconstruct the search results for user queries, without compromising the privacy of the search engine users.</abstract>
      <url hash="51580d7d">2022.lrec-1.667</url>
      <bibkey>bollegala-etal-2022-query</bibkey>
    </paper>
    <paper id="668">
      <title><fixed-case>TWEET</fixed-case>-<fixed-case>FID</fixed-case>: An Annotated Dataset for Multiple Foodborne Illness Detection Tasks</title>
      <author><first>Ruofan</first><last>Hu</last></author>
      <author><first>Dongyu</first><last>Zhang</last></author>
      <author><first>Dandan</first><last>Tao</last></author>
      <author><first>Thomas</first><last>Hartvigsen</last></author>
      <author><first>Hao</first><last>Feng</last></author>
      <author><first>Elke</first><last>Rundensteiner</last></author>
      <pages>6212–6222</pages>
      <abstract>Foodborne illness is a serious but preventable public health problem – with delays in detecting the associated outbreaks resulting in productivity loss, expensive recalls, public safety hazards, and even loss of life. While social media is a promising source for identifying unreported foodborne illnesses, there is a dearth of labeled datasets for developing effective outbreak detection models. To accelerate the development of machine learning-based models for foodborne outbreak detection, we thus present TWEET-FID (TWEET-Foodborne Illness Detection), the first publicly available annotated dataset for multiple foodborne illness incident detection tasks. TWEET-FID collected from Twitter is annotated with three facets: tweet class, entity type, and slot type, with labels produced by experts as well as by crowdsource workers. We introduce several domain tasks leveraging these three facets: text relevance classification (TRC), entity mention detection (EMD), and slot filling (SF). We describe the end-to-end methodology for dataset design, creation, and labeling for supporting model development for these tasks. A comprehensive set of results for these tasks leveraging state-of-the-art single-and multi-task deep learning methods on the TWEET-FID dataset are provided. This dataset opens opportunities for future research in foodborne outbreak detection.</abstract>
      <url hash="4b3027c7">2022.lrec-1.668</url>
      <bibkey>hu-etal-2022-tweet</bibkey>
    </paper>
    <paper id="669">
      <title>Named Entity Recognition to Detect Criminal Texts on the Web</title>
      <author><first>Paweł</first><last>Skórzewski</last></author>
      <author><first>Mikołaj</first><last>Pieniowski</last></author>
      <author><first>Grazyna</first><last>Demenko</last></author>
      <pages>6223–6231</pages>
      <abstract>This paper presents a toolkit that applies named-entity extraction techniques to identify information related to criminal activity in texts from the Polish Internet. The methodological and technical assumptions were established following the requirements of our application users from the Border Guard. Due to the specificity of the users’ needs and the specificity of web texts, we used original methodologies related to the search for desired texts, the creation of domain lexicons, the annotation of the collected text resources, and the combination of rule-based and machine-learning techniques for extracting the information desired by the user. The performance of our tools has been evaluated on 6240 manually annotated text fragments collected from Internet sources. Evaluation results and user feedback show that our approach is feasible and has potential value for real-life applications in the daily work of border guards. Lexical lookup combined with hand-crafted rules and regular expressions, supported by text statistics, can make a decent specialized entity recognition system in the absence of large data sets required for training a good neural network.</abstract>
      <url hash="68e24c5b">2022.lrec-1.669</url>
      <bibkey>skorzewski-etal-2022-named</bibkey>
    </paper>
    <paper id="670">
      <title>Task-Driven and Experience-Based Question Answering Corpus for In-Home Robot Application in the <fixed-case>H</fixed-case>ouse3<fixed-case>D</fixed-case> Virtual Environment</title>
      <author><first>Zhuoqun</first><last>Xu</last></author>
      <author><first>Liubo</first><last>Ouyang</last></author>
      <author id="yang-liu-ss"><first>Yang</first><last>Liu</last></author>
      <pages>6232–6239</pages>
      <abstract>At present, more and more work has begun to pay attention to the long-term housekeeping robot scene. Naturally, we wonder whether the robot can answer the questions raised by the owner according to the actual situation at home. These questions usually do not have a clear text context, are directly related to the actual scene, and it is difficult to find the answer from the general knowledge base (such as Wikipedia). Therefore, the experience accumulated from the task seems to be a more natural choice. We present a corpus called TEQA (task-driven and experience-based question answering) in the long-term household task. Based on a popular in-house virtual environment (AI2-THOR) and agent task experiences of ALFRED, we design six types of questions along with answering including 24 question templates, 37 answer templates, and nearly 10k different question answering pairs. Our corpus aims at investigating the ability of task experience understanding of agents for the daily question answering scenario on the ALFRED dataset.</abstract>
      <url hash="85bf6ab3">2022.lrec-1.670</url>
      <bibkey>xu-etal-2022-task</bibkey>
      <pwccode url="https://github.com/nlply/EACL2023-QE-Features" additional="false">nlply/EACL2023-QE-Features</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ai2-thor">AI2-THOR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/alfred">ALFRED</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/eqa">EQA</pwcdataset>
    </paper>
    <paper id="671">
      <title><fixed-case>ELRC</fixed-case> Action: Covering Confidentiality, Correctness and Cross-linguality</title>
      <author><first>Tom</first><last>Vanallemeersch</last></author>
      <author><first>Arne</first><last>Defauw</last></author>
      <author><first>Sara</first><last>Szoc</last></author>
      <author><first>Alina</first><last>Kramchaninova</last></author>
      <author><first>Joachim</first><last>Van den Bogaert</last></author>
      <author><first>Andrea</first><last>Lösch</last></author>
      <pages>6240–6249</pages>
      <abstract>We describe the language technology (LT) assessments carried out in the ELRC action (European Language Resource Coordination) of the European Commission, which aims towards minimising language barriers across the EU. We zoom in on the two most extensive assessments. These LT specifications do not only involve experiments with tools and techniques but also an extensive consultation round with stakeholders from public organisations, academia and industry, in order to gather insights into scenarios and best practices. The LT specifications concern (1) the field of automated anonymisation, which is motivated by the need of public and other organisations to be able to store and share data, and (2) the field of multilingual fake news processing, which is motivated by the increasingly pressing problem of disinformation and the limited language coverage of systems for automatically detecting misleading articles. For each specification, we set up a corresponding proof-of-concept software to demonstrate the opportunities and challenges involved in the field.</abstract>
      <url hash="2fda4b99">2022.lrec-1.671</url>
      <bibkey>vanallemeersch-etal-2022-elrc</bibkey>
    </paper>
    <paper id="672">
      <title><fixed-case>R</fixed-case>ad<fixed-case>QA</fixed-case>: A Question Answering Dataset to Improve Comprehension of Radiology Reports</title>
      <author><first>Sarvesh</first><last>Soni</last></author>
      <author><first>Meghana</first><last>Gudala</last></author>
      <author><first>Atieh</first><last>Pajouhi</last></author>
      <author><first>Kirk</first><last>Roberts</last></author>
      <pages>6250–6259</pages>
      <abstract>We present a radiology question answering dataset, RadQA, with 3074 questions posed against radiology reports and annotated with their corresponding answer spans (resulting in a total of 6148 question-answer evidence pairs) by physicians. The questions are manually created using the clinical referral section of the reports that take into account the actual information needs of ordering physicians and eliminate bias from seeing the answer context (and, further, organically create unanswerable questions). The answer spans are marked within the Findings and Impressions sections of a report. The dataset aims to satisfy the complex clinical requirements by including complete (yet concise) answer phrases (which are not just entities) that can span multiple lines. We conduct a thorough analysis of the proposed dataset by examining the broad categories of disagreement in annotation (providing insights on the errors made by humans) and the reasoning requirements to answer a question (uncovering the huge dependence on medical knowledge for answering the questions). The advanced transformer language models achieve the best F1 score of 63.55 on the test set, however, the best human performance is 90.31 (with an average of 84.52). This demonstrates the challenging nature of RadQA that leaves ample scope for future method research.</abstract>
      <url hash="2dd0dac0">2022.lrec-1.672</url>
      <bibkey>soni-etal-2022-radqa</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/radqa">RadQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mimic-iii">MIMIC-III</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/emrqa">emrQA</pwcdataset>
    </paper>
    <paper id="673">
      <title>Knowledge Graph - Deep Learning: A Case Study in Question Answering in Aviation Safety Domain</title>
      <author><first>Ankush</first><last>Agarwal</last></author>
      <author><first>Raj</first><last>Gite</last></author>
      <author><first>Shreya</first><last>Laddha</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <author><first>Satyanarayan</first><last>Kar</last></author>
      <author><first>Asif</first><last>Ekbal</last></author>
      <author><first>Prabhjit</first><last>Thind</last></author>
      <author><first>Rajesh</first><last>Zele</last></author>
      <author><first>Ravi</first><last>Shankar</last></author>
      <pages>6260–6270</pages>
      <abstract>In the commercial aviation domain, there are a large number of documents, like accident reports of NTSB and ASRS, and regulatory directives ADs. There is a need for a system to efficiently access these diverse repositories to serve the demands of the aviation industry, such as maintenance, compliance, and safety. In this paper, we propose a Knowledge Graph (KG) guided Deep Learning (DL) based Question Answering (QA) system to cater to these requirements. We construct a KG from aircraft accident reports and contribute this resource to the community of researchers. The efficacy of this resource is tested and proved by the proposed QA system. Questions in Natural Language are converted into SPARQL (the interface language of the RDF graph database) queries and are answered from the KG. On the DL side, we examine two different QA models, BERT-QA and GPT3-QA, covering the two paradigms of answer formulation in QA. We evaluate our system on a set of handcrafted queries curated from the accident reports. Our hybrid KG + DL QA system, KGQA + BERT-QA, achieves 7% and 40.3% increase in accuracy over KGQA and BERT-QA systems respectively. Similarly, the other combined system, KGQA + GPT3-QA, achieves 29.3% and 9.3% increase in accuracy over KGQA and GPT3-QA systems respectively. Thus, we infer that the combination of KG and DL is better than either KG or DL individually for QA, at least in our chosen domain.</abstract>
      <url hash="e9ad56a3">2022.lrec-1.673</url>
      <bibkey>agarwal-etal-2022-knowledge</bibkey>
    </paper>
    <paper id="674">
      <title>A <fixed-case>B</fixed-case>ayesian Topic Model for Human-Evaluated Interpretability</title>
      <author><first>Justin</first><last>Wood</last></author>
      <author><first>Corey</first><last>Arnold</last></author>
      <author><first>Wei</first><last>Wang</last></author>
      <pages>6271–6279</pages>
      <abstract>One desiderata of topic modeling is to produce interpretable topics. Given a cluster of document-tokens comprising a topic, we can order the topic by counting each word. It is natural to think that each topic could easily be labeled by looking at the words with the highest word count. However, this is not always the case. A human evaluator can often have difficulty identifying a single label that accurately describes the topic as many top words seem unrelated. This paper aims to improve interpretability in topic modeling by providing a novel, outperforming interpretable topic model Our approach combines two previously established subdomains in topic modeling: nonparametric and weakly-supervised topic models. Given a nonparametric topic model, we can include weakly-supervised input using novel modifications to the nonparametric generative model. These modifications lay the groundwork for a compelling setting—one in which most corpora, without any previous supervised or weakly-supervised input, can discover interpretable topics. This setting also presents various challenging sub-problems of which we provide resolutions. Combining nonparametric topic models with weakly-supervised topic models leads to an exciting discovery—a complete, self-contained and outperforming topic model for interpretability.</abstract>
      <url hash="e5c39d2d">2022.lrec-1.674</url>
      <bibkey>wood-etal-2022-bayesian</bibkey>
    </paper>
    <paper id="675">
      <title>A Large Interlinked Knowledge Graph of the <fixed-case>I</fixed-case>talian Cultural Heritage</title>
      <author><first>Stefano</first><last>Faralli</last></author>
      <author><first>Andrea</first><last>Lenzi</last></author>
      <author><first>Paola</first><last>Velardi</last></author>
      <pages>6280–6289</pages>
      <abstract>Knowledge is the lifeblood for a plethora of applications such as search, recommender systems and natural language understanding. Thanks to the efforts in the fields of Semantic Web and Linked Open Data a growing number of interlinked knowledge bases are supporting the development of advanced knowledge-based applications. Unfortunately, for a large number of domain-specific applications, these knowledge bases are unavailable. In this paper, we present a resource consisting of a large knowledge graph linking the Italian cultural heritage entities (defined in the ArCo ontology) with the concepts defined on well-known knowledge bases (i.e., DBpedia and the Getty GVP ontology). We describe the methodologies adopted for the semi-automatic resource creation and provide an in-depth analysis of the resulting interlinked graph.</abstract>
      <url hash="2e074f87">2022.lrec-1.675</url>
      <bibkey>faralli-etal-2022-large</bibkey>
    </paper>
    <paper id="676">
      <title>Training on Lexical Resources</title>
      <author><first>Kenneth</first><last>Church</last></author>
      <author><first>Xingyu</first><last>Cai</last></author>
      <author><first>Yuchen</first><last>Bian</last></author>
      <pages>6290–6299</pages>
      <abstract>We propose using lexical resources (thesaurus, VAD) to fine-tune pretrained deep nets such as BERT and ERNIE. Then at inference time, these nets can be used to distinguish synonyms from antonyms, as well as VAD distances. The inference method can be applied to words as well as texts such as multiword expressions (MWEs), out of vocabulary words (OOVs), morphological variants and more. Code and data are posted on https://github.com/kwchurch/syn_ant.</abstract>
      <url hash="e95f9550">2022.lrec-1.676</url>
      <bibkey>church-etal-2022-training</bibkey>
      <pwccode url="https://github.com/kwchurch/syn_ant" additional="false">kwchurch/syn_ant</pwccode>
    </paper>
    <paper id="677">
      <title>Challenging the Assumption of Structure-based embeddings in Few- and Zero-shot Knowledge Graph Completion</title>
      <author><first>Filip</first><last>Cornell</last></author>
      <author><first>Chenda</first><last>Zhang</last></author>
      <author><first>Jussi</first><last>Karlgren</last></author>
      <author><first>Sarunas</first><last>Girdzijauskas</last></author>
      <pages>6300–6309</pages>
      <abstract>In this paper, we report experiments on Few- and Zero-shot Knowledge Graph completion, where the objective is to add missing relational links between entities into an existing Knowledge Graph with few or no previous examples of the relation in question. While previous work has used pre-trained embeddings based on the structure of the graph as input for a neural network, nobody has, to the best of our knowledge, addressed the task by only using textual descriptive data associated with the entities and relations, much since current standard benchmark data sets lack such information. We therefore enrich the benchmark data sets for these tasks by collecting textual description data to provide a new resource for future research to bridge the gap between structural and textual Knowledge Graph completion. Our results show that we can improve the results for Knowledge Graph completion for both Few- and Zero-shot scenarios with up to a two-fold increase of all metrics in the Zero-shot setting. From a more general perspective, our experiments demonstrate the value of using textual resources to enrich more formal representations of human knowledge and in the utility of transfer learning from textual data and text collections to enrich and maintain knowledge resources.</abstract>
      <url hash="052ec6b2">2022.lrec-1.677</url>
      <bibkey>cornell-etal-2022-challenging</bibkey>
      <pwccode url="https://github.com/filco306/challenging-structural-assumptions" additional="false">filco306/challenging-structural-assumptions</pwccode>
    </paper>
    <paper id="678">
      <title>Open Terminology Management and Sharing Toolkit for Federation of Terminology Databases</title>
      <author><first>Andis</first><last>Lagzdiņš</last></author>
      <author><first>Uldis</first><last>Siliņš</last></author>
      <author><first>Toms</first><last>Bergmanis</last></author>
      <author><first>Mārcis</first><last>Pinnis</last></author>
      <author><first>Artūrs</first><last>Vasiļevskis</last></author>
      <author><first>Andrejs</first><last>Vasiļjevs</last></author>
      <pages>6310–6316</pages>
      <abstract>Consolidated access to current and reliable terms from different subject fields and languages is necessary for content creators and translators. Terminology is also needed in AI applications such as machine translation, speech recognition, information extraction, and other natural language processing tools.In this work, we facilitate standards-based sharing and management of terminology resources by providing an open terminology management solution - the EuroTermBank Toolkit. It allows organisations to manage and search their terms, create term collections, and share them within and outside the organisation by participating in the network of federated databases. The data curated in the federated databases are automatically shared with EuroTermBank, the largest multilingual terminology resource in Europe, allowing translators and language service providers as well as researchers and students to access terminology resources in their most current version.</abstract>
      <url hash="632e91b0">2022.lrec-1.678</url>
      <bibkey>lagzdins-etal-2022-open</bibkey>
    </paper>
    <paper id="679">
      <title><fixed-case>RELATE</fixed-case>: Generating a linguistically inspired Knowledge Graph for fine-grained emotion classification</title>
      <author><first>Annika Marie</first><last>Schoene</last></author>
      <author><first>Nina</first><last>Dethlefs</last></author>
      <author><first>Sophia</first><last>Ananiadou</last></author>
      <pages>6317–6327</pages>
      <abstract>Several existing resources are available for sentiment analysis (SA) tasks that are used for learning sentiment specific embedding (SSE) representations. These resources are either large, common-sense knowledge graphs (KG) that cover a limited amount of polarities/emotions or they are smaller in size (e.g.: lexicons), which require costly human annotation and cover fine-grained emotions. Therefore using knowledge resources to learn SSE representations is either limited by the low coverage of polarities/emotions or the overall size of a resource. In this paper, we first introduce a new directed KG called ‘RELATE’, which is built to overcome both the issue of low coverage of emotions and the issue of scalability. RELATE is the first KG of its size to cover Ekman’s six basic emotions that are directed towards entities. It is based on linguistic rules to incorporate the benefit of semantics without relying on costly human annotation. The performance of ‘RELATE’ is evaluated by learning SSE representations using a Graph Convolutional Neural Network (GCN).</abstract>
      <url hash="c6ecd80c">2022.lrec-1.679</url>
      <bibkey>schoene-etal-2022-relate</bibkey>
    </paper>
    <paper id="680">
      <title>Language technology practitioners as language managers: arbitrating data bias and predictive bias in <fixed-case>ASR</fixed-case></title>
      <author><first>Nina</first><last>Markl</last></author>
      <author><first>Stephen Joseph</first><last>McNulty</last></author>
      <pages>6328–6339</pages>
      <abstract>Despite the fact that variation is a fundamental characteristic of natural language, automatic speech recognition systems perform systematically worse on non-standardised and marginalised language varieties. In this paper we use the lens of language policy to analyse how current practices in training and testing ASR systems in industry lead to the data bias giving rise to these systematic error differences. We believe that this is a useful perspective for speech and language technology practitioners to understand the origins and harms of algorithmic bias, and how they can mitigate it. We also propose a re-framing of language resources as (public) infrastructure which should not solely be designed for markets, but for, and with meaningful cooperation of, speech communities.</abstract>
      <url hash="98ebc085">2022.lrec-1.680</url>
      <bibkey>markl-mcnulty-2022-language</bibkey>
    </paper>
    <paper id="681">
      <title>Masader: Metadata Sourcing for <fixed-case>A</fixed-case>rabic Text and Speech Data Resources</title>
      <author><first>Zaid</first><last>Alyafeai</last></author>
      <author><first>Maraim</first><last>Masoud</last></author>
      <author><first>Mustafa</first><last>Ghaleb</last></author>
      <author><first>Maged S.</first><last>Al-shaibani</last></author>
      <pages>6340–6351</pages>
      <abstract>The NLP pipeline has evolved dramatically in the last few years. The first step in the pipeline is to find suitable annotated datasets to evaluate the tasks we are trying to solve. Unfortunately, most of the published datasets lack metadata annotations that describe their attributes. Not to mention, the absence of a public catalogue that indexes all the publicly available datasets related to specific regions or languages. When we consider low-resource dialectical languages, for example, this issue becomes more prominent. In this paper, we create Masader, the largest public catalogue for Arabic NLP datasets, which consists of 200 datasets annotated with 25 attributes. Furthermore, we develop a metadata annotation strategy that could be extended to other languages. We also make remarks and highlight some issues about the current status of Arabic NLP datasets and suggest recommendations to address them.</abstract>
      <url hash="f3348abc">2022.lrec-1.681</url>
      <bibkey>alyafeai-etal-2022-masader</bibkey>
    </paper>
    <paper id="682">
      <title>Linghub2: Language Resource Discovery Tool for Language Technologies</title>
      <author><first>Cécile</first><last>Robin</last></author>
      <author><first>Gautham Vadakkekara</first><last>Suresh</last></author>
      <author><first>Víctor</first><last>Rodriguez-Doncel</last></author>
      <author><first>John P.</first><last>McCrae</last></author>
      <author><first>Paul</first><last>Buitelaar</last></author>
      <pages>6352–6360</pages>
      <abstract>Language resources are a key component of natural language processing and related research and applications. Users of language resources have different needs in terms of format, language, topics, etc. for the data they need to use. Linghub (McCrae and Cimiano, 2015) was first developed for this purpose, using the capabilities of linked data to represent metadata, and tackling the heterogeneous metadata issue. Linghub aimed at helping language resources and technology users to easily find and retrieve relevant data, and identify important information on access, topics, etc. This work describes a rejuvenation and modernisation of the 2015 platform into using a popular open source data management system, DSpace, as foundation. The new platform, Linghub2, contains updated and extended resources, more languages offered, and continues the work towards homogenisation of metadata through conversions, through linkage to standardisation strategies and community groups, such as the Open Digital Rights Language (ODRL) community group.</abstract>
      <url hash="1fd698c1">2022.lrec-1.682</url>
      <bibkey>robin-etal-2022-linghub2</bibkey>
    </paper>
    <paper id="683">
      <title><fixed-case>C</fixed-case>x<fixed-case>LM</fixed-case>: A Construction and Context-aware Language Model</title>
      <author><first>Yu-Hsiang</first><last>Tseng</last></author>
      <author><first>Cing-Fang</first><last>Shih</last></author>
      <author><first>Pin-Er</first><last>Chen</last></author>
      <author><first>Hsin-Yu</first><last>Chou</last></author>
      <author><first>Mao-Chang</first><last>Ku</last></author>
      <author><first>Shu-Kai</first><last>Hsieh</last></author>
      <pages>6361–6369</pages>
      <abstract>Constructions are direct form-meaning pairs with possible schematic slots. These slots are simultaneously constrained by the embedded construction itself and the sentential context. We propose that the constraint could be described by a conditional probability distribution. However, as this conditional probability is inevitably complex, we utilize language models to capture this distribution. Therefore, we build CxLM, a deep learning-based masked language model explicitly tuned to constructions’ schematic slots. We first compile a construction dataset consisting of over ten thousand constructions in Taiwan Mandarin. Next, an experiment is conducted on the dataset to examine to what extent a pretrained masked language model is aware of the constructions. We then fine-tune the model specifically to perform a cloze task on the opening slots. We find that the fine-tuned model predicts masked slots more accurately than baselines and generates both structurally and semantically plausible word samples. Finally, we release CxLM and its dataset as publicly available resources and hope to serve as new quantitative tools in studying construction grammar.</abstract>
      <url hash="fd8dac99">2022.lrec-1.683</url>
      <bibkey>tseng-etal-2022-cxlm</bibkey>
    </paper>
    <paper id="684">
      <title>The Lexometer: A Shiny Application for Exploratory Analysis and Visualization of Corpus Data</title>
      <author><first>Oufan</first><last>Hai</last></author>
      <author><first>Matthew</first><last>Sundberg</last></author>
      <author><first>Katherine</first><last>Trice</last></author>
      <author><first>Rebecca</first><last>Friedman</last></author>
      <author><first>Scott</first><last>Grimm</last></author>
      <pages>6370–6376</pages>
      <abstract>Often performing even simple data science tasks with corpus data requires significant expertise in data science and programming languages like R and Python. With the aim of making quantitative research more accessible for researchers in the language sciences, we present the Lexometer, a Shiny application that integrates numerous data analysis and visualization functions into an easy-to-use graphical user interface. Some functions of the Lexometer are: filtering large databases to generate subsets of the data and variables of interest, providing a range of graphing techniques for both single and multiple variable analysis, and providing the data in a table format which can further be filtered as well as provide methods for cleaning the data. The Lexometer aims to be useful to language researchers with differing levels of programming expertise and to aid in broadening the inclusion of corpus-based empirical evidence in the language sciences.</abstract>
      <url hash="e5d71671">2022.lrec-1.684</url>
      <bibkey>hai-etal-2022-lexometer</bibkey>
    </paper>
    <paper id="685">
      <title><fixed-case>T</fixed-case>all<fixed-case>V</fixed-case>ocab<fixed-case>L</fixed-case>2<fixed-case>F</fixed-case>i: A Tall Dataset of 15 <fixed-case>F</fixed-case>innish <fixed-case>L</fixed-case>2 Learners’ Vocabulary</title>
      <author><first>Frankie</first><last>Robertson</last></author>
      <author><first>Li-Hsin</first><last>Chang</last></author>
      <author><first>Sini</first><last>Söyrinki</last></author>
      <pages>6377–6386</pages>
      <abstract>Previous work concerning measurement of second language learners has tended to focus on the knowledge of small numbers of words, often geared towards measuring vocabulary size. This paper presents a “tall” dataset containing information about a few learners’ knowledge of many words, suitable for evaluating Vocabulary Inventory Prediction (VIP) techniques, including those based on Computerised Adaptive Testing (CAT). In comparison to previous comparable datasets, the learners are from varied backgrounds, so as to reduce the risk of overfitting when used for machine learning based VIP. The dataset contains both a self-rating test and a translation test, used to derive a measure of reliability for learner responses. The dataset creation process is documented, and the relationship between variables concerning the participants, such as their completion time, their language ability level, and the triangulated reliability of their self-assessment responses, are analysed. The word list is constructed by taking into account the extensive derivation morphology of Finnish, and infrequent words are included in order to account for explanatory variables beyond word frequency.</abstract>
      <url hash="65dd1376">2022.lrec-1.685</url>
      <bibkey>robertson-etal-2022-tallvocabl2fi</bibkey>
    </paper>
    <paper id="686">
      <title><fixed-case>CAMS</fixed-case>: An Annotated Corpus for Causal Analysis of Mental Health Issues in Social Media Posts</title>
      <author><first>Muskan</first><last>Garg</last></author>
      <author><first>Chandni</first><last>Saxena</last></author>
      <author><first>Sriparna</first><last>Saha</last></author>
      <author><first>Veena</first><last>Krishnan</last></author>
      <author><first>Ruchi</first><last>Joshi</last></author>
      <author><first>Vijay</first><last>Mago</last></author>
      <pages>6387–6396</pages>
      <abstract>The social NLP researchers and mental health practitioners have witnessed exponential growth in the field of mental health detection and analysis on social media. It has become important to identify the reason behind mental illness. In this context, we introduce a new dataset for Causal Analysis of Mental health in Social media posts (CAMS). We first introduce the annotation schema for this task of causal analysis. The causal analysis comprises of two types of annotations, viz, causal interpretation and causal categorization. We show the efficacy of our scheme in two ways: (i) crawling and annotating 3155 Reddit data and (ii) re-annotate the publicly available SDCNL dataset of 1896 instances for interpretable causal analysis. We further combine them as CAMS dataset and make it available along with the other source codes https://anonymous.4open.science/r/CAMS1/. Our experimental results show that the hybrid CNN-LSTM model gives the best performance over CAMS dataset.</abstract>
      <url hash="be97ea56">2022.lrec-1.686</url>
      <bibkey>garg-etal-2022-cams</bibkey>
      <pwccode url="https://github.com/drmuskangarg/cams" additional="false">drmuskangarg/cams</pwccode>
    </paper>
    <paper id="687">
      <title>How Does the Experimental Setting Affect the Conclusions of Neural Encoding Models?</title>
      <author><first>Xiaohan</first><last>Zhang</last></author>
      <author><first>Shaonan</first><last>Wang</last></author>
      <author><first>Chengqing</first><last>Zong</last></author>
      <pages>6397–6404</pages>
      <abstract>Recent years have witnessed the tendency of neural encoding models on exploring brain language processing using naturalistic stimuli. Neural encoding models are data-driven methods that require an encoding model to investigate the mystery of brain mechanisms hidden in the data. As a data-driven method, the performance of encoding models is very sensitive to the experimental setting. However, it is unknown how the experimental setting further affects the conclusions of neural encoding models. This paper systematically investigated this problem and evaluated the influence of three experimental settings, i.e., the data size, the cross-validation training method, and the statistical testing method. Results demonstrate that inappropriate cross-validation training and small data size can substantially decrease the performance of encoding models, especially in the temporal lobe and the frontal lobe. And different null hypotheses in significance testing lead to highly different significant brain regions. Based on these results, we suggest a block-wise cross-validation training method and an adequate data size for increasing the performance of linear encoding models. We also propose two strict null hypotheses to control false positive discovery rates.</abstract>
      <url hash="8a5f401a">2022.lrec-1.687</url>
      <bibkey>zhang-etal-2022-experimental</bibkey>
    </paper>
    <paper id="688">
      <title><fixed-case>SPADE</fixed-case>: A Big Five-Mturk Dataset of Argumentative Speech Enriched with Socio-Demographics for Personality Detection</title>
      <author><first>Elma</first><last>Kerz</last></author>
      <author><first>Yu</first><last>Qiao</last></author>
      <author><first>Sourabh</first><last>Zanwar</last></author>
      <author><first>Daniel</first><last>Wiechmann</last></author>
      <pages>6405–6419</pages>
      <abstract>In recent years, there has been increasing interest in automatic personality detection based on language. Progress in this area is highly contingent upon the availability of datasets and benchmark corpora. However, publicly available datasets for modeling and predicting personality traits are still scarce. While recent efforts to create such datasets from social media (Twitter, Reddit) are to be applauded, they often do not include continuous and contextualized language use. In this paper, we introduce SPADE, the first dataset with continuous samples of argumentative speech labeled with the Big Five personality traits and enriched with socio-demographic data (age, gender, education level, language background). We provide benchmark models for this dataset to facilitate further research and conduct extensive experiments. Our models leverage 436 (psycho)linguistic features extracted from transcribed speech and speaker-level metainformation with transformers. We conduct feature ablation experiments to investigate which types of features contribute to the prediction of individual personality traits.</abstract>
      <url hash="a8558c3f">2022.lrec-1.688</url>
      <bibkey>kerz-etal-2022-spade</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/pandora">PANDORA</pwcdataset>
    </paper>
    <paper id="689">
      <title>Progress in Multilingual Speech Recognition for Low Resource Languages <fixed-case>K</fixed-case>urmanji <fixed-case>K</fixed-case>urdish, <fixed-case>C</fixed-case>ree and Inuktut</title>
      <author><first>Vishwa</first><last>Gupta</last></author>
      <author><first>Gilles</first><last>Boulianne</last></author>
      <pages>6420–6428</pages>
      <abstract>This contribution presents our efforts to develop the automatic speech recognition (ASR) systems for three low resource languages: Kurmanji Kurdish, Cree and Inuktut. As a first step, we generate multilingual models from acoustic training data from 12 different languages in the hybrid DNN/HMM framework. We explore different strategies for combining the phones from different languages: either keep the phone labels separate for each language or merge the common phones. For Kurmanji Kurdish and Inuktut, keeping the phones separate gives much lower word error rate (WER), while merging phones gives lower WER for Cree. These WER are lower than training the acoustic models separately for each language. We also compare two different DNN architectures: factored time delay neural network (TDNN-F), and bidirectional long short-term memory (BLSTM) acoustic models. The TDNN-F acoustic models give significantly lower WER for Kurmanji Kurdish and Cree, while BLSTM acoustic models give significantly lower WER for Inuktut. We also show that for each language, training multilingual acoustic models by one more epoch with acoustic data from that language reduces the WER significantly. We also added 512-dimensional embedding features from cross-lingual pre-trained wav2vec2.0 XLSR-53 models, but they lead to only a small reduction in WER.</abstract>
      <url hash="4a307ec4">2022.lrec-1.689</url>
      <bibkey>gupta-boulianne-2022-progress</bibkey>
    </paper>
    <paper id="690">
      <title>Efficient Entity Candidate Generation for Low-Resource Languages</title>
      <author><first>Alberto</first><last>Garcia-Duran</last></author>
      <author><first>Akhil</first><last>Arora</last></author>
      <author><first>Robert</first><last>West</last></author>
      <pages>6429–6438</pages>
      <abstract>Candidate generation is a crucial module in entity linking. It also plays a key role in multiple NLP tasks that have been proven to beneficially leverage knowledge bases. Nevertheless, it has often been overlooked in the monolingual English entity linking literature, as naïve approaches obtain very good performance. Unfortunately, the existing approaches for English cannot be successfully transferred to poorly resourced languages. This paper constitutes an in-depth analysis of the candidate generation problem in the context of cross-lingual entity linking with a focus on low-resource languages. Among other contributions, we point out limitations in the evaluation conducted in previous works. We introduce a characterization of queries into types based on their difficulty, which improves the interpretability of the performance of different methods. We also propose a light-weight and simple solution based on the construction of indexes whose design is motivated by more complex transfer learning based neural approaches. A thorough empirical analysis on 9 real-world datasets under 2 evaluation settings shows that our simple solution outperforms the state-of-the-art approach in terms of both quality and efficiency for almost all datasets and query types.</abstract>
      <url hash="2cf3aa25">2022.lrec-1.690</url>
      <bibkey>garcia-duran-etal-2022-efficient</bibkey>
      <pwccode url="https://github.com/epfl-dlab/pti-candgen" additional="false">epfl-dlab/pti-candgen</pwccode>
    </paper>
    <paper id="691">
      <title>What a Creole Wants, What a Creole Needs</title>
      <author><first>Heather</first><last>Lent</last></author>
      <author><first>Kelechi</first><last>Ogueji</last></author>
      <author><first>Miryam</first><last>de Lhoneux</last></author>
      <author><first>Orevaoghene</first><last>Ahia</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>6439–6449</pages>
      <abstract>In recent years, the natural language processing (NLP) community has given increased attention to the disparity of efforts directed towards high-resource languages over low-resource ones. Efforts to remedy this delta often begin with translations of existing English datasets into other languages. However, this approach ignores that different language communities have different needs. We consider a group of low-resource languages, creole languages. Creoles are both largely absent from the NLP literature, and also often ignored by society at large due to stigma, despite these languages having sizable and vibrant communities. We demonstrate, through conversations with creole experts and surveys of creole-speaking communities, how the things needed from language technology can change dramatically from one language to another, even when the languages are considered to be very similar to each other, as with creoles. We discuss the prominent themes arising from these conversations, and ultimately demonstrate that useful language technology cannot be built without involving the relevant community.</abstract>
      <url hash="e1e6ac5e">2022.lrec-1.691</url>
      <bibkey>lent-etal-2022-creole</bibkey>
    </paper>
    <paper id="692">
      <title>Extensions to <fixed-case>B</fixed-case>rahmic script processing within the <fixed-case>N</fixed-case>isaba library: new scripts, languages and utilities</title>
      <author><first>Alexander</first><last>Gutkin</last></author>
      <author><first>Cibu</first><last>Johny</last></author>
      <author><first>Raiomond</first><last>Doctor</last></author>
      <author><first>Lawrence</first><last>Wolf-Sonkin</last></author>
      <author><first>Brian</first><last>Roark</last></author>
      <pages>6450–6460</pages>
      <abstract>The Brahmic family of scripts is used to record some of the most spoken languages in the world and is arguably the most diverse family of writing systems. In this work, we present several substantial extensions to Brahmic script functionality within the open-source Nisaba library of finite-state script normalization and processing utilities (Johny et al., 2021). First, we extend coverage from the original ten scripts to an additional ten scripts of South Asia and beyond, including some used to record endangered languages such as Dogri. Second, we augment the language layer so that scripts used by multiple languages in distinct ways can be processed correctly for more languages, such as the Bengali script when used for the low-resource language Santali. We document key changes to the finite-state engine required to support these new languages and scripts. Finally, we add new script processing utilities, including lightweight script-level reading normalization that (unlike existing visual normalization) does not preserve visual invariance, and a fixed-input transliteration mechanism specifically tailored to Brahmic text entry with ASCII characters.</abstract>
      <url hash="d913560e">2022.lrec-1.692</url>
      <bibkey>gutkin-etal-2022-extensions</bibkey>
    </paper>
    <paper id="693">
      <title>Predicting Embedding Reliability in Low-Resource Settings Using Corpus Similarity Measures</title>
      <author><first>Jonathan</first><last>Dunn</last></author>
      <author><first>Haipeng</first><last>Li</last></author>
      <author><first>Damian</first><last>Sastre</last></author>
      <pages>6461–6470</pages>
      <abstract>This paper simulates a low-resource setting across 17 languages in order to evaluate embedding similarity, stability, and reliability under different conditions. The goal is to use corpus similarity measures before training to predict properties of embeddings after training. The main contribution of the paper is to show that it is possible to predict downstream embedding similarity using upstream corpus similarity measures. This finding is then applied to low-resource settings by modelling the reliability of embeddings created from very limited training data. Results show that it is possible to estimate the reliability of low-resource embeddings using corpus similarity measures that remain robust on small amounts of data. These findings have significant implications for the evaluation of truly low-resource languages in which such systematic downstream validation methods are not possible because of data limitations.</abstract>
      <url hash="cd36b7c6">2022.lrec-1.693</url>
      <bibkey>dunn-etal-2022-predicting</bibkey>
    </paper>
    <paper id="694">
      <title><fixed-case>H</fixed-case>ausa Visual Genome: A Dataset for Multi-Modal <fixed-case>E</fixed-case>nglish to <fixed-case>H</fixed-case>ausa Machine Translation</title>
      <author><first>Idris</first><last>Abdulmumin</last></author>
      <author><first>Satya Ranjan</first><last>Dash</last></author>
      <author><first>Musa Abdullahi</first><last>Dawud</last></author>
      <author><first>Shantipriya</first><last>Parida</last></author>
      <author><first>Shamsuddeen</first><last>Muhammad</last></author>
      <author><first>Ibrahim Sa’id</first><last>Ahmad</last></author>
      <author><first>Subhadarshi</first><last>Panda</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <author><first>Bashir Shehu</first><last>Galadanci</last></author>
      <author><first>Bello Shehu</first><last>Bello</last></author>
      <pages>6471–6479</pages>
      <abstract>Multi-modal Machine Translation (MMT) enables the use of visual information to enhance the quality of translations, especially where the full context is not available to enable the unambiguous translation in standard machine translation. Despite the increasing popularity of such technique, it lacks sufficient and qualitative datasets to maximize the full extent of its potential. Hausa, a Chadic language, is a member of the Afro-Asiatic language family. It is estimated that about 100 to 150 million people speak the language, with more than 80 million indigenous speakers. This is more than any of the other Chadic languages. Despite the large number of speakers, the Hausa language is considered as a low resource language in natural language processing (NLP). This is due to the absence of enough resources to implement most of the tasks in NLP. While some datasets exist, they are either scarce, machine-generated or in the religious domain. Therefore, there is the need to create training and evaluation data for implementing machine learning tasks and bridging the research gap in the language. This work presents the Hausa Visual Genome (HaVG), a dataset that contains the description of an image or a section within the image in Hausa and its equivalent in English. The dataset was prepared by automatically translating the English description of the images in the Hindi Visual Genome (HVG). The synthetic Hausa data was then carefully postedited, taking into cognizance the respective images. The data is made of 32,923 images and their descriptions that are divided into training, development, test, and challenge test set. The Hausa Visual Genome is the first dataset of its kind and can be used for Hausa-English machine translation, multi-modal research, image description, among various other natural language processing and generation tasks.</abstract>
      <url hash="2eee5b98">2022.lrec-1.694</url>
      <bibkey>abdulmumin-etal-2022-hausa</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/havg">HaVG</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-genome">Visual Genome</pwcdataset>
    </paper>
    <paper id="695">
      <title>A Survey of Machine Translation Tasks on <fixed-case>N</fixed-case>igerian Languages</title>
      <author><first>Ebelechukwu</first><last>Nwafor</last></author>
      <author><first>Anietie</first><last>Andy</last></author>
      <pages>6480–6486</pages>
      <abstract>Machine translation is an active area of research that has received a significant amount of attention over the past decade. With the advent of deep learning models, the translation of several languages has been performed with high accuracy and precision. In spite of the development in machine translation techniques, there is very limited work focused on translating low-resource African languages, particularly Nigerian languages. Nigeria is one of the most populous countries in Africa with diverse language and ethnic groups. In this paper, we survey the current state of the art of machine translation research on Nigerian languages with a major emphasis on neural machine translation techniques. We outline the limitations of research in machine translation on Nigerian languages and propose future directions in increasing research and participation.</abstract>
      <url hash="2a523046">2022.lrec-1.695</url>
      <bibkey>nwafor-andy-2022-survey</bibkey>
    </paper>
    <paper id="696">
      <title>Automatic Speech Recognition Datasets in <fixed-case>C</fixed-case>antonese: A Survey and New Dataset</title>
      <author><first>Tiezheng</first><last>Yu</last></author>
      <author><first>Rita</first><last>Frieske</last></author>
      <author><first>Peng</first><last>Xu</last></author>
      <author><first>Samuel</first><last>Cahyawijaya</last></author>
      <author><first>Cheuk Tung</first><last>Yiu</last></author>
      <author><first>Holy</first><last>Lovenia</last></author>
      <author><first>Wenliang</first><last>Dai</last></author>
      <author><first>Elham J.</first><last>Barezi</last></author>
      <author><first>Qifeng</first><last>Chen</last></author>
      <author><first>Xiaojuan</first><last>Ma</last></author>
      <author><first>Bertram</first><last>Shi</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <pages>6487–6494</pages>
      <abstract>Automatic speech recognition (ASR) on low resource languages improves the access of linguistic minorities to technological advantages provided by artificial intelligence (AI). In this paper, we address the problem of data scarcity for the Hong Kong Cantonese language by creating a new Cantonese dataset. Our dataset, Multi-Domain Cantonese Corpus (MDCC), consists of 73.6 hours of clean read speech paired with transcripts, collected from Cantonese audiobooks from Hong Kong. It comprises philosophy, politics, education, culture, lifestyle and family domains, covering a wide range of topics. We also review all existing Cantonese datasets and analyze them according to their speech type, data source, total size and availability. We further conduct experiments with Fairseq S2T Transformer, a state-of-the-art ASR model, on the biggest existing dataset, Common Voice zh-HK, and our proposed MDCC, and the results show the effectiveness of our dataset. In addition, we create a powerful and robust Cantonese ASR model by applying multi-dataset learning on MDCC and Common Voice zh-HK.</abstract>
      <url hash="0b2f5479">2022.lrec-1.696</url>
      <bibkey>yu-etal-2022-automatic</bibkey>
      <pwccode url="https://github.com/hltchkust/cantonese-asr" additional="false">hltchkust/cantonese-asr</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/common-voice">Common Voice</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/librispeech">LibriSpeech</pwcdataset>
    </paper>
    <paper id="697">
      <title>Survey on <fixed-case>T</fixed-case>hai <fixed-case>NLP</fixed-case> Language Resources and Tools</title>
      <author><first>Ratchakrit</first><last>Arreerard</last></author>
      <author><first>Stephen</first><last>Mander</last></author>
      <author><first>Scott</first><last>Piao</last></author>
      <pages>6495–6505</pages>
      <abstract>Over the past decades, Natural Language Processing (NLP) research has been expanding to cover more languages. Recently particularly, NLP community has paid increasing attention to under-resourced languages. However, there are still many languages for which NLP research is limited in terms of both language resources and software tools. Thai language is one of the under-resourced languages in the NLP domain, although it is spoken by nearly 70 million people globally. In this paper, we report on our survey on the past development of Thai NLP research to help understand its current state and future research directions. Our survey shows that, although Thai NLP community has achieved a significant achievement over the past three decades, particularly on NLP upstream tasks such as tokenisation, research on downstream tasks such as syntactic parsing and semantic analysis is still limited. But we foresee that Thai NLP research will advance rapidly as richer Thai language resources and more robust NLP techniques become available.</abstract>
      <url hash="ae5e04d1">2022.lrec-1.697</url>
      <bibkey>arreerard-etal-2022-survey</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/polyglot-ner">Polyglot-NER</pwcdataset>
    </paper>
    <paper id="698">
      <title><fixed-case>L</fixed-case>ao<fixed-case>PLM</fixed-case>: Pre-trained Language Models for <fixed-case>L</fixed-case>ao</title>
      <author><first>Nankai</first><last>Lin</last></author>
      <author><first>Yingwen</first><last>Fu</last></author>
      <author><first>Chuwei</first><last>Chen</last></author>
      <author><first>Ziyu</first><last>Yang</last></author>
      <author><first>Shengyi</first><last>Jiang</last></author>
      <pages>6506–6512</pages>
      <abstract>Trained on the large corpus, pre-trained language models (PLMs) can capture different levels of concepts in context and hence generate universal language representations. They can benefit from multiple downstream natural language processing (NLP) tasks. Although PTMs have been widely used in most NLP applications, especially for high-resource languages such as English, it is under-represented in Lao NLP research. Previous work on Lao has been hampered by the lack of annotated datasets and the sparsity of language resources. In this work, we construct a text classification dataset to alleviate the resource-scarce situation of the Lao language. In addition, we present the first transformer-based PTMs for Lao with four versions: BERT-Small , BERT-Base , ELECTRA-Small , and ELECTRA-Base . Furthermore, we evaluate them on two downstream tasks: part-of-speech (POS) tagging and text classification. Experiments demonstrate the effectiveness of our Lao models. We release our models and datasets to the community, hoping to facilitate the future development of Lao NLP applications.</abstract>
      <url hash="c157d28f">2022.lrec-1.698</url>
      <bibkey>lin-etal-2022-laoplm</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cc100">CC100</pwcdataset>
    </paper>
    <paper id="699">
      <title>The Maaloula <fixed-case>A</fixed-case>ramaic Speech Corpus (<fixed-case>MASC</fixed-case>): From Printed Material to a Lemmatized and Time-Aligned Corpus</title>
      <author><first>Ghattas</first><last>Eid</last></author>
      <author><first>Esther</first><last>Seyffarth</last></author>
      <author><first>Ingo</first><last>Plag</last></author>
      <pages>6513–6520</pages>
      <abstract>This paper presents the first electronic speech corpus of Maaloula Aramaic, an endangered Western Neo-Aramaic variety spoken in Syria. This 64,845-word corpus is available in four formats: (1) transcriptions, (2) lemmatized transcriptions, (3) audio files and time-aligned phonetic transcriptions, and (4) an SQLite database. The transcription files are a digitized and corrected version of authentic transcriptions of tape-recorded narratives coming from a fieldwork trip conducted in the 1980s and published in the early 1990s (Arnold, 1991a, 1991b). They contain no annotation, except for some informative tagging (e.g. to mark loanwords and misspoken words). In the lemmatized version of the files, each word form is followed by its lemma in angled brackets. The time-aligned TextGrid annotations consist of four tiers: the sentence level (Tier 1), the word level (Tiers 2 and 3), and the segment level (Tier 4). These TextGrid files are downloadable together with their audio files (for the original source of the audio data see Arnold, 2003). The SQLite database enables users to access the data on the level of tokens, types, lemmas, sentences, narratives, or speakers. The corpus is now available to the scientific community at https://doi.org/10.5281/zenodo.6496714.</abstract>
      <url hash="f1c6c249">2022.lrec-1.699</url>
      <bibkey>eid-etal-2022-maaloula</bibkey>
    </paper>
    <paper id="700">
      <title><fixed-case>VIMQA</fixed-case>: A <fixed-case>V</fixed-case>ietnamese Dataset for Advanced Reasoning and Explainable Multi-hop Question Answering</title>
      <author><first>Khang</first><last>Le</last></author>
      <author><first>Hien</first><last>Nguyen</last></author>
      <author><first>Tung</first><last>Le Thanh</last></author>
      <author><first>Minh</first><last>Nguyen</last></author>
      <pages>6521–6529</pages>
      <abstract>Vietnamese is the native language of over 98 million people in the world. However, existing Vietnamese Question Answering (QA) datasets do not explore the model’s ability to perform advanced reasoning and provide evidence to explain the answer. We introduce VIMQA, a new Vietnamese dataset with over 10,000 Wikipedia-based multi-hop question-answer pairs. The dataset is human-generated and has four main features: (1) The questions require advanced reasoning over multiple paragraphs. (2) Sentence-level supporting facts are provided, enabling the QA model to reason and explain the answer. (3) The dataset offers various types of reasoning to test the model’s ability to reason and extract relevant proof. (4) The dataset is in Vietnamese, a low-resource language. We also conduct experiments on our dataset using state-of-the-art Multilingual single-hop and multi-hop QA methods. The results suggest that our dataset is challenging for existing methods, and there is room for improvement in Vietnamese QA systems. In addition, we propose a general process for data creation and publish a framework for creating multilingual multi-hop QA datasets. The dataset and framework are publicly available to encourage further research in Vietnamese QA systems.</abstract>
      <url hash="2b996ac9">2022.lrec-1.700</url>
      <bibkey>le-etal-2022-vimqa</bibkey>
      <pwccode url="https://github.com/vimqa/vimqa" additional="false">vimqa/vimqa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/uit-viquad">UIT-ViQuAD</pwcdataset>
    </paper>
    <paper id="701">
      <title>Language Identification for Austronesian Languages</title>
      <author><first>Jonathan</first><last>Dunn</last></author>
      <author><first>Wikke</first><last>Nijhof</last></author>
      <pages>6530–6539</pages>
      <abstract>This paper provides language identification models for low- and under-resourced languages in the Pacific region with a focus on previously unavailable Austronesian languages. Accurate language identification is an important part of developing language resources. The approach taken in this paper combines 29 Austronesian languages with 171 non-Austronesian languages to create an evaluation set drawn from eight data sources. After evaluating six approaches to language identification, we find that a classifier based on skip-gram embeddings reaches a significantly higher performance than alternate methods. We then systematically increase the number of non-Austronesian languages in the model up to a total of 800 languages to evaluate whether an increased language inventory leads to less precise predictions for the Austronesian languages of interest. This evaluation finds that there is only a minimal impact on accuracy caused by increasing the inventory of non-Austronesian languages. Further experiments adapt these language identification models for code-switching detection, achieving high accuracy across all 29 languages.</abstract>
      <url hash="ba97a85c">2022.lrec-1.701</url>
      <bibkey>dunn-nijhof-2022-language</bibkey>
      <pwccode url="https://github.com/jonathandunn/pacific_codeswitch" additional="false">jonathandunn/pacific_codeswitch</pwccode>
    </paper>
    <paper id="702">
      <title>A Mapudüngun <fixed-case>FST</fixed-case> Morphological Analyser and its Web Interface</title>
      <author><first>Andrés</first><last>Chandía</last></author>
      <pages>6540–6547</pages>
      <abstract>This paper describes the development and evaluation of a FST-based analyser-generator for Mapudüngun language, which is publicly available through a web interface. As far as we know, it is the first system of this kind for Mapudüngun. Following the Mapuche grammar by Smeets, we have developed a machine including the morphological and phonological aspects of Mapudüngun. Through this computational approach we have produced a finite state morphological analyser-generator capable of classifying and appropriately tagging all the components (roots and suffixes) interacting in a Mapuche word-form. A double evaluation has been carried out showing a good level of reliability. In order to face the lack of standardization of the language, additional components (an enhanced analyser, a spelling unifier and a root guesser) have been integrated in the tool. The generated corpora, the lexicons and the FST grammars are available for further development and comparison results.</abstract>
      <url hash="96eefeb2">2022.lrec-1.702</url>
      <bibkey>chandia-2022-mapudungun</bibkey>
    </paper>
    <paper id="703">
      <title>Improving Large-scale Language Models and Resources for <fixed-case>F</fixed-case>ilipino</title>
      <author><first>Jan Christian Blaise</first><last>Cruz</last></author>
      <author><first>Charibeth</first><last>Cheng</last></author>
      <pages>6548–6555</pages>
      <abstract>In this paper, we improve on existing language resources for the low-resource Filipino language in two ways. First, we outline the construction of the TLUnified dataset, a large-scale pretraining corpus that serves as an improvement over smaller existing pretraining datasets for the language in terms of scale and topic variety. Second, we pretrain new Transformer language models following the RoBERTa pretraining technique to supplant existing models trained with small corpora. Our new RoBERTa models show significant improvements over existing Filipino models in three benchmark datasets with an average gain of 4.47% test accuracy across three classification tasks with varying difficulty.</abstract>
      <url hash="b44f76cb">2022.lrec-1.703</url>
      <bibkey>cruz-cheng-2022-improving</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ccaligned">CCAligned</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsph-nli">NewsPH-NLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-tl-39">WikiText-TL-39</pwcdataset>
    </paper>
    <paper id="704">
      <title>Thirumurai: A Large Dataset of <fixed-case>T</fixed-case>amil Shaivite Poems and Classification of <fixed-case>T</fixed-case>amil Pann</title>
      <author><first>Shankar</first><last>Mahadevan</last></author>
      <author><first>Rahul</first><last>Ponnusamy</last></author>
      <author><first>Prasanna Kumar</first><last>Kumaresan</last></author>
      <author><first>Prabakaran</first><last>Chandran</last></author>
      <author><first>Ruba</first><last>Priyadharshini</last></author>
      <author><first>Sangeetha</first><last>S</last></author>
      <author><first>Bharathi Raja</first><last>Chakravarthi</last></author>
      <pages>6556–6562</pages>
      <abstract>Thirumurai, also known as Panniru Thirumurai, is a collection of Tamil Shaivite poems dating back to the Hindu revival period between the 6th and the 10th century. These poems are par excellence, in both literary and musical terms. They have been composed based on the ancient, now non-existent Tamil Pann system and can be set to music. We present a large dataset containing all the Thirumurai poems and also attempt to classify the Pann and author of each poem using transformer based architectures. Our work is the first of its kind in dealing with ancient Tamil text datasets, which are severely under-resourced. We explore several Deep Learning-based techniques for solving this challenge effectively and provide essential insights into the problem and how to address it.</abstract>
      <url hash="6925be56">2022.lrec-1.704</url>
      <bibkey>mahadevan-etal-2022-thirumurai</bibkey>
    </paper>
    <paper id="705">
      <title>Generating Monolingual Dataset for Low Resource Language <fixed-case>B</fixed-case>odo from old books using <fixed-case>G</fixed-case>oogle Keep</title>
      <author><first>Sanjib</first><last>Narzary</last></author>
      <author><first>Maharaj</first><last>Brahma</last></author>
      <author><first>Mwnthai</first><last>Narzary</last></author>
      <author><first>Gwmsrang</first><last>Muchahary</last></author>
      <author><first>Pranav Kumar</first><last>Singh</last></author>
      <author><first>Apurbalal</first><last>Senapati</last></author>
      <author><first>Sukumar</first><last>Nandi</last></author>
      <author><first>Bidisha</first><last>Som</last></author>
      <pages>6563–6570</pages>
      <abstract>Bodo is a scheduled Indian language spoken largely by the Bodo community of Assam and other northeastern Indian states. Due to a lack of resources, it is difficult for young languages to communicate more effectively with the rest of the world. This leads to a lack of research in low-resource languages. The creation of a dataset is a tedious and costly process, particularly for languages with no participatory research. This is more visible for languages that are young and have recently adopted standard writing scripts. In this paper, we present a methodology using Google Keep for OCR to generate a monolingual Bodo corpus from different books. In this work, a Bodo text corpus of 192,327 tokens and 32,268 unique tokens is generated using free, accessible, and daily-usable applications. Moreover, some essential characteristics of the Bodo language are discussed that are neglected by Natural Language Progressing (NLP) researchers.</abstract>
      <url hash="10325bfa">2022.lrec-1.705</url>
      <bibkey>narzary-etal-2022-generating</bibkey>
    </paper>
    <paper id="706">
      <title><fixed-case>A</fixed-case>s<fixed-case>NER</fixed-case> - Annotated Dataset and Baseline for <fixed-case>A</fixed-case>ssamese Named Entity recognition</title>
      <author><first>Dhrubajyoti</first><last>Pathak</last></author>
      <author><first>Sukumar</first><last>Nandi</last></author>
      <author><first>Priyankoo</first><last>Sarmah</last></author>
      <pages>6571–6577</pages>
      <abstract>We present the AsNER, a named entity annotation dataset for low resource Assamese language with a baseline Assamese NER model. The dataset contains about 99k tokens comprised of text from the speech of the Prime Minister of India and Assamese play. It also contains person names, location names and addresses. The proposed NER dataset is likely to be a significant resource for deep neural based Assamese language processing. We benchmark the dataset by training NER models and evaluating using state-of-the-art architectures for supervised named entity recognition (NER) such as Fasttext, BERT, XLM-R, FLAIR, MuRIL etc. We implement several baseline approaches with state-of-the-art sequence tagging Bi-LSTM-CRF architecture. The highest F1-score among all baselines achieves an accuracy of 80.69% when using MuRIL as a word embedding method. The annotated dataset and the top performing model are made publicly available.</abstract>
      <url hash="e573aedb">2022.lrec-1.706</url>
      <bibkey>pathak-etal-2022-asner</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wikiann-1">WikiAnn</pwcdataset>
    </paper>
    <paper id="707">
      <title><fixed-case>G</fixed-case>eez<fixed-case>S</fixed-case>witch: Language Identification in Typologically Related Low-resourced <fixed-case>E</fixed-case>ast <fixed-case>A</fixed-case>frican Languages</title>
      <author><first>Fitsum</first><last>Gaim</last></author>
      <author><first>Wonsuk</first><last>Yang</last></author>
      <author><first>Jong C.</first><last>Park</last></author>
      <pages>6578–6584</pages>
      <abstract>Language identification is one of the fundamental tasks in natural language processing that is a prerequisite to data processing and numerous applications. Low-resourced languages with similar typologies are generally confused with each other in real-world applications such as machine translation, affecting the user’s experience. In this work, we present a language identification dataset for five typologically and phylogenetically related low-resourced East African languages that use the Ge’ez script as a writing system; namely Amharic, Blin, Ge’ez, Tigre, and Tigrinya. The dataset is built automatically from selected data sources, but we also performed a manual evaluation to assess its quality. Our approach to constructing the dataset is cost-effective and applicable to other low-resource languages. We integrated the dataset into an existing language-identification tool and also fine-tuned several Transformer based language models, achieving very strong results in all cases. While the task of language identification is easy for the informed person, such datasets can make a difference in real-world deployments and also serve as part of a benchmark for language understanding in the target languages. The data and models are made available at https://github.com/fgaim/geezswitch.</abstract>
      <url hash="e59de139">2022.lrec-1.707</url>
      <bibkey>gaim-etal-2022-geezswitch</bibkey>
      <pwccode url="https://github.com/fgaim/geezswitch" additional="false">fgaim/geezswitch</pwccode>
    </paper>
    <paper id="708">
      <title>Handwritten Paleographic <fixed-case>G</fixed-case>reek Text Recognition: A Century-Based Approach</title>
      <author><first>Paraskevi</first><last>Platanou</last></author>
      <author><first>John</first><last>Pavlopoulos</last></author>
      <author><first>Georgios</first><last>Papaioannou</last></author>
      <pages>6585–6589</pages>
      <abstract>Today classicists are provided with a great number of digital tools which, in turn, offer possibilities for further study and new research goals. In this paper we explore the idea that old Greek handwriting can be machine-readable and consequently, researchers can study the target material fast and efficiently. Previous studies have shown that Handwritten Text Recognition (HTR) models are capable of attaining high accuracy rates. However, achieving high accuracy HTR results for Greek manuscripts is still considered to be a major challenge. The overall aim of this paper is to assess HTR for old Greek manuscripts. To address this statement, we study and use digitized images of the Oxford University Bodleian Library Greek manuscripts. By manually transcribing 77 images, we created and present here a new dataset for Handwritten Paleographic Greek Text Recognition. The dataset instances were organized by establishing as a leading factor the century to which the manuscript and hence the image belongs. Experimenting then with an HTR model we show that the error rate depends on the century of the image.</abstract>
      <url hash="a6e87fd0">2022.lrec-1.708</url>
      <bibkey>platanou-etal-2022-handwritten</bibkey>
    </paper>
    <paper id="709">
      <title>Quality Control for Crowdsourced Bilingual Dictionary in Low-Resource Languages</title>
      <author><first>Hiroki</first><last>Chida</last></author>
      <author><first>Yohei</first><last>Murakami</last></author>
      <author><first>Mondheera</first><last>Pituxcoosuvarn</last></author>
      <pages>6590–6596</pages>
      <abstract>In conventional bilingual dictionary creation by using crowdsourcing, the main method is to ask multiple workers to translate the same words or sentences and take a majority vote. However, when this method is applied to the creation of bilingual dictionaries for low-resource languages with few speakers, many low-quality workers are expected to participate in the majority voting, which makes it difficult to maintain the quality of the evaluation by the majority voting. Therefore, we apply an effective aggregation method using a hyper question, which is a set of single questions, for quality control. Furthermore, to select high-quality workers, we design a task-allocation method based on the reliability of workers which is evaluated by their work results.</abstract>
      <url hash="4f63d398">2022.lrec-1.709</url>
      <bibkey>chida-etal-2022-quality</bibkey>
    </paper>
    <paper id="710">
      <title>An Inflectional Database for Gitksan</title>
      <author><first>Bruce</first><last>Oliver</last></author>
      <author><first>Clarissa</first><last>Forbes</last></author>
      <author><first>Changbing</first><last>Yang</last></author>
      <author><first>Farhan</first><last>Samir</last></author>
      <author><first>Edith</first><last>Coates</last></author>
      <author><first>Garrett</first><last>Nicolai</last></author>
      <author><first>Miikka</first><last>Silfverberg</last></author>
      <pages>6597–6606</pages>
      <abstract>This paper presents a new inflectional resource for Gitksan, a low-resource Indigenous language of Canada. We use Gitksan data in interlinear glossed format, stemming from language documentation efforts, to build a database of partial inflection tables. We then enrich this morphological resource by filling in blank slots in the partial inflection tables using neural transformer reinflection models. We extend the training data for our transformer reinflection models using two data augmentation techniques: data hallucination and back-translation. Experimental results demonstrate substantial improvements from data augmentation, with data hallucination delivering particularly impressive gains. We also release reinflection models for Gitksan.</abstract>
      <url hash="8f989933">2022.lrec-1.710</url>
      <bibkey>oliver-etal-2022-inflectional</bibkey>
      <pwccode url="https://github.com/mpsilfve/gitksan-data" additional="false">mpsilfve/gitksan-data</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="711">
      <title><fixed-case>P</fixed-case>y<fixed-case>C</fixed-case>antonese: <fixed-case>C</fixed-case>antonese Linguistics and <fixed-case>NLP</fixed-case> in Python</title>
      <author><first>Jackson</first><last>Lee</last></author>
      <author><first>Litong</first><last>Chen</last></author>
      <author><first>Charles</first><last>Lam</last></author>
      <author><first>Chaak Ming</first><last>Lau</last></author>
      <author><first>Tsz-Him</first><last>Tsui</last></author>
      <pages>6607–6611</pages>
      <abstract>This paper introduces PyCantonese, an open-source Python library for Cantonese linguistics and natural language processing. After the library design, implementation, corpus data format, and key datasets included are introduced, the paper provides an overview of the currently implemented functionality: stop words, handling Jyutping romanization, word segmentation, part-of-speech tagging, and parsing Cantonese text.</abstract>
      <url hash="125238dd">2022.lrec-1.711</url>
      <bibkey>lee-etal-2022-pycantonese</bibkey>
    </paper>
    <paper id="712">
      <title>Afaan <fixed-case>O</fixed-case>romo Hate Speech Detection and Classification on Social Media</title>
      <author><first>Teshome Mulugeta</first><last>Ababu</last></author>
      <author><first>Michael Melese</first><last>Woldeyohannis</last></author>
      <pages>6612–6619</pages>
      <abstract>Hate and offensive speech on social media is targeted to attack an individual or group of community based on protected characteristics such as gender, ethnicity, and religion. Hate and offensive speech on social media is a global problem that suffers the community especially, for an under-resourced language like Afaan Oromo language. One of the most widely spoken Cushitic language families is Afaan Oromo. Our objective is to develop and test a model used to detect and classify Afaan Oromo hate speech on social media. We developed numerous models that were used to detect and classify Afaan Oromo hate speech on social media by using different machine learning algorithms (classical, ensemble, and deep learning) with the combination of different feature extraction techniques such as BOW, TF-IDF, word2vec, and Keras Embedding layers. To perform the task, we required Afaan Oromo datasets, but the datasets were unavailable. By concentrating on four thematic areas of hate speech, such as gender, religion, race, and offensive speech, we were able to collect a total of 12,812 posts and comments from Facebook. BiLSTM with pre-trained word2vec feature extraction is an outperformed algorithm that achieves better accuracy of 0.84 and 0.88 for eight classes and two classes, respectively.</abstract>
      <url hash="0082e3c2">2022.lrec-1.712</url>
      <bibkey>ababu-woldeyohannis-2022-afaan</bibkey>
    </paper>
    <paper id="713">
      <title>Cross-lingual Linking of Automatically Constructed Frames and <fixed-case>F</fixed-case>rame<fixed-case>N</fixed-case>et</title>
      <author><first>Ryohei</first><last>Sasano</last></author>
      <pages>6620–6625</pages>
      <abstract>A semantic frame is a conceptual structure describing an event, relation, or object along with its participants. Several semantic frame resources have been manually elaborated, and there has been much interest in the possibility of applying semantic frames designed for a particular language to other languages, which has led to the development of cross-lingual frame knowledge. However, manually developing such cross-lingual lexical resources is labor-intensive. To support the development of such resources, this paper presents an attempt at automatic cross-lingual linking of automatically constructed frames and manually crafted frames. Specifically, we link automatically constructed example-based Japanese frames to English FrameNet by using cross-lingual word embeddings and a two-stage model that first extracts candidate FrameNet frames for each Japanese frame by taking only the frame-evoking words into account, then finds the best alignment of frames by also taking frame elements into account. Experiments using frame-annotated sentences in Japanese FrameNet indicate that our approach will facilitate the manual development of cross-lingual frame resources.</abstract>
      <url hash="fc634d19">2022.lrec-1.713</url>
      <bibkey>sasano-2022-cross</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/framenet">FrameNet</pwcdataset>
    </paper>
    <paper id="714">
      <title>Aligning the <fixed-case>R</fixed-case>omanian Reference Treebank and the Valence Lexicon of <fixed-case>R</fixed-case>omanian Verbs</title>
      <author><first>Ana-Maria</first><last>Barbu</last></author>
      <author><first>Verginica</first><last>Barbu Mititelu</last></author>
      <author><first>Cătălin</first><last>Mititelu</last></author>
      <pages>6626–6634</pages>
      <abstract>We present here the efforts of aligning two language resources for Romanian: the Romanian Reference Treebank and the Valence Lexicon of Romanian Verbs: for each occurrence of those verbs in the treebank that were included as entries in the lexicon, a set of valence frames is automatically assigned, then manually validated by two linguists and, when necessary, corrected. Validating a valence frame also means semantically disambiguating the verb in the respective context. The validation is done by two linguists, on complementary datasets. However, a subset of verbs were validated by both annotators and Cohen’s κ is 0.87 for this subset. The alignment we have made also serves as a method of enhancing the quality of the two resources, as in the process we identify morpho-syntactic annotation mistakes, incomplete valence frames or missing ones. Information from each resource complements the information from the other, thus their value increases. The treebank and the lexicon are freely available, while the links discovered between them are also made available on GitHub.</abstract>
      <url hash="0d2ed605">2022.lrec-1.714</url>
      <bibkey>barbu-etal-2022-aligning</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="715">
      <title><fixed-case>P</fixed-case>orti<fixed-case>L</fixed-case>exicon-<fixed-case>UD</fixed-case>: a <fixed-case>P</fixed-case>ortuguese Lexical Resource according to <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies Model</title>
      <author><first>Lucelene</first><last>Lopes</last></author>
      <author><first>Magali</first><last>Duran</last></author>
      <author><first>Paulo</first><last>Fernandes</last></author>
      <author><first>Thiago</first><last>Pardo</last></author>
      <pages>6635–6643</pages>
      <abstract>This paper presents PortiLexicon-UD, a large and freely available lexicon for Portuguese delivering morphosyntactic information according to the Universal Dependencies model. This lexical resource includes part of speech tags, lemmas, and morphological information for words, with 1,221,218 entries (considering word duplication due to different combination of PoS tag, lemma, and morphological features). We report the lexicon creation process, its computational data structure, and its evaluation over an annotated corpus, showing that it has a high language coverage and good quality data.</abstract>
      <url hash="11e14cea">2022.lrec-1.715</url>
      <bibkey>lopes-etal-2022-portilexicon</bibkey>
    </paper>
    <paper id="716">
      <title>Extended Parallel Corpus for <fixed-case>A</fixed-case>mharic-<fixed-case>E</fixed-case>nglish Machine Translation</title>
      <author><first>Andargachew Mekonnen</first><last>Gezmu</last></author>
      <author><first>Andreas</first><last>Nürnberger</last></author>
      <author><first>Tesfaye Bayu</first><last>Bati</last></author>
      <pages>6644–6653</pages>
      <abstract>This paper describes the acquisition, preprocessing, segmentation, and alignment of an Amharic-English parallel corpus. It will be helpful for machine translation of a low-resource language, Amharic. We freely released the corpus for research purposes. Furthermore, we developed baseline statistical and neural machine translation systems; we trained statistical and neural machine translation models using the corpus. In the experiments, we also used a large monolingual corpus for the language model of statistical machine translation and back-translation of neural machine translation. In the automatic evaluation, neural machine translation models outperform statistical machine translation models by approximately six to seven Bilingual Evaluation Understudy (BLEU) points. Besides, among the neural machine translation models, the subword models outperform the word-based models by three to four BLEU points. Moreover, two other relevant automatic evaluation metrics, Translation Edit Rate on Character Level and Better Evaluation as Ranking, reflect corresponding differences among the trained models.</abstract>
      <url hash="764269f4">2022.lrec-1.716</url>
      <bibkey>gezmu-etal-2022-extended</bibkey>
    </paper>
    <paper id="717">
      <title>Low-resource Neural Machine Translation: Benchmarking State-of-the-art Transformer for <fixed-case>W</fixed-case>olof&lt;-&gt;<fixed-case>F</fixed-case>rench</title>
      <author><first>Cheikh M. Bamba</first><last>Dione</last></author>
      <author><first>Alla</first><last>Lo</last></author>
      <author><first>Elhadji Mamadou</first><last>Nguer</last></author>
      <author><first>Sileye</first><last>Ba</last></author>
      <pages>6654–6661</pages>
      <abstract>In this paper, we propose two neural machine translation (NMT) systems (French-to-Wolof and Wolof-to-French) based on sequence-to-sequence with attention and Transformer architectures. We trained our models on the parallel French-Wolof corpus (Nguer et al., 2020) of about 83k sentence pairs. Because of the low-resource setting, we experimented with advanced methods for handling data sparsity, including subword segmentation, backtranslation and the copied corpus method. We evaluate the models using BLEU score and find that the transformer outperforms the classic sequence-to-sequence model in all settings, in addition to being less sensitive to noise. In general, the best scores are achieved when training the models on subword-level based units. For such models, using backtranslation proves to be slightly beneficial in low-resource Wolof to high-resource French language translation for the transformer-based models. A slight improvement can also be observed when injecting copied monolingual text in the target language. Moreover, combining the copied method data with backtranslation leads to a slight improvement of the translation quality.</abstract>
      <url hash="230e4f16">2022.lrec-1.717</url>
      <bibkey>dione-etal-2022-low</bibkey>
    </paper>
    <paper id="718">
      <title>Criteria for Useful Automatic <fixed-case>R</fixed-case>omanization in <fixed-case>S</fixed-case>outh <fixed-case>A</fixed-case>sian Languages</title>
      <author><first>Isin</first><last>Demirsahin</last></author>
      <author><first>Cibu</first><last>Johny</last></author>
      <author><first>Alexander</first><last>Gutkin</last></author>
      <author><first>Brian</first><last>Roark</last></author>
      <pages>6662–6673</pages>
      <abstract>This paper presents a number of possible criteria for systems that transliterate South Asian languages from their native scripts into the Latin script, a process known as romanization. These criteria are related to either fidelity to human linguistic behavior (pronunciation transparency, naturalness and conventionality) or processing utility for people (ease of input) as well as under-the-hood in systems (invertibility and stability across languages and scripts). When addressing these differing criteria several linguistic considerations, such as modeling of prominent phonological processes and their relation to orthography, need to be taken into account. We discuss these key linguistic details in the context of Brahmic scripts and languages that use them, such as Hindi and Malayalam. We then present the core features of several romanization algorithms, implemented in a finite state transducer (FST) formalism, that address differing criteria. Implementations of these algorithms have been released as part of the Nisaba finite-state script processing library.</abstract>
      <url hash="cc883d0e">2022.lrec-1.718</url>
      <bibkey>demirsahin-etal-2022-criteria</bibkey>
    </paper>
    <paper id="719">
      <title><fixed-case>BERT</fixed-case>ology for Machine Translation: What <fixed-case>BERT</fixed-case> Knows about Linguistic Difficulties for Translation</title>
      <author><first>Yuqian</first><last>Dai</last></author>
      <author><first>Marc</first><last>de Kamps</last></author>
      <author><first>Serge</first><last>Sharoff</last></author>
      <pages>6674–6690</pages>
      <abstract>Pre-trained transformer-based models, such as BERT, have shown excellent performance in most natural language processing benchmark tests, but we still lack a good understanding of the linguistic knowledge of BERT in Neural Machine Translation (NMT). Our work uses syntactic probes and Quality Estimation (QE) models to analyze the performance of BERT’s syntactic dependencies and their impact on machine translation quality, exploring what kind of syntactic dependencies are difficult for NMT engines based on BERT. While our probing experiments confirm that pre-trained BERT “knows” about syntactic dependencies, its ability to recognize them often decreases after fine-tuning for NMT tasks. We also detect a relationship between syntactic dependencies in three languages and the quality of their translations, which shows which specific syntactic dependencies are likely to be a significant cause of low-quality translations.</abstract>
      <url hash="2737b8bf">2022.lrec-1.719</url>
      <bibkey>dai-etal-2022-bertology</bibkey>
    </paper>
    <paper id="720">
      <title><fixed-case>CVSS</fixed-case> Corpus and Massively Multilingual Speech-to-Speech Translation</title>
      <author><first>Ye</first><last>Jia</last></author>
      <author><first>Michelle</first><last>Tadmor Ramanovich</last></author>
      <author><first>Quan</first><last>Wang</last></author>
      <author><first>Heiga</first><last>Zen</last></author>
      <pages>6691–6703</pages>
      <abstract>We introduce CVSS, a massively multilingual-to-English speech-to-speech translation (S2ST) corpus, covering sentence-level parallel S2ST pairs from 21 languages into English. CVSS is derived from the Common Voice speech corpus and the CoVoST 2 speech-to-text translation (ST) corpus, by synthesizing the translation text from CoVoST 2 into speech using state-of-the-art TTS systems. Two versions of translation speech in English are provided: 1) CVSS-C: All the translation speech is in a single high-quality canonical voice; 2) CVSS-T: The translation speech is in voices transferred from the corresponding source speech. In addition, CVSS provides normalized translation text which matches the pronunciation in the translation speech. On each version of CVSS, we built baseline multilingual direct S2ST models and cascade S2ST models, verifying the effectiveness of the corpus. To build strong cascade S2ST baselines, we trained an ST model on CoVoST 2, which outperforms the previous state-of-the-art trained on the corpus without extra data by 5.8 BLEU. Nevertheless, the performance of the direct S2ST models approaches the strong cascade baselines when trained from scratch, and with only 0.1 or 0.7 BLEU difference on ASR transcribed translation when initialized from matching ST models.</abstract>
      <url hash="120a8757">2022.lrec-1.720</url>
      <bibkey>jia-etal-2022-cvss</bibkey>
      <pwccode url="https://github.com/google-research-datasets/cvss" additional="false">google-research-datasets/cvss</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cvss">CVSS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/covost">CoVoST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/common-voice">Common Voice</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/libritts">LibriTTS</pwcdataset>
    </paper>
    <paper id="721">
      <title><fixed-case>JP</fixed-case>ara<fixed-case>C</fixed-case>rawl v3.0: A Large-scale <fixed-case>E</fixed-case>nglish-<fixed-case>J</fixed-case>apanese Parallel Corpus</title>
      <author><first>Makoto</first><last>Morishita</last></author>
      <author><first>Katsuki</first><last>Chousa</last></author>
      <author><first>Jun</first><last>Suzuki</last></author>
      <author><first>Masaaki</first><last>Nagata</last></author>
      <pages>6704–6710</pages>
      <abstract>Most current machine translation models are mainly trained with parallel corpora, and their translation accuracy largely depends on the quality and quantity of the corpora. Although there are billions of parallel sentences for a few language pairs, effectively dealing with most language pairs is difficult due to a lack of publicly available parallel corpora. This paper creates a large parallel corpus for English-Japanese, a language pair for which only limited resources are available, compared to such resource-rich languages as English-German. It introduces a new web-based English-Japanese parallel corpus named JParaCrawl v3.0. Our new corpus contains more than 21 million unique parallel sentence pairs, which is more than twice as many as the previous JParaCrawl v2.0 corpus. Through experiments, we empirically show how our new corpus boosts the accuracy of machine translation models on various domains. The JParaCrawl v3.0 corpus will eventually be publicly available online for research purposes.</abstract>
      <url hash="843dda2b">2022.lrec-1.721</url>
      <bibkey>morishita-etal-2022-jparacrawl</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/aspec">ASPEC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/business-scene-dialogue">Business Scene Dialogue</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/jesc">JESC</pwcdataset>
    </paper>
    <paper id="722">
      <title>Learning How to Translate <fixed-case>N</fixed-case>orth <fixed-case>K</fixed-case>orean through <fixed-case>S</fixed-case>outh <fixed-case>K</fixed-case>orean</title>
      <author><first>Hwichan</first><last>Kim</last></author>
      <author><first>Sangwhan</first><last>Moon</last></author>
      <author><first>Naoaki</first><last>Okazaki</last></author>
      <author><first>Mamoru</first><last>Komachi</last></author>
      <pages>6711–6718</pages>
      <abstract>South and North Korea both use the Korean language. However, Korean NLP research has focused on South Korean only, and existing NLP systems of the Korean language, such as neural machine translation (NMT) models, cannot properly handle North Korean inputs. Training a model using North Korean data is the most straightforward approach to solving this problem, but there is insufficient data to train NMT models. In this study, we create data for North Korean NMT models using a comparable corpus. First, we manually create evaluation data for automatic alignment and machine translation, and then, investigate automatic alignment methods suitable for North Korean. Finally, we show that a model trained by North Korean bilingual data without human annotation significantly boosts North Korean translation accuracy compared to existing South Korean models in zero-shot settings.</abstract>
      <url hash="7f55e783">2022.lrec-1.722</url>
      <bibkey>kim-etal-2022-learning</bibkey>
    </paper>
    <paper id="723">
      <title><fixed-case>FG</fixed-case>ra<fixed-case>DA</fixed-case>: A Dataset and Benchmark for Fine-Grained Domain Adaptation in Machine Translation</title>
      <author><first>Wenhao</first><last>Zhu</last></author>
      <author><first>Shujian</first><last>Huang</last></author>
      <author><first>Tong</first><last>Pu</last></author>
      <author><first>Pingxuan</first><last>Huang</last></author>
      <author><first>Xu</first><last>Zhang</last></author>
      <author><first>Jian</first><last>Yu</last></author>
      <author><first>Wei</first><last>Chen</last></author>
      <author><first>Yanfeng</first><last>Wang</last></author>
      <author><first>Jiajun</first><last>Chen</last></author>
      <pages>6719–6727</pages>
      <abstract>Previous research for adapting a general neural machine translation (NMT) model into a specific domain usually neglects the diversity in translation within the same domain, which is a core problem for domain adaptation in real-world scenarios. One representative of such challenging scenarios is to deploy a translation system for a conference with a specific topic, e.g., global warming or coronavirus, where there are usually extremely less resources due to the limited schedule. To motivate wider investigation in such a scenario, we present a real-world fine-grained domain adaptation task in machine translation (FGraDA). The FGraDA dataset consists of Chinese-English translation task for four sub-domains of information technology: autonomous vehicles, AI education, real-time networks, and smart phone. Each sub-domain is equipped with a development set and test set for evaluation purposes. To be closer to reality, FGraDA does not employ any in-domain bilingual training data but provides bilingual dictionaries and wiki knowledge base, which can be easier obtained within a short time. We benchmark the fine-grained domain adaptation task and present in-depth analyses showing that there are still challenging problems to further improve the performance with heterogeneous resources.</abstract>
      <url hash="9e77e125">2022.lrec-1.723</url>
      <bibkey>zhu-etal-2022-fgrada</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/fgrada">FGraDA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/aspec">ASPEC</pwcdataset>
    </paper>
    <paper id="724">
      <title><fixed-case>S</fixed-case>ans<fixed-case>T</fixed-case>ib, a <fixed-case>S</fixed-case>anskrit - <fixed-case>T</fixed-case>ibetan Parallel Corpus and Bilingual Sentence Embedding Model</title>
      <author><first>Sebastian</first><last>Nehrdich</last></author>
      <pages>6728–6734</pages>
      <abstract>This paper presents the development of SansTib, a Sanskrit - Classical Tibetan parallel corpus automatically aligned on sentence-level, and a bilingual sentence embedding model. The corpus has a size of about 317,289 sentence pairs and 14,420,771 tokens and thereby is a considerable improvement over previous resources for these two languages. The data is incorporated into the BuddhaNexus database to make it accessible to a larger audience. It also presents a gold evaluation dataset and assesses the quality of the automatic alignment.</abstract>
      <url hash="674343ba">2022.lrec-1.724</url>
      <bibkey>nehrdich-2022-sanstib</bibkey>
    </paper>
    <paper id="725">
      <title><fixed-case>VISA</fixed-case>: An Ambiguous Subtitles Dataset for Visual Scene-aware Machine Translation</title>
      <author><first>Yihang</first><last>Li</last></author>
      <author><first>Shuichiro</first><last>Shimizu</last></author>
      <author><first>Weiqi</first><last>Gu</last></author>
      <author><first>Chenhui</first><last>Chu</last></author>
      <author><first>Sadao</first><last>Kurohashi</last></author>
      <pages>6735–6743</pages>
      <abstract>Existing multimodal machine translation (MMT) datasets consist of images and video captions or general subtitles which rarely contain linguistic ambiguity, making visual information not so effective to generate appropriate translations. We introduce VISA, a new dataset that consists of 40k Japanese-English parallel sentence pairs and corresponding video clips with the following key features: (1) the parallel sentences are subtitles from movies and TV episodes; (2) the source subtitles are ambiguous, which means they have multiple possible translations with different meanings; (3) we divide the dataset into Polysemy and Omission according to the cause of ambiguity. We show that VISA is challenging for the latest MMT system, and we hope that the dataset can facilitate MMT research.</abstract>
      <url hash="53987950">2022.lrec-1.725</url>
      <bibkey>li-etal-2022-visa</bibkey>
      <pwccode url="https://github.com/ku-nlp/visa" additional="false">ku-nlp/visa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/how2">How2</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
    </paper>
    <paper id="726">
      <title>A Benchmark Dataset for Multi-Level Complexity-Controllable Machine Translation</title>
      <author><first>Kazuki</first><last>Tani</last></author>
      <author><first>Ryoya</first><last>Yuasa</last></author>
      <author><first>Kazuki</first><last>Takikawa</last></author>
      <author><first>Akihiro</first><last>Tamura</last></author>
      <author><first>Tomoyuki</first><last>Kajiwara</last></author>
      <author><first>Takashi</first><last>Ninomiya</last></author>
      <author><first>Tsuneo</first><last>Kato</last></author>
      <pages>6744–6752</pages>
      <abstract>This paper presents a new benchmark test dataset for multi-level complexity-controllable machine translation (MLCC-MT), which is MT controlling the complexity of the output at more than two levels. In previous research, MLCC-MT models have been evaluated on a test dataset automatically constructed from the Newsela corpus, which is a document-level comparable corpus with document-level complexity. The existing test dataset has the following three problems: (i) A source language sentence and its target language sentence are not necessarily an exact translation pair because they are automatically detected. (ii) A target language sentence and its simplified target language sentence are not necessarily exactly parallel because they are automatically aligned. (iii) A sentence-level complexity is not necessarily appropriate because it is transferred from an article-level complexity attached to the Newsela corpus. Therefore, we create a benchmark test dataset for Japanese-to-English MLCC-MT from the Newsela corpus by introducing an automatic filtering of data with inappropriate sentence-level complexity, manual check for parallel target language sentences with different complexity levels, and manual translation. Moreover, we implement two MLCC-NMT frameworks with a Transformer architecture and report their performance on our test dataset as baselines for future research. Our test dataset and codes are released.</abstract>
      <url hash="b9999f2b">2022.lrec-1.726</url>
      <bibkey>tani-etal-2022-benchmark</bibkey>
      <pwccode url="https://github.com/k-t4n1/a-benchmarkdataset-for-complexitycontrollablenmt" additional="false">k-t4n1/a-benchmarkdataset-for-complexitycontrollablenmt</pwccode>
    </paper>
    <paper id="727">
      <title>ga<fixed-case>H</fixed-case>ealth: An <fixed-case>E</fixed-case>nglish–<fixed-case>I</fixed-case>rish Bilingual Corpus of Health Data</title>
      <author><first>Séamus</first><last>Lankford</last></author>
      <author><first>Haithem</first><last>Afli</last></author>
      <author><first>Órla</first><last>Ní Loinsigh</last></author>
      <author><first>Andy</first><last>Way</last></author>
      <pages>6753–6758</pages>
      <abstract>Machine Translation is a mature technology for many high-resource language pairs. However in the context of low-resource languages, there is a paucity of parallel data datasets available for developing translation models. Furthermore, the development of datasets for low-resource languages often focuses on simply creating the largest possible dataset for generic translation. The benefits and development of smaller in-domain datasets can easily be overlooked. To assess the merits of using in-domain data, a dataset for the specific domain of health was developed for the low-resource English to Irish language pair. Our study outlines the process used in developing the corpus and empirically demonstrates the benefits of using an in-domain dataset for the health domain. In the context of translating health-related data, models developed using the gaHealth corpus demonstrated a maximum BLEU score improvement of 22.2 points (40%) when compared with top performing models from the LoResMT2021 Shared Task. Furthermore, we define linguistic guidelines for developing gaHealth, the first bilingual corpus of health data for the Irish language, which we hope will be of use to other creators of low-resource data sets. gaHealth is now freely available online and is ready to be explored for further research.</abstract>
      <url hash="ea32d961">2022.lrec-1.727</url>
      <bibkey>lankford-etal-2022-gahealth</bibkey>
      <pwccode url="https://github.com/seamusl/gahealth" additional="false">seamusl/gahealth</pwccode>
    </paper>
    <paper id="728">
      <title>Translation Memories as Baselines for Low-Resource Machine Translation</title>
      <author><first>Rebecca</first><last>Knowles</last></author>
      <author><first>Patrick</first><last>Littell</last></author>
      <pages>6759–6767</pages>
      <abstract>Low-resource machine translation research often requires building baselines to benchmark estimates of progress in translation quality. Neural and statistical phrase-based systems are often used with out-of-the-box settings to build these initial baselines before analyzing more sophisticated approaches, implicitly comparing the first machine translation system to the absence of any translation assistance. We argue that this approach overlooks a basic resource: if you have parallel text, you have a translation memory. In this work, we show that using available text as a translation memory baseline against which to compare machine translation systems is simple, effective, and can shed light on additional translation challenges.</abstract>
      <url hash="7e93e3de">2022.lrec-1.728</url>
      <bibkey>knowles-littell-2022-translation</bibkey>
    </paper>
    <paper id="729">
      <title><fixed-case>N</fixed-case>24<fixed-case>N</fixed-case>ews: A New Dataset for Multimodal News Classification</title>
      <author><first>Zhen</first><last>Wang</last></author>
      <author><first>Xu</first><last>Shan</last></author>
      <author><first>Xiangxie</first><last>Zhang</last></author>
      <author><first>Jie</first><last>Yang</last></author>
      <pages>6768–6775</pages>
      <abstract>Current news datasets merely focus on text features on the news and rarely leverage the feature of images, excluding numerous essential features for news classification. In this paper, we propose a new dataset, N24News, which is generated from New York Times with 24 categories and contains both text and image information in each news. We use a multitask multimodal method and the experimental results show multimodal news classification performs better than text-only news classification. Depending on the length of the text, the classification accuracy can be increased by up to 8.11%. Our research reveals the relationship between the performance of a multimodal classifier and its sub-classifiers, and also the possible improvements when applying multimodal in news classification. N24News is shown to have great potential to prompt the multimodal news studies.</abstract>
      <url hash="33bbaadc">2022.lrec-1.729</url>
      <bibkey>wang-etal-2022-n24news</bibkey>
      <pwccode url="https://github.com/billywzh717/n24news" additional="false">billywzh717/n24news</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/n15news">N15News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fakeddit">Fakeddit</pwcdataset>
    </paper>
    <paper id="730">
      <title><fixed-case>M</fixed-case>ulti<fixed-case>S</fixed-case>ubs: A Large-scale Multimodal and Multilingual Dataset</title>
      <author><first>Josiah</first><last>Wang</last></author>
      <author><first>Josiel</first><last>Figueiredo</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <pages>6776–6785</pages>
      <abstract>This paper introduces a large-scale multimodal and multilingual dataset that aims to facilitate research on grounding words to images in their contextual usage in language. The dataset consists of images selected to unambiguously illustrate concepts expressed in sentences from movie subtitles. The dataset is a valuable resource as (i) the images are aligned to text fragments rather than whole sentences; (ii) multiple images are possible for a text fragment and a sentence; (iii) the sentences are free-form and real-world like; (iv) the parallel texts are multilingual. We also set up a fill-in-the-blank game for humans to evaluate the quality of the automatic image selection process of our dataset. Finally, we propose a fill-in-the-blank task to demonstrate the utility of the dataset, and present some baseline prediction models. The dataset will benefit research on visual grounding of words especially in the context of free-form sentences, and can be obtained from https://doi.org/10.5281/zenodo.5034604 under a Creative Commons licence.</abstract>
      <url hash="bcf2753f">2022.lrec-1.730</url>
      <bibkey>wang-etal-2022-multisubs</bibkey>
      <pwccode url="https://github.com/josiahwang/multisubs-eval" additional="false">josiahwang/multisubs-eval</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multisubs">MultiSubs</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
    </paper>
    <paper id="731">
      <title><fixed-case>CI</fixed-case>-<fixed-case>AVSR</fixed-case>: A <fixed-case>C</fixed-case>antonese Audio-Visual Speech Datasetfor In-car Command Recognition</title>
      <author><first>Wenliang</first><last>Dai</last></author>
      <author><first>Samuel</first><last>Cahyawijaya</last></author>
      <author><first>Tiezheng</first><last>Yu</last></author>
      <author><first>Elham J.</first><last>Barezi</last></author>
      <author><first>Peng</first><last>Xu</last></author>
      <author><first>Cheuk Tung</first><last>Yiu</last></author>
      <author><first>Rita</first><last>Frieske</last></author>
      <author><first>Holy</first><last>Lovenia</last></author>
      <author><first>Genta</first><last>Winata</last></author>
      <author><first>Qifeng</first><last>Chen</last></author>
      <author><first>Xiaojuan</first><last>Ma</last></author>
      <author><first>Bertram</first><last>Shi</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <pages>6786–6793</pages>
      <abstract>With the rise of deep learning and intelligent vehicles, the smart assistant has become an essential in-car component to facilitate driving and provide extra functionalities. In-car smart assistants should be able to process general as well as car-related commands and perform corresponding actions, which eases driving and improves safety. However, there is a data scarcity issue for low resource languages, hindering the development of research and applications. In this paper, we introduce a new dataset, Cantonese In-car Audio-Visual Speech Recognition (CI-AVSR), for in-car command recognition in the Cantonese language with both video and audio data. It consists of 4,984 samples (8.3 hours) of 200 in-car commands recorded by 30 native Cantonese speakers. Furthermore, we augment our dataset using common in-car background noises to simulate real environments, producing a dataset 10 times larger than the collected one. We provide detailed statistics of both the clean and the augmented versions of our dataset. Moreover, we implement two multimodal baselines to demonstrate the validity of CI-AVSR. Experiment results show that leveraging the visual signal improves the overall performance of the model. Although our best model can achieve a considerable quality on the clean test set, the speech recognition quality on the noisy data is still inferior and remains an extremely challenging task for real in-car speech recognition systems. The dataset and code will be released at https://github.com/HLTCHKUST/CI-AVSR.</abstract>
      <url hash="df1669cb">2022.lrec-1.731</url>
      <bibkey>dai-etal-2022-ci</bibkey>
      <pwccode url="https://github.com/hltchkust/ci-avsr" additional="false">hltchkust/ci-avsr</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ci-avsr">CI-AVSR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/muse-car">MuSe-CaR</pwcdataset>
    </paper>
    <paper id="732">
      <title>Multimodal Negotiation Corpus with Various Subjective Assessments for Social-Psychological Outcome Prediction from Non-Verbal Cues</title>
      <author><first>Nobukatsu</first><last>Hojo</last></author>
      <author><first>Satoshi</first><last>Kobashikawa</last></author>
      <author><first>Saki</first><last>Mizuno</last></author>
      <author><first>Ryo</first><last>Masumura</last></author>
      <pages>6794–6801</pages>
      <abstract>This study investigates social-psychological negotiation-outcome prediction (SPNOP), a novel task for estimating various subjective evaluation scores of negotiation, such as satisfaction and trust, from negotiation dialogue data. To investigate SPNOP, a corpus with various psychological measurements is beneficial because the interaction process of negotiation relates to many aspects of psychology. However, current negotiation corpora only include information related to objective outcomes or a single aspect of psychology. In addition, most use the “laboratory setting” that uses non-skilled negotiators and over simplified negotiation scenarios. There is a concern that such a gap with actual negotiation will intrinsically affect the behavior and psychology of negotiators in the corpus, which can degrade the performance of models trained from the corpus in real situations. Therefore, we created a negotiation corpus with three features; 1) was assessed with various psychological measurements, 2) used skilled negotiators, and 3) used scenarios of context-rich negotiation. We recorded video and audio of negotiations in Japanese to investigate SPNOP in the context of social signal processing. Experimental results indicate that social-psychological outcomes can be effectively estimated from multimodal information.</abstract>
      <url hash="1aba422d">2022.lrec-1.732</url>
      <bibkey>hojo-etal-2022-multimodal</bibkey>
    </paper>
    <paper id="733">
      <title><fixed-case>MMDAG</fixed-case>: Multimodal Directed Acyclic Graph Network for Emotion Recognition in Conversation</title>
      <author><first>Shuo</first><last>Xu</last></author>
      <author><first>Yuxiang</first><last>Jia</last></author>
      <author><first>Changyong</first><last>Niu</last></author>
      <author><first>Hongying</first><last>Zan</last></author>
      <pages>6802–6807</pages>
      <abstract>Emotion recognition in conversation is important for an empathetic dialogue system to understand the user’s emotion and then generate appropriate emotional responses. However, most previous researches focus on modeling conversational contexts primarily based on the textual modality or simply utilizing multimodal information through feature concatenation. In order to exploit multimodal information and contextual information more effectively, we propose a multimodal directed acyclic graph (MMDAG) network by injecting information flows inside modality and across modalities into the DAG architecture. Experiments on IEMOCAP and MELD show that our model outperforms other state-of-the-art models. Comparative studies validate the effectiveness of the proposed modality fusion method.</abstract>
      <url hash="2317d59d">2022.lrec-1.733</url>
      <bibkey>xu-etal-2022-mmdag</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/iemocap">IEMOCAP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/meld">MELD</pwcdataset>
    </paper>
    <paper id="734">
      <title>Automatic Gloss-level Data Augmentation for Sign Language Translation</title>
      <author><first>Jin Yea</first><last>Jang</last></author>
      <author><first>Han-Mu</first><last>Park</last></author>
      <author><first>Saim</first><last>Shin</last></author>
      <author><first>Suna</first><last>Shin</last></author>
      <author><first>Byungcheon</first><last>Yoon</last></author>
      <author><first>Gahgene</first><last>Gweon</last></author>
      <pages>6808–6813</pages>
      <abstract>Securing sufficient data to enable automatic sign language translation modeling is challenging. The data insufficiency issue exists in both video and text modalities; however, fewer studies have been performed on text data augmentation compared to video data. In this study, we present three methods of augmenting sign language text modality data, comprising 3,052 Gloss-level Korean Sign Language (GKSL) and Word-level Korean Language (WKL) sentence pairs. Using each of the three methods, the following number of sentence pairs were created: blank replacement 10,654, sentence paraphrasing 1,494, and synonym replacement 899. Translation experiment results using the augmented data showed that when translating from GKSL to WKL and from WKL to GKSL, Bi-Lingual Evaluation Understudy (BLEU) scores improved by 0.204 and 0.170 respectively, compared to when only the original data was used. The three contributions of this study are as follows. First, we demonstrated that three different augmentation techniques used in existing Natural Language Processing (NLP) can be applied to sign language. Second, we propose an automatic data augmentation method which generates quality data by utilizing the Korean sign language gloss dictionary. Lastly, we publish the Gloss-level Korean Sign Language 13k dataset (GKSL13k), which has verified data quality through expert reviews.</abstract>
      <url hash="4b1e435f">2022.lrec-1.734</url>
      <bibkey>jang-etal-2022-automatic</bibkey>
    </paper>
    <paper id="735">
      <title>Image Description Dataset for Language Learners</title>
      <author><first>Kento</first><last>Tanaka</last></author>
      <author><first>Taichi</first><last>Nishimura</last></author>
      <author><first>Hiroaki</first><last>Nanjo</last></author>
      <author><first>Keisuke</first><last>Shirai</last></author>
      <author><first>Hirotaka</first><last>Kameko</last></author>
      <author><first>Masatake</first><last>Dantsuji</last></author>
      <pages>6814–6821</pages>
      <abstract>We focus on image description and a corresponding assessment system for language learners. To achieve automatic assessment of image description, we construct a novel dataset, the Language Learner Image Description (LLID) dataset, which consists of images, their descriptions, and assessment annotations. Then, we propose a novel task of automatic error correction for image description, and we develop a baseline model that encodes multimodal information from a learner sentence with an image and accurately decodes a corrected sentence. Our experimental results show that the developed model can revise errors that cannot be revised without an image.</abstract>
      <url hash="eb3cab96">2022.lrec-1.735</url>
      <bibkey>tanaka-etal-2022-image</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
    </paper>
    <paper id="736">
      <title>The Multimodal Annotation Software Tool (<fixed-case>MAST</fixed-case>)</title>
      <author><first>Bruno</first><last>Cardoso</last></author>
      <author><first>Neil</first><last>Cohn</last></author>
      <pages>6822–6828</pages>
      <abstract>Multimodal combinations of writing and pictures have become ubiquitous in contemporary society, and scholars have increasingly been turning to analyzing these media. Here we present a resource for annotating these complex documents: the Multimodal Annotation Software Tool (MAST). MAST is an application that allows users to analyze visual and multimodal documents by selecting and annotating visual regions, and to establish relations between annotations that create dependencies and/or constituent structures. By means of schema publications, MAST allows annotation theories to be citable, while evolving and being shared. Documents can be annotated using multiple schemas simultaneously, offering more comprehensive perspectives. As a distributed, client-server system MAST allows for collaborative annotations across teams of users, and features team management and resource access functionalities, facilitating the potential for implementing open science practices. Altogether, we aim for MAST to provide a powerful and innovative annotation tool with application across numerous fields engaging with multimodal media.</abstract>
      <url hash="3fd7ec9a">2022.lrec-1.736</url>
      <bibkey>cardoso-cohn-2022-multimodal</bibkey>
    </paper>
    <paper id="737">
      <title>A Multimodal <fixed-case>G</fixed-case>erman Dataset for Automatic Lip Reading Systems and Transfer Learning</title>
      <author><first>Gerald</first><last>Schwiebert</last></author>
      <author><first>Cornelius</first><last>Weber</last></author>
      <author><first>Leyuan</first><last>Qu</last></author>
      <author><first>Henrique</first><last>Siqueira</last></author>
      <author><first>Stefan</first><last>Wermter</last></author>
      <pages>6829–6836</pages>
      <abstract>Large datasets as required for deep learning of lip reading do not exist in many languages. In this paper we present the dataset GLips (German Lips) consisting of 250,000 publicly available videos of the faces of speakers of the Hessian Parliament, which was processed for word-level lip reading using an automatic pipeline. The format is similar to that of the English language LRW (Lip Reading in the Wild) dataset, with each video encoding one word of interest in a context of 1.16 seconds duration, which yields compatibility for studying transfer learning between both datasets. By training a deep neural network, we investigate whether lip reading has language-independent features, so that datasets of different languages can be used to improve lip reading models. We demonstrate learning from scratch and show that transfer learning from LRW to GLips and vice versa improves learning speed and performance, in particular for the validation set.</abstract>
      <url hash="5e178386">2022.lrec-1.737</url>
      <bibkey>schwiebert-etal-2022-multimodal</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/glips">GLips</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/lrw">LRW</pwcdataset>
    </paper>
    <paper id="738">
      <title>Multimodality for <fixed-case>NLP</fixed-case>-Centered Applications: Resources, Advances and Frontiers</title>
      <author><first>Muskan</first><last>Garg</last></author>
      <author><first>Seema</first><last>Wazarkar</last></author>
      <author><first>Muskaan</first><last>Singh</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <pages>6837–6847</pages>
      <abstract>With the development of multimodal systems and natural language generation techniques, the resurgence of multimodal datasets has attracted significant research interests, which aims to provide new information to enrich the representation of textual data. However, there remains a lack of a comprehensive survey for this task. To this end, we take the first step and present a thorough review of this research field. This paper provides an overview of a publicly available dataset with different modalities according to the applications. Furthermore, we discuss the new frontier and give our thoughts. We hope this survey of multimodal datasets can provide the community with quick access and a general picture of the multimodal dataset for specific Natural Language Processing (NLP) applications and motivates future researches. In this context, we release the collection of all multimodal datasets easily accessible here: https://github.com/drmuskangarg/Multimodal-datasets</abstract>
      <url hash="55bce828">2022.lrec-1.738</url>
      <bibkey>garg-etal-2022-multimodality</bibkey>
      <pwccode url="https://github.com/drmuskangarg/multimodal-datasets" additional="false">drmuskangarg/multimodal-datasets</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cmu-mosei">CMU-MOSEI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/gqa">GQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/iemocap">IEMOCAP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mdid">MDID</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/meld">MELD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/memexqa">MemexQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/r2vq">R2VQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/screen2words">Screen2Words</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/summe">SumMe</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tdiuc">TDIUC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tgif-qa">TGIF-QA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tvqa">TVQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/vatex">VATEX</pwcdataset>
    </paper>
    <paper id="739">
      <title>Cross-lingual and Multilingual <fixed-case>CLIP</fixed-case></title>
      <author><first>Fredrik</first><last>Carlsson</last></author>
      <author><first>Philipp</first><last>Eisen</last></author>
      <author><first>Faton</first><last>Rekathati</last></author>
      <author><first>Magnus</first><last>Sahlgren</last></author>
      <pages>6848–6854</pages>
      <abstract>The long-standing endeavor of relating the textual and the visual domain recently underwent a pivotal breakthrough, as OpenAI released CLIP. This model distinguishes how well an English text corresponds with a given image with unprecedented accuracy. Trained via a contrastive learning objective over a huge dataset of 400M of images and captions, it is a work that is not easily replicated, especially for low resource languages. Capitalizing on the modularization of the CLIP architecture, we propose to use cross-lingual teacher learning to re-train the textual encoder for various non-English languages. Our method requires no image data and relies entirely on machine translation which removes the need for data in the target language. We find that our method can efficiently train a new textual encoder with relatively low computational cost, whilst still outperforming previous baselines on multilingual image-text retrieval.</abstract>
      <url hash="a25acf43">2022.lrec-1.739</url>
      <bibkey>carlsson-etal-2022-cross</bibkey>
      <pwccode url="https://github.com/FreddeFrallan/Multilingual-CLIP" additional="false">FreddeFrallan/Multilingual-CLIP</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/flickr30k">Flickr30k</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xtd10">XTD10</pwcdataset>
    </paper>
    <paper id="740">
      <title><fixed-case>BAN</fixed-case>-Cap: A Multi-Purpose <fixed-case>E</fixed-case>nglish-<fixed-case>B</fixed-case>angla Image Descriptions Dataset</title>
      <author><first>Mohammad Faiyaz</first><last>Khan</last></author>
      <author><first>S.M. Sadiq-Ur-Rahman</first><last>Shifath</last></author>
      <author><first>Md Saiful</first><last>Islam</last></author>
      <pages>6855–6865</pages>
      <abstract>As computers have become efficient at understanding visual information and transforming it into a written representation, research interest in tasks like automatic image captioning has seen a significant leap over the last few years. While most of the research attention is given to the English language in a monolingual setting, resource-constrained languages like Bangla remain out of focus, predominantly due to a lack of standard datasets. Addressing this issue, we present a new dataset BAN-Cap following the widely used Flickr8k dataset, where we collect Bangla captions of the images provided by qualified annotators. Our dataset represents a wider variety of image caption styles annotated by trained people from different backgrounds. We present a quantitative and qualitative analysis of the dataset and the baseline evaluation of the recent models in Bangla image captioning. We investigate the effect of text augmentation and demonstrate that an adaptive attention-based model combined with text augmentation using Contextualized Word Replacement (CWR) outperforms all state-of-the-art models for Bangla image captioning. We also present this dataset’s multipurpose nature, especially on machine translation for Bangla-English and English-Bangla. This dataset and all the models will be useful for further research.</abstract>
      <url hash="63997b74">2022.lrec-1.740</url>
      <bibkey>khan-etal-2022-ban</bibkey>
      <pwccode url="https://github.com/faiyazkhan11/ban-cap" additional="false">faiyazkhan11/ban-cap</pwccode>
    </paper>
    <paper id="741">
      <title><fixed-case>SSR</fixed-case>7000: A Synchronized Corpus of Ultrasound Tongue Imaging for End-to-End Silent Speech Recognition</title>
      <author><first>Naoki</first><last>Kimura</last></author>
      <author><first>Zixiong</first><last>Su</last></author>
      <author><first>Takaaki</first><last>Saeki</last></author>
      <author><first>Jun</first><last>Rekimoto</last></author>
      <pages>6866–6873</pages>
      <abstract>This article presents SSR7000, a corpus of synchronized ultrasound tongue and lip images designed for end-to-end silent speech recognition (SSR). Although neural end-to-end models are successfully updating the state-of-the-art technology in the field of automatic speech recognition, SSR research based on ultrasound tongue imaging has still not evolved past cascaded DNN-HMM models due to the absence of a large dataset. In this study, we constructed a large dataset, namely SSR7000, to exploit the performance of the end-to-end models. The SSR7000 dataset contains ultrasound tongue and lip images of 7484 utterances by a single speaker. It contains more utterances per person than any other SSR corpus based on ultrasound imaging. We also describe preprocessing techniques to tackle data variances that are inevitable when collecting a large dataset and present benchmark results using an end-to-end model. The SSR7000 corpus is publicly available under the CC BY-NC 4.0 license.</abstract>
      <url hash="c453bca2">2022.lrec-1.741</url>
      <bibkey>kimura-etal-2022-ssr7000</bibkey>
      <pwccode url="https://github.com/supernaiter/ssr7000" additional="false">supernaiter/ssr7000</pwccode>
    </paper>
    <paper id="742">
      <title>A Simple Yet Effective Corpus Construction Method for <fixed-case>C</fixed-case>hinese Sentence Compression</title>
      <author><first>Yang</first><last>Zhao</last></author>
      <author><first>Hiroshi</first><last>Kanayama</last></author>
      <author><first>Issei</first><last>Yoshida</last></author>
      <author><first>Masayasu</first><last>Muraoka</last></author>
      <author><first>Akiko</first><last>Aizawa</last></author>
      <pages>6874–6883</pages>
      <abstract>Deletion-based sentence compression in the English language has made significant progress over the past few decades. However, there is a lack of large-scale and high-quality parallel corpus (i.e., (sentence, compression) pairs) for the Chinese language to train an efficient compression system. To remedy this shortcoming, we present a dependency-tree-based method to construct a Chinese corpus with 151k pairs of sentences and compression based on Chinese language-specific characteristics. Subsequently, we trained both extractive and generative neural compression models using the constructed corpus. The experimental results show that our compression model can generate high-quality compressed sentences on both automatic and human evaluation metrics compared with the baselines. The results of the faithfulness evaluation also indicated that the Chinese compression model trained on our constructed corpus can produce more faithful compressed sentences. Furthermore, a dataset with 1,000 pairs of sentences and ground truth compression was manually created for automatic evaluation, which, we believe, will benefit future research on Chinese sentence compression.</abstract>
      <url hash="138a2632">2022.lrec-1.742</url>
      <bibkey>zhao-etal-2022-simple</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ocnli">OCNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sentence-compression">Sentence Compression</pwcdataset>
    </paper>
    <paper id="743">
      <title><fixed-case>JADE</fixed-case>: Corpus for <fixed-case>J</fixed-case>apanese Definition Modelling</title>
      <author><first>Han</first><last>Huang</last></author>
      <author><first>Tomoyuki</first><last>Kajiwara</last></author>
      <author><first>Yuki</first><last>Arase</last></author>
      <pages>6884–6888</pages>
      <abstract>This study investigated and released the JADE, a corpus for Japanese definition modelling, which is a technique that automatically generates definitions of a given target word and phrase. It is a crucial technique for practical applications that assist language learning and education, as well as for those supporting reading documents in unfamiliar domains. Although corpora for development of definition modelling techniques have been actively created, their languages are mostly limited to English. In this study, a corpus for Japanese, named JADE, was created following the previous study that mines an online encyclopedia. The JADE provides about 630k sets of targets, their definitions, and usage examples as contexts for about 41k unique targets, which is sufficiently large to train neural models. The targets are both words and phrases, and the coverage of domains and topics is diverse. The performance of a pre-trained sequence-to-sequence model and the state-of-the-art definition modelling method was also benchmarked on JADE for future development of the technique in Japanese. The JADE corpus has been released and available online.</abstract>
      <url hash="ee3695fc">2022.lrec-1.743</url>
      <bibkey>huang-etal-2022-jade</bibkey>
    </paper>
    <paper id="744">
      <title>Unraveling the Mystery of Artifacts in Machine Generated Text</title>
      <author><first>Jiashu</first><last>Pu</last></author>
      <author><first>Ziyi</first><last>Huang</last></author>
      <author><first>Yadong</first><last>Xi</last></author>
      <author><first>Guandan</first><last>Chen</last></author>
      <author><first>Weijie</first><last>Chen</last></author>
      <author><first>Rongsheng</first><last>Zhang</last></author>
      <pages>6889–6898</pages>
      <abstract>As neural Text Generation Models (TGM) have become more and more capable of generating text indistinguishable from human-written ones, the misuse of text generation technologies can have serious ramifications. Although a neural classifier often achieves high detection accuracy, the reason for it is not well studied. Most previous work revolves around studying the impact of model structure and the decoding strategy on ease of detection, but little work has been done to analyze the forms of artifacts left by the TGM. We propose to systematically study the forms and scopes of artifacts by corrupting texts, replacing them with linguistic or statistical features, and applying the interpretable method of Integrated Gradients. Comprehensive experiments show artifacts a) primarily relate to token co-occurrence, b) feature more heavily at the head of vocabulary, c) appear more in content word than stopwords, d) are sometimes detrimental in the form of number of token occurrences, e) are less likely to exist in high-level semantics or syntaxes, f) manifest in low concreteness values for higher-order n-grams.</abstract>
      <url hash="5a0e9341">2022.lrec-1.744</url>
      <bibkey>pu-etal-2022-unraveling</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
    </paper>
    <paper id="745">
      <title>Logic-Guided Message Generation from Raw Real-Time Sensor Data</title>
      <author><first>Ernie</first><last>Chang</last></author>
      <author><first>Alisa</first><last>Kovtunova</last></author>
      <author><first>Stefan</first><last>Borgwardt</last></author>
      <author><first>Vera</first><last>Demberg</last></author>
      <author><first>Kathryn</first><last>Chapman</last></author>
      <author><first>Hui-Syuan</first><last>Yeh</last></author>
      <pages>6899–6908</pages>
      <abstract>Natural language generation in real-time settings with raw sensor data is a challenging task. We find that formulating the task as an end-to-end problem leads to two major challenges in content selection – the sensor data is both redundant and diverse across environments, thereby making it hard for the encoders to select and reason on the data. We here present a new corpus for a specific domain that instantiates these properties. It includes handover utterances that an assistant for a semi-autonomous drone uses to communicate with humans during the drone flight. The corpus consists of sensor data records and utterances in 8 different environments. As a structured intermediary representation between data records and text, we explore the use of description logic (DL). We also propose a neural generation model that can alert the human pilot of the system state and environment in preparation of the handover of control.</abstract>
      <url hash="2cb285d9">2022.lrec-1.745</url>
      <bibkey>chang-etal-2022-logic</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/e2e">E2E</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikibio">WikiBio</pwcdataset>
    </paper>
    <paper id="746">
      <title>The Bull and the Bear: Summarizing Stock Market Discussions</title>
      <author><first>Ayush</first><last>Kumar</last></author>
      <author><first>Dhyey</first><last>Jani</last></author>
      <author><first>Jay</first><last>Shah</last></author>
      <author><first>Devanshu</first><last>Thakar</last></author>
      <author><first>Varun</first><last>Jain</last></author>
      <author><first>Mayank</first><last>Singh</last></author>
      <pages>6909–6913</pages>
      <abstract>Stock market investors debate and heavily discuss stock ideas, investing strategies, news and market movements on social media platforms. The discussions are significantly longer in length and require extensive domain expertise for understanding. In this paper, we curate such discussions and construct a first-of-its-kind of abstractive summarization dataset. Our curated dataset consists of 7888 Reddit posts and manually constructed summaries for 400 posts. We robustly evaluate the summaries and conduct experiments on SOTA summarization tools to showcase their limitations. We plan to make the dataset publicly available. The sample dataset is available here: https://dhyeyjani.github.io/RSMC</abstract>
      <url hash="50bc194e">2022.lrec-1.746</url>
      <bibkey>kumar-etal-2022-bull</bibkey>
    </paper>
    <paper id="747">
      <title>Combination of Contextualized and Non-Contextualized Layers for Lexical Substitution in <fixed-case>F</fixed-case>rench</title>
      <author><first>Kévin</first><last>Espasa</last></author>
      <author><first>Emmanuel</first><last>Morin</last></author>
      <author><first>Olivier</first><last>Hamon</last></author>
      <pages>6914–6921</pages>
      <abstract>Lexical substitution task requires to substitute a target word by candidates in a given context. Candidates must keep meaning and grammatically of the sentence. The task, introduced in the SemEval 2007, has two objectives. The first objective is to find a list of substitutes for a target word. This list of substitutes can be obtained with lexical resources like WordNet or generated with a pre-trained language model. The second objective is to rank these substitutes using the context of the sentence. Most of the methods use vector space models or more recently embeddings to rank substitutes. Embedding methods use high contextualized representation. This representation can be over contextualized and in this way overlook good substitute candidates which are more similar on non-contextualized layers. SemDis 2014 introduced the lexical substitution task in French. We propose an application of the state-of-the-art method based on BERT in French and a novel method using contextualized and non-contextualized layers to increase the suggestion of words having a lower probability in a given context but that are more semantically similar. Experiments show our method increases the BERT based system on the OOT measure but decreases on the BEST measure in the SemDis 2014 benchmark.</abstract>
      <url hash="e8db0d83">2022.lrec-1.747</url>
      <bibkey>espasa-etal-2022-combination</bibkey>
    </paper>
    <paper id="748">
      <title><fixed-case>S</fixed-case>u<fixed-case>M</fixed-case>e: A Dataset Towards Summarizing Biomedical Mechanisms</title>
      <author><first>Mohaddeseh</first><last>Bastan</last></author>
      <author><first>Nishant</first><last>Shankar</last></author>
      <author><first>Mihai</first><last>Surdeanu</last></author>
      <author><first>Niranjan</first><last>Balasubramanian</last></author>
      <pages>6922–6931</pages>
      <abstract>Can language models read biomedical texts and explain the biomedical mechanisms discussed? In this work we introduce a biomedical mechanism summarization task. Biomedical studies often investigate the mechanisms behind how one entity (e.g., a protein or a chemical) affects another in a biological context. The abstracts of these publications often include a focused set of sentences that present relevant supporting statements regarding such relationships, associated experimental evidence, and a concluding sentence that summarizes the mechanism underlying the relationship. We leverage this structure and create a summarization task, where the input is a collection of sentences and the main entities in an abstract, and the output includes the relationship and a sentence that summarizes the mechanism. Using a small amount of manually labeled mechanism sentences, we train a mechanism sentence classifier to filter a large biomedical abstract collection and create a summarization dataset with 22k instances. We also introduce conclusion sentence generation as a pretraining task with 611k instances. We benchmark the performance of large bio-domain language models. We find that while the pretraining task help improves performance, the best model produces acceptable mechanism outputs in only 32% of the instances, which shows the task presents significant challenges in biomedical language understanding and summarization.</abstract>
      <url hash="942510bb">2022.lrec-1.748</url>
      <bibkey>bastan-etal-2022-sume</bibkey>
      <pwccode url="https://github.com/StonyBrookNLP/SuMe" additional="true">StonyBrookNLP/SuMe</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/sume">SuMe</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/blue">BLUE</pwcdataset>
    </paper>
    <paper id="749">
      <title><fixed-case>CATAMARAN</fixed-case>: A Cross-lingual Long Text Abstractive Summarization Dataset</title>
      <author><first>Zheng</first><last>Chen</last></author>
      <author><first>Hongyu</first><last>Lin</last></author>
      <pages>6932–6937</pages>
      <abstract>Cross-lingual summarization, which produces the summary in one language from a given source document in another language, could be extremely helpful for humans to obtain information across the world. However, it is still a little-explored task due to the lack of datasets. Recent studies are primarily based on pseudo-cross-lingual datasets obtained by translation. Such an approach would inevitably lead to the loss of information in the original document and introduce noise into the summary, thus hurting the overall performance. In this paper, we present CATAMARAN, the first high-quality cross-lingual long text abstractive summarization dataset. It contains about 20,000 parallel news articles and corresponding summaries, all written by humans. The average lengths of articles are 1133.65 for English articles and 2035.33 for Chinese articles, and the average lengths of the summaries are 26.59 and 70.05, respectively. We train and evaluate an mBART-based cross-lingual abstractive summarization model using our dataset. The result shows that, compared with mono-lingual systems, the cross-lingual abstractive summarization system could also achieve solid performance.</abstract>
      <url hash="aaab0a4e">2022.lrec-1.749</url>
      <bibkey>chen-lin-2022-catamaran</bibkey>
    </paper>
    <paper id="750">
      <title>Emotion analysis and detection during <fixed-case>COVID</fixed-case>-19</title>
      <author><first>Tiberiu</first><last>Sosea</last></author>
      <author><first>Chau</first><last>Pham</last></author>
      <author><first>Alexander</first><last>Tekle</last></author>
      <author><first>Cornelia</first><last>Caragea</last></author>
      <author><first>Junyi Jessy</first><last>Li</last></author>
      <pages>6938–6947</pages>
      <abstract>Understanding emotions that people express during large-scale crises helps inform policy makers and first responders about the emotional states of the population as well as provide emotional support to those who need such support. We present CovidEmo, a dataset of ~3,000 English tweets labeled with emotions and temporally distributed across 18 months. Our analyses reveal the emotional toll caused by COVID-19, and changes of the social narrative and associated emotions over time. Motivated by the time-sensitive nature of crises and the cost of large-scale annotation efforts, we examine how well large pre-trained language models generalize across domains and timeline in the task of perceived emotion prediction in the context of COVID-19. Our analyses suggest that cross-domain information transfers occur, yet there are still significant gaps. We propose semi-supervised learning as a way to bridge this gap, obtaining significantly better performance using unlabeled data from the target domain.</abstract>
      <url hash="5de50221">2022.lrec-1.750</url>
      <bibkey>sosea-etal-2022-emotion</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/covidemo">COVIDEmo</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/goemotions">GoEmotions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hurricaneemo">HurricaneEmo</pwcdataset>
    </paper>
    <paper id="751">
      <title>Cross-lingual Emotion Detection</title>
      <author><first>Sabit</first><last>Hassan</last></author>
      <author><first>Shaden</first><last>Shaar</last></author>
      <author><first>Kareem</first><last>Darwish</last></author>
      <pages>6948–6958</pages>
      <abstract>Emotion detection can provide us with a window into understanding human behavior. Due to the complex dynamics of human emotions, however, constructing annotated datasets to train automated models can be expensive. Thus, we explore the efficacy of cross-lingual approaches that would use data from a source language to build models for emotion detection in a target language. We compare three approaches, namely: i) using inherently multilingual models; ii) translating training data into the target language; and iii) using an automatically tagged parallel corpus. In our study, we consider English as the source language with Arabic and Spanish as target languages. We study the effectiveness of different classification models such as BERT and SVMs trained with different features. Our BERT-based monolingual models that are trained on target language data surpass state-of-the-art (SOTA) by 4% and 5% absolute Jaccard score for Arabic and Spanish respectively. Next, we show that using cross-lingual approaches with English data alone, we can achieve more than 90% and 80% relative effectiveness of the Arabic and Spanish BERT models respectively. Lastly, we use LIME to analyze the challenges of training cross-lingual models for different language pairs.</abstract>
      <url hash="13f60a7d">2022.lrec-1.751</url>
      <bibkey>hassan-etal-2022-cross</bibkey>
    </paper>
    <paper id="752">
      <title><fixed-case>D</fixed-case>irect<fixed-case>Q</fixed-case>uote: A Dataset for Direct Quotation Extraction and Attribution in News Articles</title>
      <author><first>Yuanchi</first><last>Zhang</last></author>
      <author id="yang-liu-ict"><first>Yang</first><last>Liu</last></author>
      <pages>6959–6966</pages>
      <abstract>Quotation extraction and attribution are challenging tasks, aiming at determining the spans containing quotations and attributing each quotation to the original speaker. Applying this task to news data is highly related to fact-checking, media monitoring and news tracking. Direct quotations are more traceable and informative, and therefore of great significance among different types of quotations. Therefore, this paper introduces DirectQuote, a corpus containing 19,760 paragraphs and 10,279 direct quotations manually annotated from online news media. To the best of our knowledge, this is the largest and most complete corpus that focuses on direct quotations in news texts. We ensure that each speaker in the annotation can be linked to a specific named entity on Wikidata, benefiting various downstream tasks. In addition, for the first time, we propose several sequence labeling models as baseline methods to extract and attribute quotations simultaneously in an end-to-end manner.</abstract>
      <url hash="63987775">2022.lrec-1.752</url>
      <bibkey>zhang-liu-2022-directquote</bibkey>
      <pwccode url="https://github.com/thunlp-mt/directquote" additional="false">thunlp-mt/directquote</pwccode>
    </paper>
    <paper id="753">
      <title><fixed-case>V</fixed-case>accine<fixed-case>L</fixed-case>ies: A Natural Language Resource for Learning to Recognize Misinformation about the <fixed-case>COVID</fixed-case>-19 and <fixed-case>HPV</fixed-case> Vaccines</title>
      <author><first>Maxwell</first><last>Weinzierl</last></author>
      <author><first>Sanda</first><last>Harabagiu</last></author>
      <pages>6967–6975</pages>
      <abstract>Billions of COVID-19 vaccines have been administered, but many remain hesitant. Misinformation about the COVID-19 vaccines and other vaccines, propagating on social media, is believed to drive hesitancy towards vaccination. The ability to automatically recognize misinformation targeting vaccines on Twitter depends on the availability of data resources. In this paper we present VaccineLies, a large collection of tweets propagating misinformation about two vaccines: the COVID-19 vaccines and the Human Papillomavirus (HPV) vaccines. Misinformation targets are organized in vaccine-specific taxonomies, which reveal the misinformation themes and concerns. The ontological commitments of the misinformation taxonomies provide an understanding of which misinformation themes and concerns dominate the discourse about the two vaccines covered in VaccineLies. The organization into training, testing and development sets of VaccineLies invites the development of novel supervised methods for detecting misinformation on Twitter and identifying the stance towards it. Furthermore, VaccineLies can be a stepping stone for the development of datasets focusing on misinformation targeting additional vaccines.</abstract>
      <url hash="0ca4ba72">2022.lrec-1.753</url>
      <bibkey>weinzierl-harabagiu-2022-vaccinelies</bibkey>
      <pwccode url="https://github.com/Supermaxman/pytorch-gleam" additional="false">Supermaxman/pytorch-gleam</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/vaccinelies">VaccineLies</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/covaxlies-v1">CoVaxLies v1</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/covaxlies-v2">CoVaxLies v2</pwcdataset>
    </paper>
    <paper id="754">
      <title>Tackling Irony Detection using Ensemble Classifiers</title>
      <author><first>Christoph</first><last>Turban</last></author>
      <author><first>Udo</first><last>Kruschwitz</last></author>
      <pages>6976–6984</pages>
      <abstract>Automatic approaches to irony detection have been of interest to the NLP community for a long time, yet, state-of-the-art approaches still fall way short of what one would consider a desirable performance. In part this is due to the inherent difficulty of the problem. However, in recent years ensembles of transformer-based approaches have emerged as a promising direction to push the state of the art forward in a wide range of NLP applications. A different, more recent, development is the automatic augmentation of training data. In this paper we will explore both these directions for the task of irony detection in social media. Using the common SemEval 2018 Task 3 benchmark collection we demonstrate that transformer models are well suited in ensemble classifiers for the task at hand. In the multi-class classification task we observe statistically significant improvements over strong baselines. For binary classification we achieve performance that is on par with state-of-the-art alternatives. The examined data augmentation strategies showed an effect, but are not decisive for good results.</abstract>
      <url hash="f78b0f83">2022.lrec-1.754</url>
      <bibkey>turban-kruschwitz-2022-tackling</bibkey>
      <pwccode url="https://github.com/christophturban/lrec-irony-detection-ensemble-classifier" additional="false">christophturban/lrec-irony-detection-ensemble-classifier</pwccode>
    </paper>
    <paper id="755">
      <title>Automatic Construction of an Annotated Corpus with Implicit Aspects</title>
      <author><first>Aye</first><last>Aye Mar</last></author>
      <author><first>Kiyoaki</first><last>Shirai</last></author>
      <pages>6985–6991</pages>
      <abstract>Aspect-based sentiment analysis (ABSA) is a task that involves classifying the polarity of aspects of the products or services described in users’ reviews. Most previous work on ABSA has focused on explicit aspects, which appear as explicit words or phrases in the sentences of the review. However, users often express their opinions toward the aspects indirectly or implicitly, in which case the specific name of an aspect does not appear in the review. The current datasets used for ABSA are mainly annotated with explicit aspects. This paper proposes a novel method for constructing a corpus that is automatically annotated with implicit aspects. The main idea is that sentences containing explicit and implicit aspects share a similar context. First, labeled sentences with explicit aspects and unlabeled sentences that include implicit aspects are collected. Next, clustering is performed on these sentences so that similar sentences are merged into the same cluster. Finally, the explicit aspects are propagated to the unlabeled sentences in the same cluster, in order to construct a labeled dataset containing implicit aspects. The results of our experiments on mobile phone reviews show that our method of identifying the labels of implicit aspects achieves a maximum accuracy of 82%.</abstract>
      <url hash="1630561a">2022.lrec-1.755</url>
      <bibkey>aye-mar-shirai-2022-automatic</bibkey>
    </paper>
    <paper id="756">
      <title>A Multimodal Corpus for Emotion Recognition in Sarcasm</title>
      <author><first>Anupama</first><last>Ray</last></author>
      <author><first>Shubham</first><last>Mishra</last></author>
      <author><first>Apoorva</first><last>Nunna</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>6992–7003</pages>
      <abstract>While sentiment and emotion analysis have been studied extensively, the relationship between sarcasm and emotion has largely remained unexplored. A sarcastic expression may have a variety of underlying emotions. For example, “I love being ignored” belies sadness, while “my mobile is fabulous with a battery backup of only 15 minutes!” expresses frustration. Detecting the emotion behind a sarcastic expression is non-trivial yet an important task. We undertake the task of detecting the emotion in a sarcastic statement, which to the best of our knowledge, is hitherto unexplored. We start with the recently released multimodal sarcasm detection dataset (MUStARD) pre-annotated with 9 emotions. We identify and correct 343 incorrect emotion labels (out of 690). We double the size of the dataset, label it with emotions along with valence and arousal which are important indicators of emotional intensity. Finally, we label each sarcastic utterance with one of the four sarcasm types-Propositional, Embedded, Likeprefixed and Illocutionary, with the goal of advancing sarcasm detection research. Exhaustive experimentation with multimodal (text, audio, and video) fusion models establishes a benchmark for exact emotion recognition in sarcasm and outperforms the state-of-art sarcasm detection. We release the dataset enriched with various annotations and the code for research purposes: https://github.com/apoorva-nunna/MUStARD_Plus_Plus</abstract>
      <url hash="4d7bc8f2">2022.lrec-1.756</url>
      <bibkey>ray-etal-2022-multimodal</bibkey>
      <pwccode url="https://github.com/apoorva-nunna/mustard_plus_plus" additional="false">apoorva-nunna/mustard_plus_plus</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mustard-1">MUStARD++</pwcdataset>
    </paper>
    <paper id="757">
      <title>Annotation of Valence Unfolding in Spoken Personal Narratives</title>
      <author><first>Aniruddha</first><last>Tammewar</last></author>
      <author><first>Franziska</first><last>Braun</last></author>
      <author><first>Gabriel</first><last>Roccabruna</last></author>
      <author><first>Sebastian</first><last>Bayerl</last></author>
      <author><first>Korbinian</first><last>Riedhammer</last></author>
      <author><first>Giuseppe</first><last>Riccardi</last></author>
      <pages>7004–7013</pages>
      <abstract>Personal Narrative (PN) is the recollection of individuals’ life experiences, events, and thoughts along with the associated emotions in the form of a story. Compared to other genres such as social media texts or microblogs, where people write about experienced events or products, the spoken PNs are complex to analyze and understand. They are usually long and unstructured, involving multiple and related events, characters as well as thoughts and emotions associated with events, objects, and persons. In spoken PNs, emotions are conveyed by changing the speech signal characteristics as well as the lexical content of the narrative. In this work, we annotate a corpus of spoken personal narratives, with the emotion valence using discrete values. The PNs are segmented into speech segments, and the annotators annotate them in the discourse context, with values on a 5-point bipolar scale ranging from -2 to +2 (0 for neutral). In this way, we capture the unfolding of the PNs events and changes in the emotional state of the narrator. We perform an in-depth analysis of the inter-annotator agreement, the relation between the label distribution w.r.t. the stimulus (positive/negative) used for the elicitation of the narrative, and compare the segment-level annotations to a baseline continuous annotation. We find that the neutral score plays an important role in the agreement. We observe that it is easy to differentiate the positive from the negative valence while the confusion with the neutral label is high. Keywords: Personal Narratives, Emotion Annotation, Segment Level Annotation</abstract>
      <url hash="8b1c77ef">2022.lrec-1.757</url>
      <bibkey>tammewar-etal-2022-annotation</bibkey>
    </paper>
    <paper id="758">
      <title>A Large-Scale <fixed-case>J</fixed-case>apanese Dataset for Aspect-based Sentiment Analysis</title>
      <author><first>Yuki</first><last>Nakayama</last></author>
      <author><first>Koji</first><last>Murakami</last></author>
      <author><first>Gautam</first><last>Kumar</last></author>
      <author><first>Sudha</first><last>Bhingardive</last></author>
      <author><first>Ikuko</first><last>Hardaway</last></author>
      <pages>7014–7021</pages>
      <abstract>There has been significant progress in the field of sentiment analysis. However, aspect-based sentiment analysis (ABSA) has not been explored in the Japanese language even though it has a huge scope in many natural language processing applications such as 1) tracking sentiment towards products, movies, politicians etc; 2) improving customer relation models. The main reason behind this is that there is no standard Japanese dataset available for ABSA task. In this paper, we present the first standard Japanese dataset for the hotel reviews domain. The proposed dataset contains 53,192 review sentences with seven aspect categories and two polarity labels. We perform experiments on this dataset using popular ABSA approaches and report error analysis. Our experiments show that contextual models such as BERT works very well for the ABSA task in the Japanese language and also show the need to focus on other NLP tasks for better performance through our error analysis.</abstract>
      <url hash="8154a078">2022.lrec-1.758</url>
      <bibkey>nakayama-etal-2022-large</bibkey>
    </paper>
    <paper id="759">
      <title>A <fixed-case>J</fixed-case>apanese Dataset for Subjective and Objective Sentiment Polarity Classification in Micro Blog Domain</title>
      <author><first>Haruya</first><last>Suzuki</last></author>
      <author><first>Yuto</first><last>Miyauchi</last></author>
      <author><first>Kazuki</first><last>Akiyama</last></author>
      <author><first>Tomoyuki</first><last>Kajiwara</last></author>
      <author><first>Takashi</first><last>Ninomiya</last></author>
      <author><first>Noriko</first><last>Takemura</last></author>
      <author><first>Yuta</first><last>Nakashima</last></author>
      <author><first>Hajime</first><last>Nagahara</last></author>
      <pages>7022–7028</pages>
      <abstract>We annotate 35,000 SNS posts with both the writer’s subjective sentiment polarity labels and the reader’s objective ones to construct a Japanese sentiment analysis dataset. Our dataset includes intensity labels (<i>none</i>, <i>weak</i>, <i>medium</i>, and <i>strong</i>) for each of the eight basic emotions by Plutchik (<i>joy</i>, <i>sadness</i>, <i>anticipation</i>, <i>surprise</i>, <i>anger</i>, <i>fear</i>, <i>disgust</i>, and <i>trust</i>) as well as sentiment polarity labels (<i>strong positive</i>, <i>positive</i>, <i>neutral</i>, <i>negative</i>, and <i>strong negative</i>). Previous studies on emotion analysis have studied the analysis of basic emotions and sentiment polarity independently. In other words, there are few corpora that are annotated with both basic emotions and sentiment polarity. Our dataset is the first large-scale corpus to annotate both of these emotion labels, and from both the writer’s and reader’s perspectives. In this paper, we analyze the relationship between basic emotion intensity and sentiment polarity on our dataset and report the results of benchmarking sentiment polarity classification.</abstract>
      <url hash="bab76bf6">2022.lrec-1.759</url>
      <bibkey>suzuki-etal-2022-japanese</bibkey>
      <pwccode url="https://github.com/ids-cv/wrime" additional="false">ids-cv/wrime</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="760">
      <title>Complementary Learning of Aspect Terms for Aspect-based Sentiment Analysis</title>
      <author><first>Han</first><last>Qin</last></author>
      <author><first>Yuanhe</first><last>Tian</last></author>
      <author><first>Fei</first><last>Xia</last></author>
      <author><first>Yan</first><last>Song</last></author>
      <pages>7029–7039</pages>
      <abstract>Aspect-based sentiment analysis (ABSA) aims to predict the sentiment polarity towards a given aspect term in a sentence on the fine-grained level, which usually requires a good understanding of contextual information, especially appropriately distinguishing of a given aspect and its contexts, to achieve good performance. However, most existing ABSA models pay limited attention to the modeling of the given aspect terms and thus result in inferior results when a sentence contains multiple aspect terms with contradictory sentiment polarities. In this paper, we propose to improve ABSA by complementary learning of aspect terms, which serves as a supportive auxiliary task to enhance ABSA by explicitly recovering the aspect terms from each input sentence so as to better understand aspects and their contexts. Particularly, a discriminator is also introduced to further improve the learning process by appropriately balancing the impact of aspect recovery to sentiment prediction. Experimental results on five widely used English benchmark datasets for ABSA demonstrate the effectiveness of our approach, where state-of-the-art performance is observed on all datasets.</abstract>
      <url hash="0be22ad5">2022.lrec-1.760</url>
      <bibkey>qin-etal-2022-complementary</bibkey>
      <pwccode url="https://github.com/synlp/asa-cld" additional="false">synlp/asa-cld</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mams">MAMS</pwcdataset>
    </paper>
    <paper id="761">
      <title>Deep One-Class Hate Speech Detection Model</title>
      <author><first>Saugata</first><last>Bose</last></author>
      <author><first>Dr. Guoxin</first><last>Su</last></author>
      <pages>7040–7048</pages>
      <abstract>Hate speech detection for social media posts is considered as a binary classification problem in existing approaches, largely neglecting distinct attributes of hate speeches from other sentimental types such as “aggressive” and “racist”. As these sentimental types constitute a significant major portion of data, the classification performance is compromised. Moreover, those classifiers often do not generalize well across different datasets due to a relatively small number of hate-class samples. In this paper, we adopt a one-class perspective for hate speech detection, where the detection classifier is trained with hate-class samples only. Our model employs a BERT-BiLSTM module for feature extraction and a one-class SVM for classification. A comprehensive evaluation with four benchmarking datasets demonstrates the better performance of our model than existing approaches, as well as the advantage of training our model with a combination of the four datasets.</abstract>
      <url hash="4d42947b">2022.lrec-1.761</url>
      <bibkey>bose-su-2022-deep</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/hate-speech">Hate Speech</pwcdataset>
    </paper>
    <paper id="762">
      <title>Opinions in Interactions : New Annotations of the <fixed-case>SEMAINE</fixed-case> Database</title>
      <author><first>Valentin</first><last>Barriere</last></author>
      <author><first>Slim</first><last>Essid</last></author>
      <author><first>Chloé</first><last>Clavel</last></author>
      <pages>7049–7055</pages>
      <abstract>In this paper, we present the process we used in order to collect new annotations of opinions over the multimodal corpus SEMAINE composed of dyadic interactions. The dataset had already been annotated continuously in two affective dimensions related to the emotions: Valence and Arousal. We annotated the part of SEMAINE called <i>Solid SAL</i> composed of 79 interactions between a user and an operator playing the role of a virtual agent designed to engage a person in a sustained, emotionally colored conversation. We aligned the audio at the word level using the available high-quality manual transcriptions. The annotated dataset contains 5627 speech turns for a total of 73,944 words, corresponding to 6 hours 20 minutes of dyadic interactions. Each interaction has been labeled by three annotators at the speech turn level following a three-step process. This method allows us to obtain a precise annotation regarding the opinion of a speaker. We obtain thus a dataset dense in opinions, with more than 48% of the annotated speech turns containing at least one opinion. We then propose a new baseline for the detection of opinions in interactions improving slightly a state of the art model with RoBERTa embeddings. The obtained results on the database are promising with a F1-score at 0.72.</abstract>
      <url hash="fbd5cd59">2022.lrec-1.762</url>
      <bibkey>barriere-etal-2022-opinions</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/semaine">SEMAINE</pwcdataset>
    </paper>
    <paper id="763">
      <title>Pars-<fixed-case>ABSA</fixed-case>: a Manually Annotated Aspect-based Sentiment Analysis Benchmark on <fixed-case>F</fixed-case>arsi Product Reviews</title>
      <author><first>Taha</first><last>Shangipour ataei</last></author>
      <author><first>Kamyar</first><last>Darvishi</last></author>
      <author><first>Soroush</first><last>Javdan</last></author>
      <author><first>Behrouz</first><last>Minaei-Bidgoli</last></author>
      <author><first>Sauleh</first><last>Eetemadi</last></author>
      <pages>7056–7060</pages>
      <abstract>Due to the increased availability of online reviews, sentiment analysis witnessed a thriving interest from researchers. Sentiment analysis is a computational treatment of sentiment used to extract and understand the opinions of authors. While many systems were built to predict the sentiment of a document or a sentence, many others provide the necessary detail on various aspects of the entity (i.e., aspect-based sentiment analysis). Most of the available data resources were tailored to English and the other popular European languages. Although Farsi is a language with more than 110 million speakers, to the best of our knowledge, there is a lack of proper public datasets on aspect-based sentiment analysis for Farsi. This paper provides a manually annotated Farsi dataset, Pars-ABSA, annotated and verified by three native Farsi speakers. The dataset consists of 5,114 positive, 3,061 negative and 1,827 neutral data samples from 5,602 unique reviews. Moreover, as a baseline, this paper reports the performance of some aspect-based sentiment analysis methods focusing on transfer learning on Pars-ABSA.</abstract>
      <url hash="14760468">2022.lrec-1.763</url>
      <bibkey>shangipour-ataei-etal-2022-pars</bibkey>
      <pwccode url="https://github.com/Titowak/Pars-ABSA" additional="false">Titowak/Pars-ABSA</pwccode>
    </paper>
    <paper id="764">
      <title><fixed-case>H</fixed-case>indi<fixed-case>MD</fixed-case>: A Multi-domain Corpora for Low-resource Sentiment Analysis</title>
      <author><first>Mamta</first><last>.</last></author>
      <author><first>Asif</first><last>Ekbal</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <author><first>Tista</first><last>Saha</last></author>
      <author><first>Alka</first><last>Kumar</last></author>
      <author><first>Shikha</first><last>Srivastava</last></author>
      <pages>7061–7070</pages>
      <abstract>Social media platforms such as Twitter have evolved into a vast information sharing platform, allowing people from a variety of backgrounds and expertise to share their opinions on numerous events such as terrorism, narcotics and many other social issues. People sometimes misuse the power of social media for their agendas, such as illegal trades and negatively influencing others. Because of this, sentiment analysis has won the interest of a lot of researchers to widely analyze public opinion for social media monitoring. Several benchmark datasets for sentiment analysis across a range of domains have been made available, especially for high-resource languages. A few datasets are available for low-resource Indian languages like Hindi, such as movie reviews and product reviews, which do not address the current need for social media monitoring. In this paper, we address the challenges of sentiment analysis in Hindi and socially relevant domains by introducing a balanced corpus annotated with the sentiment classes, viz. positive, negative and neutral. To show the effective usage of the dataset, we build several deep learning based models and establish them as the baselines for further research in this direction.</abstract>
      <url hash="be487023">2022.lrec-1.764</url>
      <bibkey>-etal-2022-hindimd</bibkey>
    </paper>
    <paper id="765">
      <title>Sentiment Analysis of <fixed-case>H</fixed-case>omeric Text: The 1st Book of <fixed-case>I</fixed-case>liad</title>
      <author><first>John</first><last>Pavlopoulos</last></author>
      <author><first>Alexandros</first><last>Xenos</last></author>
      <author><first>Davide</first><last>Picca</last></author>
      <pages>7071–7077</pages>
      <abstract>Sentiment analysis studies are focused more on online customer reviews or social media, and less on literary studies. The problem is greater for ancient languages, where the linguistic expression of sentiments may diverge from modern linguistic forms. This work presents the outcome of a sentiment annotation task of the first Book of Iliad, an ancient Greek poem. The annotators were provided with verses translated into modern Greek and they annotated the perceived emotions and sentiments verse by verse. By estimating the fraction of annotators that found a verse as belonging to a specific sentiment class, we model the poem’s perceived sentiment as a multi-variate time series. By experimenting with a state of the art deep learning masked language model, pre-trained on modern Greek and fine-tuned to estimate the sentiment of our data, we registered a mean squared error of 0.063. This low error indicates that sentiment estimators built on our dataset can potentially be used as mechanical annotators, hence facilitating the distant reading of Homeric text. Our dataset is released for public use.</abstract>
      <url hash="a234a1a5">2022.lrec-1.765</url>
      <bibkey>pavlopoulos-etal-2022-sentiment</bibkey>
    </paper>
    <paper id="766">
      <title>The <fixed-case>P</fixed-case>ersian Dependency Treebank Made Universal</title>
      <author><first>Pegah</first><last>Safari</last></author>
      <author><first>Mohammad Sadegh</first><last>Rasooli</last></author>
      <author><first>Amirsaeid</first><last>Moloodi</last></author>
      <author><first>Alireza</first><last>Nourian</last></author>
      <pages>7078–7087</pages>
      <abstract>We describe an automatic method for converting the Persian Dependency Treebank (Rasooli et al., 2013) to Universal Dependencies. This treebank contains 29107 sentences. Our experiments along with manual linguistic analysis show that our data is more compatible with Universal Dependencies than the Uppsala Persian Universal Dependency Treebank (Seraji et al., 2016), larger in size and more diverse in vocabulary. Our data brings in labeled attachment F-score of 85.2 in supervised parsing. Also, our delexicalized Persian-to-English parser transfer experiments show that a parsing model trained on our data is ≈2% absolutely more accurate than that of Seraji et al. (2016) in terms of labeled attachment score.</abstract>
      <url hash="c39cc20d">2022.lrec-1.766</url>
      <bibkey>safari-etal-2022-persian</bibkey>
      <pwccode url="https://github.com/UniversalDependencies/UD_Persian-PerDT" additional="false">UniversalDependencies/UD_Persian-PerDT</pwccode>
    </paper>
    <paper id="767">
      <title><fixed-case>G</fixed-case>uj<fixed-case>MORPH</fixed-case> - A Dataset for Creating <fixed-case>G</fixed-case>ujarati Morphological Analyzer</title>
      <author><first>Jatayu</first><last>Baxi</last></author>
      <author><first>Brijesh</first><last>Bhatt</last></author>
      <pages>7088–7095</pages>
      <abstract>Computational morphology deals with the processing of a language at the word level. A morphological analyzer is a key linguistic word-level tool that returns all the constituent morphemes and their grammatical categories associated with a particular word form. For the highly inflectional and low resource languages, the creation of computational morphology-related tools is a challenging task due to the unavailability of underlying key resources. In this paper, we discuss the creation of an annotated morphological dataset- GujMORPH for the Gujarati - an indo-aryan language. For the creation of this dataset, we studied language grammar, word formation rules, and suffix attachments in depth. This dataset contains 16,527 unique inflected words along with their morphological segmentation and grammatical feature tagging information. It is a first of its kind dataset for the Gujarati language and can be used to develop morphological analyzer and generator models. The dataset is annotated in the standard Unimorph schema and evaluated on the baseline system. We also describe the tool used to annotate the data in the standard format. The dataset is released publicly along with the library. Using this library, the data can be obtained in a format that can be directly used to train any machine learning model.</abstract>
      <url hash="851f44e3">2022.lrec-1.767</url>
      <bibkey>baxi-bhatt-2022-gujmorph</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="768">
      <title>Informal <fixed-case>P</fixed-case>ersian <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependency Treebank</title>
      <author><first>Roya</first><last>Kabiri</last></author>
      <author><first>Simin</first><last>Karimi</last></author>
      <author><first>Mihai</first><last>Surdeanu</last></author>
      <pages>7096–7105</pages>
      <abstract>This paper presents the phonological, morphological, and syntactic distinctions between formal and informal Persian, showing that these two variants have fundamental differences that cannot be attributed solely to pronunciation discrepancies. Given that informal Persian exhibits particular characteristics, any computational model trained on formal Persian is unlikely to transfer well to informal Persian, necessitating the creation of dedicated treebanks for this variety. We thus detail the development of the open-source Informal Persian Universal Dependency Treebank, a new treebank annotated within the Universal Dependencies scheme. We then investigate the parsing of informal Persian by training two dependency parsers on existing formal treebanks and evaluating them on out-of-domain data, i.e. the development set of our informal treebank. Our results show that parsers experience a substantial performance drop when we move across the two domains, as they face more unknown tokens and structures and fail to generalize well. Furthermore, the dependency relations whose performance deteriorates the most represent the unique properties of the informal variant. The ultimate goal of this study that demonstrates a broader impact is to provide a stepping-stone to reveal the significance of informal variants of languages, which have been widely overlooked in natural language processing tools across languages.</abstract>
      <url hash="b67ca3bb">2022.lrec-1.768</url>
      <bibkey>kabiri-etal-2022-informal</bibkey>
      <pwccode url="https://github.com/royakabiri/modified_seraji" additional="true">royakabiri/modified_seraji</pwccode>
    </paper>
    <paper id="769">
      <title>Automatic Correction of Syntactic Dependency Annotation Differences</title>
      <author><first>Andrew</first><last>Zupon</last></author>
      <author><first>Andrew</first><last>Carnie</last></author>
      <author><first>Michael</first><last>Hammond</last></author>
      <author><first>Mihai</first><last>Surdeanu</last></author>
      <pages>7106–7112</pages>
      <abstract>Annotation inconsistencies between data sets can cause problems for low-resource NLP, where noisy or inconsistent data cannot be easily replaced. We propose a method for automatically detecting annotation mismatches between dependency parsing corpora, along with three related methods for automatically converting the mismatches. All three methods rely on comparing unseen examples in a new corpus with similar examples in an existing corpus. These three methods include a simple lexical replacement using the most frequent tag of the example in the existing corpus, a GloVe embedding-based replacement that considers related examples, and a BERT-based replacement that uses contextualized embeddings to provide examples fine-tuned to our data. We evaluate these conversions by retraining two dependency parsers—Stanza and Parsing as Tagging (PaT)—on the converted and unconverted data. We find that applying our conversions yields significantly better performance in many cases. Some differences observed between the two parsers are observed. Stanza has a more complex architecture with a quadratic algorithm, taking longer to train, but it can generalize from less data. The PaT parser has a simpler architecture with a linear algorithm, speeding up training but requiring more training data to reach comparable or better performance.</abstract>
      <url hash="bb9180c9">2022.lrec-1.769</url>
      <bibkey>zupon-etal-2022-automatic</bibkey>
    </paper>
    <paper id="770">
      <title>Building Large-Scale <fixed-case>J</fixed-case>apanese Pronunciation-Annotated Corpora for Reading Heteronymous Logograms</title>
      <author><first>Fumikazu</first><last>Sato</last></author>
      <author><first>Naoki</first><last>Yoshinaga</last></author>
      <author><first>Masaru</first><last>Kitsuregawa</last></author>
      <pages>7113–7121</pages>
      <abstract>Although screen readers enable visually impaired people to read written text via speech, the ambiguities in pronunciations of heteronyms cause wrong reading, which has a serious impact on the text understanding. Especially in Japanese, there are many common heteronyms expressed by logograms (Chinese characters or kanji) that have totally different pronunciations (and meanings). In this study, to improve the accuracy of pronunciation prediction, we construct two large-scale Japanese corpora that annotate kanji characters with their pronunciations. Using existing language resources on i) book titles compiled by the National Diet Library and ii) the books in a Japanese digital library called Aozora Bunko and their Braille translations, we develop two large-scale pronunciation-annotated corpora for training pronunciation prediction models. We first extract sentence-level alignments between the Aozora Bunko text and its pronunciation converted from the Braille data. We then perform dictionary-based pattern matching based on morphological dictionaries to find word-level pronunciation alignments. We have ultimately obtained the Book Title corpus with 336M characters (16.4M book titles) and the Aozora Bunko corpus with 52M characters (1.6M sentences). We analyzed pronunciation distributions for 203 common heteronyms, and trained a BERT-based pronunciation prediction model for 93 heteronyms, which achieved an average accuracy of 0.939.</abstract>
      <url hash="2ba1f682">2022.lrec-1.770</url>
      <bibkey>sato-etal-2022-building</bibkey>
    </paper>
    <paper id="771">
      <title><fixed-case>S</fixed-case>tyle<fixed-case>KQC</fixed-case>: A Style-Variant Paraphrase Corpus for <fixed-case>K</fixed-case>orean Questions and Commands</title>
      <author><first>Won Ik</first><last>Cho</last></author>
      <author><first>Sangwhan</first><last>Moon</last></author>
      <author><first>Jongin</first><last>Kim</last></author>
      <author><first>Seokmin</first><last>Kim</last></author>
      <author><first>Nam Soo</first><last>Kim</last></author>
      <pages>7122–7128</pages>
      <abstract>Paraphrasing is often performed with less concern for controlled style conversion. Especially for questions and commands, style-variant paraphrasing can be crucial in tone and manner, which also matters with industrial applications such as dialog systems. In this paper, we attack this issue with a corpus construction scheme that simultaneously considers the core content and style of directives, namely intent and formality, for the Korean language. Utilizing manually generated natural language queries on six daily topics, we expand the corpus to formal and informal sentences by human rewriting and transferring. We verify the validity and industrial applicability of our approach by checking the adequate classification and inference performance that fit with conventional fine-tuning approaches, at the same time proposing a supervised formality transfer task.</abstract>
      <url hash="b2004038">2022.lrec-1.771</url>
      <bibkey>cho-etal-2022-stylekqc</bibkey>
      <pwccode url="https://github.com/cynthia/stylekqc" additional="false">cynthia/stylekqc</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/stylekqc">StyleKQC</pwcdataset>
    </paper>
    <paper id="772">
      <title>Syntax-driven Approach for Semantic Role Labeling</title>
      <author><first>Yuanhe</first><last>Tian</last></author>
      <author><first>Han</first><last>Qin</last></author>
      <author><first>Fei</first><last>Xia</last></author>
      <author><first>Yan</first><last>Song</last></author>
      <pages>7129–7139</pages>
      <abstract>As an important task to analyze the semantic structure of a sentence, semantic role labeling (SRL) aims to locate the semantic role (e.g., agent) of noun phrases with respect to a given predicate and thus plays an important role in downstream tasks such as dialogue systems. To achieve a better performance in SRL, a model is always required to have a good understanding of the context information. Although one can use advanced text encoder (e.g., BERT) to capture the context information, extra resources are also required to further improve the model performance. Considering that there are correlations between the syntactic structure and the semantic structure of the sentence, many previous studies leverage auto-generated syntactic knowledge, especially the dependencies, to enhance the modeling of context information through graph-based architectures, where limited attention is paid to other types of auto-generated knowledge. In this paper, we propose map memories to enhance SRL by encoding different types of auto-generated syntactic knowledge (i.e., POS tags, syntactic constituencies, and word dependencies) obtained from off-the-shelf toolkits. Experimental results on two English benchmark datasets for span-style SRL (i.e., CoNLL-2005 and CoNLL-2012) demonstrate the effectiveness of our approach, which outperforms strong baselines and achieves state-of-the-art results on CoNLL-2005.</abstract>
      <url hash="723aed18">2022.lrec-1.772</url>
      <bibkey>tian-etal-2022-syntax</bibkey>
      <pwccode url="https://github.com/synlp/srl-mm" additional="false">synlp/srl-mm</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ontonotes-5-0">OntoNotes 5.0</pwcdataset>
    </paper>
    <paper id="773">
      <title><fixed-case>H</fixed-case>er<fixed-case>BERT</fixed-case> Based Language Model Detects Quantifiers and Their Semantic Properties in <fixed-case>P</fixed-case>olish</title>
      <author><first>Marcin</first><last>Woliński</last></author>
      <author><first>Bartłomiej</first><last>Nitoń</last></author>
      <author><first>Witold</first><last>Kieraś</last></author>
      <author><first>Jakub</first><last>Szymanik</last></author>
      <pages>7140–7146</pages>
      <abstract>The paper presents a tool for automatic marking up of quantifying expressions, their semantic features, and scopes. We explore the idea of using a BERT based neural model for the task (in this case HerBERT, a model trained specifically for Polish, is used). The tool is trained on a recent manually annotated Corpus of Polish Quantificational Expressions (Szymanik and Kieraś, 2022). We discuss how it performs against human annotation and present results of automatic annotation of 300 million sub-corpus of National Corpus of Polish. Our results show that language models can effectively recognise semantic category of quantification as well as identify key semantic properties of quantifiers, like monotonicity. Furthermore, the algorithm we have developed can be used for building semantically annotated quantifier corpora for other languages.</abstract>
      <url hash="1aff5e9f">2022.lrec-1.773</url>
      <bibkey>wolinski-etal-2022-herbert</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/klej">KLEJ</pwcdataset>
    </paper>
    <paper id="774">
      <title>Lexical Resource Mapping via Translations</title>
      <author><first>Hongchang</first><last>Bao</last></author>
      <author><first>Bradley</first><last>Hauer</last></author>
      <author><first>Grzegorz</first><last>Kondrak</last></author>
      <pages>7147–7154</pages>
      <abstract>Aligning lexical resources that associate words with concepts in multiple languages increases the total amount of semantic information that can be leveraged for various NLP tasks. We present a translation-based approach to mapping concepts across diverse resources. Our methods depend only on multilingual lexicalization information. When applied to align WordNet/BabelNet to CLICS and OmegaWiki, our methods achieve state-of-the-art accuracy, without any dependence on other sources of semantic knowledge. Since each word-concept pair corresponds to a unique sense of the word, we also demonstrate that the mapping task can be framed as word sense disambiguation. To facilitate future work, we release a set of high-precision WordNet-CLICS alignments, produced by combining three different mapping methods.</abstract>
      <url hash="dadd4758">2022.lrec-1.774</url>
      <bibkey>bao-etal-2022-lexical</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/concepticon">Concepticon</pwcdataset>
    </paper>
    <paper id="775">
      <title>Unsupervised Attention-based Sentence-Level Meta-Embeddings from Contextualised Language Models</title>
      <author><first>Keigo</first><last>Takahashi</last></author>
      <author><first>Danushka</first><last>Bollegala</last></author>
      <pages>7155–7163</pages>
      <abstract>A variety of contextualised language models have been proposed in the NLP community, which are trained on diverse corpora to produce numerous Neural Language Models (NLMs). However, different NLMs have reported different levels of performances in downstream NLP applications when used as text representations. We propose a sentence-level meta-embedding learning method that takes independently trained contextualised word embedding models and learns a sentence embedding that preserves the complementary strengths of the input source NLMs. Our proposed method is unsupervised and is not tied to a particular downstream task, which makes the learnt meta-embeddings in principle applicable to different tasks that require sentence representations. Specifically, we first project the token-level embeddings obtained by the individual NLMs and learn attention weights that indicate the contributions of source embeddings towards their token-level meta-embeddings. Next, we apply mean and max pooling to produce sentence-level meta-embeddings from token-level meta-embeddings. Experimental results on semantic textual similarity benchmarks show that our proposed unsupervised sentence-level meta-embedding method outperforms previously proposed sentence-level meta-embedding methods as well as a supervised baseline.</abstract>
      <url hash="be11146e">2022.lrec-1.775</url>
      <bibkey>takahashi-bollegala-2022-unsupervised</bibkey>
    </paper>
    <paper id="776">
      <title>Identification of Fine-Grained Location Mentions in Crisis Tweets</title>
      <author><first>Sarthak</first><last>Khanal</last></author>
      <author><first>Maria</first><last>Traskowsky</last></author>
      <author><first>Doina</first><last>Caragea</last></author>
      <pages>7164–7173</pages>
      <abstract>Identification of fine-grained location mentions in crisis tweets is central in transforming situational awareness information extracted from social media into actionable information. Most prior works have focused on identifying generic locations, without considering their specific types. To facilitate progress on the fine-grained location identification task, we assemble two tweet crisis datasets and manually annotate them with specific location types. The first dataset contains tweets from a mixed set of crisis events, while the second dataset contains tweets from the global COVID-19 pandemic. We investigate the performance of state-of-the-art deep learning models for sequence tagging on these datasets, in both in-domain and cross-domain settings.</abstract>
      <url hash="970f560d">2022.lrec-1.776</url>
      <bibkey>khanal-etal-2022-identification</bibkey>
    </paper>
    <paper id="777">
      <title><fixed-case>H</fixed-case>ate<fixed-case>BR</fixed-case>: A Large Expert Annotated Corpus of <fixed-case>B</fixed-case>razilian <fixed-case>I</fixed-case>nstagram Comments for Offensive Language and Hate Speech Detection</title>
      <author><first>Francielle</first><last>Vargas</last></author>
      <author><first>Isabelle</first><last>Carvalho</last></author>
      <author><first>Fabiana</first><last>Rodrigues de Góes</last></author>
      <author><first>Thiago</first><last>Pardo</last></author>
      <author><first>Fabrício</first><last>Benevenuto</last></author>
      <pages>7174–7183</pages>
      <abstract>Due to the severity of the social media offensive and hateful comments in Brazil, and the lack of research in Portuguese, this paper provides the first large-scale expert annotated corpus of Brazilian Instagram comments for hate speech and offensive language detection. The HateBR corpus was collected from the comment section of Brazilian politicians’ accounts on Instagram and manually annotated by specialists, reaching a high inter-annotator agreement. The corpus consists of 7,000 documents annotated according to three different layers: a binary classification (offensive versus non-offensive comments), offensiveness-level classification (highly, moderately, and slightly offensive), and nine hate speech groups (xenophobia, racism, homophobia, sexism, religious intolerance, partyism, apology for the dictatorship, antisemitism, and fatphobia). We also implemented baseline experiments for offensive language and hate speech detection and compared them with a literature baseline. Results show that the baseline experiments on our corpus outperform the current state-of-the-art for the Portuguese language.</abstract>
      <url hash="f30c59e0">2022.lrec-1.777</url>
      <bibkey>vargas-etal-2022-hatebr</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/hate-speech-and-offensive-language">Hate Speech and Offensive Language</pwcdataset>
    </paper>
    <paper id="778">
      <title><fixed-case>M</fixed-case>ental<fixed-case>BERT</fixed-case>: Publicly Available Pretrained Language Models for Mental Healthcare</title>
      <author><first>Shaoxiong</first><last>Ji</last></author>
      <author><first>Tianlin</first><last>Zhang</last></author>
      <author><first>Luna</first><last>Ansari</last></author>
      <author><first>Jie</first><last>Fu</last></author>
      <author><first>Prayag</first><last>Tiwari</last></author>
      <author><first>Erik</first><last>Cambria</last></author>
      <pages>7184–7190</pages>
      <abstract>Mental health is a critical issue in modern society, and mental disorders could sometimes turn to suicidal ideation without adequate treatment. Early detection of mental disorders and suicidal ideation from social content provides a potential way for effective social intervention. Recent advances in pretrained contextualized language representations have promoted the development of several domainspecific pretrained models and facilitated several downstream applications. However, there are no existing pretrained language models for mental healthcare. This paper trains and release two pretrained masked language models, i.e., MentalBERT and MentalRoBERTa, to benefit machine learning for the mental healthcare research community. Besides, we evaluate our trained domain-specific models and several variants of pretrained language models on several mental disorder detection benchmarks and demonstrate that language representations pretrained in the target domain improve the performance of mental health detection tasks.</abstract>
      <url hash="930904d6">2022.lrec-1.778</url>
      <bibkey>ji-etal-2022-mentalbert</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/dreaddit">Dreaddit</pwcdataset>
    </paper>
    <paper id="779">
      <title>Leveraging Hashtag Networks for Multimodal Popularity Prediction of <fixed-case>I</fixed-case>nstagram Posts</title>
      <author><first>Yu Yun</first><last>Liao</last></author>
      <pages>7191–7198</pages>
      <abstract>With the increasing commercial and social importance of Instagram in recent years, more researchers begin to take multimodal approaches to predict popular content on Instagram. However, existing popularity prediction approaches often reduce hashtags to simple features such as hashtag length or number of hashtags in a post, ignoring the structural and textual information that entangles between hashtags. In this paper, we propose a multimodal framework using post captions, image, hashtag network, and topic model to predict popular influencer posts in Taiwan. Specifically, the hashtag network is constructed as a homogenous graph using the co-occurrence relationship between hashtags, and we extract its structural information with GraphSAGE and semantic information with BERTopic. Finally, the prediction process is defined as a binary classification task (popular/unpopular) using neural networks. Our results show that the proposed framework incorporating hashtag network outperforms all baselines and unimodal models, while information captured from the hashtag network and topic model appears to be complementary.</abstract>
      <url hash="83bc56ac">2022.lrec-1.779</url>
      <bibkey>liao-2022-leveraging</bibkey>
    </paper>
    <paper id="780">
      <title>Annotating the <fixed-case>T</fixed-case>weebank Corpus on Named Entity Recognition and Building <fixed-case>NLP</fixed-case> Models for Social Media Analysis</title>
      <author><first>Hang</first><last>Jiang</last></author>
      <author><first>Yining</first><last>Hua</last></author>
      <author><first>Doug</first><last>Beeferman</last></author>
      <author><first>Deb</first><last>Roy</last></author>
      <pages>7199–7208</pages>
      <abstract>Social media data such as Twitter messages (“tweets”) pose a particular challenge to NLP systems because of their short, noisy, and colloquial nature. Tasks such as Named Entity Recognition (NER) and syntactic parsing require highly domain-matched training data for good performance. To date, there is no complete training corpus for both NER and syntactic analysis (e.g., part of speech tagging, dependency parsing) of tweets. While there are some publicly available annotated NLP datasets of tweets, they are only designed for individual tasks. In this study, we aim to create Tweebank-NER, an English NER corpus based on Tweebank V2 (TB2), train state-of-the-art (SOTA) Tweet NLP models on TB2, and release an NLP pipeline called Twitter-Stanza. We annotate named entities in TB2 using Amazon Mechanical Turk and measure the quality of our annotations. We train the Stanza pipeline on TB2 and compare with alternative NLP frameworks (e.g., FLAIR, spaCy) and transformer-based models. The Stanza tokenizer and lemmatizer achieve SOTA performance on TB2, while the Stanza NER tagger, part-of-speech (POS) tagger, and dependency parser achieve competitive performance against non-transformer models. The transformer-based models establish a strong baseline in Tweebank-NER and achieve the new SOTA performance in POS tagging and dependency parsing on TB2. We release the dataset and make both the Stanza pipeline and BERTweet-based models available “off-the-shelf” for use in future Tweet NLP research. Our source code, data, and pre-trained models are available at: <url>https://github.com/social-machines/TweebankNLP</url>.</abstract>
      <url hash="a090d0bd">2022.lrec-1.780</url>
      <bibkey>jiang-etal-2022-annotating</bibkey>
      <pwccode url="https://github.com/social-machines/tweebanknlp" additional="false">social-machines/tweebanknlp</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/tweebank">Tweebank</pwcdataset>
    </paper>
    <paper id="781">
      <title>Did that happen? Predicting Social Media Posts that are Indicative of what happened in a scene: A case study of a <fixed-case>TV</fixed-case> show</title>
      <author><first>Anietie</first><last>Andy</last></author>
      <author><first>Reno</first><last>Kriz</last></author>
      <author><first>Sharath Chandra</first><last>Guntuku</last></author>
      <author><first>Derry Tanti</first><last>Wijaya</last></author>
      <author><first>Chris</first><last>Callison-Burch</last></author>
      <pages>7209–7214</pages>
      <abstract>While popular Television (TV) shows are airing, some users interested in these shows publish social media posts about the show. Analyzing social media posts related to a TV show can be beneficial for gaining insights about what happened during scenes of the show. This is a challenging task partly because a significant number of social media posts associated with a TV show or event may not clearly describe what happened during the event. In this work, we propose a method to predict social media posts (associated with scenes of a TV show) that are indicative of what transpired during the scenes of the show. We evaluate our method on social media (Twitter) posts associated with an episode of a popular TV show, Game of Thrones. We show that for each of the identified scenes, with high AUC’s, our method can predict posts that are indicative of what happened in a scene from those that are not-indicative. Based on Twitters policy, we will make the Tweeter ID’s of the Twitter posts used for this work publicly available.</abstract>
      <url hash="5cef8899">2022.lrec-1.781</url>
      <bibkey>andy-etal-2022-happen</bibkey>
    </paper>
    <paper id="782">
      <title><fixed-case>H</fixed-case>ash<fixed-case>S</fixed-case>et - A Dataset For Hashtag Segmentation</title>
      <author><first>Prashant</first><last>Kodali</last></author>
      <author><first>Akshala</first><last>Bhatnagar</last></author>
      <author><first>Naman</first><last>Ahuja</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <author><first>Ponnurangam</first><last>Kumaraguru</last></author>
      <pages>7215–7219</pages>
      <abstract>Hashtag segmentation is the task of breaking a hashtag into its constituent tokens. Hashtags often encode the essence of user-generated posts, along with information like topic and sentiment, which are useful in downstream tasks. Hashtags prioritize brevity and are written in unique ways - transliterating and mixing languages, spelling variations, creative named entities. Benchmark datasets used for the hashtag segmentation task - STAN, BOUN - are small and extracted from a single set of tweets. However, datasets should reflect the variations in writing styles of hashtags and account for domain and language specificity, failing which the results will misrepresent model performance. We argue that model performance should be assessed on a wider variety of hashtags, and datasets should be carefully curated. To this end, we propose HashSet, a dataset comprising of: a) 1.9k manually annotated dataset; b) 3.3M loosely supervised dataset. HashSet dataset is sampled from a different set of tweets when compared to existing datasets and provides an alternate distribution of hashtags to build and validate hashtag segmentation models. We analyze the performance of SOTA models for Hashtag Segmentation, and show that the proposed dataset provides an alternate set of hashtags to train and assess models.</abstract>
      <url hash="35a7fe4d">2022.lrec-1.782</url>
      <bibkey>kodali-etal-2022-hashset</bibkey>
      <pwccode url="https://github.com/prashantkodali/hashset" additional="false">prashantkodali/hashset</pwccode>
    </paper>
    <paper id="783">
      <title>Using Convolution Neural Network with <fixed-case>BERT</fixed-case> for Stance Detection in <fixed-case>V</fixed-case>ietnamese</title>
      <author><first>Oanh</first><last>Tran</last></author>
      <author><first>Anh Cong</first><last>Phung</last></author>
      <author><first>Bach Xuan</first><last>Ngo</last></author>
      <pages>7220–7225</pages>
      <abstract>Stance detection is the task of automatically eliciting stance information towards a specific claim made by a primary author. While most studies have been done for high-resource languages, this work is dedicated to a low-resource language, namely Vietnamese. In this paper, we propose an architecture using transformers to detect stances in Vietnamese claims. This architecture exploits BERT to extract contextual word embeddings instead of using traditional word2vec models. Then, these embeddings are fed into CNN networks to extract local features to train the stance detection model. We performed extensive comparison experiments to show the effectiveness of the proposed method on a public dataset1 Experimental results show that this proposed model outperforms the previous methods by a large margin. It yielded an accuracy score of 75.57% averaged on four labels. This sets a new SOTA result for future research on this interesting problem in Vietnamese.</abstract>
      <url hash="45aba0af">2022.lrec-1.783</url>
      <bibkey>tran-etal-2022-using</bibkey>
    </paper>
    <paper id="784">
      <title>Annotation-Scheme Reconstruction for “Fake News” and <fixed-case>J</fixed-case>apanese Fake News Dataset</title>
      <author><first>Taichi</first><last>Murayama</last></author>
      <author><first>Shohei</first><last>Hisada</last></author>
      <author><first>Makoto</first><last>Uehara</last></author>
      <author><first>Shoko</first><last>Wakamiya</last></author>
      <author><first>Eiji</first><last>Aramaki</last></author>
      <pages>7226–7234</pages>
      <abstract>Fake news provokes many societal problems; therefore, there has been extensive research on fake news detection tasks to counter it. Many fake news datasets were constructed as resources to facilitate this task. Contemporary research focuses almost exclusively on the factuality aspect of the news. However, this aspect alone is insufficient to explain “fake news,” which is a complex phenomenon that involves a wide range of issues. To fully understand the nature of each instance of fake news, it is important to observe it from various perspectives, such as the intention of the false news disseminator, the harmfulness of the news to our society, and the target of the news. We propose a novel annotation scheme with fine-grained labeling based on detailed investigations of existing fake news datasets to capture these various aspects of fake news. Using the annotation scheme, we construct and publish the first Japanese fake news dataset. The annotation scheme is expected to provide an in-depth understanding of fake news. We plan to build datasets for both Japanese and other languages using our scheme. Our Japanese dataset is published at https://hkefka385.github.io/dataset/fakenews-japanese/.</abstract>
      <url hash="ac6197df">2022.lrec-1.784</url>
      <bibkey>murayama-etal-2022-annotation</bibkey>
    </paper>
    <paper id="785">
      <title><fixed-case>R</fixed-case>o<fixed-case>BERT</fixed-case>uito: a pre-trained language model for social media text in <fixed-case>S</fixed-case>panish</title>
      <author><first>Juan Manuel</first><last>Pérez</last></author>
      <author><first>Damián Ariel</first><last>Furman</last></author>
      <author><first>Laura</first><last>Alonso Alemany</last></author>
      <author><first>Franco M.</first><last>Luque</last></author>
      <pages>7235–7243</pages>
      <abstract>Since BERT appeared, Transformer language models and transfer learning have become state-of-the-art for natural language processing tasks. Recently, some works geared towards pre-training specially-crafted models for particular domains, such as scientific papers, medical documents, user-generated texts, among others. These domain-specific models have been shown to improve performance significantly in most tasks; however, for languages other than English, such models are not widely available. In this work, we present RoBERTuito, a pre-trained language model for user-generated text in Spanish, trained on over 500 million tweets. Experiments on a benchmark of tasks involving user-generated text showed that RoBERTuito outperformed other pre-trained language models in Spanish. In addition to this, our model has some cross-lingual abilities, achieving top results for English-Spanish tasks of the Linguistic Code-Switching Evaluation benchmark (LinCE) and also competitive performance against monolingual models in English Twitter tasks. To facilitate further research, we make RoBERTuito publicly available at the HuggingFace model hub together with the dataset used to pre-train it.</abstract>
      <url hash="078b204c">2022.lrec-1.785</url>
      <bibkey>perez-etal-2022-robertuito</bibkey>
      <pwccode url="https://github.com/pysentimiento/robertuito" additional="false">pysentimiento/robertuito</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/lince">LinCE</pwcdataset>
    </paper>
    <paper id="786">
      <title>Construction of Responsive Utterance Corpus for Attentive Listening Response Production</title>
      <author><first>Koichiro</first><last>Ito</last></author>
      <author><first>Masaki</first><last>Murata</last></author>
      <author><first>Tomohiro</first><last>Ohno</last></author>
      <author><first>Shigeki</first><last>Matsubara</last></author>
      <pages>7244–7252</pages>
      <abstract>In Japan, the number of single-person households, particularly among the elderly, is increasing. Consequently, opportunities for people to narrate are being reduced. To address this issue, conversational agents, e.g., communication robots and smart speakers, are expected to play the role of the listener. To realize these agents, this paper describes the collection of conversational responses by listeners that demonstrate attentive listening attitudes toward narrative speakers, and a method to annotate existing narrative speech with responsive utterances is proposed. To summarize, 148,962 responsive utterances by 11 listeners were collected in a narrative corpus comprising 13,234 utterance units. The collected responsive utterances were analyzed in terms of response frequency, diversity, coverage, and naturalness. These results demonstrated that diverse and natural responsive utterances were collected by the proposed method in an efficient and comprehensive manner. To demonstrate the practical use of the collected responsive utterances, an experiment was conducted, in which response generation timings were detected in narratives.</abstract>
      <url hash="8352d417">2022.lrec-1.786</url>
      <bibkey>ito-etal-2022-construction</bibkey>
    </paper>
    <paper id="787">
      <title>Speak: A Toolkit Using <fixed-case>A</fixed-case>mazon <fixed-case>M</fixed-case>echanical <fixed-case>T</fixed-case>urk to Collect and Validate Speech Audio Recordings</title>
      <author><first>Christopher</first><last>Song</last></author>
      <author><first>David</first><last>Harwath</last></author>
      <author><first>Tuka</first><last>Alhanai</last></author>
      <author><first>James</first><last>Glass</last></author>
      <pages>7253–7258</pages>
      <abstract>We present Speak, a toolkit that allows researchers to crowdsource speech audio recordings using Amazon Mechanical Turk (MTurk). Speak allows MTurk workers to submit speech recordings in response to a task prompt and stimulus (e.g. image, text excerpt, audio file) defined by researchers, a functionality that is not natively offered by MTurk at the time of writing this paper. Importantly, the toolkit employs numerous measures to ensure that speech recordings collected are of adequate quality, in order to avoid accepting unusable data and prevent abuse/fraud. Speak has demonstrated utility, having collected over 600,000 recordings to date. The toolkit is open-source and available for download.</abstract>
      <url hash="f0e24f52">2022.lrec-1.787</url>
      <bibkey>song-etal-2022-speak</bibkey>
    </paper>
    <paper id="788">
      <title><fixed-case>ASCEND</fixed-case>: A Spontaneous <fixed-case>C</fixed-case>hinese-<fixed-case>E</fixed-case>nglish Dataset for Code-switching in Multi-turn Conversation</title>
      <author><first>Holy</first><last>Lovenia</last></author>
      <author><first>Samuel</first><last>Cahyawijaya</last></author>
      <author><first>Genta</first><last>Winata</last></author>
      <author><first>Peng</first><last>Xu</last></author>
      <author><first>Yan</first><last>Xu</last></author>
      <author><first>Zihan</first><last>Liu</last></author>
      <author><first>Rita</first><last>Frieske</last></author>
      <author><first>Tiezheng</first><last>Yu</last></author>
      <author><first>Wenliang</first><last>Dai</last></author>
      <author><first>Elham J.</first><last>Barezi</last></author>
      <author><first>Qifeng</first><last>Chen</last></author>
      <author><first>Xiaojuan</first><last>Ma</last></author>
      <author><first>Bertram</first><last>Shi</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <pages>7259–7268</pages>
      <abstract>Code-switching is a speech phenomenon occurring when a speaker switches language during a conversation. Despite the spontaneous nature of code-switching in conversational spoken language, most existing works collect code-switching data from read speech instead of spontaneous speech. ASCEND (A Spontaneous Chinese-English Dataset) is a high-quality Mandarin Chinese-English code-switching corpus built on spontaneous multi-turn conversational dialogue sources collected in Hong Kong. We report ASCEND’s design and procedure for collecting the speech data, including annotations. ASCEND consists of 10.62 hours of clean speech, collected from 23 bilingual speakers of Chinese and English. Furthermore, we conduct baseline experiments using pre-trained wav2vec 2.0 models, achieving a best performance of 22.69% character error rate and 27.05% mixed error rate.</abstract>
      <url hash="d511230d">2022.lrec-1.788</url>
      <bibkey>lovenia-etal-2022-ascend</bibkey>
      <pwccode url="https://github.com/HLTCHKUST/ASCEND" additional="false">HLTCHKUST/ASCEND</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ascend">ASCEND</pwcdataset>
    </paper>
    <paper id="789">
      <title>A <fixed-case>R</fixed-case>omanization System and <fixed-case>W</fixed-case>eb<fixed-case>MAUS</fixed-case> Aligner for <fixed-case>A</fixed-case>rabic Varieties</title>
      <author><first>Jalal</first><last>Al-Tamimi</last></author>
      <author><first>Florian</first><last>Schiel</last></author>
      <author><first>Ghada</first><last>Khattab</last></author>
      <author><first>Navdeep</first><last>Sokhey</last></author>
      <author><first>Djegdjiga</first><last>Amazouz</last></author>
      <author><first>Abdulrahman</first><last>Dallak</last></author>
      <author><first>Hajar</first><last>Moussa</last></author>
      <pages>7269–7276</pages>
      <abstract>This paper presents the results of an ongoing collaboration to develop an Arabic variety-independent romanization system that aims to homogenize and simplify the romanization of the Arabic script, and introduces an Arabic variety-independent WebMAUS service offering a free to use forced-alignment service fully integrated within the WebMAUS services. We present the rationale for developing such a system, highlighting the need for a detailed romanization system with graphemes corresponding to the phonemic short and long vowels/consonants in Arabic varieties. We describe how the acoustic model was created, followed by several hands-on recipes for applying the forced alignment webservice either online or programatically. Finally, we discuss some of the issues we faced during the development of the system.</abstract>
      <url hash="0dc1cf7e">2022.lrec-1.789</url>
      <bibkey>al-tamimi-etal-2022-romanization</bibkey>
    </paper>
    <paper id="790">
      <title><fixed-case>B</fixed-case>emba<fixed-case>S</fixed-case>peech: A Speech Recognition Corpus for the <fixed-case>B</fixed-case>emba Language</title>
      <author><first>Claytone</first><last>Sikasote</last></author>
      <author><first>Antonios</first><last>Anastasopoulos</last></author>
      <pages>7277–7283</pages>
      <abstract>We present a preprocessed, ready-to-use automatic speech recognition corpus, BembaSpeech, consisting over 24 hours of read speech in the Bemba language, a written but low-resourced language spoken by over 30% of the population in Zambia. To assess its usefulness for training and testing ASR systems for Bemba, we explored different approaches; supervised pre-training (training from scratch), cross-lingual transfer learning from a monolingual English pre-trained model using DeepSpeech on the portion of the dataset and fine-tuning large scale self-supervised Wav2Vec2.0 based multilingual pre-trained models on the complete BembaSpeech corpus. From our experiments, the 1 billion XLS-R parameter model gives the best results. The model achieves a word error rate (WER) of 32.91%, results demonstrating that model capacity significantly improves performance and that multilingual pre-trained models transfers cross-lingual acoustic representation better than monolingual pre-trained English model on the BembaSpeech for the Bemba ASR. Lastly, results also show that the corpus can be used for building ASR systems for Bemba language.</abstract>
      <url hash="816deae8">2022.lrec-1.790</url>
      <bibkey>sikasote-anastasopoulos-2022-bembaspeech</bibkey>
      <pwccode url="https://github.com/csikasote/BembaSpeech" additional="true">csikasote/BembaSpeech</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/jw300">JW300</pwcdataset>
    </paper>
    <paper id="791">
      <title><fixed-case>B</fixed-case>ehance<fixed-case>CC</fixed-case>: A <fixed-case>C</fixed-case>hit<fixed-case>C</fixed-case>hat Detection Dataset For Livestreaming Video Transcripts</title>
      <author><first>Viet</first><last>Lai</last></author>
      <author><first>Amir</first><last>Pouran Ben Veyseh</last></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Thien</first><last>Nguyen</last></author>
      <pages>7284–7290</pages>
      <abstract>Livestreaming videos have become an effective broadcasting method for both video sharing and educational purposes. However, livestreaming videos contain a considerable amount of off-topic content (i.e., up to 50%) which introduces significant noises and data load to downstream applications. This paper presents BehanceCC, a new human-annotated benchmark dataset for off-topic detection (also called chitchat detection) in livestreaming video transcripts. In addition to describing the challenges of the dataset, our extensive experiments of various baselines reveal the complexity of chitchat detection for livestreaming videos and suggest potential future research directions for this task. The dataset will be made publicly available to foster research in this area.</abstract>
      <url hash="875683be">2022.lrec-1.791</url>
      <bibkey>lai-etal-2022-behancecc</bibkey>
    </paper>
    <paper id="792">
      <title>Adversarial Speech Generation and Natural Speech Recovery for Speech Content Protection</title>
      <author><first>Sheng</first><last>Li</last></author>
      <author><first>Jiyi</first><last>Li</last></author>
      <author><first>Qianying</first><last>Liu</last></author>
      <author><first>Zhuo</first><last>Gong</last></author>
      <pages>7291–7297</pages>
      <abstract>With the advent of the General Data Protection Regulation (GDPR) and increasing privacy concerns, the sharing of speech data is faced with significant challenges. Protecting the sensitive content of speech is the same important as the voiceprint. This paper proposes an effective speech content protection method by constructing a frame-by-frame adversarial speech generation system. We revisited the adversarial examples generating method in the recent machine learning field and selected the phonetic state sequence of sensitive speech for the adversarial examples generation. We build an adversarial speech collection. Moreover, based on the speech collection, we proposed a neural network-based frame-by-frame mapping method to recover the speech content by converting from the adversarial speech to the human speech. Experiment shows our proposed method can encode and recover any sensitive audio, and our method is easy to be conducted with publicly available resources of speech recognition technology.</abstract>
      <url hash="ddc06261">2022.lrec-1.792</url>
      <bibkey>li-etal-2022-adversarial</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/librispeech">LibriSpeech</pwcdataset>
    </paper>
    <paper id="793">
      <title>A new <fixed-case>E</fixed-case>uropean <fixed-case>P</fixed-case>ortuguese corpus for the study of Psychosis through speech analysis</title>
      <author><first>Maria</first><last>Forjó</last></author>
      <author><first>Daniel</first><last>Neto</last></author>
      <author><first>Alberto</first><last>Abad</last></author>
      <author><first>HSofia</first><last>Pinto</last></author>
      <author><first>Joaquim</first><last>Gago</last></author>
      <pages>7298–7304</pages>
      <abstract>Psychosis is a clinical syndrome characterized by the presence of symptoms such as hallucinations, thought disorder and disorganized speech. Several studies have used machine learning, combined with speech and natural language processing methods to aid in the diagnosis process of this disease. This paper describes the creation of the first European Portuguese corpus for the identification of the presence of speech characteristics of psychosis, which contains samples of 92 participants, 56 controls and 36 individuals diagnosed with psychosis and medicated. The corpus was used in a set of experiments that allowed identifying the most promising feature set to perform the classification: the combination of acoustic and speech metric features. Several classifiers were implemented to study which ones entailed the best performance depending on the task and feature set. The most promising results obtained for the entire corpus were achieved when identifying individuals with a Multi-Layer Perceptron classifier and reached an 87.5% accuracy. Focusing on the gender dependent results, the overall best results were 90.9% and 82.9% accuracy, for female and male subjects respectively. Lastly, the experiments performed lead us to conjecture that spontaneous speech presents more identifiable characteristics than read speech to differentiate healthy and patients diagnosed with psychosis.</abstract>
      <url hash="d765891d">2022.lrec-1.793</url>
      <bibkey>forjo-etal-2022-new</bibkey>
    </paper>
    <paper id="794">
      <title>Investigating Inter- and Intra-speaker Voice Conversion using Audiobooks</title>
      <author><first>Aghilas</first><last>Sini</last></author>
      <author><first>Damien</first><last>Lolive</last></author>
      <author><first>Nelly</first><last>Barbot</last></author>
      <author><first>Pierre</first><last>Alain</last></author>
      <pages>7305–7313</pages>
      <abstract>Audiobook readers play with their voices to emphasize some text passages, highlight discourse changes or significant events, or in order to make listening easier and entertaining. A dialog is a central passage in audiobooks where the reader applies significant voice transformation, mainly prosodic modifications, to realize character properties and changes. However, these intra-speaker modifications are hard to reproduce with simple text-to-speech synthesis. The manner of vocalizing characters involved in a given story depends on the text style and differs from one speaker to another. In this work, this problem is investigated through the prism of voice conversion. We propose to explore modifying the narrator’s voice to fit the context of the story, such as the character who is speaking, using voice conversion. To this end, two complementary experiments are designed: the first one aims to assess the quality of our Phonetic PosteriorGrams (PPG)-based voice conversion system using parallel data. Subjective evaluations with naive raters are conducted to estimate the quality of the signal generated and the speaker similarity. The second experiment applies an intra-speaker voice conversion, considering narration passages and direct speech passages as two distinct speakers. Data are then nonparallel and the dissimilarity between character and narrator is subjectively measured.</abstract>
      <url hash="d2343d1c">2022.lrec-1.794</url>
      <bibkey>sini-etal-2022-investigating</bibkey>
    </paper>
    <paper id="795">
      <title>Multilingual Transfer Learning for Children Automatic Speech Recognition</title>
      <author><first>Thomas</first><last>Rolland</last></author>
      <author><first>Alberto</first><last>Abad</last></author>
      <author><first>Catia</first><last>Cucchiarini</last></author>
      <author><first>Helmer</first><last>Strik</last></author>
      <pages>7314–7320</pages>
      <abstract>Despite recent advances in automatic speech recognition (ASR), the recognition of children’s speech still remains a significant challenge. This is mainly due to the high acoustic variability and the limited amount of available training data. The latter problem is particularly evident in languages other than English, which are usually less-resourced. In the current paper, we address children ASR in a number of less-resourced languages by combining several small-sized children speech corpora from these languages. In particular, we address the following research question: Does a novel two-step training strategy in which multilingual learning is followed by language-specific transfer learning outperform conventional single language/task training for children speech, as well as multilingual and transfer learning alone? Based on previous experimental results with English, we hypothesize that multilingual learning provides a better generalization of the underlying characteristics of children’s speech. Our results provide a positive answer to our research question, by showing that using transfer learning on top of a multilingual model for an unseen language outperforms conventional single language-specific learning.</abstract>
      <url hash="7ba284c3">2022.lrec-1.795</url>
      <bibkey>rolland-etal-2022-multilingual</bibkey>
    </paper>
    <paper id="796">
      <title><fixed-case>B</fixed-case>ehance<fixed-case>QA</fixed-case>: A New Dataset for Identifying Question-Answer Pairs in Video Transcripts</title>
      <author><first>Amir</first><last>Pouran Ben Veyseh</last></author>
      <author><first>Viet</first><last>Lai</last></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Thien</first><last>Nguyen</last></author>
      <pages>7321–7327</pages>
      <abstract>Question-Answer (QA) is one of the effective methods for storing knowledge which can be used for future retrieval. As such, identifying mentions of questions and their answers in text is necessary for a knowledge construction and retrieval systems. In the literature, QA identification has been well studied in the NLP community. However, most of the prior works are restricted to formal written documents such as papers or websites. As such, Questions and Answers that are presented in informal/noisy documents have not been adequately studied. One of the domains that can significantly benefit from QA identification is the domain of livestreaming video transcripts that involve abundant QA pairs to provide valuable knowledge for future users and services. Since video transcripts are often transcribed automatically for scale, they are prone to errors. Combined with the informal nature of discussion in a video, prior QA identification systems might not be able to perform well in this domain. To enable comprehensive research in this domain, we present a large-scale QA identification dataset annotated by human over transcripts of 500 hours of streamed videos. We employ Behance.net to collect the videos and their automatically obtained transcripts. Furthermore, we conduct extensive analysis on the annotated dataset to understand the complexity of QA identification for livestreaming video transcripts. Our experiments show that the annotated dataset presents unique challenges for existing methods and more research is necessary to explore more effective methods. The dataset and the models developed in this work will be publicly released for future research.</abstract>
      <url hash="5d79541e">2022.lrec-1.796</url>
      <bibkey>pouran-ben-veyseh-etal-2022-behanceqa</bibkey>
      <pwccode url="https://github.com/amirveyseh/behanceqa" additional="false">amirveyseh/behanceqa</pwccode>
    </paper>
    <paper id="797">
      <title>Bidirectional Skeleton-Based Isolated Sign Recognition using Graph Convolutional Networks</title>
      <author><first>Konstantinos M.</first><last>Dafnis</last></author>
      <author><first>Evgenia</first><last>Chroni</last></author>
      <author><first>Carol</first><last>Neidle</last></author>
      <author><first>Dimitri</first><last>Metaxas</last></author>
      <pages>7328–7338</pages>
      <abstract>To improve computer-based recognition from video of isolated signs from American Sign Language (ASL), we propose a new skeleton-based method that involves explicit detection of the start and end frames of signs, trained on the ASLLVD dataset; it uses linguistically relevant parameters based on the skeleton input. Our method employs a bidirectional learning approach within a Graph Convolutional Network (GCN) framework. We apply this method to the WLASL dataset, but with corrections to the gloss labeling to ensure consistency in the labels assigned to different signs; it is important to have a 1-1 correspondence between signs and text-based gloss labels. We achieve a success rate of 77.43% for top-1 and 94.54% for top-5 using this modified WLASL dataset. Our method, which does not require multi-modal data input, outperforms other state-of-the-art approaches on the same modified WLASL dataset, demonstrating the importance of both attention to the start and end frames of signs and the use of bidirectional data streams in the GCNs for isolated sign recognition.</abstract>
      <url hash="4462b67e">2022.lrec-1.797</url>
      <bibkey>dafnis-etal-2022-bidirectional</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/autsl">AUTSL</pwcdataset>
    </paper>
    <paper id="798">
      <title>Deep learning-based end-to-end spoken language identification system for domain-mismatched scenario</title>
      <author><first>Woohyun</first><last>Kang</last></author>
      <author><first>Md Jahangir</first><last>Alam</last></author>
      <author><first>Abderrahim</first><last>Fathan</last></author>
      <pages>7339–7343</pages>
      <abstract>Domain mismatch is a critical issue when it comes to spoken language identification. To overcome the domain mismatch problem, we have applied several architectures and deep learning strategies which have shown good results in cross-domain speaker verification tasks to spoken language identification. Our systems were evaluated on the Oriental Language Recognition (OLR) Challenge 2021 Task 1 dataset, which provides a set of cross-domain language identification trials. Among our experimented systems, the best performance was achieved by using the mel frequency cepstral coefficient (MFCC) and pitch features as input and training the ECAPA-TDNN system with a flow-based regularization technique, which resulted in a Cavg of 0.0631 on the OLR 2021 progress set.</abstract>
      <url hash="86ed6f50">2022.lrec-1.798</url>
      <bibkey>kang-etal-2022-deep</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/olr-2021">OLR 2021</pwcdataset>
    </paper>
    <paper id="799">
      <title>Handwritten Character Generation using <fixed-case>Y</fixed-case>-Autoencoder for Character Recognition Model Training</title>
      <author><first>Tomoki</first><last>Kitagawa</last></author>
      <author><first>Chee Siang</first><last>Leow</last></author>
      <author><first>Hiromitsu</first><last>Nishizaki</last></author>
      <pages>7344–7351</pages>
      <abstract>It is well-known that the deep learning-based optical character recognition (OCR) system needs a large amount of data to train a high-performance character recognizer. However, it is costly to collect a large amount of realistic handwritten characters. This paper introduces a Y-Autoencoder (Y-AE)-based handwritten character generator to generate multiple Japanese Hiragana characters with a single image to increase the amount of data for training a handwritten character recognizer. The adaptive instance normalization (AdaIN) layer allows the generator to be trained and generate handwritten character images without paired-character image labels. The experiment shows that the Y-AE could generate Japanese character images then used to train the handwritten character recognizer, producing an F1-score improved from 0.8664 to 0.9281. We further analyzed the usefulness of the Y-AE-based generator with shape images, out-of-character (OOC) images, which have different character images styles in model training. The result showed that the generator could generate a handwritten image with a similar style to that of the input character.</abstract>
      <url hash="90a45622">2022.lrec-1.799</url>
      <bibkey>kitagawa-etal-2022-handwritten</bibkey>
    </paper>
    <paper id="800">
      <title>Attention-Focused Adversarial Training for Robust Temporal Reasoning</title>
      <author><first>Lis</first><last>Kanashiro Pereira</last></author>
      <pages>7352–7359</pages>
      <abstract>We propose an enhanced adversarial training algorithm for fine-tuning transformer-based language models (i.e., RoBERTa) and apply it to the temporal reasoning task. Current adversarial training approaches for NLP add the adversarial perturbation only to the embedding layer, ignoring the other layers of the model, which might limit the generalization power of adversarial training. Instead, our algorithm searches for the best combination of layers to add the adversarial perturbation. We add the adversarial perturbation to multiple hidden states or attention representations of the model layers. Adding the perturbation to the attention representations performed best in our experiments. Our model can improve performance on several temporal reasoning benchmarks, and establishes new state-of-the-art results.</abstract>
      <url hash="bfd083fe">2022.lrec-1.800</url>
      <bibkey>kanashiro-pereira-2022-attention</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cosmosqa">CosmosQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/matres">MATRES</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mc-taco">MC-TACO</pwcdataset>
    </paper>
    <paper id="801">
      <title><fixed-case>P</fixed-case>oli<fixed-case>BERT</fixed-case>weet: A Pre-trained Language Model for Analyzing Political Content on <fixed-case>T</fixed-case>witter</title>
      <author><first>Kornraphop</first><last>Kawintiranon</last></author>
      <author><first>Lisa</first><last>Singh</last></author>
      <pages>7360–7367</pages>
      <abstract>Transformer-based models have become the state-of-the-art for numerous natural language processing (NLP) tasks, especially for noisy data sets, including social media posts. For example, BERTweet, pre-trained RoBERTa on a large amount of Twitter data, has achieved state-of-the-art results on several Twitter NLP tasks. We argue that it is not only important to have general pre-trained models for a social media platform, but also domain-specific ones that better capture domain-specific language context. Domain-specific resources are not only important for NLP tasks associated with a specific domain, but they are also useful for understanding language differences across domains. One domain that receives a large amount of attention is politics, more specifically political elections. Towards that end, we release PoliBERTweet, a pre-trained language model trained from BERTweet on over 83M US 2020 election-related English tweets. While the construction of the resource is fairly straightforward, we believe that it can be used for many important downstream tasks involving language, including political misinformation analysis and election public opinion analysis. To show the value of this resource, we evaluate PoliBERTweet on different NLP tasks. The results show that our model outperforms general-purpose language models in domain-specific contexts, highlighting the value of domain-specific models for more detailed linguistic analysis. We also extend other existing language models with a sample of these data and show their value for presidential candidate stance detection, a context-specific task. We release PoliBERTweet and these other models to the community to advance interdisciplinary research related to Election 2020.</abstract>
      <url hash="e3fed001">2022.lrec-1.801</url>
      <bibkey>kawintiranon-singh-2022-polibertweet</bibkey>
      <pwccode url="https://github.com/gu-datalab/polibertweet" additional="false">gu-datalab/polibertweet</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/tweeteval">TweetEval</pwcdataset>
    </paper>
    <paper id="802">
      <title>Modeling the Impact of Syntactic Distance and Surprisal on Cross-<fixed-case>S</fixed-case>lavic Text Comprehension</title>
      <author><first>Irina</first><last>Stenger</last></author>
      <author><first>Philip</first><last>Georgis</last></author>
      <author><first>Tania</first><last>Avgustinova</last></author>
      <author><first>Bernd</first><last>Möbius</last></author>
      <author><first>Dietrich</first><last>Klakow</last></author>
      <pages>7368–7376</pages>
      <abstract>We focus on the syntactic variation and measure syntactic distances between nine Slavic languages (Belarusian, Bulgarian, Croatian, Czech, Polish, Slovak, Slovene, Russian, and Ukrainian) using symmetric measures of insertion, deletion and movement of syntactic units in the parallel sentences of the fable “The North Wind and the Sun”. Additionally, we investigate phonetic and orthographic asymmetries between selected languages by means of the information theoretical notion of surprisal. Syntactic distance and surprisal are, thus, considered as potential predictors of mutual intelligibility between related languages. In spoken and written cloze test experiments for Slavic native speakers, the presented predictors will be validated as to whether variations in syntax lead to a slower or impeded intercomprehension of Slavic texts.</abstract>
      <url hash="eca424f8">2022.lrec-1.802</url>
      <bibkey>stenger-etal-2022-modeling</bibkey>
    </paper>
    <paper id="803">
      <title><fixed-case>BERT</fixed-case>ifying <fixed-case>S</fixed-case>inhala - A Comprehensive Analysis of Pre-trained Language Models for <fixed-case>S</fixed-case>inhala Text Classification</title>
      <author><first>Vinura</first><last>Dhananjaya</last></author>
      <author><first>Piyumal</first><last>Demotte</last></author>
      <author><first>Surangika</first><last>Ranathunga</last></author>
      <author><first>Sanath</first><last>Jayasena</last></author>
      <pages>7377–7385</pages>
      <abstract>This research provides the first comprehensive analysis of the performance of pre-trained language models for Sinhala text classification. We test on a set of different Sinhala text classification tasks and our analysis shows that out of the pre-trained multilingual models that include Sinhala (XLM-R, LaBSE, and LASER), XLM-R is the best model by far for Sinhala text classification. We also pre-train two RoBERTa-based monolingual Sinhala models, which are far superior to the existing pre-trained language models for Sinhala. We show that when fine-tuned, these pre-trained language models set a very strong baseline for Sinhala text classification and are robust in situations where labeled data is insufficient for fine-tuning. We further provide a set of recommendations for using pre-trained models for Sinhala text classification. We also introduce new annotated datasets useful for future research in Sinhala text classification and publicly release our pre-trained models.</abstract>
      <url hash="d899e196">2022.lrec-1.803</url>
      <bibkey>dhananjaya-etal-2022-bertifying</bibkey>
      <pwccode url="https://github.com/nlpcuom/Sinhala-text-classification" additional="false">nlpcuom/Sinhala-text-classification</pwccode>
    </paper>
    <paper id="804">
      <title>Pre-training and Evaluating Transformer-based Language Models for <fixed-case>I</fixed-case>celandic</title>
      <author><first>Jón Friðrik</first><last>Daðason</last></author>
      <author><first>Hrafn</first><last>Loftsson</last></author>
      <pages>7386–7391</pages>
      <abstract>In this paper, we evaluate several Transformer-based language models for Icelandic on four downstream tasks: Part-of-Speech tagging, Named Entity Recognition. Dependency Parsing, and Automatic Text Summarization. We pre-train four types of monolingual ELECTRA and ConvBERT models and compare our results to a previously trained monolingual RoBERTa model and the multilingual mBERT model. We find that the Transformer models obtain better results, often by a large margin, compared to previous state-of-the-art models. Furthermore, our results indicate that pre-training larger language models results in a significant reduction in error rates in comparison to smaller models. Finally, our results show that the monolingual models for Icelandic outperform a comparably sized multilingual model.</abstract>
      <url hash="be236ea8">2022.lrec-1.804</url>
      <bibkey>gudnason-loftsson-2022-pre</bibkey>
    </paper>
  </volume>
  <event id="lrec-2022">
    <colocated>
      <volume-id>2022.lrec-1</volume-id>
      <volume-id>2022.bucc-1</volume-id>
      <volume-id>2022.cltw-1</volume-id>
      <volume-id>2022.cmlc-1</volume-id>
      <volume-id>2022.csrnlp-1</volume-id>
      <volume-id>2022.dclrl-1</volume-id>
      <volume-id>2022.digitam-1</volume-id>
      <volume-id>2022.tdle-1</volume-id>
      <volume-id>2022.eurali-1</volume-id>
      <volume-id>2022.fnp-1</volume-id>
      <volume-id>2022.games-1</volume-id>
      <volume-id>2022.gwll-1</volume-id>
      <volume-id>2022.isa-1</volume-id>
      <volume-id>2022.lateraisse-1</volume-id>
      <volume-id>2022.law-1</volume-id>
      <volume-id>2022.legal-1</volume-id>
      <volume-id>2022.lt4hala-1</volume-id>
      <volume-id>2022.mwe-1</volume-id>
      <volume-id>2022.nidcp-1</volume-id>
      <volume-id>2022.nlperspectives-1</volume-id>
      <volume-id>2022.osact-1</volume-id>
      <volume-id>2022.pvlam-1</volume-id>
      <volume-id>2022.parlaclarin-1</volume-id>
      <volume-id>2022.politicalnlp-1</volume-id>
      <volume-id>2022.rapid-1</volume-id>
      <volume-id>2022.readi-1</volume-id>
      <volume-id>2022.restup-1</volume-id>
      <volume-id>2022.salld-1</volume-id>
      <volume-id>2022.signlang-1</volume-id>
      <volume-id>2022.sltat-1</volume-id>
      <volume-id>2022.sigul-1</volume-id>
      <volume-id>2022.smila-1</volume-id>
      <volume-id>2022.term-1</volume-id>
      <volume-id>2022.wildre-1</volume-id>
    </colocated>
  </event>
</collection>
