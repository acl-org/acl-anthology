<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.clpsych">
  <volume id="1" ingest-date="2022-06-30">
    <meta>
      <booktitle>Proceedings of the Eighth Workshop on Computational Linguistics and Clinical Psychology</booktitle>
      <editor><first>Ayah</first><last>Zirikly</last></editor>
      <editor><first>Dana</first><last>Atzil-Slonim</last></editor>
      <editor><first>Maria</first><last>Liakata</last></editor>
      <editor><first>Steven</first><last>Bedrick</last></editor>
      <editor><first>Bart</first><last>Desmet</last></editor>
      <editor><first>Molly</first><last>Ireland</last></editor>
      <editor><first>Andrew</first><last>Lee</last></editor>
      <editor><first>Sean</first><last>MacAvaney</last></editor>
      <editor><first>Matthew</first><last>Purver</last></editor>
      <editor><first>Rebecca</first><last>Resnik</last></editor>
      <editor><first>Andrew</first><last>Yates</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Seattle, USA</address>
      <month>July</month>
      <year>2022</year>
      <url hash="4c0d88af">2022.clpsych-1</url>
      <venue>clpsych</venue>
    </meta>
    <frontmatter>
      <url hash="7311482c">2022.clpsych-1.0</url>
      <bibkey>clpsych-2022-linguistics</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>DEPAC</fixed-case>: a Corpus for Depression and Anxiety Detection from Speech</title>
      <author><first>Mashrura</first><last>Tasnim</last></author>
      <author><first>Malikeh</first><last>Ehghaghi</last></author>
      <author><first>Brian</first><last>Diep</last></author>
      <author><first>Jekaterina</first><last>Novikova</last></author>
      <pages>1-16</pages>
      <abstract>Mental distress like depression and anxiety contribute to the largest proportion of the global burden of diseases. Automated diagnosis system of such disorders, empowered by recent innovations in Artificial Intelligence, can pave the way to reduce the sufferings of the affected individuals. Development of such systems requires information-rich and balanced corpora. In this work, we introduce a novel mental distress analysis audio dataset DEPAC, labelled based on established thresholds on depression and anxiety standard screening tools. This large dataset comprises multiple speech tasks per individual, as well as relevant demographic information. Alongside, we present a feature set consisting of hand-curated acoustic and linguistic features, which were found effective in identifying signs of mental illnesses in human speech. Finally, we justify the quality and effectiveness of our proposed audio corpus and feature set in predicting depression severity by comparing the performance of baseline machine learning models built on this dataset with baseline models trained on other well-known depression corpora.</abstract>
      <url hash="fe556a14">2022.clpsych-1.1</url>
      <attachment type="appendix" hash="e0da7af9">2022.clpsych-1.1.appendix.pdf</attachment>
      <bibkey>tasnim-etal-2022-depac</bibkey>
      <doi>10.18653/v1/2022.clpsych-1.1</doi>
      <video href="2022.clpsych-1.1.mp4"/>
    </paper>
    <paper id="2">
      <title>The ethical role of computational linguistics in digital psychological formulation and suicide prevention.</title>
      <author><first>Martin</first><last>Orr</last></author>
      <author><first>Kirsten</first><last>Van Kessel</last></author>
      <author><first>Dave</first><last>Parry</last></author>
      <pages>17-29</pages>
      <abstract>Formulation is central to clinical practice. Formulation has a factor weighing, pattern recognition and explanatory hypothesis modelling focus. Formulation attempts to make sense of why a person presents in a certain state at a certain time and context, and how that state may be best managed to enhance mental health, safety and optimal change. Inherent to the clinical need for formulation is an appreciation of the complexities, uncertainty and limits of applying theoretical concepts and symptom, diagnostic and risk categories to human experience; or attaching meaning or weight to any particular factor in an individual?s history or mental state without considering the broader biopsychosocial and cultural context. With specific reference to suicide prevention, this paper considers the need and potential for the computer linguistic community to be both cognisant of and ethically contribute to the clinical formulation process.</abstract>
      <url hash="cffe80fa">2022.clpsych-1.2</url>
      <bibkey>orr-etal-2022-ethical</bibkey>
      <doi>10.18653/v1/2022.clpsych-1.2</doi>
      <video href="2022.clpsych-1.2.mp4"/>
    </paper>
    <paper id="3">
      <title>Explaining Models of Mental Health via Clinically Grounded Auxiliary Tasks</title>
      <author><first>Ayah</first><last>Zirikly</last></author>
      <author><first>Mark</first><last>Dredze</last></author>
      <pages>30-39</pages>
      <abstract>Models of mental health based on natural language processing can uncover latent signals of mental health from language. Models that indicate whether an individual is depressed, or has other mental health conditions, can aid in diagnosis and treatment. A critical aspect of integration of these models into the clinical setting relies on explaining their behavior to domain experts. In the case of mental health diagnosis, clinicians already rely on an assessment framework to make these decisions; that framework can help a model generate meaningful explanations.In this work we propose to use PHQ-9 categories as an auxiliary task to explaining a social media based model of depression. We develop a multi-task learning framework that predicts both depression and PHQ-9 categories as auxiliary tasks. We compare the quality of explanations generated based on the depression task only, versus those that use the predicted PHQ-9 categories. We find that by relying on clinically meaningful auxiliary tasks, we produce more meaningful explanations.</abstract>
      <url hash="61198b97">2022.clpsych-1.3</url>
      <bibkey>zirikly-dredze-2022-explaining</bibkey>
      <doi>10.18653/v1/2022.clpsych-1.3</doi>
      <video href="2022.clpsych-1.3.mp4"/>
    </paper>
    <paper id="4">
      <title>Identifying stable speech-language markers of autism in children: Preliminary evidence from a longitudinal telephony-based study</title>
      <author><first>Sunghye</first><last>Cho</last></author>
      <author><first>Riccardo</first><last>Fusaroli</last></author>
      <author><first>Maggie Rose</first><last>Pelella</last></author>
      <author><first>Kimberly</first><last>Tena</last></author>
      <author><first>Azia</first><last>Knox</last></author>
      <author><first>Aili</first><last>Hauptmann</last></author>
      <author><first>Maxine</first><last>Covello</last></author>
      <author><first>Alison</first><last>Russell</last></author>
      <author><first>Judith</first><last>Miller</last></author>
      <author><first>Alison</first><last>Hulink</last></author>
      <author><first>Jennifer</first><last>Uzokwe</last></author>
      <author><first>Kevin</first><last>Walker</last></author>
      <author><first>James</first><last>Fiumara</last></author>
      <author><first>Juhi</first><last>Pandey</last></author>
      <author><first>Christopher</first><last>Chatham</last></author>
      <author><first>Christopher</first><last>Cieri</last></author>
      <author><first>Robert</first><last>Schultz</last></author>
      <author><first>Mark</first><last>Liberman</last></author>
      <author><first>Julia</first><last>Parish-morris</last></author>
      <pages>40-46</pages>
      <abstract>This study examined differences in linguistic features produced by autistic and neurotypical (NT) children during brief picture descriptions, and assessed feature stability over time. Weekly speech samples from well-characterized participants were collected using a telephony system designed to improve access for geographically isolated and historically marginalized communities. Results showed stable group differences in certain acoustic features, some of which may potentially serve as key outcome measures in future treatment studies. These results highlight the importance of eliciting semi-structured speech samples in a variety of contexts over time, and adds to a growing body of research showing that fine-grained naturalistic communication features hold promise for intervention research.</abstract>
      <url hash="12959302">2022.clpsych-1.4</url>
      <attachment type="appendix" hash="1183ac7d">2022.clpsych-1.4.appendix.pdf</attachment>
      <bibkey>cho-etal-2022-identifying</bibkey>
      <doi>10.18653/v1/2022.clpsych-1.4</doi>
      <video href="2022.clpsych-1.4.mp4"/>
    </paper>
    <paper id="5">
      <title>Psychotherapy is Not One Thing: Simultaneous Modeling of Different Therapeutic Approaches</title>
      <author><first>Maitrey</first><last>Mehta</last></author>
      <author><first>Derek</first><last>Caperton</last></author>
      <author><first>Katherine</first><last>Axford</last></author>
      <author><first>Lauren</first><last>Weitzman</last></author>
      <author><first>David</first><last>Atkins</last></author>
      <author><first>Vivek</first><last>Srikumar</last></author>
      <author><first>Zac</first><last>Imel</last></author>
      <pages>47-58</pages>
      <abstract>There are many different forms of psychotherapy. Itemized inventories of psychotherapeutic interventions provide a mechanism for evaluating the quality of care received by clients and for conducting research on how psychotherapy helps. However, evaluations such as these are slow, expensive, and are rarely used outside of well-funded research studies. Natural language processing research has progressed to allow automating such tasks. Yet, NLP work in this area has been restricted to evaluating a single approach to treatment, when prior research indicates therapists used a wide variety of interventions with their clients, often in the same session. In this paper, we frame this scenario as a multi-label classification task, and develop a group of models aimed at predicting a wide variety of therapist talk-turn level orientations. Our models achieve F1 macro scores of 0.5, with the class F1 ranging from 0.36 to 0.67. We present analyses which offer insights into the capability of such models to capture psychotherapy approaches, and which may complement human judgment.</abstract>
      <url hash="a0084eeb">2022.clpsych-1.5</url>
      <attachment type="appendix" hash="ac8c2fe4">2022.clpsych-1.5.appendix.pdf</attachment>
      <bibkey>mehta-etal-2022-psychotherapy</bibkey>
      <doi>10.18653/v1/2022.clpsych-1.5</doi>
      <revision id="1" href="2022.clpsych-1.5v1" hash="9ee21e43"/>
      <revision id="2" href="2022.clpsych-1.5v2" hash="a0084eeb" date="2022-07-18">Included a COI statement in Section 10 and changed the section header accordingly.</revision>
      <video href="2022.clpsych-1.5.mp4"/>
    </paper>
    <paper id="6">
      <title>Then and Now: Quantifying the Longitudinal Validity of Self-Disclosed Depression Diagnoses</title>
      <author><first>Keith</first><last>Harrigian</last></author>
      <author><first>Mark</first><last>Dredze</last></author>
      <pages>59-75</pages>
      <abstract>Self-disclosed mental health diagnoses, which serve as ground truth annotations of mental health status in the absence of clinical measures, underpin the conclusions behind most computational studies of mental health language from the last decade. However, psychiatric conditions are dynamic; a prior depression diagnosis may no longer be indicative of an individual’s mental health, either due to treatment or other mitigating factors. We ask: to what extent are self-disclosures of mental health diagnoses actually relevant over time? We analyze recent activity from individuals who disclosed a depression diagnosis on social media over five years ago and, in turn, acquire a new understanding of how presentations of mental health status on social media manifest longitudinally. We also provide expanded evidence for the presence of personality-related biases in datasets curated using self-disclosed diagnoses. Our findings motivate three practical recommendations for improving mental health datasets curated using self-disclosed diagnoses:1) Annotate diagnosis dates and psychiatric comorbidities2) Sample control groups using propensity score matching3) Identify and remove spurious correlations introduced by selection bias</abstract>
      <url hash="9b308dcb">2022.clpsych-1.6</url>
      <bibkey>harrigian-dredze-2022-now</bibkey>
      <doi>10.18653/v1/2022.clpsych-1.6</doi>
    </paper>
    <paper id="7">
      <title>Tracking Mental Health Risks and Coping Strategies in Healthcare Workers’ Online Conversations Across the <fixed-case>COVID</fixed-case>-19 Pandemic</title>
      <author><first>Molly</first><last>Ireland</last></author>
      <author><first>Kaitlin</first><last>Adams</last></author>
      <author><first>Sean</first><last>Farrell</last></author>
      <pages>76-88</pages>
      <abstract>The mental health risks of the COVID-19 pandemic are magnified for medical professionals, such as doctors and nurses. To track conversational markers of psychological distress and coping strategies, we analyzed 67.25 million words written by self-identified healthcare workers (N = 5,409; 60.5% nurses, 40.5% physicians) on Reddit beginning in June 2019. Dictionary-based measures revealed increasing emotionality (including more positive and negative emotion and more swearing), social withdrawal (less affiliation and empathy, more “they” pronouns), and self-distancing (fewer “I” pronouns) over time. Several effects were strongest for conversations that were least health-focused and self-relevant, suggesting that long-term changes in social and emotional behavior are general and not limited to personal or work-related experiences. Understanding protective and risky coping strategies used by healthcare workers during the pandemic is fundamental for maintaining mental health among front-line workers during periods of chronic stress, such as the COVID-19 pandemic.</abstract>
      <url hash="e3706c1d">2022.clpsych-1.7</url>
      <bibkey>ireland-etal-2022-tracking</bibkey>
      <doi>10.18653/v1/2022.clpsych-1.7</doi>
      <video href="2022.clpsych-1.7.mp4"/>
    </paper>
    <paper id="8">
      <title>Are You Really Okay? A Transfer Learning-based Approach for Identification of Underlying Mental Illnesses</title>
      <author><first>Ankit</first><last>Aich</last></author>
      <author><first>Natalie</first><last>Parde</last></author>
      <pages>89-104</pages>
      <abstract>Evidence has demonstrated the presence of similarities in language use across people with various mental health conditions. In this work, we investigate these correlations both in terms of literature and as a data analysis problem. We also introduce a novel state-of-the-art transfer learning-based approach that learns from linguistic feature spaces of previous conditions and predicts unknown ones. Our model achieves strong performance, with F1 scores of 0.75, 0.80, and 0.76 at detecting depression, stress, and suicidal ideation in a first-of-its-kind transfer task and offering promising evidence that language models can harness learned patterns from known mental health conditions to aid in their prediction of others that may lie latent.</abstract>
      <url hash="0f2ba16a">2022.clpsych-1.8</url>
      <bibkey>aich-parde-2022-really</bibkey>
      <doi>10.18653/v1/2022.clpsych-1.8</doi>
    </paper>
    <paper id="9">
      <title>Comparing emotion feature extraction approaches for predicting depression and anxiety</title>
      <author><first>Hannah</first><last>Burkhardt</last></author>
      <author><first>Michael</first><last>Pullmann</last></author>
      <author><first>Thomas</first><last>Hull</last></author>
      <author><first>Patricia</first><last>Areán</last></author>
      <author><first>Trevor</first><last>Cohen</last></author>
      <pages>105-115</pages>
      <abstract>The increasing adoption of message-based behavioral therapy enables new approaches to assessing mental health using linguistic analysis of patient-generated text. Word counting approaches have demonstrated utility for linguistic feature extraction, but deep learning methods hold additional promise given recent advances in this area. We evaluated the utility of emotion features extracted using a BERT-based model in comparison to emotions extracted using word counts as predictors of symptom severity in a large set of messages from text-based therapy sessions involving over 6,500 unique patients, accompanied by data from repeatedly administered symptom scale measurements. BERT-based emotion features explained more variance in regression models of symptom severity, and improved predictive modeling of scale-derived diagnostic categories. However, LIWC categories that are not directly related to emotions provided valuable and complementary information for modeling of symptom severity, indicating a role for both approaches in inferring the mental states underlying patient-generated language.</abstract>
      <url hash="aaf5cc20">2022.clpsych-1.9</url>
      <bibkey>burkhardt-etal-2022-comparing</bibkey>
      <doi>10.18653/v1/2022.clpsych-1.9</doi>
      <video href="2022.clpsych-1.9.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/goemotions">GoEmotions</pwcdataset>
    </paper>
    <paper id="10">
      <title>Detecting Suicidality with a Contextual Graph Neural Network</title>
      <author><first>Daeun</first><last>Lee</last></author>
      <author><first>Migyeong</first><last>Kang</last></author>
      <author><first>Minji</first><last>Kim</last></author>
      <author><first>Jinyoung</first><last>Han</last></author>
      <pages>116-125</pages>
      <abstract>Discovering individuals’ suicidality on social media has become increasingly important. Many researchers have studied to detect suicidality by using a suicide dictionary. However, while prior work focused on matching a word in a post with a suicide dictionary without considering contexts, little attention has been paid to how the word can be associated with the suicide-related context. To address this problem, we propose a suicidality detection model based on a graph neural network to grasp the dynamic semantic information of the suicide vocabulary by learning the relations between a given post and words. The extensive evaluation demonstrates that the proposed model achieves higher performance than the state-of-the-art methods. We believe the proposed model has great utility in identifying the suicidality of individuals and hence preventing individuals from potential suicide risks at an early stage.</abstract>
      <url hash="96b4e815">2022.clpsych-1.10</url>
      <attachment type="appendix" hash="951c9c6c">2022.clpsych-1.10.appendix.pdf</attachment>
      <bibkey>lee-etal-2022-detecting</bibkey>
      <doi>10.18653/v1/2022.clpsych-1.10</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/reddit">Reddit</pwcdataset>
    </paper>
    <paper id="11">
      <title>Identifying Distorted Thinking in Patient-Therapist Text Message Exchanges by Leveraging Dynamic Multi-Turn Context</title>
      <author><first>Kevin</first><last>Lybarger</last></author>
      <author><first>Justin</first><last>Tauscher</last></author>
      <author><first>Xiruo</first><last>Ding</last></author>
      <author><first>Dror</first><last>Ben-zeev</last></author>
      <author><first>Trevor</first><last>Cohen</last></author>
      <pages>126-136</pages>
      <abstract>There is growing evidence that mobile text message exchanges between patients and therapists can augment traditional cognitive behavioral therapy. The automatic characterization of patient thinking patterns in this asynchronous text communication may guide treatment and assist in therapist training. In this work, we automatically identify distorted thinking in text-based patient-therapist exchanges, investigating the role of conversation history (context) in distortion prediction. We identify six unique types of cognitive distortions and utilize BERT-based architectures to represent text messages within the context of the conversation. We propose two approaches for leveraging dynamic conversation context in model training. By representing the text messages within the context of the broader patient-therapist conversation, the models better emulate the therapist’s task of recognizing distorted thoughts. This multi-turn classification approach also leverages the clustering of distorted thinking in the conversation timeline. We demonstrate that including conversation context, including the proposed dynamic context methods, improves distortion prediction performance. The proposed architectures and conversation encoding approaches achieve performance comparable to inter-rater agreement. The presence of any distorted thinking is identified with relatively high performance at 0.73 F1, significantly outperforming the best context-agnostic models (0.68 F1).</abstract>
      <url hash="f85a97db">2022.clpsych-1.11</url>
      <bibkey>lybarger-etal-2022-identifying</bibkey>
      <doi>10.18653/v1/2022.clpsych-1.11</doi>
      <video href="2022.clpsych-1.11.mp4"/>
    </paper>
    <paper id="12">
      <title>Learning to Automate Follow-up Question Generation using Process Knowledge for Depression Triage on <fixed-case>R</fixed-case>eddit Posts</title>
      <author><first>Shrey</first><last>Gupta</last></author>
      <author><first>Anmol</first><last>Agarwal</last></author>
      <author><first>Manas</first><last>Gaur</last></author>
      <author><first>Kaushik</first><last>Roy</last></author>
      <author><first>Vignesh</first><last>Narayanan</last></author>
      <author><first>Ponnurangam</first><last>Kumaraguru</last></author>
      <author><first>Amit</first><last>Sheth</last></author>
      <pages>137-147</pages>
      <abstract>Conversational Agents (CAs) powered with deep language models (DLMs) have shown tremendous promise in the domain of mental health. Prominently, the CAs have been used to provide informational or therapeutic services (e.g., cognitive behavioral therapy) to patients. However, the utility of CAs to assist in mental health triaging has not been explored in the existing work as it requires a controlled generation of follow-up questions (FQs), which are often initiated and guided by the mental health professionals (MHPs) in clinical settings. In the context of ‘depression’, our experiments show that DLMs coupled with process knowledge in a mental health questionnaire generate 12.54% and 9.37% better FQs based on similarity and longest common subsequence matches to questions in the PHQ-9 dataset respectively, when compared with DLMs without process knowledge support.Despite coupling with process knowledge, we find that DLMs are still prone to hallucination, i.e., generating redundant, irrelevant, and unsafe FQs. We demonstrate the challenge of using existing datasets to train a DLM for generating FQs that adhere to clinical process knowledge. To address this limitation, we prepared an extended PHQ-9 based dataset, PRIMATE, in collaboration with MHPs. PRIMATE contains annotations regarding whether a particular question in the PHQ-9 dataset has already been answered in the user’s initial description of the mental health condition. We used PRIMATE to train a DLM in a supervised setting to identify which of the PHQ-9 questions can be answered directly from the user’s post and which ones would require more information from the user. Using performance analysis based on MCC scores, we show that PRIMATE is appropriate for identifying questions in PHQ-9 that could guide generative DLMs towards controlled FQ generation (with minimal hallucination) suitable for aiding triaging. The dataset created as a part of this research can be obtained from https://github.com/primate-mh/Primate2022</abstract>
      <url hash="4352b15e">2022.clpsych-1.12</url>
      <attachment type="appendix" hash="34b18cbf">2022.clpsych-1.12.appendix.pdf</attachment>
      <bibkey>gupta-etal-2022-learning</bibkey>
      <doi>10.18653/v1/2022.clpsych-1.12</doi>
      <video href="2022.clpsych-1.12.mp4"/>
      <pwccode url="https://github.com/primate-mh/primate2022" additional="false">primate-mh/primate2022</pwccode>
    </paper>
    <paper id="13">
      <title>Masking Morphosyntactic Categories to Evaluate Salience for Schizophrenia Diagnosis</title>
      <author><first>Yaara</first><last>Shriki</last></author>
      <author><first>Ido</first><last>Ziv</last></author>
      <author><first>Nachum</first><last>Dershowitz</last></author>
      <author><first>Eiran</first><last>Harel</last></author>
      <author><first>Kfir</first><last>Bar</last></author>
      <pages>148-157</pages>
      <abstract>Natural language processing tools have been shown to be effective for detecting symptoms of schizophrenia in transcribed speech. We analyze and assess the contribution of the various syntactic and morphological categories towards successful machine classification of texts produced by subjects with schizophrenia and by others. Specifically, we fine-tune a language model for the classification task, and mask all words that are attributed with each category of interest. The speech samples were generated in a controlled way by interviewing inpatients who were officially diagnosed with schizophrenia, and a corresponding group of healthy controls. All participants are native Hebrew speakers. Our results show that nouns are the most significant category for classification performance.</abstract>
      <url hash="a631e1c9">2022.clpsych-1.13</url>
      <bibkey>shriki-etal-2022-masking</bibkey>
      <doi>10.18653/v1/2022.clpsych-1.13</doi>
    </paper>
    <paper id="14">
      <title>Measuring Linguistic Synchrony in Psychotherapy</title>
      <author><first>Natalie</first><last>Shapira</last></author>
      <author><first>Dana</first><last>Atzil-Slonim</last></author>
      <author><first>Rivka</first><last>Tuval Mashiach</last></author>
      <author><first>Ori</first><last>Shapira</last></author>
      <pages>158-176</pages>
      <abstract>We study the phenomenon of linguistic synchrony between clients and therapists in a psychotherapy process. Linguistic Synchrony (LS) can be viewed as any observed interdependence or association between more than one person?s linguistic behavior. Accordingly, we establish LS as a methodological task. We suggest a LS function that applies a linguistic similarity measure based on the Jensen-Shannon distance across the observed part-of-speech tag distributions (JSDuPos) of the speakers in different time frames. We perform a study over a unique corpus of 872 transcribed sessions, covering 68 clients and 59 therapists. After establishing the presence of client-therapist LS, we verify its association with therapeutic alliance and treatment outcome (measured using WAI and ORS), and additionally analyse the behavior of JSDuPos throughout treatment.Results indicate that (1) higher linguistic similarity at the session level associates with higher therapeutic alliance as reported by the client and therapist at the end of the session, (2) higher linguistic similarity at the session level associates with higher level of treatment outcome as reported by the client at the beginnings of the next sessions, (3) there is a significant linear increase in linguistic similarity throughout treatment, (4) surprisingly, higher LS associates with lower treatment outcome. Finally, we demonstrate how the LS function can be used to interpret and explore the mechanism for synchrony.</abstract>
      <url hash="41b73d44">2022.clpsych-1.14</url>
      <attachment type="appendix" hash="47181297">2022.clpsych-1.14.appendix.pdf</attachment>
      <bibkey>shapira-etal-2022-measuring</bibkey>
      <doi>10.18653/v1/2022.clpsych-1.14</doi>
    </paper>
    <paper id="15">
      <title>Nonsuicidal Self-Injury and Substance Use Disorders: A Shared Language of Addiction</title>
      <author><first>Salvatore</first><last>Giorgi</last></author>
      <author><first>Mckenzie</first><last>Himelein-wachowiak</last></author>
      <author><first>Daniel</first><last>Habib</last></author>
      <author><first>Lyle</first><last>Ungar</last></author>
      <author><first>Brenda</first><last>Curtis</last></author>
      <pages>177-183</pages>
      <abstract>Nonsuicidal self-injury (NSSI), or the deliberate injuring of one?s body without intending to die, has been shown to exhibit many similarities to substance use disorders (SUDs), including population-level characteristics, impulsivity traits, and comorbidity with other mental disorders. Research has further shown that people who self-injure adopt language common in SUD recovery communities (e.g., “clean”, “relapse”, “addiction,” and celebratory language about sobriety milestones). In this study, we investigate the shared language of NSSI and SUD by comparing discussions on public Reddit forums related to self-injury and drug addiction. To this end, we build a set of LDA topics across both NSSI and SUD Reddit users and show that shared language across the two domains includes SUD recovery language in addition to other themes common to support forums (e.g., requests for help and gratitude). Next, we examine Reddit-wide posting activity and note that users posting in {emph{r/selfharm} also post in many mental health-related subreddits, while users of drug addiction related subreddits do not, despite high comorbidity between NSSI and SUDs. These results show that while people who self-injure may contextualize their disorder as an addiction, their posting habits demonstrate comorbidities with other mental disorders more so than their counterparts in recovery from SUDs. These observations have clinical implications for people who self-injure and seek support by sharing their experiences online.</abstract>
      <url hash="392fb8fe">2022.clpsych-1.15</url>
      <attachment type="appendix" hash="38e36477">2022.clpsych-1.15.appendix.pdf</attachment>
      <bibkey>giorgi-etal-2022-nonsuicidal</bibkey>
      <doi>10.18653/v1/2022.clpsych-1.15</doi>
    </paper>
    <paper id="16">
      <title>Overview of the <fixed-case>CLP</fixed-case>sych 2022 Shared Task: Capturing Moments of Change in Longitudinal User Posts</title>
      <author><first>Adam</first><last>Tsakalidis</last></author>
      <author><first>Jenny</first><last>Chim</last></author>
      <author><first>Iman Munire</first><last>Bilal</last></author>
      <author><first>Ayah</first><last>Zirikly</last></author>
      <author><first>Dana</first><last>Atzil-Slonim</last></author>
      <author><first>Federico</first><last>Nanni</last></author>
      <author><first>Philip</first><last>Resnik</last></author>
      <author><first>Manas</first><last>Gaur</last></author>
      <author><first>Kaushik</first><last>Roy</last></author>
      <author><first>Becky</first><last>Inkster</last></author>
      <author><first>Jeff</first><last>Leintz</last></author>
      <author><first>Maria</first><last>Liakata</last></author>
      <pages>184-198</pages>
      <abstract>We provide an overview of the CLPsych 2022 Shared Task, which focusses on the automatic identification of ‘Moments of Change’ in lon- gitudinal posts by individuals on social media and its connection with information regarding mental health . This year’s task introduced the notion of longitudinal modelling of the text generated by an individual online over time, along with appropriate temporally sen- sitive evaluation metrics. The Shared Task con- sisted of two subtasks: (a) the main task of cap- turing changes in an individual’s mood (dras- tic changes-‘Switches’- and gradual changes -‘Escalations’- on the basis of textual content shared online; and subsequently (b) the sub- task of identifying the suicide risk level of an individual – a continuation of the CLPsych 2019 Shared Task– where participants were encouraged to explore how the identification of changes in mood in task (a) can help with assessing suicidality risk in task (b).</abstract>
      <url hash="60b25ed4">2022.clpsych-1.16</url>
      <bibkey>tsakalidis-etal-2022-overview</bibkey>
      <doi>10.18653/v1/2022.clpsych-1.16</doi>
    </paper>
    <paper id="17">
      <title>Approximate Nearest Neighbour Extraction Techniques and Neural Networks for Suicide Risk Prediction in the <fixed-case>CLP</fixed-case>sych 2022 Shared Task</title>
      <author><first>Hermenegildo</first><last>Fabregat Marcos</last></author>
      <author><first>Ander</first><last>Cejudo</last></author>
      <author><first>Juan</first><last>Martinez-romo</last></author>
      <author><first>Alicia</first><last>Perez</last></author>
      <author><first>Lourdes</first><last>Araujo</last></author>
      <author><first>Nuria</first><last>Lebea</last></author>
      <author><first>Maite</first><last>Oronoz</last></author>
      <author><first>Arantza</first><last>Casillas</last></author>
      <pages>199-204</pages>
      <abstract>This paper describes the participation of our group on the CLPsych 2022 shared task.For task A, which tries to capture changes in mood over time, we have applied an Approximate Nearest Neighbour (ANN) extraction technique with the aim of relabelling the user messages according to their proximity, based on the representation of these messages in a vector space. Regarding the subtask B, we have used the output of the subtask A to train a Recurrent Neural Network (RNN) to predict the risk of suicide at the user level.The results obtained are very competitive considering that our team was one of the few that made use of the organisers’ proposed virtual environment and also made use of the Task A output to predict the Task B results.</abstract>
      <url hash="ffc57143">2022.clpsych-1.17</url>
      <bibkey>fabregat-marcos-etal-2022-approximate</bibkey>
      <doi>10.18653/v1/2022.clpsych-1.17</doi>
    </paper>
    <paper id="18">
      <title>Capturing Changes in Mood Over Time in Longitudinal Data Using Ensemble Methodologies</title>
      <author><first>Ana-Maria</first><last>Bucur</last></author>
      <author><first>Hyewon</first><last>Jang</last></author>
      <author><first>Farhana Ferdousi</first><last>Liza</last></author>
      <pages>205-212</pages>
      <abstract>This paper presents the system description of team BLUE for Task A of the CLPsych 2022 Shared Task on identifying changes in mood and behaviour in longitudinal textual data. These moments of change are signals that can be used to screen and prevent suicide attempts.To detect these changes, we experimented with several text representation methods, such as TF-IDF, sentence embeddings, emotion-informed embeddings and several classical machine learning classifiers. We chose to submit three runs of ensemble systems based on maximum voting on the predictions from the best performing models. Of the nine participating teams in Task A, our team ranked second in the Precision-oriented Coverage-based Evaluation, with a score of 0.499. Our best system was an ensemble of Support Vector Machine, Logistic Regression, and Adaptive Boosting classifiers using emotion-informed embeddings as input representation that can model both the linguistic and emotional information found in users? posts.</abstract>
      <url hash="44f9d61f">2022.clpsych-1.18</url>
      <attachment type="appendix" hash="06f75ca4">2022.clpsych-1.18.appendix.pdf</attachment>
      <bibkey>bucur-etal-2022-capturing</bibkey>
      <doi>10.18653/v1/2022.clpsych-1.18</doi>
      <video href="2022.clpsych-1.18.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/emotion">CARER</pwcdataset>
    </paper>
    <paper id="19">
      <title>Detecting Moments of Change and Suicidal Risks in Longitudinal User Texts Using Multi-task Learning</title>
      <author><first>Tayyaba</first><last>Azim</last></author>
      <author><first>Loitongbam</first><last>Gyanendro Singh</last></author>
      <author><first>Stuart E.</first><last>Middleton</last></author>
      <pages>213-218</pages>
      <abstract>This work describes the classification system proposed for the Computational Linguistics and Clinical Psychology (CLPsych) Shared Task 2022. We propose the use of multitask learning approach with bidirectional long-short term memory (Bi-LSTM) model for predicting changes in user’s mood and their suicidal risk level. The two classification tasks have been solved independently or in an augmented way previously, where the output of one task is leveraged for learning another task, however this work proposes an ‘all-in-one’ framework that jointly learns the related mental health tasks. The experimental results suggest that the proposed multi-task framework outperforms the remaining single-task frameworks submitted to the challenge and evaluated via timeline based and coverage based performance metrics shared by the organisers. We also assess the potential of using various types of feature embedding schemes that could prove useful in initialising the Bi-LSTM model for better multitask learning in the mental health domain.</abstract>
      <url hash="66236a40">2022.clpsych-1.19</url>
      <bibkey>azim-etal-2022-detecting</bibkey>
      <doi>10.18653/v1/2022.clpsych-1.19</doi>
      <pwccode url="https://github.com/stuartemiddleton/uos_clpsych" additional="false">stuartemiddleton/uos_clpsych</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/tweeteval">TweetEval</pwcdataset>
    </paper>
    <paper id="20">
      <title>Emotionally-Informed Models for Detecting Moments of Change and Suicide Risk Levels in Longitudinal Social Media Data</title>
      <author><first>Ulya</first><last>Bayram</last></author>
      <author><first>Lamia</first><last>Benhiba</last></author>
      <pages>219-225</pages>
      <abstract>In this shared task, we focus on detecting mental health signals in Reddit users’ posts through two main challenges: A) capturing mood changes (anomalies) from the longitudinal set of posts (called timelines), and B) assessing the users’ suicide risk-levels. Our approaches leverage emotion recognition on linguistic content by computing emotion/sentiment scores using pre-trained BERTs on users’ posts and feeding them to machine learning models, including XGBoost, Bi-LSTM, and logistic regression. For Task-A, we detect longitudinal anomalies using a sequence-to-sequence (seq2seq) autoencoder and capture regions of mood deviations. For Task-B, our two models utilize the BERT emotion/sentiment scores. The first computes emotion bandwidths and merges them with n-gram features, and employs logistic regression to detect users’ suicide risk levels. The second model predicts suicide risk on the timeline level using a Bi-LSTM on Task-A results and sentiment scores. Our results outperformed most participating teams and ranked in the top three in Task-A. In Task-B, our methods surpass all others and return the best macro and micro F1 scores.</abstract>
      <url hash="ef9bde0e">2022.clpsych-1.20</url>
      <bibkey>bayram-benhiba-2022-emotionally</bibkey>
      <doi>10.18653/v1/2022.clpsych-1.20</doi>
      <video href="2022.clpsych-1.20.mp4"/>
    </paper>
    <paper id="21">
      <title>Exploring transformers and time lag features for predicting changes in mood over time</title>
      <author><first>John</first><last>Culnan</last></author>
      <author><first>Damian</first><last>Romero Diaz</last></author>
      <author><first>Steven</first><last>Bethard</last></author>
      <pages>226-231</pages>
      <abstract>This paper presents transformer-based models created for the CLPsych 2022 shared task. Using posts from Reddit users over a period of time, we aim to predict changes in mood from post to post. We test models that preserve timeline information through explicit ordering of posts as well as those that do not order posts but preserve features on the length of time between a user’s posts. We find that a model with temporal information may provide slight benefits over the same model without such information, although a RoBERTa transformer model provides enough information to make similar predictions without custom-encoded time information.</abstract>
      <url hash="bf5fc2bb">2022.clpsych-1.21</url>
      <bibkey>culnan-etal-2022-exploring</bibkey>
      <doi>10.18653/v1/2022.clpsych-1.21</doi>
      <video href="2022.clpsych-1.21.mp4"/>
    </paper>
    <paper id="22">
      <title>Multi-Task Learning to Capture Changes in Mood Over Time</title>
      <author><first>Prasadith</first><last>Kirinde Gamaarachchige</last></author>
      <author><first>Ahmed</first><last>Husseini Orabi</last></author>
      <author><first>Mahmoud</first><last>Husseini Orabi</last></author>
      <author><first>Diana</first><last>Inkpen</last></author>
      <pages>232-238</pages>
      <abstract>This paper investigates the impact of using Multi-Task Learning (MTL) to predict mood changes over time for each individual (social media user). The presented models were developed as a part of the Computational Linguistics and Clinical Psychology (CLPsych) 2022 shared task. Given the limited number of Reddit social media users, as well as their posts, we decided to experiment with different multi-task learning architectures to identify to what extent knowledge can be shared among similar tasks. Due to class imbalance at both post and user levels and to accommodate task alignment, we randomly sampled an equal number of instances from the respective classes and performed ensemble learning to reduce prediction variance. Faced with several constraints, we managed to produce competitive results that could provide insights into the use of multi-task learning to identify mood changes over time and suicide ideation risk.</abstract>
      <url hash="80bd6eab">2022.clpsych-1.22</url>
      <bibkey>kirinde-gamaarachchige-etal-2022-multi</bibkey>
      <doi>10.18653/v1/2022.clpsych-1.22</doi>
      <video href="2022.clpsych-1.22.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/smhd">SMHD</pwcdataset>
    </paper>
    <paper id="23">
      <title>Predicting Moments of Mood Changes Overtime from Imbalanced Social Media Data</title>
      <author><first>Falwah</first><last>Alhamed</last></author>
      <author><first>Julia</first><last>Ive</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <pages>239-244</pages>
      <abstract>Social media data have been used in research for many years to understand users’ mental health. In this paper, using user-generated content we aim to achieve two goals: the first is detecting moments of mood change over time using timelines of users from Reddit. The second is predicting the degree of suicide risk as a user-level classification task. We used different approaches to address longitudinal modelling as well as the problem of the severely imbalanced dataset. Using BERT with undersampling techniques performed the best among other LSTM and basic random forests models for the first task. For the second task, extracting some features related to suicide from posts’ text contributed to the overall performance improvement. Specifically, a number of suicide-related words in a post as a feature improved the accuracy by 17{%.</abstract>
      <url hash="39b43446">2022.clpsych-1.23</url>
      <bibkey>alhamed-etal-2022-predicting</bibkey>
      <doi>10.18653/v1/2022.clpsych-1.23</doi>
    </paper>
    <paper id="24">
      <title>Towards Capturing Changes in Mood and Identifying Suicidality Risk</title>
      <author><first>Sravani</first><last>Boinepelli</last></author>
      <author><first>Shivansh</first><last>Subramanian</last></author>
      <author><first>Abhijeeth</first><last>Singam</last></author>
      <author><first>Tathagata</first><last>Raha</last></author>
      <author><first>Vasudeva</first><last>Varma</last></author>
      <pages>245-250</pages>
      <abstract>This paper describes our systems for CLPsych?s 2022 Shared Task. Subtask A involves capturing moments of change in an individual?s mood over time, while Subtask B asked us to identify the suicidality risk of a user. We explore multiple machine learning and deep learning methods for the same, taking real-life applicability into account while considering the design of the architecture. Our team achieved top results in different categories for both subtasks. Task A was evaluated on a post-level (using macro averaged F1) and on a window-based timeline level (using macro-averaged precision and recall). We scored a post-level F1 of 0.520 and ranked second with a timeline-level recall of 0.646. Task B was a user-level task where we also came in second with a micro F1 of 0.520 and scored third place on the leaderboard with a macro F1 of 0.380.</abstract>
      <url hash="4b0add8c">2022.clpsych-1.24</url>
      <bibkey>boinepelli-etal-2022-towards</bibkey>
      <doi>10.18653/v1/2022.clpsych-1.24</doi>
    </paper>
    <paper id="25">
      <title><fixed-case>WWBP</fixed-case>-<fixed-case>SQT</fixed-case>-lite: Multi-level Models and Difference Embeddings for Moments of Change Identification in Mental Health Forums</title>
      <author><first>Adithya</first><last>V Ganesan</last></author>
      <author><first>Vasudha</first><last>Varadarajan</last></author>
      <author><first>Juhi</first><last>Mittal</last></author>
      <author><first>Shashanka</first><last>Subrahmanya</last></author>
      <author><first>Matthew</first><last>Matero</last></author>
      <author><first>Nikita</first><last>Soni</last></author>
      <author><first>Sharath Chandra</first><last>Guntuku</last></author>
      <author><first>Johannes</first><last>Eichstaedt</last></author>
      <author><first>H. Andrew</first><last>Schwartz</last></author>
      <pages>251-258</pages>
      <abstract>Psychological states unfold dynamically; to understand and measure mental health at scale we need to detect and measure these changes from sequences of online posts. We evaluate two approaches to capturing psychological changes in text: the first relies on computing the difference between the embedding of a message with the one that precedes it, the second relies on a “human-aware” multi-level recurrent transformer (HaRT). The mood changes of timeline posts of users were annotated into three classes, ‘ordinary,’ ‘switching’ (positive to negative or vice versa) and ‘escalations’ (increasing in intensity). For classifying these mood changes, the difference-between-embeddings technique – applied to RoBERTa embeddings – showed the highest overall F1 score (0.61) across the three different classes on the test set. The technique particularly outperformed the HaRT transformer (and other baselines) in the detection of switches (F1 = .33) and escalations (F1 = .61).Consistent with the literature, the language use patterns associated with mental-health related constructs in prior work (including depression, stress, anger and anxiety) predicted both mood switches and escalations.</abstract>
      <url hash="c0cdeb82">2022.clpsych-1.25</url>
      <bibkey>v-ganesan-etal-2022-wwbp</bibkey>
      <doi>10.18653/v1/2022.clpsych-1.25</doi>
      <video href="2022.clpsych-1.25.mp4"/>
    </paper>
  </volume>
</collection>
