<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.cmlc">
  <volume id="1">
    <meta>
      <booktitle>Proceedings of the 8th Workshop on Challenges in the Management of Large Corpora</booktitle>
      <editor><first>Piotr</first><last>Bański</last></editor>
      <editor><first>Adrien</first><last>Barbaresi</last></editor>
      <editor><first>Simon</first><last>Clematide</last></editor>
      <editor><first>Marc</first><last>Kupietz</last></editor>
      <editor><first>Harald</first><last>Lüngen</last></editor>
      <editor><first>Ines</first><last>Pisetta</last></editor>
      <publisher>European Language Ressources Association</publisher>
      <address>Marseille, France</address>
      <month>May</month>
      <year>2020</year>
      <isbn>979-10-95546-61-0</isbn>
    </meta>
    <frontmatter>
      <url hash="1a1df40e">2020.cmlc-1.0</url>
    </frontmatter>
    <paper id="1">
      <title>Addressing Cha(lle)nges in Long-Term Archiving of Large Corpora</title>
      <author><first>Denis</first><last>Arnold</last></author>
      <author><first>Bernhard</first><last>Fisseni</last></author>
      <author><first>Pawel</first><last>Kamocki</last></author>
      <author><first>Oliver</first><last>Schonefeld</last></author>
      <author><first>Marc</first><last>Kupietz</last></author>
      <author><first>Thomas</first><last>Schmidt</last></author>
      <pages>1–9</pages>
      <abstract>This paper addresses long-term archival for large corpora. Three aspects specific to language resources are focused, namely (1) the removal of resources for legal reasons, (2) versioning of (unchanged) objects in constantly growing resources, especially where objects can be part of multiple releases but also part of different collections, and (3) the conversion of data to new formats for digital preservation. It is motivated why language resources may have to be changed, and why formats may need to be converted. As a solution, the use of an intermediate proxy object called a signpost is suggested. The approach will be exemplified with respect to the corpora of the Leibniz Institute for the German Language in Mannheim, namely the German Reference Corpus (DeReKo) and the Archive for Spoken German (AGD).</abstract>
      <url hash="d31464b5">2020.cmlc-1.1</url>
      <language>eng</language>
    </paper>
    <paper id="2">
      <title>Evaluating a Dependency Parser on <fixed-case>D</fixed-case>e<fixed-case>R</fixed-case>e<fixed-case>K</fixed-case>o</title>
      <author><first>Peter</first><last>Fankhauser</last></author>
      <author><first>Bich-Ngoc</first><last>Do</last></author>
      <author><first>Marc</first><last>Kupietz</last></author>
      <pages>10–14</pages>
      <abstract>We evaluate a graph-based dependency parser on DeReKo, a large corpus of contemporary German. The dependency parser is trained on the German dataset from the SPMRL 2014 Shared Task which contains text from the news domain, whereas DeReKo also covers other domains including fiction, science, and technology. To avoid the need for costly manual annotation of the corpus, we use the parser’s probability estimates for unlabeled and labeled attachment as main evaluation criterion. We show that these probability estimates are highly correlated with the actual attachment scores on a manually annotated test set. On this basis, we compare estimated parsing scores for the individual domains in DeReKo, and show that the scores decrease with increasing distance of a domain to the training corpus.</abstract>
      <url hash="dacb980e">2020.cmlc-1.2</url>
      <language>eng</language>
    </paper>
    <paper id="3">
      <title><fixed-case>F</fixed-case>rench Contextualized Word-Embeddings with a sip of <fixed-case>C</fixed-case>a<fixed-case>B</fixed-case>e<fixed-case>R</fixed-case>net: a New <fixed-case>F</fixed-case>rench Balanced Reference Corpus</title>
      <author><first>Murielle</first><last>Popa-Fabre</last></author>
      <author><first>Pedro Javier</first><last>Ortiz Suárez</last></author>
      <author><first>Benoît</first><last>Sagot</last></author>
      <author><first>Éric</first><last>de la Clergerie</last></author>
      <pages>15–23</pages>
      <abstract>This paper investigates the impact of different types and size of training corpora on language models. By asking the fundamental question of quality versus quantity, we compare four French corpora by pre-training four different ELMos and evaluating them on dependency parsing, POS-tagging and Named Entities Recognition downstream tasks. We present and asses the relevance of a new balanced French corpus, CaBeRnet, that features a representative range of language usage, including a balanced variety of genres (oral transcriptions, newspapers, popular magazines, technical reports, fiction, academic texts), in oral and written styles. We hypothesize that a linguistically representative corpus will allow the language models to be more efficient, and therefore yield better evaluation scores on different evaluation sets and tasks. This paper offers three main contributions: (1) two newly built corpora: (a) CaBeRnet, a French Balanced Reference Corpus and (b) CBT-fr a domain-specific corpus having both oral and written style in youth literature, (2) five versions of ELMo pre-trained on differently built corpora, and (3) a whole array of computational results on downstream tasks that deepen our understanding of the effects of corpus balance and register in NLP evaluation.</abstract>
      <url hash="e1d4beb6">2020.cmlc-1.3</url>
      <language>eng</language>
    </paper>
    <paper id="4">
      <title>Geoparsing the historical Gazetteers of <fixed-case>S</fixed-case>cotland: accurately computing location in mass digitised texts</title>
      <author><first>Rosa</first><last>Filgueira</last></author>
      <author><first>Claire</first><last>Grover</last></author>
      <author><first>Melissa</first><last>Terras</last></author>
      <author><first>Beatrice</first><last>Alex</last></author>
      <pages>24–30</pages>
      <abstract>This paper describes work in progress on devising automatic and parallel methods for geoparsing large digital historical textual data by combining the strengths of three natural language processing (NLP) tools, the Edinburgh Geoparser, spaCy and defoe, and employing different tokenisation and named entity recognition (NER) techniques. We apply these tools to a large collection of nineteenth century Scottish geographical dictionaries, and describe preliminary results obtained when processing this data.</abstract>
      <url hash="a3c3df6b">2020.cmlc-1.4</url>
      <language>eng</language>
    </paper>
    <paper id="5">
      <title>The Corpus Query Middleware of Tomorrow – A Proposal for a Hybrid Corpus Query Architecture</title>
      <author><first>Markus</first><last>Gärtner</last></author>
      <pages>31–39</pages>
      <abstract>Development of dozens of specialized corpus query systems and languages over the past decades has let to a diverse but also fragmented landscape. Today we are faced with a plethora of query tools that each provide unique features, but which are also not interoperable and often rely on very specific database back-ends or formats for storage. This severely hampers usability both for end users that want to query different corpora and also for corpus designers that wish to provide users with an interface for querying and exploration. We propose a hybrid corpus query architecture as a first step to overcoming this issue. It takes the form of a middleware system between user front-ends and optional database or text indexing solutions as back-ends. At its core is a custom query evaluation engine for index-less processing of corpus queries. With a flexible JSON-LD query protocol the approach allows communication with back-end systems to partially solve queries and offset some of the performance penalties imposed by the custom evaluation engine. This paper outlines the details of our first draft of aforementioned architecture.</abstract>
      <url hash="8778b5da">2020.cmlc-1.5</url>
      <language>eng</language>
    </paper>
    <paper id="6">
      <title>Using full text indices for querying spoken language data</title>
      <author><first>Elena</first><last>Frick</last></author>
      <author><first>Thomas</first><last>Schmidt</last></author>
      <pages>40–46</pages>
      <abstract>As a part of the ZuMult-project, we are currently modelling a backend architecture that should provide query access to corpora from the Archive of Spoken German (AGD) at the Leibniz-Institute for the German Language (IDS). We are exploring how to reuse existing search engine frameworks providing full text indices and allowing to query corpora by one of the corpus query languages (QLs) established and actively used in the corpus research community. For this purpose, we tested MTAS - an open source Lucene-based search engine for querying on text with multilevel annotations. We applied MTAS on three oral corpora stored in the TEI-based ISO standard for transcriptions of spoken language (ISO 24624:2016). These corpora differ from the corpus data that MTAS was developed for, because they include interactions with two and more speakers and are enriched, inter alia, with timeline-based annotations. In this contribution, we report our test results and address issues that arise when search frameworks originally developed for querying written corpora are being transferred into the field of spoken language.</abstract>
      <url hash="1278b161">2020.cmlc-1.6</url>
      <language>eng</language>
    </paper>
    <paper id="7">
      <title>Challenges for Making Use of a Large Text Corpus such as the ‘<fixed-case>AAC</fixed-case> – <fixed-case>A</fixed-case>ustrian Academy Corpus’ for Digital Literary Studies</title>
      <author><first>Hanno</first><last>Biber</last></author>
      <pages>47–51</pages>
      <abstract>The challenges for making use of a large text corpus such as the ‘AAC – Austrian Academy Corpus’ for the purposes of digital literary studies will be addressed in this presentation. The research question of how to use a digital text corpus of considerable size for such a specific research purpose is of interest for corpus research in general as it is of interest for digital literary text studies which rely to a large extent on large digital text corpora. The observations of the usage of lexical entities such as words, word forms, multi word units and many other linguistic units determine the way in which texts are being studied and explored. Larger entities have to be taken into account as well, which is why questions of semantic analysis and larger structures come into play. The texts of the AAC – Austrian Academy Corpus which was founded in 2001 are German language texts of historical and cultural significance from the time between 1848 and 1989. The aim of this study is to present possible research questions for corpus-based methodological approaches for the digital study of literary texts and to give examples of early experiments and experiences with making use of a large text corpus for these research purposes.</abstract>
      <url hash="3cb038f4">2020.cmlc-1.7</url>
      <language>eng</language>
    </paper>
    <paper id="8">
      <title><fixed-case>C</fixed-case>zech National Corpus in 2020: Recent Developments and Future Outlook</title>
      <author><first>Michal</first><last>Kren</last></author>
      <pages>52–57</pages>
      <abstract>The paper overviews the state of implementation of the Czech National Corpus (CNC) in all the main areas of its operation: corpus compilation, annotation, application development and user services. As the focus is on the recent development, some of the areas are described in more detail than the others. Close attention is paid to the data collection and, in particular, to the description of web application development. This is not only because CNC has recently seen a significant progress in this area, but also because we believe that end-user web applications shape the way linguists and other scholars think about the language data and about the range of possibilities they offer. This consideration is even more important given the variability of the CNC corpora.</abstract>
      <url hash="ef7eed8e">2020.cmlc-1.8</url>
      <language>eng</language>
    </paper>
    <paper id="9">
      <title>Adding a Syntactic Annotation Level to the Corpus of Contemporary <fixed-case>R</fixed-case>omanian Language</title>
      <author><first>Andrei</first><last>Scutelnicu</last></author>
      <author><first>Catalina</first><last>Maranduc</last></author>
      <author><first>Dan</first><last>Cristea</last></author>
      <pages>58–62</pages>
      <abstract>In this paper we present an experiment of augmenting the Corpus of Contemporary Romanian Language (CoRoLa) with the syntactic level of annotations, which would allow users to address queries about the syntax of Romanian sentences, in the Universal Dependency model. After a short introduction of CoRoLa, we describe the treebanks used to train the dependency parser, we show the evaluation results and the process of upgrading CoRoLa with the new level of annotations. The parser displaying the best accuracy with respect to recognition of heads and relations, out of three variants trained on manually built treebanks, was chosen. Keywords: Syntactic annotation, treebank, corpus, maltparser</abstract>
      <url hash="390a7492">2020.cmlc-1.9</url>
      <language>eng</language>
    </paper>
  </volume>
</collection>
