<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.bea">
  <volume id="1" ingest-date="2024-06-20" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 19th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2024)</booktitle>
      <editor><first>Ekaterina</first><last>Kochmar</last></editor>
      <editor><first>Marie</first><last>Bexte</last></editor>
      <editor><first>Jill</first><last>Burstein</last></editor>
      <editor><first>Andrea</first><last>Horbach</last></editor>
      <editor><first>Ronja</first><last>Laarmann-Quante</last></editor>
      <editor><first>Anaïs</first><last>Tack</last></editor>
      <editor><first>Victoria</first><last>Yaneva</last></editor>
      <editor><first>Zheng</first><last>Yuan</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Mexico City, Mexico</address>
      <month>June</month>
      <year>2024</year>
      <url hash="3de88a92">2024.bea-1</url>
      <venue>bea</venue>
    </meta>
    <frontmatter>
      <url hash="1099b797">2024.bea-1.0</url>
      <bibkey>bea-2024-innovative</bibkey>
    </frontmatter>
    <paper id="1">
      <title>How Good are <fixed-case>M</fixed-case>odern <fixed-case>LLM</fixed-case>s in Generating Relevant and High-Quality Questions at Different Bloom’s Skill Levels for <fixed-case>I</fixed-case>ndian High School Social Science Curriculum?</title>
      <author><first>Nicy</first><last>Scaria</last><affiliation>Indian Institute of Science</affiliation></author>
      <author><first>Suma Dharani</first><last>Chenna</last><affiliation>Indian Institute of Science</affiliation></author>
      <author><first>Deepak</first><last>Subramani</last><affiliation>Indian Institute of Science</affiliation></author>
      <pages>1-10</pages>
      <abstract>The creation of pedagogically effective questions is a challenge for teachers and requires significant time and meticulous planning, especially in resource-constrained economies. For example, in India, assessments for social science in high schools are characterized by rote memorization without regard to higher-order skill levels. Automated educational question generation (AEQG) using large language models (LLMs) has the potential to help teachers develop assessments at scale. However, it is important to evaluate the quality and relevance of these questions. In this study, we examine the ability of different LLMs (Falcon 40B, Llama2 70B, Palm 2, GPT 3.5, and GPT 4) to generate relevant and high-quality questions of different cognitive levels, as defined by Bloom’s taxonomy. We prompt each model with the same instructions and different contexts to generate 510 questions in the social science curriculum of a state educational board in India. Two human experts used a nine-item rubric to assess linguistic correctness, pedagogical relevance and quality, and adherence to Bloom’s skill levels. Our results showed that 91.56% of the LLM-generated questions were relevant and of high quality. This suggests that LLMs can generate relevant and high-quality questions at different cognitive levels, making them useful for creating assessments for scaling education in resource-constrained economies.</abstract>
      <url hash="b96ce05b">2024.bea-1.1</url>
      <bibkey>scaria-etal-2024-good</bibkey>
    </paper>
    <paper id="2">
      <title>Synthetic Data Generation for Low-resource Grammatical Error Correction with Tagged Corruption Models</title>
      <author><first>Felix</first><last>Stahlberg</last><affiliation>Google Research</affiliation></author>
      <author><first>Shankar</first><last>Kumar</last><affiliation>Google</affiliation></author>
      <pages>11-16</pages>
      <abstract>Tagged corruption models provide precise control over the introduction of grammatical errors into clean text. This capability has made them a powerful tool for generating pre-training data for grammatical error correction (GEC) in English. In this work, we demonstrate their application to four languages with substantially fewer GEC resources than English: German, Romanian, Russian, and Spanish. We release a new tagged-corruption dataset consisting of 2.5M examples per language that was generated by a fine-tuned PaLM 2 foundation model. Pre-training on tagged corruptions yields consistent gains across all four languages, especially for small model sizes and languages with limited human-labelled data.</abstract>
      <url hash="a9fd4b6d">2024.bea-1.2</url>
      <bibkey>stahlberg-kumar-2024-synthetic</bibkey>
    </paper>
    <paper id="3">
      <title>Pillars of Grammatical Error Correction: Comprehensive Inspection Of Contemporary Approaches In The Era of Large Language Models</title>
      <author><first>Kostiantyn</first><last>Omelianchuk</last><affiliation>Grammarly</affiliation></author>
      <author><first>Andrii</first><last>Liubonko</last><affiliation>Grammarly</affiliation></author>
      <author><first>Oleksandr</first><last>Skurzhanskyi</last><affiliation>Applied Research Scientist</affiliation></author>
      <author><first>Artem</first><last>Chernodub</last><affiliation>Grammarly</affiliation></author>
      <author><first>Oleksandr</first><last>Korniienko</last><affiliation>UCU</affiliation></author>
      <author><first>Igor</first><last>Samokhin</last><affiliation>independent</affiliation></author>
      <pages>17-33</pages>
      <abstract>In this paper, we carry out experimental research on Grammatical Error Correction, delving into the nuances of single-model systems, comparing the efficiency of ensembling and ranking methods, and exploring the application of large language models to GEC as single-model systems, as parts of ensembles, and as ranking methods. We set new state-of-the-art records with F_0.5 scores of 72.8 on CoNLL-2014-test and 81.4 on BEA-test, respectively. To support further advancements in GEC and ensure the reproducibility of our research, we make our code, trained models, and systems’ outputs publicly available, facilitating future findings.</abstract>
      <url hash="060ed83f">2024.bea-1.3</url>
      <bibkey>omelianchuk-etal-2024-pillars</bibkey>
    </paper>
    <paper id="4">
      <title>Using Adaptive Empathetic Responses for Teaching <fixed-case>E</fixed-case>nglish</title>
      <author><first>Li</first><last>Siyan</last><affiliation>Columbia University</affiliation></author>
      <author><first>Teresa</first><last>Shao</last><affiliation>Columbia University</affiliation></author>
      <author><first>Julia</first><last>Hirschberg</last><affiliation>Columbia University in the City of New York</affiliation></author>
      <author><first>Zhou</first><last>Yu</last><affiliation>Columbia University</affiliation></author>
      <pages>34-53</pages>
      <abstract>Existing English-teaching chatbots rarely incorporate empathy explicitly in their feedback, but empathetic feedback could help keep students engaged and reduce learner anxiety. Toward this end, we propose the task of negative emotion detection via audio, for recognizing empathetic feedback opportunities in language learning. We then build the first spoken English-teaching chatbot with adaptive, empathetic feedback. This feedback is synthesized through automatic prompt optimization of ChatGPT and is evaluated with English learners. We demonstrate the effectiveness of our system through a preliminary user study.</abstract>
      <url hash="711a5d47">2024.bea-1.4</url>
      <bibkey>siyan-etal-2024-using</bibkey>
    </paper>
    <paper id="5">
      <title>Beyond Flesch-Kincaid: Prompt-based Metrics Improve Difficulty Classification of Educational Texts</title>
      <author><first>Donya</first><last>Rooein</last><affiliation>Bocconi University</affiliation></author>
      <author><first>Paul</first><last>Röttger</last><affiliation>University of Oxford</affiliation></author>
      <author><first>Anastassia</first><last>Shaitarova</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Dirk</first><last>Hovy</last><affiliation>Bocconi University</affiliation></author>
      <pages>54-67</pages>
      <abstract>Using large language models (LLMs) for educational applications like dialogue-based teaching is a hot topic. Effective teaching, however, requires teachers to adapt the difficulty of content and explanations to the education level of their students. Even the best LLMs today struggle to do this well. If we want to improve LLMs on this adaptation task, we need to be able to measure adaptation success reliably. However, current Static metrics for text difficulty, like the Flesch-Kincaid Reading Ease score, are known to be crude and brittle. We, therefore, introduce and evaluate a new set of Prompt-based metrics for text difficulty. Based on a user study, we create Prompt-based metrics as inputs for LLMs. They leverage LLM’s general language understanding capabilities to capture more abstract and complex features than Static metrics. Regression experiments show that adding our Prompt-based metrics significantly improves text difficulty classification over Static metrics alone. Our results demonstrate the promise of using LLMs to evaluate text adaptation to different education levels.</abstract>
      <url hash="6a7e9010">2024.bea-1.5</url>
      <bibkey>rooein-etal-2024-beyond</bibkey>
    </paper>
    <paper id="6">
      <title>Large Language Models Are State-of-the-Art Evaluator for Grammatical Error Correction</title>
      <author><first>Masamune</first><last>Kobayashi</last><affiliation>Tokyo Metropolitan University</affiliation></author>
      <author><first>Masato</first><last>Mita</last><affiliation>CyberAgent Inc.</affiliation></author>
      <author><first>Mamoru</first><last>Komachi</last><affiliation>Hitotsubashi University</affiliation></author>
      <pages>68-77</pages>
      <abstract>Large Language Models (LLMs) have been reported to outperform existing automatic evaluation metrics in some tasks, such as text summarization and machine translation. However, there has been a lack of research on LLMs as evaluators in grammatical error correction (GEC). In this study, we investigate the performance of LLMs in GEC evaluation by employing prompts designed to incorporate various evaluation criteria inspired by previous research. Our extensive experimental results demonstrate that GPT-4 achieved Kendall’s rank correlation of 0.662 with human judgments, surpassing all existing methods. Furthermore, in recent GEC evaluations, we have underscored the significance of the LLMs scale and particularly emphasized the importance of fluency among evaluation criteria.</abstract>
      <url hash="8ec3994b">2024.bea-1.6</url>
      <bibkey>kobayashi-etal-2024-large</bibkey>
    </paper>
    <paper id="7">
      <title>Can Language Models Guess Your Identity? Analyzing Demographic Biases in <fixed-case>AI</fixed-case> Essay Scoring</title>
      <author><first>Alexander</first><last>Kwako</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Christopher</first><last>Ormerod</last><affiliation>Cambium Assessment</affiliation></author>
      <pages>78-86</pages>
      <abstract>Large language models (LLMs) are increasingly used for automated scoring of student essays. However, these models may perpetuate societal biases if not carefully monitored. This study analyzes potential biases in an LLM (XLNet) trained to score persuasive student essays, based on data from the PERSUADE corpus. XLNet achieved strong performance based on quadratic weighted kappa, standardized mean difference, and exact agreement with human scores. Using available metadata, we performed analyses of scoring differences across gender, race/ethnicity, English language learning status, socioeconomic status, and disability status. Automated scores exhibited small magnifications of marginal differences in human scoring, favoring female students over males and White students over Black students. To further probe potential biases, we found that separate XLNet classifiers and XLNet hidden states weakly predicted demographic membership. Overall, results reinforce the need for continued fairness analyses as use of LLMs expands in education.</abstract>
      <url hash="b4a2a63f">2024.bea-1.7</url>
      <bibkey>kwako-ormerod-2024-language</bibkey>
    </paper>
    <paper id="8">
      <title>Automated Scoring of Clinical Patient Notes: Findings From the <fixed-case>K</fixed-case>aggle Competition and Their Translation into Practice</title>
      <author><first>Victoria</first><last>Yaneva</last><affiliation>National Board of Medical Examiners</affiliation></author>
      <author><first>King Yiu</first><last>Suen</last><affiliation>National Board of Medical Examiners</affiliation></author>
      <author><first>Le An</first><last>Ha</last><affiliation>Ho Chi Minh City University of Foreign Languages - Information Technology (HUFLIT)</affiliation></author>
      <author><first>Janet</first><last>Mee</last><affiliation>National Board of Medical Examiners</affiliation></author>
      <author><first>Milton</first><last>Quranda</last><affiliation>National Board of Medical Examiners</affiliation></author>
      <author><first>Polina</first><last>Harik</last><affiliation>NBME</affiliation></author>
      <pages>87-98</pages>
      <abstract>Scoring clinical patient notes (PNs) written by medical students is a necessary but resource-intensive task in medical education. This paper describes the organization and key lessons from a Kaggle competition on automated scoring of such notes. 1,471 teams took part in the competition and developed an extensive, publicly available code repository of varying solutions evaluated over the first public dataset for this task. The most successful approaches from this community effort are described and utilized in the development of a PN scoring system. We discuss the choice of models and system architecture with a view to operational use and scalability, and evaluate its performance on both the public Kaggle data (10 clinical cases, 43,985 PNs) and an extended internal dataset (178 clinical cases, 6,940 PNs). The results show that the system significantly outperforms a state-of-the-art existing tool for PN scoring and that task-adaptive pretraining using masked language modeling can be an effective approach even for small training samples.</abstract>
      <url hash="7585607f">2024.bea-1.8</url>
      <bibkey>yaneva-etal-2024-automated</bibkey>
    </paper>
    <paper id="9">
      <title>A World <fixed-case>CLASSE</fixed-case> Student Summary Corpus</title>
      <author><first>Scott</first><last>Crossley</last><affiliation>Georgia State University</affiliation></author>
      <author><first>Perpetual</first><last>Baffour</last><affiliation>The Learning Agency Lab</affiliation></author>
      <author><first>Mihai</first><last>Dascalu</last><affiliation>University Politehnica of Bucharest</affiliation></author>
      <author><first>Stefan</first><last>Ruseti</last><affiliation>University Politehnica of Bucharest</affiliation></author>
      <pages>99-107</pages>
      <abstract>This paper introduces the Common Lit Augmented Student Summary Evaluation (CLASSE) corpus. The corpus comprises 11,213 summaries written over six prompts by students in grades 3-12 while using the CommonLit website. Each summary was scored by expert human raters on analytic features related to main points, details, organization, voice, paraphrasing, and language beyond the source text. The human scores were aggregated into two component scores related to content and wording. The final corpus was the focus of a Kaggle competition hosted in late 2022 and completed in 2023 in which over 2,000 teams participated. The paper includes a baseline scoring model for the corpus based on a Large Language Model (Longformer model). The paper also provides an overview of the winning models from the Kaggle competition.</abstract>
      <url hash="03089dcb">2024.bea-1.9</url>
      <bibkey>crossley-etal-2024-world</bibkey>
    </paper>
    <paper id="10">
      <title>Improving Socratic Question Generation using Data Augmentation and Preference Optimization</title>
      <author><first>Nischal</first><last>Ashok Kumar</last><affiliation>University of Massachusetts Amherst</affiliation></author>
      <author><first>Andrew</first><last>Lan</last><affiliation>University of Massachusetts Amherst</affiliation></author>
      <pages>108-118</pages>
      <abstract>The Socratic method is a way of guiding students toward solving a problem independently without directly revealing the solution to the problem by asking incremental questions. Although this method has been shown to significantly improve student learning outcomes, it remains a complex labor-intensive task for instructors. Large language models (LLMs) can be used to augment human effort by automatically generating Socratic questions for students. However, existing methods that involve prompting these LLMs sometimes produce invalid outputs, e.g., those that directly reveal the solution to the problem or provide irrelevant or premature questions. To alleviate this problem, inspired by reinforcement learning with AI feedback (RLAIF), we first propose a data augmentation method to enrich existing Socratic questioning datasets with questions that are invalid in specific ways. Also, we propose a method to optimize open-source LLMs such as LLama 2 to prefer ground-truth questions over generated invalid ones, using direct preference optimization (DPO). Our experiments on a Socratic questions dataset for student code debugging show that a DPO-optimized LLama 2-7B model can effectively avoid generating invalid questions, and as a result, outperforms existing state-of-the-art prompting methods.</abstract>
      <url hash="db40fbe9">2024.bea-1.10</url>
      <bibkey>ashok-kumar-lan-2024-improving</bibkey>
    </paper>
    <paper id="11">
      <title>Scoring with Confidence? – Exploring High-confidence Scoring for Saving Manual Grading Effort</title>
      <author><first>Marie</first><last>Bexte</last><affiliation>FernUniversität in Hagen</affiliation></author>
      <author><first>Andrea</first><last>Horbach</last><affiliation>Universität Hildesheim</affiliation></author>
      <author><first>Lena</first><last>Schützler</last><affiliation>FernUniversität in Hagen</affiliation></author>
      <author><first>Oliver</first><last>Christ</last><affiliation>FernUniversität in Hagen</affiliation></author>
      <author><first>Torsten</first><last>Zesch</last><affiliation>Computational Linguistics, FernUniversität in Hagen</affiliation></author>
      <pages>119-124</pages>
      <abstract>A possible way to save manual grading effort in short answer scoring is to automatically score answers for which the classifier is highly confident. We explore the feasibility of this approach in a high-stakes exam setting, evaluating three different similarity-based scoring methods, where the similarity score is a direct proxy for model confidence. The decision on an appropriate level of confidence should ideally be made before scoring a new prompt. We thus probe to what extent confidence thresholds are consistent across different datasets and prompts. We find that high-confidence thresholds vary on a prompt-to-prompt basis, and that the overall potential of increased performance at a reasonable cost of additional manual effort is limited.</abstract>
      <url hash="84bbcb3b">2024.bea-1.11</url>
      <bibkey>bexte-etal-2024-scoring</bibkey>
    </paper>
    <paper id="12">
      <title>Predicting Initial Essay Quality Scores to Increase the Efficiency of Comparative Judgment Assessments</title>
      <author><first>Michiel</first><last>De Vrindt</last><affiliation>KU Leuven</affiliation></author>
      <author><first>Anaïs</first><last>Tack</last><affiliation>KU Leuven; imec; UCLouvain</affiliation></author>
      <author><first>Renske</first><last>Bouwer</last><affiliation>Institute for Language Sciences; University of Utrecht</affiliation></author>
      <author><first>Wim</first><last>Van Den Noortgate</last><affiliation>imec research group itec; KU Leuven</affiliation></author>
      <author><first>Marije</first><last>Lesterhuis</last><affiliation>Center for Research and Development of Health Professions Education; UMC Utrecht</affiliation></author>
      <pages>125-136</pages>
      <abstract>Comparative judgment (CJ) is a method that can be used to assess the writing quality of student essays based on repeated pairwise comparisons by multiple assessors. Although the assessment method is known to have high validity and reliability, it can be particularly inefficient, as assessors must make many judgments before the scores become reliable. Prior research has investigated methods to improve the efficiency of CJ, yet these methods introduce additional challenges, notably stemming from the initial lack of information at the start of the assessment, which is known as a cold-start problem. This paper reports on a study in which we predict the initial quality scores of essays to establish a warm start for CJ. To achieve this, we construct informative prior distributions for the quality scores based on the predicted initial quality scores. Through simulation studies, we demonstrate that our approach increases the efficiency of CJ: On average, assessors need to make 30% fewer judgments for each essay to reach an overall reliability level of 0.70.</abstract>
      <url hash="19f870ef">2024.bea-1.12</url>
      <bibkey>de-vrindt-etal-2024-predicting</bibkey>
    </paper>
    <paper id="13">
      <title>Improving Transfer Learning for Early Forecasting of Academic Performance by Contextualizing Language Models</title>
      <author><first>Ahatsham</first><last>Hayat</last><affiliation>University of Nebrasks-Lincoln</affiliation></author>
      <author><first>Bilal</first><last>Khan</last><affiliation>Lehigh University</affiliation></author>
      <author><first>Mohammad</first><last>Hasan</last><affiliation>University of Nebrasks-Lincoln</affiliation></author>
      <pages>137-148</pages>
      <abstract>This paper presents a cutting-edge method that harnesses contextualized language models (LMs) to significantly enhance the prediction of early academic performance in STEM fields. Our approach uniquely tackles the challenge of transfer learning with limited-domain data. Specifically, we overcome this challenge by contextualizing students’ cognitive trajectory data through the integration of both distal background factors (comprising academic information, demographic details, and socioeconomic indicators) and proximal non-cognitive factors (such as emotional engagement). By tapping into the rich prior knowledge encoded within pre-trained LMs, we effectively reframe academic performance forecasting as a task ideally suited for natural language processing.Our research rigorously examines three key aspects: the impact of data contextualization on prediction improvement, the effectiveness of our approach compared to traditional numeric-based models, and the influence of LM capacity on prediction accuracy. The results underscore the significant advantages of utilizing larger LMs with contextualized inputs, representing a notable advancement in the precision of early performance forecasts. These findings emphasize the importance of employing contextualized LMs to enhance artificial intelligence-driven educational support systems and overcome data scarcity challenges.</abstract>
      <url hash="d1ccab98">2024.bea-1.13</url>
      <bibkey>hayat-etal-2024-improving</bibkey>
    </paper>
    <paper id="14">
      <title>Can <fixed-case>GPT</fixed-case>-4 do <fixed-case>L</fixed-case>2 analytic assessment?</title>
      <author><first>Stefano</first><last>Banno</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Hari Krishna</first><last>Vydana</last><affiliation>Brno Universtiy of Technology</affiliation></author>
      <author><first>Kate</first><last>Knill</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Mark</first><last>Gales</last><affiliation>Cambridge University</affiliation></author>
      <pages>149-164</pages>
      <abstract>Automated essay scoring (AES) to evaluate second language (L2) proficiency has been a firmly established technology used in educational contexts for decades. Although holistic scoring has seen advancements in AES that match or even exceed human performance, analytic scoring still encounters issues as it inherits flaws and shortcomings from the human scoring process. The recent introduction of large language models presents new opportunities for automating the evaluation of specific aspects of L2 writing proficiency. In this paper, we perform a series of experiments using GPT-4 in a zero-shot fashion on a publicly available dataset annotated with holistic scores based on the Common European Framework of Reference and aim to extract detailed information about their underlying analytic components. We observe significant correlations between the automatically predicted analytic scores and multiple features associated with the individual proficiency components.</abstract>
      <url hash="b75c77ce">2024.bea-1.14</url>
      <bibkey>banno-etal-2024-gpt</bibkey>
    </paper>
    <paper id="15">
      <title>Using Program Repair as a Proxy for Language Models’ Feedback Ability in Programming Education</title>
      <author><first>Charles</first><last>Koutcheme</last><affiliation>Aalto University</affiliation></author>
      <author><first>Nicola</first><last>Dainese</last><affiliation>Aalto University</affiliation></author>
      <author><first>Arto</first><last>Hellas</last><affiliation>Aalto University</affiliation></author>
      <pages>165-181</pages>
      <abstract>One of the key challenges in programming education is being able to provide high-quality feedback to learners. Such feedback often includes explanations of the issues in students’ programs coupled with suggestions on how to fix these issues. Large language models (LLMs) have recently emerged as valuable tools that can help in this effort. In this article, we explore the relationship between the program repair ability of LLMs and their proficiency in providing natural language explanations of coding mistakes. We outline a benchmarking study that evaluates leading LLMs (including open-source ones) on program repair and explanation tasks. Our experiments study the capabilities of LLMs both on a course level and on a programming concept level, allowing us to assess whether the programming concepts practised in exercises with faulty student programs relate to the performance of the models. Our results highlight that LLMs proficient in repairing student programs tend to provide more complete and accurate natural language explanations of code issues. Overall, these results enhance our understanding of the role and capabilities of LLMs in programming education. Using program repair as a proxy for explanation evaluation opens the door for cost-effective assessment methods.</abstract>
      <url hash="42067429">2024.bea-1.15</url>
      <bibkey>koutcheme-etal-2024-using</bibkey>
    </paper>
    <paper id="16">
      <title>Automated Evaluation of Teacher Encouragement of Student-to-Student Interactions in a Simulated Classroom Discussion</title>
      <author><first>Michael</first><last>Ilagan</last><affiliation>Educational Testing Service</affiliation></author>
      <author><first>Beata</first><last>Beigman Klebanov</last><affiliation>Educational Testing Service</affiliation></author>
      <author><first>Jamie</first><last>Mikeska</last><affiliation>Educational Testing Service</affiliation></author>
      <pages>182-198</pages>
      <abstract>Leading students to engage in argumentation-focused discussions is a challenge for elementary school teachers, as doing so requires facilitating group discussions with student-to-student interaction. The Mystery Powder (MP) Task was designed to be used in online simulated classrooms to develop teachers’ skill in facilitating small group science discussions. In order to provide timely and scaleable feedback to teachers facilitating a discussion in the simulated classroom, we employ a hybrid modeling approach that successfully combines fine-tuned large language models with features capturing important elements of the discourse dynamic to evaluate MP discussion transcripts. To our knowledge, this is the first application of a hybrid model to automate evaluation of teacher discourse.</abstract>
      <url hash="08814bdd">2024.bea-1.16</url>
      <bibkey>ilagan-etal-2024-automated</bibkey>
    </paper>
    <paper id="17">
      <title>Explainable <fixed-case>AI</fixed-case> in Language Learning: Linking Empirical Evidence and Theoretical Concepts in Proficiency and Readability Modeling of <fixed-case>P</fixed-case>ortuguese</title>
      <author><first>Luisa</first><last>Ribeiro-Flucht</last><affiliation>University of Tuebingen</affiliation></author>
      <author><first>Xiaobin</first><last>Chen</last><affiliation>Tübingen Universität</affiliation></author>
      <author><first>Detmar</first><last>Meurers</last><affiliation>Leibniz-Institut für Wissensmedien (IWM)</affiliation></author>
      <pages>199-209</pages>
      <abstract>While machine learning methods have supported significantly improved results in education research, a common deficiency lies in the explainability of the result. Explainable AI (XAI) aims to fill that gap by providing transparent, conceptually understandable explanations for the classification decisions, enhancing human comprehension and trust in the outcomes. This paper explores an XAI approach to proficiency and readability assessment employing a comprehensive set of 465 linguistic complexity measures. We identify theoretical descriptions associating such measures with varying levels of proficiency and readability and validate them using cross-corpus experiments employing supervised machine learning and Shapley Additive Explanations. The results not only highlight the utility of a diverse set of complexity measures in effectively modeling proficiency and readability in Portuguese, achieving a state-of-the-art accuracy of 0.70 in the proficiency classification task and of 0.84 in the readability classification task, but they largely corroborate the theoretical research assumptions, especially in the lexical domain.</abstract>
      <url hash="faaa40b4">2024.bea-1.17</url>
      <bibkey>ribeiro-flucht-etal-2024-explainable</bibkey>
    </paper>
    <paper id="18">
      <title>Fairness in Automated Essay Scoring: A Comparative Analysis of Algorithms on <fixed-case>G</fixed-case>erman Learner Essays from Secondary Education</title>
      <author><first>Nils-Jonathan</first><last>Schaller</last><affiliation>Leibniz Institute for Science and Mathematics Education</affiliation></author>
      <author><first>Yuning</first><last>Ding</last><affiliation>FernUniversität in Hagen</affiliation></author>
      <author><first>Andrea</first><last>Horbach</last><affiliation>Universität Hildesheim</affiliation></author>
      <author><first>Jennifer</first><last>Meyer</last><affiliation>Leibniz Institute for Science and Mathematics Education</affiliation></author>
      <author><first>Thorben</first><last>Jansen</last><affiliation>Leibniz Institute for Science and Mathematics Education</affiliation></author>
      <pages>210-221</pages>
      <abstract>Pursuing educational equity, particularly in writing instruction, requires that all students receive fair (i.e., accurate and unbiased) assessment and feedback on their texts. Automated Essay Scoring (AES) algorithms have so far focused on optimizing the mean accuracy of their scores and paid less attention to fair scores for all subgroups, although research shows that students receive unfair scores on their essays in relation to demographic variables, which in turn are related to their writing competence. We add to the literature arguing that AES should also optimize for fairness by presenting insights on the fairness of scoring algorithms on a corpus of learner texts in the German language and introduce the novelty of examining fairness on psychological and demographic differences in addition to demographic differences. We compare shallow learning, deep learning, and large language models with full and skewed subsets of training data to investigate what is needed for fair scoring. The results show that training on a skewed subset of higher and lower cognitive ability students shows no bias but very low accuracy for students outside the training set. Our results highlight the need for specific training data on all relevant user groups, not only for demographic background variables but also for cognitive abilities as psychological student characteristics.</abstract>
      <url hash="18efd4da">2024.bea-1.18</url>
      <bibkey>schaller-etal-2024-fairness</bibkey>
    </paper>
    <paper id="19">
      <title>Improving Automated Distractor Generation for Math Multiple-choice Questions with Overgenerate-and-rank</title>
      <author><first>Alexander</first><last>Scarlatos</last><affiliation>University of Massachusetts Amherst</affiliation></author>
      <author><first>Wanyong</first><last>Feng</last><affiliation>umass amherst</affiliation></author>
      <author><first>Andrew</first><last>Lan</last><affiliation>University of Massachusetts Amherst</affiliation></author>
      <author><first>Simon</first><last>Woodhead</last><affiliation>Eedi</affiliation></author>
      <author><first>Digory</first><last>Smith</last><affiliation>Eedi Ltd</affiliation></author>
      <pages>222-231</pages>
      <abstract>Multiple-choice questions (MCQs) are commonly used across all levels of math education since they can be deployed and graded at a large scale. A critical component of MCQs is the distractors, i.e., incorrect answers crafted to reflect student errors or misconceptions. Automatically generating them in math MCQs, e.g., with large language models, has been challenging. In this work, we propose a novel method to enhance the quality of generated distractors through overgenerate-and-rank, training a ranking model to predict how likely distractors are to be selected by real students. Experimental results on a real-world dataset and human evaluation with math teachers show that our ranking model increases alignment with human-authored distractors, although human-authored ones are still preferred over generated ones.</abstract>
      <url hash="bcaf8cca">2024.bea-1.19</url>
      <bibkey>scarlatos-etal-2024-improving</bibkey>
    </paper>
    <paper id="20">
      <title>Identifying Fairness Issues in Automatically Generated Testing Content</title>
      <author><first>Kevin</first><last>Stowe</last><affiliation>Educational Testing Services (ETS)</affiliation></author>
      <pages>232-250</pages>
      <abstract>Natural language generation tools are powerful and effective for generating content. However, language models are known to display bias and fairness issues, making them impractical to deploy for many use cases. We here focus on how fairness issues impact automatically generated test content, which can have stringent requirements to ensure the test measures only what it was intended to measure. Specifically, we review test content generated for a large-scale standardized English proficiency test with the goal of identifying content that only pertains to a certain subset of the test population as well as content that has the potential to be upsetting or distracting to some test takers. Issues like these could inadvertently impact a test taker’s score and thus should be avoided. This kind of content does not reflect the more commonly-acknowledged biases, making it challenging even for modern models that contain safeguards. We build a dataset of 601 generated texts annotated for fairness and explore a variety of methods for classification: fine-tuning, topic-based classification, and prompting, including few-shot and self-correcting prompts. We find that combining prompt self-correction and few-shot learning performs best, yielding an F1 score of 0.79 on our held-out test set, while much smaller BERT- and topic-based models have competitive performance on out-of-domain data.</abstract>
      <url hash="670af68d">2024.bea-1.20</url>
      <bibkey>stowe-2024-identifying</bibkey>
    </paper>
    <paper id="21">
      <title>Towards Automated Document Revision: Grammatical Error Correction, Fluency Edits, and Beyond</title>
      <author><first>Masato</first><last>Mita</last><affiliation>CyberAgent Inc.</affiliation></author>
      <author><first>Keisuke</first><last>Sakaguchi</last><affiliation>Tohoku University / RIKEN</affiliation></author>
      <author><first>Masato</first><last>Hagiwara</last><affiliation>Octanove Labs LLC</affiliation></author>
      <author><first>Tomoya</first><last>Mizumoto</last><affiliation>LINE Corporation</affiliation></author>
      <author><first>Jun</first><last>Suzuki</last><affiliation>Tohoku University / RIKEN Center for AIP</affiliation></author>
      <author><first>Kentaro</first><last>Inui</last><affiliation>Tohoku University / Riken</affiliation></author>
      <pages>251-265</pages>
      <abstract>Natural language processing (NLP) technology has rapidly improved automated grammatical error correction (GEC) tasks, and the GEC community has begun to explore document-level revision. However, there are two major obstacles to going beyond automated <i>sentence-level</i> GEC to NLP-based document-level revision support: (1) there are few public corpora with document-level revisions annotated by professional editors, and (2) it is infeasible to obtain all possible references and evaluate revision quality using such references because there are infinite revision possibilities. To address these challenges, this paper proposes a new document revision corpus, Text Revision of ACL papers (TETRA), in which multiple professional editors have revised academic papers sampled from the ACL anthology. This corpus enables us to focus on document-level and paragraph-level edits, such as edits related to coherence and consistency. Additionally, as a case study using the TETRA corpus, we investigate reference-less and interpretable methods for meta-evaluation to detect quality improvements according to document revisions. We show the uniqueness of TETRA compared with existing document revision corpora and demonstrate that a fine-tuned pre-trained language model can discriminate the quality of documents after revision even when the difference is subtle.</abstract>
      <url hash="6c7dd07c">2024.bea-1.21</url>
      <bibkey>mita-etal-2024-towards</bibkey>
    </paper>
    <paper id="22">
      <title>Evaluating Vocabulary Usage in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Matthew</first><last>Durward</last><affiliation>University of Canterbury</affiliation></author>
      <author><first>Christopher</first><last>Thomson</last><affiliation>University of Canterbury</affiliation></author>
      <pages>266-282</pages>
      <abstract>The paper focuses on investigating vocabulary usage for AI and human-generated text. We define vocabulary usage in two ways: structural differences and keyword differences. Structural differences are evaluated by converting text into Vocabulary-Managment Profiles, initially used for discourse analysis. Through VMPs, we can treat the text data as a time series, allowing an evaluation by implementing Dynamic time-warping distance measures and subsequently deriving similarity scores to provide an indication of whether the structural dynamics in AI texts resemble human texts. To analyze keywords, we use a measure that emphasizes frequency and dispersion to source ‘key’ keywords. A qualitative approach is then applied, noting thematic differences between human and AI writing.</abstract>
      <url hash="539fbedb">2024.bea-1.22</url>
      <bibkey>durward-thomson-2024-evaluating</bibkey>
    </paper>
    <paper id="23">
      <title>Exploring <fixed-case>LLM</fixed-case> Prompting Strategies for Joint Essay Scoring and Feedback Generation</title>
      <author><first>Maja</first><last>Stahl</last><affiliation>Leibniz University Hannover</affiliation></author>
      <author><first>Leon</first><last>Biermann</last><affiliation>Leibniz University Hannover</affiliation></author>
      <author><first>Andreas</first><last>Nehring</last><affiliation>Leibniz University Hannover</affiliation></author>
      <author><first>Henning</first><last>Wachsmuth</last><affiliation>Leibniz University Hannover</affiliation></author>
      <pages>283-298</pages>
      <abstract>Individual feedback can help students improve their essay writing skills. However, the manual effort required to provide such feedback limits individualization in practice. Automatically-generated essay feedback may serve as an alternative to guide students at their own pace, convenience, and desired frequency. Large language models (LLMs) have demonstrated strong performance in generating coherent and contextually relevant text. Yet, their ability to provide helpful essay feedback is unclear. This work explores several prompting strategies for LLM-based zero-shot and few-shot generation of essay feedback. Inspired by Chain-of-Thought prompting, we study how and to what extent automated essay scoring (AES) can benefit the quality of generated feedback. We evaluate both the AES performance that LLMs can achieve with prompting only and the helpfulness of the generated essay feedback. Our results suggest that tackling AES and feedback generation jointly improves AES performance. However, while our manual evaluation emphasizes the quality of the generated essay feedback, the impact of essay scoring on the generated feedback remains low ultimately.</abstract>
      <url hash="c032fedf">2024.bea-1.23</url>
      <bibkey>stahl-etal-2024-exploring</bibkey>
    </paper>
    <paper id="24">
      <title>Towards Fine-Grained Pedagogical Control over <fixed-case>E</fixed-case>nglish Grammar Complexity in Educational Text Generation</title>
      <author><first>Dominik</first><last>Glandorf</last><affiliation>University of Tübingen, Yale University</affiliation></author>
      <author><first>Detmar</first><last>Meurers</last><affiliation>Leibniz-Institut für Wissensmedien (IWM)</affiliation></author>
      <pages>299-308</pages>
      <abstract>Teaching foreign languages and fostering language awareness in subject matter teaching requires a profound knowledge of grammar structures. Yet, while Large Language Models can act as tutors, it is unclear how effectively they can control grammar in generated text and adapt to learner needs. In this study, we investigate the ability of these models to exemplify pedagogically relevant grammar patterns, detect instances of grammar in a given text, and constrain text generation to grammar characteristic of a proficiency level. Concretely, we (1) evaluate the ability of GPT3.5 and GPT4 to generate example sentences for the standard English Grammar Profile CEFR taxonomy using few-shot in-context learning, (2) train BERT-based detectors with these generated examples of grammatical patterns, and (3) control the grammatical complexity of text generated by the open Mistral model by ranking sentence candidates with these detectors. We show that the grammar pattern instantiation quality is accurate but too homogeneous, and our classifiers successfully detect these patterns. A GPT-generated dataset of almost 1 million positive and negative examples for the English Grammar Profile is released with this work. With our method, Mistral’s output significantly increases the number of characteristic grammar constructions on the desired level, outperforming GPT4. This showcases how language domain knowledge can enhance Large Language Models for specific education needs, facilitating their effective use for intelligent tutor development and AI-generated materials. Code, models, and data are available at https://github.com/dominikglandorf/LLM-grammar.</abstract>
      <url hash="9dbc85bf">2024.bea-1.24</url>
      <bibkey>glandorf-meurers-2024-towards</bibkey>
    </paper>
    <paper id="25">
      <title><fixed-case>LLM</fixed-case>s in Short Answer Scoring: Limitations and Promise of Zero-Shot and Few-Shot Approaches</title>
      <author><first>Imran</first><last>Chamieh</last><affiliation>Hochschule Ruhr West</affiliation></author>
      <author><first>Torsten</first><last>Zesch</last><affiliation>Computational Linguistics, FernUniversität in Hagen</affiliation></author>
      <author><first>Klaus</first><last>Giebermann</last><affiliation>Hochschule Ruhr West</affiliation></author>
      <pages>309-315</pages>
      <abstract>In this work, we investigate the potential of Large Language Models (LLMs) for automated short answer scoring. We test zero-shot and few-shot settings, and compare with fine-tuned models and a supervised upper-bound, across three diverse datasets. Our results, in zero-shot and few-shot settings, show that LLMs perform poorly in these settings: LLMs have difficulty with tasks that require complex reasoning or domain-specific knowledge. While the models show promise on general knowledge tasks. The fine-tuned model come close to the supervised results but are still not feasible for application, highlighting potential overfitting issues. Overall, our study highlights the challenges and limitations of LLMs in short answer scoring and indicates that there currently seems to be no basis for applying LLMs for short answer scoring.</abstract>
      <url hash="175c2314">2024.bea-1.25</url>
      <bibkey>chamieh-etal-2024-llms</bibkey>
    </paper>
    <paper id="26">
      <title>Automated Essay Scoring Using Grammatical Variety and Errors with Multi-Task Learning and Item Response Theory</title>
      <author><first>Kosuke</first><last>Doi</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Katsuhito</first><last>Sudoh</last><affiliation>Nara Women’s University</affiliation></author>
      <author><first>Satoshi</first><last>Nakamura</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <pages>316-329</pages>
      <abstract>This study examines the effect of grammatical features in automatic essay scoring (AES). We use two kinds of grammatical features as input to an AES model: (1) grammatical items that writers used correctly in essays, and (2) the number of grammatical errors. Experimental results show that grammatical features improve the performance of AES models that predict the holistic scores of essays. Multi-task learning with the holistic and grammar scores, alongside using grammatical features, resulted in a larger improvement in model performance. We also show that a model using grammar abilities estimated using Item Response Theory (IRT) as the labels for the auxiliary task achieved comparable performance to when we used grammar scores assigned by human raters. In addition, we weight the grammatical features using IRT to consider the difficulty of grammatical items and writers’ grammar abilities. We found that weighting grammatical features with the difficulty led to further improvement in performance.</abstract>
      <url hash="36da7742">2024.bea-1.26</url>
      <bibkey>doi-etal-2024-automated</bibkey>
    </paper>
    <paper id="27">
      <title>Error Tracing in Programming: A Path to Personalised Feedback</title>
      <author><first>Martha</first><last>Shaka</last><affiliation>University College Cork</affiliation></author>
      <author><first>Diego</first><last>Carraro</last><affiliation>University College Cork</affiliation></author>
      <author><first>Kenneth</first><last>Brown</last><affiliation>University College Cork</affiliation></author>
      <pages>330-342</pages>
      <abstract>Knowledge tracing, the process of estimating students’ mastery over concepts from their past performance and predicting future outcomes, often relies on binary pass/fail predictions. This hinders the provision of specific feedback by failing to diagnose precise errors. We present an error-tracing model for learning programming that advances traditional knowledge tracing by employing multi-label classification to forecast exact errors students may generate. Through experiments on a real student dataset, we validate our approach and compare it to two baseline knowledge-tracing methods. We demonstrate an improved ability to predict specific errors, for first attempts and for subsequent attempts at individual problems.</abstract>
      <url hash="7c1e7efb">2024.bea-1.27</url>
      <bibkey>shaka-etal-2024-error</bibkey>
    </paper>
    <paper id="28">
      <title>Improving Readability Assessment with Ordinal Log-Loss</title>
      <author><first>Ho Hung</first><last>Lim</last><affiliation>City University of Hong Kong</affiliation></author>
      <author><first>John</first><last>Lee</last><affiliation>City University of Hong Kong</affiliation></author>
      <pages>343-350</pages>
      <abstract>Automatic Readability Assessment (ARA) predicts the level of difficulty of a text, e.g. at Grade 1 to Grade 12. ARA is an ordinal classification task since the predicted levels follow an underlying order, from easy to difficult. However, most neural ARA models ignore the distance between the gold level and predicted level, treating all levels as independent labels. This paper investigates whether distance-sensitive loss functions can improve ARA performance. We evaluate a variety of loss functions on neural ARA models, and show that ordinal log-loss can produce statistically significant improvement over the standard cross-entropy loss in terms of adjacent accuracy in a majority of our datasets.</abstract>
      <url hash="4705fc1b">2024.bea-1.28</url>
      <bibkey>lim-lee-2024-improving</bibkey>
    </paper>
    <paper id="29">
      <title>Automated Sentence Generation for a Spaced Repetition Software</title>
      <author><first>Benjamin</first><last>Paddags</last><affiliation>Department of Computer Science, University of Copenhagen</affiliation></author>
      <author><first>Daniel</first><last>Hershcovich</last><affiliation>Department of Computer Science, University of Copenhagen</affiliation></author>
      <author><first>Valkyrie</first><last>Savage</last><affiliation>Department of Computer Science, University of Copenhagen</affiliation></author>
      <pages>351-364</pages>
      <abstract>This paper presents and tests AllAI, an app that utilizes state-of-the-art NLP technology to assist second language acquisition through a novel method of sentence-based spaced repetition. Diverging from current single word or fixed sentence repetition, AllAI dynamically combines words due for repetition into sentences, enabling learning words in context while scheduling them independently. This research explores various suitable NLP paradigms and finds a few-shot prompting approach and retrieval of existing sentences from a corpus to yield the best correctness and scheduling accuracy. Subsequently, it evaluates these methods on 26 learners of Danish, finding a four-fold increase in the speed at which new words are learned, compared to conventional spaced repetition. Users of the retrieval method also reported significantly higher enjoyment, hinting at a higher user engagement.</abstract>
      <url hash="fa0b323a">2024.bea-1.29</url>
      <bibkey>paddags-etal-2024-automated</bibkey>
    </paper>
    <paper id="30">
      <title>Using Large Language Models to Assess Young Students’ Writing Revisions</title>
      <author><first>Tianwen</first><last>Li</last><affiliation>University of Pittsburgh</affiliation></author>
      <author><first>Zhexiong</first><last>Liu</last><affiliation>University of Pittsburgh</affiliation></author>
      <author><first>Lindsay</first><last>Matsumura</last><affiliation>University of Pittsburgh</affiliation></author>
      <author><first>Elaine</first><last>Wang</last><affiliation>RAND Corporation</affiliation></author>
      <author><first>Diane</first><last>Litman</last><affiliation>University of Pittsburgh</affiliation></author>
      <author><first>Richard</first><last>Correnti</last><affiliation>University of Pittsburgh</affiliation></author>
      <pages>365-380</pages>
      <abstract>Although effective revision is the crucial component of writing instruction, few automated writing evaluation (AWE) systems specifically focus on the quality of the revisions students undertake. In this study we investigate the use of a large language model (GPT-4) with Chain-of-Thought (CoT) prompting for assessing the quality of young students’ essay revisions aligned with the automated feedback messages they received. Results indicate that GPT-4 has significant potential for evaluating revision quality, particularly when detailed rubrics are included that describe common revision patterns shown by young writers. However, the addition of CoT prompting did not significantly improve performance. Further examination of GPT-4’s scoring performance across various levels of student writing proficiency revealed variable agreement with human ratings. The implications for improving AWE systems focusing on young students are discussed.</abstract>
      <url hash="9b28e29f">2024.bea-1.30</url>
      <bibkey>li-etal-2024-using</bibkey>
    </paper>
    <paper id="31">
      <title>Automatic Crossword Clues Extraction for Language Learning</title>
      <author><first>Santiago</first><last>Berruti</last><affiliation>Universidad de la República</affiliation></author>
      <author><first>Arturo</first><last>Collazo</last><affiliation>Universidad de la República</affiliation></author>
      <author><first>Diego</first><last>Sellanes</last><affiliation>Universidad de la República</affiliation></author>
      <author><first>Aiala</first><last>Rosá</last><affiliation>Instituto de Computación, Facultad de Ingeniería, Universidad de la República</affiliation></author>
      <author><first>Luis</first><last>Chiruzzo</last><affiliation>Universidad de la Republica</affiliation></author>
      <pages>381-390</pages>
      <abstract>Crosswords are a powerful tool that could be used in educational contexts, but they are not that easy to build. In this work, we present experiments on automatically extracting clues from simple texts that could be used to create crosswords, with the aim of using them in the context of teaching English at the beginner level. We present a series of heuristic patterns based on NLP tools for extracting clues, and use them to create a set of 2209 clues from a collection of 400 simple texts. Human annotators labeled the clues, and this dataset is used to evaluate the performance of our heuristics, and also to create a classifier that predicts if an extracted clue is correct. Our best classifier achieves an accuracy of 84%.</abstract>
      <url hash="0c0703fe">2024.bea-1.31</url>
      <bibkey>berruti-etal-2024-automatic</bibkey>
    </paper>
    <paper id="32">
      <title>Anna Karenina Strikes Again: Pre-Trained <fixed-case>LLM</fixed-case> Embeddings May Favor High-Performing Learners</title>
      <author><first>Abigail</first><last>Gurin Schleifer</last><affiliation>Weizmann Institute of Science</affiliation></author>
      <author><first>Beata</first><last>Beigman Klebanov</last><affiliation>Educational Testing Service</affiliation></author>
      <author><first>Moriah</first><last>Ariely</last><affiliation>Weizmann Institute of Science</affiliation></author>
      <author><first>Giora</first><last>Alexandron</last><affiliation>Weizmann Institute of Science</affiliation></author>
      <pages>391-402</pages>
      <abstract>Unsupervised clustering of student responses to open-ended questions into behavioral and cognitive profiles using pre-trained LLM embeddings is an emerging technique, but little is known about how well this captures pedagogically meaningful information. We investigate this in the context of student responses to open-ended questions in biology, which were previously analyzed and clustered by experts into theory-driven Knowledge Profiles (KPs).Comparing these KPs to ones discovered by purely data-driven clustering techniques, we report poor discoverability of most KPs, except for the ones including the correct answers. We trace this ‘discoverability bias’ to the representations of KPs in the pre-trained LLM embeddings space.</abstract>
      <url hash="1d8a7729">2024.bea-1.32</url>
      <bibkey>gurin-schleifer-etal-2024-anna</bibkey>
    </paper>
    <paper id="33">
      <title>Assessing Student Explanations with Large Language Models Using Fine-Tuning and Few-Shot Learning</title>
      <author><first>Dan</first><last>Carpenter</last><affiliation>North Carolina State University</affiliation></author>
      <author><first>Wookhee</first><last>Min</last><affiliation>North Carolina State University</affiliation></author>
      <author><first>Seung</first><last>Lee</last><affiliation>North Carolina State University</affiliation></author>
      <author><first>Gamze</first><last>Ozogul</last><affiliation>Indiana University</affiliation></author>
      <author><first>Xiaoying</first><last>Zheng</last><affiliation>Indiana University</affiliation></author>
      <author><first>James</first><last>Lester</last><affiliation>North Carolina State University</affiliation></author>
      <pages>403-413</pages>
      <abstract>The practice of soliciting self-explanations from students is widely recognized for its pedagogical benefits. However, the labor-intensive effort required to manually assess students’ explanations makes it impractical for classroom settings. As a result, many current solutions to gauge students’ understanding during class are often limited to multiple choice or fill-in-the-blank questions, which are less effective at exposing misconceptions or helping students to understand and integrate new concepts. Recent advances in large language models (LLMs) present an opportunity to assess student explanations in real-time, making explanation-based classroom response systems feasible for implementation. In this work, we investigate LLM-based approaches for assessing the correctness of students’ explanations in response to undergraduate computer science questions. We investigate alternative prompting approaches for multiple LLMs (i.e., Llama 2, GPT-3.5, and GPT-4) and compare their performance to FLAN-T5 models trained in a fine-tuning manner. The results suggest that the highest accuracy and weighted F1 score were achieved by fine-tuning FLAN-T5, while an in-context learning approach with GPT-4 attains the highest macro F1 score.</abstract>
      <url hash="fbd61740">2024.bea-1.33</url>
      <bibkey>carpenter-etal-2024-assessing</bibkey>
    </paper>
    <paper id="34">
      <title>Harnessing <fixed-case>GPT</fixed-case> to Study Second Language Learner Essays: Can We Use Perplexity to Determine Linguistic Competence?</title>
      <author><first>Ricardo</first><last>Muñoz Sánchez</last><affiliation>Språkbanken Text, Göteborgs Universitet</affiliation></author>
      <author><first>Simon</first><last>Dobnik</last><affiliation>University of Gothenburg</affiliation></author>
      <author><first>Elena</first><last>Volodina</last><affiliation>University of Gothenburg</affiliation></author>
      <pages>414-427</pages>
      <abstract>Generative language models have been used to study a wide variety of phenomena in NLP. This allows us to better understand the linguistic capabilities of those models and to better analyse the texts that we are working with. However, these studies have mainly focused on text generated by L1 speakers of English. In this paper we study whether linguistic competence of L2 learners of Swedish (through their performance on essay tasks) correlates with the perplexity of a decoder-only model (GPT-SW3). We run two sets of experiments, doing both quantitative and qualitative analyses for each of them. In the first one, we analyse the perplexities of the essays and compare them with the CEFR level of the essays, both from an essay-wide level and from a token level. In our second experiment, we compare the perplexity of an L2 learner essay with a normalised version of it. We find that the perplexity of essays tends to be lower for higher CEFR levels and that normalised essays have a lower perplexity than the original versions. Moreover, we find that different factors can lead to spikes in perplexity, not all of them being related to L2 learner language.</abstract>
      <url hash="b682368e">2024.bea-1.34</url>
      <bibkey>munoz-sanchez-etal-2024-harnessing</bibkey>
    </paper>
    <paper id="35">
      <title><fixed-case>BERT</fixed-case>-<fixed-case>IRT</fixed-case>: Accelerating Item Piloting with <fixed-case>BERT</fixed-case> Embeddings and Explainable <fixed-case>IRT</fixed-case> Models</title>
      <author><first>Kevin P.</first><last>Yancey</last><affiliation>Duolingo</affiliation></author>
      <author><first>Andrew</first><last>Runge</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Geoffrey</first><last>LaFlair</last><affiliation>Duolingo</affiliation></author>
      <author><first>Phoebe</first><last>Mulcaire</last><affiliation>Duolingo</affiliation></author>
      <pages>428-438</pages>
      <abstract>Estimating item parameters (e.g., the difficulty of a question) is an important part of modern high-stakes tests. Conventional methods require lengthy pilots to collect response data from a representative population of test-takers. The need for these pilots limit item bank size and how often those item banks can be refreshed, impacting test security, while increasing costs needed to support the test and taking up the test-taker’s valuable time. Our paper presents a novel explanatory item response theory (IRT) model, BERT-IRT, that has been used on the Duolingo English Test (DET), a high-stakes test of English, to reduce the length of pilots by a factor of 10. Our evaluation shows how the model uses BERT embeddings and engineered NLP features to accelerate item piloting without sacrificing criterion validity or reliability.</abstract>
      <url hash="a9a04389">2024.bea-1.35</url>
      <bibkey>yancey-etal-2024-bert</bibkey>
    </paper>
    <paper id="36">
      <title>Transfer Learning of Argument Mining in Student Essays</title>
      <author><first>Yuning</first><last>Ding</last><affiliation>FernUniversität in Hagen</affiliation></author>
      <author><first>Julian</first><last>Lohmann</last><affiliation>Institute for Psychology of Learning and Instruction (IPL), Kiel University (CAU)</affiliation></author>
      <author><first>Nils-Jonathan</first><last>Schaller</last><affiliation>Leibniz Institute for Science and Mathematics Education</affiliation></author>
      <author><first>Thorben</first><last>Jansen</last><affiliation>Leibniz Institute for Science and Mathematics Education</affiliation></author>
      <author><first>Andrea</first><last>Horbach</last><affiliation>Universität Hildesheim</affiliation></author>
      <pages>439-449</pages>
      <abstract>This paper explores the transferability of a cross-prompt argument mining model trained on argumentative essays authored by native English-speaking learners (EN-L1) across educational contexts and languages. Specifically, the adaptability of a multilingual transformer model is assessed through its application to comparable argumentative essays authored by English-as-a-foreign-language learners (EN-L2) for context transfer, and a dataset composed of essays written by native German learners (DE) for both language and task transfer. To separate language effects from educational context effects, we also perform experiments on a machine-translated version of the German dataset (DE-MT). Our findings demonstrate that, even under zero-shot conditions, a model trained on native English speakers exhibits satisfactory performance on the EN-L2/DE datasets. Machine translation does not substantially enhance this performance, suggesting that distinct writing styles across educational contexts impact performance more than language differences.</abstract>
      <url hash="0404ad86">2024.bea-1.36</url>
      <bibkey>ding-etal-2024-transfer</bibkey>
    </paper>
    <paper id="37">
      <title>Building Robust Content Scoring Models for Student Explanations of Social Justice Science Issues</title>
      <author><first>Allison</first><last>Bradford</last><affiliation>University of California, Berkeley</affiliation></author>
      <author><first>Kenneth</first><last>Steimel</last><affiliation>Cisco Systems Incorporated</affiliation></author>
      <author><first>Brian</first><last>Riordan</last><affiliation>unaffiliated</affiliation></author>
      <author><first>Marcia</first><last>Linn</last><affiliation>University of California, Berkeley</affiliation></author>
      <pages>450-458</pages>
      <abstract>With increased attention to connecting science topics to real-world contexts, like issues of social justice, teachers need support to assess student progress in explaining such issues. In this work, we explore the robustness of NLP-based automatic content scoring models that provide insight into student ability to integrate their science and social justice ideas in two different environmental science contexts. We leverage encoder-only transformer models to capture the degree to which students explain a science phenomenon, understand the intersecting justice issues, and integrate their understanding of science and social justice. We developed models training on data from each of the contexts as well as from a combined dataset. We found that the models developed in one context generate educationally useful scores in the other context. The model trained on the combined dataset performed as well as or better than the models trained on separate datasets in most cases. Quadratic weighted kappas demonstrate that these models are above threshold for use in classrooms.</abstract>
      <url hash="a8205f25">2024.bea-1.37</url>
      <bibkey>bradford-etal-2024-building</bibkey>
    </paper>
    <paper id="38">
      <title>From Miscue to Evidence of Difficulty: Analysis of Automatically Detected Miscues in Oral Reading for Feedback Potential</title>
      <author><first>Beata</first><last>Beigman Klebanov</last><affiliation>Educational Testing Service</affiliation></author>
      <author><first>Michael</first><last>Suhan</last><affiliation>Educational Testing Service</affiliation></author>
      <author><first>Tenaha</first><last>O’Reilly</last><affiliation>Educational Testing Service</affiliation></author>
      <author><first>Zuowei</first><last>Wang</last><affiliation>Educational Testing Service</affiliation></author>
      <pages>459-469</pages>
      <abstract>This research is situated in the space between an existing NLP capability and its use(s) in an educational context. We analyze oral reading data collected with a deployed automated speech analysis software and consider how the results of automated speech analysis can be interpreted and used to inform the ideation and design of a new feature – feedback to learners and teachers. Our analysis shows how the details of the system’s performance and the details of the context of use both significantly impact the ideation process.</abstract>
      <url hash="af60c20f">2024.bea-1.38</url>
      <bibkey>beigman-klebanov-etal-2024-miscue</bibkey>
    </paper>
    <paper id="39">
      <title>Findings from the First Shared Task on Automated Prediction of Difficulty and Response Time for Multiple-Choice Questions</title>
      <author><first>Victoria</first><last>Yaneva</last><affiliation>National Board of Medical Examiners</affiliation></author>
      <author><first>Kai</first><last>North</last><affiliation>George Mason University</affiliation></author>
      <author><first>Peter</first><last>Baldwin</last><affiliation>NBME</affiliation></author>
      <author><first>Le An</first><last>Ha</last><affiliation>Ho Chi Minh City University of Foreign Languages, Vietnam</affiliation></author>
      <author><first>Saed</first><last>Rezayi</last><affiliation>National Board of Medical Examiners</affiliation></author>
      <author><first>Yiyun</first><last>Zhou</last><affiliation>NBME</affiliation></author>
      <author><first>Sagnik</first><last>Ray Choudhury</last><affiliation>NBME</affiliation></author>
      <author><first>Polina</first><last>Harik</last><affiliation>NBME</affiliation></author>
      <author><first>Brian</first><last>Clauser</last><affiliation>NBME</affiliation></author>
      <pages>470-482</pages>
      <abstract>This paper reports findings from the First Shared Task on Automated Prediction of Difficulty and Response Time for Multiple-Choice Questions. The task was organized as part of the 19th Workshop on Innovative Use of NLP for Building Educational Applications (BEA’24), held in conjunction with NAACL 2024, and called upon the research community to contribute solutions to the problem of modeling difficulty and response time for clinical multiple-choice questions (MCQs). A set of 667 previously used and now retired MCQs from the United States Medical Licensing Examination (USMLE®) and their corresponding difficulties and mean response times were made available for experimentation. A total of 17 teams submitted solutions and 12 teams submitted system report papers describing their approaches. This paper summarizes the findings from the shared task and analyzes the main approaches proposed by the participants.</abstract>
      <url hash="2a0002ba">2024.bea-1.39</url>
      <bibkey>yaneva-etal-2024-findings</bibkey>
    </paper>
    <paper id="40">
      <title>Predicting Item Difficulty and Item Response Time with Scalar-mixed Transformer Encoder Models and Rational Network Regression Heads</title>
      <author><first>Sebastian</first><last>Gombert</last><affiliation>DIPF | Leibniz Institute for Research and Information in Education</affiliation></author>
      <author><first>Lukas</first><last>Menzel</last><affiliation>studiumdigitale, Goethe University Frankfurt</affiliation></author>
      <author><first>Daniele</first><last>Di Mitri</last><affiliation>DIPF | Leibniz Institute for Research and Information in Education</affiliation></author>
      <author><first>Hendrik</first><last>Drachsler</last><affiliation>1) DIPF | Leibniz Institute for Research and Information in Education 2) Department of Computer Science and studiumdigitale, Goethe University Frankfurt 3) Department of Online Learning and Instruction, Open University, Heerlen, Netherlands</affiliation></author>
      <pages>483-492</pages>
      <abstract>This paper describes a contribution to the BEA 2024 Shared Task on Automated Prediction of Item Difficulty and Response Time. The participants in this shared task are to develop models for predicting the difficulty and response time of multiple-choice items in the medical field. These items were taken from the United States Medical Licensing Examination® (USMLE®), a high-stakes medical exam. For this purpose, we evaluated multiple BERT-like pre-trained transformer encoder models, which we combined with Scalar Mixing and two custom 2-layer classification heads using learnable Rational Activations as an activation function, each for predicting one of the two variables of interest in a multi-task setup. Our best models placed first out of 43 for predicting item difficulty and fifth out of 34 for predicting Item Response Time.</abstract>
      <url hash="5464d0c7">2024.bea-1.40</url>
      <bibkey>gombert-etal-2024-predicting</bibkey>
    </paper>
    <paper id="41">
      <title><fixed-case>U</fixed-case>nibuc<fixed-case>LLM</fixed-case>: Harnessing <fixed-case>LLM</fixed-case>s for Automated Prediction of Item Difficulty and Response Time for Multiple-Choice Questions</title>
      <author><first>Ana-Cristina</first><last>Rogoz</last><affiliation>University of Bucharest</affiliation></author>
      <author><first>Radu Tudor</first><last>Ionescu</last><affiliation>University of Bucharest</affiliation></author>
      <pages>493-502</pages>
      <abstract>This work explores a novel data augmentation method based on Large Language Models (LLMs) for predicting item difficulty and response time of retired USMLE Multiple-Choice Questions (MCQs) in the BEA 2024 Shared Task. Our approach is based on augmenting the dataset with answers from zero-shot LLMs (Falcon, Meditron, Mistral) and employing transformer-based models based on six alternative feature combinations. The results suggest that predicting the difficulty of questions is more challenging. Notably, our top performing methods consistently include the question text, and benefit from the variability of LLM answers, highlighting the potential of LLMs for improving automated assessment in medical licensing exams. We make our code available at: https://github.com/ana-rogoz/BEA-2024.</abstract>
      <url hash="71367bb9">2024.bea-1.41</url>
      <bibkey>rogoz-ionescu-2024-unibucllm</bibkey>
    </paper>
    <paper id="42">
      <title>The <fixed-case>B</fixed-case>ritish Council submission to the <fixed-case>BEA</fixed-case> 2024 shared task</title>
      <author><first>Mariano</first><last>Felice</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Zeynep</first><last>Duran Karaoz</last><affiliation>University of Reading</affiliation></author>
      <pages>503-511</pages>
      <abstract>This paper describes our submission to the item difficulty prediction track of the BEA 2024 shared task. Our submission included the output of three systems: 1) a feature-based linear regression model, 2) a RoBERTa-based model and 3) a linear regression ensemble built on the predictions of the two previous models. Our systems ranked 7th, 8th and 5th respectively, demonstrating that simple models can achieve optimal results. A closer look at the results shows that predictions are more accurate for items in the middle of the difficulty range, with no other obvious relationships between difficulty and the accuracy of predictions.</abstract>
      <url hash="d96db2e4">2024.bea-1.42</url>
      <bibkey>felice-duran-karaoz-2024-british</bibkey>
    </paper>
    <paper id="43">
      <title><fixed-case>ITEC</fixed-case> at <fixed-case>BEA</fixed-case> 2024 Shared Task: Predicting Difficulty and Response Time of Medical Exam Questions with Statistical, Machine Learning, and Language Models</title>
      <author><first>Anaïs</first><last>Tack</last><affiliation>KU Leuven; imec; UCLouvain</affiliation></author>
      <author><first>Siem</first><last>Buseyne</last><affiliation>KU Leuven</affiliation></author>
      <author><first>Changsheng</first><last>Chen</last><affiliation>KU Leuven</affiliation></author>
      <author><first>Robbe</first><last>D’hondt</last><affiliation>KU Leuven</affiliation></author>
      <author><first>Michiel</first><last>De Vrindt</last><affiliation>KU Leuven</affiliation></author>
      <author><first>Alireza</first><last>Gharahighehi</last><affiliation>KU Leuven</affiliation></author>
      <author><first>Sameh</first><last>Metwaly</last><affiliation>KU Leuven</affiliation></author>
      <author><first>Felipe Kenji</first><last>Nakano</last><affiliation>KU Leuven</affiliation></author>
      <author><first>Ann-Sophie</first><last>Noreillie</last><affiliation>KU Leuven</affiliation></author>
      <pages>512-521</pages>
      <abstract>This paper presents the results of our participation in the BEA 2024 shared task on the automated prediction of item difficulty and item response time (APIDIRT), hosted by the NBME (National Board of Medical Examiners). During this task, practice multiple-choice questions from the United States Medical Licensing Examination® (USMLE®) were shared, and research teams were tasked with devising systems capable of predicting the difficulty and average response time for new exam questions.Our team, part of the interdisciplinary itec research group, participated in the task. We extracted linguistic features and clinical embeddings from question items and tested various modeling techniques, including statistical regression, machine learning, language models, and ensemble methods. Surprisingly, simplermodels such as Lasso and random forest regression, utilizing principal component features from linguistic and clinical embeddings, outperformed more complex models. In the competition, our random forest model ranked 4th out of 43 submissions for difficulty prediction, while the Lasso model secured the 2nd position out of 34 submissions for response time prediction. Further analysis suggests that had we submitted the Lasso model for difficulty prediction, we would have achieved an even higher ranking. We also observed that predicting response time is easier than predicting difficulty, with features such as item length, type, exam step, and analytical thinking influencing response time prediction more significantly.</abstract>
      <url hash="08788a7e">2024.bea-1.43</url>
      <bibkey>tack-etal-2024-itec</bibkey>
    </paper>
    <paper id="44">
      <title>Item Difficulty and Response Time Prediction with Large Language Models: An Empirical Analysis of <fixed-case>USMLE</fixed-case> Items</title>
      <author><first>Okan</first><last>Bulut</last><affiliation>University of Alberta</affiliation></author>
      <author><first>Guher</first><last>Gorgun</last><affiliation>University of Alberta</affiliation></author>
      <author><first>Bin</first><last>Tan</last><affiliation>University of Alberta</affiliation></author>
      <pages>522-527</pages>
      <abstract>This paper summarizes our methodology and results for the BEA 2024 Shared Task. This competition focused on predicting item difficulty and response time for retired multiple-choice items from the United States Medical Licensing Examination® (USMLE®). We extracted linguistic features from the item stem and response options using multiple methods, including the BiomedBERT model, FastText embeddings, and Coh-Metrix. The extracted features were combined with additional features available in item metadata (e.g., item type) to predict item difficulty and average response time. The results showed that the BiomedBERT model was the most effective in predicting item difficulty, while the fine-tuned model based on FastText word embeddings was the best model for predicting response time.</abstract>
      <url hash="bf8c862b">2024.bea-1.44</url>
      <bibkey>bulut-etal-2024-item</bibkey>
    </paper>
    <paper id="45">
      <title>Utilizing Machine Learning to Predict Question Difficulty and Response Time for Enhanced Test Construction</title>
      <author><first>Rishikesh</first><last>Fulari</last><affiliation>Purdue University</affiliation></author>
      <author><first>Jonathan</first><last>Rusert</last><affiliation>Purdue University</affiliation></author>
      <pages>528-533</pages>
      <abstract>In this paper, we present the details of ourcontribution to the BEA Shared Task on Automated Prediction of Item Difficulty and Response Time. Participants in this collaborativeeffort are tasked with developing models to predict the difficulty and response time of multiplechoice items within the medical domain. Theseitems are sourced from the United States Medical Licensing Examination® (USMLE®), asignificant medical assessment. In order toachieve this, we experimented with two featurization techniques, one using lingusitic features and the other using embeddings generated by BERT fine-tuned over MS-MARCOdataset. Further, we tried several different machine learning models such as Linear Regression, Decision Trees, KNN and Boosting models such as XGBoost and GBDT. We found thatout of all the models we experimented withRandom Forest Regressor trained on Linguisticfeatures gave the least root mean squared error.</abstract>
      <url hash="2dc0edc5">2024.bea-1.45</url>
      <bibkey>fulari-rusert-2024-utilizing</bibkey>
    </paper>
    <paper id="46">
      <title>Leveraging Physical and Semantic Features of text item for Difficulty and Response Time Prediction of <fixed-case>USMLE</fixed-case> Questions</title>
      <author><first>Gummuluri</first><last>Venkata Ravi Ram</last><affiliation>National Institute of Technology Karnataka, Surathkal</affiliation></author>
      <author><first>Ashinee</first><last>Kesanam</last><affiliation>National Institute of Technology Karnataka Surathkal</affiliation></author>
      <author><first>Anand Kumar</first><last>M</last><affiliation>National Institute of Technology Karnataka</affiliation></author>
      <pages>534-541</pages>
      <abstract>This paper presents our system developed for the Shared Task on Automated Prediction of Item Difficulty and Item Response Time for USMLE questions, organized by the Association for Computational Linguistics (ACL) Special Interest Group for building Educational Applications (BEA SIGEDU). The Shared Task, held as a workshop at the North American Chapter of the Association for Computational Linguistics (NAACL) 2024 conference, aimed to advance the state-of-the-art in predicting item characteristics directly from item text, with implications for the fairness and validity of standardized exams. We compared various methods ranging from BERT for regression to Random forest, Gradient Boosting(GB), Linear Regression, Support Vector Regressor (SVR), k-nearest neighbours (KNN) Regressor, MultiLayer Perceptron(MLP) to custom-ANN using BioBERT and Word2Vec embeddings and provided inferences on which performed better. This paper also explains the importance of data augmentation to balance the data in order to get better results. We also proposed five hypotheses regarding factors impacting difficulty and response time for a question and also verified it thereby helping researchers to derive meaningful numerical attributes for accurate prediction. We achieved a RSME score of 0.315 for Difficulty prediction and 26.945 for Response Time.</abstract>
      <url hash="fabb2a20">2024.bea-1.46</url>
      <bibkey>venkata-ravi-ram-etal-2024-leveraging</bibkey>
    </paper>
    <paper id="47">
      <title><fixed-case>UPN</fixed-case>-<fixed-case>ICC</fixed-case> at <fixed-case>BEA</fixed-case> 2024 Shared Task: Leveraging <fixed-case>LLM</fixed-case>s for Multiple-Choice Questions Difficulty Prediction</title>
      <author><first>George</first><last>Duenas</last><affiliation>Universidad Pedagogica Nacional</affiliation></author>
      <author><first>Sergio</first><last>Jimenez</last><affiliation>Instituto Caro y Cuervo</affiliation></author>
      <author><first>Geral</first><last>Mateus Ferro</last><affiliation>Universidad Pedagogica Nacional</affiliation></author>
      <pages>542-550</pages>
      <abstract>We describe the second-best run for the shared task on predicting the difficulty of Multi-Choice Questions (MCQs) in the medical domain. Our approach leverages prompting Large Language Models (LLMs). Rather than straightforwardly querying difficulty, we simulate medical candidate’s responses to questions across various scenarios. For this, more than 10,000 prompts were required for the 467 training questions and the 200 test questions. From the answers to these prompts, we extracted a set of features which we combined with a Ridge Regression to which we only adjusted the regularization parameter using the training set. Our motivation stems from the belief that MCQ difficulty is influenced more by the respondent population than by item-specific content features. We conclude that the approach is promising and has the potential to improve other item-based systems on this task, which turned out to be extremely challenging and has ample room for future improvement.</abstract>
      <url hash="dfa90dc1">2024.bea-1.47</url>
      <bibkey>duenas-etal-2024-upn</bibkey>
    </paper>
    <paper id="48">
      <title>Using Machine Learning to Predict Item Difficulty and Response Time in Medical Tests</title>
      <author><first>Mehrdad</first><last>Yousefpoori-Naeim</last><affiliation>University of Alberta</affiliation></author>
      <author><first>Shayan</first><last>Zargari</last><affiliation>University of Alberta</affiliation></author>
      <author><first>Zahra</first><last>Hatami</last><affiliation>Epita Engineering School</affiliation></author>
      <pages>551-560</pages>
      <abstract>Prior knowledge of item characteristics, such as difficulty and response time, without pretesting items can substantially save time and cost in high-standard test development. Using a variety of machine learning (ML) algorithms, the present study explored several (non-)linguistic features (such as Coh-Metrix indices) along with MPNet word embeddings to predict the difficulty and response time of a sample of medical test items. In both prediction tasks, the contribution of embeddings to models already containing other features was found to be extremely limited. Moreover, a comparison of feature importance scores across the two prediction tasks revealed that cohesion-based features were the strongest predictors of difficulty, while the prediction of response time was primarily dependent on length-related features.</abstract>
      <url hash="b93717f6">2024.bea-1.48</url>
      <bibkey>yousefpoori-naeim-etal-2024-using</bibkey>
    </paper>
    <paper id="49">
      <title>Large Language Model-based Pipeline for Item Difficulty and Response Time Estimation for Educational Assessments</title>
      <author><first>Hariram</first><last>Veeramani</last><affiliation>UCLA</affiliation></author>
      <author><first>Surendrabikram</first><last>Thapa</last><affiliation>Virginia Tech</affiliation></author>
      <author><first>Natarajan Balaji</first><last>Shankar</last><affiliation>UCLA</affiliation></author>
      <author><first>Abeer</first><last>Alwan</last><affiliation>UCLA</affiliation></author>
      <pages>561-566</pages>
      <abstract>This work presents a novel framework for the automated prediction of item difficulty and response time within educational assessments. Utilizing data from the BEA 2024 Shared Task, we integrate Named Entity Recognition, Semantic Role Labeling, and linguistic features to prompt a Large Language Model (LLM). Our best approach achieves an RMSE of 0.308 for item difficulty and 27.474 for response time prediction, improving on the provided baseline. The framework’s adaptability is demonstrated on audio recordings of 3rd-8th graders from the Atlanta, Georgia area responding to the Test of Narrative Language. These results highlight the framework’s potential to enhance test development efficiency.</abstract>
      <url hash="53278003">2024.bea-1.49</url>
      <bibkey>veeramani-etal-2024-large</bibkey>
    </paper>
    <paper id="50">
      <title><fixed-case>UNED</fixed-case> team at <fixed-case>BEA</fixed-case> 2024 Shared Task: Testing different Input Formats for predicting Item Difficulty and Response Time in Medical Exams</title>
      <author><first>Alvaro</first><last>Rodrigo</last><affiliation>NLP and IR group at UNED</affiliation></author>
      <author><first>Sergio</first><last>Moreno-Álvarez</last><affiliation>UNED</affiliation></author>
      <author><first>Anselmo</first><last>Peñas</last><affiliation>NLP &amp; IR Group, UNED</affiliation></author>
      <pages>567-570</pages>
      <abstract>This paper presents the description and primary outcomes of our team’s participation in the BEA 2024 shared task. Our primary exploration involved employing transformer-based systems, particularly BERT models, due to their suitability for Natural Language Processing tasks and efficiency with computational resources. We experimented with various input formats, including concatenating all text elements and incorporating only the clinical case. Surprisingly, our results revealed different impacts on predicting difficulty versus response time, with the former favoring clinical text only and the latter benefiting from including the correct answer. Despite moderate performance in difficulty prediction, our models excelled in response time prediction, ranking highest among all participants. This study lays the groundwork for future investigations into more complex approaches and configurations, aiming to advance the automatic prediction of exam difficulty and response time.</abstract>
      <url hash="df815813">2024.bea-1.50</url>
      <bibkey>rodrigo-etal-2024-uned</bibkey>
    </paper>
    <paper id="51">
      <title>The <fixed-case>BEA</fixed-case> 2024 Shared Task on the Multilingual Lexical Simplification Pipeline</title>
      <author><first>Matthew</first><last>Shardlow</last><affiliation>Manchester Metropolitan University</affiliation></author>
      <author><first>Fernando</first><last>Alva-Manchego</last><affiliation>Cardiff University</affiliation></author>
      <author><first>Riza</first><last>Batista-Navarro</last><affiliation>Department of Computer Science, The University of Manchester</affiliation></author>
      <author><first>Stefan</first><last>Bott</last><affiliation>Universitat Pompe Fabra</affiliation></author>
      <author><first>Saul</first><last>Calderon Ramirez</last><affiliation>Tecnológico de Costa Rica</affiliation></author>
      <author><first>Rémi</first><last>Cardon</last><affiliation>CENTAL, ILC, Université Catholique de Louvain</affiliation></author>
      <author><first>Thomas</first><last>François</last><affiliation>UCLouvain, CENTAL</affiliation></author>
      <author><first>Akio</first><last>Hayakawa</last><affiliation>Universitat Pompeu Fabra</affiliation></author>
      <author><first>Andrea</first><last>Horbach</last><affiliation>Universität Hildesheim</affiliation></author>
      <author><first>Anna</first><last>Huelsing</last><affiliation>University of Hildesheim</affiliation></author>
      <author><first>Yusuke</first><last>Ide</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Joseph Marvin</first><last>Imperial</last><affiliation>University of Bath</affiliation></author>
      <author><first>Adam</first><last>Nohejl</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Kai</first><last>North</last><affiliation>George Mason University</affiliation></author>
      <author><first>Laura</first><last>Occhipinti</last><affiliation>University of Bologna</affiliation></author>
      <author><first>Nelson</first><last>Peréz Rojas</last><affiliation>Tecnológico de Costa Rica</affiliation></author>
      <author><first>Nishat</first><last>Raihan</last><affiliation>George Mason University</affiliation></author>
      <author><first>Tharindu</first><last>Ranasinghe</last><affiliation>Aston University</affiliation></author>
      <author><first>Martin</first><last>Solis Salazar</last><affiliation>Tecnológico de Costa Rica</affiliation></author>
      <author><first>Sanja</first><last>Štajner</last><affiliation>Independent Researcher</affiliation></author>
      <author><first>Marcos</first><last>Zampieri</last><affiliation>George Mason University</affiliation></author>
      <author><first>Horacio</first><last>Saggion</last><affiliation>Universitat Pompeu Fabra</affiliation></author>
      <pages>571-589</pages>
      <abstract>We report the findings of the 2024 Multilingual Lexical Simplification Pipeline shared task. We released a new dataset comprising 5,927 instances of lexical complexity prediction and lexical simplification on common contexts across 10 languages, split into trial (300) and test (5,627). 10 teams participated across 2 tracks and 10 languages with 233 runs evaluated across all systems. Five teams participated in all languages for the lexical complexity prediction task and 4 teams participated in all languages for the lexical simplification task. Teams employed a range of strategies, making use of open and closed source large language models for lexical simplification, as well as feature-based approaches for lexical complexity prediction. The highest scoring team on the combined multilingual data was able to obtain a Pearson’s correlation of 0.6241 and an ACC@1@Top1 of 0.3772, both demonstrating that there is still room for improvement on two difficult sub-tasks of the lexical simplification pipeline.</abstract>
      <url hash="3a258e99">2024.bea-1.51</url>
      <bibkey>shardlow-etal-2024-bea</bibkey>
    </paper>
    <paper id="52">
      <title><fixed-case>TMU</fixed-case>-<fixed-case>HIT</fixed-case> at <fixed-case>MLSP</fixed-case> 2024: How Well Can <fixed-case>GPT</fixed-case>-4 Tackle Multilingual Lexical Simplification?</title>
      <author><first>Taisei</first><last>Enomoto</last><affiliation>Tokyo Metropolitan University</affiliation></author>
      <author><first>Hwichan</first><last>Kim</last><affiliation>Tokyo Metropolitan University</affiliation></author>
      <author><first>Tosho</first><last>Hirasawa</last><affiliation>Tokyo Metropolitan University</affiliation></author>
      <author><first>Yoshinari</first><last>Nagai</last><affiliation>University of Tokyo Metropolitan</affiliation></author>
      <author><first>Ayako</first><last>Sato</last><affiliation>tokyo metropolitan university</affiliation></author>
      <author><first>Kyotaro</first><last>Nakajima</last><affiliation>TMU</affiliation></author>
      <author><first>Mamoru</first><last>Komachi</last><affiliation>Hitotsubashi University</affiliation></author>
      <pages>590-598</pages>
      <abstract>Lexical simplification (LS) is a process of replacing complex words with simpler alternatives to help readers understand sentences seamlessly. This process is divided into two primary subtasks: assessing word complexities and replacing high-complexity words with simpler alternatives. Employing task-specific supervised data to train models is a prevalent strategy for addressing these subtasks. However, such approach cannot be employed for low-resource languages. Therefore, this paper introduces a multilingual LS pipeline system that does not rely on supervised data. Specifically, we have developed systems based on GPT-4 for each subtask. Our systems demonstrated top-class performance on both tasks in many languages. The results indicate that GPT-4 can effectively assess lexical complexity and simplify complex words in a multilingual context with high quality.</abstract>
      <url hash="98075e25">2024.bea-1.52</url>
      <bibkey>enomoto-etal-2024-tmu</bibkey>
    </paper>
    <paper id="53">
      <title><fixed-case>ANU</fixed-case> at <fixed-case>MLSP</fixed-case>-2024: Prompt-based Lexical Simplification for <fixed-case>E</fixed-case>nglish and <fixed-case>S</fixed-case>inhala</title>
      <author><first>Sandaru</first><last>Seneviratne</last><affiliation>The Australian National University</affiliation></author>
      <author><first>Hanna</first><last>Suominen</last><affiliation>The Australian National University and University of Turku</affiliation></author>
      <pages>599-604</pages>
      <abstract>Lexical simplification, the process of simplifying complex content in text without any modifications to the syntactical structure of text, plays a crucial role in enhancing comprehension and accessibility. This paper presents an approach to lexical simplification that relies on the capabilities of generative Artificial Intelligence (AI) models to predict the complexity of words and substitute complex words with simpler alternatives. Early lexical simplification methods predominantly relied on rule-based approaches, transitioning gradually to machine learning and deep learning techniques, leveraging contextual embeddings from large language models. However, the the emergence of generative AI models revolutionized the landscape of natural language processing, including lexical simplification. In this study, we proposed a straightforward yet effective method that employs generative AI models for both predicting lexical complexity and generating appropriate substitutions. To predict lexical complexity, we adopted three distinct types of prompt templates, while for lexical substitution, we employed three prompt templates alongside an ensemble approach. Extending our experimentation to include both English and Sinhala data, our approach demonstrated comparable performance across both languages, with particular strengths in lexical substitution.</abstract>
      <url hash="1e498536">2024.bea-1.53</url>
      <bibkey>seneviratne-suominen-2024-anu</bibkey>
    </paper>
    <paper id="54">
      <title><fixed-case>ISEP</fixed-case>_<fixed-case>P</fixed-case>residency_<fixed-case>U</fixed-case>niversity at <fixed-case>MLSP</fixed-case> 2024 Shared Task: Using <fixed-case>GPT</fixed-case>-3.5 to Generate Substitutes for Lexical Simplification</title>
      <author><first>Benjamin</first><last>Dutilleul</last><affiliation>ISEP</affiliation></author>
      <author><first>Mathis</first><last>Debaillon</last><affiliation>ISEP</affiliation></author>
      <author><first>Sandeep</first><last>Mathias</last><affiliation>Presidency University</affiliation></author>
      <pages>605-609</pages>
      <abstract>Lexical substitute generation is a task where we generate substitutes for a given word to fit in the required context. It is one of the main steps for automatic lexical simplifcation. In this paper, we introduce an automatic lexical simplification system using the GPT-3 large language model. The system generates simplified candidate substitutions for complex words to aid readability and comprehension for the reader. The paper describes the system that we submitted for the Multilingual Lexical Simplification Pipeline Shared Task at the 2024 BEA Workshop. During the shared task, we experimented with Catalan, English, French, Italian, Portuguese, and German for the Lexical Simplification Shared Task. We achieved the best results in Catalan and Portuguese, and were runners-up in English, French and Italian. To further research in this domain, we also release our code upon acceptance of the paper.</abstract>
      <url hash="3e4a558a">2024.bea-1.54</url>
      <bibkey>dutilleul-etal-2024-isep</bibkey>
    </paper>
    <paper id="55">
      <title>Archaeology at MLSP 2024: Machine Translation for Lexical Complexity Prediction and Lexical Simplification</title>
      <author><first>Petru</first><last>Cristea</last><affiliation>University of Bucharest</affiliation></author>
      <author><first>Sergiu</first><last>Nisioi</last><affiliation>Human Language Technologies Research Center, University of Bucharest</affiliation></author>
      <pages>610-617</pages>
      <abstract>We present the submissions of team Archaeology for the Lexical Simplification and Lexical Complexity Prediction Shared Tasks at BEA2024. Our approach for this shared task consists in creating two pipelines for generating lexical substitutions and estimating the complexity: one using machine translation texts into English and one using the original language.For the LCP subtask, our xgb regressor is trained with engineered features (based primarily on English language resources) and shallow word structure features. For the LS subtask we use a locally-executed quantized LLM to generate candidates and sort them by complexity score computed using the pipeline designed for LCP.These pipelines provide distinct perspectives on the lexical simplification process, offering insights into the efficacy and limitations of employing Machine Translation versus direct processing on the original language data.</abstract>
      <url hash="92eec768">2024.bea-1.55</url>
      <bibkey>cristea-nisioi-2024-machine</bibkey>
    </paper>
    <paper id="56">
      <title><fixed-case>RETUYT</fixed-case>-<fixed-case>INCO</fixed-case> at <fixed-case>MLSP</fixed-case> 2024: Experiments on Language Simplification using Embeddings, Classifiers and Large Language Models</title>
      <author><first>Ignacio</first><last>Sastre</last><affiliation>Universidad de la República</affiliation></author>
      <author><first>Leandro</first><last>Alfonso</last><affiliation>Universidad de la República</affiliation></author>
      <author><first>Facundo</first><last>Fleitas</last><affiliation>Universidad de la República</affiliation></author>
      <author><first>Federico</first><last>Gil</last><affiliation>Universidad de la República</affiliation></author>
      <author><first>Andrés</first><last>Lucas</last><affiliation>Universidad de la República</affiliation></author>
      <author><first>Tomás</first><last>Spoturno</last><affiliation>Universidad de la República</affiliation></author>
      <author><first>Santiago</first><last>Góngora</last><affiliation>Universidad de la República</affiliation></author>
      <author><first>Aiala</first><last>Rosá</last><affiliation>Instituto de Computación, Facultad de Ingeniería, Universidad de la República</affiliation></author>
      <author><first>Luis</first><last>Chiruzzo</last><affiliation>Universidad de la Republica</affiliation></author>
      <pages>618-626</pages>
      <abstract>In this paper we present the participation of the RETUYT-INCO team at the BEA-MLSP 2024 shared task. We followed different approaches, from Multilayer Perceptron models with word embeddings to Large Language Models fine-tuned on different datasets: already existing, crowd-annotated, and synthetic.Our best models are based on fine-tuning Mistral-7B, either with a manually annotated dataset or with synthetic data.</abstract>
      <url hash="a10b6175">2024.bea-1.56</url>
      <bibkey>sastre-etal-2024-retuyt</bibkey>
    </paper>
    <paper id="57">
      <title><fixed-case>GMU</fixed-case> at <fixed-case>MLSP</fixed-case> 2024: Multilingual Lexical Simplification with Transformer Models</title>
      <author><first>Dhiman</first><last>Goswami</last><affiliation>George Mason University</affiliation></author>
      <author><first>Kai</first><last>North</last><affiliation>George Mason University</affiliation></author>
      <author><first>Marcos</first><last>Zampieri</last><affiliation>George Mason University</affiliation></author>
      <pages>627-634</pages>
      <abstract>This paper presents GMU’s submission to the Multilingual Lexical Simplification Pipeline (MLSP) shared task at the BEA workshop 2024. The task includes Lexical Complexity Prediction (LCP) and Lexical Simplification (LS) sub-tasks across 10 languages. Our submissions achieved rankings ranging from 1st to 5th in LCP and from 1st to 3rd in LS. Our best performing approach for LCP is a weighted ensemble based on Pearson correlation of language specific transformer models trained on all languages combined. For LS, GPT4-turbo zero-shot prompting achieved the best performance.</abstract>
      <url hash="6e9aef53">2024.bea-1.57</url>
      <bibkey>goswami-etal-2024-gmu</bibkey>
    </paper>
    <paper id="58">
      <title><fixed-case>ITEC</fixed-case> at <fixed-case>MLSP</fixed-case> 2024: Transferring Predictions of Lexical Difficulty from Non-Native Readers</title>
      <author><first>Anaïs</first><last>Tack</last><affiliation>KU Leuven; imec; UCLouvain</affiliation></author>
      <pages>635-639</pages>
      <abstract>This paper presents the results of our team’s participation in the BEA 2024 shared task on the multilingual lexical simplification pipeline (MLSP; Shardlow et al., 2024). During the task, organizers supplied data that combined two components of the simplification pipeline: lexical complexity prediction and lexical substitution. This dataset encompassed ten languages, including French. Given the absence of dedicated training data, teams were challenged with employing systems trained on pre-existing resources and evaluating their performance on unexplored test data.Our team contributed to the task using previously developed models for predicting lexical difficulty in French (Tack, 2021). These models were built on deep learning architectures, adding to our participation in the CWI 2018 shared task (De Hertog and Tack, 2018). The training dataset comprised 262,054 binary decision annotations, capturing perceived lexical difficulty, collected from a sample of 56 non-native French readers. Two pre-trained neural logistic models were used: (1) a model for predicting difficulty for words within their sentence context, and (2) a model for predicting difficulty for isolated words.The findings revealed that despite being trained for a distinct prediction task (as indicated by a negative R2 fit), transferring the logistic predictions of lexical difficulty to continuous scores of lexical complexity exhibited a positive correlation. Specifically, the results indicated that isolated predictions exhibited a higher correlation (r = .36) compared to contextualized predictions (r = .33). Moreover, isolated predictions demonstrated a remarkably higher Spearman rank correlation (ρ = .50) than contextualized predictions (ρ = .35). These results align with earlier observations by Tack (2021), suggesting that the ground truth primarily captures more lexical access difficulties than word-to-context integration problems.</abstract>
      <url hash="48c9a5fb">2024.bea-1.58</url>
      <bibkey>tack-2024-itec</bibkey>
    </paper>
  </volume>
</collection>
