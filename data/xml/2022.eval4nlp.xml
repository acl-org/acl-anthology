<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.eval4nlp">
  <volume id="1" ingest-date="2022-11-21">
    <meta>
      <booktitle>Proceedings of the 3rd Workshop on Evaluation and Comparison of NLP Systems</booktitle>
      <editor><first>Daniel</first><last>Deutsch</last></editor>
      <editor><first>Can</first><last>Udomcharoenchaikit</last></editor>
      <editor><first>Juri</first><last>Opitz</last></editor>
      <editor><first>Yang</first><last>Gao</last></editor>
      <editor><first>Marina</first><last>Fomicheva</last></editor>
      <editor><first>Steffen</first><last>Eger</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>November</month>
      <year>2022</year>
      <url hash="848df4b6">2022.eval4nlp-1</url>
      <venue>eval4nlp</venue>
    </meta>
    <frontmatter>
      <url hash="7c2427e6">2022.eval4nlp-1.0</url>
      <bibkey>eval4nlp-2022-evaluation</bibkey>
    </frontmatter>
    <paper id="1">
      <title>A <fixed-case>J</fixed-case>apanese Corpus of Many Specialized Domains for Word Segmentation and Part-of-Speech Tagging</title>
      <author><first>Shohei</first><last>Higashiyama</last></author>
      <author><first>Masao</first><last>Ideuchi</last></author>
      <author><first>Masao</first><last>Utiyama</last></author>
      <author><first>Yoshiaki</first><last>Oida</last></author>
      <author><first>Eiichiro</first><last>Sumita</last></author>
      <pages>1–10</pages>
      <url hash="728636c8">2022.eval4nlp-1.1</url>
      <doi>10.18653/v1/2022.eval4nlp-1.1</doi>
      <attachment type="SupplementaryMaterial" hash="db8be39c">2022.eval4nlp-1.1.SupplementaryMaterial.zip</attachment>
      <bibkey>higashiyama-etal-2022-japanese</bibkey>
    </paper>
    <paper id="2">
      <title>Assessing Resource-Performance Trade-off of Natural Language Models using Data Envelopment Analysis</title>
      <author><first>Shohei</first><last>Zhou</last></author>
      <author><first>Alisha</first><last>Zachariah</last></author>
      <author><first>Devin</first><last>Conathan</last></author>
      <author><first>Jeffery</first><last>Kline</last></author>
      <pages>11–20</pages>
      <url hash="96aae5d0">2022.eval4nlp-1.2</url>
      <doi>10.18653/v1/2022.eval4nlp-1.2</doi>
      <bibkey>zhou-etal-2022-assessing</bibkey>
    </paper>
    <paper id="3">
      <title>From <fixed-case>COMET</fixed-case> to <fixed-case>COMES</fixed-case> – Can Summary Evaluation Benefit from Translation Evaluation?</title>
      <author><first>Mateusz</first><last>Krubiński</last></author>
      <author><first>Pavel</first><last>Pecina</last></author>
      <pages>21–31</pages>
      <url hash="fb4b4d3b">2022.eval4nlp-1.3</url>
      <doi>10.18653/v1/2022.eval4nlp-1.3</doi>
      <bibkey>krubinski-pecina-2022-comet</bibkey>
    </paper>
    <paper id="4">
      <title>Better <fixed-case>S</fixed-case>match = Better Parser? <fixed-case>AMR</fixed-case> evaluation is not so simple anymore</title>
      <author><first>Juri</first><last>Opitz</last></author>
      <author><first>Anette</first><last>Frank</last></author>
      <pages>32–43</pages>
      <url hash="0ae2cc44">2022.eval4nlp-1.4</url>
      <doi>10.18653/v1/2022.eval4nlp-1.4</doi>
      <bibkey>opitz-frank-2022-better</bibkey>
    </paper>
    <paper id="5">
      <title><fixed-case>GLARE</fixed-case>: Generative Left-to-right <fixed-case>A</fixed-case>dversa<fixed-case>R</fixed-case>ial Examples</title>
      <author><first>Ryan Andrew</first><last>Chi</last></author>
      <author><first>Nathan</first><last>Kim</last></author>
      <author><first>Patrick</first><last>Liu</last></author>
      <author><first>Zander</first><last>Lack</last></author>
      <author><first>Ethan A</first><last>Chi</last></author>
      <pages>44–50</pages>
      <url hash="f830674b">2022.eval4nlp-1.5</url>
      <doi>10.18653/v1/2022.eval4nlp-1.5</doi>
      <bibkey>chi-etal-2022-glare</bibkey>
    </paper>
    <paper id="6">
      <title>Random Text Perturbations Work, but not Always</title>
      <author><first>Zhengxiang</first><last>Wang</last></author>
      <pages>51–57</pages>
      <url hash="4830e8b9">2022.eval4nlp-1.6</url>
      <doi>10.18653/v1/2022.eval4nlp-1.6</doi>
      <attachment type="SupplementaryMaterial" hash="bfae400a">2022.eval4nlp-1.6.SupplementaryMaterial.zip</attachment>
      <bibkey>wang-2022-random</bibkey>
    </paper>
    <paper id="7">
      <title>A Comparative Analysis of Stance Detection Approaches and Datasets</title>
      <author><first>Parush</first><last>Gera</last></author>
      <author><first>Tempestt</first><last>Neal</last></author>
      <pages>58–69</pages>
      <url hash="33cdc3a4">2022.eval4nlp-1.7</url>
      <doi>10.18653/v1/2022.eval4nlp-1.7</doi>
      <bibkey>gera-neal-2022-comparative</bibkey>
    </paper>
    <paper id="8">
      <title>Why is sentence similarity benchmark not predictive of application-oriented task performance?</title>
      <author><first>Kaori</first><last>Abe</last></author>
      <author><first>Sho</first><last>Yokoi</last></author>
      <author><first>Tomoyuki</first><last>Kajiwara</last></author>
      <author><first>Kentaro</first><last>Inui</last></author>
      <pages>70–87</pages>
      <url hash="2fc10377">2022.eval4nlp-1.8</url>
      <doi>10.18653/v1/2022.eval4nlp-1.8</doi>
      <attachment type="SupplementaryMaterial" hash="fd208917">2022.eval4nlp-1.8.SupplementaryMaterial.zip</attachment>
      <bibkey>abe-etal-2022-sentence</bibkey>
    </paper>
    <paper id="9">
      <title>Chat Translation Error Detection for Assisting Cross-lingual Communications</title>
      <author><first>Yunmeng</first><last>Li</last></author>
      <author><first>Jun</first><last>Suzuki</last></author>
      <author><first>Makoto</first><last>Morishita</last></author>
      <author><first>Kaori</first><last>Abe</last></author>
      <author><first>Ryoko</first><last>Tokuhisa</last></author>
      <author><first>Ana</first><last>Brassard</last></author>
      <author><first>Kentaro</first><last>Inui</last></author>
      <pages>88–95</pages>
      <url hash="05ab98a1">2022.eval4nlp-1.9</url>
      <doi>10.18653/v1/2022.eval4nlp-1.9</doi>
      <attachment type="SupplementaryMaterial" hash="17685daa">2022.eval4nlp-1.9.SupplementaryMaterial.zip</attachment>
      <bibkey>li-etal-2022-chat</bibkey>
    </paper>
    <paper id="10">
      <title>Evaluating the role of non-lexical markers in <fixed-case>GPT</fixed-case>-2’s language modeling behavior</title>
      <author><first>Roberta</first><last>Rocca</last></author>
      <author><first>Alejandro</first><last>de la Vega</last></author>
      <pages>96–102</pages>
      <url hash="f90eab57">2022.eval4nlp-1.10</url>
      <doi>10.18653/v1/2022.eval4nlp-1.10</doi>
      <bibkey>rocca-de-la-vega-2022-evaluating</bibkey>
    </paper>
    <paper id="11">
      <title>Assessing Neural Referential Form Selectors on a Realistic Multilingual Dataset</title>
      <author><first>Guanyi</first><last>Chen</last></author>
      <author><first>Fahime</first><last>Same</last></author>
      <author><first>Kees</first><last>Van Deemter</last></author>
      <pages>103–114</pages>
      <url hash="66bf8f1e">2022.eval4nlp-1.11</url>
      <doi>10.18653/v1/2022.eval4nlp-1.11</doi>
      <attachment type="SupplementaryMaterial" hash="edeb43dc">2022.eval4nlp-1.11.SupplementaryMaterial.zip</attachment>
      <bibkey>chen-etal-2022-assessing</bibkey>
    </paper>
  </volume>
</collection>
